- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Hyperparameter Tuning of Machine Learning Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的超参数调整
- en: This chapter describes how genetic algorithms can be used to improve the performance
    of **supervised machine learning** models by tuning the hyperparameters of the
    models. The chapter will start with a brief introduction to **hyperparameter tuning**
    in machine learning before describing the concept of a **grid search**. After
    introducing the Wine dataset and the adaptive boosting classifier, both of which
    will be used throughout this chapter, we will demonstrate hyperparameter tuning
    using both a conventional grid search and a genetic-algorithm-driven grid search.
    Finally, we will attempt to enhance the results we get by using a direct genetic
    algorithm approach for hyperparameter tuning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了如何通过调整模型的超参数，使用遗传算法来提高**监督学习模型**的性能。本章将首先简要介绍机器学习中的**超参数调整**，然后介绍**网格搜索**的概念。在介绍
    Wine 数据集和自适应提升分类器后，二者将在本章中反复使用，我们将展示如何通过传统的网格搜索和遗传算法驱动的网格搜索进行超参数调整。最后，我们将尝试通过直接的遗传算法方法来优化超参数调整结果，从而提升性能。
- en: 'By the end of this chapter, you will be able to do the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够完成以下任务：
- en: Demonstrate familiarity with the concept of hyperparameter tuning in machine
    learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示对机器学习中超参数调整概念的熟悉度
- en: Demonstrate familiarity with the Wine dataset and the adaptive boosting classifier
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示对 Wine 数据集和自适应提升分类器的熟悉度
- en: Enhance the performance of a classifier using a hyperparameter grid search
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用超参数网格搜索提升分类器的性能
- en: Enhance the performance of a classifier using a genetic-algorithm-driven hyperparameter
    grid search
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用遗传算法驱动的超参数网格搜索提升分类器的性能
- en: Enhance the performance of a classifier using a direct genetic algorithm approach
    for hyperparameter tuning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用直接的遗传算法方法提升分类器的性能，以进行超参数调整
- en: We will start this chapter with a quick overview of hyperparameters in machine
    learning. If you are a seasoned data scientist, feel free to skip the introductory
    section.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将以快速概述机器学习中的超参数开始。如果你是经验丰富的数据科学家，可以跳过引言部分。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using Python 3 with the following supporting libraries:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用 Python 3，并配备以下支持库：
- en: '**deap**'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**deap**'
- en: '**numpy**'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**numpy**'
- en: '**pandas**'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pandas**'
- en: '**matplotlib**'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**matplotlib**'
- en: '**seaborn**'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**seaborn**'
- en: '**scikit-learn**'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scikit-learn**'
- en: Important note
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: If you use the **requirements.txt** file we provide (see [*Chapter 3*](B20851_03.xhtml#_idTextAnchor091)),
    these libraries are already included in your environment.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用我们提供的**requirements.txt**文件（参见[*第3章*](B20851_03.xhtml#_idTextAnchor091)），这些库已经包含在你的环境中。
- en: 'In addition, we will be using the UCI Wine dataset: [https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用 UCI Wine 数据集：[https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine)
- en: 'The programs that will be used in this chapter can be found in this book’s
    GitHub repository:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的程序可以在本书的 GitHub 仓库中找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_08](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_08)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_08](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_08)'
- en: 'Check out the following video to see the code in action: [https://packt.link/OEBOd](https://packt.link/OEBOd)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，观看代码演示：[https://packt.link/OEBOd](https://packt.link/OEBOd)
- en: Hyperparameters in machine learning
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的超参数
- en: In [*Chapter 7*](B20851_07.xhtml#_idTextAnchor221), *Enhancing Machine Learning
    Models Using Feature Selection*, we described *supervised learning* as the programmatic
    process of adjusting (or tuning) the internal parameters of a model to produce
    the desired outputs in response to given inputs. To make this happen, each type
    of supervised learning model is accompanied by a learning algorithm that iteratively
    adjusts its internal parameters during the *learning* (or training) phase.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B20851_07.xhtml#_idTextAnchor221)《使用特征选择提升机器学习模型》中，我们将*监督学习*描述为调整（或调整）模型内部参数的程序化过程，以便在给定输入时产生期望的输出。为了实现这一目标，每种类型的监督学习模型都配有一个学习算法，在*学习*（或训练）阶段反复调整其内部参数。
- en: 'However, most models have another set of parameters that are set *before* the
    learning takes place. These are called **hyperparameters** and affect the way
    the learning is done. The following figure illustrates the two types of parameters:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数模型还有另一组参数是在学习发生之前设置的。这些参数被称为 **超参数**，并且它们影响学习的方式。以下图示了这两类参数：
- en: '![Figure 8.1: Hyperparameter tuning of a machine learning model](img/B20851_08_001.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1：机器学习模型的超参数调优](img/B20851_08_001.jpg)'
- en: 'Figure 8.1: Hyperparameter tuning of a machine learning model'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：机器学习模型的超参数调优
- en: Usually, the hyperparameters have default values that will take effect if we
    don’t specifically set them. For example, if we look at the `scikit-learn` library
    implementation of the **decision tree classifier** ([https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)),
    we will see several hyperparameters and their default values.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，超参数有默认值，如果我们没有特别设置，它们将会生效。例如，如果我们查看 `scikit-learn` 库中 **决策树分类器** 的实现（[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)），我们会看到几个超参数及其默认值。
- en: 'A few of these hyperparameters are described in the following table:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格描述了一些超参数：
- en: '| **Name** | **Type** | **Description** | **Default value** |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **类型** | **描述** | **默认值** |'
- en: '| `max_depth` | int | The maximum depth of the tree | None |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| `max_depth` | 整数 | 树的最大深度 | 无 |'
- en: '| `splitter` | enumerated | The strategy that’s used to choose the split at
    each best node:`{''``best'', ''random''}` | `''``best''` |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| `splitter` | 枚举型 | 用于选择每个最佳节点分裂的策略：`{''best'', ''random''}` | `''best''`
    |'
- en: '| `min_samples_split` | int or float | The minimum number of samples required
    to split an internal node | `2` |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| `min_samples_split` | 整数或浮动型 | 分裂内部节点所需的最小样本数 | `2` |'
- en: 'Table 8.1: Hyperparameters and their details'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1：超参数及其详细信息
- en: Each of these parameters affects the way the decision tree is constructed during
    the learning process, and their combined effect on the results of the learning
    process—and, consequently, on the performance of the model—can be significant.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数每个都会影响决策树在学习过程中构建的方式，它们对学习过程结果的综合影响——从而对模型的表现——可能是显著的。
- en: Since the choice of hyperparameters has a considerable impact on the performance
    of machine learning models, data scientists often spend significant amounts of
    time looking for the best hyperparameter combinations, a process called **hyperparameter
    tuning**. Some of the methods that are used for hyperparameter tuning will be
    described in the next subsection.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于超参数的选择对机器学习模型的性能有着重要影响，数据科学家通常会花费大量时间寻找最佳超参数组合，这个过程称为 **超参数调优**。一些用于超参数调优的方法将在下一小节中介绍。
- en: Hyperparameter tuning
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调优
- en: A common way of searching for good combinations of hyperparameters is using
    a `{2, 5, 10}`, for the `max_depth` parameter, while, for the `splitter` parameter,
    we choose both possible values—`{"best", "random"}`. Then, we try out all six
    possible combinations of these values. For each combination, the classifier is
    trained and evaluated for a certain performance criterion; for example, accuracy.
    At the end of the process, we pick the combination of hyperparameter values that
    yielded the best performance.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找超参数良好组合的常见方法是使用 `{2, 5, 10}` 来设置 `max_depth` 参数，而对于 `splitter` 参数，我们选择两个可能的值——`{"best",
    "random"}`。然后，我们尝试所有六种可能的组合。对于每个组合，分类器会根据某个性能标准（例如准确度）进行训练和评估。在过程结束时，我们选择出表现最好的超参数组合。
- en: The main drawback of the grid search is the exhaustive search it conducts over
    all the possible combinations, which can prove very lengthy. One common way to
    produce good combinations in a shorter amount of time is **random search**, where
    random combinations of hyperparameters are chosen and tested.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索的主要缺点是它对所有可能的组合进行穷举搜索，这可能非常耗时。生成良好组合的常见方法之一是 **随机搜索**，它通过选择和测试随机组合的超参数来加速过程。
- en: A better option—of particular interest to us—when it comes to performing the
    grid search is harnessing a genetic algorithm to look for the best combination(s)
    of hyperparameters within the predefined grid. This method offers the potential
    for finding the best grid combinations in a shorter amount of time than the original,
    exhaustive grid search.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们特别有意义的一个更好选择是在进行网格搜索时，利用遗传算法来寻找在预定义网格中超参数的最佳组合。这种方法比原始的全面网格搜索在更短时间内找到最佳组合的潜力更大。
- en: While grid search and random search are supported by the `scikit-learn` library,
    a genetic algorithm-driven grid search option is offered by `sklearn-deap`. This
    small library builds upon the DEAP-based genetic algorithm’s capabilities, as
    well as the existing features of `scikit-learn`. At the time of writing this book,
    this library is not in sync with the latest version of `scikit-learn`; therefore,
    we included a slightly modified version of it under the `sklearn_deap` folder
    as part of the files of [*Chapter 8*](B20851_08.xhtml#_idTextAnchor238); we will
    make use of that version.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`scikit-learn`库支持网格搜索和随机搜索，但`sklearn-deap`提供了一个遗传算法驱动的网格搜索选项。这个小型库基于DEAP遗传算法的能力，并结合了`scikit-learn`现有的功能。在撰写本书时，这个库与`scikit-learn`的最新版本不兼容，因此我们在[*第8章*](B20851_08.xhtml#_idTextAnchor238)的文件中包含了一个稍作修改的版本，并将使用该版本。
- en: In the following sections, we will compare both approaches to the grid search—exhaustive
    and genetic-algorithm-driven. But first, we’ll take a quick look at the dataset
    we are going to use for our experiment—the **UCI** **Wine dataset**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将比较两种网格搜索方法——全面搜索和遗传算法驱动的搜索。但首先，我们将快速了解一下我们将在实验中使用的数据集——**UCI** **葡萄酒数据集**。
- en: The Wine dataset
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 葡萄酒数据集
- en: A commonly used dataset from the *UCI Machine Learning Repository* ([https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)),
    the Wine dataset ([https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine))
    contains the results of a chemical analysis that was conducted for 178 different
    wines that were grown in the same region in Italy. These wines are categorized
    into one of three types.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常用的数据集来自*UCI机器学习库*（[https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)），葡萄酒数据集（[https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine)）包含对178种在意大利同一地区种植的葡萄酒进行的化学分析结果。这些葡萄酒被分为三种类型之一。
- en: 'The chemical analysis consists of 13 different measurements, representing the
    quantities of the following constituents that are found in each wine:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 化学分析由13个不同的测量组成，表示每种葡萄酒中以下成分的含量：
- en: Alcohol
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 酒精
- en: Malic acid
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 苹果酸
- en: Ash
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灰分
- en: Alkalinity of ash
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灰分的碱度
- en: Magnesium
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 镁
- en: Total phenols
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总酚
- en: Flavanoids
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类黄酮
- en: Non-flavanoid phenols
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非类黄酮酚
- en: Proanthocyanins
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原花青素
- en: Color intensity
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 色度
- en: Hue
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 色调
- en: OD280/OD315 of diluted wines
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀释葡萄酒的OD280/OD315
- en: Proline
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 脯氨酸
- en: Columns `2`–`14` of the dataset contain the values for the preceding measurements,
    while the classification outcome—the wine type itself (`1`, `2`, or `3`)—is found
    in the first column.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的`2`到`14`列包含前述测量值，而分类结果——即葡萄酒类型本身（`1`、`2`或`3`）——则位于第一列。
- en: Next, let’s look at the classifier we chose to classify this dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看我们选择的分类器，用于对这个数据集进行分类。
- en: The adaptive boosting classifier
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应提升分类器
- en: The **adaptive boosting algorithm**, or **AdaBoost**, for short, is a powerful
    machine learning model that combines the outputs of multiple instances of a simple
    learning algorithm (**weak learner**) using a weighted sum. AdaBoost adds instances
    of the weak learner during the learning process, each of which is adjusted to
    improve previously misclassified inputs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应提升算法**，简称**AdaBoost**，是一种强大的机器学习模型，通过加权求和结合多个简单学习算法（**弱学习器**）的输出。AdaBoost在学习过程中逐步添加弱学习器实例，每个实例都会调整以改进先前分类错误的输入。'
- en: 'The `scikit-learn` library’s implementation of this model, the Adaboost classifier
    ([https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)),
    uses several hyperparameters, some of which are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`库实现的此模型——AdaBoost分类器（[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)）——使用了多个超参数，其中一些如下：'
- en: '| **Name** | **Type** | **Description** | **Default value** |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **类型** | **描述** | **默认值** |'
- en: '| `n_estimators` | int | The maximum number of estimators | `50` |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `n_estimators` | 整数类型 | 最大估算器数量 | `50` |'
- en: '| `learning_rate` | float | Weight applied to each classifier at each boosting
    iteration; a higher learning rate increases the contribution of each classifier
    | `1.0` |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `learning_rate` | 浮动类型 | 每次提升迭代中应用于每个分类器的权重；较高的学习率增加每个分类器的贡献 | `1.0` |'
- en: '| `algorithm` | enumerated | The boosting algorithm to be used:`{''SAMME''
    , ''SAMME.R''}` | `''SAMME.R''` |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `algorithm` | 枚举类型 | 使用的提升算法：`{''SAMME'' , ''SAMME.R''}` | `''SAMME.R''`
    |'
- en: 'Table 8.1: Hyperparameters and their details'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1：超参数及其详细信息
- en: Interestingly, each of these three hyperparameters is of a different type—an
    int, a float, and an enumerated (or categorical) type. Later, we will find out
    how each tuning method handles these different types. We will start with two forms
    of the grid search, both of which will be described in the next section.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这三个超参数各自具有不同的类型——一个是整数类型，一个是浮动类型，一个是枚举（或分类）类型。稍后我们将探讨每种调优方法如何处理这些不同类型的参数。我们将从两种网格搜索形式开始，下一节将描述这两种形式。
- en: Tuning the hyperparameters using conventional versus genetic grid search
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用传统的与遗传网格搜索相比，调整超参数
- en: To encapsulate the hyperparameter tuning of the AdaBoost classifier for the
    Wine dataset using a grid search—both the conventional version and the genetic-algorithm-driven
    version—we created a Python class called `HyperparameterTuningGrid`. This class
    can be found in the `01_hyperparameter_tuning_grid.py` file, which is located
    at
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了封装通过网格搜索调优 AdaBoost 分类器的超参数，我们创建了一个名为 `HyperparameterTuningGrid` 的 Python
    类，专门用于 Wine 数据集。此类位于 `01_hyperparameter_tuning_grid.py` 文件中，具体位置为：
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/01_hyperparameter_tuning_grid.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/01_hyperparameter_tuning_grid.py).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/01_hyperparameter_tuning_grid.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/01_hyperparameter_tuning_grid.py)。'
- en: 'The main parts of this class are highlighted as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的主要部分如下所示：
- en: 'The **__init__()** method of the class initializes the wine dataset, the AdaBoost
    classifier, the k-fold cross-validation metric, and the grid parameters:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类的 **__init__()** 方法初始化葡萄酒数据集、AdaBoost 分类器、k 折交叉验证指标和网格参数：
- en: '[PRE0]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The **initGridParams()** method initializes the grid search by setting the
    tested values of the three hyperparameters mentioned in the previous section:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**initGridParams()** 方法通过设置前一节中提到的三个超参数的测试值来初始化网格搜索：'
- en: The **n_estimators** parameter is tested across 10 values, linearly spaced between
    10 and 100.
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_estimators** 参数在 10 个值之间进行了测试，这些值在 10 和 100 之间均匀分布。'
- en: The **learning_rate** parameter is tested across 100 values, logarithmically
    spaced between 0.1 (10 −2) and 1 (10 0).
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**learning_rate** 参数在 100 个值之间进行了测试，这些值在 0.1 (10^−2) 和 1 (10^0) 之间对数均匀分布。'
- en: Both possible values of the **algorithm** parameter, **'SAMME'** and **'SAMME.R'**,
    are tested.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**algorithm** 参数的两种可能值，**''SAMME''** 和 **''SAMME.R''**，都进行了测试。'
- en: 'This setup covers a total of 200 (10×10×2) different combinations of the grid
    parameters:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此设置覆盖了 200 种不同的网格参数组合（10×10×2）：
- en: '[PRE1]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The **getDefaultAccuracy()** method evaluates the accuracy of the classifier
    with its default hyperparameter values using the mean value of the **''****accuracy''**
    metric:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**getDefaultAccuracy()** 方法使用 **''****准确度''** 指标的均值评估分类器在其默认超参数值下的准确度：'
- en: '[PRE2]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The **gridTest()** method performs a conventional grid search over the set
    of tested hyperparameter values we defined earlier. The best combination of parameters
    is determined based on the k-fold cross-validation mean **''****accuracy''** metric:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**gridTest()** 方法在我们之前定义的测试超参数值集合上执行传统网格搜索。最优的参数组合是基于 k 折交叉验证的平均 **''****准确度''**
    指标来确定的：'
- en: '[PRE3]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The **geneticGridTest()** method performs a genetic-algorithm-driven grid search.
    It utilizes the **sklearn-deap** library’s **EvolutionaryAlgorithmSearchCV()**
    method, which was designed to be called in a very similar manner to that of the
    conventional grid search. All we need to do is add a few genetic algorithm parameters—population
    size, mutation probability, tournament size, and the number of generations:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**geneticGridTest()** 方法执行基于遗传算法的网格搜索。它使用 **sklearn-deap** 库的 **EvolutionaryAlgorithmSearchCV()**
    方法，该方法的调用方式与传统网格搜索非常相似。我们所需要做的只是添加一些遗传算法参数——种群大小、变异概率、比赛大小和代数：'
- en: '[PRE4]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, the **main()** method of the class starts by evaluating the performance
    of the classifier with its default hyperparameter values. Then, it runs the conventional,
    exhaustive grid search, followed by the genetic-algorithm-driven grid search,
    while timing each search.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，类的**main()**方法首先评估分类器使用默认超参数值时的性能。然后，它进行常规的全面网格搜索，接着进行基于基因算法的网格搜索，同时记录每次搜索的时间。
- en: The results of running the main method of this class are described in the next
    subsection.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 运行该类的主方法的结果将在下一小节中描述。
- en: Testing the classifier’s default performance
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试分类器的默认性能
- en: 'The results of the run indicate that, with the default parameter values of
    `n_estimators = 50`, `learning_rate = 1.0`, and `algorithm = ''SAMME.R''`, the
    classification accuracy of the classifier is about 66.4%:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 运行结果表明，使用默认参数值`n_estimators = 50`、`learning_rate = 1.0`和`algorithm = 'SAMME.R'`时，分类器的准确率约为66.4%：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is not a particularly good accuracy. Hopefully, grid search can improve
    this by finding a better combination of hyperparameter values.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个特别好的准确率。希望通过网格搜索可以通过找到更好的超参数组合来改进这个结果。
- en: Running the conventional grid search
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行常规的网格搜索
- en: The conventional, exhaustive grid search, covering all 200 possible combinations,
    is performed next. The search results indicated that the best combination within
    this grid was `n_estimators = 50`, `learning_rate ≈ 0.5995`, and `algorithm =
    '``SAMME.R'`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来执行常规的全面网格搜索，覆盖所有200种可能的组合。搜索结果表明，在这个网格内，最佳组合是`n_estimators = 50`、`learning_rate
    ≈ 0.5995`和`algorithm = 'SAMME.R'`。
- en: 'The classification accuracy that we achieved with these values is about 92.7%,
    which is a vast improvement over the original 66.4%. The search runtime was about
    131 seconds using a relatively old computer:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些值时，我们获得的分类准确率约为92.7%，这是对原始66.4%的大幅改进。搜索的运行时间大约是131秒，使用的是一台相对较旧的计算机：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next comes the genetic-powered grid search. Will it match these results? Let’s
    find out.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是基于基因算法的网格搜索。它能匹配这些结果吗？让我们来看看。
- en: Running the genetic-algorithm-driven grid search
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行基于基因算法的网格搜索
- en: 'The last portion of the run describes the genetic-algorithm-driven grid search,
    which is carried out with the same grid parameters. The verbose output of the
    search starts with a somewhat cryptic printout:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 运行的最后部分描述了基于基因算法的网格搜索，它与相同的网格参数一起执行。搜索的冗长输出从一个稍显晦涩的打印输出开始：
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This printout describes the grid we are searching on—a list of 10 integers
    (`n_estimators` values), an ndarray of 10 elements (`learning_rate` values), and
    a list of two strings (`algorithm` values)—as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 该打印输出描述了我们正在搜索的网格——一个包含10个整数（`n_estimators`值）的列表，一个包含10个元素（`learning_rate`值）的ndarray，以及一个包含两个字符串（`algorithm`值）的列表——如下所示：
- en: '**Types [1, 2, 1]** refers to the grid types of **[list,** **ndarray, list]**'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Types [1, 2, 1]**表示**[list, ndarray, list]**的网格类型'
- en: '**maxint [9, 9, 1]** corresponds to the list/array sizes of **[10,** **10,
    2]**'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**maxint [9, 9, 1]**对应于**[10, 10, 2]**的列表/数组大小'
- en: 'The next printed line refers to the total amount of possible grid combinations
    (10×10×2):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行打印的是可能的网格组合的总数（10×10×2）：
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The rest of the printout looks very familiar since it utilizes the same DEAP-based
    genetic algorithm tools that we have been using all along, detailing the process
    of evolving the generations and printing a statistics line for each generation:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的打印输出看起来非常熟悉，因为它使用了我们一直在使用的基于DEAP的基因算法工具，详细描述了进化代的过程，并为每一代打印统计信息：
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'At the end of the process, the best combination is printed, along with the
    score value and the time that elapsed:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在过程结束时，打印出最佳组合、得分值和所用时间：
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: These results indicate that the genetic-algorithm-driven grid search was able
    to find the same best result that was found using the exhaustive search but in
    a shorter amount of time.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，基于基因算法的网格搜索能够在较短时间内找到与全面搜索相同的最佳结果。
- en: Please note that this is a simple example that runs very quickly. In real-life
    situations, we often encounter large datasets, as well as complex models and extensive
    hyperparameter grids. In these circumstances, running an exhaustive grid search
    can be prohibitively lengthy, while the genetic-algorithm-driven grid search has
    the potential to yield good results within a reasonable amount of time.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是一个运行非常快的简单示例。在实际情况中，我们通常会遇到大型数据集、复杂模型和庞大的超参数网格。在这些情况下，执行全面的网格搜索可能需要极长的时间，而基于基因算法的网格搜索在合理的时间内有可能获得不错的结果。
- en: But still, all grid searches, genetic-driven or not, are limited to the subset
    of hyperparameter values that are defined by the grid. What if we would like to
    search outside the grid without being limited to a subset of predefined values?
    A possible solution is described in the next section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，所有网格搜索，无论是否由遗传算法驱动，都仅限于由网格定义的超参数值子集。如果我们希望在不受预定义值子集限制的情况下搜索网格外的内容呢？下节将描述一个可能的解决方案。
- en: Tuning the hyperparameters using a direct genetic approach
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用直接遗传方法调优超参数
- en: Besides offering an efficient grid search option, genetic algorithms can be
    utilized to directly search the entire parameter space, just as we used them to
    search the input space for many types of problems throughout this book. Each hyperparameter
    can be represented as a variable participating in the search, and the chromosome
    can be a combination of all these variables.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提供高效的网格搜索选项外，遗传算法还可以直接搜索整个参数空间，正如我们在本书中用于搜索许多问题的输入空间一样。每个超参数可以表示为一个参与搜索的变量，染色体可以是所有这些变量的组合。
- en: Since the hyperparameters can be of varying types—for example, the types float,
    int, and enumerated, which we have in our AdaBoost classifier—we may want to code
    each of them differently and then define the genetic operations as a combination
    of separate operators that are adapted to each of the types. However, we can also
    use a lazy approach and code all of them as float parameters to simplify the implementation
    of the algorithm, as we will see next.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于超参数可能有不同的类型——例如，我们的AdaBoost分类器中的float、int和枚举类型——我们可能希望对它们进行不同的编码，然后将遗传操作定义为适应每种类型的独立操作符的组合。然而，我们也可以使用一种懒惰的方法，将它们都作为浮动参数来简化算法的实现，正如我们接下来将看到的那样。
- en: Hyperparameter representation
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数表示
- en: 'In [*Chapter 6*](B20851_06.xhtml#_idTextAnchor197), *Optimizing Continuous
    Functions*, we used genetic algorithms to optimize the functions of real-valued
    parameters. These parameters were represented as a list of float numbers: *[1.23,*
    *7.2134, -25.309]*.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第六章*](B20851_06.xhtml#_idTextAnchor197)，《优化连续函数》中，我们使用遗传算法优化了实值参数的函数。这些参数被表示为一个浮动数字列表：*[1.23,
    7.2134, -25.309]*。
- en: Consequently, the genetic operators we used were specialized for handling lists
    of floating-point numbers.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用的遗传操作符是专门为处理浮动点数字列表而设计的。
- en: 'To adapt this approach so that it can tune the hyperparameters, we are going
    to represent each hyperparameter as a floating-point number, regardless of its
    actual type. To make this work, we need to find a way to transform each parameter
    into a floating-point number and back from a floating-point number to its original
    representation. We will implement the following transformations:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调整这种方法，使其能够调优超参数，我们将把每个超参数表示为一个浮动点数，而不管其实际类型是什么。为了使其有效，我们需要找到一种方法将每个参数转换为浮动点数，并从浮动点数转换回其原始表示。我们将实现以下转换：
- en: '**n_estimators**, originally an integer, will be represented by a float value
    in a certain range; for example, **[1, 100]**. To transform the float value back
    into an integer, we will use the Python **round()** function, which will round
    it to the nearest integer.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_estimators**，最初是一个整数，将表示为一个特定范围内的浮动值；例如，**[1, 100]**。为了将浮动值转换回整数，我们将使用Python的**round()**函数，它会将值四舍五入为最接近的整数。'
- en: '**learning_rate** is already a float, so no conversion is needed. It will be
    bound to the range of **[****0.01, 1.0]**.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**learning_rate** 已经是一个浮动点数，因此无需转换。它将绑定在**[0.01, 1.0]**范围内。'
- en: '**algorithm** can have one of two values, **''SAMME''** or **''SAMME.R''**,
    and will be represented by a float number in the range of **[0, 1]**. To transform
    the float value, we will round it to the nearest integer—**0** or **1**. Then,
    we will replace a value of **0** with **''SAMME''** and a value of **1** with
    **''SAMME.R''**.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**algorithm** 可以有两个值，**''SAMME''** 或 **''SAMME.R''**，并将由一个位于**[0, 1]**范围内的浮动数表示。为了转换该浮动值，我们将其四舍五入为最接近的整数——**0**
    或 **1**。然后，我们将**0**替换为**''SAMME''**，将**1**替换为**''SAMME.R''**。'
- en: These conversions will be carried out by two Python files, both of which will
    be described in the following subsections.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转换将由两个Python文件执行，接下来的小节中将描述这两个文件。
- en: Evaluating the classifier accuracy
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估分类器准确度
- en: We start with a Python class encapsulating the classifier’s *accuracy* evaluation,
    called `HyperparameterTuningGenetic`. This class can be found in the `hyperparameter_tuning_genetic_test.py`
    file, which is located at
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个Python类开始，该类封装了分类器的*准确度*评估，称为`HyperparameterTuningGenetic`。该类可以在`hyperparameter_tuning_genetic_test.py`文件中找到，该文件位于
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/hyperparameter_tuning_genetic_test.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/hyperparameter_tuning_genetic_test.py).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/hyperparameter_tuning_genetic_test.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/hyperparameter_tuning_genetic_test.py)。'
- en: 'The main functionality of this class is highlighted as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的主要功能如下所示：
- en: 'The **convertParam()** method of the class takes a list called **params**,
    containing the float values representing the hyperparameters, and transforms them
    into their actual values (as discussed in the previous subsection):'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该类的**convertParam()**方法接受一个名为**params**的列表，包含表示超参数的浮动值，并将其转换为实际值（如前一小节所讨论）：
- en: '[PRE11]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The **getAccuracy()** method takes a list of float numbers representing the
    hyperparameter values, uses the **convertParam()** method to transform them into
    actual values, and initializes the AdaBoost classifier with these values:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**getAccuracy()**方法接受一个浮动数字的列表，表示超参数值，使用**convertParam()**方法将其转化为实际值，并用这些值初始化AdaBoost分类器：'
- en: '[PRE12]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, it finds the accuracy of the classifier using the k-fold cross-validation
    code that we created for the wine dataset:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它通过我们为葡萄酒数据集创建的k折交叉验证代码来找到分类器的准确度：
- en: '[PRE13]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This class is utilized by the program that implements the hyperparameter-tuning
    genetic algorithm, as will be described in the next section.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 该类被实现超参数调优遗传算法的程序所使用，具体内容将在下一节中描述。
- en: Tuning the hyperparameters using genetic algorithms
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用遗传算法调整超参数
- en: The genetic-algorithm-based search for the best hyperparameter values is implemented
    by the Python program, `02_hyperparameter_tuning_genetic.py`, which is located
    at
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基于遗传算法的最佳超参数值搜索由Python程序`02_hyperparameter_tuning_genetic.py`实现，该程序位于
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/02_hyperparameter_tuning_genetic.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/02_hyperparameter_tuning_genetic.py).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/02_hyperparameter_tuning_genetic.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/02_hyperparameter_tuning_genetic.py)。'
- en: 'The following steps describe the main parts of this program:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了该程序的主要部分：
- en: 'We start by setting the lower and upper boundary for each of the float values
    representing a hyperparameter, as described in the previous subsection**—[1, 100]**
    for **n_estimators**, **[0.01, 1]** for **learning_rate**, and **[0, 1]** for
    **algorithm**:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先为表示超参数的每个浮动值设置下界和上界，如前一小节所述——**[1, 100]**用于**n_estimators**，**[0.01, 1]**用于**learning_rate**，**[0,
    1]**用于**algorithm**：
- en: '[PRE14]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we create an instance of the **HyperparameterTuningGenetic** class that
    will allow us to test the various combinations of the hyperparameters:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建了一个**HyperparameterTuningGenetic**类的实例，这将允许我们测试不同的超参数组合：
- en: '[PRE15]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Since our goal is to maximize the accuracy of the classifier, we define a single
    objective, maximizing fitness strategy:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的目标是最大化分类器的准确率，我们定义了一个单一目标——最大化适应度策略：
- en: '[PRE16]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now comes a particularly interesting part—since the solution is represented
    by a list of float values, each of a different range, we use the following loop
    to iterate over all pairs of lower-bound and upper-bound values. For each hyperparameter,
    we create a separate toolbox operator, which will be used to generate random float
    values in the appropriate range:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在进入一个特别有趣的部分——由于解的表示是一个浮动值列表，每个值的范围不同，我们使用以下循环遍历所有的下界和上界值对。对于每个超参数，我们创建一个单独的工具箱操作符，用来在适当的范围内生成随机浮动值：
- en: '[PRE17]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we create the hyperparameter tuple, which contains the specific float
    number generators we just created for each hyperparameter:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建了超参数元组，包含我们刚刚为每个超参数创建的具体浮动数字生成器：
- en: '[PRE18]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we can use this hyperparameter tuple, in conjunction with DEAP’s built-in
    **initCycle()** operator, to create a new **individualCreator** operator that
    fills up an individual instance with a combination of randomly generated hyperparameter
    values:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个超参数元组，结合 DEAP 内置的 **initCycle()** 操作符，创建一个新的 **individualCreator**
    操作符，该操作符通过随机生成的超参数值的组合填充一个个体实例：
- en: '[PRE19]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we instruct the genetic algorithm to use the **getAccuracy()** method
    of the **HyperparameterTuningGenetic** instance for fitness evaluation. As a reminder,
    the **getAccuracy()** method, which we described in the previous subsection, converts
    the given individual—a list of three floats—back into the classifier hyperparameter
    values they represent, trains the classifier with these values, and evaluates
    its accuracy using k-fold cross-validation:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们指示遗传算法使用 **HyperparameterTuningGenetic** 实例的 **getAccuracy()** 方法进行适应度评估。作为提醒，**getAccuracy()**
    方法（我们在前一小节中描述过）将给定的个体——一个包含三个浮点数的列表——转换回它们所表示的分类器超参数值，用这些值训练分类器，并通过 k 折交叉验证评估其准确性：
- en: '[PRE20]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we need to define the genetic operators. While for the **selection** operator,
    we use the usual tournament selection with a tournament size of **2**, we choose
    **crossover** and **mutation** operators that are specialized for bounded float-list
    chromosomes and provide them with the boundaries we defined for each hyperparameter:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要定义遗传操作符。对于 **selection** 操作符，我们使用常见的锦标赛选择，锦标赛大小为 **2**，我们选择专门为有界浮点列表染色体设计的
    **crossover** 和 **mutation** 操作符，并为它们提供我们为每个超参数定义的边界：
- en: '[PRE21]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In addition, we continue to use the elitist approach, where the HOF members—the
    current best individuals—are always passed untouched to the next generation:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们继续使用精英策略，即 HOF 成员——当前最佳个体——始终不受影响地传递到下一代：
- en: '[PRE22]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'By running the algorithm for five generations with a population size of 30,
    we get the following outcome:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 通过用一个种群大小为 30 的算法运行五代，我们得到了以下结果：
- en: '[PRE23]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
