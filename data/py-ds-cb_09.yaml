- en: Chapter 9. Growing Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章：生长树
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下食谱：
- en: Going from trees to forest – Random Forest
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从树到森林——随机森林
- en: Growing extremely randomized Trees
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生长极度随机化的树
- en: Growing a rotation forest
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生长旋转森林
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, we will see some more Bagging methods based on tree-based algorithms.
    Due to their robustness against noise and universal applicability to a variety
    of problems, they are very popular among the data science community.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到更多基于树的算法的袋装方法。由于它们对噪声的鲁棒性以及对各种问题的普适性，它们在数据科学社区中非常受欢迎。
- en: The claim to fame for most of these methods is that they can obtain very good
    results with zero data preparation compared to other methods, and they can be
    provided as black box tools in the hands of software engineers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些方法的名声在于它们相比其他方法能够在没有任何数据准备的情况下获得非常好的结果，而且它们可以作为黑盒工具交给软件工程师使用。
- en: Other than the tall claims made in the previous paragraphs, there are some other
    advantages as well.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前文提到的过高的要求外，还有一些其他优点。
- en: By design, bagging lends itself nicely to parallelization. Hence, these methods
    can be easily applied on a very large dataset in a cluster environment.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从设计上看，袋装法非常适合并行化。因此，这些方法可以轻松应用于集群环境中的大规模数据集。
- en: Decision Tree algorithms split the input data into various regions at each level
    of the tree. Thus, they perform implicit feature selection. Feature selection
    is one of the most important tasks in building a good model. By providing implicit
    feature selection, Decision Trees are in an advantageous position compared to
    other techniques. Hence, Bagging with Decision Trees comes with this advantage.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法在树的每一层将输入数据划分为不同的区域。因此，它们执行了隐式的特征选择。特征选择是构建良好模型中的一个重要任务。通过提供隐式特征选择，决策树相较于其他技术处于有利位置。因此，带有决策树的袋装法具备这一优势。
- en: Almost no data preparation is needed for decision trees. For example, consider
    scaling of attributes. The attribute scale has no impact on the structure of the
    decision trees. Moreover, missing values do not affect decision trees. The effect
    of outliers too is very minimal on a Decision Tree.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树几乎不需要数据准备。例如，考虑属性的缩放。属性的缩放对决策树的结构没有影响。此外，缺失值不会影响决策树。异常值对决策树的影响也很小。
- en: In some of our earlier recipes, we had used Polynomial features retaining only
    the interaction components. With an ensemble of trees, these interactions are
    taken care of. We don't have to make explicit feature transformations to accommodate
    feature interactions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的一些食谱中，我们使用了多项式特征，仅保留了交互项。通过集成树方法，这些交互关系得到了处理。我们无需进行显式的特征转换来适应特征交互。
- en: Linear Regression-based models fail in the case of the existence of a non-linear
    relationship in the input data. We saw this effect when we explained the Kernel
    PCA recipes. Tree-based algorithms are not affected by a non-linear relationship
    in the data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于线性回归的模型在输入数据中存在非线性关系时会失败。当我们解释核主成分分析（Kernel PCA）食谱时，我们看到过这种效果。基于树的算法不受数据中非线性关系的影响。
- en: One of the major complaints against the Tree-based method is the difficulty
    with pruning of trees to avoid overfitting. Big trees tend to fit the noise present
    in the underlying data as well, and hence, lead to a low bias and high variance.
    However, when we grow a lot of trees, and the final prediction is an average of
    the output of all the trees in the ensemble, we avoid the problem of variance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于树基方法的主要投诉之一是树的剪枝困难，容易导致过拟合。大树往往也会拟合底层数据中的噪声，从而导致低偏差和高方差。然而，当我们生长大量树木，并且最终预测是所有树的输出的平均值时，就能避免方差问题。
- en: In this chapter, we will see three tree-based ensemble methods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将介绍三种基于树的集成方法。
- en: Our first recipe is about implementing Random Forests for a classification problem.
    Leo Breiman is the inventor of this algorithm. The Random Forest is an ensemble
    technique which leverages a lot of trees internally to produce a model for solving
    any regression or classification problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个食谱是实现随机森林用于分类问题。Leo Breiman是这一算法的发明者。随机森林是一种集成技术，通过内部使用大量的树来构建模型，用于解决回归或分类问题。
- en: Our second recipe is about Extremely Randomized trees, an algorithm which varies
    in a very small way from Random Forests. By introducing more randomization in
    its procedure as compared to a Random Forest, it claims to address the variance
    problem more effectively. Moreover, it has a slightly reduced computational complexity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个方法是极端随机化树（Extremely Randomized trees），这是一种与随机森林非常相似的算法。通过与随机森林相比，增加更多的随机化，它声称可以更有效地解决方差问题。此外，它还稍微减少了计算复杂度。
- en: Our final recipe is about Rotation Forests. The first two recipes require a
    large number of trees to be a part of their ensemble for achieving good performance.
    Rotation forest claim that they can achieve similar or better performance with
    a fewer number of trees. Furthermore, the authors of this algorithm claim that
    the underlying estimator can be anything other than a tree. In this way, it is
    projected as a new framework for building an ensemble similar to Gradient Boosting.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一个方法是旋转森林（Rotation Forest）。前两个方法需要大量的树作为其集成的一部分，以获得良好的性能。旋转森林声称可以用较少的树实现类似或更好的性能。此外，该算法的作者声称，其基础估计器可以是任何其他的模型，而不仅仅是树。通过这种方式，它被视为构建类似于梯度提升（Gradient
    Boosting）集成的新框架。
- en: Going from trees to Forest – Random Forest
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从树到森林——随机森林
- en: The Random forest method builds a lot of trees (forest) which are uncorrelated
    to each other. Given a classification or a regression problem, the method proceeds
    to build a lot of trees, and the final prediction is either the average of predictions
    from the entire forest for regression or a majority vote classification.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林方法构建了许多相互之间不相关的树（森林）。给定一个分类或回归问题，该方法构建许多树，最终的预测结果要么是森林中所有树的预测平均值（对于回归），要么是多数投票分类的结果。
- en: This should remind you of Bagging. Random Forests is yet another Bagging methodology.
    The fundamental idea behind bagging is to use a lot of noisy estimators, handling
    the noise by averaging, and hence reducing the variance in the final output. Trees
    are highly affected by even a very small noise in the training dataset. Hence,
    being a noisy estimator, they are an ideal candidate for Bagging.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该让你想起 Bagging。随机森林是另一种 Bagging 方法。Bagging 背后的基本思想是使用大量的噪声估计器，通过平均来处理噪声，从而减少最终输出中的方差。树对训练数据集中的噪声非常敏感。由于树是噪声估计器，它们非常适合用于
    Bagging。
- en: 'Let us write down the steps involved in building a Random Forest. The number
    of trees required in the forest is a parameter specified by the user. Let T be
    the number of trees required to be built:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写下构建随机森林的步骤。森林中所需的树的数量是用户指定的一个参数。假设 T 是需要构建的树的数量：
- en: 'We start with iterating from 1 through T, that is, we build T trees:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 1 到 T 进行迭代，也就是说，我们构建 T 棵树：
- en: For each tree, draw a bootstrap sample of size D from our input dataset.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每棵树，从我们的输入数据集中抽取大小为 D 的自助抽样。
- en: 'We proceed to fit a tree t to the input data:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们继续将一棵树 t 拟合到输入数据：
- en: Randomly select m attributes.
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择 m 个属性。
- en: Pick the best attribute to use as a splitting variable using a predefined criterion.
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最好的属性作为分裂变量，使用预定义的标准。
- en: Split the data set into two. Remember, trees are binary in nature. At each level
    of the tree, the input dataset is split into two.
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分成两部分。记住，树是二叉的。在树的每一层，输入数据集都会被分成两部分。
- en: We proceed to do the preceding three steps recursively on the dataset that we
    split.
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们继续在已分割的数据集上递归地执行前面三步。
- en: Finally, we return T trees.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们返回 T 棵树。
- en: To make a prediction on a new instance, we take a majority vote amongst all
    the trees in T for a classification; for regression, we take the average value
    returned by each tree t in T.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对一个新实例做出预测，我们在 T 中所有的树上进行多数投票来做分类；对于回归问题，我们取每棵树 t 在 T 中返回的平均值。
- en: We said earlier that a Random Forest builds non-correlated trees. Let's see
    how the various trees in the ensemble are not correlated to each other. By taking
    a bootstrap sample from the dataset for each tree, we ensure that different parts
    of the data are presented to different trees. This way, each tree tries to model
    different characteristics of the dataset. Hence, we stick to the ensemble rule
    of introducing variation in the underlying estimators. But this does not guarantee
    complete non correlation between the underlying trees. When we do the node splitting,
    we don't select all attributes; rather, we randomly select a subset of attributes.
    In this manner, we try to ensure that our trees are not correlated to each other.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，随机森林构建的是非相关的树。让我们看看集成中的各个树是如何彼此不相关的。通过为每棵树从数据集中抽取自助样本，我们确保不同的树会接触到数据的不同部分。这样，每棵树都会尝试建模数据集的不同特征。因此，我们遵循集成方法引入底层估计器的变化。但这并不保证底层树之间完全没有相关性。当我们进行节点分裂时，并不是选择所有特征，而是随机选择特征的一个子集。通过这种方式，我们尝试确保我们的树之间没有相关性。
- en: Compared to Boosting, where our ensemble of estimators were weak classifiers,
    in a Random Forest, we build trees with maximum depth so that they fit the bootstrapped
    sample perfectly leading to a low bias. The consequence is the introduction of
    high variance. However, by building a large number of trees and using the averaging
    principle for the final prediction, we hope to tackle this variance problem.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Boosting 相比，我们的估计器集成在 Boosting 中是弱分类器，而在随机森林中，我们构建具有最大深度的树，以使其完美拟合自助样本，从而降低偏差。其结果是引入了高方差。然而，通过构建大量的树并使用平均化原则进行最终预测，我们希望解决这个方差问题。
- en: Let us proceed to jump into our recipe for a Random Forest.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续深入了解我们的随机森林配方。
- en: Getting ready
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: We are going to generate some classification datasets to demonstrate a Random
    Forest Algorithm. We will leverage scikit-learn's implementation of a Random Forest
    from the ensemble module.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成一些分类数据集，以演示随机森林算法。我们将利用 scikit-learn 中的随机森林实现，该实现来自集成模块。
- en: How to do it...
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will start with loading all the necessary libraries. Let us leverage the
    `make_classification` method from the `sklearn.dataset` module for generating
    the training data to demonstrate a Random Forest:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从加载所有必要的库开始。让我们利用 `sklearn.dataset` 模块中的 `make_classification` 方法来生成训练数据，以演示随机森林：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will now write the function `build_forest` to build fully grown trees and
    proceed to evaluate the forest''s performance. Then we will write the methods
    which can be used to search the optimal parameters for our forest:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将编写 `build_forest` 函数来构建完全生长的树，并继续评估森林的性能。接着我们将编写可用于搜索森林最优参数的方法：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, we write a main function for invoking the functions that we have defined
    previously:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们编写一个主函数，用于调用我们之前定义的函数：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How it works…
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Let us start with our main function. We invoke get_data to get our predictor
    attributes `x` and the response attributes `y`. Inside `get_data`, we leverage
    the `make_classification` dataset to generate our training data for Random Forest:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们的主函数开始。我们调用 `get_data` 来获取预测器特征 `x` 和响应特征 `y`。在 `get_data` 中，我们利用 `make_classification`
    数据集来生成我们的随机森林训练数据：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let us look at the parameters passed to the `make_classification` method. The
    first parameter is the number of instances required; in this case, we say we need
    500 instances. The second parameter is about the number of attributes required
    per instance. We say that we need 30\. The third parameter, `flip_y`, randomly
    interchanges 3 percent of the instances. This is done to introduce some noise
    in to our data. The next parameter specifies the number of features out of those
    30 features, which should be informative enough to be used in our classification.
    We have specified that 60 percent of our features, that is, 18 out of 30 should
    be informative. The next parameter is about the redundant features. These are
    generated as a linear combination of the informative features in order to introduce
    a correlation among the features. Finally, repeated features are the duplicate
    features, which are drawn randomly from both informative features and redundant
    features.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看传递给`make_classification`方法的参数。第一个参数是所需的实例数量；在此例中，我们需要500个实例。第二个参数是每个实例所需的属性数量。我们设定需要30个属性。第三个参数`flip_y`会随机交换3%的实例。这是为了向数据中引入一些噪声。接下来的参数指定从这30个属性中有多少个是足够有用的信息属性，用于我们的分类任务。我们设定60%的特征，即30个特征中的18个应该具有信息量。下一个参数与冗余特征有关。这些冗余特征是通过信息特征的线性组合生成的，以引入特征之间的相关性。最后，重复特征是重复的特征，它们是从信息特征和冗余特征中随机抽取的。
- en: 'Let us split the data into the training and testing set using `train_test_split`.
    We reserve 30 percent of our data for testing:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`train_test_split`将数据划分为训练集和测试集。我们将30%的数据保留用于测试：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once again, we leverage `train_test_split` to split our test data into dev
    and test:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用`train_test_split`将我们的测试数据分成开发集和测试集：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With the data divided for building, evaluating, and testing the model, we proceed
    to build our models:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据被分配用于构建、评估和测试模型后，我们继续构建我们的模型：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We invoke the `build_forest` function with our training and dev data to build
    the random forest model. Let us look inside that function:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用训练集和开发集数据调用`build_forest`函数来构建随机森林模型。让我们来看一下这个函数的内部实现：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We need 100 trees in our ensemble, so we use the variable `no_trees` to define
    the number of trees. We leverage the `RandomForestClassifier` class from scikit-learn
    check and apply throughout. As you can see, we pass the number of trees required
    as a parameter. We then proceed to fit our model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的集成中需要100棵树，所以我们使用变量`no_trees`来定义树的数量。我们利用scikit-learn中的`RandomForestClassifier`类进行检查并应用。正如你所见，我们将所需的树的数量作为参数传递。然后，我们继续拟合我们的模型。
- en: 'Now let us find the model accuracy score for our train and dev data:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们找到我们训练集和开发集的模型准确度分数：
- en: '![How it works…](img/B04041_09_01.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理…](img/B04041_09_01.jpg)'
- en: 'Not bad! We have achieved 83 percent accuracy on our dev set. Let us see if
    we can improve our scores. There are other parameters to the forest which can
    be tuned to get a better model. For the list of parameters which can be tuned,
    refer to the following link:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！我们在开发集上达到了83%的准确率。让我们看看是否可以提高我们的得分。随机森林中还有其他可调的参数，可以调节以获得更好的模型。有关可以调节的参数列表，请参考以下链接：
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)'
- en: We invoke the function `search_parameters` with the training and dev data to
    tune the various parameters for our Random Forest model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`search_parameters`函数，并使用训练数据和开发数据来调整我们随机森林模型的各项参数。
- en: 'In some of the previous recipes, we used GridSearchCV to search through the
    parameter space for finding the best parameter combination. GridSearchCV performs
    a very exhaustive search. However, in this recipe we are going to use RandomizedSearchCV.
    We provide a distribution of parameter values for each parameter, and specify
    the number of iterations needed. For each iteration, RandomizedSearchCV will pick
    a sample value from the parameter distribution and fit the model:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些前面的实例中，我们使用了GridSearchCV来遍历参数空间，以寻找最佳的参数组合。GridSearchCV进行的是非常彻底的搜索。然而，在本实例中，我们将使用RandomizedSearchCV。我们为每个参数提供一个参数值的分布，并指定所需的迭代次数。在每次迭代中，RandomizedSearchCV将从参数分布中随机选择一个值并拟合模型：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We provide a dictionary of parameters as we did in GridSearchCV. In our case,
    we want to experiment with three parameters.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供一个参数字典，就像我们在GridSearchCV中做的那样。在我们的情况下，我们想要测试三个参数。
- en: 'The first one is the number of trees in the model, represented by the `n_estimators`
    parameter. By invoking the randint function, we get a list of integers between
    75 and 200\. The size of the trees is defined by `no_iterations` parameters:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是模型中的树木数量，通过`n_estimators`参数表示。通过调用randint函数，我们获得一个75到200之间的整数列表。树木的大小由`no_iterations`参数定义：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This is the parameter we will pass to RandomizedSearchCV for the number of iterations
    we want to perform. From this array of `20` elements, RandomizedSearchCV will
    sample a single value for each iteration.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将传递给RandomizedSearchCV的参数，表示我们希望执行的迭代次数。在这`20`个元素的数组中，RandomizedSearchCV将为每次迭代随机抽取一个值。
- en: Our next parameter is the criterion, we pick randomly between gini and entropy,
    and use that as a criterion for splitting the nodes during each iteration.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个参数是准则，我们在基尼指数和熵之间随机选择，并将其作为每次迭代中拆分节点的准则。
- en: 'The most important parameter, `max_features`, defines the number of features
    that the algorithm should pick during the splitting of each node. In our pseudocode
    for describing the Random Forest, we have specified that we need to pick m attributes
    randomly during each split of the node. The parameter `max_features` defines that
    m. Here we give a list of four values. The variable `sqr_no_features` is the square
    root of the number of attributes available in the input dataset:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的参数`max_features`定义了算法在拆分每个节点时应该选择的特征数量。在我们描述随机森林的伪代码中，我们指定了每次拆分节点时需要随机选择m个特征。`max_features`参数定义了m。在这里，我们提供了一个包含四个值的列表。变量`sqr_no_features`是输入数据集中特征数量的平方根：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Other values in that list are some variations of the square root.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的其他值是平方根的一些变化。
- en: 'Let us instantiate RandomizedSearchCV with this parameter distribution:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用这个参数分布来实例化RandomizedSearchCV：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The first parameter is the underlying estimator whose parameters we are trying
    to optimize. It''s our `RandomForestClassifier`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是底层估算器，即我们试图优化其参数的模型。它是我们的`RandomForestClassifier`：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The second parameter, `param_distributions` is the distribution defined by the
    dictionary parameters. We define the number of iterations, that is, the number
    of times we want to run the RandomForestClassifier using the parameter `n_iter`.
    With the `cv` parameter, we specify the number of cross validations required `5`
    cross validations in our case.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数`param_distributions`是通过字典参数定义的分布。我们定义了迭代次数，即我们希望运行RandomForestClassifier的次数，使用参数`n_iter`。通过`cv`参数，我们指定所需的交叉验证次数，在我们的例子中是`5`次交叉验证。
- en: 'Let us proceed to fit the model, and see how well the model has turned out:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续拟合模型，看看模型的效果如何：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![How it works…](img/B04041_09_02.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_09_02.jpg)'
- en: As you can see, we have five folds, that is, we want to do a five-fold cross
    validation on each of our iterations. We have a total of `20` iterations, and
    hence, we will be building 100 models.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们有五个折叠，也就是说，我们希望在每次迭代中进行五折交叉验证。我们总共执行`20`次迭代，因此我们将构建100个模型。
- en: 'Let us look inside the function `print_model_worth`. We pass our grid object
    and dev dataset to this function. The grid object stores the evaluation metric
    for each of the models it builds inside an attribute called the `grid_scores_
    of type` list. Let us sort this list in the descending order to build the best
    model:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`print_model_worth`函数内部。我们将网格对象和开发数据集传递给这个函数。网格对象在一个名为`grid_scores_`的属性中存储了它构建的每个模型的评估指标，这个属性是一个列表。让我们将这个列表按降序排序，以构建最佳模型：
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We select the top five models as you can see from the indexing. We proceed
    to print the details of those models:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择排名前五的模型，如索引所示。接下来我们打印这些模型的详细信息：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We first print the evaluation score and follow it with the parameters of the
    model:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先打印评估分数，并接着展示模型的参数：
- en: '![How it works…](img/B04041_09_03.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_09_03.jpg)'
- en: We have ranked the modes by their scores in the descending order thus showing
    the best model parameters in the beginning. We will choose these parameters as
    our model parameters. The attribute `best_estimator_ will` return the model with
    these parameters.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据模型的得分将模式按降序排列，从而将最佳的模型参数放在最前面。我们将选择这些参数作为我们的模型参数。属性`best_estimator_`将返回具有这些参数的模型。
- en: 'Let us use these parameters and test our dev data:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这些参数并测试我们的开发数据：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The predict function will use `best_estimtor` internally:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: predict函数将内部使用`best_estimtor`：
- en: '![How it works…](img/B04041_09_04.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_09_04.jpg)'
- en: Great! We have a perfect model with a classification accuracy of 100 percent.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们有一个完美的模型，分类准确率为100%。
- en: There's more…
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Internally, the `RandomForestClassifier` uses the `DecisionTreeClassifier`.
    Refer to the following link for all the parameters that are passed for building
    a decision tree:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，`RandomForestClassifier`使用`DecisionTreeClassifier`。有关构建决策树时传递的所有参数，请参考以下链接：
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)'
- en: 'One parameter that is of some interest to us is splitter. The default value
    of splitter is set to best. Based on the `max_features` attribute, the implementation
    will choose the splitting mechanism internally. The available splitting mechanisms
    include the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个我们感兴趣的参数是splitter。splitter的默认值设置为best。根据`max_features`属性，内部实现将选择划分机制。可用的划分机制包括以下几种：
- en: 'best: Chooses the best possible split from the given set of attributes defined
    by the `max_features` parameter'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'best: 从由`max_features`参数定义的给定属性集中选择最佳可能的划分'
- en: 'random: Randomly chooses a splitting attribute'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'random: 随机选择一个划分属性'
- en: You would have noticed that this parameter is not available while instantiating
    a `RandomForestClassifier`. The only way to control is to give a value to the
    `max_features` parameter which is less than the number of attributes available
    in the dataset.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在实例化`RandomForestClassifier`时，这个参数不可用。唯一的控制方式是为`max_features`参数赋值，该值应小于数据集中的属性数量。
- en: 'In the industry, Random Forests are extensively used for variable selection.
    In Scikit learn, variable importance is calculated using gini impurity. Both the
    gini and entropy criteria used for node splitting identify the best attribute
    for splitting the node by its ability to split the dataset into subsets with high
    impurity so that subsequent splitting leads to good classification. The importance
    of a variable is decided by the amount of impurity it can induce into the split
    dataset. Refer to the following book for more details:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在工业界，随机森林被广泛用于变量选择。在Scikit learn中，变量重要性是通过基尼不纯度（gini impurity）来计算的。用于节点划分的基尼和熵准则根据它们将数据集划分为高不纯度的子集的能力来识别最佳划分属性，以便后续的划分能产生良好的分类。一个变量的重要性由它能在划分后的数据集中引入的不纯度量决定。有关更多详细信息，请参考以下书籍：
- en: '*Breiman, Friedman, "Classification and regression trees", 1984.*'
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Breiman, Friedman, "分类与回归树"，1984年。*'
- en: 'We can write a small function to print the important features:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以编写一个小函数来打印重要特征：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A Random Forest object has a variable called `feature_importances_`. We use
    this variable and create a list of tuples with the feature number and importance:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Random Forest对象有一个名为`feature_importances_`的变量。我们使用这个变量并创建一个包含特征编号和重要性的元组列表：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We proceed to sort it in descending order of importance, and select only the
    top 10 features:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着按重要性降序排列，并选择前10个特征：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We then print the top 10 features:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们打印出前10个特征：
- en: '![There''s more…](img/B04041_09_05.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多…](img/B04041_09_05.jpg)'
- en: 'Another interesting aspect of Random Forests is the `Out-of-Bag estimation
    (OOB)`. Remember that we bootstrap from the dataset initially for every tree grown
    in the forest. Because of bootstrapping, some records will not be used in some
    trees. Let us say record 1 is used in 100 trees and not used in 150 trees in our
    forest. We can then use those 150 trees to predict the class label for that record
    to figure out the classification error for that record. Out-of-bag estimation
    can be used to effectively assess the quality of our forest. The following URL
    gives an example of how the OOB can be used effectively:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的另一个有趣方面是`袋外估计（OOB）`。记住，我们最初从数据集中对每棵树进行自助采样（bootstrap）。由于自助采样，某些记录在某些树中不会被使用。假设记录1在100棵树中被使用，在150棵树中未被使用。然后，我们可以使用那150棵树来预测该记录的类别标签，从而计算该记录的分类误差。袋外估计可以有效地评估我们森林的质量。以下网址给出了OOB如何有效使用的示例：
- en: '[http://scikit-learn.org/dev/auto_examples/ensemble/plot_ensemble_oob.html](http://scikit-learn.org/dev/auto_examples/ensemble/plot_ensemble_oob.html)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/dev/auto_examples/ensemble/plot_ensemble_oob.html](http://scikit-learn.org/dev/auto_examples/ensemble/plot_ensemble_oob.html)'
- en: 'The RandomForestClassifier class in Scikit learn is derived from `ForestClassifier`.The
    source code for the same can be found at the following link:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit learn中的RandomForestClassifier类来源于`ForestClassifier`。其源代码可以在以下链接找到：
- en: '[https://github.com/scikit-learn/scikit-learn/blob/a95203b/sklearn/ensemble/forest.py#L318](https://github.com/scikit-learn/scikit-learn/blob/a95203b/sklearn/ensemble/forest.py#L318)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/scikit-learn/scikit-learn/blob/a95203b/sklearn/ensemble/forest.py#L318](https://github.com/scikit-learn/scikit-learn/blob/a95203b/sklearn/ensemble/forest.py#L318)'
- en: When we call the predict method in RandomForestClassifier, it internally calls
    the `predict_proba` method defined in ForestClassifier. Here, the final prediction
    is done not on the basis of voting but by averaging the probabilities for each
    of the classes from different trees inside the forest and deciding the final class
    based on the highest probability.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在RandomForestClassifier中调用`predict`方法时，它内部调用了在ForestClassifier中定义的`predict_proba`方法。在这里，最终的预测不是通过投票完成，而是通过对森林中不同树的每个类别的概率进行平均，并基于最高概率决定最终类别。
- en: 'The original paper by Leo Breiman on Random Forests is available for download
    at the following link:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Leo Breiman关于随机森林的原始论文可以在以下链接下载：
- en: '[http://link.springer.com/article/10.1023%2FA%3A1010933404324](http://link.springer.com/article/10.1023%2FA%3A1010933404324)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://link.springer.com/article/10.1023%2FA%3A1010933404324](http://link.springer.com/article/10.1023%2FA%3A1010933404324)'
- en: 'You can also refer to the website maintained by Leo Breiman and Adele Cutler:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以参考Leo Breiman和Adele Cutler维护的网站：
- en: '[https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)'
- en: See also
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: '*Building Decision Trees to solve Multi Class Problems* recipe in [Chapter
    6](ch06.xhtml "Chapter 6. Machine Learning 1"), *Machine Learning I*'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第6章](ch06.xhtml "第6章。机器学习I")中的*构建决策树以解决多类问题*配方，*机器学习I*。'
- en: '*Understanding Ensemble, Gradient Boosting* recipe in [Chapter 8](ch08.xhtml
    "Chapter 8. Ensemble Methods"), *Model Selection and Evaluation*'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8章](ch08.xhtml "第8章。集成方法")中的*理解集成，梯度提升*配方，*模型选择与评估*。'
- en: '*Understanding Ensemble, Bagging Method* recipe in [Chapter 8](ch08.xhtml "Chapter 8. Ensemble
    Methods"), *Model Selection and Evaluation*'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8章](ch08.xhtml "第8章。集成方法")中的*理解集成，袋装方法*配方，*模型选择与评估*。'
- en: Growing Extremely Randomized Trees
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生长极端随机树
- en: 'Extremely Randomized Trees, also known as the Extra trees algorithm differs
    from the Random Forest described in the previous recipe in two ways:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 极端随机树，也称为Extra Trees算法，与前面配方中描述的随机森林在两方面有所不同：
- en: It does not use bootstrapping to select instances for every tree in the ensemble;
    instead, it uses the complete training dataset.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它不使用自助法（bootstrapping）为集成中的每棵树选择实例；相反，它使用完整的训练数据集。
- en: Given K as the number of attributes to be randomly selected at a given node,
    it selects a random cut-point without considering the target variable.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定K作为在某个节点上要随机选择的属性数量，它选择一个随机切分点，而不考虑目标变量。
- en: As you saw in the previous recipe, Random Forests used randomization in two
    places. First, in selecting the instances to be used for the training trees in
    the forest; bootstrap was used to select the training instances. Secondly, at
    every node a random set of attributes were selected. One attribute among them
    was selected based on either the gini impurity or entropy criterion. Extremely
    randomized trees go one step further and select the splitting attribute randomly.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的配方所示，随机森林在两个地方使用了随机化。首先，选择用于训练森林中树的实例时使用了自助法来选择训练实例。其次，在每个节点，随机选择了一组属性，从中选出一个属性，依据的是基尼不纯度或熵准则。极端随机树更进一步，随机选择分裂属性。
- en: 'Extremely Randomized Trees were proposed in the following paper:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 极端随机树在以下论文中被提出：
- en: '*P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees", Machine
    Learning, 63(1), 3-42, 2006*.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*P. Geurts, D. Ernst., 和 L. Wehenkel，“极端随机树”，《机器学习》，63(1)，3-42，2006*。'
- en: 'According to this paper, there are two aspects, other than the technical aspects
    listed earlier, which make an Extremely Randomized Tree more suitable:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 根据本文，除了之前列出的技术方面，还有两个方面使得极端随机树更为适用：
- en: The rationale behind the Extra-Trees method is that the explicit randomization
    of the cut-point and attribute combined with ensemble averaging should be able
    to reduce variance more strongly than the weaker randomization schemes used by
    other methods.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Extra-Trees 方法背后的原理是，通过显式地随机化切分点和属性，结合集成平均法，应该能够比其他方法使用的较弱随机化方案更强烈地减少方差。
- en: Compared to a Random Forest, randomization of the cut-point ( the attribute
    selected to split the dataset at each node) combined with the randomization of
    cut-point, that is, ignoring any criteria, and finally, averaging the results
    from each of the tree, will result in a much superior performance on an unknown
    dataset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林相比，切分点的随机化（即在每个节点选择用于切分数据集的属性）结合切分点的随机化，即忽略任何标准，最后平均每棵树的结果，将在未知数据集上表现出更优的性能。
- en: 'The second advantage is regarding the compute complexity:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个优点是计算复杂度：
- en: From the computational point of view, the complexity of the tree growing procedure
    is, assuming balanced trees, on the order of N log N with respect to learning
    the sample size, like most other tree growing procedures. However, given the simplicity
    of the node splitting procedure we expect the constant factor to be much smaller
    than in other ensemble based methods which locally optimize cut-points
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算角度来看，假设树是平衡的，树的生长过程的复杂度与学习样本大小呈N log N的数量级，就像大多数树生长过程一样。然而，考虑到节点切分过程的简洁性，我们预计常数因子将比其他集成方法中局部优化切分点的情况要小得多。
- en: Since no computation time is spent in identifying the best attribute to split,
    this method is more computationally efficient than Random Forests.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有计算时间用于识别最佳的切分属性，这种方法比随机森林在计算上更高效。
- en: Let us write down the steps involved in building Extremely Random trees. The
    number of trees required in the forest is typically specified by the user. Let
    T be the number of trees required to be built.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写下构建极度随机化树的步骤。森林中所需的树的数量通常由用户指定。设T为需要构建的树的数量。
- en: 'We start with iterating from 1 through T, that is, we build T trees:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从1迭代到T，也就是我们构建T棵树：
- en: For each tree, we select the complete input dataset.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每棵树，我们选择完整的输入数据集。
- en: 'We then proceed to fit a tree t to the input data:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们继续拟合一棵树t到输入数据：
- en: Select m attributes randomly.
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择m个属性。
- en: Pick an attribute randomly as the splitting variable.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择一个属性作为切分变量。
- en: Split the data set into two. Remember that trees are binary in nature. At each
    level of the tree, the input dataset is split into two.
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分成两部分。请记住，树是二叉的。在树的每一层，输入数据集都会被分成两部分。
- en: Perform the preceding three steps recursively on the dataset that we split.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对我们分割的数据集递归执行前述三个步骤。
- en: Finally, we return T trees.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们返回T棵树。
- en: Let us take a look at the recipe for Extremely Randomized Trees.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们看看极度随机化树的配方。
- en: Getting ready…
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备好了...
- en: We are going to generate some classification datasets to demonstrate Extremely
    Randomized Trees. For that, we will leverage Scikit Learn's implementation of
    the Extremely Randomized Trees ensemble module.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成一些分类数据集来演示极度随机化树。为此，我们将利用Scikit Learn中极度随机化树集成模块的实现。
- en: How to do it...
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We start by loading all the necessary libraries. Let us leverage the `make_classification`
    method from the `sklearn.dataset` module to generate the training data:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从加载所有必要的库开始。让我们利用`sklearn.dataset`模块中的`make_classification`方法来生成训练数据：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We write the function `build_forest`, where we will build fully grown trees,
    and proceed to evaluate the forest''s performance:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写`build_forest`函数，在其中构建完全生长的树，并继续评估森林的性能：
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we write a main function for invoking the functions that we have defined:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们编写一个主函数来调用我们定义的函数：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How it works…
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Let us start with our main function. We invoke `get_data` to get our predictor
    attributes in the response attributes. Inside `get_data`, we leverage the make_classification
    dataset to generate the training data for our recipe as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从主函数开始。我们调用`get_data`来获取预测属性和响应属性。在`get_data`函数内部，我们利用make_classification数据集来生成我们配方的训练数据，具体如下：
- en: '[PRE23]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Let us look at the parameters that are passed to the `make_classification` method.
    The first parameter is the number of instances required; in this case, we say
    we need 500 instances. The second parameter is about the number of attributes
    required per instance. We say that we need 30\. The third parameter, `flip_y`,
    randomly interchanges 3 percent of the instances. This is done to introduce some
    noise in our data. The next parameter specifies the number of features out of
    those 30 features should be informative enough to be used in our classification.
    We have specified that 60 percent of our features, that is, 18 out of 30 should
    be informative. The next parameter is about redundant features. These are generated
    as a linear combination of the informative features in order to introduce a correlation
    among the features. Finally, repeated features are the duplicate features, which
    are drawn randomly from both informative features and redundant features.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看传递给`make_classification`方法的参数。第一个参数是所需的实例数量；在这个例子中，我们需要500个实例。第二个参数是每个实例所需的属性数量。我们需要30个属性。第三个参数，`flip_y`，会随机交换3%的实例。这样做是为了给我们的数据引入一些噪声。下一个参数指定了这些30个特征中，有多少个特征应足够信息量，以便用于分类。我们指定60%的特征，也就是30个中的18个，应该是有信息量的。下一个参数是冗余特征。这些特征是通过有信息量的特征的线性组合生成的，以便在特征之间引入相关性。最后，重复特征是从有信息量特征和冗余特征中随机选取的重复特征。
- en: 'Let us split the data into the training and testing set using `train_test_split`.
    We reserve 30 percent of our data for testing:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`train_test_split`将数据分为训练集和测试集。我们将30%的数据用于测试：
- en: '[PRE24]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once again, we leverage train_test_split to split our test data into dev and
    test:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们使用`train_test_split`将我们的测试数据分为开发集和测试集：
- en: '[PRE25]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'With the data divided for building, evaluating, and testing the model, we proceed
    to build our models:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 数据已经分割用于构建、评估和测试模型，我们继续构建我们的模型：
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We invoke the `build_forest` function with our training and dev data to build
    our Extremely Randomized trees model. Let us look inside that function:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`build_forest`函数，使用我们的训练集和开发集数据来构建极端随机化树模型。让我们来看一下这个函数：
- en: '[PRE27]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We need 100 trees in our ensemble, so we use the variable no_trees to define
    the number of trees. We leverage the `ExtraTreesClassifier` class from Scikit
    learn. As you can see, we pass the number of trees required as a parameter. A
    point to note here is the parameter bootstrap. Refer to the following URL for
    the parameters for the ExtraTreesClassifier:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要100棵树来构建我们的集成模型，因此我们使用变量no_trees来定义树的数量。我们利用Scikit Learn中的`ExtraTreesClassifier`类。如你所见，我们将所需的树的数量作为参数传递。这里有一个需要注意的点是参数bootstrap。有关ExtraTreesClassifier的参数，请参考以下网址：
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)'
- en: 'The parameter bootstrap is set to `False` by default. Compare it with the `RandomForestClassifier`
    bootstrap parameter given at the following URL:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 参数bootstrap默认设置为`False`。与以下网址给出的`RandomForestClassifier`的bootstrap参数进行对比：
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)'
- en: As explained earlier, every tree in the forest is trained with all the records.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，森林中的每棵树都是用所有记录进行训练的。
- en: 'We proceed to fit our model as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续如下拟合我们的模型：
- en: '[PRE28]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We then proceed to find the model accuracy score for our train and dev data:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续找到训练集和开发集数据的模型准确度得分：
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let us print the scores for the training and dev dataset:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出训练集和开发集的数据得分：
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![How it works…](img/B04041_09_06.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_09_06.jpg)'
- en: 'Let us now do a five-fold cross validation to look at the model predictions:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进行五折交叉验证，查看模型预测的结果：
- en: '![How it works…](img/B04041_09_07.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_09_07.jpg)'
- en: 'Pretty good results. We almost have a 90 percent accuracy rate for one of the
    folds. We can do a randomized search across the parameter space as we did for
    Random Forest. Let us invoke the function `search_parameters` with our train and
    test dataset. Refer to the previous recipe for an explanation of RandomizedSearchCV.
    We will then print the output of the `search_parameters` function:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 相当不错的结果。我们几乎在其中一个折叠中达到了90%的准确率。我们可以像对随机森林一样在参数空间内进行随机搜索。让我们调用 `search_parameters`
    函数，传入我们的训练和测试数据集。请参阅前面的内容了解 RandomizedSearchCV 的解释。然后，我们将打印 `search_parameters`
    函数的输出：
- en: '![How it works…](img/B04041_09_08.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![如何运作……](img/B04041_09_08.jpg)'
- en: As in the previous recipe, we have ranked the models by their scores in descending
    order, thus showing the best model parameters in the beginning. We will choose
    these parameters as our model parameters. The attribute `best_estimator_ will`
    return the model with these parameters.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如同前面的步骤，我们已按得分从高到低对模型进行排名，因此最好的模型参数会排在最前面。我们将选择这些参数作为我们的模型参数。属性 `best_estimator_`
    将返回具有这些参数的模型。
- en: 'What you see next is the classification report generated for the best estimator.
    The predict function will use `best_estimator_ internally`. The report was generated
    by the following code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将看到为最佳估计器生成的分类报告。预测函数将内部使用 `best_estimator_`。报告是通过以下代码生成的：
- en: '[PRE31]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Great! We have a perfect model with a classification accuracy of 100 percent.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！我们有一个完美的模型，分类准确率为100%。
- en: There's more…
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'Extremely Randomized Trees are very popular with the time series classification
    problems. Refer to the following paper for more information:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 极端随机树在时间序列分类问题中非常受欢迎。有关更多信息，请参阅以下论文：
- en: '*Geurts, P., Blanco Cuesta A., and Wehenkel, L. (2005a). Segment and combine
    approach for biological sequence classification. In: Proceedings of IEEE Symposium
    on Computational Intelligence in Bioinformatics and Computational Biology, 194–201.*'
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Geurts, P., Blanco Cuesta A., 和 Wehenkel, L. (2005a). 生物序列分类的分段与组合方法。收录于：IEEE生物信息学与计算生物学计算智能研讨会论文集，194–201。*'
- en: See also
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '*Building Decision Trees to solve Multi Class Problems* recipe in [Chapter
    6](ch06.xhtml "Chapter 6. Machine Learning 1"), *Machine Learning I*'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*构建决策树解决多类别问题* 章节内容参见 [第6章](ch06.xhtml "第6章. 机器学习 I")，*机器学习 I*'
- en: '*Understanding Ensemble, Bagging Method* recipe in [Chapter 8](ch08.xhtml "Chapter 8. Ensemble
    Methods"), *Model Selection and Evaluation*'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解集成方法，袋装法* 章节内容参见 [第8章](ch08.xhtml "第8章. 集成方法")，*模型选择与评估*'
- en: '*Growing from trees to Forest, Random Forest* recipe in [Chapter 9](ch09.xhtml
    "Chapter 9. Growing Trees"), *Machine Learning III*'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从树到森林的生长，随机森林* 章节内容参见 [第9章](ch09.xhtml "第9章. 生长树木")，*机器学习 III*'
- en: Growing Rotational Forest
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 旋转森林的生长
- en: Random forests and Bagging give impressive results with very large ensembles;
    having a large number of estimators results in an improvement in the accuracy
    of these methods. On the contrary, a Rotational forest is designed to work with
    a smaller number of ensembles.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林和袋装法在非常大的集成中能够给出令人印象深刻的结果；拥有大量估计器会提高这些方法的准确性。相反，旋转森林是设计用来处理较小数量的集成。
- en: Let us write down the steps involved in building a Rotational Forest. The number
    of trees required in the forest is typically specified by the user. Let T be the
    number of trees required to be built.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写下构建旋转森林的步骤。森林中所需的树木数量通常由用户指定。让T表示需要构建的树的数量。
- en: We start with iterating from 1 through T, that is, we build T trees.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从1开始迭代直到T，也就是说，我们构建T棵树。
- en: 'For each tree t, perform the following steps:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每棵树t，执行以下步骤：
- en: Split the attributes in the training set into K non-overlapping subsets of equal
    size.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练集中的特征分成K个大小相等的不重叠子集。
- en: 'We have K datasets, each with K attributes. For each of the K datasets, we
    proceed to do the following: Bootstrap 75 percent of the data from each K dataset,
    and use the bootstrapped sample for further steps:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有K个数据集，每个数据集包含K个特征。对于每个K数据集，我们进行如下操作：从每个K数据集中自助抽取75%的数据，并使用抽取的样本进行后续步骤：
- en: Run a Principal Component analysis on the ith subset in K. Retain all the principal
    components. For every feature j in the Kth subset, we have a principal component
    a. Let us denote it as aij, which is the principal component for the jth attribute
    in the ith subset.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对K中的第i个子集进行主成分分析。保留所有主成分。对于Kth子集中的每个特征j，我们有一个主成分a。我们将其表示为aij，这是第i个子集中的第j个属性的主成分。
- en: Store the principal components for the subset.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储子集的主成分。
- en: Create a rotation matrix of size n X n, where n is the total number of attributes.
    Arrange the principal components in the matrix such that the components match
    the position of the features in the original training dataset.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个大小为n X n的旋转矩阵，其中n是属性的总数。将主成分排列在矩阵中，使得这些成分匹配原始训练数据集中特征的位置。
- en: Project the training dataset on the Rotation matrix using matrix multiplication.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用矩阵乘法将训练数据集投影到旋转矩阵上。
- en: Build a decision tree with the projected dataset.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用投影数据集构建决策树。
- en: Store the tree and the rotational matrix.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储树和旋转矩阵。
- en: With this knowledge, let us jump to our recipe.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些知识，让我们跳入我们的配方。
- en: Getting ready…
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作…
- en: We are going to generate some classification datasets to demonstrate a Rotational
    Forest. To our knowledge, there is no Python implementation available for Rotational
    forests. Hence, we will write our own code. We will leverage Scikit Learn's implementation
    of a Decision Tree Classifier and use the `train_test_split` method for bootstrapping.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成一些分类数据集来演示旋转森林。根据我们的了解，目前没有现成的Python实现来支持旋转森林。因此，我们将编写自己的代码。我们将利用Scikit
    Learn实现的决策树分类器，并使用`train_test_split`方法进行自助抽样。
- en: How to do it...
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We will start with loading all the necessary libraries. Let us leverage the
    `make_classification` method from the `sklearn.dataset` module to generate the
    training data. We follow it with a method to select a random subset of attributes
    called `gen_random_subset`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从加载所有必要的库开始。让我们利用来自`sklearn.dataset`模块的`make_classification`方法生成训练数据。接着我们使用一个方法来选择一个随机属性子集，称为`gen_random_subset`：
- en: '[PRE32]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We now write a function `build_rotationtree_model`, where we will build fully
    grown trees, and proceed to evaluate the forest''s performance using the function
    `model_worth`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们编写一个函数`build_rotationtree_model`，在其中我们将构建完全生长的树，并使用`model_worth`函数评估森林的表现：
- en: '[PRE33]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we write a main function for invoking the functions that we have defined
    earlier:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们编写一个主函数，用于调用我们之前定义的函数：
- en: '[PRE34]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How it works…
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Let us start with our main function. We invoke `get_data` to get our predictor
    attributes in the response attributes. Inside get_data, we leverage the make_classification
    dataset to generate the training data for our recipe as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从主函数开始。我们调用`get_data`来获取响应属性中的预测器属性。在`get_data`内部，我们利用`make_classification`数据集生成我们的训练数据，具体如下：
- en: '[PRE35]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Let us look at the parameters passed to the make_classification method. The
    first parameter is the number of instances required; in this case we say we need
    500 instances. The second parameter is about the number of attributes required
    per instance. We say that we need 30\. The third parameter, `flip_y`, randomly
    interchanges 3 percent of the instances. This is done to introduce some noise
    in our data. The next parameter is about the number of features out of those 30
    features, which should be informative enough to be used in our classification.
    We have specified that 60 percent of our features, that is, 18 out of 30 should
    be informative. The next parameter is about redundant features. These are generated
    as a linear combination of the informative features in order to introduce a correlation
    among the features. Finally, repeated features are the duplicate features which
    are drawn randomly from both informative features and redundant features.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下传递给`make_classification`方法的参数。第一个参数是所需实例的数量；在这种情况下，我们需要500个实例。第二个参数是每个实例所需的属性数量。我们需要30个属性。第三个参数`flip_y`随机交换3%的实例。这样做是为了在数据中引入一些噪声。下一个参数是关于30个特征中应该具有足够信息量用于分类的特征数量。我们规定，60%的特征，也就是30个中的18个应该具有信息量。下一个参数是冗余特征。冗余特征是通过信息性特征的线性组合生成的，用于在特征之间引入相关性。最后，重复特征是从信息性特征和冗余特征中随机选择的重复特征。
- en: 'Let us split the data into the training and testing set using train_test_split.
    We reserve 30 percent of our data for testing:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`train_test_split`将数据分割成训练集和测试集。我们将30%的数据用于测试：
- en: '[PRE36]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Once again, we leverage train_test_split to split our test data into dev and
    test as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次利用`train_test_split`将测试数据分成开发集和测试集，具体如下：
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'With the data divided for building, evaluating, and testing the model, we proceed
    to build our models:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据被划分为用于构建、评估和测试模型的部分后，我们开始构建我们的模型：
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We invoke the `build_rotationtree_model` function to build our Rotational forest.
    We pass our training data, predictors `x_train` and response variable `y_train`,
    the total number of trees to be built (`25` in this case), and finally, the subset
    of features to be used (`5` in this case).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`build_rotationtree_model`函数来构建我们的旋转森林。我们传入我们的训练数据、预测变量`x_train`和响应变量`y_train`，要构建的树的总数（在本例中为`25`），以及要使用的特征子集（在本例中为`5`）。
- en: 'Let us jump to that function:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们跳到该函数：
- en: '[PRE39]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We begin with declaring three lists to store each of the decision tree, the
    rotation matrix for that tree, and finally, the subset of features used in that
    iteration. We proceed to build each tree in our ensemble.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先声明三个列表，用于存储每棵决策树、该树的旋转矩阵，以及在该迭代中使用的特征子集。然后我们继续构建我们的集成中的每棵树。
- en: 'As a first order of business, we bootstrap to retain only 75 percent of the
    data:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一项工作，我们进行自助法以保留数据的75％：
- en: '[PRE40]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We leverage the `train_test_split` function from Scikit learn for bootstrapping.
    We then decide the feature subsets as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用Scikit learn中的`train_test_split`函数进行自助法（bootstrapping）。然后我们按以下方式决定特征子集：
- en: '[PRE41]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The function `get_random_subset` takes the feature index and the number of subsets
    that require k as parameter, and returns K subsets.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_random_subset`函数接受特征索引和所需k个子集的数量作为参数，并返回K个子集。'
- en: 'Inside that function, we shuffle the feature index. The feature index is an
    array of numbers that starts from 0 and ends with the number of features in our
    training set:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在该函数内部，我们会打乱特征索引。特征索引是一个数字数组，从0开始，直到训练集中的特征数量：
- en: '[PRE42]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let us say we have 10 features and our k value is 5 indicating that we need
    subsets with 5 non-overlapping feature indices; we need to then do two iterations.
    We store the number of iterations needed in the limit variable:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有10个特征，我们的k值是5，表示我们需要具有5个不重叠特征索引的子集；我们需要进行两次迭代。我们将所需的迭代次数存储在limit变量中：
- en: '[PRE43]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'If our required subset is less than the total number of attributes, then we
    can proceed to use the first k entries in our iterable. Since we have shuffled
    our iterables, we will be returning different volumes at different times:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的所需子集少于总属性数，我们可以继续使用我们可迭代对象中的前k个条目。由于我们已经打乱了可迭代对象，我们将在不同的时间返回不同的数量：
- en: '[PRE44]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'On selecting a subset, we remove it from the iterable as we need non-overlapping
    sets:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择一个子集后，我们将其从可迭代对象中移除，因为我们需要不重叠的集合：
- en: '[PRE45]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'With all the subsets ready, we declare our rotation matrix as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有子集准备好后，我们按以下方式声明我们的旋转矩阵：
- en: '[PRE46]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'As you can see, our rotational matrix is of size n x n, where n is the number
    of attributes in our dataset. You can see that we have used the shape attribute
    to declare this matrix filled with zeros:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们的旋转矩阵的大小是n x n，其中n是我们数据集中的属性数量。您可以看到，我们使用了shape属性来声明这个填充了零的矩阵：
- en: '[PRE47]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: For each of the K subsets of data having only K features, we proceed to perform
    the principal component analysis.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个只有K个特征的数据子集，我们继续进行主成分分析。
- en: 'We fill our rotational matrix with the component values as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按如下方式填充我们的旋转矩阵：
- en: '[PRE48]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'For example, let us say that we have three attributes in our subset, in a total
    of six attributes. For illustration, let us say our subsets are:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们的子集中有三个属性，总共有六个属性。为了说明，假设我们的子集是：
- en: '[PRE49]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Our rotational matrix R is of size 6 x 6\. Assume that we want to fill the rotation
    matrix for the first subset of features. We will have three principal components,
    one each for 2, 4, and 6 of size 1 x 3.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旋转矩阵R是6 x 6的大小。假设我们要填充第一个特征子集的旋转矩阵。我们将有三个主成分，分别对应于2、4和6，大小为1 x 3。
- en: 'The output of the PCA from Scikit learn is a matrix of the size component''s
    X features. We go through each component value in the for loop. At the first run,
    our feature of interest is 2, and the cell (`0`,`0`) in the component matrix output
    from PCA gives the value of the contribution of feature 2 to component 1\. We
    have to find the right place in the rotational matrix for this value. We use the
    index from the component matrix ii and jj with the subset list to get the right
    place in the rotation matrix:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Scikit learn的PCA输出是一个大小为主成分X特征的矩阵。我们通过for循环遍历每个主成分值。在第一次运行时，我们感兴趣的特征是2，来自PCA的主成分矩阵（`0`,`0`）的单元格给出了特征2对主成分1的贡献值。我们需要找到旋转矩阵中这个值的位置。我们使用主成分矩阵中的索引ii和jj与子集列表结合来找到旋转矩阵中的正确位置：
- en: '[PRE50]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`each_subset[0]` and `each_subset[0]` will put us in cell (2,2) in the rotation
    matrix. As we go through the loop, the next component value in cell (0,1) in the
    component matrix will be placed in cell (2,4) of the rotational matrix, and the
    last one in cell (2,6) of the rotational matrix. This is done for all the attributes
    in the first subset. Let us go to the second subset; here the first attribute
    is 1\. Cell (0,0) of the component matrix corresponds to cell (1,1) in the rotation
    matrix.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`each_subset[0]`和`each_subset[0]`将使我们处于旋转矩阵中的(2,2)单元格。随着循环的进行，组件矩阵中(0,1)单元格中的下一个组件值将被放置到旋转矩阵中的(2,4)单元格，最后一个将放在旋转矩阵中的(2,6)单元格。这对于第一个子集中的所有属性都是如此。让我们进入第二个子集；这里第一个属性是1。组件矩阵中的(0,0)单元格对应于旋转矩阵中的(1,1)单元格。'
- en: Proceeding this way, you will notice that the attribute component values are
    arranged in the same order as the attributes themselves.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式进行，你会发现属性组件的值与属性本身的顺序一致。
- en: 'With our rotation matrix ready, let us project our input onto the rotation
    matrix:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在旋转矩阵准备好后，让我们将输入数据投影到旋转矩阵上：
- en: '[PRE51]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'It''s time now to fit our decision tree:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候拟合我们的决策树了：
- en: '[PRE52]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Finally, we store our models and the corresponding rotation matrices:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们存储我们的模型和相应的旋转矩阵：
- en: '[PRE53]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'With our model built, let us proceed to see how good our model is with both
    the train and the dev data, using the `model_worth` function:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 构建好模型后，让我们使用`model_worth`函数来检验模型在训练数据和开发数据上的表现：
- en: '[PRE54]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let us take a look at our model_worth function:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的`model_worth`函数：
- en: '[PRE55]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Inside the function with perform prediction using each of the tree we have
    built. However, before the prediction, we project our input using the rotation
    matrix. We store all our prediction output in a list called `predicted_ys`. Let
    us say we have 100 instances to predict, and we have 10 models in our tree. For
    each instance, we have 10 predictions. We store those as a matrix for convenience:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数内部，我们使用每棵树进行预测。然而，在预测之前，我们先使用旋转矩阵对输入数据进行投影。我们将所有预测的输出存储在一个名为`predicted_ys`的列表中。假设我们有100个实例需要预测，并且我们有10个模型。在每个实例中，我们会有10个预测结果。为了方便起见，我们将这些结果存储为一个矩阵：
- en: '[PRE56]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now we proceed to give a final classification for each of our input records:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续为每个输入记录给出最终分类：
- en: '[PRE57]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We will store our final prediction in a list called `final_prediction`. We go
    through each of the predictions for our instance. Say we are in the first instance
    (i=0 in our for loop); `pred_from_all_models` stores the output from all the trees
    in our model. It's an array of 0s and 1s indicating the class which has the model
    classified at that instance.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会将最终的预测结果存储在一个名为`final_prediction`的列表中。我们遍历每个实例的预测结果。假设我们在第一个实例（在for循环中i=0）；`pred_from_all_models`存储来自我们模型中所有树的输出。它是一个由0和1组成的数组，表示在该实例中模型所分类的类别。
- en: We make another array out of it `non_zero_pred` which has only those entries
    from the parent arrays which are non-zero.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其转换为另一个数组`non_zero_pred`，该数组只包含父数组中非零的条目。
- en: Finally, if the length of this non-zero array is greater than half the number
    of models that we have, we say our final prediction is 1 for the instance of interest.
    What we have accomplished here is the classic voting scheme.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果这个非零数组的长度大于我们模型数量的一半，我们就说我们最终的预测结果是该实例的类别为1。我们在这里实现的是经典的投票机制。
- en: 'Let us look at how good our models are now by calling `classification_report`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过调用`classification_report`来看看我们的模型现在有多好：
- en: '[PRE58]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The following is the performance of our model on the training set:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们模型在训练集上的表现：
- en: '![How it works…](img/B04041_09_09.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_09_09.jpg)'
- en: 'Let us look at our model''s performance on the dev dataset:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的模型在开发数据集上的表现：
- en: '![How it works…](img/B04041_09_10.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_09_10.jpg)'
- en: There's more…
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'More information about Rotational forests can be gathered from the following
    paper:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 有关旋转森林的更多信息可以参考以下论文：
- en: '*Rotation Forest: A New Classifier Ensemble Method, Juan J. Rodriguez, Member,
    IEEE Computer Society, Ludmila I. Kuncheva, Member, IEEE, and Carlos J. Alonso*'
  id: totrans-283
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*旋转森林：一种新的分类器集成方法，Juan J. Rodriguez，IEEE计算机学会会员，Ludmila I. Kuncheva，IEEE会员，Carlos
    J. Alonso*'
- en: ''
  id: totrans-284
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*The paper also claims that when Extremely Randomized Trees was compared to
    Bagging, AdBoost, and Random Forest on 33 datasets, Extremely Randomized Trees
    outperformed all the other three algorithms.*'
  id: totrans-285
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*该论文还声称，在33个数据集上将极度随机化树与Bagging、AdBoost和随机森林进行比较时，极度随机化树的表现优于其他三种算法。*'
- en: ''
  id: totrans-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Similar to Gradient Boosting, the authors of the paper claim that the Extremely
    Randomized method is an overall framework, and the underlying ensemble does not
    necessarily have to be a Decision Tree. Work is in progress on testing other algorithms
    like Naïve Bayes, Neural Networks, and others.*'
  id: totrans-287
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*与梯度提升法类似，论文的作者声称极度随机化方法是一个总体框架，且基础集成方法不一定非得是决策树。目前正在进行其他算法的测试工作，如朴素贝叶斯、神经网络等。*'
- en: See also
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Extracting Principal Components* recipe in [Chapter 4](ch04.xhtml "Chapter 4. Data
    Analysis – Deep Dive"), *Analyzing Data - Deep Dive*'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提取主成分* 配方见于[第4章](ch04.xhtml "第4章 数据分析 - 深度解析")，*数据分析 - 深度解析*'
- en: '*Reducing data dimension by Random Projection* recipe in [Chapter 4](ch04.xhtml
    "Chapter 4. Data Analysis – Deep Dive"), *Analyzing Data - Deep Dive*'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过随机投影减少数据维度* 配方见于[第4章](ch04.xhtml "第4章 数据分析 - 深度解析")，*数据分析 - 深度解析*'
- en: '*Building Decision Trees to solve Multi Class Problems* recipe in [Chapter
    6](ch06.xhtml "Chapter 6. Machine Learning 1"), *Machine Learning I*'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*建立决策树以解决多分类问题* 配方见于[第6章](ch06.xhtml "第6章 机器学习I")，*机器学习I*'
- en: '*Understanding Ensemble, Gradient Boosting* recipe in [Chapter 8](ch08.xhtml
    "Chapter 8. Ensemble Methods"), *Model Selection and Evaluation*'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解集成方法，梯度提升法* 配方见于[第8章](ch08.xhtml "第8章 集成方法")，*模型选择与评估*'
- en: '*Growing from trees to Forest, Random Forest* recipe in [Chapter 9](ch09.xhtml
    "Chapter 9. Growing Trees"), *Machine Learning III*'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从树到森林，随机森林* 配方见于[第9章](ch09.xhtml "第9章 树的生长")，*机器学习III*'
- en: '*Growing Extremely Randomized Trees* recipe in [Chapter 9](ch09.xhtml "Chapter 9. Growing
    Trees"), *Machine Learning III*'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生长极度随机化树* 配方见于[第9章](ch09.xhtml "第9章 树的生长")，*机器学习III*'
