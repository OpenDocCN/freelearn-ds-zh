- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Support Vector Machine
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: This chapter explores a classic algorithm that one must keep in one’s machine
    learning arsenal called the **support vector machine** (**SVM**), which is mainly
    used for classification problems rather than regression problems. Since its inception
    in the 1990s, it was commonly used to recognize patterns and outliers in data.
    Its popularity declined after the emergence of boosting algorithms such as **extreme
    gradient boost** (**XGB**). However, it prevails as one of the most commonly used
    supervised learning algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了一种经典算法，它是每个机器学习工具库中都必须掌握的算法——**支持向量机**（**SVM**），主要用于分类问题而非回归问题。自1990年代问世以来，它通常用于识别数据中的模式和异常值。在
    **极限梯度提升**（**XGB**）等提升算法出现后，它的受欢迎程度有所下降。然而，它仍然是最常用的监督学习算法之一。
- en: In the 1990s, efficient learning algorithms based on computational learning
    were developed for non-linear functions. Algorithms such as linear learning algorithms
    have well-defined theoretical properties. With this development, efficient separability
    (decision surfaces) of nonlinear regions that use kernel functions was established.
    Nonlinear SVMs are quite frequently used for the classification of real (nonlinear)
    data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在1990年代，基于计算学习的高效学习算法被开发出来，用于处理非线性函数。诸如线性学习算法等算法具有明确定义的理论特性。随着这一发展的出现，使用核函数的非线性区域的高效可分性（决策面）被建立起来。非线性
    SVM 被广泛应用于实际（非线性）数据的分类中。
- en: SVM was initially known as a binary classifier that could be used for one-class
    classification of skewed or imbalanced class distribution. This unsupervised algorithm
    could effectively learn from the majority or normal class in a dataset to classify
    new data points as either *normal* or *outlier*. The process of identifying the
    minority or rarity class generally referred to as outlier is called **anomaly
    detection**, as the outlier is an anomaly and the rest of the data is normal.
    Classification involves fitting a model on the normal data (training examples)
    and predicting whether incoming new data is normal (inlier) or outlier. One-class
    SVM is most suited for a specific problem where the minority class does not have
    a consistent pattern or is a noisy instance, making it difficult for other classification
    algorithms to learn a decision boundary. The outliers in general are treated as
    deviations from normal.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 最初被称为一种二分类器，可以用于处理类别分布不平衡或倾斜的一类分类问题。这个无监督算法可以有效地从数据集中的大多数或正常类别中学习，将新的数据点分类为
    *正常* 或 *异常值*。识别少数类或稀有类（通常被称为异常值）的过程被称为 **异常检测**，因为异常值是异常的，其他数据点则是正常的。分类过程包括在正常数据（训练样本）上拟合模型，并预测传入的新数据是正常的（内点）还是异常的。单类
    SVM 最适合于一种特定的问题，即少数类没有一致的模式或是噪声实例，导致其他分类算法难以学习决策边界。异常值通常被视为与正常数据的偏差。
- en: In general, SVMs are effective in problems where the number of variables is
    greater than the number of records, meaning, in high-dimensional spaces. The algorithm
    uses a subset of training examples in the decision function, hence it is memory-efficient.
    It turns out the algorithm is versatile, as different kernels can be specified
    for the decision function.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，SVM 在变量数量大于记录数的问题中有效，即在高维空间中。该算法在决策函数中使用训练样本的子集，因此具有内存效率。事实证明，该算法非常灵活，因为可以为决策函数指定不同的核函数。
- en: 'This chapter covers the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Support vectors in SVM
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM 中的支持向量
- en: Kernels for SVM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM 的核函数
- en: Implementation of SVM
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM 的实现
- en: We will learn about support vectors and kernels in the forthcoming sections.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中学习支持向量和核函数。
- en: Support vectors in SVM
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVM 中的支持向量
- en: 'SVM is an algorithm that can produce significantly accurate results with less
    computation power. It is widely used in data classification tasks. If a dataset
    has *n* number of features, SVM finds a hyperplane in the *n*-dimensional space,
    which is also called the **decision boundary**, to classify the data points. An
    optimal decision boundary maximizes the distance between the boundary and instances
    in both classes. The distance between data points in the classes (shown in *Figure
    5**.1a*) is known as the **margin**:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 是一种可以以较少计算资源产生显著准确结果的算法，广泛应用于数据分类任务。如果一个数据集有 *n* 个特征，SVM 会在 *n* 维空间中找到一个超平面，也称为
    **决策边界**，以分类数据点。最优的决策边界最大化边界与两类实例之间的距离。类别中数据点之间的距离（如 *图 5.1a* 所示）被称为 **间隔**：
- en: '![Figure 5.1a: Optimal hyperplane](img/Figure_05_01_B18943.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1a：最优超平面](img/Figure_05_01_B18943.jpg)'
- en: 'Figure 5.1a: Optimal hyperplane'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1a：最优超平面
- en: 'An SVM algorithm finds the optimal line in two dimensions or the optimal hyperplane
    in more than two dimensions that separates the space into classes. The optimal
    hyperplane or optimal line maximizes the margin (the distance between the data
    points of the two classes). In 3D (or more), data points become vectors and those
    (very small subset of training examples) that are closest to or on the hyperplanes
    (just outside the maximum margin) are called **support vectors** (see *Figure
    5**.1b*):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 算法在二维空间中找到最优直线，或在多维空间中找到最优超平面，从而将空间分割成不同类别。最优超平面或最优直线最大化边距（即两个类别的数据点之间的距离）。在三维空间（或更高维度）中，数据点变成了向量，而那些最靠近或在超平面上的数据点（位于最大边距外的极小子集的训练样本）被称为**支持向量**（见
    *图 5.1b*）：
- en: '![Figure 5.1b: Support vectors](img/Figure_05_02_B18943.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1b：支持向量](img/Figure_05_02_B18943.jpg)'
- en: 'Figure 5.1b: Support vectors'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1b：支持向量
- en: If all support vectors are at the same distance from the optimal hyperplane,
    the margin is said to be good. The margin shown in *Figure 5**.1b* is bad, as
    support vectors in class +1 are very close to the optimal hyperplane, while those
    in class -1 are far away from it. Moving a support vector moves the decision boundary
    or hyperplane while moving other data points has no effect.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有支持向量距离最优超平面相同，则表示边距很好。图中的边距（*图 5.1b*）较差，因为类别 +1 的支持向量非常接近最优超平面，而类别 -1 的支持向量距离最优超平面较远。移动一个支持向量会移动决策边界或超平面，而移动其他数据点则不会产生影响。
- en: 'If the number of features in an input dataset is two, the hyperplane is just
    a line. If the number is three, then the hyperplane (shown in *Figure 5**.2*)
    is a two-dimensional plane. The dimension of the decision boundary depends on
    the number of features, and the data points on either side of it (the hyperplane)
    belong to different classes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入数据集的特征数为 2，则超平面仅为一条直线。如果特征数为 3，则超平面（见 *图 5.2*）是一个二维平面。决策边界的维度取决于特征的数量，而其两侧（超平面两侧）属于不同类别的数据点：
- en: '![Figure 5.2: Hyperplane in 3D feature space](img/Figure_05_03_B18943.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2：三维特征空间中的超平面](img/Figure_05_03_B18943.jpg)'
- en: 'Figure 5.2: Hyperplane in 3D feature space'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：三维特征空间中的超平面
- en: 'Support vectors influence the position and orientation of the hyperplane. The
    margin of the classifier is maximized using support vectors. The margin is hard
    if the data is linearly separable. For most practical problems, data is not linearly
    separable, and in such cases the margin is soft. This allows for data points within
    the marginal distance (shown in *Figure 5**.3*) between two data class separators:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量影响超平面的位置和方向。使用支持向量可以最大化分类器的边距。如果数据是线性可分的，则边距为硬边距。对于大多数实际问题，数据是非线性可分的，因此在这种情况下，边距为软边距。这允许数据点位于两个类别分隔符之间的边距距离内（见
    *图 5.3*）：
- en: '![Figure 5.3: Soft margin](img/Figure_05_04_B18943.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3：软边距](img/Figure_05_04_B18943.jpg)'
- en: 'Figure 5.3: Soft margin'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：软边距
- en: It is better to have a large margin that might allow for some margin violation
    to occur. The larger the margin, the lower the error of the classifier. Maximizing
    the margin is equivalent to minimizing loss in machine learning algorithms. The
    function that helps maximize the margin is **hinge loss**. Hinge loss (error)
    is zero if data is classified correctly, meaning we have a hard margin as the
    points are not close to the hyperplane. Hinge loss is one if most of the data
    points are classified incorrectly. In general, support vectors are within the
    margin boundaries (soft margin) when the problem is not linearly separable.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的边距更好，它可能会允许一些边距违规的发生。边距越大，分类器的误差越低。最大化边距等同于在机器学习算法中最小化损失。帮助最大化边距的函数是**铰链损失**。如果数据正确分类，则铰链损失（错误）为零，意味着我们有一个硬边距，因为数据点距离超平面较远。若大多数数据点被错误分类，则铰链损失为一。通常，当问题无法线性分割时，支持向量位于边距边界内（软边距）。
- en: In the next section, the kernel trick is introduced. Kernel is a technique used
    in SVMs to classify data points that are not linearly separable. Kernel functions
    enable operation in a high-dimensional feature space without computing data coordinates
    in that space, hence this operation is not computationally expensive.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，将介绍核技巧。核函数是 SVM 中用来分类非线性可分数据点的一种技术。核函数使得我们可以在高维特征空间中进行操作，而无需计算数据在该空间中的坐标，因此该操作在计算上并不昂贵。
- en: Kernels for SVM
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVM 的核函数
- en: With a kernel trick, a 2D space is converted into a 3D space using a mapping
    function such that the nonlinear data can be classified or separated in a higher
    dimension (see *Figure 5**.4*). The transformation of original data for mapping
    into the new space is done via kernel. The kernel function defines inner products
    (measure of similarity) in the transformed space.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过核技巧，二维空间通过映射函数转换为三维空间，从而使得非线性数据能够在更高维度上进行分类或分离（见*图5**.4*）。原始数据的转换是通过核来完成的。核函数定义了在变换空间中的内积（相似性度量）。
- en: The compute and storage requirements of SVMs increase with the number of training
    examples. The core of the algorithm is a quadratic programming problem separating
    support vectors from the training dataset. A linear kernel, which is just a dot
    product, is the fastest implementation of SVM. A few examples of linear and nonlinear
    kernels are shown in *Figure 5**.5a*. The most common nonlinear SVM kernels are
    **radial basis function** (**RBF**), **sigmoid**, and **polynomial**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练样本数的增加，SVM的计算和存储需求也随之增加。该算法的核心是一个二次规划问题，将支持向量从训练数据集中分离出来。线性核是SVM最快速的实现方式，它仅是一个点积。*图5**.5a*中展示了线性和非线性核的一些示例。最常见的非线性SVM核函数有**径向基函数**（**RBF**）、**Sigmoid**和**多项式**。
- en: '![Figure 5.4: (a) Example of non-linear separator (L), and (b) Data effectively
    classified in higher dimension](img/Figure_05_05_B18943.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4：（a）非线性分隔符示例（L），和（b）数据在更高维度中有效分类](img/Figure_05_05_B18943.jpg)'
- en: 'Figure 5.4: (a) Example of non-linear separator (L), and (b) Data effectively
    classified in higher dimension'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：（a）非线性分隔符示例（L），和（b）数据在更高维度中有效分类
- en: SVMs are very effective for small datasets that are not linearly separable.
    Small data means that the number of features is more than the training size, due
    to which SVMs suffer from overfitting in some cases. The right kernel function
    and regularization (penalty function) come to the rescue in those cases. Each
    kernel has a different mathematical formulation, hence the set of parameters varies
    from one to another.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SVM对于那些非线性可分的小型数据集非常有效。所谓的小型数据，是指特征数大于训练样本数，这会导致SVM在某些情况下出现过拟合。在这些情况下，合适的核函数和正则化（惩罚函数）可以帮助解决问题。每个核函数有不同的数学形式，因此它们的参数集合各不相同。
- en: '![Figure 5.5a: Data classification using linear kernel (L), RBF kernel (M),
    and polynomial kernel (R) functions](img/Figure_05_06_B18943.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5a：使用线性核（L）、RBF核（M）和多项式核（R）函数进行数据分类](img/Figure_05_06_B18943.jpg)'
- en: 'Figure 5.5a: Data classification using linear kernel (L), RBF kernel (M), and
    polynomial kernel (R) functions'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5a：使用线性核（L）、RBF核（M）和多项式核（R）函数进行数据分类
- en: 'The parameter exponent (degree) in a polynomial kernel when set to 1 becomes
    a linear kernel and when set to 3 becomes a cubic kernel, an example of which
    is shown in *Figure 5**.5a* (rightmost). The sigmoid kernel cumulative distribution
    function goes from 0 to 1 to classify data and is mostly used as an activation
    function or perceptron in neural networks. An example of data classification by
    SVM with sigmoid kernel function is shown in *Figure 5**.5b*:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式核的参数指数（度数）设为1时成为线性核，设为3时成为三次核，其中一个示例如*图5**.5a*（最右侧）所示。Sigmoid核的累积分布函数从0到1分类数据，通常作为神经网络中的激活函数或感知机使用。使用Sigmoid核函数进行SVM数据分类的示例见*图5**.5b*：
- en: '![Figure 5.5b: Sigmoid kernel (L), data classified using sigmoid (R)](img/Figure_05_07_B18943.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5b：Sigmoid核（L），使用sigmoid分类的数据（R）](img/Figure_05_07_B18943.jpg)'
- en: 'Figure 5.5b: Sigmoid kernel (L), data classified using sigmoid (R)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5b：Sigmoid核（L），使用sigmoid分类的数据（R）
- en: All SVM kernels have a parameter that trades off the misclassification of the
    dataset against the simplicity of the separator. While training an SVM with the
    RBF kernel, which is an exponential (![](img/Formula_05_001.png)) function, the
    parameter *a* is greater than zero and defines the influence of a training example
    on the separator. The selection of the parameters in respective kernel functions
    is critical to an SVM’s performance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的SVM核函数都有一个参数，用于在数据集的误分类与分隔器的简洁性之间进行权衡。在使用RBF核（它是一个指数型函数（![](img/Formula_05_001.png))）训练SVM时，参数*a*大于零，并定义了一个训练样本对分隔器的影响。各个核函数参数的选择对于SVM的性能至关重要。
- en: 'In imbalanced datasets, the parameters dedicated to providing weights on classes
    and samples become significant, as they might be required to give more importance
    to a certain sample or class in such cases. The effect of sample weighting on
    the class boundary is shown in *Figure 5**.6*, wherein the data point size is
    proportional to the sample weights:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在不平衡数据集中，用于为类和样本提供权重的参数变得至关重要，因为在这种情况下，可能需要赋予某个样本或类更多的权重。样本加权对类边界的影响如*图 5.6*所示，其中数据点的大小与样本权重成比例：
- en: '![Figure 5.6: Classification with constant sample weight (L), with modified
    weight (R)](img/Figure_05_08_B18943.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6：使用恒定样本权重（左图）与修改后权重（右图）的分类](img/Figure_05_08_B18943.jpg)'
- en: 'Figure 5.6: Classification with constant sample weight (L), with modified weight
    (R)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：使用恒定样本权重（左图）与修改后权重（右图）的分类
- en: While various methods and algorithms can detect outliers in a dataset, the kernel
    method used by the one-class SVM algorithm has been demonstrated in this chapter.
    Other examples include the decision tree ensemble method in the Isolation Forest
    algorithm, the distance or density method in the local outlier factor algorithm,
    and so on. Anomaly types can be *point* or *collective*, and one selects the algorithm
    for detection based on the anomaly type in a dataset. *Figures 5.7a and 5.7b*
    show examples of these anomaly types. A point anomaly is a global behavior while
    a collective anomaly is a local abnormal (non-normal) behavior. There can also
    be datasets wherein an anomaly can be entirely contextual, which most of the time
    is visible in time-series data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然各种方法和算法可以检测数据集中的离群点，但本章演示了由一类支持向量机（SVM）算法使用的核方法。其他示例包括孤立森林算法中的决策树集成方法、局部离群因子算法中的距离或密度方法等。异常类型可以是*点*异常或*集体*异常，选择检测算法时需要根据数据集中的异常类型进行选择。*图
    5.7a 和 5.7b* 展示了这些异常类型的示例。点异常是全局行为，而集体异常是局部异常（非正常）行为。还可能存在一些数据集，其中异常是完全上下文相关的，这在时间序列数据中大多能看到。
- en: '![Figure 5.7a: Examples of point anomalies](img/Figure_05_09_B18943.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7a：点异常的示例](img/Figure_05_09_B18943.jpg)'
- en: 'Figure 5.7a: Examples of point anomalies'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7a：点异常的示例
- en: '![ Figure 5.7b: Example of a collective (non-point) anomaly](img/Figure_05_10_B18943.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7b：集体（非点）异常的示例](img/Figure_05_10_B18943.jpg)'
- en: 'Figure 5.7b: Example of a collective (non-point) anomaly'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7b：集体（非点）异常的示例
- en: In the following section, we will implement a one-class SVM solution using Python,
    as this solution in general has proven to be useful for problems where the (point)
    outliers forming the minority class lack structure and are predominantly noisy
    examples (i.e. severe deviations from inliers).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将使用 Python 实现一类 SVM 解决方案，因为该解决方案通常在解决那些少数类离群点没有结构并且主要是噪声样本（即与正常点的偏差严重）的情况下被证明非常有用。
- en: Implementation of SVM
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVM 实现
- en: The one-class SVM algorithm does not use (ignores) the examples that are far
    from or deviated from the observations during training. Only the observations
    that are most concentrated or dense are leveraged for (unsupervised) learning
    and such an approach is effective in specific problems where very few deviations
    from normal are expected.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一类支持向量机（SVM）算法在训练过程中不使用（忽略）远离或偏离观察值的示例。只有那些最集中或密集的观察值用于（无监督）学习，这种方法在那些期望只有极少数偏离正常的特定问题中非常有效。
- en: 'A synthetic dataset is created to implement SVM. We will have about 2% of the
    synthetic data in the minority class (outliers) denoted by `1` and 98% in the
    majority class (inliers) denoted by `0`, and leverage the RBF kernel to map the
    data into a high-dimensional space. The Python code (with the scikit-learn library)
    runs as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现 SVM，创建了一个合成数据集。该数据集约有 2% 的少数类（离群点），用 `1` 表示，98% 的多数类（正常点）用 `0` 表示，并利用 RBF
    核将数据映射到高维空间。以下是使用 Python（配合 scikit-learn 库）运行的代码：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The report of the classifier (*Figure 5**.8*) clearly shows that the one-class
    SVM model has a recall of 23%, which means the model captures 23% of outliers.
    The F1-score is the harmonic mean of the two measures, namely precision and recall:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的报告（*图 5.8*）清楚地显示，一类 SVM 模型的召回率为 23%，这意味着该模型捕捉到了 23% 的离群点。F1 分数是精确度和召回率的调和平均数：
- en: '![Figure 5.8: Classification report of one-class SVM](img/Figure_05_11_B18943.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8：一类 SVM 的分类报告](img/Figure_05_11_B18943.jpg)'
- en: 'Figure 5.8: Classification report of one-class SVM'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：一类 SVM 的分类报告
- en: 'We will visualize the outliers using the following code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码来可视化异常值：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The default threshold of the algorithm for identifying these 2% outliers can
    also be customized so that fewer or more data points are labeled as outliers depending
    on the use case. What is evident from *Figure 5**.9* is that most of the outliers
    (yellow) have been detected correctly by the classifier:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 用于识别这2%异常值的算法默认阈值也可以定制，以便根据不同的使用场景，标记更多或更少的数据点为异常值。从*图5.9*中可以明显看出，大多数异常值（黄色）已经被分类器正确检测出来：
- en: '![Figure 5.9: Classification by one-class SVM](img/Figure_05_12_B18943.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9：一类SVM分类](img/Figure_05_12_B18943.jpg)'
- en: 'Figure 5.9: Classification by one-class SVM'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：一类SVM分类
- en: One-class SVM is particularly useful as an anomaly detector and finds wide usage
    in sensor data captured from machines in the manufacturing industry.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一类SVM特别适合作为异常检测器，并在制造业中通过从机器捕获的传感器数据得到了广泛应用。
- en: Summary
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored SVM as a classifier. In addition to linear data,
    SVMs can efficiently classify non-linear data using kernel functions. The method
    used by the SVM algorithm can be extended to solve regression problems. SVM is
    utilized for novelty detection as well, wherein the training dataset is not polluted
    with outliers and the algorithm is exploited to detect a new observation as an
    anomaly, in which case the outlier is called a novelty.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们探讨了支持向量机（SVM）作为分类器的应用。除了线性数据外，SVM还可以通过使用核函数高效地分类非线性数据。SVM算法所使用的方法也可以扩展用于解决回归问题。SVM同样可以用于新颖性检测，其中训练数据集不会被异常值污染，算法被用来将新的观测值检测为异常值，这时异常值被称为新颖性。
- en: The next chapter is about graph theory, a tool that provides the necessary mathematics
    to quantify and simplify complex systems. Graph theory is the study of relations
    (connections or edges) between a set of nodes or individual entities in a dynamic
    system. It is an integral component of ML and DL because graphs provide a means
    to represent a business problem as a mathematical programming task in the form
    of nodes and edges.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍图论，它提供了必要的数学工具来量化和简化复杂系统。图论研究的是在动态系统中，节点或个体实体之间的关系（连接或边）。它是机器学习（ML）和深度学习（DL）的重要组成部分，因为图提供了一种将商业问题表示为数学编程任务的方式，形式为节点和边。
