- en: Chapter 5. Spark Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。Spark Streaming
- en: Spark Streaming adds the holy grail of big data processing—that is, real-time
    analytics—to Apache Spark. It enables Spark to ingest live data streams and provides
    real-time intelligence at a very low latency of a few seconds.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming为Apache Spark增加了大数据处理的圣杯——即实时分析。它使Spark能够摄取实时数据流，并以极低的延迟（几秒钟）提供实时智能。
- en: 'In this chapter, we''ll cover the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下配方：
- en: Word count using Streaming
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流式处理的单词计数
- en: Streaming Twitter data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式处理Twitter数据
- en: Streaming using Kafka
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kafka进行流式处理
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Streaming is the process of dividing continuously flowing input data into discreet
    units so that it can be processed easily. Familiar examples in real life are streaming
    video and audio content (though a user can download the full movie before he/she
    can watch it, a faster solution is to stream data in small chunks that start playing
    for the user while the rest of the data is being downloaded in the background).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 流式处理是将持续流动的输入数据分成离散单元的过程，以便可以轻松处理。现实生活中熟悉的例子是流式视频和音频内容（尽管用户可以在观看之前下载完整的电影，但更快的解决方案是以小块流式传输数据，这些数据开始播放给用户，而其余数据则在后台下载）。
- en: Real-world examples of streaming, besides multimedia, are the processing of
    market feeds, weather data, electronic stock trading data, and so on. All of these
    applications produce large volumes of data at very fast rates and require special
    handling of the data so that insights can be derived from data in real time.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 除了多媒体之外，流式处理的实际例子包括市场数据源、天气数据、电子股票交易数据等的处理。所有这些应用程序产生大量数据，速度非常快，并且需要对数据进行特殊处理，以便实时从数据中获取洞察。
- en: Streaming has a few basic concepts, which are better to understand before we
    focus on Spark Streaming. The rate at which a streaming application receives data
    is called **data rate** and is expressed in the form of **kilobytes per second**
    (**kbps**) or **megabytes per second** (**mbps**).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 流式处理有一些基本概念，在我们专注于Spark Streaming之前最好先了解。流式应用程序接收数据的速率称为**数据速率**，以**每秒千字节**（**kbps**）或**每秒兆字节**（**mbps**）的形式表示。
- en: One important use case of streaming is **complex event processing** (**CEP**).
    In CEP, it is important to control the scope of the data being processed. This
    scope is called window, which can be either based on time or size. An example
    of a time-based window is to analyze data that has come in last one minute. An
    example of a size-based window can be the average ask price of the last 100 trades
    of a given stock.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 流式处理的一个重要用例是**复杂事件处理**（**CEP**）。在CEP中，控制正在处理的数据范围很重要。这个范围称为窗口，可以基于时间或大小。基于时间的窗口的一个例子是分析过去一分钟内的数据。基于大小的窗口的一个例子可以是给定股票的最近100笔交易的平均要价。
- en: Spark Streaming is Spark's library that provides support to process live data.
    This stream can come from any source, such as Twitter, Kafka, or Flume.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming是Spark的库，提供支持处理实时数据。这个流可以来自任何来源，比如Twitter、Kafka或Flume。
- en: Spark Streaming has a few fundamental building blocks that we need to understand
    well before diving into the recipes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究配方之前，Spark Streaming有一些基本构建块，我们需要充分理解。
- en: Spark Streaming has a context wrapper called `StreamingContext`, which wraps
    around `SparkContext` and is the entry point to the Spark Streaming functionality.
    Streaming data, by definition, is continuous and needs to be time-sliced to process.
    This slice of time is called the **batch interval**, which is specified when `StreamingContext`
    is created. There is one-to-one mapping of RDD and batch, that is, each batch
    results in one RDD. As you can see in the following image, Spark Streaming takes
    continuous data, break it into batches and feed to Spark.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming有一个称为`StreamingContext`的上下文包装器，它包装在`SparkContext`周围，并且是Spark
    Streaming功能的入口点。流式数据根据定义是连续的，需要进行时间切片处理。这段时间被称为**批处理间隔**，在创建`StreamingContext`时指定。RDD和批处理之间是一对一的映射，也就是说，每个批处理都会产生一个RDD。正如您在下图中所看到的，Spark
    Streaming接收连续数据，将其分成批次并馈送给Spark。
- en: '![Introduction](img/3056_05_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![Introduction](img/3056_05_01.jpg)'
- en: Batch interval is important to optimize your streaming application. Ideally,
    you want to process data at least as fast as it is getting ingested; otherwise,
    your application will develop a backlog. Spark Streaming collects data for the
    duration of a batch interval, say, 2 seconds. The moment this 2 second interval
    is over, data collected in that interval will be given to Spark for processing
    and Streaming will focus on collecting data for the next batch interval. Now,
    this 2 second batch interval is all Spark has to process data, as it should be
    free to receive data from the next batch. If Spark can process the data faster,
    you can reduce the batch interval to, say, 1 second. If Spark is not able to keep
    up with this speed, you have to increase the batch interval.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理间隔对于优化流式应用程序非常重要。理想情况下，您希望至少以数据获取的速度进行处理；否则，您的应用程序将产生积压。Spark Streaming在一个批处理间隔的持续时间内收集数据，比如2秒。一旦这个2秒的间隔结束，该间隔内收集的数据将被交给Spark进行处理，而流式处理将专注于收集下一个批处理间隔的数据。现在，这个2秒的批处理间隔是Spark处理数据的全部时间，因为它应该空闲以接收下一个批处理的数据。如果Spark能够更快地处理数据，您可以将批处理间隔减少到1秒。如果Spark无法跟上这个速度，您必须增加批处理间隔。
- en: The continuous stream of RDDs in Spark Streaming needs to be represented in
    the form of an abstraction through which it can be processed. This abstraction
    is called **Discretized Stream** (**DStream**). Any operation applied on DStream
    results in an operation on underlying RDDs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming中的RDD的连续流需要以一种抽象的形式表示，通过这种抽象可以对其进行处理。这种抽象称为**离散流**（**DStream**）。对DStream应用的任何操作都会导致对底层RDD的操作。
- en: 'Every input DStream is associated with a receiver (except for file stream).
    A receiver receives data from the input source and stores it in Spark''s memory.
    There are two types of streaming sources:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入DStream都与一个接收器相关联（除了文件流）。接收器从输入源接收数据并将其存储在Spark的内存中。有两种类型的流式源：
- en: Basic sources, such as file and socket connections
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本来源，如文件和套接字连接
- en: Advanced sources, such as Kafka and Flume
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级来源，如Kafka和Flume
- en: 'Spark Streaming also provides windowed computations in which you can apply
    the transformation over a sliding window of data. A sliding window operation is
    based on two parameters:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming还提供了窗口计算，您可以在其中对数据的滑动窗口应用转换。滑动窗口操作基于两个参数：
- en: '**Window length**: This is the duration of the window. For example, if you
    want to get analytics of the last 1 minute of data, the window length will be
    1 minute.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**窗口长度**：这是窗口的持续时间。例如，如果您想要获取最后1分钟的数据分析，窗口长度将是1分钟。'
- en: '**Sliding interval**: This depicts how frequently you want to perform an operation.
    Say you want to perform the operation every 10 seconds; this means that every
    10 seconds, 1 minute of window will have 50 seconds of data common with the last
    window and 10 seconds of the new data.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滑动间隔**：这表示您希望多频繁执行操作。比如您希望每10秒执行一次操作；这意味着每10秒，窗口的1分钟将有50秒的数据与上一个窗口相同，以及10秒的新数据。'
- en: Both these parameters work on underlying RDDs that, obviously, cannot be broken
    apart; therefore, both of these should be a multiple of the batch interval. The
    window length has to be a multiple of the sliding interval as well.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个参数都作用于底层的RDD，显然不能被分开；因此，这两个参数都应该是批处理间隔的倍数。窗口长度也必须是滑动间隔的倍数。
- en: DStream also has output operations, which allow data to be pushed to external
    systems. They are similar to actions on RDDs (one higher level of abstraction
    of what you do at DStream happens to RDDs).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: DStream还具有输出操作，允许将数据推送到外部系统。它们类似于RDD上的操作（在DStream上发生的抽象级别更高）。
- en: Besides print to print content of DStream, standard RDD actions, such as `saveAsTextFile`,
    `saveAsObjectFile`, and `saveAsHadoopFile`, are supported by similar counterparts,
    such as `saveAsTextFiles`, `saveAsObjectFiles`, and `saveAsHadoopFiles`, respectively.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了打印DStream的内容之外，还支持标准RDD操作，例如`saveAsTextFile`，`saveAsObjectFile`和`saveAsHadoopFile`，分别由类似的对应物`saveAsTextFiles`，`saveAsObjectFiles`和`saveAsHadoopFiles`。
- en: One very useful output operation is `foreachRDD(func)`, which applies any arbitrary
    function to all the RDDs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常有用的输出操作是`foreachRDD(func)`，它将任意函数应用于所有RDD。
- en: Word count using Streaming
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用流媒体进行单词计数
- en: Let's start with a simple example of Streaming in which in one terminal, we
    will type some text and the Streaming application will capture it in another window.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的流媒体示例开始，在其中一个终端中，我们将输入一些文本，流媒体应用程序将在另一个窗口中捕获它。
- en: How to do it...
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell并为其提供一些额外的内存：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Stream specific imports:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流特定的导入：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Import for an implicit conversion:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐式转换的导入：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create `StreamingContext` with a 2 second batch interval:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '使用2秒批处理间隔创建`StreamingContext`： '
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create a `SocketTextStream` Dstream on localhost with port `8585` with the
    `MEMORY_ONLY` caching:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地主机上使用端口`8585`创建一个`SocketTextStream` Dstream，并使用`MEMORY_ONLY`缓存：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Divide the lines into multiple words:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将行分成多个单词：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Convert word to (word,1), that is, output `1` as the value for each occurrence
    of a word as the key:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单词转换为（单词，1），即将`1`作为单词的每次出现的值输出为键：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Use the `reduceByKey` method to add a number of occurrences for each word as
    the key (the function works on two consecutive values at a time, represented by
    `a` and `b`):'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`reduceByKey`方法为每个单词的出现次数添加一个数字作为键（该函数一次处理两个连续的值，由`a`和`b`表示）：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Print `wordCount`:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`wordCount`：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Start `StreamingContext`; remember, nothing happens until `StreamingContext`
    is started:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动`StreamingContext`；记住，直到启动`StreamingContext`之前什么都不会发生：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, in a separate window, start the netcat server:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在一个单独的窗口中，启动netcat服务器：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Enter different lines, such as `to be or not to be`:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入不同的行，例如`to be or not to be`：
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Check the Spark shell, and you will see word count results like the following
    screenshot:![How to do it...](img/3056_05_02.jpg)
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查Spark shell，您将看到类似以下截图的单词计数结果：![如何做...](img/3056_05_02.jpg)
- en: Streaming Twitter data
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流媒体Twitter数据
- en: Twitter is a famous microblogging platform. It produces a massive amount of
    data with around 500 million tweets sent each day. Twitter allows its data to
    be accessed by APIs and that makes it the poster child of testing any big data
    streaming application.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter是一个著名的微博平台。它每天产生大量数据，大约有5亿条推文。Twitter允许通过API访问其数据，这使其成为测试任何大数据流应用程序的典范。
- en: In this recipe, we will see how we can live stream data in Spark using Twitter
    streaming libraries. Twitter is just one source of providing the streaming data
    to Spark and has no special status. Therefore, there are no built-in libraries
    for Twitter. Spark does provide some APIs to facilitate integration with Twitter
    libraries, though.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将看到如何使用Twitter流媒体库在Spark中实时流式传输数据。Twitter只是提供流数据给Spark的一个来源，并没有特殊的地位。因此，Twitter没有内置的库。尽管如此，Spark确实提供了一些API来促进与Twitter库的集成。
- en: An example use of live Twitter data feed can be to find trending tweets in the
    last 5 minutes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用实时Twitter数据源的一个示例用途是查找过去5分钟内的热门推文。
- en: How to do it...
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: Create a Twitter account if you do not already have one.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您还没有Twitter帐户，请创建一个Twitter帐户。
- en: Go to [http://apps.twitter.com](http://apps.twitter.com).
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到[http://apps.twitter.com](http://apps.twitter.com)。
- en: Click on **Create New App**.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建新应用**。
- en: Enter **Name**, **Description**, **Website**, and **Callback URL**, and then
    click on **Create your Twitter Application**.![How to do it...](img/3056_05_03.jpg)
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入**名称**，**描述**，**网站**和**回调URL**，然后点击**创建您的Twitter应用程序**。![如何做...](img/3056_05_03.jpg)
- en: You will reach **Application Management** screen.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将到达**应用程序管理**屏幕。
- en: Navigate to **Keys and Access Tokens** | **Create my access Token**.![How to
    do it...](img/3056_05_04.jpg)
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到**密钥和访问令牌** | **创建我的访问令牌**。![如何做...](img/3056_05_04.jpg)
- en: 'Note down the four values in this screen that we will use in step 14:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记下屏幕上的四个值，我们将在第14步中使用：
- en: '**Consumer Key (API Key)**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**消费者密钥（API密钥）**'
- en: '**Consumer Secret (API Secret)**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**消费者密钥（API密钥）**'
- en: '**Access Token**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**访问令牌**'
- en: '**Access Token Secret**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 访问令牌密钥
- en: 'We will need to provide the values in this screen in some time, but, for now,
    let''s download the third-party libraries needed from Maven central:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将需要在一段时间内在这个屏幕上提供这些值，但是，现在，让我们从Maven中央库下载所需的第三方库：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Open the Spark shell, supplying the preceding three JARS as the dependency:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Spark shell，提供前面三个JAR作为依赖项：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Perform imports that are Twitter-specific:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行特定于Twitter的导入：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Stream specific imports:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流特定的导入：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Import for an implicit conversion:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入隐式转换：
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create `StreamingContext` with a 10 second batch interval:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用10秒批处理间隔创建`StreamingContext`：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Create `StreamingContext` with a 2 second batch interval:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用2秒批处理间隔创建`StreamingContext`：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: These are sample values and you should put your own values.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是示例值，您应该放入自己的值。
- en: 'Create Twitter DStream:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Twitter DStream：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Filter out English tweets:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤掉英文推文：
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Get text out of the tweets:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从推文中获取文本：
- en: '[PRE21]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Set the checkpoint directory:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置检查点目录：
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Start `StreamingContext`:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动`StreamingContext`：
- en: '[PRE23]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can put all these commands together using `:paste`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用`:paste`将所有这些命令放在一起：
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Streaming using Kafka
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kafka进行流处理
- en: Kafka is a distributed, partitioned, and replicated commit log service. In simple
    words, it is a distributed messaging server. Kafka maintains the message feed
    in categories called **topics**. An example of the topic can be a ticker symbol
    of a company you would like to get news about, for example, CSCO for Cisco.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是一个分布式、分区和复制的提交日志服务。简单地说，它是一个分布式消息服务器。Kafka将消息源维护在称为**主题**的类别中。主题的一个示例可以是您想要获取有关的公司的新闻的股票代码，例如Cisco的CSCO。
- en: 'Processes that produce messages are called **producers** and those that consume
    messages are called **consumers**. In traditional messaging, the messaging service
    has one central messaging server, also called **broker**. Since Kafka is a distributed
    messaging service, it has a cluster of brokers, which functionally act as one
    Kafka broker, as shown here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 生成消息的进程称为**生产者**，消费消息的进程称为**消费者**。在传统的消息传递中，消息服务有一个中央消息服务器，也称为**代理**。由于Kafka是一个分布式消息传递服务，它有一个代理集群，功能上充当一个Kafka代理，如下所示：
- en: '![Streaming using Kafka](img/B03056_05_06.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![使用Kafka进行流处理](img/B03056_05_06.jpg)'
- en: 'For each topic, Kafka maintains the partitioned log. This partitioned log consists
    of one or more partitions spread across the cluster, as shown in the following
    figure:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个主题，Kafka维护分区日志。这个分区日志由分布在集群中的一个或多个分区组成，如下图所示：
- en: '![Streaming using Kafka](img/B03056_05_07.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![使用Kafka进行流处理](img/B03056_05_07.jpg)'
- en: Kafka borrows a lot of concepts from Hadoop and other big data frameworks. The
    concept of partition is very similar to the concept of `InputSplit` in Hadoop.
    In the simplest form, while using `TextInputFormat`, an `InputSplit` is same as
    a block. A block is read in the form of a key-value pair in `TextInputFormat`,
    where the key is the byte offset of a line and the value is content of the line
    itself. In a similar way, in a Kafka partition, records are stored and retrieved
    in the form of key-value pairs, where the key is a sequential ID number called
    the offset and the value is the actual message.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka从Hadoop和其他大数据框架借鉴了许多概念。分区的概念与Hadoop中的`InputSplit`概念非常相似。在最简单的形式中，使用`TextInputFormat`时，`InputSplit`与块相同。块以`TextInputFormat`中的键值对形式读取，其中键是行的字节偏移量，值是行的内容本身。类似地，在Kafka分区中，记录以键值对的形式存储和检索，其中键是称为偏移量的顺序ID号，值是实际消息。
- en: In Kafka, message retention does not depend on the consumption by a consumer.
    Messages are retained for a configurable period of time. Each consumer is free
    to read messages in any order they like. All it needs to retain is an offset.
    Another analogy can be reading a book in which the page number is analogous to
    the offset, while the page content is analogous to the message. The reader is
    free to read whichever way he/she wants as long as they remember the bookmark
    (the current offset).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka中，消息的保留不取决于消费者的消费。消息将保留一段可配置的时间。每个消费者可以以任何他们喜欢的顺序读取消息。它需要保留的只是一个偏移量。另一个类比可以是阅读一本书，其中页码类似于偏移量，而页内容类似于消息。只要他们记住书签（当前偏移量），读者可以以任何方式阅读。
- en: To provide functionality similar to pub/sub and PTP (queues) in traditional
    messaging systems, Kafka has the concept of consumer groups. A consumer group
    is a group of consumers, which the Kafka cluster treats as a single unit. In a
    consumer group, only one consumer needs to receive a message. If consumer C1,
    in the following diagram, receives the first message for topic T1, all the following
    messages on that topic will also be delivered to this consumer. Using this strategy,
    Kafka guarantees the order of message delivery for a given topic.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供类似于传统消息系统中的发布/订阅和PTP（队列）的功能，Kafka有消费者组的概念。消费者组是一组消费者，Kafka集群将其视为一个单元。在消费者组中，只需要一个消费者接收消息。如果消费者C1在下图中接收主题T1的第一条消息，则该主题上的所有后续消息也将传递给该消费者。使用这种策略，Kafka保证了给定主题的消息传递顺序。
- en: In extreme cases, when all consumers are in one consumer group, the Kafka cluster
    acts like PTP/queue. In another extreme case, if every consumer belongs to a different
    group, it acts like pub/sub. In practice, each consumer group has a limited number
    of consumers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在极端情况下，当所有消费者都在一个消费者组中时，Kafka集群的行为类似于PTP/队列。在另一个极端情况下，如果每个消费者都属于不同的组，它的行为类似于发布/订阅。在实践中，每个消费者组有一定数量的消费者。
- en: '![Streaming using Kafka](img/B03056_05_08.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![使用Kafka进行流处理](img/B03056_05_08.jpg)'
- en: This recipe will show you how to perform a word count using data coming from
    Kafka.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例将展示如何使用来自Kafka的数据执行单词计数。
- en: Getting ready
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备好
- en: 'This recipe assumes Kafka is already installed. Kafka comes with ZooKeeper
    bundled. We are assuming Kafka''s home is in `/opt/infoobjects/kafka`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例假设Kafka已经安装。Kafka自带ZooKeeper。我们假设Kafka的主目录在`/opt/infoobjects/kafka`中：
- en: 'Start ZooKeeper:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动ZooKeeper：
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Start the Kafka server:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Kafka服务器：
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create a `test` topic:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`test`主题：
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: How to do it...
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...：
- en: 'Download the `spark-streaming-kafka` library and its dependencies:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载`spark-streaming-kafka`库及其依赖项：
- en: '[PRE28]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Start the Spark shell and provide the `spark-streaming-kafka` library:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell并提供`spark-streaming-kafka`库：
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Stream specific imports:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流特定导入：
- en: '[PRE30]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Import for implicit conversion:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐式转换导入：
- en: '[PRE31]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create `StreamingContext` with a 2 second batch interval:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建具有2秒批处理间隔的`StreamingContext`：
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Set Kafka-specific variables:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Kafka特定变量：
- en: '[PRE33]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create `topicMap`:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`topicMap`：
- en: '[PRE34]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Create Kafka DStream:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Kafka DStream：
- en: '[PRE35]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Pull the value out of lineMap:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从lineMap中取出值：
- en: '[PRE36]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Create `flatMap` of values:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建值的`flatMap`：
- en: '[PRE37]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Create the key-value pair of (word,occurrence):'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建（单词，出现次数）的键值对：
- en: '[PRE38]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Do the word count for a sliding window:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对滑动窗口进行单词计数：
- en: '[PRE39]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Set the `checkpoint` directory:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`checkpoint`目录：
- en: '[PRE40]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Start `StreamingContext`:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动`StreamingContext`：
- en: '[PRE41]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Publish a message on the `test` topic in Kafka in another window:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个窗口的Kafka中的`test`主题上发布一条消息：
- en: '[PRE42]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now, publish messages on Kafka by pressing *Enter* at step 15 and after every
    message.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过在第15步按*Enter*并在每条消息后按*Enter*来在Kafka上发布消息。
- en: Now, as you publish messages on Kafka, you will see them in the Spark shell:![How
    to do it...](img/3056_05_05.jpg)
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，当您在Kafka上发布消息时，您将在Spark shell中看到它们：![如何做...](img/3056_05_05.jpg)
- en: There's more…
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Let's say you want to maintain a running count of the occurrence of each word.
    Spark Streaming has a feature for this called `updateStateByKey` operation. The
    `updateStateByKey` operation allows you to maintain any arbitrary state while
    updating it with the new information supplied.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想要维护每个单词出现次数的运行计数。Spark Streaming具有名为`updateStateByKey`操作的功能。`updateStateByKey`操作允许您在更新时维护任意状态并使用新提供的信息进行更新。
- en: 'This arbitrary state can be an aggregation value, or just a change in state
    (like the mood of a user on twitter). Perform the following steps:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这种任意状态可以是聚合值，也可以是状态的改变（比如Twitter用户的心情）。执行以下步骤：
- en: 'Let''s call `updateStateByKey` on pairs RDD:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在对RDD对调用`updateStateByKey`：
- en: '[PRE43]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `updateStateByKey` operation returns a new "state" DStream where the state
    for each key is updated by applying the given function on the previous state of
    the key and the new values for the key. This can be used to maintain arbitrary
    state data for each key.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`updateStateByKey`操作返回一个新的“状态”DStream，其中每个键的状态都通过在键的先前状态和键的新值上应用给定函数来更新。这可以用于维护每个键的任意状态数据。'
- en: 'There are two steps involved in making this operation work:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使此操作生效涉及两个步骤：
- en: Define the state
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义状态
- en: Define the state `update` function
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义状态`update`函数
- en: The `updateStateByKey` operation is called once for each key, values represent
    the sequence of values associated with that key, very much like MapReduce and
    the state can be any arbitrary state, which we chose to make `Option[Int]`. With
    every call in step 18, the previous state gets updated by adding the sum of current
    values to it.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个键，都会调用一次`updateStateByKey`操作，值表示与该键关联的值序列，非常类似于MapReduce，状态可以是任意状态，我们选择使其为`Option[Int]`。在第18步的每次调用中，通过将当前值的总和添加到先前状态来更新先前状态。
- en: 'Print the results:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印结果：
- en: '[PRE44]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The following are all the steps combined to maintain the arbitrary state using
    the `updateStateByKey` operation:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是使用`updateStateByKey`操作来维护任意状态的所有步骤的组合：
- en: '[PRE45]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Run it by pressing *Ctrl* + *D* (which executes the code pasted using `:paste`).
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按下*Ctrl* + *D*运行它（使用`:paste`粘贴的代码）。
