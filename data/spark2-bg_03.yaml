- en: Chapter 3. Spark SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 Spark SQL
- en: Most businesses deal with quite a lot of structured data all the time. Even
    if there are too many ways to deal with unstructured data, many application use
    cases still have to have structured data. What is the major difference between
    processing structured data and unstructured data? If the data source is structured,
    and if the data processing engine knows the data structure a priori, the data
    processing engine can do lots of optimizations while processing the data, or even
    beforehand. This is very crucial when the data processing volume is huge and the
    turn around time is very critical.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数企业始终处理着大量的结构化数据。尽管处理非结构化数据的方法众多，但许多应用场景仍需依赖结构化数据。处理结构化数据与非结构化数据的主要区别是什么？如果数据源是结构化的，且数据处理引擎事先知晓数据结构，那么该引擎在处理数据时可以进行大量优化，甚至提前进行。当数据处理量巨大且周转时间极为关键时，这一点尤为重要。
- en: Proliferation of enterprise data mandated the need to empower the end users
    to query and process the data in simple and easy to use application user interfaces.
    The RDBMS vendors united and the **structured query language** (**SQL**) came
    about as a solution for this. Over the last couple of decades, everyone who deals
    with data became familiar with SQL if not power users.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 企业数据的激增要求赋予终端用户通过简单易用的应用程序用户界面查询和处理数据的能力。关系型数据库管理系统供应商联合起来，**结构化查询语言**（**SQL**）应运而生，成为解决这一问题的方案。在过去几十年里，所有与数据打交道的人，即使不是高级用户，也熟悉了SQL。
- en: The large scale Internet applications in the social networking and microblogging
    spaces, to name a few, produced data beyond the consumption of many traditional
    data processing tools. When dealing with such a sea of data, picking and choosing
    the right piece of data from it became even more important. Spark was a highly
    prevalent data processing platform and its RDD-based programming model reduced
    the data processing effort as compared to the Hadoop MapReduce data processing
    framework. But, the initial versions of Spark's RDD-based programming model remained
    elusive on making end users, such as data scientists, data analysts, and business
    analysts from using Spark. The main reason why they could not make use of RDD
    based Spark programming model is because it requires some amount of functional
    programming. The solution to this problem is Spark SQL. Spark SQL is a library
    built on top of Spark. It exposes SQL interface and DataFrame API. DataFrame API
    supports programming languages Scala, Java, Python, and R.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 社交网络和微博等大规模互联网应用产生的数据超出了许多传统数据处理工具的消耗能力。面对如此海量的数据，从中挑选并选择正确的数据变得更为重要。Spark是一个广泛使用的数据处理平台，其基于RDD的编程模型相比Hadoop
    MapReduce数据处理框架减少了数据处理的工作量。然而，Spark早期基于RDD的编程模型对于终端用户（如数据科学家、数据分析师和业务分析师）来说使用起来并不直观。主要原因是它需要一定程度的功能编程知识。解决这一问题的方案是Spark
    SQL。Spark SQL是建立在Spark之上的一个库，它提供了SQL接口和DataFrame API。DataFrame API支持Scala、Java、Python和R等编程语言。
- en: If the structure of the data is known in advance, if the data fits into the
    model of rows and columns, it doesn't matter from where the data is coming and
    Spark SQL can use all of it together and process it as if all the data is coming
    from a single source. Moreover, the querying dialect is the ubiquitous SQL.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果事先知道数据的结构，如果数据符合行和列的模型，那么数据来自哪里并不重要，Spark SQL可以将所有数据整合在一起处理，仿佛所有数据都来自单一来源。此外，查询语言是普遍使用的SQL。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Structure of data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据结构
- en: Spark SQL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Aggregations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合
- en: Multi-datasource joins
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多数据源连接
- en: Dataset
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集
- en: Data catalog
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据目录
- en: Understanding the structure of data
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据结构
- en: The structure of the data that is being discussed here needs some more elucidation.
    What do we mean by the structure of the data? The data stored in RDBMS has a way
    of storing the data in rows/columns or records/fields. Every field has a data
    type and every record is a collection of fields of the same or different data
    types. In the early days of RDBMS, the data types of the fields were scalar and
    in the recent versions, it expanded to include collection data types or composite
    data types as well. So, whether the record contains scalar data types or composite
    data types, the important point to note here is that there is a structure to the
    underlying data. Many of the data processing paradigms have adopted the concept
    of mirroring the underlying data structure persisted in the RDBMS or other stores
    in memory to make the data processing easy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此处讨论的数据结构需要进一步阐明。我们所说的数据结构是什么意思？存储在RDBMS中的数据以行/列或记录/字段的方式存储。每个字段都有数据类型，每个记录是相同或不同数据类型的字段集合。在RDBMS早期，字段的数据类型是标量的，而在近期版本中，它扩展到包括集合数据类型或复合数据类型。因此，无论记录包含标量数据类型还是复合数据类型，重要的是要注意底层数据具有结构。许多数据处理范式已采用在内存中镜像RDBMS或其他存储中持久化的底层数据结构的概念，以简化数据处理。
- en: In other words, if the data in an RDBMS table is being processed by a data processing
    application, if the same table-like data structure is available in memory to the
    programs, for the end users and programmers it is easy to model the applications
    and query the data. For example, suppose there is a set of comma-separated data
    items with a fixed number of values in each row having a specific data type for
    the values coming in the specific position in all the rows. This is a structured
    data file. It is a data table and is very similar to an RDBMS table.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 换言之，如果RDBMS表中的数据正被数据处理应用程序处理，且内存中存在与该表类似的数据结构供程序、最终用户和程序员使用，那么建模应用程序和查询数据就变得容易了。例如，假设有一组逗号分隔的数据项，每行具有固定数量的值，且每个特定位置的值都有特定的数据类型。这是一个结构化数据文件，类似于RDBMS表。
- en: In programming languages such as R, there is a data frame abstraction used to
    store data tables in memory. The Python data analysis library, named Pandas, also
    has a similar data frame concept. Once that data structure is available in memory,
    the programs can extract the data and slice and dice it as per the need. The same
    data table concept is extended to Spark, known as DataFrame, built on top of RDD,
    and there is a very comprehensive API known as DataFrame API in Spark SQL to process
    the data in the DataFrame. A SQL-like query language is also developed on top
    of the DataFrame abstraction, catering to the needs of the end users to query
    and process the underlying structured data. In summary, DataFrame is a distributed
    data table organized in rows and columns and having names for each column.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在R等编程语言中，使用数据框抽象在内存中存储数据表。Python数据分析库Pandas也有类似的数据框概念。一旦该数据结构在内存中可用，程序就可以根据需要提取数据并进行切片和切块。同样的数据表概念被扩展到Spark，称为DataFrame，建立在RDD之上，并且有一个非常全面的API，即Spark
    SQL中的DataFrame API，用于处理DataFrame中的数据。还开发了一种类似SQL的查询语言，以满足最终用户查询和处理底层结构化数据的需求。总之，DataFrame是一个分布式数据表，按行和列组织，并为每个列命名。
- en: 'The Spark SQL library built on top of Spark is developed based on the research
    paper titled *"Spark SQL: Relational Data Processing in Spark"*. It talks about
    four goals for Spark SQL and they are reproduced verbatim as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL库建立在Spark之上，是基于题为*“Spark SQL：Spark中的关系数据处理”*的研究论文开发的。它提出了Spark SQL的四个目标，并如下所述：
- en: Support relational processing both within Spark programs (on native RDDs) and
    on external data sources using a programmer-friendly API
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持在Spark程序内部（基于原生RDD）以及使用程序员友好API的外部数据源上进行关系处理
- en: Provide high performance using established DBMS techniques
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用成熟的DBMS技术提供高性能
- en: Easily support new data sources, including semi-structured data and external
    databases amenable to query federation
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轻松支持新的数据源，包括半结构化数据和易于查询联合的外部数据库
- en: Enable extension with advanced analytics algorithms such as graph processing
    and machine learning
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持扩展高级分析算法，如图形处理和机器学习
- en: DataFrame holds structured data, and it is distributed. It allows selection,
    filtering, and aggregation of data. Sounding very similar to RDD? The key difference
    between RDD and DataFrame is that DataFrame stores much more information about
    the structure of the data, such as the data types and names of the columns, than
    RDD. This allows the DataFrame to optimize the processing much more effectively
    than Spark transformations and Spark actions doing processing on RDD. The other
    most important aspect to mention here is that all the supported programming languages
    of Spark can be used to develop applications using the DataFrame API of Spark
    SQL. For all practical purposes, Spark SQL is a distributed SQL engine.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame存储结构化数据，并且是分布式的。它允许进行数据的选择、过滤和聚合。听起来与RDD非常相似吗？RDD和DataFrame的关键区别在于，DataFrame存储了关于数据结构的更多信息，如数据类型和列名，这使得DataFrame在处理优化上比基于RDD的Spark转换和操作更为有效。另一个需要提及的重要方面是，Spark支持的所有编程语言都可以用来开发使用Spark
    SQL的DataFrame API的应用程序。实际上，Spark SQL是一个分布式的SQL引擎。
- en: Tip
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Those who have worked earlier to Spark 1.3 must be familiar with SchemaRDD,
    and the concept of DataFrame is exactly built on top of SchemaRDD with API-level
    compatibility.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 那些在Spark 1.3之前工作过的人一定对SchemaRDD很熟悉，而DataFrame的概念正是建立在SchemaRDD之上，并保持了API级别的兼容性。
- en: Why Spark SQL?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为何选择Spark SQL？
- en: There is no doubt that SQL is the lingua franca for doing data analysis and
    Spark SQL is the answer from the Spark family of toolsets to do data analysis.
    So, what does it provide? It provides the ability to run SQL on top of Spark.
    Whether the data is coming from CSV, Avro, Parquet, Hive, NoSQL data stores such
    as Cassandra, or even RDBMS, Spark SQL can be used to analyze data and mix in
    with Spark programs. Many of the data sources mentioned here are supported intrinsically
    by Spark SQL and many others are supported by external packages. The most important
    aspect to highlight here is the ability of Spark SQL to deal with data from a
    very wide variety of data sources. Once it is available as a DataFrame in Spark,
    Spark SQL can process data in a completely distributed way, combining the DataFrames
    coming from various data sources to process and query as if the entire dataset
    were coming from a single source.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，SQL是进行数据分析的通用语言，而Spark SQL则是Spark工具集家族对此的回应。那么，它提供了什么？它提供了在Spark之上运行SQL的能力。无论数据来自CSV、Avro、Parquet、Hive、NoSQL数据存储如Cassandra，甚至是RDBMS，Spark
    SQL都能用于分析数据，并与Spark程序混合使用。这里提到的许多数据源都由Spark SQL内在支持，而其他许多则由外部包支持。这里最值得强调的是Spark
    SQL处理来自极其多样数据源的数据的能力。一旦数据作为DataFrame在Spark中可用，Spark SQL就能以完全分布式的方式处理数据，将来自不同数据源的DataFrames组合起来进行处理和查询，仿佛整个数据集都来自单一源。
- en: In the previous chapter, the RDD was discussed in detail and introduced as the
    Spark programming model. Is the DataFrames API and the usage of SQL dialects in
    Spark SQL replacing RDD-based programming model? Definitely not! The RDD-based
    programming model is the generic and the basic data processing model in Spark.
    RDD-based programming requires the use of real programming techniques. The Spark
    transformations and Spark actions use a lot of functional programming constructs.
    Even though the amount of code that is required to be written in the RDD-based
    programming model is less compared to Hadoop MapReduce or any other paradigm,
    there is still a need to write some amount of functional code. This is a barrier
    for many data scientists, data analysts, and business analysts, who may perform
    major exploratory kinds of data analysis or do some prototyping with the data.
    Spark SQL completely removes those constraints. Simple and easy-to-use **domain
    specific language** (**DSL**) based methods to read and write data from data sources,
    SQL-like language to select, filter, and aggregate, and the capability to read
    data from a wide variety of data sources, make it easy for anybody who knows the
    data structure to use it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，详细讨论了RDD并将其引入为Spark编程模型。Spark SQL中的DataFrames API和SQL方言的使用是否正在取代基于RDD的编程模型？绝对不是！基于RDD的编程模型是Spark中通用且基本的数据处理模型。基于RDD的编程需要使用实际的编程技术。Spark转换和Spark操作使用了许多函数式编程结构。尽管与Hadoop
    MapReduce或其他范式相比，基于RDD的编程模型所需的代码量较少，但仍需要编写一定量的函数式代码。这对于许多数据科学家、数据分析师和业务分析师来说是一个障碍，他们可能主要进行探索性的数据分析或对数据进行一些原型设计。Spark
    SQL完全消除了这些限制。简单易用的**领域特定语言**（**DSL**）方法，用于从数据源读取和写入数据，类似SQL的语言用于选择、过滤和聚合，以及从各种数据源读取数据的能力，使得任何了解数据结构的人都可以轻松使用它。
- en: Note
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: What is the best use case to use RDD and which is the best use case to use Spark
    SQL? The answer is very simple. If the data is structured, if it can be arranged
    in tables, and if each column can be given a name, then use Spark SQL. This doesn't
    mean that the RDD and DataFrame are two disparate entities. They interoperate
    very well. Conversions from RDD to DataFrame and vice versa are very much possible.
    Many of the Spark transformations and Spark actions that are typically applied
    on RDDs can also be applied on DataFrames.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 何时使用RDD，何时使用Spark SQL的最佳用例是什么？答案很简单。如果数据是结构化的，可以排列在表格中，并且可以为每一列命名，那么使用Spark
    SQL。这并不意味着RDD和DataFrame是两个截然不同的实体。它们互操作得非常好。从RDD到DataFrame以及反之的转换都是完全可能的。许多通常应用于RDD的Spark转换和Spark操作也可以应用于DataFrames。
- en: Typically, during the application design phase, business analysts generally
    do lots of analysis with the application data using SQL, and that is fed to the
    application requirements and testing artifacts. While designing big data applications,
    the same thing is needed, and in such situations, apart from business analysts,
    data scientists will also be there in the team. In a Hadoop-based ecosystem, Hive
    is used extensively for data analysis with big data. Now Spark SQL brings that
    capability to any platform with support for a whole lot of data sources. If there
    is a standalone Spark installation on commodity hardware, lots of these kinds
    of activities can be done to analyze the data. A basic Spark installation deployed
    in standalone mode on commodity hardware is enough to play around with a whole
    lot of data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在应用程序设计阶段，业务分析师通常使用SQL对应用程序数据进行大量分析，这些分析结果被用于应用程序需求和测试工件。在设计大数据应用程序时，同样需要这样做，在这种情况下，除了业务分析师之外，数据科学家也将成为团队的一部分。在基于Hadoop的生态系统中，Hive广泛用于使用大数据进行数据分析。现在，Spark
    SQL将这种能力带到了任何支持大量数据源的平台。如果商品硬件上有一个独立的Spark安装，可以进行大量此类活动来分析数据。在商品硬件上以独立模式部署的基本Spark安装足以处理大量数据。
- en: The SQL-on-Hadoop strategy has introduced many applications, such as Hive and
    Impala to name a few, providing a SQL-like interface to the underlying big data
    stored in the **Hadoop Distributed File System** (**HDFS**). Where does Spark
    SQL fit in that space? Before jumping into that, it is a good idea to touch upon
    Hive and Impala. Hive is a MapReduce-based data warehousing technology and, because
    of the use of MapReduce to process queries, Hive queries require lots of I/O operations
    before completing a query. Impala came up with a brilliant solution by doing the
    in-memory processing while making use of the Hive meta store that describes the
    data. Spark SQL uses SQLContext to do all the operations with data. But it can
    also use HiveContext, which is much more feature rich and advanced than SQLContext.
    HiveContext can do all that SQLContext can do and, on top of that, it can read
    from Hive meta store and tables, and can access Hive user-defined functions as
    well. The only requirement to use HiveContext is obviously that there should be
    an already existing Hive setup readily available. In this way, Spark SQL can easily
    co-exist with Hive.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SQL-on-Hadoop 策略引入了许多应用程序，例如 Hive 和 Impala 等，为存储在 **Hadoop 分布式文件系统** (**HDFS**)
    中的底层大数据提供类似 SQL 的接口。Spark SQL 在这个领域中处于什么位置？在深入探讨之前，了解一下 Hive 和 Impala 是个好主意。Hive
    是一种基于 MapReduce 的数据仓库技术，由于使用 MapReduce 处理查询，Hive 查询在完成之前需要进行大量的 I/O 操作。Impala
    通过进行内存处理并利用描述数据的 Hive 元存储提出了一个出色的解决方案。Spark SQL 使用 SQLContext 执行所有数据操作。但它也可以使用
    HiveContext，后者比 SQLContext 功能更丰富、更高级。HiveContext 可以执行 SQLContext 所能做的一切，并且在此基础上，它可以读取
    Hive 元存储和表，还可以访问 Hive 用户定义的函数。使用 HiveContext 的唯一要求显然是应该有一个现成的 Hive 设置。这样，Spark
    SQL 就可以轻松地与 Hive 共存。
- en: Note
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: From Spark 2.0 onwards, SparkSession is the new starting point for Spark SQL-based
    applications, which are a combination of SQLContext and HiveContext while supporting
    backward compatibility with SQLContext and HiveContext.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Spark 2.0 开始，SparkSession 是基于 Spark SQL 的应用程序的新起点，它是 SQLContext 和 HiveContext
    的组合，同时支持与 SQLContext 和 HiveContext 的向后兼容性。
- en: Spark SQL can process the data from Hive tables faster than Hive using its Hive
    Query Language. Another very interesting feature of Spark SQL is that it can read
    data from different versions of Hive, which is a great feature enabling data source
    consolidation for data processing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 处理来自 Hive 表的数据比使用 Hive 查询语言的 Hive 更快。Spark SQL 的另一个非常有趣的功能是它能够从不同版本的
    Hive 读取数据，这是一个很好的功能，可以实现数据处理的数据源整合。
- en: Note
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The library that exposes Spark SQL and DataFrame API provides interfaces that
    can be accessed through JDBC/ODBC. This opens up a whole new world of data analysis.
    For example, a **business intelligence** (**BI**) tool that connects to data sources
    using JDBC/ODBC can use a whole lot of data sources supported by Spark SQL. Moreover,
    the BI tools can push down the processor intensive join aggregation operations
    to a huge cluster of worker nodes in the Spark infrastructure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 提供 Spark SQL 和 DataFrame API 的库提供了可以通过 JDBC/ODBC 访问的接口。这为数据分析开辟了一个全新的世界。例如，使用
    JDBC/ODBC 连接到数据源的 **商业智能** (**BI**) 工具可以使用 Spark SQL 支持的大量数据源。此外，BI 工具可以将处理器密集型的连接聚合操作推送到
    Spark 基础设施中的大量工作节点上。
- en: Anatomy of Spark SQL
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL 剖析
- en: Interaction with Spark SQL library is done mainly through two methods. One is
    through SQL-like queries and the other is through DataFrame API. Before getting
    into the details of how DataFrame-based programs work, it is a good idea to see
    how the RDD-based programs work.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Spark SQL 库的交互主要通过两种方法进行。一种是通过类似 SQL 的查询，另一种是通过 DataFrame API。在深入了解基于 DataFrame
    的程序如何工作之前，了解一下基于 RDD 的程序如何工作是个好主意。
- en: The Spark transformations and Spark actions are converted into Java functions
    and they act on top of RDDs, which are nothing but Java objects acting upon data.
    Since RDD is a pure Java object, there is no way, at compile time or at run time,
    to know about what data is going to process. There is no metadata available to
    the execution engine beforehand to optimize the Spark transformations or Spark
    actions. There are no multiple execution paths or query plans available in advance
    to process that data and so, evaluation of the efficacy of various paths of execution
    is not available.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 转换和 Spark 操作被转换为 Java 函数，并在 RDD 之上执行，RDD 本质上就是作用于数据的 Java 对象。由于 RDD 是纯粹的
    Java 对象，因此在编译时或运行时都无法预知将要处理的数据。执行引擎事先没有可用的元数据来优化 Spark 转换或 Spark 操作。没有提前准备的多条执行路径或查询计划来处理这些数据，因此无法评估各种执行路径的有效性。
- en: Here, there is no optimized query plan executed because there is no schema associated,
    with data. In the case of DataFrame, the structure is well-known in advance. Because
    of this the queries can be optimized and data cache can be built beforehand.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有执行优化的查询计划，因为没有与数据关联的架构。在 DataFrame 的情况下，结构是事先已知的。因此，可以对查询进行优化并在事先构建数据缓存。
- en: 'The following *Figure 1* gives an idea about the same:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下*图 1*展示了相关内容：
- en: '![Anatomy of Spark SQL](img/image_03_002.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![Spark SQL 结构解析](img/image_03_002.jpg)'
- en: Figure 1
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1
- en: The SQL-like queries and DataFrame API calls made against DataFrame are converted
    to language-neutral expressions. The language-neutral expression corresponding
    to a SQL query or DataFrame API is called an unresolved logical plan.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 针对 DataFrame 的类似 SQL 的查询和 DataFrame API 调用被转换为与语言无关的表达式。与 SQL 查询或 DataFrame
    API 对应的与语言无关的表达式称为未解析的逻辑计划。
- en: The unresolved logical plan is converted to a logical plan by doing validations
    on column names from the metadata of the DataFrame. The logical plan is further
    optimized by applying standard rules such as simplification of expressions, evaluations
    of expressions, and other optimization rules, to form an optimized logical plan.
    The optimized logical plan is converted to multiple physical plans. The physical
    plans are created by using Spark-specific operators in the logical plan. The best
    physical plan is chosen and the resultant queries are pushed down to RDDs to act
    on the data. Because the SQL queries and DataFrame API calls are converted to
    language-neutral query expressions, the performance of these queries is consistent
    across all the supported languages. That is the same reason why the DataFrame
    API is supported by all the Spark supported languages such as Scala, Java, Python,
    and R. In the future, there is a good chance that many more languages are going
    to be supporting DataFrame API and Spark SQL because of this reason.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 未解析的逻辑计划通过验证 DataFrame 元数据中的列名来转换为逻辑计划。逻辑计划通过应用标准规则（如表达式简化、表达式求值和其他优化规则）进一步优化，形成优化的逻辑计划。优化的逻辑计划被转换为多个物理计划。物理计划是通过在逻辑计划中使用
    Spark 特定的操作符创建的。选择最佳物理计划，并将生成的查询推送到 RDD 以作用于数据。由于 SQL 查询和 DataFrame API 调用被转换为与语言无关的查询表达式，因此这些查询在所有支持的语言中的性能是一致的。这也是
    DataFrame API 被所有 Spark 支持的语言（如 Scala、Java、Python 和 R）支持的原因。未来，由于这个原因，很可能会有更多语言支持
    DataFrame API 和 Spark SQL。
- en: The query planning and optimizations of Spark SQL are worth mentioning here
    too. Any query operation done on a DataFrame through SQL queries or through DataFrame
    API is highly optimized before the corresponding operations are physically applied
    on the underlying base RDD. There are many processes in between before the real
    action happening on the RDD.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 的查询规划和优化也值得一提。通过 SQL 查询或 DataFrame API 对 DataFrame 执行的任何查询操作在物理应用于底层基本
    RDD 之前都经过了高度优化。在 RDD 上实际操作发生之前，存在许多中间过程。
- en: '*Figure 2* gives some idea about the whole query optimization process:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2* 提供了关于整个查询优化过程的一些见解：'
- en: '![Anatomy of Spark SQL](img/image_03_004.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Spark SQL 结构解析](img/image_03_004.jpg)'
- en: Figure 2
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2
- en: Two types of queries can be called against a DataFrame. They are SQL queries
    or DataFrame API calls. They go through a proper analysis to come up with a logical
    query plan of execution. Then, optimizations are applied on the logical query
    plans to arrive at an optimized logical query plan. From the final optimized logical
    query plan, one or more physical query plans are made. For each of the physical
    query plans, cost models are worked out, and based on the optimal cost, an appropriate
    physical query plan is selected, and highly optimized code is generated and run
    against the RDDs. This is the reason behind the consistent performance of queries
    of any type on DataFrame. This is the same reason why the DataFrame API calls
    from all these different languages, Scala, Java, Python, and R, give consistent
    performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 针对DataFrame，可以调用两种类型的查询：SQL查询或DataFrame API调用。它们经过适当的分析，生成逻辑查询执行计划。随后，对逻辑查询计划进行优化，以得到优化的逻辑查询计划。从最终的优化逻辑查询计划出发，制定一个或多个物理查询计划。对于每个物理查询计划，都会计算成本模型，并根据最优成本选择合适的物理查询计划，生成高度优化的代码，并针对RDDs运行。这就是DataFrame上任何类型查询性能一致的原因。这也是为什么从Scala、Java、Python和R等不同语言调用DataFrame
    API都能获得一致性能的原因。
- en: 'Let''s revisit the bigger picture once again, as given in *Figure 3*, to set
    the context and see what is being discussed here before getting into and taking
    up the use cases:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回顾*图3*所示的大局，以设定背景，了解当前讨论的内容，然后再深入探讨并处理这些用例：
- en: '![Anatomy of Spark SQL](img/image_03_006.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![Spark SQL结构](img/image_03_006.jpg)'
- en: Figure 3
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图3
- en: The use cases that are going to be discussed here will demonstrate the ability
    to mix SQL queries with Spark programs. Multiple data sources will be chosen,
    data will be read from those sources using DataFrame, and uniform data access
    will be demonstrated. The programming languages used to demonstrate are still
    Scala and Python. The usage of R to manipulate DataFrames is on the agenda of
    the book and a whole chapter is dedicated to the same.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要讨论的使用案例将展示如何将SQL查询与Spark程序结合。我们将选择多个数据源，使用DataFrame从这些源读取数据，并展示统一的数据访问。演示使用的编程语言仍然是Scala和Python。本书议程中还包括使用R操作DataFrame，并为此专门设立了一章。
- en: DataFrame programming
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame编程
- en: 'The use cases selected for elucidating the Spark SQL way of programming with
    DataFrame are given as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于阐释使用DataFrame进行Spark SQL编程方式的用例：
- en: The transaction records come as comma-separated values.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交易记录以逗号分隔的值形式呈现。
- en: Filter out only the good transaction records from the list. The account number
    should start with `SB` and the transaction amount should be greater than zero.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从列表中筛选出仅包含良好交易记录的部分。账户号码应以`SB`开头，且交易金额应大于零。
- en: Find all the high-value transaction records with a transaction amount greater
    than 1000.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有交易金额大于1000的高价值交易记录。
- en: Find all the transaction records where the account number is bad.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有账户号码异常的交易记录。
- en: Find all the transaction records where the transaction amount is less than or
    equal to zero.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有交易金额小于或等于零的交易记录。
- en: Find a combined list of all the bad transaction records.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有不良交易记录的合并列表。
- en: Find the total of all the transaction amounts.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算所有交易金额的总和。
- en: Find the maximum of all the transaction amounts.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有交易金额的最大值。
- en: Find the minimum of all the transaction amounts.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有交易金额的最小值。
- en: Find all the good account numbers.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有良好账户号码。
- en: This is exactly the same set of use cases that were used in the previous chapter
    as well, but here the programming model is totally different. Using this set of
    use cases, two types of programming models are demonstrated here. One is using
    the SQL queries and the other is using DataFrame APIs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是上一章中使用的同一组用例，但这里的编程模型完全不同。通过这组用例，我们展示了两种编程模型：一种是使用SQL查询，另一种是使用DataFrame API。
- en: Programming with SQL
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SQL编程
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala REPL提示符下，尝试以下语句：
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The retail banking transaction records come with account number, transaction
    amount and are processed using SparkSQL to get the desired results of the use
    cases. Here is the summary of what the preceding script did:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 零售银行业务的交易记录包含账户号码和交易金额，通过SparkSQL处理以获得用例所需的结果。以下是上述脚本执行的概要：
- en: A Scala case class is defined to describe the structure of the transaction record
    to be fed into the DataFrame.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala case 类被定义以描述将要输入 DataFrame 的交易记录的结构。
- en: An array is defined with the necessary transaction records.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义了一个包含必要交易记录的数组。
- en: An RDD is made from the array, split the comma-separated values, mapped it to
    create objects using the Scala case class that was defined as the first step in
    the scripts, and the RDD is converted to a DataFrame. This is one use case of
    interoperability between RDD and DataFrame.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD 由数组生成，分割逗号分隔的值，映射以使用 Scala 脚本中定义的第一个步骤中的 case 类创建对象，并将 RDD 转换为 DataFrame。这是
    RDD 与 DataFrame 之间互操作性的一个用例。
- en: A table is registered with the DataFrame with a name. This registered name of
    the table can be used in SQL statements.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个名称将表注册到 DataFrame。该注册表的名称可以在 SQL 语句中使用。
- en: Then, all the other activities are just issuing SQL statements using the `spark.sql`
    method. Here the object spark is of type the SparkSession.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，所有其他活动只是使用 `spark.sql` 方法发出 SQL 语句。这里的 spark 对象是 SparkSession 类型。
- en: The result of all these SQL statements is stored as DataFrames and, just like
    the RDD's `collect` action, DataFrame's show method is used to extract the values
    to the Spark driver program.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有这些 SQL 语句的结果存储为 DataFrames，并且就像 RDD 的 `collect` 操作一样，使用 DataFrame 的 show 方法将值提取到
    Spark 驱动程序中。
- en: The aggregate value calculations are done in two different ways. One is in the
    SQL statement way, which is the easiest way. The other is using the regular RDD-style
    Spark transformations and Spark actions. This is to show that even DataFrame can
    be operated like an RDD, and Spark transformations and Spark actions can be applied
    on top of DataFrame.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合值的计算以两种不同的方式进行。一种是使用 SQL 语句的方式，这是最简单的方式。另一种是使用常规的 RDD 风格的 Spark 转换和 Spark
    操作。这是为了展示即使 DataFrame 也可以像 RDD 一样操作，并且可以在 DataFrame 上应用 Spark 转换和 Spark 操作。
- en: At times, it is easy to do some data manipulation activities through the functional
    style operations using functions. So, there is a flexibility here to mix SQL,
    RDD, and DataFrame to have a very convenient programming model to process data.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，通过使用函数的功能样式操作进行一些数据操作活动很容易。因此，这里有一个灵活性，可以混合使用 SQL、RDD 和 DataFrame，以拥有一个非常方便的数据处理编程模型。
- en: The DataFrame contents are displayed in table format using the `show` method
    of the DataFrame.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame 内容以表格格式使用 DataFrame 的 `show` 方法显示。
- en: A detailed view of the structure of the DataFrame is displayed using the `printSchema`
    method. This is akin to the `describe` command of the database tables.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `printSchema` 方法展示 DataFrame 结构的详细视图。这类似于数据库表的 `describe` 命令。
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python REPL 提示符下，尝试以下语句：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding Python code snippet, except for a few language-specific constructs
    such as importing libraries and the definition of lambda functions, the style
    of programming is almost the same, most of the time, as the Scala code. This is
    the advantage of Spark's uniform programming model. As discussed earlier, when
    business analysts or data analysts provide the SQL for data access, it is very
    easy to integrate that along with the data processing code in Spark. This uniform
    programming style of coding is very useful for organizations to use the language
    of their choice for developing data processing applications in Spark.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 Python 代码片段中，除了一些特定于语言的构造（如导入库和定义 lambda 函数）之外，编程风格几乎与 Scala 代码相同。这是 Spark
    统一编程模型的优势。如前所述，当业务分析师或数据分析师提供数据访问的 SQL 时，很容易将其与 Spark 中的数据处理代码集成。这种统一的编程风格对于组织使用他们选择的语言在
    Spark 中开发数据处理应用程序非常有用。
- en: Tip
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'On DataFrames, if applicable Spark transformations are applied, then a Dataset
    is returned instead of a DataFrame. The concept of Dataset is introduced toward
    the end of this chapter. There is a very strong relationship between DataFrame
    and Dataset, and that is explained in the section covering Datasets. While developing
    applications, caution must be used in this kind of situation. For example, in
    the preceding code snippets, if the following transformation is tried in Scala
    REPL, it will return a dataset: `val amount = goodTransRecords.map(trans => trans.getAs[Double]("tranAmount"))amount:
    org.apache.spark.sql.Dataset[Double] = [value: double]`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '在DataFrame上，如果应用了适用的Spark转换，则会返回一个Dataset而不是DataFrame。Dataset的概念在本章末尾引入。DataFrame和Dataset之间有着非常紧密的联系，这在涵盖Datasets的部分中有所解释。在开发应用程序时，必须在这种情况下谨慎行事。例如，在前面的代码片段中，如果在Scala
    REPL中尝试以下转换，它将返回一个数据集：`val amount = goodTransRecords.map(trans => trans.getAs[Double]("tranAmount"))amount:
    org.apache.spark.sql.Dataset[Double] = [value: double]`'
- en: Programming with DataFrame API
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DataFrame API编程
- en: In this section, the code snippets will be run in the appropriate language REPLs
    as a continuation of the previous section so that the setup of the data and other
    initializations are not repeated. Like the preceding code snippets, initially,
    some DataFrame-specific basic commands are given. These are used regularly to
    see the contents and for doing some sanity tests on the DataFrame and its contents.
    These are commands that are typically used in the exploratory stage of the data
    analysis, quite often to get more insight into the structure and contents of the
    underlying data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，代码片段将在适当的语言REPL中运行，作为前一节的延续，以便数据设置和其他初始化不会重复。与前面的代码片段一样，最初给出了一些DataFrame特定的基本命令。这些命令通常用于查看内容并对DataFrame及其内容进行一些合理性测试。这些是在数据分析的探索阶段经常使用的命令，通常用于更深入地了解底层数据的结构和内容。
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala REPL提示符下，尝试以下语句：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is the summary of what the preceding script did from a DataFrame API perspective:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从DataFrame API的角度来看，这里是前面脚本的总结：
- en: The DataFrame containing the superset of data used in the preceding section
    is used here.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含前一部分所用数据超集的DataFrame在此处被使用。
- en: Filtering of the records is demonstrated next. Here, the most important aspect
    to notice is that the filter predicate is to be given exactly like the predicates
    in the SQL statements. Filters can be chained.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来演示记录的过滤。这里，最重要的是要注意过滤谓词必须像SQL语句中的谓词一样给出。过滤器可以链式使用。
- en: The aggregation methods are calculated in one go as three columns in the resultant
    DataFrame.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合方法作为结果DataFrame中的三列一次性计算。
- en: The final statements in this set are doing the selection, filtering, choosing
    distinct records, and ordering in one single chained statement.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本组中的最终语句在一个单一的链式语句中完成了选择、过滤、选择不同的记录以及排序。
- en: Finally, the transaction records are persisted in Parquet format, read from
    the Parquet store and create a DataFrame. More details on the persistence formats
    is coming in the following section.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，交易记录以Parquet格式持久化，从Parquet存储中读取并创建一个DataFrame。关于持久化格式的更多细节将在接下来的部分中介绍。
- en: In this code snippet, the Parquet format data is stored in the current directory
    from where the corresponding REPL is invoked. When it is run as a Spark program,
    the directory again will be the current directory from where the Spark submit
    is invoked.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此代码片段中，Parquet格式的数据存储在当前目录中，从该目录调用相应的REPL。当作为Spark程序运行时，目录再次将是调用Spark提交的当前目录。
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python REPL提示符下，尝试以下语句：
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding Python code snippet, except for a very few variations in the
    aggregation calculations, the programming constructs are almost similar to its
    Scala counterpart.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的Python代码片段中，除了聚合计算的极少数变化外，编程结构几乎与其Scala对应部分相似。
- en: The last few statements of the preceding Scala and Python sections are about
    the persisting of the DataFrame contents into the media. The writing and reading
    operations are very much required in any kind of data processing operations, but
    most of the tools don't have a uniform way of writing and reading. Spark SQL is
    different. The DataFrame API comes with a rich set of persistence mechanisms.
    It is very easy to write contents of a DataFrame into many supported persistence
    stores. All these writing and reading operations have very simple DSL style interfaces.
    Here are some of the built-in formats in which DataFrames can be written to and
    read from.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 前述Scala和Python部分最后几条语句涉及将DataFrame内容持久化到媒体中。在任何数据处理操作中，写入和读取操作都非常必要，但大多数工具并没有统一的写入和读取方式。Spark
    SQL则不同。DataFrame API配备了一套丰富的持久化机制。将DataFrame内容写入多种支持的持久化存储非常简便。所有这些写入和读取操作都具有非常简单的DSL风格接口。以下是DataFrame可以写入和读取的一些内置格式。
- en: 'Apart from these, there are so many other external data sources supported through
    third-party packages:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，还有许多其他外部数据源通过第三方包得到支持：
- en: JSON
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON
- en: Parquet
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet
- en: Hive
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive
- en: MySQL
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MySQL
- en: PostgreSQL
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PostgreSQL
- en: HDFS
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HDFS
- en: Plain Text
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纯文本
- en: Amazon S3
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊S3
- en: ORC
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ORC
- en: JDBC
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDBC
- en: The write and read of DataFrame into and from Parquet has been demonstrated
    in the preceding code snippets. All the preceding inherently supported data stores
    have very simple DSL style syntax for persistence and reading back, which makes
    the programming style uniform once again. The DataFrame API reference is a great
    source to know about the details of dealing with each of these data stores.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码片段中已演示了将DataFrame写入和读取自Parquet。所有这些内置支持的数据存储都具有非常简单的DSL风格语法用于持久化和读取，这使得编程风格再次统一。DataFrame
    API参考资料是了解处理这些数据存储细节的绝佳资源。
- en: The sample code in this chapter persists data in Parquet and JSON formats. The
    data store location names chosen are `python.trans.parquet`, `scala.trans.parquet`,
    and so on. This is just to give an indication of which programming language is
    used and which is the format of the data. This is not a proper convention but
    a convenience. When one run of the program is completed, these directories would
    have been created. Next time the same program is run, it will try to create the
    same and will result in an error. The workaround is to remove the directories
    manually, before the subsequent runs, and proceed. Proper error handling mechanisms
    and other nuances of fine programming are going to dilute the focus and hence
    are deliberately left out of this book.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的示例代码将数据持久化在Parquet和JSON格式中。所选数据存储位置名称如`python.trans.parquet`、`scala.trans.parquet`等。这仅是为了表明使用哪种编程语言以及数据格式是什么。这不是正式约定，而是一种便利。当程序运行一次后，这些目录将被创建。下次运行同一程序时，它将尝试创建相同的目录，这将导致错误。解决方法是手动删除这些目录，在后续运行之前进行，然后继续。适当的错误处理机制和其他精细编程的细微差别会分散注意力，因此故意未在此书中涉及。
- en: Understanding Aggregations in Spark SQL
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Spark SQL中的聚合
- en: In SQL, aggregation of data is very flexible. The same thing is true in Spark
    SQL too. Instead of running SQL statements on a single data source located in
    a single machine, here Spark SQL can do the same on distributed data sources.
    In the previous chapter, a MapReduce use case was discussed to do data aggregation
    and the same is being used here to demonstrate the aggregation capabilities of
    Spark SQL. In this section also, the use cases are approached in the SQL query
    way as well as in the DataFrame API way.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: SQL中的数据聚合非常灵活。Spark SQL亦是如此。Spark SQL并非在单机上的单一数据源上运行SQL语句，而是可以在分布式数据源上执行相同操作。在前一章中，讨论了一个MapReduce用例以进行数据聚合，此处同样使用该用例来展示Spark
    SQL的聚合能力。本节中，用例既采用SQL查询方式，也采用DataFrame API方式进行处理。
- en: 'The use cases selected for elucidating the MapReduce kind of data processing
    here are given as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此处为阐明MapReduce类型数据处理而选取的用例如下：
- en: The retail banking transaction records come with account number and transaction
    amount in comma-separated strings
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零售银行业务交易记录包含以逗号分隔的账户号和交易金额
- en: Find an account level summary of all the transactions to get the account balance
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有交易的账户级别汇总以获取账户余额
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala REPL提示符下，尝试以下语句：
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this code snippet, everything is very similar to the preceding section's
    code. The only difference is that, here, aggregations are used in the SQL queries
    as well as in the DataFrame API.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本代码片段中，与前述章节的代码非常相似。唯一的区别是，这里在SQL查询和DataFrame API中都使用了聚合。
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python REPL提示符下，尝试以下语句：
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the DataFrame API for Python, there are some minor syntax differences as
    compared to its Scala counterpart.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python的DataFrame API中，与Scala版本相比，存在一些细微的语法差异。
- en: Understanding multi-datasource joining with SparkSQL
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解SparkSQL中的多数据源合并
- en: In the previous chapter, the joining of multiple RDDs based on the key has been
    discussed. In this section, the same use case is implemented using Spark SQL.
    The use cases selected for elucidating the joining of multiple datasets using
    a key are given here.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章节中，讨论了基于键合并多个RDD的情况。本节中，将使用Spark SQL实现相同的用例。以下是用于阐明基于键合并多个数据集的用例。
- en: The first dataset contains a retail banking master records summary with account
    number, first name, and last name. The second dataset contains the retail banking
    account balance with account number and balance amount. The key on both of the
    datasets is account number. Join the two datasets and create one dataset containing
    account number, first name, last name, and balance amount. From this report, pick
    up the top three accounts in terms of the balance amount.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数据集包含零售银行业务主记录摘要，包括账号、名字和姓氏。第二个数据集包含零售银行账户余额，包括账号和余额金额。两个数据集的关键字段都是账号。将这两个数据集合并，创建一个包含账号、名字、姓氏和余额金额的数据集。从这份报告中，挑选出余额金额排名前三的账户。
- en: In this section, the concept of joining data from multiple data sources is also
    demonstrated. First the DataFrames are created from two arrays. They are persisted
    in Parquet and JSON formats. Then they are read from the disk to form the DataFrames,
    and they are joined together.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 本节还演示了从多个数据源合并数据的概念。首先，从两个数组创建DataFrame，并以Parquet和JSON格式持久化。然后从磁盘读取它们以形成DataFrame，并将它们合并在一起。
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala REPL提示符下，尝试以下语句：
- en: '[PRE6]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Continuing from the same Scala REPL session, the following lines of code get
    the same result through the DataFrame API:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一Scala REPL会话中继续，以下代码行通过DataFrame API获得相同结果：
- en: '[PRE7]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The join type selected in the preceding section of the code is inner join. Instead
    of that, any other type of join can be used, either through the SQL query way
    or through the DataFrame API way. In this particular use case, it can be seen
    that the DataFrame API is becoming a bit clunky, while the SQL query looks very
    straightforward. The point here is that depending on the situation, in the application
    code, the SQL query way and the DataFrame API way can be mixed to produce the
    desired result. The DataFrame `acDetailTop3` given in the following scripts is
    an example of that.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码段中选择的连接类型是内连接。实际上，可以通过SQL查询方式或DataFrame API方式使用其他任何类型的连接。在这个特定用例中，可以发现DataFrame
    API显得有些笨拙，而SQL查询则非常直接。关键在于，根据情况，在应用程序代码中，可以将SQL查询方式与DataFrame API方式混合使用，以产生期望的结果。以下脚本中给出的DataFrame
    `acDetailTop3`就是一个例子。
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python REPL提示符下，尝试以下语句：
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding sections, application of the RDD operations on DataFrame has
    been demonstrated. This shows the capability of Spark SQL to interoperate with
    the RDDs and vice versa. In the same way, SQL queries and the DataFrame API can
    be mixed in to have flexibility to use the easiest method of computation when
    solving real-world use cases in the applications.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述章节中，已展示了在DataFrame上应用RDD操作的情况。这表明了Spark SQL与RDD之间互操作的能力。同样地，SQL查询和DataFrame
    API可以混合使用，以便在解决应用程序中的实际用例时，能够灵活地采用最简便的计算方法。
- en: Introducing datasets
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入数据集
- en: The Spark programming paradigm has many abstractions to choose from when it
    comes to developing data processing applications. The fundamentals of Spark programming
    start with RDDs that can easily deal with unstructured, semi-structured, and structured
    data. The Spark SQL library offers highly optimized performance when processing
    structured data. This makes the basic RDDs look inferior in terms of performance.
    To fill this gap, from Spark 1.6 onwards, a new abstraction, named Dataset, was
    introduced that complements the RDD-based Spark programming model. It works pretty
    much the same way as RDD when it comes to Spark transformations and Spark actions,
    and at the same time, it is highly optimized like the Spark SQL. Dataset API provides
    strong compile-time type safety when it comes to writing programs and, because
    of that, the Dataset API is available only in Scala and Java.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Spark编程范式在开发数据处理应用时提供了多种抽象选择。Spark编程的基础始于RDD，它能轻松处理非结构化、半结构化和结构化数据。Spark SQL库在处理结构化数据时展现出高度优化的性能，使得基本RDD在性能上显得逊色。为了弥补这一差距，自Spark
    1.6起，引入了一种名为Dataset的新抽象，它补充了基于RDD的Spark编程模型。在Spark转换和Spark操作方面，Dataset的工作方式与RDD大致相同，同时它也像Spark
    SQL一样高度优化。Dataset API在编写程序时提供了强大的编译时类型安全，因此，Dataset API仅在Scala和Java中可用。
- en: The transaction banking use case discussed in the chapter covering the Spark
    programming model is taken up again here to elucidate the dataset-based programming
    model, because this programming model has a very close resemblance to RDD-based
    programming. The use case mainly deals with a set of banking transaction records
    and various processing done on those records to extract various information from
    it. The use case descriptions are not repeated here and it is not difficult to
    understand by looking at the comments and the code.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的Spark编程模型中的交易银行业务用例在此再次被提及，以阐明基于Dataset的编程模型，因为这种编程模型与基于RDD的编程非常相似。该用例主要处理一组银行交易记录以及对这些记录进行的各种处理，以从中提取各种信息。用例描述在此不再重复，通过查看注释和代码不难理解。
- en: The following code snippet demonstrates the methods used to create Dataset,
    along with its usage, conversion of RDD to DataFrame, and conversion of DataFrame
    to dataset. The RDD to DataFrame conversion has already been discussed, but is
    captured here again to to keep the concepts in context. This is mainly to prove
    that various programming models in Spark and the data abstractions are highly
    interoperable.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了创建Dataset的方法及其使用、RDD到DataFrame的转换以及DataFrame到Dataset的转换。RDD到DataFrame的转换已经讨论过，但在此再次捕捉，以保持概念的上下文。这主要是为了证明Spark中的各种编程模型和数据抽象具有高度的互操作性。
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala REPL提示符下，尝试以下语句：
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is very clear that the dataset-based programming has good applicability in
    many of the data processing use cases; at the same time, it has got high interoperability
    with other data processing abstractions within Spark itself.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，基于Dataset的编程在许多数据处理用例中具有良好的适用性；同时，它与Spark内部的其他数据处理抽象具有高度的互操作性。
- en: Tip
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'In the preceding code snippet, the DataFrame was converted to Dataset with
    a type specification `acTransRDDToDF.as[Trans]`. This type of conversion is really
    required when reading data from external data sources such as JSON, Avro, or Parquet
    files. That is when strong type checking is needed. Typically, structured data
    is read into DataFrame, and that can be converted to DataSet with strong type
    safety check like this in one shot: `spark.read.json("/transaction.json").as[Trans]`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码片段中，DataFrame通过类型指定`acTransRDDToDF.as[Trans]`转换为Dataset。当从外部数据源（如JSON、Avro或Parquet文件）读取数据时，这种转换是真正需要的，此时需要强类型检查。通常，结构化数据被读入DataFrame，然后可以像这样一次性转换为具有强类型安全检查的DataSet：`spark.read.json("/transaction.json").as[Trans]`
- en: If the Scala code snippets throughout this chapter are examined, when some of
    the methods are called on a DataFrame, instead of returning a DataFrame object,
    an object of type `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]` is
    returned. This is the important relationship between DataFrame and dataset. In
    other words, DataFrame is a dataset of type `org.apache.spark.sql.Row`. If required,
    this object of type `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]` can
    be explicitly converted to DataFrame using the `toDF()` method.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果本章中的Scala代码片段被仔细检查，当某些方法被调用在一个DataFrame上时，返回的不是一个DataFrame对象，而是一个类型为`org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]`的对象。这是DataFrame与dataset之间的重要关系。换句话说，DataFrame是一个类型为`org.apache.spark.sql.Row`的dataset。如果需要，这个类型为`org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]`的对象可以显式地使用`toDF()`方法转换为DataFrame。
- en: Too many choices confuse everybody. Here in the Spark programming model, the
    same problem is seen. But it is not as confusing as in many other programming
    paradigms. Whenever there is a need to process any kind of data with very high
    flexibility in terms of the data processing requirements and having the lowest
    API level control such as library development, the RDD-based programming model
    is ideal. Whenever there is a need to process structured data with flexibility
    for accessing and processing data with optimized performance across all the supported
    programming languages, the DataFrame-based Spark SQL programming model is ideal.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 过多的选择让所有人困惑。在Spark编程模型中，同样的问题也存在。但这并不像许多其他编程范式那样令人困惑。每当需要处理任何类型的数据，并且对数据处理要求具有极高的灵活性，以及拥有最低级别的API控制，如库开发时，基于RDD的编程模型是理想选择。每当需要处理结构化数据，并且需要跨所有支持的编程语言灵活访问和处理数据，同时优化性能时，基于DataFrame的Spark
    SQL编程模型是理想选择。
- en: Whenever there is a need to process unstructured data with optimized performance
    requirements as well as compile-time type safety but not very complex Spark transformations
    and Spark actions usage requirements, the dataset-based programming model is ideal.
    At a data processing application development level, if the programming language
    of choice permits, it is better to use dataset and DataFrame to have better performance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 每当需要处理非结构化数据，同时要求优化性能和编译时类型安全，但不需要非常复杂的Spark转换和Spark操作使用要求时，基于dataset的编程模型是理想选择。在数据处理应用开发层面，如果所选编程语言允许，使用dataset和DataFrame会获得更好的性能。
- en: Understanding Data Catalogs
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据目录
- en: The previous sections of this chapter covered programming models with DataFrames
    and datasets. Both of these programming models can deal with structured data.
    The structured data comes with metadata or the data describing the structure of
    the data. Spark SQL provides a minimalist API known as Catalog API for data processing
    applications to query and use the metadata in the applications. The Catalog API
    exposes a catalog abstraction with many databases in it. For the regular SparkSession,
    it will have only one database, namely default. But if Spark is used with Hive,
    then the entire Hive meta store will be available through the Catalog API. The
    following code snippets demonstrate the usage of Catalog API in Scala and Python.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本章前几节介绍了使用DataFrames和datasets的编程模型。这两种编程模型都能处理结构化数据。结构化数据自带元数据，即描述数据结构的数据。Spark
    SQL提供了一个极简的API，称为Catalog API，供数据处理应用查询和使用应用中的元数据。Catalog API展示了一个包含多个数据库的目录抽象。对于常规的SparkSession，它只有一个数据库，即默认数据库。但如果Spark与Hive一起使用，那么整个Hive元数据存储将通过Catalog
    API可用。以下代码片段展示了在Scala和Python中使用Catalog API的示例。
- en: 'Continuing from the same Scala REPL prompt, try the following statements:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 继续在同一个Scala REPL提示符下，尝试以下语句：
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Similarly, the Catalog API can be used from Python code as well. Since the
    dataset example was not applicable in Python, the table list will be empty. At
    the Python REPL prompt, try the following statements:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Catalog API也可以从Python代码中使用。由于dataset示例在Python中不适用，表列表将为空。在Python REPL提示符下，尝试以下语句：
- en: '[PRE11]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The Catalog API is very handy when writing data processing applications with
    the ability to process data dynamically, based on the contents in the meta store,
    especially when using it in conjunction with Hive.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Catalog API在编写能够根据元数据存储内容动态处理数据的数据处理应用时非常方便，尤其是在与Hive结合使用时。
- en: References
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'For more information you can refer to:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考：
- en: '[https://amplab.cs.berkeley.edu/wp-content/uploads/2015/03/SparkSQLSigmod2015.pdf](https://amplab.cs.berkeley.edu/wp-content/uploads/2015/03/SparkSQLSigmod2015.pdf)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Spark SQL在SIGMOD 2015上的论文](https://amplab.cs.berkeley.edu/wp-content/uploads/2015/03/SparkSQLSigmod2015.pdf)'
- en: '[http://pandas.pydata.org/](http://pandas.pydata.org/)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pandas数据分析库](http://pandas.pydata.org/)'
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Spark SQL is a very highly useful library on top of the Spark core infrastructure.
    This library makes the Spark programming more inclusive to a wider group of programmers
    who are well versed with the imperative style of programming but not as competent
    in functional programming. Apart from this, Spark SQL is the best library to process
    structured data in the Spark family of data processing libraries. Spark SQL-based
    data processing application programs can be written with SQL-like queries or DSL
    style imperative programs of DataFrame API. This chapter has also demonstrated
    various strategies of mixing RDD and DataFrames, mixing SQL-like queries and DataFrame
    API. This gives amazing flexibility for the application developers to write data
    processing programs in the way they are most comfortable with, or that is more
    appropriate to the use cases, and at the same time, without compromising performance.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是建立在Spark核心基础设施之上的一个极其有用的库。该库使得Spark编程对更广泛的熟悉命令式编程风格的程序员更加包容，尽管他们在函数式编程方面可能不那么熟练。此外，Spark
    SQL是Spark数据处理库家族中处理结构化数据的最佳库。基于Spark SQL的数据处理应用程序可以使用类似SQL的查询或DataFrame API的DSL风格命令式程序编写。本章还展示了混合RDD和DataFrames、混合类似SQL的查询和DataFrame
    API的各种策略。这为应用程序开发人员提供了极大的灵活性，使他们能够以最舒适的方式或更适合用例的方式编写数据处理程序，同时不牺牲性能。
- en: The Dataset API is as the next generation of programming model based on dataset
    in Spark, providing optimized performance and compile-time type safety.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Dataset API作为基于Spark中数据集的下一代编程模型，提供了优化的性能和编译时的类型安全。
- en: The Catalog API comes as a very handy tool to process data dynamically, based
    on the contents of the meta store.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 目录API作为一个非常便捷的工具，可根据元数据存储的内容动态处理数据。
- en: R is the language of data scientists. Till the support of R as a programming
    language in Spark SQL was available, major distributed data processing was not
    easy for them. Now, using R as a language of choice, they can seamlessly write
    distributed data processing applications as if they are using an R data frame
    from their individual machines. The next chapter is going to discuss the use of
    R to do data processing in Spark SQL.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: R是数据科学家的语言。在Spark SQL支持R作为编程语言之前，对他们来说，进行大规模分布式数据处理并不容易。现在，使用R作为首选语言，他们可以无缝地编写分布式数据处理应用程序，就像使用个人机器上的R数据框一样。下一章将讨论在Spark
    SQL中使用R进行数据处理。
