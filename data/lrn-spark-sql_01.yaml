- en: Getting Started with Spark SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用Spark SQL
- en: Spark SQL is at the heart of all applications developed using Spark. In this
    book, we will explore Spark SQL in great detail, including its usage in various
    types of applications as well as its internal workings. Developers and architects
    will appreciate the technical concepts and hands-on sessions presented in each
    chapter, as they progress through the book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是使用Spark开发的所有应用程序的核心。在本书中，我们将详细探讨Spark SQL的使用方式，包括其在各种类型的应用程序中的使用以及其内部工作原理。开发人员和架构师将欣赏到每一章中呈现的技术概念和实践会话，因为他们在阅读本书时会逐步进展。
- en: In this chapter, we will introduce you to the key concepts related to Spark
    SQL. We will start with SparkSession, the new entry point for Spark SQL in Spark
    2.0\. Then, we will explore Spark SQL's interfaces RDDs, DataFrames, and Dataset
    APIs. Later on, we will explain the developer-level details regarding the Catalyst
    optimizer and Project Tungsten.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您介绍与Spark SQL相关的关键概念。我们将从SparkSession开始，这是Spark 2.0中Spark SQL的新入口点。然后，我们将探索Spark
    SQL的接口RDDs、DataFrames和Dataset APIs。随后，我们将解释有关Catalyst优化器和Project Tungsten的开发人员级细节。
- en: Finally, we will introduce an exciting new feature in Spark 2.0 for streaming
    applications, called Structured Streaming. Specific hands-on exercises (using
    publicly available Datasets) are presented throughout the chapter, so you can
    actively follow along as you read through the various sections.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们将介绍Spark 2.0中针对流应用程序的一项令人兴奋的新功能，称为结构化流。本章中将提供特定的实践练习（使用公开可用的数据集），以便您在阅读各个部分时能够积极参与其中。 '
- en: 'More specifically, the sections in this chapter will cover the following topics
    along with practice hands-on sessions:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，本章的各节将涵盖以下主题以及实践会话：
- en: What is Spark SQL?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是Spark SQL？
- en: Introducing SparkSession
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍SparkSession
- en: Understanding Spark SQL concepts
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解Spark SQL概念
- en: Understanding RDDs, DataFrames, and Datasets
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解RDDs、DataFrames和Datasets
- en: Understanding the Catalyst optimizer
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解Catalyst优化器
- en: Understanding Project Tungsten
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解Project Tungsten
- en: Using Spark SQL in continuous applications
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在连续应用程序中使用Spark SQL
- en: Understanding Structured Streaming internals
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解结构化流内部
- en: What is Spark SQL?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Spark SQL？
- en: Spark SQL is one of the most advanced components of Apache Spark. It has been
    a part of the core distribution since Spark 1.0 and supports Python, Scala, Java,
    and R programming APIs. As illustrated in the figure below, Spark SQL components
    provide the foundation for Spark machine learning applications, streaming applications,
    graph applications, and many other types of application architectures.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是Apache Spark最先进的组件之一。自Spark 1.0以来一直是核心分发的一部分，并支持Python、Scala、Java和R编程API。如下图所示，Spark
    SQL组件为Spark机器学习应用程序、流应用程序、图应用程序以及许多其他类型的应用程序架构提供了基础。
- en: '![](img/00005.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00005.jpeg)'
- en: Such applications, typically, use Spark ML pipelines, Structured Streaming,
    and GraphFrames, which are all based on Spark SQL interfaces (DataFrame/Dataset
    API). These applications, along with constructs such as SQL, DataFrames, and Datasets
    API, receive the benefits of the Catalyst optimizer, automatically. This optimizer
    is also responsible for generating executable query plans based on the lower-level
    RDD interfaces.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应用程序通常使用Spark ML pipelines、结构化流和GraphFrames，这些都是基于Spark SQL接口（DataFrame/Dataset
    API）的。这些应用程序以及SQL、DataFrames和Datasets API等构造自动获得Catalyst优化器的好处。该优化器还负责根据较低级别的RDD接口生成可执行的查询计划。
- en: We will explore ML pipelines in more detail in [Chapter 6](part0103.html#3279U0-e9cbc07f866e437b8aa14e841622275c), *Using
    Spark SQL in Machine Learning Applications*. GraphFrames will be covered in [Chapter
    7](part0134.html#3VPBC0-e9cbc07f866e437b8aa14e841622275c), *Using Spark SQL in
    Graph Applications*. While, we will introduce the key concepts regarding Structured
    Streaming and the Catalyst optimizer in this chapter, we will get more details
    about them in [Chapter 5](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c), *Using
    Spark SQL in Streaming Applications*, and [Chapter 11](part0197.html#5RRUQ0-e9cbc07f866e437b8aa14e841622275c), *Tuning
    Spark SQL Components for Performance*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第6章](part0103.html#3279U0-e9cbc07f866e437b8aa14e841622275c)中更详细地探讨ML pipelines，*在机器学习应用程序中使用Spark
    SQL*。GraphFrames将在[第7章](part0134.html#3VPBC0-e9cbc07f866e437b8aa14e841622275c)中介绍，*在图应用程序中使用Spark
    SQL*。而在本章中，我们将介绍有关结构化流和Catalyst优化器的关键概念，我们将在[第5章](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c)和[第11章](part0197.html#5RRUQ0-e9cbc07f866e437b8aa14e841622275c)中获得更多关于它们的细节，*在流应用程序中使用Spark
    SQL*和*Tuning Spark SQL Components for Performance*。
- en: In Spark 2.0, the DataFrame API has been merged with the Dataset API, thereby
    unifying data processing capabilities across Spark libraries. This also enables
    developers to work with a single high-level and type-safe API. However, the Spark
    software stack does not prevent developers from directly using the low-level RDD
    interface in their applications. Though the low-level RDD API will continue to
    be available, a vast majority of developers are expected to (and are recommended
    to) use the high-level APIs, namely, the Dataset and DataFrame APIs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0中，DataFrame API已与Dataset API合并，从而统一了跨Spark库的数据处理能力。这也使开发人员能够使用单一的高级和类型安全的API。但是，Spark软件堆栈并不阻止开发人员直接在其应用程序中使用低级别的RDD接口。尽管低级别的RDD
    API将继续可用，但预计绝大多数开发人员将（并建议）使用高级API，即Dataset和DataFrame API。
- en: Additionally, Spark 2.0 extends Spark SQL capabilities by including a new ANSI
    SQL parser with support for subqueries and the SQL:2003 standard. More specifically,
    the subquery support now includes correlated/uncorrelated subqueries, and `IN
    / NOT IN` and `EXISTS / NOT` `EXISTS` predicates in `WHERE / HAVING` clauses.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark 2.0通过包括一个新的ANSI SQL解析器扩展了Spark SQL的功能，支持子查询和SQL:2003标准。更具体地，子查询支持现在包括相关/不相关子查询，以及`IN
    / NOT IN`和`EXISTS / NOT EXISTS`谓词在`WHERE / HAVING`子句中。
- en: At the core of Spark SQL is the Catalyst optimizer, which leverages Scala's
    advanced features, such as pattern matching, to provide an extensible query optimizer.
    DataFrames, Datasets, and SQL queries share the same execution and optimization
    pipeline; hence, there is no performance impact of using any one or the other
    of these constructs (or of using any of the supported programming APIs). The high-level
    DataFrame-based code written by the developer is converted to Catalyst expressions
    and then to low-level Java bytecode as it passes through this pipeline.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL的核心是Catalyst优化器，它利用Scala的高级特性（如模式匹配）来提供可扩展的查询优化器。DataFrame、数据集和SQL查询共享相同的执行和优化管道；因此，使用这些结构中的任何一个（或使用任何受支持的编程API）都不会对性能产生影响。开发人员编写的高级基于DataFrame的代码被转换为Catalyst表达式，然后通过该管道转换为低级Java字节码。
- en: '`SparkSession` is the entry point into Spark SQL-related functionality and
    we describe it in more detail in the next section.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`是与Spark SQL相关功能的入口点，我们将在下一节中对其进行更详细的描述。'
- en: Introducing SparkSession
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍SparkSession
- en: In Spark 2.0, `SparkSession` represents a unified entry point for manipulating
    data in Spark. It minimizes the number of different contexts a developer has to
    use while working with Spark. `SparkSession` replaces multiple context objects,
    such as the `SparkContext`, `SQLContext`, and `HiveContext`. These contexts are
    now encapsulated within the `SparkSession` object.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0中，`SparkSession`表示操作Spark中数据的统一入口点。它最小化了开发人员在使用Spark时必须使用的不同上下文的数量。`SparkSession`取代了多个上下文对象，如`SparkContext`、`SQLContext`和`HiveContext`。这些上下文现在封装在`SparkSession`对象中。
- en: In Spark programs, we use the builder design pattern to instantiate a `SparkSession`
    object. However, in the REPL environment (that is, in a Spark shell session),
    the `SparkSession` is automatically created and made available to you via an instance
    object called **Spark**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark程序中，我们使用构建器设计模式来实例化`SparkSession`对象。但是，在REPL环境（即在Spark shell会话中），`SparkSession`会自动创建并通过名为**Spark**的实例对象提供给您。
- en: At this time, start the Spark shell on your computer to interactively execute
    the code snippets in this section. As the shell starts up, you will notice a bunch
    of messages appearing on your screen, as shown in the following figure. You should
    see messages displaying the availability of a `SparkSession` object (as Spark),
    Spark version as 2.2.0, Scala version as 2.11.8, and the Java version as 1.8.x.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，在您的计算机上启动Spark shell以交互式地执行本节中的代码片段。随着shell的启动，您会注意到屏幕上出现了一堆消息，如下图所示。您应该看到显示`SparkSession`对象（作为Spark）、Spark版本为2.2.0、Scala版本为2.11.8和Java版本为1.8.x的消息。
- en: '![](img/00006.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00006.jpeg)'
- en: 'The `SparkSession` object can be used to configure Spark''s runtime config
    properties. For example, the two main resources that Spark and Yarn manage are
    the CPU and the memory. If you want to set the number of cores and the heap size
    for the Spark executor, then you can do that by setting the `spark.executor.cores`
    and the `spark.executor.memory` properties, respectively. In this example, we
    set these runtime properties to `2` cores and `4` GB, respectively, as shown:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`对象可用于配置Spark的运行时配置属性。例如，Spark和Yarn管理的两个主要资源是CPU和内存。如果要设置Spark执行程序的核心数和堆大小，可以分别通过设置`spark.executor.cores`和`spark.executor.memory`属性来实现。在本例中，我们将这些运行时属性分别设置为`2`个核心和`4`GB，如下所示：'
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `SparkSession` object can be used to read data from various sources, such
    as CSV, JSON, JDBC, stream, and so on. In addition, it can be used to execute
    SQL statements, register **User Defined Functions** (**UDFs**), and work with
    Datasets and DataFrames. The following session illustrates some of these basic
    operations in Spark.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`对象可用于从各种来源读取数据，如CSV、JSON、JDBC、流等。此外，它还可用于执行SQL语句、注册用户定义函数（UDFs）以及处理数据集和DataFrame。以下会话演示了Spark中的一些基本操作。'
- en: For this example, we use the breast cancer database created by Dr. William H.
    Wolberg, University of Wisconsin Hospitals, Madison. You can download the original
    Dataset from [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)).
    Each row in the dataset contains the sample number, nine cytological characteristics
    of breast fine needle aspirates graded `1` to `10`, and the class `label` , `benign
    (2)` or `malignant (4)`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们使用由威斯康星大学医院麦迪逊分校的William H. Wolberg博士创建的乳腺癌数据库。您可以从[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original))下载原始数据集。数据集中的每一行包含样本编号、乳腺细针抽吸的九个细胞学特征（分级为`1`到`10`）以及`label`类别，即`良性（2）`或`恶性（4）`。
- en: First, we define a schema for the records in our file. The field descriptions
    are available at the Dataset's download site.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为文件中的记录定义一个模式。字段描述可以在数据集的下载站点上找到。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we create a DataFrame from our input CSV file using the record schema
    defined in the preceding step:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用在前一步中定义的记录模式从输入CSV文件创建一个DataFrame：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The newly created DataFrame can be displayed using the `show()` method:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 新创建的DataFrame可以使用`show()`方法显示：
- en: '![](img/00007.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00007.jpeg)'
- en: The DataFrame can be registered as a SQL temporary view using the `createOrReplaceTempView()`
    method. This allows applications to run SQL queries using the `sql` function of
    the SparkSession object and return the results as a DataFrame.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame可以使用`createOrReplaceTempView()`方法注册为SQL临时视图。这允许应用程序使用SparkSession对象的`sql`函数运行SQL查询，并将结果作为DataFrame返回。
- en: 'Next, we create a temporary view for the DataFrame and execute a simple SQL
    statement against it:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为DataFrame创建一个临时视图，并对其执行一个简单的SQL语句：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The contents of results DataFrame are displayed using the `show()` method:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`show()`方法显示结果DataFrame的内容：
- en: '![](img/00008.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00008.jpeg)'
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/00009.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00009.jpeg)'
- en: '`SparkSession`  exposes methods (via the catalog attribute) of accessing the
    underlying metadata, such as the available databases and tables, registered UDFs,
    temporary views, and so on. Additionally, we can also cache tables, drop temporary
    views, and clear the cache. Some of these statements and their corresponding output
    are shown here:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`公开了访问底层元数据的方法（通过catalog属性），例如可用数据库和表、注册的UDF、临时视图等。此外，我们还可以缓存表、删除临时视图和清除缓存。这里展示了一些这些语句及其相应的输出：'
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/00010.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00010.jpeg)'
- en: 'can also use the `take` method to display a specific number of records in the
    DataFrame:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用`take`方法在DataFrame中显示特定数量的记录：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/00011.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00011.jpeg)'
- en: 'We can drop the temp table that we created earlier with the following statement:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下语句删除之前创建的临时表：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/00012.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00012.jpeg)'
- en: In the next few sections, we will describe RDDs, DataFrames, and Dataset constructs
    in more detail.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将更详细地描述RDD、DataFrame和Dataset的构造。
- en: Understanding Spark SQL concepts
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Spark SQL概念
- en: In this section, we will explore key concepts related to Resilient Distributed
    Datasets (RDD), DataFrames, and Datasets, Catalyst Optimizer and Project Tungsten.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨与弹性分布式数据集（RDD）、DataFrame和Dataset、Catalyst Optimizer和Project Tungsten相关的关键概念。
- en: Understanding Resilient Distributed Datasets (RDDs)
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解弹性分布式数据集（RDD）
- en: RDDs are Spark's primary distributed Dataset abstraction. It is a collection
    of data that is immutable, distributed, lazily evaluated, type inferred, and cacheable.
    Prior to execution, the developer code (using higher-level constructs such as
    SQL, DataFrames, and Dataset APIs) is converted to a DAG of RDDs (ready for execution).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: RDD是Spark的主要分布式数据集抽象。它是一个不可变的、分布式的、惰性评估的、类型推断的、可缓存的数据集合。在执行之前，开发人员的代码（使用诸如SQL、DataFrame和Dataset
    API等更高级别的构造）被转换为RDD的DAG（准备执行）。
- en: You can create RDDs by parallelizing an existing collection of data or accessing
    a Dataset residing in an external storage system, such as the file system or various
    Hadoop-based data sources. The parallelized collections form a distributed Dataset
    that enable parallel operations on them.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过并行化现有数据集合或访问存储在外部存储系统中的数据集合（例如文件系统或各种基于Hadoop的数据源）来创建RDD。并行化的集合形成了一个分布式数据集，使得可以对其进行并行操作。
- en: 'You can create a RDD from the input file with number of partitions specified,
    as shown:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从指定了分区数量的输入文件创建RDD，如下所示：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can implicitly convert the RDD to a DataFrame by importing the `spark.implicits`
    package and using the `toDF()` method:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过导入`spark.implicits`包并使用`toDF()`方法将RDD隐式转换为DataFrame：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To create a DataFrame with a specific schema, we define a Row object for the
    rows contained in the DataFrame. Additionally, we split the comma-separated data,
    convert it to a list of fields, and then map it to the Row object. Finally, we
    use the `createDataFrame()` to create the DataFrame with a specified schema:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建具有特定模式的DataFrame，我们为DataFrame中包含的行定义一个Row对象。此外，我们将逗号分隔的数据拆分，转换为字段列表，然后将其映射到Row对象。最后，我们使用`createDataFrame()`创建具有指定模式的DataFrame：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Further, we can easily convert the preceding DataFrame to a Dataset using the
    `case` class defined earlier:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以轻松地使用之前定义的`case`类将前述DataFrame转换为数据集：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: RDD data is logically divided into a set of partitions; additionally, all input,
    intermediate, and output data is also represented as partitions. The number of
    RDD partitions defines the level of data fragmentation. These partitions are also
    the basic units of parallelism. Spark execution jobs are split into multiple stages,
    and as each stage operates on one partition at a time, it is very important to
    tune the number of partitions. Fewer partitions than active stages means your
    cluster could be under-utilized, while an excessive number of partitions could
    impact the performance due to higher disk and network I/O.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: RDD数据在逻辑上被划分为一组分区；此外，所有输入、中间和输出数据也被表示为分区。RDD分区的数量定义了数据的碎片化程度。这些分区也是并行性的基本单元。Spark执行作业被分成多个阶段，每个阶段一次操作一个分区，因此调整分区的数量非常重要。比活跃阶段少的分区意味着您的集群可能被低效利用，而过多的分区可能会影响性能，因为会导致更高的磁盘和网络I/O。
- en: 'The programming interface to RDDs support two types of operations: transformations
    and actions. The transformations create a new Dataset from an existing one, while
    the actions return a value or result of a computation. All transformations are
    evaluated lazily--the actual execution occurs only when an action is executed
    to compute a result. The transformations form a lineage graph instead of actually
    replicating data across multiple machines. This graph-based approach enables an
    efficient fault tolerance model. For example, if an RDD partition is lost, then
    it can be recomputed based on the lineage graph.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: RDD的编程接口支持两种类型的操作：转换和动作。转换从现有数据集创建一个新的数据集，而动作返回计算结果的值。所有转换都是惰性评估的--实际执行只发生在执行动作以计算结果时。转换形成一个谱系图，而不是实际在多台机器上复制数据。这种基于图的方法实现了高效的容错模型。例如，如果丢失了一个RDD分区，那么可以根据谱系图重新计算它。
- en: You can control data persistence (for example, caching) and specify placement
    preferences for RDD partitions and then use specific operators for manipulating
    them. By default, Spark persists RDDs in memory, but it can spill them to disk
    if sufficient RAM isn't available. Caching improves performance by several orders
    of magnitude; however, it is often memory intensive. Other persistence options
    include storing RDDs to disk and replicating them across the nodes in your cluster.
    The in-memory storage of persistent RDDs can be in the form of deserialized or
    serialized Java objects. The deserialized option is faster, while the serialized
    option is more memory-efficient (but slower). Unused RDDs are automatically removed
    from the cache but, depending on your requirements; if a specific RDD is no longer
    required, then you can also explicitly release it.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以控制数据持久性（例如缓存）并指定RDD分区的放置偏好，然后使用特定的操作符对其进行操作。默认情况下，Spark将RDD持久化在内存中，但如果内存不足，它可以将它们溢出到磁盘。缓存通过几个数量级提高了性能；然而，它通常占用大量内存。其他持久性选项包括将RDD存储到磁盘并在集群中的节点之间复制它们。持久RDD的内存存储可以是反序列化或序列化的Java对象形式。反序列化选项更快，而序列化选项更节省内存（但更慢）。未使用的RDD将自动从缓存中删除，但根据您的要求；如果不再需要特定的RDD，则也可以显式释放它。
- en: Understanding DataFrames and Datasets
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解DataFrames和Datasets
- en: A DataFrame is similar to a table in a relational database, a pandas dataframe,
    or a data frame in R. It is a distributed collection of rows that is organized
    into columns. It uses the immutable, in-memory, resilient, distributed, and parallel
    capabilities of RDD, and applies a schema to the data. DataFrames are also evaluated
    lazily. Additionally, they provide a **domain-specific language** (**DSL**) for
    distributed data manipulation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame类似于关系数据库中的表、pandas dataframe或R中的数据框。它是一个分布式的行集合，组织成列。它使用RDD的不可变、内存中、弹性、分布式和并行能力，并对数据应用模式。DataFrames也是惰性评估的。此外，它们为分布式数据操作提供了领域特定语言（DSL）。
- en: Conceptually, the DataFrame is an alias for a collection of generic objects
    `Dataset[Row]`, where a row is a generic untyped object. This means that syntax
    errors for DataFrames are caught during the compile stage; however, analysis errors
    are detected only during runtime.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，DataFrame是一组通用对象`Dataset[Row]`的别名，其中行是通用的无类型对象。这意味着DataFrame的语法错误在编译阶段被捕获；然而，分析错误只在运行时被检测到。
- en: DataFrames can be constructed from a wide array of sources, such as structured
    data files, Hive tables, databases, or RDDs. The source data can be read from
    local filesystems, HDFS, Amazon S3, and RDBMSs. In addition, other popular data
    formats, such as CSV, JSON, Avro, Parquet, and so on, are also supported. Additionally,
    you can also create and use custom data sources.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame可以从各种来源构建，例如结构化数据文件、Hive表、数据库或RDD。源数据可以从本地文件系统、HDFS、Amazon S3和RDBMS中读取。此外，还支持其他流行的数据格式，如CSV、JSON、Avro、Parquet等。此外，您还可以创建和使用自定义数据源。
- en: The DataFrame API supports Scala, Java, Python, and R programming APIs. The
    DataFrames API is declarative, and combined with procedural Spark code, it provides
    a much tighter integration between the relational and procedural processing in
    your applications. DataFrames can be manipulated using Spark's procedural API,
    or using relational APIs (with richer optimizations).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API支持Scala、Java、Python和R编程API。DataFrame API是声明式的，并与Spark的过程式代码结合使用，为应用程序中的关系和过程式处理提供了更紧密的集成。可以使用Spark的过程式API或使用关系API（具有更丰富的优化）来操作DataFrame。
- en: In the early versions of Spark, you had to write arbitrary Java, Python, or
    Scala functions that operated on RDDs. In this scenario, the functions were executing
    on opaque Java objects. Hence, the user functions were essentially black boxes
    executing opaque computations using opaque objects and data types. This approach
    was very general and such programs had complete control over the execution of
    every data operation. However, as the engine did not know the code you were executing
    or the nature of the data, it was not possible to optimize these arbitrary Java
    objects. In addition, it was incumbent on the developers to write efficient programs
    that were dependent on the nature of their specific workloads.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark的早期版本中，您必须编写操作RDD的任意Java、Python或Scala函数。在这种情况下，函数是在不透明的Java对象上执行的。因此，用户函数本质上是执行不透明计算的黑匣子，使用不透明对象和数据类型。这种方法非常通用，这样的程序可以完全控制每个数据操作的执行。然而，由于引擎不知道您正在执行的代码或数据的性质，因此无法优化这些任意的Java对象。此外，开发人员需要编写依赖于特定工作负载性质的高效程序。
- en: In Spark 2.0, the main benefit of using SQL, DataFrames, and Datasets is that
    it's easier to program using these high-level programming interfaces while reaping
    the benefits of improved performance, automatically. You have to write significantly
    fewer lines of code and the programs are automatically optimized and efficient
    code is generated for you. This results in better performance while significantly
    reducing the burden on developers. Now, the developer can focus on the "what"
    rather than the "how" of something that needs to be accomplished.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0中，使用SQL、DataFrames和Datasets的主要好处是，使用这些高级编程接口编程更容易，同时自动获得性能改进的好处。您只需编写更少的代码行，程序就会自动优化，并为您生成高效的代码。这样可以提高性能，同时显著减轻开发人员的负担。现在，开发人员可以专注于“做什么”，而不是“如何完成”。
- en: The Dataset API was first added to Spark 1.6 to provide the benefits of both
    RDDs and the Spark SQL's optimizer. A Dataset can be constructed from JVM objects
    and then manipulated using functional transformations such as `map`, `filter`,
    and so on. As the Dataset is a collection of strongly-typed objects specified
    using a user-defined case class, both syntax errors and analysis errors can be
    detected at compile time.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集API首次添加到Spark 1.6中，以提供RDD和Spark SQL优化器的优点。数据集可以从JVM对象构造，然后使用`map`、`filter`等函数变换进行操作。由于数据集是使用用户定义的case类指定的强类型对象的集合，因此可以在编译时检测到语法错误和分析错误。
- en: The unified Dataset API can be used in both Scala and Java. However, Python
    does not support the Dataset API yet.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 统一的数据集API可以在Scala和Java中使用。但是Python目前还不支持数据集API。
- en: In the following example, we present a few basic DataFrame/Dataset operations.
    For this purpose, we will use two restaurant listing datasets that are typically
    used in duplicate records detection and record linkage applications. The two lists,
    one each from Zagat's and Fodor's restaurant guides, have duplicate records between
    them. To keep this example simple, we have manually converted the input files
    to a CSV format. You can download the original dataset from [http://www.cs.utexas.edu/users/ml/riddle/data.html](http://www.cs.utexas.edu/users/ml/riddle/data.html).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们介绍了一些基本的DataFrame/Dataset操作。为此，我们将使用两个餐厅列表数据集，这些数据集通常用于重复记录检测和记录链接应用。来自Zagat和Fodor餐厅指南的两个列表之间存在重复记录。为了使这个例子简单，我们手动将输入文件转换为CSV格式。您可以从[http://www.cs.utexas.edu/users/ml/riddle/data.html](http://www.cs.utexas.edu/users/ml/riddle/data.html)下载原始数据集。
- en: 'First, we define a `case` class for the records in the two files:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为两个文件中的记录定义一个`case`类：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we create Datasets from the two files:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从两个文件创建数据集：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We define a UDF to clean up and transform the phone numbers in the second Dataset
    to match the format in the first file:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个UDF来清理和转换第二个数据集中的电话号码，以匹配第一个文件中的格式：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/00013.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00013.jpeg)'
- en: 'Next, we create temporary views from our Datasets:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从我们的数据集创建临时视图：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can get a count of the number of duplicates, by executing a SQL statement
    on these tables that returns the count of the number of records with matching
    phone numbers:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在这些表上执行SQL语句来获取重复记录的数量：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/00014.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00014.jpeg)'
- en: 'Next, we execute a SQL statement that returns a DataFrame containing the rows
    with matching phone numbers:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们执行一个返回包含匹配电话号码的行的DataFrame的SQL语句：
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The results listing the name and the phone number columns from the two tables
    can be displayed to visually verify, if the results are possible duplicates:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从两个表中列出的名称和电话号码列的结果可以显示，以直观地验证结果是否可能重复：
- en: '![](img/00015.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00015.jpeg)'
- en: In the next section, we will shift our focus to Spark SQL internals, more specifically,
    to the Catalyst optimizer and Project Tungsten.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把重点转移到Spark SQL内部，更具体地说，是Catalyst优化器和Project Tungsten。
- en: Understanding the Catalyst optimizer
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Catalyst优化器
- en: The Catalyst optimizer is at the core of Spark SQL and is implemented in Scala.
    It enables several key features, such as schema inference (from JSON data), that
    are very useful in data analysis work.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器是Spark SQL的核心，用Scala实现。它实现了一些关键功能，例如模式推断（从JSON数据中），这在数据分析工作中非常有用。
- en: 'The following figure shows the high-level transformation process from a developer''s
    program containing DataFrames/Datasets to the final execution plan:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了从包含DataFrame/Dataset的开发人员程序到最终执行计划的高级转换过程：
- en: '![](img/00016.jpeg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00016.jpeg)'
- en: The internal representation of the program is a query plan. The query plan describes
    data operations such as aggregate, join, and filter, which match what is defined
    in your query. These operations generate a new Dataset from the input Dataset.
    After we have an initial version of the query plan ready, the Catalyst optimizer
    will apply a series of transformations to convert it to an optimized query plan.
    Finally, the Spark SQL code generation mechanism translates the optimized query
    plan into a DAG of RDDs that is ready for execution. The query plans and the optimized
    query plans are internally represented as trees. So, at its core, the Catalyst
    optimizer contains a general library for representing trees and applying rules
    to manipulate them. On top of this library, are several other libraries that are
    more specific to relational query processing.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的内部表示是查询计划。查询计划描述诸如聚合、连接和过滤等数据操作，这些操作与查询中定义的内容相匹配。这些操作从输入数据集生成一个新的数据集。在我们有查询计划的初始版本后，Catalyst优化器将应用一系列转换将其转换为优化的查询计划。最后，Spark
    SQL代码生成机制将优化的查询计划转换为准备执行的RDD的DAG。查询计划和优化的查询计划在内部表示为树。因此，在其核心，Catalyst优化器包含一个用于表示树和应用规则来操作它们的通用库。在这个库之上，还有几个更具体于关系查询处理的其他库。
- en: 'Catalyst has two types of query plans: **Logical** and **Physical Plans**.
    The Logical Plan describes the computations on the Datasets without defining how
    to carry out the specific computations. Typically, the Logical Plan generates
    a list of attributes or columns as output under a set of constraints on the generated
    rows. The Physical Plan describes the computations on Datasets with specific definitions
    on how to execute them (it is executable).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst有两种类型的查询计划：**逻辑**和**物理计划**。逻辑计划描述了数据集上的计算，而没有定义如何执行具体的计算。通常，逻辑计划在生成的行的一组约束下生成属性或列的列表作为输出。物理计划描述了数据集上的计算，并具体定义了如何执行它们（可执行）。
- en: Let's explore the transformation steps in more detail. The initial query plan
    is essentially an unresolved Logical Plan, that is, we don't know the source of
    the Datasets or the columns (contained in the Dataset) at this stage and we also
    don't know the types of columns. The first step in this pipeline is the analysis
    step. During analysis, the catalog information is used to convert the unresolved
    Logical Plan to a resolved Logical Plan.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨转换步骤。初始查询计划本质上是一个未解析的逻辑计划，也就是说，在这个阶段我们不知道数据集的来源或数据集中包含的列，我们也不知道列的类型。这个管道的第一步是分析步骤。在分析过程中，使用目录信息将未解析的逻辑计划转换为已解析的逻辑计划。
- en: In the next step, a set of logical optimization rules is applied to the resolved
    Logical Plan, resulting in an optimized Logical Plan. In the next step the optimizer
    may generate multiple Physical Plans and compare their costs to pick the best
    one. The first version of the **Cost-based Optimizer** (**CBO**), built on top
    of Spark SQL has been released in Spark 2.2\. More details on cost-based optimization
    are presented in [Chapter 11](part0197.html#5RRUQ0-e9cbc07f866e437b8aa14e841622275c), *Tuning
    Spark SQL Components for Performance*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，一组逻辑优化规则被应用于已解析的逻辑计划，从而产生一个优化的逻辑计划。在下一步中，优化器可能生成多个物理计划，并比较它们的成本以选择最佳的一个。建立在Spark
    SQL之上的第一个版本的**基于成本的优化器**（**CBO**）已经在Spark 2.2中发布。有关基于成本的优化的更多细节，请参阅[第11章](part0197.html#5RRUQ0-e9cbc07f866e437b8aa14e841622275c)，*调整Spark
    SQL组件以提高性能*。
- en: 'All three--**DataFrame**, **Dataset** and SQL--share the same optimization
    pipeline as illustrated in the following figure:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三个--**DataFrame**、**Dataset**和SQL--都共享如下图所示的相同优化管道：
- en: '![](img/00017.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00017.jpeg)'
- en: Understanding Catalyst optimizations
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Catalyst优化
- en: 'In Catalyst, there are two main types of optimizations: Logical and Physical:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在Catalyst中，有两种主要类型的优化：逻辑和物理：
- en: '**Logical Optimizations**: This includes the ability of the optimizer to push
    filter predicates down to the data source and enable execution to skip irrelevant
    data. For example, in the case of Parquet files, entire blocks can be skipped
    and comparisons on strings can be turned into cheaper integer comparisons via
    dictionary encoding. In the case of RDBMSs, the predicates are pushed down to
    the database to reduce the amount of data traffic.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑优化**：这包括优化器将过滤谓词下推到数据源并使执行跳过无关数据的能力。例如，在Parquet文件的情况下，整个块可以被跳过，并且字符串的比较可以通过字典编码转换为更便宜的整数比较。在关系型数据库的情况下，谓词被下推到数据库以减少数据流量。'
- en: '**Physical Optimizations**: This includes the ability to choose intelligently
    between broadcast joins and shuffle joins to reduce network traffic, performing
    lower-level optimizations, such as eliminating expensive object allocations and
    reducing virtual function calls. Hence, and performance typically improves when
    DataFrames are introduced in your programs.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物理优化**：这包括智能地选择广播连接和洗牌连接以减少网络流量，执行更低级别的优化，如消除昂贵的对象分配和减少虚拟函数调用。因此，当在程序中引入DataFrame时，性能通常会提高。'
- en: The Rule Executor is responsible for the analysis and logical optimization steps,
    while a set of strategies and the Rule Executor are responsible for the physical
    planning step. The Rule Executor transforms a tree to another of the same type
    by applying a set of rules in batches. These rules can be applied one or more
    times. Also, each of these rules is implemented as a transform. A transform is
    basically a function, associated with every tree, and is used to implement a single
    rule. In Scala terms, the transformation is defined as a partial function (a function
    defined for a subset of its possible arguments). These are typically defined as
    case statements to determine whether the partial function (using pattern matching)
    is defined for the given input.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 规则执行器负责分析和逻辑优化步骤，而一组策略和规则执行器负责物理规划步骤。规则执行器通过批量应用一组规则将一个树转换为另一个相同类型的树。这些规则可以应用一次或多次。此外，每个规则都被实现为一个转换。转换基本上是一个函数，与每个树相关联，并用于实现单个规则。在Scala术语中，转换被定义为部分函数（对其可能的参数子集定义的函数）。这些通常被定义为case语句，以确定部分函数（使用模式匹配）是否对给定输入定义。
- en: The Rule Executor makes the Physical Plan ready for execution by preparing scalar
    subqueries, ensuring that the input rows meet the requirements of the specific
    operation and applying the physical optimizations. For example, in the sort merge
    join operations, the input rows need to be sorted as per the join condition. The
    optimizer inserts the appropriate sort operations, as required, on the input rows
    before the sort merge join operation is executed.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 规则执行器使物理计划准备好执行，通过准备标量子查询，确保输入行满足特定操作的要求，并应用物理优化。例如，在排序合并连接操作中，输入行需要根据连接条件进行排序。优化器在执行排序合并连接操作之前插入适当的排序操作，如有必要。
- en: Understanding Catalyst transformations
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Catalyst转换
- en: Conceptually, the Catalyst optimizer executes two types of transformations.
    The first one converts an input tree type to the same tree type (that is, without
    changing the tree type). This type of transformation includes converting one expression
    to another expression, one Logical Plan to another Logical Plan, and one Physical
    Plan to another Physical Plan. The second type of transformation converts one
    tree type to another type, for example, from a Logical Plan to a Physical Plan.
    A Logical Plan is converted to a Physical Plan by applying a set of strategies.
    These strategies use pattern matching to convert a tree to the other type. For
    example, we have specific patterns for matching logical project and filter operators
    to physical project and filter operators, respectively.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念上，Catalyst优化器执行两种类型的转换。第一种将输入树类型转换为相同的树类型（即，不改变树类型）。这种类型的转换包括将一个表达式转换为另一个表达式，一个逻辑计划转换为另一个逻辑计划，一个物理计划转换为另一个物理计划。第二种类型的转换将一个树类型转换为另一个类型，例如，从逻辑计划转换为物理计划。通过应用一组策略，逻辑计划被转换为物理计划。这些策略使用模式匹配将树转换为另一种类型。例如，我们有特定的模式用于匹配逻辑项目和过滤运算符到物理项目和过滤运算符。
- en: A set of rules can also be combined into a single rule to accomplish a specific
    transformation. For example, depending on your query, predicates such as filter
    can be pushed down to reduce the overall number of rows before executing a join
    operation. In addition, if your query has an expression with constants in your
    query, then constant folding optimization computes the expression once at the
    time of compilation instead of repeating it for every row during runtime. Furthermore,
    if your query requires a subset of columns, then column pruning can help reduce
    the columns to the essential ones. All these rules can be combined into a single
    rule to achieve all three transformations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一组规则也可以合并成一个单一的规则来完成特定的转换。例如，根据您的查询，诸如过滤器之类的谓词可以被推送下来以减少执行连接操作之前的总行数。此外，如果您的查询中有一个带有常量的表达式，那么常量折叠优化会在编译时一次计算表达式，而不是在运行时为每一行重复计算。此外，如果您的查询需要一部分列，那么列修剪可以帮助减少列到必要的列。所有这些规则可以合并成一个单一的规则，以实现所有三种转换。
- en: In the following example, we measure the difference in execution times on Spark
    1.6 and Spark 2.2\. We use the iPinYou Real-Time Bidding Dataset for Computational
    Advertising Research in our next example. This Dataset contains the data from
    three seasons of the iPinYou global RTB bidding algorithm competition. You can
    download this Dataset from the data server at University College London at [http://data.computational-advertising.org/](http://data.computational-advertising.org/).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们测量了Spark 1.6和Spark 2.2上的执行时间差异。我们在下一个示例中使用iPinYou实时竞价数据集进行计算广告研究。该数据集包含iPinYou全球RTB竞价算法竞赛的三个赛季的数据。您可以从伦敦大学学院的数据服务器上下载该数据集，网址为[http://data.computational-advertising.org/](http://data.computational-advertising.org/)。
- en: 'First, we define the `case` classes for the records in the `bid transactions` and
    the `region`  files:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为`bid transactions`和`region`文件中的记录定义`case`类：
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we create the DataFrames from one of the `bids` files and the `region`
    file:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从一个`bids`文件和`region`文件创建DataFrames：
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we borrow a simple benchmark function (available in several Databricks
    sample notebooks) to measure the execution time:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们借用一个简单的基准函数（在几个Databricks示例笔记本中可用）来测量执行时间：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We use the SparkSession object to set the whole-stage code generation parameter
    off (this roughly translates to the Spark 1.6 environment). We also measure the
    execution time for a `join` operation between the two DataFrames:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用SparkSession对象将整体阶段代码生成参数关闭（这大致相当于Spark 1.6环境）。我们还测量了两个DataFrame之间的`join`操作的执行时间：
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we set the whole-stage code generation parameter to true and measure
    the execution time. We note that the execution time is much lower for the same
    code in Spark 2.2:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将整体阶段代码生成参数设置为true，并测量执行时间。我们注意到在Spark 2.2中，相同代码的执行时间要低得多：
- en: '[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We use the `explain()` function to print out the various stages in the Catalyst
    transformations pipeline. We will explain the following output in more detail
    in [Chapter 11](part0197.html#5RRUQ0-e9cbc07f866e437b8aa14e841622275c), *Tuning
    Spark SQL Components for Performance*:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`explain()`函数来打印出Catalyst转换管道中的各个阶段。我们将在[第11章](part0197.html#5RRUQ0-e9cbc07f866e437b8aa14e841622275c)中更详细地解释以下输出，*调整Spark
    SQL组件以提高性能*：
- en: '[PRE24]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](img/00018.jpeg)![](img/00019.jpeg)![](img/00020.jpeg)![](img/00021.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00018.jpeg)![](img/00019.jpeg)![](img/00020.jpeg)![](img/00021.jpeg)'
- en: In the next section, we present developer-relevant details of Project Tungsten.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将介绍Project Tungsten的与开发人员相关的细节。
- en: Introducing Project Tungsten
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入Project Tungsten
- en: Project Tungsten was touted as the largest change to Spark's execution engine
    since the project's inception. The motivation for Project Tungsten was the observation
    that CPU and memory, rather than I/O and network, were the bottlenecks in a majority
    of Spark workloads.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Project Tungsten被吹捧为自项目成立以来对Spark执行引擎的最大改变。Project Tungsten的动机是观察到在大多数Spark工作负载中，CPU和内存而不是I/O和网络是瓶颈。
- en: The CPU is the bottleneck now because of the improvements in hardware (for example,
    SSDs and striped HDD arrays for storage), optimizations done to Spark's I/O (for
    example, shuffle and network layer implementations, input data pruning for disk
    I/O reduction, and so on) and improvements in data formats (for example, columnar
    formats like Parquet, binary data formats, and so on). In addition, large-scale
    serialization and hashing tasks in Spark are CPU-bound operations.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 由于硬件改进（例如SSD和条带化HDD阵列用于存储）、Spark I/O的优化（例如shuffle和网络层实现、输入数据修剪以减少磁盘I/O等）和数据格式的改进（例如Parquet、二进制数据格式等），CPU现在成为瓶颈。此外，Spark中的大规模序列化和哈希任务是CPU绑定操作。
- en: Spark 1.x used a query evaluation strategy based on an iterator model (referred
    to as the Volcano model). As each operator in a query presented an interface that
    returned a tuple at a time to the next operator in the tree, this interface allowed
    query execution engines to compose arbitrary combinations of operators. Before
    Spark 2.0, a majority of the CPU cycles were spent in useless work, such as making
    virtual function calls or reading/writing intermediate data to CPU cache or memory.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 1.x使用基于迭代器模型的查询评估策略（称为Volcano模型）。由于查询中的每个运算符都呈现了一个接口，该接口每次返回一个元组给树中的下一个运算符，因此这个接口允许查询执行引擎组合任意组合的运算符。在Spark
    2.0之前，大部分CPU周期都花在无用的工作上，比如进行虚拟函数调用或者读取/写入中间数据到CPU缓存或内存。
- en: Project Tungsten focuses on three areas to improve the efficiency of memory
    and CPU to push the performance closer to the limits of the underlying hardware.
    These three areas are memory management and binary processing, cache-aware computation,
    and code generation. Additionally, the second generation Tungsten execution engine,
    integrated in Spark 2.0, uses a technique called whole-stage code generation.
    This technique enables the engine to eliminate virtual function dispatches and
    move intermediate data from memory to CPU registers, and exploits the modern CPU
    features through loop unrolling and SIMD. In addition, the Spark 2.0 engine also
    speeds up operations considered too complex for code generation by employing another
    technique, called vectorization.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Tungsten项目专注于三个领域，以提高内存和CPU的效率，将性能推向底层硬件的极限。这三个领域是内存管理和二进制处理、缓存感知计算和代码生成。此外，集成在Spark
    2.0中的第二代Tungsten执行引擎使用一种称为整体代码生成的技术。这种技术使引擎能够消除虚拟函数调度，并将中间数据从内存移动到CPU寄存器，并通过循环展开和SIMD利用现代CPU特性。此外，Spark
    2.0引擎还通过使用另一种称为矢量化的技术加速了被认为对于代码生成过于复杂的操作。
- en: Whole-stage code generation collapses the entire query into a single function.
    Further, it eliminates virtual function calls and uses CPU registers for storing
    intermediate data. This in turn, significantly improves CPU efficiency and runtime
    performance. It achieves the performance of hand-written code, while continuing
    to remain a general-purpose engine.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 整体代码生成将整个查询折叠成一个单一函数。此外，它消除了虚拟函数调用，并使用CPU寄存器存储中间数据。这反过来显著提高了CPU效率和运行时性能。它实现了手写代码的性能，同时继续保持通用引擎。
- en: In vectorization, the engine batches multiple rows together in a columnar format
    and each operator iterates over the data within a batch. However, it still requires
    putting intermediate data in-memory rather than keeping them in CPU registers.
    As a result, vectorization is only used when it is not possible to do whole-stage
    code generation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在矢量化中，引擎以列格式批处理多行数据，每个运算符在一个批次内对数据进行迭代。然而，它仍然需要将中间数据放入内存，而不是保留在CPU寄存器中。因此，只有在无法进行整体代码生成时才使用矢量化。
- en: Tungsten memory management improvements focus on storing Java objects in compact
    binary format to reduce GC overhead, denser in-memory data format to reduce spillovers
    (for example, the Parquet format), and for operators that understand data types
    (in the case of DataFrames, Datasets, and SQL) to work directly against binary
    format in memory rather than serialization/deserialization and so on.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Tungsten内存管理改进侧重于将Java对象以紧凑的二进制格式存储，以减少GC开销，将内存中的数据格式更加密集，以减少溢出（例如Parquet格式），并且对于了解数据类型的运算符（在DataFrames、Datasets和SQL的情况下）直接针对内存中的二进制格式进行操作，而不是进行序列化/反序列化等操作。
- en: Code generation exploits modern compilers and CPUs for implementing improvements.
    These include faster expression evaluation and DataFrame/SQL operators, and a
    faster serializer. Generic evaluation of expressions is very expensive on the
    JVM, due to virtual function calls, branches based on expression type, object
    creation, and memory consumption due to primitive boxing. By generating custom
    bytecode on the fly, these overheads are largely eliminated.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成利用现代编译器和CPU来实现改进。这包括更快的表达式评估和DataFrame/SQL运算符，以及更快的序列化器。在JVM上对表达式的通用评估非常昂贵，因为涉及虚拟函数调用、基于表达式类型的分支、对象创建和由于原始装箱而导致的内存消耗。通过动态生成自定义字节码，这些开销大大减少了。
- en: 'Here, we present the Physical Plan for our join operation between the bids
    and the region DataFrames from the preceding section with whole-stage code generation
    enabled. In the `explain()` output, when an operator is marked with a star `*`, then
    it means that the whole-stage code generation is enabled for that operation. In
    the following physical plan, this includes the Aggregate, Project, `SortMergeJoin`,
    Filter, and Sort operators. Exchange, however, does not implement whole-stage
    code generation because it is sending data across the network:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍了启用了整体代码生成的前一节中的投标和地区DataFrames之间的连接操作的物理计划。在`explain()`输出中，当一个运算符标有星号`*`时，这意味着该运算符已启用整体代码生成。在以下物理计划中，这包括Aggregate、Project、`SortMergeJoin`、Filter和Sort运算符。然而，Exchange不实现整体代码生成，因为它正在通过网络发送数据：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/00022.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00022.jpeg)'
- en: Project Tungsten hugely benefits DataFrames and Datasets (for all programming
    APIs--Java, Scala, Python, and R) and Spark SQL queries. Also, for many of the
    data processing operators, the new engine is orders of magnitude faster.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Tungsten项目极大地改进了DataFrames和Datasets（适用于所有编程API - Java、Scala、Python和R）和Spark
    SQL查询。此外，对于许多数据处理运算符，新引擎的速度提高了数个数量级。
- en: In the next section, we shift our focus to a new Spark 2.0 feature, called Structured
    Streaming, that supports Spark-based streaming applications.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将把重点转移到一个名为Structured Streaming的新Spark 2.0功能，它支持基于Spark的流应用程序。
- en: Using Spark SQL in streaming applications
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在流应用程序中使用Spark SQL
- en: Streaming applications are getting increasingly complex, because such computations
    don't run in isolation. They need to interact with batch data, support interactive
    analysis, support sophisticated machine learning applications, and so on. Typically,
    such applications store incoming event stream(s) on long-term storage, continuously
    monitor events, and run machine learning models on the stored data, while simultaneously
    enabling continuous learning on the incoming stream. They also have the capability
    to interactively query the stored data while providing exactly-once write guarantees,
    handling late arriving data, performing aggregations, and so on. These types of
    applications are a lot more than mere streaming applications and have, therefore,
    been termed as continuous applications.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 流应用变得越来越复杂，因为这样的计算不是孤立运行的。它们需要与批处理数据交互，支持交互式分析，支持复杂的机器学习应用等。通常，这样的应用将传入的事件流存储在长期存储中，持续监视事件，并在存储的数据上运行机器学习模型，同时在传入流上启用持续学习。它们还具有交互式查询存储的数据的能力，同时提供精确一次的写入保证，处理延迟到达的数据，执行聚合等。这些类型的应用远不止是简单的流应用，因此被称为连续应用。
- en: Before Spark 2.0, streaming applications were built on the concept of DStreams.
    There were several pain points associated with using DStreams. In DStreams, the
    timestamp was when the event actually came into the Spark system; the time embedded
    in the event was not taken into consideration. In addition, though the same engine
    can process both the batch and streaming computations, the APIs involved, though
    similar between RDDs (batch) and DStream (streaming), required the developer to
    make code changes. The DStream streaming model placed the burden on the developer
    to address various failure conditions, and it was hard to reason about data consistency
    issues. In Spark 2.0, Structured Streaming was introduced to deal with all of
    these pain points.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0之前，流应用是建立在DStreams的概念上的。使用DStreams存在一些痛点。在DStreams中，时间戳是事件实际进入Spark系统的时间；事件中嵌入的时间不被考虑。此外，尽管相同的引擎可以处理批处理和流处理计算，但涉及的API虽然在RDD（批处理）和DStream（流处理）之间相似，但需要开发人员进行代码更改。DStream流模型让开发人员承担了处理各种故障条件的负担，并且很难推理数据一致性问题。在Spark
    2.0中，引入了结构化流处理来解决所有这些痛点。
- en: Structured Streaming is a fast, fault-tolerant, exactly-once stateful stream
    processing approach. It enables streaming analytics without having to reason about
    the underlying mechanics of streaming. In the new model, the input can be thought
    of as data from an append-only table (that grows continuously). A trigger specifies
    the time interval for checking the input for the arrival of new data. As shown
    in the following figure, the query represents the queries or the operations, such
    as map, filter, and reduce on the input, and result represents the final table
    that is updated in each trigger interval, as per the specified operation. The
    output defines the part of the result to be written to the data sink in each time
    interval.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理是一种快速、容错、精确一次的有状态流处理方法。它使流分析无需考虑流的基本机制。在新模型中，输入可以被视为来自一个不断增长的追加表的数据。触发器指定了检查输入以获取新数据到达的时间间隔。如下图所示，查询表示查询或操作，例如map、filter和reduce在输入上的操作，结果表示根据指定的操作在每个触发间隔更新的最终表。输出定义了每个时间间隔写入数据接收器的结果的部分。
- en: 'The output modes can be complete, delta, or append, where the complete output
    mode means writing the full result table every time, the delta output mode writes
    the changed rows from the previous batch, and the append output mode writes the
    new rows only, respectively:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 输出模式可以是complete、delta或append，其中complete输出模式表示每次写入完整的结果表，delta输出模式写入前一批次的更改行，append输出模式分别只写入新行：
- en: '![](img/00023.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00023.jpeg)'
- en: In Spark 2.0, in addition to the static bounded DataFrames, we have the concept
    of a continuous unbounded DataFrame. Both static and continuous DataFrames use
    the same API, thereby unifying streaming, interactive, and batch queries. For
    example, you can aggregate data in a stream and then serve it using JDBC. The
    high-level streaming API is built on the Spark SQL engine and is tightly integrated
    with SQL queries and the DataFrame/Dataset APIs. The primary benefit is that you
    use the same high-level Spark DataFrame and Dataset APIs, and the Spark engine
    figures out the incremental and continuous execution required for operations.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0中，除了静态有界的DataFrame，我们还有连续无界的DataFrame的概念。静态和连续的DataFrame都使用相同的API，从而统一了流、交互和批处理查询。例如，您可以在流中聚合数据，然后使用JDBC提供服务。高级流API建立在Spark
    SQL引擎上，并与SQL查询和DataFrame/Dataset API紧密集成。主要好处是您可以使用相同的高级Spark DataFrame和Dataset
    API，Spark引擎会找出所需的增量和连续执行操作。
- en: Additionally, there are query management APIs that you can use to manage multiple,
    concurrently running, and streaming queries. For instance, you can list running
    queries, stop and restart queries, retrieve exceptions in case of failures, and
    so on. We will get more details regarding Structured Streaming in [Chapter 5](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c), *Using
    Spark SQL in Streaming Applications.*
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有查询管理API，您可以使用它来管理多个并发运行的流查询。例如，您可以列出运行中的查询，停止和重新启动查询，在失败的情况下检索异常等。我们将在[第5章](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c)中详细了解结构化流处理，*在流应用中使用Spark
    SQL*。
- en: 'In the example code below, we use two bid files from the iPinYou Dataset as
    the source for our streaming data. First, we define our input records schema and
    create a streaming input DataFrame:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例代码中，我们使用iPinYou数据集中的两个出价文件作为我们流数据的来源。首先，我们定义我们的输入记录模式并创建一个流输入DataFrame：
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we define our query with a time interval of `20 seconds` and the output
    mode as `Complete`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的查询时间间隔为`20秒`，输出模式为`Complete`：
- en: '[PRE27]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the output, you will observe that the count of bids from each region gets
    updated in each time interval as new data arrives. You will need to drop new bid
    files (or start with multiple bid files, as they will get picked up for processing
    one at a time based on the value of `maxFilesPerTrigger`) from the original Dataset
    into the `bidfiles` directory to see the updated results:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，您将观察到每个区域的出价数量在每个时间间隔中随着新数据的到达而更新。您需要将新的出价文件（或者从原始数据集中开始使用多个出价文件，它们将根据`maxFilesPerTrigger`的值依次被处理）放入`bidfiles`目录中，以查看更新后的结果：
- en: '![](img/00024.jpeg)![](img/00025.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00024.jpeg)![](img/00025.jpeg)'
- en: 'Additionally, you can also query the system for active streams, as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以查询系统中的活动流，如下所示：
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, you can stop the execution of your streaming application using the
    `stop()` method, as shown:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以使用`stop()`方法停止流应用程序的执行，如下所示：
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the next section, we conceptually describe how Structured Streaming works
    internally.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将从概念上描述结构化流的内部工作原理。
- en: Understanding Structured Streaming internals
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解结构化流的内部机制
- en: To enable the Structured Streaming functionality, the planner polls for new
    data from the sources and incrementally executes the computation on it before
    writing it to the sink. In addition, any running aggregates required by your application
    are maintained as in-memory states backed by a **Write-Ahead Log** (**WAL**).
    The in-memory state data is generated and used across incremental executions.
    The fault tolerance requirements for such applications include the ability to
    recover and replay all data and metadata in the system. The planner writes offsets
    to a fault-tolerant WAL on persistent storage, such as HDFS, before execution
    as illustrated in the figure:.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用结构化流功能，规划器会从源中轮询新数据，并在写入到接收器之前对其进行增量计算。此外，应用程序所需的任何运行聚合都将作为由**Write-Ahead
    Log**（**WAL**）支持的内存状态进行维护。内存状态数据是在增量执行中生成和使用的。这类应用程序的容错需求包括能够恢复和重放系统中的所有数据和元数据。规划器在执行之前将偏移量写入到持久存储（如HDFS）上的容错WAL中，如图所示：
- en: '![](img/00026.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00026.jpeg)'
- en: In case the planner fails on the current incremental execution, the restarted
    planner reads from the WAL and re-executes the exact range of offsets required.
    Typically, sources such as Kafka are also fault-tolerant and generate the original
    transactions data, given the appropriate offsets recovered by the planner. The
    state data is usually maintained in a versioned, key-value map in Spark workers
    and is backed by a WAL on HDFS. The planner ensures that the correct version of
    the state is used to re-execute the transactions subsequent to a failure. Additionally,
    the sinks are idempotent by design, and can handle the re-executions without double
    commits of the output. Hence, an overall combination of offset tracking in WAL,
    state management, and fault-tolerant sources and sinks provide the end-to-end
    exactly-once guarantees.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果规划器在当前的增量执行中失败，重新启动的规划器将从WAL中读取并重新执行所需的确切偏移范围。通常，诸如Kafka之类的源也是容错的，并且在规划器恢复的适当偏移量的情况下生成原始事务数据。状态数据通常在Spark工作节点中以版本化的键值映射形式进行维护，并由HDFS上的WAL支持。规划器确保使用正确的状态版本来重新执行故障后的事务。此外，接收器在设计上是幂等的，并且可以处理输出的重复执行而不会出现重复提交。因此，偏移跟踪在WAL中，状态管理以及容错源和接收器的整体组合提供了端到端的精确一次性保证。
- en: 'We can list the Physical Plan for our example of Structured Streaming using
    the `explain` method, as shown:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`explain`方法列出结构化流示例的物理计划，如下所示：
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](img/00027.jpeg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00027.jpeg)'
- en: We will explain the preceding output in more detail in [Chapter 11](part0197.html#5RRUQ0-e9cbc07f866e437b8aa14e841622275c), *Tuning
    Spark SQL Components for Performance*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第11章](part0197.html#5RRUQ0-e9cbc07f866e437b8aa14e841622275c)中更详细地解释上述输出，*调整Spark
    SQL组件以提高性能*。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced you to Spark SQL, SparkSession (primary entry
    point to Spark SQL), and Spark SQL interfaces (RDDs, DataFrames, and Dataset).
    We then described some of the internals of Spark SQL, including the Catalyst and
    Project Tungsten-based optimizations. Finally, we explored how to use Spark SQL
    in streaming applications and the concept of Structured Streaming. The primary
    goal of this chapter was to give you an overview of Spark SQL while getting you
    comfortable with the Spark environment through hands-on sessions (using public
    Datasets).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您介绍了Spark SQL、SparkSession（Spark SQL的主要入口点）和Spark SQL接口（RDD、DataFrames和Dataset）。然后，我们描述了Spark
    SQL的一些内部机制，包括基于Catalyst和Project Tungsten的优化。最后，我们探讨了如何在流应用程序中使用Spark SQL以及结构化流的概念。本章的主要目标是让您了解Spark
    SQL的概况，同时通过实际操作（使用公共数据集）让您熟悉Spark环境。
- en: In the next chapter, we will get into the details of using Spark SQL to explore
    structured and semi-structured data typical to big data applications.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将详细介绍如何使用Spark SQL来探索大数据应用程序中典型的结构化和半结构化数据。
