- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Mock Test 1
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟测试 1
- en: Questions
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Try your hand at these practice questions to test your knowledge of Apache
    Spark:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试回答这些问题以测试你对 Apache Spark 的了解：
- en: '**Question 1:**'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1**：'
- en: Which statement does not accurately describe a feature of the Spark driver?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个陈述没有准确地描述 Spark 驱动程序的功能？
- en: The Spark driver serves as the node where the main method of a Spark application
    runs to co-ordinate the application
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 驱动程序作为运行 Spark 应用程序主方法的节点，用于协调应用程序。
- en: The Spark driver can be horizontally scaled to enhance overall processing throughput
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 驱动程序可以水平扩展以提高整体处理吞吐量。
- en: The Spark driver houses the SparkContext object
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 驱动程序包含 SparkContext 对象。
- en: The Spark driver is tasked with scheduling the execution of data by using different
    worker nodes in cluster mode
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 驱动程序负责使用集群模式下的不同工作节点调度数据的执行。
- en: Optimal performance dictates that the Spark driver should be positioned as close
    as possible to worker nodes
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最佳性能要求 Spark 驱动程序应尽可能靠近工作节点。
- en: '**Question 2**:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2**：'
- en: Which of these statements accurately describes stages?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个陈述准确地描述了阶段？
- en: Tasks within a stage can be simultaneously executed by multiple machines
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阶段内的任务可以由多台机器同时执行。
- en: Various stages within a job can run concurrently
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作业中的各个阶段可以并发运行。
- en: Stages comprise one or more jobs
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阶段由一个或多个作业组成。
- en: Stages temporarily store transactions before committing them through actions
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阶段在提交之前暂时存储事务。
- en: '**Question 3:**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 3**：'
- en: Which of these statements accurately describes Spark’s cluster execution mode?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个陈述准确地描述了 Spark 的集群执行模式？
- en: Cluster mode runs executor processes on gateway nodes
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群模式在网关节点上运行执行器进程。
- en: Cluster mode involves the driver being hosted on a gateway machine
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群模式涉及驱动程序托管在网关机器上。
- en: In cluster mode, the Spark driver and the cluster manager are not co-located
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群模式下，Spark 驱动程序和集群管理器不是位于同一位置的。
- en: The driver in cluster mode is located on a worker node
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群模式下的驱动程序位于工作节点上。
- en: '**Question 4:**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 4**：'
- en: Which of these statements accurately describes Spark’s client execution mode?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个陈述准确地描述了 Spark 的客户端执行模式？
- en: Client mode runs executor processes on gateway nodes
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端模式在网关节点上运行执行器进程。
- en: In client mode, the driver is co-located with the executor
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在客户端模式下，驱动程序与执行器位于同一位置。
- en: In client mode, the Spark driver and the cluster manager are co-located
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在客户端模式下，Spark 驱动程序和集群管理器是位于同一位置的。
- en: In client mode, the driver is found on an edge node
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在客户端模式下，驱动程序位于边缘节点上。
- en: '**Question 5:**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 5**：'
- en: Which statement accurately describes Spark’s standalone deployment mode?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个陈述准确地描述了 Spark 的独立部署模式？
- en: Standalone mode utilizes only one executor per worker for each application
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 独立模式为每个应用程序在每个工作节点上使用一个执行器。
- en: In standalone mode, the driver is located on a worker node
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在独立模式下，驱动程序位于工作节点上。
- en: In standalone mode, the cluster does not need the driver
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在独立模式下，集群不需要驱动程序。
- en: In standalone mode, the driver is found on an edge node
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在独立模式下，驱动程序位于边缘节点上。
- en: '**Question 6**:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 6**：'
- en: What is a task in Spark?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中的任务是什么？
- en: The unit of work performed for each data partition within a task is slots
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个数据分区在任务中执行的工作单元是槽。
- en: Tasks are the second-smallest entity that can be executed within Spark
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务是 Spark 中可以执行的第二小实体。
- en: Tasks featuring wide dependencies can be combined into a single task
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有广泛依赖关系的任务可以合并为单个任务。
- en: A task is a single unit of work done by a partition within Spark
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务是 Spark 中分区执行的单个工作单元。
- en: '**Question 7**:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 7**：'
- en: Which of the following is the highest level in Spark’s execution hierarchy?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个是 Spark 执行层次结构中的最高级别？
- en: Job
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务
- en: Task
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务
- en: Executor
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器
- en: Stage
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阶段
- en: '**Question 8**:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 8**：'
- en: How can the concept of slots be accurately described in Spark’s context?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在 Spark 的上下文中准确描述槽的概念？
- en: The creation and termination of slots align with the workload of an executor
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 槽的创建和终止与执行器的工作负载相一致。
- en: Spark strategically stores data on disk across various slots to enhance I/O
    performance
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 通过在各个槽之间策略性地存储数据来增强 I/O 性能。
- en: Each slot is consistently confined to a solitary core
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个槽始终被限制在单个核心上。
- en: Slots enable the tasks to run in parallel
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 槽允许任务并行运行。
- en: '**Question 9**:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 9**：'
- en: What is the role of an executor in Spark?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中执行器的角色是什么？
- en: The executor’s role is to request the transformation of operations into DAG
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器的角色是将操作请求转换为 DAG。
- en: There can only be one executor within a Spark environment
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 环境中只能有一个执行器。
- en: The executor processes partitions in an optimized and distributed manner
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器以优化和分布式的方式处理分区
- en: The executor schedules queries for execution
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器安排查询以执行
- en: '**Question 10**:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 10**:'
- en: What is the role of shuffle in Spark?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Shuffle 在 Spark 中的作用是什么？
- en: Shuffle broadcasts variables to different partitions
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shuffle 将变量广播到不同的分区
- en: With shuffle, data is written to the disk
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 shuffle，数据会被写入磁盘
- en: The shuffle command transforms data in Spark
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shuffle 命令在 Spark 中转换数据
- en: Shuffles are a narrow transformation
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shuffle 是一种窄转换
- en: '**Question 11**:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 11**:'
- en: What is the role of actions in Spark?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Actions 在 Spark 中的作用是什么？
- en: Actions only read data from a disk
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Actions 只从磁盘读取数据
- en: Actions are used to modify existing RDDs
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Actions 用于修改现有的 RDD
- en: Actions trigger the execution of tasks
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Actions 触发任务的执行
- en: Actions are used to establish stage boundaries
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Actions 用于建立阶段边界
- en: '**Question 12**:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 12**:'
- en: Which of the following is one of the tasks of the cluster manager in Spark?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪项是 Spark 中集群管理器的一项任务？
- en: In the event of an executor failure, the cluster manager will collaborate with
    the driver to initiate a new executor
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行器失败的情况下，集群管理器将与驱动器协作以启动一个新的执行器
- en: The cluster manager can coalesce partitions to increase the speed of complex
    data processing
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群管理器可以将分区合并以增加复杂数据处理的速度
- en: The cluster manager collects runtime statistics of queries
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群管理器收集查询的运行时统计信息
- en: The cluster manager creates query plans
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群管理器创建查询计划
- en: '**Question 13**:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 13**:'
- en: Which of the following is one of the tasks of adaptive query execution in Spark?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪项是 Spark 中自适应查询执行的一项任务？
- en: Adaptive query execution can coalesce partitions to increase the speed of complex
    data processing
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自适应查询执行可以合并分区以增加复杂数据处理的速度
- en: In the event of an executor failure, the adaptive query execution feature will
    collaborate with the driver to initiate a new executor
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行器失败的情况下，自适应查询执行功能将与驱动器协作以启动一个新的执行器
- en: Adaptive query execution creates query plans
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自适应查询执行创建查询计划
- en: Adaptive query execution is responsible for spawning multiple executors to carry
    our tasks in Spark
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自适应查询执行负责在 Spark 中生成多个执行器以执行任务
- en: '**Question 14**:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 14**:'
- en: Which of the following operations is considered a transformation?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪项操作被认为是转换？
- en: '`df.select()`'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select()`'
- en: '`df.show()`'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.show()`'
- en: '`df.head()`'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.head()`'
- en: '`df.count()`'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.count()`'
- en: '**Question 15**:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 15**:'
- en: What is a feature of lazy evaluation in Spark?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中懒加载评估的一个特性是什么？
- en: Spark will fail a job only during execution but not during definition
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 只在执行期间失败作业，而不是在定义期间
- en: Spark will fail a job only during definition
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 只在定义期间失败作业
- en: Spark will execute upon receiving a transformation operation
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 在收到转换操作时会执行
- en: Spark will fail upon receiving an action
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 在收到操作时会失败
- en: '**Question 16**:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 16**:'
- en: Which of the following statements about Spark’s execution hierarchy is correct?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下关于 Spark 执行层次结构的哪个陈述是正确的？
- en: In Spark’s execution hierarchy, tasks are above the level of jobs
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 的执行层次结构中，任务位于作业之上
- en: In Spark’s execution hierarchy, multiple jobs are contained in a stage
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 的执行层次结构中，多个作业包含在一个阶段中
- en: In Spark’s execution hierarchy, a job can potentially span multiple stage boundaries
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 的执行层次结构中，一个作业可能跨越多个阶段边界
- en: In Spark’s execution hierarchy, slots are the smallest unit
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 的执行层次结构中，slot 是最小的单元
- en: '**Question 17**:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 17**:'
- en: Which of the following is the characteristic of the Spark driver?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪项是 Spark 驱动的特征？
- en: The worker nodes are responsible for transforming Spark operations into DAGs
    when the driver sends a command
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当驱动器发送命令时，工作节点负责将 Spark 操作转换为 DAG
- en: The Spark driver is responsible for executing tasks and returning results to
    executors
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 驱动负责执行任务并将结果返回给执行器
- en: Spark driver can be scaled by adding more machines so that the performance of
    Spark tasks can be improved
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 驱动可以通过添加更多机器来扩展，从而提高 Spark 任务的性能
- en: The Spark driver processes partitions in an optimized and distributed fashion
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 驱动以优化和分布式的方式处理分区
- en: '**Question 18**:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 18**:'
- en: Which of the following statements about broadcast variables is accurate?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下关于广播变量的哪个陈述是准确的？
- en: Broadcast variables are only present on driver nodes
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 广播变量仅存在于驱动节点上
- en: Broadcast variables can only be used for tables that fit into memory
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 广播变量只能用于适合内存的表
- en: Broadcast variables are not immutable, meaning they can be shared across clusters
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 广播变量不是不可变的，这意味着它们可以在集群之间共享
- en: Broadcast variables are not shared across the worker nodes
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 广播变量不会在工作节点之间共享
- en: '**Question 19**:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 19**:'
- en: Which of the following code blocks returns unique values in columns `employee_state`
    and `employee_salary` in DataFrame `df` for all columns?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回了 DataFrame `df` 中 `employee_state` 和 `employee_salary` 列的唯一值？
- en: '`Df.select(''employee_state'').join(df.select(''employee_salary''),` `col(''employee_state'')==col(''employee_salary''),
    ''left'').show()`'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Df.select(''employee_state'').join(df.select(''employee_salary''),` `col(''employee_state'')==col(''employee_salary''),
    ''left'').show()`'
- en: '`df.select(col(''employee_state''),` `col(''employee_salary'')).agg({''*'':
    ''count''}).show()`'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(col(''employee_state''),` `col(''employee_salary'')).agg({''*'':
    ''count''}).show()`'
- en: '`df.select(''employee_state'', ''employee_salary'').distinct().show()`'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(''employee_state'', ''employee_salary'').distinct().show()`'
- en: '`df.select(''employee_state'').union(df.select(''employee_salary'')).distinct().show()`'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(''employee_state'').union(df.select(''employee_salary'')).distinct().show()`'
- en: '**Question 20**:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 20**:'
- en: Which of the following code blocks reads a Parquet file from the `my_fle_path`
    location, where the file name is `my_file.parquet`, into a DataFrame `df`?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块从 `my_fle_path` 位置读取名为 `my_file.parquet` 的 Parquet 文件到 DataFrame `df`？
- en: '`df =` `spark.mode("parquet").read("my_fle_path/my_file.parquet")`'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df =` `spark.mode("parquet").read("my_fle_path/my_file.parquet")`'
- en: '`df =` `spark.read.path("my_fle_path/my_file.parquet")`'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df =` `spark.read.path("my_fle_path/my_file.parquet")`'
- en: '`df =` `spark.read().parquet("my_fle_path/my_file.parquet")`'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df =` `spark.read().parquet("my_fle_path/my_file.parquet")`'
- en: '`df =` `spark.read.parquet("/my_fle_path/my_file.parquet")`'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df =` `spark.read.parquet("/my_fle_path/my_file.parquet")`'
- en: '**Question 21**:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 21**:'
- en: Which of the following code blocks performs an inner join of the `salarydf`
    and `employeedf` DataFrames for columns `employeeSalaryID` and `employeeID`, respectively?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块对 `salarydf` 和 `employeedf` DataFrame 的 `employeeSalaryID` 和 `employeeID`
    列执行了内连接？
- en: '`salarydf.join(employeedf, salarydf.employeeID ==` `employeedf.employeeSalaryID)`'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salarydf.join(employeedf, salarydf.employeeID ==` `employeedf.employeeSalaryID)`'
- en: '`Salarydf.createOrReplaceTempView(salarydf)`'
  id: totrans-128
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Salarydf.createOrReplaceTempView(salarydf)`'
- en: '`employeedf.createOrReplaceTempView(''employeedf'')`'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`employeedf.createOrReplaceTempView(''employeedf'')`'
- en: '`spark.sql("SELECT * FROM salarydf CROSS JOIN employeedf ON` `employeeSalaryID
    ==employeeID")`'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.sql("SELECT * FROM salarydf CROSS JOIN employeedf ON` `employeeSalaryID
    ==employeeID")`'
- en: '`salarydf`'
  id: totrans-131
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salarydf`'
- en: '`.``join(employeedf, col(employeeID)==col(employeeSalaryID))`'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``join(employeedf, col(employeeID)==col(employeeSalaryID))`'
- en: '`Salarydf.createOrReplaceTempView(salarydf)`'
  id: totrans-133
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Salarydf.createOrReplaceTempView(salarydf)`'
- en: '`employeedf.createOrReplaceTempView(''employeedf'')`'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`employeedf.createOrReplaceTempView(''employeedf'')`'
- en: '`SELECT *` `FROM salarydf`'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SELECT *` `FROM salarydf`'
- en: '`INNER` `JOIN employeedf`'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`INNER` `JOIN employeedf`'
- en: '`ON salarydf.employeeSalaryID ==` `employeedf. employeeID`'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ON salarydf.employeeSalaryID ==` `employeedf. employeeID`'
- en: '**Question 22**:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 22**:'
- en: Which of the following code blocks returns the `df` DataFrame sorted in descending
    order by column salary, showing missing values in the end?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块按列 salary 降序排序返回 `df` DataFrame，并显示最后的缺失值？
- en: '`df.sort(nulls_last("salary"))`'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.sort(nulls_last("salary"))`'
- en: '`df.orderBy("salary").nulls_last()`'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.orderBy("salary").nulls_last()`'
- en: '`df.sort("salary", ascending=False)`'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.sort("salary", ascending=False)`'
- en: '`df.nulls_last("salary")`'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.nulls_last("salary")`'
- en: '**Question 23**:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 23**:'
- en: The following code block contains an error. The code block should return a copy
    of the `df` DataFrame, where the name of the column state is changed to `stateID`.
    Find the error.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。该代码块应该返回一个 `df` DataFrame 的副本，其中列名 `state` 被更改为 `stateID`。找出错误。
- en: 'Code block:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块：
- en: '[PRE0]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The arguments to the method `"stateID"` and `"state"` should be swapped
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法中的参数 `"stateID"` 和 `"state"` 应该交换
- en: The `withColumn` method should be replaced by the `withColumnRenamed` method
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该将 `withColumn` 方法替换为 `withColumnRenamed` 方法
- en: The `withColumn` method should be replaced by `withColumnRenamed` method, and
    the arguments to the method need to be reordered
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该将 `withColumn` 方法替换为 `withColumnRenamed` 方法，并且需要重新排序方法的参数
- en: There is no such method whereby the column name can be changed
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有这样的方法可以更改列名
- en: '**Question 24**:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 24**:'
- en: Which of the following code blocks performs an inner join between the `salarydf`
    and `employeedf` DataFrames, using the `employeeID` and `salaryEmployeeID` columns
    as join keys, respectively?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块在 `salarydf` 和 `employeedf` DataFrame 之间使用 `employeeID` 和 `salaryEmployeeID`
    列作为连接键执行了内连接？
- en: '`salarydf.join(employeedf, "inner", salarydf.employeedf ==` `employeeID.salaryEmployeeID)`'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salarydf.join(employeedf, "inner", salarydf.employeedf ==` `employeeID.salaryEmployeeID)`'
- en: '`salarydf.join(employeedf, employeeID ==` `salaryEmployeeID)`'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salarydf.join(employeedf, employeeID ==` `salaryEmployeeID)`'
- en: '`salarydf.join(employeedf, salarydf.salaryEmployeeID ==` `employeedf.employeeID,
    "inner")`'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salarydf.join(employeedf, salarydf.salaryEmployeeID ==` `employeedf.employeeID,
    "inner")`'
- en: '`salarydf.join(employeedf, salarydf.employeeID ==` `employeedf.salaryEmployeeID,
    "inner")`'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salarydf.join(employeedf, salarydf.employeeID ==` `employeedf.salaryEmployeeID`,
    "inner")`'
- en: '**Question 25**:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题25**:'
- en: 'The following code block should return a `df` DataFrame, where the `employeeID`
    column is converted into an integer. Choose the answer that correctly fills the
    blanks in the code block to accomplish this:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块应返回一个`df` DataFrame，其中`employeeID`列被转换为整数。请选择正确填充代码块空白的答案以完成此操作：
- en: '[PRE1]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`select`'
  id: totrans-161
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`select`'
- en: '`col("employeeID")`'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`col("employeeID")`'
- en: '`as`'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`as`'
- en: '`IntegerType`'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`IntegerType`'
- en: '`select`'
  id: totrans-165
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`select`'
- en: '`col("employeeID")`'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`col("employeeID")`'
- en: '`as`'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`as`'
- en: '`Integer`'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Integer`'
- en: '`cast`'
  id: totrans-169
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cast`'
- en: '`"``employeeID"`'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`"``employeeID"`'
- en: '`as`'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`as`'
- en: '`IntegerType()`'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`IntegerType()`'
- en: '`select`'
  id: totrans-173
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`select`'
- en: '`col("employeeID")`'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`col("employeeID")`'
- en: '`cast`'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cast`'
- en: '`IntegerType()`'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`IntegerType()`'
- en: '**Question 26**:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题26**:'
- en: Find the number of records that are not empty in the column department of the
    resulting DataFrame when we join the `employeedf` and `salarydf` DataFrames for
    the `employeeID` and `employeeSalaryID` columns, respectively. Which code blocks
    (in order) should be executed to achieve this?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 查找在将`employeedf`和`salarydf` DataFrames按`employeeID`和`employeeSalaryID`列分别连接后，结果DataFrame中列department不为空的记录数。以下哪些代码块（按顺序）应执行以实现此目的？
- en: 1\. `.filter(col("department").isNotNull())`
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 1. `.filter(col("department").isNotNull())`
- en: 2\. `.count()`
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 2. `.count()`
- en: 3\. `employeedf.join(salarydf, employeedf.employeeID ==` `salarydf.employeeSalaryID)`
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 3. `employeedf.join(salarydf, employeedf.employeeID ==` `salarydf.employeeSalaryID)`
- en: 4\. `employeedf.join(salarydf, employeedf.employeeID ==salarydf.` `employeeSalaryID,
    how='inner')`
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 4. `employeedf.join(salarydf, employeedf.employeeID ==salarydf.` `employeeSalaryID`,
    how='inner')`
- en: 5\. `.filter(col(department).isnotnull())`
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 5. `.filter(col(department).isnotnull())`
- en: 6\. `.sum(col(department))`
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 6. `.sum(col(department))`
- en: 3, 1, 6
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3, 1, 6
- en: 3, 1, 2
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3, 1, 2
- en: 4, 1, 2
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4, 1, 2
- en: 3, 5, 2
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3, 5, 2
- en: '**Question 27**:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题27**:'
- en: Which of the following code blocks returns only those rows from the `df` DataFrame
    in which the values in the column state are unique?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回了`df` DataFrame中列state值唯一的那些行？
- en: '`df.dropDuplicates(subset=["state"]).show()`'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.dropDuplicates(subset=["state"]).show()`'
- en: '`df.distinct(subset=["state"]).show()`'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.distinct(subset=["state"]).show()`'
- en: '`df.drop_duplicates(subset=["state"]).show()`'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.drop_duplicates(subset=["state"]).show()`'
- en: '`df.unique("state").show()`'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.unique("state").show()`'
- en: '**Question 28**:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题28**:'
- en: The following code block contains an error. The code block should return a copy
    of the `df` DataFrame with an additional column named `squared_number`, which
    has the square of the column number. Find the error.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。该代码块应返回一个包含额外列`squared_number`的`df` DataFrame副本，该列是列number的平方。请找出错误。
- en: 'Code block:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块：
- en: '[PRE2]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The arguments to the `withColumnRenamed` method need to be reordered
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`withColumnRenamed`方法的参数需要重新排序'
- en: The `withColumnRenamed` method should be replaced by the `withColumn` method
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应将`withColumnRenamed`方法替换为`withColumn`方法
- en: The `withColumnRenamed` method should be replaced by the `select` method, and
    `0.2` should be replaced with `2`
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应将`withColumnRenamed`方法替换为`select`方法，并将`0.2`替换为`2`
- en: The argument `0.2` should be replaced by `2`
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应将参数`0.2`替换为`2`
- en: '**Question 29**:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题29**:'
- en: Which of the following code blocks returns a new DataFrame in which column salary
    is renamed to `new_salary` and employee is renamed to `new_employee` in the `df`
    DataFrame?
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回了一个新的DataFrame，其中列salary被重命名为`new_salary`，employee被重命名为`new_employee`在`df`
    DataFrame中？
- en: '`df.withColumnRenamed(salary,` `new_salary).withColumnRenamed(employee, new_employee)`'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed(salary,` `new_salary).withColumnRenamed(employee, new_employee)`'
- en: '`df.withColumnRenamed("salary", "new_salary")`'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed("salary", "new_salary")`'
- en: '`df.withColumnRenamed("employee", "new_employee")`'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed("employee", "new_employee")`'
- en: '`df.withColumn("salary", "``new_salary").withColumn("employee", "new_employee")`'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("salary", "``new_salary").withColumn("employee", "new_employee")`'
- en: '`df.withColumnRenamed("salary", "``new_salary").withColumnRenamed("employee",
    "new_employee")`'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed("salary", "``new_salary").withColumnRenamed("employee",
    "new_employee")`'
- en: '**Question 30**:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题30**:'
- en: Which of the following code blocks returns a copy of the `df` DataFrame, where
    the column salary has been renamed to `employeeSalary`?
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回了一个`df` DataFrame的副本，其中列salary已被重命名为`employeeSalary`？
- en: '`df.withColumn(["salary", "employeeSalary"])`'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn(["salary", "employeeSalary"])`'
- en: '`df.withColumnRenamed("salary").alias("employeeSalary ")`'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed("salary").alias("employeeSalary ")`'
- en: '`df.withColumnRenamed("salary", "``employeeSalary ")`'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed("salary", "``employeeSalary ")`'
- en: '`df.withColumn("salary", "``employeeSalary ")`'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("salary", "``employeeSalary ")`'
- en: '**Question 31**:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 31**:'
- en: The following code block contains an error. The code block should save the `df`
    DataFrame to the `my_file_path` path as a Parquet file, appending to any existing
    parquet file. Find the error.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。代码块应该将 `df` DataFrame 保存到 `my_file_path` 路径作为 Parquet 文件，并追加到任何现有的
    Parquet 文件。找出错误。
- en: '[PRE3]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The code is not saved to the correct path
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码没有保存到正确的路径
- en: The `save()` and `format` functions should be swapped
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该交换 `save()` 和 `format` 函数
- en: The code block is missing a reference to the `DataFrameWriter`
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码块缺少对 `DataFrameWriter` 的引用
- en: The `option` mode should be overwritten to correctly write the file
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该覆盖 `option` 模式以正确写入文件
- en: '**Question 32**:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 32**:'
- en: How can we reduce the `df` DataFrame from 12 to 6 partitions?
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将 `df` DataFrame 从 12 个分区减少到 6 个分区？
- en: '`df.repartition(12)`'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.repartition(12)`'
- en: '`df.coalesce(6).shuffle()`'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.coalesce(6).shuffle()`'
- en: '`df.coalesce(6, shuffle=True)`'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.coalesce(6, shuffle=True)`'
- en: '`df.repartition(6)`'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.repartition(6)`'
- en: '**Question 33**:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 33**:'
- en: Which of the following code blocks returns a DataFrame where the timestamp column
    is converted into unix epoch timestamps in a new column named `record_timestamp`
    with a format of day, month, and year?
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回一个 DataFrame，其中时间戳列被转换为名为 `record_timestamp` 的新列，格式为日、月和年？
- en: '`df.withColumn("record_timestamp",` `from_unixtime(unix_timestamp(col("timestamp")),
    "dd-MM-yyyy"))`'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("record_timestamp", ` `from_unixtime(unix_timestamp(col("timestamp")),
    "dd-MM-yyyy"))`'
- en: '`df.withColumnRenamed("record_timestamp",` `from_unixtime(unix_timestamp(col("timestamp")),
    "dd-MM-yyyy"))`'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed("record_timestamp", ` `from_unixtime(unix_timestamp(col("timestamp")),
    "dd-MM-yyyy"))`'
- en: '`df.select ("record_timestamp",` `from_unixtime(unix_timestamp(col("timestamp")),
    "dd-MM-yyyy"))`'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select ("record_timestamp", ` `from_unixtime(unix_timestamp(col("timestamp")),
    "dd-MM-yyyy"))`'
- en: '`df.withColumn("record_timestamp",` `from_unixtime(unix_timestamp(col("timestamp")),
    "MM-dd-yyyy"))`'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("record_timestamp", ` `from_unixtime(unix_timestamp(col("timestamp")),
    "MM-dd-yyyy"))`'
- en: '**Question 34**:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 34**:'
- en: Which of the following code blocks creates a new DataFrame by appending the
    rows of the DataFrame `salaryDf` to the rows of the DataFrame `employeeDf`, regardless
    of the fact that both DataFrames have different column names?
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块通过将 DataFrame `salaryDf` 的行追加到 DataFrame `employeeDf` 的行来创建一个新的 DataFrame，而不考虑两个
    DataFrame 都有不同的列名？
- en: '`salaryDf.join(employeeDf)`'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.join(employeeDf)`'
- en: '`salaryDf.union(employeeDf)`'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.union(employeeDf)`'
- en: '`salaryDf.concat(employeeDf)`'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.concat(employeeDf)`'
- en: '`salaryDf.unionAll(employeeDf)`'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.unionAll(employeeDf)`'
- en: '**Question 35**:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 35**:'
- en: The following code block contains an error. The code block should calculate
    the total of all salaries in the `employee_salary` column across each department.
    Find the error.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。代码块应该计算每个部门 `employee_salary` 列中所有工资的总和。找出错误。
- en: '[PRE4]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Instead of `avg("value")`, `avg(col("value"))` should be used
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `avg(col("value"))` 而不是 `avg("value")`
- en: All column names should be wrapped in `col()` operators
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有列名都应该用 `col()` 运算符包裹
- en: '`"storeId"` and “`value"` should be swapped'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`"storeId"` 和 “`value"` 应该交换'
- en: '`Agg` should be replaced by `groupBy`'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Agg` 应该替换为 `groupBy`'
- en: '**Question 36**:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 36**:'
- en: The following code block contains an error. The code block is intended to perform
    a cross-join of the `salarydf` and `employeedf` DataFrames for the `employeeSalaryID`
    and `employeeID` columns, respectively. Find the error.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。代码块旨在对 `salarydf` 和 `employeedf` DataFrame 的 `employeeSalaryID`
    和 `employeeID` 列分别执行交叉连接。找出错误。
- en: '[PRE5]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The join type `"cross"` in the argument needs to be replaced with `crossJoin`
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数中的连接类型 `"cross"` 需要替换为 `crossJoin`
- en: '[`salarydf.employeeSalaryID, employeedf.employeeID`] should be replaced by
    `salarydf.employeeSalaryID ==` `employeedf.employeeID`'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salarydf.employeeSalaryID, employeedf.employeeID` 应替换为 `salarydf.employeeSalaryID
    ==` `employeedf.employeeID`'
- en: The `"cross"` argument should be eliminated since `"cross"` is the default join
    type
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该删除 `"cross"` 参数，因为 `"cross"` 是默认的连接类型
- en: The `"cross"` argument should be eliminated from the call and `join` should
    be replaced by `crossJoin`
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应从调用中删除 `"cross"` 参数，并用 `crossJoin` 替换 `join`
- en: '**Question 37**:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 37**:'
- en: The following code block contains an error. The code block should display the
    schema of the `df` DataFrame. Find the error.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。代码块应该显示 `df` DataFrame 的模式。找出错误。
- en: '[PRE6]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In Spark, we cannot print the schema of a DataFrame
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 中，我们无法打印 DataFrame 的模式
- en: '`printSchema` is not callable through `df.rdd` and should be called directly
    from `df`'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`printSchema` 不能通过 `df.rdd` 调用，而应该直接从 `df` 调用'
- en: There is no method in Spark named `printSchema()`
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 中没有名为 `printSchema()` 的方法
- en: The `print_schema()` method should be used instead of `printSchema()`
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `print_schema()` 方法而不是 `printSchema()`
- en: '**Question 38**:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 38**:'
- en: 'The following code block should write the `df` DataFrame as a Parquet file
    to the `filePath` path, replacing any existing file. Choose the answer that correctly
    fills the blanks in the code block to accomplish this:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块应该将 `df` DataFrame 写入到 `filePath` 路径的 Parquet 文件中，替换任何现有文件。选择正确填充代码块空白处的答案以完成此操作：
- en: '[PRE7]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`save`'
  id: totrans-265
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`save`'
- en: '`mode`'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`mode`'
- en: '`"``ignore"`'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`"``ignore"`'
- en: '`path`'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`path`'
- en: '`store`'
  id: totrans-269
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`store`'
- en: '`with`'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`with`'
- en: '`"``replace"`'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`"``replace"`'
- en: '`path`'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`path`'
- en: '`write`'
  id: totrans-273
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`write`'
- en: '`mode`'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`mode`'
- en: '`"``overwrite"`'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`"``overwrite"`'
- en: '`save`'
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`save`'
- en: '`save`'
  id: totrans-277
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`save`'
- en: '`mode`'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`mode`'
- en: '`"``overwrite"`'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`"``overwrite"`'
- en: '`path`'
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`path`'
- en: '**Question 39**:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 39**:'
- en: The following code block contains an error. The code block is supposed to sort
    the `df` DataFrame according to salary in descending order. Then, it should sort
    based on the bonus column, putting nulls to last. Find the error.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。代码块本应按薪资降序对 `df` DataFrame 进行排序。然后，它应该根据奖金列进行排序，将 `nulls` 放在最后。找出错误。
- en: '[PRE8]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `salary` column should be sorted in a descending way. Moreover, it should
    be wrapped in a `col()` operator
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该以降序对 `salary` 列进行排序。此外，它应该被包裹在 `col()` 操作符中
- en: The `salary` column should be wrapped by the `col()` operator
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该用 `col()` 操作符将 `salary` 列包裹起来
- en: The `bonus` column should be sorted in a descending way, putting `nulls` last
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该以降序对 `bonus` 列进行排序，将 `nulls` 放在最后
- en: The `bonus` column should be sorted by `desc_nulls_first()` instead
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `desc_nulls_first()` 对 `bonus` 列进行排序
- en: '**Question 40**:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 40**:'
- en: The following code block contains an error. The code block should use the `square_root_method`
    Python method to find the square root of the `salary` column in the `df` DataFrame
    and return it in a new column called `sqrt_salary`. Find the error.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。代码块应该使用 `square_root_method` Python 方法找到 `df` DataFrame 中 `salary`
    列的平方根，并在新列 `sqrt_salary` 中返回它。找出错误。
- en: '[PRE9]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There is no return type specified for `square_root_method`
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`square_root_method` 没有指定返回类型'
- en: In the second line of the code, Spark needs to call `squre_root_method_udf`
    instead of `square_root_method`
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二行代码中，Spark 需要调用 `squre_root_method_udf` 而不是 `square_root_method`
- en: '`udf` is not registered with Spark'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`udf` 未在 Spark 中注册'
- en: A new column needs to be added
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要添加一个新列
- en: '**Question 41**:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 41**:'
- en: The following code block contains an error. The code block should return the
    `df` DataFrame with `employeeID` renamed to `employeeIdColumn`. Find the error.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。代码块应该返回将 `employeeID` 重命名为 `employeeIdColumn` 的 `df` DataFrame。找出错误。
- en: '[PRE10]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Instead of `withColumn`, the `withColumnRenamed` method should be used
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `withColumnRenamed` 方法而不是 `withColumn`
- en: Instead of `withColumn`, the `withColumnRenamed` method should be used and argument
    `"employeeIdColumn"` should be swapped with argument `"employeeID"`
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `withColumnRenamed` 方法而不是 `withColumn`，并且参数 `"employeeIdColumn"` 应该与参数
    `"employeeID"` 交换
- en: Arguments `"employeeIdColumn"` and `"employeeID"` should be swapped
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数 `"employeeIdColumn"` 和 `"employeeID"` 应该交换
- en: The `withColumn` operator should be replaced with the `withColumnRenamed` operator
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该将 `withColumn` 操作符替换为 `withColumnRenamed` 操作符
- en: '**Question 42**:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 42**:'
- en: Which of the following code blocks will return a new DataFrame with the same
    columns as DataFrame `df`, except for the `salary` column?
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块会返回一个新的 DataFrame，其列与 DataFrame `df` 相同，除了 `salary` 列？
- en: '`df.drop("salary")`'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.drop("salary")`'
- en: '`df.drop(col(salary))`'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.drop(col(salary))`'
- en: '`df.drop(salary)`'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.drop(salary)`'
- en: '`df.delete("salary")`'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.delete("salary")`'
- en: '**Question 43**:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 43**:'
- en: Which of the following code blocks returns a DataFrame showing the mean of the
    salary column from the `df` DataFrame, grouped by column department?
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回一个 DataFrame，显示 `df` DataFrame 中 `salary` 列的平均值，按 `department` 列分组？
- en: '`df.groupBy("department").agg(avg("salary"))`'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy("department").agg(avg("salary"))`'
- en: '`df.groupBy(col(department).avg())`'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy(col(department).avg())`'
- en: '`df.groupBy("department").avg(col("salary"))`'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy("department").avg(col("salary"))`'
- en: '`df.groupBy("department").agg(average("salary"))`'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy("department").agg(average("salary"))`'
- en: '**Question 44**:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 44**:'
- en: Which of the following code blocks creates a DataFrame that shows the mean of
    the salary column of the `salaryDf` DataFrame, based on the department and state
    columns, where age is greater than 35?
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块创建了一个 DataFrame，显示基于部门和国家/地区列，年龄大于 35 的 `salaryDf` DataFrame 中 `salary`
    列的平均值？
- en: '`salaryDf.filter(col("age") >` `35)`'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.filter(col("age") >` `35)`'
- en: '`.``filter(col("employeeID")`'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``filter(col("employeeID")`'
- en: '`.``filter(col("employeeID").isNotNull())`'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``filter(col("employeeID").isNotNull())`'
- en: '`.``groupBy("department")`'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``groupBy("department")`'
- en: '`.``groupBy("department", "state")`'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``groupBy("department", "state")`'
- en: '`.``agg(avg("salary").alias("mean_salary"))`'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``agg(avg("salary").alias("mean_salary"))`'
- en: '`.``agg(average("salary").alias("mean_salary"))`'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``agg(average("salary").alias("mean_salary"))`'
- en: 1,2,5,6
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1,2,5,6
- en: 1,3,5,6
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1,3,5,6
- en: 1,3,6,7
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1,3,6,7
- en: 1,2,4,6
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1,2,4,6
- en: '**Question 45**:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 45**:'
- en: The following code block contains an error. The code block needs to cache the
    `df` DataFrame so that this DataFrame is fault-tolerant. Find the error.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。该代码块需要缓存`df` DataFrame，以便此DataFrame具有容错性。找出错误。
- en: '[PRE11]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`persist()` is not a function of the API DataFrame'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`persist()`不是DataFrame API的一个函数'
- en: '`df.write()` should be used in conjunction with `df.persist` to correctly write
    the DataFrame'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应将`df.write()`与`df.persist`结合使用以正确写入DataFrame
- en: The storage level is incorrect and should be `MEMORY_AND_DISK_2`
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储级别不正确，应为`MEMORY_AND_DISK_2`
- en: '`df.cache()` should be used instead of `df.persist()`'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应使用`df.cache()`而不是`df.persist()`
- en: '**Question 46**:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 46**:'
- en: Which of the following code blocks concatenates the rows of the `salaryDf` and
    `employeeDf` DataFrames without any duplicates (assuming the columns of both DataFrames
    are similar)?
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块在不重复的情况下连接了`salaryDf`和`employeeDf` DataFrame的行（假设两个DataFrame的列相似）？
- en: '`salaryDf.concat(employeeDf).unique()`'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.concat(employeeDf).unique()`'
- en: '`spark.union(salaryDf, employeeDf).distinct()`'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.union(salaryDf, employeeDf).distinct()`'
- en: '`salaryDf.union(employeeDf).unique()`'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.union(employeeDf).unique()`'
- en: '`salaryDf.union(employeeDf).distinct()`'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.union(employeeDf).distinct()`'
- en: '**Question 47**:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 47**:'
- en: Which of the following code blocks reads a complete folder of CSV files from
    `filePath` with column headers?
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块从`filePath`读取一个完整的CSV文件文件夹，包含列标题？
- en: '`spark.option("header",True).csv(filePath)`'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.option("header",True).csv(filePath)`'
- en: '`spark.read.load(filePath)`'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read.load(filePath)`'
- en: '`spark.read().option("header",True).load(filePath)`'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read().option("header",True).load(filePath)`'
- en: '`spark.read.format("csv").option("header",True).load(filePath)`'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read.format("csv").option("header",True).load(filePath)`'
- en: '**Question 48**:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 48**:'
- en: The following code block contains an error. The `df` DataFrame contains columns
    [`employeeID`, `salary`, and `department`]. The code block should return a DataFrame
    that contains only the `employeeID` and `salary` columns from DataFrame `df`.
    Find the error.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。`df` DataFrame包含列`[`employeeID`, `salary`, 和 `department`]。该代码块应返回一个仅包含`employeeID`和`salary`列的DataFrame。找出错误。
- en: '[PRE12]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: All column names from the `df` DataFrame should be specified in the `select`
    arguments
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应在`select`参数中指定`df` DataFrame的所有列名
- en: The `select` operator should be replaced by a `drop` operator, and all the column
    names from the `df` DataFrame should be listed as a list
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应将`select`运算符替换为`drop`运算符，并列出`df` DataFrame中的所有列名作为列表
- en: The `select` operator should be replaced by a `drop` operator
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应将`select`运算符替换为`drop`运算符
- en: The column name `department` should be listed like `col("department")`
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列名`department`应列出为`col("department")`
- en: '**Question 49**:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 49**:'
- en: The following code block contains an error. The code block should write DataFrame
    `df` as a Parquet file to the `filePath` location, after partitioning it for the
    `department` column. Find the error.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。该代码块应将DataFrame `df`作为Parquet文件写入到`filePath`位置，在按`department`列分区后。找出错误。
- en: '[PRE13]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`partitionBy()` method should be used instead of `partition()`.'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应使用`partitionBy()`方法而不是`partition()`方法。
- en: '`partitionBy()` method should be used instead of `partition()` and `filePath`
    should be added to the `parquet` method'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应使用`partitionBy()`方法而不是`partition()`，并将`filePath`添加到`parquet`方法中
- en: The `partition()` method should be called before the write method and `filePath`
    should be added to `parquet` method
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在写入方法之前应调用`partition()`方法，并将`filePath`添加到`parquet`方法中
- en: The `"department"` column should be wrapped in a `col()` operator
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应将`"department"`列用`col()`运算符包裹
- en: '**Question 50**:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 50**:'
- en: Which of the following code blocks removes the cached `df` DataFrame from memory
    and disk?
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块从内存和磁盘中移除了缓存的`df` DataFrame？
- en: '`df.unpersist()`'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.unpersist()`'
- en: '`drop df`'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`drop df`'
- en: '`df.clearCache()`'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.clearCache()`'
- en: '`df.persist()`'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.persist()`'
- en: '**Question 51**:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 51**:'
- en: 'The following code block should return a copy of the `df` DataFrame with an
    additional column: `test_column`, which has a value of `19`. Choose the answer
    that correctly fills the blanks in the code block to accomplish this:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块应该返回一个包含额外列：`test_column`，其值为`19`的`df` DataFrame的副本。请选择正确填充代码块空白处的答案以完成此操作：
- en: '[PRE14]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`withColumn`'
  id: totrans-369
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`withColumn`'
- en: '`''``test_column''`'
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`''``test_column''`'
- en: '`19`'
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`19`'
- en: '`withColumnRenamed`'
  id: totrans-372
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`withColumnRenamed`'
- en: '`test_column`'
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`test_column`'
- en: '`lit(19)`'
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`lit(19)`'
- en: '`withColumn`'
  id: totrans-375
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`withColumn`'
- en: '`''``test_column''`'
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`''test_column''`'
- en: '`lit(19)`'
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`lit(19)`'
- en: '`withColumnRenamed`'
  id: totrans-378
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`withColumnRenamed`'
- en: '`test_column`'
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`test_column`'
- en: '`19`'
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`19`'
- en: '**Question 52**:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 52**:'
- en: 'The following code block should return a DataFrame with the columns `employeeId`,
    `salary`, `bonus`, and `department` from `transactionsDf` DataFrame. Choose the
    answer that correctly fills the blanks to accomplish this:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块应该返回一个包含 `employeeId`、`salary`、`bonus` 和 `department` 列的 DataFrame，来自 `transactionsDf`
    DataFrame。选择正确填充空白的答案以完成此操作：
- en: '[PRE15]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`drop`'
  id: totrans-384
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`drop`'
- en: '`"employeeId", "salary", "``bonus", "department"`'
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`"employeeId", "salary", "bonus", "department"`'
- en: '`filter`'
  id: totrans-386
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`filter`'
- en: '`"employeeId, salary,` `bonus, department"`'
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`"employeeId, salary, bonus, department"`'
- en: '`select`'
  id: totrans-388
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`select`'
- en: '`["employeeId", "salary", "``bonus", "department"]`'
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`["employeeId", "salary", "bonus", "department"]`'
- en: '`select`'
  id: totrans-390
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`select`'
- en: '`col(["employeeId", "``salary", "bonus","department"])`'
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`col(["employeeId", "salary", "bonus", "department"])`'
- en: '**Question 53**:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 53**:'
- en: Which of the following code blocks returns a DataFrame with the `salary` column
    converted into a string in the `df` DataFrame?
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回了一个 DataFrame，其中 `salary` 列在 `df` DataFrame 中被转换为字符串？
- en: '`df.withColumn("salary",` `castString("salary", "string"))`'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("salary", `castString("salary", "string"))`'
- en: '`df.withColumn("salary", col("salary").cast("string"))`'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("salary", col("salary").cast("string"))`'
- en: '`df.select(cast("salary", "string"))`'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(cast("salary", "string"))`'
- en: '`df.withColumn("salary", col("salary").castString("string"))`'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("salary", col("salary").castString("string"))`'
- en: '**Question 54**:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 54**:'
- en: The following code block contains an error. The code block should combine data
    from DataFrames `salaryDf` and `employeeDf`, showing all rows of DataFrame `salaryDf`
    that have a matching value in column `employeeSalaryID` with a value in column
    `employeeID` of DataFrame `employeeDf`. Find the error.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含错误。该代码块应该结合来自 `salaryDf` 和 `employeeDf` DataFrame 的数据，显示 `salaryDf` DataFrame
    中所有与 `employeeDf` DataFrame 中 `employeeSalaryID` 列的值匹配的行。找出错误。
- en: '[PRE16]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `join` statement is missing the right-hand DataFrame, where the column name
    is `employeeSalaryID`
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`join` 语句缺少右侧的 DataFrame，其中列名为 `employeeSalaryID`'
- en: The `union` method should be used instead of `join`
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `union` 方法而不是 `join`
- en: Instead of `join`, `innerJoin` should have been used
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `innerJoin` 而不是 `join`
- en: '`salaryDf` should come in place of `employeeDf`'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf` 应该替换 `employeeDf`'
- en: '**Question 55**:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 55**:'
- en: Which of the following code blocks reads a JSON file stored at `my_file_path`
    as a DataFrame?
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块读取存储在 `my_file_path` 的 JSON 文件作为 DataFrame？
- en: '`spark.read.json(my_file_path)`'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read.json(my_file_path)`'
- en: '`spark.read(my_file_path, source="json")`'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read(my_file_path, source="json")`'
- en: '`spark.read.path(my_file_path)`'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read.path(my_file_path)`'
- en: '`spark.read().json(my_file_path)`'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read().json(my_file_path)`'
- en: '**Question 56**:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 56**:'
- en: The following code block contains an error. The code block should return a new
    DataFrame filtered by the rows where `salary` column is greater than 2000 in DataFrame
    `df`. Find the error.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含错误。该代码块应该返回一个新的 DataFrame，通过过滤 `df` DataFrame 中 `salary` 列大于 2000 的行。找出错误。
- en: '[PRE17]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Instead of `where()`, `filter()` should be used
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `filter()` 而不是 `where()`
- en: The argument to the `where` method should be `"col(salary) >` `2000"`
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`where` 方法的参数应该是 `"col(salary) >` `2000"`'
- en: Instead of `>=`, the operator `>` should be used
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `>` 操作符而不是 `>=`
- en: The argument to the `where` method should be `"salary >` `2000"`
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`where` 方法的参数应该是 `"salary >` `2000"`'
- en: '**Question 57**:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 57**:'
- en: Which of the following code blocks returns a DataFrame in which the `salary`
    and `state` columns are dropped from the `df` DataFrame?
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回了一个 DataFrame，其中从 `df` DataFrame 中删除了 `salary` 和 `state` 列？
- en: '`df.withColumn ("``salary", "state")`'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn ("salary", "state")`'
- en: '`df.drop(["salary", "state"])`'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.drop(["salary", "state"])`'
- en: '`df.drop("salary", "state")`'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.drop("salary", "state")`'
- en: '`df.withColumnRenamed ("``salary", "state")`'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed ("salary", "state")`'
- en: '**Question 58**:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 58**:'
- en: Which of the following code blocks returns a two-column DataFrame that contains
    counts of each department in the `df` DataFrame?
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回了一个包含 `df` DataFrame 中每个部门计数的两列 DataFrame？
- en: '`df.count("department").distinct()`'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.count("department").distinct()`'
- en: '`df.count("department")`'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.count("department")`'
- en: '`df.groupBy("department").count()`'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy("department").count()`'
- en: '`df.groupBy("department").agg(count("department"))`'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy("department").agg(count("department"))`'
- en: '**Question 59**:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 59**:'
- en: Which of the following code blocks prints the schema of a DataFrame and contains
    both column names and types?
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块打印了 DataFrame 的模式，并包含列名和类型？
- en: '`print(df.columns)`'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`print(df.columns)`'
- en: '`df.printSchema()`'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.printSchema()`'
- en: '`df.rdd.printSchema()`'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.rdd.printSchema()`'
- en: '`df.print_schema()`'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.print_schema()`'
- en: '**Question 60**:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 60**:'
- en: 'Which of the following code blocks creates a new DataFrame with three columns:
    `department`, `age`, and `max_salary` and has the maximum salary for each employee
    from each department and each age group from the `df` DataFrame?'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块创建了一个新的 DataFrame，包含三个列：`department`（部门），`age`（年龄）和`max_salary`（最高薪水），并且对于每个部门以及每个年龄组的每个员工都有最高的薪水？
- en: '`df.max(salary)`'
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.max(salary)`'
- en: '`df.groupBy(["department", "age"]).agg(max("salary").alias("max_salary"))`'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy(["department", "age"]).agg(max("salary").alias("max_salary"))`'
- en: '`df.agg(max(salary).alias(max_salary'')`'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.agg(max(salary).alias(max_salary'')`'
- en: '`df.groupby(department).agg(max(salary).alias(max_salary)`'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupby(department).agg(max(salary).alias(max_salary)`'
- en: Answers
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 答案
- en: B
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: A
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: D
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: D
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: A
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: D
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: A
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: D
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: C
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: B
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: C
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: A
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: C
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: B
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: B
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: D
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: D
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: D
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: C
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: C
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: D
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: D
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: C
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: A
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: C
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: E
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: E
- en: C
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: C
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: D
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: A
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: B
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: D
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: B
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: B
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: C
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: A
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: B
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: B
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: A
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: C
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: D
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: D
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: C
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: B
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: A
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: C
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: C
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: B
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: A
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: D
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: C
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: C
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: B
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: B
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
