- en: Machine Learning Foundations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习基础
- en: 'This chapter provides an introduction to the mathematical foundations behind
    healthcare analytics and machine learning. It is intended mainly for healthcare
    professionals with little background knowledge of the math required for doing
    healthcare analytics. By the end of the chapter, you will be familiar with the
    following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了医疗分析和机器学习背后的数学基础。该内容主要面向那些对进行医疗分析所需的数学知识了解较少的医疗专业人员。通过本章的学习，您将熟悉以下内容：
- en: Medical decision making paradigms
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医学决策制定范式
- en: The basic machine learning pipeline
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本的机器学习流程
- en: Model frameworks for medical decision making
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 医学决策制定的模型框架
- en: It is a poorly publicized fact that, in addition to the basic science courses
    and clinical rotations that they must do during their training, physicians also
    take courses in biostatistics and medical decision making. In these courses, prospective
    physicians learn some math and statistics that will help them as they sort through
    different symptoms, findings, and test results to arrive at diagnoses and treatment
    plans for their patients. Many physicians, already bombarded with endless medical
    facts and knowledge, shrug these courses off. Nevertheless, whether they learned
    it from these courses or from their own experiences, much of the reasoning that
    physicians use in their daily practice resembles the math behind some common machine
    learning algorithms. Let's explore that assertion a bit more in this section as
    we look at some popular frameworks for medical decision making and compare them
    to machine learning methods.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一个鲜为人知的事实是，除了必须完成的基础科学课程和临床轮转外，医生在培训期间还会学习生物统计学和医学决策制定课程。在这些课程中，未来的医生学习一些数学和统计学知识，帮助他们在整理不同的症状、体征和检查结果时做出诊断和治疗计划。许多医生已经被无尽的医学事实和知识淹没，他们对这些课程不以为意。然而，无论是通过这些课程还是通过自身的经验，医生在日常实践中使用的推理方法与一些常见的机器学习算法背后的数学原理非常相似。在这一部分中，我们将深入探讨这一论断，看看一些常见的医学决策制定框架，并将它们与机器学习方法进行比较。
- en: Tree-like reasoning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类树推理
- en: 'We are all familiar with tree-like reasoning; it involves branching into various
    possible actions as different decision points are met. Here we look at tree-like
    reasoning more closely and examine its machine learning counterparts: the decision
    tree and the random forest.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都熟悉类树推理；它涉及在遇到不同的决策点时分支到各种可能的行动。这里我们将更深入地看一下类树推理，并研究其机器学习对应物：决策树和随机森林。
- en: Categorical reasoning with algorithms and trees
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用算法和树进行分类推理
- en: In one medical decision making paradigm, the clinical problem can be approached
    as a **tree** or an **algorithm**. Here, an algorithm does not refer to a "machine
    learning algorithm" in the computer science sense; it can be thought of as a structured,
    ordered set of rules to reach a decision. In this type of reasoning, the root
    of the tree represents the initiation of the patient encounter. As the physician
    learns more information while asking questions, they come to various branch or
    decision points where the physician can proceed in more than one route. These
    routes represent different clinical tests or alternate lines of questioning. The
    physician will repeatedly make decisions and pick the next branch, reaching a
    terminal node at which there are no more branches. The terminal node represents
    a definitive diagnosis or a treatment plan.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在一种医学决策制定范式中，临床问题可以通过**树**或**算法**来处理。在这里，算法并不是指计算机科学中的“机器学习算法”；它可以被看作是一个有结构、有序的规则集合，用于做出决策。在这种推理方式中，树的根代表患者接诊的开始。当医生通过提问获取更多信息时，他们会到达不同的分支或决策点，医生可以选择不同的路径继续前进。这些路径代表不同的临床测试或替代的提问方向。医生会反复做出决策，并选择下一个分支，直到到达一个终端节点，此时没有更多的分支。终端节点代表明确的诊断或治疗计划。
- en: Here we have an example of a clinical management algorithm for weight and obesity
    management (National Heart, Lung, and Blood Institute, 2010). Each decision point
    (most of which are binary) is a diamond, while management plans are rectangles.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个关于体重和肥胖管理的临床管理算法示例（来源：国家心脏、肺和血液研究所，2010年）。每个决策点（其中大多数是二元的）用菱形表示，而管理计划则用矩形表示。
- en: 'For example, suppose we have a female patient with several clinical variables
    that are measured: BMI = 27, waist circumference = 90 cm, and the number of cardiac
    risk factors = 3\. Starting at node #1, we skip from Node #2 directly to Node
    #4, since the BMI > 25\. At Node #5, again the answer is "Yes." At Node #7, again
    the answer is "Yes," taking us to the management plan outlined in Node #8:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一位女性患者，测量了几项临床变量：BMI = 27，腰围 = 90厘米，心脏风险因素数 = 3。在节点#1开始，我们从节点#2直接跳到节点#4，因为BMI
    > 25。在节点#5时，答案再次是“是”。在节点#7时，答案依然是“是”，这将引导我们到节点#8中列出的管理计划：
- en: '![](img/d58ad818-5ab4-4107-b0f8-bd9a7e551a91.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d58ad818-5ab4-4107-b0f8-bd9a7e551a91.png)'
- en: A second example of an algorithm that combines both diagnosis and treatment
    is shown as follows (Haggstrom, 2014; Kirk et al., 2014). In this algorithm for
    the diagnosis/treatment of pregnancy of an unknown location, a hemodynamically
    stable patient with no pain (a patient with stable heart and blood vessel function)
    is routed to have serum hCG drawn at 0 and 48 hours after presenting to the physician.
    Depending on the results, several possible diagnoses are given, along with corresponding
    management plans.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是另一个结合了诊断和治疗的算法示例（Haggstrom, 2014; Kirk et al., 2014）。在这个关于未知位置妊娠的诊断/治疗算法中，对于一位没有疼痛的血流动力学稳定患者（即心血管功能稳定的患者），会在就医后0小时和48小时分别抽取血清hCG。根据结果，会给出几种可能的诊断，并相应提供管理计划。
- en: 'Note that in the clinical world, it is perfectly possible for these trees to
    be wrong; those cases are referred to as predictive errors. The goal in constructing
    any tree is to choose the best variables/cutpoints that minimize the error:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在临床中，这些树可能是错误的；这些情况被称为预测错误。构建任何树的目标是选择最佳的变量/切点，以最小化错误：
- en: '![](img/084c9ce1-2a8e-4e6c-be6b-802ca2c4f7de.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/084c9ce1-2a8e-4e6c-be6b-802ca2c4f7de.png)'
- en: Algorithms have a number of advantages. For one, they model human diagnostic
    reasoning as sequences of hierarchical decisions or determinations. Also, their
    goal is to eliminate uncertainty by forcing the caretaker to provide a binary
    answer at each decision point. Algorithms have been shown to improve standardization
    of care in medical practice and are in widespread use for many medical conditions
    today not only in outpatient/inpatient practice but also prior to hospital arrival
    by **emergency medical technicians** (**EMTs**).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 算法具有许多优点。首先，它们将人类诊断推理建模为一系列层次化的决策或判断。此外，它们的目标是通过强迫护理人员在每个决策点提供二元答案来消除不确定性。研究表明，算法可以改善医疗实践中的标准化护理，并且如今已广泛应用于许多医疗条件，不仅在门诊/住院治疗中，而且在**急救医疗技术员**（**EMTs**）到达医院之前也在使用。
- en: However, algorithms are often overly simplistic and don't consider the fact
    that medical symptoms, findings, or test results may not indicate 100% certainty.
    They are insufficient when multiple pieces of evidence must be weighed for arriving
    at a decision.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，算法往往过于简化，并未考虑到医学症状、检查结果或测试结果可能无法提供100%确定性的事实。当需要权衡多项证据以做出决策时，算法显得不足。
- en: Corresponding machine learning algorithms – decision tree and random forest
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相应的机器学习算法——决策树和随机森林
- en: 'In the preceding diagram, you may have noticed that the example tree most likely
    uses *subjectively* determined cutpoints in deciding which route to follow. For
    example, Diamond #5 uses a BMI cutoff of 25, and Diamond #7 uses a BMI cutoff
    of 30\. Nice, round numbers! In the decision analysis field, trees are usually
    constructed based on human inference and discussion. What if we could *objectively*
    determine the best variables to cut (and the corresponding cutpoints at which
    to cut) in order to minimize the error of the algorithm?'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示意图中，您可能已经注意到，示例树可能使用了*主观*确定的切点来决定应该走哪条路径。例如，钻石图标#5使用了BMI的25作为切点，而钻石图标#7使用了30的BMI切点——都是很漂亮的整数！在决策分析领域，树通常是基于人类推理和讨论构建的。如果我们能够*客观*地确定最佳的变量（以及应在哪些切点进行切割），以最小化算法的误差，该怎么办呢？
- en: This is just what we do when we train a formal **decision tree** using a machine
    learning algorithm. Decision trees evolved in the 1990s and used principles of
    information theory to optimize the branching variables/points of the tree to maximize
    the classification accuracy. The most common and simple algorithm for training
    a decision tree proceeds using what is known as a **greedy** approach. Starting
    at the first node, we take the training set of our data and **split** it based
    on each **variable**, using a variety of **cutpoints** for each variable. After
    each split, we calculate the entropy or information gain from the resulting split.
    Don't worry about the formulas for calculating these quantities, just know that
    they measure how much information is gained from the split, which correlates with
    how even the split is. For example, using the PUL algorithm shown previously,
    a split that results in eight normal intrauterine pregnancies and seven ectopic
    pregnancies would be favored over a split that results in 15 normal intrauterine
    pregnancies and zero ectopic pregnancies. Once we have the variable and cutpoint
    for the best split, we proceed and then repeat the method, using the remaining
    variables. To prevent **overfitting** the model to the training data, we stop
    splitting the tree when certain criteria are reached, or alternatively, we could
    train a big tree with many nodes and then remove (**prune**) some of the nodes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在使用机器学习算法训练正式**决策树**时所做的。决策树在1990年代发展起来，采用了信息理论的原则，优化了树的分支变量/节点，以最大化分类准确性。训练决策树的最常见且简单的算法采用了所谓的**贪心**方法。从第一个节点开始，我们基于每个**变量**使用不同的**切点**对数据的训练集进行**划分**。每次划分后，我们计算由划分所产生的熵或信息增益。无需担心如何计算这些量，只需知道它们衡量的是通过划分获得了多少信息，这与划分的均匀程度相关。例如，使用前面展示的PUL算法，一个结果为八个正常宫内妊娠和七个异位妊娠的划分，比一个结果为15个正常宫内妊娠和零个异位妊娠的划分更受青睐。一旦确定了最佳划分的变量和切点，我们就继续执行，并使用剩余的变量重复这一方法。为了防止模型对训练数据**过拟合**，当达到某些标准时，我们停止划分树，或者我们也可以训练一个包含多个节点的大树，然后去除（**剪枝**）一些节点。
- en: Decision trees have some limitations. For one thing, decision trees must split
    the decision space linearly at each step based on a single variable. Another problem
    is that decision trees are prone to overfitting. Because of these issues, decision
    trees typically aren't competitive with most state-of-the-art machine learning
    algorithms in terms of minimizing errors. However, the **random forest,** which
    is basically an ensemble of de-correlated decision trees, is currently among the
    most popular and accurate machine learning methods in medicine. We will make decision
    trees and random forests in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare* of this book.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有一些局限性。首先，决策树在每一步都必须基于单一变量线性地划分决策空间。另一个问题是决策树容易发生过拟合。由于这些问题，决策树通常在最小化误差方面与大多数最先进的机器学习算法竞争力较弱。然而，**随机森林**，它基本上是由去相关的决策树组成的集成方法，目前是医学领域中最流行和最准确的机器学习方法之一。我们将在本书的[第7章](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml)，*医疗领域的预测模型构建*中介绍决策树和随机森林。
- en: Probabilistic reasoning and Bayes theorem
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率推理和贝叶斯定理
- en: A second, more mathematical way of approaching the patient involves initializing
    the baseline probability of a disease for a patient and updating the probability
    of the disease with every new clinical finding discovered about the patient. The
    probability is updated using Bayes theorem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更为数学化的方式来接近患者是通过初始化患者的疾病基线概率，并根据每次新发现的临床信息更新该疾病的概率。这个概率是通过贝叶斯定理来更新的。
- en: Using Bayes theorem for calculating clinical probabilities
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理计算临床概率
- en: Briefly, Bayes theorem allows for the calculation of the post-test probability
    of a disease, given a pretest probability of disease, a test result, and the 2
    x 2 contingency table of the test. In this context, a "test" result does not have
    to be a lab test; it can be the presence or absence of any clinical finding as
    ascertained during the history and physical examination. For example, the presence
    of chest pain, whether the chest pain is substernal, the result of an exercise
    stress test, and the troponin result all qualify as clinical findings upon which
    post-test probabilities can be calculated. Although Bayes theorem can be extended
    to include continuously valued results, it is most convenient to binarize the
    test result before calculating the probabilities.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，贝叶斯定理允许根据疾病的预试概率、测试结果和测试的2 x 2列联表来计算疾病的后验概率。在这种背景下，“测试”结果不必是实验室测试；它可以是通过病史和体检确认的任何临床发现的有无。例如，胸痛的有无、胸痛是否位于胸骨后、运动压力测试的结果和肌钙蛋白的结果都可以作为临床发现，基于这些可以计算后验概率。尽管贝叶斯定理可以扩展到包括连续值结果，但在计算概率之前，将测试结果二值化通常更为方便。
- en: To illustrate the use of Bayes theorem, let's pretend you are a primary care
    physician and that a 55-year-old patient approaches you and says, "I’m having
    chest pain." When you hear the words "chest pain," the first life-threatening
    condition you are concerned about is a myocardial infarction. You can ask the
    question, "What is the likelihood that this patient is having a myocardial infarction?"
    In this case, the presence or absence of chest pain is the test (which is positive
    in this patient), and the presence or absence of myocardial infarction is what
    we're trying to calculate.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明贝叶斯定理的应用，假设你是一位初级保健医生，一位55岁的患者走进来并说：“我胸口疼。”当你听到“胸痛”这两个字时，你首先担心的致命疾病是心肌梗死。你可以问：“这个患者发生心肌梗死的可能性有多大？”在这种情况下，胸痛的有无就是测试（这个患者是阳性），而心肌梗死的有无是我们试图计算的内容。
- en: Calculating the baseline MI probability
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算基础的心肌梗死概率
- en: 'To calculate the probability that the chest-pain patient is having a **myocardial
    infarction** (**MI**), we must know three things:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算胸痛患者发生**心肌梗死**（**MI**）的概率，我们需要知道三件事：
- en: The pretest probability
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预试概率
- en: The 2 x 2 contingency table of the clinical finding for the disease in question
    (MI, in this case)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对该疾病的临床发现的2 x 2列联表（在此例中是心肌梗死）
- en: The result of this test (in this case, the patient is positive for chest pain)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该测试的结果（在本例中，患者有胸痛症状）
- en: Because the presence or absence of other findings is not yet known in the patient,
    we can take the pretest probability to be the baseline prevalence of MI in the
    population. Let's pretend that in your clinic's region, the baseline prevalence
    of MI in any given year is 5% for a 55-year-old person. Therefore, the pretest
    probability of MI in this patient is 5%. We will see later that the post-test
    probability of disease in this patient is the pretest probability multiplied by
    the likelihood ratio for positive chest pain (LR+). To get LR+, we need the 2
    x 2 contingency table.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因为患者的其他发现尚未明确，我们可以将预试概率设为该人群中心肌梗死的基础患病率。假设在你诊所所在地区，对于55岁的人群，每年心肌梗死的基础患病率为5%。因此，这位患者的心肌梗死预试概率为5%。我们稍后会看到，这位患者的后验疾病概率是预试概率与阳性胸痛似然比（LR+）的乘积。为了得到LR+，我们需要2
    x 2列联表。
- en: 2 x 2 contingency table for chest pain and myocardial infarction
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 胸痛与心肌梗死的2 x 2列联表
- en: 'Suppose the following table is the breakdown of chest pain and myocardial infarction
    in 400 patients who visited your clinic:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设以下表格是400位就诊患者中胸痛与心肌梗死的分布情况：
- en: '|  | **Myocardial Infarction present (D+)** | **Myocardial Infarction absent
    (D-)** | **Total** |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | **心肌梗死存在（D+）** | **心肌梗死不存在（D-）** | **总计** |'
- en: '| **Chest pain present (T+)** | 15 (TP) | 100 (FP) | 115 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **胸痛存在（T+）** | 15（TP） | 100（FP） | 115 |'
- en: '| **Chest pain absent (T-)** | 5 (FN) | 280 (TN) | 285 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **胸痛不存在（T-）** | 5（FN） | 280（TN） | 285 |'
- en: '| **Total** | 20 | 380 | 400 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **总计** | 20 | 380 | 400 |'
- en: Interpreting the contingency table and calculating sensitivity and specificity
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解读列联表并计算灵敏度和特异度
- en: In the preceding table, there are four numerical cells, labeled **TP**, **FP**,
    **FN**, and **TN**. These abbreviations stand for **true positives**, **false
    positives**, **false negatives**, and **true negatives**, respectively. The first
    word (true/false) indicates whether or not the test result matched the presence
    of disease as measured by the gold standard. The second word (positive/negative)
    indicates what the test result was. True positives and true negatives are desirable;
    this means that the test result is correct and the higher these numbers, the better
    the test is. On the other hand, false positives and false negatives are undesirable.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在上表中，有四个数字单元，分别标记为**TP**、**FP**、**FN**和**TN**。这些缩写分别代表**真阳性**、**假阳性**、**假阴性**和**真阴性**。第一个词（真/假）表示测试结果是否与通过金标准测量的疾病存在匹配。第二个词（阳性/阴性）表示测试结果是什么。真阳性和真阴性是期望的结果；这意味着测试结果是正确的，且这些数字越高，测试效果越好。另一方面，假阳性和假阴性是不可取的结果。
- en: 'Two important quantities that can be calculated from the true/false positives/negatives
    include the **sensitivity** and the **specificity**. The sensitivity is a measure
    of how powerful the test is in detecting disease. It is expressed as the ratio
    of positive test results over the number of total patients who had the disease:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从真阳性/假阳性/真阴性/假阴性中可以计算出两个重要的量，即**敏感性**和**特异性**。敏感性是衡量测试在检测疾病方面的能力。它表示阳性测试结果占患病总人数的比率：
- en: '![](img/f57c38f2-5d63-427c-bdfa-a4e23dcd1f97.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f57c38f2-5d63-427c-bdfa-a4e23dcd1f97.png)'
- en: 'On the other hand, the specificity is a measure of how good the test is at
    identifying patients who do not have the disease. It is expressed as the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，特异性是衡量测试识别没有疾病患者能力的指标。它的计算公式如下：
- en: '![](img/0a94bae4-db71-474c-b262-9b03ce14ca85.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a94bae4-db71-474c-b262-9b03ce14ca85.png)'
- en: These concepts can be confusing initially, so it may take some time and iterations
    before you get used to them, but the sensitivity and specificity are important
    concepts in biostatistics and machine learning.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念最初可能会让人感到困惑，因此在你习惯它们之前，可能需要一些时间和多次迭代，但敏感性和特异性是生物统计学和机器学习中的重要概念。
- en: Calculating likelihood ratios for chest pain (+ and -)
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算胸痛的似然比（+和-）
- en: 'The **likelihood ratio** is a measure of how much a test changes the likelihood
    of having a condition. It is often split into two quantities: the likelihood ratio
    of a positive test (LR+), and the likelihood ratio of a negative test (LR-).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**似然比**是衡量测试如何改变患病可能性的指标。它通常分为两个量：阳性测试的似然比（LR+）和阴性测试的似然比（LR-）。'
- en: 'The likelihood ratio for MI given a positive chest pain result is given by
    the following formulas:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 依据阳性胸痛结果，心肌梗死的似然比由以下公式给出：
- en: '![](img/d17412ae-1c9b-4868-81d8-271832e712c6.png)![](img/6f713f3f-8fd0-49d7-a536-d6ac8ad7ee18.png)![](img/51b1818a-f713-4af4-bada-fa2e065f26fb.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d17412ae-1c9b-4868-81d8-271832e712c6.png)![](img/6f713f3f-8fd0-49d7-a536-d6ac8ad7ee18.png)![](img/51b1818a-f713-4af4-bada-fa2e065f26fb.png)'
- en: 'The likelihood ratio for MI given a negative chest pain result would be given
    by the following formulas:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据阴性胸痛结果，心肌梗死的似然比由以下公式给出：
- en: '![](img/057e9976-eeea-424a-96a1-051227e920de.png)![](img/89c5d985-f456-4233-ab6c-a0d71a0391b4.png)![](img/1ddd20d5-f5d5-4aa0-878e-c86a2c91bcf9.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/057e9976-eeea-424a-96a1-051227e920de.png)![](img/89c5d985-f456-4233-ab6c-a0d71a0391b4.png)![](img/1ddd20d5-f5d5-4aa0-878e-c86a2c91bcf9.png)'
- en: 'Since the patient is positive for the presence of chest pain, only LR+ applies
    in this case. To get LR+, we use the appropriate numbers:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于患者有胸痛症状，因此在这种情况下仅适用LR+。为了得到LR+，我们使用适当的数字：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Calculating the post-test probability of MI given the presence of chest pain
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在已知胸痛症状的情况下，计算心肌梗死的后测概率
- en: 'Now that we have LR+, we multiply it by the pretest probability to get the
    post-test probability:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了LR+，我们将其乘以前测概率，以得到后测概率：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This approach for diagnosis and management of the patient seems very appealing;
    being able to calculate an exact probability of disease seemingly eliminates many
    issues in diagnosis! Unfortunately, Bayes theorem breaks down in clinical practice
    for many reasons. First, a large amount of data is required at every step to update
    the probability. No physician or database has access to all the contingency tables
    required to update the Bayes theorem with every historical element or lab test
    result discovered about the patient. Second, this method of probabilistic reasoning
    is unnatural for humans to perform. The other techniques discussed are much more
    conducive to a performance by the human brain. Third, while the model may work
    for single diseases, it doesn’t work well when there are multiple diseases and
    comorbidities. Finally, and most importantly, the assumptions of conditional independence
    and exhaustiveness and exclusiveness that are fundamental to the Bayes theorem
    don’t hold in the clinical world. The reality is that symptoms and findings are
    not completely independent of each other; the presence or absence of one finding
    can influence that of many others. Together, these facts render the probability
    calculated by the Bayes theorem to be inexact and even misleading in most cases,
    even when one succeeds in calculating it. Nevertheless, Bayes theorem is important
    in medicine for many subproblems when ample evidence is available (for example,
    using chest pain characteristics to calculate the probability of MI during the
    patient history).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种用于诊断和患者管理的方法看起来非常吸引人；能够计算出疾病的精确概率似乎消除了诊断中的许多问题！不幸的是，贝叶斯定理在临床实践中由于许多原因而无法应用。首先，每一步都需要大量的数据来更新概率。没有任何一位医生或数据库能访问到所有的应急表格，以便根据患者的每一个历史元素或实验室检查结果更新贝叶斯定理。其次，这种概率推理方法对于人类来说是不自然的。我们讨论的其他技术更有利于人类大脑的运作。第三，虽然该模型对于单一疾病有效，但当存在多种疾病和共病时，它的效果不好。最后，也是最重要的，贝叶斯定理所依赖的条件独立性和完备性、互斥性假设在临床世界中并不成立。现实情况是，症状和体征并非完全相互独立；某一症状的出现与否会影响其他许多症状的出现。综上所述，这些事实使得贝叶斯定理计算出的概率在大多数情况下是不准确的，甚至是误导性的，即使在成功计算后也是如此。尽管如此，贝叶斯定理在医学中对于许多子问题依然重要，特别是当有充足证据时（例如，使用胸痛特征来计算患者历史中的心肌梗死概率）。
- en: Corresponding machine learning algorithm – the Naive Bayes Classifier
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对应的机器学习算法——朴素贝叶斯分类器
- en: In the preceding example, we showed you how to calculate a post-test probability
    given a pretest probability, a likelihood, and a test result. The machine learning
    algorithm known as the Naive Bayes Classifier does this for every feature sequentially
    for a given observation. For example, in the preceding example, the post-test
    probability was 14.3%. Let's pretend that the patient now has a troponin drawn
    and it is elevated. 14.3% now becomes the pretest probability, and a new post-test
    probability is calculated based on the contingency table for troponin and MI,
    where the contingency tables are obtained from the training data. This is continued
    until all the features are exhausted. Again, the key assumption is that each feature
    is independent of all others. For the classifier, the category (outcome) having
    the highest post-test probability is assigned to the observation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们向您展示了如何根据预试验概率、似然性和测试结果计算后试验概率。被称为朴素贝叶斯分类器的机器学习算法会依次对给定观测值的每个特征进行此操作。例如，在前面的例子中，后试验概率是14.3%。假设患者现在进行了肌钙蛋白检查，并且结果升高了。14.3%现在成为预试验概率，并根据肌钙蛋白和心肌梗死的应急表格计算新的后试验概率，这些应急表格来自训练数据。这一过程会一直持续，直到所有特征都被耗尽。再次强调，关键假设是每个特征与其他所有特征独立。对于分类器，后试验概率最高的类别（结果）会被分配给该观测值。
- en: The Naive Bayes Classifier is popular for a select group of applications. Its
    advantages include high interpretability, robustness to missing data, and ease/speed
    for training and predicting. However, its assumptions make the model unable to
    compete with more state-of-the-art algorithms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器在特定应用领域中非常受欢迎。它的优点包括高可解释性、对缺失数据的鲁棒性以及训练和预测的简易性/快速性。然而，它的假设使得该模型无法与更先进的算法竞争。
- en: Criterion tables and the weighted sum approach
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别表和加权和方法
- en: The third medical decision making paradigm we will discuss is the criterion
    table and its similarity to linear and logistic regression.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第三种医学决策模式是判别表及其与线性回归和逻辑回归的相似性。
- en: Criterion tables
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评分表
- en: 'The use of criterion tables is partially motivated by an additional shortcoming
    of Bayes theorem: its sequential nature of considering each finding one at a time.
    Sometimes, it is more convenient to consider many factors simultaneously while
    considering diseases. What if we imagined the diagnosis of a certain disease as
    an additive sum of select factors? That is, in the MI example, the patient receives
    a point for having positive chest pain, a point for having a history of a positive
    stress test, and so on. We could establish a threshold for a point total that
    gives a positive diagnosis of MI. Because some factors are more important than
    others, we could use a weighted sum, in which each factor is multiplied by an
    importance factor before adding. For example, the presence of chest pain may be
    worth three points, and a history of a positive stress test may be worth five
    points. This is how criterion tables work.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用评分表的原因之一是贝叶斯定理的另一个缺点：考虑每次仅考虑一个发现的顺序性质。有时，同时考虑许多因素以及疾病的可能性更方便。如果我们把诊断某种疾病想象成选择性因素的加法和呢？也就是说，在心肌梗死的例子中，患者因有正性胸痛而获得一分，因有正性应力试验历史而获得一分，等等。我们可以建立一个给予正性心肌梗死诊断的总分阈值。因为有些因素比其他因素更重要，我们可以使用加权总和，其中每个因素在添加之前都乘以重要性因子。例如，胸痛的存在可能值得三分，而正性应力试验的历史可能值得五分。这就是评分表的工作方式。
- en: 'In the following table, we have given the modified wells criteria as an example.
    The modified wells criteria (derived from Clinical Prediction, 2017) are used
    to determine whether or not a patient may have a **pulmonary embolism** (**PE**):
    a blood clot in the lung that is life-threatening. Note that criterion tables
    not only provide point values for each relevant clinical finding but also give
    thresholds for interpreting the total score:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下表格中，我们以修改过的威尔斯评分为例。修改过的威尔斯评分（来源于临床预测，2017年）用于确定患者是否可能患有**肺栓塞**（PE）：肺部的血栓是一种危及生命的情况。请注意，评分表不仅为每个相关临床发现提供积分值，还给出了解释总分的阈值：
- en: '| **Clinical finding** | **Score** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **临床发现** | **评分** |'
- en: '| Clinical symptoms of deep vein thrombosis (leg swelling, pain with palpation)
    | 3.0 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 深静脉血栓形成的临床症状（下肢肿胀，压痛疼痛） | 3.0 |'
- en: '| Alternative diagnosis is less likely than pulmonary embolism | 3.0 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 替代诊断不太可能比肺栓塞 | 3.0 |'
- en: '| Heart rate > 100 beats per minute | 1.5 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 心率 > 100次/分钟 | 1.5 |'
- en: '| Immobilization for > 3 days or surgery in the previous 4 weeks | 1.5 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 长时间卧床超过3天或最近4周内手术 | 1.5 |'
- en: '| Previous diagnosis of deep vein thrombosis/pulmonary embolism | 1.5 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 先前诊断为深静脉血栓形成/肺栓塞 | 1.5 |'
- en: '| Hemoptysis | 1.0 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 咯血 | 1.0 |'
- en: '| Patient has cancer | 1.0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 患者患癌症 | 1.0 |'
- en: '|  |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '| **Risk stratification** |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| **风险分层** |  |'
- en: '| Low risk for PE | < 2.0 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 低风险的PE | < 2.0 |'
- en: '| Medium risk for PE | 2.0 - 6.0 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 中等风险的PE | 2.0 - 6.0 |'
- en: '| High risk for PE | > 6.0 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 高风险的PE | > 6.0 |'
- en: Corresponding machine learning algorithms – linear and logistic regression
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相应的机器学习算法 - 线性回归和逻辑回归
- en: Notice that a criterion table tends to use nice, whole numbers that are easy
    to add. Obviously, this is so the criteria are convenient for physicians to use
    while seeing patients. What would happen if we could somehow determine the optimal
    point values for each factor, as well as the optimal threshold? Remarkably, the
    machine learning method called logistic regression does just this.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，评分表倾向于使用易于添加的整数。显然，这样做是为了医生在看病人时使用标准方便。如果我们能够某种方式确定每个因素的最佳点值以及最佳阈值会发生什么？值得注意的是，被称为逻辑回归的机器学习方法正是如此。
- en: '**Logistic regression** is a popular statistical machine learning algorithm
    that is commonly used for binary classification tasks. It is a type of model known
    as a generalized linear model.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归**是一种流行的统计机器学习算法，通常用于二元分类任务。它是一种称为广义线性模型的模型类型。'
- en: 'To understand logistic regression, we must first understand **linear regression**.
    In linear regression, the *i*^(th) output variable (*y-hat*) is modeled as a weighted
    sum of the *p* individual predictor variables, *x[i]*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解逻辑回归，我们必须首先理解**线性回归**。在线性回归中，第*i*个输出变量(*y-hat*)被建模为*p*个个体预测变量*x[i]*的加权和：
- en: '![](img/98c7a9b8-9862-47b0-b8af-3b36482b8687.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98c7a9b8-9862-47b0-b8af-3b36482b8687.png)'
- en: 'The weights (beta) (also known as **coefficients**) of the variables can be
    determined by the following equation:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的权重（也称为**系数**）可以通过以下方程确定：
- en: '![](img/edd0183b-6d4d-44ad-9d3a-1a02b73cafe1.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/edd0183b-6d4d-44ad-9d3a-1a02b73cafe1.png)'
- en: Logistic regression is like linear regression, except that it applies a transformation
    to the output variable that limits its range to be between 0 and 1\. Therefore,
    it is well-suited to model probabilities of a positive response in classification
    tasks, since probabilities must also be between 0 and 1.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归就像线性回归，只不过它对输出变量进行了转换，将其范围限制在0和1之间。因此，它非常适合于分类任务中建模正响应的概率，因为概率也必须在0和1之间。
- en: Logistic regression has many practical advantages. First of all, it is an intuitively
    simple model that is easy to understand and explain. Understanding its mechanics
    does not require much advanced mathematics beyond high school statistics, and
    can easily be explained to both technical and nontechnical stakeholders on a project.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归有许多实际优势。首先，它是一个直观简单的模型，易于理解和解释。理解其机制并不需要超过高中统计学的高级数学知识，并且可以很容易地向项目中的技术和非技术相关人员进行解释。
- en: Second, logistic regression is not computationally intensive, in terms of time
    or memory. The coefficients are simply a collection of numbers that is as long
    as the list of predictors, and its determination only involves several matrix
    multiplications (see the preceding second equation for an example). One caveat
    to this is that the matrices may be quite large when dealing with very large datasets
    (for example, billions of data points), but this is true of most machine learning
    models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，从时间和内存的角度来看，逻辑回归并不计算密集。其系数仅仅是一个数字集合，长度与预测变量的数量相等，确定这些系数只涉及几次矩阵乘法（可以参考前面的第二个公式作为示例）。需要注意的是，当处理非常大的数据集时（例如，数十亿个数据点），矩阵可能会非常大，但这是大多数机器学习模型的共同特点。
- en: Third, logistic regression does not require much preprocessing (for example,
    centering or scaling) of the variables (although transformations that move predictors
    toward a normal distribution can increase performance). As long as the variables
    are in a numeric format, that is enough to get started with logistic regression.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，逻辑回归对变量的预处理要求较低（例如，居中或缩放）（尽管将预测变量转化为接近正态分布的形式可以提高性能）。只要变量是数值格式，就足以开始使用逻辑回归。
- en: Finally, logistic regression, especially when coupled with regularization techniques
    such as lasso regularization, can have reasonably strong performance in making
    predictions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，逻辑回归，尤其是结合了像套索正则化这样的正则化技术时，在进行预测时可以表现出相当强的性能。
- en: 'However, in today’s era of fast and powerful computing, logistic regression
    has largely been superseded by other algorithms that are more powerful, and typically
    more accurate. This is because logistic regression makes many major assumptions
    about the data and the modeling task:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在今天这个快速而强大的计算时代，逻辑回归已经在很大程度上被其他更强大且通常更准确的算法所取代。这是因为逻辑回归对数据和建模任务做出了许多重要的假设：
- en: It assumes that every predictor has a linear relationship with the outcome variable.
    This is obviously not the case in most datasets. In other words, logistic regression
    is not strong at modeling nonlinearities in the data.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它假设每个预测变量与结果变量之间具有线性关系。在大多数数据集中，这显然并非如此。换句话说，逻辑回归在建模数据的非线性方面并不擅长。
- en: It assumes that all of the predictors are independent of one another. Again,
    this is usually not the case, for example, two or more variables may interact
    to affect the prediction in a way that is more than just the linear sum of each
    variable. This can be partially remedied by adding products of predictors as interaction
    terms in the model, but choosing which interactions to model is not an easy task.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它假设所有的预测变量相互独立。再说一次，这通常并非如此，例如，两个或多个变量可能会相互作用，以一种超过各个变量线性求和的方式影响预测结果。通过将预测变量的乘积作为交互项添加到模型中，可以部分缓解这一问题，但选择哪些交互项来建模并不是一件简单的任务。
- en: It is highly and adversely sensitive to multiply correlated predictor variables.
    In the presence of such data, logistic regression may cause overfitting. To overcome
    this, there are variable selection methods, such as forward step-wise logistic
    regression, backward step-wise logistic regression, and best subset logistic regression,
    but these algorithms are imprecise and/or time-intensive.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对多重相关的预测变量非常敏感，并且这种敏感性往往会导致过拟合。在这种数据存在的情况下，逻辑回归可能会导致过拟合。为了解决这个问题，存在一些变量选择方法，比如前向逐步逻辑回归、后向逐步逻辑回归和最佳子集逻辑回归，但这些算法要么不精确，要么计算量大。
- en: Finally, logistic regression is not robust to missing data, like some classifiers
    are (for example, Naive Bayes).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，逻辑回归对缺失数据不具备鲁棒性，像某些分类器那样（例如，朴素贝叶斯）。
- en: Pattern association and neural networks
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模式关联和神经网络
- en: The last medical decision making framework strikes at the heart of our neurobiological
    understanding of how we process information and make decisions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的医学决策框架直接触及我们对神经生物学理解的核心，即我们如何处理信息和做出决策。
- en: Complex clinical reasoning
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复杂的临床推理
- en: Imagine that an elderly patient complaining of chest pain sees a highly experienced
    physician. Slowly, the clinician asks the appropriate questions and gets a representation
    of the patient as determined by the features of that patient's signs and symptoms.
    The patient says they have a history of high blood pressure but no other cardiac
    risk factors. The chest pain varies in intensity with the heartbeat (also known
    as pleuritic chest pain). The patient also reports they just came back to the
    United States from Europe. They also complain of swelling in the calf muscle.
    Slowly, the physician combines these lower level pieces of information (the absence
    of cardiac risk factors, the pleuritic chest pain, the prolonged period of immobility,
    a positive Homan's sign) and integrates it with memories of previous patients
    and the physician's own extensive knowledge to build a higher level view of this
    patient and realizes that the patient is having a pulmonary embolism. The physician
    orders a V/Q scan and proceeds to save the patient's life.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个抱怨胸痛的老年患者见到了一位经验丰富的医生。医生慢慢地提出了适当的问题，并根据患者的体征和症状特征得出患者的情况。患者说自己有高血压病史，但没有其他心脏风险因素。胸痛的强度随着心跳变化（也叫作胸膜性胸痛）。患者还报告说刚从欧洲回到美国。他们还抱怨小腿肌肉肿胀。医生慢慢地将这些较低层次的信息（缺乏心脏风险因素、胸膜性胸痛、长时间不活动、霍曼氏征阳性）与以前患者的记忆和医生自己丰富的知识相结合，建立了对患者的更高层次理解，意识到患者正在发生肺栓塞。医生安排了V/Q扫描，并采取措施挽救患者的生命。
- en: Such stories happen every day across the globe in medical clinics, hospitals,
    and emergency departments. Physicians use information from the patient history,
    exam, and test results to compose higher level understandings of their patients.
    How do they do it? The answer may lie in neural networks and deep learning.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的故事每天都在全球的诊所、医院和急诊科发生。医生通过病史、检查和测试结果的信息，构建对患者的更高层次理解。他们是如何做到的呢？答案可能在于神经网络和深度学习。
- en: Corresponding machine learning algorithm – neural networks and deep learning
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对应的机器学习算法——神经网络和深度学习
- en: How humans think and attain consciousness is certainly one of the universe's
    open questions. There is scarce knowledge on how human beings achieve rational
    thought or on how physicians make complex clinical decisions. However, perhaps
    the closest we have come to mimicking human brain performance in common cognitive
    tasks, as of this writing, is through neural networks and deep learning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 人类如何思考并获得意识无疑是宇宙中的未解之谜。关于人类如何实现理性思维，或者医生如何做出复杂临床决策的知识非常有限。然而，也许到目前为止，我们最接近模仿人类大脑在常见认知任务中的表现的技术，就是通过神经网络和深度学习。
- en: A **neural network** is modeled after the nervous system of mammals, in which
    predictor variables are connected to sequential layers of artificial "neurons”
    that aggregate and sum weighted inputs before sending their nonlinearly transformed
    outputs to the next layer. In this fashion, the data may pass through several
    layers before ultimately producing an outcome variable that indicates the likelihood
    of the target value is positive. The weights are usually trained by using the
    **backpropagation** technique, in which the negative difference between the correct
    output and predicted output is added to the weights at each iteration.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络**的模型灵感来源于哺乳动物的神经系统，其中预测变量连接到人工“神经元”的多个层次，这些神经元在发送经过非线性转换的输出到下一层之前，汇聚并加权输入数据。以这种方式，数据可能经过多个层次，最终产生一个结果变量，该变量表示目标值为正的可能性。权重通常通过**反向传播**技术训练，在每次迭代中，将正确输出与预测输出之间的负差值添加到权重中。'
- en: The neural network and the backpropagation technique was first reported in the
    1980s in a famous paper published by *Nature* journal, as was discussed in [Chapter
    1](b15b2b73-d2bb-410f-ab55-5f0f1e91730e.xhtml), *Introduction to Healthcare Analytics*
    (Rumelhart et al., 1986); in the 2010s, modern computing power along with vast
    amounts of data led to the rebranding of neural networks as "**deep learning**."
    Along with the increases in computing power and data availability, there have
    been state-of-the-art performance gains in machine learning tasks, such as speech
    recognition, image and object identification, and digit recognition.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和反向传播技术最早在1980年代通过《自然》杂志上发表的一篇著名论文中报告（如在[第一章](b15b2b73-d2bb-410f-ab55-5f0f1e91730e.xhtml)，*医疗保健分析导论*中讨论过，Rumelhart等，1986年）；进入2010年代后，现代计算能力与海量数据相结合，促使神经网络被重新命名为“**深度学习**”。随着计算能力的提升和数据的可获取性，机器学习任务在语音识别、图像和物体识别以及数字识别等方面取得了最先进的性能提升。
- en: The fundamental advantage of neural networks is that they are built to handle
    nonlinearities and complex interactions between predictor variables in the data.
    This is because each layer in a neural network is essentially performing a linear
    regression on the output of the previous layer, not simply on the input data itself.
    The more layers one has in a network, the more complex functions the network can
    model. The presence of nonlinear transformations in the neurons also contributes
    to this ability.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的根本优势在于它们能够处理数据中预测变量之间的非线性关系和复杂交互作用。这是因为神经网络中的每一层本质上都在对前一层的输出进行线性回归，而不仅仅是对输入数据本身进行回归。网络中的层数越多，网络能够建模的函数就越复杂。神经元中的非线性变换也有助于这一能力的实现。
- en: Neural networks also easily lend themselves to **multiclass problems**, in which
    there are more than two possible outcomes. Recognizing digits 0 through 9 is just
    one example of this.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络也很容易应用于**多类问题**，即有超过两个可能结果的情况。识别数字0到9就是其中的一个例子。
- en: Neural networks also have disadvantages. First of all, they have low interpretability
    and can be difficult to explain to nontechnical stakeholders on a project. Understanding
    neural networks requires knowledge of college-level calculus and linear algebra.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络也有其缺点。首先，它们的可解释性较差，且可能难以向项目中的非技术利益相关者解释。理解神经网络需要具备大学水平的微积分和线性代数知识。
- en: Second of all, neural networks can be difficult to tune. There are often many
    parameters involved (for example, how to initialize weights, the number, and size
    of hidden layers, what activation functions to use, connectivity patterns, regularization,
    and learning rates) and tuning all of them systematically is close to impossible.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，神经网络的调优可能会非常困难。通常涉及许多参数（例如，如何初始化权重、隐藏层的数量和大小、使用什么激活函数、连接模式、正则化以及学习率），并且系统地调节所有这些参数几乎是不可能的。
- en: Finally, neural networks are prone to overfitting. Overfitting is when the model
    has “memorized” the training data and cannot generalize well to previously unseen
    data. This can happen if there are too many parameters/layers and/or if the data
    is iterated over too many times.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，神经网络容易发生过拟合。过拟合是指模型“记住”了训练数据，无法很好地推广到以前未见过的数据。如果参数/层数过多和/或数据被迭代过多次，就可能发生这种情况。
- en: We will work with neural networks in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第七章](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml)，*医疗保健中的预测模型构建*中使用神经网络。
- en: Machine learning pipeline
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习管道
- en: In the last section, we spent a lot of time discussing machine learning models
    and how they correspond to frameworks for medical decision making. But how does
    one actually train a machine learning model? In healthcare, machine learning usually
    consists of a pattern of stereotyped tasks. We can refer to the collection of
    these tasks as a **pipeline**. While no two pipelines are exactly the same for
    any two machine learning applications, pipelines allow us to describe the machine
    learning process. In this section, we describe a generalized pipeline that many
    simple machine learning projects tend to follow, particularly when dealing with
    **structured data**, or data that can be organized into rows and columns.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们花了大量时间讨论了机器学习模型及其与医学决策框架的关系。但是，究竟如何训练一个机器学习模型呢？在医疗领域，机器学习通常由一系列典型的任务组成。我们可以将这些任务的集合称为**管道**。虽然每个机器学习应用的管道都不完全相同，但管道使我们能够描述机器学习的过程。在这一节中，我们描述了许多简单机器学习项目通常遵循的一个通用管道，特别是在处理**结构化数据**（即可以组织成行和列的数据）时。
- en: Loading the data
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: Before we can make computations on the data, it must be **loaded** from a storage
    location (usually a database or a real-time data feed) into a computing workspace.
    Workspaces allow the user to manipulate the data and build models using popular
    languages including R, Python, Hadoop, and Spark. Many commercial databases have
    specialized functionality in order to facilitate loading into workspaces. The
    machine learning languages themselves also have functions that read from text
    files and connect to and read from databases. Sometimes the user may also prefer
    to perform data quality control and cleansing directly in the database. This typically
    includes steps such as building a patient index, data normalization, and data
    cleaning. In [Chapter 4](e1b89921-e75b-4b16-a567-8970a173db53.xhtml), *Computing
    Foundations – Databases,* we discuss the manipulation of databases using the **Structured
    Query Language** (**SQL**) and in [Chapter 5](12ee77f2-0655-4dc5-abb1-2868d6fcc386.xhtml),
    *Computing Foundations – Introduction to Python,* we discuss methods for loading
    the data into a Python workspace.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对数据进行计算之前，数据必须从存储位置（通常是数据库或实时数据流）**加载**到计算工作区。工作区允许用户使用流行的语言（包括R、Python、Hadoop和Spark）来操作数据并构建模型。许多商业数据库具有专门的功能，方便将数据加载到工作区中。机器学习语言本身也有从文本文件中读取数据并连接和读取数据库的函数。有时，用户也可能更倾向于直接在数据库中进行数据质量控制和清理。这通常包括构建患者索引、数据标准化和数据清理等步骤。在[第4章](e1b89921-e75b-4b16-a567-8970a173db53.xhtml)，*计算基础——数据库*中，我们讨论了如何使用**结构化查询语言**（**SQL**）操作数据库，而在[第5章](12ee77f2-0655-4dc5-abb1-2868d6fcc386.xhtml)，*计算基础——Python简介*中，我们讨论了如何将数据加载到Python工作区的方法。
- en: Cleaning and preprocessing the data
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理和预处理数据
- en: There is a popular saying in data science that goes along the lines of, "For
    every 10 hours of a data scientist's time, 7 hours are spent cleaning the data."
    There are several subtasks that can be classified under **data cleansing**, and
    we will look at them now.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中有一句流行的说法，大致是：“每10个小时的数据科学家工作时间，其中7小时都在清理数据。”数据清理包含几个子任务，我们现在来看看这些任务。
- en: Aggregating data
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合数据
- en: Data is usually organized in a database as separate tables that may be bound
    together by common patient or encounter identifiers. Machine learning algorithms
    usually work on a single data structure at a time. Therefore, combining and merging
    the data from several tables into one final table is an important task. Along
    the way, you'll have to make some decisions as to which data to preserve (demographic
    data is usually indispensable) along with which data you can safely forget (the
    exact timestamps of anti-asthmatic medication administrations may not be important
    if you are trying to predict cancer onset, for example).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通常以单独的表格形式在数据库中组织，这些表格可能通过共同的患者或就诊标识符连接在一起。机器学习算法通常一次只处理一个数据结构。因此，将来自多个表格的数据合并到一个最终表格中是一个重要的任务。在这个过程中，你需要做出一些决定，保留哪些数据（人口统计数据通常是不可或缺的），以及可以安全删除哪些数据（例如，如果你想预测癌症的发生，抗哮喘药物的具体使用时间戳可能并不重要）。
- en: Parsing data
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析数据
- en: There are cases in which some or all of the data we need is in a condensed form.
    An example includes flat files of healthcare survey data in which each survey
    is encoded as an *N*-character string, with the characters at each position corresponding
    to specific survey responses. In these cases, the data we want must be broken
    down into its various components and converted into a useful format before we
    can use it. We refer to this activity as **parsing**. Even data that is expressed
    using particular medical coding systems may require some parsing.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有些情况下，我们需要的部分或全部数据是以紧凑的形式存在的。例如，健康调查数据的平面文件，每个调查被编码为一个*N*字符的字符串，每个位置的字符对应特定的调查响应。在这些情况下，我们需要将所需数据分解为其各个组成部分，并在使用之前转换为有用的格式。我们称这种活动为**解析**。即使使用特定的医疗编码系统表达的数据也可能需要一些解析。
- en: Converting types
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换类型
- en: If you are familiar at all with programming, you know that data can be stored
    as different **variable types**, ranging from simple integers to complex decimals
    to string (character) types. These types differ in terms of the operations that
    can be performed on them. For example, if the numbers 3 and 5 are stored as integer
    types, we can easily calculate 3+5= 8 using code. However, if they are stored
    as string types, adding "3" to "5" may yield an error, or it may yield "35," and
    this would cause all sorts of problems with our data, as you can imagine. Part
    of cleaning and inspecting the data is making sure every variable is stored as
    its proper type. Numerical data should correspond to numerical types, and most
    other data should correspond to string or categorical types.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对编程有所了解，你会知道数据可以存储为不同的**变量类型**，从简单的整数到复杂的小数再到字符串（字符）类型。这些类型在可以对它们执行的操作方面有所不同。例如，如果数字3和5存储为整数类型，我们可以轻松使用代码计算3+5=
    8。但是，如果它们存储为字符串类型，则将"3"加上"5"可能会导致错误，或者可能会得到"35"，这会导致我们的数据出现各种问题，可以想象。清理和检查数据的一部分工作是确保每个变量都存储为其适当的类型。数值数据应对应数值类型，而大多数其他数据应对应字符串或分类类型。
- en: In addition to the variable type, in many modeling languages, decisions must
    be made as to how to store data using more complex **data containers**, such as
    lists, vectors, and dataframes in R and lists, dictionaries, tuples, and dataframes
    in Python. Various importing and modeling functions may assume different choices
    of data structures, so once again, interconversion between data structures is
    usually necessary in order to achieve the desired result, and this is a crucial
    part of data cleansing. We will cover Python-related data structures in [Chapter
    5](12ee77f2-0655-4dc5-abb1-2868d6fcc386.xhtml), *Computing Foundations – Introduction
    to Python*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 除了变量类型外，在许多建模语言中，必须决定如何使用更复杂的**数据容器**存储数据，例如在R中的列表、向量和数据框架以及在Python中的列表、字典、元组和数据框架。各种导入和建模函数可能会假定不同的数据结构选择，因此，再次进行数据结构之间的相互转换通常是必要的，以实现期望的结果，这是数据清理的关键部分。我们将在[第5章](12ee77f2-0655-4dc5-abb1-2868d6fcc386.xhtml)，《计算基础
    - Python入门》中讨论与Python相关的数据结构。
- en: Dealing with missing data
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: 'Part of the reason why machine learning is so uniquely difficult in healthcare
    is its propensity for **missing data**. Inpatient hospital-data collection is
    often dependent on the nurses and other clinical staff to be completed thoroughly,
    and given how busy nurses and other clinical staff are, it''s no wonder that many
    inpatient datasets have certain features, such as urinary intake and output or
    timestamps of medication administrations, inconsistently reported. Another example
    is diagnosis codes: a patient may be eligible for a dozen medical diagnoses but
    in the interest of time, only five are entered into the chart by the outpatient
    physician. When details such as these are left out of our data, our models will
    be that much less accurate when applied to real patients.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习在医疗保健中如此独特困难的部分原因在于其缺失数据的倾向。住院数据收集通常依赖于护士和其他临床人员的全面完成，考虑到护士和其他临床人员的工作繁忙程度，难怪许多住院数据集有某些特征，如尿量和输出或药物管理时间戳，报告不一致。另一个例子是诊断编码：一个患者可能符合十几种医学诊断，但出于时间考虑，仅有五种被门诊医生输入到表中。当我们的数据中省略这些详细信息时，我们的模型在应用于实际患者时将会准确度大大降低。
- en: Even more problematic than the lack of detail is the effect of the missing data
    on our algorithms. Even one missing value in a dataframe that consists of thousands
    of patients and hundreds of features can prevent a model from running successfully.
    A quick fix might be simply to type in or impute a zero where the missing value
    is. But if the variable is a hemoglobin lab value, surely a hemoglobin of 0.0
    is impossible. Should we impute the missing data with the mean hemoglobin lab
    value instead? Do we use the overall mean or the gender-specific mean? Questions
    such as these are the reasons why dealing with missing data is practically a data
    science field in itself. The importance of having the basic awareness of missing
    data in your dataset cannot be overemphasized. In particular, it is important
    to know the difference between zero-valued data and missing data. Also, gaining
    some familiarity with concepts such as **zero**, **NaN** ("not a number"), **NULL**
    ("missing"), or **NA** ("not applicable") and how they are expressed in your languages
    of choice, whether SQL, Python, R, or some other language, is important.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 比缺乏细节更为严重的问题是缺失数据对我们算法的影响。即便是一个数据框中缺失的单一值——这个数据框包含了成千上万的患者和数百个特征——也可能导致模型无法成功运行。一个简单的解决办法可能就是在缺失值的位置输入或填充一个零。但如果这个变量是血红蛋白的实验室值，显然血红蛋白为0.0是不可能的。那么我们应该用平均血红蛋白值来填补缺失数据吗？我们应该使用整体平均值还是性别特定的平均值？类似的问题正是处理缺失数据几乎成了数据科学独立领域的原因。必须强调的是，基本了解数据集中的缺失数据至关重要。特别是，需要清楚地知道零值数据和缺失数据之间的区别。此外，了解像**零**、**NaN**（"不是一个数字"）、**NULL**（"缺失"）或**NA**（"不适用"）这样的概念，以及它们在你选择的编程语言中如何表示，不论是SQL、Python、R还是其他语言，都是非常重要的。
- en: The final goal of the data-cleansing stage is usually a single **data frame**,
    which is a single data structure that organizes the data into a matrix-like object
    of rows and columns, where rows comprise the individual events or observations
    and columns reflect different features of the observations using various data
    types. In an ideal world, all of the variables will have been inspected and converted
    to the appropriate type, and there would be no missing data. It should be noted
    that there may be some back-and-forth iterations between data cleansing, exploring,
    visualizing, and feature selection before reaching this final milestone. Data
    exploration/visualization and feature selection are the two pipeline steps that
    we'll discuss next.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗阶段的最终目标通常是一个单一的**数据框架**，它是一个组织数据的单一数据结构，将数据排列成类似矩阵的行列对象，其中行表示单个事件或观测，列反映观测的不同特征，并使用各种数据类型。在理想的情况下，所有变量都应该被检查并转换为适当的类型，而且应该没有缺失数据。需要注意的是，在达到最终目标之前，数据清洗、探索、可视化和特征选择可能会有多次往返迭代。数据探索/可视化和特征选择是我们接下来要讨论的两个步骤。
- en: Exploring and visualizing the data
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索和可视化数据
- en: To be done in close conjunction with parsing and cleaning the data, data exploration
    and visualization is an important part of the model-building process. This part
    of the pipeline is hard to define concretely–what exactly is one looking for when
    exploring the data? The underlying theory is that humans can do certain things
    much better than computers can–things such as making connections and identifying
    patterns. The more one looks at and analyzes the data, the more one will discover
    about how the variables are related and how they can be used to predict the target
    variable.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索与可视化是与数据解析和清洗紧密结合的，它是模型构建过程中非常重要的一部分。这个阶段很难明确定义——在探索数据时，我们到底在寻找什么？其背后的理论是，人类在某些方面比计算机做得更好——比如建立联系和识别模式。人们越是仔细查看和分析数据，就会越发现变量之间的关系以及如何利用这些变量预测目标变量。
- en: A popular exploratory activity in this step is to take a stock of all of the
    predictor variables; that is, their formats (for example, whether they are binary,
    categorical, or continuous) and how many missing values there are in each. For
    **binary variables**, it is helpful to count how many responses are positive and
    how many are negative; for **categorical variables**, it is helpful to count how
    many possible values each variable can take and the frequency histograms for each;
    and for **continuous variables**, calculating some measures of central tendency
    (for example, mean, median, mode) and dispersion (for example, standard deviation,
    percentiles) is a good idea.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤中一个常见的探索性活动是盘点所有预测变量；即，检查它们的格式（例如，是否是二元的、分类的或连续的）以及每个变量中缺失值的数量。对于**二元变量**，有助于统计正向响应和负向响应的数量；对于**分类变量**，有助于统计每个变量可以取的值的种类及其频率直方图；对于**连续变量**，计算一些集中趋势的度量（例如，均值、中位数、众数）和离散度的度量（例如，标准差、分位数）是一个不错的选择。
- en: Additional exploratory and visualization activities can be done to elucidate
    the relationships between selected predictor variables and the target variable.
    Specific plots vary depending on the formats (binary, categorical, continuous).
    For example, when both the predictor variable and target variable are continuous,
    a **scatterplot** is a popular visualization; to make a scatterplot the values
    of each variable are plotted on separate axes. If the predictor variable is continuous
    and the target variable is binary or categorical, a **dual overlapping frequency
    histogram** is a good tool, as is a **box-and-whisker plot**.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 可以进行额外的探索和可视化活动，以阐明所选预测变量与目标变量之间的关系。具体的图示依据格式（如二元、分类、连续）而有所不同。例如，当预测变量和目标变量都是连续时，**散点图**是一种流行的可视化方式；为了绘制散点图，需将每个变量的值绘制在不同的坐标轴上。如果预测变量是连续的，而目标变量是二元或分类的，**双重重叠频率直方图**是一个不错的工具，**箱线图**也很有用。
- en: In many cases, there are so many predictor variables that it becomes impossible
    to inspect manually and visualize each relationship. In these cases, automatic
    analyses, and calculating measures and statistics, such as correlation coefficients,
    become important.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，预测变量过多，以至于无法手动检查并可视化每个关系。在这些情况下，自动分析以及计算相关系数等度量和统计数据变得非常重要。
- en: Selecting features
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: When building models, more features are not always better. From an implementation
    perspective, a predictive pipeline modeling real-time clinical settings that interacts
    with multiple devices, health informatics systems, and source databases is more
    likely to fail than a simplified version with a minimal number of features. Specifically,
    while cleaning and exploring your data, you will find that not all of the features
    will be significantly related to the outcome variable.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型时，更多的特征并不总是更好。从实现的角度来看，实时临床环境中与多个设备、健康信息系统和源数据库交互的预测管道比起使用最小数量特征的简化版本，更可能失败。具体来说，在清理和探索数据时，你会发现并非所有的特征都与结果变量显著相关。
- en: Furthermore, many of the variables may be highly correlated with other variables
    and will offer little new information for making accurate predictions. Leaving
    these variables in your model could, in fact, reduce the accuracy of your model
    because they add random noise to the data. Therefore, a usual step in the machine
    learning pipeline is to perform **feature selection** and remove unwanted features
    from your data. The number and which variables to remove depends on many factors,
    including the choice of your machine learning algorithm and how interpretable
    you want the model to be.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多变量可能与其他变量高度相关，并且对于进行准确预测提供的信息不多。将这些变量保留在模型中，实际上可能会降低模型的准确性，因为它们会为数据引入随机噪声。因此，机器学习管道中的一个常见步骤是进行**特征选择**，并从数据中删除不需要的特征。删除的特征数量及其选择依赖于多个因素，包括所选择的机器学习算法以及模型的可解释性要求。
- en: There are many approaches to removing extraneous features from the final model.
    Iterative approaches, in which features are removed and the resulting model is
    built, evaluated, and compared to previous models, are popular because they allow
    one to measure how adjustments affect the performance of the model. Several algorithms
    for selecting features include **best subset selection** and forward and backward
    **step-wise regression**. There are also a variety of measures for feature importance,
    and these include the relative risk ratio, odds ratio, *p*-value significance,
    lasso regularization, correlation coefficient, and random forest out-of-bag error,
    and we will explore some of these measures in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以从最终模型中去除多余的特征。迭代方法中，特征被移除并建立结果模型，评估并与先前的模型进行比较是流行的，因为它们允许我们测量调整如何影响模型性能。选择特征的几种算法包括**最佳子集选择**、前向和后向**逐步回归**。还有许多特征重要性的度量，包括相对风险比、几率比、*p*-值显著性、套索正则化、相关系数和随机森林袋外错误，我们将在[第7章](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml)，*在医疗保健中制作预测模型*中探讨其中一些度量。
- en: Training the model parameters
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型参数
- en: Once we have our final data frame, we can think of the machine learning problem
    as minimizing an error function. All we are trying to do is make the best predictions
    on unseen patients/encounters; we are trying to minimize the difference between
    the predicted value and the observed value. For example, if we are trying to predict
    cancer onset, we want the predicted likelihood of cancer occurrence to be high
    in patients that developed cancer and low in patients that have not developed
    cancer. In machine learning, the difference between the predicted values and the
    observed values is known as an **error function** or **cost function**. Cost functions
    can take various forms, and machine learning practitioners often tinker with them
    while performing modeling. When minimizing the cost function, we need to know
    what weights we assign to certain features. In most cases, features that are more
    highly correlated to the outcome variable should be given more mathematical importance
    than features that are less highly correlated to the outcome variable. In a simplistic
    sense, we can refer to these "importance variables" as weights, or parameters.
    One of the major goals of supervised machine learning is all about finding that
    unique set of parameters or weights that minimizes our cost function. Almost every
    machine learning algorithm has its own way of assigning weights to different features.
    We will study this part of the pipeline in greater detail for the logistic regression,
    random forest, and neural network algorithms in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了最终的数据框架，我们可以将机器学习问题视为最小化误差函数。我们所试图做的就是在未见患者/接触的情况下做出最佳预测；我们试图最小化预测值与观察值之间的差异。例如，如果我们试图预测癌症发病，我们希望预测的癌症发生可能性在发展了癌症的患者中高，并且在未发展癌症的患者中低。在机器学习中，预测值与观察值之间的差异被称为**误差函数**或**成本函数**。成本函数可以采用各种形式，机器学习实践者通常在执行建模时调整它们。在最小化成本函数时，我们需要知道我们赋予某些特征的权重。在大多数情况下，与结果变量高度相关的特征应比与结果变量相关性较低的特征更重要。在简单的意义上，我们可以将这些“重要变量”称为权重或参数。监督机器学习的主要目标之一就是找到那组唯一的参数或权重，以最小化我们的成本函数。几乎每个机器学习算法都有自己分配权重给不同特征的方式。我们将在[第7章](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml)，*在医疗保健中制作预测模型*中更详细地研究这部分流程。
- en: Evaluating model performance
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型性能
- en: Finally, after building the model, it is important to evaluate its performance
    against the ground truth, so that we can adjust it if needed, compare different
    models, and report the results of our model to others. Methods for evaluating
    model performance depend on the structure of the target variable being predicted.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在构建模型之后，评估其性能对于地面真实情况至关重要，这样我们可以根据需要进行调整，比较不同的模型，并向他人报告我们模型的结果。评估模型性能的方法取决于被预测目标变量的结构。
- en: 'Often, the first step in evaluating a model is making a 2 x 2 contingency table,
    an example of which is shown as follows (Preventive Medicine, 2016). In a 2 x
    2 contingency table, all of the observations are split into four categories, which
    are further discussed in the following chart:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，评估模型的第一步是制作一个2 x 2列联表，以下是一个示例（《预防医学》，2016）。在2 x 2列联表中，所有观察值都被分为四类，具体内容将在下图中进一步讨论：
- en: '![](img/4d8cf94a-5970-48b9-be2f-ca1e51974339.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d8cf94a-5970-48b9-be2f-ca1e51974339.png)'
- en: 'For binary-valued target variables (for example, classification problems),
    there will be four types of observations:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二值目标变量（例如分类问题），将有四种类型的观察值：
- en: Those that had a positive outcome for which we predicted a positive outcome
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些是实际为阳性的结果，我们预测也是阳性结果
- en: Those that had a positive outcome for which we predicted a negative outcome
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些是实际为阳性的结果，但我们预测为阴性结果
- en: Those that had a negative outcome for which we predicted a positive outcome
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些是实际为阴性的结果，但我们预测为阳性结果
- en: Those that had a negative outcome for which we predicted a negative outcome
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些是实际为阴性的结果，我们预测也是阴性结果
- en: 'These four classes of observations are referred to respectively as:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这四类观察值分别称为：
- en: '**True positives** (**TP**)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性**（**TP**）'
- en: '**False negatives** (**FN**)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性**（**FN**）'
- en: '**False positives** (**FP**)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性**（**FP**）'
- en: '**True negatives** (**TN**)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性**（**TN**）'
- en: Various performance measures can then be calculated from these four quantities.
    We will cover the popular ones in the following sections.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以从这四个量中计算出各种性能衡量指标。我们将在以下部分介绍一些常见的指标。
- en: Sensitivity (Sn)
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 灵敏度（Sn）
- en: The **sensitivity**, also known as the **recall**, answers the question, "How
    effective is my model at incorrectly detecting observations that are positive
    for disease?"
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**灵敏度**，也称为**召回率**，回答了这个问题：“我的模型在错误地检测到病症阳性观察值方面有多有效？”'
- en: 'Its formula is given as the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 它的公式如下：
- en: '![](img/8e474c83-a92f-4d77-8186-7780c26623a4.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e474c83-a92f-4d77-8186-7780c26623a4.png)'
- en: Specificity (Sp)
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特异性（Sp）
- en: 'The **specificity** answers the question: "How effective is my model at incorrectly
    detecting observations that are negative for disease?"'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**特异性**回答了这个问题：“我的模型在错误地检测到病症阴性观察值方面有多有效？”'
- en: 'Its formula is given as the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 它的公式如下：
- en: '![](img/af2e3b6f-7279-4f3d-bf77-a7324ba933d1.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af2e3b6f-7279-4f3d-bf77-a7324ba933d1.png)'
- en: The sensitivity and specificity are complementary performance measures and are
    often reported together to measure the performance of a model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 灵敏度和特异性是互补的性能衡量标准，通常一起报告，用来衡量模型的性能。
- en: Positive predictive value (PPV)
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阳性预测值（PPV）
- en: 'The **positive predictive value** (**PPV**), also known as the **precision**,
    answers the question: "Given a positive prediction of my model, how likely is
    it to be correct?"'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**阳性预测值**（**PPV**），也称为**精确度**，回答了这个问题：“给定我模型的阳性预测，它正确的可能性有多大？”'
- en: 'Its formula is given as the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 它的公式如下：
- en: '![](img/e922c36c-f6e9-42fe-8e65-487f69e26c41.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e922c36c-f6e9-42fe-8e65-487f69e26c41.png)'
- en: Negative predictive value (NPV)
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阴性预测值（NPV）
- en: 'The **negative predictive value** (**NPV**) answers the question: "Given a
    negative prediction of my model, how likely is it to be correct?"'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**阴性预测值**（**NPV**）回答了这个问题：“给定我模型的阴性预测，它正确的可能性有多大？”'
- en: 'Its formula is given as the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 它的公式如下：
- en: '![](img/be226f63-bfec-4049-ab9a-a8c90dc8fa8e.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be226f63-bfec-4049-ab9a-a8c90dc8fa8e.png)'
- en: False-positive rate (FPR)
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假阳性率（FPR）
- en: 'The **false-positive rate** (**FPR**) answers the question: "Given a negative
    observation, what is the likelihood that my model will classify it as positive?"'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**假阳性率**（**FPR**）回答了这个问题：“给定一个阴性观察值，我的模型将其分类为阳性的可能性有多大？”'
- en: 'Its formula is given as the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 它的公式如下：
- en: '![](img/fdddf257-28aa-45c1-9a5b-b31df515e570.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fdddf257-28aa-45c1-9a5b-b31df515e570.png)'
- en: It is also equal to one minus the *specificity (1 - Sp)*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 它也等于*特异性（1 - Sp）*。
- en: Accuracy (Acc)
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确度（Acc）
- en: The **accuracy** (**Acc**) answers the question, "Given any observation, what
    is the likelihood that my model will classify it correctly?" It can be used as
    a standalone measure of model performance.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**准确度**（**Acc**）回答了这个问题：“给定任何观察值，我的模型正确分类它的可能性有多大？”它可以作为模型性能的独立衡量标准。'
- en: 'Its formula is the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 它的公式如下：
- en: '![](img/6dbbe2cb-7130-48ec-b7dd-feb3e67a6524.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6dbbe2cb-7130-48ec-b7dd-feb3e67a6524.png)'
- en: Receiver operating characteristic (ROC) curves
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 受试者工作特征（ROC）曲线
- en: When the target variable is binary, many machine learning algorithms will return
    the prediction for the observation in the form of a score that ranges from 0 to
    1\. Therefore, the positive or negative value of the prediction depends on where
    we set the threshold in that range. For example, if we build a model to predict
    cancer malignancy and determine that a particular patient's malignancy likelihood
    is 0.65, choosing a positive threshold of 0.60 makes a positive prediction for
    that patient, while choosing a threshold of 0.70 makes a negative prediction for
    that patient. All of the performance scores vary according to where we set the
    threshold. Certain thresholds will lead to better performance than others, depending
    on our goal for detection. For example, if we are interested in cancer detection,
    setting a threshold to a low value such as 0.05 will increase the sensitivity
    of our model, at the expense of the specificity, but this may be desired because
    we may not mind the false positives as long as we can identify every patient who
    is possibly at risk for cancer.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标变量为二值时，许多机器学习算法会以一个从0到1之间的分数形式返回观察值的预测结果。因此，预测的正负值取决于我们在该范围内设置的阈值。例如，如果我们建立一个模型来预测癌症恶性程度，并确定某个患者的恶性可能性为0.65，那么选择一个0.60的正阈值将对该患者做出正向预测，而选择0.70的阈值则会做出负向预测。所有的性能评分都取决于我们设置阈值的位置。某些阈值会比其他阈值带来更好的表现，这取决于我们检测的目标。例如，如果我们关注癌症检测，将阈值设置为0.05这样较低的值会提高模型的灵敏度，虽然这会以特异性为代价，但这可能是我们想要的，因为我们可能不介意假阳性，只要我们能够识别出所有可能面临癌症风险的患者。
- en: Perhaps the most common performance measurement paradigm for binary-valued outcome
    variables is to construct a **receiver operating characteristic** (**ROC**) **curve**.
    In this curve, we plot the values of two measures, the false-positive rate, and
    the sensitivity, as we vary the threshold from 0 to 1\. The sensitivity is usually
    inversely related to the false-positive rate, yielding a lowercase-r-shaped curve
    in most cases. The stronger the model, the higher the sensitivity and the lower
    the false positive rate will generally be, and the **area under the curve** (**AUC**)
    will tend to approach 1\. The AUC can, therefore, be used to compare models (for
    the same use case) while removing the dependency on the threshold's value.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最常见的二值结果变量的性能评估范式是构建**接收者操作特征**（**ROC**）**曲线**。在这条曲线中，我们绘制两个度量值的值：假阳性率和灵敏度，随着阈值从0变化到1。灵敏度通常与假阳性率成反比关系，导致大多数情况下出现一个小写的r形曲线。模型越强，灵敏度越高，假阳性率通常越低，**曲线下面积**（**AUC**）趋向于接近1。因此，AUC可以用来比较模型（在相同使用场景下），同时消除阈值值的依赖。
- en: 'The example ROC plot (Example ROC Curves, 2016) shown as follows has two ROC
    curves, a dark one and a light one. Because the red (dark) curve has a greater
    area under the curve than the lighter curve, the model measured by the dark curve
    can be seen as being better performing than that reflected by the lighter curve:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例ROC图（示例ROC曲线，2016年）显示了两条ROC曲线，一条较暗，一条较亮。由于红色（暗）曲线的曲线下面积大于较亮的曲线，因此可以认为由暗曲线衡量的模型性能优于由亮曲线反映的模型性能：
- en: '![](img/d70319db-e8f3-4126-9056-fb88d1f540c7.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d70319db-e8f3-4126-9056-fb88d1f540c7.png)'
- en: Precision-recall curves
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精度-召回曲线
- en: The **precision-recall curve** is an alternative to the ROC curve when the target
    variable is imbalanced (for example, when the positive-negative ratio is very
    low or very high). In healthcare, many use cases have a low positive-negative
    ratio, so you may see this curve often. It plots the positive predictive value
    of the sensitivity as the threshold varies from 0 to 1\. In most cases, this yields
    an uppercase-L-shaped curve.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**精度-召回曲线**是当目标变量不平衡（例如，当正负比例非常低或非常高）时，ROC曲线的替代方案。在医疗保健中，许多用例的正负比率较低，因此你可能会经常看到这条曲线。它绘制了随着阈值从0变化到1，灵敏度的正预测值。在大多数情况下，这会产生一个大写L形的曲线。'
- en: Continuously valued target variables
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续值目标变量
- en: 'For continuously valued target variables (for example, regression problems),
    there is no concept of true positives or false positives, so the previously discussed
    measures and curves cannot be calculated. Instead, the **residual sum of squares**
    (**RSS**) **error** is usually calculated: it is the sum of the squared distances
    between the actual values and the predicted values.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续值目标变量（例如回归问题），没有真正的正例或假正例的概念，因此无法计算之前讨论的度量和曲线。相反，通常会计算**残差平方和**（**RSS**）**误差**：它是实际值与预测值之间的平方距离之和。
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have toured some of the machine learning and mathematical
    foundations for performing healthcare analytics. In the next chapter, we'll continue
    exploring the foundational triumvirate of healthcare analytics by moving on to
    the computing leg.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们回顾了一些进行医疗健康分析的机器学习和数学基础。在下一章中，我们将继续探索医疗健康分析的基础三角形，接着讨论计算部分。
- en: References and further reading
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献和进一步阅读
- en: Clinical Prediction (2017). "Wells Clinical Prediction Rule for Pulmonary Embolism."[http://www.clinicalprediction.com/wells-score-for-pe/](http://www.clinicalprediction.com/wells-score-for-pe/).
    Accessed June 6, 2018.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 临床预测（2017）。“**Wells**肺栓塞临床预测规则”。[http://www.clinicalprediction.com/wells-score-for-pe/](http://www.clinicalprediction.com/wells-score-for-pe/)。访问日期：2018年6月6日。
- en: '"File: Example ROC curves.png." Wikimedia Commons, the free media repository.
    26 Nov 2016, 05:26 UTC. 11 Jul 2018, 01:53 [https://commons.wikimedia.org/w/index.php?title=File:Example_ROC_curves.png&oldid=219960771](https://commons.wikimedia.org/w/index.php?title=File:Example_ROC_curves.png&oldid=219960771).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: “文件：示例ROC曲线.png。”维基共享资源，免费媒体库。2016年11月26日，05:26 UTC。2018年7月11日，01:53 [https://commons.wikimedia.org/w/index.php?title=File:Example_ROC_curves.png&oldid=219960771](https://commons.wikimedia.org/w/index.php?title=File:Example_ROC_curves.png&oldid=219960771)。
- en: '"File: Preventive Medicine Statistics Sensitivity TPR, Specificity TNR, PPV,
    NPV, FDR, FOR, ACCuracy, Likelihood Ratio, Diagnostic Odds Ratio 2 Final.png."
    Wikimedia Commons, the free media repository. 26 Nov 2016, 04:26 UTC. 11 Jul 2018,
    01:42 [https://commons.wikimedia.org/w/index.php?title=File:Preventive_Medicine_Statistics_Sensitivity_TPR,_Specificity_TNR,_PPV,_NPV,_FDR,_FOR,_ACCuracy,_Likelihood_Ratio,_Diagnostic_Odds_Ratio_2_Final.png&oldid=219913391](https://commons.wikimedia.org/w/index.php?title=File:Preventive_Medicine_Statistics_Sensitivity_TPR,_Specificity_TNR,_PPV,_NPV,_FDR,_FOR,_ACCuracy,_Likelihood_Ratio,_Diagnostic_Odds_Ratio_2_Final.png&oldid=219913391).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: “文件：预防医学统计学 敏感性 TPR，特异性 TNR，PPV，NPV，FDR，FOR，准确率，似然比，诊断比率 2 Final.png。”维基共享资源，免费媒体库。2016年11月26日，04:26
    UTC。2018年7月11日，01:42 [https://commons.wikimedia.org/w/index.php?title=File:Preventive_Medicine_Statistics_Sensitivity_TPR,_Specificity_TNR,_PPV,_NPV,_FDR,_FOR,_ACCuracy,_Likelihood_Ratio,_Diagnostic_Odds_Ratio_2_Final.png&oldid=219913391](https://commons.wikimedia.org/w/index.php?title=File:Preventive_Medicine_Statistics_Sensitivity_TPR,_Specificity_TNR,_PPV,_NPV,_FDR,_FOR,_ACCuracy,_Likelihood_Ratio,_Diagnostic_Odds_Ratio_2_Final.png&oldid=219913391)。
- en: Häggström, Mikael (2014). "[Medical gallery of Mikael Häggström 2014](https://en.wikiversity.org/wiki/WikiJournal_of_Medicine/Medical_gallery_of_Mikael_H%C3%A4ggstr%C3%B6m_2014)".
    WikiJournal of Medicine 1 (2). [DOI](https://en.wikipedia.org/wiki/Digital_object_identifier):[10.15347/wjm/2014.008](https://doi.org/10.15347/wjm/2014.008).
    [ISSN](https://en.wikipedia.org/wiki/International_Standard_Serial_Number) [2002-4436](https://www.worldcat.org/issn/2002-4436).
    [Public Domain](https://creativecommons.org/publicdomain/zero/1.0/deed.en).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Häggström, Mikael（2014）。“[Mikael Häggström 医学画廊 2014](https://en.wikiversity.org/wiki/WikiJournal_of_Medicine/Medical_gallery_of_Mikael_H%C3%A4ggstr%C3%B6m_2014)”。《医学期刊》1（2）。[DOI](https://en.wikipedia.org/wiki/Digital_object_identifier):[10.15347/wjm/2014.008](https://doi.org/10.15347/wjm/2014.008)。[ISSN](https://en.wikipedia.org/wiki/International_Standard_Serial_Number)
    [2002-4436](https://www.worldcat.org/issn/2002-4436)。[公有领域](https://creativecommons.org/publicdomain/zero/1.0/deed.en)。
- en: 'James G, Witten D, Hastie T, Tibshirani R (2014). *An Introduction to Statistical
    Learning.* New York: Springer.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: James G, Witten D, Hastie T, Tibshirani R（2014）。*《统计学习导论》*。纽约：Springer。
- en: 'Kirk E, Bottomley C, Bourne T (2014). "Diagnosing ectopic pregnancy and current
    concepts in the management of pregnancy of unknown location". Hum. Reprod. Update
    20 (2): 250–61\. [DOI](https://en.wikipedia.org/wiki/Digital_object_identifier):[10.1093/humupd/dmt047](https://doi.org/10.1093/humupd/dmt047).
    [PMID](https://en.wikipedia.org/wiki/PMID)[24101604](https://www.ncbi.nlm.nih.gov/pubmed/24101604).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Kirk E, Bottomley C, Bourne T（2014）。“诊断异位妊娠及目前对不明位置妊娠的管理概念”。《人类生殖更新》20（2）：250–61。[DOI](https://en.wikipedia.org/wiki/Digital_object_identifier):[10.1093/humupd/dmt047](https://doi.org/10.1093/humupd/dmt047)。[PMID](https://en.wikipedia.org/wiki/PMID)[24101604](https://www.ncbi.nlm.nih.gov/pubmed/24101604)。
- en: 'Mark, DB (2005). "Decision-Making in Clinical Medicine." In Kasper DL, Braunwald
    E, Fauci AS, Hauser SL, Longo DL, Jameson JL. eds. *Harrison''s* *Principles of
    Internal Medicine*, 16e. New York, NY: McGraw-Hill.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Mark, DB（2005）。“临床医学中的决策制定。” 收录于Kasper DL、Braunwald E、Fauci AS、Hauser SL、Longo
    DL、Jameson JL 编著。*哈里森内科学原理*（第16版）。纽约，NY：McGraw-Hill。
- en: 'National Heart, Lung, and Blood Institute (2010). "Treatment Algorithm." *Guidelines
    on Overweight and Obesity: Electronic Textbook.* [https://www.nhlbi.nih.gov/health-pro/guidelines/current/obesity-guidelines/e_textbook/txgd/algorthm/algorthm.htm](https://www.nhlbi.nih.gov/health-pro/guidelines/current/obesity-guidelines/e_textbook/txgd/algorthm/algorthm.htm)
    . Accessed June 3, 2018.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 美国国家心脏、肺部和血液研究所（2010）。“治疗算法。” *超重和肥胖指南：电子教材*。[https://www.nhlbi.nih.gov/health-pro/guidelines/current/obesity-guidelines/e_textbook/txgd/algorthm/algorthm.htm](https://www.nhlbi.nih.gov/health-pro/guidelines/current/obesity-guidelines/e_textbook/txgd/algorthm/algorthm.htm)。访问日期：2018年6月3日。
- en: 'Rumelhart DE, Hinton GE, Williams RJ (1986). "Learning representations by backpropagating
    errors." *Nature *323(9): 533-536.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Rumelhart DE, Hinton GE, Williams RJ（1986）。“通过反向传播错误学习表示。” *自然* 323(9)：533-536。
