- en: Chapter 11. Introduction to Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 深度学习简介
- en: Innovators have always longed to make machines that can think. At the point
    when programmable PCs were first considered, individuals pondered whether they
    might get to be wise, over a hundred years before one was constructed (Lovelace
    in 1842).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 创新者一直渴望创造能够思考的机器。当可编程计算机首次被设想时，人们就已经在思考它们是否能够变得聪明，这比计算机的实际诞生早了一百多年（1842年由洛夫莱斯提出）。
- en: Today, **artificial intelligence** (**AI**) is a flourishing field with numerous
    reasonable applications and dynamic exploration points. We look to intelligent
    programming to automate routine work, process image and audio and extract meaning
    out of it, automate diagnoses of several diseases, and much more.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，**人工智能**（**AI**）是一个蓬勃发展的领域，拥有众多实际应用和充满活力的研究方向。我们期望智能程序能够自动化日常工作、处理图像和音频并从中提取意义、自动化多种疾病的诊断等。
- en: In the beginning, when artificial intelligence (AI) was picking up, the field
    handled and tackled issues that are mentally difficult for individuals, yet moderately
    straightforward for computers. These issues can be depicted by a rundown of formal,
    scientific principles. The genuine test for artificial intelligence turned out
    to be unraveling the undertakings that are simple for individuals to perform yet
    hard for individuals to depict formally. These issues we explain naturally, for
    example the ability of humans to understand speech (and sarcasm) and our ability
    to identify images, especially faces.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，随着人工智能（AI）的发展，该领域处理和解决了那些对人类来说在心理上较为困难，但对计算机来说却相对简单的问题。这些问题可以通过一套正式的、数学的原则来描述。人工智能的真正挑战变成了解决那些对人类来说容易执行，但对于计算机来说却很难正式描述的任务。这些任务我们通常是自然地解释的，例如人类理解语言（和讽刺）的能力，以及我们识别图像，尤其是面孔的能力。
- en: This arrangement is to permit computers to learn by gaining experience and to
    comprehend the world as far as a chain or a tree of facts, with every fact defined
    as far as its connection to more straightforward facts. By understanding these
    facts, this methodology maintains a strategic distance from the requirement for
    human administrators to formally indicate the greater part of the information
    that the computer needs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是让计算机通过积累经验进行学习，并通过一系列的事实链条或树状结构来理解世界，每个事实都通过与更简单事实的关联来定义。通过理解这些事实，这种方法避免了需要人工管理者正式指定计算机所需的所有信息。
- en: The progressive system of facts permits the computer to learn convoluted ideas
    by building them out of more straightforward ones. In the event that we draw a
    diagram indicating how these ideas are based on top of each other, the chart is
    profound, with numerous layers. Thus, we call this way to deal with AI deep learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 事实的渐进系统使计算机能够通过将复杂的概念构建为更简单的概念来学习复杂的思想。如果我们画出一个图表，表示这些概念是如何相互依赖的，那么这个图表将是深刻的，并且包含许多层次。因此，我们称这种方法为深度学习。
- en: 'A number of the early accomplishments of AI occurred in moderately sterile
    and formal situations and it was not necessary for computers to have much learning
    of the world. Let''s take an example:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的早期成就发生在相对封闭和正式的环境中，当时计算机并不需要太多关于世界的知识。让我们来看一个例子：
- en: IBM's Deep Blue chess-playing framework in 1997 defeated Mr. Gary Kasparov,
    the world champion at the time.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM的深蓝（Deep Blue）国际象棋框架在1997年击败了当时的世界冠军加里·卡斯帕罗夫（Gary Kasparov）。
- en: 'We should also consider these factors:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该考虑以下因素：
- en: Chess is obviously an extremely basic world.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国际象棋显然是一个极其简单的世界。
- en: It contains just 64 blocks and 32 elements that can only move in predefined
    ways.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它仅包含64个方块和32个元素，这些元素只能按照预定义的方式移动。
- en: Although conceiving a fruitful chess system is a huge achievement, the test
    is not due to the difficulty of describing the arrangement of chess elements and
    passable moves to the computer.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管构思一个成功的国际象棋系统是一项巨大的成就，但这个挑战并不在于如何将国际象棋元素的排列和可行的走法描述给计算机。
- en: Chess can be totally portrayed by an extremely short rundown of totally formal
    principles, effortlessly given earlier by the software engineer.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国际象棋可以完全通过一套极其简短的、完全正式的规则来描述，这些规则可以很容易地由程序员预先给出。
- en: 'Computers perform better than human beings in some of the tasks and worse in
    others:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机在某些任务上表现优于人类，而在其他任务上则表现较差：
- en: Abstract tasks that are among the most difficult mental endeavors for a person
    are among the simplest for a computer. Computers are much better suited for such
    tasks.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对人类来说，抽象任务是最具挑战性的心理工作之一，而对计算机来说却是最简单的。计算机更适合处理此类任务。
- en: An example of this is performing complex mathematical tasks.
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个例子是执行复杂的数学任务。
- en: Subjective and natural tasks are performed much better by the average human
    being than a computer.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主观和自然的任务由普通人比计算机更好地完成。
- en: A man's ordinary life requires a tremendous measure of information about the
    world.
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类的日常生活需要大量关于世界的信息。
- en: A lot of this learning is subjective and natural, and accordingly difficult
    to express formally.
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中很多知识是主观的和自然的，因此很难以正式的方式表达。
- en: Computers need to catch this same information so as to act in a wise way. One
    of the key difficulties in artificial intelligence is the means by which you get
    this casual learning onto a computer.
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机也需要捕捉这些信息，以便做出明智的决策。人工智能的一个关键挑战是如何将这种非正式的学习传递到计算机中。
- en: A few artificial intelligence ventures have looked to hard-code information
    about the world in formal dialects. A computer can reason about articulations
    in these formal dialects, consequently utilizing legitimate deduction rules. This
    is known as the information base way to deal with artificial intelligence. None
    of these activities have prompted a noteworthy success.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人工智能项目曾尝试在形式化语言中对世界的知识进行硬编码。计算机可以通过在这些形式化语言中进行推理，从而使用合乎逻辑的推理规则。这被称为基于知识库的人工智能方法。然而，这些尝试并未取得显著的成功。
- en: The difficulties confronted by frameworks depending on hard-coded information
    propose that AI frameworks require the capacity to obtain their own particular
    learning, by extracting patterns from crude information. This is known as machine
    learning, which we studied in previous chapters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖硬编码信息的系统所面临的挑战表明，人工智能系统需要能够获取自己的知识，通过从原始数据中提取模式。这就是我们在前几章中学习过的机器学习。
- en: The execution of these straightforward machine-learning calculations depends
    vigorously on the representation of the information they are given.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些简单的机器学习算法的性能在很大程度上依赖于它们所接收到的数据信息的表示。
- en: 'For instance, when logistic regression is utilized to suggest the future weather,
    the AI framework does not look at the patient straightforwardly:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当逻辑回归用于预测未来天气时，人工智能系统并不会直接考虑患者：
- en: The specialist tells the framework a few bits of important data, for example,
    the varying temperatures, wind direction and speed, humidity, and so on.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专家会向系统提供一些重要的信息，例如温度变化、风向和风速、湿度等。
- en: Every bit of the data incorporated into the representation of the weather is
    known as a feature. Logistic regression figures out how each of these features
    of the weather relates to different weather in different seasons or in other locations.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含在天气表示中的每一项数据都被称为特征。逻辑回归会了解这些天气特征如何与不同季节或其他地区的天气相关。
- en: In any case, it can't influence the way that the features are defined in any
    capacity.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，它无法以任何方式影响特征的定义。
- en: One answer for this issue is to utilize the machine, figuring out how to find
    the mapping from the representation to yield as well as the representation itself.
    This methodology is known as representation learning. Learned representations
    frequently bring about much-preferred execution over what can be acquired with
    hand-planned representations. They additionally permit AI frameworks to quickly
    adjust to new tasks, with negligible human intercession.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个方法是利用机器，找出从表示到结果的映射方式以及表示本身。这种方法被称为表示学习。学习到的表示通常能带来比手工设计的表示更优的性能。它们还使得人工智能系统能够快速适应新任务，且几乎不需要人类干预。
- en: A representation learning calculation can find a decent arrangement of features
    for a straightforward undertaking in minutes, or for a complex assignment in hours
    to months. Physically outlining highlights for a complex work require a lot of
    human time and effort, much more than for computers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一种表示学习算法可以在几分钟内为简单任务找到合适的特征集合，或为复杂任务找到特征集合，这可能需要数小时到数个月。为复杂任务手动设计特征需要大量的人力时间和精力，而计算机则大大减少了这一过程。
- en: 'In this chapter we will go through multiple topics, starting with the basic
    introduction:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涉及多个主题，首先是基本介绍：
- en: Basic foundations
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础知识
- en: Differences between machine learning and deep learning
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习与深度学习的区别
- en: What is deep learning?
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是深度学习？
- en: Deep feed-forward networks
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度前馈网络
- en: Single and multi-layer neural networks
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单层和多层神经网络
- en: Convolution networks
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积网络
- en: Practical methodology and applications
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实用的方法论与应用
- en: Revisiting linear algebra
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重温线性代数
- en: Linear algebra is a widely used branch of mathematics. Linear algebra is a part
    of discrete mathematics and not of continuous mathematics. A good understanding
    is needed to understand the machine learning and deep learning models. We will
    only revise the mathematical objects.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数是数学中广泛使用的一个分支。线性代数是离散数学的一部分，而不是连续数学的一部分。要理解机器学习和深度学习模型，需要有良好的基础理解。我们只会复习数学对象。
- en: A gist of scalars
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标量的要点
- en: A scalar is just a single number (as opposed to a large portion of alternate
    objects examined in linear algebra, which are generally arrays of various numbers).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 标量仅是一个单独的数字（与线性代数中讨论的大多数对象不同，后者通常是不同数字的数组）。
- en: A brief outline of vectors
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量的简要概述
- en: 'A vector is an organized collection or an array of numbers. We can recognize
    every individual number by its index in that list. For example:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 向量是一个有序的数字集合或数组。我们可以通过该列表中的索引来识别每个单独的数字。例如：
- en: '*x = [x1, x2, x3, x4 ..... xn]*'
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*x = [x1, x2, x3, x4 ..... xn]*'
- en: Vectors can also be thought of as identifying points in space.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量也可以被看作是空间中点的标识。
- en: Each element represents the value of coordinate along a different axis.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个元素代表沿不同轴的坐标值。
- en: We can also index the positions of these values in the vector. Therefore, it
    makes it easier to access the specific value of the array.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以对这些值在向量中的位置进行索引。因此，更容易访问数组的特定值。
- en: The importance of matrices
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵的重要性
- en: A matrix is a two-dimensional array of numbers.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵是一个二维的数字数组。
- en: Every component is identified by two indexes rather than only one.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个元素由两个索引标识，而不仅仅是一个。
- en: For example, a point in 2D space may be identified as (3,4). It means that the
    point is 3 points on the *x* axis and 4 points on the *y* axis.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，二维空间中的一个点可以表示为(3,4)。这意味着该点在 *x* 轴上是3个单位，在 *y* 轴上是4个单位。
- en: We can also have arrays of such numbers as[(3,4), (2,4), (1,0)]. Such an array
    is called a matrix.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们也可以拥有类似[(3,4), (2,4), (1,0)]的数字数组。这样的数组称为矩阵。
- en: What are tensors?
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是张量？
- en: If more than two-dimensions are needed (matrix) then we use tensors.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要多于两维（矩阵），我们则使用张量。
- en: This is an array of numbers without a defined number of axes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个没有定义轴数的数字数组。
- en: 'Such objects have a structure as follows: *T (x, y, z)*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对象的结构如下：*T (x, y, z)*
- en: '*[(1,3,5), (11,12,23), (34,32,1)]*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*[(1,3,5), (11,12,23), (34,32,1)]*'
- en: Probability and information theory
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率与信息理论
- en: Probability theory is a scientific system for speaking to questionable explanations.
    It gives a method for evaluating instability and adages for inferring new indeterminate
    statements.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 概率理论是一种用于表示不确定性命题的科学体系。它提供了一种评估不确定性的方法，并为推导新的不确定性陈述提供了准则。
- en: 'In applications of AI, we utilize probability theory as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI应用中，我们使用概率理论的方式如下：
- en: The laws of probability define how AI frameworks ought to reason, so algorithms
    are designed to figure or approximate different expressions inferred on utilizing
    probability theory
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率法则定义了AI系统应如何推理，因此算法被设计用来计算或近似基于概率理论推导出的不同表达式。
- en: Probability and statistics can be utilized to hypothetically investigate the
    behavior of proposed AI frameworks
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率和统计可以用来假设性地分析提议的AI系统的行为。
- en: While probability theory permits us to put forth indeterminate expressions and
    reason within the sight of uncertainty, data permits us to measure the degree
    of uncertainty in a probability distribution.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然概率理论允许我们提出不确定的表达式并在不确定的视野中进行推理，但数据使我们能够衡量概率分布中的不确定性程度。
- en: Why probability?
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择概率？
- en: 'Machine learning makes substantial utilization of probability theory unlike
    other branches of computer science that are mainly dependent on the deterministic
    nature of the computer system:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与主要依赖计算机系统确定性特性的其他计算机科学分支不同，机器学习大规模利用概率理论：
- en: This is on the grounds that machine learning must dependably manage uncertain
    quantities.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是因为机器学习必须始终处理不确定的量。
- en: Some of the time it may also be necessary to manage stochastic (non-deterministic)
    amounts. Uncertainty and stochasticity can emerge from numerous sources.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时候也可能需要处理随机（非确定性）量。不确定性和随机性可能来自多个来源。
- en: All exercises require some capacity to reason within the sight of uncertainty.
    Actually, with past numerical explanations that are valid by definition,, it is
    difficult to think about any suggestion that is completely valid or any occasion
    that is totally ensured to happen.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 所有活动都需要在不确定性的面前进行推理。实际上，通过过去的数学推理，既然它们是定义上有效的，我们很难想到任何完全有效的建议，或任何完全能够保证发生的事件。
- en: 'Uncertainty has are three possible sources:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性有三种可能的来源：
- en: Existing stochasticity in the framework that is being modeled.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型框架中存在的随机性。
- en: For instance, while playing a card game we make the assumption that the cards
    are truly shuffled in a random fashion.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，在玩扑克牌游戏时，我们假设牌是以完全随机的方式洗牌的。
- en: Fragmented observability. When a greater part of the variables that drive the
    conduct of the framework cannot be observed then even deterministic frameworks
    can seem stochastic.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 片段化的可观察性。当大部分驱动系统行为的变量无法被观察到时，即使是确定性系统也可能看起来是随机的。
- en: For instance, in a question with multiple-choice options as answers, one choice
    leads to the correct answer while others will result in nothing. The result given
    the challenger's decision is deterministic, yet from the candidate's perspective,
    the result is indeterminate.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，在一个带有多个选择题答案的考试中，一个选项是正确答案，而其他选项将导致错误的结果。给定挑战者的选择，结果是确定性的，但从候选人的角度看，结果是不可确定的。
- en: Fragmented modeling. When we utilize a model that must dispose of a portion
    of the data we have observed, the disposed-of data results in instability in the
    model's expectations.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 片段化建模。当我们使用一个模型，而必须丢弃我们已经观察到的一部分数据时，丢弃的数据会导致模型预测的不稳定。
- en: 'For instance, assume we manufacture a robot that can precisely watch the area
    of each article around it. In the event that the robot discretizes space while
    anticipating the future area of these objects, then the discretization makes the
    robot quickly become dubious about the exact position of the articles: every item
    could be any place inside the discrete cell that it was seen to possess.'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，假设我们制造了一个能够准确观察到周围每个物体位置的机器人。如果机器人在预测这些物体的未来位置时将空间离散化，那么离散化会使得机器人迅速变得不确定物体的确切位置：每个物体可能出现在它被看到的离散单元中的任何地方。
- en: A probability can be seen as the augmentation of rationale to manage uncertainty.
    Rationale gives an arrangement of formal rules for figuring out what suggestions
    are inferred to be true or false given the suspicion that some other arrangement
    of recommendations is true or false.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 概率可以看作是将逻辑扩展到处理不确定性的方式。逻辑提供了一套形式化的规则，用于计算在假设某些其他建议为真或假时，哪些结论可以被推断为真或假。
- en: Probability hypothesis gives an arrangement of formal principles for deciding
    the probability of a suggestion being genuine given the probability of the different
    recommendations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 概率理论提供了一套形式化的规则，用于根据不同建议的概率来确定某个建议为真或假的概率。
- en: Differences between machine learning and deep learning
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与深度学习的区别
- en: Machine learning and deep learning intend to accomplish the same objective,
    but, they are distinctive and amount to various thoughts. Machine learning is
    the most major of the two and scientists and mathematicians have been doing research
    on it for a few decades now. Deep learning is a comparatively new idea. Deep learning
    is based on learning via neural networks (multiple layers) to achieve the goal.
    Understanding the difference between the two is important to know where we should
    apply deep learning and which problems can be solved using machine learning.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和深度学习旨在实现相同的目标，但它们是不同的，代表着不同的思维方式。机器学习是两者中最主要的一种，科学家和数学家们已经研究它几十年了。深度学习是一个相对较新的概念。深度学习基于通过神经网络（多个层级）来学习以实现目标。理解两者之间的区别非常重要，这有助于我们知道在何种情况下应该应用深度学习，哪些问题可以通过机器学习解决。
- en: It was understood that a more intense approach to construct pattern recognition
    algorithms is achieved by utilizing the information that can be effortlessly mined
    relying only upon the area and the deciding objective.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 已知通过利用仅依赖于领域和决定目标的信息，可以构建一种更强大的模式识别算法，该信息可以轻松挖掘。
- en: For instance, in image recognition we accumulate various pictures and expand
    the algorithm on that. Utilizing the information as a part of these pictures,
    our model can be trained to recognize creatures, human appearances, or different
    examples.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在图像识别中，我们积累了各种图片并在此基础上扩展算法。利用这些图片中的信息，我们的模型可以被训练来识别生物、人的外貌或其他模式。
- en: Machine learning is connected to different areas and now it is not limited to
    image or character recognition. It is currently utilized intensely as a part of
    robotics, financial markets, self-driving cars, and genome analysis. We learned
    about machine learning in previous chapters and now we can go further to understand
    how different it is from deep learning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与其他领域相关，现在它不仅仅局限于图像或字符识别。目前，它在机器人技术、金融市场、自动驾驶汽车和基因组分析等领域得到了广泛应用。我们在之前的章节中学习了机器学习，现在我们可以进一步了解它与深度学习的不同之处。
- en: What is deep learning?
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是深度学习？
- en: Deep learning started becoming popular in 2006\. It is also known as hierarchical
    learning. Its applications are wide and it has increased the scope of artificial
    intelligence and machine learning. There is a huge interest in deep learning from
    the community.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在2006年开始变得流行，也被称为层次学习。它的应用广泛，极大地扩展了人工智能和机器学习的范围。社区对深度学习的兴趣巨大。
- en: 'Deep learning refers to a class of machine learning techniques which:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习指的是一类机器学习技术，具体包括：
- en: Perform unsupervised or supervised feature extraction.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行无监督或监督的特征提取。
- en: Perform pattern analysis or classification by exploiting multiple layers of
    non-linear information processing.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过利用多个非线性信息处理层，执行模式分析或分类。
- en: It consists of a hierarchy of features or factors. In this hierarchy, lower-level
    features help in defining higher-level features. Artificial neural networks are
    typically used in deep learning.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 它由一系列特征或因素构成。在这个层次结构中，低层特征有助于定义高层特征。人工神经网络通常用于深度学习。
- en: Conventional machine learning models learn patterns or clusters. Deep neural
    networks learn computations with a very small number of steps.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统的机器学习模型学习模式或聚类。深度神经网络通过极少的步骤学习计算。
- en: Generally speaking, the deeper the neural network, the more powerful it gets.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般来说，神经网络越深，其能力就越强大。
- en: Neural networks are updated according to the new data made available.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络会根据新提供的数据进行更新。
- en: Artificial neural networks are fault tolerant, which means that if some part
    of the network is destroyed, then that may affect the performance of the network,
    but the key functioning of the network may still be retained.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络具有容错性，这意味着如果网络的某些部分被破坏，可能会影响网络的性能，但网络的关键功能仍可能得以保留。
- en: Deep learning algorithms learn multiple levels of representation and do the
    computations in parallel, which may be of increasing complexity.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习算法学习多层次的表示，并行执行计算，这些计算的复杂性可能不断增加。
- en: If we fast forward to today, there is a widespread enthusiasm for something
    that many refer to as deep learning. The most prominent sorts of deep learning
    models, as they are utilized as a part of extensive scale image recognition tasks,
    are known as Convolutional Neural Nets, or essentially ConvNets.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们快速推进到今天，大家普遍对现在许多人称之为深度学习的技术充满热情。最著名的深度学习模型，特别是在大规模图像识别任务中应用的，是卷积神经网络，简称ConvNets。
- en: Deep learning emphasizes the sort of model that we need to utilize (such as
    a deep convolutional multi-layer neural system) and that we can utilize information
    to fill in the missing parameters.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习强调我们需要使用的模型类型（例如深度卷积多层神经网络），以及我们可以利用数据来填补缺失的参数。
- en: With deep learning comes incredible obligation. Since we are beginning with
    a model of the world which has a high dimensionality, we truly require a great
    deal of information which we also call big data, and a considerable measure of
    computational force (General Purpose GPUs/ High performance computing). Convolutions
    are utilized widely as a part of deep learning (particularly computer vision applications).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习带来了巨大的责任。因为我们从一个具有高维度的世界模型开始，我们实际上需要大量的数据，也就是我们所说的“大数据”，并且需要相当大的计算能力（通用GPU/高性能计算）。卷积在深度学习中被广泛使用（特别是在计算机视觉应用中）。
- en: '![What is deep learning?](img/B05321_11_01-1.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![什么是深度学习？](img/B05321_11_01-1.jpg)'
- en: 'In the previous image we saw three layers:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像中，我们看到了三层：
- en: '**Output layer**: Here this predicts a supervised target'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：在这里预测一个监督目标'
- en: '**Hidden layer**: Abstract representations of the intermediary functions'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：中间函数的抽象表示'
- en: '**Input layer**: Raw inputs'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：原始输入'
- en: Artificially simulated neurons stand for the building blocks of the multi-layer
    artificial neural systems. The essential idea is to simulate a human brain and
    how it solves a complex problem. The main idea to manufacture neural systems was
    based upon these theories and models.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 人工模拟神经元代表了多层人工神经网络的构建模块。基本的思想是模拟人类大脑以及它如何解决复杂问题。制造神经网络的主要思想是基于这些理论和模型。
- en: Numerous more significant leaps were brought about in the last few decades with
    regards to deep-learning algorithms. These can be utilized to make feature indicators
    from unlabeled information and also to pre-train deep neural networks, which are
    the neural systems that are made out of numerous layers.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年里，深度学习算法取得了许多重要进展。这些进展可以用来从无标签数据中提取特征指标，还可以预训练深度神经网络，这些神经网络由多个层次组成。
- en: Neural networks are an interesting issue in scholastic exploration, as well
    as in huge innovation organizations, for example for companies such as Facebook,
    Microsoft, and Google, who are investing heavily in deep-learning research.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是学术研究中的一个有趣问题，也是在大型科技公司中至关重要的领域，例如 Facebook、Microsoft 和 Google 等公司，正在大力投资于深度学习研究。
- en: 'Complex neural networks fueled by deep-learning calculations are considered
    as best in class with regards to critical problem solving. For example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由深度学习算法驱动的复杂神经网络被认为是解决重大问题的最先进技术。例如：
- en: '**Google''s image search**: We can search images on the Internet using the
    Google image search tool. This can be done by uploading an image or giving the
    URL of the image to search for on the Internet.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**谷歌图像搜索**：我们可以使用谷歌图像搜索工具在互联网上搜索图片。这可以通过上传图片或提供图片的 URL 来搜索。'
- en: '**Google Translate**: This tool can read text in images and understand speech
    to translate or tell meaning in several languages.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**谷歌翻译**：这个工具可以读取图片中的文本，并理解语音，进行翻译或解释多种语言的含义。'
- en: One other very famous application is used in the self-driving cars, created
    by Google or Tesla. They are powered by deep learning to find out the best path,
    drive through the traffic in real time, and perform necessary tasks as they would
    if they were being driven by a human driver.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常著名的应用是自动驾驶汽车，谷歌或特斯拉所创造的。它们由深度学习驱动，能够实时找到最佳路径，穿越交通，并执行必要的任务，像是由人类司机驾驶时一样。
- en: Deep feedforward networks
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度前馈网络
- en: 'Deep feedforward networks are the most famous deep learning models. These are
    also called the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 深度前馈网络是最著名的深度学习模型。这些也被称为以下几种：
- en: Feedforward neural networks.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈神经网络。
- en: '**Multi-layer perceptrons** (**MLPs**)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLPs**）'
- en: '![Deep feedforward networks](img/B05321_11_02.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![深度前馈网络](img/B05321_11_02.jpg)'
- en: The aim of the feed-forward neural network is to learn by their parameters and
    define a function that maps to the output *y:*
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络的目标是通过其参数进行学习，并定义一个映射到输出 *y* 的函数：
- en: '*y = f(x, theta)*'
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*y = f(x, theta)*'
- en: As also depicted in the image, the feedforward neural networks are called such
    because of their data flows in one direction. It starts from the *x* and passes
    through the function for the intermediate calculations to generate *y*.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图中所示，前馈神经网络之所以叫做前馈网络，是因为它们的数据流向是单向的。它从 *x* 开始，通过函数进行中间计算，生成 *y*。
- en: When such systems also include connections to the previous layer (feedback),
    then these are known as recurrent neural networks.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些系统还包括与上一层的连接（反馈）时，它们被称为递归神经网络。
- en: Feedforward systems are of great significance to machine learning experts. They
    frame the premise of numerous imperative business applications. For instance,
    the convolutional systems utilized for natural language processing from speech
    are a specific sort of feedforward system.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈系统对机器学习专家至关重要。它们构成了许多重要商业应用的基础。例如，用于语音自然语言处理的卷积网络就是一种特定类型的前馈系统。
- en: Feedforward systems are a reasonable stepping stone on the way to recurrent
    networks. These systems have numerous natural language applications. Feedforward
    neural networks are called networks since they are represented by forming together
    numerous different functions. The model is connected with a directed acyclic graph
    portraying how the functions are created together.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈系统是通往递归网络的合理垫脚石。这些系统在自然语言应用中有许多用途。前馈神经网络被称为网络，因为它们通过将多个不同的函数组合在一起来表示。该模型与一个有向无环图相连，描述了函数是如何组合在一起的。
- en: For example, we have three functions - *f(1)*, *f(2)*, and *f(3)*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们有三个函数——*f(1)*、*f(2)*和*f(3)*。
- en: 'They are chained or associated together as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 它们按如下方式链接或关联在一起：
- en: '*f(x) = f(3)(f(2)(f(1)(x)))*'
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*f(x) = f(3)(f(2)(f(1)(x)))*'
- en: 'These chain structures are the most normally utilized structures of neural
    systems. For this situation:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这些链式结构是神经网络中最常用的结构。在这种情况下：
- en: '*f(1)* is known as the first layer of the network.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(1)* 被称为网络的第一层。'
- en: '*f(2)* is known as the second layer, and so on.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(2)* 被称为第二层，依此类推。'
- en: The general length of the chain gives the depth of the model. It is from this
    wording the name "deep learning" emerges.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 链的总长度决定了模型的深度。正是从这个术语中，"深度学习" 这个名称产生。
- en: The final layer of a feedforward network is known as the output or yield layer.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络的最终层被称为输出层或结果层。
- en: 'Amid neural network training, we follow these steps:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络训练过程中，我们遵循以下步骤：
- en: Drive *f(x)* to coordinate *f∗(x)*. The training information has data with noise
    and inexact data off *∗(x)* assessed at different training sets.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 驱动 *f(x)* 与 *f∗(x)* 一致。训练数据包含噪声和不准确的数据 off *∗(x)*，这些数据是在不同的训练集上评估的。
- en: Every example of *x* is associated by a label *y ≈ f∗(x)*.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个* x *的示例都由标签 *y ≈ f∗(x)*关联。
- en: The training cases determine straightforwardly what the yield layer must do
    at every point *x*. That is, it must create a value that is near *y*.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练案例直接决定了每个 *x* 点上输出层应该做什么。也就是说，它必须生成一个接近 *y* 的值。
- en: Understanding the hidden layers in a neural network
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解神经网络中的隐藏层
- en: The conduct of alternate layers is not straightforwardly specified by the training
    information. The learning algorithm must choose how to utilize those layers to
    create the desired yield, yet the training information does not say what every
    individual layer ought to do.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其他层的行为并未由训练数据直接指定。学习算法必须选择如何利用这些层来生成期望的输出，但训练数据并没有说明每一层应该做什么。
- en: Rather, it is the learning algorithm which must choose how to utilize these
    layers to best execute an estimation off ∗. Since the training information does
    not demonstrate the desired yield for each of these layers, these layers are called
    hidden layers.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，必须由学习算法来选择如何利用这些层以最佳方式执行估计 off ∗。由于训练数据没有显示每一层的期望输出，这些层被称为隐藏层。
- en: The motivation of neural networks
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络的动机
- en: These systems are called neural on the grounds that they are approximately motivated
    by neuroscience.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些系统之所以被称为神经网络，是因为它们在某种程度上受到神经科学的启发。
- en: Each concealed or hidden layer of the system is generally vector-valued.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统的每一隐藏层通常都是向量值的。
- en: The dimension of *y* of these hidden layers decides the width of the model.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些隐藏层的 *y* 维度决定了模型的宽度。
- en: Every component of the vector might be translated as assuming a part comparable
    to a neuron.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量的每个分量可以理解为承担类似神经元的角色。
- en: As opposed to thinking about the layer as exhibiting a single vector-to-vector
    function, it should be thought that the layer comprises of numerous units that
    work in parallel, each exhibiting a vector-to-scalar function.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其将该层视为展示单一的向量到向量函数，不如认为该层由许多单元组成，这些单元并行工作，每个单元展示一个向量到标量的函数。
- en: Each unit looks like a neuron in respect that it gets a contribution from numerous
    different units and registers its own activation value.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单元看起来像一个神经元，因为它从许多不同的单元获取贡献并注册其自身的激活值。
- en: Using numerous layers of vector-valued representation is drawn from neuroscience.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个向量值表示的层是受到神经科学的启发。
- en: The decision of the function *f(i)(x)* used to figure out these representations
    is somewhat guided by neuroscientific observations about the functions that organic
    neurons process.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用来确定这些表示的函数 *f(i)(x)* 的选择在某种程度上是受到神经科学观察的指导，这些观察关注有机神经元处理的功能。
- en: We studied regularization in previous chapters. Let's study why this is important
    and required for deep learning models.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章中研究了正则化。现在让我们研究为什么这对深度学习模型至关重要。
- en: Understanding regularization
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解正则化。
- en: The main issue in machine learning is the means by which to make an algorithm
    that will perform well on the training information, as well as on new inputs.
    Numerous techniques utilized as a part of machine learning are expressly intended
    to diminish test errors, potentially to the detriment of increased training errors.
    These techniques are referred to aggregately as regularization.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的主要问题是如何构建一个能够在训练数据和新输入上都表现良好的算法。机器学习中使用的许多技术特别旨在减少测试误差，可能以增加训练误差为代价。这些技术统称为正则化。
- en: There are many types of regularization accessible to the deep-learning specialist.
    More effective regularization systems have been one of the research efforts in
    the field.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习专家可以使用许多种正则化方法。更有效的正则化策略一直是该领域研究的重点之一。
- en: There are numerous regularization systems.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多正则化策略。
- en: Additional constraints on a machine learning model
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型的额外约束。
- en: For example, including constraints on the parameter values.
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，包含对参数值的约束。
- en: Additional terms in the target functions that can be taken as comparing to a
    delicate requirement on the parameter values
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标函数中的附加项可以视为与参数值的精细要求进行比较。
- en: If done strategically and carefully, these additional requirements and constraints
    can result in enhanced performance on the testing data
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果策略得当且谨慎，这些附加要求和约束可以在测试数据上带来更好的性能。
- en: These constraints and restrictions can also be used to encode specific sorts
    of prior learning
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些约束和限制也可以用来编码特定类型的先前学习。
- en: These constraints and restrictions can also lead to generalization of the model
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些约束和限制也可以导致模型的泛化。
- en: Ensemble methods also use regularization to generate better results
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法也使用正则化来生成更好的结果。
- en: 'With regards to deep learning, most regularization procedures depend on regularizing
    estimators. To regulate the estimator:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 关于深度学习，大多数正则化程序依赖于正则化估计器。为了调节估计器：
- en: We need to exchange increased bias for reduced variance
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要交换增加的偏差以减少方差。
- en: An effective regularizer is one that makes a profitable exchange, which means
    it decreases the variance drastically whilst not excessively expanding the bias
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个有效的正则化器是能够做出有利交换的，这意味着它显著减少了方差，同时不会过度增加偏差。
- en: 'In overfitting and generalization we concentrate on these situations for the
    model that we are training:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在过拟合和泛化中，我们专注于训练模型时遇到的这些情况：
- en: Avoid the true information on the producing process to take into account the
    overfitting and inducing bias
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免关于生成过程的真实信息，以考虑过拟合并引入偏差。
- en: Include the true information on the producing process
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含关于生成过程的真实信息。
- en: Include information on the producing process and additionally numerous other
    information on producing processes to take into account the overfitting where
    variance instead of bias rules the estimation error
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含关于生成过程的信息，并且额外包含关于生成过程的其他众多信息，以考虑过拟合，其中方差而非偏差主导了估计误差。
- en: The objective of regularization is to take a model to the second process that
    is mentioned.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的目标是将模型带入提到的第二个过程。
- en: An excessively complex model family does not, as a matter of course, incorporate
    the target function or the genuine information producing process. In any case,
    most utilizations of deep-learning algorithms are where the genuine information
    producing procedure is in all likelihood outside the model family. Deep learning
    algorithms are normally connected, to a great degree, to complicated use cases
    such as image recognition, speech recognition, self-driving cars, and so on.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 过于复杂的模型家族并不一定包含目标函数或真实的数据生成过程。然而，大多数深度学习算法的应用场景是，真实的数据生成过程很可能超出了模型家族的范围。深度学习算法通常与复杂的应用场景密切相关，如图像识别、语音识别、自动驾驶汽车等。
- en: This means that controlling the complexity of the nature of the model is not
    just a matter of finding a model of the appropriate size with the right set of
    parameters.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，控制模型复杂性不仅仅是找到一个适当大小且具有正确参数集的模型。
- en: Optimizing deep learning models
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化深度学习模型。
- en: Optimization methods are vital in designing algorithms to extract desired knowledge
    from huge volumes of data. Deep learning is a rapidly evolving field where new
    optimization techniques are generated.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 优化方法在设计算法中至关重要，用于从海量数据中提取所需的知识。深度学习是一个快速发展的领域，新的优化技术不断涌现。
- en: Deep learning algorithms include enhancement in numerous connections. For instance,
    performing deduction in models, for example, PCA, includes taking care of an improvement
    issue.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法在许多关联中包含优化。例如，在像PCA这样的模型中执行推断，涉及解决优化问题。
- en: We regularly utilize diagnostic optimization to compose verifications or configuration
    calculations. Of the majority of the numerous optimization issues required in
    deep learning, the most difficult is preparing the neural network.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用诊断优化来编写验证或配置计算。深度学习中许多优化问题中，最难的就是训练神经网络。
- en: It is very common to contribute days, or even months, of time to many machines
    with a specific end goal to solve even a single case of the neural system-training
    problem. Since this issue is so critical and thus expensive, a specific arrangement
    of optimization strategies has been produced for enhancing it.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多机器上花费数天甚至数月的时间，以解决神经网络训练问题的单一案例，这种情况非常常见。由于这个问题如此关键且昂贵，已提出了一系列优化策略来改进它。
- en: The case of optimization
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化的案例
- en: To find the parameters θ of a neural network that significantly lessen a cost
    function J(θ), commonly incorporates an execution measure assessed on the whole
    training set and additionally extra regularization terms.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到神经网络的参数θ，从而显著减少成本函数J(θ)，通常需要评估整个训练集的执行度量，并可能包含额外的正则化项。
- en: An optimization used as a training algorithm for a machine learning task is
    different from immaculate optimization. More complex algorithms adjust their learning
    rates amid training or influence data contained in the second derivatives of the
    cost function. Finally, a few optimization methodologies are created by joining
    basic optimization algorithms into higher-level strategies.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 用作机器学习任务训练算法的优化与纯粹的优化不同。更复杂的算法在训练过程中会调整其学习率，或影响包含在成本函数二阶导数中的数据。最后，一些优化方法是通过将基本的优化算法组合成更高级的策略而产生的。
- en: 'Optimization algorithms utilized for the training of deep learning models are
    different from conventional optimization algorithms in a few ways:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练深度学习模型的优化算法与传统优化算法在几个方面有所不同：
- en: Machine learning typically acts in a roundabout way. In most machine-learning
    situations, we think about some execution measure *P*, that is defined as for
    the test set and may likewise be obstinate. We accordingly upgrade *P* just in
    a roundabout way. We decrease a different cost function *J(θ) *with the expectation
    that doing so will enhance *P*.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习通常是间接进行的。在大多数机器学习场景中，我们考虑某个执行度量*P*，该度量在测试集上定义，并且可能是顽固的。因此，我们间接地优化*P*。我们减少另一个成本函数*J(θ)*，期望通过这样做来改善*P*。
- en: This is as opposed to pure optimization, where minimizing *J* is an objective
    all by itself. Optimization algorithms for preparing deep learning models likewise
    commonly incorporate some specialization on the specific structure of machine
    learning target functions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这与纯粹的优化不同，在纯粹优化中，最小化*J*本身就是目标。用于训练深度学习模型的优化算法通常还会针对机器学习目标函数的具体结构进行一些专门化。
- en: Implementation in Julia
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Julia中的实现
- en: 'There are many good and tested libraries for deep learning in popular programming
    languages:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多好的、经过测试的深度学习库，适用于流行的编程语言：
- en: Theano (Python)  can utilize both CPU and GPU (from the MILA Lab at the University
    of Montreal)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Theano（Python）可以同时使用CPU和GPU（来自蒙特利尔大学的MILA实验室）
- en: Torch (Lua) is a Matlab-like environment (from Ronan Collobert, Clement Farabet,
    and Koray Kavukcuoglu)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torch（Lua）是一个类似Matlab的环境（来自Ronan Collobert、Clement Farabet和Koray Kavukcuoglu）
- en: Tensorflow (Python) makes use of data flow graphs
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tensorflow（Python）利用数据流图
- en: MXNet (Python, R, Julia, C++)
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MXNet（Python、R、Julia、C++）
- en: Caffe is the most popular and widely used
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caffe是最流行且广泛使用的
- en: Keras (Python) based on Theano
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras（Python）基于Theano
- en: Mocha (Julia) by Chiyuan Zhang
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mocha（Julia）由张启源编写
- en: We will mainly go through Mocha for Julia, which is an amazing package written
    by Chiyuan Zhang, a PhD student at MIT.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要介绍Mocha for Julia，这是一个由麻省理工学院博士生张启源编写的令人惊叹的包。
- en: 'To start, add the package as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，按如下方式添加包：
- en: '[PRE0]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Network architecture
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络架构
- en: 'Network architecture in Mocha refers to a set of layers:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Mocha 中的网络架构指的是一组层：
- en: '[PRE1]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The input of the `ip_layer` has the same name as the output of the `data_layer`
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ip_layer` 的输入与 `data_layer` 的输出具有相同的名称'
- en: The same name connects them
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同的名称将它们连接起来
- en: A topological sort is carried out by Mocha on a collection of layers.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Mocha 对一组层执行拓扑排序
- en: Types of layers
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层的类型
- en: Data layers
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据层
- en: These layers read information from the source and feed them to the top layers
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些层从源读取信息并将其传递给顶层
- en: Computation layers
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算层
- en: These take the input stream from the base layers, do the calculations, and feed
    the results generated to the top layers
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些层从基础层接收输入流，进行计算，并将生成的结果反馈给顶层
- en: Loss layers
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失层
- en: These layers take processed results (and ground truth names/labels) from the
    base layers and figure a scalar loss value
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些层从基础层接收处理过的结果（以及真实标签）并计算标量损失值
- en: Loss values from all the layers and regularizers in a net are included to characterize
    the final loss function of the net
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自网络中所有层和正则化器的损失值被纳入，以表征网络的最终损失函数
- en: The net parameters in the back propagation are trained with the help of the
    loss function
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播中的网络参数通过损失函数的帮助进行训练
- en: Statistics layers
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计层
- en: These take information from the base layers and generate valuable insights like
    classification accuracy
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些层从基础层接收信息，并生成有价值的见解，如分类准确性
- en: Insights are gathered all through the various iterations
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 见解在多个迭代过程中收集
- en: '`reset_statistics` can be utilized to unequivocally reset the statistics aggregation'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reset_statistics` 可用于明确重置统计汇总'
- en: Utility Layers
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具层
- en: Neurons (activation functions)
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经元（激活函数）
- en: Let's understand real neural nets (brains). Neuroscience is the study of the
    functioning of the brain and has given us good evidence about the way in which
    it works. Neurons are the real information storage of the brain. It is also very
    important to understand their connection strengths, namely how strongly one neuron
    can influence those neurons connected to it.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解真正的神经网络（大脑）。神经科学是研究大脑功能的学科，并为我们提供了关于大脑如何工作的有力证据。神经元是大脑的真实信息存储单元。理解它们的连接强度，即一个神经元如何强烈地影响与其连接的神经元，也是非常重要的。
- en: Learning or the repetition of a task and exposure to new stimulating procedures
    or environment often leads to activity in the brain which is actually the neurons
    acting according to the new data being received.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 学习或任务的重复以及对新的刺激过程或环境的暴露，通常会导致大脑活动，实际上是神经元根据接收到的新数据做出反应。
- en: The neurons, and therefore the brain, behave very differently to different stimuli
    and environments. They may react or get excited more in some scenarios as compared
    to others.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元，因而大脑，在面对不同的刺激和环境时表现得非常不同。它们在某些情境下的反应或激动程度可能比其他情境更加明显。
- en: 'Some understanding of this is important in getting to know about artificial
    neural networks:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这一点对了解人工神经网络非常重要：
- en: Neurons can be connected to any layer
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元可以连接到任何层
- en: The neuron of every layer will influence the yield in the forward pass and the
    slope in the backward pass consequently, unless it is an identity neuron
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一层的神经元都会影响前向传递中的输出以及反向传播中的梯度，除非它是一个身份神经元
- en: By default, layers have an identity neuron
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，层具有身份神经元
- en: 'Let''s go through the various types of neurons that we can utilize to make
    the network:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解一下可以用来构建网络的各种神经元类型：
- en: '`class Neurons.Identity`'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class Neurons.Identity`'
- en: This is an activation function whose input is not changed.
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一种激活函数，其输入不发生变化。
- en: '`class Neurons.ReLU`'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class Neurons.ReLU`'
- en: Rectified Linear Unit. Amid the forward pass, this restrains all restraints
    underneath some limit *ϵ*, normally 0.
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修正线性单元。在前向传递过程中，它将所有小于某个限制 *ϵ*（通常是 0）的约束限制在该值之下。
- en: It processes point-wise *y=max(ϵ,x)*.
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它逐点处理 *y=max(ϵ,x)*。
- en: '`class Neurons.LreLU`'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class Neurons.LreLU`'
- en: Leaky Rectified Linear Unit. A Leaky ReLU can settle the "dying ReLU" issue.
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泄漏修正线性单元。Leaky ReLU 可以解决“死亡 ReLU”问题。
- en: ReLU's can "die" if a sufficiently substantial gradient changes the weights
    such that the neuron never activates on new information.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果足够大的梯度改变权重，使得神经元在新信息上永远不被激活，ReLU 会“死亡”。
- en: '`class Neurons.Sigmoid`'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class Neurons.Sigmoid`'
- en: Sigmoid is a smoothed step function
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid 是一种平滑的阶跃函数
- en: It produces roughly 0 for negative information with vast absolute values and
    estimated 1 for huge positive inputs
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对于绝对值极大的负信息输出大约为 0，对于极大的正输入输出约为 1
- en: The point-wise equation is *y=1/(1+e−x)y=1/(1+e−x)*
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点对点方程是 *y=1/(1+e−x)y=1/(1+e−x)*
- en: '`class Neurons.Tanh`'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class Neurons.Tanh`'
- en: Tanh is a variation of Sigmoid
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanh是Sigmoid的一种变种
- en: It takes values in *±1±1* rather than the unit interim
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的取值为 *±1±1*，而不是单位间隔。
- en: The point-wise equation is *y=(1−e−2x)/(1+e−2x)*
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点对点方程是 *y=(1−e−2x)/(1+e−2x)*
- en: '![Neurons (activation functions)](img/image_11_003.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![神经元（激活函数）](img/image_11_003.jpg)'
- en: Understanding regularizers for ANN
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解ANN的正则化方法
- en: We studied regularizers in our previous sections. Regularizers include additional
    penalties or restrictions for network parameters to confine the complexity of
    the model. In a popular deep-learning framework, caffe, it is known as decay.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的章节中研究了正则化方法。正则化方法包括对网络参数的附加惩罚或限制，以限制模型的复杂度。在一个流行的深度学习框架中，Caffe，它被称为衰减（decay）。
- en: Weight decay and regularization are comparable in back-propagation. The theoretical
    contrast in the forward pass is that when regarded as weight decay, they are not
    considered as being a piece of the objective function.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 权重衰减和正则化在反向传播中是可以比较的。前向传递中的理论对比在于，当被看作权重衰减时，它们不被视为目标函数的一部分。
- en: By default, Mocha similarly eliminates the forward calculation for regularizers
    with a specific end goal to decrease the quantity of calculations. We utilize
    the term regularization rather than weight decay since it is less demanding to
    comprehend.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Mocha同样会删除正则化器的前向计算，目的是减少计算量。我们使用“正则化”这个术语，而不是“权重衰减”，因为它更容易理解。
- en: 'class `NoRegu`: No regularization'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '类 `NoRegu`: 无正则化'
- en: 'class `L2Regu`: L2 regularizer'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '类 `L2Regu`: L2 正则化器'
- en: 'class `L1Regu`:  L1 regularizer'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '类 `L1Regu`: L1 正则化器'
- en: Norm constraints
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 范数约束
- en: Norm restrictions are a more immediate method for limiting the complexity of
    the model by unequivocally contracting the parameters in each of the n cycles
    if the standard or norm of the parameters surpasses a given threshold.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 范数限制是一种通过在每个n周期中明确收缩参数来直接限制模型复杂度的方法，如果参数的标准或范数超过给定阈值。
- en: 'class `NoCons`: No constraints'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '类 `NoCons`: 无约束'
- en: 'class `L2Cons`: Restrict the Euclidean norm of parameters. Threshold and contracting
    are applied to every parameter. In particular, the threshold is applied to each
    filter for the filters parameter of a convolution layer.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '类 `L2Cons`: 限制参数的欧几里得范数。阈值和收缩应用于每个参数。特别是，阈值应用于卷积层的滤波器参数的每个滤波器。'
- en: Using solvers in deep neural networks
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在深度神经网络中使用求解器
- en: Mocha contains broadly useful stochastic (sub-) gradient-based solvers. These
    can be utilized to prepare deep neural networks.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Mocha包含广泛实用的随机（子）梯度优化求解器。这些求解器可以用来训练深度神经网络。
- en: 'A solver is developed by indicating a lexicon of solver parameters that give
    vital configuration:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 求解器通过指明一个求解器参数词汇表来开发，提供了重要的配置：
- en: General settings like stop conditions
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般设置，如停止条件
- en: Parameters particular to a specific calculation
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定计算的参数
- en: Additionally, it is generally recommended to take some short breaks between
    training iterations to print progress or for saving a snapshot. These are called
    coffee breaks in Mocha.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通常建议在训练迭代之间休息片刻，以打印进度或保存快照。这些在Mocha中被称为“咖啡休息”。
- en: '**Solver algorithms**'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**求解器算法**'
- en: 'class `SGD`: Stochastic Gradient Descent with momentum.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '类 `SGD`: 带动量的随机梯度下降。'
- en: '`lr_policy`: Learning rate policy.'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_policy`: 学习率策略。'
- en: '`mom_policy`: Momentum policy.'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mom_policy`: 动量策略。'
- en: 'class `Nesterov`: Stochastic Nesterov accelerated gradient method.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '类 `Nesterov`: 随机Nesterov加速梯度方法。'
- en: '`lr_policy`: Learning rate policy.'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_policy`: 学习率策略。'
- en: '`mom_policy`: Momentum policy.'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mom_policy`: 动量策略。'
- en: 'class `Adam`: A Method for Stochastic Optimization'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '类 `Adam`: 随机优化方法'
- en: '`lr_policy`: Learning rate policy.'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_policy`: 学习率策略。'
- en: '`beta1`: Exponential decay factor for first order moment estimates. *0<=beta1<1*,
    default *0.9*'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta1`: 一阶矩估计的指数衰减因子。*0<=beta1<1*，默认 *0.9*'
- en: '`beta2`: Exponential decay factor for second order moment estimates, *0<=beta1<1*,
    default *0.999*.'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta2`: 二阶矩估计的指数衰减因子，*0<=beta2<1*，默认 *0.999*。'
- en: '`epsilon`: Affects the scaling of the parameter updates for numerical conditioning,
    default *1e-8*.'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon`: 影响参数更新的缩放，用于数值条件化，默认 *1e-8*。'
- en: Coffee breaks
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 咖啡休息
- en: 'Training can become a very computationally intensive iteration of several loops.
    It is generally recommended to take some short breaks between training iterations
    to print progress or for saving a snapshot. These are called coffee breaks in
    Mocha. They are performed as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可能会变成一个非常计算密集的多次迭代过程。通常建议在训练迭代之间适当休息，打印进度或保存快照。这些被称为Mocha中的咖啡休息。它们的执行方式如下：
- en: '[PRE2]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This prints the training summary every 1,000 iterations and saves a snapshot
    every 5,000 iterations.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 每1,000次迭代打印一次训练摘要，并每5,000次迭代保存一次快照。
- en: Image classification with pre-trained Imagenet CNN
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练的Imagenet CNN进行图像分类。
- en: 'MNIST is a handwritten digit recognition dataset. It contains the following:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST是一个手写数字识别数据集，包含以下内容：
- en: 60,000 training examples
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 60,000个训练样本。
- en: 10,000 test examples
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10,000个测试样本。
- en: 28 x 28 single channel grayscale images
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 28 x 28单通道灰度图像。
- en: We can use `get-mnist.sh` script to download the dataset
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`get-mnist.sh`脚本来下载数据集。
- en: It calls `mnist.convert.jl` to convert the binary dataset into a HDF5 file that
    Mocha can read.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 它调用`mnist.convert.jl`将二进制数据集转换为Mocha可以读取的HDF5文件。
- en: '`data/train.hdf5` and `data/test.hdf5` will be generated when the conversion
    finishes.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`data/train.hdf5`和`data/test.hdf5`将在转换完成后生成。'
- en: 'We are using Mocha''s native extension here to get faster convolution:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用Mocha的本地扩展以加速卷积：
- en: '[PRE3]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This configures Mocha to use the native background and not the GPU (CUDA).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这配置Mocha使用本地后台而非GPU（CUDA）。
- en: Now, we will proceed with defining the network structure. We will start by defining
    a data layer that will read the HDF5 file. This will be the input for the network.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续定义网络结构。我们将从定义一个数据层开始，该数据层将读取HDF5文件。这将成为网络的输入。
- en: 'The `source` contains a list of real data files:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`source`包含真实数据文件的列表：'
- en: '[PRE4]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Mini-batches are formed to process the data. As the batch size increases, the
    variance decreases but it affects the computational performance.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 通过形成小批量来处理数据。随着批量大小的增加，方差减小，但会影响计算性能。
- en: Shuffling reduces the effect of ordering during the training.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 洗牌可以减少训练过程中顺序的影响。
- en: 'Now we will proceed with defining the convolution layer:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将继续定义卷积层：
- en: '[PRE5]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`name`: The name of the layer to identify it.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`: 用于标识层的名称。'
- en: '`n_filter`: The number of convolution filters.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_filter`: 卷积滤波器的数量。'
- en: '`kernel`: The size of the filter.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel`: 滤波器的大小。'
- en: '`bottoms`: An array to define where to get the input. (The HDF5 data layer
    that we defined.)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bottoms`: 一个数组，用于定义输入的位置。（我们定义的HDF5数据层。）'
- en: '`tops`: The output of the convolution layer.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tops`: 卷积层的输出。'
- en: 'Define more convolution layers as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下方式定义更多卷积层：
- en: '[PRE6]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: These are two fully connected layers after convolution and pooling layers.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是卷积层和池化层后的两个全连接层。
- en: The computation to create the layer is an inner product between the input and
    the layer weights. These are also called an `InnerProductLayer`.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 创建该层的计算是输入与层权重之间的内积。这些也被称为`InnerProductLayer`。
- en: 'The layer weights are also learned, so we also give names to the two layers:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 层的权重也会被学习，因此我们还为这两个层命名：
- en: '[PRE7]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The last inner product layer has the dimension of 10, which represents the number
    of classes (digits 0~9).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的内积层的维度为10，表示类别的数量（数字0~9）。
- en: 'This is the basic structure of LeNet. To train this network, we will define
    a loss function by adding a loss layer:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这是LeNet的基本结构。为了训练这个网络，我们将通过添加一个损失层来定义一个损失函数：
- en: '[PRE8]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can now construct our network:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以构建我们的网络：
- en: '[PRE9]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Training the neural network with Stochastic Gradient Descent is performed as
    follows:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机梯度下降法训练神经网络的过程如下：
- en: '[PRE10]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The parameters used are:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的参数如下：
- en: '`max_iter`: These are the maximum number of iterations the solver will execute
    to train the network'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_iter`: 这些是求解器将执行的最大迭代次数，用于训练网络。'
- en: '`regu_coef`: The regularization coefficient'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`regu_coef`: 正则化系数。'
- en: '`mom_policy`: The momentum policy'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mom_policy`: 动量策略。'
- en: '`lr_policy`: The learning rate policy'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_policy`: 学习率策略。'
- en: '`load_from`: Here we can load the saved model from a file or a directory'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_from`: 在这里我们可以从文件或目录加载已保存的模型。'
- en: 'Add some coffee breaks as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一些咖啡休息，如下所示：
- en: '[PRE11]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Performance is checked periodically on a separate validation set so we can see
    the progress. The validation dataset that we have will be used as the test dataset.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 性能会定期在单独的验证集上进行检查，以便我们能看到进展。我们拥有的验证数据集将用作测试数据集。
- en: 'To perform an evaluation, a new network is defined with the same architecture
    but a different data layer, which will get the input from the validation set:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行评估，定义一个新的网络，采用相同的架构，但数据层不同，它将从验证集获取输入：
- en: '[PRE12]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Add a coffee break to get the report of the validation performance, as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个咖啡休息，获取验证性能报告，具体如下：
- en: '[PRE13]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, start the training, as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，开始训练，具体如下：
- en: '[PRE14]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'These are the two networks we created:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们创建的两个网络：
- en: '![Image classification with pre-trained Imagenet CNN](img/image_11_004.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![使用预训练的 Imagenet CNN 进行图像分类](img/image_11_004.jpg)'
- en: 'Now we run the model generated on the test data that we have. We get the following
    output:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在测试数据上运行生成的模型。我们得到了以下输出：
- en: '[PRE15]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Summary
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about deep learning and how different it is from
    machine learning. Deep learning refers to a class of machine learning techniques
    that perform unsupervised or supervised feature extraction and pattern analysis
    or classification by exploiting multiple layers of non-linear information processing.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了深度学习以及它与机器学习的不同。深度学习是指一类机器学习技术，通过利用多层非线性信息处理来执行无监督或监督的特征提取、模式分析或分类。
- en: We studied deep feedforward networks, regularization, and optimizing deep learning
    models. We also learned how to create a neural network to classify hand-written
    digits using Mocha in Julia.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了深度前馈网络、正则化和优化深度学习模型。我们还学习了如何使用 Mocha 在 Julia 中创建一个神经网络来分类手写数字。
- en: References
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[http://docs.julialang.org/en/release-0.4/manual/](http://docs.julialang.org/en/release-0.4/manual/)'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://docs.julialang.org/en/release-0.4/manual/](http://docs.julialang.org/en/release-0.4/manual/)'
- en: '[https://github.com/pluskid/Mocha.jl](https://github.com/pluskid/Mocha.jl)'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/pluskid/Mocha.jl](https://github.com/pluskid/Mocha.jl)'
- en: '[http://psych.utoronto.ca/users/reingold/courses/ai/nn.html](http://psych.utoronto.ca/users/reingold/courses/ai/nn.html)'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://psych.utoronto.ca/users/reingold/courses/ai/nn.html](http://psych.utoronto.ca/users/reingold/courses/ai/nn.html)'
- en: '[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DeepLearning-NowPublishing-Vol7-SIG-039.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DeepLearning-NowPublishing-Vol7-SIG-039.pdf)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DeepLearning-NowPublishing-Vol7-SIG-039.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DeepLearning-NowPublishing-Vol7-SIG-039.pdf)'
- en: '[http://www.deeplearningbook.org/contents/intro.html](http://www.deeplearningbook.org/contents/intro.html)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.deeplearningbook.org/contents/intro.html](http://www.deeplearningbook.org/contents/intro.html)'
- en: '[http://deeplearning.net/tutorial/deeplearning.pdf](http://deeplearning.net/tutorial/deeplearning.pdf)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://deeplearning.net/tutorial/deeplearning.pdf](http://deeplearning.net/tutorial/deeplearning.pdf)'
