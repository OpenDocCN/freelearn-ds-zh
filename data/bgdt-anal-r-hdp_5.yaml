- en: Chapter 5. Learning Data Analytics with R and Hadoop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 使用R和Hadoop学习数据分析
- en: In the previous chapters we learned about the installation, configuration, and
    integration of R and Hadoop.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了R和Hadoop的安装、配置和集成。
- en: In this chapter, we will learn how to perform data analytics operations over
    an integrated R and Hadoop environment. Since this chapter is designed for data
    analytics, we will understand this with an effective data analytics cycle.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何在集成的R和Hadoop环境中执行数据分析操作。由于本章旨在介绍数据分析，我们将通过一个有效的数据分析周期来理解这一过程。
- en: 'In this chapter we will learn about:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习：
- en: Understanding the data analytics project life cycle
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据分析项目生命周期
- en: Understanding data analytics problems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据分析问题
- en: Understanding the data analytics project life cycle
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据分析项目生命周期
- en: While dealing with the data analytics projects, there are some fixed tasks that
    should be followed to get the expected output. So here we are going to build a
    data analytics project cycle, which will be a set of standard data-driven processes
    to lead data to insights effectively. The defined data analytics processes of
    a project life cycle should be followed by sequences for effectively achieving
    the goal using input datasets. This data analytics process may include identifying
    the data analytics problems, designing, and collecting datasets, data analytics,
    and data visualization.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据分析项目时，有一些固定任务需要遵循，以便获得预期的结果。因此，我们将构建一个数据分析项目周期，这将是一组标准的数据驱动流程，旨在有效地将数据转化为洞察力。项目生命周期中定义的数据分析流程应按顺序执行，以便有效地使用输入数据集实现目标。这个数据分析过程可能包括识别数据分析问题、设计和收集数据集、数据分析和数据可视化。
- en: 'The data analytics project life cycle stages are seen in the following diagram:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析项目生命周期的各个阶段见下图：
- en: '![Understanding the data analytics project life cycle](img/3282OS_05_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![理解数据分析项目生命周期](img/3282OS_05_01.jpg)'
- en: Let's get some perspective on these stages for performing data analytics.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从这些阶段的角度来看一下如何进行数据分析。
- en: Identifying the problem
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定问题
- en: Today, business analytics trends change by performing data analytics over web
    datasets for growing business. Since their data size is increasing gradually day
    by day, their analytical application needs to be scalable for collecting insights
    from their datasets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，商业分析趋势通过对Web数据集执行数据分析来推动业务增长。由于他们的数据量逐渐增加，他们的分析应用需要具备可扩展性，以便从数据集中提取洞察。
- en: With the help of web analytics, we can solve the business analytics problems.
    Let's assume that we have a large e-commerce website, and we want to know how
    to increase the business. We can identify the important pages of our website by
    categorizing them as per popularity into high, medium, and low. Based on these
    popular pages, their types, their traffic sources, and their content, we will
    be able to decide the roadmap to improve business by improving web traffic, as
    well as content.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 借助Web分析，我们可以解决商业分析问题。假设我们有一个大型电子商务网站，我们希望了解如何增加业务量。我们可以通过按受欢迎程度将网站的重要页面分为高、中、低类别，来识别这些重要页面。根据这些流行页面的类型、流量来源和内容，我们将能够制定出通过提高网站流量和改进内容来改善业务的路线图。
- en: Designing data requirement
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计数据需求
- en: To perform the data analytics for a specific problem, it needs datasets from
    related domains. Based on the domain and problem specification, the data source
    can be decided and based on the problem definition; the data attributes of these
    datasets can be defined.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对特定问题进行数据分析，需要来自相关领域的数据集。根据领域和问题规格，可以决定数据源，并根据问题定义，定义这些数据集的数据属性。
- en: For example, if we are going to perform social media analytics (problem specification),
    we use the data source as Facebook or Twitter. For identifying the user characteristics,
    we need user profile information, likes, and posts as data attributes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们要进行社交媒体分析（问题规格），我们可以使用Facebook或Twitter作为数据源。为了识别用户特征，我们需要用户档案信息、点赞和帖子作为数据属性。
- en: Preprocessing data
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: In data analytics, we do not use the same data sources, data attributes, data
    tools, and algorithms all the time as all of them will not use data in the same
    format. This leads to the performance of data operations, such as data cleansing,
    data aggregation, data augmentation, data sorting, and data formatting, to provide
    the data in a supported format to all the data tools as well as algorithms that
    will be used in the data analytics.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析中，我们并不总是使用相同的数据源、数据属性、数据工具和算法，因为它们并不总是以相同的格式使用数据。这导致需要进行数据操作，例如数据清洗、数据聚合、数据增强、数据排序和数据格式化，以便将数据提供为所有数据工具和算法所支持的格式，这些工具和算法将用于数据分析。
- en: In simple terms, preprocessing is used to perform data operation to translate
    data into a fixed data format before providing data to algorithms or tools. The
    data analytics process will then be initiated with this formatted data as the
    input.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，预处理用于执行数据操作，将数据转换为固定的数据格式，然后将数据提供给算法或工具。数据分析过程将以该格式化数据作为输入开始。
- en: In case of Big Data, the datasets need to be formatted and uploaded to **Hadoop
    Distributed File System** (**HDFS**) and used further by various nodes with Mappers
    and Reducers in Hadoop clusters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据情况下，数据集需要格式化并上传到**Hadoop分布式文件系统**（**HDFS**），并由Hadoop集群中的各种节点使用映射器和减少器进一步处理。
- en: Performing analytics over data
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对数据进行分析
- en: After data is available in the required format for data analytics algorithms,
    data analytics operations will be performed. The data analytics operations are
    performed for discovering meaningful information from data to take better decisions
    towards business with data mining concepts. It may either use descriptive or predictive
    analytics for business intelligence.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在满足数据分析算法所需格式后，将进行数据分析操作。数据分析操作旨在通过数据挖掘概念从数据中发现有意义的信息，以便做出更好的商业决策。这些操作可能会使用描述性分析或预测性分析来进行商业智能。
- en: Analytics can be performed with various machine learning as well as custom algorithmic
    concepts, such as regression, classification, clustering, and model-based recommendation.
    For Big Data, the same algorithms can be translated to MapReduce algorithms for
    running them on Hadoop clusters by translating their data analytics logic to the
    MapReduce job which is to be run over Hadoop clusters. These models need to be
    further evaluated as well as improved by various evaluation stages of machine
    learning concepts. Improved or optimized algorithms can provide better insights.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析可以通过各种机器学习和自定义算法概念进行，例如回归、分类、聚类和基于模型的推荐。对于大数据，相同的算法可以转换为MapReduce算法，在Hadoop集群上运行，通过将它们的数据分析逻辑转化为MapReduce任务，这些任务将运行在Hadoop集群上。这些模型需要通过机器学习概念的各个评估阶段进一步评估和改进。改进或优化后的算法可以提供更好的洞察。
- en: Visualizing data
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据可视化
- en: 'Data visualization is used for displaying the output of data analytics. Visualization
    is an interactive way to represent the data insights. This can be done with various
    data visualization softwares as well as R packages. R has a variety of packages
    for the visualization of datasets. They are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化用于展示数据分析的输出。可视化是一种交互式的方式来展示数据洞察。可以使用各种数据可视化软件以及R包来完成这项工作。R有多种用于数据集可视化的包。如下所示：
- en: '`ggplot2`: This is an implementation of the Grammar of Graphics by *Dr. Hadley
    Wickham* ([http://had.co.nz/](http://had.co.nz/)). For more information refer
    [http://cran.r-project.org/web/packages/ggplot2/](http://cran.r-project.org/web/packages/ggplot2/).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ggplot2`：这是*Dr. Hadley Wickham*（[http://had.co.nz/](http://had.co.nz/)）提出的图形语法的实现。欲了解更多信息，请参阅[http://cran.r-project.org/web/packages/ggplot2/](http://cran.r-project.org/web/packages/ggplot2/)。'
- en: '`rCharts`: This is an R package to create, customize, and publish interactive
    JavaScript visualizations from R by using a familiar lattice-style plotting interface
    by *Markus Gesmann* and *Diego de Castillo*. For more information refer [http://ramnathv.github.io/rCharts/](http://ramnathv.github.io/rCharts/).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rCharts`：这是一个R包，用于通过*Markus Gesmann*和*Diego de Castillo*提供的熟悉的网格式绘图界面，创建、定制并发布交互式JavaScript可视化。欲了解更多信息，请参阅[http://ramnathv.github.io/rCharts/](http://ramnathv.github.io/rCharts/)。'
- en: 'Some popular examples of visualization with R are as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用R进行可视化的一些流行示例：
- en: '**Plots for facet scales** (`ggplot`): The following figure shows the comparison
    of males and females with different measures; namely, education, income, life
    expectancy, and literacy, using `ggplot`:![Visualizing data](img/3282OS_05_20.jpg)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面板比例的图形**（`ggplot`）：下图展示了男性和女性在不同指标下的对比，具体包括教育、收入、预期寿命和识字率，使用`ggplot`：![数据可视化](img/3282OS_05_20.jpg)'
- en: '**Dashboard charts**: This is an `rCharts` type. Using this we can build interactive
    animated dashboards with R.![Visualizing data](img/3282OS_05_21.jpg)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仪表板图表**：这是一种`rCharts`类型。使用它，我们可以使用R构建交互式动画仪表板。![数据可视化](img/3282OS_05_21.jpg)'
- en: Understanding data analytics problems
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据分析问题
- en: In this section, we have included three practical data analytics problems with
    various stages of data-driven activity with R and Hadoop technologies. These data
    analytics problem definitions are designed such that readers can understand how
    Big Data analytics can be done with the analytical power of functions, packages
    of R, and the computational powers of Hadoop.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们包含了三个实际的数据分析问题，涉及使用R和Hadoop技术的各个阶段的基于数据的活动。这些数据分析问题的定义旨在帮助读者理解如何利用R的函数、包以及Hadoop的计算能力进行大数据分析。
- en: 'The data analytics problem definitions are as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析问题的定义如下：
- en: Exploring the categorization of web pages
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索网页分类
- en: Computing the frequency of changes in the stock market
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算股市变化频率
- en: Predicting the sale price of a blue book for bulldozers (case study)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测推土机蓝色书籍的销售价格（案例研究）
- en: Exploring web pages categorization
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索网页分类
- en: This data analytics problem is designed to identify the category of a web page
    of a website, which may categorized popularity wise as high, medium, or low (regular),
    based on the visit count of the pages. While designing the data requirement stage
    of the data analytics life cycle, we will see how to collect these types of data
    from **Google Analytics**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据分析问题的设计目的是识别网站网页的类别，基于页面的访问次数，可以将其按受欢迎程度分为高、中或低（常规）。在设计数据分析生命周期的数据需求阶段时，我们将看到如何从**Google
    Analytics**收集这些类型的数据。
- en: '![Exploring web pages categorization](img/3282OS_05_02.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![探索网页分类](img/3282OS_05_02.jpg)'
- en: Identifying the problem
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定问题
- en: As this is a web analytics problem, the goal of the problem is to identify the
    importance of web pages designed for websites. Based on this information, the
    content, design, or visits of the lower popular pages can be improved or increased.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个网页分析问题，该问题的目标是识别为网站设计的网页的重要性。基于这些信息，可以改善或增加较不受欢迎页面的内容、设计或访问量。
- en: Designing data requirement
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设计数据需求
- en: In this section, we will be working with data requirement as well as data collection
    for this data analytics problem. First let's see how the requirement for data
    can be achieved for this problem.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将处理该数据分析问题的数据需求和数据收集。首先，让我们看看如何为该问题实现数据需求。
- en: Since this is a web analytics problem, we will use Google Analytics data source.
    To retrieve this data from Google Analytics, we need to have an existent Google
    Analytics account with web traffic data stored on it. To increase the popularity,
    we will require the visits information of all of the web pages. Also, there are
    many other attributes available in Google Analytics with respect to dimensions
    and metrics.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个网页分析问题，我们将使用Google Analytics数据源。要从Google Analytics中提取这些数据，我们需要有一个现有的Google
    Analytics账户，并且该账户上存储有网页流量数据。为了增加受欢迎度，我们将需要所有网页的访问信息。此外，Google Analytics中还提供了许多与维度和指标相关的其他属性。
- en: Understanding the required Google Analytics data attributes
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解所需的Google Analytics数据属性
- en: 'The header format of the dataset to be extracted from Google Analytics is as
    follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从Google Analytics提取的数据集的表头格式如下：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`date`: This is the date of the day when the web page was visited'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`date`：这是网页被访问的日期'
- en: '`source`: This is the referral to the web page'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`source`：这是指向网页的推荐链接'
- en: '`pageTitle`: This is the title of the web page'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pageTitle`：这是网页的标题'
- en: '`pagePath`: This is the URL of the web page'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pagePath`：这是网页的URL'
- en: Collecting data
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据收集
- en: As we are going to extract the data from Google Analytics, we need to use `RGoogleAnalytics`,
    which is an R library for extracting Google Analytics datasets within R. To extract
    data, you need this plugin to be installed in R. Then you will be able to use
    its functions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将从 Google Analytics 中提取数据，因此我们需要使用 `RGoogleAnalytics`，这是一个用于在 R 中提取 Google
    Analytics 数据集的 R 库。要提取数据，您需要先在 R 中安装此插件。然后，您将能够使用其函数。
- en: 'The following is the code for the extraction process from Google Analytics:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从 Google Analytics 提取数据的代码：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding file will be available with the chapter contents for download.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 上述文件将在章节内容中提供下载。
- en: Preprocessing data
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Now, we have the raw data for Google Analytics available in a CSV file. We need
    to process this data before providing it to the MapReduce algorithm.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了 Google Analytics 的原始数据，存储在 CSV 文件中。在将数据提供给 MapReduce 算法之前，我们需要处理这些数据。
- en: 'There are two main changes that need to be performed into the dataset:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 需要对数据集进行两项主要更改：
- en: 'Query parameters needs to be removed from the column `pagePath` as follows:'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要从 `pagePath` 列中移除查询参数，如下所示：
- en: '[PRE2]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The new CSV file needs to be created as follows:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要创建新的 CSV 文件，如下所示：
- en: '[PRE3]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Performing analytics over data
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行数据分析
- en: To perform the categorization over website pages, we will build and run the
    MapReduce algorithm with R and Hadoop integration. As already discussed in the
    [Chapter 2](ch02.html "Chapter 2. Writing Hadoop MapReduce Programs"), *Writing
    Hadoop MapReduce Programs*, sometimes we need to use multiple Mappers and Reducers
    for performing data analytics; this means using the chained MapReduce jobs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对网站页面进行分类，我们将构建并运行与 R 和 Hadoop 集成的 MapReduce 算法。正如在[第二章](ch02.html "第二章。编写
    Hadoop MapReduce 程序")《编写 Hadoop MapReduce 程序》中已讨论的那样，*编写 Hadoop MapReduce 程序*，有时我们需要使用多个
    Mappers 和 Reducers 来执行数据分析；这意味着需要使用链式 MapReduce 任务。
- en: 'In case of chaining MapReduce jobs, multiple Mappers and Reducers can communicate
    in such a way that the output of the first job will be assigned to the second
    job as input. The MapReduce execution sequence is described in the following diagram:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在链式 MapReduce 任务中，多个 Mappers 和 Reducers 可以通过某种方式进行通信，使得第一个任务的输出作为第二个任务的输入。MapReduce
    执行顺序如下图所示：
- en: '![Performing analytics over data](img/3282OS_05_03.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![执行数据分析](img/3282OS_05_03.jpg)'
- en: Chaining MapReduce
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 链式 MapReduce
- en: 'Now let''s start with the programming task to perform analytics:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始编程任务以执行分析：
- en: 'Initialize by setting Hadoop variables and loading the `rmr2` and `rhdfs` packages
    of the RHadoop libraries:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过设置 Hadoop 变量并加载 RHadoop 库中的 `rmr2` 和 `rhdfs` 包来初始化：
- en: '[PRE4]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Upload the datasets to HDFS:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上传数据集到 HDFS：
- en: '[PRE5]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now we will see the development of Hadoop MapReduce job 1 for these analytics.
    We will divide this job into Mapper and Reducer. Since, there are two MapReduce
    jobs, there will be two Mappers and Reducers. Also note that here we need to create
    only one file for both the jobs with all Mappers and Reducers. Mapper and Reducer
    will be established by defining their separate functions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到这些分析的 Hadoop MapReduce 任务 1 的开发过程。我们将这个任务分为 Mapper 和 Reducer。由于有两个 MapReduce
    任务，因此将有两个 Mappers 和 Reducers。还需要注意的是，我们只需要为两个任务创建一个文件，包含所有的 Mappers 和 Reducers。Mapper
    和 Reducer 将通过定义它们各自的函数来建立。
- en: Let's see MapReduce job 1.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 MapReduce 任务 1。
- en: '**Mapper 1**: The code for this is as follows:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mapper 1**：代码如下：'
- en: '[PRE6]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Reducer 1**: The code for this is as follows:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reducer 1**：代码如下：'
- en: '[PRE7]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Output of MapReduce job 1**: The intermediate output for the information
    is shown in the following screenshot:![Performing analytics over data](img/3282OS_05_18.jpg)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MapReduce 任务 1 的输出**：信息的中间输出如下截图所示：![执行数据分析](img/3282OS_05_18.jpg)'
- en: The output in the preceding screenshot is only for information about the output
    of this MapReduce job 1\. This can be considered an intermediate output where
    only 100 data rows have been considered from the whole dataset for providing output.
    In these rows, 23 URLs are unique; so the output has provided 23 URLs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图中的输出仅为 MapReduce 任务 1 的输出信息。这可以视为一个中间输出，其中仅考虑了整个数据集中的 100 行数据来提供输出。在这些行中，23
    个 URL 是唯一的；因此，输出提供了 23 个 URL。
- en: 'Let''s see Hadoop MapReduce job 2:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Hadoop MapReduce 任务 2：
- en: '**Mapper 2**: The code for this is as follows:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mapper 2**：代码如下：'
- en: '[PRE8]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Reducer 2**: The code for this is as follows:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reducer 2**：代码如下：'
- en: '[PRE9]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Tip
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Before executing the MapReduce job, please start all the Hadoop daemons and
    check the HDFS connection via the `hdfs.init()` method. If your Hadoop daemons
    have not been started, you can start them by `$hduser@ubuntu :~ $HADOOP_HOME/bin/start-all.sh`.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在执行 MapReduce 作业之前，请启动所有 Hadoop 守护进程，并通过 `hdfs.init()` 方法检查 HDFS 连接。如果您的 Hadoop
    守护进程尚未启动，可以通过 `$hduser@ubuntu :~ $HADOOP_HOME/bin/start-all.sh` 启动它们。
- en: Once we are ready with the logic of the Mapper and Reducer, MapReduce jobs can
    be executed by the MapReduce method of the `rmr2` package. Here we have developed
    multiple MapReduce jobs, so we need to call the `mapreduce` function within the
    `mapreduce` function with the required parameters.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好 Mapper 和 Reducer 的逻辑，就可以通过 `rmr2` 包中的 MapReduce 方法执行 MapReduce 作业。在这里，我们开发了多个
    MapReduce 作业，因此我们需要在 `mapreduce` 函数内调用 `mapreduce` 函数，并传入所需的参数。
- en: 'The command for calling a chained MapReduce job is seen in the following figure:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 调用链式 MapReduce 作业的命令如下图所示：
- en: '![Performing analytics over data](img/3282OS_05_04.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![对数据执行分析](img/3282OS_05_04.jpg)'
- en: 'The following is the command for retrieving the generated output from HDFS:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从 HDFS 检索生成的输出的命令：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: While executing Hadoop MapReduce, the execution log output will be printed over
    the terminal for the purpose of monitoring. We will understand MapReduce job 1
    and MapReduce job 2 by separating them into different parts.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行 Hadoop MapReduce 时，执行日志输出将打印到终端，用于监控目的。我们将通过将 MapReduce 作业 1 和 MapReduce
    作业 2 分成不同的部分来理解它们。
- en: 'The details for MapReduce job 1 is as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 作业 1 的详细信息如下：
- en: '**Tracking the MapReduce job metadata**: With this initial portion of log,
    we can identify the metadata for the Hadoop MapReduce job. We can also track the
    job status with the web browser by calling the given `Tracking URL`.![Performing
    analytics over data](img/3282OS_05_05.jpg)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪 MapReduce 作业元数据**：通过这部分初始日志，我们可以识别 Hadoop MapReduce 作业的元数据。我们还可以通过调用给定的
    `Tracking URL` 使用浏览器跟踪作业状态。![对数据执行分析](img/3282OS_05_05.jpg)'
- en: '**Tracking status of Mapper and Reducer tasks**: With this portion of log,
    we can monitor the status of the Mapper or Reducer task being run on Hadoop cluster
    to get details such as whether it was a success or a failure.![Performing analytics
    over data](img/3282OS_05_06.jpg)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪 Mapper 和 Reducer 任务的状态**：通过这部分日志，我们可以监控在 Hadoop 集群上运行的 Mapper 或 Reducer
    任务的状态，获取任务是否成功或失败等详细信息。![对数据执行分析](img/3282OS_05_06.jpg)'
- en: '**Tracking HDFS output location**: Once the MapReduce job is completed, its
    output location will be displayed at the end of logs.![Performing analytics over
    data](img/3282OS_05_07.jpg)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪 HDFS 输出位置**：一旦 MapReduce 作业完成，其输出位置将在日志末尾显示。![对数据执行分析](img/3282OS_05_07.jpg)'
- en: For MapReduce job 2.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MapReduce 作业 2。
- en: '**Tracking the MapReduce job metadata**: With this initial portion of log,
    we can identify the metadata for the Hadoop MapReduce job. We can also track the
    job status with the web browser by calling the given `Tracking URL`.![Performing
    analytics over data](img/3282OS_05_08.jpg)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪 MapReduce 作业元数据**：通过这部分初始日志，我们可以识别 Hadoop MapReduce 作业的元数据。我们还可以通过调用给定的
    `Tracking URL` 使用浏览器跟踪作业状态。![对数据执行分析](img/3282OS_05_08.jpg)'
- en: '**Tracking status of the Mapper and Reducer tasks**: With this portion of log,
    we can monitor the status of the Mapper or Reducer tasks being run on the Hadoop
    cluster to get the details such as whether it was successful or failed.![Performing
    analytics over data](img/3282OS_05_09.jpg)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪 Mapper 和 Reducer 任务的状态**：通过这部分日志，我们可以监控在 Hadoop 集群上运行的 Mapper 或 Reducer
    任务的状态，获取任务是否成功或失败等详细信息。![对数据执行分析](img/3282OS_05_09.jpg)'
- en: '**Tracking HDFS output location**: Once the MapReduce job is completed, its
    output location will be displayed at the end of the logs.![Performing analytics
    over data](img/3282OS_05_10.jpg)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪 HDFS 输出位置**：一旦 MapReduce 作业完成，其输出位置将在日志末尾显示。![对数据执行分析](img/3282OS_05_10.jpg)'
- en: 'The output of this chained MapReduce job is stored at an HDFS location, which
    can be retrieved by the command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个链式 MapReduce 作业的输出存储在 HDFS 位置，可以通过以下命令检索：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The response to the preceding command is shown in the following figure (output
    only for the top 1000 rows of the dataset):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前述命令的响应如下图所示（仅显示数据集前 1000 行的输出）：
- en: '![Performing analytics over data](img/3282OS_05_11.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![对数据执行分析](img/3282OS_05_11.jpg)'
- en: Visualizing data
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据可视化
- en: We collected the web page categorization output using the three categories.
    I think the best thing we can do is simply list the URLs. But if we have more
    information, such as sources, we can represent the web pages as nodes of a graph,
    colored by popularity with directed edges when users follow the links. This can
    lead to more informative insights.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用三个类别收集了网页分类输出。我认为我们最好的做法是简单地列出这些URL。但如果我们有更多的信息，比如来源，我们可以将网页表示为一个图的节点，通过用户跟随链接的有向边来标记其流行度。这可以带来更多有价值的见解。
- en: Computing the frequency of stock market change
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算股票市场变化的频率
- en: This data analytics MapReduce problem is designed for calculating the frequency
    of stock market changes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据分析MapReduce问题旨在计算股票市场变化的频率。
- en: Identifying the problem
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定问题
- en: Since this is a typical stock market data analytics problem, it will calculate
    the frequency of past changes for one particular symbol of the stock market, such
    as a **Fourier Transformation**. Based on this information, the investor can get
    more insights on changes for different time periods. So the goal of this analytics
    is to calculate the frequencies of percentage change.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个典型的股票市场数据分析问题，它将计算某一特定符号的股票市场过去变动的频率，比如**傅里叶变换**。基于这些信息，投资者可以获得关于不同时间段变化的更多见解。因此，这次分析的目标是计算百分比变化的频率。
- en: '![Identifying the problem](img/3282OS_05_12.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![确定问题](img/3282OS_05_12.jpg)'
- en: Designing data requirement
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设计数据需求
- en: 'For this stock market analytics, we will use Yahoo! Finance as the input dataset.
    We need to retrieve the specific symbol''s stock information. To retrieve this
    data, we will use the Yahoo! API with the following parameters:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本次股票市场分析，我们将使用Yahoo! Finance作为输入数据集。我们需要检索特定符号的股票信息。为此，我们将使用Yahoo! API，传递以下参数：
- en: From month
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从某月开始
- en: From day
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从某天开始
- en: From year
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从某年开始
- en: To month
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到某月
- en: To day
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到某天
- en: To year
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到某年
- en: Symbol
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符号
- en: Tip
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: For more information on this API, visit [http://developer.yahoo.com/finance/](http://developer.yahoo.com/finance/).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此API的更多信息，请访问[http://developer.yahoo.com/finance/](http://developer.yahoo.com/finance/)。
- en: Preprocessing data
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'To perform the analytics over the extracted dataset, we will use R to fire
    the following command:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对提取的数据集进行分析，我们将使用R来执行以下命令：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Or you can also download via the terminal:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 或者你也可以通过终端下载：
- en: '[PRE13]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then upload it to HDFS by creating a specific Hadoop directory for this:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过创建一个特定的Hadoop目录将其上传到HDFS：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Performing analytics over data
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对数据进行分析
- en: To perform the data analytics operations, we will use streaming with R and Hadoop
    (without the `HadoopStreaming` package). So, the development of this MapReduce
    job can be done without any RHadoop integrated library/package.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行数据分析操作，我们将使用R和Hadoop进行流式处理（无需`HadoopStreaming`包）。因此，这个MapReduce作业的开发可以在没有任何RHadoop集成库/包的情况下完成。
- en: In this MapReduce job, we have defined Map and Reduce in different R files to
    be provided to the Hadoop streaming function.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个MapReduce作业中，我们已经在不同的R文件中定义了Map和Reduce，并将它们提供给Hadoop streaming函数。
- en: '**Mapper**: `stock_mapper.R`'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mapper**: `stock_mapper.R`'
- en: '[PRE15]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Reducer**: `stock_reducer.R`'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reducer**: `stock_reducer.R`'
- en: '[PRE16]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: From the following codes, we run MapReduce in R without installing or using
    any R library/package. There is one `system()` method in R to fire the system
    command within R console to help us direct the firing of Hadoop jobs within R.
    It will also provide the repose of the commands into the R console.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下代码中，我们可以在R中运行MapReduce，而无需安装或使用任何R库/包。R中有一个`system()`方法，可以在R控制台内触发系统命令，帮助我们在R中直接启动Hadoop作业。它还会将命令的响应提供到R控制台。
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can also run this same program via the terminal:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过终端运行这个相同的程序：
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: While running this program, the output at your R console or terminal will be
    as given in the following screenshot, and with the help of this we can monitor
    the status of the Hadoop MapReduce job. Here we will see them sequentially with
    the divided parts. Please note that we have separated the logs output into parts
    to help you understand them better.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行该程序时，R控制台或终端的输出将如以下截图所示，通过此输出我们可以监控Hadoop MapReduce作业的状态。这里我们将按顺序查看这些输出的各个部分。请注意，我们已将日志输出分为几个部分，以帮助您更好地理解。
- en: 'The MapReduce log output contains (when run from terminal):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce的日志输出包含（当从终端运行时）：
- en: With this initial portion of log, we can identify the metadata for the Hadoop
    MapReduce job. We can also track the job status with the web browser, by calling
    the given `Tracking URL`. This is how the MapReduce job metadata is tracked.![Performing
    analytics over data](img/3282OS_05_13.jpg)
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用日志的这部分，我们可以识别Hadoop MapReduce作业的元数据。我们还可以通过浏览器通过调用给定的`Tracking URL`来跟踪作业状态。这就是MapReduce作业元数据的跟踪方式。![数据分析](img/3282OS_05_13.jpg)
- en: With this portion of log, we can monitor the status of the Mapper or Reducer
    tasks being run on the Hadoop cluster to get the details like whether it was successful
    or failed. This is how we track the status of the Mapper and Reducer tasks.![Performing
    analytics over data](img/3282OS_05_14.jpg)
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这部分日志，我们可以监控在Hadoop集群上运行的Mapper或Reducer任务的状态，查看是否成功或失败。这就是我们跟踪Mapper和Reducer任务状态的方式。![数据分析](img/3282OS_05_14.jpg)
- en: Once the MapReduce job is completed, its output location will be displayed at
    the end of the logs. This is known as tracking the HDFS output location.![Performing
    analytics over data](img/3282OS_05_15.jpg)
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦MapReduce作业完成，其输出位置将在日志的最后显示。这就是跟踪HDFS输出位置的方式。![数据分析](img/3282OS_05_15.jpg)
- en: 'From the terminal, the output of the Hadoop MapReduce program can be called
    using the following command:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从终端，Hadoop MapReduce程序的输出可以通过以下命令调用：
- en: '[PRE19]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The headers of the output of your MapReduce program will look as follows:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你MapReduce程序输出的表头将如下所示：
- en: '[PRE20]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The following figure shows the sample output of MapReduce problem:![Performing
    analytics over data](img/3282OS_05_16.jpg)
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下图展示了MapReduce问题的示例输出：![数据分析](img/3282OS_05_16.jpg)
- en: Visualizing data
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据可视化
- en: We can get more insights if we visualize our output with various graphs in R.
    Here, we have tried to visualize the output with the help of the `ggplot2` package.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过各种图表在R中可视化我们的输出，我们可以获得更多的见解。在这里，我们通过`ggplot2`包尝试可视化输出。
- en: '![Visualizing data](img/3282OS_05_19.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![数据可视化](img/3282OS_05_19.jpg)'
- en: From the previous graph, we can quickly identify that most of the time the stock
    price has changed from around 0 to 1.5\. So, the stock's price movements in the
    history will be helpful at the time of investing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以迅速识别出，大部分时间股票价格从0附近变动到1.5。因此，股票历史上的价格波动在投资时会很有帮助。
- en: 'The required code for generating this graph is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 生成此图所需的代码如下：
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the next section, we have included the case study on how Big Data analytics
    is performed with R and Hadoop for the **Kaggle** data competition.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将介绍如何使用R和Hadoop进行大数据分析的案例研究，应用于**Kaggle**数据竞赛。
- en: Predicting the sale price of blue book for bulldozers – case study
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测推土机蓝皮书销售价格——案例研究
- en: This is a case study for predicting the auction sale price for a piece of heavy
    equipment to create a blue book for bulldozers.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个案例研究，旨在预测重型设备拍卖销售价格，并为推土机创建蓝皮书。
- en: Identifying the problem
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 识别问题
- en: In this example, I have included a case study by Cloudera data scientists on
    how large datasets can be resampled, and applied the random forest model with
    R and Hadoop. Here, I have considered the Kaggle blue book for bulldozers competition
    for understanding the types of Big Data problem definitions. Here, the goal of
    this competition is to predict the sale price of a particular piece of heavy equipment
    at a usage auction based on its usage, equipment type, and configuration. This
    solution has been provided by *Uri Laserson* (Data Scientist at Cloudera). The
    provided data contains the information about auction result posting, usage, and
    equipment configuration.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我包括了Cloudera数据科学家的一项案例研究，讲述了如何对大数据集进行重采样，并在R和Hadoop中应用随机森林模型。在这里，我参考了Kaggle推土机蓝皮书竞赛来理解大数据问题定义的类型。该竞赛的目标是根据设备的使用情况、设备类型和配置，预测某一特定重型设备在使用拍卖中的销售价格。这个解决方案由*Uri
    Laserson*（Cloudera的数据科学家）提供。提供的数据包含拍卖结果发布、使用情况和设备配置的信息。
- en: 'It''s a trick to model the Big Data sets and divide them into the smaller datasets.
    Fitting the model on that dataset is a traditional machine learning technique
    such as random forests or bagging. There are possibly two reasons for random forests:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个技巧，用于对大数据集建模并将其划分为较小的数据集。将模型应用于该数据集是传统的机器学习技术，如随机森林或集成方法。随机森林可能有两个原因：
- en: Large datasets typically live in a cluster, so any operations will have some
    level of parallelism. Separate models fit on separate nodes that contain different
    subsets of the initial data.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型数据集通常存储在集群中，因此任何操作都将具有一定程度的并行性。不同的模型在不同的节点上拟合，这些节点包含初始数据的不同子集。
- en: Even if you can use the entire initial dataset to fit a single model, it turns
    out that ensemble methods, where you fit multiple smaller models by using subsets
    of data, generally outperform single models. Indeed, fitting a single model with
    100M data points can perform worse than fitting just a few models with 10M data
    points each (so smaller total data outperforms larger total data).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使你可以使用整个初始数据集来拟合单个模型，事实证明，集成方法（通过使用数据子集来拟合多个较小的模型）通常优于单一模型。实际上，用1亿数据点拟合单个模型的效果可能比用每个包含1000万数据点的几个模型要差（即较小的总数据集优于较大的总数据集）。
- en: Sampling with replacement is the most popular method for sampling from the initial
    dataset for producing a collection of samples for model fitting. This method is
    equivalent to sampling from a multinomial distribution, where the probability
    of selecting any individual input data point is uniform over the entire dataset.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 有放回抽样是从初始数据集中采样以生成样本集合用于模型拟合的最常见方法。该方法等同于从多项分布中抽样，其中选择任何单个输入数据点的概率在整个数据集上是均匀的。
- en: Tip
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Kaggle is a Big Data platform where data scientists from all over the world
    compete to solve Big Data analytics problems hosted by data-driven organizations.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是一个大数据平台，来自全球的数据科学家在这里竞赛，解决由数据驱动的组织主办的大数据分析问题。
- en: Designing data requirement
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设计数据需求
- en: For this competition, Kaggle has provided real-world datasets that comprises
    approximately 4,00,000 training data points. Each data point represents the various
    attributes of sales, configuration of the bulldozer, and sale price. To find out
    where to predict the sales price, the random forest regression model needs to
    be implemented.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本次竞赛，Kaggle提供了一个包含约400,000个训练数据点的真实世界数据集。每个数据点代表了推土机销售、配置以及销售价格的各种属性。为了预测销售价格，随机森林回归模型需要被实现。
- en: Note
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The reference link for this Kaggle competition is [http://www.kaggle.com/c/bluebook-for-bulldozers](http://www.kaggle.com/c/bluebook-for-bulldozers).
    You can check the data, information, forum, and leaderboard as well as explore
    some other Big Data analytics competitions and participate in them to evaluate
    your data analytics skills.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 该Kaggle竞赛的参考链接为 [http://www.kaggle.com/c/bluebook-for-bulldozers](http://www.kaggle.com/c/bluebook-for-bulldozers)。你可以查看数据、信息、论坛和排行榜，还可以探索其他大数据分析竞赛并参与其中，以评估你的数据分析技能。
- en: We chose this model because we are interested in predicting the sales price
    in numeric values from random sets of a large dataset.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择这个模型是因为我们有兴趣从一个大型数据集的随机集合中预测销售价格的数值。
- en: 'The datasets are provided in the terms of the following data files:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集以以下数据文件的形式提供：
- en: '| File name | Description format (size) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 文件名 | 描述格式（大小） |'
- en: '| --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `Train` | This is a training set that contains data for 2011. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| `Train` | 这是一个包含2011年数据的训练集。 |'
- en: '| `Valid` | This is a validation set that contains data from January 1, 2012
    to April 30, 2012. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| `Valid` | 这是一个验证集，包含2012年1月1日至2012年4月30日的数据。 |'
- en: '| `Data dictionary` | This is the metadata of the training dataset variables.
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| `Data dictionary` | 这是训练数据集变量的元数据。 |'
- en: '| `Machine_Appendix` | This contains the correct year of manufacturing for
    a given machine along with the make, model, and product class details. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| `Machine_Appendix` | 这包含了给定机器的正确制造年份，以及品牌、型号和产品类别的详细信息。 |'
- en: '| `Test` | This tests datasets. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| `Test` | 这是测试数据集。 |'
- en: '| `random_forest_benchmark_test` | This is the benchmark solution provided
    by the host. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| `random_forest_benchmark_test` | 这是主办方提供的基准解决方案。 |'
- en: Tip
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In case you want to learn and practice Big Data analytics, you can acquire the
    Big Data sets from the Kaggle data source by participating in the Kaggle data
    competitions. These contain the datasets of various fields from industries worldwide.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想学习和实践大数据分析，可以通过参与Kaggle数据竞赛从Kaggle数据源获取大数据集。这些数据集来自世界各地不同行业的多个领域。
- en: Preprocessing data
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理
- en: To perform the analytics over the provided Kaggle datasets, we need to build
    a predictive model. To predict the sale price for the auction, we will fit the
    model over provided datasets. But the datasets are provided with more than one
    file. So we will merge them as well as perform data augmentation for acquiring
    more meaningful data. We are going to build a model from `Train.csv` and `Machine_Appendix.csv`
    for better prediction of the sale price.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对提供的Kaggle数据集执行分析，我们需要构建一个预测模型。为了预测拍卖的销售价格，我们将在提供的数据集上拟合模型。但是数据集提供了多个文件。因此，我们将它们合并，并进行数据增强以获取更有意义的数据。我们将从`Train.csv`和`Machine_Appendix.csv`构建模型，以更好地预测销售价格。
- en: 'Here are the data preprocessing tasks that need to be performed over the datasets:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是需要在数据集上执行的数据预处理任务：
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Performing analytics over data
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在数据上执行分析
- en: As we are going to perform analytics with sampled datasets, we need to understand
    how many datasets need to be sampled.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用采样的数据集执行分析，我们需要了解需要对多少个数据集进行采样。
- en: 'For random sampling, we have considered three model parameters, which are as
    follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机抽样，我们考虑了三个模型参数，如下所示：
- en: We have N data points in our initial training set. This is very large (106-109)
    and is distributed over an HDFS cluster.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在初始训练集中有N个数据点。这非常大（106-109），分布在一个HDFS集群中。
- en: We are going to train a set of M different models for an ensemble classifier.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将为一个集成分类器训练一组M个不同的模型。
- en: Each of the M models will be fitted with K data points, where typically K <<
    N. (For example, K may be 1-10 percent of N.).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个M个模型将使用K个数据点进行拟合，其中通常K << N。（例如，K可能是N的1-10%）。
- en: We have N numbers of training datasets, which are fixed and generally outside
    our control. As we are going to handle this via **Poisson** sampling, we need
    to define the total number of input vectors to be consumed into the random forest
    model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有N个训练数据集，这些数据集是固定的，通常在我们的控制之外。由于我们将通过**泊松**抽样来处理这些数据，我们需要定义要输入随机森林模型的总输入向量数。
- en: 'There are three cases to be considered:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑三种情况：
- en: '**KM < N**: In this case, we are not using the full amount of data available
    to us'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KM < N**：在这种情况下，我们没有使用我们可用的全部数据量。'
- en: '**KM = N**: In this case, we can exactly partition our dataset to produce totally
    independent samples'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KM = N**：在这种情况下，我们可以完全分割我们的数据集，以生成完全独立的样本。'
- en: '**KM > N**: In this case, we must resample some of our data with replacements'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KM > N**：在这种情况下，我们必须用替换的方式对部分数据进行重新抽样。'
- en: The Poisson sampling method described in the following section handles all the
    three cases in the same framework. However, note that for the case KM = N, it
    does not partition the data, but simply resamples it.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中描述的泊松抽样方法在相同的框架内处理所有三种情况。但是请注意，对于KM = N的情况，它不会对数据进行分区，而只是重新抽样。
- en: Understanding Poisson-approximation resampling
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解泊松近似重采样
- en: Generalized linear models are an extension of the general linear model. Poisson
    regression is a situation of generalized models. The dependent variable obeys
    Poisson distribution.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 广义线性模型是一般线性模型的扩展。泊松回归是广义模型的一种情况。因变量服从泊松分布。
- en: Poisson sampling will be run on the Map of the MapReduce task because it occurs
    for input data points. This doesn't guarantee that every data point will be considered
    into the model, which is better than multinomial resampling of full datasets.
    But it will guarantee the generation of independent samples by using N training
    input points.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 泊松抽样将在MapReduce任务的Map上运行，因为它发生在输入数据点上。这并不保证每个数据点都将被考虑到模型中，但它比全数据集的多项式重新抽样更好。但它将通过使用N个训练输入点保证生成独立样本。
- en: 'Here, the following graph indicates the amount of missed datasets that can
    be retrieved in the Poisson sampling with the function of KM/N:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，下图显示了在泊松抽样中可以检索到的遗漏数据集的数量，其功能为KM/N：
- en: '![Understanding Poisson-approximation resampling](img/3282OS_05_17.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![理解泊松近似重采样](img/3282OS_05_17.jpg)'
- en: 'The grey line indicates the value of KM=N. Now, let''s look at the pseudo code
    of the MapReduce algorithm. We have used three parameters: N, M, and K where K
    is fixed. We used T=K/N to eliminate the need for the value of N in advance.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 灰线指示了KM=N的值。现在，让我们看看MapReduce算法的伪代码。我们使用了三个参数：N、M和K，其中K是固定的。我们使用T=K/N来消除先验值N的需求。
- en: '**An example of sampling parameters**: Here, we will implement the preceding
    logic with a pseudo code. We will start by defining two model input parameters
    as `frac.per.model` and `num.models`, where `frac.per.model` is used for defining
    the fraction of the full dataset that can be used, and `num.models` is used for
    defining how many models will be fitted from the dataset.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样参数的示例**：在这里，我们将用伪代码实现前述逻辑。首先，我们将定义两个模型输入参数`frac.per.model`和`num.models`，其中`frac.per.model`用于定义可以使用的完整数据集的比例，而`num.models`用于定义将从数据集中拟合的模型数量。'
- en: '[PRE23]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Logic of Mapper**: Mapper will be designed for generating the samples of
    the full dataset by data wrangling.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mapper的逻辑**：Mapper将被设计为通过数据整理生成完整数据集的样本。'
- en: '[PRE24]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Logic of Reducer**: Reducer will take a data sample as input and fit the
    random forest model over it.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reducer的逻辑**：Reducer将接受一个数据样本作为输入，并对其进行随机森林模型的拟合。'
- en: '[PRE25]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Fitting random forests with RHadoop
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用RHadoop拟合随机森林
- en: In machine learning, fitting a model means fitting the best line into our data.
    Fitting a model can fall under several types, namely, under fitting, over fitting,
    and normal fitting. In case of under and over fitting, there are chances of high
    bias (cross validation and training errors are high) and high variance (cross
    validation error is high but training error is low) effects, which is not good.
    We will normally fit the model over the datasets.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，拟合模型意味着将最佳的线拟合到我们的数据中。拟合模型有几种类型，分别是欠拟合、过拟合和正常拟合。在欠拟合和过拟合的情况下，可能会出现较高的偏差（交叉验证和训练误差高）和较高的方差（交叉验证误差高但训练误差低）的影响，这是不理想的。我们通常会对数据集进行模型拟合。
- en: 'Here are the diagrams for fitting a model over datasets with three types of
    fitting:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是拟合模型的三种拟合类型的示意图：
- en: '**Under fitting**: In this cross validation and training errors are high![Fitting
    random forests with RHadoop](img/3282OS_05_22.jpg)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠拟合**：在这种情况下，交叉验证和训练误差较高![Fitting random forests with RHadoop](img/3282OS_05_22.jpg)'
- en: '**Normal fitting**: In this cross-validation and training errors are normal![Fitting
    random forests with RHadoop](img/3282OS_05_23.jpg)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正常拟合**：在这种情况下，交叉验证和训练误差是正常的![Fitting random forests with RHadoop](img/3282OS_05_23.jpg)'
- en: '**Over fitting**: In this the cross-validation error is high but training error
    is low![Fitting random forests with RHadoop](img/3282OS_05_24.jpg)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：在这种情况下，交叉验证误差较高但训练误差较低![Fitting random forests with RHadoop](img/3282OS_05_24.jpg)'
- en: We will fit the model over the data using the random forest technique of machine
    learning. This is a type of recursive partitioning method, particularly well suited
    for small and large problems. It involves an ensemble (or set) of classification
    (or regression) trees that are calculated on random subsets of the data, using
    a subset of randomly restricted and selected predictors for every split in each
    classification tree.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用机器学习中的随机森林技术对数据进行拟合。这是一种递归划分方法，特别适用于大规模和小规模问题。它涉及一组（或集合）分类（或回归）树，这些树是通过在数据的随机子集上计算得出的，每个分类树的每次分割都使用一个随机选择和限制的预测变量子集。
- en: Furthermore, the results of an ensemble of classification/regression trees have
    been used to produce better predictions instead of using the results of just one
    classification tree.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用分类/回归树集成的结果已经被用于生成比使用单一分类树更好的预测。
- en: 'We will now implement our Poisson sampling strategy with RHadoop. We will start
    by setting global values for our parameters:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用RHadoop实现我们的泊松采样策略。首先，我们会为参数设置全局值：
- en: '[PRE26]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Let's check how to implement Mapper as per the specifications in the pseudo
    code with RHadoop.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查如何按照伪代码的规范，使用RHadoop实现Mapper。
- en: 'Mapper is implemented in the the following manner:'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mapper的实现方式如下：
- en: '[PRE27]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Since we are using R, it's tricky to fit the model with the random forest model
    over the collected sample dataset.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们使用的是R，因此在收集的样本数据集上使用随机森林模型拟合模型是一个复杂的过程。
- en: 'Reducer is implemented in the following manner:'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reducer的实现方式如下：
- en: '[PRE28]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To fit the model, we need `model.formula`, which is as follows:'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了拟合模型，我们需要`model.formula`，其内容如下：
- en: '[PRE29]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`SalePrice` is defined as a response variable and the rest of them are defined
    as predictor variables for the random forest model.'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`SalePrice`定义为响应变量，其他变量定义为随机森林模型的预测变量。'
- en: Tip
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Random forest model with R doesn't support factor with level more than 32.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: R中的随机森林模型不支持具有超过32个级别的因子变量。
- en: 'The MapReduce job can be executed using the following command:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce作业可以通过以下命令执行：
- en: '[PRE30]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The resulting trees are dumped in HDFS at `/poisson/output`.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成的树被导出到HDFS的`/poisson/output`路径下。
- en: 'Finally, we can load the trees, merge them, and use them to classify new test
    points:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们可以加载这些树，合并它们，并用它们来分类新的测试点：
- en: '[PRE31]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Each of the 50 samples produced a random forest with 10 trees, so the final
    random forest is a collection of 500 trees, fitted in a distributed fashion over
    a Hadoop cluster.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 每个50个样本都生成了一个包含10棵树的随机森林，因此最终的随机森林是由500棵树组成的，并且以分布式的方式在Hadoop集群上进行拟合。
- en: Note
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The full set of source files is available on the official Cloudera blog at [http://blog.cloudera.com/blog/2013/02/how-to-resample-from-a-large-data-set-in-parallel-with-r-on-hadoop/](http://blog.cloudera.com/blog/2013/02/how-to-resample-from-a-large-data-set-in-parallel-with-r-on-hadoop/).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 所有源文件的完整集合可以在Cloudera官方网站的博客中找到，链接：[http://blog.cloudera.com/blog/2013/02/how-to-resample-from-a-large-data-set-in-parallel-with-r-on-hadoop/](http://blog.cloudera.com/blog/2013/02/how-to-resample-from-a-large-data-set-in-parallel-with-r-on-hadoop/)。
- en: Hopefully, we have learned a scalable approach for training ensemble classifiers
    or bootstrapping in a parallel fashion by using a Poisson approximation for multinomial
    sampling.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我们已经学习到了一种可扩展的方法，通过使用泊松近似来进行多项式采样，从而以并行方式训练集成分类器或进行自助法（bootstrapping）。
- en: Summary
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to perform Big Data analytics with various data
    driven activities over an R and Hadoop integrated environment.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何在R和Hadoop集成环境中，利用各种数据驱动活动进行大数据分析。
- en: In the next chapter, we will learn more about how R and Hadoop can be used to
    perform machine learning techniques.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习更多关于如何使用R和Hadoop来执行机器学习技术的内容。
