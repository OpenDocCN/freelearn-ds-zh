- en: My Name is Bayes, Naive Bayes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我的名字是贝叶斯，朴素贝叶斯
- en: '"Prediction is very difficult, especially if it''s about the future"'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '"预测是非常困难的，尤其是当它涉及未来时"'
- en: -Niels Bohr
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: -尼尔斯·玻尔
- en: '**Machine learning (ML)** in combination with big data is a radical combination
    that has created some great impacts in the field of research in Academia and Industry.
    Moreover, many research areas are also entering into big data since datasets are
    being generated and produced in an unprecedented way from diverse sources and
    technologies, commonly referred as the **Data Deluge**. This imposes great challenges
    on ML, data analytics tools, and algorithms to find the real **VALUE** out of
    big data criteria such as volume, velocity, and variety. However, making predictions
    from these huge dataset has never been easy.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习（ML）**与大数据的结合是一种革命性的组合，它在学术界和工业界的研究领域产生了巨大的影响。此外，许多研究领域也开始涉及大数据，因为数据集以空前的方式从不同的来源和技术中生成和产生，通常被称为**数据洪流**。这对机器学习、数据分析工具和算法提出了巨大的挑战，需要从大数据的体量、速度和多样性等标准中提取真正的**价值**。然而，从这些庞大的数据集中做出预测从未如此容易。'
- en: 'Considering this challenge, in this chapter we will dig deeper into ML and
    find out how to use a simple yet powerful method to build a scalable classification
    model and even more. In a nutshell, the following topics will be covered throughout
    this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这个挑战，在本章中我们将深入探讨机器学习，并了解如何使用一种简单而强大的方法来构建可扩展的分类模型，甚至更多。简而言之，本章将涵盖以下主题：
- en: Multinomial classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项分类
- en: Bayesian inference
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯推断
- en: Naive Bayes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Decision trees
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Naive Bayes versus decision trees
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯与决策树
- en: Multinomial classification
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项分类
- en: In ML, **multinomial** (also known as multiclass) classification is the task
    of classifying data objects or instances into more than two classes, that is,
    having more than two labels or classes. Classifying data objects or instances
    into two classes is called **binary classification**. More technically, in multinomial
    classification, each training instance belongs to one of N different classes subject
    to `N >=2`. The goal is then to construct a model that correctly predicts the
    classes to which the new instances belong. There may be numerous scenarios having
    multiple categories in which the data points belong. However, if a given point
    belongs to multiple categories, this problem decomposes trivially into a set of
    unlinked binary problems, which can be solved naturally using a binary classification
    algorithm.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，**多项式**（也称为多类）分类是将数据对象或实例分类为两个以上的类别的任务，即拥有两个以上的标签或类别。将数据对象或实例分类为两个类别称为**二元分类**。从技术角度讲，在多项分类中，每个训练实例属于N个不同类别中的一个，其中`N
    >= 2`。目标是构建一个模型，能够正确预测新实例属于哪些类别。在许多场景中，数据点可能属于多个类别。然而，如果一个给定的点属于多个类别，这个问题可以简化为一组不相关的二元问题，这些问题可以自然地使用二元分类算法解决。
- en: Readers are suggested not be confused distinguishing the multiclass classification
    with multilabel classification, where multiple labels are to be predicted for
    each instance. For more on Spark-based implementation for the multilabel classification,
    interested readers should refer to [https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#multilabel-classification](https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#multilabel-classification).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应避免将多类分类与多标签分类混淆，在多标签分类中，每个实例需要预测多个标签。关于基于Spark的多标签分类实现，感兴趣的读者可以参考[https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#multilabel-classification](https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#multilabel-classification)。
- en: 'Multiclass classification techniques can be divided into several categories
    as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 多类别分类技术可以分为以下几类：
- en: Transformation to binary
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换为二进制
- en: Extension from binary
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从二进制扩展
- en: Hierarchical classification
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次分类
- en: Transformation to binary
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换为二进制
- en: Using the transformation to binary technique, a multiclass classification problem
    can be transformed into an equivalent strategy for multiple binary classification
    problems. In other words, this technique can be referred to as a *problem transformation
    techniques*. A detailed discussion from the theoretical and practical perspectives
    is out of the scope of this chapter. Therefore, here we will discuss only one
    example of the problem transformation technique called **One-Vs-The-Rest** (**OVTR**)
    algorithm as the representative of this category.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用二分类技术转换，多类分类问题可以转化为多个二分类问题的等效策略。换句话说，这种技术可以称为 *问题转换技术*。从理论和实践的角度进行详细讨论超出了本章的范围。因此，我们这里只讨论一种问题转换技术的例子，称为
    **一对其余**（**OVTR**）算法，作为该类别的代表。
- en: Classification using One-Vs-The-Rest approach
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用一对其余方法进行分类
- en: In this subsection, we will describe an example of performing multiclass classification
    using the OVTR algorithm by converting the problem into equivalent multiple binary
    classification problems. The OVTR strategy breaks down the problem and trains
    each binary classifier per class. In other words, the OVTR classifier strategy
    consists of fitting one binary classifier per class. It then treats all the samples
    of the current class as positive samples, and consequently other samples of other
    classifiers are treated as negatives samples.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将描述使用 OVTR 算法执行多类分类的例子，方法是将问题转化为多个等效的二分类问题。OVTR 策略将问题拆解并为每个类别训练一个二分类器。换句话说，OVTR
    分类器策略包括为每个类别拟合一个二分类器。然后，它将当前类别的所有样本视为正样本，因此其他分类器的样本视为负样本。
- en: This is a modular machine learning technique no doubt. However, on the downside,
    this strategy requires a base classifier from the multiclass family. The reason
    is that the classifier must produce a real value also called *confidence scores*
    instead of a prediction of the actual labels. The second disadvantage of this
    strategy is that if the dataset (aka training set) contains discrete class labels,
    these eventually lead to vague prediction results. In that case, multiple classes
    can be predicted for a single sample. To make the preceding discussion clearer,
    now let's see an example as follows.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这无疑是一种模块化的机器学习技术。然而，缺点是该策略需要来自多类家族的基础分类器。原因是分类器必须输出一个实值，也叫做 *置信度分数*，而不是预测实际标签。该策略的第二个缺点是，如果数据集（即训练集）包含离散的类标签，最终可能导致模糊的预测结果。在这种情况下，单个样本可能会被预测为多个类别。为了使前面的讨论更清晰，下面我们来看一个例子。
- en: 'Suppose that we have a set of 50 observations divided into three classes. Thus,
    we will use the same logic as before for selecting the negative examples too.
    For the training phase, let''s have the following setting:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组 50 个观测值，分为三个类别。因此，我们将使用与之前相同的逻辑来选择负例。对于训练阶段，我们设定如下：
- en: '**Classifier 1** has 30 positive examples and 20 negative examples'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器 1** 有 30 个正例和 20 个负例'
- en: '**Classifier 2** has 36 positive examples and 14 negative examples'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器 2** 有 36 个正例和 14 个负例'
- en: '**Classifier 3** has 14 positive examples and 24 negative examples'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器 3** 有 14 个正例和 24 个负例'
- en: On the other hand, for the testing phase, suppose I have a new instance that
    need to be classified into one of the previous classes. Each of the three classifiers,
    of course, produces a probability with respect to the estimation This is an estimation
    of how low an instance belongs to the negative or positive examples in the classifier?
    In this case, we should always compare the probabilities of positive class in
    one versus the rest. Now that for *N* classes, we will have *N* probability estimates
    of the positive class for one test sample. Compare them, and whichever probability
    is the maximum of *N* probabilities belongs to that particular class. Spark provides
    multiclass to binary reduction with the OVTR algorithm, where the **Logistic Regression**
    algorithm is used as the base classifier.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对于测试阶段，假设我有一个新实例，需要将其分类到之前的某一类别中。每个分类器当然都会产生一个关于估计的概率。这个估计是指该实例属于分类器中正例或负例的概率有多低？在这种情况下，我们应始终比较一对其余中的正类概率。现在，对于
    *N* 个类别，我们将为每个测试样本获得 *N* 个正类的概率估计。比较它们，最大概率对应的类别即为该样本所属类别。Spark 提供了通过 OVTR 算法将多类问题转换为二分类问题，其中
    **逻辑回归** 算法被用作基础分类器。
- en: Now let's see another example of a real dataset to demonstrate how Spark classifies
    all the features using OVTR algorithm. The OVTR classifier eventually predicts
    handwritten characters from the **Optical Character Reader** (**OCR**) dataset.
    However, before diving into the demonstration, let's explore the OCR dataset first
    to get the exploratory nature of the data. It is to be noted that when OCR software
    first processes a document, it divides the paper or any object into a matrix such
    that each cell in the grid contains a single glyph (also known different graphical
    shapes), which is just an elaborate way of referring to a letter, symbol, or number
    or any contextual information from the paper or the object.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看另一个真实数据集的示例，演示 Spark 如何使用 OVTR 算法对所有特征进行分类。OVTR 分类器最终预测来自 **光学字符识别**
    (**OCR**) 数据集的手写字符。然而，在深入演示之前，我们先来探索一下 OCR 数据集，以了解数据的探索性特征。需要注意的是，当 OCR 软件首次处理文档时，它会将纸张或任何物体划分为一个矩阵，使得网格中的每个单元格都包含一个字形（也称为不同的图形形状），这仅仅是对字母、符号、数字或任何来自纸张或物体的上下文信息的详细描述方式。
- en: To demonstrate the OCR pipeline, let's assume that the document contains only
    alpha characters in English that match glyphs to one of the 26 capital letters,
    that is, *A* to *Z*. We will use the OCR letter dataset from the *UCI Machine
    Learning Data Repository*. The dataset was denoted by W*. Frey* and *D. J. Slate.*
    While exploring the dataset, you should observe 20,000 examples of 26 English
    capital letters. Letter written in capital letters are available as printed using
    20 different, randomly reshaped and distorted black and white fonts as glyphs
    of different shapes. In short, predicting all the characters from 26 alphabets
    turns the problem itself into a multiclass classification problem with 26 classes.
    Consequently, a binary classifier will not be able to serve our purpose.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示 OCR 流水线，假设文档仅包含与 26 个大写字母（即 *A* 到 *Z*）匹配的英文字母字符，我们将使用来自 *UCI 机器学习数据库* 的
    OCR 字母数据集。该数据集由 W* Frey* 和 *D. J. Slate* 提供。在探索数据集时，您应该会看到 20,000 个示例，包含 26 个英文字母的大写字母。大写字母通过
    20 种不同的、随机重塑和扭曲的黑白字体作为图形呈现，具有不同的形状。简而言之，从 26 个字母中预测所有字符将问题本身转化为一个具有 26 个类别的多类分类问题。因此，二分类器将无法达到我们的目的。
- en: '![](img/00362.gif)**Figure 1**: Some of the printed glyphs (Source: Letter
    recognition using Holland-style adaptive classifiers, ML, V. 6, p. 161-182, by
    W. Frey and D.J. Slate [1991])'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00362.gif)**图 1：** 一些打印字形（来源：使用荷兰风格自适应分类器的字母识别，ML，第 6 卷，第 161-182 页，W.
    Frey 和 D.J. Slate [1991]）'
- en: 'The preceding figure shows the images that I explained earlier.*The dataset*
    provides an example of some of the printed glyphs distorted in this way; therefore,
    the letters are computationally challenging for a computer to identify. Yet, these
    glyphs are easily recognized by a human being. The following figure shows the
    statistical attributes of the top 20 rows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了我之前解释过的图像。*数据集* 提供了经过这种方式扭曲的打印字形的示例，因此这些字母对计算机来说具有挑战性，难以识别。然而，人类可以轻松识别这些字形。以下图展示了前
    20 行的统计属性：
- en: '![](img/00020.jpeg)**Figure 2:** The snapshot of the dataset shown as the data
    frame'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00020.jpeg)**图 2：** 数据集的快照，显示为数据框'
- en: Exploration and preparation of the OCR dataset
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OCR 数据集的探索与准备
- en: According to the dataset description, glyphs are scanned using an OCR reader
    on to the computer then they are automatically converted into pixels. Consequently,
    all the 16 statistical attributes (in **figure 2**) are recorded to the computer
    too. The the concentration of black pixels across various areas of the box provide
    a way to differentiate 26 letters using OCR or a machine learning algorithm to
    be trained.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集描述，字形通过 OCR 阅读器扫描到计算机上，然后它们会被自动转换为像素。因此，所有 16 个统计属性（见**图 2**）也会记录到计算机中。黑色像素在框的各个区域中的浓度提供了一种方法，可以通过
    OCR 或经过训练的机器学习算法来区分 26 个字母。
- en: Recall that **support vector machines** (**SVM**), Logistic Regression, Naive
    Bayesian-based classifier, or any other classifier algorithms (along with their
    associated learners) require all the features to be numeric. LIBSVM allows you
    to use a sparse training dataset in an unconventional format. While transforming
    the normal training dataset to the LIBSVM format. Only the nonzero values that
    are also included in the dataset are stored in a sparse array/matrix form. The
    index specifies the column of the instance data (feature index). However, any
    missing data is taken as holding zero value too. The index serves as a way to
    distinguish between the features/parameters. For example, for three features,
    indices 1, 2, and 3 would correspond to the *x*, *y*, and *z* coordinates, respectively.
    The correspondence between the same index values of different data instances is
    merely mathematical when constructing the hyperplane; these serve as coordinates.
    If you skip any index in between, it should be assigned a default value of zero.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，**支持向量机**（**SVM**）、逻辑回归、朴素贝叶斯分类器或任何其他分类器算法（连同它们的学习器）都要求所有特征都是数字格式。LIBSVM
    允许你使用稀疏训练数据集，以非传统格式存储数据。在将正常的训练数据集转换为 LIBSVM 格式时，只有数据集中的非零值才会以稀疏数组/矩阵的形式存储。索引指定实例数据的列（特征索引）。然而，任何缺失的数据也会被视为零值。索引用于区分不同的特征/参数。例如，对于三个特征，索引
    1、2 和 3 分别对应于 *x*、*y* 和 *z* 坐标。不同数据实例中相同索引值之间的对应关系仅在构造超平面时才是数学上的；这些值作为坐标。如果跳过了中间的任何索引，它应被默认赋值为零。
- en: 'In most practical cases, we might need to normalize the data against all the
    features points. In short, we need to convert the current tab-separated OCR data
    into LIBSVM format to make the training step easier. Thus, I''m assuming you have
    downloaded the data and converted into LIBSVM format using their own script. The
    resulting dataset that is transformed into LIBSVM format consisting of labels
    and features is shown in the following figure:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实际情况下，我们可能需要对所有特征点进行数据归一化。简而言之，我们需要将当前的制表符分隔 OCR 数据转换为 LIBSVM 格式，以便简化训练步骤。因此，我假设你已经下载了数据并使用他们的脚本转换为
    LIBSVM 格式。转换为 LIBSVM 格式后，数据集包含标签和特征，如下图所示：
- en: '![](img/00223.gif)**Figure 3:** A snapshot of 20 rows of the OCR dataset in
    LIBSVM format'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00223.gif)**图 3：** LIBSVM 格式的 OCR 数据集 20 行快照'
- en: 'Interested readers can refer to the following research article for gaining
    in-depth knowledge: *Chih-Chung Chang* and *Chih-Jen Lin*, *LIBSVM: a library
    for support vector machines*, *ACM Transactions on Intelligent Systems and Technology*,
    2:27:1--27:27, 2011\. You can also refer to a public script provided on my GitHub
    repository at [https://github.com/rezacsedu/RandomForestSpark/](https://github.com/rezacsedu/RandomForestSpark/)
    that directly converts the OCR data in CSV into LIBSVM format. I read the data
    about all the letters and assigned a unique numeric value to each. All you need
    is to show the input and output file path and run the script.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者可以参考以下研究文章以深入了解：*Chih-Chung Chang* 和 *Chih-Jen Lin*，*LIBSVM：一个支持向量机库*，*ACM
    Intelligent Systems and Technology Transactions*，2:27:1--27:27，2011。你还可以参考我在 GitHub
    仓库提供的公共脚本，地址是 [https://github.com/rezacsedu/RandomForestSpark/](https://github.com/rezacsedu/RandomForestSpark/)，该脚本可以将
    CSV 中的 OCR 数据直接转换为 LIBSVM 格式。我读取了所有字母的数据，并为每个字母分配了唯一的数字值。你只需要显示输入和输出文件路径，并运行该脚本。
- en: Now let's dive into the example. The example that I will be demonstrating has
    11 steps including data parsing, Spark session creation, model building, and model
    evaluation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解这个例子。我将演示的例子包含 11 个步骤，包括数据解析、Spark 会话创建、模型构建和模型评估。
- en: '**Step 1\. Creating Spark session** - Create a Spark session by specifying
    master URL, Spark SQL warehouse, and application name as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1\. 创建 Spark 会话** - 通过指定主节点 URL、Spark SQL 仓库和应用名称来创建一个 Spark 会话，如下所示：'
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2\. Loading, parsing, and creating the data frame** - Load the data
    file from the HDFS or local disk and create a data frame, and finally show the
    data frame structure as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2\. 加载、解析和创建数据框** - 从 HDFS 或本地磁盘加载数据文件并创建数据框，最后显示数据框的结构，如下所示：'
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step 3\. Generating training and test set to train the model** - Let''s generate
    the training and test set by splitting 70% for training and 30% for the test:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3\. 生成训练集和测试集以训练模型** - 让我们通过将 70% 用于训练，30% 用于测试来生成训练集和测试集：'
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Step 4\. Instantiate the base classifier** - Here the base classifier acts
    as the multiclass classifier. For this case, it is the Logistic Regression algorithm
    that can be instantiated by specifying parameters such as the number of max iterations,
    tolerance, regression parameter, and Elastic Net parameters.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4\. 实例化基础分类器** - 在这里，基础分类器充当多分类分类器。在本例中，它是逻辑回归算法，可以通过指定最大迭代次数、容忍度、回归参数和弹性网络参数等参数进行实例化。'
- en: Note that Logistic Regression is an appropriate regression analysis to conduct
    when the dependent variable is dichotomous (binary). Like all regression analyses,
    Logistic Regression is a predictive analysis. Logistic regression is used to describe
    data and to explain the relationship between one dependent binary variable and
    one or more nominal, ordinal, interval, or ratio level independent variables.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当因变量是二项（即二元）时，逻辑回归是一种适合进行回归分析的方法。像所有回归分析一样，逻辑回归是一种预测分析。逻辑回归用于描述数据并解释一个二元因变量与一个或多个名义、顺序、区间或比率水平自变量之间的关系。
- en: For a a Spark-based implementation of the Logistic Regression algorithm, interested
    readers can refer to [https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression](https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于 Spark 的逻辑回归算法实现，有兴趣的读者可以参考[https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression](https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression)。
- en: 'In brief, the following parameters are used to training a Logistic Regression
    classifier:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，训练逻辑回归分类器时使用了以下参数：
- en: '`MaxIter`: This specifies the number of maximum iterations. In general, more
    is better.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MaxIter`：这指定最大迭代次数。通常，次数越多越好。'
- en: '`Tol`: This is the tolerance for the stopping criteria. In general, less is
    better, which helps the model to be trained more intensively. The default value
    is 1E-4.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tol`：这是停止标准的容忍度。通常，值越小越好，这有助于模型进行更密集的训练。默认值为 1E-4。'
- en: '`FirIntercept`: This signifies if you want to intercept the decision function
    while generating the probabilistic interpretation.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FirIntercept`：表示你是否希望在生成概率解释时拦截决策函数。'
- en: '`Standardization`: This signifies a Boolean value depending upon if would like
    to standardize the training or not.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Standardization`：这表示一个布尔值，决定是否希望标准化训练数据。'
- en: '`AggregationDepth`: More is better.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AggregationDepth`：越大越好。'
- en: '`RegParam`: This signifies the regression params. Less is better for most cases.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RegParam`：这表示回归参数。大多数情况下，值越小越好。'
- en: '`ElasticNetParam`: This signifies more advanced regression params. Less is
    better for most cases.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ElasticNetParam`：这表示更高级的回归参数。大多数情况下，值越小越好。'
- en: 'Nevertheless, you can specify the fitting intercept as a `Boolean` value as
    true or false depending upon your problem type and dataset properties:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可以根据问题类型和数据集特性指定拟合截距的`Boolean`值为真或假：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 5\. Instantiate the OVTR classifier** - Now instantiate an OVTR classifier
    to convert the multiclass classification problem into multiple binary classifications
    as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5\. 实例化 OVTR 分类器** - 现在实例化一个 OVTR 分类器，将多分类问题转化为多个二分类问题，如下所示：'
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here `classifier` is the Logistic Regression estimator. Now it's time to train
    the model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`classifier`是逻辑回归估计器。现在是时候训练模型了。
- en: '**Step 6\. Train the multiclass model** - Let''s train the model using the
    training set as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 6\. 训练多分类模型** - 让我们使用训练集来训练模型，如下所示：'
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Step 7\. Score the model on the test set** - We can score the model on test
    data using the transformer (that is, `ovrModel`) as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 7\. 在测试集上评估模型** - 我们可以使用转换器（即`ovrModel`）在测试数据上对模型进行评分，如下所示：'
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Step 8\. Evaluate the model** - In this step, we will predict the labels
    for the characters in the first column. But before that we need instantiate an
    `evaluator` to compute the classification performance metrics such as accuracy,
    precision, recall, and `f1` measure as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 8\. 评估模型** - 在这一步，我们将预测第一列字符的标签。但在此之前，我们需要实例化一个`evaluator`来计算分类性能指标，如准确率、精确度、召回率和`f1`值，具体如下：'
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Step 9\. Compute performance metrics** - Compute the classification accuracy,
    precision, recall, `f1` measure, and error on test data as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 9\. 计算性能指标** - 计算测试数据集上的分类准确率、精确度、召回率、`f1`值和错误率，如下所示：'
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Step 10.** Print the performance metrics:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 10.** 打印性能指标：'
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You should observe the value as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该观察如下值：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Step 11.** Stop the Spark session:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 11.** 停止 Spark 会话：'
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This way, we can convert a multinomial classification problem into multiple
    binary classifications problem without sacrificing the problem types. However,
    from step 10, we can observe that the classification accuracy is not good at all.
    It might be because of several reasons such as the nature of the dataset we used
    to train the model. Also even more importantly, we did not tune the hyperparameters
    while training the Logistic Regression model. Moreover, while performing the transformation,
    the OVTR had to sacrifice some accuracy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以将一个多项式分类问题转换为多个二分类问题，而不会牺牲问题类型。然而，从第10步开始，我们可以观察到分类准确率并不理想。这可能是由多个原因造成的，例如我们用于训练模型的数据集的性质。此外，更重要的是，在训练逻辑回归模型时我们并没有调整超参数。而且，在进行转换时，OVTR不得不牺牲一些准确性。
- en: Hierarchical classification
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次分类
- en: In a hierarchical classification task, the classification problem can be resolved
    by dividing the output space into a tree. In that tree, parent nodes are divided
    into multiple child nodes. The process persists until each child node depicts
    a single class. Several methods have been proposed based on the hierarchical classification
    technique. Computer vision is an example of such areas where recognizing pictures
    or written text are something that use hierarchical processing does. An extensive
    discussion on this classifier is out of the scope of this chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在层次分类任务中，分类问题可以通过将输出空间划分为树的方式来解决。在这棵树中，父节点被划分为多个子节点。这个过程一直持续，直到每个子节点代表一个单一的类别。基于层次分类技术，已经提出了几种方法。计算机视觉就是一个典型的例子，其中识别图片或书写文本是使用层次处理的应用。关于这种分类器的详细讨论超出了本章的范围。
- en: Extension from binary
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从二分类到多分类的扩展
- en: This is a technique for extending existing binary classifiers to solve multiclass
    classification problems. To address multiclass classification problems, several
    algorithms have been proposed and developed based on neural networks, DTs, Random
    forest, k-nearest neighbors, Naive Bayes, and SVM. In the following sections,
    we will discuss the Naive Bayes and the DT algorithm as two representatives of
    this category.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种将现有的二分类器扩展到多类分类问题的技术。为了解决多类分类问题，基于神经网络、决策树（DT）、随机森林、k近邻、朴素贝叶斯和支持向量机（SVM）等算法已经提出并开发出来。在接下来的章节中，我们将讨论朴素贝叶斯和决策树算法，作为该类别的两个代表。
- en: Now, before starting to solve multiclass classification problems using Naive
    Bayes algorithms, let's have a brief overview of Bayesian inference in the next
    section.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在使用朴素贝叶斯算法解决多类分类问题之前，让我们在下一节中简要回顾一下贝叶斯推理。
- en: Bayesian inference
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯推理
- en: In this section, we will briefly discuss **Bayesian inference** (**BI**) and
    its underlying theory. Readers will be familiar with this concept from the theoretical
    and computational viewpoints.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将简要讨论**贝叶斯推理**（**BI**）及其基础理论。读者将从理论和计算的角度了解这一概念。
- en: An overview of Bayesian inference
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯推理概述
- en: Bayesian inference is a statistical method based on Bayes theorem. It is used
    to update the probability of a hypothesis (as a strong statistical proof) so that
    statistical models can repeatedly update towards more accurate learning. In other
    words, all types of uncertainty are revealed in terms of statistical probability
    in the Bayesian inference approach. This is an important technique in theoretical
    as well as mathematical statistics. We will discuss the Bayes theorem broadly
    in a later section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯推理是一种基于贝叶斯定理的统计方法。它用于更新假设的概率（作为强有力的统计证据），以便统计模型能够不断更新，朝着更准确的学习方向发展。换句话说，所有类型的不确定性都以统计概率的形式在贝叶斯推理方法中揭示出来。这是理论和数学统计中的一个重要技术。我们将在后续章节中广泛讨论贝叶斯定理。
- en: Furthermore, Bayesian updating is predominantly foremost in the incremental
    learning and dynamic analysis of the sequence of the dataset. For example time
    series analysis, genome sequencing in biomedical data analytics, science, engineering,
    philosophy, and law are some example where Bayesian inference is used widely.
    From the philosophical perspective and decision theory, Bayesian inference is
    strongly correlated to predictive probability. This theory, however, is more formally
    known as the **Bayesian probability**.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，贝叶斯更新在数据集序列的增量学习和动态分析中占据主导地位。例如，时间序列分析、生物医学数据分析中的基因组测序、科学、工程、哲学和法律等领域广泛应用贝叶斯推理。从哲学视角和决策理论看，贝叶斯推理与预测概率密切相关。然而，这一理论更正式的名称是**贝叶斯概率**。
- en: What is inference?
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是推理？
- en: Inference or model evaluation is the process of updating probabilities of the
    denouement derived from the model at the end. As a result, all the probabilistic
    evidence is eventually known against the observation at hand so that observations
    can be updated while using the Bayesian model for classification analysis. Later
    on, this information is fetched to the Bayesian model by instantiating the consistency
    against all the observations in the dataset. The rules that are fetched to the
    model are referred to as prior probabilities where a probability is assessed before
    making reference to certain relevant observations, especially subjectively or
    on the assumption that all possible outcomes be given the same probability. Then
    beliefs are computed when all the evidence is known as posterior probabilities.
    These posterior probabilities reflect the levels of hypothesis computed based
    on updated evidence.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 推理或模型评估是更新从模型得出的结局概率的过程。最终，所有的概率证据都将与当前的观察结果对比，以便在使用贝叶斯模型进行分类分析时可以更新观察结果。之后，这些信息通过对数据集中所有观察结果的一致性实例化被传回贝叶斯模型。传送到模型的规则被称为先验概率，这些概率是在引用某些相关观察结果之前评估的，特别是主观地或者假设所有可能的结果具有相同的概率。然后，当所有证据都已知时，计算出信念，这就是后验概率。这些后验概率反映了基于更新证据计算的假设水平。
- en: The Bayes theorem is used to compute the posterior probabilities that signify
    a consequence of two antecedents. Based on these antecedents, a prior probability
    and a likelihood function are derived from a statistical model for the new data
    for model adaptability. We will further discuss the Bayes theorem in a later section.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理用于计算后验概率，这些概率表示两个前提的结果。基于这些前提，从统计模型中推导出先验概率和似然函数，用于新数据的模型适应性。我们将在后续章节进一步讨论贝叶斯定理。
- en: How does it work?
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: Here we discuss a general setup for a statistical inference problem. At the
    first place, from the data, we estimate the desired quantity and there might be
    unknown quantities too that we would like to estimate. It could be simply a response
    variable or predicted variable, a class, a label, or simply a number. If you are
    familiar with the *frequentist* approach, you might know that in this approach
    the unknown quantity say `θ` is assumed to be a fixed (nonrandom) quantity that
    is to be estimated by the observed data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论了一个统计推理问题的一般设置。首先，从数据中，我们估计所需的数量，可能也有一些我们希望估计的未知量。它可能只是一个响应变量或预测变量，一个类别，一个标签，或仅仅是一个数字。如果你熟悉*频率派*方法，你可能知道，在这种方法中，未知量，比如`θ`，被假定为一个固定的（非随机）量，应该通过观察到的数据来估计。
- en: However, in the Bayesian framework, an unknown quantity say `θ` is treated as
    a random variable. More specifically, it is assumed that we have an initial guess
    about the distribution of `θ`, which is commonly referred to as the **prior distribution**.
    Now, after observing some data, the distribution of `θ` is updated. This step
    is usually performed using Bayes' rule (for more details, refer to the next section).
    This is why this approach is called the Bayesian approach. However, in short,
    from the prior distribution, we can compute predictive distributions for future
    observations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在贝叶斯框架中，未知量，比如`θ`，被视为一个随机变量。更具体地说，假设我们对`θ`的分布有一个初步的猜测，这通常被称为**先验分布**。现在，在观察到一些数据后，`θ`的分布被更新。这个步骤通常是使用贝叶斯规则执行的（更多细节请参见下一节）。这就是为什么这种方法被称为贝叶斯方法的原因。简而言之，从先验分布中，我们可以计算出对未来观察结果的预测分布。
- en: This unpretentious process can be justified as the appropriate methodology to
    uncertain inference with the help of numerous arguments. However, the consistency
    is maintained with the clear principles of the rationality of these arguments.
    In spite of this strong mathematical evidence, many machine learning practitioners
    are uncomfortable with, and a bit reluctant of, using the Bayesian approach. The
    reason behind this is that often they view the selection of a posterior probability
    or prior as being arbitrary and subjective; however, in reality, this is subjective
    but not arbitrary.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个不起眼的过程可以通过大量论证证明是处理不确定推断的合适方法。然而，保持一致性的是这些论证的理性原则。尽管有强有力的数学证据，许多机器学习从业者对于使用贝叶斯方法感到不适，甚至有些不情愿。其背后的原因是，他们通常认为选择后验概率或先验概率是任意且主观的；然而，实际上，这种选择虽然主观，但并非任意的。
- en: Inappropriately, many Bayesians don't really think in true Bayesian terms. One
    can, therefore, find many pseudo-Bayesian procedures in the literature, in which
    models and priors are used that cannot be taken seriously as expressions of prior
    belief. There may also be computational difficulties with the Bayesian approach.
    Many of these can be addressed using **Markov chain Monte Carlo** methods, which
    are another main focus of my research. The details of this approach will be clearer
    as you go through this chapter.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 不恰当地，许多贝叶斯学派的人并没有真正用贝叶斯的思想来思考。因此，在文献中可以找到许多伪贝叶斯方法，其中使用的模型和先验并不能被严肃地视为先验信念的表达。贝叶斯方法也可能会遇到计算困难。许多这些问题可以通过**马尔可夫链蒙特卡洛**方法来解决，而这也是我的研究重点之一。随着你阅读本章，关于该方法的细节会更加清晰。
- en: Naive Bayes
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: In ML, **Naive Bayes** (**NB**) is an example of the probabilistic classifier
    based on the well-known Bayes' theorem with strong independence assumptions between
    the features. We will discuss Naive Bayes in detail in this section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，**朴素贝叶斯**（**NB**）是基于著名的贝叶斯定理的概率分类器示例，其假设特征之间具有强独立性。在本节中，我们将详细讨论朴素贝叶斯。
- en: An overview of Bayes' theorem
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯定理概览
- en: In probability theory, **Bayes' theorem** describes the probability of an event
    based on a prior knowledge of conditions that is related to that certain event.
    This is a theorem of probability originally stated by the Reverend Thomas Bayes.
    In other words, it can be seen as a way of understanding how the probability theory
    is true and affected by a new piece of information. For example, if cancer is
    related to age, the information about *age* can be used to assess the probability
    that a person might have cancer more accurately*.*
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率论中，**贝叶斯定理**描述了基于与某一事件相关的先验知识来计算事件发生的概率。这是一个由托马斯·贝叶斯牧师最早提出的概率定理。换句话说，它可以被看作是理解概率理论如何被新的信息所影响的方式。例如，如果癌症与年龄相关，关于*年龄*的信息可以被用来更准确地评估某人可能患癌的概率*。*
- en: 'Bayes'' theorem is stated mathematically as the following equation:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理的数学表达式如下：
- en: '![](img/00241.gif)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00241.gif)'
- en: 'In the preceding equation, *A* and *B* are events with *P (B) ≠ 0,* and the
    other terms can be described as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，*A* 和 *B* 是事件，且满足 *P (B) ≠ 0*，其他项可以描述如下：
- en: '*P*(*A*) and *P*(*B*) are the probabilities of observing *A* and *B* without
    regard to each other (that is, independence)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*A*) 和 *P*(*B*) 是观察到 *A* 和 *B* 的概率，彼此之间不考虑相关性（即独立性）'
- en: '*P*(*A* | *B*) is the conditional probability of observing event *A* given
    that *B* is true'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*A* | *B*) 是在已知*B*为真时观察事件*A*的条件概率'
- en: '*P*(*B*| *A*) is the conditional probability of observing event *B* given that
    *A* is true'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*B*| *A*) 是在已知 *A* 为真时观察事件 *B* 的条件概率'
- en: 'As you probably know, a well-known Harvard study shows that only 10% of happy
    people are rich. However, you might think that this statistic is very compelling
    but you might be somewhat interested in knowing the percentage of rich people
    are also really happy*.* Bayes'' theorem helps you out on how to calculate this
    reserving statistic using two additional clues:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能知道的那样，一项著名的哈佛研究表明，只有10%的幸福人是富有的。然而，你可能认为这个统计数字非常有说服力，但你也可能会有点兴趣知道，富有的人中有多少人也是真正幸福的*。*
    贝叶斯定理帮助你计算这个反向统计信息，方法是使用两个额外的线索：
- en: The percentage of people overall who are happy, that is, *P(A).*
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总体上，幸福的人的百分比，即*P(A)*。
- en: The percentage of people overall who are rich, that is *P(B).*
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总体上，富有的人的百分比，即*P(B)*。
- en: 'The key idea behind Bayes'' theorem is reversing the statistic considering
    the overall rates**.** Suppose that the following pieces of information are available
    as a prior:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理的关键思想是反转统计，考虑整体比例**。** 假设以下信息作为先验是已知的：
- en: 40% of people are happy and *=> P(A).*
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 40% 的人是快乐的，*=> P(A)。*
- en: 5% of people are rich *=> P(B).*
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5% 的人是富人 *=> P(B).*
- en: 'Now let''s consider that the Harvard study is correct, that is, *P(B|A) = 10%*.
    Now the fraction of rich people who are happy, that is, *P(A | B),* can be calculated
    as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们假设哈佛的研究是正确的，即 *P(B|A) = 10%*。那么富人中快乐的比例，也就是 *P(A | B)，* 可以按如下方式计算：
- en: '*P(A|B) = {P(A)* P(B| A)}/ P(B) = (40%*10%)/5% = 80%*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(A|B) = {P(A)* P(B| A)}/ P(B) = (40%*10%)/5% = 80%*'
- en: 'Consequently, a majority of the people are also happy! Nice. To make it clearer,
    now let''s assume the population of the whole world is 1,000 for simplicity. Then,
    according to our calculation, there are two facts that exist:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，大多数人也是快乐的！很好。为了更清楚地说明，现在假设世界人口为 1000，为了简化计算。然后，根据我们的计算，存在以下两个事实：
- en: 'Fact 1: This tells us 400 people are happy, and the Harvard study tells us
    that 40 of these happy people are also rich.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事实 1：这告诉我们 400 人是快乐的，而哈佛的研究表明，这些快乐的人中有 40 个人也是富人。
- en: 'Fact 2: There are 50 rich people altogether, and so the fraction who are happy
    is 40/50 = 80%.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事实 2：共有 50 个富人，因此其中快乐的比例是 40/50 = 80%。
- en: This proves the Bayes theorem and its effectiveness. However, more comprehensive
    examples can be found at [https://onlinecourses.science.psu.edu/stat414/node/43](https://onlinecourses.science.psu.edu/stat414/node/43).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了贝叶斯定理及其有效性。然而，更全面的示例可以在 [https://onlinecourses.science.psu.edu/stat414/node/43](https://onlinecourses.science.psu.edu/stat414/node/43)
    找到。
- en: My name is Bayes, Naive Bayes
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我的名字是 Bayes，Naive Bayes。
- en: I'm Bayes, Naive Bayes (NB). I'm a successful classifier based upon the principle
    of **maximum a posteriori** (**MAP**). As a classifier, I am highly scalable,
    requiring a number of parameters linear in the number of variables (features/predictors)
    in a learning problem. I have several properties, for example, I am computationally
    faster, if you can hire me to classify something I'm simple to implement, and
    I can work well with high-dimensional datasets. Moreover, I can handle missing
    values in your dataset. Nevertheless, I'm adaptable since the model can be modified
    with new training data without rebuilding the model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我是 Bayes，Naive Bayes（NB）。我是一个成功的分类器，基于 **最大后验**（**MAP**）的原理。作为分类器，我具有高度的可扩展性，需要的参数数目与学习问题中的变量（特征/预测因子）数量成线性关系。我有几个特点，例如，我计算更快，如果你雇佣我来分类，我很容易实现，并且我可以很好地处理高维数据集。此外，我可以处理数据集中的缺失值。尽管如此，我是可适应的，因为模型可以在不重新构建的情况下用新的训练数据进行修改。
- en: In Bayesian statistics, a MAP estimate is an estimate of an unknown quantity
    that equals the mode of the posterior distribution. The MAP estimate can be used
    to obtain a point estimate of an unobserved quantity on the basis of empirical
    data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯统计中，MAP 估计是未知量的估计，它等于后验分布的众数。MAP 估计可以用于基于经验数据获取一个未观察量的点估计。
- en: 'Sounds something similar to James Bond movies? Well, you/we can think a classifer
    as agent 007, right? Just kidding. I believe I am not as the parameters of the
    Naive Bayes classifier such as priori and conditional probabilities are learned
    or rather determined using a deterministic set of steps: this involves two very
    trivial operations that can be blindingly fast on modern computers, that is, counting
    and dividing. There is no *iteration*. There is no *epoch*. There is *no optimization
    of a cost equation* (which can be complex, of cubic order on an average or at
    least of square order complexity). There is no *error back-propagation*. There
    is no operation(s) involving *solving a matrix equation*. These make Naive Bayes
    and its overall training faster.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来像詹姆斯·邦德的电影情节吗？嗯，你/我们可以将分类器看作是 007 特工，对吧？开玩笑的。我相信我并不像 Naive Bayes 分类器的参数，因为它们像先验和条件概率一样，是通过一套确定的步骤来学习或决定的：这涉及到两个非常简单的操作，这在现代计算机上可以非常快速地执行，即计数和除法。没有*迭代*。没有*周期*。没有*代价方程的优化*（代价方程通常比较复杂，平均而言至少是三次方或二次方复杂度）。没有*误差反向传播*。没有涉及*解矩阵方程*的操作。这些使得
    Naive Bayes 及其整体训练更加高效。
- en: 'However, before hiring this agent, you/we can discover his pros and cons so
    that we can use this agent like a trump card by utilizing it''s best only. Well,
    here''s table summarizing the pros and cons of this agent:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在雇用此代理之前，您/我们可以发现它的优缺点，以便我们只利用它的优势像王牌一样使用它。好了，这里有一张表格总结了这个代理的优缺点：
- en: '| **Agent** | **Pros** | **Cons** | **Better at** |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **代理** | **优点** | **缺点** | **更适合** |'
- en: '| **Naive Bayes (NB)** | - Computationally fast- Simple to implement- Works
    well with high dimensions- Can handle missing values- Requires a small amount
    of data to train the model - It is scalable- Is adaptable since the model can
    be modified with new training data without rebuilding the model | - Relies on
    independence assumptions and so performs badly if the assumption does not meet-
    Relatively low accuracy- If you have no occurrences of a class label and a certain
    attribute value together then the frequency-based probability estimate will be
    zero | - When data has lots of missing values- When dependencies of features from
    each other are similar between features- Spam filtering and classification- Classifying
    a news article about technology, politics, sports, and so on.- Text mining |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **Naive Bayes (NB)** | - 计算速度快 - 实现简单 - 对高维度数据效果好 - 可以处理缺失值 - 训练模型所需数据量小
    - 可扩展 - 可适应性强，因为可以通过新增训练数据来修改模型，而无需重新构建模型 | - 依赖于独立性假设，因此如果假设不成立，表现会差 - 准确率较低
    - 如果某个类标签和某个特征值一起没有出现，那么基于频率的概率估计会是零 | - 当数据中有大量缺失值时 - 当特征之间的依赖关系相似时 - 垃圾邮件过滤和分类
    - 对新闻文章进行分类（如技术、政治、体育等） - 文本挖掘 |'
- en: '**Table 1:** Pros and the cons of the Naive Bayes algorithm'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**表格 1：** Naive Bayes 算法的优缺点'
- en: Building a scalable classifier with NB
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 NB 构建可扩展分类器
- en: In this section, we will see a step-by-step example using **Naive Bayes** (**NB**)
    algorithm. As already stated, NB is highly scalable, requiring a number of parameters
    linear in the number of variables (features/predictors) in a learning problem.
    This scalability has enabled the Spark community to make predictive analytics
    on large-scale datasets using this algorithm. The current implementation of NB
    in Spark MLlib supports both the multinomial NB and Bernoulli NB.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过一步一步的示例来展示如何使用 **Naive Bayes**（**NB**）算法。正如前面所述，NB 具有很强的可扩展性，所需的参数数量与学习问题中变量（特征/预测因子）的数量成线性关系。这种可扩展性使得
    Spark 社区能够使用该算法在大规模数据集上进行预测分析。Spark MLlib 中当前的 NB 实现支持多项式 NB 和 Bernoulli NB。
- en: Bernoulli NB is useful if the feature vectors are binary. One application would
    be text classification with a bag of words (BOW) approach. On the other hand,
    multinomial NB is typically used for discrete counts. For example, if we have
    a text classification problem, we can take the idea of Bernoulli trials one step
    further and instead of BOW in a document we can use the frequency count in a document.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征向量是二进制的，Bernoulli NB 是非常有用的。一个应用场景是使用词袋（BOW）方法进行文本分类。另一方面，多项式 NB 通常用于离散计数。例如，如果我们有一个文本分类问题，我们可以将
    Bernoulli 试验的思想进一步拓展，在文档中使用频率计数，而不是词袋（BOW）。
- en: 'In this section, we will see how to predict the digits from the **Pen-Based
    Recognition of Handwritten Digits** dataset by incorporating Spark machine learning
    APIs including Spark MLlib, Spark ML, and Spark SQL:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何通过结合 Spark 机器学习 API（包括 Spark MLlib、Spark ML 和 Spark SQL）来预测 **基于笔迹的手写数字识别**
    数据集中的数字：
- en: '**Step 1\. Data collection, preprocessing, and exploration** - The Pen-based
    recognition of handwritten digits dataset was downloaded from the UCI Machine
    Learning Repository at [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/pendigits.](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/pendigits)
    This dataset was generated after collecting around 250 digit samples each from
    44 writers, correlated to the location of the pen at fixed time intervals of 100
    milliseconds. Each digit was then written inside a 500 x 500 pixel box. Finally,
    those images were scaled to an integer value between 0 and 100 to create consistent
    scaling between each observation. A well-known spatial resampling technique was
    used to obtain 3 and 8 regularly spaced points on an arc trajectory. A sample
    image along with the lines from point to point can be visualized by plotting the
    3 or 8 sampled points based on their (x, y) coordinates; it looks like what is
    shown in the following table:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1. 数据收集、预处理和探索** - 手写数字的基于笔的识别数据集是从 UCI 机器学习库下载的，网址为 [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/pendigits.](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/pendigits)。该数据集是在从
    44 位书写者收集了大约 250 个数字样本后生成的，样本与笔的位置在每隔 100 毫秒的固定时间间隔内进行相关。每个数字都被写在一个 500 x 500
    像素的框内。最后，这些图像被缩放到 0 到 100 之间的整数值，以在每个观测值之间创建一致的缩放。使用了一种著名的空间重采样技术，以获得沿弧轨迹上均匀间隔的
    3 个和 8 个点。可以通过根据（x, y）坐标绘制 3 个或 8 个采样点来可视化一个示例图像以及从点到点的连线；它看起来像下表所示：'
- en: '| Set | ''0'' | ''1'' | ''2'' | ''3'' | ''4'' | ''5'' | ''6'' | ''7'' | ''8''
    | ''9'' | Total |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 集 | ''0'' | ''1'' | ''2'' | ''3'' | ''4'' | ''5'' | ''6'' | ''7'' | ''8''
    | ''9'' | 总计 |'
- en: '| Training | 780 | 779 | 780 | 719 | 780 | 720 | 720 | 778 | 718 | 719 | 7493
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 训练集 | 780 | 779 | 780 | 719 | 780 | 720 | 720 | 778 | 718 | 719 | 7493 |'
- en: '| Test | 363 | 364 | 364 | 336 | 364 | 335 | 336 | 364 | 335 | 336 | 3497 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 363 | 364 | 364 | 336 | 364 | 335 | 336 | 364 | 335 | 336 | 3497 |'
- en: 'Table 2: Number of digits used for the training and the test set'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：用于训练集和测试集的数字数量
- en: As shown in the preceding table, the training set consists of samples written
    by 30 writers and the testing set consists of samples written by 14 writers.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如前表所示，训练集包含 30 位书写者书写的样本，而测试集包含 14 位书写者书写的样本。
- en: '![](img/00130.jpeg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00130.jpeg)'
- en: 'Figure 4: Example of digit 3 and 8 respectively'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：数字 3 和 8 的示例
- en: 'More on this dataset can be found at [http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.names](http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.names).
    A digital representation of a sample snapshot of the dataset is shown in the following
    figure:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 关于该数据集的更多信息可以在 [http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.names](http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.names)
    上找到。数据集的一个样本快照的数字表示如下图所示：
- en: '![](img/00149.gif)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00149.gif)'
- en: 'Figure 5: A snap of the 20 rows of the hand-written digit dataset'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：手写数字数据集的 20 行快照
- en: Now to predict the dependent variable (that is, label) using the independent
    variables (that is, features), we need to train a multiclass classifier since,
    as shown previously, the dataset now has nine classes, that is, nine handwritten
    digits. For the prediction, we will use the Naive Bayes classifier and evaluate
    the model's performance.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了使用独立变量（即特征）预测因变量（即标签），我们需要训练一个多类分类器，因为如前所述，数据集现在有九个类别，即九个手写数字。为了预测，我们将使用朴素贝叶斯分类器并评估模型的性能。
- en: '**Step 2.** Load the required library and packages:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2.** 加载所需的库和包：'
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Step 3.** Create an active Spark session:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3.** 创建一个活跃的 Spark 会话：'
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that here the master URL has been set as `local[*]`, which means all the
    cores of your machine will be used for processing the Spark job. You should set
    SQL warehouse accordingly and other configuration parameter based on the requirements.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里已将主机 URL 设置为 `local[*]`，这意味着您机器的所有核心将用于处理 Spark 任务。您应根据需求相应地设置 SQL 仓库以及其他配置参数。
- en: '**Step 4\. Create the DataFrame** - Load the data stored in LIBSVM format as
    a DataFrame:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4. 创建 DataFrame** - 将存储在 LIBSVM 格式中的数据加载为 DataFrame：'
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For digits classification, the input feature vectors are usually sparse, and
    sparse vectors should be supplied as input to take advantage of sparsity. Since
    the training data is only used once, and moreover the size of the dataset is relatively
    smaller (that is, few MBs), we can cache it if you use the DataFrame more than
    once.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数字分类，输入特征向量通常是稀疏的，应当提供稀疏向量作为输入，以利用稀疏性。由于训练数据只使用一次，而且数据集的大小相对较小（即只有几MB），如果多次使用
    DataFrame，我们可以对其进行缓存。
- en: '**Step 5\. Prepare the training and test set** - Split the data into training
    and test sets (25% held out for testing):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5. 准备训练集和测试集** - 将数据拆分为训练集和测试集（25% 用于测试）：'
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Step 6\. Train the Naive Bayes model** - Train a Naive Bayes model using
    the training set as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 6. 训练朴素贝叶斯模型** - 使用训练集训练朴素贝叶斯模型，方法如下：'
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Step 7\. Calculate the prediction on the test set** - Calculate the prediction
    using the model transformer and finally show the prediction against each label
    as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 7. 计算测试集上的预测结果** - 使用模型转换器计算预测结果，并最终显示每个标签的预测结果，方法如下：'
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/00189.jpeg)**Figure 6:** Prediction against each label (that is, each
    digit)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00189.jpeg)**图 6：** 每个标签（即每个数字）的预测结果'
- en: As you can see in the preceding figure, some labels were predicted accurately
    and some of them were wrongly. Again we need to know the weighted accuracy, precision,
    recall and f1 measures without evaluating the model naively.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的图中看到的，一些标签的预测是准确的，而另一些则是错误的。我们需要了解加权准确率、精确率、召回率和 F1 值，而不是单纯地评估模型。
- en: '**Step 8\. Evaluate the model** - Select the prediction and the true label
    to compute test error and classification performance metrics such as accuracy,
    precision, recall, and f1 measure as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 8. 评估模型** - 选择预测结果和真实标签，计算测试误差和分类性能指标，如准确率、精确率、召回率和 F1 值，方法如下：'
- en: '[PRE18]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Step 9\. Compute the performance metrics** - Compute the classification accuracy,
    precision, recall, f1 measure, and error on test data as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 9. 计算性能指标** - 计算分类准确率、精确率、召回率、F1 值和测试数据上的误差，方法如下：'
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Step 10.** Print the performance metrics:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 10.** 打印性能指标：'
- en: '[PRE20]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should observe values as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该观察到如下值：
- en: '[PRE21]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The performance is not that bad. However, you can still increase the classification
    accuracy by performing hyperparameter tuning. There are further opportunities
    to improve the prediction accuracy by selecting appropriate algorithms (that is,
    classifier or regressor) through cross-validation and train split, which will
    be discussed in the following section.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 性能并不差。然而，你仍然可以通过进行超参数调优来提高分类准确率。通过交叉验证和训练集拆分选择合适的算法（即分类器或回归器），仍然有机会进一步提高预测准确率，这将在下一节中讨论。
- en: Tune me up!
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整我的参数！
- en: You already know my pros and cons, I have a con that is, my classification accuracy
    is relatively low. However, if you tune me up, I can perform much better. Well,
    should we trust Naive Bayes? If so, shouldn't we look at how to increase the prediction
    performance of this guy? Let's say using the WebSpam dataset. At first, we should
    observe the performance of the NB model, and after that we will see how to increase
    the performance using the cross-validation technique.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经知道我的优缺点，我有一个缺点，那就是我的分类准确率相对较低。不过，如果你对我进行调优，我可以表现得更好。嗯，我们应该相信朴素贝叶斯吗？如果相信，难道我们不该看看如何提高这个模型的预测性能吗？我们以
    WebSpam 数据集为例。首先，我们应该观察 NB 模型的性能，然后看看如何通过交叉验证技术提升性能。
- en: 'The WebSpam dataset that downloaded from [http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/webspam_wc_normalized_trigram.svm.bz2](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/webspam_wc_normalized_trigram.svm.bz2)
    contains features and corresponding labels, that is, spam or ham. Therefore, this
    is a supervised machine learning problem, and the task here is to predict whether
    a given message is spam or ham (that is, not spam). The original dataset size
    is 23.5 GB, where the classes are labeled as +1 or -1 (that is, a binary classification
    problem). Later on, we replaced -1 with 0.0 and +1 with 1.0 since Naive Bayes
    does not permit using signed integers. The modified dataset is shown in the following
    figure:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 从[http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/webspam_wc_normalized_trigram.svm.bz2](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/webspam_wc_normalized_trigram.svm.bz2)下载的WebSpam数据集包含特征和相应的标签，即垃圾邮件或正常邮件。因此，这是一个监督学习问题，任务是预测给定消息是否为垃圾邮件或正常邮件（即非垃圾邮件）。原始数据集大小为23.5
    GB，类别标签为+1或-1（即二元分类问题）。后来，由于朴素贝叶斯不允许使用有符号整数，我们将-1替换为0.0，+1替换为1.0。修改后的数据集如下图所示：
- en: '![](img/00054.gif)**Figure 7:** A snapshot of the 20 rows of the WebSpam dataset'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00054.gif)**图 7：** WebSpam数据集的前20行快照'
- en: 'At first, we need to import necessary packages as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要按以下方式导入必要的包：
- en: '[PRE22]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now create the Spark Session as the entry point to the code as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在按以下方式创建Spark会话作为代码的入口点：
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s load the WebSpam dataset and prepare the training set to train the Naive
    Bayes model as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载WebSpam数据集并准备训练集以训练朴素贝叶斯模型，如下所示：
- en: '[PRE24]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the preceding code, setting the seed is required for reproducibility. Now
    let''s make the prediction on the validation set as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，为了可复现性，设置种子是必需的。现在让我们对验证集进行预测，步骤如下：
- en: '[PRE25]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now let''s obtain `evaluator` and compute the classification performance metrics
    like accuracy, precision, recall, and `f1` measure as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们获取`evaluator`并计算分类性能指标，如准确度、精确度、召回率和`f1`度量，如下所示：
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now let''s compute and print the performance metrics:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算并打印性能指标：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should receive the following output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该收到以下输出：
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Although the accuracy is at a satisfactory level, we can further improve it
    by applying the cross-validation technique. The technique goes as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然准确度已经达到了令人满意的水平，但我们可以通过应用交叉验证技术进一步提高它。该技术的步骤如下：
- en: Create a pipeline by chaining an NB estimator as the only stage of the pipeline
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过链式连接一个NB估计器作为管道的唯一阶段来创建一个流水线
- en: Now prepare the param grid for tuning
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在为调整准备参数网格
- en: Perform the 10-fold cross-validation
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行10折交叉验证
- en: Now fit the model using the training set
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在使用训练集拟合模型
- en: Compute the prediction on the validation set
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算验证集上的预测
- en: The first step in model tuning techniques such as cross-validation is pipeline
    creation. A pipeline can be created by chaining a transformer, an estimator, and
    related parameters.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如交叉验证之类的模型调整技术的第一步是创建管道。通过链式连接转换器、估计器和相关参数可以创建管道。
- en: '**Step 1\. Pipeline creation** - Let''s create a Naive Bayes estimator (`nb`
    is an estimator in the following case) and create a pipeline by chaining the estimator
    as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1\. 管道创建** - 让我们创建一个朴素贝叶斯估计器（在以下情况下，`nb`是一个估计器）并通过链式连接估计器创建一个管道：'
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: A pipeline can be considered as the data workflow system for training and prediction
    using the model. ML pipelines provide a uniform set of high-level APIs built on
    top of [DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html)
    that help users create and tune practical machine learning pipelines. DataFrame,
    transformer, estimator, pipeline, and parameter are the five most important components
    in Pipeline creation. For more on Pipeline, interested readers should refer to
    [https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一个管道可以被看作是用于训练和预测的数据工作流系统。ML管道提供了一组统一的高级API，构建在[DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html)之上，帮助用户创建和调优实用的机器学习管道。DataFrame、转换器、估计器、管道和参数是管道创建中最重要的五个组件。有兴趣的读者可以参考[https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html)了解更多关于管道的信息。
- en: In the earlier case, the only stage in our pipeline is an estimator that is
    an algorithm for fitting on a DataFrame to produce a transformer to make sure
    the training is carried out successfully.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的情况下，我们的管道中唯一的阶段是一个用于在DataFrame上拟合以生成转换器以确保成功训练的算法估计器。
- en: '**Step 2\. Creating grid parameters** - Let''s use `ParamGridBuilder` to construct
    a grid of parameters to search over:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2. 创建网格参数** - 让我们使用 `ParamGridBuilder` 构建一个参数网格以进行搜索：'
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**Step 3\. Performing 10-fold cross-validation** - We now treat the pipeline
    as an estimator, wrapping it in a cross-validator instance. This will allow us
    to jointly choose parameters for all Pipeline stages. A `CrossValidator` requires
    an estimator, a set of estimator `ParamMaps`, and an evaluator. Note that the
    evaluator here is a `BinaryClassificationEvaluator`, and its default metric is
    `areaUnderROC`. However, if you use the evaluator as `MultiClassClassificationEvaluator`,
    you will be able to use the other performance metrics as well:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3. 执行 10 折交叉验证** - 现在我们将管道视为一个估计器，并将其包装在交叉验证器实例中。这将允许我们为所有管道阶段共同选择参数。`CrossValidator`
    需要一个估计器、一组估计器 `ParamMaps` 和一个评估器。请注意，这里的评估器是 `BinaryClassificationEvaluator`，其默认指标是
    `areaUnderROC`。但是，如果你使用 `MultiClassClassificationEvaluator` 作为评估器，你还可以使用其他性能指标：'
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Step 4.** Fit the cross-validation model with the training set as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4.** 使用训练集拟合交叉验证模型，如下所示：'
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '**Step 5.** Compute performance as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5.** 如下计算性能：'
- en: '[PRE33]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**Step 6.** Obtain the evaluator, compute the performance metrics, and display
    the results. Now let''s obtain `evaluator` and compute the classification performance
    metrics such as accuracy, precision, recall, and f1 measure. Here `MultiClassClassificationEvaluator`
    will be used for accuracy, precision, recall, and f1 measure:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 6.** 获取评估器，计算性能指标，并显示结果。现在让我们获取`evaluator`并计算分类性能指标，如准确率、精度、召回率和 F1 值。这里将使用
    `MultiClassClassificationEvaluator` 来计算准确率、精度、召回率和 F1 值：'
- en: '[PRE34]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now compute the classification accuracy, precision, recall, f1 measure, and
    error on test data as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在计算分类准确率、精度、召回率、F1 值和测试数据上的误差，如下所示：
- en: '[PRE35]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now let''s print the performance metrics:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打印性能指标：
- en: '[PRE36]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You should now receive the results as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该会收到以下结果：
- en: '[PRE37]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now this is much better compared to the previous one, right? Please note that
    you might receive a slightly different result due to the random split of the dataset
    and your platform.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在与之前的结果相比，效果好多了，对吧？请注意，由于数据集的随机拆分和你使用的平台，你可能会得到稍有不同的结果。
- en: The decision trees
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: In this section, we will discuss the DT algorithm in detail. A comparative analysis
    of Naive Bayes and DT will be discussed too. DTs are commonly considered as a
    supervised learning technique used for solving classification and regression tasks.
    A DT is simply a decision support tool that uses a tree-like graph (or a model
    of decisions) and their possible consequences, including chance event outcomes,
    resource costs, and utility. More technically, each branch in a DT represents
    a possible decision, occurrence, or reaction in terms of statistical probability.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将详细讨论决策树算法。还将讨论朴素贝叶斯和决策树的比较分析。决策树通常被认为是一种监督学习技术，用于解决分类和回归任务。决策树只是一个决策支持工具，使用类似树状的图（或决策模型）及其可能的结果，包括机会事件结果、资源成本和效用。从技术角度讲，决策树中的每一分支代表一个可能的决策、事件或反应，具体体现在统计概率上。
- en: Compared to Naive Bayes, DT is a far more robust classification technique. The
    reason is that at first DT splits the features into training and test set. Then
    it produces a good generalization to infer the predicted labels or classes. Most
    interestingly, DT algorithm can handle both binary and multiclass classification
    problems.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 与朴素贝叶斯相比，决策树（DT）是一种更为强大的分类技术。其原因在于，决策树首先将特征划分为训练集和测试集。然后它会生成一个良好的泛化模型，以推断预测标签或类别。更有趣的是，决策树算法可以处理二分类和多分类问题。
- en: '![](img/00081.jpeg)**Figure 8:** A sample decision tree on the admission test
    dataset using the Rattle package of R'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00081.jpeg)**图 8：** 使用 R 的 Rattle 包在入学测试数据集上生成的决策树示例'
- en: For instance, in the preceding example figure, DTs learn from the admission
    data to approximate a sine curve with a set of `if...else` decision rules. The
    dataset contains the record of each student who applied for admission, say to
    an American university. Each record contains the graduate record exam score, CGPA
    score, and the rank of the column. Now we will have to predict who is competent
    based on these three features (variables). DTs can be used to solve this kind
    of problem after training the DT model and pruning unwanted branches of the tree.
    In general, a deeper tree signifies more complex decision rules and a better fitted
    model. Therefore, the deeper the tree, the more complex the decision rules and
    the more fitted the model.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，在前面的示例图中，DT通过录取数据学习，用一组`if...else`决策规则来逼近正弦曲线。数据集包含每个申请入学的学生的记录，假设是申请美国大学的学生。每条记录包括研究生入学考试分数、CGPA分数和列的排名。现在，我们需要根据这三个特征（变量）预测谁是合格的。DT可以用于解决这种问题，在训练DT模型并修剪掉不需要的树枝后进行预测。通常，树越深，意味着决策规则越复杂，模型的拟合度越好。因此，树越深，决策规则越复杂，模型越拟合。
- en: If you would like to draw the preceding figure, just run my R script, execute
    it on RStudio, and feed the admission dataset. The script and the dataset can
    be found in my GitHub repository at [https://github.com/rezacsedu/AdmissionUsingDecisionTree](https://github.com/rezacsedu/AdmissionUsingDecisionTree).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想绘制前面的图形，只需运行我的R脚本，在RStudio中执行它，并提供录取数据集。脚本和数据集可以在我的GitHub仓库中找到：[https://github.com/rezacsedu/AdmissionUsingDecisionTree](https://github.com/rezacsedu/AdmissionUsingDecisionTree)。
- en: Advantages and disadvantages of using DTs
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DT的优缺点
- en: Before hiring me, you can discover my pros and cons and when I work best from
    Table 3 so that you don't have any late regrets!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在雇佣我之前，你可以从表3中了解我的优缺点以及我最擅长的工作时间，以免你事后后悔！
- en: '| **Agent** | **Pros** | **Cons** | **Better at** |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **代理** | **优点** | **缺点** | **更擅长于** |'
- en: '| **Decision trees (DTs)** | -Simple to implement, train, and interpret-Trees
    can be visualized-Requires little data preparation-Less model building and prediction
    time-Can handle both numeric and categorical data-Possible of validating the model
    using the statistical tests-Robust against noise and missing values-High accuracy
    | -Interpretation is hard with large and complex trees-Duplication may occur within
    the same subtree-Possible issues with diagonal decision boundaries-DT learners
    can create overcomplex trees that do not generalize data well-Sometimes DTs can
    be unstable because of small variants in the data-Learning the DTs itself an NP-complete
    problem (aka. nondeterministic polynomial time -complete problem)-DTs learners
    create biased trees if some classes dominate | -Targeting highly accurate classification-Medical
    diagnosis and prognosis-Credit risk analytics |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| **决策树（DTs）** | - 实现、训练和解释简单 - 树形结构可视化 - 数据准备要求较少 - 较少的模型构建和预测时间 - 能处理数值型和类别型数据
    - 可通过统计检验验证模型 - 对噪声和缺失值具有鲁棒性 - 高准确率 | - 大型和复杂的树难以解释 - 同一子树中可能出现重复 - 可能存在对角线决策边界问题
    - DT学习器可能创建过于复杂的树，无法很好地泛化数据 - 有时DTs可能因数据的小变动而不稳定 - 学习DT本身是一个NP完全问题（即非确定性多项式时间完全问题）
    - 如果某些类别占主导地位，DT学习器会创建有偏的树 | - 目标是高度准确的分类 - 医学诊断和预后 - 信用风险分析 |'
- en: '**Table 3:** Pros and cons of the decision tree'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**表3：** 决策树的优缺点'
- en: Decision tree versus Naive Bayes
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树与朴素贝叶斯比较
- en: As stated in the preceding table, DTs are very easy to understand and debug
    because of their flexibility for training datasets. They will work with both classification
    as well as regression problems.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如前表所述，DT由于其对训练数据集的灵活性，非常易于理解和调试。它们既适用于分类问题，也适用于回归问题。
- en: If you are trying to predict values out of categorical or continuous values,
    DTs will handle both problems. Consequently, if you just have tabular data, feed
    it to the DT and it will build the model toward classifying your data without
    any additional requirement for upfront or manual interventions. In summary, DTs
    are very simple to implement, train, and interpret. With very little data preparation,
    DTs can build the model with much less prediction time. As said earlier, they
    can handle both numeric and categorical data and are very robust against noise
    and missing values. They are very easy to validate the model using statistical
    tests. More interestingly, the constructed trees can be visualized. Overall, they
    provide very high accuracy.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试预测分类值或连续值，决策树（DT）可以同时处理这两种问题。因此，如果你只有表格数据，将其输入到决策树中，它将构建模型来分类你的数据，无需任何额外的前期或手动干预。总之，决策树非常简单，易于实现、训练和解释。只需极少的数据准备，决策树就能在更短的预测时间内构建模型。正如前面所说，它们可以处理数字数据和分类数据，并且对噪声和缺失值非常鲁棒。使用统计测试验证模型也非常简单。更有趣的是，构建的树可以进行可视化。总体而言，决策树提供了非常高的准确性。
- en: However, on the downside, DTs sometimes tend to the overfitting problem for
    the training data. This means that you generally have to prune the tree and find
    an optimal one for better classification or regression accuracy. Moreover, duplication
    may occur within the same subtree. Sometimes it also creates issues with diagonal
    decision boundaries towards overfitting and underfitting. Furthermore, DT learners
    can create over-complex trees that do not generalize the data well this makes
    overall interpretation hard. DTs can be unstable because of small variants in
    the data, and as a result learning DT is itself an NP-complete problem. Finally,
    DT learners create biased trees if some classes dominate over others.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，决策树的缺点是，它们有时会导致训练数据的过拟合问题。这意味着你通常需要修剪树并找到一个最优的树模型，以提高分类或回归的准确性。此外，同一子树中可能会出现重复现象。有时它还会在决策边界上产生斜对角问题，从而导致过拟合和欠拟合的问题。此外，决策树学习器可能会生成过于复杂的树，无法很好地泛化数据，这使得整体解释变得困难。由于数据中的微小变动，决策树可能不稳定，因此学习决策树本身是一个NP完全问题。最后，如果某些类别在数据中占主导地位，决策树学习器可能会生成有偏的树。
- en: Readers are suggested to refer to *Tables 1* and *3* to get a comparative summary
    between Naive Bayes and DTs.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以参考*表1*和*表3*，获取朴素贝叶斯和决策树的对比总结。
- en: 'On the other hand, there is a saying while using Naive Bayes: *NB requires
    you build a classification by hand*. There''s no way to feed a bunch of tabular
    data to it, and it picks the best features for the classification. In this case,
    however, choosing the right features and features that matter is up to the user,
    that is, you. On the other hand, DTs will pick the best features from tabular
    data. Given this fact, you probably need to combine Naive Bayes with other statistical
    techniques to help toward best feature extraction and classify them later on.
    Alternatively, use DTs to get better accuracy in terms of precision, recall, and
    f1 measure. Another positive thing about Naive Bayes is that it will answer as
    a continuous classifier. However, the downside is that they are harder to debug
    and understand. Naive Bayes does quite well when the training data doesn''t have
    good features with low amounts of data.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，使用朴素贝叶斯时有一种说法：*NB要求你手动构建分类器*。你无法直接将一堆表格数据输入它，它不会自动挑选最适合分类的特征。在这种情况下，选择正确的特征以及重要的特征由用户自己决定，也就是你自己。另一方面，决策树会从表格数据中选择最佳特征。鉴于这一点，你可能需要将朴素贝叶斯与其他统计技术结合，以帮助选择最佳特征并在之后进行分类。或者，使用决策树来提高准确性，特别是在精确度、召回率和F1度量方面。另一个关于朴素贝叶斯的优点是，它会作为一个连续的分类器进行输出。然而，缺点是它们更难调试和理解。朴素贝叶斯在训练数据中没有良好的特征且数据量较小时表现得相当不错。
- en: In summary, if you are trying to choose the better classifier from these two
    often times it is best to test each one to solve a problem. My recommendation
    would be to build a DT as well as a Naive Bayes classifier using the training
    data you have and then compare the performance using available performance metrics
    and then decide which one best solves your problem subject to the dataset nature.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，如果你试图从这两种方法中选择一个更好的分类器，通常最好测试每个模型来解决问题。我的建议是，使用你拥有的训练数据构建决策树和朴素贝叶斯分类器，然后使用可用的性能指标比较它们的表现，再根据数据集的特性决定哪一个最适合解决你的问题。
- en: Building a scalable classifier with DT algorithm
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树算法构建可扩展的分类器
- en: 'As you have already seen, using the OVTR classifier we observed the following
    values of the performance metrics on the OCR dataset:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经看到的，使用 OVTR 分类器时，我们在 OCR 数据集上观察到了以下性能指标值：
- en: '[PRE38]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This signifies that the accuracy of the model on that dataset is very low. In
    this section, we will see how we could improve the performance using the DT classifier.
    An example with Spark 2.1.0 will be shown using the same OCR dataset. The example
    will have several steps including data loading, parsing, model training, and,
    finally, model evaluation.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着模型在该数据集上的准确度非常低。在本节中，我们将看到如何通过使用 DT 分类器来提升性能。我们将展示一个使用 Spark 2.1.0 的例子，使用相同的
    OCR 数据集。这个例子将包含多个步骤，包括数据加载、解析、模型训练，最后是模型评估。
- en: 'Since we will be using the same dataset, to avoid redundancy, we will escape
    the dataset exploration step and will enter into the example:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用相同的数据集，为避免冗余，我们将跳过数据集探索步骤，直接进入示例：
- en: '**Step 1.** Load the required library and packages as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1 步：** 加载所需的库和包，如下所示：'
- en: '[PRE39]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Step 2.** Create an active Spark session as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 2 步：** 创建一个活动的 Spark 会话，如下所示：'
- en: '[PRE40]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note that here the master URL has been set as `local[*]`, which means all the
    cores of your machine will be used for processing the Spark job. You should set
    SQL warehouse accordingly and other configuration parameter based on requirements.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里将主 URL 设置为 `local[*]`，意味着你机器的所有核心将用于处理 Spark 作业。你应该根据需求设置 SQL 仓库和其他配置参数。
- en: '**Step 3\. Create the DataFrame** - Load the data stored in LIBSVM format as
    a DataFrame as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 3 步：创建数据框** - 将存储在 LIBSVM 格式中的数据加载为数据框，如下所示：'
- en: '[PRE41]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: For the classification of digits, the input feature vectors are usually sparse,
    and sparse vectors should be supplied as input to take advantage of the sparsity.
    Since the training data is only used once, and moreover the size of the dataset
    is relatively small (that is, a few MBs), we can cache it if you use the DataFrame
    more than once.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数字分类，输入特征向量通常是稀疏的，应该将稀疏向量作为输入，以便利用稀疏性。由于训练数据只使用一次，而且数据集的大小相对较小（即几 MB），如果你多次使用
    DataFrame，可以缓存它。
- en: '**Step 4\. Label indexing** - Index the labels, adding metadata to the label
    column. Then let''s fit on the whole dataset to include all labels in the index:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 4 步：标签索引** - 索引标签，向标签列添加元数据。然后让我们在整个数据集上进行拟合，以便将所有标签包含在索引中：'
- en: '[PRE42]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '**Step 5\. Identifying categorical features** - The following code segment
    automatically identifies categorical features and indexes them:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 5 步：识别分类特征** - 以下代码段自动识别分类特征并对其进行索引：'
- en: '[PRE43]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: For this case, if the number of features is more than four distinct values,
    they will be treated as continuous.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，如果特征的值大于四个不同的值，它们将被视为连续值。
- en: '**Step 6\. Prepare the training and test sets** - Split the data into training
    and test sets (25% held out for testing):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 6 步：准备训练集和测试集** - 将数据分成训练集和测试集（25% 用于测试）：'
- en: '[PRE44]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Step 7.** Train the DT model as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 7 步：** 按如下方式训练 DT 模型：'
- en: '[PRE45]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Step 8.** Convert the indexed labels back to original labels as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 8 步：** 按如下方式将索引标签转换回原始标签：'
- en: '[PRE46]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Step 9\. Create a DT pipeline** - Let''s create a DT pipeline by changing
    the indexers, label converter and tree together:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 9 步：创建 DT 管道** - 让我们通过组合索引器、标签转换器和树来创建一个 DT 管道：'
- en: '[PRE47]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '**Step 10\. Running the indexers** - Train the model using the transformer
    and run the indexers:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 10 步：运行索引器** - 使用变换器训练模型并运行索引器：'
- en: '[PRE48]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '**Step 11\. Calculate the prediction on the test set** - Calculate the prediction
    using the model transformer and finally show the prediction against each label
    as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 11 步：计算测试集上的预测结果** - 使用模型变换器计算预测结果，并最终显示每个标签的预测结果，如下所示：'
- en: '[PRE49]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](img/00344.jpeg)**Figure 9:** Prediction against each label (that is, each
    letter)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00344.jpeg)**图 9：** 针对每个标签的预测（即每个字母）'
- en: As you can see from the preceding figure, some labels were predicted accurately
    and some of them were predicted wrongly. However, we know the weighted accuracy,
    precision, recall, and f1 measures, but we need to evaluate the model first.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，部分标签预测准确，而部分标签预测错误。然而，我们知道加权准确率、精确率、召回率和 F1 值，但我们需要先评估模型。
- en: '**Step 12\. Evaluate the model** - Select the prediction and the true label
    to compute test error and classification performance metrics such as accuracy,
    precision, recall, and f1 measure as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 12 步：评估模型** - 选择预测结果和真实标签来计算测试误差和分类性能指标，如准确率、精确率、召回率和 F1 值，如下所示：'
- en: '[PRE50]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '**Step 13\. Compute the performance metrics** - Compute the classification
    accuracy, precision, recall, f1 measure, and error on test data as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 13.** 计算性能指标 - 计算测试数据上的分类准确率、精确率、召回率、F1值和错误率，如下所示：'
- en: '[PRE51]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '**Step 14.** Print the performance metrics:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 14.** 打印性能指标：'
- en: '[PRE52]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'You should observe values as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该观察到以下值：
- en: '[PRE53]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Now the performance is excellent, right? However, you can still increase the
    classification accuracy by performing hyperparameter tuning. There are further
    opportunities to improve the prediction accuracy by selecting appropriate algorithms
    (that is, classifier or regressor) through cross-validation and train split.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在性能很优秀，对吧？然而，你仍然可以通过执行超参数调整来提高分类准确率。通过交叉验证和训练集划分，选择合适的算法（即分类器或回归器）还有进一步提高预测准确性的机会。
- en: '**Step 15.** Print the DT nodes:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 15.** 打印决策树节点：'
- en: '[PRE54]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Finally, we will print a few nodes in the DT, as shown in the following figure:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将打印决策树中的一些节点，如下图所示：
- en: '![](img/00199.gif)**Figure 10:** A few decision tree nodes that were generated
    during the model building'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00199.gif)**图 10：** 在模型构建过程中生成的一些决策树节点'
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed some advanced algorithms in ML and found out how
    to use a simple yet powerful method of Bayesian inference to build another kind
    of classification model, multinomial classification algorithms. Moreover, the
    Naive Bayes algorithm was discussed broadly from the theoretical and technical
    perspectives. At the last pace, a comparative analysis between the DT and Naive
    Bayes algorithms was discussed and a few guidelines were provided.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们讨论了一些机器学习中的高级算法，并了解了如何使用简单而强大的贝叶斯推断方法来构建另一种分类模型——多项式分类算法。此外，我们还从理论和技术角度广泛讨论了朴素贝叶斯算法。在最后一步，我们讨论了决策树与朴素贝叶斯算法的对比分析，并提供了一些指导方针。
- en: In the next chapter*,* we will dig even deeper into ML and find out how we can
    take advantage of ML to cluster records belonging to a dataset of unsupervised
    observations.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章*中*，我们将深入研究机器学习，探索如何利用机器学习将无监督观察数据集中的记录进行聚类。
