- en: Time to Go to ClusterLand - Deploying Spark on a Cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前往集群之地的时候——在集群上部署Spark
- en: '"I see the moon like a clipped piece of silver. Like gilded bees, the stars
    cluster around her"'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '"我看见月亮像一块剪下的银子。星星像镀金的蜜蜂一样围绕着她"'
- en: '- Oscar Wilde'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 奥斯卡·王尔德'
- en: 'In the previous chapters, we have seen how to develop practical applications
    using different Spark APIs. However, in this chapter, we will see how Spark works
    in a cluster mode with its underlying architecture. Finally, we will see how to
    deploy a full Spark application on a cluster. In a nutshell, the following topics
    will be cover throughout this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们已经看到如何使用不同的Spark API开发实际应用程序。然而，在本章中，我们将看到Spark在集群模式下的工作方式及其底层架构。最后，我们将看到如何在集群上部署完整的Spark应用程序。简而言之，本章将涵盖以下主题：
- en: Spark architecture in a cluster
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中的Spark架构
- en: Spark ecosystem and cluster management
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark生态系统和集群管理
- en: Deploying Spark on a cluster
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群上部署Spark
- en: Deploying Spark on a standalone cluster
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在独立集群上部署Spark
- en: Deploying Spark on a Mesos cluster
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mesos集群上部署Spark
- en: Deploying Spark on YARN cluster
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在YARN集群上部署Spark
- en: Cloud-based deployment
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于云的部署
- en: Deploying Spark on AWS
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在AWS上部署Spark
- en: Spark architecture in a cluster
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群中的Spark架构
- en: Hadoop-based **MapReduce** framework has been widely used for the last few years;
    however, it has some issues with I/O, algorithmic complexity, low-latency streaming
    jobs, and fully disk-based operation. Hadoop provides the **Hadoop Distributed
    File System** (**HDFS**) for efficient computing and storing big data cheaply,
    but you can only do the computations with a high-latency batch model or static
    data using the Hadoop-based MapReduce framework. The main big data paradigm that
    Spark has brought for us is the introduction of in-memory computing and caching
    abstraction. This makes Spark ideal for large-scale data processing and enables
    the computing nodes to perform multiple operations by accessing the same input
    data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Hadoop的MapReduce框架在过去几年被广泛使用；然而，它在I/O、算法复杂性、低延迟流式作业和完全基于磁盘的操作方面存在一些问题。Hadoop提供了Hadoop分布式文件系统（HDFS）来进行高效的计算和廉价存储大数据，但你只能使用基于Hadoop的MapReduce框架进行高延迟批处理模型或静态数据的计算。Spark为我们带来的主要大数据范式是引入了内存计算和缓存抽象。这使得Spark非常适合大规模数据处理，并使计算节点能够通过访问相同的输入数据执行多个操作。
- en: Spark's **Resilient Distributed Dataset** (**RDD**) model can do everything
    that the MapReduce paradigm can, and even more. Nevertheless, Spark can perform
    iterative computations on your dataset at scale. This option helps to execute
    machine learning, general purpose data processing, graph analytics, and **Structured
    Query Language** (**SQL**) algorithms much faster with or without depending upon
    Hadoop. Therefore, reviving the Spark ecosystem is a demand at this point.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的弹性分布式数据集（RDD）模型可以做到MapReduce范式所能做的一切，甚至更多。然而，Spark可以在规模上对数据集进行迭代计算。这个选项有助于以更快的速度执行机器学习、通用数据处理、图分析和结构化查询语言（SQL）算法，无论是否依赖于Hadoop。因此，此时重振Spark生态系统是一个需求。
- en: Enough knowing about Spark's beauties and features. At this point, reviving
    the Spark ecosystem is your demand to know how does Spark work.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 足够了解Spark的美丽和特性。此时，重振Spark生态系统是您了解Spark如何工作的需求。
- en: Spark ecosystem in brief
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark生态系统简介
- en: 'To provide you with more advanced and additional big data processing capabilities,
    your Spark jobs can be running on top of Hadoop-based (aka YARN) or Mesos-based
    clusters. On the other hand, the core APIs in Spark, which is written in Scala,
    enable you to develop your Spark application using several programming languages
    such as Java, Scala, Python, and R. Spark provides several libraries that are
    part of the Spark ecosystems for additional capabilities for general purpose data
    processing and analytics, graph processing, large-scale structured SQL, and **machine
    learning** (**ML**) areas. The Spark ecosystem consists of the following components:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为您提供更先进和额外的大数据处理能力，您的Spark作业可以在基于Hadoop（又名YARN）或基于Mesos的集群上运行。另一方面，Spark中的核心API是用Scala编写的，使您能够使用多种编程语言（如Java、Scala、Python和R）开发您的Spark应用程序。Spark提供了几个库，这些库是Spark生态系统的一部分，用于通用数据处理和分析、图处理、大规模结构化SQL和机器学习（ML）领域的额外功能。Spark生态系统包括以下组件：
- en: '![](img/00161.jpeg)**Figure 1:** Spark ecosystem (up to Spark 2.1.0)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00161.jpeg)**图1：** Spark生态系统（截至Spark 2.1.0）'
- en: 'The core engine of Spark is written in Scala but supports different languages
    to develop your Spark application, such as R, Java, Python, and Scala. The main
    components/APIs in the Spark core engine are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的核心引擎是用Scala编写的，但支持不同的语言来开发您的Spark应用程序，如R、Java、Python和Scala。Spark核心引擎中的主要组件/
    API如下：
- en: '**SparkSQL**: This helps in seamlessly mix SQL queries with Spark programs
    so that you can query structured data inside Spark programs.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SparkSQL：这有助于无缝地将SQL查询与Spark程序混合在一起，以便在Spark程序内查询结构化数据。
- en: '**Spark Streaming**: This is for large-scale streaming application development
    that provides seamless integration of Spark with other streaming data sources
    such as Kafka, Flink, and Twitter.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark Streaming：这是用于大规模流应用程序开发的，提供了与其他流数据源（如Kafka、Flink和Twitter）无缝集成的Spark。
- en: '**SparkMLlib** and **SparKML**: These are for RDD and dataset/DataFrame-based
    machine learning and pipeline creation.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SparkMLlib和SparKML：这些是用于基于RDD和数据集/ DataFrame的机器学习和管道创建。
- en: '**GraphX**: This is for large-scale graph computation and processing to make
    your graph data object fully connected.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GraphX：这是用于大规模图计算和处理，使您的图数据对象完全连接。
- en: '**SparkR**: R on Spark helps in basic statistical computations and machine
    learning.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SparkR：R on Spark有助于基本的统计计算和机器学习。
- en: As we have already stated, it is very much possible to combine these APIs seamlessly
    to develop large-scale machine learning and data analytics applications. Moreover,
    Spark jobs can be submitted and executed through cluster managers such as Hadoop
    YARN, Mesos, and standalone, or in the cloud by accessing data storage and sources
    such as HDFS, Cassandra, HBase, Amazon S3, or even RDBMS. However, to the full
    facility of Spark, we need to deploy our Spark application on a computing cluster.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经提到的，可以无缝地结合这些API来开发大规模的机器学习和数据分析应用程序。此外，Spark作业可以通过Hadoop YARN、Mesos和独立的集群管理器提交和执行，也可以通过访问数据存储和源（如HDFS、Cassandra、HBase、Amazon
    S3甚至RDBMS）在云中执行。然而，要充分利用Spark的功能，我们需要在计算集群上部署我们的Spark应用程序。
- en: Cluster design
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群设计
- en: Apache Spark is a distributed and parallel processing system and it also provides
    in-memory computing capabilities. This type of computing paradigm needs an associated
    storage system so that you can deploy your application on top of a big data cluster.
    To make this happen, you will have to use distributed storage systems such as
    HDFS, S3, HBase, and Hive. For moving data, you will be needing other technologies
    such as Sqoop, Kinesis, Twitter, Flume, and Kafka.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个分布式和并行处理系统，它还提供了内存计算能力。这种类型的计算范式需要一个关联的存储系统，以便您可以在大数据集群上部署您的应用程序。为了实现这一点，您将需要使用HDFS、S3、HBase和Hive等分布式存储系统。为了移动数据，您将需要其他技术，如Sqoop、Kinesis、Twitter、Flume和Kafka。
- en: In practice, you can configure a small Hadoop cluster very easily. You only
    need to have a single master and multiple worker nodes. In your Hadoop cluster,
    generally, a master node consists of **NameNodes**, **DataNodes**, **JobTracker**,
    and **TaskTracker**. A worker node, on the other hand, can be configured so that
    it works both as a DataNode and as a TaskTracker.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，您可以很容易地配置一个小型的Hadoop集群。您只需要一个主节点和多个工作节点。在您的Hadoop集群中，通常一个主节点包括NameNodes、DataNodes、JobTracker和TaskTracker。另一方面，工作节点可以配置为既作为DataNode又作为TaskTracker。
- en: 'For security reasons, most of the big data cluster might set up behind a network
    firewall so that the complexity caused by the firewall can be overcome or at least
    reduced by the computing nodes. Otherwise, computing nodes cannot be accessed
    from outside of the network, that is, extranet. The following figure shows a simplified
    big data cluster that is commonly used in Spark:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 出于安全原因，大多数大数据集群可能会设置在网络防火墙后，以便计算节点可以克服或至少减少防火墙造成的复杂性。否则，计算节点无法从网络外部访问，即外部网络。以下图片显示了一个常用的Spark简化大数据集群：
- en: '![](img/00379.jpeg)**Figure 2:** A general architecture for big data processing
    with JVM'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00379.jpeg)**图2：**带有JVM的大数据处理的一般架构'
- en: The above picture shows a cluster consisting of five computing nodes. Here each
    node has a dedicated executor JVM, one per CPU core, and the Spark Driver JVM
    sitting outside the cluster. The disk is directly attached to the nodes using
    the **JBOD** (**Just a bunch of disks**) approach. Very large files are partitioned
    over the disks, and a virtual file system such as HDFS makes these chunks available
    as one large virtual file. The following simplified component model shows the
    driver JVM sitting outside the cluster. It talks to the cluster manager (see **Figure
    4**) in order to obtain permission to schedule tasks on the worker nodes because
    the cluster manager keeps track on resource allocation of all processes running
    on the cluster.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了一个由五个计算节点组成的集群。每个节点都有一个专用的执行器JVM，每个CPU核心一个，以及位于集群外部的Spark Driver JVM。磁盘直接连接到节点上，使用JBOD（Just
    a bunch of disks）方法。非常大的文件被分区存储在磁盘上，而像HDFS这样的虚拟文件系统将这些块作为一个大的虚拟文件提供。以下简化的组件模型显示了位于集群外部的驱动程序JVM。它与集群管理器（见图4）通信，以获取在工作节点上调度任务的权限，因为集群管理器跟踪集群上运行的所有进程的资源分配情况。
- en: 'If you have developed your Spark application using Scala or Java, it means
    that your job is a JVM-based process. For your JVM-based process, you can simply
    configure the Java heap space by specifying the following two parameters:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用Scala或Java开发了您的Spark应用程序，这意味着您的作业是基于JVM的进程。对于基于JVM的进程，您可以通过指定以下两个参数来简单配置Java堆空间：
- en: '**-Xmx**: **T**his one specifies the upper limit of your Java heap space'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -Xmx：这个参数指定了Java堆空间的上限
- en: '**-Xms**: This one is the lower limit of the Java heap space'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -Xms：这个参数是Java堆空间的下限
- en: 'Once you have sumitted a Spark job, heap memory need to be allocated for your
    Spark jobs. The following figure provides some insights on how:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您提交了一个Spark作业，就需要为您的Spark作业分配堆内存。以下图片提供了一些关于如何分配堆内存的见解：
- en: '![](img/00113.jpeg)**Figure 3:** JVM memory management'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00113.jpeg)**图3：**JVM内存管理'
- en: As demonstrated in the preceding figure, Spark starts a Spark job with 512 MB
    of JVM heap space. However, for an uninterrupted processing of your Spark job
    and to avoid the **Out of Memory** (**OOM**) error, Spark allows the computing
    nodes to utilize only up to 90% of the heap (that is, ~461 MB), which is eventually
    increased or decreased by controlling the `spark.storage.safetyFraction` parameter
    in Spark environment. To be more realistic, the JVM can be seen as a concatenation
    of **Storage** (60% of the Java heap), 20% of the heap for execution (aka **Shuffle**),
    and the rest of the 20% for other storage.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，Spark以512MB的JVM堆空间启动Spark作业。然而，为了保证Spark作业的不间断处理并避免内存不足（OOM）错误，Spark允许计算节点仅利用堆的90%（即约461MB），这最终通过控制Spark环境中的`spark.storage.safetyFraction`参数来增加或减少。更加现实的情况是，JVM可以被看作是存储（Java堆的60%）、执行（即Shuffle的堆的20%）和其他存储的20%的连接。
- en: Moreover, Spark is a cluster computing tool that tries to utilize both in-memory
    and disk-based computing and allows users to store some data in memory. In reality,
    Spark utilizes the main memory only for its LRU cache. For uninterrupted caching
    mechanism, a little amount of memory is required to be reserved for the application
    specific data processing. Informally, this is around 60% of the Java heap space
    controlled by the `spark.memory.fraction`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark是一种集群计算工具，试图同时利用内存和基于磁盘的计算，并允许用户将一些数据存储在内存中。实际上，Spark仅利用主内存作为其LRU缓存。为了实现不间断的缓存机制，需要保留一小部分内存用于应用程序特定的数据处理。非正式地说，这大约占据了由`spark.memory.fraction`控制的Java堆空间的60%。
- en: 'Therefore, if you would like to see or calculate how much application specific
    data you can cache in memory in your Spark application, you can just sum up all
    the heap sizes usages by all the executors and multiply it by the `safetyFraction`
    and `spark.memory.fraction`. In practice, 54% of the total heap size (276.48 MB)
    you can allow Spark computing nodes to be used. Now the shuffle memory is calculated
    as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您想要查看或计算在您的Spark应用程序中可以缓存多少应用程序特定数据，您只需将所有执行程序使用的堆大小总和，并将其乘以`safetyFraction`和`spark.memory.fraction`。实际上，您可以允许Spark计算节点使用总堆大小的54%（276.48
    MB）。现在，洗牌内存的计算如下：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The default values for `spark.shuffle.safetyFraction` and `spark.shuffle.memoryFraction`
    are 80% and 20%, respectively. Therefore, in practical, you can use up to *0.8*0.2
    = 16%* of the JVM heap for the shuffle. Finally, unroll memory is the amount of
    the main memory (in a computing node) that can be utilized by the unroll processes.
    The calculation goes as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.shuffle.safetyFraction`和`spark.shuffle.memoryFraction`的默认值分别为80%和20%。因此，在实际中，您可以使用*0.8*0.2
    = 16%*的JVM堆用于洗牌。最后，展开内存是计算节点中可以被展开进程利用的主内存量。计算如下：'
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The above is around 11% of the heap *(0.2*0.6*0.9 = 10.8~11%)*, that is, 56.32
    MB of the Java heap space.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上述计算约占堆的11%（0.2*0.6*0.9 = 10.8~11%），即Java堆空间的56.32 MB。
- en: More detailed discussion can be found at [http://spark.apache.org/docs/latest/configuration.html.](http://spark.apache.org/docs/latest/configuration.html)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细的讨论可以在[http://spark.apache.org/docs/latest/configuration.html](http://spark.apache.org/docs/latest/configuration.html)找到。
- en: As we will see later, there exist a variety of different cluster managers, some
    of them also capable of managing other Hadoop workloads or even non-Hadoop applications
    in parallel to the Spark executors. Note that the executor and driver have bidirectional
    communication all the time, so network wise they should also be sitting close
    together.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在后面看到的，存在各种不同的集群管理器，其中一些还能够同时管理其他Hadoop工作负载或非Hadoop应用程序。请注意，执行程序和驱动程序始终具有双向通信，因此在网络方面它们也应该坐得很近。
- en: '![](img/00167.jpeg)**Figure 4:** Driver, master, and worker architecture in
    Spark for cluster'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4：** Spark集群中的驱动程序、主节点和工作节点架构'
- en: Spark uses the driver (aka the driver program), master, and worker architecture
    (aka host, slave, or computing nodes). The driver program (or machine) talks to
    a single coordinator called master node. The master node actually manages all
    the workers (aka the slave or computing nodes) in which several executors run
    in parallel in a cluster. It is to be noted that the master is also a computing
    node having large memory, storage, OS, and underlying computing resources. Conceptually,
    this architecture can be shown in **Figure 4**. More details will be discussed
    later in this section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Spark使用驱动程序（又称驱动程序）、主节点和工作节点架构（又称主机、从节点或计算节点）。驱动程序（或机器）与称为主节点的协调器进行通信。主节点实际上管理所有工作节点（又称从节点或计算节点），其中多个执行程序在集群中并行运行。需要注意的是，主节点也是一个具有大内存、存储、操作系统和底层计算资源的计算节点。从概念上讲，这种架构可以在**图4**中显示。更多细节将在本节后面讨论。
- en: In a real cluster mode, the cluster manager (aka the resource manager) manages
    all the resources of computing nodes in a cluster. Generally, firewalls, while
    adding security to the cluster, also increase the complexity. Ports between system
    components need to be opened up so that they can talk to each other. For instance,
    Zookeeper is used by many components for configuration. Apache Kafka, which is
    a subscribing messaging system, uses Zookeeper for configuring its topics, groups,
    consumers, and producers. So, client ports to Zookeeper, potentially across the
    firewall, need to be open.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际的集群模式中，集群管理器（又称资源管理器）管理集群中所有计算节点的所有资源。通常，防火墙在为集群增加安全性的同时也增加了复杂性。系统组件之间的端口需要打开，以便它们可以相互通信。例如，Zookeeper被许多组件用于配置。Apache
    Kafka是一个订阅消息系统，使用Zookeeper来配置其主题、组、消费者和生产者。因此，需要打开到Zookeeper的客户端端口，可能要穿过防火墙。
- en: Finally, the allocation of systems to cluster nodes needs to be considered.
    For instance, if Apache Spark uses Flume or Kafka, then in-memory channels will
    be used. Apache Spark should not be competing with other Apache components for
    memory usage. Depending upon your data flows and memory usage, it might be necessary
    to have the Spark, Hadoop, Zookeeper, Flume, and other tools on distinct cluster
    nodes. Alternatively, resource managers such as YARN, Mesos, or Docker, for instance,
    can be used to tackle this problem as well. In standard Hadoop environments, most
    likely YARN is there anyway.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，需要考虑将系统分配给集群节点。例如，如果Apache Spark使用Flume或Kafka，那么将使用内存通道。Apache Spark不应该与其他Apache组件竞争内存使用。根据数据流和内存使用情况，可能需要在不同的集群节点上安装Spark、Hadoop、Zookeeper、Flume和其他工具。或者，也可以使用资源管理器，如YARN、Mesos或Docker等来解决这个问题。在标准的Hadoop环境中，很可能已经有YARN了。
- en: The computing nodes that act as workers, or Spark master, will need greater
    resources than the cluster processing nodes within the firewall. When many Hadoop
    ecosystem components are deployed on the cluster, all of them will need extra
    memory on the master server. You should monitor worker nodes for resource usage
    and adjust in terms of resources and/or application location as necessary. YARN,
    for instance, is taking care of this.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作为工作节点或Spark主节点的计算节点将需要比防火墙内的集群处理节点更多的资源。当集群上部署了许多Hadoop生态系统组件时，所有这些组件都将需要主服务器上额外的内存。您应该监视工作节点的资源使用情况，并根据需要调整资源和/或应用程序位置。例如，YARN正在处理这个问题。
- en: This section has briefly set the scene for the big data cluster in terms of
    Apache Spark, Hadoop, and other tools. However, how might the Apache Spark cluster
    itself, within the big data cluster, be configured? For instance, it is possible
    to have many types of Spark cluster manager. The next section will examine this
    and describe each type of Apache Spark cluster manager.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要介绍了Apache Spark、Hadoop和其他工具在大数据集群中的情况。然而，Apache Spark集群本身在大数据集群中如何配置？例如，可能有许多类型的Spark集群管理器。下一节将对此进行探讨，并描述每种类型的Apache
    Spark集群管理器。
- en: Cluster management
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群管理
- en: The Spark context can be defined through the Spark configuration object (that
    is, `SparkConf`) and a Spark URL. First, the purpose of the Spark context is to
    connect the Spark cluster manager in which your Spark jobs will be running. The
    cluster or resource manager then allocates the required resources across the computing
    nodes for your application. The second task of the cluster manager is to allocate
    the executors across the cluster worker nodes so that your Spark jobs get executed.
    Third, the resource manager also copies the driver program (aka the application
    JAR file, R code, or Python script) to the computing nodes. Finally, the computing
    tasks are assigned to the computing nodes by the resource manager.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Spark上下文可以通过Spark配置对象（即`SparkConf`）和Spark URL来定义。首先，Spark上下文的目的是连接Spark集群管理器，您的Spark作业将在其中运行。然后，集群或资源管理器会为您的应用程序在计算节点之间分配所需的资源。集群管理器的第二个任务是在集群工作节点之间分配执行程序，以便执行您的Spark作业。第三，资源管理器还会将驱动程序（也称为应用程序JAR文件、R代码或Python脚本）复制到计算节点。最后，资源管理器将计算任务分配给计算节点。
- en: 'The following subsections describe the possible Apache Spark cluster manager
    options available with the current Spark version (that is, Spark 2.1.0 during
    the writing of this book). To know about the resource management by a resource
    manager (aka the cluster manager), the following shows how YARN manages all its
    underlying computing resources. However, this is same for any cluster manager
    (for example, Mesos or YARN) you use:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节描述了当前Spark版本（即本书撰写时的Spark 2.1.0）提供的可能的Apache Spark集群管理器选项。要了解资源管理器（也称为集群管理器）的资源管理情况，以下内容显示了YARN如何管理其所有底层计算资源。但是，无论您使用的是哪种集群管理器（例如Mesos或YARN），情况都是一样的：
- en: '![](img/00380.jpeg)**Figure 5:** Resource management using YARN'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00380.jpeg)**图5：** 使用YARN进行资源管理'
- en: A detailed discussion can be found at [http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types](http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 详细讨论可在[http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types](http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types)找到。
- en: Pseudocluster mode (aka Spark local)
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伪集群模式（也称为Spark本地）
- en: 'As you already know, Spark jobs can be run in local mode. This is sometimes
    called pseudocluster mode of execution. This is also nondistributed and single
    JVM-based deployment mode where Spark issues all the execution components, for
    example, driver program, executor, LocalSchedulerBackend, and master, into your
    single JVM. This is the only mode where the driver itself is used as an executor.
    The following figure shows the high-level architecture of the local mode for submitting
    your Spark jobs:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您已经知道的，Spark作业可以在本地模式下运行。有时这被称为伪集群执行模式。这也是一种非分布式和基于单个JVM的部署模式，其中Spark将所有执行组件（例如驱动程序、执行程序、LocalSchedulerBackend和主节点）放入单个JVM中。这是唯一一种驱动程序本身被用作执行程序的模式。下图显示了提交Spark作业的本地模式的高级架构：
- en: '![](img/00214.jpeg)**Figure 6:** High-level architecture of local mode for
    Spark jobs (source: [https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-local.html](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-local.html))'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00214.jpeg)**图6：** Spark作业本地模式的高级架构（来源：[https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-local.html](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-local.html))'
- en: Is it too surprising? No, I guess, since you can achieve some short of parallelism
    as well, where the default parallelism is the number of threads (aka Core used)
    as specified in the master URL, that is, local [4] for 4 cores/threads and `local
    [*]` for all the available threads. We will discuss this topic later in this chapter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这太令人惊讶了吗？不，我想不是，因为您也可以实现某种并行性，其中默认并行性是在主URL中指定的线程数（也称为使用的核心），即local [4]表示4个核心/线程，`local
    [*]`表示所有可用的线程。我们将在本章后面讨论这个话题。
- en: Standalone
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立
- en: 'By specifying a Spark configuration local URL, it is possible to have the application
    run locally. By specifying *local[n]*, it is possible to have Spark use *n* threads
    to run the application locally. This is a useful development and test option because
    you can also test some sort of parallelization scenarios but keep all log files
    on a single machine. The standalone mode uses a basic cluster manager that is
    supplied with Apache Spark. The spark master URL will be as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定Spark配置本地URL，可以使应用程序在本地运行。通过指定*local[n]*，可以让Spark使用*n*个线程在本地运行应用程序。这是一个有用的开发和测试选项，因为您还可以测试某种并行化场景，但将所有日志文件保留在单台机器上。独立模式使用了Apache
    Spark提供的基本集群管理器。Spark主URL将如下所示：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, `<hostname>` is the name of the host on which the Spark master is running.
    I have specified 7077 as the port, which is the default value, but it is configurable.
    This simple cluster manager currently only supports **FIFO** (**first in first
    out**) scheduling. You can contrive to allow concurrent application scheduling
    by setting the resource configuration options for each application. For example,
    `spark.core.max` is used to share the processor cores between applications. A
    more detail discussion will be carried out later this chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`<hostname>`是运行Spark主的主机名。我指定了7077作为端口，这是默认值，但它是可配置的。这个简单的集群管理器目前只支持**FIFO**（先进先出）调度。您可以通过为每个应用程序设置资源配置选项来构想允许并发应用程序调度。例如，`spark.core.max`用于在应用程序之间共享处理器核心。本章后面将进行更详细的讨论。
- en: Apache YARN
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache YARN
- en: If the Spark master value is set as YARN-cluster, then the application can be
    submitted to the cluster and then terminated. The cluster will take care of allocating
    resources and running tasks. However, if the application master is submitted as
    YARN-client, then the application stays alive during the life cycle of processing
    and requests resources from YARN. These are applicable at a larger scale, when
    integrating with Hadoop YARN. A step-by-step guideline will be provided later
    in this chapter to configure a single-node YARN cluster for launching your Spark
    jobs needing minimal resources.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将Spark主值设置为YARN-cluster，则可以将应用程序提交到集群，然后终止。集群将负责分配资源和运行任务。然而，如果应用程序主作为YARN-client提交，则应用程序在处理的生命周期中保持活动，并从YARN请求资源。这在与Hadoop
    YARN集成时适用于更大规模。本章后面将提供逐步指南，以配置单节点YARN集群，以启动需要最少资源的Spark作业。
- en: Apache Mesos
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Mesos
- en: 'Apache Mesos is an open source system for resource sharing across a cluster.
    It allows multiple frameworks to share a cluster by managing and scheduling resources.
    It is a cluster manager, which provides isolation using Linux containers, allowing
    multiple systems such as Hadoop, Spark, Kafka, Storm, and more to share a cluster
    safely. This is a master-slave based system using Zookeeper for configuration
    management. This way you can scalae up your Spark jobs to thousands of nodes.
    For a single master node Mesos cluster, the Spark master URL will be in the following
    form:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Mesos是一个用于跨集群资源共享的开源系统。它允许多个框架通过管理和调度资源来共享集群。它是一个集群管理器，使用Linux容器提供隔离，允许多个系统（如Hadoop、Spark、Kafka、Storm等）安全地共享集群。这是一个基于主从的系统，使用Zookeeper进行配置管理。这样，您可以将Spark作业扩展到数千个节点。对于单个主节点Mesos集群，Spark主URL将采用以下形式：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The consequence of a Spark job submission by specifically using Mesos can be
    shown visually in the following figure:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过专门使用Mesos提交Spark作业的后果可以在以下图中以可视化方式显示：
- en: '![](img/00075.jpeg)**Figure 7:** Mesos in action (image source: [https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-architecture.html](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-architecture.html))'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/00075.jpeg)**图7：**Mesos在操作中（图片来源：[https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-architecture.html](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-architecture.html))
- en: 'In the preceding figure, where `<hostname>` is the hostname of the Mesos master
    server, and the port is defined as 5050, which is the default Mesos master port
    (this is configurable). If there are multiple Mesos master servers in a large-scale
    high availability Mesos cluster, then the Spark master URL would look like the
    following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，`<hostname>`是Mesos主服务器的主机名，端口定义为5050，这是默认的Mesos主端口（可配置）。如果在大规模高可用性Mesos集群中有多个Mesos主服务器，则Spark主URL将如下所示：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: So, the election of the Mesos master server will be controlled by Zookeeper.
    The `<hostname>` will be the name of a host in the Zookeeper quorum. Also, the
    port number 2181 is the default master port for Zookeeper.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Mesos主服务器的选举将由Zookeeper控制。`<hostname>`将是Zookeeper群的主机名。此外，端口号2181是Zookeeper的默认主端口。
- en: Cloud-based deployments
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于云的部署
- en: 'There are three different abstraction levels in the cloud computing paradigm:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算范式中有三种不同的抽象级别：
- en: '**Infrastructure as a Service** (aka **IaaS**)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施即服务（简称IaaS）
- en: '**Platform as a Service** (aka **PaaS**)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平台即服务（简称PaaS）
- en: '**Software as a Service** (aka **SaaS**)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件即服务（简称SaaS）
- en: IaaS provides the computing infrastructure through empty virtual machines for
    your software running as SaaS. This is also true for the Apache Spark on OpenStack.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: IaaS通过空虚拟机提供计算基础设施，用于运行作为SaaS的软件。这对于在OpenStack上的Apache Spark也是如此。
- en: The advantage of OpenStack is that it can be used among multiple different cloud
    providers, since it is an open standard and is also based on open source. You
    even can use OpenStack in a local data center, and transparently and dynamically
    move workloads between local, dedicated, and public cloud data centers.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack的优势在于它可以在多个不同的云提供商之间使用，因为它是一个开放标准，也是基于开源的。您甚至可以在本地数据中心使用OpenStack，并在本地、专用和公共云数据中心之间透明动态地移动工作负载。
- en: PaaS, in contrast, takes away from you the burden of installing and operating
    an Apache Spark cluster because this is provided as a Service. In other words,
    you can think it as a layer like what your OS does.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，PaaS从您身上解除了安装和操作Apache Spark集群的负担，因为这是作为服务提供的。换句话说，您可以将其视为类似于操作系统的一层。
- en: Sometimes, you can even Dockerize your Spark application and deploy on the cloud
    platform independent manner. However, there is an ongoing discussion whether Docker
    is IaaS or PaaS, but in our opinion, this is just a form of a lightweight preinstalled
    virtual machine, so more on the IaaS.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，甚至可以将Spark应用程序Docker化并以云平台独立方式部署。然而，关于Docker是IaaS还是PaaS正在进行讨论，但在我们看来，这只是一种轻量级预安装虚拟机的形式，更多的是IaaS。
- en: Finally, SaaS is an application layer provided and managed by cloud computing
    paradigm. To be frank, you won't see or have to worry about the first two layers
    (IaaS and PaaS).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，SaaS是云计算范式提供和管理的应用层。坦率地说，您不会看到或必须担心前两层（IaaS和PaaS）。
- en: Google Cloud, Amazon AWS, Digital Ocean, and Microsoft Azure are good examples
    of cloud computing services that provide these three layers as services. We will
    show an example of how to deploy your Spark cluster on top of Cloud using Amazon
    AWS later in this chapter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud，Amazon AWS，Digital Ocean和Microsoft Azure是提供这三个层作为服务的云计算服务的良好示例。我们将在本章后面展示如何在云顶部使用Amazon
    AWS部署您的Spark集群的示例。
- en: Deploying the Spark application on a cluster
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在集群上部署Spark应用程序
- en: 'In this section, we will discuss how to deploy Spark jobs on a computing cluster.
    We will see how to deploy clusters in three deploy modes: standalone, YARN, and
    Mesos. The following figure summarizes terms that are needed to refer to cluster
    concepts in this chapter:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何在计算集群上部署Spark作业。我们将看到如何在三种部署模式（独立，YARN和Mesos）中部署集群。以下图总结了本章中需要引用集群概念的术语：
- en: '![](img/00258.jpeg)**Figure 8:** Terms that are needed to refer to cluster
    concepts (source: http://spark.apache.org/docs/latest/cluster-overview.html#glossary)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00258.jpeg)**图8：**需要引用集群概念的术语（来源：http://spark.apache.org/docs/latest/cluster-overview.html#glossary）'
- en: However, before diving onto deeper, we need to know how to submit a Spark job
    in general.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在深入研究之前，我们需要了解如何一般提交Spark作业。
- en: Submitting Spark jobs
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提交Spark作业
- en: 'Once a Spark application is bundled as either a jar file (written in Scala
    or Java) or a Python file, it can be submitted using the Spark-submit script located
    under the bin directory in Spark distribution (aka `$SPARK_HOME/bin`). According
    to the API documentation provided in Spark website ([http://spark.apache.org/docs/latest/submitting-applications.html](http://spark.apache.org/docs/latest/submitting-applications.html)),
    the script takes care of the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦将Spark应用程序打包为jar文件（用Scala或Java编写）或Python文件，就可以使用Spark分发（即`$SPARK_HOME/bin`下的bin目录中的Spark-submit脚本）提交。根据Spark网站提供的API文档（[http://spark.apache.org/docs/latest/submitting-applications.html](http://spark.apache.org/docs/latest/submitting-applications.html)），该脚本负责以下内容：
- en: Setting up the classpath of `JAVA_HOME`, `SCALA_HOME` with Spark
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置`JAVA_HOME`，`SCALA_HOME`与Spark的类路径
- en: Setting up the all the dependencies required to execute the jobs
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置执行作业所需的所有依赖项
- en: Managing different cluster managers
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理不同的集群管理器
- en: Finally, deploying models that Spark supports
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，部署Spark支持的模型
- en: 'In a nutshell, Spark job submission syntax is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Spark作业提交语法如下：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, `[options]` can be: `--conf <configuration_parameters> --class <main-class>
    --master <master-url> --deploy-mode <deploy-mode> ... # other options`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，`[options]`可以是：`--conf <configuration_parameters> --class <main-class>
    --master <master-url> --deploy-mode <deploy-mode> ... # other options`'
- en: '`<main-class>` is the name of the main class name. This is practically the
    entry point for our Spark application.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<main-class>`是主类名。这实际上是我们Spark应用程序的入口点。'
- en: '`--conf` signifies all the used Spark parameters and configuration property.
    The format of a configuration property is a key=value format.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--conf`表示所有使用的Spark参数和配置属性。配置属性的格式是键=值格式。'
- en: '`<master-url>` specifies the master URL for the cluster (for example, `spark://HOST_NAME:PORT`*)*
    for connecting to the master of the Spark standalone cluster, `local` for running
    your Spark jobs locally. By default, it allows you using only one worker thread
    with no parallelism. The `local [k]` can be used for running your Spark job locally
    with *K* worker threads. It is to be noted that K is the number of cores on your
    machine. Finally, if you specify the master with `local[*]` for running Spark
    job locally, you are giving the permission to the `spark-submit` script to utilize
    all the worker threads (logical cores) on your machine have. Finally, you can
    specify the master as `mesos://IP_ADDRESS:PORT` for connecting to the available
    Mesos cluster. Alternatively, you could specify using `yarn` to run your Spark
    jobs on a YARN-based cluster.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<master-url>`指定集群的主URL（例如，`spark://HOST_NAME:PORT`*）*用于连接到Spark独立集群的主机，`local`用于在本地运行Spark作业。默认情况下，它只允许您使用一个工作线程，没有并行性。`local
    [k]`可用于在本地运行具有*K*工作线程的Spark作业。需要注意的是，K是您计算机上的核心数。最后，如果您指定主机为`local[*]`以在本地运行Spark作业，您将允许`spark-submit`脚本利用计算机上所有工作线程（逻辑核心）。最后，您可以指定主机为`mesos://IP_ADDRESS:PORT`以连接到可用的Mesos集群。或者，您可以指定使用`yarn`在基于YARN的集群上运行Spark作业。'
- en: 'For other options on Master URL, please refer to the following figure:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Master URL的其他选项，请参考以下图：
- en: '![](img/00183.jpeg)**Figure 9:** Details about the master URLs supported by
    Spark\'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00183.jpeg)**图9：**Spark支持的主URL的详细信息'
- en: '`<deploy-mode>` you have to specify this if you want to deploy your driver
    on the worker nodes (cluster) or locally as an external client (client). Four
    (4) modes are supported: local, standalone, YARN, and Mesos.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<deploy-mode>`如果要在worker节点（集群）上部署驱动程序，或者在外部客户端（客户端）上本地部署，必须指定。支持四种（4）模式：local，standalone，YARN和Mesos。'
- en: '`<app-jar>` is the JAR file you build with with dependencies. Just pass the
    JAR file while submitting your jobs.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<app-jar>`是您使用依赖项构建的JAR文件。在提交作业时，只需传递JAR文件。'
- en: '`<python-file>` is the application main source code written using Python. Just
    pass the `.py` file while submitting your jobs.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<python-file>`是使用Python编写的应用程序主要源代码。在提交作业时，只需传递`.py`文件。'
- en: '`[app-arguments]` could be input or output argument specified by an application
    developer.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[app-arguments]`可以是应用程序开发人员指定的输入或输出参数。'
- en: While submitting the Spark jobs using the spark-submit script, you can specify
    the main jar of the Spark application (and other related JARS included) using
    the `--jars` option. All the JARS will then be transferred to the cluster. URLs
    supplied after `--jars` must be separated by commas.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用spark-submit脚本提交Spark作业时，可以使用`--jars`选项指定Spark应用程序的主要jar（以及包括的其他相关JAR包）。然后所有的JAR包将被传输到集群。在`--jars`之后提供的URL必须用逗号分隔。
- en: 'However, if you specify the jar using the URLs, it is a good practice to separate
    the JARS using commas after `--jars`. Spark uses the following URL scheme to allow
    different strategies for disseminating JARS:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您使用URL指定jar包，最好在`--jars`之后使用逗号分隔JAR包。Spark使用以下URL方案来允许不同的JAR包传播策略：
- en: '**file:** Specifies the absolute paths and `file:/`'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**file:** 指定绝对路径和`file:/`'
- en: '**hdfs****:**, **http****:**, **https:**, **ftp****:** JARS or any other files
    will be pull-down from the URLs/URIs you specified as expected'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**hdfs****:**、**http****:**、**https:**、**ftp****:** JAR包或任何其他文件将从您指定的URL/URI中按预期进行下载'
- en: '**local:** A URI starting with `local:/` can be used to point local jar files
    on each computing node'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local:** 以`local:/`开头的URI可用于指向每个计算节点上的本地jar文件'
- en: It is to be noted that dependent JARs, R codes, Python scripts, or any other
    associated data files need to be copied or replicated to the working directory
    for each SparkContext on the computing nodes. This sometimes creates a significant
    overhead and needs a pretty large amount of disk space. The disk usages increase
    over time. Therefore, at a certain period of time, unused data objects or associated
    code files need to be cleaned up. This is, however, quite easy with YARN. YARN
    handles the cleanup periodically and can be handled automatically. For example,
    with the Spark standalone mode, automatic cleanup can be configured with the `spark.worker.cleanup.appDataTtl`
    property while submitting the Spark jobs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，依赖的JAR包、R代码、Python脚本或任何其他相关的数据文件需要复制或复制到每个计算节点上的工作目录中。这有时会产生很大的开销，并且需要大量的磁盘空间。磁盘使用量会随时间增加。因此，在一定时间内，需要清理未使用的数据对象或相关的代码文件。然而，使用YARN可以很容易地实现这一点。YARN会定期处理清理工作，并可以自动处理。例如，在Spark独立模式下，可以通过`spark.worker.cleanup.appDataTtl`属性配置自动清理提交Spark作业时。
- en: Computationally, the Spark is designed such that during the job submission (using
    `spark-submit` script), default Spark config values can be loaded and propagate
    to Spark applications from a property file. Master node will read the specified
    options from the configuration file named `spark-default.conf`. The exact path
    is `SPARK_HOME/conf/spark-defaults.conf` in your Spark distribution directory.
    However, if you specify all the parameters in the command line, this will get
    higher priority and will be used accordingly.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算上，Spark被设计为在作业提交时（使用`spark-submit`脚本），可以从属性文件加载默认的Spark配置值，并将其传播到Spark应用程序。主节点将从名为`spark-default.conf`的配置文件中读取指定的选项。确切的路径是您的Spark分发目录中的`SPARK_HOME/conf/spark-defaults.conf`。然而，如果您在命令行中指定了所有参数，这将获得更高的优先级，并且将相应地使用。
- en: Running Spark jobs locally and in standalone
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地和独立运行Spark作业
- en: The examples are shown [Chapter 13](part0413.html#C9ROA1-21aec46d8593429cacea59dbdcd64e1c),
    *My Name is Bayes, Naive Bayes*, and can be made scalable for even larger dataset
    to solve different purposes. You can package all these three clustering algorithms
    with all the required dependencies and submit them as Spark job in the cluster.
    If you don't know how to make a package and create jar files out of the Scala
    class, you can bundle your application with all the dependencies using SBT or
    Maven.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 示例显示在[第13章](part0413.html#C9ROA1-21aec46d8593429cacea59dbdcd64e1c)，*我的名字是贝叶斯，朴素贝叶斯*，并且可以扩展到更大的数据集以解决不同的目的。您可以将这三个聚类算法与所有必需的依赖项打包，并将它们作为Spark作业提交到集群中。如果您不知道如何制作一个包并从Scala类创建jar文件，您可以使用SBT或Maven将应用程序与所有依赖项捆绑在一起。
- en: 'According to Spark documentation at [http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management](http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management),
    both the SBT and Maven have assembly plugins for packaging your Spark application
    as a fat jar. If your application is already bundled with all the dependencies,
    use the following lines of code to submit your Spark job of k-means clustering,
    for example (use similar syntax for other classes), for Saratoga NY Homes dataset.
    For submitting and running a Spark job locally, run the following command on 8
    cores:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Spark文档[http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management](http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management)，SBT和Maven都有汇编插件，用于将您的Spark应用程序打包为一个fat
    jar。如果您的应用程序已经捆绑了所有的依赖项，可以使用以下代码行提交您的k-means聚类Spark作业，例如（对其他类使用类似的语法），用于Saratoga
    NY Homes数据集。要在本地提交和运行Spark作业，请在8个核心上运行以下命令：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the preceding code, `com.chapter15.KMeansDemo` is the main class file written
    in Scala. Local [8] is the master URL utilizing eight cores of your machine. `KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar`
    is the application JAR file we just generated by Maven project; `Saratoga_NY_Homes.txt`
    is the input text file for the Saratoga NY Homes dataset. If the application executed
    successfully, you will find the message including the output in the following
    figure (abridged):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`com.chapter15.KMeansDemo`是用Scala编写的主类文件。Local [8]是使用您机器的八个核心的主URL。`KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar`是我们刚刚通过Maven项目生成的应用程序JAR文件；`Saratoga_NY_Homes.txt`是Saratoga
    NY Homes数据集的输入文本文件。如果应用程序成功执行，您将在下图中找到包括输出的消息（摘要）：
- en: '![](img/00015.gif)**Figure 10:** Spark job output on terminal [local mode]'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00015.gif)**图10:** 终端上的Spark作业输出[本地模式]'
- en: Now, let's dive into the cluster setup in standalone mode. To install Spark
    standalone mode, you should place prebuilt versions of Spark with each release
    on each node on the cluster. Alternatively, you can build it yourself and use
    it according to the instruction at [http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入研究独立模式下的集群设置。要安装Spark独立模式，您应该在集群的每个节点上放置每个版本的预构建版本的Spark。或者，您可以自己构建它，并根据[http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html)上的说明使用它。
- en: 'To configure the environment as a Spark standalone mode, you will have to provide
    the prebuilt versions of Spark with the desired version to each node on the cluster.
    Alternatively, you can build it yourself and use it according to the instruction
    at [http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html).
    Now we will see how to start a standalone cluster manually. You can start a standalone
    master by executing the following command:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要将环境配置为Spark独立模式，您将需要为集群的每个节点提供所需版本的预构建版本的Spark。或者，您可以自己构建它，并根据[http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html)上的说明使用它。现在我们将看到如何手动启动独立集群。您可以通过执行以下命令启动独立主节点：
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once started, you should observe the following logs on terminal:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动，您应该在终端上观察以下日志：
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should be able to access Spark web UI at `http://localhost:8080` by default.
    Observe the following UI as shown in the following figure:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该能够默认访问`http://localhost:8080`的Spark Web UI。观察以下UI，如下图所示：
- en: '![](img/00321.jpeg)**Figure 11:** Spark master as standalone'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00321.jpeg)**图11：**Spark主节点作为独立节点'
- en: 'You can change the port number by editing the following parameter:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过编辑以下参数更改端口号：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the `SPARK_HOME/sbin/start-master.sh`, just change the port number and then
    apply the following command:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在`SPARK_HOME/sbin/start-master.sh`中，只需更改端口号，然后应用以下命令：
- en: '[PRE10]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Alternatively, you can restart the Spark master to effect the preceding change.
    However, you will have to make a similar change in the `SPARK_HOME/sbin/start-slave.sh`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以重新启动Spark主节点以实现前面的更改。但是，您将不得不在`SPARK_HOME/sbin/start-slave.sh`中进行类似的更改。
- en: 'As you can see here, there are no active workers associated with the master
    node. Now to create a slave node (aka a worker node or computing node), create
    workers and connect them to the master using the following command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在这里所看到的，没有与主节点关联的活动工作节点。现在，要创建一个从节点（也称为工作节点或计算节点），请创建工作节点并使用以下命令将其连接到主节点：
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Upon successful completion of the preceding command, you should observe the
    following logs on terminal:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 成功完成上述命令后，您应该在终端上观察以下日志：
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once you have one of your worker nodes started, you can look at its status
    on the Spark web UI at `http://localhost:8081`. However, if you start another
    worker node, you can access it''s status in the consecutive ports (that is, 8082,
    8083, and so on). You should also see the new node listed there, along with its
    number of CPUs and memory, as shown in the following figure:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的一个工作节点启动，您可以在Spark Web UI的`http://localhost:8081`上查看其状态。但是，如果您启动另一个工作节点，您可以在连续的端口（即8082、8083等）上访问其状态。您还应该在那里看到新节点的列表，以及其CPU和内存的数量，如下图所示：
- en: '![](img/00055.jpeg)**Figure 12:** Spark worker as standalone'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00055.jpeg)**图12：**Spark工作节点作为独立节点'
- en: 'Now, if you refresh `http://localhost:8080`, you should see that one worker
    node that is associated with your master node has been added, as shown in the
    following figure:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您刷新`http://localhost:8080`，您应该看到与您的主节点关联的一个工作节点已添加，如下图所示：
- en: '![](img/00104.jpeg)**Figure 13:** Spark master has now one worker node as standalone'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00104.jpeg)**图13：**Spark主节点现在有一个独立的工作节点'
- en: 'Finally, as shown in the following figure, these are all the configuration
    options that can be passed to the master and worker nodes:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如下图所示，这些都是可以传递给主节点和工作节点的配置选项：
- en: '![](img/00195.jpeg)**Figure 14:** Configuration options that can be passed
    to the master and worker nodes (source: [http://spark.apache.org/docs/latest/spark-standalone.html#starting-a-cluster-manually](http://spark.apache.org/docs/latest/spark-standalone.html#starting-a-cluster-manually))'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00195.jpeg)**图14：**可以传递给主节点和工作节点的配置选项（来源：[http://spark.apache.org/docs/latest/spark-standalone.html#starting-a-cluster-manually](http://spark.apache.org/docs/latest/spark-standalone.html#starting-a-cluster-manually))'
- en: 'Now one of your master node and a worker node are reading and active. Finally,
    you can submit the same Spark job as standalone rather than local mode using the
    following commands:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的一个主节点和一个工作节点正在读取和活动。最后，您可以提交与本地模式不同的独立模式下的相同Spark作业，使用以下命令：
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Once the job started, access Spark web UI at `http://localhost:80810` for master
    and `http://localhost:8081` for the worker, you can see the progress of your job
    as discussed in [Chapter 14](part0434.html#CTSK41-21aec46d8593429cacea59dbdcd64e1c),
    *Time to Put Some Order - Cluster Your Data with Spark MLlib*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 作业启动后，访问`http://localhost:80810`的Spark Web UI以查看主节点和`http://localhost:8081`的工作节点，您可以看到作业的进度，如[第14章](part0434.html#CTSK41-21aec46d8593429cacea59dbdcd64e1c)中所讨论的那样，*Time
    to Put Some Order - Cluster Your Data with Spark MLlib*。
- en: 'To summarize this section, we would like to redirect you to the following image
    (that is, **Figure 15**) that shows the usages of the following shell scripts
    for launching or stopping your cluster:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这一部分，我们想引导您查看下图（即**图15**），显示了以下shell脚本用于启动或停止集群的用法：
- en: '![](img/00295.jpeg)**Figure 15:** The usages of the shell scripts for launching
    or stopping your cluster\'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00295.jpeg)**图15：**用于启动或停止集群的shell脚本的用法'
- en: Hadoop YARN
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop YARN
- en: 'As already discussed, the Apache Hadoop YARN has to main components: a scheduler
    and an applications manager, as shown in the following figure:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Apache Hadoop YARN有两个主要组件：调度程序和应用程序管理器，如下图所示：
- en: '![](img/00250.jpeg)**Figure 16:** Apache Hadoop YARN architecture (blue: system
    components; yellow and pink: two applications running)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '**图16：**Apache Hadoop YARN架构（蓝色：系统组件；黄色和粉色：两个正在运行的应用程序）'
- en: 'Now that using the scheduler and the applications manager, the following two
    deploy modes can be configured to launch your Spark jobs on a YARN-based cluster:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用调度程序和应用程序管理器，可以配置以下两种部署模式来在基于YARN的集群上启动Spark作业：
- en: '**Cluster mode**: In the cluster mode, the Spark driver works within the master
    process of an application managed by YARN''s application manager. Even the client
    can be terminated or disconnected away when the application has been initiated.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群模式**：在集群模式下，Spark驱动程序在YARN的应用程序管理器管理的应用程序的主进程内工作。即使客户端在应用程序启动后被终止或断开连接，应用程序也可以继续运行。'
- en: '**Client mode**: In this mode, the Spark driver runs inside the client process.
    After that, Spark master is used only for requesting computing resources for the
    computing nodes from YARN (YARN resource manager).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户端模式**：在此模式下，Spark驱动程序在客户端进程内运行。之后，Spark主节点仅用于从YARN（YARN资源管理器）请求计算节点的计算资源。'
- en: In the Spark standalone and Mesos modes, the URL of the master (that is, address)
    needs to be specified in the `--master` parameter. However, in the YARN mode,
    the address of the resource manager is read from the Hadoop configuration file
    in your Hadoop setting. Consequently, the `--master` parameter is `yarn`. Before
    submitting our Spark jobs, we, however, you need to set up your YARN cluster.
    The next subsection shows a step-by-step of doing so.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark独立模式和Mesos模式中，需要在`--master`参数中指定主节点（即地址）。然而，在YARN模式中，资源管理器的地址是从Hadoop配置文件中读取的。因此，`--master`参数是`yarn`。在提交Spark作业之前，您需要设置好YARN集群。下一小节将逐步展示如何操作。
- en: Configuring a single-node YARN cluster
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置单节点YARN集群
- en: 'In this subsection, we will see how to set up your YARN cluster before running
    your Spark jobs on YARN cluster. There are several steps so keep patience and
    do the following step-by-step:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将看到如何在在YARN集群上运行Spark作业之前设置YARN集群。有几个步骤，所以请耐心按照以下步骤操作：
- en: 'Step 1: Downloading Apache Hadoop'
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤1：下载Apache Hadoop
- en: 'Download the latest distribution from the Hadoop website ([http://hadoop.apache.org/](http://hadoop.apache.org/)).
    I used the latest stable version 2.7.3 on Ubuntu 14.04 as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 从Hadoop网站（[http://hadoop.apache.org/](http://hadoop.apache.org/)）下载最新的发行版。我在Ubuntu
    14.04上使用了最新的稳定版本2.7.3，如下所示：
- en: '[PRE14]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, create and extract the package in `/opt/yarn` as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，按以下方式创建并提取包在`/opt/yarn`中：
- en: '[PRE15]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Step 2: Setting the JAVA_HOME'
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤2：设置JAVA_HOME
- en: Refer to the section of Java setup in [Chapter 1](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c),
    *Introduction to Scala*, for details and apply the same changes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅[第1章](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c)中的Java设置部分，*Scala简介*，并应用相同的更改。
- en: 'Step 3: Creating users and groups'
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤3：创建用户和组
- en: 'The following `yarn`, `hdfs`, and `mapred` user accounts for `hadoop` group
    can be created as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 可以按以下方式创建`hadoop`组的`yarn`、`hdfs`和`mapred`用户帐户：
- en: '[PRE16]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Step 4: Creating data and log directories'
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤4：创建数据和日志目录
- en: 'To run your Spark jobs using Hadoop, it needs to have the data and the log
    directories with various permissions. You can use the following command:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Hadoop运行Spark作业，需要具有具有各种权限的数据和日志目录。您可以使用以下命令：
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now you need to create the log directory where YARN is installed and then set
    the owner and group as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您需要创建YARN安装的日志目录，然后按以下方式设置所有者和组：
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Step 5: Configuring core-site.xml'
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤5：配置core-site.xml
- en: 'Two properties (that is, `fs.default.name` and `hadoop.http.staticuser.user`)
    need to be set to the `etc/hadoop/core-site.xml` file. Just copy the following
    lines of codes:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 两个属性（即`fs.default.name`和`hadoop.http.staticuser.user`）需要设置到`etc/hadoop/core-site.xml`文件中。只需复制以下代码行：
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Step 6: Configuring hdfs-site.xml'
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤6：配置hdfs-site.xml
- en: 'Five properties (that is, `dfs.replication` , `dfs.namenode.name.dir` , `fs.checkpoint.dir`
    , `fs.checkpoint.edits.dir`, and `dfs.datanode.data.dir`) need to be set to the
    `etc/hadoop/ hdfs-site.xml` file. Just copy the following lines of codes:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 五个属性（即`dfs.replication`，`dfs.namenode.name.dir`，`fs.checkpoint.dir`，`fs.checkpoint.edits.dir`和`dfs.datanode.data.dir`）需要设置到`etc/hadoop/hdfs-site.xml`文件中。只需复制以下代码行：
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Step 7: Configuring mapred-site.xml'
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤7：配置mapred-site.xml
- en: 'One property (that is, `mapreduce.framework.name`) needs to be set to the `etc/hadoop/
    mapred-site.xml` file. First, copy and replace the original template file to the
    `mapred-site.xml` as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个属性（即`mapreduce.framework.name`）需要设置到`etc/hadoop/mapred-site.xml`文件中。首先，将原始模板文件复制并替换为以下内容到`mapred-site.xml`中：
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, just copy the following lines of codes:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，只需复制以下代码行：
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Step 8: Configuring yarn-site.xml'
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤8：配置yarn-site.xml
- en: 'Two properties (that is, `yarn.nodemanager.aux-services` and `yarn.nodemanager.aux-services.mapreduce.shuffle.class`)
    need to be set to the `etc/hadoop/yarn-site.xml` file. Just copy the following
    lines of codes:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 两个属性（即`yarn.nodemanager.aux-services`和`yarn.nodemanager.aux-services.mapreduce.shuffle.class`）需要设置到`etc/hadoop/yarn-site.xml`文件中。只需复制以下代码行：
- en: '[PRE23]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Step 9: Setting Java heap space'
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤9：设置Java堆空间
- en: 'To run your Spark job on Hadoop-based YARN cluster, you need to specify enough
    heap space for the JVM. You need to edit the `etc/hadoop/hadoop-env.sh` file.
    Enable the following properties:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要在基于Hadoop的YARN集群上运行Spark作业，需要为JVM指定足够的堆空间。您需要编辑`etc/hadoop/hadoop-env.sh`文件。启用以下属性：
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now you also need to edit the `mapred-env.sh` file with the following line:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您还需要编辑`mapred-env.sh`文件，添加以下行：
- en: '[PRE25]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, make sure that you have edited `yarn-env.sh` to make the changes permanent
    for Hadoop YARN:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请确保已编辑`yarn-env.sh`以使更改对Hadoop YARN永久生效：
- en: '[PRE26]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Step 10: Formatting HDFS'
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤10：格式化HDFS
- en: 'If you want to start your HDFS NameNode, Hadoop needs to initialize the directory
    where it will store or persist its data for tracking all the metadata for your
    file system. The formatting will destroy everything and sets up a new file system.
    Then it uses the values of the parameters set on `dfs.namenode.name.dir` in `etc/hadoop/hdfs-site.xml`.
    For doing the format, at first, move to the `bin` directory and execute the following
    commands:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要启动HDFS NameNode，Hadoop需要初始化一个目录，用于存储或持久化其用于跟踪文件系统所有元数据的数据。格式化将销毁所有内容并设置一个新的文件系统。然后它使用`etc/hadoop/hdfs-site.xml`中`dfs.namenode.name.dir`参数设置的值。要进行格式化，首先转到`bin`目录并执行以下命令：
- en: '[PRE27]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If the preceding command executed successfully, you should see the following
    on your Ubuntu terminal:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的命令执行成功，您应该在Ubuntu终端上看到以下内容：
- en: '[PRE28]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Step 11: Starting the HDFS'
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11步：启动HDFS
- en: 'From the `bin` directory in step 10, execute the following command:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10步的`bin`目录中，执行以下命令：
- en: '[PRE29]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Upon successful execution of the preceding command, you should see the following
    on your terminal:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行前面的命令成功后，您应该在终端上看到以下内容：
- en: '[PRE30]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To start the `secondarynamenode` and the `datanode`, you should use the following
    command:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动`secondarynamenode`和`datanode`，您应该使用以下命令：
- en: '[PRE31]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You should receive the following message on your terminal if the preceding
    commands succeed:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的命令成功，您应该在终端上收到以下消息：
- en: '[PRE32]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then use the following command to start the data node:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用以下命令启动数据节点：
- en: '[PRE33]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You should receive the following message on your terminal if the preceding
    commands succeed:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的命令成功，您应该在终端上收到以下消息：
- en: '[PRE34]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now make sure that, you check all the services related to those nodes are running
    use the following command:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在确保检查所有与这些节点相关的服务是否正在运行，请使用以下命令：
- en: '[PRE35]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You should observe something like the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该观察到类似以下的内容：
- en: '[PRE36]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Step 12: Starting YARN'
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12步：启动YARN
- en: 'For working with YARN, one `resourcemanager` and one node manager have to be
    started as the user yarn:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用YARN，必须以用户yarn启动一个`resourcemanager`和一个节点管理器：
- en: '[PRE37]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You should receive the following message on your terminal if the preceding
    commands succeed:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的命令成功，您应该在终端上收到以下消息：
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Tehn execute the following command to start the node manager:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然后执行以下命令启动节点管理器：
- en: '[PRE39]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You should receive the following message on your terminal if the preceding
    commands succeed:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的命令成功，您应该在终端上收到以下消息：
- en: '[PRE40]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If you want to make sure that every services in those nodes are running, you
    should use the `$jsp` command. Moreover, if you want to stop your resource manager
    or `nodemanager`, use the following `g` commands:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要确保这些节点中的所有服务都在运行，应该使用`$jsp`命令。此外，如果要停止资源管理器或`nodemanager`，请使用以下`g`命令：
- en: '[PRE41]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Step 13: Verifying on the web UI'
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13步：在Web UI上进行验证
- en: Access `http://localhost:50070` to view the status of the NameNode, and access
    `http://localhost:8088` for the resource manager on your browser.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 访问`http://localhost:50070`查看NameNode的状态，并在浏览器上访问`http://localhost:8088`查看资源管理器。
- en: The preceding steps show how to configure a Hadoop-based YARN cluster with only
    a few nodes. However, if you want to configure your Hadoop-based YARN clusters
    ranging from a few nodes to extremely large clusters with thousands of nodes,
    refer to [https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤展示了如何配置基于Hadoop的YARN集群，只有几个节点。但是，如果您想要配置从几个节点到拥有数千个节点的极大集群的基于Hadoop的YARN集群，请参考[https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html)。
- en: Submitting Spark jobs on YARN cluster
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在YARN集群上提交Spark作业
- en: 'Now that our YARN cluster with the minimum requirement (for executing a small
    Spark job to be frank) is ready, to launch a Spark application in a cluster mode
    of YARN, you can use the following submit command:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的YARN集群已经满足最低要求（用于执行一个小的Spark作业），要在YARN的集群模式下启动Spark应用程序，可以使用以下提交命令：
- en: '[PRE42]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'For running our `KMeansDemo`, it should be done like this:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行我们的`KMeansDemo`，应该这样做：
- en: '[PRE43]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The preceding `submit` command starts a YARN cluster mode with the default application
    master. Then `KMeansDemo` will be running as a child thread of the application
    master. For the status updates and for displaying them in the console, the client
    will periodically poll the application master. When your application (that is,
    `KMeansDemo` in our case) has finished its execution, the client will be exited.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的`submit`命令以默认应用程序主节点启动YARN集群模式。然后`KMeansDemo`将作为应用程序主节点的子线程运行。为了获取状态更新并在控制台中显示它们，客户端将定期轮询应用程序主节点。当您的应用程序（即我们的情况下的`KMeansDemo`）执行完毕时，客户端将退出。
- en: "Upon submission of your job, you might want to see the progress using the Spark\
    \ web UI or Spark history server. Moreover, you should refer to [Chapter 18](part0550.html#GCGLC1-21aec46d8593429cacea59dbdcd64e1c),\uFEFF\
    \ *Testing and Debugging Spark*) to know how to analyze driver and executor logs."
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 提交作业后，您可能希望使用Spark web UI或Spark历史服务器查看进度。此外，您应该参考[第18章](part0550.html#GCGLC1-21aec46d8593429cacea59dbdcd64e1c)，*测试和调试Spark*）以了解如何分析驱动程序和执行程序日志。
- en: 'To launch a Spark application in a client mode, you should use the earlier
    command, except that you will have to replace the cluster with the client. For
    those who want to work with Spark shell, use the following in client mode:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 要以客户端模式启动Spark应用程序，应该使用之前的命令，只是您将不得不将集群替换为客户端。对于想要使用Spark shell的人，请在客户端模式下使用以下命令：
- en: '[PRE44]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Advance job submissions in a YARN cluster
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在YARN集群中进行高级作业提交
- en: If you opt for the more advanced way of submitting Spark jobs to be computed
    in your YARN cluster, you can specify additional parameters. For example, if you
    want to enable the dynamic resource allocation, make the `spark.dynamicAllocation.enabled`
    parameter true. However, to do so, you also need to specify `minExecutors`, `maxExecutors`,
    and `initialExecutors` as explained in the following. On the other hand, if you
    want to enable the shuffling service, set `spark.shuffle.service.enabled` as `true`.
    Finally, you could also try specifying how many executor instances will be running
    using the `spark.executor.instances` parameter.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择更高级的方式将Spark作业提交到您的YARN集群中进行计算，您可以指定其他参数。例如，如果要启用动态资源分配，请将`spark.dynamicAllocation.enabled`参数设置为true。但是，为了这样做，您还需要指定`minExecutors`，`maxExecutors`和`initialExecutors`，如下所述。另一方面，如果要启用洗牌服务，请将`spark.shuffle.service.enabled`设置为`true`。最后，您还可以尝试使用`spark.executor.instances`参数指定将运行多少执行程序实例。
- en: 'Now, to make the preceding discussion more concrete, you can refer to the following
    submission command:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了使前面的讨论更具体，您可以参考以下提交命令：
- en: '[PRE45]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: However, the consequence of the preceding job submission script is complex and
    sometimes nondeterministic. From my previous experience, if you increase the number
    of partitions from code and the number of executors, then the app will finish
    faster, which is okay. But if you increase only the executor-cores, the finish
    time is the same. However, you might expect the time to be lower than initial
    time. Second, if you launch the preceding code twice, you might expect both jobs
    to finish in say 60 seconds, but this also might not happen. Often, both jobs
    might finish after 120 seconds instead. This is a bit weird, isn't it? However,
    here goes the explanation that would help you understand this scenario.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，前面的作业提交脚本的后果是复杂的，有时是不确定的。根据我的以往经验，如果您从代码中增加分区和执行程序的数量，那么应用程序将更快完成，这是可以接受的。但是，如果您只增加执行程序核心，完成时间是相同的。然而，您可能期望时间比初始时间更短。其次，如果您两次启动前面的代码，您可能期望两个作业都在60秒内完成，但这也可能不会发生。通常情况下，两个作业可能在120秒后才完成。这有点奇怪，不是吗？然而，下面是一个解释，可以帮助您理解这种情况。
- en: Suppose you have 16 cores and 8 GB memory on your machine. Now, if you use four
    executors with one core each, what will happen? Well, when you use an executor,
    Spark reserves it from YARN and YARN allocates the number of cores (for example,
    one in our case) and the memory required. The memory is required more than you
    asked for actually for faster processing. If you ask for 1 GB, it will, in fact,
    allocate almost 1.5 GB with 500 MB overhead. In addition, it will probably allocate
    an executor for the driver with probably 1024 MB memory usage (that is, 1 GB).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的机器上有16个核心和8GB内存。现在，如果您使用四个每个核心的执行程序，会发生什么？当您使用执行程序时，Spark会从YARN中保留它，并且YARN会分配所需的核心数（例如，在我们的情况下为1）和所需的内存。实际上，为了更快地处理，所需的内存要比您实际请求的更多。如果您请求1GB，实际上它将分配几乎1.5GB，其中包括500MB的开销。此外，它可能会为驱动程序分配一个执行程序，可能使用1024MB内存（即1GB）。
- en: Sometimes, it doesn't matter how much memory your Spark job wants but how much
    it reserves. In the preceding example, it will not take 50 MB of the test but
    around 1.5 GB (including the overhead) per executor. We will discuss how to configure
    Spark cluster on AWS later this chapter.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，不管您的Spark作业需要多少内存，而是需要预留多少内存。在前面的例子中，它不会占用50MB的内存，而是大约1.5GB（包括开销）每个执行程序。我们将在本章后面讨论如何在AWS上配置Spark集群。
- en: Apache Mesos
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Mesos
- en: The Mesos master usually replaces the Spark master as the cluster manager (aka
    the resource manager) when using Mesos. Now, when a driver program creates a Spark
    job and starts assigning the related tasks for scheduling, Mesos determines which
    computing nodes handle which tasks. We assume that you have already configured
    and installed Mesos on your machine.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Mesos时，Mesos主节点通常会取代Spark主节点作为集群管理器（也称为资源管理器）。现在，当驱动程序创建一个Spark作业并开始分配相关任务进行调度时，Mesos确定哪些计算节点处理哪些任务。我们假设您已经在您的机器上配置和安装了Mesos。
- en: To get started, following links may be helpful to install Mesos on your machine.
    [http://blog.madhukaraphatak.com/mesos-single-node-setup-ubuntu/,](http://blog.madhukaraphatak.com/mesos-single-node-setup-ubuntu/)
    [https://mesos.apache.org/gettingstarted/.](https://mesos.apache.org/gettingstarted/)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，以下链接可能有助于在您的机器上安装Mesos。[http://blog.madhukaraphatak.com/mesos-single-node-setup-ubuntu/,](http://blog.madhukaraphatak.com/mesos-single-node-setup-ubuntu/)
    [https://mesos.apache.org/gettingstarted/.](https://mesos.apache.org/gettingstarted/)
- en: Depending upon hardware configuration, it takes a while. On my machine (Ubuntu
    14.04 64-bit, with Core i7 and 32 GB of RAM), it took 1 hour to complete the build.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 根据硬件配置的不同，需要一段时间。在我的机器上（Ubuntu 14.04 64位，带有Core i7和32GB RAM），完成构建需要1小时。
- en: To submit and compute your Spark jobs by utilizing the Mesos cluster mode, make
    sure to check that the Spark binary packages are available in a place accessible
    by Mesos. Additionally, make sure that your Spark driver program can be configured
    in such a way that it is automatically connected to Mesos. The second option is
    installing Spark in the same location as the Mesos slave nodes. Then, you will
    have to configure the `spark.mesos.executor.home` parameter to point the location
    of Spark distribution. It is to be noted that the default location that could
    point is the `SPARK_HOME`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过利用Mesos集群模式提交和计算您的Spark作业，请确保检查Spark二进制包是否可在Mesos可访问的位置。此外，请确保您的Spark驱动程序可以配置成自动连接到Mesos。第二个选项是在与Mesos从属节点相同的位置安装Spark。然后，您将需要配置`spark.mesos.executor.home`参数来指向Spark分发的位置。需要注意的是，可能指向的默认位置是`SPARK_HOME`。
- en: When Mesos executes a Spark job on a Mesos worker node (aka computing node)
    for the first time, the Spark binary packages have to be available on that worker
    node. This will ensure that the Spark Mesos executor is running in the backend.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 当Mesos在Mesos工作节点（也称为计算节点）上首次执行Spark作业时，Spark二进制包必须在该工作节点上可用。这将确保Spark Mesos执行程序在后台运行。
- en: 'The Spark binary packages can be hosted to Hadoop to make them accessible:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Spark二进制包可以托管到Hadoop上，以便让它们可以被访问：
- en: 1\. Having the URIs/URLs (including HTTP) via `http://`,
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 通过`http://`使用URI/URL（包括HTTP），
- en: 2\. Using the Amazon S3 via `s3n://`,
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 通过`s3n://`使用Amazon S3，
- en: 3\. Using the HDFS via `hdfs://`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 通过`hdfs://`使用HDFS。
- en: If you set the `HADOOP_CONF_DIR` environment variable, the parameter is usually
    set as `hdfs://...`; otherwise `file://`.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设置了`HADOOP_CONF_DIR`环境变量，参数通常设置为`hdfs://...`；否则为`file://`。
- en: 'You can specify the Master URLs for Mesos as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按以下方式指定Mesos的主URL：
- en: '`mesos://host:5050` for a single-master Mesos cluster, and `mesos://zk://host1:2181,host2:2181,host3:2181/mesos`
    for a multimaster Mesos cluster controlled by the ZooKeeper.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于单主Mesos集群，使用`mesos://host:5050`，对于由ZooKeeper控制的多主Mesos集群，使用`mesos://zk://host1:2181,host2:2181,host3:2181/mesos`。
- en: For a more detailed discussion, please refer to [http://spark.apache.org/docs/latest/running-on-mesos.html](http://spark.apache.org/docs/latest/running-on-mesos.html).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更详细的讨论，请参阅[http://spark.apache.org/docs/latest/running-on-mesos.html](http://spark.apache.org/docs/latest/running-on-mesos.html)。
- en: Client mode
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端模式
- en: 'In this mode, the Mesos framework works in such a way that the Spark job is
    launched on the client machine directly. It then waits for the computed results,
    also called the driver output. To interact properly with the Mesos, the driver,
    however, expects that there are some application-specific configurations specified
    in `SPARK_HOME/conf/spark-env.sh` . To make this happened, modify the `spark-env.sh.template`
    file at `$SPARK_HOME /conf`, and before using this client mode, in your `spark-env.sh`,
    set the following environment variables:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在此模式下，Mesos框架以这样的方式工作，即Spark作业直接在客户端机器上启动。然后等待计算结果，也称为驱动程序输出。然而，为了与Mesos正确交互，驱动程序期望在`SPARK_HOME/conf/spark-env.sh`中指定一些特定于应用程序的配置。为了实现这一点，在`$SPARK_HOME
    /conf`下修改`spark-env.sh.template`文件，并在使用此客户端模式之前，在您的`spark-env.sh`中设置以下环境变量：
- en: '[PRE46]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This path is typically `/usr/local /lib/libmesos.so` on Ubuntu. On the other
    hand, on macOS X, the same library is called `libmesos.dylib` instead of `libmesos.so`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在Ubuntu上，此路径通常为`/usr/local /lib/libmesos.so`。另一方面，在macOS X上，相同的库称为`libmesos.dylib`，而不是`libmesos.so`：
- en: '[PRE47]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, when submitting and starting a Spark application to be executed on the
    cluster, you will have to pass the Mesos `:// HOST:PORT` as the master URL. This
    is usually done while creating the `SparkContext` in your Spark application development
    as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当提交和启动要在集群上执行的Spark应用程序时，您将需要将Mesos `:// HOST:PORT`作为主URL传递。这通常是在Spark应用程序开发中创建`SparkContext`时完成的，如下所示：
- en: '[PRE48]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The second option of doing so is using the `spark-submit` script and configure
    `spark.executor.uri` in the `SPARK_HOME/conf/spark-defaults.conf` file. When running
    a shell, the `spark.executor.uri` parameter is inherited from `SPARK_EXECUTOR_URI`,
    so it does not need to be redundantly passed in as a system property. Just use
    the following command to access the client mode from your Spark shell:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用`spark-submit`脚本，并在`SPARK_HOME/conf/spark-defaults.conf`文件中配置`spark.executor.uri`。在运行shell时，`spark.executor.uri`参数从`SPARK_EXECUTOR_URI`继承，因此不需要作为系统属性冗余传递。只需使用以下命令从您的Spark
    shell访问客户端模式：
- en: '[PRE49]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Cluster mode
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群模式
- en: Spark on Mesos also supports cluster mode. If the driver is already launched
    Spark job (on a cluster) and the computation is also finished, client can access
    the result (of driver) from the Mesos Web UI. If you have started `MesosClusterDispatcher`
    in your cluster through the `SPARK_HOME/sbin/start-mesos-dispatcher.sh` script,
    you can use the cluster mode.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos上的Spark还支持集群模式。如果驱动程序已经启动了Spark作业（在集群上），并且计算也已经完成，客户端可以从Mesos Web UI访问（驱动程序的）结果。如果您通过`SPARK_HOME/sbin/start-mesos-dispatcher.sh`脚本在集群中启动了`MesosClusterDispatcher`，则可以使用集群模式。
- en: Again, the condition is that you have to pass the Mesos master URL (for example,
    `mesos://host:5050`) while creating the `SparkContext` in your Spark application.
    Starting the Mesos in the cluster mode also starts the `MesosClusterDispatcher`
    as a daemon running on your host machine.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，条件是在创建Spark应用程序的`SparkContext`时，您必须传递Mesos主URL（例如，`mesos://host:5050`）。在集群模式下启动Mesos还会启动作为守护程序在主机上运行的`MesosClusterDispatcher`。
- en: To gain a more flexible and advanced execution of your Spark jobs, you can also
    use the **Marathon**. The advantageous thing about using the Marathon is that
    you can run the `MesosClusterDispatcher` with Marathon. If you do that, make sure
    that the `MesosClusterDispatcher` is running in the foreground.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更灵活和高级的执行Spark作业，您还可以使用**Marathon**。使用Marathon的优点是可以使用Marathon运行`MesosClusterDispatcher`。如果这样做，请确保`MesosClusterDispatcher`在前台运行。
- en: '**Marathon** is a framework for Mesos that is designed to launch long-running
    applications, and in Mesosphere, it serves as a replacement for a traditional
    init system. It has many features that simplify running applications in a clustered
    environment, such as high-availability, node constraints, application health checks,
    an API for scriptability and service discovery, and an easy-to-use web user interface.
    It adds its scaling and self-healing capabilities to the Mesosphere feature set.
    Marathon can be used to start other Mesos frameworks, and it can also launch any
    process that can be started in the regular shell. As it is designed for long-running
    applications, it will ensure that applications it has launched will continue running,
    even if the slave node(s) they are running on fails. For more information on using
    Marathon with the Mesosphere, refer to the GitHub page at [https://github.com/mesosphere/marathon](https://github.com/mesosphere/marathon).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**Marathon**是Mesos的一个框架，旨在启动长时间运行的应用程序，在Mesosphere中，它作为传统init系统的替代品。它具有许多功能，简化了在集群环境中运行应用程序，如高可用性、节点约束、应用程序健康检查、用于脚本编写和服务发现的API，以及易于使用的Web用户界面。它将其扩展和自我修复功能添加到Mesosphere功能集中。Marathon可用于启动其他Mesos框架，还可以启动可以在常规shell中启动的任何进程。由于它设计用于长时间运行的应用程序，它将确保其启动的应用程序将继续运行，即使它们正在运行的从节点失败。有关在Mesosphere中使用Marathon的更多信息，请参考GitHub页面[https://github.com/mesosphere/marathon](https://github.com/mesosphere/marathon)。'
- en: 'To be more specific, from the client, you can submit a Spark job to your Mesos
    cluster by using the `spark-submit` script and specifying the master URL to the
    URL of the `MesosClusterDispatcher` (for example, `mesos://dispatcher:7077`).
    It goes as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，从客户端，您可以使用`spark-submit`脚本提交Spark作业到您的Mesos集群，并指定主URL为`MesosClusterDispatcher`的URL（例如，`mesos://dispatcher:7077`）。操作如下：
- en: '[PRE50]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You can view driver statuses on the Spark cluster web UI. For example, use
    the following job submission command for doing so:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Spark集群web UI上查看驱动程序状态。例如，使用以下作业提交命令来执行：
- en: '[PRE51]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Note that JARS or Python files that are passed to Spark-submit should be URIs
    reachable by Mesos slaves, as the Spark driver doesn''t automatically upload local
    jars. Finally, Spark can run over Mesos in two modes: *coarse-grained* (default)
    and *fine-grained* (deprecated). For more details, please refer to [http://spark.apache.org/docs/latest/running-on-mesos.html](http://spark.apache.org/docs/latest/running-on-mesos.html).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，传递给Spark-submit的JARS或Python文件应该是Mesos从节点可以访问的URI，因为Spark驱动程序不会自动上传本地jar文件。最后，Spark可以在Mesos上以两种模式运行：*粗粒度*（默认）和*细粒度*（已弃用）。有关更多详细信息，请参考[http://spark.apache.org/docs/latest/running-on-mesos.html](http://spark.apache.org/docs/latest/running-on-mesos.html)。
- en: 'In a cluster mode, the Spark driver runs on a different machine, that is, driver,
    master, and computing nodes are different machines. Therefore, if you try adding
    JARS using `SparkContext.addJar`, this will not work. To avoid this issue, make
    sure that the jar files on the client are also available to `SparkContext.addJar`,
    using the `--jars` option in the launch command:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群模式下，Spark驱动程序在不同的机器上运行，也就是说，驱动程序、主节点和计算节点是不同的机器。因此，如果尝试使用`SparkContext.addJar`添加JARS，这将不起作用。为了避免这个问题，请确保客户端上的jar文件也可以通过`SparkContext.addJar`使用启动命令中的`--jars`选项。
- en: '[PRE52]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Deploying on AWS
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AWS上部署
- en: In the previous section, we illustrated how to submit spark jobs in local, standalone,
    or deploy mode (YARN and Mesos). Here, we are going to show how to run spark application
    in real cluster mode on AWS EC2\. To make our application running on spark cluster
    mode and for better scalability, we consider the **Amazon Elastic Compute Cloud**
    (**EC2**) services as IaaS or **Platform as a Service** (**PaaS**). For pricing
    and related information, please refer to [https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们说明了如何在本地、独立或部署模式（YARN和Mesos）中提交spark作业。在这里，我们将展示如何在AWS EC2上的真实集群模式中运行spark应用程序。为了使我们的应用程序在spark集群模式下运行并实现更好的可扩展性，我们将考虑**Amazon弹性计算云**（**EC2**）服务作为IaaS或**平台即服务**（**PaaS**）。有关定价和相关信息，请参考[https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/)。
- en: 'Step 1: Key pair and access key configuration'
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤1：密钥对和访问密钥配置
- en: 'We assume that you have EC2 accounts already created. Well! The first requirement
    is to create EC2 key pairs and AWS access keys. The EC2 key pair is the private
    key that you need when you will make a secure connection through SSH to your EC2
    server or instances. For making the key, you have to go through AWS console at
    [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair).
    Please refer to the following figure that shows the key-pair creation page for
    an EC2 account:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您已经创建了EC2账户。首先要求是创建EC2密钥对和AWS访问密钥。EC2密钥对是您在通过SSH进行安全连接到EC2服务器或实例时需要的私钥。要创建密钥，您必须通过AWS控制台进行操作，网址为[http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair)。请参考以下图示，显示了EC2账户的密钥对创建页面：
- en: '![](img/00237.jpeg)**Figure 17:** AWS key-pair generation window'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00237.jpeg)**图17:** AWS密钥对生成窗口'
- en: 'Name it `aws_key_pair.pem` once you have downloaded it and save it on your
    local machine. Then ensure the permission by executing the following command (you
    should store this file in a secure location for security purpose, say `/usr/local/key`):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 下载后将其命名为`aws_key_pair.pem`并保存在本地计算机上。然后通过执行以下命令确保权限（出于安全目的，您应该将此文件存储在安全位置，例如`/usr/local/key`）：
- en: '[PRE53]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Now what you need are the AWS access keys and the credentials of your account.
    These are needed if you want to submit your Spark job to computing nodes from
    your local machine using the `spark-ec2` script. To generate and download the
    keys, login to your AWS IAM services at [http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey](http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您需要的是AWS访问密钥和您的帐户凭据。如果您希望使用`spark-ec2`脚本从本地机器提交Spark作业到计算节点，则需要这些内容。要生成并下载密钥，请登录到您的AWS
    IAM服务，网址为[http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey](http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey)。
- en: 'Upon the completion of download (that is, `/usr/local/key`), you need to set
    two environment variables in your local machine. Just execute following commands:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后（即`/usr/local/key`），您需要在本地机器上设置两个环境变量。只需执行以下命令：
- en: '[PRE54]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Step 2: Configuring Spark cluster on EC2'
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2步：在EC2上配置Spark集群
- en: Up to Spark 1.6.3 release, Spark distribution (that is, `/SPARK_HOME/ec2`) provides
    a shell script called **spark-ec2** for launching Spark Cluster in EC2 instances
    from your local machine. This eventually helps in launching, managing, and shutting
    down the Spark Cluster that you will be using on AWS. However, since Spark 2.x,
    the same script was moved to AMPLab so that it would be easier to fix bugs and
    maintain the script itself separately.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 1.6.3版本发布之前，Spark分发（即`/SPARK_HOME/ec2`）提供了一个名为**spark-ec2**的shell脚本，用于从本地机器启动EC2实例中的Spark集群。这最终有助于在AWS上启动、管理和关闭您将在其中使用的Spark集群。然而，自Spark
    2.x以来，相同的脚本已经移至AMPLab，以便更容易修复错误并单独维护脚本本身。
- en: The script can be accessed and used from the GitHub repo at [https://github.com/amplab/spark-ec2](https://github.com/amplab/spark-ec2).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本可以从GitHub仓库[https://github.com/amplab/spark-ec2](https://github.com/amplab/spark-ec2)中访问和使用。
- en: Starting and using a cluster on AWS will cost money. Therefore, it is always
    a good practice to stop or destroy a cluster when the computation is done. Otherwise,
    it will incur additional cost to you. For more about AWS pricing, please refer
    to [https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS上启动和使用集群将会产生费用。因此，当计算完成时，停止或销毁集群始终是一个好习惯。否则，这将给您带来额外的费用。有关AWS定价的更多信息，请参阅[https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/)。
- en: 'You also need to create an IAM Instance profile for your Amazon EC2 instances
    (Console). For details, refer to [http://docs.aws.amazon.com/codedeploy/latest/userguide/getting-started-create-iam-instance-profile.html](https://github.com/amplab/spark-ec2).
    For simplicity, let''s download the script and place it under a directory `ec2`
    in Spark home (`$SPARK_HOME/ec2`). Once you execute the following command to launch
    a new instance, it sets up Spark, HDFS, and other dependencies on the cluster
    automatically:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要为您的Amazon EC2实例（控制台）创建IAM实例配置文件。有关详细信息，请参阅[http://docs.aws.amazon.com/codedeploy/latest/userguide/getting-started-create-iam-instance-profile.html](https://github.com/amplab/spark-ec2)。为简单起见，让我们下载脚本并将其放置在Spark主目录（`$SPARK_HOME/ec2`）下的一个名为`ec2`的目录中。一旦您执行以下命令启动一个新实例，它会自动在集群上设置Spark、HDFS和其他依赖项：
- en: '[PRE55]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We believe that these parameters are self-explanatory. Alternatively, for more
    details, please refer to [https://github.com/amplab/spark-ec2#readme](https://github.com/amplab/spark-ec2#readme).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信这些参数是不言自明的。或者，如需更多详细信息，请参阅[https://github.com/amplab/spark-ec2#readme](https://github.com/amplab/spark-ec2#readme)。
- en: '**If you already have a Hadoop cluster and want to deploy spark on it:** If
    you are using Hadoop-YARN (or even Apache Mesos), running a spark job is relatively
    easier. Even if you don''t use either, Spark can run in standalone mode. Spark
    runs a driver program, which, in turn, invokes spark executors. This means that
    you need to tell Spark the nodes where you want your spark daemons to run (in
    terms of master/slave). In your `spark/conf` directory, you can see a file `slaves`.
    Update it to mention all the machines you want to use. You can set up spark from
    source or use a binary from the website. You always should use the **Fully Qualified
    Domain Names** (**FQDN**) for all your nodes, and make sure that each of those
    machines are passwordless SSH accessible from your master node.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您已经有一个Hadoop集群并希望在其上部署spark：**如果您正在使用Hadoop-YARN（甚至是Apache Mesos），运行spark作业相对较容易。即使您不使用其中任何一个，Spark也可以以独立模式运行。Spark运行一个驱动程序，然后调用spark执行程序。这意味着您需要告诉Spark您希望您的spark守护程序在哪些节点上运行（以主/从的形式）。在您的`spark/conf`目录中，您可以看到一个名为`slaves`的文件。更新它以提及您想要使用的所有机器。您可以从源代码设置spark，也可以从网站使用二进制文件。您应该始终为所有节点使用**完全限定域名**（**FQDN**），并确保这些机器中的每一台都可以从您的主节点无密码访问。'
- en: 'Suppose that you have already created and configured an instance profile. Now
    you are ready to launch the EC2 cluster. For our case, it would be something like
    the following:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经创建并配置了一个实例配置文件。现在您已经准备好启动EC2集群。对于我们的情况，它可能类似于以下内容：
- en: '[PRE56]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The following figure shows your Spark home on AWS:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了您在AWS上的Spark主目录：
- en: '![](img/00063.jpeg)**Figure 18:** Cluster home on AWS'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：AWS上的集群主页
- en: After the successful completion, spark cluster will be instantiated with two
    workers (slaves) nodes on your EC2 account. This task, however, sometimes might
    take half an hour approximately, depending on your Internet speed and hardware
    configuration. Therefore, you'd love to have a coffee break. Upon successful competition
    of the cluster setup, you will get the URL of the Spark cluster on the terminal.
    To make sure if the cluster is really running, check `https://<master-hostname>:8080`
    on your browser, where the `master-hostname` is the URL you receive on the terminal.
    If every think was okay, you will find your cluster running; see cluster home
    in **Figure 18**.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 成功完成后，spark集群将在您的EC2帐户上实例化两个工作节点（从节点）。然而，这个任务有时可能需要大约半个小时，具体取决于您的互联网速度和硬件配置。因此，您可能想要休息一下。在集群设置成功完成后，您将在终端上获得Spark集群的URL。为了确保集群真的在运行，可以在浏览器上检查`https://<master-hostname>:8080`，其中`master-hostname`是您在终端上收到的URL。如果一切正常，您将发现您的集群正在运行；请参见**图18**中的集群主页。
- en: 'Step 3: Running Spark jobs on the AWS cluster'
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3步：在AWS集群上运行Spark作业
- en: 'Now you master and worker nodes are active and running. This means that you
    can submit your Spark job to them for computing. However, before that, you need
    to log in the remote nodes using SSH. For doing so, execute the following command
    to SSH remote Spark cluster:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的主节点和工作节点都是活动的并正在运行。这意味着您可以将Spark作业提交给它们进行计算。但在此之前，您需要使用SSH登录远程节点。为此，请执行以下命令以SSH远程Spark集群：
- en: '[PRE57]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'For our case, it should be something like the following:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的情况，应该是以下内容：
- en: '[PRE58]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now copy your application, that is, JAR file (or python/R script) to the remote
    instance (that is, `ec2-52-48-119-121.eu-west-1.compute.amazonaws.com` in our
    case) by executing the following command (in a new terminal):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将您的应用程序，即JAR文件（或python/R脚本），复制到远程实例（在我们的情况下是`ec2-52-48-119-121.eu-west-1.compute.amazonaws.com`）中，通过执行以下命令（在新的终端中）：
- en: '[PRE59]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Then you need to copy your data (`/usr/local/data/Saratoga_NY_Homes.txt`, in
    our case) to the same remote instance by executing the following command:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过执行以下命令将您的数据（在我们的情况下是`/usr/local/data/Saratoga_NY_Homes.txt`）复制到同一远程实例：
- en: '[PRE60]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Note that if you have already configured HDFS on your remote machine and put
    your code/data file, you don't need to copy the JAR and data files to the slaves;
    the master will do it automatically.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您已经在远程机器上配置了HDFS并放置了您的代码/数据文件，您就不需要将JAR和数据文件复制到从节点；主节点会自动执行这些操作。
- en: 'Well done! You are almost done! Now, finally, you will have to submit your
    Spark job to be computed by the slaves or worker nodes. To do so, just execute
    the following commands:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！您几乎完成了！现在，最后，您需要提交您的Spark作业以由从节点进行计算。要这样做，只需执行以下命令：
- en: '[PRE61]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Place your input file under `file:///input.txt` if HDFS is not set on your machine.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的机器上没有设置HDFS，请将输入文件放在`file:///input.txt`下。
- en: 'If you have already put your data on HDFS, you should issue the submit command
    something like following:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经将数据放在HDFS上，您应该发出类似以下命令的提交命令：
- en: '[PRE62]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Upon successful completion of the job computation, you are supposed to see the
    status and related statistics of your job at port 8080.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在作业计算成功完成后，您应该在端口8080上看到作业的状态和相关统计信息。
- en: 'Step 4: Pausing, restarting, and terminating the Spark cluster'
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4步：暂停、重新启动和终止Spark集群
- en: 'When your computation is done, it is better to stop your cluster to avoid additional
    cost. To stop your clusters, execute the following commands from your local machine:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的计算完成后，最好停止您的集群以避免额外的成本。要停止您的集群，请从本地机器执行以下命令：
- en: '[PRE63]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'For our case, it would be the following:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的情况，应该是以下内容：
- en: '[PRE64]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'To restart the cluster later on, execute the following command:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 要在以后重新启动集群，请执行以下命令：
- en: '[PRE65]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'For our case, it will be something like the following:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的情况，应该是以下内容：
- en: '[PRE66]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Finally, to terminate your Spark cluster on AWS we use the following code:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要在AWS上终止您的Spark集群，我们使用以下代码：
- en: '[PRE67]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'In our case, it would be the following:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，应该是以下内容：
- en: '[PRE68]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Spot instances are great for reducing AWS costs, sometimes cutting instance
    costs by a whole order of magnitude. A step-by-step guideline using this facility
    can be accessed at [http://blog.insightdatalabs.com/spark-cluster-step-by-step/](http://blog.insightdatalabs.com/spark-cluster-step-by-step/).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Spot实例非常适合降低AWS成本，有时可以将实例成本降低一个数量级。使用这种设施的逐步指南可以在[http://blog.insightdatalabs.com/spark-cluster-step-by-step/](http://blog.insightdatalabs.com/spark-cluster-step-by-step/)上找到。
- en: Sometimes, it's difficult to move large dataset, say 1 TB of raw data file.
    In that case, and if you want your application to scale up even more for large-scale
    datasets, the fastest way of doing so is loading them from Amazon S3 or EBS device
    to HDFS on your nodes and specifying the data file path using `hdfs://`.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，移动大型数据集，比如1TB的原始数据文件，是困难的。在这种情况下，如果您希望您的应用程序能够扩展到更大规模的数据集，最快的方法是将它们从Amazon
    S3或EBS设备加载到节点上的HDFS，并使用`hdfs://`指定数据文件路径。
- en: 'The data files or any other files (data, jars, scripts, and so on) can be hosted
    on HDFS to make them highly accessible:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 数据文件或任何其他文件（数据、jar包、脚本等）都可以托管在HDFS上，以使它们具有高度的可访问性：
- en: 1\. Having the URIs/URLs (including HTTP) via `http://`
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 通过`http://`获取URI/URL（包括HTTP）
- en: 2\. Using the Amazon S3 via `s3n://`
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 通过`s3n://`使用Amazon S3
- en: 3\. Using the HDFS via `hdfs://`
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 通过`hdfs://`使用HDFS
- en: If you set `HADOOP_CONF_DIR` environment variable, the parameter is usually
    set as `hdfs://...`; otherwise `file://`.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设置了`HADOOP_CONF_DIR`环境变量，参数通常设置为`hdfs://...`；否则为`file://`。
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how Spark works in a cluster mode with its underlying
    architecture. You also saw how to deploy a full Spark application on a cluster.
    You saw how to deploy cluster for running Spark application in different cluster
    modes such as local, standalone, YARN, and Mesos. Finally, you saw how to configure
    Spark cluster on AWS using EC2 script. We believe that this chapter will help
    you to gain some good understanding of Spark. Nevertheless, due to page limitation,
    we could not cover many APIs and their underlying functionalities.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了Spark在集群模式下的工作原理及其基础架构。您还看到了如何在集群上部署完整的Spark应用程序。您看到了如何在不同的集群模式（如本地、独立、YARN和Mesos）中部署集群以运行Spark应用程序。最后，您看到了如何使用EC2脚本在AWS上配置Spark集群。我们相信本章将帮助您对Spark有一些良好的理解。然而，由于页面限制，我们无法涵盖许多API及其底层功能。
- en: If you face any issues, please don't forget to report this to Spark user mailing
    list at `user@spark.apache.org`. Before doing so, make sure that you have subscribed
    to it. In the next chapter, you will see how to test and debug Spark applications.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遇到任何问题，请不要忘记向Spark用户邮件列表`user@spark.apache.org`报告。在这样做之前，请确保您已经订阅了它。在下一章中，您将看到如何测试和调试Spark应用程序。
