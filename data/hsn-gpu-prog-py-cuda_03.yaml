- en: Getting Started with PyCUDA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyCUDA 入门
- en: In the last chapter, we set up our programming environment. Now, with our drivers
    and compilers firmly in place, we will begin the actual GPU programming! We will
    start by learning how to use PyCUDA for some basic and fundamental operations.
    We will first see how to query our GPU—that is, we will start by writing a small
    Python program that will tell us what the characteristics of our GPU are, such
    as the core count, architecture, and memory. We will then spend some time getting
    acquainted with how to transfer memory between Python and the GPU with PyCUDA's
    `gpuarray` class and how to use this class for basic computations. The remainder
    of this chapter will be spent showing how to write some basic functions (which
    we will refer to as **CUDA Kernels**) that we can directly launch onto the GPU.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们设置了我们的编程环境。现在，随着我们的驱动程序和编译器的稳定，我们将开始实际的 GPU 编程！我们将从学习如何使用 PyCUDA 进行一些基本和基本操作开始。我们首先将了解如何查询我们的
    GPU——也就是说，我们将编写一个小型的 Python 程序，它会告诉我们我们 GPU 的特性，例如核心数量、架构和内存。然后，我们将花一些时间熟悉如何使用
    PyCUDA 的 `gpuarray` 类在 Python 和 GPU 之间传输内存，以及如何使用此类进行基本计算。本章的其余部分将展示如何编写一些基本函数（我们将它们称为
    **CUDA 内核**），我们可以直接在 GPU 上启动这些函数。
- en: 'The learning outcomes for this chapter are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的学习成果如下：
- en: Determining GPU characteristics, such as memory capacity or core count, using
    PyCUDA
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyCUDA 确定GPU特性，如内存容量或核心数量
- en: Understanding the difference between host (CPU) and device (GPU) memory and
    how to use PyCUDA's `gpuarray` class to transfer data between the host and device
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解主机（CPU）和设备（GPU）内存之间的区别以及如何使用 PyCUDA 的 `gpuarray` 类在主机和设备之间传输数据
- en: How to do basic calculations using only `gpuarray` objects
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何仅使用 `gpuarray` 对象进行基本计算
- en: How to perform basic element-wise operations on the GPU with the PyCUDA `ElementwiseKernel`
    function
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 PyCUDA 的 `ElementwiseKernel` 函数在 GPU 上执行基本的元素级操作
- en: Understanding the functional programming concept of reduce/scan operations and
    how to make a basic reduction or scan CUDA kernel
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 reduce/scan 操作的函数式编程概念以及如何创建基本的 reduce 或 scan CUDA 内核
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016 onward) is required
    for this chapter, with all necessary GPU drivers and the CUDA Toolkit (9.0 onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要一台配备现代 NVIDIA GPU（2016 年及以后）的 Linux 或 Windows 10 PC，并安装所有必要的 GPU 驱动程序和 CUDA
    Toolkit（9.0 及以后版本）。还需要一个合适的 Python 2.7 安装（例如 Anaconda Python 2.7），并包含 PyCUDA 模块。
- en: This chapter's code is also available on GitHub at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码也可在 GitHub 上找到，链接为 [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)。
- en: For more information about the prerequisites, check the *Preface* of this book;
    for the software and hardware requirements, check the `README` section in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于先决条件的信息，请参阅本书的 *前言*；关于软件和硬件要求，请查看 [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)
    中的 `README` 部分。
- en: Querying your GPU
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询您的 GPU
- en: 'Before we begin to program our GPU, we should really know something about its
    technical capacities and limits. We can determine this by doing what is known
    as a **GPU query**. A GPU query is a very basic operation that will tell us the
    specific technical details of our GPU, such as available GPU memory and core count.
    NVIDIA includes a command-line example written in pure CUDA-C called `deviceQuery`
    in the `samples` directory (for both Windows and Linux) that we can run to perform
    this operation. Let''s take a look at the output that is produced on the author''s
    Windows 10 laptop (which is a Microsoft Surface Book 2 with a GTX 1050 GPU):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始编写我们的 GPU 程序之前，我们真的应该了解一些关于其技术能力和限制的知识。我们可以通过执行所谓的 **GPU 查询** 来确定这一点。GPU
    查询是一个非常基本的操作，它将告诉我们我们 GPU 的具体技术细节，例如可用的 GPU 内存和核心数量。NVIDIA 在 `samples` 目录中包含了一个名为
    `deviceQuery` 的纯 CUDA-C 命令行示例（适用于 Windows 和 Linux），我们可以运行此操作。让我们看看作者在 Windows
    10 笔记本电脑（这是一台配备 GTX 1050 GPU 的 Microsoft Surface Book 2）上产生的输出：
- en: '![](img/ef6b22de-9871-49b2-ad73-4e7aff2017ac.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ef6b22de-9871-49b2-ad73-4e7aff2017ac.png)'
- en: 'Let''s look at some of the essentials of all of the technical information displayed
    here. First, we see that there is only one GPU installed, Device 0—it is possible
    that a host computer has multiple GPUs and makes use of them, so CUDA will designate
    each *GPU device* an individual number. There are some cases where we may have
    to be specific about the device number, so it is always good to know. We can also
    see the specific type of device that we have (here, GTX 1050), and which CUDA
    version we are using. There are two more things we will take note of for now:
    the total number of cores (here, 640), and the total amount of global memory on
    the device (in this case, 2,048 megabytes, that is, 2 gigabytes).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这里显示的所有技术信息的要点。首先，我们看到只有一个GPU安装，设备0——可能主机计算机有多个GPU并使用它们，因此CUDA将为每个*GPU设备*分配一个单独的编号。在某些情况下，我们可能需要具体指定设备编号，所以了解这一点总是好的。我们还可以看到我们拥有的特定设备类型（在这里，GTX
    1050），以及我们正在使用的CUDA版本。目前，我们还将注意两件事：核心总数（在这里，640），以及设备上的全局内存总量（在这种情况下，2,048兆字节，即2千兆字节）。
- en: While you can see many other technical details from `deviceQuery`, the core
    count and amount of memory are usually the first two things your eyes should zero
    in on the first time you run this on a new GPU, since they can give you the most
    immediate idea of the capacity of your new device.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以从`deviceQuery`中看到许多其他技术细节，但核心数和内存量通常是你在新GPU上第一次运行此程序时应该首先关注的前两件事，因为它们可以给你关于新设备容量的最直接的想法。
- en: Querying your GPU with PyCUDA
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyCUDA查询GPU
- en: Now, finally, we will begin our foray into the world of GPU programming by writing
    our own version of `deviceQuery` in Python. Here, we will primarily concern ourselves
    with only the amount of available memory on the device, the compute capability,
    the number of multiprocessors, and the total number of CUDA cores.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最后，我们将通过编写我们自己的`deviceQuery`版本来开始我们的GPU编程之旅。在这里，我们主要关注设备上的可用内存量、计算能力、多处理器数量以及CUDA核心总数。
- en: 'We will begin by initializing CUDA as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先按照以下方式初始化CUDA：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that we will always have to initialize PyCUDA with `pycuda.driver.init()`
    or by importing the PyCUDA `autoinit` submodule with `import pycuda.autoinit`!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们总是必须使用`pycuda.driver.init()`或通过导入PyCUDA的`autoinit`子模块`import pycuda.autoinit`来初始化PyCUDA！
- en: 'We can now immediately check how many GPU devices we have on our host computer
    with this line:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以立即检查我们的主机计算机上有多少GPU设备，使用以下行：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s type this into IPython and see what happens:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其输入到IPython中，看看会发生什么：
- en: '![](img/9c6850ad-552d-48ed-a6d5-4145c4f7407f.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c6850ad-552d-48ed-a6d5-4145c4f7407f.png)'
- en: Great! So far, I have verified that my laptop does indeed have one GPU in it.
    Now, let's extract some more interesting information about this GPU (and any other
    GPU on the system) by adding a few more lines of code to iterate over each device
    that can be individually accessed with `pycuda.driver.Device` (indexed by number).
    The name of the device (for example, GeForce GTX 1050) is given by the `name`
    function. We then get the **compute capability** of the device with the `compute_capability`
    function and total amount of device memory with the `total_memory` function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！到目前为止，我已经验证了我的笔记本电脑确实有一个GPU。现在，让我们通过添加一些额外的代码来迭代每个可以通过`pycuda.driver.Device`（按编号索引）单独访问的设备，以提取有关此GPU（以及系统上的任何其他GPU）的一些更有趣的信息。设备的名称（例如，GeForce
    GTX 1050）由`name`函数给出。然后我们使用`compute_capability`函数获取设备的**计算能力**，以及使用`total_memory`函数获取设备内存的总量。
- en: '**Compute capability** can be thought of as a *version number* for each NVIDIA
    GPU architecture; this will give us some important information about the device
    that we can''t otherwise query, as we will see in a minute.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算能力**可以被视为每个NVIDIA GPU架构的*版本号*；这将给我们一些重要的信息，这些信息我们无法通过其他方式查询，正如我们将在下面看到的那样。'
- en: 'Here''s how we will write it:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将如何编写它的：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we are ready to look at some of the remaining attributes of our GPU, which
    PyCUDA yields to us in the form of a Python dictionary type. We will use the following
    lines to convert this into a dictionary that is indexed by strings indicating
    attributes:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好查看GPU的一些剩余属性，PyCUDA以Python字典类型的形式提供给我们。我们将使用以下行将其转换为按表示属性的字符串索引的字典：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now determine the number of *multiprocessors* on our device with the
    following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下方式确定设备上的多处理器数量：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A GPU divides its individual cores up into larger units known as **Streaming**
    **Multiprocessors (SMs)**; a GPU device will have several SMs, which will each
    individually have a particular number of CUDA cores, depending on the compute
    capability of the device. To be clear: the number of cores per multiprocessor
    is not indicated directly by the GPU—this is given to us implicitly by the compute
    capability. We will have to look up some technical documents from NVIDIA to determine
    the number of cores per multiprocessor (see [http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities)),
    and then create a lookup table to give us the number of cores per multiprocessor.
    We do so as such, using the `compute_capability` variable to look up the number
    of cores:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 将其单个核心划分为更大的单元，称为 **流式多处理器 (SMs)**；一个 GPU 设备将具有多个 SMs，每个 SM 将根据设备的计算能力具有特定数量的
    CUDA 核心。为了明确：每个多处理器的核心数不是直接由 GPU 指示的——这是通过计算能力隐式给出的。我们将不得不查阅一些来自 NVIDIA 的技术文档来确定每个多处理器的核心数（见
    [http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities)），然后创建一个查找表以给出每个多处理器的核心数。我们使用
    `compute_capability` 变量来查找核心数：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now finally determine the total number of cores on our device by multiplying
    these two numbers:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过将这两个数字相乘来最终确定我们设备上的核心总数：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We now can finish up our program by iterating over the remaining keys in our
    dictionary and printing the corresponding values:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过遍历字典中剩余的键并打印相应的值来完成我们的程序：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'So, now we finally completed our first true GPU program of the text! (Also
    available at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py)).
    Now, we can run it as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们终于完成了文本中的第一个真正的 GPU 程序！（也可在 [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py)
    找到。）现在，我们可以按照以下方式运行它：
- en: '![](img/59a5907a-0a76-4c08-bfe6-349d9ce48c71.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59a5907a-0a76-4c08-bfe6-349d9ce48c71.png)'
- en: We can now have a little pride that we can indeed write a program to query our
    GPU! Now, let's actually begin to learn to *use* our GPU, rather than just observe
    it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以有点自豪，因为我们确实可以编写一个查询我们的 GPU 的程序！现在，让我们真正开始学习如何 *使用* 我们的 GPU，而不仅仅是观察它。
- en: Using PyCUDA's gpuarray class
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyCUDA 的 gpuarray 类
- en: Much like how NumPy's `array` class is the cornerstone of numerical programming
    within the NumPy environment, PyCUDA's `gpuarray` class plays an analogously prominent
    role within GPU programming in Python. This has all of the features you know and
    love from NumPy—multidimensional vector/matrix/tensor shape structuring, array-slicing,
    array unraveling, and overloaded operators for point-wise computations (for example,
    `+`, `-`, `*`, `/`, and `**`).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 NumPy 的 `array` 类是 NumPy 环境中数值编程的基石一样，PyCUDA 的 `gpuarray` 类在 Python 的 GPU
    编程中扮演着类似突出的角色。它具有您所熟知和喜爱的所有 NumPy 功能——多维向量/矩阵/张量形状结构、数组切片、数组展开，以及用于点运算的重载运算符（例如，`+`、`-`、`*`、`/`
    和 `**`）。
- en: '`gpuarray` is really an indispensable tool for any budding GPU programmer.
    We will spend this section going over this particular data structure and gaining
    a strong grasp of it before we move on.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`gpuarray` 真的是任何初学者 GPU 程序员不可或缺的工具。在本节中，我们将详细介绍这个特定的数据结构，并在继续前进之前对其有一个牢固的理解。'
- en: Transferring data to and from the GPU with gpuarray
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 gpuarray 在 GPU 之间传输数据
- en: As we note from writing our prior `deviceQuery` program in Python, a GPU has
    its own memory apart from the host computer's memory, which is known as **device
    memory**. (Sometimes this is known more specifically as **global device memory***,*
    to differentiate this from the additional cache memory, shared memory, and register
    memory that is also on the GPU.) For the most part, we treat (global) device memory
    on the GPU as we do dynamically allocated heap memory in C (with the `malloc`
    and `free` functions) or C++ (as with the `new` and `delete` operators); in CUDA
    C, this is complicated further with the additional task of transferring data back
    and forth between the CPU to the GPU (with commands such as `cudaMemcpyHostToDevice`
    and `cudaMemcpyDeviceToHost`), all while keeping track of multiple pointers in
    both the CPU and GPU space and performing proper memory allocations (`cudaMalloc`)
    and deallocations (`cudaFree`).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从先前的Python编写的`deviceQuery`程序中注意到的，GPU有自己的内存，除了主机计算机的内存之外，这被称为**设备内存**。（有时这更具体地被称为**全局设备内存**，以区分GPU上额外的缓存内存、共享内存和寄存器内存。）大部分情况下，我们将GPU上的（全局）设备内存视为C（使用`malloc`和`free`函数）或C++（使用`new`和`delete`运算符）中的动态分配堆内存；在CUDA
    C中，这由于需要在CPU和GPU之间来回传输数据（使用如`cudaMemcpyHostToDevice`和`cudaMemcpyDeviceToHost`等命令）而变得更加复杂，同时还要在CPU和GPU空间中跟踪多个指针，并执行适当的内存分配（`cudaMalloc`）和释放（`cudaFree`）。
- en: Fortunately, PyCUDA covers all of the overhead of memory allocation, deallocation,
    and data transfers with the `gpuarray` class. As stated, this class acts similarly
    to NumPy arrays, using vector/ matrix/tensor shape structure information for the
    data. `gpuarray` objects even perform automatic cleanup based on the lifetime,
    so we do not have to worry about *freeing* any GPU memory stored in a `gpuarray`
    object when we are done with it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，PyCUDA通过`gpuarray`类涵盖了所有内存分配、释放和数据传输的开销。正如所述，这个类与NumPy数组类似，使用向量/矩阵/张量形状结构信息来处理数据。`gpuarray`对象甚至根据生命周期自动清理，因此当我们完成使用时，我们不需要担心在`gpuarray`对象中释放任何GPU内存。
- en: How exactly do we use this to transfer data from the host to the GPU? First,
    we must contain our host data in some form of NumPy array (let's call it `host_data`),
    and then use the `gpuarray.to_gpu(host_data)` command to transfer this over to
    the GPU and create a new GPU array.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们究竟如何使用这个方法将数据从主机传输到GPU呢？首先，我们必须以某种形式将主机数据包含在NumPy数组中（让我们称它为`host_data`），然后使用`gpuarray.to_gpu(host_data)`命令将数据传输到GPU并创建一个新的GPU数组。
- en: 'Let''s now perform a simple computation within the GPU (pointwise multiplication
    by a constant on the GPU), and then retrieve the GPU data into a new with the
    `gpuarray.get` function. Let''s load up IPython and see how this works (note that
    here we will initialize PyCUDA with `import pycuda.autoinit`):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将在GPU内部执行一个简单的计算（在GPU上对常数进行逐点乘法），然后使用`gpuarray.get`函数将GPU数据检索到一个新的数组中。让我们加载IPython来看看这是如何工作的（注意，在这里我们将使用`import
    pycuda.autoinit`初始化PyCUDA）：
- en: '![](img/14eef3e5-273f-45c9-b42f-99d07628f9d8.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/14eef3e5-273f-45c9-b42f-99d07628f9d8.png)'
- en: 'One thing to note is that we specifically denoted that the array on the host
    had its type specifically set to a NumPy `float32` type with the `dtype` option
    when we set up our NumPy array; this corresponds directly with the float type
    in C/C++. Generally speaking, it''s a good idea to specifically set data types
    with NumPy when we are sending data to the GPU. The reason for this is twofold:
    first, since we are using a GPU for increasing the performance of our application,
    we don''t want any unnecessary overhead of using an unnecessary type that will
    possibly take up more computational time or memory, and second, since we will
    soon be writing portions of code in inline CUDA C, we will have to be very specific
    with types or our code won''t work correctly, keeping in mind that C is a statically-typed
    language.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个需要注意的事项是，当我们设置NumPy数组时，我们特别指定了主机上的数组类型被设置为NumPy的`float32`类型，这是通过`dtype`选项来实现的；这直接对应于C/C++中的浮点类型。一般来说，当我们向GPU发送数据时，明确设置数据类型是一个好主意。这样做的原因有两方面：首先，因为我们使用GPU来提高应用程序的性能，我们不希望使用任何不必要的类型，这可能会占用更多的计算时间或内存；其次，由于我们很快将编写部分CUDA
    C代码，我们必须非常具体地指定类型，否则我们的代码将无法正确运行，考虑到C是一种静态类型语言。
- en: Remember to specifically set data types for NumPy arrays that will be transferred
    to the GPU. This can be done with the `dtype` option in the constructor of the
    `numpy.array` class.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 记住要为将要传输到GPU的NumPy数组明确设置数据类型。这可以通过`numpy.array`类的构造函数中的`dtype`选项来完成。
- en: Basic pointwise arithmetic operations with gpuarray
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用gpuarray进行基本点对点算术运算
- en: 'In the last example, we saw that we can use the (overloaded) Python multiplication
    operator (`*` ) to multiply each element in a `gpuarray` object by a scalar value
    (here it was 2); note that a pointwise operation is intrinsically parallelizable,
    and so when we use this operation on a `gpuarray` object PyCUDA is able to offload
    each multiplication operation onto a single thread, rather than computing each
    multiplication in serial, one after the other (in fairness, some versions of NumPy
    can use the advanced SSE instructions found in modern x86 chips for these computations,
    so in some cases the performance will be comparable to a GPU). To be clear: these
    pointwise operations performed on the GPU are in parallel since the computation
    of one element is not dependent on the computation of any other element.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个例子中，我们看到了我们可以使用（重载的）Python乘法运算符（`*`）将`gpuarray`对象中的每个元素乘以一个标量值（这里为2）；请注意，点对点操作本质上是可并行化的，因此当我们对`gpuarray`对象使用此操作时，PyCUDA能够将每个乘法操作卸载到单个线程，而不是按顺序串行计算每个乘法（公平地说，某些版本的NumPy可以使用现代x86芯片中发现的先进SSE指令进行这些计算，因此在某些情况下性能将与GPU相当）。为了明确：在GPU上执行这些点对点操作是并行的，因为一个元素的计算不依赖于任何其他元素的计算。
- en: 'To get a feel for how the operators work, I would suggest that the reader load
    up IPython and create a few `gpuarray` objects on the GPU, and then play around
    with these operations for a few minutes to see that these operators do work similarly
    to arrays in NumPy. Here is some inspiration:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解操作符的工作方式，我建议读者加载IPython并在GPU上创建几个`gpuarray`对象，然后花几分钟时间对这些操作进行实验，以确认这些操作符的工作方式与NumPy中的数组相似。以下是一些灵感：
- en: '![](img/fd5469e2-c573-472e-a1a9-6da1dc61ddc5.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fd5469e2-c573-472e-a1a9-6da1dc61ddc5.png)'
- en: Now, we can see that `gpuarray` objects act predictably and are in accordance
    with how NumPy arrays act. (Notice that we will have to pull the output off the
    GPU with the `get` function!) Let's now do some comparison between CPU and GPU
    computation time to see if and when there is any advantage to doing these operations
    on the GPU.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到`gpuarray`对象的行为可预测，并且与NumPy数组的行为一致。（请注意，我们将必须使用`get`函数将输出从GPU拉取！）现在，让我们做一些CPU和GPU计算时间的比较，看看是否以及何时在GPU上执行这些操作有任何优势。
- en: A speed test
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 速度测试
- en: 'Let''s write up a little program (`time_calc0.py`) that will do a speed comparison
    test between a scalar multiplication on the CPU and then the same operation on
    the GPU. We will then use NumPy''s `allclose` function to compare the two output
    values. We will generate an array of 50 million random 32-bit floating point values
    (this will amount to roughly 48 megabytes of data, so this should be entirely
    feasible with several gigabytes of memory on any somewhat modern host and GPU
    device), and then we will time how long it takes to scalar multiply the array
    by two on both devices. Finally, we will compare the output values to ensure that
    they are equal. Here''s how it''s done:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个小程序（`time_calc0.py`），它将在CPU上执行标量乘法，然后在GPU上执行相同的操作。然后我们将使用NumPy的`allclose`函数来比较两个输出值。我们将生成一个包含5000万个随机32位浮点数的数组（这将大约是48兆字节的数据，所以这应该在任何稍微现代的主机和GPU设备上使用几个GB的内存都是完全可行的），然后我们将测量在两个设备上对数组进行标量乘以2所需的时间。最后，我们将比较输出值以确保它们相等。以下是具体操作方法：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: (You can find the `time_calc0.py` file on the repository provided to you earlier.)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: （您可以在您之前提供的存储库中找到`time_calc0.py`文件。）
- en: 'Now, let''s load up IPython and run this a few times to get an idea of the
    general speed of these, and see if there is any variance. (Here, this is being
    run on a 2017-era Microsoft Surface Book 2 with a Kaby Lake i7 processor and a
    GTX 1050 GPU.):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载IPython并运行几次，以了解这些操作的总体速度，并看看是否有任何变化。（这里是在2017年的Microsoft Surface Book
    2上运行的，配备Kaby Lake i7处理器和GTX 1050 GPU。）
- en: '![](img/a278a2f1-e099-4907-805f-708f2884a7c3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a278a2f1-e099-4907-805f-708f2884a7c3.png)'
- en: We first notice that the CPU computation time is about the same for each computation
    (roughly 0.08 seconds). Yet, we notice that the GPU computation time is far slower
    than the CPU computation the first time we run this (1.09 seconds), and it becomes
    much faster in the subsequent run, which remains roughly constant in every following
    run (in the range of 7 or 9 milliseconds). If you exit IPython, and then run the
    program again, the same thing will occur. What is the reason for this phenomenon?
    Well, let's do some investigative work using IPython's built-in `prun` profiler.
    (This works similarly to the `cProfiler` module that was featured in [Chapter
    1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml), *Why GPU Programming?*.)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先注意到，对于每次计算，CPU 计算时间大致相同（大约 0.08 秒）。然而，我们注意到，第一次运行这个程序时，GPU 计算时间远慢于 CPU 计算时间（1.09
    秒），而在随后的运行中变得更快，并且在每次随后的运行中保持大致恒定（在 7 或 9 毫秒的范围内）。如果你退出 IPython，然后再次运行程序，同样的事情会发生。这种现象的原因是什么？好吧，让我们使用
    IPython 内置的 `prun` 分析器做一些调查工作。（这与在[第 1 章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml)，*为什么进行
    GPU 编程？*中介绍的 `cProfiler` 模块的工作方式类似。）
- en: 'First, let''s load our program as text within IPython with the following lines,
    which we can then run with our profiler via Python''s `exec` command:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用以下行在 IPython 中将我们的程序作为文本加载，然后我们可以通过 Python 的 `exec` 命令使用我们的分析器运行它：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We now type `%prun -s cumulative exec(time_calc_code)` into our IPython console
    (with the leading `%`) and see what operations are taking the most time:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在 IPython 控制台中输入 `%prun -s cumulative exec(time_calc_code)`（带有前导 `%`）并查看哪些操作花费了最多时间：
- en: '![](img/7dfbfc79-dcc1-4cc8-b7b6-7f11103f54e6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7dfbfc79-dcc1-4cc8-b7b6-7f11103f54e6.png)'
- en: 'Now, there are a number of suspicious calls to a Python module file, `compiler.py`;
    these take roughly one second total, a little less than the time it takes to do
    the GPU computation here. Now let''s run this again and see if there are any differences:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有一些可疑的对 Python 模块文件 `compiler.py` 的调用；这些调用总共花费大约一秒钟，略少于在这里进行 GPU 计算所需的时间。现在让我们再次运行并看看是否有任何差异：
- en: '![](img/da5995b8-f05d-45d7-950c-f921d79b3886.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/da5995b8-f05d-45d7-950c-f921d79b3886.png)'
- en: Notice that this time, there are no calls to `compiler.py`. Why is this? By
    the nature of the PyCUDA library, GPU code is often compiled and linked with NVIDIA's
    `nvcc` compiler the first time it is run in a given Python session; it is then
    cached and, if the code is called again, then it doesn't have to be recompiled.
    This may include even *simple* operations such as this scalar multiply! (We will
    see eventually see that this can be ameliorated by using the pre-compiled code
    in, [Chapter 10](5383b46f-8dc6-4e17-ab35-7f6bd35f059f.xhtml), *Working with Compiled
    GPU Code*, or by using NVIDIA's own linear algebra libraries with the Scikit-CUDA
    module, which we will see in [Chapter 7](55146879-4b7e-4774-9a8b-cc5c80c04ed8.xhtml),
    *Using the CUDA Libraries with Scikit-CUDA*).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这次没有对 `compiler.py` 的调用。为什么是这样？根据 PyCUDA 库的性质，GPU 代码在给定 Python 会话中第一次运行时通常使用
    NVIDIA 的 `nvcc` 编译器进行编译和链接；然后它被缓存起来，如果代码再次被调用，则不需要重新编译。这甚至可能包括像这样的标量乘法这样的简单操作！（我们最终会看到，这可以通过在[第
    10 章](5383b46f-8dc6-4e17-ab35-7f6bd35f059f.xhtml)，*与编译的 GPU 代码一起工作*中使用预编译的代码来改善，或者通过使用
    Scikit-CUDA 模块与 NVIDIA 自己的线性代数库，我们将在[第 7 章](55146879-4b7e-4774-9a8b-cc5c80c04ed8.xhtml)，*使用
    CUDA 库与 Scikit-CUDA*中看到）。
- en: In PyCUDA, GPU code is often compiled at runtime with the NVIDIA `nvcc` compiler
    and then subsequently called from PyCUDA. This can lead to an unexpected slowdown,
    usually the first time a program or GPU operation is run in a given Python session.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyCUDA 中，GPU 代码通常在运行时使用 NVIDIA `nvcc` 编译器进行编译，然后由 PyCUDA 从外部调用。这可能导致意外的减速，通常是在给定
    Python 会话中第一次运行程序或 GPU 操作时。
- en: Using PyCUDA's ElementWiseKernel for performing pointwise computations
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyCUDA 的 ElementWiseKernel 进行逐点计算
- en: We will now see how to program our own point-wise (or equivalently, *element-wise*)
    operations directly onto our GPU with the help of PyCUDA's `ElementWiseKernel`
    function. This is where our prior knowledge of C/C++ programming will become useful—we'll
    have to write a little bit of *inline code* in CUDA C, which is compiled externally
    by NVIDIA's `nvcc` compiler and then launched at runtime by our code via PyCUDA.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到如何使用 PyCUDA 的 `ElementWiseKernel` 函数直接在我们的 GPU 上编程我们的自己的逐点（或等价地，*逐元素*）操作。这是我们的先前
    C/C++ 编程知识变得有用的地方——我们将不得不编写一些 *内联代码*，这些代码由 NVIDIA 的 `nvcc` 编译器外部编译，然后通过我们的代码通过
    PyCUDA 在运行时启动。
- en: We use the term **kernel** quite a bit in this text; by *kernel*, we always
    mean a function that is launched directly onto the GPU by CUDA. We will use several
    functions from PyCUDA that generate templates and design patterns for different
    types of kernels, easing our transition into GPU programming.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们经常使用术语**内核**；通过**内核**，我们始终指的是由CUDA直接在GPU上启动的函数。我们将使用PyCUDA的几个函数来生成不同类型内核的模板和设计模式，这将有助于我们过渡到GPU编程。
- en: 'Let''s dive right in; we''re going to start by explicitly rewriting the code
    to multiply each element of a `gpuarray` object by 2 in CUDA-C; we will use the
    `ElementwiseKernel` function from PyCUDA to generate our code. You should try
    typing the following code directly into an IPython console. (The less adventurous
    can just download this from this text''s Git repository, which has the filename
    `simple_element_kernel_example0.py`):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接深入探讨；我们将首先明确重写代码，以在CUDA-C中将`gpuarray`对象的每个元素乘以2；我们将使用PyCUDA的`ElementwiseKernel`函数来生成我们的代码。你应该尝试直接在IPython控制台中输入以下代码。（不那么冒险的人可以从中下载这个文本的Git仓库中的文件，文件名为`simple_element_kernel_example0.py`）：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let's take a look at how this is set up; this is, of course, several lines of
    inline C. We first set the input and output variables in the first line ( `"float
    *in, float *out"` ), which will generally be in the form of C pointers to allocated
    memory on the GPU. In the second line, we define our element-wise operation with
    `"out[i] = 2*in[i];"`, which will multiply each point in `in` by two and place
    this in the corresponding index of `out`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是如何设置的；这当然是一段内联C代码。我们首先在第一行设置输入和输出变量（`"float *in, float *out"`），这通常是以C指针的形式在GPU上分配的内存。在第二行，我们定义了我们的逐元素操作`"out[i]
    = 2*in[i];"`，这将把`in`中的每个点乘以2，并将其放置在`out`的相应索引中。
- en: Note that PyCUDA automatically sets up the integer index `i` for us. When we
    use `i` as our index, `ElementwiseKernel` will automatically parallelize our calculation
    over `i` among the many cores in our GPU. Finally, we give our piece of code its
    internal CUDA C kernel name ( `"gpu_2x_ker"` ). Since this refers to CUDA C's
    namespace and not Python's, it's fine (and also convenient) to give this the same
    name as in Python.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意PyCUDA会自动为我们设置整数索引`i`。当我们使用`i`作为索引时，`ElementwiseKernel`将自动在我们的GPU的许多核心之间并行化我们的计算。最后，我们给我们的代码块赋予其内部的CUDA
    C内核名称（`"gpu_2x_ker"`）。由于这指的是CUDA C的命名空间而不是Python的，所以给它取与Python中相同的名字是完全可以的（并且也很方便）。
- en: 'Now, let''s do a speed comparison:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行速度比较：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let''s run this program:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行这个程序：
- en: '![](img/02db7f9f-e682-41fb-af4c-2833d054a746.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/02db7f9f-e682-41fb-af4c-2833d054a746.png)'
- en: 'Whoa! That doesn''t look good. Let''s run the `speedcomparison()` function
    a few times from IPython:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！这看起来不太好。让我们在IPython中多次运行`speedcomparison()`函数：
- en: '![](img/9514e819-e7cd-42c3-b1af-d5ea832c6864.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9514e819-e7cd-42c3-b1af-d5ea832c6864.png)'
- en: As we can see, the speed increases dramatically after the first time we use
    a given GPU function. Again, as with the prior example, this is because PyCUDA
    compiles our inline CUDA C code the first time a given GPU kernel function is
    called using the `nvcc` compiler. After the code is compiled, then it is cached
    and re-used for the remainder of a given Python session.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在第一次使用给定的GPU函数之后，速度会显著增加。同样，正如先前的例子一样，这是因为PyCUDA在第一次使用`nvcc`编译器调用给定的GPU内核函数时编译我们的内联CUDA
    C代码。代码编译后，它会被缓存并用于整个Python会话的剩余部分。
- en: 'Now, let''s cover something else important before we move on, which is very
    subtle. The little kernel function we defined operates on C float pointers; this
    means that we will have to allocate some empty memory on the GPU that is pointed
    to by the `out` variable. Take a look at this portion of code again from the `speedcomparison()`
    function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们继续之前，让我们先讨论另一个重要但非常微妙的问题。我们定义的小内核函数操作C浮点指针；这意味着我们将在GPU上分配一些空内存，这些内存由`out`变量指向。再次查看`speedcomparison()`函数中的这部分代码：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As we did before, we send a NumPy array over to the GPU (`host_data`) via the
    `gpuarray.to_gpu` function, which automatically allocates data onto the GPU and
    copies it over from the CPU space. We will plug this into the `in` part of our
    kernel function. In the next line, we allocate empty memory on the GPU with the
    `gpuarray.empty_like` function. This acts as a plain `malloc` in C, allocating
    an array of the same size and data type as `device_data`, but without copying
    anything. We can now use this for the `out` part of our kernel function. We now
    look at the next line in `speedcomparison()` to see how to launch our kernel function
    onto the GPU (ignoring the lines we use for timing):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如同之前，我们通过 `gpuarray.to_gpu` 函数将 NumPy 数组发送到 GPU (`host_data`)，该函数自动在 GPU 上分配数据并将其从
    CPU 空间复制过来。我们将把这个插入到内核函数的 `in` 部分。在下一行，我们使用 `gpuarray.empty_like` 函数在 GPU 上分配空内存。这相当于
    C 中的 `malloc`，分配一个与 `device_data` 相同大小和数据类型的数组，但不复制任何内容。现在我们可以使用这个数组作为内核函数的 `out`
    部分。我们现在查看 `speedcomparison()` 中的下一行，看看如何将内核函数启动到 GPU 上（忽略我们用于计时的行）：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Again, the variables we set correspond directly to the first line we defined
    with `ElementwiseKernel` (here being, `"float *in, float *out"`).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们设置的变量直接对应于我们使用 `ElementwiseKernel` 定义的第一个行（这里是指，`"float *in, float *out"`）。
- en: Mandelbrot revisited
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视曼德布罗特
- en: 'Let''s again look at the problem of generating the Mandelbrot set from [Chapter
    1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml), *Why GPU Programming?*. The original
    code is available under the `1` folder in the repository, with the filename `mandelbrot0.py`,
    which you should take another look at before we continue. We saw that there were
    two main components of this program: the first being the generation of the Mandelbrot
    set, and the second concerning dumping the Mandelbrot set into a PNG file. In
    the first chapter, we realized that we could parallelize only the generation of
    the Mandelbrot set, and considering that this takes the bulk of the time for the
    program to do, this would be a good candidate for an algorithm to offload this
    onto a GPU. Let''s figure out how to do this. (We will refrain from re-iterating
    over the definition of the Mandelbrot set, so if you need a deeper review, please
    re-read the *Mandelbrot* *revisited* section of [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml),
    *Why GPU Programming?*)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看看从[第1章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml)，“为什么进行 GPU 编程？”中生成曼德布罗特集的问题。原始代码存储在存储库中的
    `1` 文件夹下，文件名为 `mandelbrot0.py`，在我们继续之前你应该再次查看它。我们注意到这个程序有两个主要组成部分：第一个是生成曼德布罗特集，第二个是将曼德布罗特集输出到
    PNG 文件。在第1章中，我们意识到我们只能并行化生成曼德布罗特集的部分，考虑到这部分占程序运行时间的大部分，这将是一个很好的候选算法，可以将这部分工作卸载到
    GPU 上。让我们找出如何做到这一点。（我们将避免重复曼德布罗特集的定义，所以如果你需要更深入的了解，请重新阅读[第1章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml)，“为什么进行
    GPU 编程？”中的*曼德布罗特* *重新审视*部分。）
- en: 'First, let''s make a new Python function based on `simple_mandelbrot` from
    the original program. We''ll call it `gpu_mandelbrot`, and this will take in the
    same exact input as before:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们基于原始程序中的 `simple_mandelbrot` 创建一个新的 Python 函数。我们将称之为 `gpu_mandelbrot`，并且它将接受与之前完全相同的输入：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We will proceed a little differently from here. We will start by building a
    complex lattice that consists of each point in the complex plane that we will
    analyze.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们将采取稍微不同的方法。我们将首先构建一个复数格子，它由我们将要分析的复平面上的每个点组成。
- en: 'Here, we''ll use some tricks with the NumPy matrix type to easily generate
    the lattice, and then typecast the result from a NumPy `matrix` type to a two-dimensional
    NumPy `array` (since PyCUDA can only handle NumPy `array` types, not `matrix`
    types). Notice how we are very carefully setting our NumPy types:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用一些 NumPy 矩阵类型的技巧来轻松生成格子，然后将结果从 NumPy `matrix` 类型转换为二维 NumPy `array`（因为
    PyCUDA 只能处理 NumPy `array` 类型，不能处理 `matrix` 类型）。注意我们是如何非常小心地设置我们的 NumPy 类型的：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'So, we now have a two-dimensional complex array that represents the lattice
    from which we will generate our Mandelbrot set; as we will see, we can operate
    on this very easily within the GPU. Let''s now transfer our lattice to the GPU
    and allocate an array that we will use to represent our Mandelbrot set:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在有一个表示我们将从中生成曼德布罗特集的格子的二维复数数组；正如我们将看到的，我们可以在 GPU 中非常容易地操作这个数组。现在让我们将我们的格子传输到
    GPU，并分配一个我们将用来表示曼德布罗特集的数组：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To reiterate—the `gpuarray.to_array` function only can operate on NumPy `array`
    types, so we were sure to have type-cast this beforehand before we sent it to
    the GPU. Next, we have to allocate some memory on the GPU with the `gpuarray.empty`
    function, specifying the size/shape of the array and the type. Again, you can
    think of this as acting similarly to `malloc` in C; remember that we won't have
    to deallocate or `free` this memory later, due to the `gpuarray` object destructor
    taking care of memory clean-up automatically when the end of the scope is reached.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重申——`gpuarray.to_array`函数只能操作NumPy `array`类型，所以我们确保在将其发送到GPU之前先进行了类型转换。接下来，我们必须使用`gpuarray.empty`函数在GPU上分配一些内存，指定数组的大小/形状和类型。同样，你可以将其视为与C中的`malloc`类似的行为；记住，由于`gpuarray`对象在作用域结束时自动处理内存清理，我们不需要释放或`free`此内存。
- en: When you allocate memory on the GPU with the PyCUDA functions `gpuarray.empty`
    or `gpuarray.empty_like`, you do not have to deallocate this memory later due
    to the destructor of the `gpuarray` object managing all memory clean up.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用PyCUDA函数`gpuarray.empty`或`gpuarray.empty_like`在GPU上分配内存时，由于`gpuarray`对象的析构函数管理所有内存清理，你不需要在之后释放此内存。
- en: We're now ready to launch the kernel; the only change we have to make is to
    change the
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以启动内核了；我们唯一需要做的更改是更改
- en: 'We haven''t written our kernel function yet to generate the Mandelbrot set,
    but let''s just write how we want the rest of this function to go:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有编写生成Mandelbrot集的内核函数，但让我们先写一下我们希望这个函数的其他部分如何进行：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So this is how we want our new kernel to act—the first input will be the complex
    lattice of points (NumPy `complex64` type) we generated, the second will be a
    pointer to a two-dimensional floating point array (NumPy `float32` type) that
    will indicate which elements are members of the Mandelbrot set, the third will
    be an integer indicating the maximum number of iterations for each point, and
    the final input will be the upper bound for each point used for determining membership
    in the Mandelbrot class. Notice that we are *very* careful in typecasting everything
    that goes into the GPU!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是我们希望我们的新内核如何工作的——第一个输入将是生成的复数点阵（NumPy `complex64`类型），第二个将是一个指向二维浮点数组的指针（NumPy
    `float32`类型），它将指示哪些元素是Mandelbrot集的成员，第三个将是一个整数，表示每个点的最大迭代次数，最后一个输入将用于确定每个点是否属于Mandelbrot类的上界。请注意，我们对所有输入到GPU中的类型转换都非常小心！
- en: The next line retrieves the Mandelbrot set we generated from the GPU back into
    CPU space, and the end value is returned. (Notice that the input and output of
    `gpu_mandelbrot` is exactly the same as that of `simple_mandelbrot`).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行将我们从GPU生成的Mandelbrot集检索回CPU空间，并返回结束值。（注意，`gpu_mandelbrot`的输入和输出与`simple_mandelbrot`完全相同）。
- en: 'Let''s now look at how to properly define our GPU kernel. First, let''s add
    the appropriate `include` statements to the header:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何正确地定义我们的GPU内核。首先，让我们在头文件中添加适当的`include`语句：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We are now ready to write our GPU kernel! We''ll show it here and then go over
    this line-by-line:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以编写我们的GPU内核了！我们将在这里展示它，然后逐行解释：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: First, we set our input with the first string passed to `ElementwiseKernel`.
    We have to realize that when we are working in CUDA-C, particular C datatypes
    will correspond directly to particular Python NumPy datatypes. Again, note that
    when arrays are passed into a CUDA kernel, they are seen as C pointers by CUDA.
    Here, a CUDA C `int` type corresponds exactly to a NumPy `int32` type, while a
    CUDA C `float` type corresponds to a NumPy `float32` type. An internal PyCUDA
    class template is then used for complex types—here PyCUDA `::complex<float>` corresponds
    to Numpy `complex64`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用传递给`ElementwiseKernel`的第一个字符串设置我们的输入。我们必须意识到，当我们使用CUDA-C时，特定的C数据类型将直接对应于特定的Python
    NumPy数据类型。再次注意，当数组传递到CUDA内核时，它们被CUDA视为C指针。在这里，CUDA C `int`类型正好对应于NumPy `int32`类型，而CUDA
    C `float`类型对应于NumPy `float32`类型。然后使用内部PyCUDA类模板用于复杂数据类型——这里PyCUDA `::complex<float>`对应于Numpy
    `complex64`。
- en: Let's look at the content of the second string, which is deliminated with three
    quotes (`"""`). This allows us to use multiple lines within the string; we will
    use this when we write larger inline CUDA kernels in Python.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看第二个字符串的内容，它由三个引号(`"""`)分隔。这允许我们在字符串中使用多行；当我们用Python编写较大的内联CUDA内核时，我们会用到这个功能。
- en: While the arrays we have passed in are two-dimensional arrays in Python, CUDA
    will only see these as being one-dimensional and indexed by `i`. Again, `ElementwiseKernel`
    indexes `i` across multiple cores and threads for us automatically. We initialize
    each point in the output to one with `mandelbrot_graph[i] = 1;`, as `i` will be
    indexed over every single element of our Mandelbrot set; we're going to assume
    that every point will be a member unless proven otherwise. (Again, the Mandelbrot
    set is over two dimensions, real and complex, but `ElementwiseKernel` will automatically
    translate everything into a one-dimensional set. When we interact with the data
    again in Python, the two-dimensional structure of the Mandelbrot set will be preserved.)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们传递的数组在Python中是二维数组，但CUDA只会将其视为一维数组，并通过`i`索引。再次强调，`ElementwiseKernel`会自动为我们跨多个核心和线程索引`i`。我们将输出中的每个点初始化为1，使用`mandelbrot_graph[i]
    = 1;`，因为`i`将索引曼德布罗特集的每个元素；我们将假设每个点都是成员，除非有其他证明。 （再次强调，曼德布罗特集跨越两个维度，实部和复数，但`ElementwiseKernel`会自动将所有内容转换为一维集。当我们再次在Python中与数据交互时，曼德布罗特集的二维结构将被保留。）
- en: We set up our `c` value as in Python to the appropriate lattice point with `pycuda::complex<float>
    c = lattice[i];` and initialize our `z` value to `0` with `pycuda::complex<float>
    z(0,0);` (the first zero corresponds to the real part, while the second corresponds
    to the imaginary part). We then perform a loop over a new iterator, `j`, with
    `for(int j = 0; j < max_iters; j++)`. (Note that this algorithm will not be parallelized
    over `j` or any other index—only `i`! This `for` loop will run serially over `j`—but
    the entire piece of code will be parallelized across `i`.)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`c`值设置为Python中的适当格点，使用`pycuda::complex<float> c = lattice[i];`，并将`z`值初始化为`0`，使用`pycuda::complex<float>
    z(0,0);`（第一个零对应于实部，而第二个对应于虚部）。然后，我们使用新的迭代器`j`执行循环，`for(int j = 0; j < max_iters;
    j++)`。（请注意，此算法不会在`j`或任何其他索引上并行化——只有`i`！这个`for`循环将按顺序在`j`上运行——但整个代码块将在`i`上并行化。）
- en: We then set the new value of `*z*` with `z = z*z + c;` as per the Mandelbrot
    algorithm. If the absolute value of this element exceeds the upper bound ( `if(abs(z)
    > upper_bound)` ), we set this point to 0 ( `mandelbrot_graph[i] = 0;` ) and break
    out of the loop with the `break` keyword.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们按照曼德布罗特算法设置新的`*z*`值，使用`z = z*z + c;`。如果这个元素的绝对值超过上限（`if(abs(z) > upper_bound)`），我们将这个点设置为0（`mandelbrot_graph[i]
    = 0;`），并使用`break`关键字跳出循环。
- en: In the final string passed into `ElementwiseKernel` we give the kernel its internal
    CUDA C name, here `"mandel_ker"`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在传递给`ElementwiseKernel`的最终字符串中，我们给出内核其内部的CUDA C名称，这里为`"mandel_ker"`。
- en: 'We''re now ready to launch the kernel; the only change we have to make is to
    change the reference from `simple_mandelbrot` in the main function to `gpu_mandelbrot`,
    and we''re ready to go. Let''s launch this from IPython:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好启动内核；我们唯一要做的更改是将主函数中的`simple_mandelbrot`引用更改为`gpu_mandelbrot`，然后我们就可以出发了。让我们从IPython中启动它：
- en: '![](img/f00d5080-4975-4023-9f14-397a8e007ac4.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f00d5080-4975-4023-9f14-397a8e007ac4.png)'
- en: 'Let''s check the dumped image to make sure this is correct:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查导出的图像以确保这是正确的：
- en: '![](img/6fa6851a-bcce-46d0-a63d-8023766da21a.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6fa6851a-bcce-46d0-a63d-8023766da21a.png)'
- en: 'This is certainly the same Mandelbrot image that is produced in the first chapter,
    so we have successfully implemented this onto a GPU! Let''s now look at the speed
    increase we''re getting: in the first chapter, it took us 14.61 seconds to produce
    this graph; here, it only took 0.894 seconds. Keep in mind that PyCUDA also has
    to compile and link our CUDA C code at runtime, and the time it takes to make
    the memory transfers to and from the GPU. Still, even with all of that extra overhead,
    it is a very worthwhile speed increase! (You can view the code for our GPU Mandelbrot
    with the file named `gpu_mandelbrot0.py` in the Git repository.)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是与第一章中产生的相同曼德布罗特图像，因此我们成功将其在GPU上实现了！现在让我们看看我们获得的速度提升：在第一章中，我们用了14.61秒来生成这个图表；在这里，它只用了0.894秒。记住，PyCUDA还必须在运行时编译和链接我们的CUDA
    C代码，以及将内存从GPU传输到和从GPU传输回来的时间。尽管如此，即使有所有这些额外的开销，这仍然是一个非常值得的速度提升！（您可以通过Git仓库中名为`gpu_mandelbrot0.py`的文件查看我们GPU曼德布罗特的代码。）
- en: A brief foray into functional programming
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 函数式编程的简要探索
- en: Before we continue, let's briefly do a review of two functions available in
    Python for **functional programming***—*`map` and `reduce`. These are both considered
    to be *functional* because they both act on *functions* for their operation. We
    find these interesting because these both correspond to common design patterns
    in programming, so we can swap out different functions in the input to get a multitude
    of different (and useful) operations.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们简要回顾一下 Python 中用于函数式编程的两个函数——**`map` 和 `reduce`**。这两个函数都被认为是**函数式**的，因为它们都作用于**函数**进行操作。我们发现这些很有趣，因为它们都对应于编程中的常见设计模式，因此我们可以替换输入中的不同函数以获得多种不同（且有用）的操作。
- en: 'Let''s first recall the `lambda` keyword in Python. This allows us to define
    an **anonymous function**—in most cases, these can be thought of as a `throwaway`
    function that we may only wish to use once, or functions that are able to be defined
    on a single line. Let''s open up IPython right now and define a little function
    that squares a number as such—`pow2 = lambda x : x**2`. Let''s test it out on
    a few numbers:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们首先回忆一下 Python 中的 `lambda` 关键字。这允许我们定义一个**匿名函数**——在大多数情况下，这些可以被视为一次性的“丢弃”函数，或者可以单行定义的函数。让我们现在打开
    IPython 并定义一个简单的函数，它将数字平方——`pow2 = lambda x : x**2`。让我们在几个数字上测试它：'
- en: '![](img/f7154a51-4486-4292-9ed0-89415e526394.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f7154a51-4486-4292-9ed0-89415e526394.png)'
- en: 'Let''s recall that `map` acts on two input values: a function and a `list`
    of objects that the given function can act on. `map` outputs a list of the function''s
    output for each element in the original list. Let''s now define our squaring operation
    as an anonymous function which we input into map, and a list of the last few numbers
    we checked with the following—`map(lambda x : x**2, [2,3,4])`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们回忆一下 `map` 作用于两个输入值：一个函数和一个对象列表，该函数可以作用于这些对象。`map` 输出一个列表，其中包含函数对原始列表中每个元素的输出。现在，让我们将平方操作定义为匿名函数，并将其输入到
    `map` 中，以及以下我们检查的最后几个数字列表——`map(lambda x : x**2, [2,3,4])`：'
- en: '![](img/c41f370e-9cba-40ba-a5df-e84857044437.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c41f370e-9cba-40ba-a5df-e84857044437.png)'
- en: 'We see that `map` acts as `ElementwiseKernel`! This is actually a standard
    design pattern in functional programming. Now, let''s look at `reduce`; rather
    than taking in a list and outputting a directly corresponding list, reduce takes
    in a list, performs a recursive binary operation on it, and outputs a singleton.
    Let''s get a notion of this design pattern by typing `reduce(lambda x, y : x +
    y, [1,2,3,4])`. When we type this in IPython, we will see that this will output
    a single number, 10, which is indeed the sum of *1+2+3+4*. You can try replacing
    the summation above with multiplication, and seeing that this indeed works for
    recursively multiplying a long list of numbers together. Generally speaking, we
    use reduce operations with *associative binary operations*; this means that, no
    matter the order we perform our operation between sequential elements of the list,
    will always invariably give the same result, provided that the list is kept in
    order. (This is not to be confused with the *commutative property*.)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '我们看到 `map` 作为 `ElementwiseKernel`！这实际上是函数式编程中的一个标准设计模式。现在，让我们看看 `reduce`；与直接接受一个列表并输出一个直接对应的列表不同，`reduce`
    接受一个列表，对其执行递归二进制操作，并输出一个单例。让我们通过输入 `reduce(lambda x, y : x + y, [1,2,3,4])` 来理解这种设计模式。当我们输入
    IPython 时，我们会看到这将输出一个单一的数字，10，这确实是 *1+2+3+4* 的和。您可以尝试将上面的求和替换为乘法，并看到这确实适用于递归地将一长串数字相乘。一般来说，我们使用具有**结合二进制操作**的缩减操作；这意味着，无论我们以何种顺序在列表的连续元素之间执行操作，只要列表保持有序，总会得到相同的结果。（这不同于**交换律**。）'
- en: We will now see how PyCUDA handles programming patterns akin to `reduce`—with
    **parallel scan** and **reduction kernels**.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到 PyCUDA 如何处理类似于 `reduce` 的编程模式——使用**并行扫描**和**缩减内核**。
- en: Parallel scan and reduction kernel basics
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行扫描和缩减内核基础
- en: 'Let''s look at a basic function in PyCUDA that reproduces the functionality
    of reduce—`InclusiveScanKernel`. (You can find the code under the `simple_scankernal0.py`
    filename.) Let''s execute a basic example that sums a small list of numbers on
    the GPU:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 PyCUDA 中一个基本的函数，它实现了 reduce 的功能——`InclusiveScanKernel`。（您可以在名为 `simple_scankernal0.py`
    的文件下找到代码。）让我们执行一个基本的示例，在 GPU 上对一组小数字进行求和：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We construct our kernel by first specifying the input/output type (here, NumPy
    `int32`) and in the string, `"a+b"`. Here, `InclusiveScanKernel` sets up elements
    named `a` and `b` in the GPU space automatically, so you can think of this string
    input as being analogous to `lambda a,b: a + b` in Python. We can really put any
    (associative) binary operation here, provided we remember to write it in C.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过首先指定输入/输出类型（这里，NumPy `int32`）和在字符串 `"a+b"` 中，来构建我们的内核。在这里，`InclusiveScanKernel`
    会自动在 GPU 空间中设置名为 `a` 和 `b` 的元素，所以你可以将这个字符串输入看作是 Python 中的 `lambda a,b: a + b`
    的类似物。我们真的可以在这里放置任何（结合）二元运算，只要我们记得用 C 语言来编写它。'
- en: 'When we run `sum_gpu`, we see that we will get an array of the same size as
    the input array. Each element in the array represents the value for each step
    in the calculation (the NumPy `cumsum` function gives the same output, as we can
    see). The last element will be the final output that we are seeking, which corresponds
    to the output of reduce:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行 `sum_gpu` 时，我们看到我们将得到一个与输入数组大小相同的数组。数组中的每个元素代表计算中的每个步骤的值（NumPy 的 `cumsum`
    函数给出相同的输出，正如我们所看到的）。最后一个元素将是我们要寻找的最终输出，这对应于 reduce 的输出：
- en: '![](img/98e28110-698a-4ab5-a827-9c1a8a2e31d4.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/98e28110-698a-4ab5-a827-9c1a8a2e31d4.png)'
- en: 'Let''s try something a little more challenging; let''s find the maximum value
    in a `float32` array:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个更有挑战性的东西；让我们在一个 `float32` 数组中找到最大值：
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: (You can find the complete code in the file named `simple_scankernal1.py`.)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: （你可以在名为 `simple_scankernal1.py` 的文件中找到完整的代码。）
- en: 'Here, the main change we made is to replace the `a + b` string with `a > b
    ? a : b`. (In Python, this would be rendered within a `reduce` statement as `lambda
    a, b: max(a,b)`). Here, we are using a trick to give the max among `a` and `b`
    with the C language''s `?` operator. We finally display the last value of the
    resulting element in the output array, which will be exactly the last element
    (which we can always retrieve with the `[-1]` index in Python).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们做的主要改变是将字符串 `a + b` 替换为 `a > b ? a : b`。（在 Python 中，这将在 `reduce` 语句中呈现为
    `lambda a, b: max(a,b)`）。在这里，我们使用了一个技巧，利用 C 语言的 `?` 操作符来给出 `a` 和 `b` 中的最大值。我们最终在输出数组中显示结果的最后一个元素，这将是最后一个元素（我们总是可以用
    Python 中的 `[-1]` 索引检索到它）。'
- en: 'Now, let''s finally look one more PyCUDA function for generating GPU kernels—`ReductionKernel`.
    Effectively, `ReductionKernel` acts like a `ElementwiseKernel` function followed
    by a parallel scan kernel. What algorithm is a good candidate for implementing
    with a `ReductionKernel`? The first that tends to come to mind is the dot product
    from linear algebra. Let''s remember computing the dot product of two vectors
    has two steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们最后看看一个 PyCUDA 函数，用于生成 GPU 内核——`ReductionKernel`。实际上，`ReductionKernel`
    类似于一个 `ElementwiseKernel` 函数，后面跟着一个并行扫描内核。用 `ReductionKernel` 实现哪种算法是好的候选者？首先想到的是线性代数中的点积。让我们记住计算两个向量的点积有两个步骤：
- en: Multiply the vectors pointwise
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点积向量
- en: Sum the resulting pointwise multiples
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 求点积的结果之和
- en: 'These two steps are also called *multiply and accumulate*. Let''s set up a
    kernel to do this computation now:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤也被称为 *乘法和累加*。现在，让我们设置一个内核来完成这个计算：
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: First, note the datatype we use for our kernel (a `float32`). We then set up
    the input arguments to our CUDA C kernel with `arguments`, (here two float arrays
    representing each vector designated with `float *`) and set the pointwise calculation
    with `map_expr`, here it is pointwise multiplication. As with `ElementwiseKernel`,
    this is indexed over `i`. We set up `reduce_expr` the same as with `InclusiveScanKernel`.
    This will take the resulting output from the element-wise operation and perform
    a reduce-type operation on the array. Finally, we set the *neutral element* with
    neutral. This is an element that will act as an identity for `reduce_expr`; here,
    we set `neutral=0`, because `0` is always the identity under addition (under multiplication,
    one is the identity). We'll see why exactly we have to set this up when we cover
    parallel prefix in greater depth later in this book.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意我们用于内核的数据类型（一个 `float32`）。然后，我们使用 `arguments` 设置 CUDA C 内核的输入参数（这里有两个表示每个向量的浮点数组
    `float *`），并使用 `map_expr` 设置点积计算。与 `ElementwiseKernel` 一样，这是在 `i` 上索引的。我们按照与 `InclusiveScanKernel`
    相同的方式设置 `reduce_expr`。这将从逐元素操作的结果中取出输出，并在数组上执行 reduce 类型的操作。最后，我们使用 `neutral`
    设置 *中性元素*。这是一个在 `reduce_expr` 中充当恒等元的元素；在这里，我们设置 `neutral=0`，因为 `0` 在加法下总是恒等元（在乘法下，1
    是恒等元）。我们将在本书后面更深入地介绍并行前缀时看到为什么必须设置这个。
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We first saw how to query our GPU from PyCUDA, and with this re-create the CUDA
    `deviceQuery` program in Python. We then learned how to transfer NumPy arrays
    to and from the GPU's memory with the PyCUDA `gpuarray` class and its `to_gpu`
    and `get` functions. We got a feel for using `gpuarray` objects by observing how
    to use them to do basic calculations on the GPU, and we learned to do a little
    investigative work using IPython's `prun` profiler. We saw there is sometimes
    some arbitrary slowdown when running GPU functions from PyCUDA for the first time
    in a session, due to PyCUDA launching NVIDIA's `nvcc` compiler to compile inline
    CUDA C code. We then saw how to use the `ElementwiseKernel` function to compile
    and launch element-wise operations, which are automatically parallelized onto
    the GPU from Python. We did a brief review of functional programming in Python
    (in particular the `map` and `reduce` functions), and finally, we covered how
    to do some basic reduce/scan-type computations on the GPU using the `InclusiveScanKernel`
    and `ReductionKernel` functions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先看到了如何从 PyCUDA 查询我们的 GPU，并使用这个方法在 Python 中重新创建 CUDA 的 `deviceQuery` 程序。然后我们学习了如何使用
    PyCUDA 的 `gpuarray` 类及其 `to_gpu` 和 `get` 函数在 GPU 的内存之间传输 NumPy 数组。我们通过观察如何使用它们在
    GPU 上进行基本计算来感受 `gpuarray` 对象的使用，并且我们学会了使用 IPython 的 `prun` 分析器进行一些调查工作。我们看到了有时在会话中第一次从
    PyCUDA 运行 GPU 函数时，由于 PyCUDA 启动 NVIDIA 的 `nvcc` 编译器来编译内联 CUDA C 代码，所以会有一些任意的减速。然后我们看到了如何使用
    `ElementwiseKernel` 函数编译和启动元素级操作，这些操作会自动从 Python 并行到 GPU 上。我们对 Python 中的函数式编程（特别是
    `map` 和 `reduce` 函数）进行了简要回顾，最后，我们介绍了如何使用 `InclusiveScanKernel` 和 `ReductionKernel`
    函数在 GPU 上进行一些基本的 reduce/scan 类型的计算。
- en: Now that we have the absolute basics down about writing and launching kernel
    functions, we should realize that PyCUDA has covered the vast amount of the overhead
    in writing a kernel for us with its templates. We will spend the next chapter
    learning about the principles of CUDA kernel execution, and how CUDA arranges
    concurrent threads in a kernel into abstract **grids** and **blocks**.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了编写和启动内核函数的绝对基础知识，我们应该意识到 PyCUDA 已经通过其模板覆盖了大量编写内核时的开销。我们将在下一章学习 CUDA
    内核执行的原则，以及 CUDA 如何将内核中的并发线程安排成抽象的 **网格** 和 **块**。
- en: Questions
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: In `simple_element_kernel_example0.py`, we don't consider the memory transfers
    to and from the GPU in measuring the time for the GPU computation. Try measuring
    the time that the `gpuarray` functions, `to_gpu` and `get`, take with the Python
    time command. Would you say it's worth offloading this particular function onto
    the GPU, with the memory transfer times in consideration?
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `simple_element_kernel_example0.py` 中，我们在测量 GPU 计算的时间时没有考虑从 GPU 到 GPU 的内存传输。尝试使用
    Python 的 `time` 命令测量 `gpuarray` 函数 `to_gpu` 和 `get` 所花费的时间。考虑到内存传输时间，你认为将这个特定函数卸载到
    GPU 上是否值得？
- en: In [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml), *Why GPU Programming?*,
    we had a discussion of Amdahl's Law, which gives us some idea of the gains we
    can potentially get by offloading portions of a program onto a GPU. Name two issues
    that we have seen in this chapter that Amdahl's law does not take into consideration.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 [第 1 章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml) *为什么进行 GPU 编程？* 中，我们讨论了
    Amdahl 的定律，它给我们一些关于通过将程序的部分卸载到 GPU 上可能获得的收益的想法。在本章中，我们看到了两个 Amdahl 的定律没有考虑的问题。
- en: Modify `gpu_mandel0.py` to use smaller and smaller lattices of complex numbers,
    and compare this to the same lattices CPU version of the program. Can we choose
    a small enough lattice such that the CPU version is actually faster than the GPU
    version?
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `gpu_mandel0.py` 修改为使用越来越小的复数格点，并将其与程序的相同格点的 CPU 版本进行比较。我们能否选择足够小的格点，使得 CPU
    版本实际上比 GPU 版本更快？
- en: Create a kernel with `ReductionKernel` that takes two `complex64` arrays on
    the GPU of the same length and returns the absolute largest element among both
    arrays.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `ReductionKernel` 创建一个内核，该内核接受两个长度相同的 `complex64` 数组，并返回两个数组中绝对最大的元素。
- en: What happens if a `gpuarray` object reaches end-of-scope in Python?
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个 `gpuarray` 对象在 Python 中达到作用域的末尾会发生什么？
- en: Why do you think we need to define `neutral` when we use `ReductionKernel`?
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你认为为什么在使用 `ReductionKernel` 时我们需要定义 `neutral`？
- en: 'If in `ReductionKernel` we set `reduce_expr ="a > b ? a : b"`, and we are operating
    on int32 types, then what should we set "`neutral`" to?'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '如果在 `ReductionKernel` 中我们设置 `reduce_expr = "a > b ? a : b"`，并且我们操作的是 int32
    类型，那么我们应该将 "`neutral`" 设置为什么？'
