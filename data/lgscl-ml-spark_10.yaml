- en: Chapter 10.  Configuring and Working with External Libraries
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。配置和使用外部库
- en: 'This chapter guides you on using external libraries to expand your data analysis
    to make the Spark more versatile. Examples will be given for deploying third-party-developed
    packages or libraries for machine learning applications with Spark core and ML/MLlib.
    We will also discuss how to compile and use external libraries with the core libraries
    of Spark for time series. As promised, we will also discuss how to configure SparkR
    to increase exploratory data manipulation and operations. In a nutshell, the following
    topics will be covered throughout this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章指导您如何使用外部库来扩展数据分析，使Spark更加多功能。将提供示例，用于部署第三方开发的软件包或库，用于Spark核心和ML/MLlib的机器学习应用。我们还将讨论如何编译和使用外部库与Spark的核心库进行时间序列分析。如约定，我们还将讨论如何配置SparkR以增加探索性数据操作和操作。简而言之，本章将涵盖以下主题：
- en: Third-party ML libraries with Spark
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark的第三方ML库
- en: Using external libraries when deploying Spark ML on a cluster
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群上部署Spark ML时使用外部库
- en: Time series analysis using the Spark-TS package of Cloudera
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Cloudera的Spark-TS包进行时间序列分析
- en: Configuring SparkR with RStudio
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RStudio配置SparkR
- en: Configuring Hadoop run-time on Windows
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Windows上配置Hadoop运行时
- en: 'In order to provide a user-friendly environment for the developer, it is also
    possible to incorporate third-party APIs and libraries with Spark Core and other
    APIs such as Spark MLlib/ML, Spark Streaming, GraphX, and so on. Interested readers
    should refer to the following website that is listed on the Spark website as the
    **Third-Party Packages**: [https://spark-packages.org/](https://spark-packages.org/).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为开发人员提供用户友好的环境，还可以将第三方API和库与Spark Core和其他API（如Spark MLlib/ML、Spark Streaming、GraphX等）结合使用。感兴趣的读者应该参考Spark网站上列出的以下网站，该网站被列为**第三方软件包**：[https://spark-packages.org/](https://spark-packages.org/)。
- en: 'This website is a community index of third-party packages for Apache Spark.
    To date, there are total 252 packages registered on this site, as shown in *Table
    1*:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网站是Apache Spark的第三方软件包的社区索引。截至目前，该网站上注册了总共252个软件包，如*表1*所示：
- en: '| **Domain** | **No. of packages** | **URL** |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| **领域** | **软件包数量** | **URL** |'
- en: '| Spark core | 9 | [https://spark-packages.org/?q=tags%3A%22Core%22](https://spark-packages.org/?q=tags%3A%22Core%22)
    |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| Spark核心 | 9 | [https://spark-packages.org/?q=tags%3A%22Core%22](https://spark-packages.org/?q=tags%3A%22Core%22)
    |'
- en: '| Data sources | 39 | [https://spark-packages.org/?q=tags%3A%22Data%20Sources%22](https://spark-packages.org/?q=tags%3A%22Data%20Sources%22)
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 数据源 | 39 | [https://spark-packages.org/?q=tags%3A%22Data%20Sources%22](https://spark-packages.org/?q=tags%3A%22Data%20Sources%22)
    |'
- en: '| Machine learning | 55 | [https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22](https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22)
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 机器学习 | 55 | [https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22](https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22)
    |'
- en: '| Streaming | 36 | [https://spark-packages.org/?q=tags%3A%22Streaming%22](https://spark-packages.org/?q=tags%3A%22Streaming%22)
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 流 | 36 | [https://spark-packages.org/?q=tags%3A%22Streaming%22](https://spark-packages.org/?q=tags%3A%22Streaming%22)
    |'
- en: '| Graph processing | 13 | [https://spark-packages.org/?q=tags%3A%22Graph%22](https://spark-packages.org/?q=tags%3A%22Graph%22)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 图处理 | 13 | [https://spark-packages.org/?q=tags%3A%22Graph%22](https://spark-packages.org/?q=tags%3A%22Graph%22)
    |'
- en: '| Spark with Python | 5 | [https://spark-packages.org/?q=tags%3A%22PySpark%22](https://spark-packages.org/?q=tags%3A%22PySpark%22)
    |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 使用Python的Spark | 5 | [https://spark-packages.org/?q=tags%3A%22PySpark%22](https://spark-packages.org/?q=tags%3A%22PySpark%22)
    |'
- en: '| Cluster deployment | 10 | [https://spark-packages.org/?q=tags%3A%22Deployment%22](https://spark-packages.org/?q=tags%3A%22Deployment%22)
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 集群部署 | 10 | [https://spark-packages.org/?q=tags%3A%22Deployment%22](https://spark-packages.org/?q=tags%3A%22Deployment%22)
    |'
- en: '| Data processing example | 18 | [https://spark-packages.org/?q=tags%3A%22Examples%22](https://spark-packages.org/?q=tags%3A%22Examples%22)
    |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 数据处理示例 | 18 | [https://spark-packages.org/?q=tags%3A%22Examples%22](https://spark-packages.org/?q=tags%3A%22Examples%22)
    |'
- en: '| Applications | 10 | [https://spark-packages.org/?q=tags%3A%22Applications%22](https://spark-packages.org/?q=tags%3A%22Applications%22)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 应用程序 | 10 | [https://spark-packages.org/?q=tags%3A%22Applications%22](https://spark-packages.org/?q=tags%3A%22Applications%22)
    |'
- en: '| Tools | 24 | [https://spark-packages.org/?q=tags%3A%22Tools%22](https://spark-packages.org/?q=tags%3A%22Tools%22)
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 工具 | 24 | [https://spark-packages.org/?q=tags%3A%22Tools%22](https://spark-packages.org/?q=tags%3A%22Tools%22)
    |'
- en: '| Total Packages: 252 |   |   |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 总软件包数量：252 |   |   |'
- en: 'Table 1: Third-party libraries for Spark based on application domain'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：基于应用领域的Spark第三方库
- en: Third-party ML libraries with Spark
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark的第三方ML库
- en: The 55 third-party machine learning libraries include libraries for neural data
    analysis, generalized clustering, streaming, topic modelling, feature selection,
    matrix factorization, distributed DataFrame for distributed ML, model matrix,
    Stanford Core NLP wrapper for Spark, social network analysis, deep learning module
    running, assembly of fundamental statistics, binary classifier calibration, and
    tokenizer for DataFrame.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 55个第三方机器学习库包括神经数据分析库、广义聚类库、流处理库、主题建模库、特征选择库、矩阵分解库、分布式DataFrame库、模型矩阵库、用于Spark的Stanford
    Core NLP包装器、社交网络分析库、深度学习模块运行库、基本统计汇编库、二元分类器校准库和DataFrame的标记器。
- en: '*Table 2* provides a summary of the most useful packages based on use cases
    and application areas of machine learning. Interested readers should visit the
    respective websites for more insights:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*表2*提供了基于使用情况和机器学习应用领域的最有用的软件包的摘要。感兴趣的读者应该访问相应的网站以获取更多见解：'
- en: '| **Third party ML library for Spark** | **Use cases** |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **Spark的第三方ML库** | **使用情况** |'
- en: '| thunderScalaNetwork | Neural networkLarge-scale neural data analysis with
    Spark where the neural network implementation is done with Scala. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| thunderScalaNetwork | 神经网络使用Scala进行大规模神经数据分析的Spark实现。 |'
- en: '| generalized-kmeans-clusteringpatchworkbisecting-kmeansspark-knn | ClusteringThis
    project generalizes the Spark MLLIB K-means cluster to support arbitrary distance
    functions.Highly scalable grid-density clustering algorithm for Spark MLlib.This
    is a prototype implementation of Bisecting K-Means Clustering on Spark.k-nearest
    neighbors algorithm on Spark. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| generalized-kmeans-clusteringpatchworkbisecting-kmeansspark-knn | 聚类这个项目将Spark
    MLLIB K均值聚类泛化，以支持任意距离函数。用于Spark MLlib的高度可扩展的网格密度聚类算法。这是Bisecting K-Means聚类在Spark上的原型实现。在Spark上的k最近邻算法。
    |'
- en: '| spark-ml-streamingstreaming-matrix-factorizationtwitter-stream-ml | StreamingVisualize
    streaming machine learning in Spark.Streaming Recommendation engine using matrix
    factorization with user and product bias.Machine learning over Twitter''s stream.
    Using Apache Spark, Web Server and Lightning Graph server. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| spark-ml-streamingstreaming-matrix-factorizationtwitter-stream-ml | 在Spark中可视化流式机器学习。使用矩阵分解和用户产品偏差的流式推荐引擎。在Twitter的流上进行机器学习。使用Apache
    Spark，Web服务器和Lightning图形服务器。 |'
- en: '| pipeline | Docker-based pipeliningEnd-to-End, real-time, advanced analytics
    big data reference pipeline using Spark, Spark SQL, Spark Streaming, ML, MLlib,
    GraphX, Kafka, Cassandra, Redis, Apache Zeppelin, Spark-Notebook, iPython/Jupyter
    Notebook, Tableau, H2O Flow, and Tachyon. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| pipeline | 基于Docker的管道化使用Spark、Spark SQL、Spark Streaming、ML、MLlib、GraphX、Kafka、Cassandra、Redis、Apache
    Zeppelin、Spark-Notebook、iPython/Jupyter Notebook、Tableau、H2O Flow和Tachyon的端到端、实时、高级分析大数据参考管道。
    |'
- en: '| dllibCaffeOnSpark`dl4j-spark-ml` | Deep learningdllib is a deep learning
    tool running on Apache Spark. Users need to download the tools as .jar and then
    can integrate with Spark and  develop deep-learning-based applications.CaffeOnSpark
    is a scalable deep learning running with the Spark executors. It is based on peer-to-peer
    (P2P) communication. `dl4j-spark-ml` can be used to develop deep-learning-based
    ML applications by integrating with Spark ML. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| dllibCaffeOnSpark`dl4j-spark-ml` | 深度学习dllib是在Apache Spark上运行的深度学习工具。用户需要下载工具作为.jar文件，然后可以与Spark集成并开发基于深度学习的应用程序。CaffeOnSpark是在Spark执行器上运行的可扩展的深度学习。它基于点对点（P2P）通信。`dl4j-spark-ml`可以通过与Spark
    ML集成来开发基于深度学习的ML应用程序。 |'
- en: '| kNN_ISsparkboostspark-calibration | ClassificationkNN-IS: An Iterative Spark-based
    design of the k-Nearest Neighbours classifier for big data.A distributed implementation
    of AdaBoost.MH and MP-Boost using Apache Spark.Assesses binary classifier calibration
    (that is, how well classifier outputs match observed class proportions) in Spark.
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| kNN_ISsparkboostspark-calibration | 分类kNN-IS：用于大数据的k最近邻分类器的迭代式Spark设计。使用Apache
    Spark进行AdaBoost.MH和MP-Boost的分布式实现。在Spark中评估二元分类器的校准（即分类器输出与观察到的类比例匹配程度）。 |'
- en: '| Zen | RegressionZen provides a platform for large-scale and efficient machine
    learning on top of Spark. For example, logistic regression, linear regression,
    Latent Dirichlet Allocation (LDA), factorization machines and Deep Neural Network
    (DNN) are implemented in the current release. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| Zen | 回归Zen提供了一个在Spark上进行大规模高效机器学习的平台。例如，逻辑回归、线性回归、潜在狄利克雷分配（LDA）、因子分解机和深度神经网络（DNN）都在当前版本中实现了。
    |'
- en: '| modelmatrixspark-infotheoretic-feature-selection | Feature engineeringspark-infotheoretic-feature-selection
    tools provide an alternative to Spark for developing large-scale machine learning
    applications. They provides robust feature engineering through the pipelining
    including the feature extractors, feature selectors. It is focused on building
    sparse feature-vector-based pipelines.On the other hand, it can be used as a feature
    selection framework based on Information Theory. Algorithms based on Information
    Theory include mRMR, InfoGain, JMI, and other commonly used FS filters. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| modelmatrixspark-infotheoretic-feature-selection | 特征工程spark-infotheoretic-feature-selection工具为开发大规模机器学习应用提供了一种替代方案。它通过管道提供了稳健的特征工程，包括特征提取器和特征选择器。它专注于构建基于稀疏特征向量的管道。另一方面，它可以作为基于信息理论的特征选择框架。基于信息理论的算法包括mRMR、InfoGain、JMI和其他常用的FS过滤器。
    |'
- en: '| spark-knn-graphs | Graph processingSpark algorithms for building and processing
    k-nn graphs |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| spark-knn-graphs | 图处理Spark算法用于构建和处理k-nn图 |'
- en: '| TopicModeling | Topic modellingDistributed Topic Modelling on Apache Spark
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| TopicModeling | 主题建模在Apache Spark上进行分布式主题建模 |'
- en: '| Spark.statistics | StatisticsApart from SparkR, Spark.statistics works as
    an assembler of basic statistics implementation based on the Spark core |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| Spark.statistics | 统计除了SparkR，Spark.statistics作为基于Spark核心的基本统计实现的组装程序 |'
- en: 'Table 2: Summary of the most useful third-party packages based on use cases
    and application areas of machine learning with Spark'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：基于Spark的机器学习用例和应用领域的最有用的第三方包的总结
- en: Using external libraries with Spark Core
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark Core的外部库
- en: 'In order to work with these external libraries, instead of placing the jars
    in any specific folder, a simple fix would be to start the `pyspark` shell or
    spark-shell with the following arguments:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用这些外部库，而不是将jar文件放在任何特定的文件夹中，一个简单的解决方法是使用以下参数启动`pyspark` shell或spark-shell：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will automatically load the required `spark-csv` jars. However, these
    two jar files have to be downloaded to the Spark distribution using the following
    command in Ubuntu:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这将自动加载所需的`spark-csv` jar文件。但是，这两个jar文件必须使用以下命令在Ubuntu中下载到Spark分发中：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, to create an active Spark session, use the following line of codes:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，要创建一个活跃的Spark会话，使用以下代码行：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once you have instantiated an active Spark session, use the following lines
    of code to read the csv input file:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您实例化了一个活跃的Spark会话，使用以下代码行来读取csv输入文件：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that here we define the `com.databricks.spark.csv` input format by using
    the `format()` method, dedicatedly developed by Databricks for faster CSV file
    reading and parsing, and by setting the auxiliary option for the header as true
    using the `option()` method. Finally, the `load()` method loads the input data
    from the `input/letterdata.data` location, for example.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里使用`format()`方法定义了`com.databricks.spark.csv`输入格式，这是由Databricks专门开发的，用于更快的CSV文件读取和解析，并使用`option()`方法将辅助选项设置为true。最后，`load()`方法从`input/letterdata.data`位置加载输入数据，例如。
- en: As a continuation, in the next section, we will discuss configuring the Spark-TS
    library for time series data analysis.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在下一节中，我们将讨论配置Spark-TS库以进行时间序列数据分析。
- en: Tip
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Interested readers should visit the third-party ML packages web page for Spark
    at [https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22](https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22)
    for the package-specific discussion, updates, and configuration procedures.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的读者应该访问Spark的第三方ML包网页[https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22](https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22)了解特定包的讨论、更新和配置程序。
- en: Time series analysis using the Cloudera Spark-TS package
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Cloudera Spark-TS包进行时间序列分析
- en: As discussed in *[Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data")*, *Advanced
    Machine Learning with Streaming and Graph Data*, we will see how to configure
    the Spark-TS package developed by Cloudera. Mainly, we will talk about the TimeSeriesRDD
    in this section.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在*[第9章](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d "第9章。流式和图数据的高级机器学习")*中讨论的，我们将看到如何配置由Cloudera开发的Spark-TS包。主要是，我们将在本节讨论TimeSeriesRDD。
- en: Time series data
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列数据
- en: 'Time series data consists of sequences of measurements, each occurring at a
    point in time. A variety of terms are used to describe time series data, and many
    of them apply to conflicting or overlapping concepts. In the interest of clarity,
    in Spark-TS, Cloudera sticks to a particular vocabulary. Three objects are important
    in time series data analysis: time series, instant, and observation:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据由一系列测量组成，每个测量发生在某个时间点。有许多术语用于描述时间序列数据，其中许多适用于冲突或重叠的概念。为了清晰起见，在Spark-TS中，Cloudera坚持使用特定的词汇。时间序列数据分析中有三个重要的对象：时间序列、瞬时和观察：
- en: A time series is a sequence of real (that is, floating-point) values, each linked
    to a specific timestamp. Particularly, this sticks with time series as meaning
    a univariate time series. In Scala, a time series is usually represented by a
    Breeze presented at [https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze)
    vector, and in Python, a 1-D NumPy array (refer to [http://www.numpy.org/](http://www.numpy.org/)
    for more), and has a `DateTimeIndex` as shown at [https://github.com/sryza/spark-timeseries/blob/master/src/main/scala/com/cloudera/sparkts/DateTimeIndex.scala](https://github.com/sryza/spark-timeseries/blob/master/src/main/scala/com/cloudera/sparkts/DateTimeIndex.scala).
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列是一系列实数（即浮点数）值，每个值都与特定的时间戳相关联。特别是，这与时间序列的含义一致，即一元时间序列。在Scala中，时间序列通常由Breeze在[https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze)中表示的向量表示，在Python中，是一个1-D
    NumPy数组（更多信息请参考[http://www.numpy.org/](http://www.numpy.org/)），并且具有`DateTimeIndex`，如[https://github.com/sryza/spark-timeseries/blob/master/src/main/scala/com/cloudera/sparkts/DateTimeIndex.scala](https://github.com/sryza/spark-timeseries/blob/master/src/main/scala/com/cloudera/sparkts/DateTimeIndex.scala)所示。
- en: On the other hand, an instant is the vector of values in a collection of time
    series corresponding to a single point in time. In the Spark-TS library, each
    time series is typically labeled with a key that enables it to be identified among
    a collection of time series.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，瞬时是与单个时间点对应的时间序列集合中的值向量。在Spark-TS库中，每个时间序列通常都带有一个键，使其能够在时间序列集合中被识别。
- en: Finally, an observation is a tuple of (timestamp, key, value), that is, a single
    value in a time series or instant.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，观察是一个元组（时间戳，键，值），也就是时间序列或瞬时中的单个值。
- en: However, not all data with timestamps are time series data. For example, logs
    don't fit directly into time series since they consist of discrete events, not
    scalar measurements taken at intervals. However, measurements of log messages
    per hour would constitute a time series.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有带有时间戳的数据都是时间序列数据。例如，日志不直接适用于时间序列，因为它们由离散事件组成，而不是在间隔中进行的标量测量。但是，每小时的日志消息测量将构成一个时间序列。
- en: Configuring Spark-TS
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Spark-TS
- en: 'The most straightforward way to access Spark-TS from Scala is to depend on
    it in a Maven project. Do this by including the following repo in `pom.xml`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从Scala中访问Spark-TS的最直接的方法是在Maven项目中依赖它。通过在`pom.xml`中包含以下repo来实现：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To get the raw `pom.xml` file, interested readers should go to the following
    URL:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取原始的`pom.xml`文件，感兴趣的读者应该访问以下网址：
- en: '[https://github.com/sryza/spark-timeseries/blob/master/pom.xml](https://github.com/sryza/spark-timeseries/blob/master/pom.xml)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/sryza/spark-timeseries/blob/master/pom.xml](https://github.com/sryza/spark-timeseries/blob/master/pom.xml)'
- en: 'Alternatively, to access it in a spark-shell, download the JAR from [https://repository.cloudera.com/cloudera/libs-release-local/com/cloudera/sparkts/sparkts/0.1.0/sparkts-0.1.0-jar-with-dependencies.jar](https://repository.cloudera.com/cloudera/libs-release-local/com/cloudera/sparkts/sparkts/0.1.0/sparkts-0.1.0-jar-with-dependencies.jar),
    and then launch the shell with the following command as discussed in the *Using
    external libraries with Spark Core* section in this chapter:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，要在spark-shell中访问它，请从[https://repository.cloudera.com/cloudera/libs-release-local/com/cloudera/sparkts/sparkts/0.1.0/sparkts-0.1.0-jar-with-dependencies.jar](https://repository.cloudera.com/cloudera/libs-release-local/com/cloudera/sparkts/sparkts/0.1.0/sparkts-0.1.0-jar-with-dependencies.jar)下载JAR文件，然后按照本章的*使用Spark
    Core的外部库*部分中讨论的命令启动shell。
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: TimeSeriesRDD
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TimeSeriesRDD
- en: According to the Spark-TS engineering blog written on the Cloudera website at
    [http://blog.cloudera.com/blog/2015/12/spark-ts-a-new-library-for-analyzing-time-series-data-with-apache-spark/](http://blog.cloudera.com/blog/2015/12/spark-ts-a-new-library-for-analyzing-time-series-data-with-apache-spark/),
    TimeSeriesRDD is central to Spark-TS, where each object in the RDD stores a full
    univariate series. Operations that tend to apply exclusively to time series are
    much more efficient. For example, if you want to generate a set of lagged time
    series from your original collection of time series, each lagged series can be
    computed just by looking at a single record in the input RDD.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Cloudera网站上的Spark-TS工程博客[http://blog.cloudera.com/blog/2015/12/spark-ts-a-new-library-for-analyzing-time-series-data-with-apache-spark/](http://blog.cloudera.com/blog/2015/12/spark-ts-a-new-library-for-analyzing-time-series-data-with-apache-spark/)，TimeSeriesRDD是Spark-TS的核心，RDD中的每个对象存储完整的单变量系列。对于时间序列专用的操作效率更高。例如，如果您想从原始时间序列集生成一组滞后时间序列，每个滞后序列可以通过查看输入RDD中的单个记录来计算。
- en: Similarly, with imputing missing values based on surrounding values, or fitting
    time series models to each series, all of the data needed is present in a single
    array. Therefore, the central abstraction of the Spark-TS library is TimeSeriesRDD,
    which is simply a collection of time series on which you can operate in a distributed
    fashion. This approach allows you to avoid storing timestamps for each series
    and instead store a single `DateTimeIndex` to which all the series vectors conform.
    `TimeSeriesRDD[K]` extends `RDD[(K, Vector[Double])]`, where K is the key type
    (usually a string), and the second element in the tuple is a Breeze vector representing
    the time series.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，通过根据周围值填充缺失值或为每个系列拟合时间序列模型，所有所需的数据都存在于单个数组中。因此，Spark-TS库的核心抽象是TimeSeriesRDD，它只是可以以分布式方式操作的时间序列集合。这种方法允许您避免为每个系列存储时间戳，而是存储一个单一的`DateTimeIndex`，所有系列向量都符合该索引。`TimeSeriesRDD[K]`扩展了`RDD[(K,
    Vector[Double])]`，其中K是键类型（通常是字符串），元组中的第二个元素是表示时间序列的Breeze向量。
- en: 'A more technical discussion can be found in the GitHub URL: [https://github.com/sryza/spark-timeseries](https://github.com/sryza/spark-timeseries).
    Since this is a Third Party Package, a detailed discussion is out of the scope
    of this book, we believe.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在GitHub网址[https://github.com/sryza/spark-timeseries](https://github.com/sryza/spark-timeseries)找到更多技术讨论。由于这是一个第三方包，详细讨论超出了本书的范围，我们认为。
- en: Configuring SparkR with RStudio
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RStudio配置SparkR
- en: 'Let''s assume you have RStudio installed on your machine. Follow the steps
    mentioned here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的计算机上安装了RStudio。按照这里提到的步骤：
- en: 'Now open RStudio and create a new R script; then write the following code:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在打开RStudio并创建一个新的R脚本；然后编写以下代码：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Load the necessary package for SparkR by using this code:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用以下代码加载SparkR所需的软件包：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Configure the SparkR environment as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置SparkR环境如下：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now let''s create the first DataFrame and print the first few rows, as follows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们创建第一个DataFrame并打印前几行，如下所示：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You might need to install the following packages in order to make the `devtools`
    package work:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可能需要安装以下软件包才能使`devtools`软件包正常工作：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Morever, you might need to install `libcurl` for RCurl, which devtools depends
    on. To do this, just run this command:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，您可能需要安装`libcurl`来支持devtools所依赖的RCurl。只需运行此命令：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now configure the `ggplot2.SparkR` package from GitHub using the following
    code:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用以下代码从GitHub配置`ggplot2.SparkR`包：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now let''s compute the skewness and kurtosis for the sample DataFrame that
    we have just created. Before that, load the necessary packages:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们计算刚刚创建的样本DataFrame的偏度和峰度。在此之前，加载必要的软件包：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s create the DataFrame for the daily exercise example shown in the *Feature
    engineering and data exploration* section in *[Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering")*, *Extracting Knowledge
    through Feature Engineering*, and show the first few rows using `head` command:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们为*特征工程和数据探索*部分中的每日锻炼示例创建DataFrame，显示前几行使用`head`命令：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now calculate the skewness and kurtosis, as follows:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在按照以下步骤计算偏度和峰度：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You are probably aware that we used the two terms `skewness` and `kurtosis`
    in [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Knowledge
    through Feature Engineering*. If you are not familiar with these two terms, here
    is a bit of definition of them. Well, from the statistical perspective, `skewness`
    is a measure of symmetry. Alternatively and more precisely, it signifies the lack
    of symmetry in a distribution of the dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到我们在*特征工程*部分中使用了`偏度`和`峰度`这两个术语。如果您对这两个术语不熟悉，这里是它们的定义。从统计角度来看，`偏度`是对称性的度量。或者更准确地说，它表示数据集的分布中的对称性缺失。
- en: Now you might be wondering what symmetric is. Well, a distribution of the dataset
    is symmetric if it looks the same to the left and right of the center point.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可能想知道对称是什么。嗯，如果数据集的分布在中心点的左侧和右侧看起来相同，则数据集是对称的。
- en: 'Kurtosis, on the other hand, is a measure of whether the data are heavy-tailed
    or light-tailed relative to a normal distribution:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，峰度是衡量数据相对于正态分布是重尾还是轻尾的指标：
- en: 'Finally, let''s plot the density plot graph by calling the `ggplot()` method
    of the `ggplot2.SparkR` package:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过调用`ggplot2.SparkR`包的`ggplot()`方法来绘制密度图。
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you are not familiar with the `ggplot2` R package, note that `ggplot2` is
    a plotting system for R based on the grammar of graphics of base and lattice graphics.
    It provides many fiddly details of the graphics that make plotting a hassle, for
    example, placing or drawing legends in a graph, as well as providing a powerful
    model of graphics. This will make your life easier in order to produce simple
    as well as complex multi-layered graphics.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉`ggplot2` R包，请注意，`ggplot2`是基于R的绘图系统，基于基本和格栅图形的图形语法。它提供了许多使绘图成为一种麻烦的图形的琐碎细节，例如，在图形中放置或绘制图例，以及提供强大的图形模型。这将使您的生活更轻松，以便生成简单和复杂的多层图形。
- en: Tip
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'More info about `ggplot2` and its documentation can be found at the following
    website: [http://docs.ggplot2.org/current/](http://docs.ggplot2.org/current/).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`ggplot2`及其文档的更多信息，请访问以下网站：[http://docs.ggplot2.org/current/](http://docs.ggplot2.org/current/)。
- en: Configuring Hadoop run-time on Windows
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Windows上配置Hadoop运行时
- en: If you are developing your machine learning application on windows using Eclipse
    (as Maven project of course), probably you will face a problem since Spark expects
    that there is a runtime environment for Hadoop on Windows too.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Eclipse（当然是Maven项目）在Windows上开发您的机器学习应用程序，可能会遇到问题，因为Spark也希望在Windows上有Hadoop的运行时环境。
- en: 'More specifically, suppose you are running a Spark project written in Java
    with main class as `JavaNaiveBayes_ML.java`, then you will experience an IO exception
    saying that:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，假设您正在运行一个用Java编写的Spark项目，主类为`JavaNaiveBayes_ML.java`，那么您将遇到一个IO异常，说：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Configuring Hadoop run-time on Windows](img/00125.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![在Windows上配置Hadoop运行时](img/00125.jpeg)'
- en: 'Figure 1: IO exception due to the missing Hadoop runtime'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：由于缺少Hadoop运行时而导致的IO异常
- en: The reason is that by default Hadoop is developed for the Linux environment
    and if you are developing your Spark applications on windows platform, a bridge
    is required that will provide the Hadoop environment for the Hadoop runtime for
    Spark to be properly executed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是默认情况下，Hadoop是为Linux环境开发的，如果您在Windows平台上开发Spark应用程序，则需要一个桥梁，它将为Spark提供Hadoop环境，以便正确执行Hadoop运行时。
- en: Now, how to get rid of this problem then? The solution is straight forward.
    As the error message says, we need to have an executable namely `winutils.exe`.
    Now download the `winutils.exe` file from the code directory of Packt for this
    chapter and copy and paste it in the Spark distribution directory and configure
    Eclipse.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如何摆脱这个问题呢？解决方案很简单。正如错误消息所说，我们需要一个名为`winutils.exe`的可执行文件。现在从Packt的代码目录下载`winutils.exe`文件，并将其复制粘贴到Spark分发目录中，并配置Eclipse。
- en: More specifically, suppose your Spark distribution containing Hadoop is located
    at `C:/Users/spark-2.0.0-bin-hadoop2.7`. Inside the Spark distribution there is
    a directory named `bin.` Now, paste the executable there (that is, `path = C:/Users/spark-2.0.0-binhadoop2.7/``bin/`).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，假设您的包含Hadoop的Spark分发位于`C:/Users/spark-2.0.0-bin-hadoop2.7`。在Spark分发中有一个名为`bin`的目录。现在，将可执行文件粘贴到那里（即`path
    = C:/Users/spark-2.0.0-binhadoop2.7/``bin/`）。
- en: The second phase of the solution is going to Eclipse, select the main class
    (that is, `JavaNaiveBayes_ML.java` in this case), and then go to the **Run** menu.
    From the **Run** menu go to the **Run Configurations** option and from this option
    select the **Environment** tab. If you select the tab, you a will have the option
    to create a new environmental variable for Eclipse suing the JVM.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案的第二阶段是进入Eclipse，选择主类（即在本例中为`JavaNaiveBayes_ML.java`），然后转到**运行**菜单。从**运行**菜单转到**运行配置**选项，然后从该选项中选择**环境**选项卡。如果选择该选项卡，您将有一个选项可以为Eclipse使用JVM创建一个新的环境变量。
- en: Now create a new environmental variable and put the value as `C:/Users/spark-2.0.0-bin-hadoop2.7/`.
    Now press on **apply** and re-run your application and your problem should be
    resolved.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建一个新的环境变量，并将值设置为`C:/Users/spark-2.0.0-bin-hadoop2.7/`。现在点击**应用**，重新运行您的应用程序，您的问题应该得到解决。
- en: 'More technically, the details of the IO exception can be described as follows
    in Figure 1:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，IO异常的细节可以在图1中描述如下：
- en: Summary
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we showed how to use external libraries with Spark to expand
    data analyses.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了如何使用Spark扩展数据分析的外部库。
- en: More and more Spark as well as third-party packages are being developed by open
    source contributors. Readers should be updated with the latest news and release
    on the Spark website. They also should be notified of about the latest machine
    learning APIs, since the development of Spark is continuous and innovative and,
    of course, sometimes after a certain package becomes obsolete or deprecated.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的Spark以及第三方包正在由开源贡献者开发。读者应该及时了解Spark网站上的最新消息和发布。他们还应该得到有关最新机器学习API的通知，因为Spark的开发是持续和创新的，当然，有时在某个包变得过时或被弃用后。
- en: Throughout this book, we have tried to guide you on how to use the most popular
    and widely used machine learning algorithms that have been developed by Spark.
    However, there are other algorithms too that we could not discuss, and more and
    more algorithms will be added to the Spark ML and MLlib packages.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们试图指导您如何使用由Spark开发的最流行和广泛使用的机器学习算法。然而，还有其他算法，我们无法讨论，而且越来越多的算法将被添加到Spark
    ML和MLlib包中。
- en: This is more or less the end of our little journey with Spark. Now a general
    suggestion from our side to you as readers, or if you are relatively new to machine
    learning, Java, or Spark at first try to understand whether a problem is really
    a machine learning problem. If it is a machine learning problem, try to guess
    what type of learning algorithms should be the best fit, that is, classification,
    clustering, regression, recommendation, or frequent pattern mining.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是我们与Spark的小旅程的结束。现在我们向您作为读者的一般建议，或者如果您对机器学习、Java或Spark相对较新，首先要了解一个问题是否真的是一个机器学习问题。如果是一个机器学习问题，试着猜测哪种类型的学习算法应该是最合适的，即分类、聚类、回归、推荐或频繁模式挖掘。
- en: Then define and formulate the problem. After that you should generate or download
    the appropriate data based on the feature engineering concept of Spark that we
    have discussed. Then you can select an ML model that will provide better results
    in terms of accuracy. However, as discussed earlier, the model selection really
    depends on your data and problem type.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后定义和规划问题。之后，您应该基于我们讨论过的Spark的特征工程概念生成或下载适当的数据。然后，您可以选择一个ML模型，该模型将在准确性方面提供更好的结果。但是，正如前面讨论的那样，模型选择确实取决于您的数据和问题类型。
- en: Now that you have your data ready to train the model, go straight to train the
    model towards making predictive analytics.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好训练模型的数据，可以直接开始训练模型以进行预测分析。
- en: When your model is trained, evaluate it to see how it goes and fulfills your
    prediction expectations. Well, if you are not happy with the performance, try
    changing to other ML algorithms towards model selection. As discussed in [Chapter
    7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d "Chapter 7. Tuning
    Machine Learning Models"), *Tuning Machine Learning Models*, even proper model
    selection cannot provide the best result sometimes because of the nature of the
    data you have.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的模型训练好后，评估它以查看其表现并满足您的预测期望。如果您对性能不满意，请尝试切换到其他ML算法以进行模型选择。正如在[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "第7章。调整机器学习模型")中讨论的那样，*调整机器学习模型*，即使适当的模型选择有时也无法提供最佳结果，因为您拥有的数据的性质。
- en: So what is to be done? It's simple. Tune your ML model using the available tuning
    algorithms to properly set the hyperparameters. You might also need to make your
    model adaptable for new data types, especially if you are developing an ML application
    for a dynamic environment such as time series analysis or streaming analytics.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 那么应该做什么呢？很简单。使用可用的调整算法来调整您的ML模型，以正确设置超参数。您可能还需要使您的模型适应新的数据类型，特别是如果您正在为时间序列分析或流式分析等动态环境开发ML应用程序。
- en: Finally, deploy your model and you have it as a robust ML application.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，部署您的模型，您将拥有一个强大的ML应用程序。
- en: Our final recommendation to the readers is to browse the Spark website (at [http://spark.apache.org/](http://spark.apache.org/))
    regularly to get updates and also try to incorporate the regular Spark provided
    APIs with other third-party applications to get the best result of the collaboration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终向读者推荐定期浏览Spark网站（位于[http://spark.apache.org/](http://spark.apache.org/)），以获取更新，并尝试将常规提供的Spark
    API与其他第三方应用程序结合起来，以获得合作的最佳结果。
- en: '***This eBook was posted by AlenMiler on AvaxHome!***'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这本电子书是由AlenMiler在AvaxHome上发布的！
- en: '***Many New eBooks in my Blog:*** [http://avxhome.in/blogs/AlenMiler](https://tr.im/fgrfegtr)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '***我的博客中有许多新的电子书：*** [http://avxhome.in/blogs/AlenMiler](https://tr.im/fgrfegtr)'
- en: '***Mirror:*** [https://avxhome.unblocked.tw/blogs/AlenMiler](https://tr.im/geresttre)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '***镜像：*** [https://avxhome.unblocked.tw/blogs/AlenMiler](https://tr.im/geresttre)'
