- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Having already written an introductory book on the Hadoop ecosystem, I was pleased
    to be asked by Packt to write a book on Apache Spark. Being a practical person
    with a support and maintenance background, I am drawn to system builds and integration.
    So, I always ask the questions "how can the systems be used?", "how do they fit
    together?" and "what do they integrate with?" In this book, I will describe each
    module of Spark, and explain how they can be used with practical examples. I will
    also show how the functionality of Spark can be extended with extra libraries
    like H2O from [http://h2o.ai/](http://h2o.ai/).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 已经写了一本关于Hadoop生态系统的介绍性书籍，我很高兴Packt邀请我写一本关于Apache Spark的书。作为一个有支持和维护背景的实用主义者，我对系统构建和集成很感兴趣。因此，我总是问自己“系统如何被使用？”，“它们如何相互配合？”，“它们与什么集成？”在本书中，我将描述Spark的每个模块，并通过实际例子解释它们如何被使用。我还将展示如何通过额外的库（如来自[http://h2o.ai/](http://h2o.ai/)的H2O）扩展Spark的功能。
- en: I will show how Apache Spark's Graph processing module can be used in conjunction
    with the Titan graph database from Aurelius (now DataStax). This provides a coupling
    of graph-based processing and storage by grouping together Spark GraphX and Titan.
    The streaming chapter will show how data can be passed to Spark streams using
    tools like Apache Flume and Kafka.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我将展示Apache Spark的图处理模块如何与Aurelius（现在是DataStax）的Titan图数据库一起使用。这将通过将Spark GraphX和Titan组合在一起，提供基于图的处理和存储的耦合。流处理章节将展示如何使用Apache
    Flume和Kafka等工具将数据传递给Spark流。
- en: Given that in the last few years there has been a large-scale migration to cloud-based
    services, I will examine the Spark cloud service available at [https://databricks.com/](https://databricks.com/).
    I will do so from a practical viewpoint, this book does not attempt to answer
    the question "server or cloud", as I believe it to be a subject of a separate
    book; it just examines the service that is available.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到过去几年已经有大规模迁移到基于云的服务，我将检查[https://databricks.com/](https://databricks.com/)提供的Spark云服务。我将从实际的角度来做，本书不试图回答“服务器还是云”的问题，因为我认为这是另一本书的主题；它只是检查了可用的服务。
- en: What this book covers
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书涵盖的内容
- en: '[Chapter 1](ch01.html "Chapter 1. Apache Spark"), *Apache Spark*, will give
    a complete overview of Spark, functionalities of its modules, and the tools available
    for processing and storage. This chapter will briefly give the details of SQL,
    Streaming, GraphX, MLlib, Databricks, and Hive on Spark.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一章](ch01.html "第一章. Apache Spark") *Apache Spark*，将全面介绍Spark，其模块的功能以及用于处理和存储的工具。本章将简要介绍SQL、流处理、GraphX、MLlib、Databricks和Hive
    on Spark的细节。'
- en: '[Chapter 2](ch02.html "Chapter 2. Apache Spark MLlib"), *Apache Spark MLlib*,
    covers the MLlib module, where MLlib stands for Machine Learning Library. This
    describes the Apache Hadoop and Spark cluster that I will be using during this
    book, as well as the operating system that is involved—CentOS. It also describes
    the development environment that is being used: Scala and SBT. It provides examples
    of both installing and building Apache Spark. A worked example of classification
    using the Naïve Bayes algorithm is explained, as is clustering with KMeans. Finally,
    an example build is used to extend Spark to include some Artificial Neural Network
    (ANN) work by Bert Greevenbosch ([www.bertgreevenbosch.nl](http://www.bertgreevenbosch.nl)).
    I have always been interested in neural nets, and being able to use Bert''s work
    (with his permission) in this chapter was enjoyable. So, the final topic in this
    chapter classifies some small images including distorted images using a simple
    ANN. The results and the resulting score are quite good!'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[第二章](ch02.html "第二章. Apache Spark MLlib") *Apache Spark MLlib*，涵盖了MLlib模块，其中MLlib代表机器学习库。它描述了本书中将使用的Apache
    Hadoop和Spark集群，以及涉及的操作系统——CentOS。它还描述了正在使用的开发环境：Scala和SBT。它提供了安装和构建Apache Spark的示例。解释了使用朴素贝叶斯算法进行分类的示例，以及使用KMeans进行聚类的示例。最后，使用Bert
    Greevenbosch（[www.bertgreevenbosch.nl](http://www.bertgreevenbosch.nl)）的工作扩展Spark以包括一些人工神经网络（ANN）工作的示例。我一直对神经网络很感兴趣，能够在本章中使用Bert的工作（在得到他的许可后）是一件令人愉快的事情。因此，本章的最后一个主题是使用简单的ANN对一些小图像进行分类，包括扭曲的图像。结果和得分都相当不错！'
- en: '[Chapter 3](ch03.html "Chapter 3. Apache Spark Streaming"), *Apache Spark Streaming*,
    covers the comparison of Apache Spark to Storm and especially Spark Streaming,
    but I think that Spark offers much more functionality. For instance, the data
    used in one Spark module can be passed to and used in another. Also, as shown
    in this chapter, Spark streaming integrates easily with big data movement technologies
    like Flume and Kafka.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[第三章](ch03.html "第三章. Apache Spark Streaming") *Apache Spark Streaming*，涵盖了Apache
    Spark与Storm的比较，特别是Spark Streaming，但我认为Spark提供了更多的功能。例如，一个Spark模块中使用的数据可以传递到另一个模块中并被使用。此外，正如本章所示，Spark流处理可以轻松集成大数据移动技术，如Flume和Kafka。'
- en: So, the streaming chapter starts by giving an overview of checkpointing, and
    explains when you might want to use it. It gives Scala code examples of how it
    can be used, and shows the data can be stored on HDFS. It then moves on to give
    practical examples in Scala, as well as execution examples of TCP, File, Flume,
    and the Kafka streaming. The last two options are shown by processing an RSS data
    stream and finally storing it on HDFS.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，流处理章节首先概述了检查点，并解释了何时可能需要使用它。它给出了Scala代码示例，说明了如何使用它，并展示了数据如何存储在HDFS上。然后，它继续给出了Scala的实际示例，以及TCP、文件、Flume和Kafka流处理的执行示例。最后两个选项通过处理RSS数据流并最终将其存储在HDFS上来展示。
- en: '[Chapter 4](ch04.html "Chapter 4. Apache Spark SQL"), *Apache Spark SQL*, explains
    the Spark SQL context in Scala code terms. It explains File I/O as text, Parquet,
    and JSON formats. Using Apache Spark 1.3 it explains the use of data frames by
    example, and shows the methods that they make available for data analytics. It
    also introduces Spark SQL by Scala-based example, showing how temporary tables
    can be created, and how the SQL-based operations can be used against them.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[第4章](ch04.html "第4章。Apache Spark SQL") *Apache Spark SQL*，用Scala代码术语解释了Spark
    SQL上下文。它解释了文本、Parquet和JSON格式的文件I/O。使用Apache Spark 1.3，它通过示例解释了数据框架的使用，并展示了它们提供的数据分析方法。它还通过基于Scala的示例介绍了Spark
    SQL，展示了如何创建临时表，以及如何对其进行SQL操作。'
- en: Next, the Hive context is introduced. Initially, a local context is created
    and the Hive QL operations are then executed against it. Then, a method is introduced
    to integrate an existing distributed CDH 5.3 Hive installation to a Spark Hive
    context. Operations against this context are then shown to update a Hive database
    on the cluster. In this way, the Spark applications can be created and scheduled
    so that the Hive operations are driven by the real-time Spark engine.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，介绍了Hive上下文。首先创建了一个本地上下文，然后执行了Hive QL操作。然后，介绍了一种方法，将现有的分布式CDH 5.3 Hive安装集成到Spark
    Hive上下文中。然后展示了针对此上下文的操作，以更新集群上的Hive数据库。通过这种方式，可以创建和调度Spark应用程序，以便Hive操作由实时Spark引擎驱动。
- en: Finally, the ability to create user-defined functions (UDFs) is introduced,
    and the UDFs that are created are then used in the SQL calls against the temporary
    tables.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，介绍了创建用户定义函数（UDFs），然后使用创建的UDFs对临时表进行SQL调用。
- en: '[Chapter 5](ch05.html "Chapter 5. Apache Spark GraphX"), *Apache Spark GraphX*,
    introduces the Apache Spark GraphX module and the graph processing module. It
    works through a series of graph functions by example from based counting to triangle
    processing. It then introduces Kenny Bastani''s Mazerunner work which integrates
    the Neo4j NoSQL database with Apache Spark. This work has been introduced with
    Kenny''s permission; take a look at [www.kennybastani.com](http://www.kennybastani.com).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](ch05.html "第5章。Apache Spark GraphX") *Apache Spark GraphX*，介绍了Apache
    Spark GraphX模块和图形处理模块。它通过一系列基于示例的图形函数工作，从基于计数到三角形处理。然后介绍了Kenny Bastani的Mazerunner工作，该工作将Neo4j
    NoSQL数据库与Apache Spark集成。这项工作已经得到Kenny的许可；请访问[www.kennybastani.com](http://www.kennybastani.com)。'
- en: This chapter works through the introduction of Docker, then Neo4j, and then
    it gives an introduction to the Neo4j interface. Finally, it works through some
    of the Mazerunner supplied functionality via the supplied REST interface.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过Docker的介绍，然后是Neo4j，然后介绍了Neo4j接口。最后，通过提供的REST接口介绍了一些Mazerunner提供的功能。
- en: '[Chapter 6](ch06.html "Chapter 6. Graph-based Storage"), *Graph-based Storage*,
    examines graph-based storage as Apache Spark Graph processing was introduced in
    this book. I looked for a product that could integrate with Hadoop, was open sourced,
    could scale to a very high degree, and could integrate with Apache Spark.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[第6章](ch06.html "第6章。基于图形的存储") *基于图形的存储*，检查了基于图形的存储，因为本书介绍了Apache Spark图形处理。我寻找一个能够与Hadoop集成、开源、能够高度扩展，并且能够与Apache
    Spark集成的产品。'
- en: Although it is still a relatively young product both in terms of community support
    and development, I think that Titan from Aurelius (now DataStax) fits the bill.
    The 0.9.x releases that are available, as I write now, use Apache TinkerPop for
    graph processing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在社区支持和开发方面仍然相对年轻，但我认为Aurelius（现在是DataStax）的Titan符合要求。截至我写作时，可用的0.9.x版本使用Apache
    TinkerPop进行图形处理。
- en: This chapter provides worked examples of graph creation and storage using Gremlin
    shell and Titan. It shows how both HBase and Cassandra can be used for backend
    Titan storage.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了使用Gremlin shell和Titan创建和存储图形的示例。它展示了如何将HBase和Cassandra用于后端Titan存储。
- en: '[Chapter 7](ch07.html "Chapter 7. Extending Spark with H2O"), *Extending Spark
    with H2O*, talks about the H2O library set developed at [http://h2o.ai/](http://h2o.ai/),
    which is a machine learning library system that can be used to extend the functionality
    of Apache Spark. In this chapter, I examine the sourcing and installation of H2O,
    as well as the Flow interface for data analytics. The architecture of Sparkling
    Water is examined, as is data quality and performance tuning.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](ch07.html "第7章。使用H2O扩展Spark") *使用H2O扩展Spark*，讨论了在[http://h2o.ai/](http://h2o.ai/)开发的H2O库集，这是一个可以用来扩展Apache
    Spark功能的机器学习库系统。在本章中，我研究了H2O的获取和安装，以及用于数据分析的Flow接口。还研究了Sparkling Water的架构、数据质量和性能调优。'
- en: Finally, a worked example of deep learning is created and executed. [Chapter
    2](ch02.html "Chapter 2. Apache Spark MLlib"), *Spark MLlib*, used a simple ANN
    for neural classification. This chapter uses a highly configurable and tunable
    H2O deep learning neural network for classification. The result is a fast and
    accurate trained neural model, as you will see.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，创建并执行了一个深度学习的示例。[第2章](ch02.html "第2章。Apache Spark MLlib") *Spark MLlib*，使用简单的人工神经网络进行神经分类。本章使用了一个高度可配置和可调整的H2O深度学习神经网络进行分类。结果是一个快速而准确的训练好的神经模型，你会看到的。
- en: '[Chapter 8](ch08.html "Chapter 8. Spark Databricks"), *Spark Databricks*, introduces
    the [https://databricks.com/](https://databricks.com/) AWS cloud-based Apache
    Spark cluster system. It offers a step-by-step process of setting up both an AWS
    account and the Databricks account. It then steps through the [https://databricks.com/](https://databricks.com/)
    account functionality in terms of Notebooks, Folders, Jobs, Libraries, development
    environments, and more.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](ch08.html "第8章。Spark Databricks") *Spark Databricks*，介绍了[https://databricks.com/](https://databricks.com/)
    AWS基于云的Apache Spark集群系统。它提供了逐步设置AWS账户和Databricks账户的过程。然后，它逐步介绍了[https://databricks.com/](https://databricks.com/)账户功能，包括笔记本、文件夹、作业、库、开发环境等。'
- en: It examines the table-based storage and processing in Databricks, and also introduces
    the DBUtils package for Databricks utilities functionality. This is all done by
    example to give you a good understanding of how this cloud-based system can be
    used.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它检查了Databricks中基于表的存储和处理，并介绍了Databricks实用程序功能的DBUtils包。这一切都是通过示例完成的，以便让您对这个基于云的系统的使用有一个很好的理解。
- en: '[Chapter 9](ch09.html "Chapter 9. Databricks Visualization"), *Databricks Visualization*,
    extends the Databricks coverage by concentrating on data visualization and dashboards.
    It then examines the Databricks REST interface, showing how clusters can be managed
    remotely using various example REST API calls. Finally, it looks at data movement
    in terms of table''s folders and libraries.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](ch09.html "第9章 数据可视化")，*Databricks可视化*，通过专注于数据可视化和仪表板来扩展Databricks的覆盖范围。然后，它检查了Databricks的REST接口，展示了如何使用各种示例REST
    API调用远程管理集群。最后，它从表的文件夹和库的角度看数据移动。'
- en: The cluster management section of this chapter shows that it is possible to
    launch Apache Spark on AWS EC2 using scripts supplied with the Spark release.
    The [https://databricks.com/](https://databricks.com/) service takes this functionality
    a step further by providing a method to easily create and resize multiple EC2-based
    Spark clusters. It provides extra functionality for cluster management and usage,
    as well as user access and security as these two chapters show. Given that the
    people who brought us Apache Spark have created this service, it must be worth
    considering and examining.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的集群管理部分显示，可以使用Spark发布的脚本在AWS EC2上启动Apache Spark。[https://databricks.com/](https://databricks.com/)服务通过提供一种轻松创建和调整多个基于EC2的Spark集群的方法，进一步提供了这种功能。它为集群管理和使用提供了额外的功能，以及用户访问和安全性，正如这两章所示。考虑到为我们带来Apache
    Spark的人们创建了这项服务，它一定值得考虑和审查。
- en: What you need for this book
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书所需内容
- en: The practical examples in this book use Scala and SBT for Apache Spark-based
    code development and compilation. A Cloudera CDH 5.3 Hadoop cluster on CentOS
    6.5 Linux servers is also used. Linux Bash shell and Perl scripts are used both,
    to assist in Spark applications and provide data feeds. Hadoop administration
    commands are used to move and examine data during Spark applications tests.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的实际示例使用Scala和SBT进行基于Apache Spark的代码开发和编译。还使用了基于CentOS 6.5 Linux服务器的Cloudera
    CDH 5.3 Hadoop集群。Linux Bash shell和Perl脚本都用于帮助Spark应用程序并提供数据源。在Spark应用程序测试期间，使用Hadoop管理命令来移动和检查数据。
- en: Given the skill overview previously, it would be useful for the reader to have
    a basic understanding of Linux, Apache Hadoop, and Spark. Having said that, and
    given that there is an abundant amount of information available on the internet
    today, I would not want to stop an intrepid reader from just having a go. I believe
    that it is possible to learn more from mistakes than successes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到之前的技能概述，读者对Linux、Apache Hadoop和Spark有基本的了解会很有帮助。话虽如此，鉴于今天互联网上有大量信息可供查阅，我不想阻止一个勇敢的读者去尝试。我相信从错误中学到的东西可能比成功更有价值。
- en: Who this book is for
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这本书是为谁准备的
- en: This book is for anyone interested in Apache Hadoop and Spark who would like
    to learn more about Spark. It is for the user who would like to learn how the
    usage of Spark can be extended with systems like H2O. It is for the user who is
    interested in graph processing but would like to learn more about graph storage.
    If the reader wants to know about Apache Spark in the cloud then he/she can learn
    about [https://databricks.com/](https://databricks.com/), the cloud-based system
    developed by the people who brought them Spark. If you are a developer with some
    experience with Spark and want to strengthen your knowledge of how to get around
    in the world of Spark, then this book is ideal for you. Basic knowledge of Linux,
    Hadoop, and Spark is required to understand this book; reasonable knowledge of
    Scala is also expected.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书适用于任何对Apache Hadoop和Spark感兴趣的人，他们想了解更多关于Spark的知识。它适用于希望了解如何使用Spark扩展H2O等系统的用户。对于对图处理感兴趣但想了解更多关于图存储的用户。如果读者想了解云中的Apache
    Spark，那么他/她可以了解由为他们带来Spark的人开发的[https://databricks.com/](https://databricks.com/)。如果您是具有一定Spark经验的开发人员，并希望加强对Spark世界的了解，那么这本书非常适合您。要理解本书，需要具备Linux、Hadoop和Spark的基本知识；同时也需要合理的Scala知识。
- en: Conventions
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 约定
- en: In this book, you will find a number of text styles that distinguish between
    different kinds of information. Here are some examples of these styles and an
    explanation of their meaning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，您会发现一些文本样式，用于区分不同类型的信息。以下是这些样式的一些示例及其含义的解释。
- en: 'Code words in text, database table names, folder names, filenames, file extensions,
    pathnames, dummy URLs, user input, and Twitter handles are shown as follows: "The
    first step is to ensure that a Cloudera repository file exists under the `/etc/yum.repos.d`
    directory, on the server hc2nn and all of the other Hadoop cluster servers."'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 文本中的代码单词、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟URL、用户输入和Twitter句柄显示如下：“第一步是确保`/etc/yum.repos.d`目录下存在Cloudera存储库文件，在服务器hc2nn和所有其他Hadoop集群服务器上。”
- en: 'A block of code is set as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块设置如下：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出都是这样写的：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**New terms** and **important words** are shown in bold. Words that you see
    on the screen, for example, in menus or dialog boxes, appear in the text like
    this: "Select the **User Actions** option, and then select **Manage Access Keys**."'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**新术语**和**重要单词**以粗体显示。您在屏幕上看到的单词，例如菜单或对话框中的单词，会以这样的方式出现在文本中：“选择**用户操作**选项，然后选择**管理访问密钥**。”'
- en: Note
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Warnings or important notes appear in a box like this.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 警告或重要说明会以这样的方式出现在框中。
- en: Tip
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Tips and tricks appear like this.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 提示和技巧会以这样的方式出现。
- en: Reader feedback
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读者反馈
- en: Feedback from our readers is always welcome. Let us know what you think about
    this book—what you liked or disliked. Reader feedback is important for us as it
    helps us develop titles that you will really get the most out of.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终欢迎读者的反馈。让我们知道您对这本书的看法——您喜欢或不喜欢什么。读者的反馈对我们很重要，因为它有助于我们开发您真正能够充分利用的标题。
- en: To send us general feedback, simply e-mail `<[feedback@packtpub.com](mailto:feedback@packtpub.com)>`,
    and mention the book's title in the subject of your message.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要向我们发送一般反馈，只需发送电子邮件至`<[feedback@packtpub.com](mailto:feedback@packtpub.com)>`，并在主题中提及书名。
- en: If there is a topic that you have expertise in and you are interested in either
    writing or contributing to a book, see our author guide at [www.packtpub.com/authors](http://www.packtpub.com/authors).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在某个主题上有专业知识，并且有兴趣撰写或为书籍做出贡献，请参阅我们的作者指南[www.packtpub.com/authors](http://www.packtpub.com/authors)。
- en: Customer support
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户支持
- en: Now that you are the proud owner of a Packt book, we have a number of things
    to help you to get the most from your purchase.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您是Packt书籍的自豪所有者，我们有很多事情可以帮助您充分利用您的购买。
- en: Downloading the example code
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载示例代码
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[http://www.packtpub.com](http://www.packtpub.com)的帐户中为您购买的所有Packt Publishing图书下载示例代码文件。如果您在其他地方购买了这本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便将文件直接发送到您的电子邮件。
- en: Errata
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 勘误
- en: Although we have taken every care to ensure the accuracy of our content, mistakes
    do happen. If you find a mistake in one of our books—maybe a mistake in the text
    or the code—we would be grateful if you could report this to us. By doing so,
    you can save other readers from frustration and help us improve subsequent versions
    of this book. If you find any errata, please report them by visiting [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    selecting your book, clicking on the **Errata Submission Form** link, and entering
    the details of your errata. Once your errata are verified, your submission will
    be accepted and the errata will be uploaded to our website or added to any list
    of existing errata under the Errata section of that title.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经尽一切努力确保内容的准确性，但错误确实会发生。如果您在我们的书中发现错误——可能是文本或代码中的错误——我们将不胜感激，如果您能向我们报告。通过这样做，您可以帮助其他读者避免挫折，并帮助我们改进本书的后续版本。如果您发现任何勘误，请访问[http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)报告，选择您的书，点击**勘误提交表**链接，并输入您的勘误详情。一旦您的勘误经过验证，您的提交将被接受，并且勘误将被上传到我们的网站或添加到该标题的勘误部分下的任何现有勘误列表中。
- en: To view the previously submitted errata, go to [https://www.packtpub.com/books/content/support](https://www.packtpub.com/books/content/support)
    and enter the name of the book in the search field. The required information will
    appear under the **Errata** section.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看先前提交的勘误，请访问[https://www.packtpub.com/books/content/support](https://www.packtpub.com/books/content/support)并在搜索框中输入书名。所需信息将出现在**勘误**部分下。
- en: Piracy
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 盗版
- en: Piracy of copyrighted material on the Internet is an ongoing problem across
    all media. At Packt, we take the protection of our copyright and licenses very
    seriously. If you come across any illegal copies of our works in any form on the
    Internet, please provide us with the location address or website name immediately
    so that we can pursue a remedy.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网上的版权材料盗版是所有媒体的持续问题。在Packt，我们非常重视保护我们的版权和许可。如果您在互联网上以任何形式发现我们作品的非法副本，请立即向我们提供位置地址或网站名称，以便我们采取补救措施。
- en: Please contact us at `<[copyright@packtpub.com](mailto:copyright@packtpub.com)>`
    with a link to the suspected pirated material.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请通过`<[copyright@packtpub.com](mailto:copyright@packtpub.com)>`与我们联系，并附上涉嫌盗版材料的链接。
- en: We appreciate your help in protecting our authors and our ability to bring you
    valuable content.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢您帮助保护我们的作者和我们提供有价值内容的能力。
- en: Questions
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: If you have a problem with any aspect of this book, you can contact us at `<[questions@packtpub.com](mailto:questions@packtpub.com)>`,
    and we will do our best to address the problem.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对本书的任何方面有问题，可以通过`<[questions@packtpub.com](mailto:questions@packtpub.com)>`与我们联系，我们将尽力解决问题。
