- en: Chapter 3. Data Analysis – Explore and Wrangle
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 数据分析——探索与清洗
- en: 'We will cover the following recipes in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下几种方法：
- en: Analyzing univariate data graphically
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图形化分析单变量数据
- en: Grouping the data and using dot plots
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分组数据并使用点图
- en: Using scatter plots for multivariate data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用散点图展示多变量数据
- en: Using heat maps
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用热力图
- en: Performing summary statistics and plots
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行汇总统计和绘制图表
- en: Using a box-and-whisker plot
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用箱线图
- en: Imputing the data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填补缺失数据
- en: Performing random sampling
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行随机抽样
- en: Scaling the data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放数据
- en: Standardizing the data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化数据
- en: Performing tokenization
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行分词
- en: Removing stop words
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除停用词
- en: Stemming the words
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干化
- en: Performing word lemmatization
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行词形还原
- en: Representing the text as a bag of words
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本表示为词袋模型
- en: Calculating term frequencies and inverse document frequencies
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算词频和逆文档频率
- en: Introduction
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Before you venture into any data science application, it is always helpful in
    the long run to have a good understanding of the data that you are about to process.
    An understanding of the underlying data will help you choose the right algorithm
    to use for the problem at hand. Exploring the data at various levels of granularity
    is called **Exploratory Data Analysis** (**EDA**). In many cases, **EDA** can
    uncover patterns that are typically revealed by a data mining algorithm **EDA**
    helps us understand data characteristics and provides you with the proper guidance
    in order to choose the right algorithm for the given problem.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始任何数据科学应用之前，长期来看，深入了解你即将处理的数据总是很有帮助的。对底层数据的理解将帮助你为当前问题选择正确的算法。以不同粒度级别探索数据的过程叫做**探索性数据分析**（**EDA**）。在许多情况下，**EDA**能够揭示通常通过数据挖掘算法发现的模式。**EDA**帮助我们理解数据特征，并为你提供适当的指导，以便选择适合当前问题的算法。
- en: In this chapter, we will cover **EDA** in detail. We will look into the practical
    techniques and tools that are used to perform **EDA** operations in an effective
    way.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将详细介绍**EDA**。我们将探讨用于有效执行**EDA**操作的实践技巧和工具。
- en: Data preprocessing and transformation are two other important processes that
    can improve the quality of data science models and increase the success rate of
    data science projects.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理和转换是另外两个重要的过程，它们能够提高数据科学模型的质量，并增加数据科学项目的成功率。
- en: Data preprocessing is the process of making the data ready in order to be ingested
    either by a data mining method or machine learning algorithm. It encompasses many
    things such as data cleaning, attribute subset selection, data transformation,
    and others. We will cover both numerical data preprocessing and text data preprocessing
    in this chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理是使数据准备好供数据挖掘方法或机器学习算法使用的过程。它包括很多内容，如数据清洗、属性子集选择、数据转换等。我们将在本章中介绍数值数据预处理和文本数据预处理。
- en: Text data is a different beast than the numerical data. We need different transformation
    methods in order to make it suitable for ingestion in the machine learning algorithms.
    In this chapter, we will see how we can transform the text data. Typically, text
    transformation is a staged process with various components in the form of a pipeline.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据与数值数据有所不同。我们需要不同的转换方法，以使其适合机器学习算法的输入。在本章中，我们将看到如何转换文本数据。通常，文本转换是一个分阶段的过程，包含多个组件，形成一个管道。
- en: 'Some of the components are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一些组件如下所示：
- en: Tokenization
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词
- en: Stop word removal
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用词移除
- en: Base form conversion
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本形式转换
- en: Feature derivation
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征推导
- en: Typically, these components are applied to a given text in order to extract
    features. At the end of the pipeline, the text data is transformed in a way that
    it can be fed as input to the machine learning algorithms. In this chapter, we
    will see recipes for every component listed in the preceding pipeline.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些组件会应用于给定的文本，以提取特征。在管道的最后，文本数据将被转换成适合机器学习算法输入的形式。在本章中，我们将为每个组件提供相应的解决方案。
- en: Many times, a lot of errors may be introduced during the data collection phase.
    These may be due to human errors, limitations, or bugs in the data measuring or
    collective process/device. Data inconsistency is a big challenge. We will start
    our data preprocessing journey with data imputation is a way to handle errors
    in the incoming data and then proceed to other methods.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，在数据收集阶段可能会引入很多错误。这些错误可能是由于人为错误、限制或数据测量/收集过程中的缺陷。数据不一致性是一个大挑战。我们将从数据填充开始我们的数据预处理工作，填充是处理输入数据中的错误的方法，然后继续采用其他方法。
- en: Analyzing univariate data graphically
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过图形分析单变量数据
- en: 'Datasets with only one variable/column are called univariate data. Univariate
    is a general term in mathematics, which refers to any expression, equation, function,
    or polynomial with only one variable. In our case, we will restrict the univariate
    function to datasets. Let''s say that we will measure the heights of a group of
    people in meters; the data will look as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 仅包含一个变量/列的数据集称为单变量数据。单变量是数学中的一个通用术语，指的是只有一个变量的表达式、方程式、函数或多项式。在我们的例子中，我们将限制单变量函数于数据集。假设我们将测量一组人的身高（单位：米），数据将如下所示：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Our measurement is only about a single attribute of people, height. This is
    an example of univariate data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测量仅涉及人群的一个属性——身高。这是单变量数据的一个示例。
- en: Getting ready
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Let''s start our **EDA** recipe by looking at a sample univariate dataset through
    visualization. It is easy to analyze the data characteristics through the right
    visualization techniques. We will use `pyplot` to draw graphs in order to visualize
    the data. Pyplot is the state-machine interface to the matplotlib plotting library.
    Figures and axes are implicitly and automatically created to achieve the desired
    plot. The following link is a good reference for `pyplot`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过可视化查看样本单变量数据来开始我们的**EDA**流程。通过正确的可视化技术，分析数据特征变得容易。我们将使用`pyplot`绘制图表以可视化数据。Pyplot是matplotlib绘图库的状态机接口。图形和坐标轴会自动创建，以生成所需的图表。以下链接是`pyplot`的一个很好的参考：
- en: '[http://matplotlib.org/users/pyplot_tutorial.html](http://matplotlib.org/users/pyplot_tutorial.html)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://matplotlib.org/users/pyplot_tutorial.html](http://matplotlib.org/users/pyplot_tutorial.html)'
- en: 'For this example, we will use a number of Presidential Requests of Congress
    in State of the Union Address. The following URL contains the data:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本例中，我们将使用国情咨文中总统向国会提出的请求数量。以下链接包含数据：
- en: '[http://www.presidency.ucsb.edu/data/sourequests.php](http://www.presidency.ucsb.edu/data/sourequests.php)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.presidency.ucsb.edu/data/sourequests.php](http://www.presidency.ucsb.edu/data/sourequests.php)'
- en: 'The following is a sample of the data:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数据的示例：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will visually look at this data and identify any outliers present in the
    data. We will follow a recursive approach with respect to the outliers. Once we
    have identified the outliers, we will remove them from the dataset and plot the
    remaining data in order to find any new outliers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将直观地查看这些数据，并识别数据中的任何异常值。我们将采用递归方法处理异常值。一旦识别出异常值，我们将从数据集中移除它们，并重新绘制剩余数据，以便寻找新的异常值。
- en: Note
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Recursively looking into the data after removing the perceived outlier in every
    iteration is a common approach in detection of outliers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中移除识别出的异常值后，递归检查数据是检测异常值的常见方法。
- en: How to do it…
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: We will load the data using NumPy's data loading utility. Then, we will address
    the data quality issues; in this case, we will address how to handle the null
    values. As you can see in the data, the years 1956 and 1958 have null entries.
    Let's replace the null values by `0` using the lambda function.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用NumPy的数据加载工具加载数据。然后，我们将解决数据质量问题；在此案例中，我们将解决如何处理空值。正如你所看到的数据，1956年和1958年存在空值。让我们使用lambda函数将空值替换为`0`。
- en: 'Following this, let''s plot the data to look for any trends:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们绘制数据，寻找任何趋势：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s calculate the percentile values and plot them as references in the plot
    that has been generated:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算百分位数值，并将其作为参考线绘制到已经生成的图中：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, let''s inspect the data visually for outliers and then remove them
    using the mask function. Let''s plot the data again without the outliers:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过可视化检查数据中的异常值，然后使用掩码函数将其移除。接下来，我们再次绘制去除异常值后的数据：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works…
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'In the first step, we will put some data loading techniques that we learnt
    in the previous chapter to action. You will have noticed that the years `1956`
    and `1958` are left blank. We will replace them with `0` using an anonymous function:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们将应用在前一章中学到的一些数据加载技巧。你会注意到年份`1956`和`1958`被留空。我们将使用匿名函数将它们替换为`0`：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `fill_data` lambda function will replace any null value in the dataset;
    in this case, line no 11 and 13 with 0:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`fill_data`匿名函数将替换数据集中的任何空值；在这种情况下，第11行和第13行将替换为0：'
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will pass `fill_data` to the `genfromtxt` function''s `converters` parameter.
    Note that `converters` takes a dictionary as its input. The key in the dictionary
    dictates which column our function should be applied to. The value indicates the
    function. In this case, we specified `fill_data` as the function and set the key
    to 1 indicating that the `fill_data` function has to be applied to column 1\.
    Now let''s look at the data in the console:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`fill_data`传递给`genfromtxt`函数的`converters`参数。请注意，`converters`接受一个字典作为输入。字典中的键指定我们的函数应该应用于哪一列。值表示函数。在此案例中，我们指定`fill_data`作为函数，并将键设置为1，表示`fill_data`函数必须应用于第1列。现在让我们在控制台中查看数据：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As we can see, the years `1956` and `1958` have a `0` value added to them.
    For the ease of plotting, we will load the year data in x and the number of Presidential
    Requests to Congress in the State of Union Address to y:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，年份`1956`和`1958`被添加了一个`0`值。为了便于绘图，我们将把年份数据加载到x轴，国情咨文中总统向国会提出的请求数量加载到y轴：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, in the first column, the year is loaded in `x` and the next
    column in `y`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在第一列，年份被加载到`x`中，下一列加载到`y`中。
- en: 'In step 2, we will plot the data with the x axis as the year and y axis representing
    the values:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步中，我们将绘制数据，x轴为年份，y轴表示值：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We will first close any previous graphs that are open from the previous programs:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先关闭之前程序中打开的任何图形：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will give a number to our plot. This is very useful when we have a lot of
    graphs in a program:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为我们的图形指定一个编号。当程序中有很多图形时，这非常有用：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will specify a title for our plot:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为我们的图形指定一个标题：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we will plot x and y. The ''ro'' parameter tells plyplot to plot x
    and y as dots (0) in the color red (r):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将绘制x和y。'ro'参数告诉pyplot将x和y绘制为红色（r）的小圆点（0）：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, the *x* and *y* axes labels are provided.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，提供了*x*和*y*轴的标签。
- en: 'The output looks as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![How it works…](img/B04041_03_02.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_03_02.jpg)'
- en: 'A casual look at this graph shows that the data is spread everywhere and no
    trends or patterns can be found in the first glance. However, with a keen eye,
    you can notice three points: one point at the top on the right-hand side and others
    to the immediate left of **1960** in the *x* axis. They are starkly different
    from all the other points in the sample, and hence, they are outliers.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 随便看一下这张图，你会发现数据分布得很广泛，第一眼看不出任何趋势或模式。然而，仔细观察，你会注意到三个点：一个位于右上方，另两个则位于**1960**年附近的*x*轴上。它们与样本中的所有其他点截然不同，因此，它们是异常值。
- en: Note
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: An outlier is an observation that lies outside the overall pattern of a distribution
    (Moore and McCabe 1999).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是指位于分布整体模式之外的观测值（Moore和McCabe 1999）。
- en: In order to understand these points further, we will take the help of percentiles.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解这些点，我们将借助百分位数来分析。
- en: Note
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If we have a vector V of length N, the qth percentile of V is the qth ranked
    value in a sorted copy of V. The values and distances of the two nearest neighbors
    as well as the *interpolation* parameter will determine the percentile if the
    normalized ranking does not match q exactly. This function is the same as the
    median if`q=50`, the same as the minimum if `q=0`, and the same as the maximum
    if `q=100`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个长度为N的向量V，V的第q个百分位数是V的排序副本中的第q个排名值。如果归一化排名与q不完全匹配，则两个最邻近点的值和距离，以及*插值*参数将决定百分位数。此函数与中位数相同，如果`q=50`，与最小值相同，如果`q=0`，与最大值相同，如果`q=100`。
- en: Refer to [http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html](http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html)
    for more information.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html](http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html)了解更多信息。
- en: Why don't we use averages? We will look into averages in the summary statistics
    section; however, looking at the percentiles has its own advantages. Average values
    are typically skewed by outliers; outliers such as the one at the top on the right-hand
    side can drag the average to a higher value and the outliers near 1960 can do
    the opposite. Percentiles give us a better clarity about the range of values in
    our dataset. We can calculate the percentiles using NumPy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不使用平均值？我们将在总结统计部分探讨平均值；然而，查看百分位数有其独特的优势。平均值通常会被异常值所扭曲；例如，右上方的异常值会将平均值拉高，而接近1960的异常值则可能相反。百分位数能更清晰地反映数据集中值的范围。我们可以使用NumPy来计算百分位数。
- en: In step 3, we will calculate the percentiles and print them.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，我们将计算百分位数并打印出来。
- en: 'The percentile values calculated and printed for this dataset are as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 计算并打印出的百分位数值如下：
- en: '![How it works…](img/B04041_03_12.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_03_12.jpg)'
- en: Note
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Interpreting the percentiles:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 百分位数的解读：
- en: 25% of the points in the dataset are below 13.00 (25th percentile value).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有25%的点小于13.00（25百分位数值）。
- en: 50% of the points in the dataset are below 18.50 (50th percentile value).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有50%的点小于18.50（50百分位数值）。
- en: 75% of the points in the dataset are below 25.25 (75th percentile value).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有75%的点小于25.25（75百分位数值）。
- en: A point to note is that the 50th percentile is the median. Percentiles give
    us a good idea of the range of our values.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，50百分位数是中位数。百分位数给我们提供了数据值范围的良好视角。
- en: 'In step 4, we will plot these percentile values as horizontal lines in our
    graph in order to enhance our visualization:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步中，我们将在图表中以水平线的形式绘制这些百分位数值，以增强我们的可视化效果：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We used the `plt.axhline()` function to draw these horizontal lines. This function
    will draw a line at the given y value from the minimum of x to the maximum of
    x. Using the label parameter, we gave it a name and set the color of the line
    through the c parameter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`plt.axhline()`函数来绘制这些水平线。这个函数会在给定的y值处，从x的最小值绘制到x的最大值。通过label参数，我们给它命名，并通过c参数设置线条的颜色。
- en: Tip
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: A good way to understand any function is to pass the function name to `help()`
    in the Python console. In this case, help (plt.axhline) in the Python console
    will give you the details.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 理解任何函数的好方法是将函数名传递给Python控制台中的`help()`。在这种情况下，在Python控制台中输入`help(plt.axhline)`将提供相关详情。
- en: Finally, we will place the legend using `plt.legend()`, and using the `loc`
    parameter, ask pyplot to determine the best location to put the legend so that
    it does not affect the plot readability.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用`plt.legend()`添加图例，并通过`loc`参数，要求pyplot自动选择最佳位置放置图例，以免影响图表的可读性。
- en: 'Our graph is now as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图表现在如下所示：
- en: '![How it works…](img/B04041_03_03.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_03_03.jpg)'
- en: 'In step 5, we will move on to remove the outliers using the mask function in
    NumPy:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5步中，我们将使用NumPy中的掩码函数来移除异常值：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Masking is a convenient way to hide some of the values without removing them
    from our array. We used the `ma.masked_where` function, where we passed a condition
    and an array. The function then masks the values in the array that meet the condition.
    Our first condition was to mask all the points in the `y` array, where the array
    value was `0`. We stored the new masked array as `y_masked`. Then, we applied
    another condition on `y_masked` to remove point 54.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码是隐藏数组中某些值的便捷方法，而不需要将其从数组中删除。我们使用了`ma.masked_where`函数，其中传入了一个条件和一个数组。该函数会根据条件掩盖数组中符合条件的值。我们的第一个条件是掩盖`y`数组中所有值为`0`的点。我们将新的掩盖后的数组存储为`y_masked`。然后，我们对`y_masked`应用另一个条件，移除第54个点。
- en: 'Finally, in step 6, we will repeat the plotting steps. Our final plot looks
    as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第6步中，我们将重复绘图步骤。我们的最终图表如下所示：
- en: '![How it works…](img/B04041_03_04.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_03_04.jpg)'
- en: See also
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Creating Anonymous functions* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创建匿名函数* 例子见[第1章](ch01.xhtml "第1章. Python数据科学"), *Python数据科学应用*'
- en: '*Pre-processing columns* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预处理列* 例子见[第1章](ch01.xhtml "第1章. Python数据科学"), *Python数据科学应用*'
- en: '*Acquiring data with Python* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用Python获取数据* 例子见[第1章](ch01.xhtml "第1章. Python数据科学"), *Python数据科学应用*'
- en: '*Outliers* recipe in [Chapter 4](ch04.xhtml "Chapter 4. Data Analysis – Deep
    Dive"), *Analyzing Data - Deep Dive*'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*离群值* 相关内容请参考 [第4章](ch04.xhtml "第4章 数据分析 – 深度剖析")，*数据分析 - 深度剖析*'
- en: Grouping the data and using dot plots
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对数据进行分组并使用点图
- en: '**EDA** is about zooming in and out of the data from multiple angles in order
    to get a better grasp of the data. Let''s now see the data from a different angle
    using dot plots. A dot plot is a simple plot where the data is grouped and plotted
    in a simple scale. It''s up to us to decide how we want to group the data.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**EDA** 是通过从多个角度放大和缩小数据，以更好地理解数据。现在让我们用点图从不同的角度来看数据。点图是一种简单的图表，其中数据被分组并绘制在简单的尺度上。我们可以决定如何分组数据。'
- en: Note
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Dot plots are best used for small-sized to medium-sized datasets. For large-sized
    data, a histogram is usually used.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 点图最好用于小到中等大小的数据集。对于大型数据，通常使用直方图。
- en: Getting ready
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: For this exercise, we will use the same data as the previous section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本练习，我们将使用与上一节相同的数据。
- en: How to do it…
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Let''s load the necessary libraries. We will follow it up with the loading
    of our data and along the way, we will handle the missing values. Finally, we
    will group the data using a frequency counter:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载必要的库。接着加载我们的数据，并在此过程中处理缺失值。最后，我们将使用频率计数器对数据进行分组：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will proceed to group the data by the year range and plot it:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续按年份范围对数据进行分组并绘制：
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we will prepare the data for a simple dot plot and proceed with plotting
    it:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将准备数据用于简单的点图，并继续绘制：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works…
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In step 1, we will load the data. This is the same as the data loading discussed
    in the previous recipe. Before we start plotting the data, we want to group them
    in order to see the overall data characteristics.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们将加载数据。这与前面的食谱中讨论的数据加载是一样的。在开始绘制数据之前，我们想对它们进行分组，以便查看整体数据特征。
- en: In steps 2 and 3, we will group the data using different criteria.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步和第3步中，我们将使用不同的标准对数据进行分组。
- en: Let's look at step 2.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看第2步。
- en: Here, we will use a function called `Counter()` from the `collections` package.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用来自 `collections` 包的 `Counter()` 函数。
- en: Note
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Given a set of points, `Counter()` returns a dictionary where key is a data
    point and value is the frequency of the data points in the dataset.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组点，`Counter()` 返回一个字典，其中键是数据点，值是数据点在数据集中的频率。
- en: We will pass our dataset to `Counter()` and extract the keys from the actual
    data point and values, the respective frequency from this dictionary into numpy
    arrays `x_` and `y_` for ease of plotting. Thus, we have now grouped our data
    using frequency.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把数据集传递给 `Counter()`，从实际数据点中提取键，并将值（即频率）提取到 `numpy` 数组 `x_` 和 `y_` 中，以便绘图。因此，我们现在已经通过频率对数据进行了分组。
- en: Before we move on to plot this, we will perform another grouping with this data
    in step 3.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续绘制之前，我们将在第3步对这些数据进行再次分组。
- en: 'We know that the x axis is years. Our data is also sorted by the year in an
    ascending order. In this step, we will group our data in a range of years, five
    in this case; that is, let''s say that we will make a group from the first five
    years, our second group is the next five years, and so on:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道x轴是年份。我们的数据也按年份升序排列。在这一步中，我们将按年份范围对数据进行分组，这里每组5年；也就是说，假设我们将前五年作为一组，第二组是接下来的五年，以此类推：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `group` variable defines how many years we want in a single group; in this
    example, we have 5 groups and `keys` and `values` are two empty lists. We will
    proceed to fill them with values from `x` and `y` till `group_count` reaches `group`,
    that is, `5`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`group` 变量定义了我们希望在一个组中包含多少年；在这个例子中，我们有5个组，`keys` 和 `values` 是两个空列表。我们将继续从 `x`
    和 `y` 中填充它们，直到 `group_count` 达到 `group`，也就是 `5`：'
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `x_group` is the name of the dictionary that now stores the group of values.
    We will need to preserve the order in which we will insert our records and so,
    we will use `OrderedDict` in this case.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`x_group` 是现在存储值组的字典名称。我们需要保留插入记录的顺序，因此在这种情况下，我们将使用 `OrderedDict`。'
- en: Note
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '`OrderedDict` preserves the order in which the keys are inserted.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`OrderedDict` 会保留插入键的顺序。'
- en: Now let's proceed to plot these values.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续绘制这些值。
- en: 'We want to plot all our graphs in a single window; hence, we will use the `subplot`
    parameter to the subplot, which defines the number of rows (3, the number in the
    hundredth place), number of columns (1, the number in the tenth place), and finally
    the plot number (1 in the unit place). Our plot output is as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将所有图形绘制在单个窗口中；因此，我们将使用`subplot`参数到子图中，该参数定义了行数（3，百位数中的数字），列数（1，十位数中的数字），最后是绘图编号（个位数中的1）。我们的绘图输出如下：
- en: '![How it works…](img/B04041_03_05.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/B04041_03_05.jpg)'
- en: In the top graph, the data is grouped by frequency. Here, our x axis is the
    count and y axis is the number of Presidential Requests. We can see that 30 or
    more Presidential Requests have occurred only once. As said before, the dot plot
    is good at analyzing the range of the data points under different groupings.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部图表中，数据按频率分组。在这里，我们的 x 轴是计数，y 轴是总统请求的数量。我们可以看到，30次或更多总统请求仅发生了一次。如前所述，点图在分析不同分组下数据点的范围方面表现良好。
- en: The middle graph can be viewed as a very simple histogram. As the title of the
    graph (`in plt.title()`) says, it's the simplest form of a dot plot, where the
    *x* axis is the actual values and y axis is the number of times this x value occurs
    in the dataset. In a histogram, the bin size has to be set carefully; if not,
    it can distort the complete picture about the data. However, this can be avoided
    in this simple dot plot.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 中间图可以看作是一个非常简单的直方图。正如图表标题（`plt.title()`中）所说，这是点图的最简单形式，其中*x*轴是实际值，y轴是数据集中该x值出现的次数。在直方图中，必须仔细设置箱体大小；否则，可能会扭曲关于数据的完整图像。然而，在这个简单的点图中可以避免这种情况。
- en: In the bottom graph, we have grouped the data by years.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在底部图表中，我们按年份对数据进行了分组。
- en: See also
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '*Creating Anonymous functions* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在[第1章](ch01.xhtml "Chapter 1. Python for Data Science")中的*创建匿名函数*章节中，*使用Python进行数据科学*'
- en: '*Pre-processing columns* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在[第1章](ch01.xhtml "Chapter 1. Python for Data Science")中的*数据预处理*章节中，*使用Python进行数据科学*'
- en: '*Acquiring data with Python* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在[第1章](ch01.xhtml "Chapter 1. Python for Data Science")中的*使用Python获取数据*章节中，*使用Python进行数据科学*'
- en: '*Using Dictionary objects* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在[第1章](ch01.xhtml "Chapter 1. Python for Data Science")中的*使用字典对象*章节中，*使用Python进行数据科学*'
- en: Using scatter plots for multivariate data
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用散点图进行多变量数据分析
- en: 'From a single column, we will now move on to multiple columns. In multivariate
    data analysis, we are interested in seeing if there any relationships between
    the columns that we are analyzing. In two column/variable cases, the best place
    to start is a standard scatter plot. There can be four types of relationships,
    as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从单列开始，我们现在将转向多列。在多变量数据分析中，我们有兴趣看到我们分析的列之间是否有任何关系。在两列/变量情况下，最好的起点是标准散点图。关系可以分为四种类型，如下所示：
- en: No relationship
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无关系
- en: Strong
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强
- en: Simple
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单
- en: Multivariate (not simple) relationship
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多变量（不简单）关系
- en: Getting ready
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the Iris dataset. It's a multivariate dataset introduced by Sir
    Ronald Fisher. Refer to [https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)
    for more information.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用鸢尾花数据集。这是由罗纳德·菲舍尔爵士引入的多变量数据集。有关更多信息，请参阅[https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)。
- en: The Iris dataset has 150 instances and four attributes/columns. The 150 instances
    are composed of 50 records from each of the three species of the Iris flower (Setosa,
    virginica, and versicolor). The four attributes are the sepal length in cm, sepal
    width in cm, petal length in cm, and petal width in cm. Thus, the Iris dataset
    also serves as a great classification dataset. A classification method can be
    written in such a way that, given a record, we can classify which species that
    record belongs to after appropriate training.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集有150个实例和四个属性/列。这150个实例由鸢尾花的三个物种（山鸢尾、维吉尼亚鸢尾和变色鸢尾）的每种50条记录组成。四个属性分别是以厘米为单位的萼片长度、萼片宽度、花瓣长度和花瓣宽度。因此，鸢尾花数据集还是一个很好的分类数据集。可以编写分类方法，通过适当的训练，给定一条记录，我们可以分类出该记录属于哪个物种。
- en: How to do it…
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s load the necessary libraries and extract the Iris data:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载必要的库并提取鸢尾花数据：
- en: '[PRE21]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will proceed with demonstrating with a scatter plot:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续演示如何使用散点图：
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How it works…
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'The scikit library provides a convenient function to load the Iris dataset
    called `load_iris()`. We will use this to load the Iris data in the variable data
    in step 1\. The `data` is a dictionary object. Using the data and target keys,
    we will retrieve the records and class labels. We will look at the `x` and `y`
    values:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: scikit 库提供了一个方便的函数 `load_iris()` 来加载鸢尾花数据集。我们将在步骤 1 中使用此函数将鸢尾花数据加载到变量 `data`
    中。`data` 是一个字典对象。通过 `data` 和 `target` 键，我们可以检索记录和类标签。我们将查看 `x` 和 `y` 值：
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As you can see, `x` is a matrix with `150` rows and four columns; `y` is a
    vector of length `150`. The `data` dictionary can also be queried to view the
    column names using the `feature_names` keyword, as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`x` 是一个包含 `150` 行和四列的矩阵；`y` 是一个长度为 `150` 的向量。还可以使用 `feature_names` 关键字查询
    `data` 字典以查看列名，如下所示：
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We will then create a scatter plot of the iris variables in step 2\. As we
    did before, we will use subplot here to accommodate all the plots in a single
    figure. We will get two combinations of our column using `itertools.Combination`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将在步骤 2 中创建一个散点图，展示鸢尾花数据的各个变量。像之前一样，我们将使用 `subplot` 来将所有图形容纳在一个单一的图形中。我们将使用
    `itertools.Combination` 来获取我们列的两个组合：
- en: '[PRE25]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can iterate `col_pairs` to get two combinations of our column and plot a
    scatter plot for each, as you can see in the following line of code:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以迭代 `col_pairs` 来获取两个列的组合，并为每个组合绘制一个散点图，如下所示：
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We will pass a `c` parameter in order to indicate the color of the points. In
    this case, we will pass our y variable (class label) so that the different species
    of iris are plotted in different colors in our scatter plot.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将传递一个 `c` 参数来指定点的颜色。在这种情况下，我们将传递我们的 `y` 变量（类标签），这样鸢尾花的不同物种将在散点图中以不同的颜色绘制。
- en: 'The resulting plot is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图形如下：
- en: '![How it works…](img/B04041_03_06.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_03_06.jpg)'
- en: As you can see, we have plotted two combinations of our columns. We also have
    the class labels represented using three different colors. Let's look at the bottom
    left plot, petal length versus petal width. We see that different range of values
    belong to different class labels. Now, this gives us a great clue for classification;
    the petal width and length variables are good candidates if the problem in hand
    is classification.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们已经绘制了两个列的组合。我们还使用三种不同的颜色来表示类标签。让我们看一下左下角的图形，花瓣长度与花瓣宽度的关系。我们看到，不同的值范围属于不同的类标签。现在，这为分类问题提供了很好的线索；如果问题是分类，那么花瓣宽度和花瓣长度是很好的候选变量。
- en: Note
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For the Iris dataset, the petal width and length can alone classify the records
    in their respective flower family.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于鸢尾花数据集，花瓣宽度和花瓣长度就可以将记录按其所属的花卉种类分类。
- en: These kinds of observations can be quickly made during the feature selection
    process with the help of bivariate scatter plots.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征选择过程中，可以通过双变量散点图快速做出此类观察。
- en: See also
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Using iterables* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python for Data
    Science"), *Using Python for Data Science*'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第 1 章](ch01.xhtml "第 1 章. Python 数据科学")的 *使用迭代器* 这一节中，*使用 Python 数据科学*，
- en: '*Working with itertools* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第 1 章](ch01.xhtml "第 1 章. Python 数据科学")的 *与 itertools 一起工作* 这一节中，*使用 Python
    数据科学*，
- en: Using heat maps
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用热力图
- en: 'Heat maps are another interesting visualization technique. In a heat map, the
    data is represented as a matrix where the range of values taken by attributes
    are represented as color gradients. Look at the following Wikipedia reference
    for a general introduction to heat maps:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 热力图是另一种有趣的可视化技术。在热力图中，数据以矩阵的形式表示，其中属性的取值范围以颜色渐变的形式表示。查看以下维基百科参考资料，了解热力图的一般介绍：
- en: '[http://en.wikipedia.org/wiki/Heat_map](http://en.wikipedia.org/wiki/Heat_map)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://en.wikipedia.org/wiki/Heat_map](http://en.wikipedia.org/wiki/Heat_map)'
- en: Getting ready
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will again resort to the Iris dataset in order to demonstrate how to build
    a heat map. We will also see the various ways that heat maps can be used on this
    data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用鸢尾花数据集来演示如何构建热力图。我们还将看到热力图在这组数据上可以如何使用的不同方式。
- en: In this recipe, we will see how we can represent the whole data as a heat map
    and how the various interpretations of the data can be made from the heat map.
    Let's proceed to build a heat map of the Iris dataset.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将看到如何将整个数据表示为热力图，并如何从热力图中对数据进行各种解读。让我们继续构建鸢尾花数据集的热力图。
- en: How to do it…
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Let''s load the necessary libraries and import the Iris dataset. We will proceed
    with scaling the variables in the data by their mean value:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载必要的库并导入Iris数据集。我们将通过数据的均值来缩放变量：
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s plot our heat map:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制热图：
- en: '[PRE28]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: How it works…
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'In step 1, we will load the Iris dataset. Similar to the other recipes, we
    will take the data dictionary objects and store them as x and y for clarity. In
    step 2, we will scale the variables by their means:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们将加载Iris数据集。与其他方法类似，我们将数据字典对象存储为x和y，以确保清晰。在第2步中，我们将通过均值缩放变量：
- en: '[PRE29]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: With the parameter standard set to false, the scale function will use only the
    mean of the columns in order to normalize the data.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数标准设置为false的情况下，scale函数将只使用列的均值来规范化数据。
- en: The reason for the scaling is to adjust the range of values that each column
    takes to a common scale, typically between 0 and 1\. Having them in the same scale
    is very important for the heat map visualization as the values decide the color
    gradients.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放的原因是将每一列的值的范围调整到一个共同的尺度，通常是在0到1之间。将它们缩放到相同的尺度对于热图可视化非常重要，因为值决定了颜色梯度。
- en: Tip
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Don't forget to scale your variables to bring them to the same range. Not having
    a proper scaling may lead to variables with a bigger range and scale, thus dominating
    others.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 别忘了缩放变量，使它们处于相同的范围内。如果没有适当的缩放，可能会导致某些变量的范围和尺度较大，从而主导其他变量。
- en: 'In step 3, we will perform the actual plotting. Before we plot, we will subset
    the data:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，我们将执行实际的绘图操作。在绘图之前，我们将对数据进行子集选择：
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As you can see, we selected only the first 25 records from the dataset. We
    did so in order to have the labels in the y axis to be readable. We will store
    the labels for the x and y axes in `col_names` and `y_labels`, respectively. Finally,
    we will use the `pcolor` function from pyplot to plot a heat map of the Iris data.
    We will do a little more tinkering with pcolor to make it look nice:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们只选择了数据集中的前25条记录。我们这么做是为了使y轴上的标签可读。我们将把x轴和y轴的标签分别存储在`col_names`和`y_labels`中。最后，我们将使用pyplot中的`pcolor`函数绘制Iris数据的热图，并稍微调整一下pcolor，使其看起来更加美观：
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The *x* and *y* axis ticks are set uniformly:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*轴和*y*轴的刻度被均匀设置：'
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The x axis ticks are displayed at the top of the graph:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: x轴刻度显示在图表的顶部：
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The y axis ticks are displayed to the left:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: y轴刻度显示在图表的左侧：
- en: '[PRE34]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Finally, we will pass on the label values.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将传递标签值。
- en: 'The output plot is shown as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 输出图如下所示：
- en: '![How it works…](img/B04041_03_07.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_03_07.jpg)'
- en: There's more...
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Another interesting way to use a heat map is to view the variables separated
    by their respective classes; for example, in the Iris dataset, we will plot three
    different heat maps for the three classes that are present. The code is as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有趣的使用热图的方法是查看按其各自类别分离的变量；例如，在Iris数据集中，我们将为三个类别绘制三个不同的热图。代码如下：
- en: '[PRE35]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let''s look at the plot:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个图：
- en: '![There''s more...](img/B04041_03_08.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多...](img/B04041_03_08.jpg)'
- en: The first 50 records belong to the `setosa` class, the next 50 to `versicolor`,
    and the last 50 belong to `virginica`. We will make three heat maps for each of
    these classes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 前50条记录属于`setosa`类，接下来的50条属于`versicolor`类，最后50条属于`virginica`类。我们将为每个类绘制三个热图。
- en: The cells are filled with the actual values of the records. You can notice that,
    for `setosa`, the sepal width has a good variation but doesn't show any significance
    in the case of `versicolor` and `virginica`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 单元格中填充的是记录的实际值。你可以注意到，对于`setosa`，萼片宽度变化较大，但对于`versicolor`和`virginica`来说则没有表现出明显的差异。
- en: See also
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Scaling the data* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data Analysis
    – Explore and Wrangle"), *Analyzing Data - Explore & Wrangle*'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据缩放*方法见[第3章](ch03.xhtml "第3章 数据分析 - 探索与整理")，*数据分析 - 探索与整理*'
- en: Performing summary statistics and plots
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行总结统计和绘图
- en: The primary purpose of using summary statistics is to get a good understanding
    of the location and dispersion of the data. By summary statistics, we refer to
    mean, median, and standard deviation. These quantities are quite easy to calculate.
    However, one should be careful when using them. If the underlying data is not
    unimodal, that is, it has multiple peaks, these quantities may not be of much
    use.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用总结统计的主要目的是很好地了解数据的位置和离散程度。总结统计包括均值、中位数和标准差，这些量是比较容易计算的。然而，在使用这些统计量时需要小心。如果底层数据不是单峰的，而是具有多个峰值，那么这些统计量可能并不十分有用。
- en: Note
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If the given data is unimodal, that is, having only one peak, the mean, which
    gives the location, and standard deviation, which gives the variance, are valuable
    metrics.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果给定的数据是单峰的，即只有一个峰值，那么均值（给出位置）和标准差（给出方差）是非常有价值的指标。
- en: Getting ready
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Let's use our Iris dataset to explore some of these summary statistics. In this
    section, we don't have a wholesome program producing a single output; however,
    we will have different steps demonstrating different summary measures.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用鸢尾花数据集来探索这些汇总统计量。在这一部分，我们没有一个完整的程序输出单一结果；然而，我们将通过不同的步骤展示不同的汇总度量。
- en: How to do it…
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Let''s begin by importing the necessary libraries. We will follow it up with
    the loading of the Iris dataset:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先导入必要的库。接下来，我们将加载鸢尾花数据集：
- en: '[PRE36]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let''s now demonstrate how to calculate the mean, trimmed mean, and range values:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们演示如何计算均值、修剪均值和范围值：
- en: '[PRE37]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, we will show the variance, standard deviation, mean absolute deviation,
    and median absolute deviation calculations:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将展示方差、标准差、均值绝对偏差和中位数绝对偏差的计算：
- en: '[PRE38]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: How it works…
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The loading of the Iris dataset is not repeated in this recipe. It's assumed
    that the reader can look at the previous recipe to do the same. Further, we will
    assume that the x variable is loaded with all the instance of the Iris records
    with each record having four columns.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集的加载在此配方中没有重复。假设读者可以参考前面的配方来完成相同的操作。此外，我们假设 x 变量已经加载了所有鸢尾花记录的实例，每条记录有四列。
- en: 'Step 1 prints the mean value of each of the column in the Iris dataset. We
    used NumPy''s `mean` function for the same. The output of the print statement
    is as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 1 打印了鸢尾花数据集中每一列的均值。我们使用了 NumPy 的 `mean` 函数来完成此操作。打印语句的输出如下：
- en: '![How it works…](img/B04041_03_13.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的……](img/B04041_03_13.jpg)'
- en: 'As you can see, we have the mean value for each column. The code to calculate
    the mean is as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们得到了每列的均值。计算均值的代码如下：
- en: '[PRE39]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We passed all the rows and columns in the loop. Thus, we get the mean value
    by columns.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在循环中遍历了所有的行和列。因此，我们通过列计算均值。
- en: Another interesting measure is what is called trimmed mean. It has its own advantages.
    The 10% trimmed mean of a given sample is computed by excluding the 10% largest
    and 10% smallest values from the sample and taking the arithmetic mean of the
    remaining 80% of the sample.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的度量是所谓的修剪均值。它有其自身的优点。给定样本的 10% 修剪均值是通过排除样本中最大和最小的 10% 值，计算剩余 80% 样本的算术均值来得出的。
- en: Note
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Compared to the regular mean, a trimmed mean is less sensitive to outliers.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 与常规均值相比，修剪均值对异常值的敏感度较低。
- en: 'SciPy provides us with a trim mean function. We will demonstrate the trimmed
    mean calculation in step 2\. The output is as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: SciPy 提供了一个修剪均值函数。我们将在步骤 2 中演示修剪均值的计算。输出结果如下：
- en: '![How it works…](img/B04041_03_14.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的……](img/B04041_03_14.jpg)'
- en: With the Iris dataset, we don't see a lot of difference, but in real-world datasets,
    the trimmed mean is very handy as it gives a better picture of the location of
    the data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 使用鸢尾花数据集时，我们没有看到太大的差异，但在实际数据集中，修剪均值非常有用，因为它能更好地反映数据的位置。
- en: 'Till now, what we saw was the location of the data and that the mean and trimmed
    mean gives a good inference on the data location. Another important aspect to
    look at is the dispersion of the data. The simplest way to look at the data dispersion
    is range, which is defined as follows, given a set of values, x, the range is
    the maximum value of x – minimum value of x. In Step 3, we will calculate and
    print the same:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所看到的是数据的位置，均值和修剪均值能很好地推断出数据的位置。另一个重要的方面是数据的离散程度。查看数据离散度的最简单方法是范围，它的定义如下：给定一组值
    x，范围是 x 的最大值减去 x 的最小值。在步骤 3 中，我们将计算并打印相同的结果：
- en: '![How it works…](img/B04041_03_15.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的……](img/B04041_03_15.jpg)'
- en: Note
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If the data falls in a very narrow range, say, most of the values cluster around
    a single value and we have a few extreme values, then the range may be misleading.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据的范围非常窄，例如，大多数值集中在单一值附近，且我们有一些极端值，那么范围可能会产生误导。
- en: When the data falls in a very narrow range and clusters around a single value,
    variance is used as a typical measure of the dispersion/spread of the data. Variance
    is the sum of the squared difference between the individual values and the mean
    value divided by the number of instances. In step 4, we will see the variance
    calculation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据落在一个非常狭窄的范围内，并且围绕单个值聚集时，方差被用作数据分布/扩展的典型度量。方差是单个值与均值之间差的平方的和，然后除以实例数。在第4步中，我们将看到方差计算。
- en: 'In the preceding code, in addition to variance, we can see std-dev, that is,
    standard deviation. As variance is the square of the difference, it''s not in
    the same measurement scale as the original data. We will use standard deviation,
    which is the square root of the variance, in order to get the data back into its
    original scale. Let''s look at the output of the print statement, where we listed
    both the variance and standard deviation:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，除了方差，我们还可以看到标准差，即标准差。由于方差是差的平方，因此它与原始数据的度量尺度不同。我们将使用标准差，它是方差的平方根，以便将数据恢复到原始尺度。让我们来看一下打印语句的输出，其中列出了方差和标准差：
- en: '![How it works…](img/B04041_03_16.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_03_16.jpg)'
- en: 'As we mentioned earlier, the mean is very sensitive to outliers; variance also
    uses the mean, and hence, it''s prone to the same issues as the mean. We can use
    other measures for variance to avoid this trap. One such measure is absolute average
    deviation; instead of taking the square of the difference between the individual
    values and mean and dividing it by the number of instances, we will take the absolute
    of the difference between the mean and individual values and divide it by the
    number of instances. In step 5, we will define a function for this:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，均值对异常值非常敏感；方差也使用均值，因此也容易受到与均值相同的问题影响。我们可以使用其他方差度量来避免这一陷阱。一个这样的度量是绝对平均偏差；它不是取单个值与均值之间差的平方并将其除以实例数，而是取均值与单个值之间差的绝对值，并将其除以实例数。在第5步中，我们将为此定义一个函数：
- en: '[PRE40]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'As you can see, the function returns the absolute difference between the mean
    and individual values. The output of this step is as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，函数返回均值与单个值之间的绝对差异。本步骤的输出如下：
- en: '![How it works…](img/B04041_03_17.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_03_17.jpg)'
- en: With the data having many outliers, there is another set of metrics that come
    in handy. They are the median and percentiles. We already saw percentiles in the
    previous section while plotting the univariate data. Traditionally, median is
    defined as a value from the dataset such that half of all the points in the dataset
    are smaller and the other half is larger than the median value.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据包含许多异常值时，另有一组度量非常有用。它们是中位数和百分位数。我们在前一部分绘制单变量数据时已经看到过百分位数。传统上，中位数定义为数据集中一个值，使得数据集中一半的点小于该值，另一半大于该值。
- en: Note
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Percentiles are a generalization of the concept of median. The 50th percentile
    is the traditional median value.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 百分位数是中位数概念的推广。第50百分位数就是传统的中位数值。
- en: 'We saw the 25th and 75th percentiles in the previous section. The 25th percentile
    is a value such that 25% of all the points in the dataset are smaller than this
    value:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一部分看到了第25百分位数和第75百分位数。第25百分位数是这样一个值，数据集中有25%的点小于这个值：
- en: '[PRE41]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The median is the measure of the location of the data distribution. Using percentiles,
    we can get a metric for the dispersion of the data, the interquartile range. The
    interquartile range is the distance between the 75th percentile and 25th percentile.
    Similar to the mean absolute deviation as explained previously, we also have the
    median absolute deviation.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 中位数是数据分布位置的度量。通过使用百分位数，我们可以得到数据分布的一个度量，即四分位差。四分位差是第75百分位数与第25百分位数之间的距离。类似于之前解释的均值绝对偏差，我们也有中位数绝对偏差。
- en: 'In step 6, we will calculate and display both the interquartile range and median
    absolute deviation. We will define the following function in order to calculate
    the median absolute deviation:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6步中，我们将计算并显示四分位差和中位数绝对偏差。我们将定义以下函数以计算中位数绝对偏差：
- en: '[PRE42]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output is as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![How it works…](img/B04041_03_18.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_03_18.jpg)'
- en: See also
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Grouping Data and Using Plots* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Analyzing Data - Explore & Wrangle*'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第3章](ch03.xhtml "第3章 数据分析 – 探索与整理")中的 *分组数据与使用图表* 章节，*数据分析 - 探索与整理*'
- en: Using a box-and-whisker plot
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用箱形图和须状图
- en: 'A box-and-whisker plot is a good companion with the summary statistics to view
    the statistical summary of the data in hand. Box-and-whiskers can effectively
    represent quantiles in data and also outliers, if any, emphasizing the overall
    structure of the data. A box plot consists of the following features:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 箱形图和须状图是与汇总统计信息非常配合的工具，可以查看数据的统计摘要。箱形图可以有效地表示数据中的四分位数，也可以显示离群值（如果有的话），突出显示数据的整体结构。箱形图由以下几个特征组成：
- en: A horizontal line indicating the median that indicates the location of the data
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一条水平线表示中位数，标示数据的位置
- en: A box spanning the interquartile range, measuring the dispersion
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨越四分位距的箱体，表示测量分散度
- en: A set of whiskers that extends from the central box horizontally and vertically,
    which indicates the tail of the distribution
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组从中央箱体水平和垂直延伸的须状线，它们表示数据分布的尾部。
- en: Getting ready
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: Let's use the box plot to look at the Iris dataset.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用箱形图来观察 Iris 数据集。
- en: How to do it…
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let''s load the necessary libraries to begin with. We will follow this with
    loading the Iris dataset:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先加载必要的库，然后加载 Iris 数据集：
- en: '[PRE43]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s demonstrate how to create a box-and-whisker plot:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们演示如何创建一个箱形图：
- en: '[PRE44]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: How it works…
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: 'The code is very straightforward. We will load the Iris data in x and pass
    the x values to the box plot function from pyplot. As you know, our x has four
    columns. The box plot is as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码非常简单。我们将加载 Iris 数据集到 x，并将 x 的值传递给 pyplot 中的箱形图函数。如你所知，我们的 x 有四列。箱形图如下所示：
- en: '![How it works…](img/B04041_03_09.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理…](img/B04041_03_09.jpg)'
- en: The box plot has captured both the location and variation of all the four columns
    in a single plot.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 箱形图在一个图中捕捉了四个列的位置和变化。
- en: The horizontal red line indicates the median, which is the location of the data.
    You can see that the sepal length has a higher median than the rest of the columns.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 水平的红线表示中位数，它标示了数据的位置。你可以看到，花萼长度的中位数高于其余列。
- en: The box spanning the interquartile range measuring the dispersion can be seen
    for all the four variables.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到跨越四个变量的箱体，表示测量分散度的四分位距。
- en: You can see a set of whiskers that extends from the central box horizontally
    and vertically, which indicates the tail of the distribution. Whiskers help you
    to see the extreme values in the datasets.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到一组从中央箱体水平和垂直延伸的须状线，它们表示数据分布的尾部。须状线帮助你观察数据集中的极端值。
- en: There's more…
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'It will also be interesting to see how the data is distributed across the various
    class labels. Similar to how we did in the scatter plots, let''s do the same with
    the box-and-whisker plot. The following code and chart explains how to plot a
    box plot across various class labels:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 看到数据如何在不同类标签中分布也是很有趣的。类似于我们在散点图中的做法，接下来我们将通过箱形图实现相同的操作。以下代码和图表展示了如何在不同类标签之间绘制箱形图：
- en: '[PRE45]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'As you can see in the following chart, we now have a box-and-whisker plot for
    each class label:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们现在为每个类标签绘制了一个箱形图：
- en: '![There''s more…](img/B04041_03_10.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![更多内容…](img/B04041_03_10.jpg)'
- en: Imputing the data
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据填充
- en: In many real-world scenarios, we have the problem of incomplete or missing data.
    We need a strategy to handle the incomplete data. This strategy can be formulated
    either using the data alone or in conjunction with the class labels, if the labels
    are present.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多现实世界的场景中，我们会遇到不完整或缺失数据的问题。我们需要一个策略来处理这些不完整的数据。这个策略可以单独使用数据，或者在有类标签的情况下结合类标签来制定。
- en: Getting ready
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: Let's first look at the ways of imputing the data without using the class labels.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看看不使用类标签进行数据填充的方法。
- en: A simple technique is to ignore the missing value and hence, avoid the overhead
    of data imputation. However, this can be applied when the data is available in
    abundance, which is not always the case. If the dataset has very few missing values
    and the percentage of the missing values is minimal, we can ignore them. Typically,
    it's not about ignoring a single value of a variable, it's about ignoring a tuple
    that contains this variable. We have to be more careful when ignoring a whole
    tuple, as the other attributes in this tuple may be very critical for our task.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的技术是忽略缺失值，因此避免数据填充的开销。然而，这只适用于数据充足的情况，而这并非总是如此。如果数据集中有非常少的缺失值，并且缺失值的百分比很小，我们可以忽略它们。通常情况下，问题不在于忽略单个变量的单个值，而在于忽略包含此变量的元组。在忽略整个元组时，我们必须更加小心，因为该元组中的其他属性可能对我们的任务非常关键。
- en: A better way to handle the missing data is to estimate it. Now, the estimation
    process can be carried out considering only the data or in conjunction with the
    class label. In the case of a continuous variable, the mean, median, or the most
    frequent value can be used to replace the missing value. Scikit-learn provides
    you with an `Imputer()` function in module preprocessing to handle the missing
    data. Let's see an example where we will perform data imputation. To better understand
    the imputation technique, we will artificially introduce some missing values in
    the Iris dataset.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失数据的更好方法是进行估算。现在，估算过程可以仅考虑数据或与类标签一起进行。对于连续变量，可以使用均值、中位数或最频繁的值来替换缺失值。Scikit-learn在preprocessing模块中提供了一个`Imputer()`函数来处理缺失数据。让我们看一个例子，我们将在鸢尾花数据集中执行数据填充以更好地理解填充技术。
- en: How to do it…
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Let''s load the necessary libraries to begin with. We will load the Iris dataset
    as usual and introduce some arbitrary missing values:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先加载必要的库来开始。我们将像往常一样加载鸢尾花数据集，并引入一些任意的缺失值：
- en: '[PRE46]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s see some data imputation in action:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据填充的实际效果：
- en: '[PRE47]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: How it works…
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: Step 1 is about loading the Iris data in memory. In step 2, we will introduce
    some missing values; in this case, we will set all the columns in the third row
    to `0`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将鸢尾花数据加载到内存中。在第二步中，我们将引入一些缺失值；在本例中，我们将所有第三行的列都设置为`0`。
- en: 'In step 3, we will use the Imputer object to handle the missing data:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，我们将使用Imputer对象处理缺失数据：
- en: '[PRE48]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As you can see, we will need two parameters, `missing_valu`es to specify the
    missing values, and strategy, which is a way to impute these missing values. The
    Imputer object provides the following three strategies:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们需要两个参数，`missing_values`来指定缺失值，以及策略，这是一种处理这些缺失值的方法。Imputer对象提供以下三种策略：
- en: mean
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值
- en: median
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中位数
- en: most_frequent
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: most_frequent
- en: Using the mean, any cell with the `0` value will be replaced by the mean value
    of the column that the cell belongs to. In the case of the median, the median
    value is used to replace `0`, and in `most_frequent`, as the name suggests, the
    most frequent value is used to replace `0`. Based on the context of our application,
    one of these strategies can be applied.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 使用均值，任何值为`0`的单元格都将被其所属列的均值替换。对于中位数，将使用中位数值来替换`0`，而对于`most_frequent`，正如其名称所示，将使用最频繁出现的值来替换`0`。基于我们应用程序的上下文，可以应用其中一种策略。
- en: 'The intial value of x[2,:] is as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: x[2,:]的初始值如下：
- en: '[PRE49]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We will make it `0` in all the columns and use an imputer with the mean strategy.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有列中的值设置为`0`，并使用均值策略的填充器。
- en: 'Before we look at the imputer output, let''s calculate the mean values for
    all the columns:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看填充器输出之前，让我们计算所有列的均值：
- en: '[PRE50]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE51]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, let''s look at the imputed output for row number 2:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看第2行的填充输出：
- en: '[PRE52]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The following is the output:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE53]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As you can see, the imputer has filled the missing values with the mean value
    of the respective columns.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，填充器已经用各自列的均值填充了缺失的值。
- en: There's more…
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'As we discussed, we can also leverage the class labels and impute the missing
    values either using the mean or median:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论的，我们还可以利用类标签并使用均值或中位数来填补缺失值：
- en: '[PRE54]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Instead of using the mean or median of the whole dataset, what we did was to
    subset the data by the class variable of the missing tuple:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 与其使用整个数据集的均值或中位数，我们将数据子集化到缺失元组的类变量中：
- en: '[PRE55]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We introduced the missing value in the third record. We will take the class
    label associated with this record to the `missing_y` variable:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第三条记录中引入了缺失值。我们将把与此记录相关的类别标签赋值给 `missing_y` 变量：
- en: '[PRE56]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now, we will take all the tuples that have the same class label:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将提取所有具有相同类别标签的元组：
- en: '[PRE57]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We can now apply the mean or median strategy by replacing the missing tuple
    with the mean or median of all the tuples that belong to this class label.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过用所有属于该类别标签的元组的均值或中位数来替换缺失的元组，应用均值或中位数策略。
- en: We took the mean/median value of this subset for the data imputation process.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们取了该子集的均值/中位数作为数据插补过程。
- en: See also
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '*Performing Summary Statistics* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Analyzing Data - Explore & Wrangle*'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.xhtml "第3章 数据分析 - 探索与整理")中的*执行汇总统计*食谱，*分析数据 - 探索与整理*
- en: Performing random sampling
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行随机采样
- en: In this we will learn to how to perform a random sampling of data.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将学习如何执行数据的随机采样。
- en: Getting ready
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Typically, in scenarios where it's very expensive to access the whole dataset,
    sampling can be effectively used to extract a portion of the dataset for analysis.
    Sampling can be effectively used in EDA as well. A sample should be a good representative
    of the underlying dataset. It should have approximately the same characteristics
    as the underlying dataset. For example, with respect to the mean, the sample mean
    should be as close to the original data's mean value as possible. There are several
    sampling techniques; we will cover one of them here.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在访问整个数据集非常昂贵的情况下，采样可以有效地用于提取数据集的一部分进行分析。采样在探索性数据分析（EDA）中也可以有效使用。样本应该是底层数据集的良好代表。它应具有与底层数据集相同的特征。例如，就均值而言，样本均值应尽可能接近原始数据的均值。
- en: In simple random sampling, there is an equal chance of selecting any tuple.
    For our example, we want to sample ten records randomly from the Iris dataset.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单随机采样中，每个元组被选中的机会是均等的。对于我们的示例，我们想从鸢尾花数据集中随机选择十条记录。
- en: How to do it…
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何执行...
- en: 'We will begin with loading the necessary libraries and importing the Iris dataset:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从加载必要的库并导入鸢尾花数据集开始：
- en: '[PRE58]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Let''s demonstrate how sampling is performed:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们演示如何执行采样：
- en: '[PRE59]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: How it works…
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In step 1, we will load the Iris dataset. In step 2, we will do a random selection
    using the `choice` function from `numpy.random`.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 1 中，我们将加载鸢尾花数据集。在步骤 2 中，我们将使用 `numpy.random` 中的 `choice` 函数进行随机选择。
- en: The two parameters that we will pass to the choice functions are a range variable
    for the total number of rows in the original dataset and the sample size that
    we require. From zero to the total number of rows in the original dataset, the
    choice function randomly picks n integers, where n is the size of the sample,
    which is dictated by `no_records` in our case.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将传递给 `choice` 函数的两个参数是原始数据集中所有行的范围变量以及我们需要的样本大小。从零到原始数据集中的总行数，`choice` 函数会随机选择
    n 个整数，其中 n 是样本的大小，由我们的 `no_records` 决定。
- en: Another important aspect is that one of the parameters to the choice function
    is `replace` and it's set to True by default; it specifies whether we need to
    sample with replacement or without replacement. Sampling without replacement removes
    the sampled item from the original list so it will not be a candidate for future
    sampling. Sampling with replacement does the opposite; every element has an equal
    chance to be sampled in future sampling even though it's been sampled before.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要方面是 `choice` 函数的一个参数是 `replace`，默认设置为 True；它指定是否需要进行有放回或无放回采样。无放回采样会从原始列表中移除已采样的项，因此它不会成为未来采样的候选项。有放回采样则相反；每个元素在未来的采样中都有相等的机会被再次采样，即使它之前已经被采样过。
- en: There's more…
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Stratified sampling
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分层采样
- en: If the underlying dataset consists of different groups, a simple random sampling
    may fail to capture adequate samples in order to be able to represent the data.
    For example, in a two-class classification problem, 10% of the data belongs to
    the positive class and 90% belongs to the negative class. This kind of problem
    is called class imbalance problem in machine learning. When we do sampling on
    such imbalanced datasets, the sample should also reflect the preceding percentages.
    This kind of sampling is called stratified sampling. We will look more into stratified
    sampling in future chapters on machine learning.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如果底层数据集由不同的组组成，简单的随机抽样可能无法捕获足够的样本以代表数据。例如，在一个二分类问题中，10%的数据属于正类，90%属于负类。这种问题在机器学习中被称为类别不平衡问题。当我们在这种不平衡数据集上进行抽样时，样本也应该反映前述的百分比。这种抽样方法叫做分层抽样。我们将在未来的机器学习章节中详细讨论分层抽样。
- en: Progressive sampling
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 渐进式抽样
- en: How do we determine the correct sample size that we need for a given problem?
    We discussed several sampling techniques before but we don't have a strategy to
    select the correct sample size. There is no simple answer for this. One way to
    do this is to use progressive sampling. Select a sample size and get the samples
    through any of the sampling techniques, apply the desired operation on the data,
    and record the results. Now, increase the sample size and repeat the steps. This
    iterative process is called progressive sampling.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何确定给定问题所需的正确样本大小？我们之前讨论了几种抽样技术，但我们没有选择正确样本大小的策略。这个问题没有简单的答案。一种方法是使用渐进式抽样。选择一个样本大小，通过任何抽样技术获取样本，对数据执行所需的操作，并记录结果。然后，增加样本大小并重复这些步骤。这种迭代过程叫做渐进式抽样。
- en: Scaling the data
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据缩放
- en: In this we will learn to how to scale the data.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将学习如何进行数据缩放。
- en: Getting ready
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Scaling is an important type of data transformation. Typically, by doing scaling
    on a dataset, we can control the range of values that the data type can assume.
    In a dataset with multiple columns, the columns with a bigger range and scale
    tend to dominate other columns. We will perform scaling of the dataset in order
    to avoid these interferences.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放是一种重要的数据转换类型。通常，通过对数据集进行缩放，我们可以控制数据类型可以采用的值的范围。在一个包含多列的数据集中，范围和尺度更大的列往往会主导其他列。我们将对数据集进行缩放，以避免这些干扰。
- en: Let's say that we are comparing two software products based on the number of
    features and the number of lines of code. The difference in the number of lines
    of code will be very high compared to the difference in the number of features.
    In this case, our comparison will be dominated by the number of lines of code.
    If we use any similarity measure, the similarity or difference will be dominated
    by the number of lines of code. To avoid such a situation, we will adopt scaling.
    The simplest scaling is min-max scaling. Let's look at min-max scaling on a randomly
    generated dataset.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在根据功能数量和代码行数来比较两个软件产品。与功能数量的差异相比，代码行数的差异会非常大。在这种情况下，我们的比较将被代码行数所主导。如果我们使用任何相似度度量，结果的相似度或差异将主要受代码行数的影响。为了避免这种情况，我们将采用缩放。最简单的缩放方法是最小-最大缩放。让我们来看看在一个随机生成的数据集上如何进行最小-最大缩放。
- en: How to do it…
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Let''s generate some random data in order to test our scaling functionality:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一些随机数据来测试我们的缩放功能：
- en: '[PRE60]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now, we will demonstrate scaling:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将演示缩放：
- en: '[PRE61]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: How it works…
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'In step 1, we will generate a list of random numbers between 10 and 25\. In
    step 2, we will define a function to perform min-max scaling on the given input.
    Min-max scaling is defined as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1中，我们将生成一个介于10到25之间的随机数字列表。在步骤2中，我们将定义一个函数来对给定的输入执行最小-最大缩放。最小-最大缩放的定义如下：
- en: '[PRE62]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In step 2 we define a function to do the above task.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，我们定义一个函数来执行上述任务。
- en: This transforms the range of the given value. After transformation, the values
    will fall in the [ 0,1 ] range.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 这会将给定值的范围进行转换。转换后，值将落在[0, 1]范围内。
- en: 'In step 3, we will first print the original input list. The output is as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤3中，我们将首先打印原始输入列表。输出结果如下：
- en: '[PRE63]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We will pass this list to our `min_max` function in order to get the scaled
    output, which is as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个列表传递给我们的`min_max`函数，以获得缩放后的输出，结果如下：
- en: '[PRE64]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: You can see the scaling in action; `10`, which is the smallest number, has been
    assigned a value of `0.0` and `23`, the highest number, is assigned a value of
    `1.0`. Thus, we scaled the data in the [0,1] range.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到缩放的效果；最小的数字 `10` 被赋值为 `0.0`，而最大的数字 `23` 被赋值为 `1.0`。因此，我们将数据缩放到 [0,1] 范围内。
- en: There's more…
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Scikit-learn provides a MinMaxScaler function for the same:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 提供了一个 MinMaxScaler 函数来实现这个功能：
- en: '[PRE65]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The output is as follows:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE66]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We saw examples where we scaled the data to a range (0,1); this can be extended
    to any range. Let''s say that our new range is `nr_min,nr_max`, then the min-max
    formula is modified as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到过将数据缩放到范围（0,1）的例子；这可以扩展到任何范围。假设我们的新范围是 `nr_min, nr_max`，那么最小-最大公式会修改如下：
- en: '[PRE67]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The following will be the Python code:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Python 代码：
- en: '[PRE68]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'where, range_values is a tuple of two elements, where the 0th element is the
    new range''s lower end and the first element is the higher end. Let''s invoke
    this function on our input and see how the output is, as follows:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，range_values 是一个包含两个元素的元组，0 位置是新范围的下限，1 位置是上限。让我们在输入数据上调用这个函数，看看输出结果如下：
- en: '[PRE69]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The lowest value, `10`, is now scaled to `100` and the highest value, `23`,
    is scaled to `200`.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 最低值 `10` 现在被缩放到 `100`，最高值 `23` 被缩放到 `200`。
- en: Standardizing the data
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据标准化
- en: Standardization is the process of converting the input so that it has a mean
    of `0` and standard deviation of 1.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化是将输入数据转换为均值为 `0`，标准差为 `1` 的过程。
- en: Getting ready
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'If you are given a vector X, the mean of `0` and standard deviation of 1 for
    X can be achieved by the following equation:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你给定一个向量 X，X 的均值为 `0` 且标准差为 `1` 可以通过以下公式实现：
- en: Note
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Standardized X = x– mean(value) / standard deviation (X)
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化 X = x – 均值(值) / 标准差(X)
- en: Let's see how this can be achieved in Python.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在 Python 中实现这个过程。
- en: How to do it…
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Let''s import the necessary libraries to begin with. We will follow this with
    the generation of the input data:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库。接着，我们将生成输入数据：
- en: '[PRE70]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We are now ready to demonstrate standardization:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备演示标准化：
- en: '[PRE71]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: How it works…
  id: totrans-405
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'We will generate some random data using np.random:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 np.random 生成一些随机数据：
- en: '[PRE72]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We will perform standardization using the `scale` function from scikit-learn:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 scikit-learn 的 `scale` 函数进行标准化：
- en: '[PRE73]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The `x_centered` is scaled using only the mean; you can see the `with_mean`
    parameter set to `True` and `with_std` set to `False`.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '`x_centered` 只使用均值进行缩放；你可以看到 `with_mean` 参数设置为 `True`，`with_std` 设置为 `False`。'
- en: The `x_standard` is standardized using both mean and standard deviation.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '`x_standard` 使用均值和标准差进行了标准化。'
- en: Now let us look at the output.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下输出。
- en: 'The original data is as follows:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据如下：
- en: '[PRE74]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: There's more…
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Note
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Standardization can be generalized to any level and spread, as follows:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化可以推广到任何级别和范围，具体如下：
- en: Standardized value = value – level / spread
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化值 = 值 – 水平 / 范围
- en: 'Let''s break the preceding equation in two parts: just the numerator part,
    which is called centering, and the whole equation, which is called standardization.
    Using the mean values, centering plays a critical role in regression. Consider
    a dataset that has two attributes, weight and height. We will center the data
    such that the predictor, weight, has a mean of `0`. This makes the interpretation
    of intercept easier. The intercept will be interpreted as what is the expected
    height when the predictor values are set to their mean.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将前面的公式分成两部分：仅分子部分，称为居中，和整个公式，称为标准化。使用均值，居中在回归中起着关键作用。考虑一个包含两个属性的数据集，体重和身高。我们将对数据进行居中，使得预测变量体重的均值为
    `0`。这使得解释截距变得更加容易。截距将被解释为在预测变量值设为其均值时，预期的身高。
- en: Performing tokenization
  id: totrans-420
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行分词
- en: 'When you are given any text, the first job is to tokenize the text into a format
    that is based on the given problem requirements. Tokenization is a very broad
    term; we can tokenize the text at the following various levels of granularity:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 当你得到任何文本时，第一步是将文本分词成一个基于给定问题需求的格式。分词是一个非常广泛的术语；我们可以在以下不同的粒度级别进行分词：
- en: The paragraph level
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 段落级
- en: The sentence level
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子级
- en: The word level
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词级
- en: In this section, we will see sentence level and word level tokenization. The
    methods are similar and can be easily applied to a paragraph level or any other
    level of granularity as required by the problem at hand.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到句子级别和词语级别的分词。方法相似，可以轻松应用于段落级别或根据问题需要的其他粒度级别。
- en: Getting ready
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: We will see how to perform sentence level and word level tokenization in a single
    recipe.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到如何在一个示例中执行句子级别和词语级别的分词。
- en: How to do it…
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何执行……
- en: 'Let''s start with the demonstration of sentence tokenization:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始演示句子分词：
- en: '[PRE75]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'A quick peek at how NLTK performs its sentence tokenization in the following
    way:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 下面简要了解一下NLTK如何执行句子分词：
- en: '[PRE76]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: How it works…
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In step 1, we will initialize a variable sentence with a paragraph. This is
    the same example that we used in the dictionary recipe. In step 2, we will use
    nltk's `sent_tokenize` function to extract sentences from the given text.You can
    look into the source of `sent_tokenize` in nltk in the documentation found at
    [http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize).
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步，我们将初始化一个名为sentence的变量，并为其赋值一个段落。这与我们在字典示例中使用的例子相同。在第2步，我们将使用nltk的`sent_tokenize`函数从给定的文本中提取句子。你可以查看[nltk的`sent_tokenize`函数](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize)的源代码。
- en: As you can see, `sent_tokenize` loads a prebuilt tokenizer model, and using
    this model, it tokenizes the given text and returns the output. The tokenizer
    model is an instance of PunktSentenceTokenizer from the `nltk.tokenize.punkt`
    module. There are several pretrained instances of this tokenizer available in
    different languages. In our case, you can see that the language parameter is set
    to English.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`sent_tokenize`加载了一个预构建的分词模型，使用这个模型，它将给定文本进行分词并返回输出。这个分词模型是`nltk.tokenize.punkt`模块中的PunktSentenceTokenizer实例。这个分词器在不同语言中有多个预训练的实例。在我们的案例中，你可以看到语言参数设置为英文。
- en: 'Let''s look at the output of this step:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下这一步的输出：
- en: '[PRE77]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'As you can see, the sentence tokenizer has split our input text into three
    sentences. Let''s proceed to step 3, where we will tokenize these sentences into
    words. Here, we will use the `word_tokenize` function in order to extract the
    words from each of the sentences and store them in a dictionary, where the key
    is the sentence number and the value is the list of words for that sentence. Let''s
    look at the output of the print statement:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，句子分词器已将输入文本拆分为三句。让我们继续进行第3步，在该步骤中我们将把这些句子分词成单词。这里，我们将使用`word_tokenize`函数从每个句子中提取单词，并将它们存储在一个字典中，其中键是句子编号，值是该句子的单词列表。让我们查看打印语句的输出：
- en: '[PRE78]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: The `word_tokenize` uses a regular expression to split the sentences into words.
    It will be useful to look at the source of `word_tokenize` found at [http://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize](http://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '`word_tokenize`使用正则表达式将句子分割成单词。查看`word_tokenize`的源代码会很有帮助，源代码可以在[http://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize](http://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize)找到。'
- en: There's more…
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'For sentence tokenization, we saw a way of doing it in NLTK. There are other
    methods available. The `nltk.tokenize.simple` module has a `line_tokenize` method.
    Let''s take the same input sentence as before and run it using `line_tokenize`:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 对于句子分词，我们已经在NLTK中看到了实现方法。还有其他可用的方法。`nltk.tokenize.simple`模块有一个`line_tokenize`方法。让我们使用之前相同的输入句子，使用`line_tokenize`进行处理：
- en: '[PRE79]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The output is as follows:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE80]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: You can see that we have only the sentence retrieved from the input.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们只提取了输入中的句子。
- en: 'Let''s now modify our input in order to include new line characters:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们修改输入，包含换行符：
- en: '[PRE81]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Note that we have a new line character added. We will again apply `line_tokenize`
    to get the following output:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们添加了一个换行符。我们将再次应用`line_tokenize`来获得以下输出：
- en: '[PRE82]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: You can see that it has tokenized our sentences at the new line and now we have
    three sentences.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到它已经根据换行符对我们的句子进行了分词，现在我们有了三句。
- en: See *Chapter 3* of the *NLTK* book; it has more references for sentence and
    word tokenization. It can be found at [http://www.nltk.org/book/ch03.html](http://www.nltk.org/book/ch03.html).
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅*第3章*的*NLTK*书籍，它有更多关于句子和词语分词的参考。可以在[http://www.nltk.org/book/ch03.html](http://www.nltk.org/book/ch03.html)找到。
- en: See also
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Using Dictionary object* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.xhtml "第1章 Python数据科学")中使用*字典对象*，*使用Python进行数据科学*
- en: '*Writing list* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python for Data
    Science"), *Using Python for Data Science*'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.xhtml "第1章 Python数据科学")中使用*写入列表*，*使用Python进行数据科学*
- en: Removing stop words
  id: totrans-456
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除停用词
- en: In text processing, we are interested in words or phrases that will help us
    differentiate the given text from the other text in the corpus. Let's call these
    words or phrases as key phrases. Every text mining application needs a way to
    find out the key phrases. An information retrieval application needs key phrases
    for the easy retrieval and ranking of search results. A text classification system
    needs key phrases as its features that are to be fed to a classifier.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本处理过程中，我们关注的是能够帮助我们将给定文本与语料库中其他文本区分开来的词语或短语。我们将这些词语或短语称为关键词短语。每个文本挖掘应用程序都需要一种方法来找出关键词短语。信息检索应用程序需要关键词短语来轻松检索和排序搜索结果。文本分类系统则需要关键词短语作为特征，以供分类器使用。
- en: This is where stop words come into the picture.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是停用词的作用所在。
- en: '*"Sometimes, some extremely common words which would appear to be of little
    value in helping select documents matching a user need are excluded from the vocabulary
    entirely. These words are called stop words."*'
  id: totrans-459
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“有时，一些极为常见的词语，它们在帮助选择与用户需求匹配的文档方面似乎没有什么价值，会被完全从词汇表中排除。这些词语被称为停用词。”*'
- en: '*Introduction to Information Retrieval By Christopher D. Manning, Prabhakar
    Raghavan, and Hinrich Schütze*'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '*由Christopher D. Manning, Prabhakar Raghavan和Hinrich Schütze编写的《信息检索导论》*'
- en: 'The Python NLTK library provides us with a default stop word corpus that we
    can leverage, as follows:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: Python NLTK库为我们提供了一个默认的停用词语料库，我们可以加以利用，如下所示：
- en: '[PRE83]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: You can see that we have printed the list of stop words in English.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们已经打印出了英语的停用词列表。
- en: How to do it…
  id: totrans-464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'Let''s load the necessary library and introduce our input text:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载必要的库并引入输入文本：
- en: '[PRE84]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Let''s now demonstrate the stop words removal process:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们演示停用词移除的过程：
- en: '[PRE85]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: How it works…
  id: totrans-469
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In step 1, we will import the necessary libraries from nltk. We will need the
    list of English stop words, so we will import the stop word corpus. We will need
    to tokenize our input text into words. For this, we will import the `word_tokenize`
    function from the `nltk.tokenize` module.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们将从nltk导入必要的库。我们需要英语停用词列表，因此我们将导入停用词语料库。我们还需要将输入文本分解为单词。为此，我们将从`nltk.tokenize`模块中导入`word_tokenize`函数。
- en: For our input text, we took the introduction paragraph from Wikipedia on text
    mining, which can be found at [http://en.wikipedia.org/wiki/Text_mining](http://en.wikipedia.org/wiki/Text_mining).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的输入文本，我们从维基百科的文本挖掘介绍部分获取了段落，详细内容可以在[http://en.wikipedia.org/wiki/Text_mining](http://en.wikipedia.org/wiki/Text_mining)中找到。
- en: 'Finally, we will tokenize the input text into words using the word_tokenize
    function. The words is now a list of all the words tokenized from the input. Let''s
    look at the output of the print function, where we will print the length of the
    words list:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用`word_tokenize`函数将输入文本分词。现在，`words`是一个包含所有从输入中分词的单词的列表。让我们看看打印函数的输出，打印出`words`列表的长度：
- en: '[PRE86]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: We have a total of 259 words in our list.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的列表中共有259个词。
- en: In step 2, we will compile a list of the English stop words in a list called
    `stop_words`.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步中，我们将编译一个包含英语停用词的列表，称为`stop_words`。
- en: 'In step 2, we will use a list comprehension to get a final list of the words;
    only those words that are not in the stop word list that we created in step 2\.
    This way, we can remove the stop words from our input. Let''s now look at the
    output of our print statement, where we will print the final list where the stop
    words have been removed:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步中，我们将使用列表推导式来获取最终的词汇表；只有那些不在我们在第2步中创建的停用词列表中的词汇才会被保留。这样，我们就能从输入文本中移除停用词。现在让我们看看我们打印语句的输出，打印出移除停用词后的最终列表：
- en: '[PRE87]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: You can see that we chopped off nearly 64 words from our input text, which were
    the stop words.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们从输入文本中去除了将近64个停用词。
- en: There's more…
  id: totrans-479
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'Stop words are not limited to proper English words. It''s contextual, depending
    on the application in hand and how you want to program your system. Ideally, if
    we are not interested in special characters, we can include them in our stop word
    list. Let''s look at the following code:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词不仅限于常规的英语单词。它是基于上下文的，取决于手头的应用程序以及您希望如何编程您的系统。理想情况下，如果我们不关心特殊字符，我们可以将它们包含在停用词列表中。让我们看一下以下代码：
- en: '[PRE88]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Here, we will run another list comprehension in order to remove punctuations
    from our words. Now, the output looks as follows:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将执行另一个列表推导式，以便从单词中移除标点符号。现在，输出结果如下所示：
- en: '[PRE89]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Tip
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Remember that stop word removal is contextual and based on the application.
    If you are working on a sentiment analysis application on mobile or chat room
    text, emoticons are highly useful. You don't remove them as they form a very good
    feature set for the downstream machine learning application.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，停用词移除是基于上下文的，并且取决于应用程序。如果您正在处理移动端或聊天室文本的情感分析应用，表情符号是非常有用的。您不会将它们移除，因为它们构成了下游机器学习应用的一个非常好的特征集。
- en: Typically, in a document, the frequency of stop words is very high. However,
    there may be other words in your corpus that may have a very high frequency. Based
    on your context, you can add them to your stop word list.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，在文档中，停用词的频率是非常高的。然而，您的语料库中可能还有其他词语，它们的频率也可能非常高。根据您的上下文，您可以将它们添加到停用词列表中。
- en: See also
  id: totrans-487
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Performing Tokenization* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Analyzing Data - Explore & Wrangle*'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*执行分词* 方案在 [第 3 章](ch03.xhtml "第 3 章. 数据分析 – 探索与清洗")，*分析数据 - 探索与清洗*'
- en: '*List generation* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python for Data
    Science"), *Using Python for Data Science*'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*列表生成* 方案在 [第 1 章](ch01.xhtml "第 1 章. Python 数据科学入门")，*使用 Python 进行数据科学*'
- en: Stemming the words
  id: totrans-490
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词干提取
- en: In this we will see how to stem the word.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将看到如何进行词干提取。
- en: Getting ready
  id: totrans-492
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Standardization of the text is a different beast and we need different tools
    to tame it. In this section, we will look into how we can convert words to their
    base forms in order to bring consistency to our processing. We will start with
    traditional ways that include stemming and lemmatization. English grammar dictates
    how certain words are used in sentences. For example, perform, performing, and
    performs indicate the same action; they appear in different sentences based on
    the grammar rules.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的标准化是一个不同的课题，我们需要不同的工具来应对它。在本节中，我们将探讨如何将单词转换为其基本形式，以便为我们的处理带来一致性。我们将从传统的方法开始，包括词干提取和词形还原。英语语法决定了某些单词在句子中的用法。例如，perform、performing
    和 performs 表示相同的动作；它们根据语法规则出现在不同的句子中。
- en: '*The goal of both stemming and lemmatization is to reduce inflectional forms
    and sometimes derivationally related forms of a word to a common base form.*'
  id: totrans-494
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*词干提取和词形还原的目标是将单词的屈折形式，有时还包括衍生形式，归约为一个共同的基本形式。*'
- en: ''
  id: totrans-495
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Introduction to Information Retrieval By Christopher D. Manning, Prabhakar
    Raghavan & Hinrich Schütze*'
  id: totrans-496
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*《信息检索简介》 作者：Christopher D. Manning, Prabhakar Raghavan & Hinrich Schütze*'
- en: 'Let''s look into how we can perform word stemming using Python NLTK. NLTK provides
    us with a rich set of functions that can help us do the stemming pretty easily:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何使用 Python NLTK 执行词干提取。NLTK 为我们提供了一套丰富的功能，可以帮助我们轻松完成词干提取：
- en: '[PRE90]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'We can see the list of functions in the module, and for our interest, we have
    the following stemmers:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模块中的函数列表，针对我们的兴趣，以下是一些词干提取器：
- en: Porter – porter stemmer
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Porter – Porter 词干提取器
- en: Lancaster – Lancaster stemmer
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lancaster – Lancaster 词干提取器
- en: Snowball – snowball stemmer
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snowball – Snowball 词干提取器
- en: Porter is the most commonly used stemmer. The algorithm is not very aggressive
    when moving words to their root form.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: Porter 是最常用的词干提取器。该算法在将单词转化为根形式时并不是特别激进。
- en: Snowball is an improvement over porter. It is also faster than porter in terms
    of the computational time.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: Snowball 是对 Porter 的改进。它在计算时间上比 Porter 更快。
- en: Lancaster is the most aggressive stemmer. With porter and snowball, the final
    word tokens would still be readable by humans, but with Lancaster, it is not readable.
    It's the fastest of the trio.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: Lancaster 是最激进的词干提取器。与 Porter 和 Snowball 不同，最终的单词令牌在经过 Lancaster 处理后将无法被人类读取，但它是这三者中最快的。
- en: In this recipe, we will use some of them to see how the stemming of words can
    be performed.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在本方案中，我们将使用其中一些函数来查看如何进行单词的词干提取。
- en: How to do it…
  id: totrans-507
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'To begin with, let''s load the necessary libraries and declare the dataset
    against which we would want to demonstrate stemming:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们加载必要的库并声明我们希望在其上演示词干提取的数据集：
- en: '[PRE91]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Let''s jump into the different stemming algorithms, as follows:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解以下不同的词干提取算法：
- en: '[PRE92]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: How it works…
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In step 1, we will import the stem module from nltk. We will also create a list
    of words that we want to stem. If you observe carefully, the words have been chosen
    to have different suffixes, including s, ies, ed, ing, and so on. Additionally,
    there are some words in their root form already, such as throttle and fry. The
    idea is to see how the stemming algorithm treats them.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们将从nltk导入词干模块。我们还将创建一个我们希望进行词干提取的单词列表。如果你仔细观察，这些单词被选择时已经具有不同的词缀，包括s、ies、ed、ing等。此外，还有一些单词已经处于根形式，如“throttle”和“fry”。这个示例的目的是查看词干提取算法如何处理它们。
- en: 'Steps 2, 3, and 4 are very similar; we will invoke the porter, lancaster, and
    snowball stemmers on the input and print the output. We will use a list comprehension
    to apply these words to our input and finally, print the output. Let''s look at
    the print output to understand the effect of stemming:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 第2、3和4步非常相似；我们将分别对输入文本应用porter、lancaster和snowball词干提取器，并打印输出。我们将使用列表推导式将这些单词应用到输入上，最后打印输出。让我们查看打印输出，了解词干提取的效果：
- en: '[PRE93]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'This is the output from step 2\. Porter stemming was applied to our input words.
    We can see that the words with the suffixes ies, s, ed , and ing have been reduced
    to their root forms:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第2步的输出。我们对输入单词应用了Porter词干提取。我们可以看到，带有词缀ies、s、ed和ing的单词已被简化为它们的根形式：
- en: '[PRE94]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: It's interesting to note that throttle is changed to throttle.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，“throttle”没有发生变化。
- en: 'In step 3, we will print the output of lancaster, which is as follows:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，我们将打印Lancaster词干提取器的输出，结果如下：
- en: '[PRE95]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: The word throttle has been left as it is. Note what has happened to movies.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 单词“throttle”保持不变。请注意“movies”发生了什么变化。
- en: 'Similarly, let''s look at the output produced by the snowball stemmer in step
    4:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，让我们看看在第4步中Snowball词干提取器的输出：
- en: '[PRE96]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: The output is pretty similar to the porter stemmer.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 输出与porter词干提取器的结果非常相似。
- en: There's more…
  id: totrans-525
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: All the three algorithms are pretty involved; going into the details of these
    algorithms is beyond the scope of this book. I will recommend you to look to the
    web for more details on these algorithms.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种算法都相当复杂；深入了解这些算法的细节超出了本书的范围。我建议你可以通过网络进一步了解这些算法的更多细节。
- en: 'For details of the porter and snowball stemmers, refer to the following link:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 有关porter和snowball词干提取器的详细信息，请参考以下链接：
- en: '[http://snowball.tartarus.org/algorithms/porter/stemmer.html](http://snowball.tartarus.org/algorithms/porter/stemmer.html)'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://snowball.tartarus.org/algorithms/porter/stemmer.html](http://snowball.tartarus.org/algorithms/porter/stemmer.html)'
- en: See also
  id: totrans-529
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*List Comprehension* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python for
    Data Science"), *Using Python for Data Science*'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第1章](ch01.xhtml "第1章. 数据科学中的Python")中的*列表推导式*示例，*使用Python进行数据科学*'
- en: Performing word lemmatization
  id: totrans-531
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行单词词形还原
- en: In this we will learn how to perform word lemmatization.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分中，我们将学习如何执行单词词形还原。
- en: Getting ready
  id: totrans-533
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Stemming is a heuristic process, which goes about chopping the word suffixes
    in order to get to the root form of the word. In the previous recipe, we saw that
    it may end up chopping even the right words, that is, chopping the derivational
    affixes.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取是一个启发式过程，通过去除单词的词缀来获得单词的根形式。在前面的示例中，我们看到它可能会错误地去除正确的词汇，也就是去除派生词缀。
- en: 'See the following Wikipedia link for the derivational patterns:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见以下维基百科链接，了解派生模式：
- en: '[http://en.wikipedia.org/wiki/Morphological_derivation#Derivational_patterns](http://en.wikipedia.org/wiki/Morphological_derivation#Derivational_patterns)'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://en.wikipedia.org/wiki/Morphological_derivation#Derivational_patterns](http://en.wikipedia.org/wiki/Morphological_derivation#Derivational_patterns)'
- en: On the other hand, lemmatization uses a morphological analysis and vocabulary
    to get the lemma of a word. It tries to change only the inflectional endings and
    give the base word from a dictionary.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，词形还原使用形态学分析和词汇来获取单词的词根。它尝试仅改变屈折结尾，并从字典中给出基础单词。
- en: See Wikipedia for more information on inflection at [http://en.wikipedia.org/wiki/Inflection](http://en.wikipedia.org/wiki/Inflection).
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅维基百科有关屈折变化的更多信息：[http://en.wikipedia.org/wiki/Inflection](http://en.wikipedia.org/wiki/Inflection)。
- en: In this recipe, we will use NLTK's `WordNetLemmatizer`.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用NLTK的`WordNetLemmatizer`。
- en: How to do it…
  id: totrans-540
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'To begin with, we will load the necessary libraries. Once again, as we did
    in the previous recipes, we will prepare a text input in order to demonstrate
    lemmatization. We will then proceed to implement lemmantization in the following
    way:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载必要的库。和之前的示例一样，我们将准备一个文本输入来演示词形还原。然后我们将以如下方式实现词形还原：
- en: '[PRE97]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: How it works…
  id: totrans-543
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Step 1 is very similar to our stemming recipe. We will provide the input. In
    step 2, we will do the lemmatization. This lemmatizer uses Wordnet's built-in
    morphy-function.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步与我们的词干提取食谱非常相似。我们将提供输入。在第2步中，我们将进行词形还原。这个词形还原器使用Wordnet的内建morphy函数。
- en: '[https://wordnet.princeton.edu/man/morphy.7WN.html](https://wordnet.princeton.edu/man/morphy.7WN.html)'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://wordnet.princeton.edu/man/morphy.7WN.html](https://wordnet.princeton.edu/man/morphy.7WN.html)'
- en: 'Let''s look at the output from the print statement:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看打印语句的输出：
- en: '[PRE98]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: The first thing to strike is the word movie. You can see that it has got this
    right. Porter and the other algorithms had chopped the last letter e.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个注意到的是单词movie。你可以看到它得到了正确的处理。Porter和其他算法将最后一个字母e去掉了。
- en: There's more…
  id: totrans-549
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Let''s look into a small example using lemmatizer:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看使用词形还原器的一个小例子：
- en: '[PRE99]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: The word running should ideally be run and our lemmatizer should have gotten
    it right. We can see that it has not made any changes to running. However, our
    heuristic-based stemmers have got it right! Then, what has gone wrong with our
    lemmatizer?
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 单词running理应还原为run，我们的词形还原器应该处理得当。我们可以看到它没有对running做出任何更改。然而，我们的启发式词干提取器处理得很好！那么，我们的词形还原器怎么了？
- en: Tip
  id: totrans-553
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'By default, the lemmatizer assumes that the input is a noun; this can be rectified
    by passing the POS tag of the word to our lemmatizer, as follows:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，词形还原器假定输入是一个名词；我们可以通过将词汇的词性标签（POS）传递给词形还原器来进行修正，如下所示：
- en: '[PRE100]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: See also
  id: totrans-556
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Performing Tokenization* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Analyzing Data - Explore & Wrangle*'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.xhtml "第3章 数据分析 – 探索与整理")中执行*分词*食谱，*分析数据 - 探索与整理*
- en: Representing the text as a bag of words
  id: totrans-558
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本表示为词袋模型
- en: In this we will learn how represent the text as a bag of words.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将学习如何将文本表示为词袋模型。
- en: Getting ready
  id: totrans-560
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'In order to do machine learning on text, we will need to convert the text to
    numerical feature vectors. In this section, we will look into the bag of words
    representation, where the text is converted to numerical vectors and the column
    names are the underlying words and values can be either of thw following points:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对文本进行机器学习，我们需要将文本转换为数值特征向量。在本节中，我们将探讨词袋模型表示，其中文本被转换为数值向量，列名是底层的单词，值可以是以下几种：
- en: Binary, which indicates whether the word is present/absent in the given document
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制，表示词汇是否出现在给定的文档中。
- en: Frequency, which indicates the count of the word in the given document
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频率，表示单词在给定文档中的出现次数。
- en: TFIDF, which is a score that we will cover subsequently
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TFIDF，这是一个分数，我们将在后面讲解。
- en: 'Bag of words is the most frequent way of representing the text. As the name
    suggests, the order of words is ignored and only the presence/absence of words
    are key to this representation. It is a two-step process, as follows:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型是最常见的文本表示方式。顾名思义，词语的顺序被忽略，只有词语的存在/缺失对这种表示方法至关重要。这是一个两步过程，如下所示：
- en: For every word in the document that is present in the training set, we will
    assign an integer and store this as a dictionary.
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于文档中每个出现在训练集中的单词，我们将分配一个整数并将其存储为字典。
- en: For every document, we will create a vector. The columns of the vectors are
    the actual words itself. They form the features. The values of the cell are binary,
    frequency, or TFIDF.
  id: totrans-567
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个文档，我们将创建一个向量。向量的列是实际的单词本身。它们构成了特征。单元格的值可以是二进制、频率或TFIDF。
- en: How to do it…
  id: totrans-568
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Let''s load the necessary libraries and prepare the dataset for the demonstration
    of bag of words:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载必要的库并准备数据集来演示词袋模型：
- en: '[PRE101]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Let''s jump into how to transform the text into a bag of words representation:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解如何将文本转换为词袋模型表示：
- en: '[PRE102]'
  id: totrans-572
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: How it works…
  id: totrans-573
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'In step 1, we will define the input. This is the same input that we used for
    the stop word removal recipe. In step 2, we will import the sentence tokenizer
    and tokenize the given input into sentences. We will treat every sentence here
    as a document:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们将定义输入。这与我们用于停用词去除的输入相同。在第2步中，我们将导入句子分词器，并将给定的输入分割成句子。我们将把这里的每个句子当作一个文档：
- en: Tip
  id: totrans-575
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Depending on your application, the notion of a document can change. In this
    case, our sentence is considered as a document. In some cases, we can also treat
    a paragraph as a document. In web page mining, a single web page can be treated
    as a document or parts of the web page separated by the <p> tags can also be treated
    as a document.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的应用，文档的概念可以有所变化。在这个例子中，我们的句子被视为一个文档。在某些情况下，我们也可以将一个段落视为文档。在网页挖掘中，单个网页可以视为一个文档，或者被
    `<p>` 标签分隔的网页部分也可以视为一个文档。
- en: '[PRE103]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: If we print the length of the sentence list, we will get six, and so in our
    case, we have six documents.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印句子列表的长度，我们会得到六，因此在我们的例子中，我们有六个文档。
- en: In step 3, we will import `CountVectorizer` from the `scikitlearn.feature_extraction`
    text package. It converts a collection of documents—in this case, a list of sentences—to
    a matrix, where the rows are sentences and the columns are the words in these
    sentences. The count of these words are inserted in the value of these cells.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 步中，我们将从 `scikit-learn.feature_extraction` 文本包中导入 `CountVectorizer`。它将文档集合——在此案例中是句子列表——转换为矩阵，其中行是句子，列是这些句子中的单词。这些单词的计数将作为单元格的值插入。
- en: 'We will transform the list of sentences into a term document matrix using `CountVectorizer`.
    Let''s dissect the output one by one. First, we will look into `count_v`, which
    is a `CountVectorizer` object. We had mentioned in the introduction that we need
    to build a dictionary of all the words in the given text. The `vocabulary_` of
    `count_v` attribute provides us with the list of words and their associated IDs
    or feature indices:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `CountVectorizer` 将句子列表转换为一个词项文档矩阵。让我们逐一解析输出。首先，我们来看一下 `count_v`，它是一个
    `CountVectorizer` 对象。我们在引言中提到过，我们需要构建一个包含给定文本中所有单词的字典。`count_v` 的 `vocabulary_`
    属性提供了单词及其相关 ID 或特征索引的列表：
- en: '![How it works…](img/B04041_03_19.jpg)'
  id: totrans-581
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_03_19.jpg)'
- en: 'This dictionary can be retrieved using the `vocabulary_` attribute. This is
    a map of the terms in order to feature indices. We can also use the following
    function to get the list of words (features):'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 `vocabulary_` 属性检索这个字典。它是一个将词项映射到特征索引的映射。我们还可以使用以下函数获取单词（特征）的列表：
- en: '[PRE104]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Let''s now move on to look at `tdm`, which is the object that we received after
    transforming the given input using CountVectorizer:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下 `tdm`，这是我们使用 `CountVectorizer` 转换给定输入后得到的对象：
- en: '[PRE105]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'As you can see, tdm is a sparse matrix object. Refer to the following link
    to understand more about the sparse matrix representation:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，tdm 是一个稀疏矩阵对象。请参考以下链接，了解更多关于稀疏矩阵表示的信息：
- en: '[http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html)'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html)'
- en: 'We can look into the shape of this object and also inspect some of the elements,
    as follows:'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看这个对象的形状，并检查其中的一些元素，如下所示：
- en: '![How it works…](img/B04041_03_20.jpg)'
  id: totrans-589
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_03_20.jpg)'
- en: We can see that the shape of the matrix is 6 X 122\. We have six documents,
    that is, sentences in our context and 122 words that form the vocabulary. Note
    that this is a sparse matrix representation; as all the sentences will not have
    all the words, a lot of the cell values will have zero as an entry and hence,
    we will print only the indices that have non-zero entries.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到矩阵的形状是 6 X 122。我们有六个文档，也就是在我们的语境下的句子，122 个单词构成了词汇表。请注意，这是稀疏矩阵表示形式；因为并非所有句子都会包含所有单词，所以很多单元格的值会是零，因此我们只会打印那些非零条目的索引。
- en: 'From `tdm.indptr`, we know that document 1''s entry starts from `0` and ends
    at 18 in the `tdm.data` and `tdm.indices` arrays, as follows:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `tdm.indptr` 中，我们知道文档 1 的条目从 `0` 开始，在 `tdm.data` 和 `tdm.indices` 数组中结束于 18，如下所示：
- en: '[PRE106]'
  id: totrans-592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'We can verify this in the following way:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式验证这一点：
- en: '[PRE107]'
  id: totrans-594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: We can see that `107`, which corresponds to the word text, has occurred four
    times in the first sentence, and similarly, mining has occurred once. Thus, in
    this recipe, we converted a given text into a feature vector, where the features
    are words.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`107` 对应单词 *text*，在第一句话中出现了四次，类似地，*mining* 出现了一次。因此，在这个示例中，我们将给定的文本转换成了一个特征向量，其中的特征是单词。
- en: There's more…
  id: totrans-596
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'The `CountVectorizer` class has a lot of other features to offer in order to
    transform the text into feature vectors. Let''s look at some of them:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer` 类提供了许多其他功能，用于将文本转换为特征向量。我们来看看其中的一些功能：'
- en: '[PRE108]'
  id: totrans-598
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: The first one is binary, which is set to `False`; we can also have it set to
    `True`. Then, the final matrix would not have the count but will have one or zero,
    based on the presence or absence of the word in the document.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是二进制的，设置为`False`；我们也可以将其设置为`True`。这样，最终的矩阵将不再显示计数，而是根据单词是否出现在文档中，显示1或0。
- en: The lowercase is set to `True` by default; the input text is transformed into
    lowercase before the mapping of the words to feature indices is performed.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`lowercase`设置为`True`；输入文本会在单词映射到特征索引之前转为小写。
- en: 'While creating a mapping of the words to feature indices, we can ignore some
    words by providing a stop word list. Observe the following example:'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建单词与特征索引的映射时，我们可以通过提供一个停用词列表来忽略一些单词。请查看以下示例：
- en: '[PRE109]'
  id: totrans-602
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'If we print the size of the vocabulary that has been built, we can see the
    following:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印出已构建的词汇表的大小，我们可以看到以下内容：
- en: '[PRE110]'
  id: totrans-604
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: We can see that we have 106 now as compared to 122 that we had before.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到现在有106个词，而之前是122个。
- en: We can also give a fixed set of vocabulary to `CountVectorizer`. The final sparse
    matrix columns will be only from these fixed sets and anything that is not in
    this set will be ignored.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以为`CountVectorizer`提供一个固定的词汇集。最终的稀疏矩阵列将仅来自这些固定集，任何不在该集合中的内容将被忽略。
- en: 'The next interesting parameter is the ngram range. You can see that a tuple
    (1,1) has been passed. This ensures that only one grams or single words are used
    while creating a feature set. For example, this can be changed to (1,2), which
    tells `CountVectorizer` to create both unigrams and bigrams. Let''s look at the
    following code and the output:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个有趣的参数是n-gram范围。你可以看到传递了一个元组（1,1）。这确保了在创建特征集时只使用单个词语或一元词。例如，可以将其更改为（1,2），这会告诉`CountVectorizer`创建一元词和二元词。让我们看一下下面的代码和输出：
- en: '[PRE111]'
  id: totrans-608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: Both the unigrams and bigrams are now a part of our feature set.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一元词和二元词都成为了我们的特征集的一部分。
- en: 'I will leave you to explore the other parameters. The documentation for these
    parameters is available at the following link:'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 我将留给你去探索其他参数。这些参数的文档可以通过以下链接访问：
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)'
- en: See also
  id: totrans-612
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*Using Dictionaries* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python for
    Data Science"), *Using Python for Data Science*'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用字典*食谱在[第1章](ch01.xhtml "第1章 Python数据科学入门")中，*Python数据科学入门*'
- en: '*Removing Stop words, Stemming of words, Lemmatization of words* recipe in
    [Chapter 3](ch03.xhtml "Chapter 3. Data Analysis – Explore and Wrangle"), *Analyzing
    Data - Explore & Wrangle*'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*去除停用词、词干提取、词形还原*食谱在[第3章](ch03.xhtml "第3章 数据分析——探索与整理")中，*数据分析——探索与整理*'
- en: Calculating term frequencies and inverse document frequencies
  id: totrans-615
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算词频和逆文档频率
- en: In this we will learn how to calculate term frequencies and inverse document
    frequencies.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将学习如何计算词频和逆文档频率。
- en: Getting ready
  id: totrans-617
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Occurrences and counts are good as feature values, but they suffer from some
    problems. Let's say that we have four documents of unequal length. This will give
    a higher weightage to the terms in the longer documents than those in the shorter
    ones. So, instead of using the plain vanilla occurrence, we will normalize it;
    we will divide the number of occurrences of a word in a document by the total
    number of words in the document. This metric is called term frequencies. Term
    frequency is also not without problems. There are words that will occur in many
    documents. These words would dominate the feature vector but they are not informative
    enough to distinguish the documents in the corpus. Before we look into a new metric
    that can avoid this problem, let's define document frequency. Similar to word
    frequency, which is local with respect to a document, we can calculate a score
    called document frequency, which is the number of documents that the word occurs
    in the corpus divided by the total number of documents in the corpus.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 出现次数和计数作为特征值是不错的选择，但它们也存在一些问题。假设我们有四篇长度不同的文档，这会使得较长文档中的词语比较短文档中的词语权重更高。因此，我们将不使用原始的词频，而是对其进行归一化处理；我们将一个词在文档中出现的次数除以文档中的总词数。这个度量叫做词频。词频也并非没有问题。有些词语会出现在很多文档中。这些词会主导特征向量，但它们不足以区分语料库中的文档。在我们探讨一个可以避免这个问题的新度量之前，先来定义一下文档频率。类似于词频，它是相对于文档的局部度量，我们可以计算一个称为文档频率的得分，它是词语在语料库中出现的文档数除以语料库中的总文档数。
- en: The final metric that we will use for the words is the product of the term frequency
    and the inverse of the document frequency. This is called the TFIDF score.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的最终度量是词频和文档频率倒数的乘积，这就是所谓的 TFIDF 得分。
- en: How to do it…
  id: totrans-620
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Load the necessary libraries and declare the input data that will be used for
    the demonstration of term frequencies and inverse document frequencies:'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 加载必要的库并声明将用于展示词频和逆文档频率的输入数据：
- en: '[PRE112]'
  id: totrans-622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Let''s see how to find the term frequency and inverse document frequency:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何计算词频和逆文档频率：
- en: '[PRE113]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: How it works…
  id: totrans-625
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Steps 1, 2, and 3 are the same as the previous recipe. Let''s look at step
    4, where we will pass the output of step 3 in order to calculate the TFIDF score:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 1、2 和 3 与之前的教程相同。让我们来看一下步骤 4，在这一步骤中，我们将传递步骤 3 的输出，以计算 TFIDF 得分：
- en: '[PRE114]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Tdm is a sparse matrix. Now, let''s look at the values of these matrices, using
    indices, data, and index pointer:'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: Tdm 是一个稀疏矩阵。现在，让我们使用索引、数据和索引指针来查看这些矩阵的值：
- en: '![How it works…](img/B04041_03_21.jpg)'
  id: totrans-629
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的…](img/B04041_03_21.jpg)'
- en: The data shows the values, we don't have the occurences, but the normalized
    TFIDF score for the words.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 数据显示的是值，而不是出现次数，而是单词的归一化 TFIDF 得分。
- en: There's more…
  id: totrans-631
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'Once again, we can delve deeper into the TFIDF transformer by looking into
    the parameters that can be passed:'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 再一次，我们可以通过查看可以传递的参数，深入了解 TFIDF 转换器：
- en: '[PRE115]'
  id: totrans-633
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: The documentation for this is available at [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html).
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 相关文档可以在[http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)找到。
