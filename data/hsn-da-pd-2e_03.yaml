- en: '*Chapter 2*: Working with Pandas DataFrames'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 2 章*：使用 Pandas DataFrame'
- en: The time has come for us to begin our journey into the `pandas` universe. This
    chapter will get us comfortable working with some of the basic, yet powerful,
    operations we will be performing when conducting our data analyses with `pandas`.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候开始我们的 `pandas` 之旅了。本章将让我们熟悉在进行数据分析时使用 `pandas` 执行一些基本但强大的操作。
- en: 'We will begin with an introduction to the main `pandas`. Data structures provide
    us with a format for organizing, managing, and storing data. Knowledge of `pandas`
    data structures will prove infinitely helpful when it comes to troubleshooting
    or looking up how to perform an operation on the data. Keep in mind that these
    data structures are different from the standard Python data structures for a reason:
    they were created for specific analysis tasks. We must remember that a given method
    may only work on a certain data structure, so we need to be able to identify the
    best structure for the problem we are looking to solve.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从介绍主要的 `pandas` 开始。数据结构为我们提供了一种组织、管理和存储数据的格式。了解 `pandas` 数据结构在解决问题或查找如何对数据执行某项操作时将无比有帮助。请记住，这些数据结构与标准
    Python 数据结构不同，原因是它们是为特定的分析任务而创建的。我们必须记住，某个方法可能只能在特定的数据结构上使用，因此我们需要能够识别最适合我们要解决的问题的数据结构。
- en: Next, we will bring our first dataset into Python. We will learn how to collect
    data from an API, create `DataFrame` objects from other data structures in Python,
    read in files, and interact with databases. Initially, you may wonder why we would
    ever need to create a `DataFrame` object from other Python data structures; however,
    if we ever want to test something quickly, create our own data, pull data from
    an API, or repurpose Python code from another project, then we will find this
    knowledge indispensable. Finally, we will master ways to inspect, describe, filter,
    and summarize our data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把第一个数据集导入 Python。我们将学习如何从 API 获取数据、从其他 Python 数据结构创建 `DataFrame` 对象、读取文件并与数据库进行交互。起初，你可能会想，为什么我们需要从其他
    Python 数据结构创建 `DataFrame` 对象；然而，如果我们想要快速测试某些内容、创建自己的数据、从 API 拉取数据，或者重新利用其他项目中的
    Python 代码，那么我们会发现这些知识是不可或缺的。最后，我们将掌握检查、描述、过滤和总结数据的方法。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Pandas data structures
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas 数据结构
- en: Creating DataFrame objects from files, API requests, SQL queries, and other
    Python objects
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文件、API 请求、SQL 查询和其他 Python 对象创建 DataFrame 对象
- en: Inspecting DataFrame objects and calculating summary statistics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查 DataFrame 对象并计算总结统计量
- en: Grabbing subsets of the data via selection, slicing, indexing, and filtering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择、切片、索引和过滤获取数据的子集
- en: Adding and removing data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加和删除数据
- en: Chapter materials
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章内容
- en: The files we will be working with in this chapter can be found in the GitHub
    repository at [https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_02](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_02).
    We will be working with earthquake data from the `data/` directory.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将使用的文件可以在 GitHub 仓库中找到，地址是 [https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_02](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_02)。我们将使用来自
    `data/` 目录的地震数据。
- en: There are four CSV files and a SQLite database file, all of which will be used
    at different points throughout this chapter. The `earthquakes.csv` file contains
    data that's been pulled from the USGS API for September 18, 2018 through October
    13, 2018\. For our discussion of data structures, we will work with the `example_data.csv`
    file, which contains five rows and a subset of the columns from the `earthquakes.csv`
    file. The `tsunamis.csv` file is a subset of the data in the `earthquakes.csv`
    file for all earthquakes that were accompanied by tsunamis during the aforementioned
    date range. The `quakes.db` file contains a SQLite database with a single table
    for the tsunamis data. We will use this to learn how to read from and write to
    a database with `pandas`. Lastly, the `parsed.csv` file will be used for the end-of-chapter
    exercises, and we will also walk through the creation of it during this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中会使用四个 CSV 文件和一个 SQLite 数据库文件，它们将在不同的时间点被使用。`earthquakes.csv`文件包含从 USGS API
    拉取的 2018年9月18日到10月13日的数据。对于数据结构的讨论，我们将使用`example_data.csv`文件，该文件包含五行数据，并且是`earthquakes.csv`文件中的列的子集。`tsunamis.csv`文件是`earthquakes.csv`文件中所有伴随海啸的地震数据的子集，时间范围为上述日期。`quakes.db`文件包含一个
    SQLite 数据库，其中有一个表存储着海啸数据。我们将利用这个数据库学习如何使用`pandas`从数据库中读取和写入数据。最后，`parsed.csv`文件将用于本章结尾的练习，我们也将在本章中演示如何创建它。
- en: The accompanying code for this chapter has been divided into six Jupyter Notebooks,
    which are numbered in the order they are to be used. They contain the code snippets
    we will be running throughout this chapter, along with the full output of any
    command that has to be trimmed for this text. Each time we are to switch notebooks,
    the text will indicate to do so.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的伴随代码已被分成六个 Jupyter Notebooks，按照使用顺序编号。它们包含了我们在本章中将运行的代码片段，以及任何需要为本文本进行裁剪的命令的完整输出。每次需要切换笔记本时，文本会指示进行切换。
- en: In the `1-pandas_data_structures.ipynb` notebook, we will start learning about
    the main `pandas` data structures. Afterward, we will discuss the various ways
    to create `DataFrame` objects in the `2-creating_dataframes.ipynb` notebook. Our
    discussion on this topic will continue in the `3-making_dataframes_from_api_requests.ipynb`
    notebook, where we will explore the USGS API to gather data for use with `pandas`.
    After learning about how we can collect our data, we will begin to learn how to
    conduct `4-inspecting_dataframes.ipynb` notebook. Then, in the `5-subsetting_data.ipynb`
    notebook, we will discuss various ways to select and filter data. Finally, we
    will learn how to add and remove data in the `6-adding_and_removing_data.ipynb`
    notebook. Let's get started.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在`1-pandas_data_structures.ipynb`笔记本中，我们将开始学习主要的`pandas`数据结构。之后，我们将在`2-creating_dataframes.ipynb`笔记本中讨论创建`DataFrame`对象的各种方式。我们将在`3-making_dataframes_from_api_requests.ipynb`笔记本中继续讨论此话题，探索
    USGS API 以收集数据供`pandas`使用。学习完如何收集数据后，我们将开始学习如何在`4-inspecting_dataframes.ipynb`笔记本中检查数据。然后，在`5-subsetting_data.ipynb`笔记本中，我们将讨论各种选择和过滤数据的方式。最后，我们将在`6-adding_and_removing_data.ipynb`笔记本中学习如何添加和删除数据。让我们开始吧。
- en: Pandas data structures
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pandas 数据结构
- en: 'Python has several data structures already, such as tuples, lists, and dictionaries.
    Pandas provides two main structures to facilitate working with data: `Series`
    and `DataFrame`. The `Series` and `DataFrame` data structures each contain another
    `pandas` data structure, `Index`, that we must also be aware of. However, in order
    to understand these data structures, we need to first take a look at NumPy ([https://numpy.org/doc/stable/](https://numpy.org/doc/stable/)),
    which provides the n-dimensional arrays that `pandas` builds upon.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Python 本身已经提供了几种数据结构，如元组、列表和字典。Pandas 提供了两种主要的数据结构来帮助处理数据：`Series`和`DataFrame`。`Series`和`DataFrame`数据结构中各自包含了另一种`pandas`数据结构——`Index`，我们也需要了解它。然而，为了理解这些数据结构，我们首先需要了解
    NumPy（[https://numpy.org/doc/stable/](https://numpy.org/doc/stable/)），它提供了`pandas`所依赖的
    n 维数组。
- en: The aforementioned data structures are implemented as Python `CapWords`, while
    objects are written in `snake_case`. (More Python style guidelines can be found
    at [https://www.python.org/dev/peps/pep-0008/](https://www.python.org/dev/peps/pep-0008/).)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的数据结构以 Python `CapWords`风格实现，而对象则采用`snake_case`书写。（更多 Python 风格指南请参见[https://www.python.org/dev/peps/pep-0008/](https://www.python.org/dev/peps/pep-0008/)。）
- en: We use a `pandas` function to read a CSV file into an object of the `DataFrame`
    class, but we use methods on our `DataFrame` objects to perform actions on them,
    such as dropping columns or calculating summary statistics. With `pandas`, we
    will often want to access the `pandas` object, such as dimensions, column names,
    data types, and whether it is empty.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`pandas`函数将CSV文件读取为`DataFrame`类的对象，但我们使用`DataFrame`对象的方法对其执行操作，例如删除列或计算汇总统计数据。使用`pandas`时，我们通常希望访问`pandas`对象的属性，如维度、列名、数据类型以及是否为空。
- en: Important note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For the remainder of this book, we will refer to `DataFrame` objects as dataframes,
    `Series` objects as series, and `Index` objects as index/indices, unless we are
    referring to the class itself.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，我们将`DataFrame`对象称为dataframe，`Series`对象称为series，`Index`对象称为index/indices，除非我们明确指的是类本身。
- en: 'For this section, we will work in the `1-pandas_data_structures.ipynb` notebook.
    To begin, we will import `numpy` and use it to read the contents of the `example_data.csv`
    file into a `numpy.array` object. The data comes from the USGS API for earthquakes
    (source: [https://earthquake.usgs.gov/fdsnws/event/1/](https://earthquake.usgs.gov/fdsnws/event/1/)).
    Note that this is the only time we will use NumPy to read in a file and that this
    is being done for illustrative purposes only; the important part is to look at
    the way the data is represented with NumPy:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节内容，我们将在`1-pandas_data_structures.ipynb`笔记本中进行操作。首先，我们将导入`numpy`并使用它读取`example_data.csv`文件的内容到一个`numpy.array`对象中。数据来自美国地质调查局（USGS）的地震API（来源：[https://earthquake.usgs.gov/fdsnws/event/1/](https://earthquake.usgs.gov/fdsnws/event/1/)）。请注意，这是我们唯一一次使用NumPy读取文件，并且这样做仅仅是为了演示；重要的是要查看NumPy表示数据的方式：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We now have our data in a NumPy array. Using the `shape` and `dtype` attributes,
    we can gather information about the dimensions of the array and the data types
    it contains, respectively:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将数据存储在一个NumPy数组中。通过使用`shape`和`dtype`属性，我们可以分别获取数组的维度信息和其中包含的数据类型：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Each of the entries in the array is a row from the CSV file. NumPy arrays contain
    a single data type (unlike lists, which allow mixed types); this allows for fast,
    vectorized operations. When we read in the data, we got an array of `numpy.void`
    objects, which are used to store flexible types. This is because NumPy had to
    store several different data types per row: four strings, a float, and an integer.
    Unfortunately, this means that we can''t take advantage of the performance improvements
    NumPy provides for single data type objects.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数组中的每个条目都是CSV文件中的一行。NumPy数组包含单一的数据类型（不同于允许混合类型的列表）；这使得快速的矢量化操作成为可能。当我们读取数据时，我们得到了一个`numpy.void`对象的数组，它用于存储灵活的类型。这是因为NumPy必须为每一行存储多种不同的数据类型：四个字符串，一个浮点数和一个整数。不幸的是，这意味着我们不能利用NumPy为单一数据类型对象提供的性能提升。
- en: 'Say we want to find the maximum magnitude—we can use a `numpy.void` object.
    This makes a list, meaning that we can take the maximum using the `max()` function.
    We can use the `%%timeit` `%`) to see how long this implementation takes (times
    will vary):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想找出最大幅度——我们可以使用`numpy.void`对象。这会创建一个列表，意味着我们可以使用`max()`函数来找出最大值。我们还可以使用`%%timeit`
    `%`）来查看这个实现所花费的时间（时间会有所不同）：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that we should use a list comprehension whenever we would write a `for`
    loop with just a single line under it or want to run an operation against the
    members of some initial list. This is a rather simple list comprehension, but
    we can make them more complex with the addition of `if...else` statements. List
    comprehensions are an extremely powerful tool to have in our arsenal. More information
    can be found in the Python documentation at https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每当我们编写一个只有一行内容的`for`循环，或者想要对初始列表的成员执行某个操作时，应该使用列表推导式。这是一个相对简单的列表推导式，但我们可以通过添加`if...else`语句使其更加复杂。列表推导式是我们工具箱中一个非常强大的工具。更多信息可以参考Python文档：https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions。
- en: Tip
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: '**IPython** ([https://ipython.readthedocs.io/en/stable/index.html](https://ipython.readthedocs.io/en/stable/index.html))
    provides an interactive shell for Python. Jupyter Notebooks are built on top of
    IPython. While knowledge of IPython is not required for this book, it can be helpful
    to be familiar with some of its functionality. IPython includes a tutorial in
    their documentation at [https://ipython.readthedocs.io/en/stable/interactive/](https://ipython.readthedocs.io/en/stable/interactive/).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**IPython** ([https://ipython.readthedocs.io/en/stable/index.html](https://ipython.readthedocs.io/en/stable/index.html))
    提供了一个Python的交互式Shell。Jupyter笔记本是建立在IPython之上的。虽然本书不要求掌握IPython，但熟悉一些IPython的功能会有所帮助。IPython在其文档中提供了一个教程，链接是
    [https://ipython.readthedocs.io/en/stable/interactive/](https://ipython.readthedocs.io/en/stable/interactive/)。'
- en: 'If we create a NumPy array for each column instead, this operation is much
    easier (and more efficient) to perform. To do so, we will use a **dictionary comprehension**
    (https://www.python.org/dev/peps/pep-0274/) to make a dictionary where the keys
    are the column names and the values are NumPy arrays of the data. Again, the important
    part here is how the data is now represented using NumPy:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们为每一列创建一个NumPy数组，那么这项操作将变得更加简单（且更高效）。为了实现这一点，我们将使用**字典推导式** ([https://www.python.org/dev/peps/pep-0274/](https://www.python.org/dev/peps/pep-0274/))
    来创建一个字典，其中键是列名，值是包含数据的NumPy数组。同样，重要的部分在于数据现在是如何使用NumPy表示的：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Grabbing the maximum magnitude is now simply a matter of selecting the `mag`
    key and calling the `max()` method on the NumPy array. This is nearly twice as
    fast as the list comprehension implementation, when dealing with just five entries—imagine
    how much worse the first attempt will perform on large datasets:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，获取最大值的幅度仅仅是选择`mag`键并在NumPy数组上调用`max()`方法。这比列表推导式的实现速度快近两倍，尤其是处理仅有五个条目的数据时——想象一下，第一个尝试在大数据集上的表现将会有多糟糕：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'However, this representation has other issues. Say we wanted to grab all the
    information for the earthquake with the maximum magnitude; how would we go about
    that? We need to find the index of the maximum, and then for each of the keys
    in the dictionary, grab that index. The result is now a NumPy array of strings
    (our numeric values were converted), and we are now in the format that we saw
    earlier:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种表示方式还有其他问题。假设我们想获取最大幅度的地震的所有信息；我们该如何操作呢？我们需要找到最大值的索引，然后对于字典中的每一个键，获取该索引。结果现在是一个包含字符串的NumPy数组（我们的数值已被转换），并且我们现在处于之前看到的格式：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Consider how we would go about sorting the data by magnitude from smallest to
    largest. In the first representation, we would have to sort the rows by examining
    the third index. With the second representation, we would have to determine the
    order of the indices from the `mag` column, and then sort all the other arrays
    with those same indices. Clearly, working with several NumPy arrays containing
    different data types at once is a bit cumbersome; however, `pandas` builds on
    top of NumPy arrays to make this easier. Let's start our exploration of `pandas`
    with an overview of the `Series` data structure.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑如何按幅度从小到大排序数据。在第一种表示方式中，我们需要通过检查第三个索引来对行进行排序。而在第二种表示方式中，我们需要确定`mag`列的索引顺序，然后按照这些相同的索引排序所有其他数组。显然，同时操作多个包含不同数据类型的NumPy数组有些繁琐；然而，`pandas`是在NumPy数组之上构建的，可以让这一过程变得更加简单。让我们从`Series`数据结构的概述开始，探索`pandas`。
- en: Series
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Series
- en: 'The `Series` class provides a data structure for arrays of a single type, just
    like the NumPy array. However, it comes with some additional functionality. This
    one-dimensional representation can be thought of as a column in a spreadsheet.
    We have a name for our column, and the data we hold in it is of the same type
    (since we are measuring the same variable):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`Series`类提供了一种数据结构，用于存储单一类型的数组，就像NumPy数组一样。然而，它还提供了一些额外的功能。这个一维表示可以被看作是电子表格中的一列。我们为我们的列命名，而其中的数据是相同类型的（因为我们测量的是相同的变量）：'
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note the numbers on the left of the result; these correspond to the row number
    in the original dataset (offset by 1 since, in Python, we start counting at 0).
    These row numbers form the index, which we will discuss in the following section.
    Next to the row numbers, we have the actual value of the row, which, in this example,
    is a string indicating where the earthquake occurred. Notice that we have `dtype:
    object` next to the name of the `Series` object; this is telling us that the data
    type of `place` is `object`. A string will be classified as `object` in `pandas`.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '注意结果左侧的数字；这些数字对应于原始数据集中行号（由于 Python 中的计数是从 0 开始的，因此行号比实际行号少 1）。这些行号构成了索引，我们将在接下来的部分讨论。行号旁边是行的实际值，在本示例中，它是一个字符串，指示地震发生的地点。请注意，在
    `Series` 对象的名称旁边，我们有 `dtype: object`；这表示 `place` 的数据类型是 `object`。在 `pandas` 中，字符串会被分类为
    `object`。'
- en: 'To access attributes of the `Series` object, we use attribute notation of the
    form `<object>.<attribute_name>`. The following are some common attributes we
    will access. Notice that `dtype` and `shape` are available, just as we saw with
    the NumPy array:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 `Series` 对象的属性，我们使用 `<object>.<attribute_name>` 这种属性表示法。以下是我们将要访问的一些常用属性。注意，`dtype`
    和 `shape` 是可用的，正如我们在 NumPy 数组中看到的那样：
- en: '![Figure 2.1 – Commonly used series attributes'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1 – 常用的系列属性'
- en: '](img/Figure_2.1_B16834.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.1_B16834.jpg)'
- en: Figure 2.1 – Commonly used series attributes
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 常用的系列属性
- en: Important note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For the most part, `pandas` objects use NumPy arrays for their internal data
    representations. However, for some data types, `pandas` builds upon NumPy to create
    its own arrays (https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html).
    For this reason, depending on the data type, `values` can return either a `pandas.array`
    or a `numpy.array` object. Therefore, if we need to ensure we get a specific type
    back, it is recommended to use the `array` attribute or `to_numpy()` method, respectively,
    instead of `values`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，`pandas` 对象使用 NumPy 数组来表示其内部数据。然而，对于某些数据类型，`pandas` 在 NumPy 的基础上构建了自己的数组（https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html）。因此，根据数据类型，`values`
    方法返回的可能是 `pandas.array` 或 `numpy.array` 对象。因此，如果我们需要确保获得特定类型的数据，建议使用 `array` 属性或
    `to_numpy()` 方法，而不是 `values`。
- en: Be sure to bookmark the `pandas.Series` documentation ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html))
    for reference later. It contains more information on how to create a `Series`
    object, the full list of attributes and methods that are available, as well as
    a link to the source code. With this high-level introduction to the `Series` class,
    we are ready to move on to the `Index` class.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请务必将 `pandas.Series` 文档（[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html)）收藏以便以后参考。它包含有关如何创建
    `Series` 对象、所有可用属性和方法的完整列表，以及源代码链接。在了解了 `Series` 类的高层次介绍后，我们可以继续学习 `Index` 类。
- en: Index
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引
- en: 'The addition of the `Index` class makes the `Series` class significantly more
    powerful than a NumPy array. The `Index` class gives us row labels, which enable
    selection by row. Depending on the type, we can provide a row number, a date,
    or even a string to select our row. It plays a key role in identifying entries
    in the data and is used for a multitude of operations in `pandas`, as we will
    see throughout this book. We can access the index through the `index` attribute:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`Index` 类的引入使得 `Series` 类比 NumPy 数组更为强大。`Index` 类为我们提供了行标签，使得我们可以通过行号选择数据。根据索引的类型，我们可以提供行号、日期，甚至字符串来选择行。它在数据条目的标识中起着关键作用，并在
    `pandas` 中的多种操作中被使用，正如我们在本书中将要看到的那样。我们可以通过 `index` 属性访问索引：'
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that this is a `RangeIndex` object. Its values start at `0` and end at
    `4`. The step of `1` indicates that the indices are all `1` apart, meaning that
    we have all the integers in that range. The default index class is `RangeIndex`;
    however, we can change the index, as we will discuss in [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*. Often, we will either work with an `Index` object
    of row numbers or date(time)s.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是一个 `RangeIndex` 对象。它的值从 `0` 开始，到 `4` 结束。步长为 `1` 表明索引值之间的差距为 1，意味着我们有该范围内的所有整数。默认的索引类是
    `RangeIndex`；但是，我们可以更改索引，正如我们将在[*第 3 章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)
    *《数据清理与 Pandas》*中讨论的那样。通常，我们要么使用行号的 `Index` 对象，要么使用日期（时间）的 `Index` 对象。
- en: 'As with `Series` objects, we can access the underlying data via the `values`
    attribute. Note that this `Index` object is built on top of a NumPy array:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与`Series`对象一样，我们可以通过`values`属性访问底层数据。请注意，这个`Index`对象是基于一个NumPy数组构建的：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Some of the useful attributes of `Index` objects include the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`Index`对象的一些有用属性包括：'
- en: '![Figure 2.2 – Commonly used index attributes'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.2 – 常用的索引属性'
- en: '](img/Figure_2.2_B16834.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.2_B16834.jpg)'
- en: Figure 2.2 – Commonly used index attributes
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 常用的索引属性
- en: 'Both NumPy and `pandas` support arithmetic operations, which will be performed
    element-wise. NumPy will use the position in the array for this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy和`pandas`都支持算术运算，这些运算将按元素逐一执行。NumPy会使用数组中的位置来进行运算：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With `pandas`, this element-wise arithmetic is performed on matching index
    values. If we add a `Series` object with an index from `0` to `4` (stored in `x`)
    and another, `y`, from `1` to `5`, we will only get results were the indices align
    (`1` through `4`). In [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*, we will discuss some ways to change and align the
    index so that we can perform these types of operations without losing data:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在`pandas`中，这种按元素逐一执行的算术运算是基于匹配的索引值进行的。如果我们将一个索引从`0`到`4`的`Series`对象（存储在`x`中）与另一个索引从`1`到`5`的`y`对象相加，只有当索引对齐时，我们才会得到结果（`1`到`4`）。在[*第3章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)，*使用Pandas进行数据整理*中，我们将讨论一些方法来改变和对齐索引，这样我们就可以执行这些类型的操作而不丢失数据：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we have had a primer on both the `Series` and `Index` classes, we are
    ready to learn about the `DataFrame` class. Note that more information on the
    `Index` class can be found in the respective documentation at [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了`Series`和`Index`类的基础知识，接下来我们可以学习`DataFrame`类。请注意，关于`Index`类的更多信息可以在相应的文档中找到：[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html)。
- en: DataFrame
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据框
- en: 'With the `Series` class, we essentially had columns of a spreadsheet, with
    the data all being of the same type. The `DataFrame` class builds upon the `Series`
    class and can have many columns, each with its own data type; we can think of
    it as representing the spreadsheet as a whole. We can turn either of the NumPy
    representations we built from the example data into a `DataFrame` object:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Series`类中，我们本质上处理的是电子表格的列，数据类型都是相同的。`DataFrame`类是在`Series`类基础上构建的，可以拥有多个列，每列都有其自己的数据类型；我们可以将其看作是代表整个电子表格。我们可以将我们从示例数据中构建的NumPy表示形式转化为`DataFrame`对象：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This gives us a dataframe of six series. Note the column before the `time`
    column; this is the `Index` object for the rows. When creating a `DataFrame` object,
    `pandas` aligns all the series to the same index. In this case, it is just the
    row number, but we could easily use the `time` column for this, which would enable
    some additional `pandas` features, as we will see in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了一个由六个系列组成的数据框。请注意`time`列前面的那一列；它是行的`Index`对象。在创建`DataFrame`对象时，`pandas`会将所有的系列对齐到相同的索引。在这种情况下，它仅仅是行号，但我们也可以轻松地使用`time`列作为索引，这将启用一些额外的`pandas`功能，正如我们在[*第4章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)，*聚合Pandas数据框*中将看到的那样：
- en: '![Figure 2.3 – Our first dataframe'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3 – 我们的第一个数据框'
- en: '](img/Figure_2.3_B16834.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.3_B16834.jpg)'
- en: Figure 2.3 – Our first dataframe
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 我们的第一个数据框
- en: 'Our columns each have a single data type, but they don''t all share the same
    data type:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的列每一列都有单一的数据类型，但它们并非都具有相同的数据类型：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The values of the dataframe look very similar to the initial NumPy representation
    we had:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框的值看起来与我们最初的NumPy表示非常相似：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can access the column names via the `columns` attribute. Note that they
    are actually stored in an `Index` object as well:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`columns`属性访问列名。请注意，它们实际上也存储在一个`Index`对象中：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following are some commonly used dataframe attributes:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些常用的数据框属性：
- en: '![Figure 2.4 – Commonly used dataframe attributes'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.4 – 常用的数据框属性'
- en: '](img/Figure_2.4_B16834.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.4_B16834.jpg)'
- en: Figure 2.4 – Commonly used dataframe attributes
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 常用的数据框属性
- en: 'Note that we can also perform arithmetic on dataframes. For example, we can
    add `df` to itself, which will sum the numeric columns and concatenate the string
    columns:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们也可以对数据框执行算术运算。例如，我们可以将`df`加到它自己上，这将对数值列进行求和，并将字符串列进行连接：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Pandas will only perform the operation when both the index and column match.
    Here, `pandas` concatenated the string columns (`time`, `place`, `magType`, and
    `alert`) across dataframes. The numeric columns (`mag` and `tsunami`) were summed:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 只有在索引和列都匹配时才会执行操作。在这里，`pandas`将字符串类型的列（`time`、`place`、`magType` 和 `alert`）在数据框之间进行了合并。而数值类型的列（`mag`
    和 `tsunami`）则进行了求和：
- en: '![Figure 2.5 – Adding dataframes'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5 – 添加数据框'
- en: '](img/Figure_2.5_B16834.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.5_B16834.jpg)'
- en: Figure 2.5 – Adding dataframes
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 添加数据框
- en: More information on `DataFrame` objects and all the operations that can be performed
    directly on them is available in the official documentation at [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html);
    be sure to bookmark it for future reference. Now, we are ready to begin learning
    how to create `DataFrame` objects from a variety of sources.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`DataFrame`对象以及可以直接对其执行的所有操作的更多信息，请参考官方文档：[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)；请务必将其添加书签以备将来参考。现在，我们已经准备好开始学习如何从各种来源创建`DataFrame`对象。
- en: Creating a pandas DataFrame
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 pandas DataFrame
- en: Now that we understand the data structures we will be working with, we can discuss
    the different ways we can create them. Before we dive into the code however, it's
    important to know how to get help right from Python. Should we ever find ourselves
    unsure of how to use something in Python, we can utilize the built-in `help()`
    function. We simply run `help()`, passing in the package, module, class, object,
    method, or function that we want to read the documentation on. We can, of course,
    look up the documentation online; however, in most cases, the `help()` will be
    equivalent to this since they are used to generate the documentation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了将要使用的数据结构，接下来可以讨论创建它们的不同方式。然而，在深入代码之前，了解如何直接从 Python 获取帮助是非常重要的。如果我们在使用
    Python 时遇到不确定的地方，可以使用内置的`help()`函数。我们只需要运行`help()`，并传入我们想查看文档的包、模块、类、对象、方法或函数。当然，我们也可以在线查找文档；然而，在大多数情况下，`help()`与在线文档是等效的，因为它们用于生成文档。
- en: Assuming we first ran `import pandas as pd`, we can run `help(pd)` to display
    information about the `pandas` package; `help(pd.DataFrame)` for all the methods
    and attributes of `DataFrame` objects (note we can also pass in a `DataFrame`
    object instead); and `help(pd.read_csv)` to learn more about the `pandas` function
    for reading CSV files into Python and how to use it. We can also try using the
    `dir()` function and the `__dict__` attribute, which will give us a list or dictionary
    of what's available, respectively; these might not be as useful as the `help()`
    function, though.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们首先运行了`import pandas as pd`，然后可以运行`help(pd)`来显示有关`pandas`包的信息；`help(pd.DataFrame)`来查看所有关于`DataFrame`对象的方法和属性（注意，我们也可以传入一个`DataFrame`对象）；`help(pd.read_csv)`以了解有关`pandas`读取
    CSV 文件到 Python 中的函数及其使用方法。我们还可以尝试使用`dir()`函数和`__dict__`属性，它们将分别为我们提供可用项的列表或字典；不过，它们可能没有`help()`函数那么有用。
- en: 'Additionally, we can use `?` and `??` to get help, thanks to IPython, which
    is part of what makes Jupyter Notebooks so powerful. Unlike the `help()` function,
    we can use question marks by putting them after whatever we want to know more
    about, as if we were asking Python a question; for example, `pd.read_csv?` and
    `pd.read_csv??`. These three will yield slightly different outputs: `help()` will
    give us the docstring; `?` will give the docstring, plus some additional information,
    depending on what we are inquiring about; and `??` will give us even more information
    and, if possible, the source code behind it.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以使用`?`和`??`来获取帮助，这得益于 IPython，它是 Jupyter Notebooks 强大功能的一部分。与`help()`函数不同，我们可以在想要了解更多的内容后加上问号，就像在问
    Python 一个问题一样；例如，`pd.read_csv?`和`pd.read_csv??`。这三者会输出略有不同的信息：`help()`会提供文档字符串；`?`会提供文档字符串，并根据我们的查询增加一些附加信息；而`??`会提供更多信息，且在可能的情况下，还会显示源代码。
- en: 'Let''s now turn to the next notebook, `2-creating_dataframes.ipynb`, and import
    the packages we will need for the upcoming examples. We will be using `datetime`
    from the Python standard library, along with the third-party packages `numpy`
    and `pandas`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转到下一个笔记本文件`2-creating_dataframes.ipynb`，并导入我们即将使用的包。我们将使用 Python 标准库中的`datetime`，以及第三方包`numpy`和`pandas`：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Important note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We have `pandas` package by referring to it with the alias we assign to be `pd`,
    which is the most common way of importing it. In fact, we can only refer to it
    as `pd`, since that is what we imported into the namespace. Packages need to be
    imported before we can use them; installation puts the files we need on our computer,
    but, in the interest of memory, Python won't load every installed package when
    we start it up—just the ones we tell it to.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将`pandas`包引入并为其指定别名`pd`，这是导入`pandas`最常见的方式。事实上，我们只能用`pd`来引用它，因为那是我们导入到命名空间中的别名。包需要在使用之前导入；安装将所需的文件放在我们的计算机上，但为了节省内存，Python不会在启动时加载所有已安装的包——只有我们明确告诉它加载的包。
- en: We are now ready to begin using `pandas`. First, we will learn how to create
    `pandas` objects from other Python objects. Then, we will learn how to do so with
    flat files, tables in a database, and responses from API requests.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好开始使用`pandas`了。首先，我们将学习如何从其他Python对象创建`pandas`对象。接着，我们将学习如何从平面文件、数据库中的表格以及API请求的响应中创建`pandas`对象。
- en: From a Python object
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Python对象
- en: Before we cover all the ways we can create a `DataFrame` object from a Python
    object, we should learn how to make a `Series` object. Remember that a `Series`
    object is essentially a column in a `DataFrame` object, so, once we know this,
    it should be easy to understand how to create a `DataFrame` object. Say that we
    wanted to create a series of five random numbers between `0` and `1`. We could
    use NumPy to generate the random numbers as an array and create the series from
    that.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在讲解如何从Python对象创建`DataFrame`对象的所有方法之前，我们应该先了解如何创建`Series`对象。记住，`Series`对象本质上是`DataFrame`对象中的一列，因此，一旦我们掌握了这一点，理解如何创建`DataFrame`对象应该就不难了。假设我们想创建一个包含五个介于`0`和`1`之间的随机数的序列，我们可以使用NumPy生成随机数数组，并从中创建序列。
- en: Tip
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: NumPy makes it very easy to generate numerical data. Aside from generating random
    numbers, we can use it to get evenly-spaced numbers in a certain range with the
    `np.linspace()` function; obtain a range of integers with the `np.arange()` function;
    sample from the standard normal with the `np.random.normal()` function; and easily
    create arrays of all zeros with the `np.zeros()` function and all ones with the
    `np.ones()` function. We will be using NumPy throughout this book.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy使得生成数值数据变得非常简单。除了生成随机数外，我们还可以使用`np.linspace()`函数在某个范围内生成均匀分布的数值；使用`np.arange()`函数获取一系列整数；使用`np.random.normal()`函数从标准正态分布中抽样；以及使用`np.zeros()`函数轻松创建全零数组，使用`np.ones()`函数创建全一数组。本书中我们将会一直使用NumPy。
- en: 'To ensure that the result is reproducible, we will set the seed here. The `Series`
    object with any list-like structure (such as NumPy arrays):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保结果是可重复的，我们将在这里设置种子。任何具有类似列表结构的`Series`对象（例如NumPy数组）：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Making a `DataFrame` object is an extension of making a `Series` object; it
    will be composed of one or more series, and each will be distinctly named. This
    should remind us of dictionary-like structures in Python: the keys are the column
    names, and the values are the contents of the columns. Note that if we want to
    turn a single `Series` object into a `DataFrame` object, we can use its `to_frame()`
    method.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`DataFrame`对象是创建`Series`对象的扩展；它由一个或多个系列组成，每个系列都会有不同的名称。这让我们联想到Python中的字典结构：键是列名，值是列的内容。注意，如果我们想将一个单独的`Series`对象转换为`DataFrame`对象，可以使用它的`to_frame()`方法。
- en: Tip
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: In computer science, a `__init__()` method. When we run `pd.Series()`, Python
    calls `pd.Series.__init__()`, which contains instructions for instantiating a
    new `Series` object. We will learn more about the `__init__()` method in [*Chapter
    7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146), *Financial Analysis – Bitcoin
    and the Stock Market*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，`__init__()`方法。当我们运行`pd.Series()`时，Python会调用`pd.Series.__init__()`，该方法包含实例化新`Series`对象的指令。我们将在[*第7章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146)中进一步了解`__init__()`方法，*金融分析
    – 比特币与股票市场*。
- en: 'Since columns can all be different data types, let''s get a little fancy with
    this example. We are going to create a `DataFrame` object containing three columns,
    with five observations each:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于列可以是不同的数据类型，让我们通过这个例子来做一些有趣的事情。我们将创建一个包含三列、每列有五个观察值的`DataFrame`对象：
- en: '`random`: Five random numbers between `0` and `1` as a NumPy array'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random`：五个介于`0`和`1`之间的随机数，作为一个NumPy数组'
- en: '`text`: A list of five strings or `None`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`：一个包含五个字符串或`None`的列表'
- en: '`truth`: A list of five random Booleans'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truth`：一个包含五个随机布尔值的列表'
- en: We will also create a `DatetimeIndex` object with the `pd.date_range()` function.
    The index will contain five dates (`periods=5`), all one day apart (`freq='1D'`),
    ending with April 21, 2019 (`end`), and will be called `date`. Note that more
    information on the values the `pd.date_range()` function accepts for frequencies
    can be found at [https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用`pd.date_range()`函数创建一个`DatetimeIndex`对象。该索引将包含五个日期（`periods=5`），日期之间相隔一天（`freq='1D'`），并以2019年4月21日（`end`）为结束日期，索引名称为`date`。请注意，关于`pd.date_range()`函数接受的频率值的更多信息，请参见[https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)。
- en: 'All we have to do is package the columns in a dictionary using the desired
    column names as the keys and pass this in when we call the `pd.DataFrame()` constructor.
    The index gets passed as the `index` argument:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所需要做的，就是将列打包成字典，使用所需的列名作为键，并在调用`pd.DataFrame()`构造函数时传入该字典。索引通过`index`参数传递：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Important note
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: By convention, we use `_` to hold variables in a loop that we don't care about.
    Here, we use `range()` as a counter, and its values are unimportant. More information
    on the roles `_` plays in Python can be found at [https://hackernoon.com/understanding-the-underscore-of-python-309d1a029edc](https://hackernoon.com/understanding-the-underscore-of-python-309d1a029edc).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 按照约定，我们使用`_`来存放在循环中我们不关心的变量。在这里，我们使用`range()`作为计数器，其值不重要。有关`_`在Python中作用的更多信息，请参见[https://hackernoon.com/understanding-the-underscore-of-python-309d1a029edc](https://hackernoon.com/understanding-the-underscore-of-python-309d1a029edc)。
- en: 'Having dates in the index makes it easy to select entries by date (or even
    in a date range), as we will see in [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引中包含日期，使得通过日期（甚至日期范围）选择条目变得容易，正如我们在[*第3章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)《Pandas数据处理》中将看到的那样：
- en: '![Figure 2.6 – Creating a dataframe from a dictionary'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.6 – 从字典创建数据框'
- en: '](img/Figure_2.6_B16834.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.6_B16834.jpg)'
- en: Figure 2.6 – Creating a dataframe from a dictionary
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 从字典创建数据框
- en: 'In cases where the data isn''t a dictionary, but rather a list of dictionaries,
    we can still use `pd.DataFrame()`. Data in this format is what we would expect
    when consuming from an API. Each entry in the list will be a dictionary, where
    the keys of the dictionary are the column names and the values of the dictionary
    are the values for that column at that index:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据不是字典而是字典列表的情况下，我们仍然可以使用`pd.DataFrame()`。这种格式的数据通常来自API。当数据以这种格式时，列表中的每个条目将是一个字典，字典的键是列名，字典的值是该索引处该列的值：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This gives us a dataframe of three rows (one for each entry in the list) with
    two columns (one for each key in the dictionaries):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个包含三行（每个列表条目对应一行）和两列（每个字典的键对应一列）的数据框：
- en: '![Figure 2.7 – Creating a dataframe from a list of dictionaries'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.7 – 从字典列表创建数据框'
- en: '](img/Figure_2.7_B16834.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.7_B16834.jpg)'
- en: Figure 2.7 – Creating a dataframe from a list of dictionaries
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – 从字典列表创建数据框
- en: 'In fact, `pd.DataFrame()` also works for lists of tuples. Note that we can
    also pass in the column names as a list through the `columns` argument:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，`pd.DataFrame()`也适用于元组列表。注意，我们还可以通过`columns`参数将列名作为列表传入：
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Each tuple is treated like a record and becomes a row in the dataframe:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 每个元组被当作记录处理，并成为数据框中的一行：
- en: '![Figure 2.8 – Creating a dataframe from a list of tuples'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.8 – 从元组列表创建数据框'
- en: '](img/Figure_2.8_B16834.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.8_B16834.jpg)'
- en: Figure 2.8 – Creating a dataframe from a list of tuples
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 – 从元组列表创建数据框
- en: 'We also have the option of using `pd.DataFrame()` with NumPy arrays:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以选择使用`pd.DataFrame()`与NumPy数组：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This will have the effect of stacking each entry in the array as rows in a dataframe,
    giving us a result that's identical to *Figure 2.8*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这样会将数组中的每个条目按行堆叠到数据框中，得到的结果与*图 2.8*完全相同。
- en: From a file
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从文件
- en: The data we want to analyze will most often come from outside Python. In many
    cases, we may obtain a **data dump** from a database or website and bring it into
    Python to sift through it. A data dump gets its name from containing a large amount
    of data (possibly at a very granular level) and often not discriminating against
    any of it initially; for this reason, they can be unwieldy.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要分析的数据大多数来自 Python 之外。在很多情况下，我们可能会从数据库或网站获得一个**数据转储**，然后将其带入 Python 进行筛选。数据转储之所以得名，是因为它包含大量数据（可能是非常详细的层次），且最初往往不加区分；因此，它们可能显得笨重。
- en: Often, these data dumps will come in the form of a text file (`.txt`) or a CSV
    file (`.csv`). Pandas provides many methods for reading in different types of
    files, so it is simply a matter of looking up the one that matches our file format.
    Our earthquake data is a CSV file; therefore, we use the `pd.read_csv()` function
    to read it in. However, we should always do an initial inspection of the file
    before attempting to read it in; this will inform us of whether we need to pass
    additional arguments, such as `sep` to specify the delimiter or `names` to provide
    the column names ourselves in the absence of a header row in the file.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些数据转储会以文本文件（`.txt`）或CSV文件（`.csv`）的形式出现。Pandas提供了许多读取不同类型文件的方法，因此我们只需查找匹配我们文件格式的方法即可。我们的地震数据是CSV文件，因此我们使用`pd.read_csv()`函数来读取它。然而，在尝试读取之前，我们应始终先进行初步检查；这将帮助我们确定是否需要传递其他参数，比如`sep`来指定分隔符，或`names`来在文件没有表头行的情况下手动提供列名。
- en: Important note
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '**Windows users**: Depending on your setup, the commands in the next few code
    blocks may not work. The notebook contains alternatives if you encounter issues.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows 用户**：根据您的设置，接下来的代码块中的命令可能无法正常工作。如果遇到问题，笔记本中有替代方法。'
- en: 'We can perform our due diligence directly in our Jupyter Notebook thanks to
    IPython, provided we prefix our commands with `!` to indicate they are to be run
    as shell commands. First, we should check how big the file is, both in terms of
    lines and in terms of bytes. To check the number of lines, we use the `wc` utility
    (word count) with the `–l` flag to count the number of lines. We have 9,333 rows
    in the file:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接在Jupyter Notebook中进行尽职调查，得益于IPython，只需在命令前加上`!`，表示这些命令将作为Shell命令执行。首先，我们应该检查文件的大小，既要检查行数，也要检查字节数。要检查行数，我们使用`wc`工具（单词计数）并加上`-l`标志来计算行数。我们文件中有9,333行：
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, let''s check the file''s size. For this task, we will use `ls` on the
    `data` directory. This will show us the list of files in that directory. We can
    add the `-lh` flag to get information about the files in a human-readable format.
    Finally, we send this output to the `grep` utility, which will help us isolate
    the files we want. This tells us that the `earthquakes.csv` file is 3.4 MB:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查一下文件的大小。为此，我们将使用`ls`命令查看`data`目录中的文件列表。我们可以添加`-lh`标志，以便以易于阅读的格式获取文件信息。最后，我们将此输出发送到`grep`工具，它将帮助我们筛选出我们想要的文件。这告诉我们，`earthquakes.csv`文件的大小为3.4
    MB：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Note that IPython also lets us capture the result of the command in a Python
    variable, so if we aren''t comfortable with pipes (`|`) or `grep`, we can do the
    following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，IPython还允许我们将命令的结果捕获到Python变量中，因此，如果我们不熟悉管道符（`|`）或`grep`，我们可以这样做：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, let''s take a look at the top few rows to see if the file comes with headers.
    We will use the `head` utility and specify the number of rows with the `-n` flag.
    This tells us that the first row contains the headers for the data and that the
    data is delimited with commas (just because the file has the `.csv` extension
    does not mean it is comma-delimited):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下文件的顶部几行，看看文件是否包含表头。我们将使用`head`工具，并通过`-n`标志指定行数。这告诉我们，第一行包含数据的表头，并且数据是以逗号分隔的（仅仅因为文件扩展名是`.csv`并不意味着它是逗号分隔的）：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that we should also check the bottom rows to make sure there is no extraneous
    data that we will need to ignore by using the `tail` utility. This file is fine,
    so the result won't be reproduced here; however, the notebook contains the result.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还应该检查文件的底部几行，以确保没有多余的数据需要通过`tail`工具忽略。这个文件没有问题，因此结果不会在此处重复；不过，笔记本中包含了结果。
- en: 'Lastly, we may be interested in seeing the column count in our data. While
    we could just count the fields in the first row of the result of `head`, we have
    the option of using the `awk` utility (for pattern scanning and processing) to
    count our columns. The `-F` flag allows us to specify the delimiter (a comma,
    in this case). Then, we specify what to do for each record in the file. We choose
    to print `NF`, which is a predefined variable whose value is the number of fields
    in the current record. Here, we say `exit` immediately after the print so that
    we print the number of fields in the first row of the file; then, we stop. This
    will look a little complicated, but this is by no means something we need to memorize:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可能对查看数据中的列数感兴趣。虽然我们可以仅通过计算`head`命令结果的第一行中的字段数来实现，但我们也可以选择使用`awk`工具（用于模式扫描和处理）来计算列数。`-F`标志允许我们指定分隔符（在这种情况下是逗号）。然后，我们指定对文件中的每个记录执行的操作。我们选择打印`NF`，这是一个预定义变量，其值是当前记录中字段的数量。在这里，我们在打印之后立即使用`exit`，以便只打印文件中第一行的字段数，然后停止。这看起来有点复杂，但这绝不是我们需要记住的内容：
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Since we know that the first line of the file contains headers and that the
    file is comma-separated, we can also count the columns by using `head` to get
    the headers and Python to parse them:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道文件的第一行包含标题，并且该文件是逗号分隔的，我们也可以通过使用`head`获取标题并用Python解析它们来计算列数：
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Important note
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The ability to run shell commands directly from our Jupyter Notebook dramatically
    streamlines our workflow. However, if we don't have past experience with the command
    line, it may be complicated to learn these commands initially. IPython has some
    helpful information on running shell commands in their documentation at [https://ipython.readthedocs.io/en/stable/interactive/reference.html#system-shell-access](https://ipython.readthedocs.io/en/stable/interactive/reference.html#system-shell-access).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 直接在Jupyter Notebook中运行Shell命令极大地简化了我们的工作流程。然而，如果我们没有命令行的经验，最初学习这些命令可能会很复杂。IPython的文档提供了一些关于运行Shell命令的有用信息，您可以在[https://ipython.readthedocs.io/en/stable/interactive/reference.html#system-shell-access](https://ipython.readthedocs.io/en/stable/interactive/reference.html#system-shell-access)找到。
- en: 'To summarize, we now know that the file is 3.4 MB and is comma-delimited with
    26 columns and 9,333 rows, with the first one being the header. This means that
    we can use the `pd.read_csv()` function with the defaults:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们现在知道文件大小为3.4MB，使用逗号分隔，共有26列和9,333行，第一行是标题。这意味着我们可以使用带有默认设置的`pd.read_csv()`函数：
- en: '[PRE28]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Note that we aren''t limited to reading in data from files on our local machines;
    file paths can be URLs as well. As an example, let''s read in the same CSV file
    from GitHub:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不仅仅局限于从本地机器上的文件读取数据；文件路径也可以是URL。例如，我们可以从GitHub读取相同的CSV文件：
- en: '[PRE29]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Pandas is usually very good at figuring out which options to use based on the
    input data, so we often won''t need to add arguments to this call; however, there
    are many options available should we need them, some of which include the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas通常非常擅长根据输入数据自动判断需要使用的选项，因此我们通常不需要为此调用添加额外的参数；然而，若有需要，仍有许多选项可以使用，其中包括以下几种：
- en: '![Figure 2.9 – Helpful parameters when reading data from a file'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.9 – 读取文件时有用的参数'
- en: '](img/Figure_2.9_B16834.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.9_B16834.jpg)'
- en: Figure 2.9 – Helpful parameters when reading data from a file
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 – 读取文件时有用的参数
- en: Throughout this book, we will be working with CSV files; however, note that
    we can use the `read_excel()` function to read in Excel files, the `read_json()`
    function for `\t`), we can use the `read_csv()` function with the `sep` argument
    equal to the delimiter.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，我们将处理CSV文件；但请注意，我们也可以使用`read_excel()`函数读取Excel文件，使用`read_json()`函数读取`json`文件，或者使用带有`sep`参数的`read_csv()`函数来处理不同的分隔符。
- en: 'It would be remiss if we didn''t also learn how to save our dataframe to a
    file so that we can share it with others. To write our dataframe to a CSV file,
    we call its `to_csv()` method. We have to be careful here; if our dataframe''s
    index is just row numbers, we probably don''t want to write that to our file (it
    will have no meaning to the consumers of the data), but it is the default. We
    can write our data without the index by passing in `index=False`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不学习如何将数据框保存到文件中，以便与他人分享，那将是失职。为了将数据框写入CSV文件，我们调用其`to_csv()`方法。在这里我们必须小心；如果数据框的索引只是行号，我们可能不想将其写入文件（对数据的使用者没有意义），但这是默认设置。我们可以通过传入`index=False`来写入不包含索引的数据：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As with reading from files, `Series` and `DataFrame` objects have methods to
    write data to Excel (`to_excel()`) and JSON files (`to_json()`). Note that, while
    we use functions from `pandas` to read our data in, we must use methods to write
    our data; the reading functions create the `pandas` objects that we want to work
    with, but the writing methods are actions that we take using the `pandas` object.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与从文件中读取数据一样，`Series` 和 `DataFrame` 对象也有方法将数据写入 Excel（`to_excel()`）和 JSON 文件（`to_json()`）。请注意，虽然我们使用
    `pandas` 中的函数来读取数据，但我们必须使用方法来写入数据；读取函数创建了我们想要处理的 `pandas` 对象，而写入方法则是我们使用 `pandas`
    对象执行的操作。
- en: Tip
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The preceding file paths to read from and write to were `/home/myuser/learning/hands_on_pandas/data.csv`
    and our current directory is `/home/myuser/learning/hands_on_pandas`, then we
    can simply use the relative path of `data.csv` as the file path.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 上述读取和写入的文件路径是 `/home/myuser/learning/hands_on_pandas/data.csv`，而我们当前的工作目录是 `/home/myuser/learning/hands_on_pandas`，因此我们可以简单地使用
    `data.csv` 的相对路径作为文件路径。
- en: 'Pandas provides us with capabilities to read and write from many other data
    sources, including databases, which we will discuss next; pickle files (containing
    serialized Python objects—see the *Further reading* section for more information);
    and HTML pages. Be sure to check out the following resource in the `pandas` documentation
    for the full list of capabilities: [https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 提供了从许多其他数据源读取和写入的功能，包括数据库，我们接下来会讨论这些内容；pickle 文件（包含序列化的 Python 对象——有关更多信息，请参见
    *进一步阅读* 部分）；以及 HTML 页面。请务必查看 `pandas` 文档中的以下资源，以获取完整的功能列表：[https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)。
- en: From a database
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从数据库中读取
- en: Pandas can interact with SQLite databases without the need for us to install
    any additional packages; however, the SQLAlchemy package needs to be installed
    in order to interact with other database flavors. Interaction with a SQLite database
    can be achieved by opening a connection to the database using the `sqlite3` module
    in the Python standard library and then using either the `pd.read_sql()` function
    to query the database or the `to_sql()` method on a `DataFrame` object to write
    it to the database.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 可以与 SQLite 数据库进行交互，而无需安装任何额外的软件包；不过，若要与其他类型的数据库进行交互，则需要安装 SQLAlchemy
    包。与 SQLite 数据库的交互可以通过使用 Python 标准库中的 `sqlite3` 模块打开数据库连接来实现，然后使用 `pd.read_sql()`
    函数查询数据库，或在 `DataFrame` 对象上使用 `to_sql()` 方法将数据写入数据库。
- en: 'Before we read from a database, let''s write to one. We simply call `to_sql()`
    on our dataframe, telling it which table to write to, which database connection
    to use, and how to handle if the table already exists. There is already a SQLite
    database in the folder for this chapter in this book''s GitHub repository: `data/quakes.db`.
    Note that, to create a new database, we can change `''data/quakes.db''` to the
    path for the new database file. Let''s write the tsunami data from the `data/tsunamis.csv`
    file to a table in the database called `tsunamis`, replacing the table if it already
    exists:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们从数据库中读取数据之前，先来写入数据。我们只需在我们的 DataFrame 上调用 `to_sql()`，并告诉它要写入哪个表，使用哪个数据库连接，以及如果表已存在该如何处理。本书
    GitHub 仓库中的这一章节文件夹里已经有一个 SQLite 数据库：`data/quakes.db`。请注意，要创建一个新的数据库，我们可以将 `'data/quakes.db'`
    更改为新数据库文件的路径。现在让我们把 `data/tsunamis.csv` 文件中的海啸数据写入名为 `tsunamis` 的数据库表中，如果表已存在，则替换它：
- en: '[PRE31]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Querying the database is just as easy as writing to it. Note this will require
    knowledge of `pandas` compares to SQL and [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*, for some examples of how `pandas` actions relate
    to SQL statements.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 查询数据库与写入数据库一样简单。请注意，这需要了解 `pandas` 与 SQL 的对比关系，并且可以参考 [*第 4 章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)，*聚合
    Pandas DataFrames*，了解一些 `pandas` 操作与 SQL 语句的关系示例。
- en: 'Let''s query our database for the full `tsunamis` table. When we write a SQL
    query, we first state the columns that we want to select, which in our case is
    all of them, so we write `"SELECT *"`. Next, we state the table to select the
    data from, which for us is `tsunamis`, so we add `"FROM tsunamis"`. This is our
    full query now (of course, it can get much more complicated than this). To actually
    query the database, we use `pd.read_sql()`, passing in our query and the database
    connection:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查询数据库中的完整`tsunamis`表。当我们编写SQL查询时，首先声明我们要选择的列，在本例中是所有列，因此我们写`"SELECT *"`。接下来，我们声明要从哪个表中选择数据，在我们这里是`tsunamis`，因此我们写`"FROM
    tsunamis"`。这就是我们完整的查询（当然，它可以比这更复杂）。要实际查询数据库，我们使用`pd.read_sql()`，传入查询和数据库连接：
- en: '[PRE32]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We now have the tsunamis data in a dataframe:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在数据框中已经有了海啸数据：
- en: '![Figure 2.10 – Reading data from a database'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.10 – 从数据库读取数据'
- en: '](img/Figure_2.10_B16834.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.10_B16834.jpg)'
- en: Figure 2.10 – Reading data from a database
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 – 从数据库读取数据
- en: Important note
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The `connection` object we created in both code blocks is an example of a `with`
    statement, automatically handles cleanup after the code in the block executes
    (closing the connection, in this case). This makes cleanup easy and makes sure
    we don't leave any loose ends. Be sure to check out `contextlib` from the standard
    library for utilities using the `with` statement and context managers. The documentation
    is at [https://docs.python.org/3/library/contextlib.html](https://docs.python.org/3/library/contextlib.html).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个代码块中创建的`connection`对象是`with`语句的一个示例，自动在代码块执行后进行清理（在本例中是关闭连接）。这使得清理工作变得简单，并确保我们不会留下任何未完成的工作。一定要查看标准库中的`contextlib`，它提供了使用`with`语句和上下文管理器的工具。文档请参考
    [https://docs.python.org/3/library/contextlib.html](https://docs.python.org/3/library/contextlib.html)。
- en: From an API
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自API
- en: We can now easily create `Series` and `DataFrame` objects from data we have
    in Python or from files we obtain, but how can we get data from online resources,
    such as APIs? There is no guarantee that each data source will give us data in
    the same format, so we must remain flexible in our approach and be comfortable
    examining the data source to find the appropriate import method. In this section,
    we will request some earthquake data from the USGS API and see how we can make
    a dataframe out of the result. In [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*, we will work with another API to gather weather
    data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以轻松地从Python中的数据或从获得的文件中创建`Series`和`DataFrame`对象，但如何从在线资源（如API）获取数据呢？无法保证每个数据源都会以相同的格式提供数据，因此我们必须在方法上保持灵活，并能够检查数据源以找到合适的导入方法。在本节中，我们将从USGS
    API请求一些地震数据，并查看如何从结果中创建数据框。在[*第3章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)《使用Pandas进行数据清理》中，我们将使用另一个API收集天气数据。
- en: 'For this section, we will be working in the `3-making_dataframes_from_api_requests.ipynb`
    notebook, so we have to import the packages we need once again. As with the previous
    notebook, we need `pandas` and `datetime`, but we also need the `requests` package
    to make API requests:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在`3-making_dataframes_from_api_requests.ipynb`笔记本中工作，因此我们需要再次导入所需的包。与之前的笔记本一样，我们需要`pandas`和`datetime`，但我们还需要`requests`包来发起API请求：
- en: '[PRE33]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we will make a `GET` request to the USGS API for a JSON payload (a dictionary-like
    response containing the data that''s sent with a request or response) by specifying
    the format of `geojson`. We will ask for earthquake data for the last 30 days
    (we can use `dt.timedelta` to perform arithmetic on `datetime` objects). Note
    that we are using `yesterday` as the end of our date range, since the API won''t
    have complete information for today yet:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向USGS API发起`GET`请求，获取一个JSON负载（包含请求或响应数据的类似字典的响应），并指定`geojson`格式。我们将请求过去30天的地震数据（可以使用`dt.timedelta`对`datetime`对象进行运算）。请注意，我们将`yesterday`作为日期范围的结束日期，因为API尚未提供今天的完整数据：
- en: '[PRE34]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Important note
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: '`GET` is an HTTP method. This action tells the server we want to read some
    data. Different APIs may require that we use different methods to get the data;
    some will require a `POST` request, where we authenticate with the server. You
    can read more about API requests and HTTP methods at [https://nordicapis.com/ultimate-guide-to-all-9-standard-http-methods/](https://nordicapis.com/ultimate-guide-to-all-9-standard-http-methods/).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`GET` 是一种 HTTP 方法。这个操作告诉服务器我们想要读取一些数据。不同的 API 可能要求我们使用不同的方法来获取数据；有些会要求我们发送
    `POST` 请求，在其中进行身份验证。你可以在[https://nordicapis.com/ultimate-guide-to-all-9-standard-http-methods/](https://nordicapis.com/ultimate-guide-to-all-9-standard-http-methods/)上了解更多关于
    API 请求和 HTTP 方法的信息。'
- en: 'Before we try to create a dataframe out of this, we should make sure that our
    request was successful. We can do this by checking the `status_code` attribute
    of the `response` object. A listing of status codes and their meanings can be
    found at [https://en.wikipedia.org/wiki/List_of_HTTP_status_codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes).
    A `200` response will indicate that everything is OK:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们尝试从中创建 dataframe 之前，应该先确认我们的请求是否成功。我们可以通过检查`response`对象的`status_code`属性来做到这一点。状态码及其含义的列表可以在[https://en.wikipedia.org/wiki/List_of_HTTP_status_codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)找到。`200`响应将表示一切正常：
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Our request was successful, so let''s see what the data we got looks like.
    We asked the API for a JSON payload, which is essentially a dictionary, so we
    can use dictionary methods on it to get more information about its structure.
    This is going to be a lot of data; hence, we don''t want to print it to the screen
    just to inspect it. We need to isolate the JSON payload from the HTTP response
    (stored in the `response` variable), and then look at the keys to view the main
    sections of the resulting data:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的请求成功了，接下来让我们看看我们得到的数据是什么样的。我们请求了一个 JSON 负载，它本质上是一个字典，因此我们可以使用字典方法来获取更多关于它结构的信息。这将是大量的数据；因此，我们不想只是将它打印到屏幕上进行检查。我们需要从
    HTTP 响应（存储在`response`变量中）中提取 JSON 负载，然后查看键以查看结果数据的主要部分：
- en: '[PRE36]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can inspect what kind of data we have as values for each of these keys;
    one of them will be the data we are after. The `metadata` portion tells us some
    information about our request. While this can certainly be useful, it isn''t what
    we are after right now:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查这些键对应的值是什么样的数据；其中一个将是我们需要的数据。`metadata`部分告诉我们一些关于请求的信息。虽然这些信息确实有用，但它不是我们现在需要的：
- en: '[PRE37]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `features` key looks promising; if this does indeed contain all our data,
    we should check what type it is so that we don''t end up trying to print everything
    to the screen:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`features` 键看起来很有前景；如果它确实包含了我们所有的数据，我们应该检查它的数据类型，以避免试图将所有内容打印到屏幕上：'
- en: '[PRE38]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This key contains a list, so let''s take a look at the first entry to see if
    this is the data we want. Note that the USGS data may be altered or added to for
    dates in the past as more information on the earthquakes comes to light, meaning
    that querying for the same date range may yield a different number of results
    later on. For this reason, the following is an example of what an entry looks
    like:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这个键包含一个列表，所以让我们查看第一个条目，看看这是不是我们想要的数据。请注意，USGS 数据可能会随着更多关于地震信息的披露而被修改或添加，因此查询相同的日期范围可能会得到不同数量的结果。基于这个原因，以下是一个条目的示例：
- en: '[PRE39]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This is definitely the data we are after, but do we need all of it? Upon closer
    inspection, we only really care about what is inside the `properties` dictionary.
    Now, we have a problem because we have a list of dictionaries where we only want
    a specific key from inside them. How can we pull this information out so that
    we can make our dataframe? We can use a list comprehension to isolate the `properties`
    section from each of the dictionaries in the `features` list:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝对是我们需要的数据，但我们需要全部数据吗？仔细检查后，我们只关心`properties`字典中的内容。现在，我们面临一个问题，因为我们有一个字典的列表，而我们只需要从中提取一个特定的键。我们该如何提取这些信息，以便构建我们的
    dataframe 呢？我们可以使用列表推导式从`features`列表中的每个字典中隔离出`properties`部分：
- en: '[PRE40]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Finally, we are ready to create our dataframe. Pandas knows how to handle data
    in this format already (a list of dictionaries), so all we have to do is pass
    in the data when we call `pd.DataFrame()`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备创建我们的 dataframe。Pandas 已经知道如何处理这种格式的数据（字典列表），因此我们只需要在调用`pd.DataFrame()`时传入数据：
- en: '[PRE41]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Now that we know how to create dataframes from a variety of sources, we can
    start learning how to work with them.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何从各种数据源创建 dataframes，我们可以开始学习如何操作它们。
- en: Inspecting a DataFrame object
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查一个 DataFrame 对象
- en: The first thing we should do when we read in our data is inspect it; we want
    to make sure that our dataframe isn't empty and that the rows look as we would
    expect. Our main goal is to verify that it was read in properly and that all the
    data is there; however, this initial inspection will also give us ideas with regard
    to where we should direct our data wrangling efforts. In this section, we will
    explore ways in which we can inspect our dataframes in the `4-inspecting_dataframes.ipynb`
    notebook.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取数据时应该做的第一件事就是检查它；我们需要确保数据框不为空，并且行数据符合预期。我们的主要目标是验证数据是否正确读取，并且所有数据都存在；然而，这次初步检查还会帮助我们了解应将数据处理工作重点放在哪里。在本节中，我们将探索如何在`4-inspecting_dataframes.ipynb`笔记本中检查数据框。
- en: 'Since this is a new notebook, we must once again handle our setup. This time,
    we need to import `pandas` and `numpy`, as well as read in the CSV file with the
    earthquake data:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个新笔记本，我们必须再次处理设置。此次，我们需要导入`pandas`和`numpy`，并读取包含地震数据的CSV文件：
- en: '[PRE42]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Examining the data
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查数据
- en: 'First, we want to make sure that we actually have data in our dataframe. We
    can check the `empty` attribute to find out:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们要确保数据框中确实有数据。我们可以检查`empty`属性来了解情况：
- en: '[PRE43]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'So far, so good; we have data. Next, we should check how much data we read
    in; we want to know the number of observations (rows) and the number of variables
    (columns) we have. For this task, we use the `shape` attribute. Our data contains
    9,332 observations of 26 variables, which matches our initial inspection of the
    file:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利；我们有数据。接下来，我们应检查读取了多少数据；我们想知道观察数（行数）和变量数（列数）。为此，我们使用`shape`属性。我们的数据包含9,332个观察值和26个变量，这与我们最初检查文件时的结果一致：
- en: '[PRE44]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, let''s use the `columns` attribute to see the names of the columns in
    our dataset:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`columns`属性查看数据集中列的名称：
- en: '[PRE45]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Important note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Having a list of columns doesn't necessarily mean that we know what all of them
    mean. Especially in cases where our data comes from the Internet, be sure to read
    up on what the columns mean before drawing any conclusions. Information on the
    fields in the `geojson` format, including what each field in the JSON payload
    means (along with some example values), can be found on the USGS website at [https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php](https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有列的列表并不意味着我们知道每一列的含义。特别是在数据来自互联网的情况下，在得出结论之前，务必查阅列的含义。有关`geojson`格式中字段的信息，包括每个字段在JSON负载中的含义（以及一些示例值），可以在美国地质调查局（USGS）网站上的[https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php](https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php)找到。
- en: 'We know the dimensions of our data, but what does it actually look like? For
    this task, we can use the `head()` and `tail()` methods to look at the top and
    bottom rows, respectively. This will default to five rows, but we can change this
    by passing a different number to the method. Let''s take a look at the first few
    rows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道数据的维度，但它实际是什么样的呢？为此，我们可以使用`head()`和`tail()`方法，分别查看顶部和底部的行。默认情况下，这将显示五行数据，但我们可以通过传入不同的数字来更改这一设置。让我们看看前几行数据：
- en: '[PRE46]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The following are the first five rows we get using `head()`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们使用`head()`方法获得的前五行：
- en: '![Figure 2.11 – Examining the top five rows of a dataframe'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.11 – 检查数据框的前五行'
- en: '](img/Figure_2.11_B16834.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.11_B16834.jpg)'
- en: Figure 2.11 – Examining the top five rows of a dataframe
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11 – 检查数据框的前五行
- en: 'To get the last two rows, we use the `tail()` method and pass `2` as the number
    of rows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取最后两行，我们使用`tail()`方法并传入`2`作为行数：
- en: '[PRE47]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The following is the result:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '![Figure 2.12 – Examining the bottom two rows of a dataframe'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.12 – 检查数据框的底部两行'
- en: '](img/Figure_2.12_B16834.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.12_B16834.jpg)'
- en: Figure 2.12 – Examining the bottom two rows of a dataframe
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12 – 检查数据框的底部两行
- en: Tip
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: By default, when we print dataframes with many columns in a Jupyter Notebook,
    only a subset of them will be displayed. This is because `pandas` has a limit
    on the number of columns it will show. We can modify this behavior using `pd.set_option('display.max_columns',
    <new_value>)`. Consult the documentation at [https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html](https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html)
    for additional information. The notebook also contains a few example commands.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，当我们在Jupyter Notebook中打印包含许多列的数据框时，只有一部分列会显示出来。这是因为`pandas`有一个显示列数的限制。我们可以使用`pd.set_option('display.max_columns',
    <new_value>)`来修改此行为。有关更多信息，请查阅[https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html](https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html)。该文档中还包含了一些示例命令。
- en: 'We can use the `dtypes` attribute to see the data types of the columns, which
    makes it easy to see when columns are being stored as the wrong type. (Remember
    that strings will be stored as `object`.) Here, the `time` column is stored as
    an integer, which is something we will learn how to fix in [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`dtypes`属性查看各列的数据类型，这样可以轻松地发现哪些列被错误地存储为不正确的类型。（记住，字符串会被存储为`object`。）这里，`time`列被存储为整数，这是我们将在[*第3章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)《数据清洗与
    Pandas》中学习如何修复的问题：
- en: '[PRE48]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Lastly, we can use the `info()` method to see how many non-null entries of
    each column we have and get information on our index. `pandas`, will typically
    be represented as `None` for objects and `NaN` (`float` or `integer` column:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`info()`方法查看每列中有多少非空条目，并获取关于索引的信息。`pandas`通常会将对象类型的值表示为`None`，而`NaN`（`float`或`integer`类型的列）表示缺失值：
- en: '[PRE49]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: After this initial inspection, we know a lot about the structure of our data
    and can now begin to try and make sense of it.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步检查之后，我们已经了解了数据的结构，现在可以开始尝试理解数据的含义。
- en: Describing and summarizing the data
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述和总结数据
- en: 'So far, we''ve examined the structure of the `DataFrame` object we created
    from the earthquake data, but we don''t know anything about the data other than
    what a couple of rows look like. The next step is to calculate summary statistics,
    which will help us get to know our data better. Pandas provides several methods
    for easily doing so; one such method is `describe()`, which also works on `Series`
    objects if we are only interested in a particular column. Let''s get a summary
    of the numeric columns in our data:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经检查了从地震数据创建的`DataFrame`对象的结构，但除了几行数据的样子，我们对数据一无所知。接下来的步骤是计算总结统计数据，这将帮助我们更好地了解数据。Pandas提供了几种方法来轻松实现这一点；其中一种方法是`describe()`，如果我们只对某一列感兴趣，它也适用于`Series`对象。让我们获取数据中数字列的总结：
- en: '[PRE50]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This gives us the 5-number summary, along with the count, mean, and standard
    deviation of the numeric columns:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这会为我们提供5个数字总结，以及数字列的计数、均值和标准差：
- en: '![Figure 2.13 – Calculating summary statistics'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.13 – 计算总结统计数据'
- en: '](img/Figure_2.13_B16834.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.13_B16834.jpg)'
- en: Figure 2.13 – Calculating summary statistics
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13 – 计算总结统计数据
- en: Tip
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: If we want different percentiles, we can pass them in with the `percentiles`
    argument. For example, if we wanted only the 5th and 95th percentiles, we would
    run `df.describe(percentiles=[0.05, 0.95])`. Note we will still get the 50th percentile
    back because that is the median.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要不同的百分位数，可以通过`percentiles`参数传递它们。例如，如果我们只想要5%和95%的百分位数，我们可以运行`df.describe(percentiles=[0.05,
    0.95])`。请注意，我们仍然会得到第50个百分位数的结果，因为那是中位数。
- en: 'By default, `describe()` won''t give us any information about the columns of
    type `object`, but we can either provide `include=''all''` as an argument or run
    it separately for the data of type `np.object`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`describe()`不会提供关于`object`类型列的任何信息，但我们可以提供`include='all'`作为参数，或者单独运行它来查看`np.object`类型的数据：
- en: '[PRE51]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'When describing non-numeric data, we still get the count of non-null occurrences
    (**count**); however, instead of the other summary statistics, we get the number
    of unique values (**unique**), the mode (**top**), and the number of times the
    mode was observed (**freq**):'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当描述非数字数据时，我们仍然可以得到非空出现的计数（**count**）；然而，除了其他总结统计数据外，我们会得到唯一值的数量（**unique**）、众数（**top**）以及众数出现的次数（**freq**）：
- en: '![Figure 2.14 – Summary statistics for categorical columns'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.14 – 类别列的总结统计数据'
- en: '](img/Figure_2.14_B16834.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.14_B16834.jpg)'
- en: Figure 2.14 – Summary statistics for categorical columns
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14 – 类别列的总结统计数据
- en: Important note
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The `describe()` method only gives us summary statistics for non-null values.
    This means that, if we had 100 rows and half of our data was null, then the average
    would be calculated as the sum of the 50 non-null rows divided by 50.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe()` 方法只会为非空值提供摘要统计信息。这意味着，如果我们有100行数据，其中一半是空值，那么平均值将是50个非空行的总和除以50。'
- en: 'It is easy to get a snapshot of our data using the `describe()` method, but
    sometimes, we just want a particular statistic, either for a specific column or
    for all the columns. Pandas makes this a cinch as well. The following table includes
    methods that will work for both `Series` and `DataFrame` objects:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `describe()` 方法可以轻松获取数据的快照，但有时我们只想要某个特定的统计数据，不论是针对某一列还是所有列。Pandas 也使得这变得非常简单。下表列出了适用于
    `Series` 和 `DataFrame` 对象的方法：
- en: '![Figure 2.15 – Helpful calculation methods for series and dataframes'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.15 – 对系列和数据框架的有用计算方法'
- en: '](img/Figure_2.15_B16834.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.15_B16834.jpg)'
- en: Figure 2.15 – Helpful calculation methods for series and dataframes
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 2.15 – 对系列和数据框架的有用计算方法
- en: Tip
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Python makes it easy to count how many times something is `True`. Under the
    hood, `True` evaluates to `1` and `False` evaluates to `0`. Therefore, we can
    run the `sum()` method on a series of Booleans and get the count of `True` outputs.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Python 使得计算某个条件为 `True` 的次数变得容易。在底层，`True` 计算为 `1`，`False` 计算为 `0`。因此，我们可以对布尔值序列运行
    `sum()` 方法，得到 `True` 输出的计数。
- en: 'With `Series` objects, we have some additional methods for describing our data:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `Series` 对象，我们有一些额外的方法来描述我们的数据：
- en: '`unique()`: Returns the distinct values of the column.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unique()`: 返回列中的不同值。'
- en: '`value_counts()`: Returns a frequency table of the number of times each unique
    value in a given column appears, or, alternatively, the percentage of times each
    unique value appears when passed `normalize=True`.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value_counts()`: 返回给定列中每个唯一值出现的频率表，或者，当传入`normalize=True`时，返回每个唯一值出现的百分比。'
- en: '`mode()`: Returns the most common value of the column.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode()`: 返回列中最常见的值。'
- en: Consulting the USGS API documentation for the `alert` field (which can be found
    at [https://earthquake.usgs.gov/data/comcat/data-eventterms.php#alert](https://earthquake.usgs.gov/data/comcat/data-eventterms.php#alert))
    tells us that it can be `'green'`, `'yellow'`, `'orange'`, or `'red'` (when populated),
    and that it is the alert level from the `alert` column is a string of two unique
    values and that the most common value is `'green'`, with many null values. What
    is the other unique value, though?
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 查阅 USGS API 文档中的 `alert` 字段（可以在 [https://earthquake.usgs.gov/data/comcat/data-eventterms.php#alert](https://earthquake.usgs.gov/data/comcat/data-eventterms.php#alert)
    找到）告诉我们，`alert` 字段的值可以是 `'green'`、`'yellow'`、`'orange'` 或 `'red'`（当字段被填充时），并且
    `alert` 列中的警报级别是两个唯一值的字符串，其中最常见的值是 `'green'`，但也有许多空值。那么，另一个唯一值是什么呢？
- en: '[PRE52]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now that we understand what this field means and the values we have in our
    data, we expect there to be far more `''green''` than `''red''`; we can check
    our intuition with a frequency table by using `value_counts()`. Notice that we
    only get counts for the non-null entries:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了该字段的含义以及数据中包含的值，我们预计 `'green'` 的数量会远远大于 `'red'`；我们可以通过使用 `value_counts()`
    来检查我们的直觉，得到一个频率表。注意，我们只会得到非空条目的计数：
- en: '[PRE53]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Note that `Index` objects also have several methods that can help us describe
    and summarize our data:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`Index` 对象也有多个方法，能够帮助我们描述和总结数据：
- en: '![Figure 2.16 – Helpful methods for the index'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.16 – 对索引的有用方法'
- en: '](img/Figure_2.16_B16834.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.16_B16834.jpg)'
- en: Figure 2.16 – Helpful methods for the index
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 2.16 – 对索引的有用方法
- en: When we used `unique()` and `value_counts()`, we got a preview of how to select
    subsets of our data. Now, let's go into more detail and cover selection, slicing,
    indexing, and filtering.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用 `unique()` 和 `value_counts()` 时，我们已经预览了如何选择数据的子集。现在，让我们更详细地讨论选择、切片、索引和过滤。
- en: Grabbing subsets of the data
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据的子集
- en: So far, we have learned how to work with and summarize the data as a whole;
    however, we will often be interested in performing operations and/or analyses
    on subsets of our data. There are many types of subsets we may look to isolate
    from our data, such as selecting only specific columns or rows as a whole or when
    a specific criterion is met. In order to obtain subsets of the data, we need to
    be familiar with selection, slicing, indexing, and filtering.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何处理和总结整个数据；然而，我们通常会对对数据子集进行操作和/或分析感兴趣。我们可能希望从数据中提取许多类型的子集，比如选择特定的列或行，或者当满足特定条件时选择某些列或行。为了获取数据的子集，我们需要熟悉选择、切片、索引和过滤等操作。
- en: 'For this section, we will work in the `5-subsetting_data.ipynb` notebook. Our
    setup is as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在`5-subsetting_data.ipynb`笔记本中进行操作。我们的设置如下：
- en: '[PRE54]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Selecting columns
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择列
- en: 'In the previous section, we saw an example of column selection when we looked
    at the unique values in the `alert` column; we accessed the column as an attribute
    of the dataframe. Remember that a column is a `Series` object, so, for example,
    selecting the `mag` column in the earthquake data gives us the magnitudes of the
    earthquakes as a `Series` object:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分，我们看到了列选择的例子，当时我们查看了`alert`列中的唯一值；我们作为数据框的属性访问了这个列。记住，列是一个`Series`对象，因此，例如，选择地震数据中的`mag`列将给我们返回一个包含地震震级的`Series`对象：
- en: '[PRE55]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Pandas provides us with a few ways to select columns. An alternative to using
    attribute notation to select a column is to access it with a dictionary-like notation:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas为我们提供了几种选择列的方法。使用字典式的符号来选择列是替代属性符号选择列的一种方法：
- en: '[PRE56]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Tip
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: We can also select columns using the `get()` method. This has the benefits of
    not raising an error if the column doesn't exist and allowing us to provide a
    backup value—the default is `None`. For example, if we call `df.get('event', False)`,
    it will return `False` since we don't have an `event` column.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`get()`方法来选择列。这样做的好处是，如果列不存在，不会抛出错误，而且可以提供一个备选值，默认值是`None`。例如，如果我们调用`df.get('event',
    False)`，它将返回`False`，因为我们没有`event`列。
- en: 'Note that we aren''t limited to selecting one column at a time. By passing
    a list to the dictionary lookup, we can select many columns, giving us a `DataFrame`
    object that is a subset of our original dataframe:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们并不局限于一次只选择一列。通过将列表传递给字典查找，我们可以选择多列，从而获得一个`DataFrame`对象，它是原始数据框的一个子集：
- en: '[PRE57]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This gives us the full `mag` and `title` columns from the original dataframe:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就得到了来自原始数据框的完整`mag`和`title`列：
- en: '![Figure 2.17 – Selecting multiple columns of a dataframe'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.17 – 选择数据框的多列'
- en: '](img/Figure_2.17_B16834.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.17_B16834.jpg)'
- en: Figure 2.17 – Selecting multiple columns of a dataframe
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.17 – 选择数据框的多列
- en: 'String methods are a very powerful way to select columns. For example, if we
    wanted to select all the columns that start with `mag`, along with the `title`
    and `time` columns, we would do the following:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串方法是选择列的一种非常强大的方式。例如，如果我们想选择所有以`mag`开头的列，并同时选择`title`和`time`列，我们可以这样做：
- en: '[PRE58]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We get back a dataframe composed of the four columns that matched our criteria.
    Notice how the columns were returned in the order we requested, which is not the
    order they originally appeared in. This means that if we want to reorder our columns,
    all we have to do is select them in the order we want them to appear:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个由四列组成的数据框，这些列符合我们的筛选条件。注意，返回的列顺序是我们要求的顺序，而不是它们最初出现的顺序。这意味着如果我们想要重新排序列，所要做的就是按照希望的顺序选择它们：
- en: '![Figure 2.18 – Selecting columns based on names'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.18 – 根据列名选择列'
- en: '](img/Figure_2.18_B16834.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.18_B16834.jpg)'
- en: Figure 2.18 – Selecting columns based on names
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.18 – 根据列名选择列
- en: 'Let''s break this example down. We used a list comprehension to go through
    each of the columns in the dataframe and only keep the ones whose names started
    with `mag`:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分析这个例子。我们使用列表推导式遍历数据框中的每一列，只保留那些列名以`mag`开头的列：
- en: '[PRE59]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Then, we added this result to the other two columns we wanted to keep (`title`
    and `time`):'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这个结果与另外两个我们想要保留的列（`title`和`time`）合并：
- en: '[PRE60]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Finally, we were able to use this list to run the actual column selection on
    the dataframe, resulting in the dataframe in *Figure 2.18*:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们能够使用这个列表在数据框上执行实际的列选择操作，最终得到了*图 2.18*中的数据框：
- en: '[PRE61]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Tip
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: A complete list of string methods can be found in the Python 3 documentation
    at [https://docs.python.org/3/library/stdtypes.html#string-methods](https://docs.python.org/3/library/stdtypes.html#string-methods).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串方法的完整列表可以在Python 3文档中找到：[https://docs.python.org/3/library/stdtypes.html#string-methods](https://docs.python.org/3/library/stdtypes.html#string-methods)。
- en: Slicing
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 切片
- en: 'When we want to extract certain rows (slices) from our dataframe, we use `DataFrame`
    slicing works similarly to slicing with other Python objects, such as lists and
    tuples, with the first index being inclusive and the last index being exclusive:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要从数据框中提取特定的行（切片）时，我们使用`DataFrame`切片，切片的方式与其他Python对象（如列表和元组）类似，第一个索引是包含的，最后一个索引是不包含的：
- en: '[PRE62]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'When specifying a slice of `100:103`, we get back rows `100`, `101`, and `102`:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 当指定切片`100:103`时，我们会返回行`100`、`101`和`102`：
- en: '![Figure 2.19 – Slicing a dataframe to extract specific rows'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.19 – 切片数据框以提取特定行](img/Figure_2.19_B16834.jpg)'
- en: '](img/Figure_2.19_B16834.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.19_B16834.jpg)'
- en: Figure 2.19 – Slicing a dataframe to extract specific rows
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.19 – 切片数据框以提取特定行
- en: 'We can combine our row and column selections by using what is known as **chaining**:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用**链式操作**来结合行和列的选择：
- en: '[PRE63]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'First, we selected the `title` and `time` columns for all the rows, and then
    we pulled out rows with indices `100`, `101`, and `102`:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们选择了所有行中的`title`和`time`列，然后提取了索引为`100`、`101`和`102`的行：
- en: '![Figure 2.20 – Selecting specific rows and columns with chaining'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.20 – 使用链式操作选择特定行和列](img/Figure_2.20_B16834.jpg)'
- en: '](img/Figure_2.20_B16834.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.20_B16834.jpg)'
- en: Figure 2.20 – Selecting specific rows and columns with chaining
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.20 – 使用链式操作选择特定行和列
- en: 'In the preceding example, we selected the columns and then sliced the rows,
    but the order doesn''t matter:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们选择了列，然后切片了行，但顺序并不重要：
- en: '[PRE64]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Tip
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Note that we can slice on whatever is in our index; however, it would be difficult
    to determine the string or date after the last one we want, so with `pandas`,
    slicing dates and strings is different from integer slicing and is inclusive of
    both endpoints. Date slicing will work as long as the strings we provide can be
    parsed into a `datetime` object. In [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*, we'll see some examples of this and also learn how
    to change what we use as the index, thus making this type of slicing possible.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以对索引中的任何内容进行切片；然而，确定我们想要的最后一个字符串或日期后面的内容会很困难，因此在使用`pandas`时，切片日期和字符串的方式与整数切片不同，并且包含两个端点。只要我们提供的字符串可以解析为`datetime`对象，日期切片就能正常工作。在[*第3章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)《使用Pandas进行数据清洗》中，我们将看到一些相关示例，并学习如何更改作为索引的内容，从而使这种类型的切片成为可能。
- en: If we decide to use chaining to update the values in our data, we will find
    `pandas` complaining that we aren't doing so correctly (even if it works). This
    is to warn us that setting data with a sequential selection may not give us the
    result we anticipate. (More information can be found at [https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy).)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们决定使用链式操作来更新数据中的值，我们会发现`pandas`会抱怨我们没有正确执行（即使它能正常工作）。这是在提醒我们，使用顺序选择来设置数据可能不会得到我们预期的结果。（更多信息请参见[https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy)。）
- en: 'Let''s trigger this warning to understand it better. We will try to update
    the entries in the `title` column for a few earthquakes so that they''re in lowercase:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们触发这个警告，以便更好地理解它。我们将尝试更新一些地震事件的`title`列，使其变为小写：
- en: '[PRE65]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'As indicated by the warning, to be an effective `pandas` user, it''s not enough
    to know selection and slicing—we must also master **indexing**. Since this is
    just a warning, our values have been updated, but this may not always be the case:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 正如警告所示，成为一个有效的`pandas`用户，不仅仅是知道如何选择和切片—我们还必须掌握**索引**。由于这只是一个警告，我们的值已经更新，但这并不总是如此：
- en: '[PRE66]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Now, let's discuss how to use indexing to set values properly.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论如何使用索引正确设置值。
- en: Indexing
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引
- en: 'Pandas indexing operations provide us with a one-method way to select both
    the rows and the columns we want. We can use `loc[]` and `iloc[]` to subset our
    dataframe using label-based or integer-based lookups, respectively. A good way
    to remember the difference is to think of them as **loc**ation versus **i**nteger
    **loc**ation. For all indexing methods, we provide the row indexer first and then
    the column indexer, with a comma separating them:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas的索引操作为我们提供了一种单一方法，来选择我们想要的行和列。我们可以使用`loc[]`和`iloc[]`，分别通过标签或整数索引来选择数据子集。记住它们的区别的好方法是将它们想象为**loc**ation（位置）与**i**nteger
    **loc**ation（整数位置）。对于所有的索引方法，我们先提供行索引器，再提供列索引器，两者之间用逗号分隔：
- en: '[PRE67]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Note that by using `loc[]`, as indicated in the warning message, we no longer
    trigger any warnings from `pandas` for this operation. We also changed the end
    index from `113` to `112` because `loc[]` is inclusive of endpoints:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用`loc[]`时，如警告信息所示，我们不再触发`pandas`的任何警告。我们还将结束索引从`113`改为`112`，因为`loc[]`是包含端点的：
- en: '[PRE68]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We can select all the rows (columns) if we use `:` as the row (column) indexer,
    just like with regular Python slicing. Let''s grab all the rows of the `title`
    column with `loc[]`:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`:`作为行（列）索引器，就可以选择所有的行（列），就像普通的Python切片一样。让我们使用`loc[]`选择`title`列的所有行：
- en: '[PRE69]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We can select multiple rows and columns at the same time with `loc[]`:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以同时选择多行和多列，使用`loc[]`：
- en: '[PRE70]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This leaves us with rows `10` through `15` for the `title` and `mag` columns
    only:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们仅选择`10`到`15`行的`title`和`mag`列：
- en: '![Figure 2.21 – Selecting specific rows and columns with indexing'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.21 – 使用索引选择特定的行和列'
- en: '](img/Figure_2.21_B16834.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.21_B16834.jpg)'
- en: Figure 2.21 – Selecting specific rows and columns with indexing
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.21 – 使用索引选择特定的行和列
- en: 'As we have seen, when using `loc[]`, our end index is inclusive. This isn''t
    the case with `iloc[]`:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，使用`loc[]`时，结束索引是包含的。但`iloc[]`则不是这样：
- en: '[PRE71]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Observe how we had to provide a list of integers to select the same columns;
    these are the column numbers (starting from `0`). Using `iloc[]`, we lost the
    row at index `15`; this is because the integer slicing that `iloc[]` employs is
    exclusive of the end index, as with Python slicing syntax:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 观察我们如何需要提供一个整数列表来选择相同的列；这些是列的编号（从`0`开始）。使用`iloc[]`时，我们丢失了索引为`15`的行；这是因为`iloc[]`使用的整数切片在结束索引上是排除的，类似于Python切片语法：
- en: '![Figure 2.22 – Selecting specific rows and columns by position'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.22 – 通过位置选择特定的行和列'
- en: '](img/Figure_2.22_B16834.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.22_B16834.jpg)'
- en: Figure 2.22 – Selecting specific rows and columns by position
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.22 – 通过位置选择特定的行和列
- en: 'We aren''t limited to using the slicing syntax for the rows, though; columns
    work as well:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们并不限于只对行使用切片语法；列同样适用：
- en: '[PRE72]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'By using slicing, we can easily grab adjacent rows and columns:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 通过切片，我们可以轻松地抓取相邻的行和列：
- en: '![Figure 2.23 – Selecting ranges of adjacent rows and columns by position'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.23 – 通过位置选择相邻行和列的范围'
- en: '](img/Figure_2.23_B16834.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.23_B16834.jpg)'
- en: Figure 2.23 – Selecting ranges of adjacent rows and columns by position
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.23 – 通过位置选择相邻行和列的范围
- en: 'When using `loc[]`, this slicing can be done on the column names as well. This
    gives us many ways to achieve the same result:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`loc[]`时，切片操作也可以在列名上进行。这给我们提供了多种实现相同结果的方式：
- en: '[PRE73]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'To look up scalar values, we use `at[]` and `iat[]`, which are faster. Let''s
    select the magnitude (the `mag` column) of the earthquake that was recorded in
    the row at index `10`:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找标量值，我们使用`at[]`和`iat[]`，它们更快。让我们选择记录在索引为`10`的行中的地震幅度（`mag`列）：
- en: '[PRE74]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The magnitude column has a column index of `8`; therefore, we can also look
    up the magnitude with `iat[]`:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '"幅度"列的列索引为`8`；因此，我们也可以通过`iat[]`查找幅度：'
- en: '[PRE75]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: So far, we have seen how to get subsets of our data using row/column names and
    ranges, but how do we only take the data that meets some criteria? For this, we
    need to learn how to filter our data.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何使用行/列名称和范围来获取数据子集，但如何只获取符合某些条件的数据呢？为此，我们需要学习如何过滤数据。
- en: Filtering
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤
- en: 'Pandas gives us a few options for filtering our data, including `True`/`False`
    values; `pandas` can use this to select the appropriate rows/columns for us. There
    are endless possibilities for creating Boolean masks—all we need is some code
    that returns one Boolean value for each row. For example, we can see which entries
    in the `mag` column had a magnitude greater than two:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas为我们提供了几种过滤数据的方式，包括`True`/`False`值；`pandas`可以使用这些值来为我们选择适当的行/列。创建布尔掩码的方式几乎是无限的——我们只需要一些返回每行布尔值的代码。例如，我们可以查看`mag`列中震级大于
    2 的条目：
- en: '[PRE76]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'While we can run this on the entire dataframe, it wouldn''t be too useful with
    our earthquake data since we have columns of various data types. However, we can
    use this strategy to get the subset of the data where the magnitude of the earthquake
    was greater than or equal to 7.0:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以在整个数据框上运行此操作，但由于我们的地震数据包含不同类型的列，这样做可能不太有用。然而，我们可以使用这种策略来获取一个子集，其中地震的震级大于或等于
    7.0：
- en: '[PRE77]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Our resulting dataframe has just two rows:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的结果数据框只有两行：
- en: '![Figure 2.24 – Filtering with Boolean masks'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.24 – 使用布尔掩码过滤'
- en: '](img/Figure_2.24_B16834.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.24_B16834.jpg)'
- en: Figure 2.24 – Filtering with Boolean masks
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.24 – 使用布尔掩码过滤
- en: 'We got back a lot of columns we didn''t need, though. We could have chained
    a column selection to the end of the last code snippet; however, `loc[]` can handle
    Boolean masks as well:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们得到了很多不需要的列。我们本可以将列选择附加到最后一个代码片段的末尾；然而，`loc[]`同样可以处理布尔掩码：
- en: '[PRE78]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The following dataframe has been filtered so that it only contains relevant
    columns:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 以下数据框已经过滤，只包含相关列：
- en: '![Figure 2.25 – Indexing with Boolean masks'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.25 – 使用布尔掩码进行索引'
- en: '](img/Figure_2.25_B16834.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.25_B16834.jpg)'
- en: Figure 2.25 – Indexing with Boolean masks
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.25 – 使用布尔掩码进行索引
- en: 'We aren''t limited to just one criterion, either. Let''s grab the earthquakes
    with a red alert and a tsunami. To combine masks, we need to surround each of
    our conditions with parentheses and use the `&`) to require *both* to be true:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也不局限于只使用一个条件。让我们筛选出带有红色警报和海啸的地震。为了组合多个条件，我们需要将每个条件用括号括起来，并使用`&`来要求*两个*条件都为真：
- en: '[PRE79]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'There was only a single earthquake in the data that met our criteria:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中只有一个地震满足我们的标准：
- en: '![Figure 2.26 – Combining filters with AND'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.26 – 使用 AND 组合过滤条件'
- en: '](img/Figure_2.26_B16834.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.26_B16834.jpg)'
- en: Figure 2.26 – Combining filters with AND
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.26 – 使用 AND 组合过滤条件
- en: 'If, instead, we want *at least one* of our conditions to be true, we can use
    the `|`):'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要*至少一个*条件为真，则可以使用`|`：
- en: '[PRE80]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Notice that this filter is much less restrictive since, while both conditions
    can be true, we only require that one of them is:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个过滤器要宽松得多，因为虽然两个条件都可以为真，但我们只要求其中一个为真：
- en: '![Figure 2.27 – Combining filters with OR'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.27 – 使用 OR 组合过滤条件'
- en: '](img/Figure_2.27_B16834.jpg)'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.27_B16834.jpg)'
- en: Figure 2.27 – Combining filters with OR
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.27 – 使用 OR 组合过滤条件
- en: Important note
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When creating Boolean masks, we must use bitwise operators (`&`, `|`, `~`) instead
    of logical operators (`and`, `or`, `not`). A good way to remember this is that
    we want a Boolean for each item in the series we are testing rather than a single
    Boolean. For example, with the earthquake data, if we want to select the rows
    where the magnitude is greater than 1.5, then we want one Boolean value for each
    row, indicating whether the row should be selected. In cases where we want a single
    value for the data, perhaps to summarize it, we can use `any()`/`all()` to condense
    a Boolean series into a single Boolean value that can be used with logical operators.
    We will work with the `any()` and `all()` methods in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建布尔掩码时，我们必须使用位运算符（`&`、`|`、`~`）而不是逻辑运算符（`and`、`or`、`not`）。记住这一点的一个好方法是：我们希望对我们正在测试的系列中的每一项返回一个布尔值，而不是返回单一的布尔值。例如，在地震数据中，如果我们想选择震级大于
    1.5 的行，那么我们希望每一行都有一个布尔值，表示该行是否应该被选中。如果我们只希望对数据得到一个单一的值，或许是为了总结它，我们可以使用`any()`/`all()`将布尔系列压缩成一个可以与逻辑运算符一起使用的布尔值。我们将在[*第
    4 章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)《聚合 Pandas 数据框》中使用`any()`和`all()`方法。
- en: 'In the previous two examples, our conditions involved equality; however, we
    are by no means limited to this. Let''s select all the earthquakes in Alaska where
    we have a non-null value for the `alert` column:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面两个示例中，我们的条件涉及到相等性；然而，我们并不局限于此。让我们选择所有在阿拉斯加的地震数据，其中`alert`列具有非空值：
- en: '[PRE81]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'All the earthquakes in Alaska that have a value for `alert` are `green`, and
    some were accompanied by tsunamis, with the highest magnitude being 5.1:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 所有阿拉斯加的地震，`alert`值为`green`，其中一些伴随有海啸，最大震级为5.1：
- en: '![Figure 2.28 – Creating Boolean masks with non-numeric columns'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.28 – 使用非数字列创建布尔掩码'
- en: '](img/Figure_2.28_B16834.jpg)'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.28_B16834.jpg)'
- en: Figure 2.28 – Creating Boolean masks with non-numeric columns
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.28 – 使用非数字列创建布尔掩码
- en: 'Let''s break down how we got this. `Series` objects have some string methods
    that can be accessed via the `str` attribute. Using this, we can create a Boolean
    mask of all the rows where the `place` column contained the word `Alaska`:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分解一下我们是如何得到这个的。`Series`对象有一些字符串方法，可以通过`str`属性访问。利用这一点，我们可以创建一个布尔掩码，表示`place`列中包含单词`Alaska`的所有行：
- en: '[PRE82]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'To get all the rows where the `alert` column was not null, we used the `Series`
    object''s `notnull()` method (this works for `DataFrame` objects as well) to create
    a Boolean mask of all the rows where the `alert` column was not null:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取`alert`列不为null的所有行，我们使用了`Series`对象的`notnull()`方法（这同样适用于`DataFrame`对象），以创建一个布尔掩码，表示`alert`列不为null的所有行：
- en: '[PRE83]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Tip
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: We can use the `~`), also called `True` values `False` and vice versa. So, `df.alert.notnull()`
    and `~df.alert.isnull()`are equivalent.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`~`，也称为`True`值和`False`的反转。所以，`df.alert.notnull()`和`~df.alert.isnull()`是等价的。
- en: 'Then, like we did previously, we combine the two conditions with the `&` operator
    to complete our mask:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，像我们之前做的那样，我们使用`&`运算符将两个条件结合起来，完成我们的掩码：
- en: '[PRE84]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Note that we aren't limited to checking if each row contains text; we can use
    regular expressions as well. `r` character outside the quotes; this lets Python
    know it is a `\`) characters in the string without Python thinking we are trying
    to escape the character immediately following it (such as when we use `\n` to
    mean a new line character instead of the letter `n`). This makes it perfect for
    use with regular expressions. The `re` module in the Python standard library ([https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html))
    handles regular expression operations; however, `pandas` lets us use regular expressions
    directly.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不仅限于检查每一行是否包含文本；我们还可以使用正则表达式。`r`字符出现在引号外面；这样，Python就知道这是一个`\`）字符，而不是在尝试转义紧随其后的字符（例如，当我们使用`\n`表示换行符时，而不是字母`n`）。这使得它非常适合与正则表达式一起使用。Python标准库中的`re`模块（[https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)）处理正则表达式操作；然而，`pandas`允许我们直接使用正则表达式。
- en: 'Using a regular expression, let''s select all the earthquakes in California
    that have magnitudes of at least 3.8\. We need to select entries in the `place`
    column that end in `CA` or `California` because the data isn''t consistent (we
    will look at how to fix this in the next section). The `$` character means *end*
    and `''CA$''` gives us entries that end in `CA`, so we can use `''CA|California$''`
    to get entries that end in either:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则表达式，让我们选择所有震级至少为3.8的加利福尼亚地震。我们需要选择`place`列中以`CA`或`California`结尾的条目，因为数据不一致（我们将在下一节中学习如何解决这个问题）。`$`字符表示*结束*，`'CA$'`给我们的是以`CA`结尾的条目，因此我们可以使用`'CA|California$'`来获取以任一项结尾的条目：
- en: '[PRE85]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'There were only two earthquakes in California with magnitudes greater than
    3.8 during the time period we are studying:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究的时间段内，加利福尼亚只有两次震级超过3.8的地震：
- en: '![Figure 2.29 – Filtering with regular expressions'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.29 – 使用正则表达式进行过滤'
- en: '](img/Figure_2.29_B16834.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.29_B16834.jpg)'
- en: Figure 2.29 – Filtering with regular expressions
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.29 – 使用正则表达式进行过滤
- en: Tip
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'Regular expressions are extremely powerful, but unfortunately, also difficult
    to get right. It is often helpful to grab some sample lines for parsing and use
    a website to test them. Note that regular expressions come in many flavors, so
    be sure to select Python. This website supports Python flavor regular expressions,
    and also provides a nice cheat sheet on the side: https://regex101.com/.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式功能非常强大，但不幸的是，也很难正确编写。通常，抓取一些示例行进行解析并使用网站测试它们会很有帮助。请注意，正则表达式有很多种类型，因此务必选择Python类型。这个网站支持Python类型的正则表达式，并且还提供了一个不错的备忘单：
    https://regex101.com/。
- en: 'What if we want to get all earthquakes with magnitudes between 6.5 and 7.5?
    We could use two Boolean masks—one to check for magnitudes greater than or equal
    to 6.5, and another to check for magnitudes less than or equal to 7.5—and then
    combine them with the `&` operator. Thankfully, `pandas` makes this type of mask
    much easier to create by providing us with the `between()` method:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想获取震级在 6.5 和 7.5 之间的所有地震怎么办？我们可以使用两个布尔掩码——一个检查震级是否大于或等于 6.5，另一个检查震级是否小于或等于
    7.5——然后用 `&` 运算符将它们结合起来。幸运的是，`pandas` 使得创建这种类型的掩码变得更容易，它提供了 `between()` 方法：
- en: '[PRE86]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The result contains all the earthquakes with magnitudes in the range [6.5,
    7.5]—it''s inclusive of both ends by default, but we can pass in `inclusive=False`
    to change this:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 结果包含所有震级在 [6.5, 7.5] 范围内的地震——默认情况下包括两个端点，但我们可以传入 `inclusive=False` 来更改这一点：
- en: '![Figure 2.30 – Filtering using a range of values'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.30 – 使用数值范围进行过滤'
- en: '](img/Figure_2.30_B16834.jpg)'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.30_B16834.jpg)'
- en: Figure 2.30 – Filtering using a range of values
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.30 – 使用数值范围进行过滤
- en: 'We can use the `isin()` method to create a Boolean mask for values that match
    one of a list of values. This means that we don''t have to write one mask for
    each of the values that we could match and then use `|` to join them. Let''s utilize
    this to filter on the `magType` column, which indicates the measurement technique
    that was used to quantify the earthquake''s magnitude. We will take a look at
    earthquakes measured with either the `mw` or `mwb` magnitude type:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `isin()` 方法创建一个布尔掩码，用于匹配某个值是否出现在值列表中。这意味着我们不必为每个可能匹配的值编写一个掩码，然后使用 `|`
    将它们连接起来。让我们利用这一点来过滤 `magType` 列，这一列表示用于量化地震震级的测量方法。我们将查看使用 `mw` 或 `mwb` 震级类型测量的地震：
- en: '[PRE87]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'We have two earthquakes that were measured with the `mwb` magnitude type and
    four that were measured with the `mw` magnitude type:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个震级采用 `mwb` 测量类型的地震，四个震级采用 `mw` 测量类型的地震：
- en: '![Figure 2.31 – Filtering using membership in a list'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.31 – 使用列表中的成员关系进行过滤'
- en: '](img/Figure_2.31_B16834.jpg)'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.31_B16834.jpg)'
- en: Figure 2.31 – Filtering using membership in a list
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.31 – 使用列表中的成员关系进行过滤
- en: 'So far, we have been filtering on specific values, but suppose we wanted to
    see all the data for the lowest-magnitude and highest-magnitude earthquakes. Rather
    than finding the minimum and maximum of the `mag` column first and then creating
    a Boolean mask, we can ask `pandas` to give us the index where these values occur,
    and easily filter to grab the full rows. We can use `idxmin()` and `idxmax()`
    for the indices of the minimum and maximum, respectively. Let''s grab the row
    numbers for the lowest-magnitude and highest-magnitude earthquakes:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在基于特定的值进行过滤，但假设我们想查看最低震级和最高震级地震的所有数据。与其先找到 `mag` 列的最小值和最大值，再创建布尔掩码，不如让
    `pandas` 给我们这些值出现的索引，并轻松地过滤出完整的行。我们可以分别使用 `idxmin()` 和 `idxmax()` 来获取最小值和最大值的索引。让我们抓取最低震级和最高震级地震的行号：
- en: '[PRE88]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'We can use these indices to grab the rows themselves:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些索引来抓取相应的行：
- en: '[PRE89]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The minimum magnitude earthquake occurred in Alaska and the highest magnitude
    earthquake occurred in Indonesia, accompanied by a tsunami. We will discuss the
    earthquake in Indonesia in [*Chapter 5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106),
    *Visualizing Data with Pandas and Matplotlib*, and [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 最小震级的地震发生在阿拉斯加，最大震级的地震发生在印度尼西亚，并伴随海啸。我们将在 [*第 5 章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106)，《使用
    Pandas 和 Matplotlib 可视化数据》，以及 [*第 6 章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)，《使用
    Seaborn 绘图与自定义技术》中讨论印度尼西亚的地震：
- en: '![Figure 2.32 – Filtering to isolate the rows containing the minimum and maximum
    of a column'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.32 – 过滤以隔离包含列的最小值和最大值的行'
- en: '](img/Figure_2.32_B16834.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.32_B16834.jpg)'
- en: Figure 2.32 – Filtering to isolate the rows containing the minimum and maximum
    of a column
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.32 – 过滤以隔离包含列的最小值和最大值的行
- en: Important note
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Be advised that the `filter()` method does not filter the data according to
    its values, as we did in this section; rather, it can be used to subset rows or
    columns based on their names. Examples with `DataFrame` and `Series` objects can
    be found in the notebook.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`filter()` 方法并不是像我们在本节中所做的那样根据值来过滤数据；相反，它可以根据行或列的名称来子集化数据。有关 `DataFrame`
    和 `Series` 对象的示例，请参见笔记本。
- en: Adding and removing data
  id: totrans-433
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加和移除数据
- en: In the previous sections, we frequently selected a subset of the columns, but
    if columns/rows aren't useful to us, we should just get rid of them. We also frequently
    selected data based on the value of the `mag` column; however, if we had made
    a new column holding the Boolean values for later selection, we would have only
    needed to calculate the mask once. Very rarely will we get data where we neither
    want to add nor remove something.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们经常选择列的子集，但如果某些列/行对我们不有用，我们应该直接删除它们。我们也常常根据`mag`列的值来选择数据；然而，如果我们创建了一个新列，用于存储布尔值以便后续选择，那么我们只需要计算一次掩码。非常少情况下，我们会遇到既不想添加也不想删除数据的情况。
- en: 'Before we begin adding and removing data, it''s important to understand that
    while most methods will return a new `DataFrame` object, some will be in-place
    and change our data. If we write a function where we pass in a dataframe and change
    it, it will change our original dataframe as well. Should we find ourselves in
    a situation where we don''t want to change the original data, but rather want
    to return a new copy of the data that has been modified, we must be sure to copy
    our dataframe before making any changes:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始添加和删除数据之前，理解一个重要概念非常关键：虽然大多数方法会返回一个新的`DataFrame`对象，但有些方法是就地修改数据的。如果我们编写一个函数，传入一个数据框并修改它，那么它也会改变原始的数据框。如果我们遇到这种情况，即不想改变原始数据，而是希望返回一个已经修改过的数据副本，那么我们必须在做任何修改之前确保复制我们的数据框：
- en: '[PRE90]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Important note
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: By default, `df.copy()` makes a `deep=False`, we can obtain a **shallow copy**—changes
    to the shallow copy affect the original and vice versa. We will almost always
    want the deep copy, since we can change it without affecting the original. More
    information can be found in the documentation at [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`df.copy()`会创建一个`deep=False`的**浅拷贝**，对浅拷贝的修改会影响原数据框，反之亦然。我们通常希望使用深拷贝，因为我们可以修改深拷贝而不影响原始数据。更多信息可以参考文档：[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html)。
- en: 'Now, let''s turn to the final notebook, `6-adding_and_removing_data.ipynb`,
    and get set up for the remainder of this chapter. We will once again be working
    with the earthquake data, but this time, we will only read in a subset of the
    columns:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转向最后一个笔记本`6-adding_and_removing_data.ipynb`，并为本章剩余部分做准备。我们将再次使用地震数据，但这次我们只读取一部分列：
- en: '[PRE91]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Creating new data
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建新数据
- en: 'Creating new columns can be achieved in the same fashion as variable assignment.
    For example, we can create a column to indicate the source of our data; since
    all our data came from the same source, we can take advantage of **broadcasting**
    to set every row of this column to the same value:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新列可以通过与变量赋值相同的方式来实现。例如，我们可以创建一列来表示数据的来源；由于我们所有的数据都来自同一来源，我们可以利用**广播**将这一列的每一行都设置为相同的值：
- en: '[PRE92]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'The new column is created to the right of the original columns, with a value
    of `USGS API` for every row:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 新列被创建在原始列的右侧，并且每一行的值都是`USGS API`：
- en: '![Figure 2.33 – Adding a new column'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.33 – 添加新列](img/Figure_2.33_B16834.jpg)'
- en: '](img/Figure_2.33_B16834.jpg)'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.33 – 添加新列](img/Figure_2.33_B16834.jpg)'
- en: Figure 2.33 – Adding a new column
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.33 – 添加新列
- en: Important note
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We cannot create the column with attribute notation (`df.source`) because the
    dataframe doesn't have that attribute yet, so we must use dictionary notation
    (`df['source']`).
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能通过属性符号（`df.source`）创建新列，因为数据框还没有这个属性，因此必须使用字典符号（`df['source']`）。
- en: 'We aren''t limited to broadcasting one value to the entire column; we can have
    the column hold the result of Boolean logic or a mathematical equation. For example,
    if we had data on distance and time, we could create a speed column that is the
    result of dividing the distance column by the time column. With our earthquake
    data, let''s create a column that tells us whether the earthquake''s magnitude
    was negative:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅仅限于将一个值广播到整列；我们可以让这一列存储布尔逻辑结果或数学公式。例如，如果我们有关于距离和时间的数据，我们可以创建一列速度，它是通过将距离列除以时间列得到的结果。在我们的地震数据中，我们可以创建一列，告诉我们地震的震级是否为负数：
- en: '[PRE93]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Note that the new column has been added to the right:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，新列已添加到右侧：
- en: '![Figure 2.34 – Storing a Boolean mask in a new column'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.34 – 在新列中存储布尔掩码](img/Figure_2.34_B16834.jpg)'
- en: '](img/Figure_2.34_B16834.jpg)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.33 – 添加新列](img/Figure_2.33_B16834.jpg)'
- en: Figure 2.34 – Storing a Boolean mask in a new column
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.34 – 在新列中存储布尔掩码
- en: 'In the previous section, we saw that the `place` column has some data consistency
    issues—we have multiple names for the same entity. In some cases, earthquakes
    occurring in California are marked as `CA` and as `California` in others. Needless
    to say, this is confusing and can easily cause issues for us if we don''t carefully
    inspect our data beforehand. For example, by just selecting `CA`, we miss out
    on 124 earthquakes marked as `California`. This isn''t the only place with an
    issue either (`Nevada` and `NV` are also both present). By using a regular expression
    to extract everything in the `place` column after the comma, we can see some of
    the issues firsthand:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们看到`place`列存在一些数据一致性问题——同一个实体有多个名称。在某些情况下，加利福尼亚的地震标记为`CA`，而在其他情况下标记为`California`。不言而喻，这会引起混淆，如果我们没有仔细检查数据，可能会导致问题。例如，仅选择`CA`时，我们错过了124个标记为`California`的地震。这并不是唯一存在问题的地方（`Nevada`和`NV`也都有）。通过使用正则表达式提取`place`列中逗号后的所有内容，我们可以亲眼看到一些问题：
- en: '[PRE94]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: If we want to treat countries and anything near them as a single entity, we
    have some additional work to do (see `Ecuador` and `Ecuador region`). In addition,
    our naive attempt at parsing the location by looking at the information after
    the comma appears to have failed; this is because, in some cases, we don't have
    a comma. We will need to change our approach to parsing.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将国家及其附近的任何地方视为一个整体实体，我们还需要做一些额外的工作（参见`Ecuador`和`Ecuador region`）。此外，我们通过查看逗号后面的信息来解析位置的简单尝试显然失败了；这是因为在某些情况下，我们并没有逗号。我们需要改变解析的方式。
- en: 'This is an `df.place.unique()`), we can simply look through and infer how to
    properly match up these names. Then, we can use the `replace()` method to replace
    patterns in the `place` column as we see fit:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个`df.place.unique()`，我们可以简单地查看并推断如何正确地匹配这些名称。然后，我们可以使用`replace()`方法根据需要替换`place`列中的模式：
- en: '[PRE95]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Now, we can check the parsed places we are left with. Notice that there is
    arguably still more to fix here with `South Georgia and South Sandwich Islands`
    and `South Sandwich Islands`. We could address this with another call to `replace()`;
    however, this goes to show that entity recognition can be quite challenging:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以检查剩下的解析地点。请注意，关于`South Georgia and South Sandwich Islands`和`South Sandwich
    Islands`，可能还有更多需要修正的地方。我们可以通过另一次调用`replace()`来解决这个问题；然而，这表明实体识别确实可能相当具有挑战性：
- en: '[PRE96]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Important note
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In practice, entity recognition can be an extremely difficult problem, where
    we may look to employ **natural language processing** (**NLP**) algorithms to
    help us. While this is well beyond the scope of this book, more information can
    be found at https://www.kdnuggets.com/2018/12/introduction-named-entity-recognition.html.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，实体识别可能是一个极其困难的问题，我们可能会尝试使用**自然语言处理**（**NLP**）算法来帮助我们。虽然这超出了本书的范围，但可以在 https://www.kdnuggets.com/2018/12/introduction-named-entity-recognition.html
    上找到更多信息。
- en: 'Pandas also provides us with a way to make many new columns at once in one
    method call. With the `assign()` method, the arguments are the names of the columns
    we want to create (or overwrite), and the values are the data for the columns.
    Let''s create two new columns; one will tell us if the earthquake happened in
    California, and the other will tell us if it happened in Alaska. Rather than just
    show the first five entries (which are all in California), we will use `sample()`
    to randomly select five rows:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 还提供了一种通过一次方法调用创建多个新列的方式。使用`assign()`方法，参数是我们想要创建（或覆盖）的列名，而值是这些列的数据。我们将创建两个新列；一个列将告诉我们地震是否发生在加利福尼亚，另一个列将告诉我们地震是否发生在阿拉斯加。我们不仅仅展示前五行（这些地震都发生在加利福尼亚），我们将使用`sample()`随机选择五行：
- en: '[PRE97]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Note that `assign()` doesn''t change our original dataframe; instead, it returns
    a new `DataFrame` object with these columns added. If we want to replace our original
    dataframe with this, we just use variable assignment to store the result of `assign()`
    in `df` (for example, `df = df.assign(...)`):'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`assign()`并不会改变我们的原始数据框；相反，它返回一个包含新列的`DataFrame`对象。如果我们想用这个新的数据框替换原来的数据框，我们只需使用变量赋值将`assign()`的结果存储在`df`中（例如，`df
    = df.assign(...)`）：
- en: '![Figure 2.35 – Creating multiple new columns at once'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.35 – 一次创建多个新列'
- en: '](img/Figure_2.35_B16834.jpg)'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.35_B16834.jpg)'
- en: Figure 2.35 – Creating multiple new columns at once
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.35 – 一次创建多个新列
- en: 'The `assign()` method also accepts `assign()` will pass the dataframe into
    the `lambda` function as `x`, and we can work from there. This makes it possible
    for us to use the columns we are creating in `assign()` to calculate others. For
    example, let''s once again create the `in_ca` and `in_alaska` columns, but this
    time also create a new column, `neither`, which is `True` if both `in_ca` and
    `in_alaska` are `False`:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '`assign()` 方法也接受 `assign()`，它会将数据框传递到 `lambda` 函数作为 `x`，然后我们可以在这里进行操作。这使得我们可以利用在
    `assign()` 中创建的列来计算其他列。例如，让我们再次创建 `in_ca` 和 `in_alaska` 列，这次还会创建一个新列 `neither`，如果
    `in_ca` 和 `in_alaska` 都是 `False`，那么 `neither` 就为 `True`：'
- en: '[PRE98]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Remember that `~` is the bitwise negation operator, so this allows us to create
    a column with the result of `NOT in_ca AND NOT in_alaska` per row:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`~` 是按位取反运算符，所以这允许我们为每一行创建一个列，其结果是 `NOT in_ca AND NOT in_alaska`：
- en: '![Figure 2.36 – Creating multiple new columns at once with lambda functions'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.36 – 使用 lambda 函数一次性创建多个新列'
- en: '](img/Figure_2.36_B16834.jpg)'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.36_B16834.jpg)'
- en: Figure 2.36 – Creating multiple new columns at once with lambda functions
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.36 – 使用 lambda 函数一次性创建多个新列
- en: Tip
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: When working with `pandas`, it's crucial to get comfortable with `lambda` functions,
    as they can be used with much of the functionality available and will dramatically
    improve the quality and readability of the code. Throughout this book, we will
    see various places where `lambda` functions can be used.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `pandas` 时，熟悉 `lambda` 函数至关重要，因为它们可以与许多功能一起使用，并且会显著提高代码的质量和可读性。在本书中，我们将看到许多可以使用
    `lambda` 函数的场景。
- en: 'Now that we have seen how to add new columns, let''s take a look at adding
    new rows. Say we were working with two separate dataframes; one with earthquakes
    accompanied by tsunamis and the other with earthquakes without tsunamis:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何添加新列，让我们来看一下如何添加新行。假设我们正在处理两个不同的数据框：一个包含地震和海啸的数据，另一个则是没有海啸的地震数据：
- en: '[PRE99]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'If we wanted to look at earthquakes as a whole, we would want to concatenate
    the dataframes into a single one. To append rows to the bottom of our dataframe,
    we can either use `pd.concat()` or the `append()` method of the dataframe itself.
    The `concat()` function allows us to specify the axis that the operation will
    be performed along—`0` for appending rows to the bottom of the dataframe, and
    `1` for appending to the right of the last column with respect to the leftmost
    `pandas` object in the concatenation list. Let''s use `pd.concat()` with the default
    `axis` of `0` for rows:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想查看所有的地震数据，我们可能需要将两个数据框合并成一个。要将行追加到数据框的底部，我们可以使用 `pd.concat()` 或者数据框本身的
    `append()` 方法。`concat()` 函数允许我们指定操作的轴——`0` 表示将行追加到底部，`1` 表示将数据追加到最后一列的右侧，依据的是连接列表中最左边的
    `pandas` 对象。让我们使用 `pd.concat()` 并保持默认的 `axis=0` 来处理行：
- en: '[PRE100]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Note that the previous result is equivalent to running the `append()` method
    on the dataframe. This still returns a new `DataFrame` object, but it saves us
    from having to remember which axis is which, since `append()` is actually a wrapper
    around the `concat()` function:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，之前的结果等同于在数据框上运行 `append()` 方法。它仍然返回一个新的 `DataFrame` 对象，但避免了我们需要记住哪个轴是哪个，因为
    `append()` 实际上是 `concat()` 函数的一个包装器：
- en: '[PRE101]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'So far, we have been working with a subset of the columns from the CSV file,
    but suppose that we now want to work with some of the columns we ignored when
    we read in the data. Since we have added new columns in this notebook, we won''t
    want to read in the file and perform those operations again. Instead, we will
    concatenate along the columns (`axis=1`) to add back what we are missing:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理 CSV 文件中的部分列，但假设我们现在想处理读取数据时忽略的一些列。由于我们已经在这个笔记本中添加了新列，所以我们不想重新读取文件并再次执行这些操作。相反，我们将沿列方向（`axis=1`）进行合并，添加回我们缺失的内容：
- en: '[PRE102]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Since the indices of the dataframes align, the additional columns are placed
    to the right of our original columns:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据框的索引对齐，附加的列被放置在原始列的右侧：
- en: '![Figure 2.37 – Concatenating columns with matching indices'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.37 – 按照匹配的索引连接列'
- en: '](img/Figure_2.37_B16834.jpg)'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.37_B16834.jpg)'
- en: Figure 2.37 – Concatenating columns with matching indices
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.37 – 按照匹配的索引连接列
- en: 'The `concat()` function uses the index to determine how to concatenate the
    values. If they don''t align, this will generate additional rows because `pandas`
    won''t know how to align them. Say we forgot that our original dataframe had the
    row numbers as the index, and we read in the additional columns by setting the
    `time` column as the index:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '`concat()`函数使用索引来确定如何连接值。如果它们不对齐，这将生成额外的行，因为`pandas`不知道如何对齐它们。假设我们忘记了原始DataFrame的索引是行号，并且我们通过将`time`列设置为索引来读取了其他列：'
- en: '[PRE103]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Despite the additional columns containing data for the first two rows, `pandas`
    creates a new row for them because the index doesn''t match. In [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*, we will see how to reset the index and set the index,
    both of which could resolve this issue:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管额外的列包含了前两行的数据，`pandas`仍然为它们创建了一个新行，因为索引不匹配。在[*第3章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)，*使用Pandas进行数据清洗*中，我们将看到如何重置索引和设置索引，这两种方法都可以解决这个问题：
- en: '![Figure 2.38 – Concatenating columns with mismatching indices'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.38 – 连接具有不匹配索引的列'
- en: '](img/Figure_2.38_B16834.jpg)'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.38_B16834.jpg)'
- en: Figure 2.38 – Concatenating columns with mismatching indices
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.38 – 连接具有不匹配索引的列
- en: Important note
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082), *Aggregating
    Pandas DataFrames*, we will discuss merging, which will also handle some of these
    issues when we're augmenting the columns in the dataframe. Often, we will use
    `concat()` or `append()` to add rows, but `merge()` or `join()` to add columns.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082)，*聚合Pandas DataFrame*中，我们将讨论合并操作，这将处理一些在增加DataFrame列时遇到的问题。通常，我们会使用`concat()`或`append()`来添加行，但会使用`merge()`或`join()`来添加列。
- en: 'Say we want to concatenate the `tsunami` and `no_tsunami` dataframes, but the
    `no_tsunami` dataframe has an additional column (suppose we added a new column
    to it called `type`). The `join` parameter specifies how to handle any overlap
    in column names (when appending to the bottom) or in row names (when concatenating
    to the right). By default, this is `outer`, so we keep everything; however, if
    we use `inner`, we will only keep what they have in common:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想连接`tsunami`和`no_tsunami`这两个DataFrame，但`no_tsunami` DataFrame多了一列（假设我们向其中添加了一个名为`type`的新列）。`join`参数指定了如何处理列名重叠（在底部添加时）或行名重叠（在右侧连接时）。默认情况下，这是`outer`，所以我们会保留所有内容；但是，如果使用`inner`，我们只会保留它们共有的部分：
- en: '[PRE104]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Notice that the `type` column from the `no_tsunami` dataframe doesn''t show
    up because it wasn''t present in the `tsunami` dataframe. Take a look at the index,
    though; these were the row numbers from the original dataframe before we divided
    it into `tsunami` and `no_tsunami`:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`no_tsunami` DataFrame中的`type`列没有出现，因为它在`tsunami` DataFrame中不存在。不过，看看索引；这些是原始DataFrame的行号，在我们将其分为`tsunami`和`no_tsunami`之前：
- en: '![Figure 2.39 – Appending rows and keeping only shared columns'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.39 – 添加行并仅保留共享列'
- en: '](img/Figure_2.39_B16834.jpg)'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.39_B16834.jpg)'
- en: Figure 2.39 – Appending rows and keeping only shared columns
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.39 – 添加行并仅保留共享列
- en: 'If the index is not meaningful, we can also pass in `ignore_index` to get sequential
    values in the index:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 如果索引没有实际意义，我们还可以传入`ignore_index`来获取连续的索引值：
- en: '[PRE105]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'The index is now sequential, and the row numbers no longer match the original
    dataframe:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 现在索引是连续的，行号与原始DataFrame不再匹配：
- en: '![Figure 2.40 – Appending rows and resetting the index'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.40 – 添加行并重置索引'
- en: '](img/Figure_2.40_B16834.jpg)'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.40_B16834.jpg)'
- en: Figure 2.40 – Appending rows and resetting the index
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.40 – 添加行并重置索引
- en: 'Be sure to consult the `pandas` documentation for more information on the `concat()`
    function and other operations for combining data, which we will discuss in *Chapter
    4*, *Aggregating Pandas DataFrames*: http://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#concatenating-objects.'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 确保查阅`pandas`文档以获取有关`concat()`函数和其他数据合并操作的更多信息，我们将在*第4章*，*聚合Pandas DataFrame*中讨论这些内容：http://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#concatenating-objects。
- en: Deleting unwanted data
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除不需要的数据
- en: After adding that data to our dataframe, we can see the need to delete unwanted
    data. We need a way to undo our mistakes and get rid of data that we aren't going
    to use. Like adding data, we can use dictionary syntax to delete unwanted columns,
    just as we would when removing keys from a dictionary. Both `del df['<column_name>']`
    and `df.pop('<column_name>')` will work, provided that there is indeed a column
    with that name; otherwise, we will get a `KeyError`. The difference here is that
    while `del` removes it right away, `pop()` will return the column that we are
    removing. Remember that both of these operations will change our original dataframe,
    so use them with care.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据添加到我们的数据框后，我们可以看到有删除不需要数据的需求。我们需要一种方法来撤销我们的错误并去除那些我们不打算使用的数据。和添加数据一样，我们可以使用字典语法删除不需要的列，就像从字典中删除键一样。`del
    df['<column_name>']` 和 `df.pop('<column_name>')` 都可以工作，前提是确实有一个名为该列的列；否则，我们会得到一个
    `KeyError`。这里的区别在于，虽然 `del` 会立即删除它，`pop()` 会返回我们正在删除的列。记住，这两个操作都会修改原始数据框，因此请小心使用它们。
- en: 'Let''s use dictionary notation to delete the `source` column. Notice that it
    no longer appears in the result of `df.columns`:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用字典语法删除 `source` 列。注意，它不再出现在 `df.columns` 的结果中：
- en: '[PRE106]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Note that if we aren''t sure whether the column exists, we should put our column
    deletion code in a `try...except` block:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果我们不确定列是否存在，应该将我们的列删除代码放在 `try...except` 块中：
- en: '[PRE107]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Earlier, we created the `mag_negative` column for filtering our dataframe;
    however, we no longer want this column as part of our dataframe. We can use `pop()`
    to grab the series for the `mag_negative` column, which we can use as a Boolean
    mask later without having it in our dataframe:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们创建了 `mag_negative` 列来过滤数据框；然而，我们现在不再希望将这个列包含在数据框中。我们可以使用 `pop()` 获取 `mag_negative`
    列的系列，这样我们可以将它作为布尔掩码稍后使用，而不必将其保留在数据框中：
- en: '[PRE108]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'We now have a Boolean mask in the `mag_negative` variable that used to be a
    column in `df`:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在 `mag_negative` 变量中有一个布尔掩码，它曾经是 `df` 中的一列：
- en: '[PRE109]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Since we used `pop()` to remove the `mag_negative` series rather than deleting
    it, we can still use it to filter our dataframe:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用 `pop()` 移除了 `mag_negative` 系列而不是删除它，我们仍然可以使用它来过滤数据框：
- en: '[PRE110]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'This leaves us with the earthquakes that had negative magnitudes. Since we
    also called `head()`, we get back the first five such earthquakes:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就得到了具有负震级的地震数据。由于我们还调用了 `head()`，因此返回的是前五个这样的地震数据：
- en: '![Figure 2.41 – Using a popped column as a Boolean mask'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.41 – 使用弹出的列作为布尔掩码'
- en: '](img/Figure_2.41_B16834.jpg)'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.41_B16834.jpg)'
- en: Figure 2.41 – Using a popped column as a Boolean mask
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.41 – 使用弹出的列作为布尔掩码
- en: '`DataFrame` objects have a `drop()` method for removing multiple rows or columns
    either in-place (overwriting the original dataframe without having to reassign
    it) or returning a new `DataFrame` object. To remove rows, we pass the list of
    indices. Let''s remove the first two rows:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame` 对象有一个 `drop()` 方法，用于删除多行或多列，可以原地操作（覆盖原始数据框而不需要重新赋值）或返回一个新的 `DataFrame`
    对象。要删除行，我们传入索引列表。让我们删除前两行：'
- en: '[PRE111]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Notice that the index starts at `2` because we dropped `0` and `1`:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，索引从 `2` 开始，因为我们删除了 `0` 和 `1`：
- en: '![Figure 2.42 – Dropping specific rows'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.42 – 删除特定的行'
- en: '](img/Figure_2.42_B16834.jpg)'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.42_B16834.jpg)'
- en: Figure 2.42 – Dropping specific rows
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.42 – 删除特定的行
- en: 'By default, `drop()` assumes that we want to delete rows (`axis=0`). If we
    want to drop columns, we can either pass `axis=1` or specify our list of column
    names using the `columns` argument. Let''s delete some more columns:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`drop()` 假设我们要删除的是行（`axis=0`）。如果我们想删除列，我们可以传入 `axis=1`，或者使用 `columns`
    参数指定我们要删除的列名列表。让我们再删除一些列：
- en: '[PRE112]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'This drops all the columns that aren''t in the list we wanted to keep:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 这会删除所有不在我们想保留的列表中的列：
- en: '![Figure 2.43 – Dropping specific columns'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.43 – 删除特定的列'
- en: '](img/Figure_2.43_B16834.jpg)'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.43_B16834.jpg)'
- en: Figure 2.43 – Dropping specific columns
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.43 – 删除特定的列
- en: 'Whether we decide to pass `axis=1` to `drop()` or use the `columns` argument,
    our result will be equivalent:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们决定将 `axis=1` 传递给 `drop()` 还是使用 `columns` 参数，我们的结果都是等效的：
- en: '[PRE113]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'By default, `drop()` will return a new `DataFrame` object; however, if we really
    want to remove the data from our original dataframe, we can pass in `inplace=True`,
    which will save us from having to reassign the result back to our dataframe. The
    result is the same as in *Figure 2.43*:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`drop()` 会返回一个新的 `DataFrame` 对象；然而，如果我们确实想从原始数据框中删除数据，我们可以传入 `inplace=True`，这将避免我们需要将结果重新赋值回数据框。结果与
    *图 2.43* 中的相同：
- en: '[PRE114]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: Always be careful with in-place operations. In some cases, it may be possible
    to undo them; however, in others, it may require starting over from the beginning
    and recreating the dataframe.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 使用就地操作时要始终小心。在某些情况下，可能可以撤销它们；然而，在其他情况下，可能需要从头开始并重新创建`DataFrame`。
- en: Summary
  id: totrans-545
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to use `pandas` for the data collection portion
    of data analysis and to describe our data with statistics, which will be helpful
    when we get to the drawing conclusions phase. We learned the main data structures
    of the `pandas` library, along with some of the operations we can perform on them.
    Next, we learned how to create `DataFrame` objects from a variety of sources,
    including flat files and API requests. Using earthquake data, we discussed how
    to summarize our data and calculate statistics from it. Subsequently, we addressed
    how to take subsets of data via selection, slicing, indexing, and filtering. Finally,
    we practiced adding and removing both columns and rows from our dataframe.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用`pandas`进行数据分析中的数据收集部分，并使用统计数据描述我们的数据，这将在得出结论阶段时派上用场。我们学习了`pandas`库的主要数据结构，以及我们可以对其执行的一些操作。接下来，我们学习了如何从多种来源创建`DataFrame`对象，包括平面文件和API请求。通过使用地震数据，我们讨论了如何总结我们的数据并从中计算统计数据。随后，我们讲解了如何通过选择、切片、索引和过滤来提取数据子集。最后，我们练习了如何添加和删除`DataFrame`中的列和行。
- en: These tasks also form the backbone of our `pandas` workflow and the foundation
    for the new topics we will cover in the next few chapters on data wrangling, aggregation,
    and data visualization. Be sure to complete the exercises provided in the next
    section before moving on.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务也是我们`pandas`工作流的核心，并为接下来几章关于数据清理、聚合和数据可视化的新主题奠定了基础。请确保在继续之前完成下一节提供的练习。
- en: Exercises
  id: totrans-548
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Using the `data/parsed.csv` file and the material from this chapter, complete
    the following exercises to practice your `pandas` skills:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`data/parsed.csv`文件和本章的材料，完成以下练习以练习你的`pandas`技能：
- en: Find the 95th percentile of earthquake magnitude in Japan using the `mb` magnitude
    type.
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`mb`震级类型计算日本地震的95百分位数。
- en: Find the percentage of earthquakes in Indonesia that were coupled with tsunamis.
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出印度尼西亚与海啸相关的地震百分比。
- en: Calculate summary statistics for earthquakes in Nevada.
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算内华达州地震的汇总统计。
- en: Add a column indicating whether the earthquake happened in a country or US state
    that is on the Ring of Fire. Use Alaska, Antarctica (look for Antarctic), Bolivia,
    California, Canada, Chile, Costa Rica, Ecuador, Fiji, Guatemala, Indonesia, Japan,
    Kermadec Islands, Mexico (be careful not to select New Mexico), New Zealand, Peru,
    Philippines, Russia, Taiwan, Tonga, and Washington.
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一列，指示地震是否发生在环太平洋火山带上的国家或美国州。使用阿拉斯加、南极洲（查找Antarctic）、玻利维亚、加利福尼亚、加拿大、智利、哥斯达黎加、厄瓜多尔、斐济、危地马拉、印度尼西亚、日本、克麦得岛、墨西哥（注意不要选择新墨西哥州）、新西兰、秘鲁、菲律宾、俄罗斯、台湾、汤加和华盛顿。
- en: Calculate the number of earthquakes in the Ring of Fire locations and the number
    outside of them.
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算环太平洋火山带内外地震的数量。
- en: Find the tsunami count along the Ring of Fire.
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算环太平洋火山带上的海啸数量。
- en: Further reading
  id: totrans-556
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Those with an R and/or SQL background may find it helpful to see how the `pandas`
    syntax compares:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 具有R和/或SQL背景的人可能会发现查看`pandas`语法的比较会有所帮助：
- en: '*Comparison with R / R Libraries*: https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与R / R库的比较*: https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html'
- en: '*Comparison with SQL*: https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与SQL的比较*: https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html'
- en: '*SQL Queries*: https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SQL查询*: https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html'
- en: 'The following are some resources on working with serialized data:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关于处理序列化数据的资源：
- en: '*Pickle in Python: Object Serialization*: https://www.datacamp.com/community/tutorials/pickle-python-tutorial'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python中的Pickle: 对象序列化*: https://www.datacamp.com/community/tutorials/pickle-python-tutorial'
- en: '*Read RData/RDS files into pandas.DataFrame objects (pyreader)*: https://github.com/ofajardo/pyreadr'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将RData/RDS文件读取到pandas.DataFrame对象中（pyreader）*: https://github.com/ofajardo/pyreadr'
- en: 'Additional resources for working with APIs are as follows:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关于使用API的附加资源：
- en: '*Documentation for the requests package*: https://requests.readthedocs.io/en/master/'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*requests包文档*: https://requests.readthedocs.io/en/master/'
- en: '*HTTP Methods*: https://restfulapi.net/http-methods/'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HTTP 方法*: https://restfulapi.net/http-methods/'
- en: '*HTTP Status Codes*: https://restfulapi.net/http-status-codes/'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HTTP 状态码*: https://restfulapi.net/http-status-codes/'
- en: 'To learn more about regular expressions, consult the following resources:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于正则表达式的知识，请参考以下资源：
- en: '*Mastering Python Regular Expressions by Félix López, Víctor Romero*: https://www.packtpub.com/application-development/mastering-python-regular-expressions'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《精通 Python 正则表达式》 作者：Félix López, Víctor Romero*: https://www.packtpub.com/application-development/mastering-python-regular-expressions'
- en: '*Regular Expression Tutorial — Learn How to Use Regular Expressions*: https://www.regular-expressions.info/tutorial.html'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正则表达式教程 — 学习如何使用正则表达式*: https://www.regular-expressions.info/tutorial.html'
