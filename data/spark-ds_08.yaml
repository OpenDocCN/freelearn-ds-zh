- en: Chapter 8.  Analyzing Unstructured Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。分析非结构化数据
- en: In this Big Data era, the proliferation of unstructured data is overwhelming.
    Numerous methods such as data mining, **Natural Language Processing** (**NLP**),
    information retrieval, and so on, exist for analyzing unstructured data. Due to
    the rapid growth of unstructured data in all kinds of businesses, scalable solutions
    have become the need of the hour. Apache Spark is equipped with out of the box
    algorithms for text analytics, and it also supports custom development of algorithms
    that are not available by default.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个大数据时代，非结构化数据的激增是令人震惊的。存在许多方法，如数据挖掘、自然语言处理（NLP）、信息检索等，用于分析非结构化数据。由于各种业务中非结构化数据的快速增长，可扩展的解决方案已成为当务之急。Apache
    Spark配备了用于文本分析的开箱即用算法，并支持自定义开发默认情况下不可用的算法。
- en: In the previous chapter we have shown how SparkR, an R API to Spark for R programmers
    can harness the power of Spark, without learning a new language .  In this chapter,
    we are going to step into a whole new dimension and explore algorithms and techniques
    to extract information out of unstructured data by leveraging Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们已经展示了SparkR如何利用Spark的R API来发挥其强大功能，而无需学习一种新语言。在本章中，我们将步入一个全新的维度，探索利用Spark从非结构化数据中提取信息的算法和技术。
- en: 'As a prerequisite for this chapter, a basic understanding of programming in
    Python or Scala and an overall understanding of text analytics and machine learning
    are nice to have. However, we have covered some theoretical basics with the right
    set of practical examples to make those more comprehendible and easy to implement.
    The topics covered in this chapter are:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的先决条件，对Python或Scala编程的基本理解以及对文本分析和机器学习的整体理解是很有帮助的。然而，我们已经涵盖了一些理论基础，并提供了一套实际示例，使其更易于理解和实施。本章涵盖的主题包括：
- en: Sources of unstructured data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非结构化数据的来源
- en: Processing unstructured data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理非结构化数据
- en: Count vectorizer
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计数向量化器
- en: TF-IDF
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Stop-word removal
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用词去除
- en: Normalization/scaling
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化/缩放
- en: Word2Vec
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec
- en: n-gram modeling
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n-gram建模
- en: Text classification
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Naive Bayes classifier
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: Text clustering
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本聚类
- en: K-means
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值
- en: Dimensionality reduction
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维
- en: Singular value decomposition
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: Principal component analysis
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Summary
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: Sources of unstructured data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非结构化数据的来源
- en: Data analytics has come very far since the spreadsheets and the BI tools in
    the eighties and nineties. Tremendous improvements in computing power, sophisticated
    algorithms, and an open source culture fueled unprecedented growth in data analytics,
    as well as in other fields. These advances in technologies paved the way for new
    opportunities and new challenges. Businesses started looking at generating insights
    from hitherto impossible to handle data sources such as internal memos, emails,
    customer satisfaction surveys, and the like. Data analytics now encompass this
    unstructured, usually text based data along with traditional rows and columns
    of data. Between the highly structured data stored in RDBMS table and completely
    unstructured plain text, we have semi-structured data sources in NoSQL data stores,
    XML or JSON documents, and graph or network data sources. As per current estimates,
    unstructured data forms about 80 percent of enterprise data and is growing rapidly.
    Satellite images, atmospheric data, social networks, blogs and other web pages,
    patient records and physicians' notes, companies' internal communications, and
    so on - all these combined are just a subset of unstructured data sources.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自上世纪八九十年代的电子表格和商业智能工具以来，数据分析已经取得了长足的进步。计算能力的巨大提升、复杂算法和开源文化的推动，促成了数据分析以及其他领域的前所未有的增长。这些技术的进步为新的机遇和新的挑战铺平了道路。企业开始着眼于从以往难以处理的数据源中生成见解，如内部备忘录、电子邮件、客户满意度调查等。数据分析现在包括这种非结构化的、通常是基于文本的数据，以及传统的行和列数据。在关系型数据库管理系统表中存储的高度结构化数据和完全非结构化的纯文本之间，我们有NoSQL数据存储、XML或JSON文档以及图形或网络数据源等半结构化数据源。根据目前的估计，非结构化数据约占企业数据的80%，并且正在迅速增长。卫星图像、大气数据、社交网络、博客和其他网页、患者记录和医生笔记、公司内部通信等等
    - 所有这些组合只是非结构化数据源的一个子集。
- en: We have already been seeing successful data products that leverage unstructured
    data along with structured data. Some of the companies leverage the power of social
    networks to provide actionable insights to their customers. New fields such as
    **Sentiment Analysis** and **Multimedia Analytics** are emerging to draw insights
    from unstructured data. However, analyzing unstructured data is still a daunting
    feat. For example, contemporary text analytics tools and techniques cannot identify
    sarcasm. However, the potential benefits undoubtedly outweigh the limitations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了成功利用非结构化数据和结构化数据的数据产品。一些公司利用社交网络的力量为他们的客户提供可操作的见解。新兴领域，如情感分析和多媒体分析，正在从非结构化数据中获取见解。然而，分析非结构化数据仍然是一项艰巨的任务。例如，当代文本分析工具和技术无法识别讽刺。然而，潜在的好处无疑超过了局限性。
- en: Processing unstructured data
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理非结构化数据
- en: 'Unstructured data does not lend itself to most of the programming tasks. It
    has to be processed in various different ways as applicable, to be able to serve
    as an input to any machine learning algorithm or for visual analysis. Broadly,
    the unstructured data analysis can be viewed as a series of steps as shown in
    the following diagram:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据不适用于大多数编程任务。它必须以各种不同的方式进行处理，以便作为任何机器学习算法的输入或进行可视化分析。广义上，非结构化数据分析可以被视为以下图表所示的一系列步骤：
- en: '![Processing unstructured data](img/image_08_001.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![处理非结构化数据](img/image_08_001.jpg)'
- en: Data pre-processing is the most vital step in any unstructured data analysis.
    Fortunately, there have been several proven techniques accumulated over time that
    come in handy. Spark offers most of these techniques out of the box through the
    `ml.features` package. Most of the techniques aim to convert text data to concise
    numerical vectors that can be easily consumed by machine learning algorithms.
    Developers should understand the specific requirements of their organizations
    to arrive at the best pre-processing workflow. Remember that better, relevant
    data is the key to generate better insights.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理是任何非结构化数据分析中最关键的步骤。幸运的是，随着时间的积累，已经积累了几种被证明有效的技术，这些技术非常有用。Spark通过`ml.features`包提供了大部分这些技术。大多数技术旨在将文本数据转换为简洁的数字向量，这些向量可以轻松地被机器学习算法消化。开发人员应该了解其组织的具体要求，以确定最佳的预处理工作流程。请记住，更好、相关的数据是产生更好洞察的关键。
- en: Let us explore a couple of examples that process raw text and convert them into
    data frames. First example takes some text as input and extracts all date-like
    strings whereas the second example extracts tags from twitter text. First example
    is just a warm-up, using a simple, regex (regular expression) tokenizer feature
    transformer without using any spark-specific libraries. It also draws your attention
    to the possibility of misinterpretation. For example, a product code of the form
    1-11-1111 may be interpreted as a date. The second example illustrates a non-trivial,
    multi-step extraction process that resulted in just the required tags. **User
    defined functions** (**udf**) and ML pipelines come in handy in developing such
    multi-step extraction processes. Remaining part of this section describes some
    more handy tools supplied out of box in apache Spark.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一些处理原始文本并将其转换为数据框的示例。第一个示例将一些文本作为输入并提取所有类似日期的字符串，而第二个示例从twitter文本中提取标签。第一个示例只是一个热身，使用简单的正则表达式分词器特征转换器，而不使用任何特定于spark的库。它还引起了您对可能的误解的注意。例如，形式为1-11-1111的产品代码可能被解释为日期。第二个示例说明了一个非平凡的、多步骤的提取过程，最终只得到了所需的标签。**用户定义的函数**（**udf**）和ML管道在开发这种多步骤的提取过程中非常有用。本节的其余部分描述了apache
    Spark中提供的一些更方便的工具。
- en: '**Example-1:** **Extract date like strings from text**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例-1：** **从文本中提取类似日期的字符串**'
- en: '**Scala:**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Python:**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding example defined a regular expression pattern to recognize date
    strings. The regex pattern and the sample text DataFrame are passed to the `RegexTokenizer`
    to extract matching, date like strings. The `gaps=False` option picks matching
    strings and a value of `False` would use the given pattern as a separator. Note
    that `1-21-1111`, which is obviously not a date, is also selected.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子定义了一个正则表达式模式，用于识别日期字符串。正则表达式模式和示例文本DataFrame被传递给`RegexTokenizer`以提取匹配的日期字符串。`gaps=False`选项选择匹配的字符串，`False`的值将使用给定的模式作为分隔符。请注意，显然不是日期的`1-21-1111`也被选中。
- en: Next example extracts tags from twitter text and identifies most popular tags.
    You can use the same approach to collect hash (`#`) tags too.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例从twitter文本中提取标签并识别最流行的标签。您也可以使用相同的方法收集哈希（`#`）标签。
- en: This example uses a built in function `explode`, which converts a single row
    with an array of values into multiple rows, one value per array element.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例使用内置函数`explode`，它将单个具有值数组的行转换为多个行，每个数组元素一个值。
- en: "**Example-2: Extract tags from twitter \"\x80\x9Ctext\"\x80\x9D**"
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例-2：从twitter“文本”中提取标签**'
- en: '**Scala:**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Python**:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Count vectorizer
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计数向量化器
- en: Count vectorizer extracts vocabulary (tokens) from documents and generates a
    `CountVectorizerModel` model when a dictionary is not available priori. As the
    name indicates, a text document is converted into a vector of tokens and counts.
    The model produces a sparse representation of the documents over the vocabulary.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 计数向量化器从文档中提取词汇（标记）并在没有字典的情况下生成`CountVectorizerModel`模型。正如其名称所示，文本文档被转换为标记和计数的向量。该模型生成文档对词汇的稀疏表示。
- en: You can fine tune the behavior to limit the vocabulary size, minimum token count,
    and much more as applicable in your business case.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以微调行为以限制词汇量大小、最小标记计数等，使其适用于您的业务案例。
- en: '//Example 3: Count Vectorizer example'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: //示例3：计数向量化器示例
- en: '**Scala**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Python**:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Input**:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：'
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Output**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding example demonstrates how `CountVectorizer` works as an estimator
    to extract the vocabulary and generate a `CountVectorizerModel`. Note that the
    features vector order corresponds to vocabulary and not the input sequence. Let's
    also look at how the same can be achieved by building a dictionary a-priori. However,
    keep in mind that they have their own use cases.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子演示了`CountVectorizer`作为估计器的工作原理，用于提取词汇并生成`CountVectorizerModel`。请注意，特征向量的顺序对应于词汇而不是输入序列。让我们也看看如何通过预先构建字典来实现相同的效果。但是，请记住它们有自己的用例。
- en: 'Example 4: define CountVectorizerModel with a-priori vocabulary'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 示例4：使用先验词汇定义CountVectorizerModel
- en: '**Scala:**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Python**:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: Not available as of Spark 2.0.0
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 截至Spark 2.0.0尚不可用
- en: TF-IDF
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF
- en: The **Term Frequency-Inverse Document Frequency** (**TF-IDF**) is perhaps one
    of the most popular measures in text analytics. This metric indicates the importance
    of a given term in a given document within a set of documents. This consists two
    measurements, **Term Frequency** (**TF**) and **Inverse Document Frequency** (**IDF**).
    Let us discuss them one by one and then see their combined effect.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**词频-逆文档频率**（**TF-IDF**）可能是文本分析中最流行的度量之一。这个度量指示了给定术语在一组文档中的重要性。它包括两个度量，**词频**（**TF**）和**逆文档频率**（**IDF**）。让我们逐一讨论它们，然后看看它们的综合效果。'
- en: TF is a measure of the relative importance of a term in a document, which is
    usually the frequency of that term divided by the number of terms in that document.
    Consider a text document containing 100 words wherein the word *apple* appears
    eight times. The TF for *apple* would be *TF = (8 / 100) = 0.08*. So, the more
    frequently a term occurs in a document, the larger is its TF coefficient.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: TF是一个词在文档中相对重要性的度量，通常是该词频率除以文档中的词数。假设一个文本文档包含100个单词，其中单词*apple*出现了八次。*apple*的TF将是*TF
    = (8 / 100) = 0.08*。因此，一个词在文档中出现的频率越高，其TF系数就越大。
- en: IDF is a measure of the importance of a particular term in the entire collection
    of documents, that is, how infrequently the word occurs across all the documents.
    The importance of a term is inversely proportional to its frequency. Spark provides
    two separate methods to perform these tasks. Assume we have 6 million documents
    and the word *apple* appears in 6000 of these. Then, IDF is calculated as *IDF
    = Log(6,000,000 / 6,000) = 3*. If you observe this carefully, the lower the denominator,
    the higher is the IDF value. This means that the fewer the number of documents
    containing a particular word, the higher would be its importance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: IDF是一个词在整个文档集合中的重要性的度量，也就是说，这个词在所有文档中出现的频率有多低。一个词的重要性与其频率成反比。Spark提供了两种单独的方法来执行这些任务。假设我们有600万个文档，单词*apple*出现在其中的6000个文档中。那么，IDF被计算为*IDF
    = Log(6,000,000 / 6,000) = 3*。如果你仔细观察这个例子，分母越小，IDF值就越大。这意味着包含特定词的文档数量越少，它的重要性就越高。
- en: Thus, the TF-IDF score would be *TF * IDF = 0.08 * 3 = 0.24*. Note that it would
    penalize the words that are more frequent across documents and less important,
    such as *the*, *this*, *a*, and so on, and give more weight to the ones that are
    important.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，TF-IDF得分将是*TF * IDF = 0.08 * 3 = 0.24*。请注意，它会惩罚在文档中更频繁出现且不重要的词，比如*the*、*this*、*a*等，并赋予更重要的词更大的权重。
- en: In Spark, TF is implemented as HashingTF. It takes a sequence of terms (often
    the output of a tokenizer) and produces a fixed length features vector. It performs
    feature hashing to convert the terms into fixed length indices. IDF then takes
    that features vector (the output of HashingTF) as input and scales it based on
    the term frequency in the set of documents. The previous chapter has an example
    of this transformation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，TF被实现为HashingTF。它接受一系列术语（通常是分词器的输出）并产生一个固定长度的特征向量。它执行特征哈希将术语转换为固定长度的索引。然后IDF接受这些特征向量（HashingTF的输出）作为输入，并根据文档集中的词频进行缩放。上一章有一个这种转换的示例。
- en: Stop-word removal
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停用词移除
- en: Common words such as *is*, *was*, and *the* are called stop-words. They do not
    usually add value to analysis and should be dropped during the data preparation
    step. Spark provides `StopWordsRemover` transformer, which does just that. It
    takes a sequence of tokens as a series of string inputs, such as the output of
    a tokenizer, and removes all the stop words. Spark has a stop-words list by default
    that you may override by providing your own stop-words list as a parameter. You
    may optionally turn on `caseSensitive` match which is off by default.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的词如*is*、*was*和*the*被称为停用词。它们通常不会增加分析的价值，并且在数据准备阶段应该被删除。Spark提供了`StopWordsRemover`转换器，它可以做到这一点。它接受一系列标记作为字符串输入的序列，比如分词器的输出，并移除所有的停用词。Spark默认有一个停用词列表，你可以通过提供自己的停用词列表来覆盖它。你可以选择打开默认关闭的`caseSensitive`匹配。
- en: 'Example 5: Stopword Remover'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 示例5：停用词移除
- en: '**Scala:**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Python:**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Assume that we have the following DataFrame with columns `id` and `raw_text`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下带有`id`和`raw_text`列的DataFrame：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After applying `StopWordsRemover` with `raw_text` as the input column and `processed_text`
    as the output column for the preceding example, we should get the following output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在对前面的示例应用`StopWordsRemover`，将`raw_text`作为输入列，`processed_text`作为输出列后，我们应该得到以下输出：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Normalization/scaling
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归一化/缩放
- en: Normalization is a common and preliminary step in data preparation. Most of
    the machine learning algorithms work better when all features are on the same
    scale. For example, if there are two features where the value of one is about
    100 times greater than the other, bringing them to the same scale reflects meaningful
    relative activity between the two variables. Any non-numeric values, such as high,
    medium, and low, should ideally be converted to appropriate numerical quantification
    as a best practice. However, you need to be careful in doing so as it may require
    domain expertise. For example, if you assign 3, 2, and 1 for high, medium, and
    low respectively, then it should be checked that these three units are equidistant
    from each other.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化是数据准备中常见的预处理步骤。大多数机器学习算法在所有特征处于相同尺度时效果更好。例如，如果有两个特征，其中一个的值大约是另一个的100倍，将它们调整到相同的尺度可以反映出两个变量之间有意义的相对活动。任何非数值的值，比如高、中、低，最好都转换为适当的数值量化作为最佳实践。然而，在这样做时需要小心，因为可能需要领域专业知识。例如，如果你为高、中、低分别分配3、2和1，那么应该检查这三个单位是否相互等距。
- en: The common methods of feature normalization are *scaling*, *mean subtraction*,
    and *feature standardization*, just to name a few. In scaling, each numerical
    feature vector is rescaled such that its value range is between *-1* to *+1* or
    *0* to *1* or something similar. In mean subtraction, you compute mean of a numerical
    feature vector and subtract that mean from each of the values. We are interested
    in the relative deflection from the mean, while the absolute value could be immaterial.
    Feature standardization refers to setting the data to zero mean and unit (1) variance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 特征归一化的常见方法包括*缩放*、*均值减法*和*特征标准化*，仅举几例。在缩放中，每个数值特征向量被重新缩放，使其值范围在*-1*到*+1*或*0*到*1*或类似的范围内。在均值减法中，你计算数值特征向量的均值，并从每个值中减去该均值。我们对相对于均值的相对偏差感兴趣，而绝对值可能并不重要。特征标准化是指将数据设置为零均值和单位（1）方差。
- en: Spark provides a `Normalizer` feature transformer to normalize each vector to
    have unit norm; `StandardScaler` to have unit norm and zero mean; and `MinMaxScaler`
    to rescale each feature to a specific range of values. By default, min and max
    are 0 and 1 but you may set the value parameters yourself as per the data requirement.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了`Normalizer`特征转换器，将每个向量归一化为单位范数；`StandardScaler`将单位范数和零均值；`MinMaxScaler`将每个特征重新缩放到特定范围的值。默认情况下，最小值和最大值为0和1，但您可以根据数据要求自行设置值参数。
- en: Word2Vec
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Vec
- en: The Word2Vec is a type of PCA (you will find out more about this shortly) that
    takes a sequence of words and produces a map (of string, vector). The string is
    the word and the vector is a unique fixed size vector. The resulting word vector
    representation is useful in many machine learning and NLP applications, such as
    named entity recognition and tagging. Let us look at an example.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是一种PCA（您很快会了解更多）的类型，它接受一系列单词并生成一个映射（字符串，向量）。字符串是单词，向量是一个独特的固定大小的向量。生成的单词向量表示在许多机器学习和自然语言处理应用中非常有用，比如命名实体识别和标记。让我们看一个例子。
- en: '**Example 6: Word2Vec**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例6：Word2Vec**'
- en: '**Scala**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Python:**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: n-gram modelling
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: n-gram建模
- en: 'An n-gram is a contiguous sequence of *n* items from a given sequence of text
    or speech. An n-gram of size *1* is referred to as a *unigram*, size *2* is a
    *bigram*, and size *3* is a *trigram*. Alternatively, they can be referred to
    by the value of *n*, for example, four-gram, five-gram, and so on. Let us take
    a look at an example to understand the possible outcomes of this model:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram是给定文本或语音序列中*n*个项目的连续序列。大小为*1*的n-gram称为*unigram*，大小为*2*的称为*bigram*，大小为*3*的称为*trigram*。或者，它们可以根据*n*的值来命名，例如four-gram，five-gram等。让我们看一个例子来理解这个模型可能的结果：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is an example of words to n-gram letters. The same is the case for sentence
    (or tokenized words) to n-gram words. For example, the 2-gram equivalent of the
    sentence *Kids love to eat chocolates* is:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个单词到n-gram字母的例子。对于句子（或标记化的单词）到n-gram单词也是一样的。例如，句子*Kids love to eat chocolates*的2-gram等效于：
- en: '''Kids love'', ''love to'', ''to eat'', ''eat chocolates''.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '''Kids love'', ''love to'', ''to eat'', ''eat chocolates''.'
- en: There are various applications of n-gram modelling in text mining and NLP. One
    of the examples is predicting the probability of each word occurring given a prior
    context (conditional probability).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram建模在文本挖掘和自然语言处理中有各种应用。其中一个例子是预测每个单词在先前上下文中出现的概率（条件概率）。
- en: In Spark, `NGram` is a feature transformer that converts the input array (for
    example, the output of a Tokenizer) of strings into an array of n-grams. Null
    values in the input array are ignored by default. It returns an array of n-grams
    where each n-gram is represented by a space-separated string of words.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，`NGram`是一个特征转换器，它将字符串的输入数组（例如，分词器的输出）转换为n-gram的数组。默认情况下，输入数组中的空值将被忽略。它返回一个n-gram的数组，其中每个n-gram由一个用空格分隔的单词字符串表示。
- en: '**Example 7: NGram**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例7：NGram**'
- en: '**Scala**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Python:**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Text classification
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类
- en: Text classification is about assigning a topic, subject category, genre, or
    something similar to the text blob. For example, spam filters assign spam or not
    spam to an email.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类是指将主题、主题类别、流派或类似内容分配给文本块。例如，垃圾邮件过滤器将垃圾邮件或非垃圾邮件分配给电子邮件。
- en: Apache Spark supports various classifiers through MLlib and ML packages. The
    SVM classifier and Naive Bayes classifier are popular classifiers, and the former
    was already covered in the previous chapter. Let's take a look at the latter now.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark通过MLlib和ML包支持各种分类器。SVM分类器和朴素贝叶斯分类器是流行的分类器，前者已经在前一章中介绍过。现在让我们来看看后者。
- en: Naive Bayes classifier
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: The **Naive Bayes** (**NB**) classifier is a multiclass probabilistic classifier
    and is one of the best classification algorithms. It assumes strong independence
    between every pair of features. It computes the conditional probability distribution
    of each feature and a given label, and then applies Bayes' theorem to compute
    the conditional probability of a label given an observation. In terms of document
    classification, an observation is a document to be classified into some class.
    Despite its strong assumptions on data, it is quite popular. It works with small
    amount of training data - whether real or discrete. It works very efficiently
    because it takes a single pass through the training data; one constraint is that
    the feature vectors must be non-negative. By default, ML package supports multinomial
    NB. However, you may set the parameter `modelType` to `Bernoulli` if bernoulli
    NB is required.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**（**NB**）分类器是一种多类概率分类器，是最好的分类算法之一。它假设每对特征之间有很强的独立性。它计算每个特征和给定标签的条件概率分布，然后应用贝叶斯定理来计算给定观察结果的标签的条件概率。在文档分类方面，观察结果是要分类到某个类别的文档。尽管它对数据有很强的假设，但它非常受欢迎。它可以处理少量的训练数据-无论是真实的还是离散的。它非常高效，因为它只需一次通过训练数据；一个约束是特征向量必须是非负的。默认情况下，ML包支持多项式NB。但是，如果需要伯努利NB，可以将参数`modelType`设置为`Bernoulli`。'
- en: The **laplace smoothing** technique may be applied by specifying the smoothing
    parameters and is extremely useful in situations where you want to assign a small
    non-zero probability to a rare word or new word so that the posterior probabilities
    do not suddenly drop to zero.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**拉普拉斯平滑**技术可以通过指定平滑参数来应用，并且在您想要为罕见的单词或新单词分配一个小的非零概率以使后验概率不会突然降为零的情况下非常有用。'
- en: 'Spark also provides some other hyper parameters such as `thresholds` also to
    gain fine grain control. Here is an example that categorizes twitter text. This
    example contains some hand-coded rules that assign a category to the train data.
    A particular category is assigned if any of the corresponding words are found
    in the text. For example, the category is "survey" if text contains "survey" or
    "poll". The model is trained based on this train data and evaluated on a different
    text sample collected at a different time:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Spark还提供了一些其他超参数，如`thresholds`，以获得细粒度控制。以下是一个将Twitter文本分类的示例。此示例包含一些手工编码的规则，将类别分配给训练数据。如果文本包含相应的单词，则分配特定的类别。例如，如果文本包含"survey"或"poll"，则类别为"survey"。该模型是基于此训练数据进行训练的，并且在不同时间收集的不同文本样本上进行评估。
- en: '**Example 8: Naive Bayes**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例8：朴素贝叶斯**'
- en: '**Scala:**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Scala：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Python:**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Python：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Once this is done, a model can be trained with the output of this step, which
    can classify a text blob or file.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，可以使用此步骤的输出训练模型，该模型可以对文本块或文件进行分类。
- en: Text clustering
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本聚类
- en: Clustering is an unsupervised learning technique. Intuitively, clustering groups
    objects into disjoint sets. We do not know how many groups exist in the data,
    or what might be the commonality within these groups (clusters).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督学习技术。直观地，聚类将对象分成不相交的集合。我们不知道数据中存在多少组，或者这些组（簇）之间可能存在什么共性。
- en: Text clustering has several applications. For example, an organizational entity
    may want to organize its internal documents into similar clusters based on some
    similarity measure. The notion of similarity or distance is central to the clustering
    process. Common measures used are TF-IDF and cosine similarity. Cosine similarity,
    or the cosine distance, is the cos product of the word frequency vectors of two
    documents. Spark provides a variety of clustering algorithms that can be effectively used
    in text analytics.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 文本聚类有几个应用。例如，组织实体可能希望根据某种相似度度量将其内部文档组织成相似的簇。相似性或距离的概念对聚类过程至关重要。常用的度量包括TF-IDF和余弦相似度。余弦相似度或余弦距离是两个文档的词频向量的余弦乘积。Spark提供了各种聚类算法，可以有效地用于文本分析。
- en: K-means
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means
- en: Perhaps K-means is the most intuitive of all the clustering algorithms. The
    idea is to segregate data points as *K* different clusters based on some similarity
    measure, say cosine distance or Euclidean distance. This algorithm that starts
    with *K* random single point clusters, and each of the remaining data points are
    assigned to nearest cluster. Then cluster centers are recomputed and the algorithm
    loops through the data points once again. This process continues iteratively until
    there are no re-assignments or when pre-defined iteration count is reached.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 也许K-means是所有聚类算法中最直观的。其想法是根据某种相似度度量（如余弦距离或欧几里得距离）将数据点分隔为*K*个不同的簇。该算法从*K*个随机单点簇开始，然后将其余数据点分配到最近的簇。然后重新计算簇中心，并且算法再次循环遍历数据点。这个过程迭代地继续，直到没有重新分配或达到预定义的迭代次数为止。
- en: How to fix the number of clusters (*K*) is not obvious. Identifying the initial
    cluster centers is also not obvious. Sometimes the business requirement may dictate
    the number of clusters; for example, partition all existing documents into 10
    different sections. But in most of the real world scenarios, we need to find *K*
    through trial and error. One way is to progressively increase the *K* value and
    compute the cluster quality, such as cluster variance. The quality ceases to improve
    significantly beyond a certain value of *K,* which could be your ideal *K*. There
    are various other techniques, such as the elbow method, **Akaike information criterion**
    (**AIC**), and **Bayesian information criterion** (**BIC**).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如何确定簇的数量（*K*）并不明显。确定初始簇中心也不明显。有时业务需求可能决定簇的数量；例如，将所有现有文档分成10个不同的部分。但在大多数真实世界的场景中，我们需要通过试错找到*K*。一种方法是逐渐增加*K*值并计算簇质量，例如簇方差。在某个*K*值之后，质量停止显着改善，这可能是您理想的*K*。还有其他各种技术，如肘部法，**阿卡奇信息准则**（**AIC**）和**贝叶斯信息准则**（**BIC**）。
- en: Likewise, start with different starting points until the cluster quality is
    satisfactory. Then you may wish to validate your result using techniques such
    as Silhouette Score. However, these activities are computationally intensive.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，从不同的起始点开始，直到簇的质量令人满意。然后您可能希望使用Silhouette Score等技术验证您的结果。但是，这些活动需要大量计算。
- en: Spark provides K-means from MLlib as well as ml packages. You may specify maximum
    iterations or convergence tolerance to fine tune algorithm performance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了来自MLlib和ml包的K-means。您可以指定最大迭代次数或收敛容差来微调算法性能。
- en: Dimensionality reduction
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: Imagine a large matrix with many rows and columns. In many matrix applications,
    this large matrix can be represented by some narrow matrices with small number
    of rows and columns that still represents the original matrix. Then processing
    this smaller matrix may yield similar results as that of the original matrix.
    This can be computationally efficient.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个有许多行和列的大矩阵。在许多矩阵应用中，这个大矩阵可以由一些窄矩阵代表，这些窄矩阵具有少量行和列，但仍代表原始矩阵。然后处理这个较小的矩阵可能会产生与原始矩阵相似的结果。这可能是计算效率高的。
- en: Dimensionality reduction is about finding that small matrix. MLLib supports
    two algorithms, SVD and PCA for dimensionality reduction on RowMatrix class. Both
    of these  algorithms allow us to specify the number of dimensions we are interested
    in retaining. Let us look at example first and then delve into the underlying
    theory .
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 降维是关于找到那个小矩阵的。MLLib支持RowMatrix类的两种算法，SVD和PCA，用于降维。这两种算法都允许我们指定我们感兴趣的保留维度的数量。让我们先看一个例子，然后深入研究其中的理论。
- en: '**Example 9: Dimensionality reduction**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例9：降维**'
- en: '**Scala:**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Scala：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Python:**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Python：
- en: Not available in Python as of Spark 2.0.0
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 截至Spark 2.0.0，Python中不可用。
- en: Singular Value Decomposition
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: The **Singular Value Decomposition** (**SVD**) is one of the centerpieces of
    linear algebra and is widely used for many real-world modeling requirements. It
    provides a convenient way of breaking a matrix into simpler, smaller matrices.
    This leads to a low-dimensional representation of a high-dimensional matrix. It
    helps us eliminate less important parts of the matrix to produce an approximate
    representation. This technique is useful in dimensionality reduction and data
    compression.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）是线性代数的重要组成部分之一，广泛用于许多实际建模需求。它提供了一种将矩阵分解为更简单、更小的矩阵的便捷方法。这导致了高维矩阵的低维表示。它帮助我们消除矩阵中不太重要的部分，以产生一个近似表示。这种技术在降维和数据压缩中非常有用。'
- en: Let *M* be a matrix of size m-rows and n-columns. The rank of a matrix is the
    number of rows that are linearly independent. A row is considered independent
    if it has at least one non-zero element and it is not a linear combination of
    one or more rows. The same rank will be obtained if we considered columns instead
    of rows - as in linear algebra.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 设*M*是一个大小为m行n列的矩阵。矩阵的秩是线性无关的行数。如果一行至少有一个非零元素，并且不是一个或多个行的线性组合，则该行被认为是独立的。如果我们考虑列而不是行，那么将得到相同的秩
    - 就像在线性代数中一样。
- en: 'If the elements of one row are the sum of two rows, then that row is not independent.
    Then as a result of SVD, we find three matrices, *U*, *∑*, and *V* that satisfy
    the following equation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一行的元素是两行的和，则该行不是独立的。因此，作为SVD的结果，我们找到了满足以下方程的三个矩阵*U*、*∑*和*V*：
- en: '*M = U∑VT*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*M = U∑VT*'
- en: 'These three matrices have the following properties:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个矩阵具有以下特性：
- en: '**U**: This is a column-orthonormal matrix with m rows and r columns. An orthonormal
    matrix implies that each of the columns is a unit vector and the pairwise dot
    product between any two columns is 0.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**U**：这是一个具有m行和r列的列正交矩阵。正交矩阵意味着每个列都是单位向量，并且任意两列之间的点积为0。'
- en: '**V**: This is a column-orthonormal matrix with *n* rows and *r* columns.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**V**：这是一个具有*n*行和*r*列的列正交矩阵。'
- en: '**∑**: This is an *r* x *r* diagonal matrix with non-negative real numbers
    as principal diagonal values in descending order. In a diagonal matrix, all elements
    except the ones on the principal diagonal are zero.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**∑**：这是一个*r* x *r*对角矩阵，其主对角线值为非负实数，按降序排列。在对角矩阵中，除了主对角线上的元素外，其他元素都是零。'
- en: 'The principal diagonal values in the *∑* matrix are called singular values.
    They are considered as the underlying *concepts* or *components* that connect
    the rows and columns of the matrix. Their magnitude represents the strength of
    the corresponding components. For example, imagine that the matrix in the previous
    example contains ratings of five books by six readers. SVD allows us to split
    them into three matrices: *∑* containing the singular values representing the
    *strength* of underlying topics; *U* connecting people to concepts; and *V* connecting
    concepts to books.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*∑*矩阵中的主对角线值被称为奇异值。它们被认为是连接矩阵的行和列的基本*概念*或*组件*。它们的大小代表了相应组件的强度。例如，想象一下，前面例子中的矩阵包含了六个读者对五本书的评分。SVD允许我们将它们分成三个矩阵：*∑*包含奇异值，代表了基本主题的*强度*；*U*将人连接到概念；*V*将概念连接到书籍。'
- en: In a large matrix, we can replace the lower magnitude singular values to zero
    and thereby reduce the corresponding rows in the remaining two matrices. Note
    that if we re-compute the matrix product on the right hand side and compare the
    value with the original matrix on the left hand side, they will be almost similar.
    We can use this technique to retain the desired number of dimensions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个大矩阵中，我们可以将较低幅度的奇异值替换为零，从而减少剩余两个矩阵中的相应行。请注意，如果我们在右侧重新计算矩阵乘积，并将其与左侧的原始矩阵进行比较，它们将几乎相似。我们可以使用这种技术来保留所需的维度。
- en: Principal Component Analysis
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析
- en: "**Principal Component Analysis** (**PCA**) is a technique that takes n-dimensional\
    \ data points and project onto a smaller (fewer dimensions) subspace with minimum\
    \ loss of information. A set of data points in a high dimensional space find the\
    \ directions along which these tuples line up best. In other words, we need to\
    \ find a rotation such that the first coordinate has the largest variance possible,\
    \ and each succeeding coordinate in turn has the largest variance possible. The\
    \ idea is to treat the set of tuples as a matrix *M* and fi\x81nd the eigenvectors\
    \ for MMT."
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）是一种将n维数据点投影到具有最小信息损失的较小（更少维度）子空间的技术。在高维空间中，一组数据点找到了这些元组最佳排列的方向。换句话说，我们需要找到一个旋转，使得第一个坐标具有可能的最大方差，然后依次每个坐标具有可能的最大方差。这个想法是将元组集合视为矩阵*M*，并找到MMT的特征向量。'
- en: If *A* is a square matrix, *e* is a column matrix with the same number of rows
    as *A*, and *λ* is a constant such that *Me = λe*, then *e* is called the eigenvector
    of *M* and *λ* is called the eigenvalue of *M*. In terms of n-dimensional plane,
    the eigenvector is the direction and the eigenvalue is a measure of variance along
    that direction. We can drop the dimensions with a low eigenvalue, thereby finding
    a smaller subspace without loss of information.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*A*是一个方阵，*e*是一个列矩阵，行数与*A*相同，*λ*是一个常数，使得*Me = λe*，那么*e*被称为*M*的特征向量，*λ*被称为*M*的特征值。在n维平面上，特征向量是方向，特征值是沿着该方向的方差的度量。我们可以丢弃特征值较低的维度，从而找到一个更小的子空间，而不会丢失信息。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we examined the sources of unstructured data and the motivation
    behind analyzing the unstructured data. We explained various techniques that are
    required in pre-processing unstructured data and how Spark provides most of these
    tools out of the box. We also covered some of the algorithms supported by Spark
    that can be used in text analytics.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了非结构化数据的来源以及分析非结构化数据背后的动机。我们解释了预处理非结构化数据所需的各种技术，以及Spark如何提供大部分这些工具。我们还介绍了Spark支持的一些可用于文本分析的算法。
- en: In the next chapter, we will go through different types of visualization techniques
    that are insightful in different stages of data analytics lifecycle.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍不同类型的可视化技术，这些技术在数据分析生命周期的不同阶段都具有洞察力。
- en: 'References:'
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料：
- en: 'The following are the references:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是参考资料：
- en: '[http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf](http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf](http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf)'
- en: '[https://www.cs.nyu.edu/web/Research/Theses/borthwick_andrew.pdf](https://www.cs.nyu.edu/web/Research/Theses/borthwick_andrew.pdf)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.cs.nyu.edu/web/Research/Theses/borthwick_andrew.pdf](https://www.cs.nyu.edu/web/Research/Theses/borthwick_andrew.pdf)'
- en: '[https://web.stanford.edu/class/cs124/lec/naivebayes.pdf](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://web.stanford.edu/class/cs124/lec/naivebayes.pdf](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)'
- en: '[http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html](http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html](http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)'
- en: '[http://www.mmds.org/](http://www.mmds.org/)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.mmds.org/](http://www.mmds.org/)'
- en: '[http://sebastianraschka.com/Articles/2014_pca_step_by_step.html](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://sebastianraschka.com/Articles/2014_pca_step_by_step.html](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)'
- en: '[http://arxiv.org/pdf/1404.1100.pdf](http://arxiv.org/pdf/1404.1100.pdf)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://arxiv.org/pdf/1404.1100.pdf](http://arxiv.org/pdf/1404.1100.pdf)'
- en: '[http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html](http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html](http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html)'
- en: 'Count Vectorizer:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 计数向量化器：
- en: '[https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/ml/feature/CountVectorizer.html](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/ml/feature/CountVectorizer.html)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/ml/feature/CountVectorizer.html](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/ml/feature/CountVectorizer.html)'
- en: 'n-gram modeling:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram建模：
- en: '[https://en.wikipedia.org/wiki/N-gram](https://en.wikipedia.org/wiki/N-gram)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/N-gram](https://en.wikipedia.org/wiki/N-gram)'
