- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Building Blocks of Deep Learning for Time Series
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列的深度学习构建模块
- en: While we laid the foundations of deep learning in the previous chapter, it was
    very general. Deep learning is a vast field with applications in all possible
    domains, but in this book, we will focus on the applications in time series forecasting.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在上一章奠定了深度学习的基础，但那是非常通用的。深度学习是一个庞大的领域，应用涉及各个领域，但在本书中，我们将重点讨论其在时间序列预测中的应用。
- en: So in this chapter, let’s strengthen the foundation by looking at a few building
    blocks of deep learning that are commonly used in time series forecasting. Even
    though the global machine learning models perform well in time series problems,
    some deep learning approaches have also shown good promise. They are a good addition
    to your toolset due to the flexibility they allow when modeling.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，让我们通过查看一些深度学习中常用于时间序列预测的构建模块来强化基础。尽管全球机器学习模型在时间序列问题中表现良好，但一些深度学习方法也显示出了良好的前景。由于它们在建模时的灵活性，它们是你工具箱中的一个良好补充。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将涵盖以下内容：
- en: Understanding the encoder-decoder paradigm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解编码器-解码器范式
- en: Feed-forward networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络
- en: Recurrent neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Long short-term memory networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: Gated recurrent unit
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 门控循环单元
- en: Convolution networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积网络
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need to set up the **Anaconda** environment, following the instructions
    in the *Preface* of the book, to get a working environment with all the libraries
    and datasets required for the code in this book. Any additional libraries will
    be installed while running the notebooks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要设置**Anaconda**环境，按照本书*前言*中的说明进行操作，以获得一个包含所有所需库和数据集的工作环境。任何额外的库将在运行笔记本时自动安装。
- en: The associated code for this chapter can be found at [https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter12](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter12).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章相关的代码可以在[https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter12](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter12)找到。
- en: Understanding the encoder-decoder paradigm
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解编码器-解码器范式
- en: 'In *Chapter 5*, *Time Series Forecasting as Regression*, we saw that machine
    learning is all about learning a function that maps our inputs to the desired
    output:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5章*，*时间序列预测作为回归问题*中，我们看到机器学习的核心就是学习一个将我们的输入映射到期望输出的函数：
- en: '*y* = *h*(*x*)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *h*(*x*)'
- en: where *x* is the input and *y* is our desired output.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*x* 是输入，*y* 是我们期望的输出。
- en: 'Adapting this to time series forecasting (using univariate time series forecasting
    to keep things simple), we can rewrite it as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 将其应用于时间序列预测（为了简化起见，使用单变量时间序列预测），我们可以将其重写如下：
- en: '*y*[t] = *h*(*y*[t][-1], *y*[t][-2], …, *y*[t-N])'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*[t] = *h*(*y*[t][-1], *y*[t][-2], …, *y*[t-N])'
- en: Here, *t* is the current timestep and *N* is the total amount of history available
    at time *t*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*t* 是当前时间步，*N* 是在时间*t*时可用的历史总量。
- en: Deep learning, like any other machine learning approach, is tasked with learning
    this function, which maps history to the future. In *Chapter 11*, *Introduction
    to Deep Learning*, we saw how deep learning learns good features using representation
    learning, and then it uses the learned features to carry out the task at hand.
    This understanding can be further refined to the time series perspective by using
    the encoder-decoder paradigm.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，像任何其他机器学习方法一样，旨在学习一个将历史映射到未来的函数。在*第11章*，*深度学习简介*中，我们看到深度学习如何通过表示学习来学习良好的特征，然后使用这些学习到的特征来执行当前任务。通过使用编码器-解码器范式，这一理解可以进一步从时间序列的角度进行细化。
- en: Like everything in research, it is not entirely clear when and who proposed
    this idea of the encoder-decoder architecture. In 1997, Ramon Neco and Mikel Forcada
    proposed an architecture for machine translation that had ideas reminiscent of
    the encoder-decoder paradigm. In 2013, Nal Kalchbrenner and Phil Blunsom proposed
    an encoder-decoder model for machine translation, although they did not call it
    that. But it was when Ilya Sutskever et al. (2014) and Cho et al. (2014) proposed
    two new models for machine translation that worked independently that this idea
    took off. Cho et al. called it the encoder-decoder architecture, while Sutskever
    et al. called it the Seq2Seq architecture. The key innovation it drove was the
    ability to model variable-length inputs and outputs in an end-to-end fashion.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 就像研究中的所有内容一样，关于编码器-解码器架构的提出时间和提议者并不完全明确。1997年，Ramon Neco 和 Mikel Forcada 提出了一个机器翻译架构，其理念与编码器-解码器范式相似。2013年，Nal
    Kalchbrenner 和 Phil Blunsom 提出了一个机器翻译的编码器-解码器模型，尽管他们没有使用这个名称。但当 Ilya Sutskever
    等人（2014年）和 Cho 等人（2014年）分别提出了两种独立的机器翻译新模型时，这一理念才真正兴起。Cho 等人称之为编码器-解码器架构，而 Sutskever
    等人称之为 Seq2Seq 架构。它的关键创新是能够以端到端的方式建模可变长度的输入和输出。
- en: '**Reference check**:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research papers by Ramon Neco et al., Nal Kalchbrenner et al., Cho et al.,
    and Ilya Sutskever et al. are cited in the *References* section as *1*, *2*, *3*,
    and *4*, respectively.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Ramon Neco 等人、Nal Kalchbrenner 等人、Cho 等人以及 Ilya Sutskever 等人的研究论文分别在*参考文献*部分被标注为*1*、*2*、*3*和*4*。
- en: The idea is very straightforward, but before we get into that, we need to have
    a high-level understanding of latent spaces and feature/input spaces.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法非常直接，但在我们深入探讨之前，我们需要对潜在空间和特征/输入空间有一个高层次的理解。
- en: The **feature space**, or the **input space**, is the vector space where your
    data resides. If the data has 10 dimensions, then the input space is the 10-dimensional
    vector space. Latent space is an abstract vector space that encodes a meaningful
    internal representation of the feature space. To understand this, we can think
    about how we, as humans, recognize a tiger. We do not remember every minute detail
    of a tiger; we just have a general idea of what a tiger looks like and its prominent
    features, such as its stripes. It is a compressed understanding of this concept
    that helps our brains process and recognize a tiger faster.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征空间**，或 **输入空间**，是数据所在的向量空间。如果数据有 10 个维度，那么输入空间就是 10 维的向量空间。潜在空间是一个抽象的向量空间，它编码了特征空间的有意义的内部表示。为了理解这一点，我们可以想象人类如何识别老虎。我们不会记住老虎的每一个细节，而是对老虎的外观和显著特征（如条纹）有一个大致的了解。这种压缩的理解帮助我们的大脑更快地处理和识别老虎。'
- en: In the machine learning domain, there are techniques like **Principal Component
    Analysis** (**PCA**) that do similar transformations to a latent space, preserving
    the essential features of the input data. With this intuition, rereading the definition
    may bring a bit more clarity to the concept.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，像**主成分分析**（**PCA**）这样的技术会对潜在空间进行类似的转换，保持输入数据的关键特征。通过这个直觉，重新阅读定义可能会让我们对这个概念有更清晰的理解。
- en: Now that we have an idea about latent spaces, let’s see what an encoder-decoder
    architecture does.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对潜在空间有了一些了解，让我们来看一下编码器-解码器架构的作用。
- en: 'An encoder-decoder architecture has two main parts—an encoder and a decoder:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器架构有两个主要部分——编码器和解码器：
- en: '**Encoder**: The encoder takes in the input vector, *x*, and encodes it into
    a latent space. This encoded representation is called the latent vector, *z*.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：编码器接收输入向量 *x*，并将其编码为潜在空间。这个编码后的表示被称为潜在向量 *z*。'
- en: '**Decoder**: The decoder takes in the latent vector, *z*, and decodes it into
    the kind of output we need (![](img/B22389_05_001.png)).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：解码器接收潜在向量 *z*，并将其解码成我们所需的输出形式（![](img/B22389_05_001.png)）。'
- en: 'The following diagram shows the encoder-decoder setup visually:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了编码器-解码器的配置：
- en: '![Figure 12.1 – The encoder-decoder architecture ](img/B22389_12_01.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.1 – 编码器-解码器架构](img/B22389_12_01.png)'
- en: 'Figure 12.1: The encoder-decoder architecture'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1：编码器-解码器架构
- en: 'In the context of time series forecasting, the encoder consumes the history
    and retains the information that is required for the decoder to generate the forecast.
    As we learned previously, time series forecasting can be written as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列预测的背景下，编码器消耗历史数据，并保留解码器生成预测所需的信息。正如我们之前所学，时间序列预测可以表示为如下：
- en: '*y*[t] = *h*(*y*[t][-1], *y*[t][-2], …, *y*[t-N])'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*[t] = *h*(*y*[t][-1], *y*[t][-2], …, *y*[t-N])'
- en: 'Now, using the encoder-decoder paradigm, we can rewrite it as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用编码器-解码器范式，我们可以将其重写如下：
- en: '*z*[t] = *h*(*y*[t][-1], *y*[t][-2], …, *y*[t-N])'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*z*[t] = *h*(*y*[t][-1], *y*[t][-2], …, *y*[t-N])'
- en: '*y*[t] = *g*(*z*[t])'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*[t] = *g*(*z*[t])'
- en: Here, *h* is the encoder and *g* is the decoder.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*h* 是编码器，*g* 是解码器。
- en: Each encoder and decoder can be some special architecture suited for time series
    forecasting. Let’s look at a few common components that are used in the encoder-decoder
    paradigm.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 每个编码器和解码器都可以是适合时间序列预测的特殊架构。让我们来看一下在编码器-解码器范式中常用的几个组件。
- en: Feed-forward networks
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈网络
- en: '**Feed-forward networks** (**FFNs**) or **fully connected networks** are the
    most basic architecture a neural network can take. We discussed perceptrons in
    *Chapter 11*, *Introduction to Deep Learning*. If we stack multiple perceptrons
    (both linear units and non-linear activations) and create a network of such units,
    we get what we call an FFN. The following diagram will help us understand this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**前馈网络**（**FFNs**）或**全连接网络**是神经网络可以采用的最基本架构。我们在*第11章*《深度学习导论》中讨论了感知器。如果我们将多个感知器（包括线性单元和非线性激活）堆叠起来并创建一个这样的单元网络，我们就得到了我们所说的FFN。下面的图示将帮助我们理解这一点：'
- en: '![Figure 12.2 – Feed-forward network ](img/B22389_12_02.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.2 – 前馈网络](img/B22389_12_02.png)'
- en: 'Figure 12.2: A Feed Forward Network (FFN)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2：前馈网络（FFN）
- en: An FFN takes a fixed-size input vector and passes it through a series of computational
    layers leading up to the desired output. This architecture is called feed-forward
    because the information is fed forward through the network. This is also called
    a **fully connected network** because every unit in a layer is connected to every
    unit in the previous layer and every unit in the next layer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 FFN 接受一个固定大小的输入向量，并通过一系列计算层传递，直到得到所需的输出。该架构称为前馈，因为信息是通过网络向前传递的。这也被称为**全连接网络**，因为每一层的每个单元都与前一层的每个单元和下一层的每个单元相连接。
- en: The first layer is called the input layer, and this is equal to the dimension
    of the input. The last layer is called the output layer, which is defined as per
    our desired output. If we need a single output, we will need 1 unit, while if
    we need 10 outputs, we will need 10 units. All the layers in between are called
    **hidden layers**. Two hyperparameters define the structure of the network—the
    number of hidden layers and the number of units in each layer. For instance, in
    *Figure 12.2*, we have a network with two hidden layers and eight units per layer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层称为输入层，其大小等于输入的维度。最后一层称为输出层，其定义根据我们的期望输出。如果我们需要一个输出，就需要一个单元；如果我们需要10个输出，就需要10个单元。中间的所有层称为**隐藏层**。有两个超参数定义了网络的结构——隐藏层的数量和每层单元的数量。例如，在*图12.2*中，我们有一个具有两个隐藏层且每层有八个单元的网络。
- en: In the time series forecasting context, an FFN can be used as an encoder as
    well as a decoder. As an encoder, we can use an FFN just like we used machine
    learning models in *Chapter 5*, *Time Series Forecasting as Regression*. We embed
    time and convert a time series problem into a regression problem before feeding
    it into the FFN. As a decoder, we use it on the latent vector (the output from
    the encoder) to get to the output (this is the most common usage of an FFN in
    time series forecasting).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列预测的背景下，FFN可以作为编码器和解码器使用。作为编码器，我们可以像在*第5章*《时间序列预测作为回归》中使用机器学习模型一样使用FFN。我们嵌入时间并将时间序列问题转化为回归问题，然后输入到FFN中。作为解码器，我们在潜在向量（编码器的输出）上使用它来获得输出（这是FFN在时间序列预测中最常见的用法）。
- en: '**Additional reading**:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**额外阅读**：'
- en: We are going to be using PyTorch throughout this book to work with deep learning.
    If you are not comfortable with PyTorch, don’t worry—I’ll try and explain the
    concepts when necessary. To get a head start, you can go through the `01-PyTorch_Basics.ipynb`
    notebook in `Chapter12`, where we have explored the basic functionalities of tensors
    and trained a very small neural network from scratch using PyTorch. I also suggest
    heading over to the *Further reading* section at the end of this chapter, where
    you’ll find a few resources to learn PyTorch.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将始终使用 PyTorch 来处理深度学习。如果你不熟悉 PyTorch，别担心——我会在必要时解释相关概念。为了快速入门，你可以查看*第12章*中的`01-PyTorch_Basics.ipynb`笔记本，在其中我们探索了张量的基本功能，并使用
    PyTorch 从头开始训练了一个非常小的神经网络。我还建议你访问本章末尾的*进一步阅读*部分，在那里你会找到一些学习 PyTorch 的资源。
- en: Now, let’s put on our practical hats and see some of these in action. PyTorch
    is an open source deep learning framework developed primarily by the **Facebook
    AI Research** (**FAIR**) **Lab**. Although it is a library that can manipulate
    **tensors** (which are *n*-dimensional matrices) and accelerate such manipulations
    with a GPU, a large part of the use case for such a library is in building and
    training deep learning systems. Because of that, PyTorch provides a lot of ready-to-use
    components that we can use to build a deep learning system. Let’s see how we can
    use PyTorch for an FFN.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们戴上实践的帽子，看看这些如何实际应用。PyTorch是一个开源的深度学习框架，主要由**Facebook AI Research**（**FAIR**）**实验室**开发。虽然它是一个可以操作**张量**（即*n*维矩阵）并通过GPU加速这些操作的库，但这个库的主要用途之一是构建和训练深度学习系统。因此，PyTorch提供了许多可以直接使用的组件，帮助我们构建深度学习系统。让我们看看如何使用PyTorch构建一个FFN。
- en: '**Notebook alert**:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒**：'
- en: To follow along with the complete code, use the `02-Building_Blocks.ipynb` notebook
    in the `Chapter12` folder and the code in the `src` folder.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随完整的代码，请使用`Chapter12`文件夹中的`02-Building_Blocks.ipynb`笔记本和`src`文件夹中的代码。
- en: As we learned earlier in the section, an FFN is a network of linear and non-linear
    units arranged in a network. A linear operation consists of multiplying the input
    vector, *X*, with a weight matrix, *W*, and adding a bias term, *b*. This operation,
    *WX* + *b*, is encapsulated in a `Linear` class in the `nn` module of the **PyTorch**
    library. We can import this from the library using `torch.nn import Linear`. But
    usually, we must import the whole **nn** module because we would be using a lot
    of components from that module. For non-linearity, let’s use **ReLU** (as introduced
    in *Chapter 11*, *Introduction to Deep Learning*), which is also a class in the
    **nn** module.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节中之前学到的，FFN是一个由线性和非线性单元组成的网络。线性操作包括将输入向量*X*与权重矩阵*W*相乘，并加上一个偏置项*b*。这个操作，即*WX*
    + *b*，被封装在**PyTorch**库的`nn`模块中的`Linear`类中。我们可以通过`torch.nn import Linear`从库中导入这个类。但通常，我们需要导入整个**nn**模块，因为我们会使用该模块中的许多组件。对于非线性部分，我们使用**ReLU**（如*第11章
    深度学习简介*中介绍），它也是**nn**模块中的一个类。
- en: 'Before moving on, let’s create a random walk time series whose length is **20**:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们创建一个随机游走时间序列，其长度为**20**：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can use this tensor directly in the FFN, but usually, we use a sliding window
    technique to split the tensor and train the networks. We do this for multiple
    reasons:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接在FFN中使用这个张量，但通常，我们会使用滑动窗口技术来拆分张量并训练网络。我们这样做有多个原因：
- en: We can see this as a data augmentation technique that creates a greater number
    of samples as opposed to using the entire sequence just once.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将其视为一种数据增强技术，与仅使用整个序列一次不同，它可以创建更多的样本。
- en: It helps us reduce and restrict computation by limiting the calculation to a
    fixed window.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过将计算限制在一个固定的窗口内，帮助我们减少和限制计算。
- en: 'Let’s do that now:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, we have a tensor, `ts_dataset`, whose size is *6x15* (this can create 6
    samples of 15 input features each when we move the sliding window across the length
    of the series). For a standard FFN, the input shape is specified as *batch size
    x input features*. So 6 becomes our batch size and 15 becomes the input feature
    size.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一个张量`ts_dataset`，其大小为*6x15*（当我们在序列的长度上滑动窗口时，可以创建6个样本，每个样本包含15个输入特征）。对于标准的FFN，输入形状指定为*批次大小
    x 输入特征*。所以6是我们的批次大小，15是输入特征的大小。
- en: 'Now, let’s define the layers in the FFN. For this exercise, let’s assume the
    network’s structure is as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义FFN中的各层。对于这个练习，我们假设网络结构如下：
- en: '![Figure 12.3 – FFNs – a matrix multiplication perspective ](img/B22389_12_03.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.3 – FFN – 矩阵乘法视角](img/B22389_12_03.png)'
- en: 'Figure 12.3: FFNs—a matrix multiplication perspective'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3：FFN——矩阵乘法视角
- en: The input data (6x15) will be passed through these layers one by one. Here,
    we can see how the tensor dimensions change as they flow through the network.
    Each of the linear layers is essentially a matrix multiplication that converts
    the input into the output of a specified dimension. After each linear transformation,
    we stack a non-linear activation function in there. These alternative linear and
    non-linear modules are what give the neural network the expressive power it has.
    The linear layers are an affine transformation of the vector space (rotation,
    translation, and so on), and the non-linearity *squashes* the vector space. Together,
    they can morph the input space so that it’s useful for the task at hand. Now,
    let’s see how we can code this in PyTorch.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据（6x15）将依次通过这些层。在这里，我们可以看到随着数据通过网络时张量维度的变化。每一层线性变换基本上是一个矩阵乘法，它将输入转换成指定维度的输出。在每次线性变换后，我们会堆叠一个非线性激活函数。这些交替的线性和非线性模块赋予神经网络表达能力。线性层是对向量空间的仿射变换（旋转、平移等），而非线性函数则将向量空间“压缩”。它们共同作用，可以将输入空间转化为适合当前任务的形式。现在，让我们看看如何用PyTorch编写代码实现这一过程。
- en: 'We are going to use a handy module from PyTorch called **Sequential**, which
    allows us to stack different sub-components together and use them with ease:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用PyTorch中一个非常方便的模块，叫做**Sequential**，它允许我们将不同的子组件堆叠在一起，并轻松使用它们：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we have defined the FFN, let’s see how we can use it:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了FFN，接下来我们来看一下如何使用它：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will return a tensor whose shape is based on *batch size x output units*.
    We can have any number of output units, not just one. Therefore, when using an
    encoder, we can have an arbitrary dimension for the latent vector. Then, when
    we use it as a decoder, we can have the output units equal the number of timesteps
    we forecast.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个张量，其形状基于*批处理大小 x 输出单元*。我们可以有任意数量的输出单元，而不仅仅是一个。因此，在使用编码器时，我们可以为潜在向量设置任意维度。然后，当我们将其用作解码器时，可以让输出单元等于我们预测的时间步数。
- en: '**Sneak peak**:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**预览**：'
- en: We have not seen multi-step forecasting until now because it will be covered
    in more detail in *Chapter 18*, *Multi-Step Forecasting*. But for now, just understand
    that there are cases where we will need to forecast multiple timesteps into the
    future. The classical statistical models do this out of the box. But for machine
    learning and deep learning, we need to design systems that can do that. Fortunately,
    there are a few different techniques to do so, which will be covered later in
    this chapter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直到现在还没有看到多步预测，因为它将在*第18章*，*多步预测*中更详细地讨论。但现在，只需要理解有些情况下我们需要预测未来多个时间步。经典的统计模型可以直接做到这一点。但对于机器学习和深度学习，我们需要设计能够做到这一点的系统。幸运的是，有几种不同的技术可以实现这一点，这将在本章后面进行介绍。
- en: FFNs are designed for non-temporal data. We can use FFNs by embedding our data
    temporally and then passing that to the network. Also, the computational cost
    in an FFN is directly proportional to the memory we use in the embedding (the
    number of previous timesteps we include as features). We will also not be able
    to handle variable-length sequences in this setting.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: FFN（前馈神经网络）是为非时间序列数据设计的。我们可以通过将数据嵌入时间序列中，再将其传递给网络来使用FFN。此外，FFN的计算成本与我们在嵌入中使用的内存（我们作为特征包含的前几个时间步数）直接成正比。在这种设置下，我们也无法处理变长序列。
- en: Now, let’s look at another common architecture that is specifically designed
    for temporal data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下另一种专门为时间序列数据设计的常见架构。
- en: Recurrent neural networks
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: '**Recurrent neural networks** (**RNNs**) are a family of neural networks specifically
    designed to handle sequential data. They were first proposed by *Rumelhart et
    al*. (1986) in their seminal work, *Learning Representations by Back-Propagating
    Errors*. The work borrows ideas such as parameter sharing and recurrence from
    previous work in statistics and machine learning, resulting in a neural network
    architecture that helps overcome many of the disadvantages that FFNs have when
    processing sequential data.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络**（**RNN**）是一类专门为处理序列数据而设计的神经网络。它们最早由*Rumelhart 等人*（1986年）在他们的开创性工作《通过反向传播误差学习表示》中提出。该工作借鉴了统计学和机器学习中以前工作的思想，如参数共享和递归，从而得到了一种神经网络架构，帮助克服了FFN在处理序列数据时的许多缺点。'
- en: RNN architecture
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN架构
- en: '**Parameter sharing** is when we use the same set of parameters for different
    parts of a model. Apart from a regularization effect (restricting the model to
    using the same set of weights for multiple tasks, which regularizes the model
    by constraining the search space while optimizing the model), parameter sharing
    enables us to extend and apply the model to examples of different forms. RNNs
    can scale to much longer sequences because of this. In an FFN, each timestep (each
    feature) has a fixed weight, and even if the motif we are looking for shifts by
    just one timestep, the network may not capture it correctly. In an RNN enabled
    by parameter sharing, they are captured in a much better way.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数共享**是指在模型的不同部分使用相同的一组参数。除了具有正则化效果（限制模型在多个任务中使用相同的权重，这通过在优化模型时约束搜索空间来正则化模型）外，参数共享使我们能够扩展并将模型应用于不同形式的示例。正因如此，RNN
    可以扩展到更长的序列。在 FFN 中，每个时间步（每个特征）都有固定的权重，即使我们要寻找的模式仅偏移一个时间步，网络也可能无法正确捕捉到它。而在启用了参数共享的
    RNN 中，模式能够以更好的方式被捕捉到。'
- en: In a sentence (which is also a sequence), we may want a model to recognize that
    “*Tomorrow I will go to the bank*” and “*I will go to the bank tomorrow*” are
    the same thing. An FFN can’t do this, but an RNN will be able to because it uses
    the same parameters at all positions and will be able to identify the motif “*I
    will go to the bank*” wherever it occurs. Intuitively, we can think of RNNs as
    applying the same FFN at each time window but enhanced with some kind of memory
    to store relevant information for the task at hand.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在一句话中（它也是一个序列），我们可能希望模型识别出“*明天我去银行*”和“*我明天去银行*”是相同的。一个 FFN 做不到这一点，但一个 RNN 可以做到，因为它在所有位置使用相同的参数，并能够识别出模式“*我去银行*”无论它出现在何处。直观地说，我们可以将
    RNN 看作是在每个时间窗口应用相同的 FFN，但通过某种记忆机制增强，以便存储与当前任务相关的信息。
- en: 'Let’s visualize how an RNN processes inputs:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来直观地理解一下 RNN 如何处理输入：
- en: '![Figure 12.4 – How an RNN processes input sequences ](img/B22389_12_04.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.4 – RNN 如何处理输入序列](img/B22389_12_04.png)'
- en: 'Figure 12.4: How an RNN processes input sequences'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4：RNN 如何处理输入序列
- en: Let’s assume we are talking about a sequence with four elements in it, *x*[1]
    to *x*[4]. Any RNN block (let’s consider it as a black box for now) consumes input
    and a hidden state (memory), producing an output. In the beginning, there is no
    memory, so we start with an initial memory (*H*[0]), which is typically an array
    filled with zeroes. Now, the RNN block takes in the first input (*x*[1]) along
    with the initial hidden state (*H*[0]), producing an output (*o*[1]) and a hidden
    state (*H*[1]).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们讨论的是一个包含四个元素的序列，*x*[1] 到 *x*[4]。任何 RNN 块（暂时把它当作一个黑盒）都会消耗输入和隐藏状态（记忆），并生成一个输出。一开始没有记忆，因此我们从初始记忆
    (*H*[0]) 开始，这通常是一个全为零的数组。现在，RNN 块接收第一个输入 (*x*[1]) 和初始隐藏状态 (*H*[0])，生成输出 (*o*[1])
    和新的隐藏状态 (*H*[1])。
- en: To process the second element in the sequence, *the same RNN* block takes in
    the hidden state from the previous timestep (*H*[1]) and the input at the current
    timestep (*x*[2]), producing the output at the second timestep (*o*[2]) and a
    new hidden state (*H*[2]). This process continues until we reach the end of the
    sequence. After processing the entire sequence, we will have all the outputs at
    each timestep (*o*[1] through *o*[4]) and the final hidden state (*H*[4]).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理序列中的第二个元素，*同一个 RNN* 块会接收来自上一个时间步的隐藏状态 (*H*[1]) 和当前时间步的输入 (*x*[2])，生成第二个时间步的输出
    (*o*[2]) 和新的隐藏状态 (*H*[2])。这个过程会持续直到我们处理完序列的所有元素。处理完整个序列后，我们将获得每个时间步的所有输出 (*o*[1]
    到 *o*[4]) 和最终的隐藏状态 (*H*[4])。
- en: 'These outputs and the hidden state will have encoded the information contained
    in the sequence and can be used for further processing, such as to predict the
    next step using a decoder. The RNN block can also be used as a decoder that takes
    in the encoded representation and produces the outputs. Because of this flexibility,
    the RNN blocks can be arranged to suit a wide variety of input and output combinations,
    such as the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些输出和隐藏状态将会编码序列中包含的信息，并可用于进一步处理，例如使用解码器预测下一步。RNN 块也可以作为解码器，接受编码后的表示并生成输出。由于这种灵活性，RNN
    块可以根据各种输入和输出组合进行排列，如下所示：
- en: Many-to-one, where we have many inputs and a single output—for instance, single-step
    forecasting or time series classification
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对一，其中有多个输入和一个输出——例如，单步预测或时间序列分类
- en: Many-to-many, where we have many inputs and many outputs—for instance, multi-step
    forecasting
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对多，其中我们有多个输入和多个输出——例如，多步预测
- en: Now, let’s look at what happens inside an RNN.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看RNN内部发生了什么。
- en: 'Let the input to the RNN at time *t* be *x*[t] and the hidden state from the
    previous timestep be *H*[t][-1]. The updated equations are as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 设RNN在时间*t*的输入为*x*[t]，上一时间步的隐状态为*H*[t][-1]。更新的方程如下：
- en: '![](img/B22389_12_002.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_002.png)'
- en: '![](img/B22389_12_003.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_003.png)'
- en: '![](img/B22389_12_004.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_004.png)'
- en: Here, *U*, *V*, and *W* are learnable weight matrices, and *b*[1] and *b*[2]
    are two learnable bias vectors. *U*, *V*, and *W* can be easily remembered as
    *input-to-hidden*, *hidden-to-output*, and *hidden-to-hidden* matrices based on
    the kind of transformation they perform, respectively. Intuitively, we can think
    of the operation that the RNN does as a kind of learning and forgetting information
    as it sees fit. The *tanh* activation, as we saw in *Chapter 11*, *Introduction
    to Deep Learning*, produces a value between -1 and 1, which acts analogous to
    forgetting and remembering. So the RNN transforms the input into a latent dimension,
    uses the *tanh* activation to decide what information from the current timestep
    and previous memory to keep and forget, and uses this new memory to generate an
    output.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*U*、*V*和*W*是可学习的权重矩阵，*b*[1]和*b*[2]是两个可学习的偏置向量。根据它们执行的变换类型，*U*、*V*和*W*可以很容易地记住为*输入到隐层*、*隐层到输出*和*隐层到隐层*矩阵。直观地，我们可以将RNN执行的操作理解为一种学习和遗忘信息的方式，它根据需要决定保留和遗忘什么信息。*tanh*激活函数，如我们在*第11章
    深度学习导论*中所看到的，生成一个介于-1和1之间的值，类似于遗忘和记忆。因此，RNN将输入转换为一个潜在的维度，使用*tanh*激活函数决定保留和遗忘当前时间步和之前记忆中的哪些信息，并使用这个新的记忆生成输出。
- en: In standard backpropagation, we backpropagate gradients from one unit to another.
    But in recurrent nets, we have a special situation where we have to backpropagate
    the gradients within a single unit, but through time or the different timesteps.
    A special case of backpropagation, called **Back Propagation Through Time** (**BPTT**),
    has been developed for RNNs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的反向传播中，我们将梯度从一个单元反向传播到另一个单元。但在递归神经网络（RNN）中，我们有一个特殊情况，必须在单个单元内部进行梯度反向传播，但通过时间或不同的时间步长。为RNN开发了一种特殊的反向传播方式，叫做**通过时间的反向传播**（**BPTT**）。
- en: Thankfully, all the major deep learning frameworks are capable of doing this
    without any problems. For a more detailed understanding and the mathematical foundations
    of BPTT, please refer to the *Further reading* section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，所有主要的深度学习框架都能够顺利地完成这项工作。有关BPTT的更详细理解及其数学基础，请参考*进一步阅读*部分。
- en: PyTorch has made RNNs available as ready-to-use modules—all you need to do is
    import one of the modules from the library and start using it. But before we do
    that, we need to understand a few more concepts.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch已经将RNN作为即用模块提供——你只需导入库中的一个模块并开始使用它。但在这之前，我们需要理解一些其他的概念。
- en: The first concept we will look at is the possibility of *stacking multiple layers*
    of RNNs on top of each other so that the outputs at each timestep become the input
    to the RNN in the next layer. Each layer will have a hidden state or memory. This
    enables hierarchical feature learning, which is one of the bedrocks of successful
    deep learning today.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要看的第一个概念是将*多个层*的RNN堆叠在一起的可能性，使得每个时间步的输出成为下一层RNN的输入。每一层将具有一个隐状态或记忆。这使得层次化特征学习成为可能，这是当今成功深度学习的基石之一。
- en: Another concept is *bidirectional* RNNs, introduced by Schuster and Paliwal
    in 1997\. Bidirectional RNNs are very similar to RNNs. In a vanilla RNN, we process
    the inputs sequentially from start to end (forward). However, a bidirectional
    RNN uses one set of input-to-hidden and hidden-to-hidden weights to process the
    inputs from start to end, and then it uses another set to process the inputs in
    reverse (end to start) and concatenate the hidden states from both directions.
    It is on this concatenated hidden state that we apply the output equation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个概念是*双向*RNN，由Schuster和Paliwal在1997年提出。双向RNN与RNN非常相似。在普通的RNN中，我们按顺序从开始到结束（前向）处理输入。然而，双向RNN使用一组输入到隐层和隐层到隐层的权重从开始到结束处理输入，然后使用另一组权重按反向（从结束到开始）处理输入，并将来自两个方向的隐层状态连接起来。我们在这个连接的隐层状态上应用输出方程。
- en: '**Reference check**:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research papers by Rumelhart. et al and Schuster and Paliwal are cited in
    the *References* section as *5* and *6*, respectively.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Rumelhart 等人的研究论文和 Schuster 和 Paliwal 的论文分别在 *参考文献* 部分被引用为 *5* 和 *6*。
- en: RNN in PyTorch
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch中的RNN
- en: 'Now, let’s understand the PyTorch implementation of RNN. As with the **Linear**
    module, the **RNN** module is also available from `torch.nn`. Let’s look at the
    different parameters that the implementation provides while initializing:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解一下 PyTorch 中 RNN 的实现。与 **线性** 模块一样，**RNN** 模块也可以通过 `torch.nn` 获得。让我们来看一下初始化时实现所提供的不同参数：
- en: '`input_size`: The number of expected features in the input. If we use just
    the history of the time series, this is 1\. However, when we use history along
    with some other features, this is >1.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_size`：输入中预期的特征数。如果我们只使用时间序列的历史数据，则为 1。但是，当我们同时使用历史数据和其他一些特征时，则为大于 1
    的值。'
- en: '`hidden_size`: The dimension of the hidden state. This defines the size of
    the input-to-hidden and hidden-to-hidden matrices.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`：隐藏状态的维度。这定义了输入到隐藏层和隐藏层到隐藏层的矩阵大小。'
- en: '`num_layers`: This is the number of RNNs that will be stacked on top of each
    other. The default is **1**.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers`：这是将堆叠在一起的 RNN 数量。默认值是 **1**。'
- en: '`nonlinearity`: The non-linearity to use. Although tanh is the originally proposed
    non-linearity, PyTorch also allows us to use ReLU (`relu`). The default is `tanh`.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nonlinearity`：使用的非线性函数。虽然 tanh 是最初提出的非线性函数，但 PyTorch 也允许我们使用 ReLU（`relu`）。默认值是
    `tanh`。'
- en: '`bias`: This parameter decides whether or not to add bias to the update equations
    we discussed earlier. If the parameter is **False**, there will be no bias. The
    default is **True**.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias`：该参数决定是否将偏置项添加到我们之前讨论的更新方程中。如果参数为 **False**，则没有偏置。默认值是 **True**。'
- en: '`batch_first`: There are two input data configurations that the RNN cell can
    use—we can have the input as (*batch size, sequence length, number of features*)
    or (*sequence length, batch size, number of features*). `batch_first = True` selects
    the former as the expected input dimensions. The default is **False**.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_first`：RNN 单元可以使用两种输入数据配置——我们可以将输入设置为（*batch size*，*sequence length*，*number
    of features*）或（*sequence length*，*batch size*，*number of features*）。`batch_first
    = True` 选择前者作为预期的输入维度。默认值是 **False**。'
- en: '`dropout`: This parameter, if non-zero, uses a dropout layer on the outputs
    of each RNN layer except the last. Dropout is a popular regularization technique
    where randomly selected neurons are ignored during training (the *Further reading*
    section contains a link to the paper that proposed this). The dropout probability
    will be equal to **dropout**. The default is **0**.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout`：该参数如果不为零，会在每个 RNN 层的输出上使用 dropout 层，除了最后一层。Dropout 是一种常用的正则化技术，在训练过程中随机忽略选中的神经元（*进一步阅读*部分包含了提出该技术的论文链接）。dropout
    的概率将等于 **dropout**。默认值是 **0**。'
- en: '`bidirectional`: This parameter enables a bidirectional RNN. If **True**, a
    bidirectional RNN is used. The default is **False**.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bidirectional`：该参数启用双向 RNN。如果 **True**，则使用双向 RNN。默认值是 **False**。'
- en: 'To continue applying the model to the same synthetic data we generated earlier
    in this chapter, let’s initialize the RNN model, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续在本章中使用我们之前生成的相同合成数据，我们将初始化 RNN 模型，如下所示：
- en: '[PRE4]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, let’s look at the inputs and outputs that are expected from an RNN cell.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 RNN 单元预期的输入和输出。
- en: As opposed to the **Linear** layer we saw earlier, the RNN cell takes in *two
    inputs*—the input sequence and the hidden state vector. The input sequence can
    be either (*batch size*, *sequence length*, *number of features*) or (*sequence
    length*, *batch size*, *number of features*), depending on whether we have set
    `batch_first=True`. The hidden state is a tensor whose size is (*D*number of layers*,
    *batch size, hidden size*), where *D* = 1 for `bidirectional=False` and *D* =
    2 for `bidirectional=True`. The hidden state is an optional input and will default
    to zero tensors if left blank.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前看到的**线性**层不同，RNN单元接收*两个输入*——输入序列和隐藏状态向量。输入序列可以是（*batch size*，*sequence
    length*，*number of features*）或（*sequence length*，*batch size*，*number of features*），具体取决于我们是否设置了
    `batch_first=True`。隐藏状态是一个张量，大小为（*D*层数，*batch size*，*hidden size*），其中 *D* = 1
    对于 `bidirectional=False`，*D* = 2 对于 `bidirectional=True`。隐藏状态是一个可选输入，如果未填写，则默认为零张量。
- en: 'There are two outputs of the RNN cell: an output and a hidden state. The output
    can be either (*batch size*, *sequence length*, *D*hidden size*) or (*sequence
    length*, *batch size*, *D*hidden size*), depending on `batch_first`. The hidden
    state has the dimension of (*D*number of layers*, *batch size*, *hidden size*).
    Here, *D* = 1 or 2 is based on the **bidirectional** parameter.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 单元有两个输出：一个输出和一个隐藏状态。输出可以是（*batch size*，*sequence length*，*D*隐藏层大小）或（*sequence
    length*，*batch size*，*D*隐藏层大小），具体取决于`batch_first`。隐藏状态的维度是（*D*层数，*batch size*，*hidden
    size*）。这里，*D* = 1 或 2 取决于**双向**参数。
- en: 'So let’s run our sequence through an RNN and look at the inputs and outputs
    (for more detailed steps, refer to the accompanying notebook):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们通过 RNN 运行我们的序列，并观察输入和输出（有关更详细的步骤，请参考附带的笔记本）：
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Although we saw that the RNN cell contains the output as well as the hidden
    state, we also know that the output is just an affine transformation of the hidden
    state. Therefore, to provide flexibility to the users, PyTorch only implements
    the update equations regarding the hidden states in the module. There are cases
    where we have no use for the outputs at each timestep (such as in a many-to-one
    scenario) and we can save computation if we do not do the output update at each
    step. Therefore, `output` from the PyTorch RNN is just the hidden states at each
    timestep, and `hidden_states` is the latest hidden state.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们看到 RNN 单元包含输出和隐藏状态，但我们也知道输出只是隐藏状态的仿射变换。因此，为了给用户提供灵活性，PyTorch 只在模块中实现了关于隐藏状态的更新方程。对于某些情况（如多对一场景），我们可能根本不需要每个时间步的输出，如果我们不在每个步骤中执行输出更新，就可以节省计算。因此，PyTorch
    RNN 的`output`只是每个时间步的隐藏状态，而`hidden_states`则是最新的隐藏状态。
- en: 'We can verify this by checking whether the hidden-state tensor is equal to
    the last-output tensor:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查隐藏状态张量是否等于最后的输出张量来验证这一点：
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To make this clearer, let’s look at it visually:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地说明这一点，让我们用视觉化的方式来看：
- en: '![Figure 12.5 – PyTorch implementation of stacked RNNs ](img/B22389_12_05.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.5 – PyTorch 实现的堆叠 RNN](img/B22389_12_05.png)'
- en: 'Figure 12.5: PyTorch implementation of stacked RNNs'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5：PyTorch 实现的堆叠 RNN
- en: The hidden states at each timestep are used as input for the subsequent layer
    of RNNs and the hidden states of the last layer of RNNs are collected as the output.
    But each layer has a hidden state (that’s not shared with the others), and the
    PyTorch RNN collects the last hidden state from each layer and gives us that as
    well.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时间步的隐藏状态作为输入传递给后续的 RNN 层，最后一层 RNN 的隐藏状态被收集作为输出。但每一层都有一个隐藏状态（它不会与其他层共享），PyTorch
    的 RNN 会收集每一层的最后一个隐藏状态，并将其作为输出返回。
- en: Now, it is up to us to decide how to use these outputs. For instance, in a one-step-ahead
    forecast, we can use the output hidden states and stack a few linear layers on
    top of it to get the next timestep prediction. Alternatively, we can use the hidden
    states to transfer memory into another RNN as a decoder and generate predictions
    for multiple timesteps. There are many more ways we can use this output and PyTorch
    gives us that flexibility.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由我们来决定如何使用这些输出。例如，在一步预测中，我们可以使用输出的隐藏状态并在其上堆叠几个线性层，以获取下一个时间步的预测。或者，我们可以使用隐藏状态将记忆传递给另一个
    RNN 作为解码器，并生成多个时间步的预测。我们可以使用输出的方式有很多，PyTorch 给了我们这种灵活性。
- en: RNNs, while very effective in modeling sequences, have one big flaw. Because
    of BPTT, the number of units through which you need to backpropagate increases
    drastically with the length of the sequence to be used for training. When we have
    to backpropagate through such a long computational graph, we will encounter **vanishing**
    or **exploding gradients**. This is when the gradient, as it is backpropagated
    through the network, either shrinks to zero or explodes to a very high number.
    The former makes the network stop learning, while the latter makes the learning
    unstable.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 在建模序列时虽然非常有效，但有一个大缺点。由于 BPTT，反向传播所需经过的单元数量随着训练使用的序列长度增加而急剧增加。当我们必须在这么长的计算图中进行反向传播时，我们会遇到**梯度消失**或**梯度爆炸**的问题。这时，梯度在网络中反向传播时，要么缩小为零，要么爆炸成一个非常大的数值。前者使得网络停止学习，而后者则使得学习变得不稳定。
- en: We can think of what’s happening as akin to what happens when we multiply a
    scalar number repeatedly by itself. If the number is less than one, with every
    subsequent multiplication, the number becomes smaller and smaller until it is
    practically zero. If the number is greater than one, then the number becomes larger
    and larger at an exponential scale. This was discovered, independently, by Hochreiter
    in his diploma thesis (1991) and Yoshua Bengio et al. in two papers published
    in 1993 and 1994, respectively. Over the years, many tweaks to the model and training
    process have been proposed to tackle this disadvantage. Nowadays, vanilla RNNs
    are hardly used in practice and have been replaced almost completely by their
    newer cousins.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将发生的事情类比于将一个标量数字反复与自身相乘的过程。如果这个数字小于一，那么每次相乘后，这个数字会变得越来越小，直到几乎为零。如果这个数字大于一，那么它会以指数级别越来越大。这一发现最早由Hochreiter在其1991年的学位论文中独立提出，之后Yoshua
    Bengio等人在1993年和1994年分别发表了两篇相关论文。多年来，许多关于该模型和训练过程的改进方案应运而生，以应对这一缺点。如今，传统的RNN几乎不再使用，几乎完全被其更新版本所取代。
- en: '**Reference check**:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The references for Hochreiter (1991) and Bengio et al. (1993, 1994) are cited
    in the *References* section as *7*, *8*, and *9*, respectively.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Hochreiter（1991）和Bengio等人（1993，1994）的相关文献在*参考文献*部分被列为*7*、*8*和*9*。
- en: Now, let’s look at two key improvements that have been made to the RNN architecture
    that have shown good performance, gaining popularity in the machine learning community.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下对RNN架构所做的两项关键改进，这些改进在机器学习社区中表现良好，获得了广泛的关注。
- en: Long short-term memory (LSTM) networks
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）网络
- en: Hochreiter and Schmidhuber proposed a modification of the classical RNNs in
    1997—LSTM networks. It aimed to resolve the vanishing and exploding gradients
    in vanilla RNNs. The design of the LSTM was inspired by the logic gates of a computer.
    It introduces a new component, called a **memory cell**, which serves as long-term
    memory and is used in addition to the hidden-state memory of classical RNNs. In
    an LSTM, multiple gates are tasked with reading, adding, and forgetting information
    from these memory cells. This memory cell acts as a *gradient highway*, allowing
    the gateways to pass relatively unhindered through a network. This is the key
    innovation that avoided vanishing gradients in RNNs.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Hochreiter和Schmidhuber在1997年提出了对经典RNN的修改——LSTM网络。它旨在解决传统RNN中的梯度消失和梯度爆炸问题。LSTM的设计灵感来自计算机中的逻辑门。它引入了一个新的组件——**记忆单元**，作为长期记忆，除了经典RNN中的隐藏状态记忆外，它还被用于存储信息。在LSTM中，多个门负责从这些记忆单元中读取、添加和遗忘信息。这个记忆单元作为一个*梯度高速公路*，使得信息可以相对不受阻碍地通过网络传递。这正是避免RNN中梯度消失的关键创新。
- en: LSTM architecture
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM架构
- en: Let’s imagine that the input to the LSTM at time *t* is *x*[t], and the hidden
    state from the previous timestep is *H*[t][-1]. Now, there are three gates that
    process information. Each gate is nothing but two learnable weight matrices (one
    for the input and one for the hidden state from the last step) and a bias term
    that is multiplied/added to the input and hidden state, and finally, it is passed
    through a sigmoid activation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 假设LSTM在时间*t*的输入是*x*[t]，上一时刻的隐藏状态是*H*[t][-1]。现在，有三个门处理信息。每个门实际上由两个可学习的权重矩阵组成（一个用于输入，一个用于上一时刻的隐藏状态），以及一个偏置项，它会与输入和隐藏状态相乘/相加，最后通过一个sigmoid激活函数。
- en: 'The output of these gates will be a real number between 0 and 1\. Let’s look
    at each of these gates in detail:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些门的输出将是一个介于0和1之间的实数。让我们详细了解每个门的作用：
- en: '**Input gate**: The function of this gate is to decide how much information
    to read from the current input and previous hidden state. The update equation
    for this is:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**：此门的功能是决定从当前输入和前一个隐藏状态中读取多少信息。其更新方程为：'
- en: '![](img/B22389_12_005.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_005.png)'
- en: '**Forget gate**: The forget gate decides how much information to forget from
    long-term memory. The updated equation for this is:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**：遗忘门决定了从长期记忆中应忘记多少信息。其更新方程为：'
- en: '![](img/B22389_12_006.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_006.png)'
- en: '**Output gate**: The output gate decides how much of the current cell state
    should be used to create the current hidden state, which is the output of the
    cell. The update equation for this is:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**：输出门决定了当前单元状态中有多少应当用于生成当前的隐藏状态，隐藏状态即为该单元的输出。其更新方程为：'
- en: '![](img/B22389_12_007.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_007.png)'
- en: Here, *W*[xi], *W*[xf], *W*[xo], *W*[hi], *W*[hf], and *W*[ho] are learnable
    weight parameters, and *b*[i], *b*[f], and *b*[o] are learnable bias parameters.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W*[xi]、*W*[xf]、*W*[xo]、*W*[hi]、*W*[hf] 和 *W*[ho] 是可学习的权重参数，*b*[i]、*b*[f]
    和 *b*[o] 是可学习的偏置参数。
- en: 'Now, we can introduce a new long-term memory (cell state), *C*[t]. The three
    gates mentioned previously serve to update and forget from this memory. If the
    cell state from the previous timestep is *C*[t-1], then the LSTM cell calculates
    a candidate cell state, ![](img/B22389_12_008.png), using another gate, but this
    time with `tanh` activation:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以引入一个新的长期记忆（单元状态），*C*[t]。之前提到的三个门控机制用于更新和遗忘该记忆。如果前一时刻的单元状态是 *C*[t-1]，那么
    LSTM 单元将使用另一个门计算候选单元状态，![](img/B22389_12_008.png)，这次使用 `tanh` 激活函数：
- en: '![](img/B22389_12_009.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_009.png)'
- en: Here, *W*[xc], and *W*[xh] are learnable weight parameters and *b*[c] is the
    learnable bias parameter.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W*[xc] 和 *W*[xh] 是可学习的权重参数，*b*[c] 是可学习的偏置参数。
- en: 'Now, let’s look at the key update equation, which updates the cell state or
    long-term memory of the cell:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下关键的更新方程式，它用于更新单元的状态或长期记忆：
- en: '![](img/B22389_12_010.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_010.png)'
- en: Here, ![](img/B22389_12_011.png) is elementwise multiplication. We use the forget
    gate to decide how much information from the previous timestep to carry forward,
    and we use the input gate to decide how much of the current candidate cell state
    will be written into long-term memory.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B22389_12_011.png) 是逐元素相乘。我们使用遗忘门来决定从前一时刻传递多少信息，并使用输入门来决定当前候选单元状态中多少将被写入长期记忆。
- en: 'Last but not least, we use the newly created current cell state and the output
    gate to decide how much information to pass on to the predictor through the current
    hidden state:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们使用新创建的当前单元状态和输出门来决定通过当前隐藏状态向预测器传递多少信息：
- en: '![](img/B22389_12_012.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_012.png)'
- en: A visual representation of this process can be seen in *Figure 12.6*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的可视化表示可以在 *图 12.6* 中看到。
- en: '![](img/B22389_12_06.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_06.png)'
- en: 'Figure 12.6: A gating diagram of LSTM'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6：LSTM 的门控示意图
- en: LSTM in PyTorch
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch 中的 LSTM
- en: 'Now, let’s understand the PyTorch implementation of LSTM. It is very similar
    to the RNN implementation we saw earlier, but it has one key difference: the parameters
    to initialize the class are pretty much the same. The API for this can be found
    at [https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM).
    The key difference here is how the hidden states are used. While the RNN has a
    single tensor as a hidden state, the LSTM expects a **tuple** of tensors of the
    same dimensions: (**hidden state**, **cell state**).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解一下 PyTorch 中 LSTM 的实现。它与我们之前看到的 RNN 实现非常相似，但有一个关键区别：初始化该类的参数几乎相同。该 API
    可以在 [https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)
    上找到。这里的关键区别在于隐藏状态的使用方式。虽然 RNN 有一个张量作为隐藏状态，但 LSTM 期望的是一个**元组**，包含两个相同维度的张量：（**隐藏状态**，**单元状态**）。
- en: LSTMs, just like RNNs, have stacked and bidirectional variants, and PyTorch
    handles them in the same way.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 和 RNN 一样，也有堆叠和双向变体，PyTorch 以相同的方式处理它们。
- en: 'Now, let’s initialize some LSTM modules and use the synthetic data we have
    been using to see them in action:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们初始化一些 LSTM 模块，并使用我们一直在使用的合成数据来查看它们的实际效果：
- en: '[PRE7]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, let’s look at another modification that’s been made to vanilla RNNs that
    has resolved the vanishing and exploding gradient problems.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看对普通 RNN 所做的另一个改进，它解决了梯度消失和梯度爆炸问题。
- en: Gated recurrent unit (GRU)
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 门控循环单元（GRU）
- en: In 2014, Cho et al. proposed another variant of the RNN that has a much simpler
    structure than an LSTM, called a GRU. The intuition behind this is similar to
    when we use a bunch of gates to regulate the information that flows through the
    cell, but a GRU eliminates the long-term memory component and uses just the hidden
    state to propagate information. So instead of the memory cell becoming the *gradient
    highway*, the hidden state itself becomes the “gradient highway.” In keeping with
    the same notation convention we used in the previous section, let’s look at the
    updated equations for a GRU.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 2014 年，Cho 等人提出了另一种 RNN 变体，它的结构比 LSTM 简单得多，叫做 GRU。其背后的直觉类似于我们使用多个门来调节信息流动，但
    GRU 消除了长期记忆部分，仅使用隐藏状态来传播信息。因此，记忆单元不再成为 *梯度高速公路*，而是隐藏状态本身成为“梯度高速公路”。遵循我们在上一节中使用的相同符号约定，让我们来看一下
    GRU 的更新方程。
- en: GRU architecture
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GRU 结构
- en: 'While we had three gates in an LSTM, we only have two in a GRU:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 LSTM 中有三个门，但 GRU 中只有两个门：
- en: '**Reset gate**: This gate decides how much of the previous hidden state will
    be considered as the candidate’s hidden state of the current timestep. The equation
    for this is:'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重置门**：该门决定了前一个隐藏状态的多少部分会被作为当前时间步的候选隐藏状态。其方程为：'
- en: '![](img/B22389_12_013.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_013.png)'
- en: '**Update gate**: The update gate decides how much of the previous hidden state
    should be carried forward and how much of the current candidate’s hidden state
    will be written into the hidden state. The equation for this is:'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新门**：更新门决定了前一个隐藏状态的多少部分应被传递下去，以及当前候选隐藏状态的多少部分会被写入隐藏状态。其方程为：'
- en: '![](img/B22389_12_014.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_014.png)'
- en: Here *W*[xr], *W*[xu], *W*[hr], and *W*[hu] are learnable weight parameters,
    and *b*[r] and \*b*[u] are learnable bias parameters.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 *W*[xr]、*W*[xu]、*W*[hr] 和 *W*[hu] 是可学习的权重参数，而 *b*[r] 和 *b*[u] 是可学习的偏置参数。
- en: 'Now, we can calculate the candidate’s hidden state (![](img/B22389_12_015.png)),
    as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算候选隐藏状态（![](img/B22389_12_015.png)），如下所示：
- en: '![](img/B22389_12_016.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_016.png)'
- en: Here, *W*[xh] and *W*[hh] are learnable weight parameters, and *b*[h] is the
    learnable bias parameter. Here, we use the reset gate to throttle the information
    flow from the previous hidden state to the current candidate’s hidden state.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W*[xh] 和 *W*[hh] 是可学习的权重参数，而 *b*[h] 是可学习的偏置参数。这里，我们使用重置门来限制从前一个隐藏状态到当前候选隐藏状态的信息流。
- en: 'Finally, the current hidden state (the output that goes to a predictor) is
    computed using the following equation:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当前隐藏状态（即传递给预测器的输出）通过以下方程计算：
- en: '![](img/B22389_12_017.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_017.png)'
- en: We use the update gate to decide how much from the previous hidden state and
    how much from the current candidate will be passed to the next timestep or predictor.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用更新门来决定从前一个隐藏状态和当前候选状态中传递给下一个时间步或预测器的比例。
- en: '**Reference check**:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research papers for LSTM and GRUs are cited in the *References* section
    as *10* and *11*, respectively.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 和 GRU 的研究论文分别在*参考文献*部分列为*10*和*11*。
- en: 'A visual representation of this process can be found in *Figure 12.7*:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的可视化表示可以在*图 12.7*中找到：
- en: '![Figure 12.6 – A gating diagram of LSTM versus GRU  ](img/B22389_12_07.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.6 – LSTM 与 GRU 的门控图](img/B22389_12_07.png)'
- en: 'Figure 12.7: A gating diagram of GRU'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7：GRU 的门控图
- en: GRU in PyTorch
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch 中的 GRU
- en: 'Now, let’s understand the PyTorch implementation of the GRU. The APIs, inputs,
    and outputs are the same as with an RNN. The API for this can be referenced here:
    [https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU).
    The key difference is the internal workings of the modules, where the GRU update
    equations are used instead of the standard RNN ones.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解 PyTorch 中 GRU 的实现。API、输入和输出与 RNN 相同。可以在这里参考该 API：[https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU)。关键的区别在于模块的内部工作原理，其中使用了
    GRU 更新方程，而不是标准的 RNN 方程。
- en: 'Now, let’s initialize a GRU module and use the synthetic data we have been
    using to see it in action:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们初始化一个 GRU 模块，并使用我们一直在使用的合成数据来看它的实际应用：
- en: '[PRE8]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, let’s look at another major component that can be used for sequential data.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看另一种可以用于序列数据的主要组件。
- en: Convolution networks
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积网络
- en: '**Convolution networks**, also called **convolutional neural networks** (**CNNs**),
    are like neural networks for processing data in the form of a grid. This grid
    can be 2D (such as an image), 1D (such as a time series), 3D (such as data from
    LIDAR sensors), and so on. Although this book is about time series and, typically,
    1D convolutions are used in time series forecasting, it’s easier to understand
    convolutions in the 2D context (an image and so on) and then move back to a single-dimensional
    grid for time series.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积网络**，也称为 **卷积神经网络** (**CNNs**)，类似于处理网格形式数据的神经网络。这个网格可以是二维（如图像）、一维（如时间序列）、三维（如来自激光雷达传感器的数据）等。尽管本书涉及的是时间序列，通常时间序列预测中使用的是一维卷积，但从二维（如图像）理解卷积会更容易，然后再回到一维网格处理时间序列。'
- en: The basic idea behind CNNs is inspired by how human vision works. In 1979, Fukushima
    proposed Neocognitron (Reference *12*). It was a one-of-a-kind architecture that
    was directly inspired by how human vision works. But CNNs came into existence
    as we know them today in 1989 when Yann Le Cun used backpropagation to learn such
    a network, proving it by getting state-of-the-art results in handwritten digit
    recognition (Reference *13*). In 2012, when AlexNet (a CNN architecture for image
    recognition) won the annual challenge of image recognition called ImageNet, that
    too by a large margin between it and competing non-deep learning approaches, the
    interest and research in CNNs peaked. People soon figured out that, apart from
    images, CNNs are effective with sequences, such as language and time series data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 的基本思想灵感来源于人类视觉的工作原理。1979 年，福岛提出了 Neocognitron（参考文献 *12*）。这是一种独特的架构，直接受到人类视觉工作原理的启发。但
    CNN 如我们今天所知，直到 1989 年才出现，当时 Yann Le Cun 使用反向传播算法学习了这种网络，并通过在手写数字识别中取得最先进的成果（参考文献
    *13*）来证明这一点。2012 年，当 AlexNet（用于图像识别的 CNN 架构）在年度图像识别挑战赛 ImageNet 中获胜时，且与竞争的非深度学习方法相比，差距巨大，CNN
    的兴趣和研究达到了巅峰。人们很快意识到，除了图像外，CNN 对于序列数据（如语言和时间序列数据）同样有效。
- en: Convolution
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积
- en: At the heart of CNNs is a mathematical operation called **convolution**. The
    mathematical interpretation of a convolution operation is beyond the scope of
    this book, but there are a couple of links in the *Further reading* section if
    you want to learn more. For our purposes, we’ll develop an intuitive understanding
    of the convolution operation.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 的核心是一个叫做 **卷积** 的数学运算。卷积操作的数学解释超出了本书的范围，但如果你想了解更多，可以在 *进一步阅读* 部分找到一些相关链接。为了我们的目的，我们将对卷积操作形成直观的理解。
- en: Since CNNs rose to popularity using image data, let’s start by discussing the
    image domain and then move on to the sequence domain.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CNN 在使用图像数据时获得了广泛的关注，我们先从图像领域开始讨论，然后再转向序列领域。
- en: 'Any image (for simplicity, let’s assume it’s grayscale) can be considered as
    a grid of pixel values, each value denoting how bright a point is, with 1 being
    pure white and 0 being pure black. Before we start discussing convolution, let’s
    understand what a **kernel** is. For now, let’s think of a kernel as a 2D matrix
    with some values in it. Typically, the kernel’s size is smaller than the size
    of the image we are using. Since the kernel is smaller than the image, we can
    “fit” the kernel inside the image. Let’s start with the kernel aligned with the
    top-left edge. With the kernel at the current position, there is a set of values
    in the image that this kernel is superpositioned over. We can perform elementwise
    multiplication between this subset of the image and the kernel, and then we add
    up all the elements into a single scalar. Now, we can repeat this process by “sliding”
    the kernel into all positions in the image. For instance, the following shows
    a sample image input whose size is 4x4 and how a convolution operation is carried
    out on that, using a kernel whose size is 2x2:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 任何图像（为了简化，假设它是灰度图像）可以看作是一个像素值的网格，每个值表示一个点的亮度，1 代表纯白色，0 代表纯黑色。在我们开始讨论卷积之前，先了解什么是
    **卷积核**。目前，我们可以将卷积核看作一个包含某些值的二维矩阵。通常，卷积核的大小小于我们使用的图像的大小。由于卷积核小于图像，因此我们可以将卷积核“放入”图像中。我们从将卷积核对齐到左上角边缘开始。卷积核在当前位置时，图像中有一组值被该卷积核覆盖。我们可以对图像的这一子集和卷积核进行逐元素相乘，然后将所有元素加起来得到一个标量。现在，我们可以通过将卷积核“滑动”到图像的所有位置，重复此过程。例如，下面展示了一个
    4x4 的示例输入图像，并演示了如何使用一个 2x2 的卷积核对其进行卷积操作：
- en: '![Figure 12.7 – A convolution operation on 2D and 1D inputs ](img/B22389_12_08.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.7 – 在 2D 和 1D 输入上进行的卷积操作](img/B22389_12_08.png)'
- en: 'Figure 12.8: A convolution operation on 2D and 1D inputs'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.8：在 2D 和 1D 输入上进行的卷积操作
- en: So if we place the 2x2 kernel at the top-left position and perform the element-wise
    multiplication and the summation, we get the top-left item in the 3x3 output.
    If we slide the kernel by one position to the right, we get the next element in
    the top row of the output, and so on. Similarly, if we slide the kernel by one
    position down, we get the second element in the first column in the output.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们将 2x2 的卷积核放置在左上角位置，并执行逐元素乘法和求和操作，我们将得到 3x3 输出中的左上角项。如果我们将卷积核向右滑动一个位置，我们将得到输出顶部行中的下一个元素，以此类推。同样，如果我们将卷积核向下滑动一个位置，我们将得到输出中第一列的第二个元素。
- en: While this is interesting, we want to understand convolutions from a time series
    perspective. To do so, let’s shift our paradigm to 1D convolutions—convolutions
    performed on 1D data such as a sequence. In the preceding diagram, we can also
    see an example of a 1D convolution where we take the 1D kernel and slide it across
    the sequence to get an output of 1x3.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这很有趣，但我们想从时间序列的角度理解卷积。为此，让我们将视角转向 1D 卷积——即在1D数据（如序列）上执行的卷积操作。在前面的图示中，我们也可以看到一个
    1D 卷积的例子，其中我们将 1D 核滑动到序列上，以得到一个 1x3 的输出。
- en: Although we have set the kernel weights so that they’re convenient to understand
    and compute, in practice, these weights are learned by the network from data.
    If we set the kernel size as *n* and all the kernel weights as 1/*n*, what would
    such a convolution give us? This is something we covered in *Chapter 6*, *Feature
    Engineering for Time Series Forecasting*. Yes, they result in the rolling means
    with a window of *n*. Remember, we learned this as a feature engineering technique
    for machine learning models. So 1D convolutions can be thought of as a more powerful
    feature generator, where the features are learned from data. With different weights
    on the kernels, we will extract different features. It is this knowledge that
    we should hold onto while learning about CNNs for time series data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经设置了方便理解和计算的核权重，但在实际应用中，这些权重是通过网络从数据中学习得到的。如果我们将核大小设置为*n*，并且所有的核权重都设置为1/*n*，那么这种卷积会给我们带来什么呢？这是我们在*第六章*，*时间序列预测的特征工程*中讨论过的内容。是的，它们的结果就是具有*n*窗口的滚动均值。记住，我们曾经将其作为机器学习模型的特征工程技巧来学习。因此，1D
    卷积可以被看作是一个更强大的特征生成器，其中特征是从数据中学习到的。通过在核上使用不同的权重，我们可以提取不同的特征。正是这一点知识，我们在学习时间序列数据的卷积神经网络时需要牢记。
- en: Padding, stride, and dilations
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充、步幅和扩张
- en: Now that we understand what a convolution operation is, we need to understand
    a few more terms, such as **padding**, **stride**, and **dilations**.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了卷积操作是什么，我们需要了解更多的术语，如**填充**、**步幅**和**扩张**。
- en: 'Before we start discussing these terms, let’s look at an equation that gives
    the output dimensions (*O*) of a convolutional layer, given the input dimensions
    (*L*), kernel size (*k*), padding size (*p*[l] for left padding and *p*[r] for
    right padding), stride (*s*), and dilation (*d*):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始讨论这些术语之前，先来看一个给定输入维度（*L*）、核大小（*k*）、填充大小（*p*[l]为左填充，*p*[r]为右填充）、步幅（*s*）和扩张（*d*）时，卷积层输出维度（*O*）的公式：
- en: '![](img/B22389_12_018.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_12_018.png)'
- en: The default values (padding, strides, and dilations are special cases of a convolution
    process) of these terms are *p*[r], *p*[l] = 0, *s* = 1, *d* = 1\. Don’t worry
    if you don’t understand the formula or the terms in it—just keep the default values
    in mind so that when we understand each term, we can negate the others.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这些术语的默认值（填充、步幅和扩张是卷积过程的特例）是 *p*[r]，*p*[l] = 0，*s* = 1，*d* = 1。即使你暂时不理解公式或其中的术语，也不要担心——只要记住这些默认值，当我们理解每个术语时，其他的可以忽略。
- en: In *Figure 12.8*, we saw that the convolution operation always reduces the size
    of the input. So in the default case, the formula becomes *O* = *L* – (*k* - 1).
    This is because the earliest position we can place the kernel in the sequence
    is from *t* = 0 to *t* = *k*. Then, by convolving through the sequence, we get
    *L* – (*k* - 1) terms in the output. Padding is when we add some values to the
    beginning or the end of the sequence. The value we use for padding is dependent
    on the problem. Typically, we choose zero as a padding value. So padding a sequence
    essentially increases the size of the input. Therefore, in the preceding formula,
    we can think of *L* + *p*[l] + *p*[r] as the effective length of the sequence
    after padding.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图12.8*中，我们看到卷积操作总是会减少输入的大小。因此，在默认情况下，公式变为 *O* = *L* – (*k* - 1)。这是因为我们可以将核放置在序列中的最早位置，即从*t*
    = 0 到*t* = *k*。然后，通过在序列上进行卷积，我们可以在输出中得到 *L* – (*k* - 1) 项。填充是指我们在序列的开始或结束处添加一些值。我们用于填充的值取决于问题。通常，我们选择零作为填充值。因此，填充序列本质上是增加了输入的大小。因此，在前面的公式中，我们可以将
    *L* + *p*[l] + *p*[r] 视为填充后序列的有效长度。
- en: 'The next two terms (stride and dilation) are closely related to the **receptive
    field** of the convolutional layer. The receptive field of a convolutional layer
    is the region in the input space that influences the feature that’s generated
    by the convolutional layer. In other words, it is the size of the window of input
    over which we have performed the convolution operation. For a single convolutional
    layer (with default settings), this is pretty much the kernel size. For multi-layered
    CNNs, this calculation becomes a bit more complicated because of the hierarchical
    structure (the *Further reading* section contains a link to a paper by Arujo et
    al. who derived a formula to calculate the receptive field of a CNN). But generally,
    increasing the receptive field of a CNN is associated with an increase in the
    accuracy of the CNN. For computer vision, Araujo et al. noted the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个术语（步幅和扩张）与卷积层的**感受野**密切相关。卷积层的感受野是输入空间中影响由卷积层生成的特征的区域。换句话说，它是我们在进行卷积操作时，所使用的输入窗口的大小。对于单个卷积层（使用默认设置），这几乎就是内核的大小。对于多层
    CNN，这个计算变得更加复杂，因为它具有层次结构（*进一步阅读*部分包含了Arujo等人提出的一个公式，用于计算CNN的感受野）。但通常来说，增加CNN的感受野与提高CNN的准确度相关。对于计算机视觉，Araujo等人指出：
- en: ”We observe a logarithmic relationship between classification accuracy and receptive
    field size, which suggests that large receptive fields are necessary for high-level
    recognition tasks, but with diminishing rewards.”
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们观察到分类准确度与感受野大小之间呈对数关系，这表明大感受野对于高层次的识别任务是必要的，但回报递减。”
- en: 'In time series, this is important because if the receptive field of a CNN is
    smaller than the long-term dependency, such as the seasonality, that we want to
    capture, then the network will fail to do so. Making the CNN deeper by stacking
    more convolutional layers on top of the others is one way to increase the receptive
    field of a network. However, there are a few ways to increase the receptive field
    of a single convolutional layer. Strides and dilations are two such ways:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列中，这一点很重要，因为如果 CNN 的感受野小于我们想要捕捉的长期依赖性（如季节性），那么网络就无法做到这一点。通过在卷积层上堆叠更多的卷积层来加深
    CNN 是增加网络感受野的一种方式。然而，也有几种方法可以增加单个卷积层的感受野。步幅和扩张就是其中的两种方法：
- en: '**Stride**: Earlier, when we talked about *sliding* the kernel over the sequence,
    we mentioned that we move the kernel by one position at a time. This is called
    the stride of the convolutional layer, and there is no necessity that the stride
    should be 1\. If we set the stride to 2, the convolution operation would be performed
    by skipping a position in between, as shown in *Figure 12.9*. This can make each
    layer in the convolutional network look at a larger slice of history, thereby
    increasing the receptive field.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步幅**：之前，当我们讨论将内核在序列上*滑动*时，我们提到我们每次移动内核一个位置。这被称为卷积层的步幅，并且步幅不一定非得是 1。如果我们将步幅设置为
    2，那么卷积操作将跳过一个位置，如*图 12.9*所示。这可以使卷积网络中的每一层查看更大范围的历史，从而增加感受野。'
- en: '**Dilation**: Another way we can tweak the basic convolutional layer is by
    dilating the input connections. In the standard convolutional layer with a kernel
    size of 3, we apply the kernel to three consecutive elements in the input with
    a dilation of 1\. If we increase the dilation to 2, then the kernel will be dilated
    spatially and will be applied. Instead of being applied to three consecutive elements,
    an element in between will be skipped. *Figure 12.8* shows how this works. As
    we can see, this can also increase the receptive field of the network.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩张**：我们可以通过扩张输入连接来调整基本的卷积层。在标准卷积层中，假设内核大小为 3，我们将内核应用于输入中的三个连续元素，扩张值为 1。如果我们将扩张设置为
    2，那么内核将被空间扩张，并且将被应用。它不再应用于三个连续的元素，而是跳过其中的一个元素。*图 12.8*展示了这个过程。正如我们所看到的，这也可以增加网络的感受野。'
- en: 'Both these techniques are similar but different and are compatible with each
    other. The following diagram shows what happens when we apply strides and dilations
    together (although this doesn’t happen frequently):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种技术虽然相似，但有所不同，并且可以互相兼容。下图展示了当我们同时应用步幅和扩张时会发生什么（尽管这种情况不常见）：
- en: '![Figure 12.8 – Strides and dilations in convolutions ](img/B22389_12_09.png)Figure
    12.9: Strides and dilations in convolutions'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.8 – 卷积中的步幅和扩张](img/B22389_12_09.png)图12.9: 卷积中的步幅和扩张'
- en: 'Now, what if we want to make the output dimensions the same as the input dimensions?
    By using some basic algebra and rearranging the previous formula, we get the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想要使输出维度与输入维度相同怎么办？通过使用一些基本的代数和重新排列前面的公式，我们得到了以下结果：
- en: '*P*[l] + *p*[r] = *d*(*k*-1) + *L*(*s*-1) – (*s*-1)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*[l] + *p*[r] = *d*(*k*-1) + *L*(*s*-1) – (*s*-1)'
- en: And in time series, we typically pad on the left rather than the right because
    of the strong autocorrelation that is typically present. Padding the latest few
    entries with zeros or some other values will make the learning of the prediction
    function very hard, as the latest hidden states are directly influenced by the
    padded values. The *Further reading* section contains a link to an article by
    Kilian Batzner about autoregressive convolutions. It is a must-read if you wish
    to really understand the concepts we have discussed here and also understand a
    few limitations. The *Further reading* section also contains a link to a GitHub
    repository that contains animations of convolutions for 2D inputs, which will
    give you a good intuition of what is happening.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列中，我们通常在左侧进行填充，而不是右侧，因为通常存在强自相关性。用零或其他值填充最近的几个条目会使预测函数的学习非常困难，因为最新的隐藏状态直接受到填充值的影响。*进一步阅读*部分包含了Kilian
    Batzner关于自回归卷积的文章链接。如果你希望真正理解我们在这里讨论的概念，并了解其中的一些限制，这是必读的。*进一步阅读*部分还包含了一个GitHub存储库的链接，其中包含了二维输入卷积动画，这将帮助你直观地理解发生了什么。
- en: There is just one more term that you may hear often about convolutions, especially
    in time series—**causal convolutions**. All you have to keep in mind is that causal
    convolutions are not special types of convolutions. So long as we ensure that
    we won’t use future timesteps to predict the current timestep while training,
    we are performing causal operations. This is typically done by offsetting the
    target and padding the inputs.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积中，有一个常见的术语，尤其是在时间序列中经常听到的——**因果卷积**。你只需记住因果卷积并不是特殊类型的卷积。只要我们确保在训练时不使用未来的时间步来预测当前的时间步，我们就在执行因果操作。通常通过偏移目标和填充输入来实现这一点。
- en: Convolution in PyTorch
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch中的卷积
- en: 'Now, let’s understand the PyTorch implementation of the CNN (a one-dimensional
    CNN, which is typically used for sequences such as time series). Let’s look at
    the different parameters the implementation provides while initializing. We have
    just discussed the following terms, so they should be familiar to you by now:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来了解CNN在PyTorch中的实现（一维CNN，通常用于时间序列等序列）。让我们看看在初始化时实现提供的不同参数。我们刚刚讨论了以下术语，所以现在它们应该对你来说很熟悉了：
- en: '`in_channels`: The number of expected features in the input. If we are using
    just the history of the time series, then this would be 1\. But when we use history
    along with some other features, then this will be >1\. For subsequent layers,
    the `out_channels` you used in the previous layer become your `in_channels` in
    the current layer.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in_channels`: 输入中预期的特征数。如果我们仅使用时间序列的历史记录，那么这个值将为1。但是，当我们同时使用历史记录和其他特征时，这个值将大于1。对于后续的层，你在前一层中使用的`out_channels`将成为当前层的`in_channels`。'
- en: '`out_channels`: The number of kernels or filters applied to the input. Each
    kernel/filter produces a convolution operation with its own weights.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_channels`: 应用于输入的核或过滤器的数量。每个核/过滤器都会产生一个具有自己权重的卷积操作。'
- en: '`kernel_size`: This is the size of the kernel we use for convolving.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel_size`: 这是我们用于卷积的核的大小。'
- en: '`stride`: The stride of the convolution. The default is `1`.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`: 卷积的步幅。默认值为`1`。'
- en: '`padding`: This is the padding that is added to *both* sides. If we set the
    value as `2`, the sequence that we pass to the layer will have a padded position
    on both the left and right. We can also give `valid` or `same` as input. These
    are easy ways of mentioning the kind of padding we need to add. `padding=''valid''`
    is the same as no padding. `padding=''same''` pads the input so that the output
    has the shape as the input. However, this mode doesn’t support any stride values
    other than `1`. The default is `0`.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`：这是添加到*两边*的填充。如果我们将值设置为`2`，那么传递给该层的序列将在左右两边都有填充位置。我们还可以输入`valid`或`same`。这两者是表示所需填充类型的简便方法。`padding=''valid''`相当于没有填充。`padding=''same''`会对输入进行填充，使得输出形状与输入相同。然而，这种模式不支持除`1`以外的任何步幅值。默认值为`0`。'
- en: '`padding_mode`: This defines how the padded positions should be filled with
    values. The most common and default option is *zeros*, where all the padded tokens
    are filled with zeros. Another useful mode that is relevant for time series is
    **replicate**, which behaves like forward and backward fill in pandas. The other
    two options—**reflect** and **circular**—are more esoteric and are only used for
    specific use cases. The default is **zeros**.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_mode`：定义如何用值填充填充位置。最常见和默认的选项是*零*，即所有填充的标记都填充为零。另一个对时间序列相关的有用模式是**复制**，其行为类似于pandas中的前向和后向填充。另两个选项——**反射**和**循环**——则更为特殊，仅用于特定的用例。默认值为**零**。'
- en: '`dilation`: The dilation of the convolution. The default is `1`.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dilation`：卷积的膨胀。默认值为`1`。'
- en: '`groups`: This parameter lets you control the way input channels are connected
    to output channels. The number specified in `groups` specifies how many groups
    will be formed so that the convolutions happen within a group and not across.
    For instance, `group=2` means that half the input channels will be convolved by
    one set of kernels and that the other half will be convolved by a separate set
    of kernels. This is equivalent to running two convolution layers side by side.
    Check the documentation for more information on this parameter. Again, this is
    for an esoteric use case. The default is `1`.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groups`：此参数允许你控制输入通道与输出通道的连接方式。`groups`中指定的数字决定了会形成多少组，以便卷积仅在组内进行，而不会跨组进行。例如，`group=2`表示一半的输入通道将由一组核进行卷积，而另一半将由另一组核进行卷积。这相当于并行运行两个卷积层。有关此参数的更多信息，请查看文档。再次强调，这适用于一些特殊的用例。默认值为`1`。'
- en: '`bias`: This parameter adds a learnable bias to the convolutions. The default
    is `True`.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias`：此参数为卷积添加一个可学习的偏置。默认值为`True`。'
- en: 'Let’s apply a CNN model to the same synthetic data we generated earlier in
    this chapter with a kernel size of 3:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对本章早些时候生成的相同合成数据应用CNN模型，卷积核大小为3：
- en: '[PRE9]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, let’s look at the inputs and outputs that are expected from a CNN.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下CNN预期的输入和输出。
- en: '`Conv1d` expects the inputs to have three dimensions`—(batch size, number of
    channels, sequence length)`. For the initial input layer, the number of channels
    is the number of features you feed into the network; for intermediate layers,
    it is the number of kernels we used in the previous layer. The output from `Conv1d`
    is in the form of `(batch size, number of channels (output), sequence length (output))`.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`Conv1d`要求输入具有三维`—(批次大小，通道数，序列长度)`。对于初始输入层，通道数是输入网络的特征数；对于中间层，它是上一层使用的核的数量。`Conv1d`的输出形式为`(批次大小，通道数（输出），序列长度（输出）)`。'
- en: 'So let’s run our sequence through `Conv1d` and look at the inputs and outputs
    (for more detailed steps, refer to the `02-Building_Blocks.ipynb` notebook):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们通过`Conv1d`运行我们的序列，并查看输入和输出（有关更详细的步骤，请参阅`02-Building_Blocks.ipynb`笔记本）：
- en: '[PRE10]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The notebook provides a slightly more detailed analysis of `Conv1d`, with tables
    showing the impact that the hyperparameters have on the shape of the output, what
    kind of padding is used to make the input and output dimensions the same, and
    how a convolution with equal weights is just like a rolling mean. I highly suggest
    that you check it out and play around with the different options to get a feel
    of what the layer does for you.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本提供了对`Conv1d`的稍微详细的分析，表格展示了超参数对输出形状的影响，填充方式如何使输入和输出维度相同，以及如何用相等权重的卷积就像一个滚动平均。我强烈建议你查看并尝试不同的选项，以更好地理解该层为你做了什么。
- en: The inbuilt padding in `Conv1d` has its roots in image processing, so the padding
    technique defaults to adding to both sides. However, for sequences, it is preferable
    to use padding on the left, and because of that, it is also preferable to handle
    how the input sequences are padded separately and not use the inbuilt mechanism.
    `torch.nn.functional` has a handy method called `pad` that can be used to this
    effect.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`Conv1d`中的内置填充源自图像处理，因此填充技术默认是在两侧都添加填充。然而，对于序列数据，最好在左侧使用填充，因此，最好单独处理输入序列的填充方式，而不是使用内置机制。`torch.nn.functional`提供了一个方便的`pad`方法，可以用来实现这一点。'
- en: 'Other building blocks are used in time series forecasting because the architecture
    of a deep neural network is only limited by creativity. But the point of this
    chapter was to introduce you to the common ones that appear in many different
    architectures. We also intentionally left out one of the most popular architectures
    used nowadays: the transformer. This is because we have devoted another chapter
    (*Chapter 14*, *Attention and Transformers for Time Series*) to understanding
    attention before we look at transformers. Another major block that is slowly gaining
    popularity is graph neural networks, which can be thought of as specialized CNNs
    that operate on graph-based data rather than grids. However, these are outside
    the scope of this book, since they are an area of active research.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 其他构建模块也被用于时间序列预测，因为深度神经网络的架构只受创意的限制。但本章的重点是介绍一些在许多不同架构中出现的常见模块。我们故意没有介绍目前最流行的架构之一：变换器（Transformer）。这是因为我们已将另一章（*第14章*，*时间序列中的注意力与变换器*）专门用于理解注意力机制，然后再研究变换器。另一个逐渐受到关注的重要模块是图神经网络（GNN），它可以被看作是专门处理基于图形数据的卷积神经网络，而不是网格数据。然而，这些超出了本书的范围，因为它们仍是一个活跃的研究领域。
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: After introducing deep learning in the previous chapter, in this chapter, we
    gained a deeper understanding of the common architectural blocks that are used
    for time series forecasting. The encoder-decoder paradigm was explained as a fundamental
    way to structure a deep neural network for forecasting. Then, we learned about
    FFNs, RNNs (LSTMs and GRUs), and CNNs, exploring how they are used to process
    time series. We also saw how we could use all these major blocks in PyTorch by
    using the associated notebook, and then we got our hands dirty with some PyTorch
    code.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章介绍了深度学习之后，本章我们更深入地了解了用于时间序列预测的常见架构模块。我们解释了编码器-解码器范式，它是构建深度神经网络用于预测的基本方式。然后，我们学习了前馈神经网络（FFN）、循环神经网络（RNN，包括LSTM和GRU）和卷积神经网络（CNN），并探讨了它们是如何用于处理时间序列的。我们还看到如何通过使用相关的笔记本，在PyTorch中使用这些主要模块，并且动手写了一些PyTorch代码。
- en: In the next chapter, we’ll learn about a few major patterns we can use to arrange
    these blocks to perform time series forecasting.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习一些主要的模式，这些模式可以用来安排这些模块以进行时间序列预测。
- en: References
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The following references were used in this chapter:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用了以下参考文献：
- en: 'Neco, R. P. and Forcada, M. L. (1997), *Asynchronous translations with recurrent
    neural nets*. Neural Networks, 1997., International Conference on (Vol. 4, pp.
    2535–2540). IEEE: [https://ieeexplore.ieee.org/document/614693](https://ieeexplore.ieee.org/document/614693).'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Neco, R. P. 和 Forcada, M. L. (1997)，*使用递归神经网络的异步翻译*。神经网络，1997年，国际会议（第4卷，第2535–2540页）。IEEE：[https://ieeexplore.ieee.org/document/614693](https://ieeexplore.ieee.org/document/614693).
- en: 'Kalchbrenner, N. and Blunsom, P. (2013), *Recurrent Continuous Translation
    Models*. EMNLP (Vol. 3, No. 39, p. 413): [https://aclanthology.org/D13-1176/](https://aclanthology.org/D13-1176/).'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kalchbrenner, N. 和 Blunsom, P. (2013)，*循环连续翻译模型*。EMNLP（第3卷，第39期，第413页）：[https://aclanthology.org/D13-1176/](https://aclanthology.org/D13-1176/).
- en: 'Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
    Bougares, Holger Schwenk, and Yoshua Bengio. (2014), *Learning Phrase Representations
    using RNN Encoder-Decoder for Statistical Machine Translation*. Proceedings of
    the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    pages 1724–1734, Doha, Qatar. Association for Computational Linguistics: [https://aclanthology.org/D14-1179/](https://aclanthology.org/D14-1179/).'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
    Bougares, Holger Schwenk 和 Yoshua Bengio. (2014)，*使用RNN编码器-解码器进行短语表示学习，用于统计机器翻译*。2014年自然语言处理实证方法会议（EMNLP）论文集，第1724–1734页，卡塔尔多哈。计算语言学协会：[https://aclanthology.org/D14-1179/](https://aclanthology.org/D14-1179/).
- en: 'Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. (2014), *Sequence to sequence
    learning with neural networks.* Proceedings of the 27th International Conference
    on Neural Information Processing Systems – Volume 2: [https://dl.acm.org/doi/10.5555/2969033.2969173](https://dl.acm.org/doi/10.5555/2969033.2969173).'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Ilya Sutskever, Oriol Vinyals 和 Quoc V. Le. (2014), *基于神经网络的序列到序列学习.* 第27届国际神经信息处理系统会议论文集
    – 第2卷: [https://dl.acm.org/doi/10.5555/2969033.2969173](https://dl.acm.org/doi/10.5555/2969033.2969173).'
- en: 'Rumelhart, D., Hinton, G., and Williams, R (1986). *Learning representations
    by back-propagating errors*. Nature 323, 533–536: [https://doi.org/10.1038/323533a0](https://doi.org/10.1038/323533a0).'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Rumelhart, D., Hinton, G., 和 Williams, R (1986). *通过反向传播误差学习表示*. 自然 323, 533–536:
    [https://doi.org/10.1038/323533a0](https://doi.org/10.1038/323533a0).'
- en: 'Schuster, M. and Paliwal, K. K. (1997). *Bidirectional recurrent neural networks*.
    IEEE Transactions on Signal Processing, 45(11), 2673–2681: [https://doi.org/10.1109/78.650093](https://doi.org/10.1109/78.650093).'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Schuster, M. 和 Paliwal, K. K. (1997). *双向递归神经网络*. IEEE信号处理学报, 45(11), 2673–2681:
    [https://doi.org/10.1109/78.650093](https://doi.org/10.1109/78.650093).'
- en: 'Sepp Hochreiter (1991) *Untersuchungen zu dynamischen neuronalen Netzen*. Diploma
    thesis, TU Munich: [https://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf](https://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf).'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Sepp Hochreiter (1991) *关于动态神经网络的研究*. 硕士论文, 慕尼黑工业大学: [https://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf](https://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf).'
- en: 'Y. Bengio, P. Frasconi, and P. Simard (1993), *The problem of learning long-term
    dependencies in recurrent networks*. IEEE International Conference on Neural Networks,
    pp. 1183-1188 vol.3: 10.1109/ICNN.1993.298725.'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Y. Bengio, P. Frasconi 和 P. Simard (1993), *递归网络中学习长期依赖关系的问题*. IEEE国际神经网络会议,
    第3卷，第1183-1188页: 10.1109/ICNN.1993.298725.'
- en: 'Y. Bengio, P. Simard, and P. Frasconi (1994) *Learning long-term dependencies
    with gradient descent is difficult* in IEEE Transactions on Neural Networks, vol.
    5, no. 2, pp. 157–166, March 1994: 10.1109/72.279181.'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Y. Bengio, P. Simard 和 P. Frasconi (1994) *使用梯度下降学习长期依赖关系的困难*，发表于IEEE神经网络事务，第5卷，第2期，第157–166页，1994年3月:
    10.1109/72.279181.'
- en: 'Hochreiter, S. and Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural
    Computation, 9(8), 1735–1780: [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735).'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hochreiter, S. 和 Schmidhuber, J. (1997). *长短期记忆*. 神经计算, 9(8), 1735–1780: [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735).'
- en: 'Cho, K., Merrienboer, B.V., Gülçehre, Ç., Bahdanau, D., Bougares, F., Schwenk,
    H., and Bengio, Y. (2014). *Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation.* EMNLP: [https://www.aclweb.org/anthology/D14-1179.pdf](https://www.aclweb.org/anthology/D14-1179.pdf).'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Cho, K., Merrienboer, B.V., Gülçehre, Ç., Bahdanau, D., Bougares, F., Schwenk,
    H., 和 Bengio, Y. (2014). *使用RNN编码器-解码器学习短语表示用于统计机器翻译.* EMNLP: [https://www.aclweb.org/anthology/D14-1179.pdf](https://www.aclweb.org/anthology/D14-1179.pdf).'
- en: 'Fukushima, K. *Neocognitron: A self-organizing neural network model for a mechanism
    of pattern recognition unaffected by shift in position*. Biol. Cybernetics 36,
    193–202 (1980): [https://doi.org/10.1007/BF00344251](https://doi.org/10.1007/BF00344251).'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Fukushima, K. *Neocognitron：一种自组织神经网络模型，用于位置偏移不影响的模式识别机制*. 生物控制论 36, 193–202
    (1980): [https://doi.org/10.1007/BF00344251](https://doi.org/10.1007/BF00344251).'
- en: 'Y. Le Cun, B. Boser, J. S. Denker, R. E. Howard, W. Habbard, L. D. Jackel,
    and D. Henderson. 1990\. *Handwritten Digit Recognition with a Back-Propagation
    Network*. Advances in neural information processing systems 2\. Morgan Kaufmann
    Publishers Inc., San Francisco, CA, USA, 396–404: [https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf](https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf).'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Y. Le Cun, B. Boser, J. S. Denker, R. E. Howard, W. Habbard, L. D. Jackel 和
    D. Henderson. 1990年. *使用反向传播网络进行手写数字识别*. 神经信息处理系统进展 2\. Morgan Kaufmann出版社，美国旧金山,
    396–404: [https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf](https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf).'
- en: Further reading
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Take a look at the following resources to learn more about the topics that
    were covered in this chapter:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下资源，以进一步了解本章所涉及的主题：
- en: 'Official PyTorch tutorials: [https://pytorch.org/tutorials/beginner/basics/intro.html](https://pytorch.org/tutorials/beginner/basics/intro.html)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '官方PyTorch教程: [https://pytorch.org/tutorials/beginner/basics/intro.html](https://pytorch.org/tutorials/beginner/basics/intro.html)'
- en: '*Essence of linear algebra*, by 3Blue1Brown: [https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*线性代数的本质*，作者：3Blue1Brown: [https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)'
- en: '*Neural Networks – A Linear Algebra Perspective*, by Manu Joseph: [https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/](https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经网络 – 线性代数视角*，作者：Manu Joseph: [https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/](https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/)'
- en: '*Deep Learning*, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville: [https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/](https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度学习*，作者：Ian Goodfellow, Yoshua Bengio, 和 Aaron Courville: [https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/](https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/)'
- en: '*Understanding LSTMs*, by Christopher Olah: [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解LSTM*，作者：Christopher Olah: [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
- en: '*Intuitive Guide to Convolution*: [https://betterexplained.com/articles/intuitive-convolution/](https://betterexplained.com/articles/intuitive-convolution/)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卷积直观指南*: [https://betterexplained.com/articles/intuitive-convolution/](https://betterexplained.com/articles/intuitive-convolution/)'
- en: '*Computing Receptive Fields of Convolutional Neural Networks*, by Andre Araujo,
    Wade Norris, and Jack Sim: [https://distill.pub/2019/computing-receptive-fields/](https://distill.pub/2019/computing-receptive-fields/)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算卷积神经网络的感受野*，作者：Andre Araujo, Wade Norris, 和 Jack Sim: [https://distill.pub/2019/computing-receptive-fields/](https://distill.pub/2019/computing-receptive-fields/)'
- en: '*Convolutions in Autoregressive Neural Networks*, by Kilian Batzner: [https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/](https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自回归神经网络中的卷积*，作者：Kilian Batzner: [https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/](https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/)'
- en: '*Convolution Arithmetic*, by Vincent Dumoulin and Francesco Visin: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卷积算术*，作者：Vincent Dumoulin 和 Francesco Visin: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)'
- en: '*Dropout: A Simple Way to Prevent Neural Networks from Overfitting*, by Nitish
    Srivastava et al.: [https://jmlr.org/papers/v15/srivastava14a.html](https://jmlr.org/papers/v15/srivastava14a.html)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dropout：一种简单的防止神经网络过拟合的方法*，作者：Nitish Srivastava 等人: [https://jmlr.org/papers/v15/srivastava14a.html](https://jmlr.org/papers/v15/srivastava14a.html)'
- en: Join our community on Discord
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with authors and other readers:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者及其他读者讨论：
- en: '[https://packt.link/mts](https://packt.link/mts)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mts](https://packt.link/mts)'
- en: '![](img/QR_Code15080603222089750.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code15080603222089750.png)'
