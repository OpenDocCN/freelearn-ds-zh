- en: '*Chapter 3*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第三章*'
- en: Introduction to Machine Learning via Scikit-Learn
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 Scikit-Learn 进行机器学习简介
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将能够：
- en: Prepare data for different types of supervised learning models.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为不同类型的有监督学习模型准备数据。
- en: Tune model hyperparameters using a grid search.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网格搜索调整模型的超参数。
- en: Extract feature importance from a tuned model.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从调整过的模型中提取特征重要性。
- en: Evaluate performance of classification and regression models.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估分类和回归模型的表现。
- en: In this chapter, we will be covering the important concepts of handling data
    and making the data ready for analysis.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讲解处理数据并使数据准备好进行分析的关键概念。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '**scikit-learn** is a free, open source library built for Python that contains
    an assortment of supervised and unsupervised machine learning algorithms. Additionally,
    scikit-learn provides functions for data preprocessing, hyperparameter tuning,
    and model evaluation, which we will be covering in the upcoming chapters. It streamlines
    the model-building process and is easy to install on a wide variety of platforms.
    scikit-learn started in 2007 as a Google Summer of Code project by David Corneapeau,
    and after a series of developments and releases, scikit-learn has evolved into
    one of the premier tools used by academics and professionals for machine learning.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**scikit-learn** 是一个免费的开源库，专为 Python 构建，包含一系列有监督和无监督的机器学习算法。此外，scikit-learn
    提供数据预处理、超参数调优和模型评估的功能，我们将在接下来的章节中涉及到这些内容。它简化了模型构建过程，并且易于在各种平台上安装。scikit-learn
    起步于 2007 年，由 David Corneapeau 在 Google Summer of Code 项目中创建，经过一系列的开发和发布，scikit-learn
    已经发展成学术界和专业人士广泛使用的机器学习工具之一。'
- en: In this chapter, we will learn to build a variety of widely used modeling algorithms,
    namely, linear and logistic regression, support vector machines (SVMs), decision
    trees, and random forests. First, we will cover linear and logistic regression.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将学习构建一系列广泛使用的建模算法，即线性回归和逻辑回归、支持向量机（SVM）、决策树和随机森林。首先，我们将介绍线性回归和逻辑回归。
- en: Introduction to Linear and Logistic Regression
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归与逻辑回归简介
- en: 'In regression, a single dependent, or outcome variable is predicted using one
    or more independent variables. Use cases for regression are included, but are
    not limited to predicting:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归分析中，使用一个或多个自变量来预测单一的因变量或结果变量。回归的应用场景包括但不限于以下预测：
- en: The win percentage of a team, given a variety of team statistics
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据各种球队统计数据预测球队的胜率
- en: The risk of heart disease, given family history and a number of physical and
    psychological characteristics
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据家族病史及一系列身体和心理特征预测患心脏病的风险
- en: The likelihood of snowfall, given several climate measurements
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据多个气候指标预测降雪的可能性
- en: 'Linear and logistic regression are popular choices for predicting such outcomes
    due to the ease and transparency of interpretability, as well as the ability to
    extrapolate to values not seen in the training data. The end goal of linear regression
    is to draw a straight line through the observations that minimizes the absolute
    distance between the line and observations (that is, the line of best fit). Therefore,
    in linear regression, it is assumed that the relationship between the feature(s)
    and the continuous dependent variable follows a straight line. Lines are defined
    in slope-intercept form (that is, *y = a + bx*), where *a* is the intercept (that
    is, the value of *y* when *x* is 0), *b* is the slope, and *x* is the independent
    variable. There are two types of linear regression that will be covered in this
    chapter: simple linear regression and multiple linear regression.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于线性回归和逻辑回归具有易于解释和透明的特点，并且能够对未在训练数据中出现的值进行外推，因此它们是预测此类结果的流行选择。线性回归的最终目标是绘制一条直线，使得这条直线与观测值之间的绝对距离最小（即最佳拟合线）。因此，在线性回归中，假设特征与连续因变量之间的关系是线性的。线的形式通常是斜截式（即
    *y = a + bx*），其中 *a* 为截距（即当 *x* 为 0 时 *y* 的值），*b* 为斜率，*x* 为自变量。本章将介绍两种线性回归：简单线性回归和多重线性回归。
- en: Simple Linear Regression
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单线性回归
- en: Simple linear regression models define the relationship between one feature
    and the continuous outcome variable using *y =* *α* *+* *β**x*. This equation
    is like the slope-intercept form, where *y* denotes the predicted value of the
    dependent variable, *α* denotes the intercept, *β* (beta) represents the slope,
    and *x* is the value of the independent variable. Given *x*, regression models
    compute the values for *α* and *β* that minimize the absolute difference between
    predicted *y* values (that is, *ŷ*) and actual *y* values.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归模型通过*y =* *α* *+* *β**x*定义一个特征与连续结果变量之间的关系。这个方程类似于斜截式，其中*y*表示因变量的预测值，*α*表示截距，*β*（贝塔）代表斜率，*x*是自变量的值。给定*x*值后，回归模型计算出能够最小化预测的*y*值（即*ŷ*）与实际*y*值之间绝对差异的*α*和*β*的值。
- en: For example, if we are predicting the weight of an individual in kilograms (kg)
    using height in meters (m) as the lone predictor variable, and the simple linear
    regression model computes 1.5 as the value for *α* and 50 as the coefficient for
    *β*, this model can be interpreted as for every 1 m increase in height, weight
    increases by 50 kg. Thus, we can predict that the weight of an individual who
    is 1.8 m is 91.5 kg using y = 1.5 + (50 x 1.8). In the following exercises, we
    will demonstrate simple linear regression using scikit-learn.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们使用身高（米）作为唯一预测变量来预测一个人的体重（千克），并且简单线性回归模型计算出*α*的值为1.5，*β*的系数为50，那么该模型可以解释为：每增加1米身高，体重大约增加50千克。因此，我们可以预测身高为1.8米的人的体重大约是91.5千克，计算公式为y
    = 1.5 + (50 x 1.8)。在接下来的练习中，我们将演示如何使用scikit-learn进行简单线性回归。
- en: 'Exercise 21: Preparing Data for a Linear Regression Model'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 21：为线性回归模型准备数据
- en: 'To prepare our data for a simple linear regression model, we will use a random
    subset of the Weather in Szeged 2006-2016 dataset, which consists of hourly weather
    measurements from April 1, 2006, to September 9, 2016, in Szeged, Hungary. The
    adapted data is provided as a `.csv` file (https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter02/weather.csv)
    and consists of 10,000 observations of 8 variables:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备我们的数据以用于简单的线性回归模型，我们将使用一个随机子集，该子集来自2006至2016年间匈牙利塞格德市的天气数据集。该数据集包含从2006年4月1日到2016年9月9日的每小时天气测量数据。经过处理的数据以`.csv`文件形式提供（https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter02/weather.csv），并包含10,000条观察记录，涵盖8个变量：
- en: '`Temperature_c`: The temperature in Celsius'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Temperature_c`：温度，单位为摄氏度'
- en: '`Humidity`: The proportion of humidity'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Humidity`：湿度比例'
- en: '`Wind_Speed_kmh`: The wind speed in kilometers per hour'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Wind_Speed_kmh`：风速，单位为公里每小时'
- en: '`Wind_Bearing_Degrees`: The wind direction in degrees clockwise from due north'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Wind_Bearing_Degrees`：风向，按顺时针方向从正北开始的角度'
- en: '`Visibility_km`: The visibility in kilometers'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Visibility_km`：能见度，单位为公里'
- en: '`Pressure_millibars`: The atmospheric pressure as measured in millibars'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pressure_millibars`：大气压力，单位为毫巴'
- en: '`Rain`: rain = 1, snow = 0'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Rain`：雨=1，雪=0'
- en: '`Description`: Warm, normal, or cold'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Description`：温暖、正常或寒冷'
- en: 'Import the `weather.csv` dataset using the following code:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码导入`weather.csv`数据集：
- en: '[PRE0]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Explore the data using `df.info()`:![Figure 3.1: Information describing df'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`df.info()`探索数据：![图 3.1：描述df的信息](img/C13322_03_01.jpg)
- en: '](img/C13322_03_01.jpg)'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_01.jpg)'
- en: 'Figure 3.1: Information describing df'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.1：描述df的信息
- en: 'The `Description` column is the lone categorical variable in `df`. Check the
    number of levels in `Description` as follows:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Description`列是`df`中的唯一类别变量。可以按如下方式检查`Description`中的层级数量：'
- en: '[PRE1]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The number of levels is shown in the following screenshot:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层级的数量如下图所示：
- en: '![Figure 3.2: Number of levels in the ''Description'' column](img/C13322_03_02.jpg)'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.2：''Description''列中的层级数量](img/C13322_03_02.jpg)'
- en: 'Figure 3.2: Number of levels in the ''Description'' column'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.2：'Description'列中的层级数量
- en: Note
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Multi-class, categorical variables must be converted into dummy variables via
    a process termed "dummy coding." Dummy coding a multi-class, categorical variable
    creates n-1 new binary features, which correspond to the levels within the categorical
    variable. For example, a multi-class, categorical variable with three levels will
    create two binary features. After the multi-class, categorical feature has been
    dummy coded, the original feature must be dropped.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多类类别变量必须通过一种称为“虚拟编码”的过程转换为虚拟变量。对多类类别变量进行虚拟编码会创建n-1个新的二元特征，这些特征对应于类别变量中的层级。例如，一个具有三个层级的多类类别变量将创建两个二元特征。经过虚拟编码后，必须删除原始特征。
- en: 'To dummy code all multi-class, categorical variables, refer to the following
    code:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要对所有多分类的类别变量进行虚拟编码，请参考以下代码：
- en: '[PRE2]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The original DataFrame, `df`, consisted of eight columns, one of which (that
    is, Description) was a multi-class, categorical variable with three levels.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 原始的DataFrame `df`包含八列，其中一列（即`Description`）是一个具有三个级别的多分类类别变量。
- en: 'In step 4, we transformed this feature into n-1 (that is, 2), separated dummy
    variables, and dropped the original feature, `Description`. Thus, `df_dummies`
    should now contain one more column than df (that is, 9 columns). Check this out
    using the following code:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第4步中，我们将这个特征转换为n-1（即2）个分开的虚拟变量，并删除了原始特征`Description`。因此，`df_dummies`现在应该包含比`df`多一列（即9列）。使用以下代码检查这一点：
- en: '[PRE3]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Figure 3.3: Number of columns after dummy coding'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.3：虚拟编码后列的数量'
- en: '](img/C13322_03_03.jpg)'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_03.jpg)'
- en: 'Figure 3.3: Number of columns after dummy coding'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.3：虚拟编码后列的数量
- en: 'To remove any possible order effects in the data, it is good practice to first
    shuffle the rows of the data prior to splitting the data into features (`X`) and
    outcome (`y`). To shuffle the rows in `df_dummies`, refer to the code here:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了消除数据中可能存在的顺序效应，良好的实践是先对数据的行进行洗牌，然后再将数据拆分为特征（`X`）和结果（`y`）。要对`df_dummies`中的行进行洗牌，请参考以下代码：
- en: '[PRE4]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that the data has been shuffled, we will split the rows in our data into
    features (`X`) and the dependent variable (`y`).
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在数据已经被洗牌，我们将把数据的行拆分为特征（`X`）和因变量（`y`）。
- en: Note
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Linear regression is used for predicting a continuous outcome. Thus, in this
    exercise, we will pretend that the continuous variable `Temperature_c` (the temperature
    in Celsius) is the dependent variable, and that we are preparing data to fit a
    linear regression model.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性回归用于预测连续的结果。因此，在本次练习中，我们假设连续变量`Temperature_c`（摄氏温度）是因变量，我们正在准备数据来拟合一个线性回归模型。
- en: 'Split `df_shuffled` into `X` and `y` as follows:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`df_shuffled`拆分为`X`和`y`如下所示：
- en: '[PRE5]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Split `X` and `y` into testing and training data using the code here:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将`X`和`y`拆分为测试数据和训练数据：
- en: '[PRE6]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that the data has been dummy coded, shuffled, split into `X` and `y`, and
    further divided into testing and training datasets, it is ready to be used in
    a linear or logistic regression model.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，数据已经进行了虚拟编码、洗牌、拆分成`X`和`y`，并进一步划分为测试数据集和训练数据集，准备好用于线性回归或逻辑回归模型。
- en: 'The screenshot here shows the first five rows of `X_train`:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的截图展示了`X_train`的前五行：
- en: '![Figure 3.4: The first five rows of X_train'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.4：`X_train`的前五行'
- en: '](img/C13322_03_04.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_04.jpg)'
- en: 'Figure 3.4: The first five rows of X_train'
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.4：`X_train`的前五行
- en: 'Exercise 22: Fitting a Simple Linear Regression Model and Determining the Intercept
    and Coefficient'
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习22：拟合简单线性回归模型并确定截距和系数
- en: In this exercise, we will continue using the data we prepared in Exercise 21
    to fit a simple linear regression model to predict the temperature in Celsius
    from the humidity.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将继续使用练习21中准备的数据，通过简单线性回归模型来预测摄氏温度与湿度之间的关系。
- en: 'Continuing from Exercise 21, perform the following steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从练习21继续，执行以下步骤：
- en: 'To instantiate a linear regression model, refer to the code here:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要实例化一个线性回归模型，请参考以下代码：
- en: '[PRE7]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Fit the model to the `Humidity` column in the training data using this code:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将模型拟合到训练数据中的`Humidity`列：
- en: '[PRE8]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Figure 3.5: The output from fitting the simple linear regression model'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.5：拟合简单线性回归模型的输出'
- en: '](img/C13322_03_05.jpg)'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_05.jpg)'
- en: 'Figure 3.5: The output from fitting the simple linear regression model'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.5：拟合简单线性回归模型的输出
- en: 'Extract the value for the intercept using the following code:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码提取截距的值：
- en: '[PRE9]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Extract the value for the `coefficient` as follows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码提取`coefficient`的值：
- en: '[PRE10]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we can print a message with the formula for predicting temperature in
    Celsius using the code here:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下代码打印出预测摄氏温度的公式：
- en: '[PRE11]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Figure 3.6: A formula to predict temperature in Celsius from humidity using
    simple linear regression'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.6：使用简单线性回归从湿度预测摄氏温度的公式'
- en: '](img/C13322_03_06.jpg)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_06.jpg)'
- en: 'Figure 3.6: A formula to predict temperature in Celsius from humidity using
    simple linear regression'
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.6：使用简单线性回归从湿度预测摄氏温度的公式
- en: Great work! According to this simple linear regression model, a day with a 0.78
    humidity value has a predicted a temperature of 10.56 degrees Celsius. Now that
    we are familiar with extracting the intercept and coefficients of our simple linear
    regression model, it is time to generate predictions and subsequently evaluate
    how the model performs on unseen, test data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！根据这个简单线性回归模型，一个湿度值为0.78的日子预测温度为10.56摄氏度。现在我们已经熟悉了提取简单线性回归模型的截距和系数，是时候生成预测并评估模型在未见过的测试数据上的表现了。
- en: Teaching tip
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 教学提示
- en: Practice calculating temperature at various levels of humidity.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 练习在不同湿度水平下计算温度。
- en: 'Exercise 23: Generating Predictions and Evaluating the Performance of a Simple
    Linear Regression Model'
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习23：生成预测并评估简单线性回归模型的表现
- en: The very purpose of supervised learning is to use existing, labeled data to
    generate predictions. Thus, this exercise will demonstrate how to generate predictions
    on the test feature and generate model performance metrics by comparing the predictions
    to the actual values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的核心目的是使用现有的标记数据生成预测。因此，本练习将演示如何在测试特征上生成预测，并通过将预测与实际值进行比较来生成模型性能指标。
- en: 'Continuing from *Exercise 22*, perform the following steps:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从*练习22*继续，执行以下步骤：
- en: 'Generate predictions on the test data using the following:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码在测试数据上生成预测：
- en: '[PRE12]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: A common way to evaluate model performance is to examine the correlation between
    the predicted and actual values using a scatterplot. The scatterplot displays
    the relationship between the actual and predicted values. A perfect regression
    model will display a straight, diagonal line between predicted and actual values.
    The relationship between the predicted and actual values can be quantified using
    the Pearson r correlation coefficient. In the following step, we will create a
    scatterplot of the predicted and actual values.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估模型性能的一种常见方法是使用散点图检查预测值与实际值之间的相关性。散点图展示了实际值与预测值之间的关系。一个完美的回归模型将在预测值和实际值之间显示一条直线。预测值与实际值之间的关系可以通过皮尔逊r相关系数来量化。在接下来的步骤中，我们将创建一个预测值与实际值的散点图。
- en: 'It is helpful if the correlation coefficient is displayed in the plot''s title.
    The following code will show us how to do this:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果相关系数显示在图表标题中会更有帮助。以下代码将演示如何做到这一点：
- en: '[PRE13]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is the resultant output:'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是生成的输出结果：
- en: '![Figure 3.7: Predicted versus actual values from a simple linear regression
    model'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.7：简单线性回归模型的预测值与实际值对比'
- en: '](img/C13322_03_07.jpg)'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_07.jpg)'
- en: 'Figure 3.7: Predicted versus actual values from a simple linear regression
    model'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.7：简单线性回归模型的预测值与实际值对比
- en: Note
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: With a Pearson r value of 0.62, there is a moderate, positive, linear correlation
    between the predicted and actual values. A perfect model would have all points
    on the plot in a straight line and an r value of 1.0.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 皮尔逊r值为0.62，表明预测值与实际值之间存在中等程度的正向线性相关性。一个完美的模型会使散点图中的所有点都在一条直线上，并且r值为1.0。
- en: 'A model that fits the data very well will have normally distributed residuals.
    To create a density plot of the residuals, refer to the following code:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个与数据拟合得非常好的模型，其残差应该呈正态分布。要创建残差的密度图，请参照以下代码：
- en: '[PRE14]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Refer to the resultant output here:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参阅此处的输出结果：
- en: '![Figure 3.8: A histogram of residuals from a simple linear regression model'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.8：简单线性回归模型的残差直方图'
- en: '](img/C13322_03_08.jpg)'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_08.jpg)'
- en: 'Figure 3.8: A histogram of residuals from a simple linear regression model'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.8：简单线性回归模型的残差直方图
- en: Note
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The histogram shows us that the residuals are negatively skewed and the value
    of the Shapiro W p-value in the title tells us that the distribution is not normal.
    This gives us further evidence that our model has room for improvement.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直方图显示残差呈负偏态，标题中的Shapiro W p值告诉我们该分布不是正态分布。这进一步证明我们的模型还有改进空间。
- en: 'Lastly, we will compute metrics for mean absolute error, mean squared error,
    root mean squared error, and R-squared, and put them into a DataFrame using the
    code here:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将计算平均绝对误差、均方误差、均方根误差和R平方值，并使用以下代码将它们放入一个数据框中：
- en: '[PRE15]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Please refer to the resultant output:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参阅生成的输出结果：
- en: '![Figure 3.9: Model evaluation metrics from a simple linear regression model'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.9：简单线性回归模型的模型评估指标'
- en: '](img/C13322_03_09.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_09.jpg)'
- en: 'Figure 3.9: Model evaluation metrics from a simple linear regression model'
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.9：简单线性回归模型的模型评估指标
- en: '**Mean absolute error** (**MAE**) is the average absolute difference between
    the predicted values and the actual values. **Mean squared error** (**MSE**) is
    the average of the squared differences between the predicted and actual values.
    **Root mean squared error** (**RMSE**) is the square root of the MSE. R-squared
    tells us the proportion of variance in the dependent variable that can be explained
    by the model. Thus, in this simple linear regression model, humidity explained
    only 38.9% of the variance in temperature. Additionally, our predictions were
    within ± 6.052 degrees Celsius.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）是预测值与实际值之间的平均绝对差。**均方误差**（**MSE**）是预测值与实际值之间差的平方的平均值。**均方根误差**（**RMSE**）是MSE的平方根。R方告诉我们可以由模型解释的因变量方差的比例。因此，在这个简单线性回归模型中，湿度仅能解释温度方差的38.9%。此外，我们的预测值在±6.052摄氏度范围内。'
- en: Here, we have successfully used scikit-learn to fit and evaluate a simple linear
    regression model. This is the first step in a very exciting journey to becoming
    a machine learning guru. Next, we will continue expanding our knowledge of regression
    and improving this model by exploring multiple linear regression.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们成功地使用 scikit-learn 拟合和评估了一个简单线性回归模型。这是成为机器学习专家之旅中的第一步。接下来，我们将继续扩展我们对回归的知识，并通过探索多元线性回归来改进这个模型。
- en: Multiple Linear Regression
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: Multiple linear regression models define the relationship between two or more
    features and the continuous outcome variable using *y =* *α* *+* *β**1**x**i1*
    *+* *β**2**x**i2* *+ … +* *β**p-1**x**i,p-1*. Again, *α* represents the intercept
    and *β* denotes the slope for each feature (*x*) in the model. Thus, if we are
    predicting the weight of an individual in kg using height in *m*, total cholesterol
    in milligrams per deciliter (*mg/dL)*, and minutes of cardiovascular exercise
    per day, and the multiple linear regression model computes 1.5 as the value for
    *α*, 50 as the coefficient for *β**1*, 0.1 as the coefficient for *β**2*, and
    -0.4 as the coefficient for *β**3*, this model can be interpreted as for every
    1 *m* increase in height, weight increases by 50 kg, controlling for all other
    features in the model. Additionally, for every 1 mg/dL increase in total cholesterol,
    weight increases by 0.1 kg, controlling for all other features in the model. Lastly,
    for every minute of cardiovascular exercise per day, weight decreases by 0.4 kg,
    controlling for all other features in the model. Thus, we can predict the weight
    of an individual who is 1.8 m tall, with total cholesterol of 200 mg/dL, and completes
    30 minutes of cardiovascular exercise per day as 99.5 kg using *y = 1.5 + (0.1
    x 50) + (200 x 0.5) + (30 x -0.4)*. In the following exercise, we will demonstrate
    conducting multiple linear regression using scikit-learn.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 多元线性回归模型通过 *y =* *α* *+* *β**1**x**i1* *+* *β**2**x**i2* *+ … +* *β**p-1**x**i,p-1*
    定义了两个或更多特征与连续结果变量之间的关系。再次说明，*α* 表示截距，*β* 表示模型中每个特征（*x*）的斜率。因此，如果我们使用身高（*m*）、总胆固醇（*mg/dL*）和每日心血管运动分钟数来预测个体的体重（kg），且多元线性回归模型计算出
    *α* 为1.5，*β**1* 为50，*β**2* 为0.1，*β**3* 为-0.4，这个模型可以解释为：在控制模型中的所有其他特征的情况下，每增加1
    *m* 的身高，体重增加50 kg。此外，每增加1 mg/dL 的总胆固醇，体重增加0.1 kg。最后，每天每增加1分钟的心血管运动，体重减少0.4 kg。因此，我们可以预测一个身高为1.8
    m、总胆固醇为200 mg/dL，并每天完成30分钟心血管运动的个体的体重为99.5 kg，使用 *y = 1.5 + (0.1 x 50) + (200
    x 0.5) + (30 x -0.4)*。在下一项练习中，我们将演示如何使用 scikit-learn 进行多元线性回归。
- en: 'Exercise 24: Fitting a Multiple Linear Regression Model and Determining the
    Intercept and Coefficients'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 24：拟合多元线性回归模型并确定截距和系数
- en: In this exercise, we will continue using the data we prepared in *Exercise 21*,
    *Preparing Data for a Linear Regression Model*, to fit a multiple linear regression
    model to predict the temperature in Celsius from all the features in the data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将继续使用我们在*练习21*中准备的数据，拟合一个多元线性回归模型，以预测数据中所有特征对应的温度（摄氏度）。
- en: 'Continuing from Exercise 23, perform the following steps:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 继续自练习 23，执行以下步骤：
- en: 'To instantiate a linear regression model, refer to the code here:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要实例化一个线性回归模型，请参考以下代码：
- en: '[PRE16]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Fit the model to the training data using this code:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将模型拟合到训练数据中：
- en: '[PRE17]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Figure 3.10: The output from fitting the multiple linear regression model'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.10：拟合多元线性回归模型的输出结果'
- en: '](img/C13322_03_10.jpg)'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_10.jpg)'
- en: 'Figure 3.10: The output from fitting the multiple linear regression model'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.10：拟合多元线性回归模型的输出结果
- en: 'Extract the value for the intercept using the following code:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码提取截距值：
- en: '[PRE18]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Extract the value for the coefficients as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方法提取系数值：
- en: '[PRE19]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we can print a message with the formula for predicting temperature in
    Celsius using the code here:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以打印出一个消息，显示使用此代码预测摄氏温度的公式：
- en: '[PRE20]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Our output should look like this:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的输出应如下所示：
- en: '![Figure 3.11: A formula to predict temperature in Celsius from humidity using
    multiple linear regression'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.11：使用多元线性回归根据湿度预测摄氏温度的公式'
- en: '](img/C13322_03_11.jpg)'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_11.jpg)'
- en: 'Figure 3.11: A formula to predict temperature in Celsius from humidity using
    multiple linear regression'
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.11：使用多元线性回归根据湿度预测摄氏温度的公式
- en: Nice job! According to this multiple regression model, a day with 0.78 humidity,
    5.0 km/h wind speed, wind direction at 81 degrees clockwise from due north, 3
    km of visibility, 1000 millibars of pressure, no rain, and is described as normal,
    has a predicted temperature in Celsius of 5.72 degrees. Now that we are familiar
    with extracting the intercept and coefficients of our multiple linear regression
    model, we can generate predictions and evaluate how the model performs on the
    test data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！根据这个多元回归模型，一天湿度为 0.78，风速为 5.0 km/h，风向为自正北顺时针 81 度，能见度为 3 km，气压为 1000 毫巴，没有降雨，且天气被描述为正常，那么该天预测的摄氏温度为
    5.72 度。现在我们已经熟悉了如何提取多元线性回归模型的截距和系数，我们可以生成预测并评估模型在测试数据上的表现。
- en: 'Activity 5: Generating Predictions and Evaluating the Performance of a Multiple
    Linear Regression Model'
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 5：生成预测并评估多元线性回归模型的性能
- en: In *Exercise 23*, *Generating Predictions and Evaluating the Performance of
    a Simple Linear Regression Model*, we learned how to generate predictions and
    evaluate the performance of a simple linear regression model using a variety of
    methods. To reduce the code redundancy, we will evaluate the performance of our
    multiple linear regression model using the metrics in *step 4* of *Exercise 23*,
    and we will determine if the multiple linear regression model performed better
    or worse in relation to the simple linear regression model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习 23*中，*生成预测并评估简单线性回归模型的性能*，我们学习了如何使用多种方法生成预测并评估简单线性回归模型的性能。为了减少代码的冗余，我们将使用*练习
    23*中*步骤 4*的指标来评估多元线性回归模型的性能，并确定多元线性回归模型相对于简单线性回归模型的表现是更好还是更差。
- en: 'Continuing from Exercise 24, perform the following steps:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从练习 24 开始，执行以下步骤：
- en: Generate predictions on the test data using all the features.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有特征在测试数据上生成预测。
- en: Plot predictions versus actual using a scatterplot.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用散点图绘制预测值与实际值的关系。
- en: Plot the distribution of the residuals.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制残差的分布图。
- en: Calculate the metrics for mean absolute error, mean squared error, root mean
    squared error, and R-squared and put them into a DataFrame.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算均值绝对误差、均方误差、均方根误差和 R 平方，并将结果放入 DataFrame 中。
- en: Determine if the multiple linear regression model performed better or worse
    in relation to the simple linear regression model.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定多元线性回归模型相较于简单线性回归模型的表现是更好还是更差。
- en: Note
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 343.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第 343 页找到。
- en: You should find that the multiple linear regression model performed better on
    every metric relative to the simple linear regression model. Most notably, in
    the simple linear regression model, only 38.9% of the variance in temperature
    was described by the model. However, in the multiple linear regression model,
    86.6% of the variance in temperature was explained by the combination of features.
    Additionally, our simple linear regression model predicted temperatures, on average,
    within ± 6.052 degrees, while our multiple linear regression model predicted temperatures,
    on average, within ± 2.861 degrees.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该发现，相较于简单线性回归模型，多元线性回归模型在每个指标上表现得更好。最显著的是，在简单线性回归模型中，模型只能解释温度方差的 38.9%。然而，在多元线性回归模型中，特征的组合解释了
    86.6% 的温度方差。此外，我们的简单线性回归模型预测的温度平均误差为 ± 6.052 度，而我们的多元线性回归模型预测的温度平均误差为 ± 2.861
    度。
- en: The transparent nature of the intercept and beta coefficients make linear regression
    models very easy to interpret. In business, it is commonly requested that data
    scientists explain the effect of a certain feature on an outcome. Thus, linear
    regression provides metrics allowing a reasonable response to the business inquiry
    earlier.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 截距和回归系数的透明性使得线性回归模型非常易于解释。在商业中，通常要求数据科学家解释某一特征对结果的影响。因此，线性回归提供了可用的指标，从而合理回应了之前的业务询问。
- en: However, much of the time, a problem requires the data scientist to predict
    an outcome measure that is not continuous, but categorical. For example, in insurance,
    given certain features of a customer, what is the probability that this customer
    will not renew their policy? In this case, there is not a linear relationship
    between the features in the data and the outcome variable, so linear regression
    will falter. A viable option for conducting regression analysis on a categorical
    dependent variable is logistic regression.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数情况下，问题要求数据科学家预测一个不是连续的，而是分类的结果变量。例如，在保险领域，给定客户的某些特征，预测该客户是否会续保的概率是多少？在这种情况下，数据中的特征与结果变量之间不存在线性关系，因此线性回归方法会失败。对于分类因变量，进行回归分析的一个可行选择是逻辑回归。
- en: Logistic Regression
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression uses categorical and continuous variables to predict a categorical
    outcome. When the dependent variable of choice has two categorical outcomes, the
    analysis is termed binary logistic regression. However, if the outcome variable
    consists of more than two levels, the analysis is referred to as multinomial logistic
    regression. For the purposes of this chapter, we will focus our learning on the
    former.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归使用分类变量和连续变量来预测分类结果。当所选因变量有两个分类结果时，分析称为二元逻辑回归。然而，如果结果变量有多个水平，则该分析称为多项式逻辑回归。本章的学习将集中在前者上。
- en: When predicting a binary outcome, we do not have a linear relationship between
    the features and the outcome variable; an assumption of linear regression. Thus,
    to express a nonlinear relationship in a linear way, we must transform the data
    using logarithmic transformation. As a result, logistic regression allows us to
    predict the probability of the binary outcome occurring given the feature(s) in
    the model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测二元结果时，特征与结果变量之间并不存在线性关系，这违反了线性回归的假设。因此，为了以线性方式表达非线性关系，我们必须使用对数转换来转换数据。因此，逻辑回归使我们能够预测在给定模型中特征的情况下二元结果发生的概率。
- en: 'For logistic regression with 1 predictor, the logistic regression equation
    is shown here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有 1 个预测变量的逻辑回归，逻辑回归方程如下所示：
- en: '![Figure 3.12: Logistic regression formula with 1 predictor'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.12：具有 1 个预测变量的逻辑回归公式'
- en: '](img/C13322_03_12.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_12.jpg)'
- en: 'Figure 3.12: Logistic regression formula with 1 predictor'
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.12：具有 1 个预测变量的逻辑回归公式
- en: 'In the preceding figure, *P(Y)* is the probability of the outcome occurring,
    *e* is the base of natural logarithms, *α* is the intercept, *β* is the beta coefficient,
    and *x* is the value of the predictor. This equation can be extended to multiple
    predictors using the formula here:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，*P(Y)* 是结果发生的概率，*e* 是自然对数的底数，*α* 是截距，*β* 是回归系数，*x* 是预测变量的值。该方程可以扩展到多个预测变量，使用以下公式：
- en: '![Figure 3.13: Logistic regression formula with more than one predictor'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.13：具有多个预测变量的逻辑回归公式'
- en: '](img/C13322_03_13.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_13.jpg)'
- en: 'Figure 3.13: Logistic regression formula with more than one predictor'
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.13：具有多个预测变量的逻辑回归公式
- en: Thus, using logistic regression to model the probability of an event occurring
    is the same as fitting a linear regression model, except the continuous outcome
    variable has been replaced by the log odds (an alternate way of expressing probabilities)
    of success for a binary outcome variable. In linear regression, we assumed a linear
    relationship between the predictor variable(s) and the outcome variable. Logistic
    regression, on the other hand, assumes a linear relationship between the predictor
    variable(s) and the natural log of *p/(1-p)*, where *p* is the probability of
    the event occurring.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用逻辑回归来建模事件发生的概率，就相当于拟合一个线性回归模型，只是连续的结果变量已被替换为成功的对数比（另一种表达概率的方式），用于二元结果变量。在线性回归中，我们假设预测变量与结果变量之间存在线性关系。而逻辑回归则假设预测变量与
    *p/(1-p)* 的自然对数之间存在线性关系，其中 *p* 是事件发生的概率。
- en: In the following exercise, we will use the `weather.csv` dataset to demonstrate
    building a logistic regression model to predict the probability of rain using
    all the features in our data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将使用 `weather.csv` 数据集，演示如何构建一个逻辑回归模型，利用我们数据中的所有特征来预测降雨的概率。
- en: 'Exercise 25: Fitting a Logistic Regression Model and Determining the Intercept
    and Coefficients'
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 25：拟合逻辑回归模型并确定截距和系数
- en: To model the probability of rain (as opposed to snow) using all the features
    in our data, we will use the `weather.csv` file and store the dichotomous variable
    `Rain` as the outcome measure.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用数据中的所有特征来建模降雨的概率（而非雪），我们将使用 `weather.csv` 文件，并将二元变量 `Rain` 作为结果测量。
- en: 'Import data using the following code:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码导入数据：
- en: '[PRE21]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Dummy code the `Description` variable as follows:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示对 `Description` 变量进行虚拟编码：
- en: '[PRE22]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Shuffle `df_dummies` using the code here:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此处的代码对 `df_dummies` 进行洗牌：
- en: '[PRE23]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Split the features and outcome into `X` and `y`, respectively, as follows:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，将特征和结果分别拆分为 `X` 和 `y`：
- en: '[PRE24]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Split the features and outcome into training and testing data using the code
    here:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将特征和结果拆分为训练数据和测试数据：
- en: '[PRE25]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Instantiate a logistic regression model using this code:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码实例化一个逻辑回归模型：
- en: '[PRE26]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Fit the logistic regression model to the training data using `model.fit(X_train,
    y_train`). We should get the following output:![Figure 3.14: The output from fitting
    a logistic regression model'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `model.fit(X_train, y_train)` 将逻辑回归模型拟合到训练数据中。我们应该得到以下输出：![图 3.14：拟合逻辑回归模型的输出
- en: '](img/C13322_03_14.jpg)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_14.jpg)'
- en: 'Figure 3.14: The output from fitting a logistic regression model'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.14：拟合逻辑回归模型的输出
- en: 'Get the intercept using the following:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码获取截距：
- en: '[PRE27]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Extract the coefficients using the following:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码提取系数：
- en: '[PRE28]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Place the coefficients into a list as follows:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，将系数放入列表：
- en: '[PRE29]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Match features to their coefficients, place them in a DataFrame, and print
    the DataFrame to the console as follows:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征与它们的系数匹配，放入 DataFrame 中，并按如下方式将 DataFrame 打印到控制台：
- en: '[PRE30]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Refer to the resultant output here:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参考此处的输出：
- en: '![Figure 3.15: Features and their coefficients from the logistic regression
    model'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.15：逻辑回归模型的特征及其系数'
- en: '](img/C13322_03_15.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_15.jpg)'
- en: 'Figure 3.15: Features and their coefficients from the logistic regression model'
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.15：逻辑回归模型的特征及其系数
- en: The coefficient for temperature can be interpreted as for every 1-degree increase
    in temperature, the log odds of rain increase by 5.69, controlling for all other
    features in the model. To generate predictions, we could convert the log odds
    to odds and the odds to probability. However, scikit-learn has functionality to
    generate predicted probability, as well as predicted classes.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 温度的系数可以解释为：每增加 1 度温度，降雨的对数几率增加 5.69，同时控制模型中的其他特征。为了生成预测，我们可以将对数几率转换为几率，再将几率转换为概率。然而，scikit-learn
    提供了生成预测概率以及预测类别的功能。
- en: 'Exercise 26: Generating Predictions and Evaluating the Performance of a Logistic
    Regression Model'
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 26：生成预测并评估逻辑回归模型的表现
- en: In *Exercise 25*, we learned how to fit a logistic regression model and extract
    the elements necessary to generate predictions. However, scikit-learn makes our
    lives much easier by providing us with functions to predict the probability of
    an outcome, as well as the classes of an outcome. In this exercise, we will learn
    to generate predicted probabilities and classes, as well as evaluating a model
    performance using a confusion matrix and a classification report.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习 25*中，我们学习了如何拟合一个逻辑回归模型，并提取生成预测所需的元素。然而，scikit-learn 通过提供函数来预测结果的概率以及预测结果的类别，使我们的工作变得更加轻松。在本练习中，我们将学习如何生成预测的概率和类别，并使用混淆矩阵和分类报告评估模型的表现。
- en: 'Continuing from Exercise 25, perform the following steps:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从练习 25 开始，执行以下步骤：
- en: 'Generate predicted probabilities using the following code:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码生成预测概率：
- en: '[PRE31]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Generate predicted classes using the following:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码生成预测类别：
- en: '[PRE32]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Evaluate a performance using a confusion matrix as follows:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用混淆矩阵按如下方式评估表现：
- en: '[PRE33]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Refer to the resultant output here:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参考此处的输出：
- en: '![Figure 3.16: The confusion matrix from our logistic regression model'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.16：我们逻辑回归模型的混淆矩阵'
- en: '](img/C13322_03_16.jpg)'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_16.jpg)'
- en: 'Figure 3.16: The confusion matrix from our logistic regression model'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.16：我们逻辑回归模型的混淆矩阵
- en: Note
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: From the confusion matrix, we can see that, of the 383 observations that were
    not classified as rainy, 377 of them were correctly classified, and of the 2917
    observations that were classified as rainy, 2907 of them were correctly classified.
    To further inspect our model's performance using metrics such as precision, recall,
    and f1-score, we will generate a classification report.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从混淆矩阵中，我们可以看到，在383个未被分类为“雨天”的观测值中，377个被正确分类；在2917个被分类为“雨天”的观测值中，2907个被正确分类。为了进一步检查我们模型的性能，我们将使用精度、召回率和`f1`得分等指标生成分类报告。
- en: 'Generate a classification report using the following code:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码生成分类报告：
- en: '[PRE34]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Refer to the resultant output:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参考结果输出：
- en: '![Figure 3.17: The classification report generated from our logistic regression
    model'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.17：我们逻辑回归模型生成的分类报告'
- en: '](img/C13322_03_17.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_17.jpg)'
- en: 'Figure 3.17: The classification report generated from our logistic regression
    model'
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.17：我们逻辑回归模型生成的分类报告
- en: As we can see from our confusion matrix and classification report, our model
    is performing very well and may be difficult to improve upon. However, machine
    learning models including logistic regression consist of numerous hyperparameters
    that can be adjusted to further improve model performance. In the next exercise,
    we will learn to find the optimal combination of hyperparameters to maximize model
    performance.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的混淆矩阵和分类报告中可以看出，我们的模型表现非常好，可能很难进一步改进。然而，机器学习模型，包括逻辑回归，包含许多可以调整的超参数，调优这些超参数可以进一步提高模型性能。在下一个练习中，我们将学习如何找到超参数的最佳组合，以最大化模型性能。
- en: 'Exercise 27: Tuning the Hyperparameters of a Multiple Logistic Regression Model'
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 27：调优多重逻辑回归模型的超参数
- en: In *step 7* of *Exercise 25*, we fit a logistic regression model and the subsequent
    output from that model is displayed in Figure 3.14\. Each of those arguments inside
    the `LogisticRegression()` function is set to a default hyperparameter. To tune
    the model, we will use scikit-learn's grid search function, which fits a model
    for every combination of possible hyperparameter values and determines the value
    for each hyperparameter resulting in the best model. In this exercise, we will
    learn how to use grid search to tune models.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习 25*的*步骤 7*中，我们拟合了一个逻辑回归模型，随后该模型的输出显示在图 3.14 中。`LogisticRegression()`函数中的每个参数都设置为默认超参数。为了调优模型，我们将使用
    scikit-learn 的网格搜索功能，它会为每个可能的超参数值组合拟合模型，并确定每个超参数的最佳值，从而得到最好的模型。在本练习中，我们将学习如何使用网格搜索来调优模型。
- en: 'Continuing from *Exercise 26*:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 继续从*练习 26*：
- en: 'The data has already been prepared for us (see Exercise 26); thus, we can jump
    right into instantiating a grid of possible hyperparameter values as follows:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据已经为我们准备好了（见练习 26）；因此，我们可以直接开始实例化一个可能的超参数值网格，如下所示：
- en: '[PRE35]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Instantiate a grid search model to find the model with the greatest `f1` score
    (that is, the harmonic average of precision and recall) as follows:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个网格搜索模型，寻找具有最大`f1`得分（即精度和召回率的调和平均数）的模型，如下所示：
- en: '[PRE36]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Fit the model on the training using `model.fit(X_train, y_train)` (keep in
    mind, this may take a while) and find the resultant output here:![Figure 3.18:
    The output from our logistic regression grid search model'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`model.fit(X_train, y_train)`在训练集上拟合模型（请记住，这可能需要一些时间），并找到此处的结果输出：![图 3.18：我们逻辑回归网格搜索模型的输出
- en: '](img/C13322_03_18.jpg)'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_18.jpg)'
- en: 'Figure 3.18: The output from our logistic regression grid search model'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.18：我们逻辑回归网格搜索模型的输出
- en: 'We can return the optimal combination of hyperparameters as a dictionary as
    follows:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以如下所示返回超参数的最佳组合，以字典形式表示：
- en: '[PRE37]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Refer to the resultant output here:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参考此处的结果输出：
- en: '![Figure 3.19: The tuned hyperparameters from our logistic regression grid
    search model'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.19：我们逻辑回归网格搜索模型调优后的超参数'
- en: '](img/C13322_03_19.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_19.jpg)'
- en: 'Figure 3.19: The tuned hyperparameters from our logistic regression grid search
    model'
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.19：我们逻辑回归网格搜索模型调优后的超参数
- en: We have found the combination of hyperparameters that maximizes the `f1` score.
    Remember, simply using the default hyperparameters in *Exercise 25* resulted in
    a model that performed very well on the test data. Thus, in the following activity,
    we will evaluate how the model with tuned hyperparameters performed on the test
    data.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经找到了最大化`f1`得分的超参数组合。请记住，仅仅使用*练习 25*中的默认超参数就能得到一个在测试数据上表现非常好的模型。因此，在接下来的活动中，我们将评估调优过的超参数模型在测试数据上的表现。
- en: 'Activity 6: Generating Predictions and Evaluating Performance of a Tuned Logistic
    Regression Model'
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 6：生成预测并评估调优后的逻辑回归模型性能
- en: Once the best combination of hyperparameters has been converged upon, we need
    to evaluate model performance much like we did in *Exercise 25*.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦找到了最佳的超参数组合，我们需要像在*练习 25* 中一样评估模型的性能。
- en: 'Continuing from Exercise 27:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 从练习 27 继续：
- en: Generate the predicted probabilities of rain.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成预测的降雨概率。
- en: Generate the predicted class of rain.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成预测的降雨类别。
- en: Evaluate performance with a confusion matrix and store it as a DataFrame.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用混淆矩阵评估性能，并将其存储为数据框。
- en: Print a classification report.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印分类报告。
- en: Note
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 346.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第 346 页找到。
- en: By tuning the hyperparameters of the logistic regression model, we were able
    to improve upon a logistic regression model that was already performing very well.
    We will continue to expand upon tuning different types of models in the following
    exercises and activities.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整逻辑回归模型的超参数，我们能够改进已经表现非常良好的逻辑回归模型。我们将在接下来的练习和活动中继续扩展调整不同类型的模型。
- en: Max Margin Classification Using SVMs
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大边界分类使用 SVM
- en: SVM is an algorithm for supervised learning that solves both classification
    and regression problems. However, SVM is most commonly used in classification
    problems, so, for the purposes of this chapter, we will focus on SVM as a binary
    classifier. The goal of SVM is to determine the best location of a hyperplane
    that create a class boundary between data points plotted on a multidimensional
    space. To help clarify this concept, refer to Figure 3.20.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 是一种监督学习算法，可以解决分类和回归问题。然而，SVM 最常用于分类问题，因此，在本章中，我们将重点讨论 SVM 作为二分类器。SVM 的目标是确定超平面的最佳位置，在多维空间中创建数据点之间的类别边界。为帮助澄清这一概念，请参见图
    3.20。
- en: '![Figure 3.20: Hyperplane (blue) separating the circles from the squares in
    three dimensions'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.20：超平面（蓝色）在三维空间中将圆形与方形分开'
- en: '](img/C13322_03_20.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_20.jpg)'
- en: 'Figure 3.20: Hyperplane (blue) separating the circles from the squares in three
    dimensions'
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.20：超平面（蓝色）在三维空间中将圆形与方形分开
- en: In Figure 3.20, the squares and circles are observations in the same DataFrame
    that represent different classes. In this figure, the hyperplane is depicted by
    a semi-transparent blue boundary lying between the circles and squares that separate
    the observations into two distinct classes. In this example, the observations
    are said to be linearly separable.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 3.20 中，方形和圆形是同一数据框中的观测值，代表不同的类别。在该图中，超平面由一个半透明的蓝色边界表示，位于圆形和方形之间，划分出两个不同的类别。在这个例子中，观测点被认为是线性可分的。
- en: 'The location of the hyperplane is determined by finding the position that creates
    the maximum separation (that is, margin) between the two classes. Thus, this is
    referred to as the **Maximum Margin Hyperplane** (MMH) and improves the likelihood
    that the points will remain on the correct side of the hyperplane boundary. It
    is possible to express the MMH using the points from each class that are closest
    to the MMH. These points are termed support vectors and each class has at least
    1\. Figure 3.21 visually depicts the support vectors in relation to the MMH in
    2 dimensions:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 超平面的位置是通过找到在两类之间创建最大分隔（即边界）的位置来确定的。因此，这被称为**最大边界超平面**（MMH），并提高了点保持在超平面边界正确一侧的可能性。可以通过每个类中最接近
    MMH 的点来表达 MMH。这些点被称为支持向量，每个类至少有 1 个。图 3.21 直观地展示了支持向量与 MMH 在二维空间中的关系：
- en: '![Figure 3.21: Support vectors in relation to the MMH'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.21：支持向量与最大边界超平面（MMH）的关系'
- en: '](img/C13322_03_21.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_21.jpg)'
- en: 'Figure 3.21: Support vectors in relation to the MMH'
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.21：支持向量与 MMH 的关系
- en: 'In reality, most data is not linearly separable. In this case, SVM makes use
    of a slack variable, which creates a soft margin (as opposed to a maximum margin),
    allowing some observations to fall on the incorrect side of the line. See the
    following plot:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，大多数数据是不可线性分割的。在这种情况下，SVM 使用松弛变量，创建一个软边界（与最大边界相对），允许一些观测点落在边界的错误一侧。请参见以下图示：
- en: '![Figure 3.22: 2 observations (as denoted with grey shading and the Greek letter
    Χi) fall on the incorrect side of the soft margin line'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.22：2 个观测点（用灰色阴影和希腊字母 Χi 表示）落在软边界线的错误一侧'
- en: '](img/C13322_03_22.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_22.jpg)'
- en: 'Figure 3.22: 2 observations (as denoted with grey shading and the Greek letter
    Χi) fall on the incorrect side of the soft margin line'
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.22：两个观测值（用灰色阴影和希腊字母Χi表示）位于软边界线的错误一侧
- en: A cost value is applied to the misclassified data points and, instead of finding
    the maximum margin, the algorithm minimizes the total cost. As the cost parameter
    increases, a harder SVM optimization will go for 100% separation and may overfit
    the training data. Conversely, lower cost parameters emphasize a wider margin
    and may underfit the training data. Thus, to create SVM models that perform well
    on the test data, it is important to determine a cost parameter that balances
    overfitting and underfitting.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 对误分类的数据点应用一个成本值，算法的目标不是找到最大边界，而是最小化总成本。随着成本参数的增加，SVM优化将更加严格，目标是100%的分类分离，这可能导致过拟合训练数据。相反，较低的成本参数则强调更宽的边界，可能导致欠拟合训练数据。因此，为了创建在测试数据上表现良好的SVM模型，重要的是确定一个平衡过拟合和欠拟合的成本参数。
- en: Additionally, data that is not linearly separable can be transformed into a
    higher-dimension space using the kernel trick. After this mapping to a higher-dimensional
    space, a nonlinear relationship can appear linear. By transforming the original
    data, SVM can discover associations not explicitly apparent in the original features.
    scikit-learn uses the Gaussian RBF kernel by default, but comes equipped with
    common kernels such as linear, polynomial, and sigmoid as well. In order to maximize
    the performance of an SVM classifier model, the optimal combination of the kernel
    and cost function must be determined. Luckily, this can be easily achieved using
    grid search hyperparameter tuning, as introduced in Exercise 27\. In the following
    exercises and activities, we will learn how this feat is accomplished.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，非线性可分的数据可以通过使用核技巧转换到更高维度的空间。在映射到高维空间后，原本的非线性关系可能变得线性。通过对原始数据进行转换，SVM可以发现原始特征中没有明确体现的关联。scikit-learn默认使用高斯RBF核，但也提供了常用的核函数，如线性核、多项式核和sigmoid核。为了最大化SVM分类器模型的性能，必须确定核函数和成本函数的最佳组合。幸运的是，可以通过网格搜索超参数调整轻松实现这一点，正如练习27中介绍的那样。在接下来的练习和活动中，我们将学习如何实现这一目标。
- en: 'Exercise 28: Preparing Data for the Support Vector Classifier (SVC) Model'
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 28：为支持向量分类器（SVC）模型准备数据
- en: 'Before fitting an SVM classifier model to predict a binary outcome variable;
    in this case, rain or snow, we must prepare our data. Since SVM is a black box,
    meaning the processes between input and output are not explicit, we do not need
    to worry about interpretability. Thus, we will transform the features in our data
    into z-scores prior to fitting the model. The following steps will show how to
    do this:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在为预测二元结果变量（在此案例中为雨或雪）拟合SVM分类器模型之前，我们必须先准备数据。由于SVM是一个“黑箱”模型，即输入和输出之间的过程不明确，因此我们不需要担心模型的可解释性。因此，在拟合模型之前，我们将数据中的特征转换为z分数。以下步骤将展示如何进行：
- en: 'Import `weather.csv` using the following code:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码导入`weather.csv`：
- en: '[PRE38]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Dummy code the categorical feature, `Description`, as follows:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分类特征`Description`进行虚拟编码，方法如下：
- en: '[PRE39]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Shuffle `df_dummies` to remove any ordering effects using the code here:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码对`df_dummies`进行洗牌，以去除任何排序效应：
- en: '[PRE40]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Split `df_shuffled` into `X` and `y` using the following code:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将`df_shuffled`拆分为`X`和`y`：
- en: '[PRE41]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Split `X` and `y` into testing and training data using the code here:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将`X`和`y`划分为测试数据和训练数据：
- en: '[PRE42]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To prevent any data leakage, scale `X_train` and `X_test` by fitting a scaler
    model to `X_train` and transforming them to z-scores separately, as follows:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了防止数据泄漏，通过拟合一个缩放器模型到`X_train`并分别将其转换为z分数，按如下方式对`X_train`和`X_test`进行缩放：
- en: '[PRE43]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now that our data has been properly divided into features and outcome variables,
    split into testing and training data, and scaled separately, we can tune the hyperparameters
    of our SVC model using a grid search.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的数据已经正确地划分为特征和结果变量，分为测试数据和训练数据，并且已分别进行了缩放，因此我们可以使用网格搜索调整SVC模型的超参数。
- en: 'Exercise 29: Tuning the SVC Model Using Grid Search'
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 29：使用网格搜索调整SVC模型
- en: Previously, we discussed the importance of determining the optimal cost function
    and kernel for SVM classifier models. In Exercise 27, we learned how to find the
    optimal combination of hyperparameters using scikit-learn's grid search function.
    In this exercise, we will demonstrate using grid search to find the best combination
    of the cost function and kernel.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论了确定 SVM 分类器模型的最佳成本函数和核函数的重要性。在练习 27 中，我们学习了如何使用 scikit-learn 的网格搜索功能找到超参数的最佳组合。在本练习中，我们将演示如何使用网格搜索来找到最佳的成本函数和核函数组合。
- en: 'Continuing from *Exercise 28*:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 从*练习 28*继续：
- en: 'Instantiate the grid for which to search using the following code:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码实例化要搜索的网格：
- en: '[PRE44]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Instantiate the `GridSearchCV` model with the `gamma` hyperparameter set to
    `auto` to avoid warnings, and set probability to `True` so we can extract probability
    of rain as follows:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `gamma` 超参数设置为 `auto` 来避免警告，并将概率设置为 `True`，以便我们可以提取雨的概率，如下所示：
- en: '[PRE45]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Fit the grid search model using `model.fit(X_train_scaled, y_train)`:![Figure
    3.23: The output from fitting the SVC grid search model'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `model.fit(X_train_scaled, y_train)` 拟合网格搜索模型：![图 3.23：拟合 SVC 网格搜索模型的输出
- en: '](img/C13322_03_23.jpg)'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_23.jpg)'
- en: 'Figure 3.23: The output from fitting the SVC grid search model'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.23：拟合 SVC 网格搜索模型的输出
- en: 'Print the best parameters using the following code:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码打印最佳参数：
- en: '[PRE46]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'See the resultant output below:'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参见下面的结果输出：
- en: '![Figure 3.24: Tuned hyperparameters for our SVC grid search model'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.24：我们 SVC 网格搜索模型的调优超参数'
- en: '](img/C13322_03_24.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_24.jpg)'
- en: 'Figure 3.24: Tuned hyperparameters for our SVC grid search model'
  id: totrans-293
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.24：我们 SVC 网格搜索模型的调优超参数
- en: Once the optimal combination of hyperparameters has been determined, it is time
    to generate predictions and subsequently evaluate how our model performed on the
    unseen test data.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了超参数的最佳组合，就可以开始生成预测，并随后评估我们的模型在未见测试数据上的表现。
- en: 'Activity 7: Generating Predictions and Evaluating the Performance of the SVC
    Grid Search Model'
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 7：生成预测并评估 SVC 网格搜索模型的性能
- en: In previous exercises/activities, we learned to generate predictions and evaluate
    classifier model performance. In this activity we will, again, evaluate the performance
    of our model by generating predictions, creating a confusion matrix, and printing
    a classification report.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的练习/活动中，我们学习了如何生成预测并评估分类器模型的性能。在本活动中，我们将再次评估模型的性能，通过生成预测、创建混淆矩阵并打印分类报告。
- en: 'Continuing from Exercise 29:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 从练习 29 继续：
- en: Extract the predicted classes.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取预测的类别。
- en: Create and print a confusion matrix.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建并打印混淆矩阵。
- en: Generate and print a classification report.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成并打印分类报告。
- en: Note
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 348.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 348 页找到。
- en: Here, we demonstrated how to tune the hyperparameters of an SVC model using
    grid search. After tuning the SVC model, it did not perform as well as the tuned
    logistic regression model in predicting rain/snow. Additionally, SVC models are
    a **black box** in that they do not provide insight into the contribution of features
    on the outcome measure. In the upcoming *Decision Trees* section, we will introduce
    a different algorithm known as a decision tree, which uses a "*divide and conquer*"
    approach to generate predictions and offers a feature importance attribute for
    determining the importance of each feature on the outcome.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了如何通过网格搜索调优 SVC 模型的超参数。调优后的 SVC 模型在预测雨/雪方面的表现不如调优后的逻辑回归模型。此外，SVC 模型是一个**黑箱**，因为它们无法提供特征对结果的贡献的洞察。在接下来的*决策树*部分，我们将介绍一种不同的算法——决策树，它采用“*分而治之*”的方法生成预测，并提供特征重要性属性，用于确定每个特征对结果的影响。
- en: Decision Trees
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: Imagine we are considering changing jobs. We are weighing the pros and cons
    of prospective job opportunities and, after a few years of being in our current
    position, we start to realize the things that are important to us. However, not
    all aspects of a career are of equal importance. In fact, after being in the job
    for a few years, we decide that the most important aspect of a position is our
    interest in the projects we will be doing, followed by compensation, then work-related
    stress, trailed by commute time, and, lastly, benefits. We have just created the
    scaffolding of a cognitive decision tree. We can go into further detail by saying
    that we want a job where we are very interested in the allocated projects, paying
    at least $55k/year, with low work-related stress, a commute of under 30 minutes,
    and good dental insurance. Creating mental decision trees is a decision-making
    process we all utilize by nature and is one of the reasons why decision trees
    are one of the most widely used machine learning algorithms today.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在考虑换工作。我们正在权衡潜在工作机会的利弊，在当前职位待了几年后，我们开始意识到对我们来说重要的东西。然而，职业的各个方面并非都同等重要。事实上，在工作几年后，我们决定职位最重要的方面是我们对所做项目的兴趣，其次是薪酬，再然后是工作压力，接着是通勤时间，最后是福利。我们刚刚创建了一个认知决策树的框架。我们可以进一步详细说明，我们希望找到一份工作，其中分配的项目我们非常感兴趣，年薪至少为$55k，工作压力较低，通勤时间不超过30分钟，并且有良好的牙科保险。创建心理决策树是我们天生就会利用的一种决策过程，也是决策树成为当今最广泛使用的机器学习算法之一的原因。
- en: In machine learning, decision trees use either *gini* impurity or *entropy*
    information gain as the criterion to measure the quality of a split. First, the
    decision tree algorithm determines the feature that maximizes the value indicating
    quality of a split. This becomes referred to as the root node, as it is the most
    important feature in the data. In the job offer mentioned earlier, being very
    interested in the prospective projects would be considered the root node. Taking
    into consideration the root node, the job opportunities are divided into those
    with very interesting projects and those without very interesting projects.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，决策树使用*基尼*不纯度或*熵*信息增益作为衡量拆分质量的标准。首先，决策树算法确定能够最大化拆分质量值的特征。这被称为根节点，因为它是数据中最重要的特征。在前面提到的工作机会中，对潜在项目非常感兴趣会被视为根节点。考虑到根节点，工作机会被分为那些有非常有趣项目的和那些没有非常有趣项目的。
- en: Next, each of these two categories are divided into the next most important
    feature, given the previous feature(s), and so on and so forth, until the potential
    jobs are identified as being of interest or not.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在考虑前一个特征的基础上，将这两个类别细分为下一个最重要的特征，以此类推，直到潜在工作被识别为感兴趣或不感兴趣。
- en: 'This approach is termed recursive partitioning, or "*divide and conquer*",
    because it continues the process of splitting and subsetting the data until the
    algorithm determines the subsets in the data as sufficiently homogenous, or:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为递归划分，或称为"*分而治之*"，因为它持续不断地拆分和细分数据，直到算法判断数据子集已足够同质化，或者：
- en: Nearly all the observations at the corresponding node have the same class (that
    is, purity).
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对应节点的几乎所有观察结果都属于同一类别（即纯度）。
- en: There are no further features in the data for which to split.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中没有进一步的特征可供拆分。
- en: The tree has reached the size limit decided upon a priori.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树已达到事先决定的大小限制。
- en: For example, if purity is determined by entropy, we must understand that entropy
    is a measure of randomness within a set of values. Decision trees operate by choosing
    the splits that minimize entropy (randomness) and, in turn, maximize information
    gain. Information gain is calculated as the difference in entropy between the
    split and all other following splits. The total entropy is then computed by taking
    the sum of the entropy in each partition, weighted by the proportion of observations
    in the partition. Luckily, scikit-learn provides us with a function that does
    all of this for us. In the following exercises and activities, we will implement
    the decision tree classifier model to predict whether it is raining or snowing,
    using the familiar `weather.csv` dataset.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果纯度是通过熵来决定的，我们必须理解熵是衡量一组值内部随机性的指标。决策树通过选择最小化熵（随机性）的切分来运作，进而最大化信息增益。信息增益是通过计算切分与所有后续切分之间的熵差值来确定的。然后，计算总熵的方法是将每个分区的熵加权求和，权重是该分区中观察值的比例。幸运的是，scikit-learn提供了一个函数来帮助我们完成这些操作。在接下来的练习和活动中，我们将实现决策树分类器模型，用以预测是否在下雨或下雪，使用熟悉的`weather.csv`数据集。
- en: 'Activity 8: Preparing Data for a Decision Tree Classifier'
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动8：为决策树分类器准备数据
- en: 'In this activity, we will prepare our data for a decision tree classifier model.
    Perform the following steps to complete the activity:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将为决策树分类器模型准备数据。执行以下步骤完成活动：
- en: Import `weather.csv` and store it as a DataFrame
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`weather.csv`并将其存储为DataFrame
- en: Dummy code the multi-level, categorical feature `Summary`
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为多层次的分类特征`Summary`编写虚拟代码
- en: Shuffle the data to remove any possible order effects
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打乱数据，以去除可能的顺序效应
- en: Split the data into features and outcome
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为特征和结果
- en: Further divide the features and outcome into testing and training data
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步将特征和结果拆分为测试数据和训练数据
- en: 'Scale `X_train` and `X_test` using the following code:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码缩放`X_train`和`X_test`：
- en: '[PRE47]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Note
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 349
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第349页找到
- en: In the following exercise, we will learn to tune and fit a decision tree classifier
    model..
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将学习如何调优和拟合决策树分类器模型。
- en: 'Exercise 30: Tuning a Decision Tree Classifier Using Grid Search'
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习30：使用网格搜索调优决策树分类器
- en: In the current exercise, we will instantiate a hyperparameter space and tune
    the hyperparameters of a decision tree classifier using a grid search.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将实例化一个超参数空间，并使用网格搜索来调优决策树分类器的超参数。
- en: 'Continuing from *Activity 8*, perform the following steps:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 继续进行*活动8*，执行以下步骤：
- en: 'Specify the hyperparameter space as follows:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式指定超参数空间：
- en: '[PRE48]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Instantiate a grid search model using the code here:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码实例化网格搜索模型：
- en: '[PRE49]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Fit to the training set using the following:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下方法拟合训练集：
- en: '[PRE50]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'See the resultant output displayed here:'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看此处显示的结果输出：
- en: '![Figure 3.25: The output from fitting our decision tree classifier grid search
    model'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.25：拟合我们的决策树分类器网格搜索模型的输出'
- en: '](img/C13322_03_25.jpg)'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_25.jpg)'
- en: 'Figure 3.25: The output from fitting our decision tree classifier grid search
    model'
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.25：拟合我们的决策树分类器网格搜索模型的输出
- en: 'Print the tuned parameters:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印调优后的参数：
- en: '[PRE51]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'See the resultant output below:'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看下面的结果输出：
- en: '![](img/C13322_03_26.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C13322_03_26.jpg)'
- en: 'Figure 3.26: The tuned hyperparameters for our decision tree classifier grid
    search model'
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.26：我们决策树分类器网格搜索模型的调优超参数
- en: We can see from Figure 3.26 that it used **gini** impurity as the criterion
    to measure the quality of a split. Further explanations of the hyperparameters
    are outside the scope of this chapter but can be found in the decision tree classifier
    scikit-learn documentation.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从图3.26中看到，它使用了**gini**不纯度作为切分质量的标准。关于超参数的进一步解释超出了本章范围，但可以在决策树分类器的scikit-learn文档中找到。
- en: Remember, in practice, it is common for decision makers to ask how various features
    are affecting the predictions. In linear and logistic regression, the intercept
    and coefficient(s) make model predictions very transparent.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在实际应用中，决策者常常会询问不同特征如何影响预测。在线性回归和逻辑回归中，截距和系数使得模型预测非常透明。
- en: Note
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Decision trees can also be very easy to interpret, as we can see where the decisions
    were made, but this requires an installation and proper configuration of Graphviz,
    as well as unscaled features.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树也非常容易解释，因为我们可以看到做出决策的地方，但这需要安装并正确配置Graphviz，并且特征需要未经过缩放。
- en: Instead of plotting the tree in the following exercise, we will explore an attribute
    found in scitkit-learn's tree-based model algorithms, '`feature_importances_`',
    which returns an array containing values of relative feature importance for each
    feature. It is important to note that this attribute is unavailable from a grid
    search model. As a result, in the next exercise, we will learn to programmatically
    extract values from the `best_parameters` dictionary and re-fit the tuned decision
    tree model, allowing us to access the attributes provided by the decision tree
    classifier function.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 与接下来的练习不同，我们将探索 scikit-learn 树形模型算法中找到的一个属性 '`feature_importances_`'，该属性返回一个包含每个特征相对重要性值的数组。需要注意的是，这个属性在网格搜索模型中不可用。因此，在下一个练习中，我们将学习如何从
    `best_parameters` 字典中程序化提取值，并重新拟合调优后的决策树模型，从而访问决策树分类器函数提供的属性。
- en: 'Exercise 31: Programmatically Extracting Tuned Hyperparameters from a Decision
    Tree Classifier Grid Search Model'
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 31：程序化地从决策树分类器网格搜索模型中提取调优后的超参数
- en: In the previous exercise, we saved the tuned hyperparameters as key value pairs
    in the `best_parameters` dictionary. This allows us to programmatically access
    the values and assign them to the appropriate hyperparameters of a decision tree
    classifier model. By fitting the tuned decision tree model, we will be able to
    access the attributes made available from the scikit-learn decision tree classifier
    function.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个练习中，我们将调优后的超参数保存为 `best_parameters` 字典中的键值对。这使我们能够以编程方式访问这些值并将它们分配给决策树分类器模型的相应超参数。通过拟合调优后的决策树模型，我们将能够访问
    scikit-learn 决策树分类器函数提供的属性。
- en: 'Continuing from *Exercise 30*, perform the following steps:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 继续进行 *练习 30*，执行以下步骤：
- en: 'Prove that we can access the value for ''`Tree_criterion`'' using:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 证明我们可以通过以下方式访问 '`Tree_criterion`' 的值：
- en: '[PRE52]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'See the resultant output here:'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里查看结果输出：
- en: '![Figure 3.27: The value assigned to the ‘Tree_criterion’ key in the best_parameters
    dictionary'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.27：在 `best_parameters` 字典中分配给 ''Tree_criterion'' 键的值'
- en: '](img/C13322_03_27.jpg)'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_27.jpg)'
- en: 'Figure 3.27: The value assigned to the ''Tree_criterion'' key in the best_parameters
    dictionary'
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.27：在 `best_parameters` 字典中分配给 'Tree_criterion' 键的值
- en: 'Instantiate decision tree classifier model and assign the values to the corresponding
    hyperparameters as follows:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化决策树分类器模型，并将相应的超参数值分配如下：
- en: '[PRE53]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Fit the grid search model to the scaled training data using the following:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将网格搜索模型拟合到标准化的训练数据：
- en: '[PRE54]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![Figure 3.28: The output from fitting the decision tree classifier model with
    tuned hyperparameters'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.28：使用调优后的超参数拟合决策树分类器模型的输出'
- en: '](img/C13322_03_28.jpg)'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_28.jpg)'
- en: 'Figure 3.28: The output from fitting the decision tree classifier model with
    tuned hyperparameters'
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.28：使用调优后的超参数拟合决策树分类器模型的输出
- en: 'Extract `feature_importances` attribute using:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码提取 `feature_importances` 属性：
- en: '[PRE55]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![Figure 3.29: An array of feature importance from our tuned decision tree
    classifier model'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.29：我们调优后的决策树分类器模型的特征重要性数组'
- en: '](img/C13322_03_29.jpg)'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_29.jpg)'
- en: 'Figure 3.29: An array of feature importance from our tuned decision tree classifier
    model'
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.29：我们调优后的决策树分类器模型的特征重要性数组
- en: From the array in Figure 3.29, we can see that the first feature completely
    dominated the other variables in terms of feature importance.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从图 3.29 的数组中，我们可以看到第一个特征在特征重要性上完全主导了其他变量。
- en: 'Visualize this using the following code:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码可视化：
- en: '[PRE56]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'See the resultant output here:'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里查看结果输出：
- en: '![Figure 3.30: Feature importance from a tuned decision tree classifier model'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.30：调优后的决策树分类器模型的特征重要性'
- en: '](img/C13322_03_30.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_30.jpg)'
- en: 'Figure 3.30: Feature importance from a tuned decision tree classifier model'
  id: totrans-375
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.30：调优后的决策树分类器模型的特征重要性
- en: It looks like temperature in Celsius was the sole driver in this classification
    problem. With the outcome measure being `rain ('Rain'=1)` or `snow ('Rain'=0)`
    and the way in which decision trees make split decisions via "*divide and conquer*,"
    it makes sense that the algorithm used temperature to determine if there was rainfall
    or snowfall at the time of measurement. In the upcoming activity, we will evaluate
    how the model performed.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来摄氏温度是这个分类问题的唯一驱动因素。由于结果衡量标准是 `rain ('Rain'=1)` 或 `snow ('Rain'=0)`，而决策树通过“*分而治之*”的方式做出划分决策，因此算法使用温度来判断测量时是否有降雨或降雪，这也有其合理性。在接下来的活动中，我们将评估模型的表现。
- en: 'Activity 9: Generating Predictions and Evaluating the Performance of a Decision
    Tree Classifier Model'
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动9：生成预测并评估决策树分类器模型的性能
- en: We have generated predictions and evaluated the model performance in previous
    exercises and activities. We will be taking the same approach in this activity
    to evaluate the performance of our tuned decision tree classifier model.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的练习和活动中，我们已经生成了预测并评估了模型性能。在本次活动中，我们将采取相同的方法来评估我们调整过的决策树分类器模型的性能。
- en: 'Continuing from Exercise 31, perform the following steps:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 从第31次练习继续，执行以下步骤：
- en: Generate the predicted probabilities of rain.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成降水的预测概率。
- en: Generate the predicted classes of rain.
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成降水的预测类别。
- en: Generate and print a confusion matrix.
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成并打印混淆矩阵。
- en: Print a classification report.
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印分类报告。
- en: Note
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 350.
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第350页找到。
- en: You should find that there was only one misclassified observation. Thus, by
    tuning a decision tree classifier model on our `weather.csv` dataset, we were
    able to predict rain (or snow) with great accuracy. We can see that the sole driving
    feature was temperature in Celsius. This makes sense due to the way in which decision
    trees use recursive partitioning to make predictions.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会发现只有一个观测值被误分类。因此，通过在我们的 `weather.csv` 数据集上调整决策树分类器模型，我们能够高精度地预测降雨（或降雪）。我们可以看到，唯一的驱动特征是摄氏温度。这是有道理的，因为决策树使用递归分割的方法来进行预测。
- en: Sometimes, after evaluation, a single model is a weak learner and does not perform
    well. However, by combining weak learners, we create a stronger learner. The approach
    of combining numerous weak learners to create a stronger learner is termed ensemble.
    Random forest models combine numerous decision tree models to create a stronger
    ensemble model. Random forests can be used for classification or regression problems.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，经过评估后，单一模型是一个弱学习器，表现不佳。然而，通过组合多个弱学习器，我们可以创建一个更强的学习器。将多个弱学习器组合成一个强学习器的方式称为集成。随机森林模型将多个决策树模型组合在一起，形成一个更强的集成模型。随机森林可以用于分类或回归问题。
- en: Random Forests
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: As briefly mentioned earlier, random forests are ensembles of decision trees
    that can be used to solve classification or regression problems. Random forests
    use a small portion of the data to fit each tree, so they can handle very large
    datasets, and they are less prone to the "*curse of dimensionality*" relative
    to other algorithms. The curse of dimensionality is a situation in which an abundance
    of features in the data diminishes the performance of the model. Predictions of
    the random forest are then determined by combining the predictions of each tree.
    Like SVM, random forests are a **black box** with inputs and outputs which cannot
    be interpreted.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，随机森林是决策树的集成，可以用来解决分类或回归问题。随机森林使用数据的一小部分来拟合每棵树，因此能够处理非常大的数据集，而且相较于其他算法，它们对“*维度灾难*”的敏感性较低。维度灾难是指数据中有大量特征时，模型的性能会下降。随机森林的预测是通过将每棵树的预测结果结合起来得到的。与支持向量机（SVM）一样，随机森林是一个**黑箱**，其输入和输出无法解释。
- en: In the upcoming exercises and activities, we will tune and fit a random forest
    regressor using grid search to predict the temperature in Celsius. Then, we will
    evaluate the performance of the model.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习和活动中，我们将使用网格搜索调整并拟合随机森林回归器，以预测摄氏温度。然后，我们将评估模型的性能。
- en: 'Exercise 32: Preparing Data for a Random Forest Regressor'
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习32：为随机森林回归器准备数据
- en: 'First, we will prepare the data for the random forest regressor with ''`Temperature_c`''
    as the dependent variable, just as we did in Exercise 21:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将为随机森林回归器准备数据，使用 '`Temperature_c`' 作为因变量，就像我们在第21次练习中做的那样：
- en: 'Import ''`weather.csv`'' and save it as `df` using the following code:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 '`weather.csv`' 并使用以下代码将其保存为 `df`：
- en: '[PRE57]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Dummy code the multi-class, categorical variable, Description, as follows:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对多类分类变量 `Description` 进行虚拟编码，方法如下：
- en: '[PRE58]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Remove any possible ordering effects by shuffling `df_dummies` using the following
    code:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码通过打乱 `df_dummies` 来去除可能的排序效应：
- en: '[PRE59]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Split `df_shuffled` into `X` and `y` using the following code:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将 `df_shuffled` 划分为 `X` 和 `y`：
- en: '[PRE60]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Split `X` and `y` into testing and training data as follows:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `X` 和 `y` 按照如下方式划分为测试数据和训练数据：
- en: '[PRE61]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Scale `X_train` and `X_test` using the code here:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码对 `X_train` 和 `X_test` 进行缩放：
- en: '[PRE62]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now that we have imported, shuffled, separated our data into features (`X`)
    and dependent variable (`y`), split `X` and `y` into testing and training data,
    and scaled `X_train` and `X_test`, we will tune a random forest regressor model
    using grid search.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经导入、打乱、将数据分为特征（`X`）和因变量（`y`），将`X`和`y`拆分为测试数据和训练数据，并对`X_train`和`X_test`进行缩放，我们将使用网格搜索调优随机森林回归模型。
- en: 'Activity 10: Tuning a Random Forest Regressor'
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 10：调优随机森林回归模型
- en: The data has been prepared for inclusion in a random forest regressor. Now,
    we must set up the hyperparameter space and find the optimal combination of hyperparameters
    using a grid search.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 数据已准备好用于随机森林回归模型。现在，我们必须设置超参数空间，并使用网格搜索找到超参数的最佳组合。
- en: 'Continuing from Exercise 32, perform the following steps:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 从练习 32 继续，执行以下步骤：
- en: Specify the hyperparameter space.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定超参数空间。
- en: Instantiate the `GridSearchCV` model optimizing the explained variance.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`GridSearchCV`模型以优化解释方差。
- en: Fit the grid search model to the training set.
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将网格搜索模型拟合到训练集。
- en: Print the tuned parameters.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印调优后的参数。
- en: Note
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 351.
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第 351 页找到。
- en: After performing a grid search of our random forest regressor hyperparameters,
    we need to fit a random forest regressor model with the tuned hyperparameters.
    We will programmatically extract the values in the `best_parameters` dictionary
    and assign them to the corresponding hyperparameters in the random forest regressor
    function, so we can access the attributes from the random forest regressor function.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在对我们的随机森林回归超参数进行网格搜索后，我们需要使用调优后的超参数拟合一个随机森林回归模型。我们将编程提取`best_parameters`字典中的值，并将它们分配给随机森林回归函数中的相应超参数，以便我们可以访问来自该函数的属性。
- en: 'Exercise 33: Programmatically Extracting Tuned Hyperparameters and Determining
    Feature Importance from a Random Forest Regressor Grid Search Model'
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 33：从随机森林回归网格搜索模型中编程提取调优的超参数并确定特征重要性
- en: By extracting the value from the key-value pairs in the `best_parameters` dictionary,
    we eliminate the possibility of manual errors, as well as make our code more automated.
    In this exercise, we will replicate the steps from *Exercise 31*, but will adapt
    our code for the random forest regressor model.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从`best_parameters`字典中的键值对提取值，我们消除了手动错误的可能性，并且使我们的代码更加自动化。在本练习中，我们将复制*练习 31*中的步骤，但将代码调整为适应随机森林回归模型。
- en: 'Continuing from *Activity 10*, perform the following steps:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 从*活动 10*继续，执行以下步骤：
- en: 'Instantiate a random forest regressor model with the values for each key from
    the `best_parameters` dictionary assigned to the corresponding hyperparameter:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个随机森林回归模型，将`best_parameters`字典中每个键的值分配给相应的超参数：
- en: '[PRE63]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Fit the model on the training data using the following:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码在训练数据上拟合模型：
- en: '[PRE64]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Find the resultant output here:'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在此处查找结果输出：
- en: '![Figure 3.31: The output from fitting the random forest regressor model with
    tuned hyperparameters'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.31：使用调优超参数拟合随机森林回归模型的输出'
- en: '](img/C13322_03_31.jpg)'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_31.jpg)'
- en: 'Figure 3.31: The output from fitting the random forest regressor model with
    tuned hyperparameters'
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.31：使用调优超参数拟合随机森林回归模型的输出
- en: 'Plot feature importance in descending order using the following code:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码按降序绘制特征重要性：
- en: '[PRE65]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'See the resultant output here:'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在此处查看结果输出：
- en: '![Figure 3.32: Feature importance from a random forest regressor model with
    tuned hyperparameters'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.32：来自调优超参数的随机森林回归模型的特征重要性'
- en: '](img/C13322_03_32.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_32.jpg)'
- en: 'Figure 3.32: Feature importance from a random forest regressor model with tuned
    hyperparameters'
  id: totrans-432
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.32：来自调优超参数的随机森林回归模型的特征重要性
- en: From Figure 3.32, we can see that the '`Description_Warm`' dummy variable and
    '`Humidity`' are the main drivers of temperature in Celsius. Meanwhile, '`Visibility_km`'
    and '`Wind_Bearing_degrees`' have a small effect on the temperature. Let's now
    check to see how our model performs on the test data.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 3.32中，我们可以看到'`Description_Warm`'虚拟变量和'`Humidity`'是摄氏温度的主要驱动因素。与此同时，'`Visibility_km`'和'`Wind_Bearing_degrees`'对温度的影响较小。接下来，我们将检查我们的模型在测试数据上的表现。
- en: 'Activity 11: Generating Predictions and Evaluating the Performance of a Tuned
    Random Forest Regressor Model'
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 11：生成预测并评估调优后的随机森林回归模型的性能
- en: In *Exercise 23* and *Activity 5*, we learned to generate predictions and evaluate
    the performance of regression models that predict a continuous outcome. In this
    activity, we will be taking the same approach to evaluate the performance of our
    random forest regressor model to predict temperature in Celsius.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习23*和*活动5*中，我们学习了如何生成预测并评估回归模型在预测连续结果时的性能。在本次活动中，我们将采用相同的方法来评估我们随机森林回归模型在预测摄氏温度时的性能。
- en: 'Continuing from *Exercise 33*, perform the following steps:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 继续执行*练习33*中的步骤，进行以下操作：
- en: Generate predictions on the test data.
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上生成预测。
- en: Plot the correlation of predicted and actual values.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制预测值与实际值的相关性图。
- en: Plot the distribution of residuals.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制残差的分布图。
- en: Compute metrics, then place them in a DataFrame and print it.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算指标，然后将其放入数据框并打印出来。
- en: Note
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 352.
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以在第352页找到。
- en: The random forest regressor model seems to underperform compared to the multiple
    linear regression, as evidenced by greater MAE, MSE, and RMSE values, as well
    as less explained variance. Additionally, there was a weaker correlation between
    the predicted and actual values, and the residuals were further from being normally
    distributed. Nevertheless, by leveraging ensemble methods using a random forest
    regressor, we constructed a model that explains 75.8% of the variance in -temperature
    and predicts temperature in Celsius ± 3.781 degrees.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林回归模型似乎表现不如多元线性回归，这一点通过更大的MAE、MSE和RMSE值以及较低的解释方差得到了证实。此外，预测值与实际值之间的相关性较弱，残差也远未呈正态分布。然而，通过利用随机森林回归器的集成方法，我们构建了一个模型，该模型解释了75.8%的温度方差，并预测摄氏温度±3.781度。
- en: Summary
  id: totrans-444
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we were introduced to the open source machine learning library
    for Python, scikit-learn. You learned to preprocess data, as well as how to tune
    and fit a few different regression and classification algorithms. Lastly, you
    learned how to quickly and effectively evaluate the performance of classification
    and regression models. This was a very comprehensive introduction to the scikit-learn
    library, and the strategies employed here can be applied to building numerous
    additional algorithms provided by scikit-learn.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了Python的开源机器学习库scikit-learn。你学习了如何预处理数据，以及如何调整和拟合几种不同的回归和分类算法。最后，你学习了如何快速有效地评估分类和回归模型的性能。这是对scikit-learn库的全面介绍，在这里使用的策略可以应用于构建scikit-learn提供的众多其他算法。
- en: In the next chapter, you will learn about dimensionality reduction and unsupervised
    learning.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将学习降维和无监督学习。
