- en: Trident Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Trident介绍
- en: In the previous chapters, we covered the architecture of Storm, its topology,
    bolts, spouts, tuples, and so on. In this chapter, we are covering Trident, which
    is a high-level abstraction over Storm.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们介绍了Storm的架构、拓扑、bolt、spout、元组等。在本章中，我们介绍了Trident，它是Storm的高级抽象。
- en: 'We are covering the following points in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下内容：
- en: Introducing Trident
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trident介绍
- en: Understanding Trident's data model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Trident的数据模型
- en: Writing Trident functions, filters, and projections
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写Trident函数、过滤器和投影
- en: Trident repartitioning operations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trident重新分区操作
- en: Trident aggregators
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trident聚合器
- en: When to use Trident
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时使用Trident
- en: Trident introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Trident介绍
- en: Trident is a high-level abstraction built on top of Storm. Trident supports
    stateful stream processing, while pure Storm is a stateless processing framework.
    The main advantage of using Trident is that it guarantees that every message entered
    into the topology is processed only once, which would be difficult to achieve
    with vanilla Storm. The concept of Trident is similar to high-level batch processing
    tools, such as Cascading and Pig, developed over Hadoop. To achieve exactly-once
    processing, Trident processes the input stream in small batches. We will cover
    this in more detail in the [Chapter 5](part0102.html#318PC0-6149dd15e07b443593cc93f2eb31ee7b),
    *Trident Topology and Uses*, *Trident state* section.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Trident是建立在Storm之上的高级抽象。Trident支持有状态的流处理，而纯Storm是一个无状态的处理框架。使用Trident的主要优势在于它保证每个进入拓扑的消息只被处理一次，这在纯Storm中很难实现。Trident的概念类似于高级批处理工具，如Cascading和Pig，它们是在Hadoop上开发的。为了实现精确一次处理，Trident会将输入流分批处理。我们将在[第5章](part0102.html#318PC0-6149dd15e07b443593cc93f2eb31ee7b)的*Trident拓扑和用途*、*Trident状态*部分详细介绍。
- en: In the first three chapters, we learned that, in Storm's topology, the spout
    is the source of tuples. A tuple is a unit of data that can be processed by a
    Storm application, and a bolt is the processing powerhouse where we write the
    transformation logic. But in the Trident topology, the bolt is replaced with the
    higher level semantics of functions, aggregates, filters, and states.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在前三章中，我们了解到，在Storm的拓扑中，spout是元组的来源。元组是Storm应用程序可以处理的数据单元，而bolt是我们编写转换逻辑的处理引擎。但在Trident拓扑中，bolt被更高级的函数、聚合、过滤器和状态的语义所取代。
- en: Understanding Trident's data model
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Trident的数据模型
- en: The Trident tuple is the data model of a Trident topology. The Trident tuple
    is the basic unit of data that can be processed by a Trident topology. Each tuple
    consists of a predefined list of fields. The value of each field can be a byte,
    char, integer, long, float, double, Boolean, or byte array. During the construction
    of a topology, operations are performed on a tuple, which will either add new
    fields to the tuple or replace the tuple with a new set of fields.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Trident元组是Trident拓扑的数据模型。Trident元组是可以被Trident拓扑处理的数据的基本单元。每个元组由预定义的字段列表组成。每个字段的值可以是字节、字符、整数、长整型、浮点数、双精度浮点数、布尔值或字节数组。在构建拓扑时，对元组执行操作，这些操作要么向元组添加新字段，要么用一组新字段替换元组。
- en: Each of the fields in a tuple can be accessed by name, `(getValueByField(String))`,
    or its positional index, `(getValue(int))`, in the tuple. The Trident tuple also
    provides convenience methods, such as `getIntegerByField(String)`, which saves
    you from typecasting the objects.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 元组中的每个字段都可以通过名称`(getValueByField(String))`或其位置索引`(getValue(int))`来访问。Trident元组还提供了方便的方法，如`getIntegerByField(String)`，可以避免您对对象进行类型转换。
- en: Writing Trident functions, filters, and projections
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写Trident函数、过滤器和投影
- en: This section covers the definition of Trident functions, filters, and projections.
    Trident functions, filters, and projections are used to modify/filter the input
    tuples based on certain criteria. This section also covers how we can write Trident
    functions, filters, and projections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了Trident函数、过滤器和投影的定义。Trident函数、过滤器和投影用于根据特定条件修改/过滤输入元组。本节还介绍了如何编写Trident函数、过滤器和投影。
- en: Trident function
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Trident函数
- en: Trident functions contain logic to modify the original tuple. A Trident function
    gets a set of fields of the tuple as input and emits one or more tuples as output.
    The fields of the output tuples are merged with the fields of the input tuple
    to form the complete tuple, which will pass to the next action in the topology.
    If the Trident function emits no tuples corresponding to the input tuple, then
    that tuple is removed from the stream.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Trident函数包含修改原始元组的逻辑。Trident函数接收元组的一组字段作为输入，并输出一个或多个元组。输出元组的字段与输入元组的字段合并，形成完整的元组，然后传递给拓扑中的下一个操作。如果Trident函数没有输出与输入元组对应的元组，则该元组将从流中移除。
- en: We can write a custom Trident function by extending the `storm.trident.operation.BaseFunction`
    class and implementing the `execute(TridentTuple tuple, TridentCollector collector)`
    method.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过扩展`storm.trident.operation.BaseFunction`类并实现`execute(TridentTuple tuple,
    TridentCollector collector)`方法来编写自定义的Trident函数。
- en: 'Let''s write the sample Trident function, which will return the new field called
    `sum`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个示例的Trident函数，它将返回一个名为`sum`的新字段：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Suppose we get `dummyStream` as input, which contains four fields, `a`, `b`,
    `c`, `d`, and only fields `a` and `b` are passed as input fields to the `SumFunction`
    function. The `SumFunction` class emits new a field, `sum`. The `sum` field emitted
    by the `execute` method of the `SumFunction` class is merged with the input tuple
    to form the complete tuple. Hence, the total number of fields in the output tuple
    is `5 (a, b, c, d, sum)`. Here is a sample piece of code that shows how we can
    pass the input fields and the name of the new field to the Trident function:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将`dummyStream`作为输入，其中包含四个字段`a`、`b`、`c`、`d`，并且只有字段`a`和`b`作为输入字段传递给`SumFunction`函数。`SumFunction`类会发出一个新字段`sum`。`SumFunction`类的`execute`方法发出的`sum`字段与输入元组合并，形成完整的元组。因此，输出元组中的字段总数为`5
    (a, b, c, d, sum)`。以下是一个示例代码片段，展示了如何将输入字段和新字段的名称传递给Trident函数：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following diagram shows the input tuples, `SumFunction`, and the output
    tuples. The output tuples contain five fields, `a`, `b`, `c`, `d`, and `sum`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了输入元组，`SumFunction`和输出元组。输出元组包含五个字段，`a`，`b`，`c`，`d`和`sum`：
- en: '![](img/00026.gif)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00026.gif)'
- en: Trident filter
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Trident过滤器
- en: A Trident filter gets a set of fields as input and returns either true or false
    depending on whether a certain condition is satisfied or not. If true is returned,
    then the tuple is kept in the output stream; otherwise, the tuple is removed from
    the stream.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Trident过滤器以一组字段作为输入，并根据某种条件是否满足返回true或false。如果返回true，则元组保留在输出流中；否则，元组从流中移除。
- en: We can write a custom Trident filter by extending the `storm.trident.operation.BaseFilter`
    class and implementing the `isKeep(TridentTuple tuple)` method.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过扩展`storm.trident.operation.BaseFilter`类并实现`isKeep(TridentTuple tuple)`方法来编写自定义的Trident过滤器。
- en: 'Let''s write a sample Trident filter that will check whether the sum of the
    input fields is even or odd. If the sum is even, then the Trident filter emits
    true; otherwise it emits false:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个示例Trident过滤器，检查输入字段的和是偶数还是奇数。如果和是偶数，则Trident过滤器发出true；否则发出false：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Suppose we get `dummyStream` as input, which contains four fields, `a`, `b`,
    `c`, `d`, and only fields `a` and `b` are passed as input fields to the `CheckEvenSumFilter`
    filter. The `execute` method of the `CheckEvenSumFilter` class will emit only
    those tuples whose sum of `a` and `b` is even. Here is a sample piece of code
    that shows how we can define the input fields for a Trident filter:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们得到了名为`dummyStream`的输入，其中包含四个字段，`a`，`b`，`c`，`d`，并且只有字段`a`和`b`作为输入字段传递给`CheckEvenSumFilter`过滤器。`CheckEvenSumFilter`类的`execute`方法将仅发出那些`a`和`b`的和为偶数的元组。以下是一段示例代码，展示了如何为Trident过滤器定义输入字段：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following diagram shows the input tuples, `CheckEvenSumFilter`, and output
    tuples. `outputStream` contains only those tuples whose sum of fields `a` and
    `b` is even:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了输入元组，`CheckEvenSumFilter`和输出元组。`outputStream`仅包含那些字段`a`和`b`的和为偶数的元组：
- en: '![](img/00027.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00027.jpeg)'
- en: Trident projection
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Trident投影
- en: 'A Trident projection keeps only those fields in the stream that are specified
    in the projection operation. Suppose an input stream contains three fields, `x`,
    `y`, and `z`, and we are passing field `x` to the projection operation, then the
    output tuples will contain a single field, `x`. Here is the piece of code that
    shows how we can use the projection operation:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Trident投影仅保留流中在投影操作中指定的字段。假设输入流包含三个字段，`x`，`y`和`z`，并且我们将字段`x`传递给投影操作，那么输出元组将包含一个字段`x`。以下是一段代码，展示了如何使用投影操作：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following diagram shows the Trident projection:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了Trident投影：
- en: '![](img/00028.gif)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00028.gif)'
- en: Trident repartitioning operations
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Trident重新分区操作
- en: By performing repartitioning operations, a user can partition tuples across
    multiple tasks. The repartitioning operation doesn't make any changes to the content
    of the tuples. Also, the tuples will only pass over the network for the repartitioning
    operation. Here are the different types of repartitioning operation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行重新分区操作，用户可以将元组分布在多个任务中。重新分区操作不会对元组的内容进行任何更改。此外，元组只会通过网络进行重新分区操作。以下是不同类型的重新分区操作。
- en: Utilizing shuffle operation
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用shuffle操作
- en: 'This repartitioning operation partitions the tuples in a uniform, random way
    across multiple tasks. This repartitioning operation is generally used when we
    want to distribute the processing load uniformly across the tasks. The following
    diagram shows how the input tuples are repartitioned using the `shuffle` operation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重新分区操作以一种均匀随机的方式将元组分布在多个任务中。当我们希望在任务之间均匀分配处理负载时，通常会使用这种重新分区操作。以下图显示了如何使用`shuffle`操作重新分区输入元组：
- en: '![](img/00029.gif)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00029.gif)'
- en: 'Here is a piece of code that shows how we can use the `shuffle` operation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一段代码，展示了如何使用`shuffle`操作：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Utilizing partitionBy operation
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用partitionBy操作
- en: 'This repartitioning operation enables you to partition the stream on the basis
    of the fields in the tuples. For example, if you want all the tweets from a particular
    user to go to the same target partition, then you can partition the tweet stream
    by applying `partitionBy` to the `username` field in the following manner:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重新分区操作使您能够根据元组中的字段对流进行分区。例如，如果您希望来自特定用户的所有推文都发送到同一个目标分区，则可以通过以下方式对推文流进行分区，即应用`partitionBy`到`username`字段：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `partitionBy` operation applies the following formula to decide the target
    partition:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`partitionBy`操作应用以下公式来决定目标分区：'
- en: '*Target Partition = hash(fields) % (number of target partition)*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标分区 = 哈希(字段) % (目标分区数)*'
- en: As the preceding formula shows, the `partitionBy` operation calculates the hash
    of the input fields to decide the target partition. Hence, it does not guarantee
    that all the tasks will get tuples to process. For example, if you have applied
    a `partitionBy` to a field, say `X`, with only two possible values, `A` and `B`,
    and created two tasks for the `MyFilter` filter, then it might be possible that
    hash (`A`) % 2 and hash (`B`) % 2 are equal, which will result in all the tuples
    being routed to a single task and the other tuples being completely idle.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的公式所示，`partitionBy`操作计算输入字段的哈希以决定目标分区。因此，它不能保证所有任务都会得到元组进行处理。例如，如果您对一个字段应用了`partitionBy`，比如`X`，只有两个可能的值，`A`和`B`，并为`MyFilter`过滤器创建了两个任务，那么可能会出现哈希(`A`)
    % 2和哈希(`B`) % 2相等的情况，这将导致所有元组都被路由到一个任务，而其他元组完全空闲。
- en: 'The following diagram shows how the input tuples are repartitioned using the
    `partitionBy` operation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了如何使用`partitionBy`操作重新分区输入元组：
- en: '![](img/00030.gif)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00030.gif)'
- en: As the preceding diagram shows, **Partition 0** and **Partition 2** contain
    a set of tuples, but **Partition 1** is empty.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，**Partition 0**和**Partition 2**包含一组元组，但**Partition 1**为空。
- en: Utilizing global operation
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用全局操作
- en: 'This repartitioning operation routes all the tuples to the same partition.
    Hence, the same target partition is selected for all the batches in the stream.
    Here is a diagram that shows how the tuples are repartitioned using the `global`
    operation:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重新分配操作将所有元组路由到同一分区。因此，流中所有批次都选择相同的目标分区。以下是一个显示如何使用`global`操作重新分配元组的图表：
- en: '![](img/00031.gif)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00031.gif)'
- en: 'Here is a piece of code that shows how we can use the `global` operation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一段代码，显示了如何使用`global`操作：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Utilizing broadcast operation
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用broadcast操作
- en: 'The `broadcast` operation is a special repartitioning operation that does not
    partition the tuples, but replicates them to all partitions. Here is a diagram
    that shows how the tuples are sent over the network:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`broadcast`操作是一种特殊的重新分配操作，不会对元组进行分区，而是将它们复制到所有分区。以下是一个显示元组如何通过网络发送的图表：'
- en: '![](img/00032.gif)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00032.gif)'
- en: 'Here is a piece of code that shows how we can use the `broadcast` operation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一段代码，显示了如何使用`broadcast`操作：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Utilizing batchGlobal operation
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用batchGlobal操作
- en: 'This repartitioning operation sends all the tuples belonging to one batch into
    the same partition. The other batches of the same stream may go to a different
    partition. As the name suggests, this repartition is global at the batch level.
    Here is a diagram that shows how the tuples are repartitioned using the `batchGlobal`
    operation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重新分配操作将属于同一批次的所有元组发送到同一分区。同一流的其他批次可能会进入不同的分区。正如其名称所示，此重新分配在批次级别是全局的。以下是一个显示如何使用`batchGlobal`操作重新分配元组的图表：
- en: '![](img/00033.gif)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00033.gif)'
- en: 'Here is a piece of code that shows how we can use the `batchGlobal` operation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一段代码，显示了如何使用`batchGlobal`操作：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Utilizing partition operation
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用分区操作
- en: If none of the preceding repartitioning fits your use case, you can define your
    own custom repartition function by implementing the `org.apche.storm.grouping.CustomStreamGrouping`
    interface.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的重新分配都不适合您的用例，您可以通过实现`org.apche.storm.grouping.CustomStreamGrouping`接口来定义自己的自定义重新分配函数。
- en: 'Here is a sample custom repartition that partitions the stream on the basis
    of the value of the `country` field:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例自定义重新分配，根据`country`字段的值对流进行分区：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `CountryRepartition` class implements the `org.apache.storm.grouping.CustomStreamGrouping`
    interface. The `chooseTasks()` method contains the repartitioning logic to identify
    the next task in the topology for the input tuple. The `prepare()` method is called
    at the start and performs the initialization activity.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountryRepartition`类实现了`org.apache.storm.grouping.CustomStreamGrouping`接口。`chooseTasks()`方法包含重新分配逻辑，用于确定拓扑中输入元组的下一个任务。`prepare()`方法在开始时被调用，并执行初始化活动。'
- en: Trident aggregator
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Trident聚合器
- en: 'The Trident aggregator is used to perform the aggregation operation on the
    input batch, partition, or input stream. For example, if a user wants to count
    the number of tuples present in each batch, then we can use the count aggregator
    to count the number of tuples in each batch. The output of the aggregator completely
    replaces the value of the input tuple. There are three types of aggregator available
    in Trident:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Trident聚合器用于对输入批次、分区或输入流执行聚合操作。例如，如果用户想要计算每个批次中元组的数量，则可以使用计数聚合器来计算每个批次中元组的数量。聚合器的输出完全替换输入元组的值。Trident中有三种可用的聚合器：
- en: '`partitionAggregate`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`partitionAggregate`'
- en: '`aggregate`'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregate`'
- en: '`persistenceAggregate`'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`persistenceAggregate`'
- en: Let's understand each type of aggregator in detail.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解每种类型的聚合器。
- en: partitionAggregate
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: partitionAggregate
- en: 'As the name suggests, the `partitionAggregate` works on each partition instead
    of the whole batch. The output of `partitionAggregate` completely replaces the
    input tuple. Also, the output of `partitionAggregate` contains a single-field
    tuple. Here is a piece of code that shows how we can use `partitionAggregate`
    :'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，`partitionAggregate`在每个分区上工作，而不是整个批次。`partitionAggregate`的输出完全替换输入元组。此外，`partitionAggregate`的输出包含一个单字段元组。以下是一段代码，显示了如何使用`partitionAggregate`：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For example, we get an input stream containing the fields `x` and `y` and we
    apply a `partitionAggregate` function to each partition; the output tuples contain
    a single field called `count`. The `count` field represent the number of tuples
    presents in the input partition:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们得到一个包含字段`x`和`y`的输入流，并对每个分区应用`partitionAggregate`函数；输出元组包含一个名为`count`的字段。`count`字段表示输入分区中元组的数量：
- en: '![](img/00034.gif)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00034.gif)'
- en: aggregate
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: aggregate
- en: 'The `aggregate` works on each batch. During the aggregate process, the tuples
    are first repartitioned using the global operation to combine all the partitions
    of the same batch into a single partition and then the aggregation function is
    run on each batch. Here is a piece of code that shows how we can use `aggregate`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregate`在每个批次上工作。在聚合过程中，首先使用全局操作对元组进行重新分配，将同一批次的所有分区合并为单个分区，然后对每个批次运行聚合函数。以下是一段代码，显示了如何使用`aggregate`：'
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There are three types of aggregator interface available in Trident:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Trident中有三种可用的聚合器接口：
- en: '`ReducerAggregator`'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReducerAggregator`'
- en: '`Aggregator`'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Aggregator`'
- en: '`CombinerAggregator`'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CombinerAggregator`'
- en: These three aggregator interfaces can also be used with `partitionAggregate`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种聚合器接口也可以与`partitionAggregate`一起使用。
- en: ReducerAggregator
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReducerAggregator
- en: 'The `ReducerAggregator` first runs the global repartitioning operation on the
    input stream to combine all the partitions of the same batch into a single partition,
    and then runs the aggregation function on each batch. The `ReducerAggregator<T>`
    interface contains the following methods:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReducerAggregator`首先对输入流运行全局重新分配操作，将同一批次的所有分区合并为单个分区，然后对每个批次运行聚合函数。`ReducerAggregator<T>`接口包含以下方法：'
- en: '`init()`: This method returns the initial value'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init()`: 此方法返回初始值'
- en: '`Reduce(T curr, TridentTuple tuple)`: This method iterates over the input tuples
    and emits a single tuple with a single value'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Reduce(T curr, TridentTuple tuple)`: 此方法遍历输入元组，并发出一个具有单个值的单个元组'
- en: 'This example shows how we can implement `Sum` using the `ReducerAggregator`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例显示了如何使用`ReducerAggregator`实现`Sum`：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Aggregator
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合器
- en: 'The `Aggregator` first runs the global repartitioning operation on the input
    stream to combine all the partitions of the same batch into a single partition,
    and then runs the aggregation function on each batch. By definition, the `Aggregator`
    looks very similar to the `ReduceAggregator`. The `BaseAggregator<State>` contains
    the following methods:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`Aggregator`首先在输入流上运行全局重分区操作，将同一批次的所有分区组合成单个分区，然后在每个批次上运行聚合函数。根据定义，`Aggregator`与`ReduceAggregator`非常相似。`BaseAggregator<State>`包含以下方法：'
- en: '`init(Object batchId, TridentCollector collector)`: The `init()` method is
    called before starting the processing of a batch. This method returns the `State`
    object, which will be used to save the state of the batch. This object is used
    by the `aggregate()` and `complete()` methods.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init(Object batchId, TridentCollector collector)`: 在开始处理批次之前调用`init()`方法。此方法返回将用于保存批次状态的`State`对象。此对象由`aggregate()`和`complete()`方法使用。'
- en: '`aggregate (State s, TridentTuple tuple, TridentCollector collector)`: This
    method iterates over each tuple of a given batch. This method updates the state
    in the `State` object after processing each tuple.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregate (State s, TridentTuple tuple, TridentCollector collector)`: 此方法迭代给定批次的每个元组。此方法在处理每个元组后更新`State`对象中的状态。'
- en: '`complete(State state, TridentCollector tridentCollector)`: This method is
    called at the end, if all the tuples of a given batch are processed. This method
    returns a single tuple corresponding to each batch.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`complete(State state, TridentCollector tridentCollector)`: 如果给定批次的所有元组都已处理完毕，则调用此方法。此方法返回与每个批次对应的单个元组。'
- en: 'Here is an example that shows how we can implement a sum using the `BaseAggregator`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，展示了如何使用`BaseAggregator`实现求和：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: CombinerAggregator
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CombinerAggregator
- en: The `CombinerAggregator` first runs the `partitionAggregate` on each partition,
    then runs the global repartitioning operation to combine all the partitions of
    the same batch into a single partition, and then reruns the `aggregator` on the
    final partition to emit the desired output. The network transfer here is less
    compared to the other two aggregators. Hence, the overall performance of the `CombinerAggregator`
    is better than the `Aggregator` and `ReduceAggregator`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`CombinerAggregator`首先在每个分区上运行`partitionAggregate`，然后运行全局重分区操作，将同一批次的所有分区组合成单个分区，然后在最终分区上重新运行`aggregator`以发出所需的输出。与其他两个聚合器相比，这里的网络传输较少。因此，`CombinerAggregator`的整体性能优于`Aggregator`和`ReduceAggregator`。'
- en: 'The `CombinerAggregator<T>` interface contains the following methods:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`CombinerAggregator<T>`接口包含以下方法：'
- en: '`init()`: This method runs on each input tuple to retrieve the fields'' value
    from the tuple.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init()`: 此方法在每个输入元组上运行，以从元组中检索字段的值。'
- en: '`combine(T val1, T val2)`: This method combines the values of the tuples. This
    method emits a single tuple with a single field as the output.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`combine(T val1, T val2)`: 此方法组合元组的值。此方法发出具有单个字段的单个元组作为输出。'
- en: '`zero()`: This method returns zero if the input partition contains no tuple.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zero()`: 如果输入分区不包含元组，则此方法返回零。'
- en: 'This example shows how we can implement `Sum` using `CombinerAggregator`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例显示了如何使用`CombinerAggregator`实现`Sum`：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: persistentAggregate
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: persistentAggregate
- en: 'The `persistentAggregate` works on all the tuples across all the batches in
    a stream and persists the aggregate result into the source of state (memory, Memcached,
    Cassandra, or some other database). Here is some code that shows how we can use
    the `persistentAggregate`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`persistentAggregate`适用于流中所有批次的所有元组，并将聚合结果持久化到状态源（内存、Memcached、Cassandra或其他数据库）中。以下是一些代码，展示了如何使用`persistentAggregate`：'
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We will discuss in more detail in the [Chapter 5](part0102.html#318PC0-6149dd15e07b443593cc93f2eb31ee7b),
    *Trident Topology and Uses*, *Trident state* section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第5章](part0102.html#318PC0-6149dd15e07b443593cc93f2eb31ee7b) *Trident Topology
    and Uses*，*Trident state*部分进行更详细的讨论。
- en: Aggregator chaining
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合器链接
- en: 'Trident provides a feature to apply multiple aggregators to the same input
    stream, and this process is called **aggregator chaining**. Here is a piece of
    code that shows how we can use aggregator chaining:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Trident提供了一种功能，可以将多个聚合器应用于同一输入流，这个过程称为**聚合器链接**。以下是一段代码，展示了如何使用聚合器链接：
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We have applied the `Average()` and `Sum()` aggregators to each partition. The
    output of `chainedAgg()` contains a single tuple corresponding to each input partition.
    The output tuple contains two fields, `sum` and `average`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将`Average()`和`Sum()`聚合器应用于每个分区。`chainedAgg()`的输出包含与每个输入分区对应的单个元组。输出元组包含两个字段，`sum`和`average`。
- en: 'The following diagram shows how aggregator chaining works:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了聚合器链接的工作原理：
- en: '![](img/00035.gif)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00035.gif)'
- en: Utilizing the groupBy operation
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用groupBy操作
- en: 'The `groupBy` operation doesn''t involve any repartitioning. The `groupBy`
    operation converts the input stream into a grouped stream. The main function of
    the `groupBy` operation is to modify the behavior of the subsequent aggregate
    function. The following diagram shows how the `groupBy` operation groups the tuples
    of a single partition:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupBy`操作不涉及任何重分区。`groupBy`操作将输入流转换为分组流。`groupBy`操作的主要功能是修改后续聚合函数的行为。以下图表显示了`groupBy`操作如何对单个分区的元组进行分组：'
- en: '![](img/00036.gif)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00036.gif)'
- en: 'The behavior of `groupBy` is dependent on a position where it is used. The
    following behavior is possible:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupBy`的行为取决于其使用的位置。可能有以下行为：'
- en: If the `groupBy` operation is used before a `partitionAggregate`, then the `partitionAggregate`
    will run the `aggregate` on each group created within the partition.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在`partitionAggregate`之前使用`groupBy`操作，则`partitionAggregate`将在分区内创建的每个组上运行`aggregate`。
- en: If the `groupBy` operation is used before an `aggregate`, the tuples of the
    same batch are first repartitioned into a single partition, then `groupBy` is
    applied to each single partition, and at the end it will perform the `aggregate`
    operation on each group.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在聚合之前使用`groupBy`操作，同一批次的元组首先被重新分区到一个单一分区，然后`groupBy`被应用于每个单一分区，最后对每个组执行`aggregate`操作。
- en: When to use Trident
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时使用Trident
- en: It is very easy to achieve exactly-once processing using the Trident topology,
    and Trident was designed for this purpose. It would be difficult to achieve exactly-once
    processing with vanilla Storm, so Trident will be useful when we need exactly-once
    processing.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Trident拓扑非常容易实现一次性处理，并且Trident就是为此目的而设计的。使用原始的Storm很难实现一次性处理，因此当我们需要一次性处理时，Trident会很有用。
- en: Trident is not fit for all use cases, especially for high-performance use cases,
    because Trident adds complexity to Storm and manages the state.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Trident并不适用于所有用例，特别是对于高性能的用例，因为Trident会给Storm增加复杂性并管理状态。
- en: Summary
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we mainly concentrated on Trident high-level abstraction over
    Storm and learned about the Trident filter, function, aggregator, and repartitioning
    operations.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们主要集中讨论了Trident作为Storm的高级抽象，并学习了Trident的过滤器、函数、聚合器和重新分区操作。
- en: In the next chapter, we will cover non-transactional topology, Trident topology,
    and Trident topology using a distributed RPC.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将涵盖非事务拓扑、Trident拓扑和使用分布式RPC的Trident拓扑。
