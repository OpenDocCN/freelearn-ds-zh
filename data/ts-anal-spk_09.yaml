- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Going to Production
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进入生产环境
- en: With our time series analysis model built and tested, and the ability to scale
    seen in the previous chapter, we will now explore the practical considerations
    and steps involved in deploying time series models into production with the Spark
    framework. This information is crucial to guide you through the transition from
    development to real-world implementation, ensuring the reliability and effectiveness
    of time series models in operational environments. With many machine learning
    projects stuck in development and proof of concept stages, grasping the nuances
    of deploying to production enhances your ability to integrate time series analyses
    seamlessly into decision-making processes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们已构建并测试了我们的时间序列分析模型，并展示了其可扩展性，接下来我们将探讨将时间序列模型部署到生产环境中的实际考量和步骤，使用Spark框架。此信息对于指导你从开发过渡到实际应用至关重要，确保时间序列模型在操作环境中的可靠性和有效性。许多机器学习项目停滞在开发和概念验证阶段，掌握部署到生产环境的细节将增强你将时间序列分析无缝集成到决策过程中。
- en: 'While in [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087) we covered the broader
    end-to-end view of a time series analysis project, in this chapter, we’re going
    to focus on going to production with the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B18568_04.xhtml#_idTextAnchor087)中，我们介绍了时间序列分析项目的整体端到端视角，而在本章中，我们将专注于以下几个主要主题，帮助项目进入生产环境：
- en: Workflows
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流
- en: Monitoring and reporting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控与报告
- en: Additional considerations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外的考虑事项
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will explore with code examples the deployment of a scalable
    end-to-end workflow for time series analysis in our own container-based environment.
    A tremendous amount of work goes into building a production-ready environment,
    which goes way beyond what we can reasonably cover in this chapter. We will focus
    instead on providing an example as a starting point. We will see how what we have
    learned so far about time series analysis comes together to create an end-to-end
    workflow.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过代码示例探讨如何在基于容器的环境中部署可扩展的时间序列分析端到端工作流。构建一个生产就绪的环境需要大量工作，远超我们在本章中可以合理涵盖的范围。我们将专注于提供一个示例作为起点。我们将看到迄今为止关于时间序列分析的知识如何结合起来，形成一个完整的端到端工作流。
- en: 'The code for this chapter can be found at this URL: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch9](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch9)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下网址找到：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch9](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch9)
- en: Let’s start by setting up our environment for the example.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先从为示例设置环境开始。
- en: Environment setup
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境设置
- en: We will be using Docker containers, like in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063)
    and [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087), for the platform infrastructure.
    Follow the instructions in the *Using a container for deployment* section of [*Chapter
    3*](B18568_03.xhtml#_idTextAnchor063) and the *Environment setup* section of [*Chapter
    4*](B18568_04.xhtml#_idTextAnchor087) on setting up the container environment.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Docker容器，如[*第3章*](B18568_03.xhtml#_idTextAnchor063)和[*第4章*](B18568_04.xhtml#_idTextAnchor087)中所述，用于平台基础设施。请按照[*第3章*](B18568_03.xhtml#_idTextAnchor063)中*使用容器进行部署*部分和[*第4章*](B18568_04.xhtml#_idTextAnchor087)中*环境设置*部分的说明设置容器环境。
- en: 'Once the environment is set up, download the deployment script from the Git
    repository for this chapter, which is at the following URL:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦环境设置好，从本章的Git仓库下载部署脚本，链接地址为：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch9](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch9)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch9](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch9)'
- en: You can then start the container environment as per the *Environment startup*
    section of [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087). Do a quick visual
    validation of the components as per the *Accessing the UIs* section of the same
    chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以按照[*第4章*](B18568_04.xhtml#_idTextAnchor087)中*环境启动*部分的说明，启动容器环境。根据同一章中的*访问UI*部分，快速进行组件的可视化验证。
- en: Before we get into the nitty-gritty of the code, let’s step back with an overview
    of the workflow to see the big picture of what we will be building in this section.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入代码细节之前，让我们回顾一下工作流的概述，看看我们将在本节中构建的整体框架。
- en: Workflows
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作流
- en: The code example in this chapter includes two workflows. These are implemented
    as **Directed Acyclic Graphs** (**DAGs**) in Airflow, like in [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087).
    The best way to visualize the workflows is from the DAG views in Airflow, as per
    *Figures 9.1* and *9.2*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码示例包含两个工作流。它们作为 **有向无环图**（**DAGs**）在 Airflow 中实现，类似于 [*第 4 章*](B18568_04.xhtml#_idTextAnchor087)。可视化工作流的最佳方式是通过
    Airflow 中的 DAG 视图，如 *图 9.1* 和 *9.2* 所示。
- en: 'The two workflows are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个工作流如下：
- en: '`ts-spark_ch9_data-ml-ops`: This is an example of the end-to-end process, shown
    in *Figure 9**.1*, which includes the following tasks:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ts-spark_ch9_data-ml-ops`：这是端到端流程的示例，见 *图 9.1*，包括以下任务：'
- en: '`get_config`'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_config`'
- en: '`ingest_train_data`'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ingest_train_data`'
- en: '`transform_train_data`'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transform_train_data`'
- en: '`train_and_log_model`'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_and_log_model`'
- en: '`forecast`'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forecast`'
- en: '`ingest_eval_data`'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ingest_eval_data`'
- en: '`transform_eval_data`'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transform_eval_data`'
- en: '`eval_forecast`'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_forecast`'
- en: '![](img/B18568_09_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_09_01.jpg)'
- en: 'Figure 9.1: Airflow DAG for the end-to-end workflow'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：端到端工作流的 Airflow DAG
- en: '`ts-spark_ch9_data-ml-ops_runall`: This second workflow, shown in *Figure 9**.2*,
    calls the preceding one multiple times with different ranges of dates. It simulates
    what happens in the real world whereby the preceding end-to-end workflow is launched
    at a regular interval, say daily or weekly, with new data.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ts-spark_ch9_data-ml-ops_runall`：这是第二个工作流，如 *图 9.2* 所示，它多次调用前一个工作流，并使用不同的日期范围。它模拟了现实中的情况，其中前一个端到端工作流会在定期间隔（如每日或每周）内启动，并使用新数据。'
- en: '![](img/B18568_09_02.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_09_02.jpg)'
- en: 'Figure 9.2: Airflow DAG with multiple calls to the end-to-end workflow'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：Airflow DAG，多个调用端到端工作流
- en: The code for these Airflow DAGs is in the `dags` folder. They are in Python
    (`.py` files) and can be visualized via a text, or preferably a code, editor.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 Airflow DAG 的代码位于 `dags` 文件夹中。它们是用 Python 编写的（`.py` 文件），可以通过文本编辑器或代码编辑器进行可视化查看。
- en: Modularity
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化
- en: It is worth noting that for the example here, we could have joined up all the
    code of these individual tasks into one big task. Instead, we have broken the
    workflow into multiple tasks to illustrate the best practice of modularization.
    In a real-world situation, this facilitates independent code change, scaling,
    and task reruns. Different teams may have ownership of the tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在这里的示例中，我们本可以将这些独立任务的所有代码合并成一个大任务。然而，我们将工作流拆分为多个任务，以说明模块化的最佳实践。在实际情况下，这样做有利于独立的代码修改、扩展以及任务的重新执行。不同的团队可能拥有不同任务的所有权。
- en: Workflow separation
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流分离
- en: The workflows we are demonstrating in this example can be further separated
    in your own implementation. For instance, it is common to have the model-training-related
    tasks, the forecasting, and the model evaluation in their own individual workflows
    launched at different time intervals.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此示例中演示的工作流，在您自己的实现中可以进一步拆分。例如，通常会将模型训练相关任务、预测和模型评估拆分为各自独立的工作流，并在不同的时间间隔内启动。
- en: We will explain each of these DAGs and related tasks in detail in the upcoming
    sections, starting with `ts-spark_ch9_data-ml-ops_runall`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中详细解释每个 DAG 和相关任务，从 `ts-spark_ch9_data-ml-ops_runall` 开始。
- en: Simulation and runs
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟与运行
- en: As we saw in *Figure 9**.2*, `ts-spark_ch9_data-ml-ops_runall` has five tasks,
    which we will explain further here.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在 *图 9.2* 中看到的，`ts-spark_ch9_data-ml-ops_runall` 有五个任务，我们将在此进一步解释这些任务。
- en: The purpose of the `_runall` workflow is to simulate the real-world execution
    of the cycle of training, forecasting, and evaluation at regular intervals. In
    our example, each task of the `_runall` workflow corresponds to one cycle of training,
    forecasting, and evaluation. We will call each task a run and have five runs in
    all, corresponding to the five tasks of `_runall`. These tasks will be scheduled
    at regular intervals, daily, weekly, monthly, or so. In the example here, we are
    just running them sequentially, one after the other.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`_runall` 工作流的目的是模拟在定期间隔内，训练、预测和评估周期的真实执行过程。在我们的示例中，`_runall` 工作流的每个任务对应一次训练、预测和评估的循环。我们将每个任务称为一次运行，所有任务总共有五次运行，对应
    `_runall` 的五个任务。这些任务将在定期的间隔内调度，如每日、每周、每月等。在这里的示例中，我们只是顺序执行它们，一个接一个。'
- en: 'Each task calls the `ts-spark_ch9_data-ml-ops` workflow with a different set
    of parameters. They are as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个任务都使用不同的参数调用 `ts-spark_ch9_data-ml-ops` 工作流。它们如下：
- en: '`runid`: An integer to identify the run'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`runid`：一个整数，用于标识运行'
- en: '`START_DATE`: The start date in the time series dataset to use for training'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`START_DATE`：用于训练的时间序列数据集的起始日期'
- en: '`TRAIN_END_DATE`: The end date in the time series dataset for training'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TRAIN_END_DATE`：训练数据集中的时间序列结束日期'
- en: '`EVAL_END_DATE`: The end date in the time series dataset for evaluation'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EVAL_END_DATE`：评估数据集中的时间序列结束日期'
- en: The way the different runs are configured is with a sliding window of 5 years
    of training data and 1 year of evaluation data in our example. In a real-world
    scenario, the evaluation date range is likely to be shorter, corresponding to
    a shorter forecasting horizon.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 不同运行的配置方式是使用一个滑动窗口，其中训练数据为5年，评估数据为1年。在实际场景中，评估日期范围可能更短，对应于更短的预测期。
- en: 'The run configurations are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 运行配置如下：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The tasks are defined as follows to trigger the `ts-spark_ch9_data-ml-ops`
    workflow passing the run configuration as a parameter:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 任务按以下方式定义，以触发`ts-spark_ch9_data-ml-ops`工作流并将运行配置作为参数传递：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The tasks are then launched sequentially as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 任务随后按以下顺序依次启动：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can kick off this `ts-spark_ch9_data-ml-ops_runall` Airflow DAG from the
    Airflow DAG view as per *Figure 9**.3*, by clicking on the run (**>**) button
    highlighted in green.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从Airflow DAG视图中启动此`ts-spark_ch9_data-ml-ops_runall` Airflow DAG，如*图9.3*所示，通过点击绿色突出显示的运行按钮（**>**）。
- en: '![](img/B18568_09_03.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_09_03.jpg)'
- en: 'Figure 9.3: Run the Airflow DAG'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：运行Airflow DAG
- en: The outcome of this DAG can be seen in *Figure 9**.2*, showing the status of
    the individual tasks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本DAG的结果可以在*图9.2*中看到，显示了各个任务的状态。
- en: We will now discuss the details of these tasks, which, as we have seen, call
    the `ts-spark_ch9_data-ml-ops` workflow with different parameters. Let’s start
    with the first step, `get_config`, tasked with handling these parameters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论这些任务的细节，正如我们所见，它们使用不同的参数调用`ts-spark_ch9_data-ml-ops`工作流。我们从第一个步骤`get_config`开始，它负责处理这些参数。
- en: Configuration
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置
- en: The first task in the `ts-spark_ch9_data-ml-ops` workflow is `t0` and it calls
    the `get_config` function to retrieve the configuration needed to run the workflow.
    These are passed as parameters when calling the workflow. They are, as mentioned
    earlier, the run identifier and date ranges of the time series data for which
    we want to run the workflow. We will see how they are used in the subsequent tasks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`ts-spark_ch9_data-ml-ops`工作流中的第一个任务是`t0`，它调用`get_config`函数来获取运行工作流所需的配置。这些配置作为参数传递给工作流。正如前面提到的，它们是运行标识符和时间序列数据的日期范围，工作流将基于这些范围运行。我们将看到它们在随后的任务中是如何使用的。'
- en: 'The code that defines task `t0` is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 定义任务`t0`的代码如下：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `get_config` function, which is called by task `t0`, is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_config`函数由任务`t0`调用，其代码如下：'
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'def ingest_train_data(_vars, **kwargs):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'def ingest_train_data(_vars, **kwargs):'
- en: sdf = spark.read.csv(
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: sdf = spark.read.csv(
- en: DATASOURCE, header=True, inferSchema=True
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: DATASOURCE, header=True, inferSchema=True
- en: )
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: sdf = sdf.filter(
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: sdf = sdf.filter(
- en: (F.col('date') >= F.lit(_vars['START_DATE'])) &
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (F.col('date') >= F.lit(_vars['START_DATE'])) &
- en: (F.col('date') <= F.lit(_vars['TRAIN_END_DATE']))
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (F.col('date') <= F.lit(_vars['TRAIN_END_DATE']))
- en: )
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: data_ingest_count = sdf.count()
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: data_ingest_count = sdf.count()
- en: sdf.write.format("delta").mode("overwrite").save(
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: sdf.write.format("delta").mode("overwrite").save(
- en: f"/data/delta/ts-spark_ch9_bronze_train_{_vars['runid']}"
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: f"/data/delta/ts-spark_ch9_bronze_train_{_vars['runid']}"
- en: )
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: _vars['train_ingest_count'] = data_ingest_count
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: _vars['train_ingest_count'] = data_ingest_count
- en: return _vars
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: return _vars
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'def transform_train_data(_vars, **kwargs):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 'def transform_train_data(_vars, **kwargs):'
- en: sdf = spark.read.format("delta").load(
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: sdf = spark.read.format("delta").load(
- en: f"/data/delta/ts-spark_ch9_bronze_train_{_vars['runid']}"
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: f"/data/delta/ts-spark_ch9_bronze_train_{_vars['runid']}"
- en: )
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: sdf = sdf.selectExpr(
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: sdf = sdf.selectExpr(
- en: '"date as ds",'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '"date as ds",'
- en: '"cast(daily_min_temperature as double) as y"'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '"cast(daily_min_temperature as double) as y"'
- en: )
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: sdf = sdf.dropna()
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: sdf = sdf.dropna()
- en: data_transform_count = sdf.count()
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: data_transform_count = sdf.count()
- en: sdf.write.format("delta").mode("overwrite").save(
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: sdf.write.format("delta").mode("overwrite").save(
- en: f"/data/delta/ts-spark_ch9_silver_train_{_vars['runid']}"
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: f"/data/delta/ts-spark_ch9_silver_train_{_vars['runid']}"
- en: )
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: _vars['train_transform_count'] = data_transform_count
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: _vars['train_transform_count'] = data_transform_count
- en: return _vars
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: return _vars
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'def train_and_log_model(_vars, **kwargs):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 'def train_and_log_model(_vars, **kwargs):'
- en: sdf = spark.read.format("delta").load(
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: sdf = spark.read.format("delta").load(
- en: f"/data/delta/ts-spark_ch9_silver_train_{_vars['runid']}"
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: f"/data/delta/ts-spark_ch9_silver_train_{_vars['runid']}"
- en: )
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: pdf = sdf.toPandas()
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: pdf = sdf.toPandas()
- en: mlflow.set_experiment(
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.set_experiment(
- en: '''ts-spark_ch9_data-ml-ops_time_series_prophet_train'''
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '''ts-spark_ch9_data-ml-ops_time_series_prophet_train'''
- en: )
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: mlflow.start_run()
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.start_run()
- en: mlflow.log_param("DAG_NAME", DAG_NAME)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.log_param("DAG_NAME", DAG_NAME)
- en: mlflow.log_param("TRAIN_START_DATE", _vars['START_DATE'])
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.log_param("TRAIN_START_DATE", _vars['START_DATE'])
- en: …
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: mlflow.log_metric(
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.log_metric(
- en: '''train_ingest_count'', _vars[''train_ingest_count''])'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '''train_ingest_count'', _vars[''train_ingest_count''])'
- en: …
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: model = Prophet().fit(pdf)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: model = Prophet().fit(pdf)
- en: …
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: cv_metrics_name = [
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: cv_metrics_name = [
- en: '"mse", "rmse", "mae", "mdape", "smape", "coverage"]'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '"mse", "rmse", "mae", "mdape", "smape", "coverage"]'
- en: cv_params = cross_validation(
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: cv_params = cross_validation(
- en: …
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: )
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: _cv_metrics = performance_metrics(cv_params)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: _cv_metrics = performance_metrics(cv_params)
- en: cv_metrics = {
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: cv_metrics = {
- en: 'n: _cv_metrics[n].mean() for n in cv_metrics_name}'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 'n: _cv_metrics[n].mean() for n in cv_metrics_name}'
- en: …
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: signature = infer_signature(train, predictions)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: signature = infer_signature(train, predictions)
- en: mlflow.prophet.log_model(
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.prophet.log_model(
- en: model, artifact_path=ARTIFACT_DIR,
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: model, artifact_path=ARTIFACT_DIR,
- en: signature=signature, registered_model_name=model_name,)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: signature=signature, registered_model_name=model_name,)
- en: mlflow.log_params(param)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.log_params(param)
- en: mlflow.log_metrics(cv_metrics)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.log_metrics(cv_metrics)
- en: …
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: mlflow.end_run()
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.end_run()
- en: return _vars
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: return _vars
- en: '[PRE7]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'def forecast(_vars, **kwargs):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 'def forecast(_vars, **kwargs):'
- en: '# Load the model from the Model Registry'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '# Load the model from the Model Registry'
- en: model_uri = f"models:/{model_name}/{model_version}"
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: model_uri = f"models:/{model_name}/{model_version}"
- en: _model = mlflow.prophet.load_model(model_uri)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: _model = mlflow.prophet.load_model(model_uri)
- en: forecast = _model.predict(
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: forecast = _model.predict(
- en: _model.make_future_dataframe(
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: _model.make_future_dataframe(
- en: periods=365, include_history = False))
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: periods=365, include_history = False))
- en: sdf = spark.createDataFrame(forecast[
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: sdf = spark.createDataFrame(forecast[
- en: '[''ds'', ''yhat'', ''yhat_lower'', ''yhat_upper'']])'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[''ds'', ''yhat'', ''yhat_lower'', ''yhat_upper'']])'
- en: sdf.write.format("delta").mode("overwrite").save(
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: sdf.write.format("delta").mode("overwrite").save(
- en: f"/data/delta/ts-spark_ch9_gold_forecast_{_vars['runid']}")
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: f"/data/delta/ts-spark_ch9_gold_forecast_{_vars['runid']}")
- en: print(f"forecast:\n${forecast.tail(30)}")
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: print(f"forecast:\n${forecast.tail(30)}")
- en: mlflow.end_run()
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.end_run()
- en: return _vars
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: return _vars
- en: '[PRE8]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'def eval_forecast(_vars, **kwargs):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 'def eval_forecast(_vars, **kwargs):'
- en: sdf = spark.read.format("delta").load(
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: sdf = spark.read.format("delta").load(
- en: f"/data/delta/ts-spark_ch9_silver_eval_{_vars['runid']}")
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: f"/data/delta/ts-spark_ch9_silver_eval_{_vars['runid']}")
- en: sdf_forecast = spark.read.format("delta").load(
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: sdf_forecast = spark.read.format("delta").load(
- en: f"/data/delta/ts-spark_ch9_gold_forecast_{_vars['runid']}")
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: f"/data/delta/ts-spark_ch9_gold_forecast_{_vars['runid']}")
- en: sdf_eval = sdf.join(sdf_forecast, 'ds', "inner")
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: sdf_eval = sdf.join(sdf_forecast, 'ds', "inner")
- en: …
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: evaluator = RegressionEvaluator(
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: evaluator = RegressionEvaluator(
- en: labelCol='y', predictionCol='yhat', metricName='rmse')
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: labelCol='y', predictionCol='yhat', metricName='rmse')
- en: eval_rmse = evaluator.evaluate(sdf_eval)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: eval_rmse = evaluator.evaluate(sdf_eval)
- en: …
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: mlflow.set_experiment('ts-spark_ch9_data-ml-ops_time_series_prophet_eval')
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.set_experiment('ts-spark_ch9_data-ml-ops_time_series_prophet_eval')
- en: mlflow.start_run()
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.start_run()
- en: mlflow.log_param("DAG_NAME", DAG_NAME)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.log_param("DAG_NAME", DAG_NAME)
- en: mlflow.log_param("EVAL_START_DATE", _vars['START_DATE'])
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.log_param("EVAL_START_DATE", _vars['START_DATE'])
- en: …
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: mlflow.log_metric('eval_rmse', _vars['eval_rmse'])
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.log_metric('eval_rmse', _vars['eval_rmse'])
- en: mlflow.end_run()
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.end_run()
- en: return _vars
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: return _vars
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: t1 >> t2 >> t3 >> t4 >> t5
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: t1 >> t2 >> t3 >> t4 >> t5
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: t1 >> t2 >> [t3, t4] >> t5
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: t1 >> t2 >> [t3, t4] >> t5
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
