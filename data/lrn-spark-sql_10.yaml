- en: Using Spark SQL in Deep Learning Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在深度学习应用中使用Spark SQL
- en: Deep learning has emerged as a superior solution to several difficult problems
    in machine learning over the past decade. We hear about deep learning being deployed
    across many different areas, including computer vision, speech recognition, natural
    language processing, audio recognition, social media applications, machine translation,
    and biology. Often, the results produced using deep learning approaches have been
    comparable to or better than those produced by human experts.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，深度学习已经成为解决机器学习中几个困难问题的优越解决方案。我们听说深度学习被部署到许多不同领域，包括计算机视觉、语音识别、自然语言处理、音频识别、社交媒体应用、机器翻译和生物学。通常，使用深度学习方法产生的结果与或优于人类专家产生的结果。
- en: There have been several different types of deep learning models that have been
    applied to different problems. We will review the basic concepts of these models
    and present some code. This is an emerging area in Spark, so even though there
    are several different libraries available, many are in their early releases or
    evolving on a daily basis. We will provide a brief overview of some of these libraries,
    including some code examples using Spark 2.1.0, Scala, and BigDL. We chose BigDL
    because it is one the few libraries that run directly on top of Spark Core (similar
    to other Spark packages) and works with Spark SQL DataFrame API and ML pipelines
    using a Scala API.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有几种不同类型的深度学习模型被应用到不同的问题上。我们将回顾这些模型的基本概念并呈现一些代码。这是Spark中一个新兴的领域，所以尽管有几种不同的库可用，但很多都还处于早期版本或者每天都在不断发展。我们将简要概述其中一些库，包括使用Spark
    2.1.0、Scala和BigDL的一些代码示例。我们选择BigDL是因为它是少数几个直接在Spark Core上运行的库之一（类似于其他Spark包），并且使用Scala
    API与Spark SQL DataFrame API和ML pipelines一起工作。
- en: 'More specifically, in this chapter, you will learn the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，在本章中，您将学习以下内容：
- en: What is deep learning?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是深度学习？
- en: Understanding the key concepts of various deep learning models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解各种深度学习模型的关键概念
- en: Understanding deep learning in Spark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解Spark中的深度学习
- en: Working with BigDL and Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用BigDL和Spark
- en: Introducing neural networks
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络介绍
- en: A neural network, or an **artificial neural network** (**ANN**), is a set of
    algorithms, or actual hardware, that is loosely modeled after the human brain.
    They are essentially an interconnected set of processing nodes that are designed
    to recognize patterns. They adapt to, or learn from, a set of training patterns
    such as images, sound, text, time series, and so on.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，或者人工神经网络（ANN），是一组松散模拟人脑的算法或实际硬件。它们本质上是一组相互连接的处理节点，旨在识别模式。它们适应于或从一组训练模式中学习，如图像、声音、文本、时间序列等。
- en: Neural networks are typically organized into layers that consist of interconnected
    nodes. These nodes communicate with each other by sending signals over the connections.
    Patterns are presented to the network via an input layer, which is then passed
    on to one or more hidden layers. Actual computations are executed in these hidden
    layers. The last hidden layer connects to an output layer that outputs the final
    answer.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通常组织成由相互连接的节点组成的层。这些节点通过连接彼此进行通信。模式通过输入层呈现给网络，然后传递给一个或多个隐藏层。实际的计算是在这些隐藏层中执行的。最后一个隐藏层连接到一个输出层，输出最终答案。
- en: The total input to a particular node is typically a function of the output from
    each of the connected nodes. The contribution from these inputs to a node can
    be excitatory or inhibitory, and ultimately helps determine whether and to what
    extent the signal progresses further through the network (via an activation function).
    Typically, sigmoid activation functions have been very popular. In some applications,
    a linear, semilinear, or the hyperbolic tan (`Tanh`) function have also been used.
    In cases where the output of a node is a stochastic function of the total input,
    the input determines the probability that a given node gets a high activation
    value.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 特定节点的总输入通常是连接节点的每个输出的函数。这些输入对节点的贡献可以是兴奋的或抑制的，并最终有助于确定信号是否以及在多大程度上通过网络进一步传播（通过激活函数）。通常，Sigmoid激活函数非常受欢迎。在一些应用中，也使用了线性、半线性或双曲正切（`Tanh`）函数。在节点的输出是总输入的随机函数的情况下，输入决定了给定节点获得高激活值的概率。
- en: The weights of the connections within the network are modified based on the
    learning rules; for example, when a neural network is initially presented with
    a pattern, it makes a guess as to what the weights might be. It then evaluates
    how far its answer is the actual one and makes appropriate adjustments to its
    connection weights.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 网络内部连接的权重根据学习规则进行修改；例如，当神经网络最初呈现一种模式时，它会猜测权重可能是什么。然后，它评估其答案与实际答案的差距，并对其连接权重进行适当调整。
- en: 'For good introduction to the basics of neural networks, refer: "*A Basic Introduction
    To Neural Networks*" by Bolo, available at: [http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html](http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有关神经网络基础知识的良好介绍，请参考：Bolo的《神经网络基础介绍》，网址：[http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html](http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html)。
- en: We will present more specific details of various types of neural networks in
    the following sections.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍各种类型的神经网络的更具体细节。
- en: Understanding deep learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解深度学习
- en: Deep learning is the application of ANNs to learn tasks. Deep learning methods
    are based on learning data representations, instead of task-specific algorithms.
    Though the learning can be supervised or unsupervised, the recent focus has been
    toward creating efficient systems that learn these representations from large-scale,
    unlabeled Datasets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是将人工神经网络应用于学习任务。深度学习方法基于学习数据表示，而不是特定于任务的算法。尽管学习可以是监督的或无监督的，但最近的重点是创建能够从大规模未标记数据集中学习这些表示的高效系统。
- en: 'The following figure depicts a simple deep learning neural network with two
    hidden layers:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了一个具有两个隐藏层的简单深度学习神经网络：
- en: '![](img/00282.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00282.jpeg)'
- en: Deep learning typically comprises of multiple layers of processing units with
    the learning of feature representation occurring within each layer. These layers
    form a hierarchy of features and deep learning assumes that the hierarchy corresponds
    to the levels of abstraction. Hence, it exploits the idea of hierarchical explanatory
    factors, where the more abstract concepts at a higher level are learned from the
    lower-level ones. Varying the numbers of layers and layer sizes can provide different
    amounts of abstraction, as required by the use case.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通常包括多层处理单元，每一层都在其中学习特征表示。这些层形成特征的层次结构，深度学习假设这种层次结构对应于抽象级别。因此，它利用了分层解释因素的思想，更高级别的更抽象的概念是从更低级别的概念中学习的。通过改变层数和层大小，可以提供不同数量的抽象，根据使用情况的需要。
- en: Understanding representation learning
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解表示学习
- en: Deep learning methods are representation-learning methods with multiple levels
    of abstraction. Here, the non-linear modules transform the raw input into a representation
    at a higher, slightly more abstract level. Ultimately, very complex functions
    can be learned by composing sufficient numbers of such layers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法是具有多个抽象层次的表示学习方法。在这里，非线性模块将原始输入转换为更高、稍微更抽象级别的表示。最终，通过组合足够数量的这样的层，可以学习非常复杂的函数。
- en: For a review paper on deep learning, refer to *Deep Learning*, by Yann LeCun,
    Yoshua Bengio, and Geoffrey Hinton, which is available at [http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html?foxtrotcallback=true](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html?foxtrotcallback=true).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有关深度学习的综述论文，请参阅Yann LeCun、Yoshua Bengio和Geoffrey Hinton的《深度学习》，可在[http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html?foxtrotcallback=true](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html?foxtrotcallback=true)上找到。
- en: 'Now, we illustrate the process of learning representations and features in
    a traditional pattern recognition task:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将说明在传统模式识别任务中学习表示和特征的过程：
- en: '![](img/00283.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00283.jpeg)'
- en: Traditional machine learning techniques were limited in their ability to process
    natural data in its original or raw form. Building such machine-learning systems
    required deep domain expertise and substantial effort to identify (and keep updated)
    the features from which the learning subsystem, often a classifier, can detect
    or classify patterns in the input.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器学习技术在处理自然数据的原始形式时受到限制。构建这样的机器学习系统需要深入的领域专业知识和大量的努力，以识别（并保持更新）学习子系统，通常是分类器，可以从中检测或分类输入中的模式的特征。
- en: Many of these conventional machine learning applications used linear classifiers
    on top of handcrafted features. Such classifiers typically required a good feature
    extractor that produced representations that were selective to the aspects of
    the image. However, all this effort is not required if good features could be
    learned, automatically, using a general-purpose learning procedure. This particular
    aspect of deep learning represents one of the key advantages of deep learning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 许多传统的机器学习应用程序使用手工制作的特征上的线性分类器。这样的分类器通常需要一个良好的特征提取器，产生对图像方面有选择性的表示。然而，如果可以使用通用学习程序自动学习良好的特征，那么所有这些努力都是不必要的。深度学习的这一特定方面代表了深度学习的一个关键优势。
- en: 'In contrast to the earlier machine learning techniques, the high-level process
    in deep learning is, typically, in which the end-to-end learning process involves
    features that are also learned from the data. This is illustrated here:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期的机器学习技术相比，深度学习中的高级过程通常是，其中端到端的学习过程还涉及从数据中学习的特征。这在这里有所说明：
- en: '![](img/00284.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00284.jpeg)'
- en: In the next section, we will briefly discuss one of most commonly used functions,
    stochastic gradient descent, for adjusting the weights in a network.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将简要讨论一种最常用的函数，即随机梯度下降，用于调整网络中的权重。
- en: Understanding stochastic gradient descent
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解随机梯度下降
- en: A deep learning system can consist of millions of adjustable weights, and millions
    of labeled examples are used to train the machine. In practice, **stochastic gradient
    descent** (**SGD**) optimization is used widely in many different situations.
    In SGD, the gradient describes the relationship between the network's error and
    a single weight, that is, how does the error vary as the weight is adjusted.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统可以包括数百万个可调整的权重，并且使用数百万个标记的示例来训练机器。在实践中，**随机梯度下降**（**SGD**）优化被广泛应用于许多不同的情况。在SGD中，梯度描述了网络的错误与单个权重之间的关系，即当调整权重时错误如何变化。
- en: 'This optimization approach consists of:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优化方法包括：
- en: Presenting the input vector for a few examples
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为一些示例呈现输入向量
- en: Computing outputs and the errors
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算输出和错误
- en: Computing average gradient for the examples
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算示例的平均梯度
- en: Adjusting weights, appropriately
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适当调整权重
- en: This process is repeated for many small sets of training examples. The process
    stops when the average of the objective function stops decreasing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程对许多小的训练示例集重复进行。当目标函数的平均值停止减少时，过程停止。
- en: This simple procedure usually produces a good set of weights very efficiently
    compared to the more sophisticated optimization techniques. Additionally, the
    training process takes a much shorter time as well. After the training process
    is complete, the performance of the system is measured by running the trained
    model on a test Dataset. The test set contains new inputs that have not been seen
    before (during the training phase) by the machine.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与更复杂的优化技术相比，这个简单的过程通常能够非常有效地产生一组良好的权重。此外，训练过程所需的时间也要短得多。训练过程完成后，通过在测试数据集上运行经过训练的模型来衡量系统的性能。测试集包含机器之前在训练阶段未见过的新输入。
- en: In deep learning neural networks, the activation function is typically set at
    the layer level and applies to all the neurons or nodes in a particular layer.
    Additionally, the output layer of a multilayered deep learning neural network
    plays a specific role; for example, in supervised learning (with labeled input),
    it applies the most likely label based on the signals received from the previous
    layer. Each node on the output layer represents one label, and that node produces
    one of the two possible outcomes, a `0` or a `1`. While such neural networks produce
    a binary output, the input they receive is often continuous; for example, the
    input to a recommendation engine can include factors such as how much the customer
    spent in the previous month and the average number of customer visits per week
    over the past one month. The output layer has to process such signals into a probability
    measure for the given input.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习神经网络中，激活函数通常设置在层级，并应用于特定层中的所有神经元或节点。此外，多层深度学习神经网络的输出层起着特定的作用；例如，在监督学习（带有标记的输入）中，它基于从前一层接收到的信号应用最可能的标签。输出层上的每个节点代表一个标签，并且该节点产生两种可能的结果之一，即`0`或`1`。虽然这样的神经网络产生二进制输出，但它们接收的输入通常是连续的；例如，推荐引擎的输入可以包括客户上个月的消费金额和过去一个月每周平均客户访问次数等因素。输出层必须将这些信号处理成给定输入的概率度量。
- en: Introducing deep learning in Spark
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark中介绍深度学习
- en: In this section, we will review some of the more popular deep learning libraries
    using Spark. These include CaffeOnSpark, DL4J, TensorFrames, and BigDL.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾一些使用Spark的更受欢迎的深度学习库。这些包括CaffeOnSpark、DL4J、TensorFrames和BigDL。
- en: Introducing CaffeOnSpark
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍CaffeOnSpark
- en: CaffeOnSpark was developed by Yahoo for large-scale distributed deep learning
    on Hadoop clusters. By combining the features from the deep learning framework
    Caffe with Apache Spark (and Apache Hadoop), CaffeOnSpark enables distributed
    deep learning on a cluster of GPU and CPU servers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: CaffeOnSpark是Yahoo为Hadoop集群上的大规模分布式深度学习开发的。通过将深度学习框架Caffe的特性与Apache Spark（和Apache
    Hadoop）结合，CaffeOnSpark实现了在GPU和CPU服务器集群上的分布式深度学习。
- en: For more details on CaffeOnSpark, refer to [https://github.com/yahoo/CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有关CaffeOnSpark的更多详细信息，请参阅[https://github.com/yahoo/CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark)。
- en: CaffeOnSpark supports neural network model training, testing, and feature extraction.
    It is complementary to non-deep learning libraries, Spark MLlib and Spark SQL.
    CaffeOnSpark's Scala API provides Spark applications with an easy mechanism to
    invoke deep learning algorithms over distributed Datasets. Here, deep learning
    is typically conducted in the same cluster as the existing data processing pipelines
    to support feature engineering and traditional machine learning applications.
    Hence, CaffeOnSpark allows deep learning training and testing processes to be
    embedded into Spark applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CaffeOnSpark支持神经网络模型的训练、测试和特征提取。它是非深度学习库Spark MLlib和Spark SQL的补充。CaffeOnSpark的Scala
    API为Spark应用程序提供了一种简单的机制，以在分布式数据集上调用深度学习算法。在这里，深度学习通常是在现有数据处理流水线的同一集群中进行，以支持特征工程和传统的机器学习应用。因此，CaffeOnSpark允许将深度学习训练和测试过程嵌入到Spark应用程序中。
- en: Introducing DL4J
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍DL4J
- en: DL4J supports training neural networks on a Spark cluster in order to accelerate
    network training. The current version of DL4J uses a process of parameter averaging
    on each cluster node in order to train the network. The training is complete when
    the master has a copy of the trained network.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J支持在Spark集群上训练神经网络，以加速网络训练。当前版本的DL4J在每个集群节点上使用参数平均化的过程来训练网络。当主节点拥有训练好的网络的副本时，训练就完成了。
- en: For more details on DL4J, refer to [https://deeplearning4j.org/spark](https://deeplearning4j.org/spark).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有关DL4J的更多详细信息，请参阅[https://deeplearning4j.org/spark](https://deeplearning4j.org/spark)。
- en: Introducing TensorFrames
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍TensorFrames
- en: Experimental TensorFlow binding for Scala and Apache Spark is currently available
    on GitHub. TensorFrames is essentially TensorFlow on Spark Dataframes that lets
    you manipulate Apache Spark's DataFrames with TensorFlow programs. The Scala support
    is currently more limited than Python--the Scala DSL features a subset of TensorFlow
    transforms.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 实验性的Scala和Apache Spark的TensorFlow绑定目前在GitHub上可用。TensorFrames本质上是Spark Dataframes上的TensorFlow，它允许您使用TensorFlow程序操作Apache
    Spark的DataFrames。目前，Scala支持比Python更有限--Scala DSL具有TensorFlow变换的子集。
- en: For more details on TensorFrames, visit [https://github.com/databricks/tensorframes](https://github.com/databricks/tensorframes).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有关TensorFrames的更多详细信息，请访问[https://github.com/databricks/tensorframes](https://github.com/databricks/tensorframes)。
- en: In Scala, the operations can be loaded from an existing graph defined in the
    `ProtocolBuffers` format or using a simple Scala DSL. However, given the overall
    popularity of TensorFlow, this library is gaining traction and is more popular
    with the Python community.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中，操作可以从以`ProtocolBuffers`格式定义的现有图形中加载，也可以使用简单的Scala DSL。然而，鉴于TensorFlow的整体流行，这个库正在受到关注，并且在Python社区中更受欢迎。
- en: Working with BigDL
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BigDL
- en: 'BigDL is an open source distributed deep learning library for Apache Spark.
    It was initially developed and open sourced by Intel. With BigDL, the developers
    can write deep learning applications as standard Spark programs. These programs
    directly run on top of the existing Spark or Hadoop clusters, as illustrated in
    this figure:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: BigDL是Apache Spark的开源分布式深度学习库。最初由英特尔开发并开源。使用BigDL，开发人员可以将深度学习应用程序编写为标准的Spark程序。这些程序直接在现有的Spark或Hadoop集群上运行，如图所示：
- en: '![](img/00285.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00285.jpeg)'
- en: 'BigDL is modeled after Torch and it provides support for deep learning, including
    numeric computing (via Tensors) and [neural networks](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/nn).
    Additionally, the developers can load pretrained [Caffe](http://caffe.berkeleyvision.org/)
    or [Torch](http://torch.ch/) models into BigDL-Spark programs, as illustrated
    in the following figure:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: BigDL是基于Torch建模的，它支持深度学习，包括数值计算（通过张量）和[神经网络](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/nn)。此外，开发人员可以将预训练的[Caffe](http://caffe.berkeleyvision.org/)或[Torch](http://torch.ch/)模型加载到BigDL-Spark程序中，如下图所示：
- en: '![](img/00286.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00286.jpeg)'
- en: To achieve high performance, BigDL uses [Intel MKL](https://software.intel.com/en-us/intel-mkl)
    and multithreaded programming in each Spark task.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现高性能，BigDL在每个Spark任务中使用[Intel MKL](https://software.intel.com/en-us/intel-mkl)和多线程编程。
- en: For BigDL documentation, examples, and API guides, check out [https://bigdl-project.github.io/master/](https://bigdl-project.github.io/master/).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有关BigDL文档、示例和API指南，请访问[https://bigdl-project.github.io/master/](https://bigdl-project.github.io/master/)。
- en: 'The following figure shows how a BigDL program is executed at a high-level on
    a Spark cluster. With the help of a cluster manager and the driver program, Spark
    tasks are distributed across the Spark worker nodes or containers (executors):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了BigDL程序在Spark集群上的高级执行方式。借助集群管理器和驱动程序，Spark任务分布在Spark工作节点或容器（执行器）上：
- en: '![](img/00287.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00287.jpeg)'
- en: We will execute a few examples of deep neural networks available in the BigDL
    distribution in the later sections of this chapter. At this time, this is one
    of the few libraries that work with the Spark SQL DataFrame API and ML pipelines.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的后面几节中执行BigDL分发中提供的几个深度神经网络的示例。目前，这是少数几个与Spark SQL DataFrame API和ML管道一起使用的库之一。
- en: In the next section, we will highlight how Spark can be leveraged for tuning
    hyperparameters in parallel.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将重点介绍如何利用Spark并行调整超参数。
- en: Tuning hyperparameters of deep learning models
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整深度学习模型的超参数
- en: 'When building a neural network, there are many important hyperparameters to
    choose carefully. Consider the given examples:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 构建神经网络时，有许多重要的超参数需要仔细选择。考虑以下示例：
- en: 'Number of neurons in each layer: Very few neurons will reduce the expressive
    power of the network, but too many will substantially increase the running time
    and return noisy estimates'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每层神经元的数量：很少的神经元会降低网络的表达能力，但太多的神经元会大大增加运行时间并返回嘈杂的估计值
- en: 'Learning rate: If it is too high, the neural network will only focus on the
    last few samples seen and disregard all the experience accumulated before, and
    if it is too low, it will take too long to reach a good state'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：如果学习率太高，神经网络将只关注最近看到的几个样本，并忽略之前积累的所有经验；如果学习率太低，将需要很长时间才能达到良好的状态
- en: The hyperparameter tuning process is "embarrassingly parallel" and can be distributed
    using Spark.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整过程是“尴尬并行”的，可以使用Spark进行分布。
- en: For more details, refer to *Deep Learning with Apache Spark and TensorFlow*,
    by Tim Hunter, at [https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参阅Tim Hunter的*Deep Learning with Apache Spark and TensorFlow*，网址为[https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html)。
- en: Introducing deep learning pipelines
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍深度学习管道
- en: There is an emerging library for supporting deep learning pipelines in Spark,
    which provides high-level APIs for scalable deep learning in Python with Apache
    Spark. Currently, TensorFlow and TensorFlow-backed Keras workflows are supported,
    with a focus on model inference/scoring and transfer learning on image data at
    scale.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中有一个新兴的库，用于支持深度学习管道，它提供了用于Python中可扩展深度学习的高级API。目前支持TensorFlow和基于TensorFlow的Keras工作流程，重点是在规模化图像数据上进行模型推断/评分和迁移学习。
- en: To follow developments on deep learning pipelines in Spark, visit [https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要关注Spark中深度学习管道的发展，请访问[https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning)。
- en: Furthermore, it provides tools for data scientists and machine learning experts
    to turn deep learning models into SQL UDFs that can be used by a much wider group
    of users. This is also a good way to produce a deep learning model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它为数据科学家和机器学习专家提供了工具，可以将深度学习模型转换为SQL UDF，这样更广泛的用户群体就可以使用。这也是生产深度学习模型的一种好方法。
- en: In the next section, we will shift our focus to supervised learning.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把重点转移到监督学习上。
- en: Understanding Supervised learning
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解监督学习
- en: The most common form of machine learning is supervised learning; for example,
    if we are building a system to classify a specific set of images, we first collect
    a large Dataset of images from the same categories. During training, the machine
    is shown an image, and it produces an output in the form of a vector of scores,
    one for each category. As a result of the training, we expect the desired category
    to have the highest score out of all the categories.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的机器学习形式是监督学习；例如，如果我们正在构建一个用于分类特定图像集的系统，我们首先收集来自相同类别的大量图像数据集。在训练期间，机器显示一幅图像，并产生一个以每个类别为一个分数的向量形式的输出。作为训练的结果，我们期望所需的类别在所有类别中具有最高的分数。
- en: A particular type of deep network--the **convolutional neural network** (**ConvNet**/**CNN**)--is
    much easier to train and generalizes much better than fully-connected networks.
    In supervised learning scenarios, deep convolutional networks have significantly
    improved the results of processing images, video, speech, and audio data. Similarly,
    recurrent nets have shone the light on sequential data, such as text and speech.
    We will explore these types of neural networks in the subsequent sections.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络的一种特殊类型——卷积神经网络（ConvNet/CNN）——比全连接网络更容易训练，泛化能力也更好。在监督学习场景中，深度卷积网络显著改善了图像、视频、语音和音频数据的处理结果。同样，循环网络也为顺序数据（如文本和语音）带来了曙光。我们将在接下来的部分探讨这些类型的神经网络。
- en: Understanding convolutional neural networks
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解卷积神经网络
- en: Convolutional neural networks are a special kind of multilayered neural networks
    that are designed to recognize visual patterns directly from pixel images with
    minimal preprocessing. They can recognize patterns having wide variability and
    can effectively deal with distortions and simple geometric transformations. CNNs
    are also trained using a version of the backpropagation algorithm.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络是一种特殊类型的多层神经网络，它们被设计来直接从像素图像中识别视觉模式，需要最少的预处理。它们可以识别具有广泛变化的模式，并且可以有效地处理扭曲和简单的几何变换。CNN也是使用反向传播算法的一种版本进行训练。
- en: The architecture of a typical ConvNet is structured as a series of stages containing
    several stages of stacked convolution, and non-linearity and pooling layers, followed
    by additional convolutional and fully-connected layers. The non-linearity function
    is typically the **Rectified Linear Unit** (**ReLU**) function, and the role of
    the pooling layer is to semantically merge similar features into one. Thus, the
    pooling allows representations to vary very little when elements in the previous
    layer vary in position and appearance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 典型ConvNet的架构被构造为一系列包含多个堆叠卷积、非线性和池化层的阶段，然后是额外的卷积和全连接层。非线性函数通常是**修正线性单元**（ReLU）函数，池化层的作用是将相似特征语义地合并为一个。因此，池化允许表示在前一层的元素在位置和外观上变化很少时也能变化很小。
- en: LeNet-5 is a convolutional network designed for handwritten and machine-printed
    character recognition. Here, we present an example of Lenet-5 available in the
    BigDL distribution.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet-5是一个专为手写和机器打印字符识别设计的卷积网络。在这里，我们介绍了BigDL分发中可用的Lenet-5的一个例子。
- en: The full source code for the example is available at [https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/lenet](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/lenet).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例的完整源代码可在[https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/lenet](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/lenet)找到。
- en: Here, we will use Spark shell to execute the same code. Note that the values
    of the constants have all been taken from the source code available at the aforementioned
    site.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用Spark shell执行相同的代码。请注意，常量的值都取自上述网站提供的源代码。
- en: 'First, execute the `bigdl` shell script to set the environment:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，执行`bigdl` shell脚本来设置环境：
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then start the Spark shell with the appropriate BigDL JAR specified, as
    follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用适当指定BigDL JAR启动Spark shell：
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Dataset for this example can be downloaded from [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子的数据集可以从[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)下载。
- en: 'The Spark shell session for this example is, as shown:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的Spark shell会话如下所示：
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the next section, we will present an example of text classification.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍一个文本分类的例子。
- en: Using neural networks for text classification
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络进行文本分类
- en: Other applications gaining importance involve natural language understanding
    and speech recognition.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其他越来越重要的应用包括自然语言理解和语音识别。
- en: The example in this section is available as a part of the BigDL distribution
    and the full source code is available at [https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/textclassification](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/textclassification).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的示例作为BigDL分发的一部分可用，完整的源代码可在[https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/textclassification](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/example/textclassification)找到。
- en: It uses a pretrained GloVe embedding to convert words to vectors, and then uses
    it to train the text classification model on a twenty Newsgroup Dataset with twenty
    different categories. This model can achieve over 90% accuracy after only two
    epochs of training.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用预训练的GloVe嵌入将单词转换为向量，然后用它在包含二十个不同类别的二十个新闻组数据集上训练文本分类模型。这个模型在只训练两个时期后就可以达到90%以上的准确率。
- en: 'The key portions the code defining the CNN model and optimizer are presented
    here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里呈现了定义CNN模型和优化器的关键部分代码：
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The input Datasets are described, as follows, along with their download URLs:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据集的描述如下，以及它们的下载URL：
- en: '**Embedding**: 100-dimensional pretrained GloVe embeddings of 400 k words trained
    on a 2014 dump of English Wikipedia. Download pretrained GloVe word embeddings
    from [http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入**：400k个单词的100维预训练GloVe嵌入，训练于2014年英文维基百科的转储数据。从[http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip)下载预训练的GloVe单词嵌入。'
- en: '**Training data**: "20 Newsgroup dataset" containing 20 categories and with
    a total of 19,997 texts. Download 20 Newsgroup datasets as the training data from
    [http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据**：“20 Newsgroup数据集”，包含20个类别，共19,997个文本。从[http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz)下载20
    Newsgroup数据集作为训练数据。'
- en: 'In our example, we have reduced the number of categories to eight to avoid
    `Out-of-Memory` exceptions on laptops with less than 16 GB RAM. Put these Datasets
    in `BASE_DIR`; the final directory structure should be as illustrated:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将类别数量减少到八个，以避免在内存小于16GB的笔记本电脑上出现`内存不足`异常。将这些数据集放在`BASE_DIR`中；最终的目录结构应如图所示：
- en: '![](img/00288.jpeg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00288.jpeg)'
- en: 'Use the following command to execute the text classifier:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令执行文本分类器：
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The sample output is given here for your reference:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里给出了示例输出以供参考：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the next section, we will explore the use of deep neural networks for language
    processing.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨使用深度神经网络进行语言处理。
- en: Using deep neural networks for language processing
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度神经网络进行语言处理
- en: As discussed in [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c),
    *Developing Applications with Spark SQL*, the standard approach to statistical
    modeling of language is typically based on counting the frequency of the occurrences
    of n-grams. This usually requires very large training corpora in most real-world
    use cases. Additionally, n-grams treat each word as an independent unit, so they
    cannot generalize across semantically related sequences of words. In contrast,
    neural language models associate each word with a vector of real-value features
    and therefore semantically-related words end up close to each other in that vector
    space. Learning word vectors also works very well when the word sequences come
    from a large corpus of real text. These word vectors are composed of learned features
    that are automatically discovered by the neural network.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第9章](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c)中所讨论的，*使用Spark
    SQL开发应用程序*，语言的统计建模通常基于n-grams的出现频率。在大多数实际用例中，这通常需要非常大的训练语料库。此外，n-grams将每个单词视为独立单元，因此它们无法概括语义相关的单词序列。相比之下，神经语言模型将每个单词与一组实值特征向量相关联，因此语义相关的单词在该向量空间中靠近。学习单词向量在单词序列来自大型真实文本语料库时也非常有效。这些单词向量由神经网络自动发现的学习特征组成。
- en: Vector representations of words learned from text are now very widely used in
    natural-language applications. In the next section, we will explore Recurrent
    Neural Networks and their application to a text classification task.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本中学习的单词的向量表示现在在自然语言应用中被广泛使用。在下一节中，我们将探讨递归神经网络及其在文本分类任务中的应用。
- en: Understanding Recurrent Neural Networks
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解递归神经网络
- en: Generally, for tasks involving sequential inputs, it is recommended to use **Recurrent
    Neural Networks** (**RNNs**). Such input is processed one element at a time, while
    maintaining a "state vector" (in hidden units). The state implicitly contains
    information about all the past elements in the sequence.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于涉及顺序输入的任务，建议使用**递归神经网络**（**RNNs**）。这样的输入一次处理一个元素，同时保持一个“状态向量”（在隐藏单元中）。状态隐含地包含有关序列中所有过去元素的信息。
- en: Typically, in conventional RNNs, it is difficult to store information for a
    long time. In order to remember the input for a long time, the network can be
    augmented with explicit memory. Also, this is the approach used in the **Long
    Short-Term Memory** (**LSTM**) networks; they use hidden units that can remember
    the input. LSTM networks have proved to be more effective than conventional RNNs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在传统的RNN中，很难长时间存储信息。为了长时间记住输入，网络可以增加显式内存。这也是**长短期记忆**（**LSTM**）网络中使用的方法；它们使用可以记住输入的隐藏单元。LSTM网络已被证明比传统的RNN更有效。
- en: 'In this section, we will explore the Recurrent Neural Networks for modeling
    sequential data. The following figure illustrates a simple Recurrent Neural Network
    or an Elman network:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨用于建模序列数据的递归神经网络。下图说明了一个简单的递归神经网络或Elman网络：
- en: '![](img/00289.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00289.jpeg)'
- en: This is probably the simplest possible version of a Recurrent Neural Network
    that is easy to implement and train. The network has an input layer, a hidden
    layer (also called the context layer or state), and an output layer. The input
    to the network in time `t` is **Input(t)**, output is denoted as **Output(t)**,
    and **Context(t)** is state of the network (hidden layer). Input vector is formed
    by concatenating the vector representing the current word, and output from neurons
    in the context layer at time `t − 1`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是最简单的递归神经网络的版本，易于实现和训练。网络有一个输入层，一个隐藏层（也称为上下文层或状态），和一个输出层。网络在时间`t`的输入是**Input(t)**，输出表示为**Output(t)**，**Context(t)**是网络的状态（隐藏层）。输入向量是通过连接表示当前单词的向量和时间`t-1`的上下文层中神经元的输出来形成的。
- en: These networks are trained in several epochs, in which all the data from the
    training corpus is sequentially presented. To train the network, we can use the
    standard backpropagation algorithm with stochastic gradient descent. After each
    epoch, the network is tested on validation data. If the log-likelihood of validation
    data increases, the training continues in the new epoch. If no significant improvement
    is observed, the learning rate can be halved at the start of each new epoch. If
    there is no significant improvement as a result of changing the learning rate,
    the training is finished. Convergence of such networks is usually achieved after
    10-20 epochs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络在几个时期内进行训练，其中训练语料库中的所有数据都被顺序呈现。为了训练网络，我们可以使用随机梯度下降的标准反向传播算法。每个时期后，网络都会在验证数据上进行测试。如果验证数据的对数似然性增加，训练将在新的时期继续。如果没有观察到显著的改善，学习率可以在每个新时期开始时减半。如果改变学习率没有显著改善，训练就结束了。这样的网络通常在10-20个时期后收敛。
- en: 'Here, the output layer represents a probability distribution of the next word
    when given the previous word and **Context(t − 1)**. Softmax ensures that the
    probability distribution is valid. At each training step, the error vector is
    computed, and the weights are updated with the standard backpropagation algorithm,
    as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，输出层表示在给定上一个单词和**Context(t − 1)**时下一个单词的概率分布。Softmax确保概率分布是有效的。在每个训练步骤中，计算错误向量，并使用标准的反向传播算法更新权重，如下所示：
- en: '*error(t) = desired(t) − Output(t)*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*error(t) = desired(t) − Output(t)*'
- en: Here, desired is a vector using `1-of-N` coding, representing the word that
    should have been predicted in the particular context, and **Output(t)** is the
    actual output from the network.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，desired是使用`1-of-N`编码的向量，表示在特定上下文中应该被预测的单词，**Output(t)**是网络的实际输出。
- en: To improve performance, we can merge all the words that occur less often than
    a given threshold value (in the training text) into a special rare token. Hence,
    all the rare words are thus treated equally, that is, the probability is distributed
    uniformly between them.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能，我们可以将在训练文本中出现次数少于给定阈值的所有单词合并为一个特殊的稀有标记。因此，所有稀有单词都被平等对待，即它们之间的概率均匀分布。
- en: Now, we execute a simple RNN example provided in the BigDL library. The network
    is a fully-connected RNN where the output is fed back into the input. The example
    model supports sequence-to-sequence processing and is an implementation of a simple
    Recurrent Neural Network for language modeling.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们执行BigDL库中提供的一个简单的RNN示例。该网络是一个全连接的RNN，其中输出被反馈到输入中。该示例模型支持序列到序列处理，并且是用于语言建模的简单循环神经网络的实现。
- en: For the full source code of this example, refer to [https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/rnn](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/rnn).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此示例的完整源代码，请参阅[https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/rnn](https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/models/rnn)。
- en: The input Dataset, Tiny Shakespeare Texts, can be downloaded from [https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据集Tiny Shakespeare Texts可以从[https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)下载。
- en: After downloading the text, place it into an appropriate directory. We split
    the input Dataset into separate `train.txt` and `val.txt` files. In our example,
    we select 80% of the input to be the training Dataset, and the remaining 20 percent
    to be the validation Dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 下载文本后，将其放入适当的目录。我们将输入数据集拆分为单独的`train.txt`和`val.txt`文件。在我们的示例中，我们选择80%的输入作为训练数据集，剩下的20%作为验证数据集。
- en: 'Split the input Dataset by executing these commands:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下命令将输入数据集拆分：
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `SentenceSplitter` and `SentenceTokenizer` classes use the `Apache OpenNLP`
    library. The trained model files--`en-token.bin` and `en-sent.bin`--can be downloaded
    from [http://opennlp.sourceforge.net/models-1.5/](http://opennlp.sourceforge.net/models-1.5/).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`SentenceSplitter`和`SentenceTokenizer`类使用`Apache OpenNLP`库。训练模型文件--`en-token.bin`和`en-sent.bin`--可以从[http://opennlp.sourceforge.net/models-1.5/](http://opennlp.sourceforge.net/models-1.5/)下载。'
- en: 'The key parts of the code related to the model and the optimizer are listed
    here:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与模型和优化器相关的关键部分代码如下：
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following command executes the training program. Modify the parameters
    specific to your environment:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行训练程序。修改特定于您的环境的参数：
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The next extract is from the output generated during the training process:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是训练过程中生成的输出的一部分：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we use the saved model to run on the test Dataset, as shown:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用保存的模型在测试数据集上运行，如下所示：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Introducing autoencoders
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入自动编码器
- en: An autoencoder neural network is an unsupervised learning algorithm that sets
    the target values to be equal to the input values. Hence, the autoencoder attempts
    to learn an approximation of an identity function.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器神经网络是一种无监督学习算法，它将目标值设置为等于输入值。因此，自动编码器试图学习一个恒等函数的近似。
- en: 'Learning an identity function does not seem to be a worthwhile exercise; however,
    by placing constraints on the network, such as limiting the number of hidden units,
    we can discover interesting structures about the data. The key components of an
    autoencoder are depicted in this figure:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 学习一个恒等函数似乎并不是一项值得的练习；然而，通过对网络施加约束，比如限制隐藏单元的数量，我们可以发现关于数据的有趣结构。自动编码器的关键组件如下图所示：
- en: '![](img/00290.jpeg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00290.jpeg)'
- en: 'The original input, the compressed representation, and the output layers for
    an autoencoder are also illustrated in the following figure. More specifically,
    this figure represents a situation where, for example, an input image has pixel-intensity
    values from a 10×10 image (100 pixels), and there are `50` hidden units in layer
    two. Here, the network is forced to learn a "compressed" representation of the
    input, in which it must attempt to "reconstruct" the 100-pixel input using `50`
    hidden units:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 原始输入，压缩表示以及自动编码器的输出层也在下图中进行了说明。更具体地说，该图表示了一个情况，例如，输入图像具有来自10×10图像（100像素）的像素强度值，并且在第二层中有`50`个隐藏单元。在这里，网络被迫学习输入的“压缩”表示，其中它必须尝试使用`50`个隐藏单元“重建”100像素的输入：
- en: '![](img/00291.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/00291.jpeg)
- en: For more details on autoencoders, refer to *Reducing the Dimensionality of Data
    with Neural Networks* by G. E. Hinton and R. R. Salakhutdinov, available at [https://www.cs.toronto.edu/~hinton/science.pdf](https://www.cs.toronto.edu/~hinton/science.pdf).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有关自动编码器的更多细节，请参阅G.E. Hinton和R.R. Salakhutdinov的《使用神经网络降低数据的维度》，可在[https://www.cs.toronto.edu/~hinton/science.pdf](https://www.cs.toronto.edu/~hinton/science.pdf)上获得。
- en: Now, we present an example of an autoencoder from the BigDL distribution against
    the MNIST Dataset.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们展示了BigDL分发中针对MNIST数据集的自动编码器示例。
- en: To train the autoencoder, you will need to download the MNIST Dataset from [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练自动编码器，您需要从[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)下载MNIST数据集。
- en: 'You will need to download the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要下载以下内容：
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, you have to unzip them to get the following files:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您需要解压它们以获得以下文件：
- en: '[PRE12]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For our implementation, ReLU is used as the activation function and the mean
    square error is used as the loss function. Key parts of the model and the optimizer
    code used in this example are listed here:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实现，ReLU被用作激活函数，均方误差被用作损失函数。此示例中使用的模型和优化器代码的关键部分如下所示：
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following is the command used to execute the autoencoder example:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是执行自动编码器示例的命令：
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output generated by the example is as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 示例生成的输出如下：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced deep learning in Spark. We discussed various
    types of deep neural networks and their application. We also explored a few code
    examples provided in the BigDL distribution. As this is a rapidly evolving area
    in Spark, presently, we expect these libraries to provide a lot more functionalities
    using Spark SQL and the DataFrame/Dataset APIs. Additionally, we also expect them
    to mature and become more stable over the coming months.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了Spark中的深度学习。我们讨论了各种类型的深度神经网络及其应用。我们还探索了BigDL分发中提供的一些代码示例。由于这是Spark中一个快速发展的领域，目前，我们期望这些库能够提供更多使用Spark
    SQL和DataFrame/Dataset API的功能。此外，我们还期望它们在未来几个月内变得更加成熟和稳定。
- en: In the next chapter, we will shift our focus to tuning Spark SQL applications.
    We will cover key foundational aspects regarding serialization/deserialization
    using encoders and the logical and physical plans associated with query executions,
    and then present the details of the **cost-based optimization** (**CBO**) feature
    released in Spark 2.2\. Additionally, we will present some tips and tricks that
    developers can use to improve the performance of their applications.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把重点转向调整Spark SQL应用程序。我们将涵盖关于使用编码器进行序列化/反序列化以及与查询执行相关的逻辑和物理计划的关键基础知识，然后介绍Spark
    2.2中发布的**基于成本的优化**（**CBO**）功能的详细信息。此外，我们还将介绍开发人员可以使用的一些技巧和窍门来提高其应用程序的性能。
