- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Large-Scale Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模算法
- en: Large-scale algorithms are specifically designed to tackle sizable and intricate
    problems. They distinguish themselves by their demand for multiple execution engines
    due to the sheer volume of data and processing requirements. Examples of such
    algorithms include **Large Language Models** (**LLMs**) like ChatGPT, which require
    distributed model training to manage the extensive computational demands inherent
    to deep learning. The resource-intensive nature of such complex algorithms highlights
    the requirement for robust, parallel processing techniques critical for training
    the model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模算法专门设计用于解决庞大且复杂的问题。它们的特点在于需要多个执行引擎来应对大量的数据和处理需求。此类算法的例子包括**大语言模型**（**LLMs**），如ChatGPT，它们需要分布式模型训练来应对深度学习固有的巨大计算需求。此类复杂算法的资源密集型特性突显了强大并行处理技术的必要性，这对于训练模型至关重要。
- en: In this chapter, we will start by introducing the concept of large-scale algorithms
    and then proceed to discuss the efficient infrastructure required to support them.
    Additionally, we will explore various strategies for managing multi-resource processing.
    Within this chapter, we will examine the limitations of parallel processing, as
    outlined by Amdahl’s law, and investigate the use of **Graphics Processing Units**
    (**GPUs**). Upon completing this chapter, you will have gained a solid foundation
    in the fundamental strategies essential for designing large-scale algorithms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从介绍大规模算法的概念开始，然后讨论支撑它们所需的高效基础设施。此外，我们还将探索管理多资源处理的各种策略。在本章中，我们将研究由Amdahl定律提出的并行处理局限性，并探讨**图形处理单元**（**GPUs**）的使用。完成本章后，您将掌握设计大规模算法所需的基本策略，并打下坚实的基础。
- en: 'The topics covered in this chapter include:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涉及的主题包括：
- en: Introduction to large-scale algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模算法简介
- en: Efficient infrastructure for large-scale algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模算法的高效基础设施
- en: Strategizing multi-resource processing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多资源处理策略
- en: Using the power of clusters/cloud to run large-scale algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用集群/云的力量运行大规模算法
- en: Let’s start with the introduction.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简介开始。
- en: Introduction to large-scale algorithms
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模算法简介
- en: Throughout history, humans have tackled complex problems, from predicting locust
    swarm locations to discovering the largest prime numbers. Our curiosity and determination
    have led to continuous innovation in problem-solving methods. The invention of
    computers was a pivotal moment in this journey, giving us the ability to handle
    intricate algorithms and calculations. Nowadays, computers enable us to process
    massive datasets, execute complex computations, and simulate various scenarios
    with remarkable speed and accuracy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在历史上，人类一直在解决复杂问题，从预测蝗虫群体的位置到发现最大的质数。我们的好奇心和决心推动了问题解决方法的不断创新。计算机的发明是这一历程中的一个关键时刻，使我们能够处理复杂的算法和计算。如今，计算机使我们能够以惊人的速度和精确度处理海量数据集、执行复杂计算并模拟各种场景。
- en: 'However, as we encounter increasingly complex challenges, the resources of
    a single computer often prove insufficient. This is where large-scale algorithms
    come into play, harnessing the combined power of multiple computers working together.
    Large-scale algorithm design constitutes a dynamic and extensive field within
    computer science, focusing on creating and analyzing algorithms that efficiently
    utilize the computational resources of numerous machines. These large-scale algorithms
    allow two types of computing – distributed and parallel. In distributed computing,
    we divide a single task between multiple computers. They each work on a segment
    of the task and combine their results at the end. Think of it like assembling
    a car: different workers handle different parts, but together, they build the
    entire vehicle. Parallel computing, conversely, involves multiple processors performing
    multiple tasks simultaneously, similar to an assembly line where every worker
    does a different job at the same time.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着我们遇到越来越复杂的挑战，单台计算机的资源往往不足以应对。这时，大规模算法便派上了用场，它们利用多台计算机共同工作的强大计算力。大规模算法设计是计算机科学中的一个动态且广泛的领域，致力于创建和分析能够高效利用多台机器计算资源的算法。这些大规模算法支持两种类型的计算——分布式计算和并行计算。在分布式计算中，我们将一个任务拆分到多台计算机上，每台计算机处理任务的一部分，最后将结果汇总。可以把它想象成组装一辆汽车：不同的工人处理不同的部件，但共同完成整辆车的组装。相比之下，并行计算则是多台处理器同时执行多个任务，类似于流水线，每个工人同时进行不同的工作。
- en: LLMs, such as OpenAI’s GPT-4, hold a crucial position in this vast domain, as
    they represent a form of large-scale algorithms. LLMs are designed to comprehend
    and generate human-like text by processing extensive amounts of data and identifying
    patterns within languages. However, training these models is a heavy-duty task.
    It involves working with billions, sometimes trillions, of data units, known as
    tokens. This training includes steps that need to be done one by one, like getting
    the data ready. There are also steps that can be done at the same time, like figuring
    out the changes needed across different layers of the model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLM，如OpenAI的GPT-4，在这个广阔的领域中占据着至关重要的地位，因为它们代表了一种大规模算法的形式。LLM旨在通过处理大量数据并识别语言中的模式来理解和生成类似人类的文本。然而，训练这些模型是一项重负荷的任务。它涉及处理数十亿，甚至数万亿的数据单元，称为“标记”（tokens）。这个训练过程包含需要逐步完成的步骤，比如准备数据，也有一些步骤可以同时进行，比如确定模型不同层次所需的变化。
- en: It’s not an understatement to say that this is a massive job. Because of this
    scale, it’s a common practice to train LLMs using multiple computers at once.
    We call these “distributed systems.” These systems use several GPUs – these are
    the parts of computers that do the heavy lifting for creating images or processing
    data. It’s more accurate to say that LLMs are almost always trained on many machines
    working together to teach a single model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作并非夸大其词。由于其规模庞大，通常的做法是同时使用多台计算机来训练LLM。我们称这些为“分布式系统”。这些系统使用多个GPU——这些是计算机中负责重负载工作的部件，用于创建图像或处理数据。更准确地说，LLM几乎总是在多台计算机协同工作来训练单一模型的情况下进行训练。
- en: In this context, let us first characterize a well-designed large-scale algorithm,
    one that can fully harness the potential of modern computing infrastructure, such
    as cloud computing, clusters, and GPUs/TPUs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种背景下，我们首先要描述一个设计良好的大规模算法，它能够充分利用现代计算基础设施的潜力，如云计算、集群和GPU/TPU。
- en: Characterizing performant infrastructure for large-scale algorithms
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 描述大规模算法的高效基础设施
- en: To efficiently run large-scale algorithms, we want performant systems as they
    are designed to handle increased workloads by adding more computing resources
    to distribute the processing. Horizontal scaling is a key technique for achieving
    scalability in distributed systems, enabling the system to expand its capacity
    by allocating tasks to multiple resources. These resources are typically hardware
    (like **Central Processing Units** (**CPUs**) or GPUs) or software elements (like
    memory, disk space, or network bandwidth) that the system can utilize to perform
    tasks. For a scalable system to efficiently address computational requirements,
    it should exhibit elasticity and load balancing, as discussed in the following
    section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效运行大规模算法，我们需要高性能的系统，因为这些系统被设计成通过增加计算资源来处理更大的工作负载，从而分配处理任务。水平扩展是实现分布式系统可扩展性的关键技术，它使系统能够通过将任务分配给多个资源来扩展其处理能力。这些资源通常是硬件（如
    **中央处理单元**（**CPU**）或 GPU）或软件元素（如内存、磁盘空间或网络带宽），系统可以利用这些资源执行任务。为了让可扩展的系统高效地满足计算需求，它应具备弹性和负载均衡，以下部分将详细讨论这一点。
- en: Elasticity
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性
- en: Elasticity refers to the capacity of infrastructure to dynamically scale resources
    according to changing requirements. One common method of implementing this feature
    is autoscaling, a prevalent strategy in cloud computing platforms such as **Amazon
    Web Services** (**AWS**). In the context of cloud computing, a server group is
    a collection of virtual servers or instances that are orchestrated to work together
    to handle specific workloads. These server groups can be organized into clusters
    to provide high availability, fault tolerance, and load balancing. Each server
    within a group can be configured with specific resources, such as CPU, memory,
    and storage, to perform optimally for the intended tasks. Autoscaling allows the
    server group to adapt to fluctuating demands by modifying the number of nodes
    (virtual servers) in operation. In an elastic system, resources can be added (scaling
    out) to accommodate increased demand, and similarly, resources can be released
    (scaling in) when the demand decreases. This dynamic adjustment allows for efficient
    use of resources, helping to balance performance needs with cost-effectiveness.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性是指基础设施根据变化的需求动态扩展资源的能力。实现这一特性的常见方法是自动扩展，这是云计算平台（如 **Amazon Web Services**（**AWS**））中常见的策略。在云计算的背景下，服务器组是由一组虚拟服务器或实例组成，这些服务器或实例经过编排协同工作以处理特定的工作负载。这些服务器组可以组织成集群，以提供高可用性、容错能力和负载均衡。组内的每台服务器可以配置特定的资源，如
    CPU、内存和存储，以优化执行预定任务的性能。自动扩展允许服务器组通过修改运行中的节点（虚拟服务器）数量来适应波动的需求。在弹性系统中，可以通过增加资源（横向扩展）来应对需求增加，同样，当需求减少时，也可以释放资源（横向缩减）。这种动态调整能有效利用资源，帮助在性能需求与成本效益之间保持平衡。
- en: AWS provides an autoscaling service, which integrates with other AWS services
    like **EC2** (**Elastic Compute Cloud**) and **ELB** (**Elastic Load Balancing**),
    to automatically adjust the number of server instances in the group. This ensures
    optimal resource allocation and consistent performance, even during periods of
    heavy traffic or system failures.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了一项自动扩展服务，它与其他 AWS 服务（如 **EC2**（**弹性计算云**）和 **ELB**（**弹性负载均衡**））集成，能够自动调整组内服务器实例的数量。这确保了在流量高峰或系统故障期间，资源能够得到优化分配，性能保持稳定。
- en: Characterizing a well-designed, large-scale algorithm
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述一个设计良好的大规模算法
- en: A well-designed, large-scale algorithm is capable of processing vast amounts
    of information and is designed to be adaptable, resilient, and efficient. It is
    resilient and adaptable to accommodate the fluctuating dynamics of a large-scale
    environment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个设计良好的大规模算法能够处理海量信息，并且被设计为具备适应性、弹性和高效性。它具有弹性，能够适应大规模环境中不断变化的动态。
- en: 'A well-designed, large-scale algorithm has the following two characteristics:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个设计良好的大规模算法具有以下两个特点：
- en: '**Parallelism**: Parallelism is a feature that lets an algorithm do several
    things at once. For big computing jobs, an algorithm should be able to divide
    tasks between many computers. This speeds up calculations because they are happening
    all at the same time. In the context of large-scale computing, an algorithm should
    be capable of splitting tasks across multiple machines, thereby expediting computations
    through simultaneous processing.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行性**：并行性是让算法一次性处理多个任务的特性。对于大型计算任务，算法应能够将任务分配到多台计算机上。因为这些计算是同时进行的，计算速度得以加快。在大规模计算的背景下，算法应能将任务拆分到多台机器上，从而通过并行处理加速计算过程。'
- en: '**Fault tolerance**: Given the increased risk of system failures in large-scale
    environments due to the sheer number of components, it’s essential that algorithms
    are built to withstand these faults. They should be able to recover from failures
    without substantial loss of data or inaccuracies in output.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：由于大规模环境中组件数量庞大，系统故障的风险增大，因此构建能够承受这些故障的算法至关重要。算法应具备在不丧失大量数据或输出精度的情况下从故障中恢复的能力。'
- en: The three cloud computing giants, Google, Amazon, and Microsoft, provide highly
    elastic infrastructures. Due to the gigantic size of their shared resource pools,
    there are very few companies that have the potential to match the elasticity of
    the infrastructure of these three companies.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 三大云计算巨头，谷歌、亚马逊和微软，提供高度弹性的基础设施。由于它们共享资源池的庞大规模，几乎没有公司能够与这三家公司基础设施的弹性匹敌。
- en: The performance of a large-scale algorithm is intricately linked to the quality
    of the underlying infrastructure. This foundation should provide adequate computational
    resources, extensive storage capacity, high-speed network connectivity, and reliable
    performance to ensure the algorithm’s optimal operation. Let us characterize a
    suitable infrastructure for a large-scale algorithm.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模算法的性能与底层基础设施的质量密切相关。这个基础设施应提供充足的计算资源、广泛的存储能力、高速的网络连接和可靠的性能，以确保算法的最佳运行。让我们描述一个适合大规模算法的基础设施。
- en: Load balancing
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡
- en: Load balancing is an essential practice in large-scale distributed computing
    algorithms. By evenly managing and distributing the workload, it prevents resource
    overload and maintains high system performance. It plays a significant role in
    ensuring efficient operations, optimal resource usage, and high throughput in
    the realm of distributed deep learning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡是大规模分布式计算算法中的一项核心实践。通过均衡地管理和分配工作负载，负载均衡避免了资源过载，并保持了系统的高性能。它在确保高效运作、优化资源利用和实现高吞吐量方面发挥着重要作用，尤其是在分布式深度学习领域。
- en: '*Figure 15.1* illustrates this concept visually. It shows a user interacting
    with a load balancer, which in turn manages the load on multiple nodes. In this
    case, there are four nodes, **Node 1**, **Node 2**, **Node 3**, and **Node 4**.
    The load balancer continually monitors the state of all nodes, distributing incoming
    user requests between them. The decision to assign a task to a specific node depends
    on the node’s current load and the load balancer’s algorithm. By preventing any
    single node from being overwhelmed while others remain underutilized, the load
    balancer ensures optimal system performance:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.1* 直观地展示了这一概念。它展示了一个用户与负载均衡器交互，负载均衡器则管理多个节点的负载。在此例中，有四个节点，**节点 1**、**节点
    2**、**节点 3** 和 **节点 4**。负载均衡器不断监控所有节点的状态，将传入的用户请求在它们之间分配。将任务分配给某个特定节点的决定，取决于该节点的当前负载以及负载均衡器的算法。通过防止任何单一节点被压垮，而其他节点仍处于低负载状态，负载均衡器确保系统性能的最佳化：'
- en: '![Diagram  Description automatically generated](img/B18046_15_01.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18046_15_01.png)'
- en: 'Figure 15.1: Load balancing'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.1：负载均衡
- en: In the broader context of cloud computing, AWS offers a feature called **Elastic
    Load Balancing** (**ELB**). ELB automatically distributes incoming application
    traffic across multiple targets within the AWS ecosystem, such as Amazon EC2 instances,
    IP addresses, or Lambda functions. By doing this, ELB prevents resource overload
    and maintains high application availability and performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在云计算的更广泛背景下，AWS 提供了一个名为**弹性负载均衡（ELB）**的功能。ELB 会自动将传入的应用流量分配到 AWS 生态系统内的多个目标，如
    Amazon EC2 实例、IP 地址或 Lambda 函数。通过这种方式，ELB 可以防止资源过载，并保持应用的高可用性和性能。
- en: 'ELB: Combining elasticity and load balancing'
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ELB：结合弹性和负载均衡
- en: ELB represents an advanced technique that combines the elements of elasticity
    and load balancing into a single solution. It utilizes clusters of server groups
    to augment the responsiveness, efficiency, and scalability of computing infrastructure.
    The objective is to maintain a uniform distribution of workloads across all available
    resources, while simultaneously enabling the infrastructure to dynamically adjust
    its size in response to demand fluctuations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ELB代表一种先进的技术，它将弹性和负载均衡的元素结合到一个单一的解决方案中。它利用服务器集群来增强计算基础设施的响应性、效率和可扩展性。其目标是在所有可用资源之间保持均匀的工作负载分配，同时使基础设施能够根据需求波动动态调整其规模。
- en: '*Figure 15.2* shows a load balancer managing four server groups. Note that
    a server group is a collection of nodes tasked with specific computational functions.
    A server group here refers to an assembly of nodes, each given a unique computational
    role to fulfill.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.2* 显示了一个负载均衡器管理四个服务器组。请注意，服务器组是一个节点集合，负责执行特定的计算功能。在此，服务器组指的是由节点组成的一个集合，每个节点被赋予一个独特的计算任务。'
- en: 'One of the key features of a server group is its elasticity – its ability to
    flexibly add or remove nodes depending on the situation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器组的关键特性之一是其弹性——根据情况灵活地添加或移除节点的能力：
- en: '![Graphical user interface, diagram  Description automatically generated](img/B18046_15_02.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，图表 描述自动生成](img/B18046_15_02.png)'
- en: 'Figure 15.2: Intelligent load balancing server autoscaling'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.2：智能负载均衡服务器自动扩展
- en: Load balancers operate by continuously monitoring workload metrics in real time.
    When computational tasks become increasingly complex, the requirement for processing
    power correspondingly increases. To address this spike in demand, the system triggers
    a “scale-up” operation, integrating additional nodes into the existing server
    groups. In this context, “scaling up” is the process of increasing the computational
    capacity to accommodate the expanded workload. Conversely, when the demand decreases,
    the infrastructure can initiate a “scale-down” operation, in which some nodes
    are deallocated. This dynamic reallocation of nodes across server groups ensures
    an optimal resource utilization ratio. By adapting resource allocation to match
    the prevailing workload, the system prevents resource over-provisioning or under-provisioning.
    This dynamic resource management strategy results in an enhancement of operational
    efficiency and cost-effectiveness, while maintaining high-caliber performance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器通过实时监控工作负载指标来运行。当计算任务变得越来越复杂时，处理能力的需求也相应增加。为应对这种需求激增，系统会触发“扩容”操作，将额外的节点集成到现有的服务器组中。在此背景下，“扩容”是指增加计算能力以适应扩展的工作负载。相反，当需求下降时，基础设施可以启动“缩容”操作，移除一些节点。通过这种跨服务器组的动态节点重新分配，确保了资源的最优利用比率。通过将资源分配调整以匹配当前的工作负载，系统可以避免资源过度配置或不足配置。这种动态资源管理策略提高了运营效率和成本效益，同时保持了高水平的性能。
- en: Strategizing multi-resource processing
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略化多资源处理
- en: In the early days of strategizing multi-resource processing, large-scale algorithms
    were executed on powerful machines called supercomputers. These monolithic machines
    had a shared memory space, enabling quick communication between different processors
    and allowing them to access common variables through the same memory. As the demand
    for running large-scale algorithms grew, supercomputers transformed into **Distributed
    Shared Memory** (**DSM**) systems, where each processing node owned a segment
    of the physical memory. Subsequently, clusters emerged, constituting loosely connected
    systems that depend on message passing between processing nodes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在多资源处理策略的早期，大规模算法是在被称为超级计算机的强大机器上执行的。这些单体机器具有共享内存空间，能够让不同的处理器之间进行快速通信，并通过相同的内存访问共享变量。随着运行大规模算法的需求增长，超级计算机转变为**分布式共享内存**（**DSM**）系统，在这种系统中，每个处理节点拥有一段物理内存。随后，集群应运而生，构成了松散连接的系统，这些系统依赖于处理节点之间的消息传递。
- en: 'Effectively running large-scale algorithms requires multiple execution engines
    operating in parallel to tackle intricate challenges. Three primary strategies
    can be utilized to achieve this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有效运行大规模算法需要多个执行引擎并行工作以解决复杂的挑战。可以利用三种主要策略来实现这一点：
- en: '**Look within**: Exploit the existing resources on a computer by using the
    hundreds of cores available on a GPU to execute large-scale algorithms. For instance,
    a data scientist aiming to train an intricate deep learning model could harness
    the GPU’s power to augment computing capabilities.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向内看**：通过利用计算机上的现有资源，使用GPU上成百上千的核心来执行大规模算法。例如，一位数据科学家希望训练一个复杂的深度学习模型时，可以利用GPU的计算能力来增强计算能力。'
- en: '**Look outside**: Implement distributed computing to access supplementary computing
    resources that can collaboratively address large-scale issues. Examples include
    cluster computing and cloud computing, which enable running complex, resource-demanding
    algorithms by leveraging distributed resources.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向外看**：实施分布式计算以访问补充的计算资源，这些资源可以协同解决大规模问题。例子包括集群计算和云计算，它们通过利用分布式资源，使得运行复杂且资源需求高的算法成为可能。'
- en: '**Hybrid strategy**: Merge distributed computing with GPU acceleration on each
    node to expedite algorithm execution. A scientific research organization processing
    vast amounts of data and conducting sophisticated simulations might adopt this
    approach. As illustrated in *Figure 15.3*, the computational workload is distributed
    across multiple nodes (**Node 1**, **Node 2**, and **Node 3**), each equipped
    with its own GPU. This figure effectively demonstrates the hybrid strategy, showcasing
    how simulations and computations are accelerated by capitalizing on the advantages
    of both distributed computing and GPU acceleration within each node:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合策略**：将分布式计算与每个节点上的GPU加速结合，以加快算法执行。一些处理大量数据并进行复杂模拟的科研组织可能会采用这种方法。如*图 15.3*所示，计算负载被分配到多个节点（**节点
    1**、**节点 2** 和 **节点 3**）上，每个节点都配备有自己的GPU。该图有效展示了混合策略，展示了如何利用分布式计算和GPU加速的优势，推动每个节点中的模拟和计算的加速：'
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B18046_15_03.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图，描述自动生成，信心中等](img/B18046_15_03.png)'
- en: 'Figure 15.3: Hybrid strategy for multi-resource processing'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.3：多资源处理的混合策略
- en: As we explore the potential of parallel computing in running large-scale algorithms,
    it is equally important to understand the theoretical limitations that govern
    its efficiency.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索并行计算在运行大规模算法中的潜力时，理解支配其效率的理论限制同样重要。
- en: In the following section, we will delve into the fundamental constraints of
    parallel computing, shedding light on the factors that influence its performance
    and the extent to which it can be optimized.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将深入探讨并行计算的基本约束，揭示影响其性能的因素，以及它可以被优化的程度。
- en: Understanding theoretical limitations of parallel computing
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解并行计算的理论限制
- en: It is important to note that parallel algorithms are not a silver bullet. Even
    the best-designed parallel architectures may not give the performance that we
    may expect. The complexities of parallel computing, such as communication overhead
    and synchronization, make it challenging to achieve optimal efficiency. One law
    that has been developed to help navigate these complexities and better understand
    the potential gains and limitations of parallel algorithms is Amdahl’s law.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，并行算法并不是万能的。即使是设计最好的并行架构，也可能无法达到我们预期的性能。并行计算的复杂性，如通信开销和同步问题，使得实现最佳效率变得具有挑战性。为了帮助我们应对这些复杂性，并更好地理解并行算法的潜在收益和局限性，提出了阿姆达尔定律。
- en: Amdahl’s law
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阿姆达尔定律
- en: Gene Amdahl was one of the first people to study parallel processing in the
    1960s. He proposed Amdahl’s law, which is still applicable today and is a basis
    on which to understand the various trade-offs involved when designing a parallel
    computing solution. Amdahl’s law provides a theoretical limit on the maximum improvement
    in execution time that can be achieved with a parallelized version of an algorithm,
    given the proportion of the algorithm that can be parallelized.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基因·阿姆达尔（Gene Amdahl）是最早研究并行处理的人之一，他在20世纪60年代提出了阿姆达尔定律，该定律至今仍然适用，并成为理解设计并行计算解决方案时涉及的各种权衡的基础。阿姆达尔定律提供了一个理论极限，即在给定可并行化部分的算法下，使用并行化版本的算法能够实现的最大执行时间改进。
- en: It is based on the concept that in any computing process, not all processes
    can be executed in parallel. There will be a sequential portion of the process
    that cannot be parallelized.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 它基于这样一个概念：在任何计算过程中，并非所有过程都可以并行执行。总会有一部分过程是顺序执行的，无法并行化。
- en: Deriving Amdahl’s law
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推导阿姆达尔定律
- en: Consider an algorithm or task that can be divided into a parallelizable fraction
    (*f*) and a serial fraction (*1 - f*). The parallelizable fraction refers to the
    portion of the task that can be executed simultaneously across multiple resources
    or processors. These tasks don’t depend on each other and can be run in parallel,
    hence the term “parallelizable.” On the other hand, the serial fraction is part
    of the task that cannot be divided and must be executed in sequence, one after
    the other, hence “serial.”
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个可以分为并行部分（*f*）和串行部分（*1 - f*）的算法或任务。并行部分指的是可以在多个资源或处理器上同时执行的任务部分。这些任务相互独立，可以并行执行，因此称为“并行可分”。另一方面，串行部分是任务中无法拆分的部分，必须按顺序依次执行，因此称为“串行”。
- en: 'Let *Tp(1)* represent the time required to process this task on a single processor.
    This can be expressed as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 令 *Tp(1)* 表示在单个处理器上处理该任务所需的时间。可以表达为：
- en: '*T*[p]*(1) = N(1 - f)**τ*[p] *+ N(f)**τ*[p] *= N**τ*[p]'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*T*[p]*(1) = N(1 - f)**τ*[p] *+ N(f)**τ*[p] *= N**τ*[p]'
- en: 'In these equations, *N* and *τ**p* denote:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方程中，*N* 和 *τ*[p] 表示：
- en: '*N*: The total number of tasks or iterations that the algorithm or task must
    perform, consistent across both single and parallel processors.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*：算法或任务必须执行的任务总数或迭代次数，在单个处理器和并行处理器上是一致的。'
- en: '*τ*[p]: The time taken by a processor to complete a single unit of work, task,
    or iteration, which remains constant regardless of the number of processors used.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*τ*[p]：处理器完成单个工作单元、任务或迭代所需的时间，不论使用多少个处理器，这个时间保持不变。'
- en: The preceding equation calculates the total time taken to process all tasks
    on a single processor. Now, let’s examine a scenario where the task is executed
    on *N* parallel processors.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程计算了在单个处理器上处理所有任务所需的总时间。现在，让我们看看任务在*N*个并行处理器上执行的情况。
- en: The time taken for this execution can be represented as *T*[p]*(N)*. In the
    following diagram, on the X-axis, we have **Number of processors**. This represents
    the number of computing units or cores used to execute our program. As we move
    right along the X-axis, we are increasing the number of processors used. The Y-axis
    represents **Speedup**. This is a measure of how much faster our program runs
    with multiple processors compared to using just one. As we move up the Y-axis,
    the speed of our program increases proportionally, resulting in more efficient
    task execution.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 执行所需的时间可以表示为 *T*[p]*(N)*。在下面的图示中，X轴表示**处理器数量**，即执行我们程序所使用的计算单元或核心的数量。当我们沿着X轴向右移动时，使用的处理器数量增加。Y轴表示**加速比**，这是衡量使用多个处理器时程序运行速度相比仅使用一个处理器时的提升程度。当我们沿着Y轴向上移动时，程序的速度成比例地增加，导致任务执行效率更高。
- en: The graph in *Figure 15.4* and Amdahl’s law show us that more processors can
    improve performance, but there’s a limit due to the sequential part of our code.
    This principle is a classic example of diminishing returns in parallel computing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.4* 中的图表和阿姆达尔定律向我们展示了，更多的处理器可以提高性能，但由于代码中的串行部分，性能提升是有限的。这个原理是并行计算中收益递减的经典例子。'
- en: '*N = N(1 - f)**τ*[p] *+ (f)**τ*[p]'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*N = N(1 - f)**τ*[p] *+ (f)**τ*[p]'
- en: Here, the first term on the **RHS** (**Right Hand Side**) represents the time
    taken to process the serial part of the task, while the second term denotes the
    time taken to process the parallel part.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**RHS**（**右侧**）的第一项表示处理任务的串行部分所需的时间，而第二项表示处理并行部分所需的时间。
- en: 'The speedup in this case is due to the distribution of the parallel part of
    the task over *N* processors. Amdahl’s law defines the speedup *S(N)* achieved
    by using *N* processors as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，加速比是由于任务的并行部分分布到*N*个处理器上。阿姆达尔定律定义了使用*N*个处理器时实现的加速比*S(N)*，公式为：
- en: '![](img/B18046_15_001.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_15_001.png)'
- en: 'For significant speedup, the following condition must be satisfied:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于显著的加速，必须满足以下条件：
- en: '*1 - f << f / N (4.4)*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*1 - f << f / N (4.4)*'
- en: This inequality indicates that the parallel portion (*f*) must be very close
    to unity, especially when *N* is large.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个不等式表示并行部分（*f*）必须非常接近于1，尤其是在*N*很大的时候。
- en: 'Now, let’s look at a typical graph that explains Amdahl’s law:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一个典型的图表，解释阿姆达尔定律：
- en: '![Chart  Description automatically generated](img/B18046_15_04.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18046_15_04.png)'
- en: 'Figure 15.4: Diminishing returns in parallel processing: visualizing Amdahl’s
    law'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.4：并行处理中的收益递减：可视化阿姆达尔定律
- en: In *Figure 15.4*, the X-axis represents the number of processors (*N*), corresponding
    to the computing units or cores used to execute the program. As we move to the
    right along the X-axis, *N* increases. The Y-axis denotes the speedup (*S*), a
    measure of the improvement in the program’s execution time *T*[p] with multiple
    processors compared to using just one. Moving up the Y-axis indicates an increase
    in the program’s execution speed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 15.4* 中，X 轴表示处理器的数量 (*N*)，对应用于执行程序的计算单元或核心。沿着 X 轴向右移动，*N* 增加。Y 轴表示加速比 (*S*)，这是一个衡量程序在多个处理器上执行时相较于只使用一个处理器执行的时间
    *T*[p] 改进的指标。沿 Y 轴向上移动表示程序执行速度的提升。
- en: 'The graph features four lines, each representing the speedup *S* obtained from
    parallel processing for different percentages of the parallelizable fraction (*f*):
    50%, 75%, 90%, and 95%:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了四条线，每条线代表不同并行化比例 (*f*) 下获得的加速比 *S*，分别为 50%、75%、90% 和 95%：
- en: '50% parallel (*f = 0.5*): This line exhibits the smallest speedup *S*. Although
    more processors (*N*) are added, half of the program runs sequentially, limiting
    the speedup to a maximum of 2.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '50% 并行 (*f = 0.5*): 这一行展示了最小的加速比 *S*。尽管添加了更多的处理器 (*N*)，程序的一半仍然按顺序执行，限制了加速比的最大值为
    2。'
- en: '75% parallel (*f = 0.75*): The speedup *S* is higher compared to the 50% case.
    However, 25% of the program still runs sequentially, constraining the overall
    speedup.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '75% 并行 (*f = 0.75*): 与 50% 的情况相比，加速比 *S* 更高。然而，程序的 25% 仍然是顺序执行，这限制了整体加速比。'
- en: '90% parallel (*f = 0.9*): In this case, a significant speedup *S* is observed.
    Nevertheless, the sequential 10% of the program imposes a limit on the speedup.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '90% 并行 (*f = 0.9*): 在这种情况下，观察到显著的加速比 *S*。然而，程序的 10% 顺序部分对加速比施加了限制。'
- en: '95% parallel (*f = 0.95*): This line demonstrates the highest speedup *S*.
    Yet, the sequential 5% still imposes an upper limit on the speedup.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '95% 并行 (*f = 0.95*): 这一行展示了最高的加速比 *S*。然而，顺序执行的 5% 仍然对加速比施加了上限。'
- en: The graph, in conjunction with Amdahl’s law, highlights that while increasing
    the number of processors (*N*) can enhance performance, there exists an inherent
    limit due to the sequential part of the code (*1 - f*). This principle serves
    as a classic illustration of diminishing returns in parallel computing.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该图与阿姆达尔定律结合，强调了虽然增加处理器数量 (*N*) 可以提高性能，但由于代码中的顺序部分 (*1 - f*)，仍然存在一个固有的限制。这个原则是并行计算中收益递减的经典示例。
- en: 'Amdahl’s law provides valuable insights into the potential performance gains
    achievable in multiprocessor systems and the importance of the parallelizable
    fraction (*f*) in determining the system’s overall speedup. After discussing the
    theoretical limitations of parallel computing, it is crucial to introduce and
    explore another powerful and widely-used parallel processing technology: the GPU
    and its associated programming framework, CUDA.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 阿姆达尔定律提供了有关多处理器系统中可以实现的性能增益的宝贵见解，并强调并行化部分 (*f*) 在决定系统整体加速比中的重要性。在讨论了并行计算的理论限制之后，重要的是要介绍并探讨另一种强大且广泛使用的并行处理技术：GPU
    及其相关的编程框架 CUDA。
- en: 'CUDA: Unleashing the potential of GPU architectures in parallel computing'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'CUDA: 释放 GPU 架构在并行计算中的潜力'
- en: GPUs were originally designed for graphics processing but have since evolved,
    exhibiting distinct characteristics that set them apart from CPUs and resulting
    in an entirely different computation paradigm.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 最初是为图形处理设计的，但后来发展演变，展现出与 CPU 不同的独特特点，形成了完全不同的计算范式。
- en: Unlike CPUs, which have a limited number of cores, GPUs are composed of thousands
    of cores. It’s essential to recognize, however, that these cores, in isolation,
    are not as individually powerful as a CPU core. But, GPUs are quite efficient
    at executing numerous relatively simple computations in parallel.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 与处理器核心数量有限的 CPU 不同，GPU 由数千个核心组成。然而，需要注意的是，这些核心单独来看并不像 CPU 核心那样强大，但 GPU 在并行执行大量相对简单的计算任务时非常高效。
- en: As GPUs were originally designed for graphic processing, GPU architecture is
    ideal for graphic processing where multiple operations can be executed independently.
    For example, rendering an image involves the computation of color and brightness
    for each pixel in the image. These calculations are largely independent of each
    other and hence can be conducted simultaneously, leveraging the multi-core architecture
    of the GPU.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPU最初是为图形处理设计的，GPU架构非常适合图形处理，其中多个操作可以独立执行。例如，渲染图像涉及对每个像素的颜色和亮度进行计算。这些计算彼此基本独立，因此可以同时进行，充分利用GPU的多核架构。
- en: Bottom of form
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表格底部
- en: 'This design choice allows GPUs to be extremely efficient at tasks they’re designed
    for, like rendering graphics and processing large datasets. Here is the architecture
    of GPUs shown in *Figure 15.5*:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计选择使得GPU在它们所设计的任务上变得极其高效，比如渲染图形和处理大规模数据集。下面是*图15.5*所示的GPU架构：
- en: '![A picture containing table  Description automatically generated](img/B18046_15_05.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含表格的图片，描述自动生成](img/B18046_15_05.png)'
- en: 'Figure 15.5: Architecture of GPUs'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5：GPU架构
- en: This unique architecture is not only beneficial for graphics processing but
    also significantly advantageous for other types of computational problems. Any
    problem that can be segmented into smaller, independent tasks can exploit this
    architecture for faster processing. This includes domains like scientific computing,
    machine learning, and even cryptocurrency mining, where massive datasets and complex
    computations are the norm.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这种独特的架构不仅有利于图形处理，还对其他类型的计算问题具有显著的优势。任何可以分解成较小、独立任务的问题，都可以利用这种架构进行更快速的处理。这包括像科学计算、机器学习，甚至是加密货币挖矿等领域，这些领域的数据集庞大且计算复杂。
- en: Soon after GPUs became mainstream, data scientists started exploring them for
    their potential to efficiently perform parallel operations. As a typical GPU has
    thousands of **Arithmetic Logic Units** (**ALUs**), it has the potential to spawn
    thousands of concurrent processes. Note that the ALU is the workhorse of the core
    that performs most of the actual computations. This large number of ALUs makes
    GPUs well suited to tasks where the same operation needs to be performed on many
    data points simultaneously, such as vector and matrix operations common in data
    science and machine learning. Hence, algorithms that can perform parallel computations
    are best suited to GPUs. For example, an object search in a video is known to
    be at least 20 times faster in GPUs, compared to CPUs. Graph algorithms, which
    were discussed in *Chapter 5*, *Graph Algorithms*, are known to run much faster
    on GPUs than on CPUs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU成为主流之后，数据科学家开始探索它们在高效执行并行操作方面的潜力。由于典型的GPU拥有成千上万的**算术逻辑单元**（**ALUs**），它有潜力产生成千上万的并发进程。需要注意的是，ALU是核心的主力部件，负责执行大部分实际的计算。大量的ALU使得GPU非常适合执行需要对许多数据点同时进行相同操作的任务，例如数据科学和机器学习中常见的向量和矩阵运算。因此，能够执行并行计算的算法最适合在GPU上运行。例如，在GPU上进行视频中的对象搜索，速度至少比CPU快20倍。*第五章*中讨论的图算法，已知在GPU上的运行速度远快于CPU。
- en: In 2007, NVIDIA developed the open-source framework called **Compute Unified
    Device Architecture** (**CUDA**) to enable data scientists to harness the power
    of GPUs for their algorithms. CUDA abstracts the CPU and GPU as the host and the
    device, respectively.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 2007年，NVIDIA开发了一个名为**计算统一设备架构**（**CUDA**）的开源框架，以便数据科学家能够利用GPU的强大计算能力来处理他们的算法。CUDA将CPU和GPU分别抽象为主机和设备。
- en: '**Host** refers to the CPU and main memory, responsible for executing the main
    program and offloading data-parallel computations to the GPU.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**主机**指的是CPU和主内存，负责执行主程序并将数据并行计算任务卸载到GPU上。'
- en: '**Device** refers to the GPU and its memory (VRAM), responsible for executing
    kernels that perform data-parallel computations.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**设备**指的是GPU及其内存（VRAM），负责执行执行数据并行计算的内核。'
- en: In a typical CUDA program, the host allocates memory on the device, transfers
    input data, and invokes a kernel. The device performs the computation, and the
    results are stored back in its memory. The host then retrieves the results. This
    division of labor leverages the strengths of each component, with the CPU handling
    complex logic and the GPU managing large-scale, data-parallel computations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的CUDA程序中，主机在设备上分配内存，传输输入数据并调用内核。设备执行计算，结果存储回其内存。主机随后检索结果。通过这种劳动分工，充分发挥了每个组件的优势，CPU处理复杂逻辑，GPU负责大规模数据并行计算。
- en: CUDA runs on NVIDIA GPUs and requires OS kernel support, initially starting
    with Linux and later extending to Windows. The CUDA Driver API bridges the programming
    language API and the CUDA driver, with support for C, C++, and Python.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA在NVIDIA GPU上运行，并需要操作系统内核的支持，最初从Linux开始，后来扩展到Windows。CUDA驱动API连接了编程语言API和CUDA驱动，支持C、C++和Python。
- en: 'Parallel processing in LLMs: A case study in Amdahl’s law and diminishing returns'
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM中的并行处理：一个关于阿姆达尔定律和收益递减的案例研究
- en: LLMs, like ChatGPT, are intricate systems that generate text remarkably similar
    to human-written prose, given an initial prompt. This task involves a series of
    complex operations, which can be broadly divided into sequential and parallelizable
    tasks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 像ChatGPT这样的LLM是复杂的系统，能够根据给定的初始提示生成与人类写作非常相似的文本。这项任务涉及一系列复杂的操作，可以大致分为顺序任务和可并行化任务。
- en: Sequential tasks are those that must occur in a specific order, one after the
    other. These tasks may include preprocessing steps like tokenizing, where the
    input text is broken down into smaller pieces, often words or phrases, which the
    model can understand. It may also encompass post-processing tasks like decoding,
    where the model’s output, often in the form of token probabilities, is translated
    back into human-readable text. These sequential tasks are critical to the function
    of the model, but by nature, they cannot be split up and run concurrently.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序任务是指那些必须按照特定顺序依次进行的任务。这些任务可能包括像分词这样的预处理步骤，其中输入文本被拆分成更小的部分，通常是单词或短语，模型能够理解。它还可能包括像解码这样的后处理任务，在这些任务中，模型的输出（通常是以词元概率的形式呈现）被转化回人类可读的文本。这些顺序任务对模型的功能至关重要，但由于其本质，它们无法拆分并同时执行。
- en: On the other hand, parallelizable tasks are those that can be broken down and
    run simultaneously. A key example of this is the forward propagation stage in
    the model’s neural network. Here, computations for each layer in the network can
    be performed concurrently. This operation constitutes the vast majority of the
    model’s computation time, and it is here where the power of parallel processing
    can be harnessed.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，能够并行化的任务是那些可以拆分并同时运行的任务。一个典型的例子是模型神经网络中的前向传播阶段。在这里，网络中每一层的计算可以并行执行。这个操作构成了模型大部分的计算时间，正是在这里可以发挥并行处理的强大优势。
- en: Now, assume that we’re working with a GPU that has 1,000 cores. In the context
    of language models, the parallelizable portion of the task might involve the forward
    propagation stage, where computations for each layer in the neural network can
    be performed concurrently. Let’s posit that this constitutes 95% of the total
    computation time. The remaining 5% of the task, which could involve operations
    like tokenizing and decoding, is sequential and cannot be parallelized.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们使用的是一款拥有1000个核心的GPU。在语言模型的背景下，任务中可以并行化的部分可能涉及前向传播阶段，在这个阶段中，神经网络每一层的计算可以并行执行。我们假设这占总计算时间的95%。其余5%的任务可能涉及诸如分词和解码之类的操作，这些是顺序进行的，无法并行化。
- en: 'Applying Amdahl’s law to this scenario gives us:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 将阿姆达尔定律应用于此场景给出了以下结果：
- en: '*Speedup = 1 / ((1 - 0.95) + 0.95/1000) = 1 / (0.05 + 0.00095) = 19.61*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*加速 = 1 / ((1 - 0.95) + 0.95/1000) = 1 / (0.05 + 0.00095) = 19.61*'
- en: Under ideal circumstances, this indicates that our language processing task
    could be completed about 19.61 times faster on a 1,000-core GPU than on a single-core
    CPU.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，这表明我们的语言处理任务在1000核心的GPU上比在单核CPU上要快约19.61倍。
- en: 'To further illustrate the diminishing returns of parallel computing, let’s
    adjust the number of cores to 2, 50, and 100:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明并行计算的收益递减，我们来调整核心数量，分别为2、50和100：
- en: 'For 2 cores: *Speedup = 1 / ((1 - 0.95) + 0.95/2) = 1.67*'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于2个核心：*加速 = 1 / ((1 - 0.95) + 0.95/2) = 1.67*
- en: 'For 50 cores: *Speedup = 1 / ((1 - 0.95) + 0.95/50) = 14.71*'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于50个核心：*加速 = 1 / ((1 - 0.95) + 0.95/50) = 14.71*
- en: 'For 100 cores: *Speedup = 1 / ((1 - 0.95) + 0.95/100) = 16.81*'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于100个核心：*加速 = 1 / ((1 - 0.95) + 0.95/100) = 16.81*
- en: As seen from our calculations, adding more cores to a parallel computing setup
    doesn’t lead to an equivalent increase in speedup. This is a prime example of
    the concept of diminishing returns in parallel computing. Despite doubling the
    number of cores from 2 to 4, or increasing them 50-fold from 2 to 100, the speedup
    doesn’t double or increase 50 times. Instead, the speedup hits a theoretical limit
    as per Amdahl’s law.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的计算结果来看，向并行计算环境中添加更多核心并不会导致速度的等效提升。这是并行计算中收益递减概念的一个典型例子。即使将核心数量从2增加到4，或者从2增加到100时增加50倍，速度提升也不会翻倍或增加50倍。相反，速度提升会根据阿姆达尔定律达到一个理论限制。
- en: The primary factor behind this diminishing return is the existence of a non-parallelizable
    portion in the task. In our case, operations like tokenizing and decoding form
    this sequential part, accounting for 5% of the total computation time. No matter
    how many cores we add to the system or how efficiently we can carry out the parallelizable
    part, this sequential portion places an upper limit on the achievable speedup.
    It will always be there, demanding its share of the computation time.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这种收益递减的主要原因是任务中存在无法并行化的部分。在我们的例子中，像标记化和解码这样的操作构成了这一顺序部分，占总计算时间的5%。无论我们向系统添加多少核心，或者我们能多高效地执行并行化部分，这一顺序部分都会对可实现的加速造成上限。它将始终存在，要求占用其计算时间份额。
- en: Amdahl’s law elegantly captures this characteristic of parallel computing. It
    states that the maximum potential speedup using parallel processing is dictated
    by the non-parallelizable part of the task. The law serves as a reminder to algorithm
    designers and system architects that while parallelism can dramatically speed
    up computation, it is not an infinite resource to be tapped for speed. It underscores
    the importance of identifying and optimizing the sequential components of an algorithm
    in order to maximize the benefits of parallel processing.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 阿姆达尔定律优雅地捕捉了并行计算的这一特性。它指出，使用并行处理的最大潜在加速由任务中无法并行化的部分决定。该定律提醒算法设计师和系统架构师，尽管并行性可以显著加速计算，但它并不是一个可以无限利用来提高速度的资源。它强调了识别和优化算法中顺序部分的重要性，以便最大化并行处理的优势。
- en: This understanding is particularly important in the context of LLMs, where the
    sheer scale of computations makes efficient resource utilization a key concern.
    It underlines the need for a balanced approach that combines parallel computing
    strategies with efforts to optimize the performance of the sequential parts of
    the task.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模语言模型（LLM）的背景下，这一理解尤为重要，因为计算的庞大规模使得高效的资源利用成为一个关键问题。它强调了需要一种平衡的方法，将并行计算策略与优化任务中顺序部分的性能的努力结合起来。
- en: Rethinking data locality
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新思考数据局部性
- en: Traditionally, in parallel and distributed processing, the data locality principle
    is pivotal in deciding the optimal resource allocation. It fundamentally suggests
    that the movement of data should be discouraged in distributed infrastructures.
    Whenever possible, instead of moving data, it should be processed locally on the
    node where it resides; otherwise, it reduces the benefit of parallelization and
    horizontal scaling, where horizontal scaling is the process of increasing a system’s
    capacity by adding more machines or nodes to distribute the workload, enabling
    it to handle higher amounts of traffic or data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的并行和分布式处理中，数据局部性原理在决定最优资源分配方面至关重要。它从根本上表明，在分布式基础设施中应尽量避免数据移动。只要可能，数据应在其所在节点上本地处理，而不是移动数据；否则，它将减少并行化和水平扩展的好处，其中水平扩展是通过增加更多机器或节点来分配工作负载，从而提高系统容量，使其能够处理更高的流量或数据量。
- en: As networking bandwidth has improved over the years, the limitations imposed
    by data locality have become less significant. The increased data transfer speeds
    enable efficient communication between nodes in a distributed computing environment,
    reducing the reliance on data locality for performance optimization. The network
    bandwidth can be quantified by the network bisection bandwidth, which is the bandwidth
    between two equal parts of a network. This is important in distributed computing
    with resources that are physically distributed. If we draw a line somewhere between
    two sets of resources in a distributed network, the bisectional bandwidth is the
    rate of communication at which servers on one side of the line can communicate
    with servers on the other side, as shown in *Figure 15.6*. For distributed computing
    to work efficiently, this is the most important parameter to consider. If we do
    not have enough network bisection bandwidth, the benefits gained by the availability
    of multiple execution engines in distributed computing will be overshadowed by
    slow communication links.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 随着网络带宽多年来的提升，数据局部性带来的限制变得不再那么显著。更高的数据传输速度使得分布式计算环境中节点之间的通信更加高效，从而减少了对数据局部性进行性能优化的依赖。网络带宽可以通过网络分段带宽来量化，分段带宽是指网络中两部分之间的带宽。这在资源物理分布的分布式计算中尤为重要。如果我们在分布式网络中的两组资源之间画一条线，分段带宽就是指一侧的服务器与另一侧的服务器之间的通信速度，如
    *图 15.6* 所示。为了使分布式计算高效运行，这是需要考虑的最重要参数。如果没有足够的网络分段带宽，分布式计算中多个执行引擎所带来的好处将被缓慢的通信链路所掩盖。
- en: '![A picture containing text  Description automatically generated](img/B18046_15_06.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text  Description automatically generated](img/B18046_15_06.png)'
- en: 'Figure 15.6: Bisection bandwidth'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.6：分段带宽
- en: A high bisectional bandwidth enables us to process the data where it is without
    copying it. These days, major cloud computing providers offer exceptional bisection
    bandwidth. For example, within a Google data center, the bisection bandwidth is
    as high as 1 petabyte per second. Other major Cloud vendors offer similar bandwidth.
    In contrast, a typical enterprise network might only provide bisection bandwidth
    in the range of 1 to 10 gigabytes per second.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 高分段带宽使我们能够在数据所在的地方进行处理，而无需复制数据。如今，主要的云计算提供商提供卓越的分段带宽。例如，在 Google 数据中心，分段带宽高达每秒
    1 petabyte。其他主要云供应商也提供类似的带宽。相比之下，典型的企业网络可能只提供每秒 1 到 10 gigabytes 的分段带宽。
- en: This vast difference in speed demonstrates the remarkable capabilities of modern
    Cloud infrastructure, making it well suited to large-scale data processing tasks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这种速度上的巨大差异展示了现代云基础设施的卓越能力，使其非常适合大规模数据处理任务。
- en: The increased petabit bisectional bandwidth has opened up new options and design
    patterns for efficiently storing and processing big data. These new options include
    alternative methods and design patterns that have become viable due to the increased
    network capacity, enabling faster and more efficient data processing.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 增加的宠物比特分段带宽为高效存储和处理大数据开辟了新的选项和设计模式。这些新选项包括由于网络容量的增加而变得可行的替代方法和设计模式，使得数据处理变得更快速、更高效。
- en: Benefiting from cluster computing using Apache Spark
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用 Apache Spark 进行集群计算的优势
- en: Apache Spark is a widely used platform for managing and leveraging cluster computing.
    In this context, “cluster computing” involves grouping together several machines
    and making them work together as a single system to solve a problem. Spark doesn’t
    merely implement this; it creates and controls these clusters to achieve high-speed
    data processing.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个广泛使用的平台，用于管理和利用集群计算。在这个背景下，“集群计算”是指将多台机器组合在一起，使它们作为一个单一的系统共同工作以解决问题。Spark
    不仅仅实现了这一点，它还创建并控制这些集群以实现高速数据处理。
- en: Within Apache Spark, data undergoes a transformation into what’s known as **Resilient
    Distributed Datasets** (**RDDs**). These are effectively the backbone of Apache
    Spark’s data abstraction.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Apache Spark 中，数据会转换成被称为 **弹性分布式数据集** (**RDDs**) 的形式。这些实际上是 Apache Spark 数据抽象的核心。
- en: RDDs are immutable, meaning they can’t be altered once created, and are collections
    of elements that can be processed in parallel. In other words, different pieces
    of these datasets can be worked on at the same time, thereby accelerating data
    processing.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: RDD是不可变的，意味着一旦创建后，它们无法被更改，是可以并行处理的元素集合。换句话说，这些数据集的不同部分可以同时进行处理，从而加速数据处理过程。
- en: When we say “fault-tolerant,” we mean that RDDs have the ability to recover
    from potential failures or errors during execution. This makes them robust and
    reliable for big data processing tasks. They’re split into smaller chunks known
    as “partitions,” which are then distributed across various nodes or individual
    computers in the cluster. The size of these partitions can vary and is primarily
    determined by the nature of the task and the configuration of the Spark application.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说“容错”时，指的是RDD具有从执行过程中的潜在失败或错误中恢复的能力。这使得它们在大数据处理任务中具有强大的可靠性和稳健性。RDD被划分为多个较小的块，称为“分区”，然后分布在集群中的多个节点或独立计算机上。这些分区的大小可以变化，主要由任务的性质和Spark应用的配置决定。
- en: Spark’s distributed computing framework allows the tasks to be distributed across
    multiple nodes, which can significantly improve processing speed and efficiency.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的分布式计算框架使得任务可以分布在多个节点上，从而显著提高处理速度和效率。
- en: The Spark architecture is composed of several main components, including the
    driver program, executor, worker node, and cluster manager.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Spark架构由多个主要组件组成，包括驱动程序、执行器、工作节点和集群管理器。
- en: '**Driver program**: The driver program is a key component in a Spark application,
    functioning much like the control center of operations. It resides in its own
    separate process, often located on a machine known as the driver machine. The
    driver program’s role is like that of a conductor of an orchestra; it runs the
    primary Spark program and oversees the many tasks within it.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**驱动程序**：驱动程序是Spark应用中的关键组件，功能类似于操作的控制中心。它存在于一个独立的进程中，通常位于称为驱动机器的机器上。驱动程序的角色类似于管弦乐队的指挥；它运行主Spark程序，并监督其中的众多任务。'
- en: Among the main tasks of the driver program are handling and running the SparkSession.
    The SparkSession is crucial to the Spark application as it wraps around the SparkContext.
    The SparkContext is like the central nervous system of the Spark application –
    it’s the gateway for the application to interact with the Spark computational
    ecosystem.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 驱动程序的主要任务之一是处理和运行SparkSession。SparkSession对于Spark应用至关重要，因为它封装了SparkContext。SparkContext就像Spark应用的中枢神经系统——它是应用与Spark计算生态系统交互的门户。
- en: To simplify, imagine the Spark application as an office building. The driver
    program is like the building manager, responsible for overall operation and maintenance.
    Within this building, the SparkSession represents an individual office, and the
    SparkContext is the main entrance to that office. The essence is that these components
    – the driver program, SparkSession, and SparkContext – work together to coordinate
    tasks and manage resources within a Spark application. The SparkContext is packed
    with fundamental functions and context information that is pre-loaded at the start
    time of the application. Moreover, it carries vital details about the cluster,
    such as its configuration and status, which are crucial for the application to
    run and execute tasks effectively.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了简化理解，可以将Spark应用比作一栋办公大楼。驱动程序就像大楼管理员，负责整体运作和维护。在这栋大楼中，SparkSession代表一个独立的办公室，而SparkContext是通往该办公室的主要入口。关键是，这些组件——驱动程序、SparkSession和SparkContext——协同工作，以协调任务并管理Spark应用中的资源。SparkContext包含应用启动时预加载的基本功能和上下文信息。此外，它还携带关于集群的重要细节，如配置和状态，这对于应用的运行和任务的有效执行至关重要。
- en: '**Cluster manager**: The driver program interacts seamlessly with the cluster
    manager. The cluster manager is an external service that provides and manages
    resources across the cluster, such as computing power and memory. The driver program
    and the cluster manager work hand in hand to identify available resources in the
    cluster, allocate them effectively, and manage their usage throughout the life
    cycle of the Spark application.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群管理器**：驱动程序与集群管理器无缝互动。集群管理器是一个外部服务，负责提供和管理集群中的资源，如计算能力和内存。驱动程序和集群管理器密切合作，以识别集群中可用的资源，进行有效分配，并在Spark应用的生命周期内管理其使用。'
- en: '**Executors**: An executor refers to a dedicated computational process that
    is spawned specifically for an individual Spark application running on a node
    within a cluster. Each of these executor processes operates on a worker node,
    effectively acting as the computational “muscle” behind your Spark application.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行器**：执行器是指专门为在集群中某个节点上运行的单个 Spark 应用程序而启动的计算进程。每个执行器进程都运行在工作节点上，实际上充当着 Spark
    应用程序背后的计算“肌肉”。'
- en: The sharing of both memory and global parameters in this way can significantly
    enhance the speed and efficiency of task execution, making Spark a highly performant
    framework for big data processing.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以这种方式共享内存和全局参数可以显著提高任务执行的速度和效率，使得 Spark 成为一个高性能的大数据处理框架。
- en: '**Worker node**: A worker node, true to its name, is charged with carrying
    out the actual execution of tasks within the distributed Spark system.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作节点**：工作节点，顾名思义，负责在分布式 Spark 系统中执行任务的实际操作。'
- en: 'Each worker node is capable of hosting multiple executors, which in turn can
    serve numerous Spark applications:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个工作节点能够托管多个执行器，这些执行器又可以为多个 Spark 应用程序提供服务：
- en: '![A diagram of a cluster manager  Description automatically generated with
    medium confidence](img/B18046_15_07.png)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![集群管理器的示意图 自动生成的描述，信心中等](img/B18046_15_07.png)'
- en: 'Figure 15.7: Spark’s distributed architecture'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 15.7：Spark 的分布式架构
- en: How Apache Spark empowers large-scale algorithm processing
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 如何支持大规模算法处理
- en: Apache Spark has emerged as a leading platform for processing and analyzing
    big data, thanks to its powerful distributed computing capabilities, fault-tolerant
    nature, and ease of use. In this section, we will explore how Apache Spark empowers
    large-scale algorithm processing, making it an ideal choice for complex, resource-intensive
    tasks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 已成为处理和分析大数据的领先平台，这得益于其强大的分布式计算能力、容错特性和易用性。在本节中，我们将探讨 Apache Spark
    如何支持大规模算法处理，使其成为复杂、资源密集型任务的理想选择。
- en: Distributed computing
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式计算
- en: At the core of Apache Spark’s architecture is the concept of data partitioning,
    which allows data to be divided across multiple nodes in a cluster. This feature
    enables parallel processing and efficient resource utilization, both of which
    are crucial for running large-scale algorithms. Spark’s architecture comprises
    a driver program and multiple executor processes distributed across worker nodes.
    The driver program is responsible for managing and distributing tasks across the
    executors, while each executor runs multiple tasks concurrently in separate threads,
    allowing for high throughput.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 架构的核心概念是数据分区，这使得数据可以在集群中的多个节点之间分配。这个特性使得并行处理和高效的资源利用成为可能，而这两者对于运行大规模算法至关重要。Spark
    的架构由一个驱动程序和分布在工作节点上的多个执行器进程组成。驱动程序负责管理并分配任务到各个执行器，而每个执行器则在多个线程中并行运行任务，从而实现高吞吐量。
- en: In-memory processing
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存处理
- en: One of Spark’s standout features is its in-memory processing capability. Unlike
    traditional disk-based systems, Spark can cache intermediate data in memory, significantly
    speeding up iterative algorithms that require multiple passes over the data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的一大亮点是其内存处理能力。与传统的基于磁盘的系统不同，Spark 可以将中间数据缓存到内存中，显著加速需要多次遍历数据的迭代算法。
- en: This in-memory processing capability is particularly beneficial for large-scale
    algorithms, as it minimizes the time spent on disk I/O, leading to faster computation
    times and more efficient use of resources.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种内存处理能力对于大规模算法尤为有利，因为它最小化了磁盘 I/O 的时间，从而加快了计算速度，并更高效地利用资源。
- en: Using large-scale algorithms in cloud computing
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云计算中使用大规模算法
- en: The rapid growth of data and the increasing complexity of machine learning models
    have made distributed model training an essential component of modern deep learning
    pipelines. Large-scale algorithms demand vast amounts of computational resources
    and necessitate efficient parallelism to optimize their training times. Cloud
    computing offers an array of services and tools that facilitate distributed model
    training, allowing you to harness the full potential of resource-hungry, large-scale
    algorithms.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的快速增长以及机器学习模型日益复杂，使得分布式模型训练成为现代深度学习管道中不可或缺的一部分。大规模算法需要大量的计算资源，并且需要高效的并行处理来优化其训练时间。云计算提供了一系列服务和工具，支持分布式模型训练，使你能够充分发挥资源密集型大规模算法的潜力。
- en: 'Some of the key advantages of using the Cloud for distributed model training
    include:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用云计算进行分布式模型训练的一些关键优势包括：
- en: '**Scalability**: The Cloud provides virtually unlimited resources, allowing
    you to scale your model training workloads to meet the demands of large-scale
    algorithms.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：云计算提供几乎无限的资源，使你能够根据大规模算法的需求扩展模型训练工作负载。'
- en: '**Flexibility**: The Cloud supports a wide range of machine learning frameworks
    and libraries, enabling you to choose the most suitable tools for your specific
    needs.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：云计算支持多种机器学习框架和库，使你能够选择最适合你特定需求的工具。'
- en: '**Cost-effectiveness**: With the Cloud, you can optimize your training costs
    by selecting the right instance types and leveraging spot instances to reduce
    expenses.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性价比**：使用云计算，你可以通过选择合适的实例类型和利用抢占实例来优化培训成本，从而降低开支。'
- en: Example
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: As we delve deeper into machine learning models, especially those dealing with
    **Natural Language Processing** (**NLP**) tasks, we notice an increasing need
    for computational resources. For instance, transformers like GPT-3 for large-scale
    language modeling tasks can have billions of parameters, demanding substantial
    processing power and memory. Training such models on colossal datasets, such as
    Common Crawl, which contains billions of web pages, further escalates these requirements.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们深入研究机器学习模型，尤其是处理**自然语言处理**（**NLP**）任务的模型时，我们发现对计算资源的需求越来越大。例如，像 GPT-3 这样的变压器模型，用于大规模语言建模任务，可能拥有数十亿个参数，需求巨大的处理能力和内存。在庞大的数据集上训练这样的模型，如包含数十亿网页的
    Common Crawl，进一步加剧了这些需求。
- en: Cloud computing emerges as a potent solution here. It offers services and tools
    for distributed model training, enabling us to access an almost infinite pool
    of resources, scale our workloads, and select the most suitable machine learning
    frameworks. What’s more, cloud computing facilitates cost optimization by providing
    flexible instance types and spot instances – essentially bidding for spare computing
    capacity. By delegating these resource-heavy tasks to the cloud, we can concentrate
    more on innovative work, speeding up the training process, and developing more
    powerful models.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算在这里成为了一种强有力的解决方案。它提供了分布式模型训练的服务和工具，使我们能够访问几乎无限的资源池，扩展工作负载，并选择最适合的机器学习框架。更重要的是，云计算通过提供灵活的实例类型和抢占实例来促进成本优化——本质上是在竞标空闲的计算能力。通过将这些资源密集型任务委托给云计算，我们可以更加专注于创新工作，加快训练过程，并开发更强大的模型。
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we examined the concepts and principles of large-scale and
    parallel algorithm design. The pivotal role of parallel computing was analyzed,
    with particular emphasis on its capacity to effectively distribute computational
    tasks across multiple processing units. The extraordinary capabilities of GPUs
    were studied in detail, illustrating their utility in executing numerous threads
    concurrently. Moreover, we discussed distributed computing platforms, specifically
    Apache Spark and cloud computing environments. Their importance in facilitating
    the development and deployment of large-scale algorithms was underscored, providing
    a robust, scalable, and cost-effective infrastructure for high-performance computations.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们探讨了大规模并行算法设计的概念和原理。我们分析了并行计算的重要作用，特别是它在将计算任务有效地分配到多个处理单元方面的能力。详细研究了GPU的非凡能力，展示了它们在并发执行大量线程时的实用性。此外，我们还讨论了分布式计算平台，特别是Apache
    Spark和云计算环境。它们在促进大规模算法的开发和部署方面的重要性被强调，为高性能计算提供了强大、可扩展且具成本效益的基础设施。
- en: Learn more on Discord
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入本书的 Discord 社区——在这里你可以分享反馈、向作者提问，并了解新版本——请扫描下面的二维码：
- en: '[https://packt.link/WHLel](https://packt.link/WHLel)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/WHLel](https://packt.link/WHLel)'
- en: '![](img/QR_Code1955211820597889031.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1955211820597889031.png)'
