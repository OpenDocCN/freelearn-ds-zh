- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Introduction to Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: In the previous chapter, we learned how to use modern machine learning models
    to tackle time series forecasting. Now, let’s focus our attention on a subfield
    of machine learning that has shown a lot of promise in the last few years—**deep
    learning**. We will be trying to demystify deep learning and go into why it is
    popular nowadays. We will also break down deep learning into major components
    and learn about the workhorse behind deep learning—gradient descent.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何使用现代机器学习模型来解决时间序列预测问题。现在，让我们把注意力集中在机器学习的一个子领域——近年来表现出巨大潜力的**深度学习**上。我们将试图揭开深度学习的神秘面纱，探讨为什么它现在如此流行。我们还将把深度学习分解成主要的组成部分，学习支撑深度学习的“核心力量”——梯度下降。
- en: 'In this chapter, we will be covering these main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主要内容：
- en: What is deep learning and why now?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是深度学习，为什么是现在？
- en: Components of a deep learning system
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习系统的组成部分
- en: Representation learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示学习
- en: Linear layers and activation functions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性层和激活函数
- en: Gradient descent
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need to set up the **Anaconda** environment following the instructions
    in the *Preface* of the book to get a working environment with all the libraries
    and datasets required for the code in this book. Any additional libraries will
    be installed while running the notebooks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要按照本书*前言*中的说明设置**Anaconda**环境，以便获得一个包含所有代码所需库和数据集的工作环境。在运行笔记本时，任何额外的库都会被自动安装。
- en: The associated code for the chapter can be found at [https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter11](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter11).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章相关代码可以在[https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter11](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter11)找到。
- en: What is deep learning and why now?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是深度学习，为什么是现在？
- en: 'In *Chapter 5*, *Time Series Forecasting as Regression*, we talked about machine
    learning and borrowed a definition from Arthur Samuel: “*Machine Learning is a
    field of study that gives computers the ability to learn without being explicitly
    programmed*.” And we further saw how we can learn useful functions from data using
    machine learning. Deep learning is a subfield of this same field of study. The
    objective of deep learning is also to learn useful functions from data but with
    a few specifications on how it does that.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5章*，*时间序列预测作为回归*中，我们讨论了机器学习，并借用了Arthur Samuel的定义：“*机器学习是一个让计算机无需明确编程即可学习的研究领域*。”接着我们进一步探讨了如何通过机器学习从数据中学习有用的函数。深度学习是这个研究领域的一个子领域。深度学习的目标同样是从数据中学习有用的函数，但它在实现方法上有一些特定的要求。
- en: Before we talk about what is special about deep learning, let’s answer another
    question first. Why are we talking about this subfield of machine learning as
    a separate topic? The answer to that lies in the unreasonable effectiveness of
    deep learning methods in countless applications. Deep learning has taken the world
    of machine learning by storm, overthrowing state-of-the-art systems across types
    of data such as images, videos, text, and so on. If you remember the speech recognition
    systems on phones a decade ago, they were more meme-worthy than really useful.
    But today, you can say *Hey Google, play Pink Floyd*, and *Comfortably Numb* will
    start playing on your phone or speakers. Multiple deep learning systems made this
    process possible in a smooth way. The voice assistant on your phone, self-driving
    cars, web search, language translation—the list of applications of deep learning
    in our day-to-day lives just keeps on going.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论深度学习的特别之处之前，我们先回答另一个问题。为什么我们要把这个机器学习的子领域单独作为一个话题来讨论？答案就在于深度学习方法在众多应用中的不合理有效性。深度学习已经风靡机器学习领域，推翻了各种类型数据（如图像、视频、文本等）上的最先进系统。如果你记得十年前的手机语音识别系统，它们当时更多是用来娱乐的，而不是真的实用。但如今，你可以说*嘿，谷歌，播放Pink
    Floyd*，然后*Comfortably Numb*会在你的手机或扬声器上播放。多个深度学习系统使这个过程得以流畅实现。手机上的语音助手、自动驾驶汽车、网络搜索、语言翻译——深度学习在我们日常生活中的应用清单不断扩展。
- en: By now, you might be wondering what this new technology called deep learning
    is all about, right? Deep learning is not a new technology. The origins of deep
    learning can be traced way back to the late 1940s and early 1950s. It only appears
    to be new because of the recent surge in popularity of the field.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能在想，这种新技术“深度学习”到底是怎么回事，对吧？其实，深度学习并不是一项新技术。深度学习的起源可以追溯到20世纪40年代末和50年代初。它之所以显得新颖，是因为近年来该领域的流行度急剧上升。
- en: Let’s quickly see why deep learning is suddenly popular.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解一下为什么深度学习突然变得如此流行。
- en: Why now?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么是现在？
- en: 'There are two main reasons why deep learning has gained a lot of ground in
    the last two decades:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在过去二十年里取得了显著进展，主要有两个原因：
- en: Increase in compute availability
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算能力的增加
- en: Increase in data availability
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可用性的增加
- en: Let’s discuss the preceding points in detail in the following sections.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细讨论前述的各个要点。
- en: Increase in compute availability
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算能力的增加
- en: Back in 1960, Frank Rosenblatt wrote a paper (Reference *5*) about a three-layer
    neural network and stated that it went a long way in demonstrating the ability
    of neural networks as a pattern-recognizing device. But in the same paper, he
    noted that the burden on a digital computer (of the 1960s) was too great as we
    increased the number of connections. However, in the decades that followed, computer
    hardware showed close to 50,000 times more improvement, which provided a good
    boost to neural networks and deep learning. However, it was still not enough as
    neural networks were still not considered to be good enough for *large-scale applications*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 早在1960年，Frank Rosenblatt就写了一篇论文（参考文献*5*）讨论了一个三层神经网络，并表示这项工作在证明神经网络作为模式识别设备的能力方面做出了重要贡献。但在同一篇论文中，他指出，当我们增加连接数时，1960年代的数字计算机承载的负担过于沉重。然而，在接下来的几十年里，计算机硬件几乎提高了50,000倍，这为神经网络和深度学习提供了强大的推动力。然而，仍然不足够，因为神经网络当时仍然不被认为适合*大规模应用*。
- en: This is when a particular type of hardware, which was initially developed for
    gaming, came to the rescue—GPUs. It’s not entirely clear who started using GPUs
    for deep learning. Kyoung-Su Oh and Keechul Jung published a paper titled *GPU
    implementation of neural networks* back in 2004, which seems to be the first to
    show massive speed-ups in using GPUs for deep learning. One of the earliest and
    more popular research papers on the topic came from Rajat Raina, Anand Madhavan,
    and Andrew Ng, who published a paper titled *Large-scale deep unsupervised learning
    using graphics processors* back in 2009\. It showed the effectiveness of GPUs
    for deep learning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，一种最初为游戏开发的特定硬件——GPU（图形处理单元）开始发挥作用。虽然尚不完全清楚是谁首先将GPU用于深度学习，但Kyoung-Su Oh和Keechul
    Jung于2004年发表了一篇名为*GPU实现神经网络*的论文，这篇论文似乎是首个展示GPU在深度学习中能带来显著加速的研究。有关此话题的早期且较为流行的研究论文来自Rajat
    Raina、Anand Madhavan和Andrew Ng，他们在2009年发布了一篇名为*使用图形处理器进行大规模深度无监督学习*的论文，证明了GPU在深度学习中的有效性。
- en: Although many groups led by LeCun, Schmidhuber, Bengio, and so on were playing
    around with using GPUs, the turning point came when Alex Krizhevsky, Ilya Sutskever,
    and Geoffrey E. Hinton used a GPU-based deep learning system that outperformed
    all the other competing technologies in an image recognition contest called the
    *ImageNet Large Scale Visual Recognition Challenge 2012*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多由LeCun、Schmidhuber、Bengio等人领导的团队一直在尝试使用GPU，但转折点出现在Alex Krizhevsky、Ilya Sutskever和Geoffrey
    E. Hinton使用基于GPU的深度学习系统，该系统在一个名为*ImageNet大规模视觉识别挑战2012*的图像识别竞赛中超越了所有其他竞争技术。
- en: The introduction of GPUs provided a much-needed boost to the widespread use
    of deep learning and accelerated the progress in the field.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: GPU的引入为深度学习的广泛应用提供了急需的推动力，并加速了该领域的进展。
- en: '**Reference check**:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research papers *GPU implementation of neural networks*, *Large-scale deep
    unsupervised learning using graphics processors*, and *ImageNet Classification
    with Deep Convolutional Neural Networks* are cited in the *References* section
    under *1*, *2*, and *3*, respectively.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 论文*GPU实现神经网络*、*使用图形处理器进行大规模深度无监督学习*和*使用深度卷积神经网络进行ImageNet分类*分别在*参考文献*部分的*1*、*2*和*3*中被引用。
- en: Increase in data availability
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据可用性的增加
- en: In addition to the skyrocketing compute capability, the other main factor that
    helped deep learning was the sheer increase in data. As the world became more
    and more digitized, the amount of data that we generated increased drastically.
    Tables that had hundreds and thousands of rows now exploded into millions and
    billions of rows, and the ever-decreasing cost of storage helped this explosion
    of data collection.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算能力的飞速提升，深度学习的另一个主要推动因素是数据量的急剧增加。随着世界越来越数字化，我们生成的数据量也大幅增加。曾经只有几百或几千行的表格，如今已经膨胀到数百万、数十亿行，而存储成本的不断降低也促进了数据收集的爆炸式增长。
- en: And why would an increase in data availability help deep learning? This lies
    in the way deep learning works. Deep learning is quite data-hungry and needs large
    amounts of data to learn good models. Therefore, if we keep increasing the data
    that we provide to a deep learning model, the model will be able to learn better
    and better functions. However, the same can’t be said for traditional machine
    learning models. Let’s cement this learning with a chart that Andrew Ng, a world-renowned
    machine learning educator and an adjunct professor at Stanford, popularized in
    his famous machine learning course—*Machine Learning by Stanford University* on
    Coursera ([https://www.coursera.org/specializations/machine-learning-introduction](https://www.coursera.org/specializations/machine-learning-introduction))
    (*Figure 11.1*).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么数据量的增加对深度学习有帮助呢？这与深度学习的工作方式有关。深度学习对数据的需求非常大，需要大量的数据来学习出优秀的模型。因此，如果我们不断增加提供给深度学习模型的数据量，模型将能够学习到越来越好的函数。然而，传统机器学习模型并非如此。我们可以通过安德鲁·吴（Andrew
    Ng）在他著名的机器学习课程——斯坦福大学的*Machine Learning by Stanford University*（Coursera）中推广的一张图表来加深理解（[https://www.coursera.org/specializations/machine-learning-introduction](https://www.coursera.org/specializations/machine-learning-introduction)）（*图
    11.1*）。
- en: '![Figure 11.1 – Deep learning versus traditional machine learning as we increase
    the data size ](img/B22389_11_01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 随着数据量增加，深度学习与传统机器学习的对比](img/B22389_11_01.png)'
- en: 'Figure 11.1: Deep learning versus traditional machine learning as we increase
    the data size'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：随着数据量增加，深度学习与传统机器学习的对比
- en: In *Figure 11.1*, which was popularized by Andrew Ng, we can see that as we
    increase the data size, traditional machine learning hits a plateau and won’t
    improve anymore.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在安德鲁·吴推广的*图 11.1*中，我们可以看到，随着数据量的增加，传统机器学习会达到一个平台期，之后不再有所改善。
- en: It has been proven empirically that there are significant benefits to the overparameterization
    of a deep learning model. **Overparameterization** means that there are more parameters
    in the model than the number of data points available to train. In classical statistics,
    this is a big no-no because, under this scenario, the model invariably overfits.
    But deep learning seems to flaunt this rule with ease. One of the examples of
    overparameterization is the current state-of-the-art image recognition system,
    **NoisyStudent**. It has 480 million parameters, but it was trained on *ImageNet*
    with 1.2 million data points.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过经验已经证明，深度学习模型的过参数化具有显著的优势。**过参数化**是指模型中的参数数量超过了用于训练的数据点数量。在经典统计学中，这是一个大忌，因为在这种情况下，模型不可避免地会过拟合。然而，深度学习似乎能够轻松地挑战这一规则。一个过参数化的例子是当前最先进的图像识别系统——**NoisyStudent**。它拥有4.8亿个参数，但是在包含120万个数据点的*ImageNet*上进行训练的。
- en: 'It has been argued that the way deep learning models are trained (stochastic
    gradient descent, which we will be explaining soon) is the key because it has
    a regularizing effect. In a research paper titled *The Computational Limits of
    Deep Learning*, Niel C. Thompson and others tried to illustrate this using a simple
    experiment. They set up a dataset with 1,000 features, but only 10 of them had
    any signal in them. Then, they tried to learn four models based on the dataset
    using varying dataset sizes:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有人认为，深度学习模型的训练方式（随机梯度下降法，稍后会详细解释）是关键，因为它具有正则化效应。在一篇名为《深度学习的计算极限》的研究论文中，Niel
    C. Thompson等人尝试通过一个简单的实验来说明这一点。他们设置了一个包含1000个特征的数据集，但只有其中10个特征具有信号。然后，他们尝试基于不同的数据集大小，使用该数据集学习四个不同的模型：
- en: '**Oracle model**: A model that uses the exact 10 parameters that have any signal
    in them.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Oracle 模型**：使用精确的10个参数，这些参数包含所有信号。'
- en: '**Expert model**: A model that uses 9 out of 10 significant parameters.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专家模型**：使用10个显著参数中的9个。'
- en: '**Flexible model**: A model that uses all 1,000 parameters.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活模型**：使用所有1000个参数。'
- en: '**Regularized model**: A model that uses all 1,000 parameters, but is now a
    regularized (**lasso**) model. (We covered regularization back in *Chapter 8*,
    *Forecasting Time Series with Machine Learning Models*.)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化模型**：一个使用所有1,000个参数的模型，但现在是一个正则化的（**lasso**）模型。（我们在*第8章*《使用机器学习模型预测时间序列》中讨论过正则化。）'
- en: 'Let’s see *Figure 11.2* from the research paper with the study:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看研究论文中的*图 11.2*：
- en: '![Figure 11.2 – The chart shows how different models perform under different
    sizes of data ](img/B22389_11_02.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 该图展示了不同模型在不同数据量下的表现](img/B22389_11_02.png)'
- en: 'Figure 11.2: The chart shows how different models perform under different sizes
    of data'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：该图展示了不同模型在不同数据量下的表现
- en: The chart has a number of data points used on the *x*-axis and the performance
    (*-log(Mean Squared Error)*) on the *y*-axis. The different colored lines show
    the different types of models. The Oracle model sets the upper limit for learning
    because it has access to perfect information.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 该图的横坐标表示数据点，纵坐标表示性能（*-log(均方误差)*）。不同颜色的线条表示不同类型的模型。Oracle模型设置了学习的上限，因为它能够访问完美的信息。
- en: The expert model plateaus because there is a definite lack of information as
    it doesn’t have access to one of the ten features that are important. The flexible
    model (which uses all 1,000 features) takes a large number of data points to start
    recognizing the important ones but still keeps approaching the Oracle performance
    as the data size gets bigger. The regularized model (which is a proxy for deep
    learning models) keeps improving as we give the model more and more data. This
    model uses regularization to figure out which of these 1,000 features is relevant
    to the problem and starts to leverage them with much fewer data points, and performance
    keeps increasing with more data points. This strengthens the concept Andrew Ng
    popularized once more—with more data, deep learning starts to outperform traditional
    machine learning.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 专家模型在某一水平上停滞，因为它缺乏信息，并且无法访问十个重要特征中的一个。灵活模型（使用所有1,000个特征）需要大量的数据点才能开始识别重要特征，但随着数据量的增加，它仍然趋近于Oracle模型的表现。正则化模型（作为深度学习模型的代表）随着我们给模型提供越来越多的数据而不断改进。该模型通过正则化来识别哪些特征与问题相关，并开始以较少的数据点有效利用这些特征，性能随着数据点的增加而持续提升。这再次印证了Andrew
    Ng曾经推广的概念——随着数据量的增加，深度学习开始超越传统机器学习。
- en: A lot more factors, apart from compute and data availability, have contributed
    to the success of deep learning. Sara Hooker, in her essay *The Hardware Lottery*
    (Reference *9*), talks about how an idea wins not necessarily because it is superior
    to other ideas but because it is suited to the software and hardware available
    at the time. And once a research direction wins the lottery, it snowballs because
    more funding and big research organizations get behind that idea and it eventually
    becomes the most prominent idea in the space of ideas.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算能力和数据可用性之外，许多其他因素也推动了深度学习的成功。Sara Hooker在她的文章《硬件彩票》（参考文献*9*）中谈到了一个观点：一个想法之所以能够成功，不一定是因为它优于其他想法，而是因为它适合当时的软件和硬件环境。而一旦一个研究方向赢得了“彩票”，它就会像滚雪球一样发展，因为更多的资金和大型研究机构会支持这一想法，最终它会成为该领域最突出的思想。
- en: We have talked about deep learning for some time but have still not understood
    what it is. Let’s do that now.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了深度学习一段时间，但仍未真正理解它是什么。现在我们来了解一下。
- en: What is deep learning?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是深度学习？
- en: 'There is no single definition of deep learning because it means slightly different
    things to different people. However, a large majority of people agree on one thing:
    a model is called deep learning when it involves automatic feature learning from
    raw data. As Yoshua Bengio (a Turing Award winner and one of the *godfathers*
    of AI) explains in his 2021 paper titled *Deep Learning of Representations for
    Unsupervised and Transfer Learning*:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习没有单一的定义，因为它对不同的人来说意味着略有不同的东西。然而，绝大多数人达成了一个共识：当一个模型能够从原始数据中自动学习特征时，它就被称为深度学习。正如Yoshua
    Bengio（图灵奖得主，AI的*教父*之一）在他2021年发表的论文《无监督学习与迁移学习的深度表示学习》中所解释的：
- en: ”Deep learning algorithms seek to exploit the unknown structure in the input
    distribution in order to discover good representations, often at multiple levels,
    with higher-level learned features defined in terms of lower-level features.”
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “深度学习算法试图利用输入分布中未知的结构，以发现良好的表示，通常是在多个层次上，较高层次的学习特征是通过较低层次特征定义的。”
- en: 'In a 2016 presentation, *Deep Learning and Understandability versus Software
    Engineering and Verification*, Peter Norvig, the Director of Research at Google,
    had a similar but simpler definition:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年的一次演讲《*深度学习与可理解性对比软件工程与验证*》中，谷歌研究总监Peter Norvig给出了一个类似但更简单的定义：
- en: ”A kind of learning where the representation you form have (sic) several levels
    of abstraction, rather than a direct input to output.”
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “一种学习方式，其中你所构建的表示具有多个抽象层次，而不是直接的输入到输出。”
- en: 'Another key feature of deep learning a lot of people agree upon is compositionality.
    Yann LeCun, a Turing Award winner and another one of the *godfathers* of AI, has
    a slightly more complex but more exact definition of deep learning (tweet from
    `@ylecun` on January 9^(th), 2020):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的另一个关键特性是合成性，很多人都认同这一点。杨·勒昆（Yann LeCun），图灵奖得主，以及AI的另一位*奠基人*，给出了一个略微复杂但更准确的深度学习定义（2020年1月9日来自`@ylecun`的推文）：
- en: '”DL is methodology: building a model by assembling parameterized modules into
    (possibly dynamic) graphs and optimizing it with gradient-based methods.”'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “深度学习是一种方法论：通过将参数化模块组装成（可能是动态的）图形，并通过基于梯度的方法进行优化来构建模型。”
- en: 'The key points we would like to highlight here are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在此强调的关键点如下：
- en: '**Assembling parametrized modules**: This refers to the compositionality of
    deep learning. Deep learning systems, as we will shortly see, are composed of
    a few submodules with a few parameters (some without) assembled into a graph-like
    structure.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组装参数化模块**：这指的是深度学习的合成性。正如我们稍后将看到的，深度学习系统由一些具有多个参数（有些没有）的子模块组装而成，形成类似图形的结构。'
- en: '**Optimizing it with gradient-based methods**: Although having gradient-based
    learning as a sufficient criterion for deep learning is not widely accepted, we
    can still see empirically that, today, most successful deep learning systems are
    trained using gradient-based methods. (If you are not aware of what a gradient-based
    optimization method is, don’t worry. We will be covering it soon in this chapter.)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用基于梯度的方法进行优化**：尽管将基于梯度的学习方法作为深度学习的充分标准并未得到广泛认可，但我们从经验上看到，今天大多数成功的深度学习系统都是通过基于梯度的方法进行训练的。（如果你不了解什么是基于梯度的优化方法，不用担心。我们会在本章中很快涉及这一内容。）'
- en: If you have read anything about deep learning before, you may have seen neural
    networks and deep learning used together or interchangeably. But we haven’t talked
    about neural networks till now. Before we do that, let’s look at a fundamental
    unit of any neural network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前读过有关深度学习的内容，可能已经见过神经网络和深度学习一起使用或互换使用。但直到现在我们才讨论神经网络。在此之前，让我们先看一下任何神经网络的基本单元。
- en: Perceptron, the first neural network
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知机——第一个神经网络
- en: A lot of what we call deep learning and neural networks are deeply influenced
    by the human brain and its inner workings. Although recent studies have shown
    very little similarity between human brains and artificial neural networks, the
    seed behind the idea was inspired by human biology. The human desire to create
    intelligent beings like themselves was manifested as early as back in Greek mythology
    (Galatea and Pandora). And owing to this desire, humans have studied and looked
    for inspiration from human anatomy for years. One of the organs of the human body
    that has been studied intensely is the brain because it is the center of intelligence,
    creativity, and everything else that makes a human.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的深度学习和神经网络，很多都深受人类大脑及其内部运作的影响。尽管最近的研究表明人类大脑与人工神经网络之间几乎没有相似之处，但这一理念背后的种子却受到人类生物学的启发。人类想要创造像自己一样的智能生命的愿望早在希腊神话中就有所体现（如加拉提亚与潘多拉）。正因如此，人类多年来一直在研究并从人类解剖学中寻找灵感。大脑作为人体的一个器官，一直是被深入研究的对象，因为它是智慧、创造力及人类一切功能的核心。
- en: 'Even though we still don’t know a lot about the brain, we do know a bit about
    it, and we use that little information to design artificial systems. The fundamental
    unit of the human brain is something we call a **neuron**, as shown here:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们对于大脑的了解依然有限，但我们确实知道一些基本信息，并利用这些信息来设计人工系统。人脑的基本单元就是我们所称之为**神经元**，如图所示：
- en: '![Figure 11.3 – A biological neuron ](img/B22389_11_03.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – 生物神经元](img/B22389_11_03.png)'
- en: 'Figure 11.3: A biological neuron'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3：生物神经元
- en: 'Many of you might have come across this in biology or in the context of machine
    learning as well. But let’s refresh this anyway. The biological neuron has the
    following parts:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你们中的许多人可能已经在生物学或机器学习的相关领域接触过这个概念。但我们还是要再回顾一下。生物神经元有以下几个部分：
- en: '**Dendrites** are branched extensions of the nerve cell that collect inputs
    from surrounding cells or other neurons.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树突**是神经细胞的分支延伸部分，收集来自周围细胞或其他神经元的输入。'
- en: '**Soma**, or the cell body, collects these inputs, joins them, and is passed
    on.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索玛**，即细胞体，收集这些输入，结合它们并传递出去。'
- en: '**The** **axon hillock** connects the soma to the axon, and it controls the
    firing of the neuron. If the strength of a signal exceeds a threshold, the axon
    hillock fires an electrical signal through the axon.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轴突丘**连接细胞体和轴突，并控制神经元的放电。如果信号的强度超过阈值，轴突丘就会通过轴突放电信号。'
- en: '**Axon** is the fiber that connects the soma to the nerve endings. It is the
    axon’s duty to pass on the electrical signal to the endpoints.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轴突**是连接细胞体和神经末梢的纤维。它的职责是将电信号传递到终端。'
- en: '**Synapses** are the end points of the nerve cell and transmit the signal to
    other nerve cells.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**突触**是神经细胞的终端，并将信号传递给其他神经细胞。'
- en: 'McCulloch and Pitts (1943) were the first to design a mathematical model for
    the biological neuron. However, the McCulloch-Pitts model had a few limitations:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: McCulloch和Pitts（1943）是第一个设计生物神经元数学模型的人。然而，McCulloch-Pitts模型有几个局限性：
- en: It only accepted binary variables.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它只接受二进制变量。
- en: It considered all input variables equally important.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它认为所有输入变量同等重要。
- en: There was only one parameter, a threshold, which was not learnable.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个参数，阈值，这个参数是不可学习的。
- en: In 1957, Frank Rosenblatt generalized the McCulloch-Pitts model and made it
    a full model whose parameters could be learned. The similarity between modern
    deep learning networks and the human brain ends here. The fundamental unit of
    learning that started this line of research was inspired by human biology and
    was a rather cheap imitation of it as well.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 1957年，Frank Rosenblatt推广了McCulloch-Pitts模型，并使其成为一个完整的模型，其参数可以被学习。现代深度学习网络与人脑的相似性到此为止。开始这一研究方向的学习基本单元受人类生物学的启发，同时它也是对人类生物学的一个相当廉价的模仿。
- en: '**Reference check**:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The original research paper for Frank Rosenblatt’s *perceptron* is cited in
    *References* under reference *5*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Frank Rosenblatt的原始研究论文《感知机》在**参考文献**中列为参考文献*5*。
- en: 'Let’s understand the perceptron in detail because it is the fundamental building
    block of all neural networks:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解感知机，因为它是所有神经网络的基本构建块：
- en: '![Figure 11.4 – Perceptron ](img/B22389_11_04.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4 – 感知机](img/B22389_11_04.png)'
- en: 'Figure 11.4: Perceptron'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：感知机
- en: 'As we see from *Figure 11.4*, the perceptron has the following components:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图11.4*所示，感知机有以下组成部分：
- en: '**Inputs**: These are the real-valued inputs that are fed to a perceptron.
    This is like the dendrites in neurons that collect the input.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：这些是传递给感知机的实值输入，就像神经元中的树突一样，收集输入信号。'
- en: '**Weighted sum**: Each input is multiplied by a corresponding weight and summed
    up. The weights determine the importance of each input in determining the outcome.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权和**：每个输入乘以对应的权重并求和。权重决定了每个输入在确定结果时的重要性。'
- en: '**Non-linearity**: The weighted sum goes through a non-linear function. For
    the original perceptron, it was a step function with a threshold activation. The
    output would be positive or negative based on the weighted sum and the threshold
    of the unit. Modern-day perceptrons and neural networks use different kinds of
    activation functions, but we will see that later on.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非线性**：加权和通过一个非线性函数。对于原始的感知机，它是一个带有阈值激活的阶跃函数。输出将根据加权和和单元的阈值为正或负。现代的感知机和神经网络使用不同种类的激活函数，但我们稍后会看到这些。'
- en: 'We can write the perceptron in the mathematical form as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将感知机写成如下数学形式：
- en: '![](img/B22389_11_05.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_11_05.png)'
- en: 'Figure 11.5: Perceptron, a math perspective'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：感知机，数学视角
- en: 'As shown in *Figure 11.5*, the perceptron output is defined by the weighted
    sum of inputs, which is passed in through a non-linear function. Now, we can think
    of this using linear algebra as well. This is an important perspective for two
    reasons:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图11.5*所示，感知机的输出由输入的加权和定义，并通过一个非线性函数传递。现在，我们也可以通过线性代数来理解这一点。这是一个重要的视角，原因有二：
- en: The linear algebra perspective will help you understand neural networks faster.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性代数的视角将帮助你更快地理解神经网络。
- en: It will also make the whole thing feasible because matrix multiplications are
    something that our modern-day computers and GPUs are really good at. Without linear
    algebra, multiplying these inputs with corresponding weights would require us
    to loop through the inputs, and it quickly becomes infeasible.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这也使整个过程变得可行，因为矩阵乘法是我们现代计算机和GPU擅长的事情。没有线性代数，将这些输入与相应权重相乘将要求我们循环遍历输入，这很快变得不可行。
- en: '**Linear algebra intuition recap**'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**线性代数直觉回顾**'
- en: Let’s take a look at a couple of concepts as a refresher. Feel free to jump
    ahead if you are already aware of vectors, vector spaces, and matrix multiplication.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们回顾一些概念。如果您已经了解向量、向量空间和矩阵乘法，请随时跳过。
- en: '**Vectors and vector spaces**'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**向量和向量空间**'
- en: 'At the superficial level, a **vector** is an array of numbers. However, in
    linear algebra, a vector is an entity that has both magnitude and direction. Let’s
    take an example to elucidate:'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在表面上，**向量**是一个数字数组。然而，在线性代数中，向量是一个具有大小和方向的实体。让我们举个例子来阐明：
- en: '![](img/B22389_11_001.png)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B22389_11_001.png)'
- en: We can see that this is an array of numbers. But if we plot this point in the
    two-dimensional coordinate space, we get a point. And if we draw a line from the
    origin to this point, we will get an entity with direction and magnitude. This
    is a vector.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到这是一个数字数组。但是，如果我们在二维坐标空间中绘制这一点，我们会得到一个点。如果我们从原点到这一点画一条线，我们将得到一个具有方向和大小的实体。这就是一个向量。
- en: The two-dimensional coordinate space is called a **vector space**. A two-dimensional
    vector space, informally, is all the possible vectors with two entries. Extending
    it to *n*-dimensions, an *n*-dimensional vector space is all the possible vectors
    with *n* entries.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 二维坐标空间称为**向量空间**。一个二维向量空间，在非正式情况下，是所有具有两个条目的可能向量。将其扩展到*n*维，一个*n*维向量空间是所有具有*n*个条目的可能向量。
- en: 'The final intuition I want to leave with you is this: *a vector is a point
    in the n-dimensional vector space*.'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我想留给您的最终直觉是：*向量是n维向量空间中的一个点*。
- en: '**Matrices and transformations**'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**矩阵和变换**'
- en: 'Again, at the superficial level, a **matrix** is a rectangular arrangement
    of numbers that looks like this:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次，在表面上，**矩阵**是一个看起来像这样的数字的矩形排列：
- en: '![](img/B22389_11_002.png)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B22389_11_002.png)'
- en: Matrices have many uses but the one intuition that is most relevant for us is
    that a matrix specifies a linear transformation of the vector space it resides
    in. When we multiply a vector with a matrix, we are essentially transforming the
    vector, and the values and dimensions of the matrix define the kind of transformation
    that happens. Depending on the content of the matrix, it does *rotation, reflection,
    scaling, shearing*, and so on.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 矩阵有许多用途，但对我们最相关的直觉是，矩阵指定了它所在的向量空间的线性变换。当我们将一个向量与一个矩阵相乘时，我们实质上是在转换向量，矩阵的值和维度定义了发生的变换类型。根据矩阵的内容，它可以进行*旋转、反射、缩放、剪切*等操作。
- en: We have included a notebook in the `Chapter11` folder titled `01-Linear_Algebra_Intuition.ipynb`,
    which explores matrix multiplication as a transformation. We also apply these
    transformation matrices to vector spaces to develop intuition on how matrix multiplication
    can rotate and warp the vector spaces.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在`Chapter11`文件夹中包含了一个名为`01-Linear_Algebra_Intuition.ipynb`的笔记本，其中探讨了矩阵乘法作为一种转换。我们还将这些转换矩阵应用于向量空间，以发展关于矩阵乘法如何旋转和扭曲向量空间的直觉。
- en: I highly suggest heading over to the *Further reading* section, where we have
    given a few resources to get started and solidify necessary intuition.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我强烈建议您前往*进一步阅读*部分，在那里我们提供了一些资源来开始并巩固必要的直觉。
- en: 'If we consider the inputs as vectors in the feature space (vector space with
    *m*-dimensions), the term ![](img/B22389_11_003.png) is nothing but a linear combination
    of input vectors. We can rewrite the equation in vector form as below:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将输入视为特征空间（具有*m*维的向量空间）中的向量，则术语![](img/B22389_11_003.png)不过是输入向量的线性组合。我们可以将方程重写为以下向量形式：
- en: '![](img/B22389_11_004.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_11_004.png)'
- en: where, ![](img/B22389_11_005.png)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B22389_11_005.png)
- en: The bias is also included here as a dummy input with a fixed value of 1 and
    adding ![](img/B22389_11_006.png) to the ![](img/B22389_11_007.png) vector.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置也包括在这里作为一个固定值为1的虚拟输入，并将![](img/B22389_11_006.png)添加到![](img/B22389_11_007.png)向量中。
- en: Now that we have had an introduction to deep learning, let us recall one of
    the aspects of deep learning we discussed earlier—compositionality—and explore
    it a bit more deeply in the next section.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经初步了解了深度学习，让我们回顾一下我们之前讨论的深度学习的一个方面——组合性——并在下一节中对其进行更深入的探讨。
- en: Components of a deep learning system
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习系统的组成部分
- en: 'Let us recall Yann LeCun’s definition of deep learning:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下 Yann LeCun 对深度学习的定义：
- en: '”Deep learning is a methodology: building a model by assembling parameterized
    modules into (possibly dynamic) graphs and optimizing it with gradient-based methods.”'
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “深度学习是一种方法论：通过将参数化模块组装成（可能是动态的）图，并使用基于梯度的方法优化它，来构建模型。”
- en: 'The core idea here is that deep learning is an extremely modular system. Deep
    learning is not just one model but, rather, a language to express any model in
    terms of a few parametrized modules with these specific properties:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的核心思想是，深度学习是一个高度模块化的系统。深度学习不仅仅是一个模型，而是一种语言，通过几个具有特定属性的参数化模块来表达任何模型：
- en: It should be able to produce an output from a given input through a series of
    computations.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它应该能够通过一系列计算从给定的输入中产生输出。
- en: If the desired output is given, it should be able to pass on information to
    its inputs on how to change, to arrive at the desired output. For instance, if
    the output is lower than what is desired, the module should be able to tell its
    inputs to change in some direction so that the output becomes closer to the desired
    one.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果给定了期望的输出，它应该能够将信息传递给其输入，告诉它们如何改变，以达到期望的输出。例如，如果输出低于预期，模块应该能够告诉其输入在某个方向上进行变化，从而使输出更接近期望的结果。
- en: The more mathematically inclined may have figured out the connection to the
    second point of differentiation. And you would be correct. To optimize these kinds
    of systems, we predominantly use gradient-based optimization methods. Therefore,
    condensing the two properties into one, we can say that these parameterized modules
    should be *differentiable functions*.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数学倾向较强的人可能已经弄明白了与第二个导数点的关联。你是对的。为了优化这类系统，我们主要使用基于梯度的优化方法。因此，将这两个属性合并为一个，我们可以说这些参数化模块应该是*可微分的函数*。
- en: Let’s take the help of a visual to aid further discussion.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们借助一张图来进一步辅助讨论。
- en: '![Figure 11.6 – A deep learning system ](img/B22389_11_06.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 – 深度学习系统](img/B22389_11_06.png)'
- en: 'Figure 11.6: A deep learning system'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6：深度学习系统
- en: As shown in *Figure 11.6*, deep learning can be thought of as a system that
    takes in raw input data through a series of linear and non-linear transforms to
    provide us with an output. It also can adjust its internal parameters to make
    the output as close as possible to the desired output through learning. To make
    the diagram simpler, we have chosen a paradigm that fits most of the popular deep
    learning systems. It all starts with raw input data. The raw input data goes through
    *N* blocks of linear and non-linear functions that do representation learning.
    Let’s explore these blocks in some detail.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 11.6*所示，深度学习可以被看作是一个系统，通过一系列线性和非线性变换从原始输入数据中获取信息，并提供输出。它还可以调整其内部参数，通过学习使输出尽可能接近所期望的输出。为了简化图示，我们选择了一个适用于大多数流行深度学习系统的范式。它的一切从原始输入数据开始。原始输入数据经过*N*个块的线性和非线性函数进行表示学习。让我们详细探讨这些模块。
- en: Representation learning
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示学习
- en: '**Representation learning**, informally, learns the best features by which
    we can make the problem linearly separable. Linearly separable means when we can
    separate the different classes (in a classification problem) with a straight line
    (*Figure 11.7*):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**表示学习**，通俗地说，就是学习最佳特征，使得我们可以使问题变得线性可分。线性可分意味着我们可以用一条直线将不同的类别（在分类问题中）分开（*图
    11.7*）：'
- en: '![Figure 11.7 – Transforming non-linearly separable data into linearly separable
    using a function,  ](img/B22389_11_07.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.7 – 使用一个函数将非线性可分的数据转换为线性可分](img/B22389_11_07.png)'
- en: 'Figure 11.7: Transforming non-linearly separable data into linearly separable
    using a function, ![](img/B22389_07_003.png)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7：使用一个函数将非线性可分的数据转换为线性可分，![](img/B22389_07_003.png)
- en: The *representation learning* block in *Figure 11.6* may have multiple linear
    and non-linear functions stacked on top of each other, and the overall function
    of the block is to learn a function, ![](img/B22389_07_003.png), which transforms
    the raw input into good features that make the problem linearly separable.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.6*中的*表示学习*模块可能有多个线性和非线性函数堆叠在一起，整个模块的功能是学习一个函数，![](img/B22389_07_003.png)，将原始输入转换为使问题线性可分的良好特征。'
- en: Another way to look at this is through the lens of linear algebra. As we explored
    earlier in the chapter, matrix multiplication can be thought of as a linear transformation
    of vectors. And if we extend that intuition to the vector spaces, we can see that
    matrix multiplication warps the vector space in some way or another. When we stack
    multiple linear and non-linear transformations on top of each other, we are essentially
    warping, twisting, and squeezing the input vector space (with the features) into
    another space. When we ask a parameterized system to warp the input space (pixels
    of images) in such a way as to perform a particular task (such as the classification
    of dogs versus cats), the representation learning block learn the right transformations,
    which makes the task (separating cats from dogs) easier.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看待这一问题的方式是通过线性代数的视角。正如我们在本章之前所探讨的，矩阵乘法可以被视为向量的线性变换。如果我们将这种直觉扩展到向量空间上，我们可以看到矩阵乘法在某种程度上会扭曲向量空间。当我们将多个线性和非线性变换叠加时，本质上我们是在扭曲、旋转和压缩输入的向量空间（包含特征），将其映射到另一个空间。当我们要求一个有参数的系统以某种方式扭曲输入空间（如图像的像素），以执行特定任务（例如区分狗与猫的分类），表示学习模块便会学习到正确的变换，从而使任务变得更容易（例如区分猫与狗）。
- en: 'I have created a video illustrating this because nothing establishes intuition
    better than a video of what is happening. I’ve taken a sample dataset that is
    not linearly separable, trained a neural network on the problem to classify, and
    then visualized how the input space was transformed by the model into a linearly
    separable representation. You can find the video here: [https://www.youtube.com/watch?v=5xYEa9PPDTE](https://www.youtube.com/watch?v=5xYEa9PPDTE).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我制作了一个视频来说明这一点，因为没有什么比一个展示实际情况的视频更能帮助建立直觉的了。我使用了一个非线性可分的数据集，训练了一个神经网络来进行分类，然后将模型如何将输入空间转换为线性可分表示进行了可视化。你可以在这里找到这个视频：[https://www.youtube.com/watch?v=5xYEa9PPDTE](https://www.youtube.com/watch?v=5xYEa9PPDTE)。
- en: Now, let’s look inside the representation learning block. We can see there is
    a linear transformation and a non-linear activation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看表示学习模块。我们可以看到其中有一个线性变换和一个非线性激活。
- en: Linear transformation
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性变换
- en: Linear transformations are just transformations that are applied to the vector
    space. When we say linear transformation in a neural network context, we actually
    mean affine transformations.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 线性变换只是应用于向量空间的变换。当我们在神经网络的上下文中提到线性变换时，实际上指的是仿射变换。
- en: A linear transformation fixes the origin while applying the transformation,
    but an affine transformation doesn’t. Rotation, reflection, scaling, and so on
    are purely linear transformations because the origin won’t change while we do
    this. But something like a translation, which moves the vector space, is an affine
    transformation. Therefore, *A***X*^T is a linear transformation, but *A***X*^T
    +*b* is an affine transformation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 线性变换在应用变换时会固定原点，但仿射变换则不会。旋转、反射、缩放等都是纯粹的线性变换，因为在进行这些操作时原点不会发生变化。但像平移这样的操作，会移动向量空间，它就是一种仿射变换。因此，*A***X*^T是线性变换，但*A***X*^T
    +*b*是仿射变换。
- en: So, linear transformations are simply matrix multiplications that transform
    the input vector space, and this is at the heart of any neural network or deep
    learning system today.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，线性变换只是矩阵乘法，它将输入的向量空间进行转换，这也是当今任何神经网络或深度学习系统的核心。
- en: 'What happens if we stack linear transformations on top of each other? For instance,
    we first multiply the input, *X*, with a transformation matrix, *A*, and then
    multiply the results with another transformation matrix, *B*:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将多个线性变换叠加在一起，会发生什么呢？例如，我们首先将输入*X*与变换矩阵*A*相乘，然后将结果与另一个变换矩阵*B*相乘：
- en: '![](img/B22389_11_010.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_11_010.png)'
- en: 'By the associative property (which is applicable to linear algebra as well),
    we can rewrite this equation as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结合律（也适用于线性代数），我们可以将这个方程重写为如下形式：
- en: '![](img/B22389_11_011.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_11_011.png)'
- en: Generalizing this to a stack of *N* transformation matrices, we can see that
    it all works out to be a single linear transformation. This kind of defeats the
    purpose of stacking *N* layers, doesn’t it?
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一概念推广到多个*N*个变换矩阵的堆叠，我们可以看到这一切最终会变成单一的线性变换。这种做法似乎违背了堆叠*N*层的初衷，不是吗？
- en: This is where the non-linearity becomes essential and we introduce non-linearities
    by using a non-linear function, which we call activation functions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是非线性变得至关重要的地方，我们通过使用非线性函数引入非线性性，这些非线性函数被称为激活函数。
- en: Activation functions
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: '**Activation functions** are non-linear differentiable functions. In a biological
    neuron, the axon hillock decides whether to fire a signal based on the inputs.
    The activation functions serve a similar function and are key to the neural network’s
    ability to model non-linear data. In other words, activation functions are key
    in neural networks’ ability to transform input vector space (which is linearly
    inseparable) to a linearly separable vector space, informally. To *unwarp* a space
    such that linearly inseparable points become linearly separable, we need to have
    non-linear transformations.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**是非线性可微分函数。在生物神经元中，轴突小丘基于输入信号决定是否发出信号。激活函数具有类似的功能，并且对于神经网络建模非线性数据的能力至关重要。换句话说，激活函数是神经网络将输入向量空间（线性不可分）转换为线性可分向量空间的关键，非正式地说。为了*扭曲*空间，使得线性不可分的点变得线性可分，我们需要进行非线性变换。'
- en: 'We repeated the same experiment we did in the last section, where we visualized
    the trained transformation of a neural network on the input vector space, but
    this time, without any non-linearities. The resulting video can be found here:
    [https://www.youtube.com/watch?v=z-nV8oBpH2w](https://www.youtube.com/watch?v=z-nV8oBpH2w).
    The best transformation that the model learned is just not sufficient and the
    points are still linearly inseparable.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复了上一节中的实验，进行神经网络对输入向量空间的训练变换的可视化，但这次没有使用任何非线性变换。生成的视频可以在此观看：[https://www.youtube.com/watch?v=z-nV8oBpH2w](https://www.youtube.com/watch?v=z-nV8oBpH2w)。模型学习到的最佳变换仍然不够充分，点仍然是线性不可分的。
- en: Theoretically, an activation function can be any non-linear differentiable (differentiable
    almost everywhere, to be exact) function. However, over the course of time, there
    are a few non-linear functions that are popularly used as activation functions.
    Let’s look at a few of them.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，激活函数可以是任何非线性可微分的（严格来说是几乎处处可微分）函数。然而，随着时间的推移，有一些非线性函数被广泛用于作为激活函数。我们来看看其中一些。
- en: Sigmoid
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Sigmoid is one of the most common activation functions around, and probably
    one of the oldest. It is also known as the logistic function. When we discussed
    perceptron, we mentioned a step (also called *Heaviside* in literature) function
    as the activation function. The step function is not a continuous function and
    hence *is not* differentiable everywhere. A very close substitute is the sigmoid
    function.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 是最常见的激活函数之一，可能也是最古老的。它也被称为逻辑函数。当我们讨论感知机时，我们提到了一种步进（在文献中也称为 *Heaviside*）函数作为激活函数。步进函数不是连续函数，因此在任何地方都**不可微分**。一个非常接近的替代品就是
    Sigmoid 函数。
- en: 'It is defined as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 它的定义如下：
- en: '![](img/B22389_11_012.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_11_012.png)'
- en: Where *g* is the sigmoid function and *x* is the input value.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *g* 是 Sigmoid 函数，*x* 是输入值。
- en: Sigmoid is a continuous function and therefore *is* differentiable everywhere.
    The derivative is also computationally simpler to calculate. Because of these
    properties of the sigmoid, it was adopted widely in the early days of deep learning
    as a standard activation function.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 是一个连续函数，因此在任何地方都是可微分的。其导数也较容易计算。由于这些性质，Sigmoid 在深度学习的早期作为标准激活函数被广泛采用。
- en: 'Let’s see what a sigmoid function looks like and how it transforms a vector
    space:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Sigmoid 函数长什么样子，以及它如何转换向量空间：
- en: '![Figure 11.8 – Sigmoid activation function (left) and original, and activated
    vector space (middle and right) ](img/B22389_11_08.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – Sigmoid 激活函数（左），原始和激活后的向量空间（中间和右）](img/B22389_11_08.png)'
- en: 'Figure 11.8: Sigmoid activation function (left) and original and activated
    vector space (middle and right)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8：Sigmoid 激活函数（左），原始和激活后的向量空间（中间和右）
- en: The sigmoid function squashes the input between 0 and 1, as seen in *Figure
    11.8 (left)*. We can observe the same phenomenon in the vector space. One of the
    drawbacks of the sigmoid function is that the gradients tend to zero on the flat
    portions of the sigmoid. When a neuron approaches this area in the function, the
    gradients that it receives and propagates become negligible and the unit stops
    learning. We call this *saturating of the activation*. Because of this, nowadays,
    *sigmoid* is not typically used in deep learning, except in the output layer (we
    will be talking about this usage soon).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid函数将输入压缩到0和1之间，如*图11.8（左）*所示。我们可以在向量空间中观察到相同的现象。sigmoid函数的一个缺点是，梯度在sigmoid的平坦部分趋近于零。当神经元接近该区域时，它接收到的梯度变得微不足道，传播的梯度也停止，导致单元停止学习。我们称这种现象为激活的*饱和*。由于这个原因，现在*sigmoid*通常不再用于深度学习中，除非在输出层（我们很快会讨论这种用法）。
- en: Hyperbolic tangent (tanh)
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双曲正切（tanh）
- en: 'Hyperbolic tangents are another popular activation. They can be easily defined
    as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切是另一种流行的激活函数。它可以很容易地定义如下：
- en: '![](img/B22389_11_013.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_11_013.png)'
- en: 'It is very similar to sigmoid. In fact, we can express *tanh* as a function
    of sigmoid. Let’s see what the activation function looks like:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 它与sigmoid非常相似。实际上，我们可以将*tanh*表示为sigmoid的一个函数。让我们看看这个激活函数的样子：
- en: '![Figure 11.9 – TanH activation function (left) and original, and activated
    vector space (middle and right) ](img/B22389_11_09.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图11.9 – TanH激活函数（左）和原始、激活后的向量空间（中和右）](img/B22389_11_09.png)'
- en: 'Figure 11.9: Tanh activation function (left) and original and activated vector
    space (middle and right)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9：Tanh激活函数（左）和原始、激活后的向量空间（中和右）
- en: We can see that the shape is similar to sigmoid, although a bit sharper. But
    the key difference is that the *tanh* function outputs a value between -1 and
    1\. And because of the sharpness, we can also see the vector space getting pushed
    out to the edges as well. The fact that the function outputs a value that is symmetrical
    around the origin (0) works well with the optimization of the network and hence
    *tanh* was preferred over *sigmoid*. But since the *tanh* function is also a saturating
    function, the same problem of very small gradients hampering the flow of gradients
    and, in turn, learning plagues *tanh* activations as well.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到其形状类似于sigmoid，但要尖锐一些。关键的不同之处在于，*tanh*函数输出的值在-1和1之间。而由于其尖锐性，我们还可以看到向量空间被推向了边缘。该函数输出一个对称于原点（0）的值，这有助于网络的优化，因此*tanh*被优于*sigmoid*。但由于*tanh*函数也是一个饱和函数，当梯度非常小，妨碍梯度流动进而影响学习时，这一问题也困扰着*tanh*激活。
- en: Rectified linear units and variants
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 整流线性单元及其变种
- en: As neuroscience gained more information about the human brain, researchers found
    out that only one to four percent of neurons in the brain are activated at any
    time. But with all the activation functions such as *sigmoid* or *tanh*, almost
    half of the neurons in a network are activated. In 2010, Vinod Nair and Geoffrey
    Hinton proposed **rectified linear units** (**ReLUs**) in the seminal paper *Rectified
    Linear Units Improve Restricted Boltzmann Machines*. Since then, ReLUs have taken
    over as the de facto activation functions for deep neural networks.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 随着神经科学对人脑的了解不断深入，研究人员发现大脑中只有1%到4%的神经元在任何时候被激活。然而，在使用诸如*sigmoid*或*tanh*等激活函数时，网络中几乎一半的神经元都会被激活。2010年，Vinod
    Nair和Geoffrey Hinton在开创性的论文《*Rectified Linear Units Improve Restricted Boltzmann
    Machines*》中提出了**整流线性单元**（**ReLU**）。从那时起，ReLU成为深度神经网络中事实上的激活函数。
- en: ReLU
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReLU
- en: 'A ReLU is defined as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU的定义如下：
- en: '*g*(*x*) = *max*(*x*, 0)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*g*(*x*) = *max*(*x*, 0)'
- en: 'It is just a linear function but with a kink at zero. Any value greater than
    zero is retained as is, but all values below zero are squashed to zero. The range
    of the output goes from 0 to ![](img/B22389_11_014.png). Let’s see how it looks
    visually:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是一个线性函数，但在零点处有一个拐角。大于零的任何值都会保持不变，而所有小于零的值都会被压缩为零。输出的范围从0到![](img/B22389_11_014.png)。让我们来看一下它的可视化效果：
- en: '![Figure 11.10 – ReLU activation function (left) and original, and activated
    vector space (middle and right) ](img/B22389_11_10.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图11.10 – ReLU激活函数（左）和原始、激活后的向量空间（中和右）](img/B22389_11_10.png)'
- en: 'Figure 11.10: ReLU activation function (left) and original and activated vector
    space (middle and right)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10：ReLU激活函数（左）和原始、激活后的向量空间（中和右）
- en: We can see that the points in the left and bottom quadrants are all pushed into
    the axes’ lines. This squashing is what gives the non-linearity to the activation
    function. Because of the way the activation sharply becomes zero and does not
    tend to zero like the sigmoid or tanh, ReLUs are non-saturating.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到左下象限的点都被压缩到坐标轴的线上。这种压缩赋予了激活函数非线性。由于激活函数以一种突然变为零而不是像 sigmoid 或 tanh 那样趋近于零的方式变为零，ReLU
    是非饱和的。
- en: '**Reference check**:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献检查**：'
- en: The research paper that proposed ReLU is cited in *References* under reference
    *7*.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了 ReLU 的研究论文在*参考文献*中被引用，参考文献编号为*7*。
- en: 'There are a few advantages to using ReLUs:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ReLU 有一些优点：
- en: The computations of the activation function as well as its gradients are really
    cheap.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数及其梯度的计算成本非常低。
- en: Training converges much faster than those with saturating activation functions.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练收敛速度比使用饱和激活函数的情况要快得多。
- en: ReLU helps bring sparsity in the network (by having the activation as zero,
    a large majority of neurons in the network can be turned off) and resembles how
    biological neurons work.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU 有助于在网络中引入稀疏性（通过将激活设为零，网络中绝大多数神经元可以被关闭），并且类似于生物神经元的工作方式。
- en: 'But ReLUs are not without problems:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，ReLU 也不是没有问题的：
- en: When *x* < 0, the gradients become zero. This means a neuron that has an output
    < 0 will have zero gradients and, therefore, the unit will not learn anymore.
    These are called dead ReLUs.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 *x* < 0 时，梯度变为零。这意味着输出 < 0 的神经元将会有零梯度，因此该单元将不再学习。这些被称为死 ReLU。
- en: Another disadvantage is that the average output of a ReLU unit is positive,
    and when we stack multiple layers, this might lead to a positive bias in the output.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个缺点是，ReLU 单元的平均输出是正值，当我们堆叠多个层时，这可能导致输出产生正偏差。
- en: Let’s see a few variants that try to resolve the problems we discussed for ReLU.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些变种，尝试解决我们讨论过的 ReLU 问题。
- en: Leaky ReLU and parametrized ReLU
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Leaky ReLU 和参数化 ReLU
- en: 'Leaky ReLU is a variant of standard ReLU that resolves the *dead ReLU* problem.
    It was proposed by Maas and others in 2013\. A leaky ReLU can be defined as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU 是标准 ReLU 的变种，解决了 *死 ReLU* 问题。它由 Maas 等人于 2013 年提出。Leaky ReLU 可以定义如下：
- en: '![](img/B22389_11_015.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_11_015.png)'
- en: 'Here, ![](img/B22389_04_009.png) is the slope parameter (typically set to a
    very small value such as 0.001) and is considered a hyperparameter. This makes
    sure the gradients are not zero when *x* < *0* and thereby ensures there are no
    *dead* ReLUs. But the sparsity that ReLU provides is lost here because there is
    no zero output that turns off a unit completely. Let’s visualize this activation
    function:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B22389_04_009.png) 是斜率参数（通常设置为非常小的值，例如 0.001），并被视为超参数。这确保了当 *x* <
    *0* 时梯度不为零，从而避免了*死* ReLU 的问题。但这里丧失了 ReLU 提供的稀疏性，因为没有零输出来完全关闭神经元。我们来可视化这个激活函数：
- en: '![Figure 10.11 – Leaky ReLU activation function (left) and original, and activated
    vector space (middle and right) ](img/B22389_11_11.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.11 – Leaky ReLU 激活函数（左）和原始与激活后的向量空间（中和右）](img/B22389_11_11.png)'
- en: 'Figure 11.11: Leaky ReLU activation function (left) and original and activated
    vector space (middle and right)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11：Leaky ReLU 激活函数（左）和原始与激活后的向量空间（中和右）
- en: In 2015, K. He and others proposed another minor modification to leaky ReLU
    called **parametrized ReLU**. In parametrized ReLU, instead of considering ![](img/B22389_04_009.png)
    as a hyperparameter, they considered it as a learnable parameter.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，K. He 等人提出了一种对 Leaky ReLU 的小改进，称为 **参数化 ReLU**。在参数化 ReLU 中，他们不再将 ![](img/B22389_04_009.png)
    视为超参数，而是将其视为一个可学习的参数。
- en: '**Rerefence check**:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献检查**：'
- en: The research paper that proposed leaky ReLU is cited in *References* under reference
    *8*, and parametrized ReLU is cited under reference *9*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了 Leaky ReLU 的研究论文在*参考文献*中被引用，参考文献编号为*8*，而参数化 ReLU 在参考文献编号为*9*中被引用。
- en: 'There are many other activation functions that are less popularly used but
    still have enough use cases to be included in *PyTorch*. You can find a list of
    them here: [https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).
    We encourage you to use the notebook titled `02-Activation_Functions.ipynb` in
    the `Chapter 11` folder to try out different activation functions and see how
    they warp the vector space.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他激活函数，虽然不太流行，但在 *PyTorch* 中仍具有足够的使用案例。您可以在这里找到它们的列表：[https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)。我们建议您使用
    `Chapter 11` 文件夹中的笔记本 `02-Activation_Functions.ipynb` 尝试不同的激活函数，看看它们如何改变向量空间。
- en: And with that, we now have an idea of the components of the first block in *Figure
    11.6*, representation learning. The next block in there is the linear classifier,
    which has a linear transformation and an output activation. We already know what
    a linear transformation is, but what is an output activation?
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们已经了解了第一块图中的组件，即*图 11.6*，表示学习。那里的下一个块是线性分类器，它具有线性变换和输出激活。我们已经知道线性变换是什么，但输出激活是什么？
- en: Output activation functions
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出激活函数
- en: Output activation functions are functions that enforce a few desirable properties
    to the output of the network.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 输出激活函数是强制网络输出具有几个理想属性的函数。
- en: '**Additional reading**:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**额外阅读**：'
- en: These functions have a deeper connection with **maximum likelihood estimation**
    (**MLE**) and the chosen loss function, but we will not be getting into that because
    it is out of the scope of this book. We have linked to the book *Deep Learning*
    by Ian Goodfellow, Yoshua Bengio, and Aaron Courville in the *Further reading*
    section. If you are interested in a deeper understanding of deep learning, we
    suggest you use the book to that effect.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数与**最大似然估计**（**MLE**）和选择的损失函数有着更深的联系，但我们不会深入探讨，因为这超出了本书的范围。我们在*进一步阅读*部分中链接到了Ian
    Goodfellow、Yoshua Bengio 和 Aaron Courville 的书籍 *深度学习*。如果你对深度学习有更深入的理解兴趣，我们建议你使用该书。
- en: If we want the neural network to predict a continuous number in the case of
    regression, we just use a linear activation function (which is like saying there
    is no activation function). The raw output from the network is considered the
    prediction and fed into the loss function.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望神经网络在回归的情况下预测一个连续数值，我们只需使用线性激活函数（这就像说没有激活函数）。来自网络的原始输出被视为预测结果并输入到损失函数中。
- en: But in the case of classification, the desired output is a class out of all
    possible classes. If there are only two classes, we can use our old friend, the
    `sigmoid` function, which has an output between 0 and 1\. We can also use *tanh*
    because its output is going to be between -1 and 1\. The `sigmoid` function is
    preferred because of the intuitive probabilistic interpretation that comes along
    with it. The closer the value is to one, the more confident the network is about
    that prediction.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 但在分类的情况下，期望的输出是所有可能类别中的一个类。如果只有两个类别，我们可以使用我们的老朋友 `sigmoid` 函数，其输出介于 0 和 1 之间。我们还可以使用
    *tanh*，因为其输出将介于 -1 和 1 之间。`sigmoid` 函数更受青睐，因为它具有直观的概率解释。值越接近一，网络对该预测的信心就越大。
- en: Now, *sigmoid* works for binary classification. What about multiclass classification
    where the possible classes are more than two?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，*sigmoid* 适用于二元分类。那么对于可能类别超过两个的多类分类呢？
- en: Softmax
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Softmax
- en: '*Softmax* is a function that converts a vector of *K* real values into another
    *K*-positive real value, which sums up to 1\. *Softmax* is defined as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '*Softmax* 是一个将 *K* 个实数值向量转换为另一个 *K* 正实数值向量的函数，其总和为 1。*Softmax* 的定义如下：'
- en: '![](img/B22389_11_018.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_11_018.png)'
- en: 'This function converts the raw output from a network into something that resembles
    a probability across *K* classes. This has a strong relation with *sigmoid*—*sigmoid*
    is a special case of *softmax* when *K* = *2*. In the following figure, let’s
    see how a random vector of size 3 is converted into probabilities that add up
    to 1:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将网络的原始输出转换为类似于 *K* 类概率的形式。这与 *sigmoid* 有着紧密的关系——当 *K* = *2* 时，*sigmoid* 是
    *softmax* 的特例。在接下来的图中，让我们看看如何将大小为 3 的随机向量转换为总和为 1 的概率：
- en: '![Figure 11.12 – Raw output versus softmax output ](img/B22389_11_12.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.12 – 原始输出与 softmax 输出](img/B22389_11_12.png)'
- en: 'Figure 11.12: Raw output versus softmax output'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11.12: 原始输出与 softmax 输出'
- en: If we look closely, we can see that in addition to converting the real values
    into something that resembles probability, it also increases the relative gap
    between the maximum and the rest of the values. This activation is a standard
    output activation for multiclass classification problems.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细观察，我们会发现，除了将真实值转换成类似概率的东西外，它还增加了最大值与其他值之间的相对差距。这种激活是多类分类问题的标准输出激活。
- en: Now, there is only one major component left in the diagram (*Figure 11.6*)—the
    loss function.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，图表中只剩下一个主要组件——损失函数（*图11.6*）。
- en: Loss function
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: The loss function we touched upon in *Chapter 5*, *Time Series Forecasting as
    Regression*, translates nicely to deep learning. In deep learning also, the loss
    function is a way to tell how good the predictions of the model are. If the predictions
    are way off the target, the loss function will be higher, and as we get closer
    to the truth, it becomes smaller. In the deep learning paradigm, we just have
    one more additional requirement of the loss function—it should be differentiable.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第五章*中我们提到的损失函数，*时间序列预测作为回归问题*，同样适用于深度学习。在深度学习中，损失函数也是一种衡量模型预测质量的方式。如果预测偏离目标很远，损失函数值会很高；而当我们越来越接近真实值时，损失函数值会变小。在深度学习范式中，损失函数还有一个额外的要求——它应该是可微的。
- en: 'Common loss functions from classical machine learning, such as **mean squared
    error** or **mean absolute error,** are valid in deep learning as well. In fact,
    in regression tasks, they are the default choices that practitioners adopt. For
    classification tasks, we adopt a concept borrowed from information theory called
    **cross-entropy loss**. However, since deep learning is a very flexible framework,
    we can use any loss function as long as it is differentiable. There are a lot
    of loss functions people have already tried and found work in many situations.
    A lot of them are part of PyTorch’s API as well. You can find them here: [https://pytorch.org/docs/stable/nn.html#loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习中的常见损失函数，如**均方误差**或**均绝对误差**，在深度学习中同样适用。事实上，在回归任务中，它们是实践者默认选择的损失函数。对于分类任务，我们采用一个来自信息论的概念——**交叉熵损失**。然而，由于深度学习是一个非常灵活的框架，只要损失函数是可微的，我们就可以使用任何损失函数。已经有很多损失函数被人们尝试并且在许多情况下证明有效。其中很多也已经成为了PyTorch的API的一部分。你可以在这里找到它们：[https://pytorch.org/docs/stable/nn.html#loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions)。
- en: Now that we have covered all the components of a deep learning system, let’s
    also briefly look at how we train the whole system.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了深度学习系统的所有组件，让我们简要看看如何训练整个系统。
- en: Forward and backward propagation
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前向和后向传播
- en: In *Figure 11.6*, we can see two sets of arrows, one going toward the desired
    output from the input, marked as *Forward Computation*, and another going backward
    to the input from the desired output, marked *Backward Computation*. These two
    steps are at the core of learning a deep learning system. In *Forward Computation*,
    popularly known as **forward propagation**, we use the series of computations
    that are defined in the layers and propagate the input all the way through the
    network to get the output. Now that we have the output, we would use the loss
    function to assess how close or far we are from the desired output. This information
    is now used in *Backward Computation*, popularly known as **backward propagation**,
    to calculate the gradient with respect to all the parameters.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图11.6*中，我们可以看到两组箭头，一组从输入指向期望的输出，标记为*前向计算*，另一组则从期望的输出指向输入，标记为*后向计算*。这两个步骤是深度学习系统学习过程的核心。在*前向计算*中，通常称为**前向传播**，我们使用在各层中定义的一系列计算，将输入从网络的起始点传递到输出端。现在我们得到了输出，我们会使用损失函数来评估我们离期望输出有多近或多远。这些信息现在被用于*后向计算*，通常称为**反向传播**，以计算相对于所有参数的梯度。
- en: Now, what is a gradient and why do we need it? In high school math, we might
    have come across gradients or derivatives in another form called **slope**. It
    is the rate of change of a quantity when we change a variable by unit measure.
    Derivatives inform us of the local slope of a scalar function. While derivatives
    are always with respect to a single variable, gradients are a generalization of
    derivatives to multivariate functions. Intuitively, both gradients and derivatives
    inform us of the local slope of the function. And with the gradient of the loss
    function, we can use one of the techniques from mathematical optimization, called
    **gradient descent**, to optimize our loss function.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，梯度是什么，为什么我们需要它？在高中的数学中，我们可能会遇到梯度或导数，它们也被称为**斜率**。斜率是指当我们以单位度量改变一个变量时，量的变化速率。导数告诉我们标量函数的局部斜率。导数总是与单一变量相关，而梯度是导数对多变量函数的推广。直观地说，梯度和导数都告诉我们函数的局部斜率。通过损失函数的梯度，我们可以使用数学优化中的一种技术——**梯度下降**，来优化我们的损失函数。
- en: Let’s see this with an example.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来看看。
- en: Gradient descent
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Any machine learning or deep learning model can be thought of as a function
    that converts an input, *x*, to an output, ![](img/B22389_05_001.png), using a
    few parameters, ![](img/B22389_04_016.png). Here, ![](img/B22389_04_016.png) can
    be the collection of all the matrix transformations that we do to the input throughout
    the network. But to simplify the example, let’s assume there are only two parameters,
    *a* and *b*. If we think about the whole process of learning a bit, we will see
    that by keeping the input and expected output the same, the way to change your
    loss would be by changing the parameters of the model. Therefore, we can postulate
    the loss function to be parameterized by the parameters–in this case, *a* and
    *b*.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习或深度学习模型都可以看作是一个将输入 *x* 转换为输出 ![](img/B22389_05_001.png) 的函数，使用一些参数 ![](img/B22389_04_016.png)。在这里，![](img/B22389_04_016.png)
    可以是我们在整个网络中对输入进行的所有矩阵变换的集合。但为了简化例子，我们假设只有两个参数 *a* 和 *b*。如果我们稍微思考一下整个学习过程，就会发现，通过保持输入和预期输出不变，改变损失函数的方式就是通过调整模型的参数。因此，我们可以假设损失函数是通过这些参数来参数化的——在这个例子中就是
    *a* 和 *b*。
- en: '**Notebook alert**:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提示**：'
- en: To follow along with the complete code, use the notebook named `03-Gradient_Descent.ipynb`
    in the `Chapter11` folder and the code in the `src` folder.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 若要跟随完整代码，请使用 `Chapter11` 文件夹中的名为 `03-Gradient_Descent.ipynb` 的笔记本，以及 `src` 文件夹中的代码。
- en: 'Let’s assume the loss function takes the following form:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 假设损失函数的形式如下：
- en: '![](img/B22389_11_022.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_11_022.png)'
- en: Let’s see what the function looks like. We can use a three-dimensional plot
    to visualize a function with two parameters, as seen in *Figure 11.13*. Two dimensions
    will be used to denote the two parameters and, at each point in that two-dimensional
    mesh, we can plot the loss value in the third dimension.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个函数是什么样子的。我们可以使用三维图来可视化一个有两个参数的函数，如 *图 11.13* 所示。两个维度将用来表示这两个参数，在那个二维网格的每一点上，我们可以在第三维度中绘制损失值。
- en: This kind of plot of the loss function is also called a loss curve (in univariate
    settings), or loss surface (in multivariate settings).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这种损失函数的图表也称为损失曲线（在单变量情况下），或损失面（在多变量情况下）。
- en: '![Figure 11.13 – Loss surface plot ](img/B22389_11_13.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.13 – 损失面图](img/B22389_11_13.png)'
- en: 'Figure 11.13: Loss surface plot'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13：损失面图
- en: The lighter portion of the 3D shape is where the loss function is less, and
    as we move away from there, it increases.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 形状的较亮部分表示损失函数较小，而离开该部分时，损失值增大。
- en: In machine learning, our aim is to minimize the loss function, or in other words,
    find the parameters that make our predicted output as close as possible to the
    ground truth. This falls under the realm of mathematical optimization, and a particular
    technique lends itself suitable for this approach—**gradient descent**.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们的目标是最小化损失函数，换句话说，就是找到能够使我们的预测输出尽可能接近真实值的参数。这属于数学优化的范畴，而有一种特别的技术非常适合这种方法——**梯度下降**。
- en: Gradient descent is a mathematical optimization algorithm used to minimize a
    cost function by iteratively moving in the direction of the steepest descent.
    In a univariate function, the derivative (or the slope) gives us the direction
    (and magnitude) of the steepest ascent. For instance, if we know that the slope
    of a function is 1, we know if we move to the right, we are climbing up the slope,
    and moving to the left, we will be climbing down. Similarly, in the multivariate
    setting, the gradient of a function at any point will give us the direction (and
    magnitude) of the steepest ascent. And since we are concerned with minimizing
    a loss function, we will be using the negative gradient, which will point us in
    the direction of the steepest descent.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种数学优化算法，通过在最陡下降的方向上迭代地移动来最小化代价函数。在单变量函数中，导数（或斜率）给出我们最陡上升的方向（和大小）。例如，如果我们知道一个函数的斜率为1，那么我们知道如果向右移动，我们是在沿着斜率向上爬坡，向左移动则是在向下爬坡。同样，在多变量的情况下，函数在任何一点的梯度将给出最陡上升的方向（和大小）。而由于我们关注的是最小化损失函数，因此我们将使用负梯度，它会指引我们向最陡下降的方向移动。
- en: 'So, let’s define the gradient for our loss function. We are using high school
    calculus, but even if you are not comfortable, you don’t need to worry:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们为我们的损失函数定义梯度。我们使用的是高中级别的微积分，但即使你不太擅长，也无需担心：
- en: '![](img/B22389_11_023.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_11_023.png)'
- en: 'Now, how does the algorithm work? Very simply, as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，算法是如何工作的呢？非常简单，具体如下：
- en: Initialize the parameters to random values.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将参数初始化为随机值。
- en: Compute the gradient at that point.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算该点的梯度。
- en: Make a step in the direction opposite to the gradient.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 沿着梯度的反方向迈出一步。
- en: Repeat steps 2 and 3 until it converges or we reach maximum iterations.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和步骤3，直到收敛或达到最大迭代次数。
- en: 'There is just one more aspect that needs more clarity: how much of a step do
    we take in each iteration?'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个需要澄清的方面：我们在每次迭代中应该迈出多大一步？
- en: Ideally, the magnitude of the gradient tells you how fast the function is changing
    in that direction, and we should just take the step equal to the gradient. But
    there is a property of the gradient that makes that a bad idea. The gradient only
    defines the direction and magnitude of the steepest ascent in the infinitesimally
    small locality of the current point and is blind to what happens beyond it. Therefore,
    we use a hyperparameter, commonly called the **learning rate**, to temper the
    steps we take in each iteration. Therefore, instead of taking a step equal to
    the gradient, we take a step equal to the learning rate multiplied by the gradient.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，梯度的大小告诉你函数在该方向变化的快慢，我们应该直接按照梯度的大小进行步长调整。但梯度有一个特性使得直接这样做并不好。梯度只定义了当前点附近最陡上升的方向和大小，并且对它之外的变化一无所知。因此，我们使用一个超参数，通常称为**学习率**，来调整我们在每次迭代中所采取的步长。因此，我们不直接采取与梯度相等的步长，而是将步长设置为学习率与梯度的乘积。
- en: 'Mathematically, if ![](img/B22389_04_016.png) is the vector of parameters,
    at each iteration, we update the parameters using the following formula:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，如果 ![](img/B22389_04_016.png) 是参数的向量，在每次迭代时，我们使用以下公式更新参数：
- en: '![](img/B22389_11_025.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_11_025.png)'
- en: Here, ![](img/B22389_04_044.png) is the learning rate, and ![](img/B22389_11_027.png)
    is the gradient at the point.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B22389_04_044.png) 是学习率，![](img/B22389_11_027.png) 是该点的梯度。
- en: 'Let’s see a very simple implementation of gradient descent (refer to the notebook
    in the GitHub repository for the end-to-end definition and working code). First,
    let’s define a function that returns the gradient at any point:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个非常简单的梯度下降实现（有关完整定义和代码，请参阅 GitHub 仓库中的 notebook）。首先，让我们定义一个函数，返回任意点的梯度：
- en: '[PRE0]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we define a few initial parameters such as the maximum iterations, learning
    rate, and initial value of *a* and *b*:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义一些初始参数，比如最大迭代次数、学习率，以及*a* 和 *b* 的初始值：
- en: '[PRE1]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We know the minimum for this function will be at *a* = *8* and *b* = *2* because
    that would make the loss function zero. And gradient descent finds a solution
    that is pretty accurate—*a* = *8.000000000000005* and *b* = *2.000000002230101*.
    We can also visualize the path it took to reach the minimum, as seen in *Figure
    11.14*:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道这个函数的最小值出现在 *a* = *8* 和 *b* = *2*，因为这会使损失函数为零。而梯度下降法能找到一个非常准确的解——*a* = *8.000000000000005*
    和 *b* = *2.000000002230101*。我们还可以将它到达最小值的路径可视化，如*图 11.14*所示：
- en: '![Figure 11.14 – Gradient descent optimization on the loss surface ](img/B22389_11_14.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图11.14 – 损失面上的梯度下降优化](img/B22389_11_14.png)'
- en: 'Figure 11.14: Gradient descent optimization on the loss surface'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '图11.14: 损失面上的梯度下降优化'
- en: We can see that even though we initialized the parameters far from the actual
    origin, the optimization algorithm makes a direct path to the optimal point. At
    each point, the algorithm looks at the gradient of the point, moves in the opposite
    direction, and eventually converges on the optimum.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们将参数初始化远离实际原点，优化算法也会直接路径到达最优点。在每个点上，算法查看点的梯度，朝相反方向移动，最终收敛于最优点。
- en: When gradient descent is adopted in a learning task, there are a few kinks to
    be noted. Let’s say we have a dataset of *N* samples. There are three popular
    variants of gradient descent that are used in learning and each of them has its
    pros and cons.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 当梯度下降在学习任务中被采用时，需要注意一些问题。假设我们有一个包含*N*个样本的数据集。有三种流行的梯度下降变体用于学习，每种都有其优缺点。
- en: Batch gradient descent
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: We run *all N* samples through the network and average the losses across *all
    N* instances. Now, we use this loss to calculate the gradient, make a step in
    the right direction, and repeat.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将*所有N*个样本通过网络，并计算所有*N*个实例的损失平均值。现在，我们使用这个损失来计算梯度，在正确方向上迈出一步，然后重复此过程。
- en: 'The pros are as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 优点如下：
- en: The optimization path is direct.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化路径是直接的。
- en: The optimization path has guaranteed convergence.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化路径有保证的收敛性。
- en: 'The cons are as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点如下：
- en: The entire dataset needs to be evaluated for a single step, and that is computationally
    expensive. The computation per optimization step becomes prohibitively high for
    huge datasets.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要对单个步骤评估整个数据集，这在计算上是昂贵的。对于庞大的数据集，每个优化步骤的计算量变得非常高。
- en: The time taken per optimization step is high, and hence, the convergence will
    also be slow.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个优化步骤的所需时间较长，因此收敛速度也较慢。
- en: Stochastic gradient descent (SGD)
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）
- en: In SGD, we randomly sample *one* instance from *N* samples, calculate the loss
    and gradients, and then update the parameters.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机梯度下降（SGD）中，我们从*N*个样本中随机抽取*一个*实例，计算损失和梯度，然后更新参数。
- en: 'The pros are as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 优点如下：
- en: Since we only use a single instance to do the optimization step, computation
    per optimization step is very low.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为我们只使用单个实例来进行优化步骤，每个优化步骤的计算量非常低。
- en: The time taken per optimization step is also faster.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个优化步骤的所需时间也更短。
- en: Stochastic sampling also acts as regularization and helps to avoid overfitting.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机抽样也作为正则化，有助于避免过拟合。
- en: 'The cons are as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点如下：
- en: The gradient estimates are noisy because we make the step based on just one
    instance. Therefore, the path toward optimum will be choppy and noisy.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度估计具有噪声，因为我们仅基于一个实例进行步骤。因此，朝向最优点的路径将是不稳定和嘈杂的。
- en: Just because the time taken per optimization is low, it need not mean convergence
    is faster. We may not be taking the right step many times because of noisy gradient
    estimates.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅因为每个优化步骤的时间较短，并不意味着收敛速度更快。由于嘈杂的梯度估计，我们可能许多次没有采取正确的步骤。
- en: Mini-batch gradient descent
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: Mini-batch gradient descent is a technique that falls somewhere between the
    spectrum of batch gradient descent and SGD. In this variant, we have another quality
    called mini-batch size (or simply batch size), *b*. In each optimization step,
    we randomly pick *b* instances from *N* samples and calculate gradients on the
    average loss of all *b* instances. With *b* = *N*, we have *batch gradient descent*,
    and with *b* = *1*, we have *stochastic gradient descent*. This is the most popular
    way neural networks are trained today. By varying the batch size, we can travel
    between the two variants and manage the pros and cons of each option.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量梯度下降是一种介于批量梯度下降和SGD之间的技术。在这种变体中，我们还有另一个质量叫做小批量大小（或简称批量大小），*b*。在每个优化步骤中，我们从*N*个样本中随机选择*b*个实例，并计算所有*b*个实例的平均损失梯度。当*b*
    = *N*时，我们有批量梯度下降；当*b* = *1*时，我们有随机梯度下降。这是今天训练神经网络最流行的方法。通过改变批量大小，我们可以在这两种变体之间移动，并管理每种选择的优缺点。
- en: Nothing develops intuition better than a visual playground where we can see
    the effects of the different components we discussed. *Tensorflow Playground*
    is an excellent resource (see the link in the *Further reading* section) to do
    just that. I strongly urge you to head over there and play with the tool, train
    a few neural networks right in the browser, and see in real time how the learning
    happens.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么能比得上一个视觉化的操作平台，更能帮助我们直观地理解我们讨论的不同组件的效果。*Tensorflow Playground* 是一个极好的资源（见
    *进一步阅读* 部分中的链接），它可以帮助你做到这一点。我强烈建议你前往那里，玩一玩这个工具，在浏览器中训练几个神经网络，并实时观察学习过程是如何发生的。
- en: Summary
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We kicked off a new part of the book with an introduction to deep learning.
    We started with a bit of history to understand why deep learning is so popular
    today and we also explored its humble beginnings in perceptron. We understood
    the composability of deep learning and understood and dissected the different
    components of deep learning, such as the representation learning block, linear
    layers, activation functions, and so on. Finally, we rounded off the discussion
    by looking at how a deep learning system uses gradient descent to learn from data.
    With that understanding, we are now ready to move on to the next chapter, where
    we will drive the narrative toward time series models.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以介绍深度学习开始了本书的新部分。我们从一段历史开始，了解为什么深度学习在今天如此流行，并探讨了它在感知器中的朴素起步。我们理解了深度学习的可组合性，并分析了深度学习的不同组件，如表示学习块、线性层、激活函数等。最后，我们通过观察深度学习系统如何使用梯度下降从数据中学习来总结讨论。基于这些理解，我们现在准备进入下一章，在那里我们将把叙事引向时间序列模型。
- en: References
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Following is the list of references used throughout this chapter:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章中使用的参考文献列表：
- en: 'Kyoung-Su Oh and Keechul Jung. (2004), *GPU implementation of neural networks*.
    Pattern Recognition, Volume 37, Issue 6, 2004: [https://doi.org/10.1016/j.patcog.2004.01.013](https://doi.org/10.1016/j.patcog.2004.01.013).'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Kyoung-Su Oh 和 Keechul Jung. (2004), *神经网络的GPU实现*。模式识别，第37卷，第6期，2004年: [https://doi.org/10.1016/j.patcog.2004.01.013](https://doi.org/10.1016/j.patcog.2004.01.013)。'
- en: 'Rajat Raina, Anand Madhavan, and Andrew Y. Ng. (2009), *Large-scale deep unsupervised
    learning using graphics processors*. In Proceedings of the 26th Annual International
    Conference on Machine Learning (ICML ‘09): [https://doi.org/10.1145/1553374.1553486](https://doi.org/10.1145/1553374.1553486).'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Rajat Raina, Anand Madhavan 和 Andrew Y. Ng. (2009), *使用图形处理单元进行大规模深度无监督学习*。第26届国际机器学习大会(ICML
    ''09)论文集: [https://doi.org/10.1145/1553374.1553486](https://doi.org/10.1145/1553374.1553486)。'
- en: 'Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. (2012), *ImageNet
    Classification with Deep Convolutional Neural Networks*. Commun. ACM 60, 6 (June
    2017), 84–90: [https://doi.org/10.1145/3065386](https://doi.org/10.1145/3065386).'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Alex Krizhevsky, Ilya Sutskever 和 Geoffrey E. Hinton. (2012), *使用深度卷积神经网络进行ImageNet分类*。Commun.
    ACM 60, 6 (2017年6月)，84–90: [https://doi.org/10.1145/3065386](https://doi.org/10.1145/3065386)。'
- en: 'Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. (2020).
    *The Computational Limits of Deep Learning*. arXiv:2007.05558v1 [cs.LG]: [https://arxiv.org/abs/2007.05558v1](https://arxiv.org/abs/2007.05558v1).'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Neil C. Thompson, Kristjan Greenewald, Keeheon Lee 和 Gabriel F. Manso. (2020).
    *深度学习的计算极限*。arXiv:2007.05558v1 [cs.LG]: [https://arxiv.org/abs/2007.05558v1](https://arxiv.org/abs/2007.05558v1)。'
- en: Frank Rosenblatt. (1957), *The perceptron – A perceiving and recognizing automaton*,
    Technical Report 85-460-1, Cornell Aeronautical Laboratory.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Frank Rosenblatt. (1957), *感知器——一个感知与识别的自动机*，技术报告85-460-1，康奈尔航空实验室。
- en: 'Charu C. Aggarwal, Alexander Hinneburg, and Daniel A. Keim. (2001). *On the
    Surprising Behavior of Distance Metrics in High Dimensional Spaces.* In Proceedings
    of the 8th International Conference on Database Theory (ICDT ‘01). Springer-Verlag,
    Berlin, Heidelberg, 420–434: [https://dl.acm.org/doi/10.5555/645504.656414](https://dl.acm.org/doi/10.5555/645504.656414).'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Charu C. Aggarwal, Alexander Hinneburg 和 Daniel A. Keim. (2001). *高维空间中距离度量的惊人表现*。第8届国际数据库理论会议(ICDT
    ''01)论文集。Springer-Verlag，柏林，海德堡，420–434: [https://dl.acm.org/doi/10.5555/645504.656414](https://dl.acm.org/doi/10.5555/645504.656414)。'
- en: 'Nair, V. and Hinton, G.E. (2010). *Rectified Linear Units Improve Restricted
    Boltzmann Machines*. ICML: [https://icml.cc/Conferences/2010/papers/432.pdf](https://icml.cc/Conferences/2010/papers/432.pdf).'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Nair, V. 和 Hinton, G.E. (2010). *修正线性单元改进了限制玻尔兹曼机*。ICML: [https://icml.cc/Conferences/2010/papers/432.pdf](https://icml.cc/Conferences/2010/papers/432.pdf)。'
- en: 'Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. (2013). *Rectifier nonlinearities
    improve neural network acoustic models*. ICML Workshop on Deep Learning for Audio,
    Speech, and Language Processing: [https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf).'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Andrew L. Maas, Awni Y. Hannun, 和 Andrew Y. Ng。（2013）。*激活函数非线性改善神经网络声学模型*。ICML
    深度学习音频、语音和语言处理工作坊：[https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)。
- en: 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (2015). *Delving Deep
    into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification*.
    2015 IEEE International Conference on Computer Vision (ICCV), 1026-1034: [https://ieeexplore.ieee.org/document/7410480](https://ieeexplore.ieee.org/document/7410480).'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun（2015）。*深入研究激活函数：在 ImageNet
    分类上超越人类水平表现*。2015 IEEE 国际计算机视觉大会（ICCV），1026-1034：[https://ieeexplore.ieee.org/document/7410480](https://ieeexplore.ieee.org/document/7410480)。
- en: 'Sara Hooker. (2021). *The hardware lottery*. Commun. ACM, Volume 64: [https://doi.org/10.1145/3467017](https://doi.org/10.1145/3467017).'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sara Hooker。（2021）。*硬件彩票*。Commun. ACM，第 64 卷：[https://doi.org/10.1145/3467017](https://doi.org/10.1145/3467017)。
- en: Further reading
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can check out the following sources if you want to read more about a few
    topics covered in this chapter:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解本章中涉及的某些主题，可以查阅以下资源：
- en: '*Linear Algebra* course from Gilbert Strang: [https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/videos/](https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/videos/)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*线性代数* 课程，由 Gilbert Strang 主讲：[https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/videos/](https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/videos/)'
- en: '*Essence of Linear Algebra* from 3Blue1Brown: [https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*线性代数的本质*，由 3Blue1Brown 提供：[https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)'
- en: '*Neural Networks – A Linear Algebra Perspective* by Manu Joseph: [https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/](https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经网络——线性代数视角*，作者 Manu Joseph：[https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/](https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/)'
- en: '*Deep Learning* – Ian Goodfellow, Yoshua Bengio, Aaron Courville: [https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/](https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度学习* – Ian Goodfellow, Yoshua Bengio, Aaron Courville：[https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/](https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/)'
- en: '*Tensorflow Playground*: [https://playground.tensorflow.org/](https://playground.tensorflow.org/  )'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Tensorflow Playground*：[https://playground.tensorflow.org/](https://playground.tensorflow.org/)'
- en: Join our community on Discord
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with authors and other readers:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mts](https://packt.link/mts)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mts](https://packt.link/mts)'
- en: '![](img/QR_Code15080603222089750.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code15080603222089750.png)'
