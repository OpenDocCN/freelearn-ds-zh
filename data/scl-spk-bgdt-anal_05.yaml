- en: Tackle Big Data – Spark Comes to the Party
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应对大数据——Spark 登场
- en: An approximate answer to the right problem is worth a good deal more than an
    exact answer to an approximate problem.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正确问题的近似答案，比对于近似问题的精确答案更有价值。
- en: '- John Tukey'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 约翰·图基'
- en: In this chapter, you learn about data analysis and big data; we see the challenges
    that big data provides and how they are dealt with. You will learn about distributed
    computing and the approach suggested by functional programming; we introduce Google's
    MapReduce, Apache Hadoop, and finally Apache Spark and see how they embrace this
    approach and these techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习数据分析和大数据；我们将看到大数据带来的挑战以及如何应对这些挑战。你将学习分布式计算以及函数式编程提出的方法；我们将介绍 Google
    的 MapReduce、Apache Hadoop，最后是 Apache Spark，并展示它们如何采纳这些方法和技术。
- en: 'In a nutshell, the following topics will be covered throughout this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章将涵盖以下主题：
- en: Introduction to data analytics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析简介
- en: Introduction to big data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据简介
- en: Distributed computing using Apache Hadoop
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Apache Hadoop 的分布式计算
- en: Here comes Apache Spark
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 来了
- en: Introduction to data analytics
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析简介
- en: '**Data analytics** is the process of applying qualitative and quantitative
    techniques when examining data with the goal of providing valuable insights. Using
    various techniques and concepts, data analytics can provide the means to explore
    the data **Exploratory Data Analysis** (**EDA**) as well as draw conclusions about
    the data **Confirmatory Data Analysis** (**CDA**). EDA and CDA are fundamental
    concepts of data analytics, and it is important to understand the difference between
    the two.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据分析**是应用定性和定量技术来检查数据的过程，目的是提供有价值的洞察。通过各种技术和概念，数据分析可以为探索数据提供手段，即**探索性数据分析**（**EDA**），并对数据得出结论，即**验证性数据分析**（**CDA**）。EDA
    和 CDA 是数据分析的基本概念，理解这两者之间的区别非常重要。'
- en: EDA involves methodologies, tools, and techniques used to explore data with
    the intention of finding patterns in the data and relationships between various
    elements of the data. CDA involves methodologies, tools, and techniques used to
    provide an insight or conclusion on a specific question based on a hypothesis
    and statistical techniques or simple observation of the data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: EDA 涉及用于探索数据的各种方法、工具和技术，目的是在数据中发现模式以及数据中各元素之间的关系。CDA 涉及用于根据假设和统计技术或对数据的简单观察，提供对特定问题的洞察或结论的方法、工具和技术。
- en: A quick example to understand these ideas is that of a grocery store, which
    has asked you to give them ways to improve sales and customer satisfaction as
    well as keep the cost of operations low.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速的例子来帮助理解这些概念是关于一个杂货店，它要求你提供提高销售和顾客满意度的方法，同时保持低运营成本。
- en: 'The following is a grocery store with aisles of various products:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个拥有各类商品货架的杂货店：
- en: '![](img/00107.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00107.jpeg)'
- en: Assume that all sales at the grocery store are stored in some database and that
    you have access to the data for the last 3 months. Typically, businesses store
    data for years as you need sufficient data over a period of time to establish
    any hypothesis or observe any patterns. In this example, our goal is to perform
    better placement of products in various aisles based on how customers are buying
    the products. One hypothesis is that customers often buy products, that are both
    at eye level and also close together. For instance, if Milk is on one corner of
    the store and Yogurt is in other corner of the store, some customers might just
    choose either Milk or Yogurt and just leave the store, causing a loss of business.
    More adverse affects might result in customers choosing another store where products
    are better placed because if the feeling that *things are hard to find at this
    store*. Once that feeling sets in, it also percolates to friends and family eventually
    causing a bad social presence. This phenomenon is not uncommon in the real world
    causing some businesses to succeed while others fail while both seem to be very
    similar in products and prices.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设所有的超市销售数据都存储在某个数据库中，并且你可以访问过去3个月的数据。通常情况下，企业会存储多年的数据，因为你需要一定时间范围的数据来建立假设或观察到某些模式。在这个例子中，我们的目标是根据客户购买产品的方式，优化商品在各个过道的摆放。一个假设是，客户通常购买那些既在眼平线位置，又相互靠近的商品。例如，如果牛奶在商店的一角，而酸奶在商店的另一角，一些客户可能只会选择牛奶或酸奶，然后直接离开商店，这将导致销售损失。更严重的影响可能是客户选择另一个商店，因为那里的商品摆放更好，他们会觉得*在这个商店里很难找到东西*。一旦产生这种感觉，它也会传递给朋友和家人，最终导致糟糕的社交形象。这种现象在现实世界中并不少见，它导致一些企业成功，而另一些则失败，尽管两者在产品和价格上非常相似。
- en: There are many ways to approach this problem starting from customer surveys
    to professional statisticians to machine learning scientists. Our approach will
    be to understand what we can from just the sales transactions alone.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法有很多种，从客户调查到专业统计学家，再到机器学习科学家。我们的方法将是仅通过销售交易记录来理解我们能得出的信息。
- en: 'The following is an example of what the transactions might look like:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是交易记录可能的样子：
- en: '![](img/00111.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00111.jpeg)'
- en: 'The following are the steps you could follow as part of EDA:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你可以作为EDA步骤的一部分进行的操作：
- en: Calculate *Average number of products bought per day = Total of all products
    sold in a day / Total number of receipts for the* *day*.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算*每天购买的平均产品数量 = 每天销售的所有产品总数 / 该天的收据总数*。
- en: Repeat the preceding step for last 1 week, month, and quarter.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对前述步骤进行重复，分别针对最近1周、1个月和1季度进行分析。
- en: Try to understand if there is a difference between weekends and weekdays and
    also time of the day (morning, noon, and evening)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试理解周末和平日之间是否存在差异，以及一天中的不同时间段（早晨、正午、傍晚）是否有区别。
- en: For each product, create a list of all other products to see which products
    are usually bought together (same receipt)
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个产品创建一个列表，列出所有其他产品，看看哪些产品通常会一起购买（同一张收据）
- en: Repeat the preceding step for 1 day, 1 week, month, and quarter.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对1天、1周、1个月和1季度重复前述步骤。
- en: Try to determine which products should be placed closer together by the number
    of transactions (sorted in descending order).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试通过交易次数（按降序排序）来确定哪些产品应该放得更近。
- en: Once we have completed the preceding 6 steps, we can try to reach some conclusions
    for CDA.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前述的6个步骤后，我们可以尝试为CDA得出一些结论。
- en: 'Let''s assume this is the output we get:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这是我们得到的输出：
- en: '| **Item** | **Day Of Week** | **Quantity** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **项目** | **星期几** | **数量** |'
- en: '| Milk | Sunday | 1244 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 牛奶 | 星期天 | 1244 |'
- en: '| Bread | Monday | 245 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 面包 | 周一 | 245 |'
- en: '| Milk | Monday | 190 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 牛奶 | 周一 | 190 |'
- en: 'In this case, we could state that **Milk** is bought more on *weekends* so
    its better to increase the quantity and variety of Milk products over weekends.
    Take a look at the following table:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以指出，**牛奶**在*周末*购买得更多，因此最好在周末增加牛奶产品的数量和种类。看看下面的表格：
- en: '| **Item1** | **Item2** | **Quantity** |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **项目1** | **项目2** | **数量** |'
- en: '| Milk | Eggs | 360 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 牛奶 | 鸡蛋 | 360 |'
- en: '| Bread | Cheese | 335 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 面包 | 奶酪 | 335 |'
- en: '| Onions | Tomatoes | 310 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 洋葱 | 西红柿 | 310 |'
- en: In this case, we could state that **Milk** and **Eggs** are bought by *more*
    customers in one purchase followed by **Bread** and **Cheese.** So, we could recommend
    that the store realigns the aisles and shelves to move **Milk** and **Eggs** *closer*
    to each other.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以指出，**牛奶**和**鸡蛋**通常会被*更多*的客户在一次购买中选购，其次是**面包**和**奶酪**。因此，我们建议商店重新排列过道和货架，将**牛奶**和**鸡蛋**放得*更近*。
- en: 'The two conclusions we have are:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得出的两个结论是：
- en: '**Milk** is bought more on *weekends,* so it''s better to increase the quantity
    and variety of Milk products over weekends.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**牛奶**在*周末*购买量更多，因此最好在周末增加牛奶产品的数量和种类。'
- en: '**Milk** and **Eggs** are bought by *more* customers in one purchase followed
    by **Bread** and **Cheese.** So, we could recommend that the store realigns the
    aisles and shelves to move **Milk** and **Eggs** *closer* to each other.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**牛奶**和**鸡蛋**在一次购买中被*更多*的顾客购买，其次是**面包**和**奶酪**。因此，我们建议商店重新调整货架和过道，将**牛奶**和**鸡蛋**放得*更近*一些。'
- en: Conclusions are usually tracked over a period of time to evaluate the gains.
    If there is no significant impact on sales even after adopting the preceding two
    recommendations for 6 months, we simply invested in the recommendations which
    are not able to give you a good Return On Investment (ROI).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 结论通常会在一段时间内进行跟踪，以评估效果。如果即使在采用前两个建议6个月后销售没有显著影响，那么我们就可以认定这些建议没有带来良好的投资回报率（ROI）。
- en: Similarly, you can also perform some analysis with respect to the Profit margin
    and pricing optimizations. This is why you will typically see a single item costing
    more than the average of multiple numbers of the same item bought. Buy one Shampoo
    for $7 or two bottles of Shampoo for $12.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，你也可以针对利润率和定价优化进行一些分析。这就是为什么你通常会看到单一商品的价格高于同类多个商品的平均价格。例如，买一瓶洗发水需要$7，而买两瓶洗发水则只需要$12。
- en: Think about other aspects you can explore and recommend for the grocery store.
    For example, can you guess which products to position near checkout registers
    just based on fact that these have no affinity toward any particular product--chewing
    gum, magazines, and so on.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想你可以为杂货店探索并推荐的其他方面。例如，你能仅仅通过这些产品没有特别的关联性——比如口香糖、杂志等，来推测哪些产品应当放在收银台附近吗？
- en: Data analytics initiatives support a wide variety of business uses. For example,
    banks and credit card companies analyze withdrawal and spending patterns to prevent
    fraud and identity theft. Advertising companies analyze website traffic to identify
    prospects with a high likelihood of conversion to a customer. Department stores
    analyze customer data to figure out if better discounts will help boost sales.
    Cell Phone operators can figure out pricing strategies. Cable companies are constantly
    looking for customers who are likely to churn unless given some offer or promotional
    rate to retain their customer. Hospitals and pharmaceutical companies analyze
    data to come up with better products and detect problems with prescription drugs
    or measure the performance of prescription drugs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析项目支持各种商业用途。例如，银行和信用卡公司分析取款和消费模式以防止欺诈和身份盗窃。广告公司分析网站流量，识别那些有较高转化为顾客可能性的潜在客户。百货商店分析顾客数据，以判断更优惠的折扣是否有助于提升销售。手机运营商可以找出定价策略。有线电视公司不断寻找那些可能会流失的客户，除非提供某种优惠或促销价格来留住他们。医院和制药公司分析数据，以开发更好的产品，发现处方药的问题或评估处方药的效果。
- en: Inside the data analytics process
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析过程中的内容
- en: Data analytics applications involve more than just analyzing data. Before any
    analytics can be planned, there is also a need to invest time and effort in collecting,
    integrating, and preparing data, checking the quality of the data and then developing,
    testing, and revising analytical methodologies. Once data is deemed ready, data
    analysts and scientists can explore and analyze the data using statistical methods
    such as SAS or machine learning models using Spark ML. The data itself is prepared
    by data engineering teams and the data quality team checks the data collected.
    Data governance becomes a factor too to ensure the proper collection and protection
    of the data. Another not commonly known role is that of a Data Steward who specializes
    in understanding data to the byte, exactly where it is coming from, all transformations
    that occur, and what the business really needs from the column or field of data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析应用不仅仅是分析数据。在进行任何分析之前，还需要花时间和精力收集、整合和准备数据，检查数据质量，然后开发、测试和修订分析方法。数据一旦准备好，数据分析师和科学家们就可以利用统计方法（如SAS）或使用Spark
    ML的机器学习模型对数据进行探索和分析。数据本身由数据工程团队准备，而数据质量团队则负责检查收集的数据。数据治理也是一个需要考虑的因素，以确保数据的正确收集和保护。另一个不太为人所知的角色是数据管家，他们专注于理解数据的每个细节，准确了解数据的来源、所有的转换过程以及业务对某一列或数据字段的真正需求。
- en: Various entities in the business might be dealing with addresses differently,
    **123 N Main St** as opposed to **123 North Main Street.** But, our analytics
    depends on getting the correct address field; otherwise both the addresses mentioned
    above will be considered different and our analytics will not have the same accuracy.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 业务中的不同实体可能以不同方式处理地址，如**123 N Main St**与**123 North Main Street**。但是，我们的分析依赖于获取正确的地址字段；否则，上述两个地址将被视为不同，导致我们的分析准确性降低。
- en: The analytics process starts with data collection based on what the analysts
    might need from the data warehouse, collecting all sorts of data in the organization
    (Sales, Marketing, Employee, Payroll, HR, and so on). Data stewards and the Governance
    team are important here to make sure the right data is collected and that any
    information deemed confidential or private is not accidentally exported out even
    if the end users are all employees.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 分析过程始于根据分析师可能需要的数据仓库中的数据进行数据收集，收集组织中各种数据（如销售、市场营销、员工、薪资、HR等）。数据管理员和治理团队在这里非常重要，确保收集到正确的数据，并且任何被认为是机密或私密的信息不会被意外导出，即使最终用户都是员工。
- en: Social Security Numbers or full addresses might not be a good idea to include
    in analytics as this can cause a lot of problems to the organization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 社会保障号码或完整地址可能不适合包含在分析中，因为这可能会给组织带来许多问题。
- en: Data quality processes must be established to make sure the data being collected
    and engineered is correct and will match the needs of the data scientists. At
    this stage, the main goal is to find and fix data quality problems that could
    affect the accuracy of analytical needs. Common techniques are profiling the data
    and cleansing the data to make sure that the information in a dataset is consistent,
    and also that any errors and duplicate records are removed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 必须建立数据质量流程，以确保收集和处理的数据是正确的，并且能够满足数据科学家的需求。在这一阶段，主要目标是发现和解决可能影响分析需求准确性的数据质量问题。常用的技术包括数据概况分析和数据清洗，以确保数据集中的信息一致，并删除任何错误和重复记录。
- en: Data from disparate source systems may need to be combined, transformed, and
    normalized using various data engineering techniques, such as distributed computing
    or MapReduce programming, Stream processing, or SQL queries, and then stored on
    Amazon S3, Hadoop cluster, NAS, or SAN storage devices or a traditional data warehouse
    such as Teradata. Data preparation or engineering work involves techniques to
    manipulate and organize the data for the planned analytics use.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 来自不同源系统的数据可能需要使用各种数据工程技术进行合并、转换和规范化，例如分布式计算或MapReduce编程、流处理或SQL查询，然后将数据存储在Amazon
    S3、Hadoop集群、NAS或SAN存储设备上，或者传统的数据仓库，如Teradata。数据准备或工程工作涉及使用技术来操作和组织数据，以满足计划中的分析需求。
- en: Once we have the data prepared and checked for quality, and it is available
    for the Data scientists or analysts to use, the actual analytical work starts.
    A Data scientist can now build an analytical model using predictive modeling tools
    and languages such as SAS, Python, R, Scala, Spark, H2O, and so on. The model
    is initially run against a partial dataset to test its accuracy in the *training
    phase*. Several iterations of the training phase are common and expected in any
    analytical project. After adjustments at the model level, or sometimes going all
    the way to the Data Steward to get or fix some data being collected or prepared,
    the model output tends to get better and better. Finally, a stable state is reached
    when further tuning does not change the outcome noticeably; at this time, we can
    think of the model as being ready for production usage.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好数据并检查数据质量，并且数据可供数据科学家或分析师使用，实际的分析工作就开始了。数据科学家可以使用预测建模工具和语言（如SAS、Python、R、Scala、Spark、H2O等）构建分析模型。该模型最初会在部分数据集上运行，以测试其在*训练阶段*中的准确性。训练阶段通常会进行多次迭代，这是任何分析项目中都很常见的。经过模型层面的调整，或者有时需要回到数据管理员处获取或修复正在收集或准备的一些数据，模型的输出会越来越好。最终，当进一步调整不再显著改变结果时，就达到了稳定状态；此时，我们可以认为该模型已准备好投入生产使用。
- en: Now, the model can be run in production mode against the full dataset and generate
    outcomes or results based on how we trained the model. The choices made in building
    the analysis, either statistical or machine learning, directly affect the quality
    and the purpose of the model. You cannot look at the sales from groceries and
    figure out if Asians buy more milk than Mexicans as that needs additional elements
    from demographical data. Similarly, if our analysis was focused on customer experience
    (returns or exchanges of products) then it is based on different techniques and
    models than if we are trying to focus on revenue or up-sell customers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型可以在生产模式下针对完整数据集进行运行，并根据我们训练模型的方式生成结果或输出。无论是统计分析还是机器学习，构建分析时做出的选择直接影响模型的质量和目的。你无法仅凭杂货销售数据判断亚洲人是否比墨西哥人买更多牛奶，因为这需要额外的、来自人口统计数据的元素。同样，如果我们的分析重点是客户体验（产品的退货或换货），那么它基于的技术和模型与我们试图关注收入或向客户推销的模型是不同的。
- en: You will see various machine learning techniques in later chapters.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在后续章节中看到各种机器学习技术。
- en: Analytical applications can thus be realized using several disciplines, teams,
    and skillsets. Analytical applications can be used to generate reports all the
    way to automatically triggering business actions. For example, you can simply
    create daily sales reports to be emailed out to all managers every day at 8 a.m.
    in the morning. But, you can also integrate with Business process management applications
    or some custom stock trading application to take action, such as buying, selling,
    or alerting on activities in the stock market. You can also think of taking in
    news articles or social media information to further influence the decisions to
    be made.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 分析应用可以通过多个学科、团队和技能组合来实现。分析应用可以用于生成报告，也可以自动触发业务动作。例如，你可以简单地创建每日销售报告，并在每天早上8点通过电子邮件发送给所有经理。但你也可以与业务流程管理应用程序或一些定制的股票交易应用程序集成，执行一些操作，如买入、卖出或对股市活动进行提醒。你还可以考虑引入新闻文章或社交媒体信息，以进一步影响决策的制定。
- en: Data visualization is an important piece of data analytics and it's hard to
    understand numbers when you are looking at a lot of metrics and calculation. Rather,
    there is an increasing dependence on **Business Intelligence** (**BI**) tools,
    such as Tableau, QlikView, and so on, to explore and analyze data. Of course,
    large-scale visualization such as showing all Uber cars in the country or heat
    maps showing the water supply in New York City requires more custom applications
    or specialized tools to be built.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化是数据分析中的一个重要部分，当你面对大量的指标和计算时，理解数字变得非常困难。相反，越来越多地依赖于**商业智能**（**BI**）工具，如
    Tableau、QlikView 等，来探索和分析数据。当然，大规模的可视化，例如显示全国所有 Uber 车辆或显示纽约市水供应的热力图，需要构建更多的定制应用或专门的工具。
- en: Managing and analyzing data has always been a challenge across many organizations
    of different sizes across all industries. Businesses have always struggled to
    find a pragmatic approach to capturing information about their customers, products,
    and services. When the company only had a handful of customers who bought a few
    of their items, it was not that difficult. It was not as big a challenge. But
    over time, companies in the markets started growing. Things have become more complicated.
    Now, we have branding Information and social media. We have things that are sold
    and bought over the Internet. We need to come up with different solutions. Web
    development, organizations, pricing, social networks, and segmentations; there's
    a lot of different data that we're dealing with that brings a lot more complexity
    when it comes to dealing, managing, organizing, and trying to gain some insight
    from the data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在各行各业，不同规模的组织一直面临着数据管理和分析的挑战。企业一直在努力寻找一种务实的方法来捕捉有关客户、产品和服务的信息。当公司只有少数几个客户并且他们只购买几种商品时，这并不难。这不是一个大挑战。但随着时间的推移，市场中的公司开始增长，情况变得更加复杂。现在，我们有品牌信息和社交媒体，有通过互联网买卖的商品。我们需要提出不同的解决方案。网站开发、组织、定价、社交网络和细分市场；我们正在处理许多不同的数据，这使得在处理、管理、组织数据以及试图从数据中获得见解时变得更加复杂。
- en: Introduction to big data
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据简介
- en: As seen in the preceding section, data analytics incorporates techniques, tools,
    and methodologies to explore and analyze data to produce quantifiable outcomes
    for the business. The outcome could be a simple choice of a color to paint the
    storefront or more complicated predictions of customer behavior. As businesses
    grow, more and more varieties of analytics are coming into the picture. In 1980s
    or 1990s , all we could get was what was available in a SQL Data Warehouse; nowadays
    a lot of external factors are all playing an important role in influencing the
    way businesses run.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，数据分析结合了技术、工具和方法，以探索和分析数据，从而为企业提供可量化的成果。结果可能是简单地选择一种颜色来粉刷店面，或者是更复杂的客户行为预测。随着企业的增长，越来越多种类的分析方法开始出现在视野中。在1980年代或1990年代，我们能得到的只是SQL数据仓库中可用的数据；而如今，许多外部因素都在发挥重要作用，影响着企业的运营方式。
- en: Twitter, Facebook, Amazon, Verizon, Macy's, and Whole Foods are all companies
    that run their business using data analytics and base many of the decisions on
    it. Think about what kind of data they are collecting, how much data they might
    be collecting, and then how they might be using the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter、Facebook、Amazon、Verizon、Macy's 和 Whole Foods 等公司都在利用数据分析运营业务，并基于此做出许多决策。想一想他们正在收集什么样的数据，收集了多少数据，以及他们可能如何使用这些数据。
- en: Let's look at our grocery store example seen earlier. What if the store starts
    expanding its business to set up 100s of stores. Naturally, the sales transactions
    will have to be collected and stored on a scale that is 100s of times more than
    the single store. But then, no business works independently any more. There is
    a lot of information out there starting from local news, tweets, yelp reviews,
    customer complaints, survey activities, competition from other stores, changing
    demographics, or the economy of the local area, and so on. All such additional
    data can help in better understanding customer behavior and revenue models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下之前提到的杂货店例子。如果商店开始扩展业务，开设数百家分店，显然，销售交易的数据将需要在比单个商店大100倍的规模上进行收集和存储。但此时，任何企业都不再是独立运作的。外部有大量的信息，包括本地新闻、推特、Yelp评论、客户投诉、调查活动、其他商店的竞争、人口变化以及当地经济状况等等。所有这些额外的数据都能帮助更好地理解客户行为和收入模型。
- en: For example, if we see increasing negative sentiment regarding the store parking
    facility, then we could analyze this and take corrective action such as validated
    parking or negotiating with the city public transportation department to provide
    more frequent trains or buses for better reach.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们发现关于商店停车设施的负面情绪在增加，那么我们可以分析这种情况并采取纠正措施，比如验证停车位，或者与城市公共交通部门谈判，提供更频繁的列车或公交服务，以提高到达的便利性。
- en: Such increasing quantity and a variety of data while provides better analytics
    also poses challenges to the business IT organization trying to store, process,
    and analyze all the data. It is, in fact, not uncommon to see TBs of data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数量庞大且多样化的数据，虽然提供了更好的分析能力，但也对企业IT组织提出了挑战，要求它们存储、处理和分析所有数据。事实上，看到TB级别的数据并不罕见。
- en: Every day, we create more than 2 quintillion bytes of data (2 Exa Bytes), and
    it is estimated that more than 90% of the data has been generated in the last
    few years alone.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每天，我们都会创造超过2亿亿字节的数据（2 Exa 字节），而且估计超过90%的数据仅在过去几年内生成。
- en: '**1 KB = 1024 Bytes**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 KB = 1024 字节**'
- en: '**1 MB = 1024 KB**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 MB = 1024 KB**'
- en: '**1 GB = 1024 MB**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 GB = 1024 MB**'
- en: '**1 TB = 1024 GB ~ 1,000,000 MB**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 TB = 1024 GB ~ 1,000,000 MB**'
- en: '**1 PB = 1024 TB ~ 1,000,000 GB ~ 1,000,000,000 MB**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 PB = 1024 TB ~ 1,000,000 GB ~ 1,000,000,000 MB**'
- en: '**1 EB = 1024 PB ~ 1,000,000 TB ~ 1,000,000,000 GB ~ 1,000,000,000,000 MB**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 EB = 1024 PB ~ 1,000,000 TB ~ 1,000,000,000 GB ~ 1,000,000,000,000 MB**'
- en: Such large amounts of data since the 1990s, and the need to understand and make
    sense of the data, gave rise to the term *big data*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自1990年代以来，大量数据的出现以及对这些数据的理解和利用需求，催生了*大数据*这一术语。
- en: The term big data, which spans computer science and statistics/econometrics,
    probably originated in the lunch-table conversations at Silicon Graphics in the
    mid-1990s, in which John Mashey figured prominently.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据这一术语，跨越了计算机科学和统计学/计量经济学领域，可能源于1990年代中期硅图形公司（Silicon Graphics）午餐桌上的讨论，其中约翰·马谢（John
    Mashey）是重要人物。
- en: In 2001, Doug Laney, then an analyst at consultancy Meta Group Inc (which got
    acquired by Gartner) introduced the idea of 3Vs (variety, velocity, and volume).
    Now, we refer to 4 Vs instead of 3Vs with the addition of Veracity of data to
    the 3Vs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 2001年，Doug Laney，当时是咨询公司Meta Group Inc（后被Gartner收购）的分析师，提出了3Vs（多样性、速度和体积）的概念。现在，我们使用4Vs而不是3Vs，增加了数据的真实性（Veracity）这一项。
- en: 4 Vs of big data
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据的4个V
- en: The following are the 4 Vs of big data used to describe the properties of big
    data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是大数据的4个V，用于描述大数据的特性。
- en: Variety of Data
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据种类
- en: Data can be from weather sensors, car sensors, census data, Facebook updates,
    tweets, transactions, sales, and marketing. The data format is both structured
    and unstructured as well. Data types can also be different; binary, text, JSON,
    and XML.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以来自天气传感器、汽车传感器、人口普查数据、Facebook更新、推文、交易、销售和营销。数据格式既有结构化数据也有非结构化数据，数据类型也各不相同；二进制、文本、JSON和XML。
- en: Velocity of Data
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的速度
- en: Data can be obtained from a data warehouse, batch mode file archives, near real-time
    updates, or instantaneous real-time updates from the Uber ride you just booked.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以来自数据仓库、批量模式文件存档、近实时更新或您刚刚预订的Uber行程的即时实时更新。
- en: Volume of Data
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据量
- en: Data can be collected and stored for an hour, a day, a month, a year, or 10
    years. The size of data is growing to 100s of TBs for many companies.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以被收集和存储一个小时、一整天、一整月、一整年，甚至长达10年。许多公司的数据量正在增长，达到数百TB。
- en: Veracity of Data
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的真实性
- en: Data can be analyzed for actionable insights, but with so much data of all types
    being analyzed from across data sources, it is very difficult to ensure correctness
    and proof of accuracy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以被分析以获得可操作的洞察，但由于来自不同数据源的大量各种类型的数据被分析，确保数据的正确性和准确性证明非常困难。
- en: 'The following are the 4 Vs of big data:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是大数据的4个V：
- en: '![](img/00115.jpeg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00115.jpeg)'
- en: To make sense of all the data and apply data analytics to big data, we need
    to expand the concept of data analytics to operate at a much larger scale dealing
    with the 4 Vs of big data. This changes not only the tools, technologies, and
    methodologies used in analyzing data, but also the way we even approach the problem.
    If a SQL database was used for data in a business in 1999, now to handle the data
    for the same business we will need a distributed SQL database scalable and adaptable
    to the nuances of the big data space.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解所有这些数据并将数据分析应用于大数据，我们需要扩展数据分析的概念，以更大规模地操作，处理大数据的4个V。这不仅改变了分析数据时使用的工具、技术和方法，还改变了我们处理问题的方式。如果1999年某个企业使用SQL数据库来处理数据，那么现在处理同一企业的数据，我们需要一个可扩展且能够适应大数据领域细微差别的分布式SQL数据库。
- en: Big data analytics applications often include data from both internal systems
    and external sources, such as weather data or demographic data on consumers compiled
    by third-party information services providers. In addition, streaming analytics
    applications are becoming common in big data environments, as users look to do
    real-time analytics on data fed into Hadoop systems through Spark's Spark streaming
    module or other open source stream processing engines, such as Flink and Storm.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据分析应用通常包括来自内部系统和外部来源的数据，例如天气数据或由第三方信息服务提供商编制的消费者人口统计数据。此外，随着用户希望对通过Spark的Spark
    Streaming模块或其他开源流处理引擎（如Flink和Storm）将数据传入Hadoop系统进行实时分析，流分析应用在大数据环境中变得越来越普遍。
- en: Early big data systems were mostly deployed on-premises particularly in large
    organizations that were collecting, organizing, and analyzing massive amounts
    of data. But cloud platform vendors, such as **Amazon Web Services** (**AWS**)
    and Microsoft, have made it easier to set up and manage Hadoop clusters in the
    cloud, as have Hadoop suppliers such as Cloudera and Hortonworks, which support
    their distributions of the big data framework on the AWS and Microsoft Azure clouds.
    Users can now spin up clusters in the cloud, run them for as long as needed, and
    then take them offline, with usage-based pricing that doesn't require ongoing
    software licenses.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的大数据系统主要部署在本地，特别是在收集、组织和分析海量数据的大型组织中。然而，云平台供应商，如**亚马逊 Web 服务**（**AWS**）和微软，已经使得在云中设置和管理Hadoop集群变得更加容易，像Cloudera和Hortonworks这样的Hadoop供应商也支持其大数据框架的分发版本在AWS和Microsoft
    Azure云上运行。现在，用户可以在云中快速启动集群，根据需要运行，并在使用完毕后将其下线，按需计费，无需持续的软件许可。
- en: Potential pitfalls that can trip up organizations on big data analytics initiatives
    include a lack of internal analytics skills and the high cost of hiring experienced
    data scientists and data engineers to fill the gaps.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据分析项目中可能遇到的潜在陷阱包括缺乏内部分析技能，以及招聘经验丰富的数据科学家和数据工程师填补空缺的高昂成本。
- en: The amount of data that's typically involved, and its variety, can cause data
    management issues in areas including data quality, consistency, and governance;
    also, data silos can result from the use of different platforms and data stores
    in a big data architecture. In addition, integrating Hadoop, Spark, and other
    big data tools into a cohesive architecture that meets an organization's big data
    analytics needs is a challenging proposition for many IT and analytics teams,
    which have to identify the right mix of technologies and then put the pieces together.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及的数据量及其多样性可能会导致数据管理问题，涉及数据质量、一致性和治理等领域；此外，使用不同平台和数据存储在大数据架构中可能会导致数据孤岛问题。与此同时，将
    Hadoop、Spark 和其他大数据工具整合到一个有凝聚力的架构中，以满足组织的大数据分析需求，对于许多 IT 和分析团队来说是一项具有挑战性的任务，他们必须找出合适的技术组合并将其拼接在一起。
- en: Distributed computing using Apache Hadoop
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Hadoop 进行分布式计算
- en: Our world is filled with devices starting from the smart refrigerator, smart
    watch, phone, tablet, laptops, kiosks at the airport, ATM dispensing cash to you,
    and many many more. We are able to do things we could not even imagine just a
    few years ago. Instagram, Snapchat, Gmail, Facebook, Twitter, and Pinterest are
    a few of the applications we are now so used to; it is difficult to imagine a
    day without access to such applications.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的世界充满了各种设备，从智能冰箱、智能手表、手机、平板电脑、笔记本电脑，到机场的自助服务机、为你提供现金的 ATM 机，等等。我们能够做出几年前我们无法想象的事情。Instagram、Snapchat、Gmail、Facebook、Twitter
    和 Pinterest 是我们现在已经习以为常的一些应用，几乎无法想象没有这些应用的一天。
- en: With the advent of Cloud computing, using a few clicks we are able to launch
    100s if not, 1000s of machines in AWS, Azure (Microsoft), or Google Cloud among
    others and use immense resources to realize our business goals of all sorts.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 随着云计算的出现，我们只需几次点击，就能在 AWS、Azure（微软）或 Google Cloud 等平台上启动数百甚至数千台机器，利用庞大的资源实现各种业务目标。
- en: Cloud computing has introduced us to the concepts of IaaS, PaaS, and SaaS, which
    gives us the ability to build and operate scalable infrastructures serving all
    types of use cases and business needs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算引入了 IaaS、PaaS 和 SaaS 的概念，使我们能够构建和运营可扩展的基础设施，以服务于各种类型的使用场景和业务需求。
- en: '**IaaS** (**Infrastructure as a Service**) - Reliable-managed hardware is provided
    without the need for a Data center, power cords, Airconditioning, and so on.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**IaaS**（**基础设施即服务**）- 提供可靠的托管硬件，无需数据中心、电源线、空调等设施。'
- en: '**PaaS** (**Platform as a Service**) - On top of IaaS, managed platforms such
    as Windows, Linux , Databases and so on are provided.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**PaaS**（**平台即服务**）- 在 IaaS 基础上，提供托管的平台，如 Windows、Linux、数据库等。'
- en: '**SaaS** (**Software as a Service**) - On top of SaaS, managed services such
    as SalesForce, [Kayak.com](https://www.kayak.co.in/?ispredir=true) and so on are
    provided to everyone.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**SaaS**（**软件即服务**）- 在 SaaS 基础上，提供托管服务，如 SalesForce、[Kayak.com](https://www.kayak.co.in/?ispredir=true)
    等，供所有人使用。'
- en: Behind the scenes is the world of highly scalable distributed computing, which
    makes it possible to store and process PB (PetaBytes) of data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后是高度可扩展的分布式计算世界，它使得存储和处理 PB（PetaBytes）级别的数据成为可能。
- en: 1 ExaByte = 1024 PetaBytes (50 Million Blue Ray Movies)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 1 ExaByte = 1024 PetaBytes (5000 万部蓝光电影)
- en: 1 PetaByte = 1024 Tera Bytes (50,000 Blue Ray Movies)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 1 PetaByte = 1024 Tera Bytes (50,000 部蓝光电影)
- en: 1 TeraByte = 1024 Giga Bytes (50 Blue Ray Movies)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 1 TeraByte = 1024 Giga Bytes (50 部蓝光电影)
- en: Average size of 1 Blue Ray Disc for a Movie is ~ 20 GB
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 1 部蓝光电影的平均光盘大小约为 20 GB
- en: Now, the paradigm of Distributed Computing is not really a genuinely new topic
    and has been pursued in some shape or form over decades primarily at research
    facilities as well as by a few commercial product companies. **Massively Parallel
    Processing** (**MPP**) is a paradigm that was in use decades ago in several areas
    such as Oceanography, Earthquake monitoring, and Space exploration. Several companies
    such as Teradata also implemented MPP platforms and provided commercial products
    and applications. Eventually, tech companies such as Google and Amazon among others
    pushed the niche area of scalable distributed computing to a new stage of evolution,
    which eventually led to the creation of Apache Spark by Berkeley University.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，分布式计算的范式并不是一个真正的新话题，几十年来，它在研究机构以及一些商业公司中以某种形式得到追求。**大规模并行处理**（**MPP**）是几十年前在多个领域（如海洋学、地震监测和太空探索）使用的一种范式。一些公司，如
    Teradata，也实施了 MPP 平台并提供了商业产品和应用程序。最终，像 Google 和 Amazon 等科技公司推动了可扩展分布式计算的细分领域，进入了一个新的进化阶段，这最终导致了伯克利大学创建了
    Apache Spark。
- en: Google published a paper on **Map Reduce** (**MR**) as well as **Google File
    System** (**GFS**), which brought the principles of distributed computing to everyone.
    Of course, due credit needs to be given to Doug Cutting, who made it possible
    by implementing the concepts given in the Google white papers and introducing
    the world to Hadoop.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Google 发布了关于 **Map Reduce**（**MR**）和 **Google 文件系统**（**GFS**）的论文，将分布式计算的原理传递给了每个人。当然，也需要给予
    Doug Cutting 应有的荣誉，他通过实现 Google 白皮书中的概念，并向世界介绍了 Hadoop，使这一切成为可能。
- en: The Apache Hadoop Framework is an open source software framework written in
    Java. The two main areas provided by the framework are storage and processing.
    For Storage, the Apache Hadoop Framework uses **Hadoop Distributed File System**
    (**HDFS**), which is based on the Google File System paper released on October
    2003\. For processing or computing, the framework depends on MapReduce, which
    is based on a Google paper on MR released in December 2004.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop 框架是一个用 Java 编写的开源软件框架。该框架提供的两个主要功能是存储和处理。在存储方面，Apache Hadoop 框架使用
    **Hadoop 分布式文件系统**（**HDFS**），该文件系统基于 2003 年 10 月发布的 Google 文件系统论文。在处理或计算方面，框架依赖于
    MapReduce，该框架基于 2004 年 12 月发布的 Google 关于 MR 的论文。
- en: The MapReduce framework evolved from V1 (based on Job Tracker and Task Tracker)
    to V2 (based on YARN).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 框架从 V1（基于作业跟踪器和任务跟踪器）发展到 V2（基于 YARN）。
- en: Hadoop Distributed File System (HDFS)
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop 分布式文件系统（HDFS）
- en: HDFS is a software-based filesystem implemented in Java and sits on top of the
    native file system. The main concept behind HDFS is that it divides a file into
    blocks (typically 128 MB) instead of dealing with a file as a whole. This allowed
    many features such as distribution, replication, failure recovery, and more importantly
    distributed processing of the blocks using multiple machines.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 是一个基于软件的文件系统，使用 Java 实现，运行在本地文件系统之上。HDFS 的主要概念是将文件分割成块（通常为 128 MB），而不是将文件视为整体处理。这使得许多功能成为可能，如分布式存储、数据复制、故障恢复，以及更重要的，使用多台机器对这些块进行分布式处理。
- en: Block sizes can be 64 MB, 128 MB, 256 MB, or 512 MB, whatever suits the purpose.
    For a 1 GB file with 128 MB blocks, there will be 1024 MB / 128 MB = 8 blocks.
    If you consider replication factor of 3, this makes it 24 blocks.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小可以是 64 MB、128 MB、256 MB 或 512 MB，根据需求选择。对于一个 1 GB 的文件，使用 128 MB 的块，计算方式为
    1024 MB / 128 MB = 8 块。如果考虑复制因子为 3，则总共有 24 块。
- en: 'HDFS provides a distributed storage system with fault tolerance and failure
    recovery. HDFS has two main components: name node and data node(s). Name node
    contains all the metadata of all content of the file system. Data nodes connect
    to the Name Node and rely on the name node for all metadata information regarding
    the content in the file system. If the name node does not know any information,
    data node will not be able to serve it to any client who wants to read/write to
    the HDFS.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 提供了一个具有容错性和故障恢复功能的分布式存储系统。HDFS 主要有两个组件：名称节点和数据节点。名称节点包含文件系统所有内容的所有元数据。数据节点与名称节点连接，并依赖名称节点获取有关文件系统内容的所有元数据。如果名称节点不知道任何信息，数据节点将无法为任何需要读写
    HDFS 的客户端提供服务。
- en: 'The following is the HDFS architecture:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 HDFS 架构：
- en: '![](img/00120.jpeg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00120.jpeg)'
- en: NameNode and DataNode are JVM processes so any machine that supports Java can
    run the NameNode or the DataNode process. There is only one NameNode (the second
    NameNode will be there too if you count the HA deployment) but 100s if not 1000s
    of DataNodes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: NameNode 和 DataNode 是 JVM 进程，因此任何支持 Java 的机器都可以运行 NameNode 或 DataNode 进程。只有一个
    NameNode（如果计算 HA 部署，第二个 NameNode 也会存在），但是有数百甚至上千个 DataNode。
- en: It is not advisable to have 1000s of DataNodes because all operations from all
    the DataNodes will tend to overwhelm the NameNode in a real production environment
    with a lot of data-intensive applications.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 不建议拥有上千个 DataNode，因为所有 DataNode 的操作会在实际生产环境中倾向于压倒 NameNode，尤其是在有大量数据密集型应用的情况下。
- en: The existence of a single NameNode in a cluster greatly simplifies the architecture
    of the system. The NameNode is the arbitrator and repository for all HDFS metadata
    and any client, that wants to read/write data first contacts the NameNode for
    the metadata information. The data never flows directly through the NameNode,
    which allows 100s of DataNodes (PBs of data) to be managed by 1 NameNode.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中只有一个 NameNode，这极大简化了系统的架构。NameNode 是所有 HDFS 元数据的仲裁者和存储库，任何想要读写数据的客户端都必须首先联系
    NameNode 以获取元数据信息。数据不会直接通过 NameNode 流动，这使得 1 个 NameNode 能够管理数百个 DataNode（PB 级数据）。
- en: HDFS supports a traditional hierarchical file organization with directories
    and files similar to most other filesystems. You can create, move, and delete
    files, and directories. The NameNode maintains the filesystem namespace and records
    all changes and the state of the filesystem. An application can specify the number
    of replicas of a file that should be maintained by HDFS and this information is
    also stored by the NameNode.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 支持传统的层次化文件组织结构，具有类似于大多数其他文件系统的目录和文件。您可以创建、移动和删除文件和目录。NameNode 维护文件系统的命名空间，并记录所有更改和文件系统的状态。应用程序可以指定
    HDFS 应该维护的文件副本数量，这些信息也由 NameNode 存储。
- en: HDFS is designed to reliably store very large files in a distributed manner
    across machines in a large cluster of data nodes. To deal with replication, fault
    tolerance, as well as distributed computing, HDFS stores each file as a sequence
    of blocks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 旨在以分布式的方式可靠地存储非常大的文件，这些文件分布在大型数据节点集群中的多台机器上。为了应对复制、容错以及分布式计算，HDFS 将每个文件存储为一系列块。
- en: The NameNode makes all decisions regarding the replication of blocks. This is
    mainly dependent on a Block report from each of the DataNodes in the cluster received
    periodically at a heart beat interval. A block report contains a list of all blocks
    on a DataNode, which the NameNode then stores in its metadata repository.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: NameNode 做出所有有关块复制的决策。这主要依赖于来自集群中每个 DataNode 的块报告，块报告会定期在心跳间隔期间发送。块报告包含 DataNode
    上所有块的列表，NameNode 随后将其存储在元数据存储库中。
- en: The NameNode stores all metadata in memory and serves all requests from clients
    reading from/writing to HDFS. However, since this is the master node maintaining
    all the metadata about the HDFS, it is critical to maintain consistent and reliable
    metadata information. If this information is lost, the content on the HDFS cannot
    be accessed.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: NameNode 将所有元数据存储在内存中，并处理所有来自客户端的读写请求。然而，由于这是维护所有 HDFS 元数据的主节点，因此保持一致且可靠的元数据是至关重要的。如果这些信息丢失，HDFS
    上的内容将无法访问。
- en: For this purpose, HDFS NameNode uses a transaction log called the EditLog, which
    persistently records every change that occurs to the metadata of the filesystem.
    Creating a new file updates EditLog, so does moving a file or renaming a file,
    or deleting a file. The entire filesystem namespace, including the mapping of
    blocks to files and filesystem properties, is stored in a file called the `FsImage`.
    The **NameNode** keeps everything in memory as well. When a NameNode starts up,
    it loads the EditLog and the `FsImage` initializes itself to set up the HDFS.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，HDFS NameNode 使用一个称为 EditLog 的事务日志，它会持久记录文件系统元数据发生的每一个更改。创建新文件时会更新 EditLog，移动文件、重命名文件或删除文件时也是如此。整个文件系统命名空间，包括块到文件的映射以及文件系统属性，都存储在一个名为
    `FsImage` 的文件中。**NameNode** 也将所有内容存储在内存中。当 NameNode 启动时，它会加载 EditLog，并且 `FsImage`
    会初始化自身以设置 HDFS。
- en: The DataNodes, however, have no idea about the HDFS, purely relying on the blocks
    of data stored. DataNodes rely entirely on the NameNode to perform any operations.
    Even when a client wants to connect to read a file or write to a file, it's the
    NameNode that tells the client where to connect to.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据节点并不了解HDFS，它们仅依赖于存储的数据块。数据节点完全依赖于NameNode执行任何操作。即使客户端要连接以读取或写入文件，也是NameNode告诉客户端应该连接到哪里。
- en: HDFS High Availability
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS 高可用性
- en: HDFS is a Master-Slave cluster with the NameNode as the master and the 100s,
    if not 1000s of DataNodes as slaves, managed by the master node. This introduces
    a **Single Point of Failure** (**SPOF**) in the cluster as if the Master NameNode
    goes down for some reason, the entire cluster is going to be unusable. HDFS 1.0
    supports an additional Master Node known as the **Secondary NameNode** to help
    with recovery of the cluster. This is done by maintaining a copy of all the metadata
    of the filesystem and is by no means a Highly Available System requiring manual
    interventions and maintenance work. HDFS 2.0 takes this to the next level by adding
    support for full **High Availability** (**HA**).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 是主从集群，NameNode作为主节点，而数百甚至数千个DataNode作为从节点，由主节点管理。这在集群中引入了**单点故障**（**SPOF**）的问题，如果主NameNode由于某种原因出现故障，整个集群将无法使用。HDFS
    1.0支持额外的主节点称为**辅助NameNode**，用于帮助集群的恢复。它通过维护文件系统所有元数据的副本来实现，但不是一个高度可用的系统，需要手动干预和维护工作。HDFS
    2.0通过添加全面支持**高可用性**（**HA**）将其提升到了一个新的水平。
- en: HA works by having two Name Nodes in an active-passive mode such that one Name
    Node is active and other is passive. When the primary NameNode has a failure,
    the passive Name Node will take over the role of the Master Node.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: HA 的工作方式是使用两个NameNode，以主备模式运行，其中一个是活动的，另一个是待机的。当主NameNode发生故障时，备用NameNode将接管主节点的角色。
- en: 'The following diagram shows how the active-passive pair of NameNodes will be
    deployed:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了主备NameNode的部署方式：
- en: '![](img/00123.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00123.jpeg)'
- en: HDFS Federation
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS 联邦
- en: 'HDFS Federation is a way of using multiple name nodes to spread the filesystem
    namespace over. Unlike the first HDFS versions, which simply managed entire clusters
    using a single NameNode, which does not scale that well as the size of the cluster
    grows, HDFS Federation can support significantly larger clusters and horizontally
    scales the NameNode or name service using multiple federated name nodes. Take
    a look at the following diagram:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 联邦是使用多个NameNode来扩展文件系统命名空间的一种方式。与第一个HDFS版本不同，后者仅使用单个NameNode管理整个集群，随着集群规模的增长，这种管理方式无法很好地扩展。HDFS
    联邦可以支持规模显著更大的集群，并通过多个联合的NameNode水平扩展NameNode或名称服务。请看下图：
- en: '![](img/00127.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00127.jpeg)'
- en: HDFS Snapshot
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS 快照
- en: 'Hadoop 2.0 also added a new capability: taking a snapshot (read-only copy and
    copy-on-write) of the filesystem (data blocks) stored on the data nodes. Using
    Snapshots, you can take a copy of directories seamlessly using the NameNode''s
    metadata of the data blocks. Snapshot creation is instantaneous and doesn''t require
    interference with other regular HDFS operations.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2.0还增加了一项新功能：使用快照（只读副本和写时复制）拍摄数据节点上存储的文件系统（数据块）。使用快照，您可以在不干扰其他常规HDFS操作的情况下无缝地拍摄目录，利用NameNode的数据块元数据。快照创建是即时的。
- en: 'The following is an illustration of how snapshot works on specific directories:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是关于如何在特定目录上工作的快照工作示例：
- en: '![](img/00131.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00131.jpeg)'
- en: HDFS Read
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS 读取
- en: Client connects to the NameNode and ask about a file using the name of the file.
    NameNode looks up the block locations for the file and returns the same to the
    client. The client can then connect to the DataNodes and read the blocks needed.
    NameNode does not participate in the data transfer.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端连接到NameNode，并根据文件名询问文件的位置。NameNode查找文件的块位置并返回给客户端。然后客户端可以连接到数据节点并读取所需的块。NameNode不参与数据传输。
- en: The following is the flow of a read request from a client. First, the client
    gets the locations and then pulls the blocks from the DataNodes. If a DataNode
    fails in the middle, then the client gets the replica of the block from another
    DataNode.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是客户端读取请求的流程。首先，客户端获取位置信息，然后从数据节点拉取数据块。如果某个数据节点在中途失败，客户端则从另一个数据节点获取该块的副本。
- en: '![](img/00264.jpeg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00264.jpeg)'
- en: HDFS Write
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS 写入
- en: The client connects to the NameNode and asks the NameNode to let it write to
    the HDFS. The NameNode looks up information and plans the blocks, the Data Nodes
    to be used to store the blocks, and the replication strategy to be used. The NameNode
    does not handle any data and only tells the client where to write. Once the first
    DataNode receives the block, based on the replication strategy, the NameNode tells
    the first DataNode where else to replicate. So, the DataNode that is received
    from client sends the block over to the second DataNode (where the copy of the
    block is supposed to be written to) and then the second DataNode sends it to a
    third DataNode (if replication-factor is 3).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端连接到NameNode，并请求NameNode允许其写入HDFS。NameNode查找信息并规划使用哪些块、哪些DataNode来存储这些块，以及使用什么复制策略。NameNode不处理任何数据，它只是告诉客户端该写到哪里。一旦第一个DataNode接收到块，基于复制策略，NameNode会告诉第一个DataNode在哪里进行复制。因此，客户端接收到的块会发送到第二个DataNode（复制块应该写入的位置），然后第二个DataNode会将其发送到第三个DataNode（如果复制因子为3的话）。
- en: The following is the flow of a write request from a client. First, the client
    gets the locations and then writes to the first DataNode. The DataNode that receives
    the block replicates the block to the DataNodes that should hold the replica copy
    of the block. This happens for all the blocks being written to from the client.
    If a DataNode fails in the middle, then the block gets replicated to another DataNode
    as determined by the NameNode.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个客户端写请求的流程。首先，客户端获取位置，然后写入第一个DataNode。接收块的DataNode会将块复制到应该存储块副本的其他DataNode。这一过程适用于从客户端写入的所有块。如果在中途某个DataNode发生故障，块会按照NameNode的指示被复制到另一个DataNode。
- en: '![](img/00267.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00267.jpeg)'
- en: So far, we have seen how HDFS provides a distributed filesystem using blocks,
    the NameNode, and DataNodes. Once data is stored at a PB scale, it is also important
    to actually process the data to serve the various use cases of the business.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了HDFS如何通过使用块、NameNode和DataNode提供分布式文件系统。一旦数据达到PB级别存储，实际上处理数据也变得非常重要，以服务于业务的各种用例。
- en: MapReduce framework was created in the Hadoop framework to perform distributed
    computation. We will look at this further in the next section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce框架是在Hadoop框架中创建的，用于执行分布式计算。我们将在下一节进一步探讨这一点。
- en: MapReduce framework
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce框架
- en: '**MapReduce** (**MR**) framework enables you to write distributed applications
    to process large amounts of data from a filesystem such as HDFS in a reliable
    and fault-tolerant manner. When you want to use the MapReduce Framework to process
    data, it works through the creation of a job, which then runs on the framework
    to perform the tasks needed.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**MapReduce**（**MR**）框架使你能够编写分布式应用程序，以可靠且容错的方式处理来自像HDFS这样的文件系统的大量数据。当你想使用MapReduce框架处理数据时，它通过创建一个作业来运行，这个作业在框架上执行所需的任务。'
- en: A MapReduce job usually works by splitting the input data across worker nodes
    running **Mapper** tasks in a parallel manner. At this time, any failures that
    happen either at the HDFS level or the failure of a Mapper task are handled automatically
    to be fault-tolerant. Once the Mappers are completed, the results are copied over
    the network to other machines running **Reducer** tasks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce作业通常通过将输入数据分割到运行**Mapper**任务的工作节点上以并行方式工作。在此过程中，HDFS级别的故障或Mapper任务的失败都会被自动处理，从而实现容错。一旦Mapper完成，结果将通过网络复制到其他运行**Reducer**任务的机器上。
- en: An easy way to understand this concept is to imagine that you and your friends
    want to sort out piles of fruit into boxes. For that, you want to assign each
    person the task of going through one raw basket of fruit (all mixed up) and separate
    out the fruit into various boxes. Each person then does the same with this basket
    of fruit.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这个概念的一个简单方法是，假设你和你的朋友们要把一堆水果分类到箱子里。为此，你希望把每个人分配一个任务，让他们处理一篮原料水果（全部混在一起），并将水果分开放入不同的箱子。每个人然后都按同样的方法处理这篮水果。
- en: In the end, you end up with a lot of boxes of fruit from all your friends. Then,
    you can assign a group to put the same kind of fruit together in a box, weight
    the box, and seal the box for shipping.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你会得到来自所有朋友的一大堆水果箱。然后，你可以指派一组人把相同种类的水果放在同一个箱子里，称重并封箱以便运输。
- en: 'The following depicts the idea of taking fruit baskets and sorting the fruit
    by the type of fruit:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了通过不同种类的水果来分类水果篮子的概念：
- en: '![](img/00270.jpeg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00270.jpeg)'
- en: MapReduce framework consists of a single resource manager and multiple node
    managers (usually Node Managers coexist with the data nodes of HDFS). When an
    application wants to run, the client launches the application master, which then
    negotiates with the resource manager to get resources in the cluster in form of
    containers.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce框架由一个资源管理器和多个节点管理器组成（通常节点管理器与HDFS的数据节点共存）。当应用程序需要运行时，客户端启动应用程序主控器，然后与资源管理器协商，在集群中以容器的形式获取资源。
- en: A container represents CPUs (cores) and memory allocated on a single node to
    be used to run tasks and processes. Containers are supervised by the node manager
    and scheduled by the resource manager.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 容器代表了分配给单个节点上的CPU（核心）和内存，用于运行任务和进程。容器由节点管理器监督，并由资源管理器调度。
- en: 'Examples of containers:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 容器示例：
- en: 1 core + 4 GB RAM
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 1个核心 + 4 GB内存
- en: 2 cores + 6 GB RAM
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 2个核心 + 6 GB内存
- en: 4 cores + 20 GB RAM
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 4个核心 + 20 GB内存
- en: Some Containers are assigned to be Mappers and other to be Reducers; all this
    is coordinated by the application master in conjunction with the resource manager.
    This framework is called **Yet Another Resource Negotiator** (**YARN**)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一些容器被分配为Mappers，另一些容器被分配为Reducers；这一切都由应用程序主控器与资源管理器共同协调。这个框架叫做**另一个资源协商器**（**YARN**）
- en: 'The following is a depiction of YARN:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是YARN的示意图：
- en: '![](img/00276.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00276.jpeg)'
- en: 'A classic example showing the MapReduce framework at work is the word count
    example. The following are the various stages of processing the input data, first
    splitting the input across multiple worker nodes and then finally generating the
    output counts of words:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的例子，展示了MapReduce框架的工作原理，就是词频统计示例。以下是处理输入数据的各个阶段，首先将输入数据拆分到多个工作节点上，最后生成单词的输出计数：
- en: '![](img/00279.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00279.jpeg)'
- en: Though MapReduce framework is very successful all across the world and has been
    adopted by most companies, it does run into issues mainly because of the way it
    processes data. Several technologies have come into existence to try and make
    MapReduce easier to use such as Hive and Pig but the complexity remains.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MapReduce框架在全球范围内非常成功，并且被大多数公司采用，但由于其数据处理方式，它会遇到一些问题。为了让MapReduce更易于使用，出现了多种技术，如Hive和Pig，但复杂性依然存在。
- en: 'Hadoop MapReduce has several limitations such as:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop MapReduce有多个限制，诸如：
- en: Performance bottlenecks due to disk-based processing
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于基于磁盘的处理导致性能瓶颈
- en: Batch processing doesn't serve all needs
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理无法满足所有需求
- en: Programming can be verbose and complex
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程可能冗长且复杂
- en: Scheduling of the tasks is slow as there is not much reuse of resources
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于资源重复使用较少，任务调度较慢
- en: No good way to do real-time event processing
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有很好的方式进行实时事件处理
- en: Machine learning takes too long as usually ML involves iterative processing
    and MR is too slow for this
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习通常需要较长时间，因为机器学习通常涉及迭代处理，而MapReduce处理速度太慢，无法满足这一需求。
- en: Hive was created by Facebook as a SQL-like interface to MR. Pig was created
    by Yahoo with a scripting interface to MR. Moreover, several enhancements such
    as Tez (Hortonworks) and LLAP (Hive2.x) are in use, which makes use of in-memory
    optimizations to circumvent the limitations of MapReduce.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Hive是由Facebook创建的，作为MapReduce的SQL类似接口。Pig是由Yahoo创建的，作为MapReduce的脚本接口。此外，还使用了多个增强技术，如Tez（Hortonworks）和LLAP（Hive2.x），它们通过内存优化来绕过MapReduce的局限性。
- en: In the next section, we will look at Apache Spark, which has already solved
    some of the limitations of Hadoop technologies.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍Apache Spark，它已经解决了一些Hadoop技术的局限性。
- en: Here comes Apache Spark
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这里介绍Apache Spark
- en: Apache Spark is a unified distributed computing engine across different workloads
    and platforms. Spark can connect to different platforms and process different
    data workloads using a variety of paradigms such as Spark streaming, Spark ML,
    Spark SQL, and Spark GraphX.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个统一的分布式计算引擎，能够在不同的工作负载和平台之间运行。Spark可以连接到不同的平台，并使用多种范式（如Spark流处理、Spark
    ML、Spark SQL和Spark GraphX）处理不同的数据工作负载。
- en: Apache Spark is a fast in-memory data processing engine with elegant and expressive
    development APIs to allow data workers to efficiently execute streaming machine
    learning or SQL workloads that require fast interactive access to data sets. Apache
    Spark consists of Spark core and a set of libraries. The core is the distributed
    execution engine and the Java, Scala, and Python APIs offer a platform for distributed
    application development. Additional libraries built on top of the core allow workloads
    for streaming, SQL, Graph processing, and machine learning. Spark ML, for instance,
    is designed for data science and its abstraction makes data science easier.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个快速的内存数据处理引擎，具有优雅和表达性强的开发 API，允许数据工作者高效地执行需要快速交互访问数据集的流式机器学习或
    SQL 工作负载。Apache Spark 由 Spark 核心和一组库组成。核心是分布式执行引擎，Java、Scala 和 Python API 提供了分布式应用程序开发的平台。建立在核心之上的附加库支持流式处理、SQL、图形处理和机器学习等工作负载。例如，Spark
    ML 旨在用于数据科学，其抽象使得数据科学变得更加容易。
- en: Spark provides real-time streaming, queries, machine learning, and graph processing.
    Before Apache Spark, we had to use different technologies for different types
    of workloads, one for batch analytics, one for interactive queries, one for real-time
    streaming processing and another for machine learning algorithms. However, Apache
    Spark can do all of these just using Apache Spark instead of using multiple technologies
    that are not always integrated.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供实时流处理、查询、机器学习和图形处理。在 Apache Spark 之前，我们必须使用不同的技术来处理不同类型的工作负载，一个用于批量分析，一个用于交互式查询，一个用于实时流处理，另一个用于机器学习算法。然而，Apache
    Spark 可以只使用 Apache Spark 来处理所有这些工作负载，而不必使用多个不总是集成的技术。
- en: Using Apache Spark, all types of workload can be processed and Spark also supports
    Scala, Java, R, and Python as a means of writing client programs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Apache Spark，可以处理所有类型的工作负载，Spark 还支持 Scala、Java、R 和 Python 作为编写客户端程序的手段。
- en: 'Apache Spark is an open-source distributed computing engine which has key advantages
    over the MapReduce paradigm:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个开源分布式计算引擎，相比 MapReduce 模式具有显著优势：
- en: Uses in-memory processing as much as possible
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能使用内存处理
- en: General purpose engine to be used for batch, real-time workloads
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于批处理和实时工作负载的通用引擎
- en: Compatible with YARN and also Mesos
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 YARN 和 Mesos 兼容
- en: Integrates well with HBase, Cassandra, MongoDB, HDFS, Amazon S3, and other file
    systems and data sources
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 HBase、Cassandra、MongoDB、HDFS、Amazon S3 和其他文件系统及数据源兼容良好
- en: 'Spark was created in Berkeley back in 2009 and was a result of the project
    to build Mesos, a cluster management framework to support different kinds of cluster
    computing systems. Take a look at the following table:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 于 2009 年在伯克利创建，源于一个旨在构建 Mesos（支持不同类型集群计算系统的集群管理框架）的项目。请看以下表格：
- en: '| Version | Release date | Milestones |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 版本 | 发布日期 | 里程碑 |'
- en: '| 0.5 | 2012-10-07 | First available version for non-production usage |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 2012-10-07 | 第一个可用于非生产环境的版本 |'
- en: '| 0.6 | 2013-02-07 | Point release with various changes |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 0.6 | 2013-02-07 | 各种更改的版本发布 |'
- en: '| 0.7 | 2013-07-16 | Point release with various changes |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 0.7 | 2013-07-16 | 各种更改的版本发布 |'
- en: '| 0.8 | 2013-12-19 | Point release with various changes |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 0.8 | 2013-12-19 | 各种更改的版本发布 |'
- en: '| 0.9 | 2014-07-23 | Point release with various changes |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 0.9 | 2014-07-23 | 各种更改的版本发布 |'
- en: '| 1.0 | 2014-08-05 | First production ready, backward-compatible release. Spark
    Batch, Streaming, Shark, MLLib, GraphX |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 | 2014-08-05 | 第一个生产就绪、向后兼容的版本发布。包括 Spark Batch、Streaming、Shark、MLLib、GraphX
    |'
- en: '| 1.1 | 2014-11-26 | Point release with various changes |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 1.1 | 2014-11-26 | 各种更改的版本发布 |'
- en: '| 1.2 | 2015-04-17 | Structured Data, SchemaRDD (subsequently evolved into
    DataFrames) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 1.2 | 2015-04-17 | 结构化数据、SchemaRDD（后续发展为 DataFrames） |'
- en: '| 1.3 | 2015-04-17 | API to provide a unified API to read from structured and
    semi-structured sources |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 1.3 | 2015-04-17 | 提供统一的 API 来读取结构化和半结构化数据源 |'
- en: '| 1.4 | 2015-07-15 | SparkR, DataFrame API, Tungsten improvements |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 1.4 | 2015-07-15 | SparkR、DataFrame API、Tungsten 改进 |'
- en: '| 1.5 | 2015-11-09 | Point release with various changes |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 1.5 | 2015-11-09 | 各种更改的版本发布 |'
- en: '| 1.6 | 2016-11-07 | Dataset DSL introduced |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 1.6 | 2016-11-07 | 引入了 Dataset DSL |'
- en: '| 2.0 | 2016-11-14 | DataFrames and Datasets API as fundamental layer for ML,
    Structured Streaming,SparkR improvements. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 2.0 | 2016-11-14 | DataFrames 和 Datasets API 作为机器学习的基础层，结构化流、SparkR 改进。 |'
- en: '| 2.1 | 2017-05-02 | Event time watermarks, ML, GraphX improvements |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 2.1 | 2017-05-02 | 事件时间水印、ML、GraphX 改进 |'
- en: 2.2 has been released 2017-07-11 which has several improvements especially Structured
    Streaming which is now GA.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2 版本已于 2017-07-11 发布，包含了若干改进，尤其是结构化流处理（Structured Streaming）现在已进入 GA 阶段。
- en: 'Spark is a platform for distributed computing that has several features:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个分布式计算平台，具有以下几个特点：
- en: Transparently processes data on multiple nodes via a simple API
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过简单的 API 透明地在多个节点上处理数据
- en: Resiliently handles failures
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性地处理故障
- en: Spills data to disk as necessary though predominantly uses memory
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据需要将数据溢出到磁盘，但主要使用内存
- en: Java, Scala, Python, R, and SQL APIs are supported
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 Java、Scala、Python、R 和 SQL API
- en: The same Spark code can run standalone, in Hadoop YARN, Mesos, and the cloud
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同的 Spark 代码可以独立运行，也可以在 Hadoop YARN、Mesos 和云中运行
- en: Scala features such as implicits, higher-order functions, structured types,
    and so on allow us to easily build DSL's and integrate them with the language.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 特性，如隐式转换、高阶函数、结构化类型等，允许我们轻松构建 DSL 并将其与语言集成。
- en: Apache Spark does not provide a Storage layer and relies on HDFS or Amazon S3
    and so on. Hence, even if Apache Hadoop technologies are replaced with Apache
    Spark, HDFS is still needed to provide a reliable storage layer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 不提供存储层，而是依赖于 HDFS 或 Amazon S3 等。因此，即使 Apache Hadoop 技术被 Apache
    Spark 替代，HDFS 仍然是必需的，以提供可靠的存储层。
- en: Apache Kudu provides an alternative to HDFS and there is already integration
    between Apache Spark and Kudu Storage layer, further decoupling Apache Spark and
    the Hadoop ecosystem.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kudu 提供了一个替代 HDFS 的方案，且 Apache Spark 与 Kudu 存储层已实现集成，进一步解耦了 Apache Spark
    和 Hadoop 生态系统。
- en: Hadoop and Apache Spark are both popular big data frameworks, but they don't
    really serve the same purposes. While Hadoop provides distributed storage and
    a MapReduce distributed computing framework, Spark on the other hand is a data
    processing framework that operates on the distributed data storage provided by
    other technologies.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 和 Apache Spark 都是流行的大数据框架，但它们并不完全相同。Hadoop 提供分布式存储和 MapReduce 分布式计算框架，而
    Spark 是一个数据处理框架，依赖于其他技术提供的分布式数据存储。
- en: Spark is generally a lot faster than MapReduce because of the way it processes
    data. MapReduce operates on splits using Disk operations, Spark operates on the
    dataset much more efficiently than MapReduce, with the main reason behind the
    performance improvement in Apache Spark being the efficient off-heap in-memory
    processing rather than solely relying on disk-based computations.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据处理方式的不同，Spark 通常比 MapReduce 快得多。MapReduce 使用磁盘操作处理数据拆分，而 Spark 在数据集上的操作效率远高于
    MapReduce，Spark 性能提升的主要原因是高效的堆外内存处理，而不是仅依赖基于磁盘的计算。
- en: MapReduce's processing style can be sufficient if you were data operations and
    reporting requirements are mostly static and it is okay to use batch processing
    for your purposes, but if you need to do analytics on streaming data or your processing
    requirements need multistage processing logic, you will probably want to want
    to go with Spark.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据操作和报告需求大多数是静态的，并且可以接受使用批处理处理，你可能会选择 MapReduce，但如果需要对流数据进行分析或处理需求需要多阶段的处理逻辑，你可能会选择
    Spark。
- en: There are three layers in the Spark stack. The bottom layer is the cluster manager,
    which can be standalone, YARN, or Mesos.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 堆栈有三层。底层是集群管理器，可以是独立模式、YARN 或 Mesos。
- en: Using local mode, you don't need a cluster manager to process.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本地模式时，你不需要集群管理器来进行处理。
- en: In the middle, above the cluster manager, is the layer of Spark core, which
    provides all the underlying APIs to perform task scheduling and interacting with
    storage.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间，集群管理器之上是 Spark 核心层，它提供所有底层 API，用于任务调度和与存储的交互。
- en: At the top are modules that run on top of Spark core such as Spark SQL to provide
    interactive queries, Spark streaming for real-time analytics, Spark ML for machine
    learning, and Spark GraphX for graph processing.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部是运行在 Spark 核心之上的模块，例如 Spark SQL 提供交互式查询，Spark streaming 用于实时分析，Spark ML 用于机器学习，Spark
    GraphX 用于图形处理。
- en: 'The three layers are as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 三个层次如下：
- en: '![](img/00283.jpeg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00283.jpeg)'
- en: As seen in the preceding diagram, the various libraries such as Spark SQL, Spark
    streaming, Spark ML, and GraphX all sit on top of Spark core, which is the middle
    layer. The bottom layer shows the various cluster manager options.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，各种库，如 Spark SQL、Spark streaming、Spark ML 和 GraphX 都位于 Spark 核心之上，核心是中间层。底层展示了各种集群管理器的选项。
- en: 'Let''s now look at each of the component briefly:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们简要了解一下每个组件：
- en: Spark core
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 核心
- en: Spark core is the underlying general execution engine for the Spark platform
    that all other functionality is built upon. Spark core contains basic Spark functionalities
    required for running jobs and needed by other components. It provides in-memory
    computing and referencing datasets in external storage systems, the most important
    being the **Resilient Distributed Dataset** (**RDD**).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 核心是 Spark 平台的底层通用执行引擎，所有其他功能都是在其上构建的。Spark 核心包含运行作业所需的基本 Spark 功能，并且其他组件也需要这些功能。它提供内存计算和对外部存储系统数据集的引用，最重要的是**弹性分布式数据集**（**RDD**）。
- en: In addition, Spark core contains logic for accessing various filesystems, such
    as HDFS, Amazon S3, HBase, Cassandra, relational databases, and so on. Spark core
    also provides fundamental functions to support networking, security, scheduling,
    and data shuffling to build a high scalable, fault-tolerant platform for distributed
    computing.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark 核心包含访问各种文件系统的逻辑，如 HDFS、Amazon S3、HBase、Cassandra、关系型数据库等。Spark 核心还提供支持网络、安防、调度和数据洗牌的基本功能，用于构建一个具有高可扩展性和容错能力的分布式计算平台。
- en: We cover Spark core in detail in [Chapter 6](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c),
    *Start Working with Spark - REPL* and RDDs and [Chapter 7](part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c),
    *Special RDD Operations*.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第6章](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c)，*开始使用 Spark
    - REPL* 和 RDDs，以及[第7章](part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c)，*特殊
    RDD 操作*中详细介绍了 Spark 核心。
- en: DataFrames and datasets built on top of RDDs and introduced with Spark SQL are
    becoming the norm now over RDDs in many use cases. RDDs are still more flexible
    in terms of handling totally unstructured data, but in future datasets, API might
    eventually become the core API.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 RDD 构建的数据帧和数据集，并通过 Spark SQL 引入，现在在许多使用场景中已成为比 RDD 更为常见的选择。尽管 RDD 在处理完全非结构化数据时仍然更具灵活性，但在未来，数据集
    API 可能最终会成为核心 API。
- en: Spark SQL
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Spark SQL is a component on top of Spark core that introduces a new data abstraction
    called **SchemaRDD**, which provides support for structured and semi-structured
    data. Spark SQL provides functions for manipulating large sets of distributed,
    structured data using an SQL subset supported by Spark and Hive QL. Spark SQL
    simplifies the handling of structured data through DataFrames and datasets at
    a much more performant level as part of the Tungsten initative. Spark SQL also
    supports reading and writing data to and from various structured formats and data
    sources, files, parquet, orc, relational databases, Hive, HDFS, S3, and so on.
    Spark SQL provides a query optimization framework called **Catalyst** to optimize
    all operations to boost the speed (compared to RDDs Spark SQL is several times
    faster). Spark SQL also includes a Thrift server, which can be used by external
    systems to query data through Spark SQL using classic JDBC and ODBC protocols.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 是 Spark 核心之上的一个组件，引入了一种新的数据抽象——**SchemaRDD**，它为结构化和半结构化数据提供支持。Spark
    SQL 提供了使用 Spark 和 Hive QL 支持的 SQL 子集操作大型分布式结构化数据的功能。Spark SQL 通过数据帧和数据集简化了结构化数据的处理，且性能远超以往，是
    Tungsten 计划的一部分。Spark SQL 还支持从各种结构化格式和数据源中读取和写入数据，如文件、parquet、orc、关系型数据库、Hive、HDFS、S3
    等。Spark SQL 提供了一个查询优化框架——**Catalyst**，用于优化所有操作，以提高速度（与 RDDs 相比，Spark SQL 的速度快了好几倍）。Spark
    SQL 还包括一个 Thrift 服务器，外部系统可以通过 Spark SQL 使用经典的 JDBC 和 ODBC 协议查询数据。
- en: We cover Spark SQL in detail in [Chapter 8](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c),
    *Introduce a Little Structure - Spark SQL*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第8章](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c)，*引入一点结构 - Spark
    SQL*中详细介绍了 Spark SQL。
- en: Spark streaming
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 流处理
- en: Spark streaming leverages Spark core's fast scheduling capability to perform
    streaming analytics by ingesting real-time streaming data from various sources
    such as HDFS, Kafka, Flume, Twitter, ZeroMQ, Kinesis, and so on. Spark streaming
    uses micro-batches of data to process the data in chunks and, uses a concept known
    as DStreams, Spark streaming can operate on the RDDs, applying transformations
    and actions as regular RDDs in the Spark core API. Spark streaming operations
    can recover from failure automatically using various techniques. Spark streaming
    can be combined with other Spark components in a single program, unifying real-time
    processing with machine learning, SQL, and graph operations.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Spark流处理利用Spark核心的快速调度能力，通过从HDFS、Kafka、Flume、Twitter、ZeroMQ、Kinesis等各种数据源摄取实时流数据来执行流处理分析。Spark流处理使用数据的微批处理方式进行数据分块处理，并使用一个称为DStreams的概念，Spark流处理可以像Spark核心API中的常规RDD一样对RDD进行转换和操作。Spark流处理操作可以使用多种技术自动从故障中恢复。Spark流处理可以与其他Spark组件结合，在单个程序中统一实时处理、机器学习、SQL和图形操作。
- en: We cover Spark streaming in detail in the [Chapter 9](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c),
    *Stream Me Up, Scotty - Spark Streaming*.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第9章](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c)中详细介绍了Spark流处理，*Stream
    Me Up, Scotty - Spark Streaming*。
- en: In addition, the new Structured Streaming API makes Spark streaming programs
    more similar to Spark batch programs and also allows real-time querying on top
    of streaming data, which is complicated with the Spark streaming library before
    Spark 2.0+.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，新的结构化流处理API使得Spark流处理程序更类似于Spark批处理程序，同时也允许在流数据上进行实时查询，这在Spark 2.0+之前的Spark流处理库中是非常复杂的。
- en: Spark GraphX
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark GraphX
- en: GraphX is a distributed graph processing framework on top of Spark. Graphs are
    data structures comprising vertices and the edges connecting them. GraphX provides
    functions for building graphs, represented as Graph RDDs. It provides an API for
    expressing graph computation that can model user-defined graphs by using the Pregel
    abstraction API. It also provides an optimized runtime for this abstraction. GraphX
    also contains implementations of the most important algorithms of graph theory,
    such as page rank, connected components, shortest paths, SVD++, and others.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX是一个基于Spark的分布式图处理框架。图是由顶点和连接它们的边组成的数据结构。GraphX提供了构建图的功能，图表示为Graph RDD。它提供了一个API，用于表达图计算，能够通过使用Pregel抽象API来建模用户定义的图。它还为该抽象提供了优化的运行时。GraphX还包含图论中最重要算法的实现，如PageRank、连通组件、最短路径、SVD++等。
- en: We cover Spark Graphx in detail in [Chapter 10](part0326.html#9MSNC1-21aec46d8593429cacea59dbdcd64e1c),
    *Everything is Connected - GraphX*.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第10章](part0326.html#9MSNC1-21aec46d8593429cacea59dbdcd64e1c)中详细介绍了Spark
    GraphX，*Everything is Connected - GraphX*。
- en: A newer module known as GraphFrames is in development, which makes it easier
    to do Graph processing using DataFrame-based Graphs. GraphX is to RDDs what GraphFrames
    are to DataFrames/datasets. Also, this is currently separate from GraphX and is
    expected to support all the functionality of GraphX in the future, when there
    might be a switch over to GraphFrames.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新的模块GraphFrames正在开发中，它使得使用基于DataFrame的图形更容易进行图处理。GraphX对于RDD就像GraphFrames对于DataFrame/数据集一样。此外，目前GraphFrames与GraphX是分开的，预计将来会支持GraphX的所有功能，届时可能会切换到GraphFrames。
- en: Spark ML
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark ML
- en: MLlib is a distributed machine learning framework above Spark core and handles
    machine-learning models used for transforming datasets in the form of RDDs. Spark
    MLlib is a library of machine-learning algorithms providing various algorithms
    such as logistic regression, Naive Bayes classification, **Support Vector Machines**
    (**SVMs**), decision trees, random forests, linear regression, **Alternating Least
    Squares** (**ALS**), and k-means clustering. Spark ML integrates very well with
    Spark core, Spark streaming, Spark SQL, and GraphX to provide a truly integrated
    platform where data can be real-time or batch.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib是一个分布式机器学习框架，位于Spark核心之上，处理用于转换RDD格式数据集的机器学习模型。Spark MLlib是一个机器学习算法库，提供各种算法，如逻辑回归、朴素贝叶斯分类、**支持向量机**（**SVMs**）、决策树、随机森林、线性回归、**交替最小二乘法**（**ALS**）和K-means聚类。Spark
    ML与Spark核心、Spark流处理、Spark SQL和GraphX紧密集成，提供一个真正集成的平台，可以处理实时或批处理数据。
- en: We cover Spark ML in detail in [Chapter 11](part0170.html#523VK1-21aec46d8593429cacea59dbdcd64e1c),
    *Learning Machine Learning - Spark MLlib and ML*.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第11章](part0170.html#523VK1-21aec46d8593429cacea59dbdcd64e1c)中详细介绍了Spark
    ML，*Learning Machine Learning - Spark MLlib and ML*。
- en: In addition, PySpark and SparkR are also available as means to interact with
    Spark clusters and use the Python and R APIs. Python and R integrations truly
    open up Spark to a population of Data scientists and Machine learning modelers
    as the most common languages used by Data scientists in general are Python and
    R. This is the reason why Spark supports Python integration and also R integration,
    so as to avoid the costly process of learning a new language of Scala. Another
    reason is that there might be a lot of existing code written in Python and R,
    and if we can leverage some of the code, that will improve the productivity of
    the teams rather than building everything again from scratch.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，PySpark 和 SparkR 也可以作为与 Spark 集群交互并使用 Python 和 R API 的手段。Python 和 R 的集成真正为数据科学家和机器学习建模人员打开了
    Spark，因为数据科学家通常使用的最常见语言是 Python 和 R。这就是 Spark 支持 Python 和 R 集成的原因，以避免学习 Scala
    这种新语言的高昂成本。另一个原因是可能存在大量用 Python 和 R 编写的现有代码，如果我们能够利用其中的一些代码，将提升团队的生产力，而不是从头开始重新构建所有内容。
- en: There is increasing popularity for, and usage of, notebook technologies such
    as Jupyter and Zeppelin, which make it significantly easier to interact with Spark
    in general, but particularly very useful in Spark ML where a lot of hypotheses
    and analysis are expected.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本技术如 Jupyter 和 Zeppelin 正在越来越受到欢迎并被广泛使用，它们使得与 Spark 的互动变得更加简便，特别是在 Spark ML
    中尤为有用，因为在该领域通常需要进行大量的假设和分析。
- en: PySpark
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark
- en: PySpark uses Python-based `SparkContext` and Python scripts as tasks and then
    uses sockets and pipes to executed processes to communicate between Java-based
    Spark clusters and Python scripts. PySpark also uses `Py4J`, which is a popular
    library integrated within PySpark that lets Python interface dynamically with
    Java-based RDDs.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 使用基于 Python 的 `SparkContext` 和 Python 脚本作为任务，然后通过套接字和管道执行进程，在基于 Java
    的 Spark 集群与 Python 脚本之间进行通信。PySpark 还使用 `Py4J`，这是一个流行的库，集成在 PySpark 中，可以让 Python
    动态与基于 Java 的 RDD 进行交互。
- en: Python must be installed on all worker nodes running the Spark executors.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 必须在所有运行 Spark 执行器的工作节点上安装 Python。
- en: 'The following is how PySpark works by communicating between Java processed
    and Python scripts:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 PySpark 如何通过在 Java 处理和 Python 脚本之间通信来工作的方式：
- en: '![](img/00286.jpeg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00286.jpeg)'
- en: SparkR
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkR
- en: '`SparkR` is an R package that provides a light-weight frontend to use Apache
    Spark from R. SparkR provides a distributed data frame implementation that supports
    operations such as selection, filtering, aggregation, and so on. SparkR also supports
    distributed machine learning using MLlib. SparkR uses R-based `SparkContext` and
    R scripts as tasks and then uses JNI and pipes to executed processes to communicate
    between Java-based Spark clusters and R scripts.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkR` 是一个 R 包，提供了一个轻量级的前端接口，用于从 R 中使用 Apache Spark。SparkR 提供了一个分布式数据框架实现，支持选择、过滤、聚合等操作。SparkR
    还支持使用 MLlib 进行分布式机器学习。SparkR 使用基于 R 的 `SparkContext` 和 R 脚本作为任务，然后通过 JNI 和管道执行进程，在基于
    Java 的 Spark 集群与 R 脚本之间进行通信。'
- en: R must be installed on all worker nodes running the Spark executors.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 必须在所有运行 Spark 执行器的工作节点上安装 R。
- en: 'The following is how SparkR works by communicating between Java processed and
    R scripts:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 SparkR 如何通过在 Java 处理和 R 脚本之间通信来工作的方式：
- en: '![](img/00289.jpeg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00289.jpeg)'
- en: Summary
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We explored the evolution of the Hadoop and MapReduce frameworks and discussed
    YARN, HDFS concepts, HDFS Reads and Writes, and key features as well as challenges.
    Then, we discussed the evolution of Apache Spark, why Apache Spark was created
    in the first place, and the value it can bring to the challenges of big data analytics
    and processing.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了 Hadoop 和 MapReduce 框架的演变，并讨论了 YARN、HDFS 概念、HDFS 的读写操作、关键特性以及挑战。然后，我们讨论了
    Apache Spark 的演变，Apache Spark 最初为何被创建，以及它能为大数据分析和处理的挑战带来何种价值。
- en: Finally, we also took a peek at the various components in Apache Spark, namely,
    Spark core, Spark SQL, Spark streaming, Spark GraphX, and Spark ML as well as
    PySpark and SparkR as a means of integrating Python and R language code with Apache
    Spark.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还简单了解了 Apache Spark 中的各个组件，即 Spark Core、Spark SQL、Spark Streaming、Spark
    GraphX 和 Spark ML，以及 PySpark 和 SparkR，它们是将 Python 和 R 语言代码与 Apache Spark 集成的手段。
- en: Now that we have seen big data analytics, the space and the evolution of the
    Hadoop Distributed computing platform, and the eventual development of Apache
    Spark along with a high-level overview of how Apache Spark might solve some of
    the challenges, we are ready to start learning Spark and how to use it in our
    use cases.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了大数据分析、Hadoop分布式计算平台的空间以及演变过程，还了解了Apache Spark的发展，并对Apache Spark如何解决一些挑战有了一个高层次的概述，我们已经准备好开始学习Spark，并了解如何在我们的应用场景中使用它。
- en: "In the next chapter, we will delve more deeply into Apache Spark and start\
    \ to look under the hood of how it all works in [Chapter 6\uFEFF](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c),\
    \ *Start Working with Spark - REPL and RDDs*."
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地探讨Apache Spark，并开始了解其内部运作原理，详细内容请参考[第六章](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c)，*开始使用Spark
    - REPL和RDDs*。
