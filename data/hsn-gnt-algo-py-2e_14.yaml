- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: This chapter explores how genetic algorithms can enhance the performance of
    **natural language processing** (**NLP**) tasks while offering insights into their
    underlying mechanisms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了遗传算法如何增强**自然语言处理**（**NLP**）任务的性能，并深入了解其潜在机制。
- en: The chapter begins by introducing the field of NLP and explaining the concept
    of **word embeddings**. We employ this technique to task a genetic algorithm with
    playing a *Semantle*-like mystery-word game, challenging it to guess the mystery
    word.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过介绍NLP领域并解释**词嵌入**的概念开始。我们运用这一技术，利用遗传算法来玩类似*Semantle*的神秘词游戏，挑战算法猜测神秘词。
- en: Subsequently, we investigate **n-grams** and **document classification**. We
    harness genetic algorithms to pinpoint a compact yet effective subset of features,
    shedding light on the classifier’s operation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们研究了**n-gram**和**文档分类**。我们利用遗传算法来确定一个紧凑而有效的特征子集，揭示分类器的运作原理。
- en: 'By the end of this chapter, you will have achieved the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将达到以下目标：
- en: Become familiar with the field of NLP and its applications
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉NLP领域及其应用
- en: Gained an understanding of the concept of word embeddings and their importance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解了词嵌入的概念及其重要性
- en: Implemented a mystery-word game using word embeddings and created a genetic
    algorithms-driven player to guess the mystery word
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词嵌入实现了一个神秘词游戏，并创建了一个由遗传算法驱动的玩家来猜测神秘词
- en: Acquired knowledge about n-grams and their role in document processing
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取了有关n-gram及其在文档处理中的作用的知识
- en: Developed a process to significantly reduce the size of the feature set used
    for message classification
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发了一种过程，显著减少了用于消息分类的特征集大小
- en: Utilized a minimal feature set to gain insights into the classifier’s operation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最小特征集来洞察分类器的运作
- en: We will start this chapter with a quick overview of NLP. If you are a seasoned
    data scientist, feel free to skip the introductory section.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将以快速概述自然语言处理（NLP）开始。如果你是经验丰富的数据科学家，可以跳过引言部分。
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using Python 3 with the following supporting libraries:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用Python 3，并配备以下支持库：
- en: '**deap**'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**deap**'
- en: '**numpy**'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**numpy**'
- en: '**pandas**'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pandas**'
- en: '**matplotlib**'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**matplotlib**'
- en: '**seaborn**'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**seaborn**'
- en: '**scikit-learn**'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scikit-learn**'
- en: '**gensim**—introduced in this chapter'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**gensim**——在本章中介绍'
- en: Important note
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you use the **requirements.txt** file we provide (see [*Chapter 3*](B20851_03.xhtml#_idTextAnchor091)),
    these libraries are already included in your environment.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用我们提供的**requirements.txt**文件（参见[*第三章*](B20851_03.xhtml#_idTextAnchor091)），这些库已经包含在你的环境中。
- en: 'The code for this chapter can be found here:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在这里找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_11](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_11)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_11](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_11)'
- en: 'Check out the following video to see the code in action:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，看看代码如何运行：
- en: '[https://packt.link/OEBOd](https://packt.link/OEBOd)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/OEBOd](https://packt.link/OEBOd)'
- en: Understanding NLP
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解NLP
- en: NLP is a fascinating branch of **artificial intelligence** that focuses on the
    interaction between computers and human language. NLP combines linguistics, computer
    science, and **machine learning** to enable machines to understand, interpret,
    and generate human language in a way that’s both meaningful and useful. Over the
    last several years, NLP has been progressively taking on an increasing role in
    our daily lives, with applications spanning numerous domains, from virtual assistants
    and chatbots to sentiment analysis, language translation, and information retrieval.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是**人工智能**的一个迷人分支，专注于计算机与人类语言之间的互动。NLP结合了语言学、计算机科学和**机器学习**，使机器能够理解、解释和生成有意义且有用的人类语言。在过去几年中，NLP在我们的日常生活中扮演着越来越重要的角色，应用范围涵盖多个领域，从虚拟助手和聊天机器人到情感分析、语言翻译和信息检索等。
- en: One of the primary goals of NLP is to bridge the communication gap between humans
    and machines; this is crucial as language is the principal medium through which
    people interact and communicate their thoughts, ideas, and desires. This goal
    of bridging the communication gap between humans and machines has driven significant
    advancements in the field of NLP. A recent notable milestone in this journey is
    the development of **large language models** (**LLMs**), such as OpenAI’s **ChatGPT**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）的主要目标之一是弥合人类与机器之间的沟通鸿沟；这是至关重要的，因为语言是人们进行互动和表达思想、观点和愿望的主要媒介。弥合人类与机器之间沟通鸿沟的目标推动了NLP领域的显著进展。最近这一领域的一个重要里程碑是**大型语言模型**（**LLMs**）的开发，例如OpenAI的**ChatGPT**。
- en: To create a bridge for human-computer communication, there must be a method
    in place that can transform human language into numerical representations, allowing
    machines to understand and process text data more effectively. One such technique
    is the use of **word embeddings**, described in the next section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创造人机沟通的桥梁，必须有一种方法能够将人类语言转化为数值表示，使机器能够更有效地理解和处理文本数据。一个这样的技术就是使用**词嵌入**，在下一节中将对此进行描述。
- en: Word embeddings
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Word embeddings are numerical representations of words in the English language
    (or other languages). Each word is encoded using a fixed-length vector of real
    numbers. These vectors effectively capture semantic and contextual information
    associated with the words they represent.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是英语（或其他语言）中单词的数值表示。每个单词都使用一个固定长度的实数向量进行编码。这些向量有效地捕捉了与它们所表示的单词相关的语义和上下文信息。
- en: Word embeddings are created by training **neural networks** (**NNs**) to create
    numerical representations for words from large collections of written or spoken
    texts, where words with similar contexts are mapped to nearby points in a continuous
    vector space.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是通过训练**神经网络**（**NNs**）来创建单词的数值表示，这些神经网络从大量的书面或口语文本中学习，其中具有相似上下文的单词会映射到连续向量空间中的相邻点。
- en: Common techniques for creating word embeddings include **Word2Vec**, **Global
    Vectors for Word Representation** (**GloVe**), and **fastText**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 创建词嵌入的常见技术包括**Word2Vec**、**全局词向量表示**（**GloVe**）和**fastText**。
- en: The typical dimensionality of word embeddings can vary, but common choices are
    50, 100, 200, or 300 dimensions. Higher-dimensional embeddings can capture more
    nuanced relationships but may require more data and computational resources.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入的典型维度可以变化，但常见的选择是50、100、200或300维。更高维度的嵌入可以捕捉到更多细微的关系，但可能需要更多的数据和计算资源。
- en: 'For example, the word “dog” in a 50-dimensional Word2Vec embedding space might
    look something like the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，“dog”这个词在50维的Word2Vec嵌入空间中的表示可能如下所示：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Each of these 50 values represents a different aspect of the word “dog” in the
    context of the training data. Related words, such as “cat” or “pet,” would have
    word vectors that are closer to the “dog” vector in this space, indicating their
    semantic similarity. These embeddings not only capture semantic information but
    also maintain relationships between words, enabling NLP models to understand word
    relationships, context, and even sentence- and document-level semantics.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些50个值中的每一个代表了在训练数据上下文中“dog”这个词的不同方面。相关的词汇，如“cat”或“pet”，在这个空间中的词向量会接近“dog”向量，表示它们在语义上的相似性。这些嵌入不仅捕捉了语义信息，还保持了单词之间的关系，使得NLP模型能够理解单词关系、上下文，甚至句子和文档级的语义。
- en: The following figure is a 2-dimensional visualization of 50-dimensional vectors
    representing various English words. This image was created using **t-Distributed
    Stochastic Neighbor Embedding** (**t-SNE**), a dimensionality reduction technique
    often used to visualize and explore word embeddings. t-SNE projects word embeddings
    into a lower-dimensional space while preserving relationships and similarities
    between data points. This figure demonstrates how certain groups of words, such
    as fruit or animals, are closer together. Relations between words are apparent
    as well—for example, the relation between “son” and “boy” resembles that between
    “daughter” and “girl:”
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是50维向量的二维可视化，代表了各种英语单词。此图是使用**t-分布随机邻域嵌入**（**t-SNE**）创建的，t-SNE是一种常用于可视化和探索词嵌入的降维技术。t-SNE将词嵌入投影到一个低维空间，同时保持数据点之间的关系和相似性。此图展示了某些单词组（例如水果或动物）之间的接近关系。单词之间的关系也显而易见——例如，“son”和“boy”之间的关系类似于“daughter”和“girl”之间的关系：
- en: '![Figure 11.1: Two-dimensional t-SNE plot of word embeddings](img/B20851_11_1.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1：词嵌入的二维 t-SNE 图](img/B20851_11_1.jpg)'
- en: 'Figure 11.1: Two-dimensional t-SNE plot of word embeddings'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：词嵌入的二维 t-SNE 图
- en: In addition to their traditional role in NLP, word embeddings can find use in
    genetic algorithms, as we will see in the next section.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在自然语言处理中的传统作用，词嵌入还可以应用于遗传算法，正如我们在下一节中将看到的那样。
- en: Word embeddings and genetic algorithms
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词嵌入和遗传算法
- en: In previous chapters of this book, we implemented numerous examples of genetic
    algorithms using fixed-length real-valued vectors (or lists) as the chromosome
    representation of candidate solutions. Given that word embeddings enable us to
    represent words (such as “dog”) using fixed-length vectors of real-valued numbers,
    these vectors can effectively serve as the genetic representation of words in
    genetic algorithm-based applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几章中，我们实现了多个使用固定长度实值向量（或列表）作为候选解染色体表示的遗传算法示例。鉴于词嵌入使我们能够使用固定长度的实值数字向量来表示单词（如“dog”），这些向量可以有效地作为遗传算法应用中的单词遗传表示。
- en: This means we can leverage genetic algorithms to solve problems in which candidate
    solutions are words in the English language, utilizing word embeddings as the
    translation mechanism between words and their genetic representation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以利用遗传算法来解决候选解是英语单词的问题，利用词嵌入作为单词及其遗传表示之间的翻译机制。
- en: To demonstrate this concept, we will embark on solving a fun word game using
    a genetic algorithm, as described in the next sections.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这一概念，我们将通过一个有趣的词汇游戏来演示如何使用遗传算法解决问题，如下节所述。
- en: Finding the mystery word using genetic algorithms
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用遗传算法找出谜底词
- en: In recent years, online mystery-word games have gained significant popularity.
    One standout among them is *Semantle*, a game that challenges you to guess the
    word of the day based on its meaning.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，在线谜词游戏获得了显著的流行。其中一个突出的例子是*Semantle*，这是一款根据词义来挑战你猜测每日词汇的游戏。
- en: This game provides feedback on how semantically similar your guesses are to
    the target word and features a “Hot and Cold” meter that indicates the proximity
    of your guess to the secret word.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这款游戏会根据你猜测的词与目标词的语义相似度提供反馈，并且具有一个“热与冷”指示器，显示你的猜测与秘密词的接近程度。
- en: 'Behind the scenes, Semantle employs word embeddings, specifically Word2Vec,
    to represent both the mystery word and players’ guesses. It calculates the semantic
    similarity between them by measuring the difference between their representations:
    the closer the vectors, the greater the resemblance between the words. The similarity
    score returned by the game ranges from -100 (very different from the answer) to
    100 (identical to the answer).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，Semantle 使用词嵌入，特别是 Word2Vec 来表示谜词和玩家的猜测。它通过计算它们表示之间的差异来衡量它们的语义相似度：向量越接近，词汇之间的相似度就越高。游戏返回的相似度分数范围从-100（与答案差异很大）到100（与答案完全相同）。
- en: 'In the following subsections, we will create two Python programs. The first
    serves as a simulation of the Semantle game, while the other embodies a player
    or solver driven by a genetic algorithm, attempting to uncover the mystery word
    by maximizing the game’s similarity score. Both programs rely on word embedding
    models; however, to maintain a clear separation, mirroring a real-world scenario,
    each program employs its own, distinct model. The interaction between the player
    and the game is limited to exchanging actual guessed words and their corresponding
    scores, and no embedding vectors are exchanged. This is depicted in the following
    diagram:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子章节中，我们将创建两个 Python 程序。第一个程序模拟了 Semantle 游戏，另一个程序则是一个由遗传算法驱动的玩家或解算器，旨在通过最大化游戏的相似度分数来揭示谜底。两个程序都依赖于词嵌入模型；然而，为了保持清晰的区分，模拟现实世界的场景，每个程序都使用其独特的模型。玩家和游戏之间的互动仅限于交换实际的猜测单词及其对应的分数，且不交换嵌入向量。以下是示意图：
- en: '![Figure 11.2: Component diagram of the Python modules and their interaction](img/B20851_11_2.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2：Python 模块的组件图及其交互](img/B20851_11_2.jpg)'
- en: 'Figure 11.2: Component diagram of the Python modules and their interaction'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：Python 模块的组件图及其交互
- en: To add an extra layer of intrigue, we’ve decided to have each program utilize
    a completely different embedding model. For that to work, we assume that both
    embedding models share a substantial overlap in their vocabularies.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加额外的神秘感，我们决定使每个程序使用完全不同的嵌入模型。为了使其工作，我们假设两个嵌入模型在词汇表中有显著的重叠。
- en: The following section provides a detailed account of the Python implementation
    of these programs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节详细介绍了这些程序的Python实现。
- en: Python implementation
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python实现
- en: We will begin by creating the Python implementation of word embedding models
    using the `gensim` library, as detailed in the following subsection.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用`gensim`库创建单词嵌入模型的Python实现，如下一小节所述。
- en: The gensim library
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: gensim库
- en: The `gensim` library is a versatile Python package primarily recognized for
    its role in NLP and text analysis tasks. `gensim` simplifies the process of working
    with word vectors by offering a comprehensive suite of tools for creating, training,
    and using word embeddings efficiently. One of its key features is its ability
    to serve as a provider of pre-trained word embedding models, of which we will
    take advantage in our first Python module, as described next.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim`库是一个多才多艺的Python包，主要用于自然语言处理和文本分析任务。`gensim`通过提供一整套工具，使得处理单词向量的创建、训练和使用变得高效。其主要特点之一是作为预训练单词嵌入模型的提供者，我们将在第一个Python模块中利用它，如下所述。'
- en: The Embeddings class
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Embeddings** 类'
- en: 'We start with a Python class called `Embeddings`, encapsulating a `gensim`-based
    pre-trained word embedding model. This class can be found in the `embeddings.py`
    file, which is located at the following link:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个名为`Embeddings`的Python类开始，该类封装了基于`gensim`的预训练单词嵌入模型。可以在以下链接找到这个类，它位于`embeddings.py`文件中：
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/embeddings.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/embeddings.py)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/embeddings.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/embeddings.py)'
- en: 'The main functionality of this class is highlighted as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此类的主要功能如下所示：
- en: 'The class’s **__init__()** method initializes the random seed (if given), and
    then proceeds to initialize the chosen (or default) **gensim** model using the
    **_init_model()** and **_download_and_save_model()** private methods. The former
    method uploads the model’s embedding information from a local file, if available.
    Otherwise, the latter method downloads the model from the **gensim** repository,
    separates the essential part for embedding, **KeyedVectors**, and saves it locally
    to be used the next time:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类的 **__init__()** 方法初始化随机种子（如果有），然后使用 **_init_model()** 和 **_download_and_save_model()**
    私有方法初始化选择的（或默认的）**gensim**模型。前者从本地文件上传模型的嵌入信息（如果可用）。否则，后者从**gensim**仓库下载模型，分离用于嵌入的关键部分**KeyedVectors**，并将其保存在本地以便下次使用：
- en: '[PRE1]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The **pick_random_embedding()** method can be used to pick a random word out
    of the model’s vocabulary.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**pick_random_embedding()** 方法可用于从模型的词汇表中随机选择一个词。'
- en: The **get_similarity()** method is used to retrieve the similarity value of
    the model between two specified words.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**get_similarity()** 方法用于检索模型在两个指定词之间的相似性值。'
- en: The **vec2_nearest_word()** method utilizes the **gensim** model’s **similar_by_vector()**
    method to retrieve the word that is closest to the specified embedding vector.
    As we will see shortly, this enables the genetic algorithm to use arbitrary vectors
    (such as randomly generated ones) and have them represent an existing word in
    the model’s vocabulary.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**vec2_nearest_word()** 方法利用 **gensim** 模型的 **similar_by_vector()** 方法检索与指定嵌入向量最接近的词。很快我们将看到，这使得遗传算法可以使用任意向量（例如随机生成的向量），并使它们代表模型词汇表中的现有词。'
- en: Lastly, the **list_models()** method can be used to retrieve and display information
    about the available embedding models provided by the **gensim** library.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，**list_models()** 方法可用于检索和显示**gensim**库提供的可用嵌入模型的信息。
- en: As mentioned earlier, this class is used by both the `Player` and `Game` components,
    discussed in the following subsections.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这个类被`Player`和`Game`组件共同使用，将在下一小节中讨论。
- en: The MysteryWordGame class
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**MysteryWordGame** 类'
- en: 'The `MysteryWordGame` Python class encapsulates the `Game` component. It can
    be found in the `mystery_word_game.py` file, which is located at the following
    link:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`MysteryWordGame` Python 类封装了 `Game` 组件。它可以在以下链接的 `mystery_word_game.py` 文件中找到：'
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/mystery_word_game.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/mystery_word_game.py)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/mystery_word_game.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/mystery_word_game.py)'
- en: 'The main functionality of this class is highlighted as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的主要功能如下：
- en: The class employs the **glove-twitter-50** **gensim** pre-trained embedding
    model developed by Stanford University. This model was specifically designed for
    Twitter text data and utilizes 50-dimensional embedding vectors.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该类使用了斯坦福大学开发的**glove-twitter-50** **gensim** 预训练嵌入模型。该模型专门为Twitter文本数据设计，使用了50维的嵌入向量。
- en: 'The **__init__()** method of the class initializes the embedding model it will
    internally use, and then either selects a random mystery word or uses a word specified
    as an argument for the mystery word:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该类的 **__init__()** 方法初始化它将内部使用的嵌入模型，然后随机选择一个神秘单词或使用作为参数传递的指定单词：
- en: '[PRE2]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The **score_guess()** method calculates the score returned by the game for
    a given guessed word. If the word is not present in the model’s vocabulary, which
    can occur as the player module may use a potentially different model, the score
    is set to the minimum value of -100\. Otherwise, the calculated score value will
    be a number between -100 and 100:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**score_guess()** 方法计算游戏返回的给定猜测单词的得分。如果该单词不在模型的词汇表中（可能是因为玩家模块使用了一个可能不同的模型），则得分设置为最小值
    -100。否则，计算出的得分值将是一个介于 -100 和 100 之间的数字：'
- en: '[PRE3]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The **main()** method tests the class’s functionality by creating an instance
    of the game, selecting the word **"dog"**, and evaluating several guessed words
    related to it, such as **"canine"** and **"hound"**. It also includes an unrelated
    word (**"computer"**) and a word that does not exist in the vocabulary (**"asdghf"**):'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**main()** 方法通过创建游戏的实例来测试该类的功能，选择单词 **"dog"**，并评估与其相关的多个猜测单词，例如 **"canine"**
    和 **"hound"**。它还包括一个不相关的单词（**"computer"**）和一个在词汇表中不存在的单词（**"asdghf"**）：'
- en: '[PRE4]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Executing the `main()` method of the class yields the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 执行该类的 `main()` 方法会产生以下输出：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We are now ready for the interesting component—the program that attempts to
    solve the game.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经准备好进入有趣的部分——试图解决游戏的程序。
- en: The genetic algorithms-based player program
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于遗传算法的玩家程序
- en: As mentioned earlier, this module uses a different embedding model from the
    one used by the game, although it has the option to use the same model. In this
    case, we have selected the `glove-wiki-gigaword-50` `gensim` pre-trained embedding
    model, which was trained on a substantial corpus of text from the English *Wikipedia*
    website and the *Gigaword* dataset.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，该模块使用了与游戏中使用的模型不同的嵌入模型，尽管它也可以选择使用相同的模型。在这种情况下，我们选择了 `glove-wiki-gigaword-50`
    `gensim` 预训练嵌入模型，该模型是在来自英语 *Wikipedia* 网站和 *Gigaword* 数据集的大量语料库上训练的。
- en: Solution representation
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解的表示
- en: The solution representation in the genetic algorithm in this case is a real-valued
    vector (or a list) of the same dimension as the embedding model. This allows each
    solution to serve as an embedding vector, although not perfectly. Initially, the
    algorithm employs randomly generated vectors, and through crossover and mutation
    operations, it’s likely that at least some of the vectors won’t directly correspond
    to existing words in the model’s vocabulary. To address this issue, we utilize
    the `vec2_nearest_word()` method from the `Embedding` class, which returns the
    closest word in the vocabulary. This approach exemplifies the **genotype-to-phenotype
    mapping** concept, as discussed in [*Chapter 4*](B20851_04.xhtml#_idTextAnchor155),
    *Combinatorial Optimization*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在此案例中，遗传算法中的解表示为一个实值向量（或列表），其维度与嵌入模型相同。这使得每个解可以作为一个嵌入向量，尽管并不完全完美。最初，算法使用随机生成的向量，并通过交叉和变异操作，至少可以保证部分向量不会直接与模型词汇中的现有单词对应。为了解决这个问题，我们使用
    `Embedding` 类中的 `vec2_nearest_word()` 方法，该方法返回词汇中最接近的单词。这种方法体现了**基因型到表型映射**的概念，如在[*第4章*](B20851_04.xhtml#_idTextAnchor155)《组合优化》中讨论的那样。
- en: Early convergence criteria
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 早期收敛标准
- en: 'In most cases discussed so far, the solution does not possess the knowledge
    of the best possible score that can be achieved during the optimization process.
    However, in this case, we know that the best possible score is 100\. Once it is
    achieved, the correct word has been found, and there is no point in continuing
    the evolutionary cycle. Therefore, we modified the main loop of our genetic algorithm
    to break if the maximum score is reached. The modified method is called `eaSimple_modified()`
    and can be found in the `elitism_modified.py` module. It accepts an optional parameter
    called `max_fitness`. When this parameter is provided with a value, the main loop
    breaks if the best fitness value found so far reaches or exceeds this value:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在迄今讨论的大多数情况下，解决方案并不知道在优化过程中可以达到的最佳得分。然而，在这种情况下，我们知道最佳得分是100。一旦达到，就找到了正确的单词，继续进化循环就没有意义了。因此，我们修改了遗传算法的主循环以在达到最大分数时中断。修改后的方法称为
    `eaSimple_modified()`，可以在 `elitism_modified.py` 模块中找到。它接受一个名为 `max_fitness` 的可选参数。当此参数提供了一个值时，如果迄今为止找到的最佳适应度值达到或超过此值，则主循环中断：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Printing out the current best-guessed word
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 打印出当前猜测最佳单词
- en: 'Additionally, the `eaSimple_modified()` method includes the printing of the
    guessed word corresponding to the individual with the best fitness found so far,
    as part of the statistics summary generated for every individual:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`eaSimple_modified()` 方法包括打印与迄今为止找到的最佳适应度个体对应的猜测单词，作为为每个个体生成的统计摘要的一部分：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The genetic algorithm implementation
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 遗传算法实现
- en: 'The genetic algorithm-based player for the mystery-word game search for the
    best hyperparameter values is implemented by the `01_find_mystery_word.py` Python
    program, which is located at the following link:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基于遗传算法的玩家为神秘单词游戏寻找最佳超参数值，由位于以下链接的 `01_find_mystery_word.py` Python 程序实现：
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/01_find_mystery_word.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/01_find_mystery_word.py)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/01_find_mystery_word.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/01_find_mystery_word.py)'
- en: 'The following steps describe the main parts of this program:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了这个程序的主要部分：
- en: 'We begin by creating an instance of the **Embeddings** class, which will serve
    as the word embeddings model for the solver program:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '我们首先创建一个 **Embeddings** 类的实例，它将作为解决程序的词嵌入模型： '
- en: '[PRE8]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we create an instance of the **MysteryWordGame** class, which represents
    the game we will be playing. We instruct it to use the word “dog” for demonstration
    purposes. This word can later be replaced with others, or we can allow the game
    to choose a random word if we omit the **given_mystery_word** parameter:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建 **MysteryWordGame** 类的一个实例，代表我们将要玩的游戏。我们指示它使用单词“dog”作为演示目的。稍后可以用其他单词替换这个词，或者如果省略
    **given_mystery_word** 参数，我们可以让游戏选择一个随机单词：
- en: '[PRE9]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Since our goal is to maximize the game’s score, we define a single-objective
    strategy for maximizing fitness:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的目标是最大化游戏的得分，我们定义了一个单目标策略来最大化适应度：
- en: '[PRE10]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To create random individuals representing word embeddings, we create a **randomFloat()**
    function and register it with the toolbox:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建表示词嵌入的随机个体，我们创建一个 **randomFloat()** 函数，并将其注册到工具箱中：
- en: '[PRE11]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The **score()** function is used to evaluate the fitness of each solution,
    and this process consists of two steps: first, we employ the local **embeddings**
    model to find the vocabulary word nearest to the evaluated vector (this is where
    the genotype-to-phenotype mapping takes place). Next, we send this word to the
    **Game** component and request it to score it as a guessed word. The score returned
    by the game, a value ranging from -100 to 100, is directly used as the fitness
    value:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**score()** 函数用于评估每个解决方案的适应度，这个过程包括两个步骤：首先，我们使用本地 **embeddings** 模型找到评估向量最接近的词汇单词（这是基因型到表现型映射发生的地方）。接下来，我们将这个词汇发送到
    **Game** 组件，并请求其评分作为猜测的单词。游戏返回的分数，一个从 -100 到 100 的值，直接用作适应度值：'
- en: '[PRE12]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we need to define genetic operators. While for the *selection* operator,
    we use the usual *tournament selection* with a tournament size of 2, we choose
    *crossover* and *mutation* operators that are specialized for bounded float-list
    chromosomes and provide them with the boundaries we defined for each hyperparameter:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要定义遗传操作符。对于*选择*操作符，我们使用常见的*tournament selection*（锦标赛选择），锦标赛大小为2；而对于*交叉*和*变异*操作符，我们选择专门针对有界浮动列表染色体的操作符，并为每个超参数定义了相应的边界：
- en: '[PRE13]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In addition, we continue to employ the elitist approach, where the **Hall of
    Fame** (**HOF**) members—the current best individuals—are always passed untouched
    to the next generation. However, in this iteration, we use the **eaSimple_modified**
    algorithm, where—in addition—the main loop will terminate when the score reaches
    the maximum known score:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们继续使用精英主义方法，其中**名人堂**（**HOF**）成员——当前最优个体——始终被无修改地传递到下一代。然而，在本次迭代中，我们使用了**eaSimple_modified**算法，其中——此外——主循环将在得分达到已知的最高分时终止：
- en: '[PRE14]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'By running the algorithm with a population size of 30, we get the following
    outcome:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行算法，种群大小为30，得到了以下结果：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'From this printout, we can observe the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个输出中，我们可以观察到以下几点：
- en: Two distinct word embedding models were loaded, one for the player and the other
    for the game, as designed
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载了两个不同的词嵌入模型，一个用于玩家，另一个用于游戏，按照设计进行。
- en: The mystery word that was set to **'dog'** was correctly guessed by the genetic
    algorithm-driven player after 20 generations
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置为**‘狗’**的神秘词汇，在20代之后被遗传算法驱动的玩家正确猜测。
- en: As soon as the word was found, the player quit playing, even though the maximum
    number of generations was set to 1000
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦找到词汇，玩家就停止了游戏，尽管最大代数设置为1000。
- en: 'We can see how the current best-guessed word has evolved:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以看到当前最优猜测词汇的演变过程：
- en: '**stories** → **bucket** → **toys** → **family** → **pet** → **dog**'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故事** → **桶** → **玩具** → **家庭** → **宠物** → **狗**'
- en: This looks great! However, keep in mind that it’s just one example. You are
    encouraged to try out other words, as well as different settings for the genetic
    algorithm; perhaps even change the embedding models. Are there model pairs that
    are less compatible than others?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很棒！不过，请记住这只是一个示例。我们鼓励你尝试其他词汇，以及调整遗传算法的不同设置；也许还可以改变嵌入模型。是否有某些模型对比其他模型兼容性差呢？
- en: In the next portion of this chapter, we will explore **document classification**.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的下一部分，我们将探索**文档分类**。
- en: Document classification
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档分类
- en: Document classification is a critical task in NLP, involving the categorization
    of textual documents into predefined classes or categories based on their content.
    This process is essential for organizing, managing, and extracting meaningful
    information from large volumes of textual data. Applications of document classification
    are vast and diverse, spanning various industries and domains.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 文档分类是自然语言处理中的一项关键任务，涉及根据文本内容将文档分到预定义的类别或类目中。这个过程对于组织、管理和从大量文本数据中提取有意义的信息至关重要。文档分类的应用广泛，涵盖各行各业和领域。
- en: In the field of information retrieval, document classification plays a crucial
    role in **search engines**. By categorizing web pages, articles, and documents
    into relevant topics or genres, search engines can deliver more accurate and targeted
    search results to users. This enhances the overall user experience and ensures
    that individuals can quickly access the information they seek.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息检索领域，文档分类在**搜索引擎**中扮演着至关重要的角色。通过将网页、文章和文档分类到相关的主题或类型，搜索引擎可以为用户提供更精确、更有针对性的搜索结果。这提升了整体用户体验，确保用户能够快速找到所需信息。
- en: In customer service and support, document classification enables the **automatic
    routing** of customer inquiries and messages to the appropriate departments or
    teams. For instance, emails received by a company can be classified into categories
    such as “Billing Inquiries,” “Technical Support,” or “General Inquiries,” ensuring
    that each message reaches the right team for prompt response and resolution.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户服务和支持中，文档分类能够实现**自动路由**客户咨询和信息到相关部门或团队。例如，公司收到的电子邮件可以分类为“账单查询”、“技术支持”或“一般咨询”，确保每一条消息都能及时传达给正确的团队进行处理。
- en: In the legal domain, document classification is instrumental for tasks such
    as **e-discovery**, where large volumes of legal documents need to be analyzed
    for relevance to a case. Classification helps identify documents that are potentially
    pertinent to a legal matter, streamlining the review process and reducing the
    time and resources required for legal proceedings.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在法律领域，文档分类在**电子发现**等任务中至关重要，其中需要分析大量的法律文档以确定它们是否与案件相关。分类有助于识别潜在与法律事务相关的文档，从而简化审查过程，减少法律程序所需的时间和资源。
- en: Moreover, document classification is pivotal in **sentiment analysis**, where
    it can be used to categorize social media posts, reviews, and comments into positive,
    negative, or neutral sentiments. This information is invaluable for businesses
    looking to gauge customer feedback, monitor brand reputation, and make data-driven
    decisions to improve their products or services.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，文档分类在**情感分析**中至关重要，可以用来将社交媒体帖子、评论和意见分类为正面、负面或中性情感。这些信息对于希望评估客户反馈、监控品牌声誉并做出数据驱动决策以改进产品或服务的企业来说是无价的。
- en: One effective method for performing document classification is by leveraging
    n-grams, as elaborated in the upcoming section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 执行文档分类的一个有效方法是利用n-gram，详细内容将在接下来的部分中讲解。
- en: N-grams
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: N-gram
- en: An n-gram is a contiguous sequence of *n* items, which can be characters, words,
    or even phrases, extracted from a larger body of text. By breaking down text into
    these smaller units, n-grams enable the extraction of valuable linguistic patterns,
    relationships, and context.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram是由*n*个项目组成的连续序列，这些项目可以是字符、单词或甚至短语，从更大的文本中提取出来。通过将文本分解成这些较小的单位，n-gram能够提取出有价值的语言模式、关系和语境。
- en: For example, in the case of *character n-grams*, a 3-gram might break the word
    “apple” into “app,” “ppl,” and “ple.”
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在*字符n-gram*的情况下，3-gram可能会将单词“apple”拆分为“app”，“ppl”和“ple”。
- en: 'Here are some examples of *word n-grams*:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些*单词n-gram*的示例：
- en: '**Unigrams (1-grams)**:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单元组（1-gram）**：'
- en: '*Text*: “I love to code.”'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*文本*：“我爱编程。”'
- en: '*Unigrams*: [“I”, “love”, “to”, “code”]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*单元组*：[“我”，“爱”，“编程”]'
- en: '**Bigrams (2-grams)**:'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元组（2-gram）**：'
- en: '*Text*: “Natural language processing is fascinating.”'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*文本*：“自然语言处理是迷人的。”'
- en: '*Bigrams*: [“Natural language”, “language processing”, “processing is”, “is
    fascinating”]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*二元组*：[“自然语言”，“语言处理”，“处理是”，“是迷人”]'
- en: '**Trigrams (3-grams)**:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**三元组（3-gram）**：'
- en: '*Text*: “Machine learning models can generalize.”'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*文本*：“机器学习模型可以泛化。”'
- en: '*Trigrams*: [“Machine learning models”, “learning models can”, “models can
    generalize”]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*三元组*：[“机器学习模型”，“学习模型可以”，“模型可以泛化”]'
- en: N-grams provide insights into textual content by revealing the sequential arrangement
    of words or characters, identifying frequent patterns, and extracting features.
    They help understand language structure, context, and patterns, making them valuable
    for text analysis tasks such as document classification.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram通过揭示单词或字符的顺序排列，识别频繁的模式并提取特征，从而提供对文本内容的洞察。它们有助于理解语言结构、语境和模式，使其在文档分类等文本分析任务中非常有价值。
- en: Selecting a subset of n-grams
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择n-gram的一个子集
- en: 'In [*Chapter 7*](B20851_07.xhtml#_idTextAnchor221), *Enhancing Machine Learning
    Models Using Feature Selection*, we demonstrated the importance of selecting a
    meaningful subset of features, known as “feature selection.” This process is equally
    valuable in document classification, especially when dealing with a large number
    of extracted n-grams, a common occurrence in large documents. The advantages of
    identifying a relevant subset of n-grams include the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B20851_07.xhtml#_idTextAnchor221)，“*通过特征选择增强机器学习模型*”中，我们展示了选择有意义特征子集的重要性，这一过程在文档分类中同样具有价值，尤其是当处理大量提取的n-gram时，这在大型文档中很常见。识别相关n-gram子集的优势包括以下几点：
- en: '**Dimensionality reduction**: Reducing the number of n-grams makes computations
    more efficient and prevents overfitting'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：减少n-gram的数量可以提高计算效率，防止过拟合'
- en: '**Focus on key features**: Selecting discriminative n-grams helps the model
    concentrate on crucial features'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注关键特征**：选择具有区分性的n-gram有助于模型集中关注关键特征'
- en: '**Noise reduction**: Filtering out uninformative n-grams minimizes noise in
    the data'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声减少**：过滤掉无信息的n-gram最小化数据中的噪声'
- en: '**Enhanced generalization**: A well-chosen subset improves the model’s ability
    to handle new documents'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强泛化能力**：精心选择的子集提高了模型处理新文档的能力'
- en: '**Efficiency**: Smaller feature sets speed up model training and prediction'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率**：较小的特征集加速模型的训练和预测'
- en: Furthermore, identifying a relevant subset of n-grams in document classification
    can be valuable for model interpretability. By narrowing down the features to
    a manageable subset, it becomes easier to understand and interpret the factors
    influencing the model’s predictions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在文档分类中识别相关的n-gram子集对于模型可解释性非常重要。通过将特征缩小到一个可管理的子集，理解和解释影响模型预测的因素变得更加容易。
- en: Similarly to what we did in [*Chapter 7*](B20851_07.xhtml#_idTextAnchor221),
    we will apply a genetic algorithms-based search here to identify a relevant subset
    of n-grams. However, considering that the number of n-grams we anticipate is substantially
    larger than the number of features in the common datasets we’ve previously used,
    we won’t be searching for the overall best subset. Instead, our goal will be to
    find a fixed-size subset of features, such as the best 1,000 or 100 n-grams to
    use.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在[*第7章*](B20851_07.xhtml#_idTextAnchor221)中所做的，我们将在这里应用基因算法搜索，以识别相关的n-gram子集。然而，考虑到我们预期的n-gram数量远大于我们之前使用的常见数据集中的特征数量，我们不会寻找整体最好的子集。相反，我们的目标是找到一个固定大小的特征子集，例如最好的1,000个或100个n-gram。
- en: Using genetic algorithms to search for a fixed-size subset
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用基因算法搜索固定大小的子集
- en: 'As we need to identify a good, fixed-size subset of items within a very large
    group, let’s try to define the usual components needed for the genetic algorithm
    to work:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要在一个非常大的群体中识别一个良好的、固定大小的子集，下面我们尝试定义基因算法所需的常见组件：
- en: '**Solution representation**: Since the subset size is much smaller than the
    full dataset, it’s more efficient to use a fixed-size list of integers representing
    the indices of the items within the large dataset. For instance, if we aim to
    create a subset of size 3 from 100 items, a possible solution could be represented
    as a list, such as [5, 42, 88] or [73, 11, 42].'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解空间表示**：由于子集的大小远小于完整数据集，因此使用表示大数据集中项索引的固定大小整数列表更加高效。例如，如果我们旨在从100个项目中创建一个大小为3的子集，则一个可能的解决方案可以表示为列表，如[5,
    42, 88]或[73, 11, 42]。'
- en: '**Crossover operation**: To ensure valid offspring, we must prevent the same
    index from appearing more than once in each offspring. In the previous example,
    the item “42” appears in both lists. If we used a single-point crossover, for
    example, we could end up with the offspring [5, 42, 42], which in effect will
    have only two unique items rather than three. One simple crossover method that
    overcomes this issue would be as follows:'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉操作**：为了确保有效的后代，我们必须防止同一个索引在每个后代中出现多次。在前面的示例中，项“42”出现在两个列表中。如果我们使用单点交叉，可能会得到后代[5,
    42, 42]，这实际上只有两个唯一的项，而不是三个。一种克服这个问题的简单交叉方法如下：'
- en: Create a set containing all unique items present in both parents. In our example,
    this set would be {5, 11, 42, 73, 88}.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个集合，包含两个父代中所有唯一的项。在我们的示例中，这个集合是{5, 11, 42, 73, 88}。
- en: Generate offspring by randomly selecting from the set mentioned previously.
    Each offspring should select three items (in this case). A possible result could
    be [5, 11, 88] and [11, 42, 88].
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从前面提到的集合中随机选择生成后代。每个后代应该选择三个项（在本例中）。可能的结果可以是[5, 11, 88]和[11, 42, 88]。
- en: '**Mutation operation**: A straightforward method to generate a valid mutated
    individual from an existing one is as follows:'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变异操作**：生成一个有效变异个体的简单方法如下：'
- en: For each item in the list, with a specified probability, the item will be replaced
    by one that does exist in the current list.
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于列表中的每个项，按指定的概率，将该项替换为当前列表中存在的项。
- en: For example, if we consider the list [11, 42, 88], there’s a possibility that
    the second item (42) could be replaced with, say, 27, resulting in the list [11,
    27, 88].
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，如果我们考虑列表[11, 42, 88]，则有可能将第二个项（42）替换为27，得到列表[11, 27, 88]。
- en: Python implementation
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python实现
- en: 'In the following sections, we will implement the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将实现以下内容：
- en: A document classifier that will train on document data from two newsgroups and
    use n-grams to predict to which newsgroup each document belongs
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个文档分类器，它将在来自两个新闻组的文档数据上训练，并使用n-gram来预测每个文档属于哪个新闻组
- en: A genetic algorithms-driven optimizer that seeks to find the best subset of
    n-grams to use for this classification task, given the desired size of the subset
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个由基因算法驱动的优化器，旨在根据所需的子集大小，寻找用于此分类任务的最佳n-gram子集
- en: We will start with the class implementing the classifier, as described in the
    next subsection.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从实现分类器的类开始，如下一个子章节所述：
- en: Newsgroup document classifier
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 新闻组文档分类器
- en: 'We start with a Python class called `NewsgroupClassifier`, implementing a `scikit-learn`-based
    document classifier that uses n-grams as features and learns to distinguish between
    posts from two different newsgroups. This class can be found in the `newsgroup_classifier.py`
    file, which is located at the following link:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个名为`NewsgroupClassifier`的Python类开始，实现一个基于`scikit-learn`的文档分类器，该分类器使用n-gram作为特征，并学习区分来自两个不同新闻组的帖子。该类可以在`newsgroup_classifier.py`文件中找到，该文件位于以下链接：
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/newsgroup_classifier.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/newsgroup_classifier.py)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/newsgroup_classifier.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/newsgroup_classifier.py)'
- en: 'The main functionality of this class is highlighted as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的主要功能如下所示：
- en: 'The class’s **init_data()** method, called by **__init__()**, creates training
    and testing sets from **scikit-learn**’s built-in dataset of newsgroup posts.
    It retrieves posts from two categories, **''rec.autos''** and **''rec.motorcycles''**,
    and preprocesses them to remove headers, footers, and quotes:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该类的**init_data()**方法由**__init__()**调用，从**scikit-learn**的内置新闻组数据集中创建训练集和测试集。它从两个类别**'rec.autos'**和**'rec.motorcycles'**中检索帖子，并进行预处理，去除标题、页脚和引用：
- en: '[PRE16]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we create two **TfidfVectorizer** objects: one using word n-grams in
    the range of 1 to 3 words, and the other using character n-grams in the range
    of 1 to 10 characters. These vectorizers convert text documents into numerical
    feature vectors based on the relative frequency of n-grams within each document
    compared to the entire set of documents. These two vectorizers are then combined
    into a single **vectorizer** instance to extract features from the provided newsgroup
    messages:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建两个**TfidfVectorizer**对象：一个使用1到3个词的词n-gram，另一个使用1到10个字符的字符n-gram。这些向量化器根据每个文档中n-gram的相对频率与整个文档集合进行比较，将文本文档转换为数值特征向量。然后，这两个向量化器被合并成一个**vectorizer**实例，以从提供的新闻组消息中提取特征：
- en: '[PRE17]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We proceed by allowing the **vectorizer** instance to “learn” the relevant
    n-gram information from the training data, and then convert both the training
    and test data into datasets of vectors containing their corresponding n-gram-based
    features:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过允许**vectorizer**实例从训练数据中“学习”相关的n-gram信息，然后将训练数据和测试数据转换为包含相应n-gram特征的向量数据集：
- en: '[PRE18]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The **get_predictions()** method generates “reduced” versions of both the training
    and testing sets, utilizing the subset of features provided via the **features_indices**
    parameter. It subsequently employs an instance of **MultinomialNB**, a classifier
    commonly used in the context of text classification, which trains on the reduced
    training set and generates predictions for the reduced testing set:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**get_predictions()**方法生成“简化”版本的训练集和测试集，利用通过**features_indices**参数提供的特征子集。随后，它使用**MultinomialNB**的一个实例，这是一个在文本分类中常用的分类器，它在简化后的训练集上进行训练，并为简化后的测试集生成预测：'
- en: '[PRE19]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The **get_accuracy()** and **get_f1_score()** methods use the **get_predictions()**
    method to calculate and return the accuracy and the f-score of the classifier,
    respectively.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**get_accuracy()**和**get_f1_score()**方法使用**get_predictions()**方法来分别计算和返回分类器的准确率和f-score：'
- en: 'The `main()` method yields the following output:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`main()`方法产生以下输出：'
- en: '[PRE20]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can see that using all 51,280 features, the classifier can achieve an f1-score
    of 0.87, while using a random subset of 100 features has brought the score down
    to 0.59\. Let’s find out if selecting the subset of features using a genetic algorithm
    will enable us to get closer to a higher score.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用所有51,280个特征时，分类器能够达到0.87的f1-score，而使用100个随机特征子集时，得分降至0.59。让我们看看使用遗传算法选择特征子集是否能帮助我们接近更高的得分。
- en: Finding the best feature subset using a genetic algorithm
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用遗传算法找到最佳特征子集
- en: 'The genetic algorithm-based search for the best subset of 100 features (out
    of the original 51,280) is implemented by the `02_solve_newsgroups.py` Python
    program, which is located at the following link:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 基于遗传算法的搜索，用于寻找100个特征的最佳子集（从原始的51,280个特征中挑选），是通过`02_solve_newsgroups.py` Python程序实现的，程序位于以下链接：
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/02_solve_newsgroups.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/02_solve_newsgroups.py)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/02_solve_newsgroups.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/02_solve_newsgroups.py)'
- en: 'The following steps describe the main parts of this program:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了该程序的主要部分：
- en: 'We start by creating an instance of the **NewsgroupClassifier** class that
    will allow us to test the various fixed-size feature subsets:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过创建**NewsgroupClassifier**类的一个实例，来测试不同的固定大小特征子集：
- en: '[PRE21]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We then define two specialized fixed-subset genetic operators, **cxSubset()**—implementing
    the crossover—and **mutSubset()**— implementing the mutation, as we discussed
    earlier.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义了两个专门的固定子集遗传操作符，**cxSubset()**——实现交叉——和**mutSubset()**——实现变异，正如我们之前所讨论的那样。
- en: 'Since our goal is to maximize the f1-score of the classifier, we define a single-objective
    strategy for maximizing fitness:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的目标是最大化分类器的f1分数，我们定义了一个单一目标策略来最大化适应度：
- en: '[PRE22]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To create random individuals representing feature indices, we create a **randomOrder()**
    function, which utilizes **random.sample()** to generate a random set of indices
    within the desired range of 51,280\. We can then use this function to create individuals:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了创建表示特征索引的随机个体，我们创建了**randomOrder()**函数，该函数利用**random.sample()**在51,280的期望范围内生成一个随机的索引集。然后，我们可以使用这个函数来创建个体：
- en: '[PRE23]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The **get_score()** function is used to evaluate the fitness of each solution
    (or subset of features) by calling the **get_f1_score()** method of the **NewsgroupClassifier**
    instance:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**get_score()**函数用于评估每个解（或特征子集）的适应度，它通过调用**NewsgroupClassifier**实例的**get_f1_score()**方法来实现：'
- en: '[PRE24]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we need to define genetic operators. While for the *selection* operator,
    we use the usual *tournament selection* with a tournament size of 2, we choose
    the specialized *crossover* and *mutation* functions that we defined earlier:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要定义遗传操作符。对于*选择*操作符，我们使用常规的*锦标赛选择*，锦标赛大小为2；而对于*交叉*和*变异*操作符，我们选择之前定义的专用函数：
- en: '[PRE25]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, it is time to invoke the genetic algorithm flow, where we continue
    to employ the elitist approach, where the HOF members—the current best individuals—are
    always passed untouched to the next generation:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，是时候调用遗传算法流程，我们继续使用精英主义方法，其中HOF成员——当前最佳个体——始终不加修改地传递到下一代：
- en: '[PRE26]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'By running the algorithm for 5 generations with a population size of 30, we
    get the following outcome:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行该算法5代，种群大小为30，我们得到以下结果：
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The results demonstrate that we successfully identified a subset of 100 features
    with an f1-score of 85.2%, which is remarkably close to the 87.2% score achieved
    using all 51,280 features.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，我们成功识别出了一个包含100个特征的子集，f1分数为85.2%，这一结果与使用全部51,280个特征得到的87.2%分数非常接近。
- en: 'When examining the plots displaying the maximum and average fitness over the
    generations, shown next, it suggests that further improvements might have been
    possible had we extended the evolutionary process:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看显示最大适应度和平均适应度随代数变化的图表时，接下来的结果显示，如果我们延长进化过程，可能会获得进一步的改进：
- en: '![Figure 11.3: Stats of the program searching for the best feature subset](img/B20851_11_3.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3：程序搜索最佳特征子集的统计数据](img/B20851_11_3.jpg)'
- en: 'Figure 11.3: Stats of the program searching for the best feature subset'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：程序搜索最佳特征子集的统计数据
- en: Further reducing the subset size
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 进一步减少子集大小
- en: 'What if we aim to further reduce the subset size to just 10 features? The outcome
    may surprise you. By adjusting the `SUBSET_SIZE` constant to 10, we still achieve
    a commendable f1-score of 76.1%. Notably, when we examine the 10 selected features,
    they appear to be fragments of familiar words. In the context of our classification
    task, which involves distinguishing between posts in a newsgroup dedicated to
    motorcycles and those related to cars, these features start to reveal their relevance:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望进一步将子集大小减少到仅有10个特征呢？结果可能会让你惊讶。通过将`SUBSET_SIZE`常数调整为10，我们依然取得了一个值得称赞的f1得分：76.1%。值得注意的是，当我们检查这10个选定的特征时，它们似乎是一些熟悉单词的片段。在我们的分类任务中，任务是区分专注于摩托车的新闻组和与汽车相关的帖子，这些特征开始展现出它们的相关性：
- en: '[PRE28]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Removing the character n-grams
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 移除字符n-gram
- en: 'The preceding results raise the question of whether we should exclusively utilize
    word n-grams and eliminate character n-grams. We can implement this by employing
    a single vectorizer, as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以上结果引发了一个问题：我们是否应该仅使用单词n-gram，而去除字符n-gram呢？我们可以通过使用一个单一的向量化器来实现，具体方法如下：
- en: '[PRE29]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Initializing newsgroup data...
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 正在初始化新闻组数据...
- en: Number of features = 2666, train set size = 1192, test set size = 794
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 特征数量 = 2666，训练集大小 = 1192，测试集大小 = 794
- en: 'f1 score using all features: 0.8551359241014413'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有特征的f1得分：0.8551359241014413
- en: 'f1 score using random subset of 100 features: 0.6333756056319708'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 使用100个随机特征子集的f1得分：0.6333756056319708
- en: '[PRE30]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: -- Best Ever Fitness =  0.750101164515984
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: -- 最佳健身得分 = 0.750101164515984
- en: -- Features subset selected =
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: -- 选定的特征子集 =
- en: 1:    1669 = oil change
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 1:    1669 = 换油
- en: 2:    472 = cars
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 2:    472 = 汽车
- en: 3:    459 = car
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 3:    459 = 汽车
- en: 4:    361 = bike
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 4:    361 = 自行车
- en: 5:    725 = detector
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 5:    725 = 检测器
- en: 6:    303 = autos
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 6:    303 = 汽车
- en: 7:    296 = auto
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 7:    296 = 自动
- en: 8:    998 = ford
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 8:    998 = 福特
- en: 9:    2429 = toyota
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 9:    2429 = 丰田
- en: 10:    2510 = v6
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 10:    2510 = v6
- en: '[PRE31]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
