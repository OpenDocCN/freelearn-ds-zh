- en: Unsupervised Learning - PCA and Clustering
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习 - PCA 和聚类
- en: Unsupervised learning is one of the most important branches of machine learning.
    It enables us to make predictions when we don't have target labels. In unsupervised
    learning, the model learns only via features because the dataset doesn't have
    a target label column. Most machine learning problems start with something that
    helps automate the process. For example, when you want to develop a prediction
    model for detecting diabetes patients, you need a set of target labels for each
    patient in your dataset. In the initial stages, arranging target labels for any
    machine learning problem is not an easy task, because it requires changing the
    business process to get the labels, whether by manual in-house labeling or collecting
    the data again with labels.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是机器学习中最重要的分支之一。它使我们能够在没有目标标签的情况下进行预测。在无监督学习中，模型仅通过特征进行学习，因为数据集没有目标标签列。大多数机器学习问题从某些能够自动化过程的事物开始。例如，当你想要开发一个预测模型来检测糖尿病患者时，你需要为数据集中的每个患者设置目标标签。在初期阶段，为任何机器学习问题安排目标标签并非易事，因为这需要改变业务流程来获得标签，无论是通过手动内部标注还是再次收集带标签的数据。
- en: In this chapter, our focus is on learning about unsupervised learning techniques
    that can handle situations where target labels are not available. We will especially
    cover dimensionality reduction techniques and clustering techniques. Dimensionality
    reduction will be used where we have a large number of features and we want to
    reduce that amount. This will reduce the model complexity and training cost because
    it means we can achieve the results we want with just a few features.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的重点是学习无监督学习技术，这些技术可以处理没有目标标签的情况。我们将特别介绍降维技术和聚类技术。当我们有大量特征时，降维技术将被使用，以减少这些特征的数量。这将减少模型复杂性和训练成本，因为这意味着我们可以仅通过少量特征就能实现我们想要的结果。
- en: Clustering techniques find groups in data based on similarity. These groups
    essentially represent *unsupervised classification*. In clustering, classes or
    labels for feature observations are found in an unsupervised manner. Clustering
    is useful for various business operations, such as cognitive search, recommendations,
    segmentation, and document clustering.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类技术根据相似性在数据中找到组。这些组本质上代表了*无监督分类*。在聚类中，特征观察的类或标签是以无监督的方式找到的。聚类在各种业务操作中非常有用，例如认知搜索、推荐、细分和文档聚类。
- en: 'Here are the topics of this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主题如下：
- en: Unsupervised learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reducing the dimensions of data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低数据的维度
- en: Principal component analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Clustering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Partitioning data using k-means clustering
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 K-means 聚类对数据进行划分
- en: Hierarchical clustering
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类
- en: DBSCAN clustering
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN 聚类
- en: Spectral clustering
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谱聚类
- en: Evaluating clustering performance
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估聚类性能
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章有以下技术要求：
- en: 'You can find the code and the datasets at the following GitHub link: [https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter11](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter11).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过以下 GitHub 链接找到代码和数据集：[https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter11](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter11)。
- en: All the code blocks are available in the `ch11.ipynb` file.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有代码块都可以在 `ch11.ipynb` 文件中找到。
- en: This chapter uses only one CSV file (`diabetes.csv`) for practice purposes.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章仅使用一个 CSV 文件（`diabetes.csv`）进行实践。
- en: In this chapter, we will use the pandas and scikit-learn Python libraries.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 pandas 和 scikit-learn Python 库。
- en: Unsupervised learning
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Unsupervised learning means learning by observation, not by example. This type
    of learning works with unlabeled data. Dimensionality reduction and clustering
    are examples of such learning. Dimensionality reduction is used to reduce a large
    number of attributes to just a few that can produce the same results. There are
    several methods that are available for reducing the dimensionality of data, such
    as **principal component analysis** (**PCA**), t-SNE, wavelet transformation,
    and attribute subset selection.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习意味着通过观察学习，而不是通过示例学习。这种学习类型适用于无标签的数据。降维和聚类就是这种学习的例子。降维用于将大量特征减少到只有少数几个特征，但能产生相同的结果。有几种方法可以减少数据的维度，例如**主成分分析**（**PCA**）、t-SNE、小波变换和特征子集选择。
- en: 'The term cluster means a group of similar items that are closely related to
    each other. Clustering is an approach for generating units or groups of items
    that are similar to each other. This similarity is computed based on certain features
    or characteristics of items. We can say that a cluster is a set of data points
    that are similar to others in its cluster and dissimilar to data points of other
    clusters. Clustering has numerous applications, such as in searching documents,
    business intelligence, information security, and recommender systems:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 术语“聚类”指的是一组相似的项目，它们彼此密切相关。聚类是一种生成相似单元或项目组的方法。此相似性是基于项目的某些特征或特性计算的。我们可以说，聚类是一组数据点，它们与其聚类中的其他数据点相似，并且与其他聚类的数据点不相似。聚类具有许多应用，例如搜索文档、业务智能、信息安全和推荐系统：
- en: '![](img/6c4790eb-c2f7-4f6e-8e3d-fb3daf08f6ab.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c4790eb-c2f7-4f6e-8e3d-fb3daf08f6ab.png)'
- en: In the preceding diagram, we can see how clustering puts data records or observations
    into a few groups, and dimensionality reduction reduces the number of features
    or attributes. Let's look at each of these topics in detail in the upcoming sections.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以看到聚类如何将数据记录或观察结果分成少数几组，而降维则减少了特征或属性的数量。让我们在接下来的部分详细讨论每个主题。
- en: Reducing the dimensionality of data
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少数据的维度
- en: Reducing dimensionality, or dimensionality reduction, entails scaling down a
    large number of attributes or columns (features) into a smaller number of attributes.
    The main objective of this technique is to get the best number of features for
    classification, regression, and other unsupervised approaches. In machine learning,
    we face a problem called the curse of dimensionality. This is where there is a
    large number of attributes or features. This means more data, causing complex
    models and overfitting problems.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 减少维度，即降维，意味着将大量属性或列（特征）缩减为较少数量的属性。该技术的主要目标是获得最佳的特征数用于分类、回归和其他无监督方法。在机器学习中，我们面临一个称为维度灾难的问题。这意味着有大量属性或特征。这意味着更多的数据，导致复杂的模型和过拟合问题。
- en: 'Dimensionality reduction helps us to deal with the curse of dimensionality.
    It can transform data linearly and nonlinearly. Techniques for linear transformations
    include PCA, linear discriminant analysis, and factor analysis. Non-linear transformations
    include techniques such as t-SNE, Hessian eigenmaps, spectral embedding, and isometric
    feature mapping. Dimensionality reduction offers the following benefits:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 降低数据的维度有助于应对维度灾难。它可以线性和非线性地转换数据。线性转换技术包括PCA、线性判别分析和因子分析。非线性转换包括t-SNE、Hessian特征映射、谱嵌入和等距特征映射等技术。降维提供以下好处：
- en: It filters redundant and less important features.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤冗余和不重要的特征。
- en: It reduces model complexity with less dimensional data.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少模型复杂性，使用较少维度的数据。
- en: It reduces memory and computation costs for model generation.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少模型生成的内存和计算成本。
- en: It visualizes high-dimensional data.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可视化高维数据。
- en: In the next section, we will focus on one of the important and popular dimension
    reduction techniques, PCA.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将专注于一种重要且流行的降维技术之一，PCA。
- en: PCA
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA
- en: In machine learning, it is considered that having a large amount of data means
    having a good-quality model for prediction, but a large dataset also poses the
    challenge of higher dimensionality (or the curse of dimensionality). It causes
    an increase in complexity for prediction models due to the large number of attributes.
    PCA is the most commonly used dimensionality reduction method and helps us to
    identify patterns and correlations in the original dataset to transform it into
    a lower-dimension dataset with no loss of information.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，认为拥有大量数据意味着拥有预测模型的高质量，但大型数据集也带来了更高维度的挑战（或维度灾难）。由于属性数量众多，这导致了预测模型复杂度的增加。PCA是最常用的降维方法，帮助我们识别原始数据集中的模式和相关性，将其转换为一个低维数据集，同时不丢失信息。
- en: 'The main concept of PCA is the discovery of unseen relationships and correlations
    among attributes in the original dataset. Highly correlated attributes are so
    similar as to be redundant. Therefore, PCA removes such redundant attributes.
    For example, if we have 200 attributes or columns in our data, it becomes very
    difficult for us to proceed, what with such a huge number of attributes. In such
    cases, we need to reduce that number to 10 or 20 variables. Another objective
    of PCA is to reduce the dimensionality without affecting the significant information.
    For *p*-dimensional data, the PCA equation can be written as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的主要概念是发现原始数据集中属性之间未见的关系和关联。高度相关的属性是如此相似，以至于它们是冗余的。因此，PCA去除了这些冗余的属性。例如，如果我们的数据中有200个属性或列，那么面对这么多属性时，我们将难以继续处理。在这种情况下，我们需要将这些属性的数量减少到10或20个变量。PCA的另一个目标是减少维度，同时不影响重要信息。对于*p*维数据，PCA的方程可以写成如下：
- en: '![](img/84002ca2-8f82-4924-aa1d-eff456166baf.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84002ca2-8f82-4924-aa1d-eff456166baf.png)'
- en: Principal components are a weighted sum of all the attributes. Here, ![](img/dfcee7e5-ede5-4f6e-979a-853c8d328e60.png)
    are the attributes in the original dataset and ![](img/b146c101-372c-42a9-ac1a-44464567cfe2.png)
    are the weights of the attributes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分是所有属性的加权和。这里，![](img/dfcee7e5-ede5-4f6e-979a-853c8d328e60.png)是原始数据集中的属性，![](img/b146c101-372c-42a9-ac1a-44464567cfe2.png)是属性的权重。
- en: Let's take an example. Let's consider the streets in a given city as attributes,
    and let's say you want to visit this city. Now the question is, how many streets
    you will visit? Obviously, you will want to visit the popular or main streets
    of the city, which let's say is 10 out of the 50 available streets. These 10 streets
    will give you the best understanding of that city. These streets are then principal
    components, as they explain enough of the variance in the data (the city's streets).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子。假设我们将一个城市的街道作为属性，并且假设你想参观这个城市。那么问题是，你会参观多少条街道？显然，你会想参观城市中的热门或主要街道，假设这些街道是50条中的10条。这10条街道将为你提供对这座城市的最佳了解。这些街道就是主成分，因为它们解释了数据（城市街道）中的大部分方差。
- en: Performing PCA
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行PCA
- en: 'Let''s perform PCA from scratch in Python:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头开始在Python中执行PCA：
- en: Compute the correlation or covariance matrix of a given dataset.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算给定数据集的相关或协方差矩阵。
- en: Find the eigenvalues and eigenvectors of the correlation or covariance matrix.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 求解相关或协方差矩阵的特征值和特征向量。
- en: Multiply the eigenvector matrix by the original dataset and you will get the
    principal component matrix.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征向量矩阵与原始数据集相乘，你将得到主成分矩阵。
- en: 'Let''s implement PCA from scratch:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头开始实现PCA：
- en: 'We will begin by importing libraries and defining the dataset:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先导入库并定义数据集：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Calculate the covariance matrix:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Calculate the eigenvalues and eigenvector of the covariance matrix:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵的特征值和特征向量：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Multiply the original data matrix by the eigenvector matrix:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始数据矩阵与特征向量矩阵相乘：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This results in the following output:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生如下输出：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we have computed a principal component matrix from scratch. First, we
    centered the data and computed the covariance matrix. After calculating the covariance
    matrix, we calculated the eigenvalues and eigenvectors. Finally, we chose two
    principal components (the number of components should be equal to the number of
    eigenvalues greater than 1) and multiplied the original data by the sorted and
    selected eigenvectors. We can also perform PCA using the scikit-learn library.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从头开始计算了主成分矩阵。首先，我们对数据进行了中心化并计算了协方差矩阵。计算协方差矩阵后，我们求得了特征值和特征向量。最后，我们选择了两个主成分（主成分的数量应等于特征值大于1的数量），并将原始数据与排序和选择的特征向量相乘。我们也可以使用scikit-learn库来执行PCA。
- en: 'Let''s perform PCA using scikit-learn in Python:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Python中使用scikit-learn进行PCA：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This results in the following output:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生如下输出：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code, we performed PCA using the scikit-learn library. First,
    we created the dataset and instantiated the PCA object. After this, we performed
    `fit_transform()` and generated the principal components.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们使用scikit-learn库执行了PCA。首先，我们创建了数据集并实例化了PCA对象。之后，我们执行了`fit_transform()`并生成了主成分。
- en: That was all about PCA. Now it's time to learn about another unsupervised learning
    concept, clustering.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以上就是PCA的内容。现在是时候了解另一个无监督学习的概念——聚类。
- en: Clustering
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: 'Clustering means grouping items that are similar to each other. Grouping similar
    products, grouping similar articles or documents, and grouping similar customers
    for market segmentation are all examples of clustering. The core principle of
    clustering is minimizing the intra-cluster distance and maximizing the intercluster
    distance. The intra-cluster distance is the distance between data items within
    a group, and the inter-cluster distance is the distance between different groups.
    The data points are not labeled, so clustering is a kind of unsupervised problem.
    There are various methods for clustering and each method uses a different way
    to group the data points. The following diagram shows how data observations are
    grouped together using clustering:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是将相似的项目进行分组。将相似的产品分组、将相似的文章或文档分组、以及将相似的客户进行市场细分，都是聚类的例子。聚类的核心原理是最小化簇内距离并最大化簇间距离。簇内距离是同一组内数据项之间的距离，而簇间距离是不同组之间的距离。由于数据点没有标签，因此聚类是一个无监督问题。聚类有多种方法，每种方法使用不同的方式将数据点分组。下图展示了如何使用聚类将数据观察值分组：
- en: '![](img/4aee9f30-6327-4a98-afc5-4fbd8cd8d687.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4aee9f30-6327-4a98-afc5-4fbd8cd8d687.png)'
- en: 'As we are combining similar data points, the question that arises here is how
    to find the similarity between two data points so we can group similar data objects
    into the same cluster. In order to measure the similarity or dissimilarity between
    data points, we can use distance measures such as Euclidean, Manhattan, and Minkowski
    distance:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将相似的数据点进行合并时，出现的问题是如何找到两个数据点之间的相似性，从而将相似的数据对象归为同一个簇。为了衡量数据点之间的相似性或差异性，我们可以使用距离度量，如欧几里得距离、曼哈顿距离和闵可夫斯基距离：
- en: '![](img/c5729621-be25-414c-a572-53ab75533e6d.png)![](img/9d297c3f-c5a0-4b20-8e0f-757fbe84ac29.png)![](img/363377f5-ed0b-4456-9d38-b5f05ff03c58.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5729621-be25-414c-a572-53ab75533e6d.png)![](img/9d297c3f-c5a0-4b20-8e0f-757fbe84ac29.png)![](img/363377f5-ed0b-4456-9d38-b5f05ff03c58.png)'
- en: Here, the distance formula calculates the distance between two k-dimensional
    vectors, x[i] and y[i].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，距离公式计算的是两个k维向量x[i]和y[i]之间的距离。
- en: Now we know what clustering is, but the most important question is, how many
    numbers of clusters should be considered when grouping the data? That's the biggest
    challenge for most clustering algorithms. There are lots of ways to decide the
    number of clusters. Let's discuss those methods in the next section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了什么是聚类，但最重要的问题是，在对数据进行分组时，应该考虑多少个簇？这是大多数聚类算法面临的最大挑战。有很多方法可以决定簇的数量。让我们在接下来的章节中讨论这些方法。
- en: Finding the number of clusters
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查找簇数
- en: 'In this section, we will focus on the most fundamental issue of clustering
    algorithms, which is discovering the number of clusters in a dataset – there is
    no definitive answer. However, not all clustering algorithms require a predefined
    number of clusters. In hierarchical and DBSCAN clustering, there is no need to
    define the number of clusters, but in k-means, k-medoids, and spectral clustering,
    we need to define the number of clusters. Selecting the right value for the number
    of clusters is tricky, so let''s look at a couple of the methods for determining
    the best number of clusters:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点讨论聚类算法中的最基本问题，即发现数据集中的簇数——这个问题没有明确的答案。然而，并非所有的聚类算法都需要预定义簇数。在层次聚类和DBSCAN聚类中，不需要定义簇数，但在k-means、k-medoids和谱聚类中，我们需要定义簇数。选择簇数的正确值是非常棘手的，因此我们来看看几种确定最佳簇数的方法：
- en: The elbow method
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肘部法则
- en: The silhouette method
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轮廓法
- en: Let's look at these methods in detail.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解这些方法。
- en: The elbow method
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 肘部法则
- en: 'The elbow method is a well-known method for finding out the best number of
    clusters. In this method, we focus on the percentage of variance for the different
    numbers of clusters. The core concept of this method is to select the number of
    clusters that appending another cluster should not cause a huge change in the
    variance. We can plot a graph for the sum of squares within a cluster using the
    number of clusters to find the optimal value. The sum of squares is also known
    as the **Within-Cluster Sum of Squares** (**WCSS**) or inertia:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部法则是一种广为人知的确定最佳簇数的方法。在这种方法中，我们关注不同簇数下的方差百分比。该方法的核心概念是选择一个簇数，使得再增加一个簇时，方差不会发生巨大的变化。我们可以使用簇数绘制一个图，表示簇内平方和，以找到最佳值。簇内平方和也被称为**簇内平方和**（**WCSS**）或惯性：
- en: '![](img/1fcf7f70-3fae-44a8-bc68-82a8557f1729.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fcf7f70-3fae-44a8-bc68-82a8557f1729.png)'
- en: 'Here![](img/38f9dbcc-1544-468e-9c78-f801e65b255c.png) is the cluster centroid
    and ![](img/ebbb69a9-9801-48c0-bf4a-ac68a5d2db80.png) is the data points in each
    cluster:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里！[](img/38f9dbcc-1544-468e-9c78-f801e65b255c.png) 是聚类的质心，![](img/ebbb69a9-9801-48c0-bf4a-ac68a5d2db80.png)
    是每个聚类中的数据点：
- en: '![](img/770d743c-1fe4-4a53-9339-1c68692c8cb8.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/770d743c-1fe4-4a53-9339-1c68692c8cb8.png)'
- en: As you can see, at k = 3, the graph begins to flatten significantly, so we would
    choose 3 as the number of clusters.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，在 k = 3 时，图形开始显著变平，因此我们会选择 3 作为聚类数。
- en: 'Let''s find the optimal number of clusters using the elbow method in Python:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用肘部法则在 Python 中找到最佳聚类数：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This results in the following output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/6dbc98cd-5a3e-4775-b09c-8dd3aa022614.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6dbc98cd-5a3e-4775-b09c-8dd3aa022614.png)'
- en: In the preceding example, we created a DataFrame with two columns, `X` and `Y`.
    We generated the clusters using `K-means` and computed the WCSS. After this, we
    plotted the number of clusters and inertia. As you can see at k = 2, the graph
    begins to flatten significantly, so we would choose 2 as the best number of clusters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们创建了一个包含 `X` 和 `Y` 两列的 DataFrame。我们使用 `K-means` 生成了聚类并计算了 WCSS。之后，我们绘制了聚类数量和惯性图。如图所示，在
    k = 2 时，图形开始显著变平，因此我们会选择 2 作为最佳聚类数。
- en: The silhouette method
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 轮廓法
- en: 'The silhouette method assesses and validates cluster data. It finds how well
    each data point is classified. The plot of the silhouette score helps us to visualize
    and interpret how well data points are tightly grouped within their own clusters
    and separated from others. It helps us to evaluate the number of clusters. Its
    score ranges from -1 to +1\. A positive value indicates a well-separated cluster
    and a negative value indicates incorrectly assigned data points. The more positive
    the value, the further data points are from the nearest clusters; a value of zero
    indicates data points that are at the separation line between two clusters. Let''s
    see the formula for the silhouette score:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓法评估并验证聚类数据。它找出每个数据点的分类效果。轮廓分数的图表帮助我们可视化和解释数据点在各自聚类内部的紧密程度，以及与其他聚类的分离程度。它帮助我们评估聚类的数量。其得分范围从
    -1 到 +1。正值表示聚类分离良好，负值表示数据点被错误地分配。值越正，数据点与最近的聚类的距离越远；零值表示数据点位于两个聚类之间的分隔线上。让我们看看轮廓分数的公式：
- en: '![](img/ceb5698c-b585-447b-99fb-a7fa76c37ca7.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ceb5698c-b585-447b-99fb-a7fa76c37ca7.png)'
- en: '*a[i]* is the average distance of the *i*th data point from other points within
    the cluster.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*a[i]* 是第 *i* 个数据点与聚类内其他点的平均距离。'
- en: '*b[i]* is the average distance of the *i*th data point from other cluster points.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*b[i]* 是第 *i* 个数据点与其他聚类点的平均距离。'
- en: This means we can easily say that *S(i)* would be between [-1, 1]. So, for *S(i)*
    to be near to 1, *a[i]* must be very small compared to *b*[*i*, that is,]*e. a[i]
    << b[i]*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以轻松地说 *S(i)* 的值将在[-1, 1]之间。所以，*S(i)* 要接近 1，*a[i]* 必须相较于 *b*[*i*，即]*e，a[i]
    << b[i]*。
- en: 'Let''s find the optimum number of clusters using the silhouette score in Python:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用轮廓分数在 Python 中找到最佳聚类数：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/586ffef3-8bdc-417c-87a0-f59b7ecf6cdd.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/586ffef3-8bdc-417c-87a0-f59b7ecf6cdd.png)'
- en: In the preceding example, we created a DataFrame with two columns, `X` and `Y`.
    We generated clusters with different numbers of clusters on the created DataFrame
    using `K-means` and computed the silhouette score. After this, we plotted the
    number of clusters and the silhouette scores using a barplot. As you can see,
    at k = 2, the silhouette score has the highest value, so we would choose 2 clusters.
    Let's jump to the k-means clustering technique.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们创建了一个包含 `X` 和 `Y` 两列的 DataFrame。我们使用 `K-means` 在创建的 DataFrame 上生成了不同数量的聚类并计算了轮廓分数。之后，我们使用条形图绘制了聚类数量和轮廓分数。如图所示，在
    k = 2 时，轮廓分数达到最高值，因此我们会选择 2 个聚类。接下来，我们进入 k-means 聚类技术。
- en: Partitioning data using k-means clustering
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 k-means 聚类对数据进行分区
- en: 'k-means is one of the simplest, most popular, and most well-known clustering
    algorithms. It is a kind of partitioning clustering method. It partitions input
    data by defining a random initial cluster center based on a given number of clusters.
    In the next iteration, it associates the data items to the nearest cluster center
    using Euclidean distance. In this algorithm, the initial cluster center can be
    chosen manually or randomly. k-means takes data and the number of clusters as
    input and performs the following steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: k-means是最简单、最流行且最著名的聚类算法之一。它是一种划分聚类方法。它通过定义一个基于给定聚类数目的随机初始聚类中心来划分输入数据。在下一次迭代中，它使用欧氏距离将数据项与最近的聚类中心关联。在该算法中，初始聚类中心可以手动选择或随机选择。k-means以数据和聚类数目作为输入，执行以下步骤：
- en: Select *k* random data items as the initial centers of clusters.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择*k*个随机数据项作为聚类的初始中心。
- en: Allocate the data items to the nearest cluster center.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据项分配到最近的聚类中心。
- en: Select the new cluster center by averaging the values of other cluster items.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算其他聚类项的平均值来选择新的聚类中心。
- en: Repeat steps 2 and 3 until there is no change in the clusters.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和3，直到聚类不再发生变化。
- en: 'This algorithm aims to minimize the sum of squared errors:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法旨在最小化平方误差之和：
- en: '![](img/daea790b-7faa-48c1-bd2b-5d94b91df8fb.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/daea790b-7faa-48c1-bd2b-5d94b91df8fb.png)'
- en: 'k-means is one of the fastest and most robust algorithms of its kind. It works
    best with a dataset with distinct and separate data items. It generates spherical
    clusters. k-means requires the number of clusters as input at the beginning. If
    data items are very much overlapped, it doesn''t work well. It captures the local
    optima of the squared error function. It doesn''t perform well with noisy and
    non-linear data. It also doesn''t work well with non-spherical clusters. Let''s
    create a clustering model using k-means clustering:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: k-means是同类算法中最快且最稳健的算法之一。它在数据集具有明显且分离的数据项时效果最佳。它生成球形聚类。k-means在开始时需要聚类数作为输入。如果数据项高度重叠，它的表现不佳。它捕捉到的是平方误差函数的局部最优解。它在处理噪声和非线性数据时表现不佳。它也不适用于非球形聚类。让我们使用k-means聚类创建一个聚类模型：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This results in the following output:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/eeacfb97-2443-49b0-b672-981e3fb70076.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eeacfb97-2443-49b0-b672-981e3fb70076.png)'
- en: In the preceding code example, we imported the `KMeans` class and created its
    object or model. This model will fit it on the dataset (without labels). After
    training, the model is ready to make predictions using the `predict()` method.
    After predicting the results, we plotted the cluster results using a scatter plot.
    In this section, we have seen how k-means works and its implementation using the
    scikit-learn library. In the next section, we will look at hierarchical clustering.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们导入了`KMeans`类并创建了它的对象或模型。该模型将在数据集（无标签）上进行拟合。训练后，模型已经准备好使用`predict()`方法进行预测。在预测结果后，我们使用散点图绘制了聚类结果。在本节中，我们已经了解了k-means的工作原理以及如何使用scikit-learn库实现它。在下一节中，我们将介绍层次聚类。
- en: Hierarchical clustering
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: 'Hierarchical clustering groups data items based on different levels of a hierarchy.
    It combines the items in groups based on different levels of a hierarchy using
    top-down or bottom-up strategies. Based on the strategy used, hierarchical clustering
    can be of two types – agglomerative or divisive:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类根据不同的层次结构级别对数据项进行分组。它使用自顶向下或自底向上的策略，根据不同的层次结构级别将数据项组合成组。根据所使用的策略，层次聚类可以分为两种类型——凝聚型或分裂型：
- en: The agglomerative type is the most widely used hierarchical clustering technique.
    It groups similar data items in the form of a hierarchy based on similarity. This
    method is also called **Agglomerative Nesting** (**AGNES**). This algorithm starts
    by considering every data item as an individual cluster and combines clusters
    based on similarity. It iteratively collects small clusters and combines them
    into a single large cluster. This algorithm gives its result in the form of a
    tree structure. It works in a bottom-up manner; that is, every item is initially
    considered as a single element cluster and in each iteration of the algorithm,
    the two most similar clusters are combined and form a bigger cluster.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合型层次聚类是最广泛使用的层次聚类技术。它基于相似性将相似的数据项分组成层次结构。这种方法也叫做**聚合嵌套**（**AGNES**）。该算法从将每个数据项视为单独的聚类开始，并根据相似性合并聚类。它迭代地收集小的聚类，并将其合并为一个大的聚类。该算法的结果以树形结构呈现。它以自底向上的方式工作；也就是说，每个项最初被视为一个单独的元素聚类，在算法的每次迭代中，两个最相似的聚类会被合并，形成一个更大的聚类。
- en: 'Divisive hierarchical clustering is a top-down strategy algorithm. It is also
    known as **Divisive Analysis** (**DIANA**). It starts with all the data items
    as a single big cluster and partitions recursively. In each iteration, clusters
    are divided into two non-similar or heterogeneous sub-clusters:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分裂式层次聚类是一种自上而下的策略算法。它也被称为**分裂分析**（**DIANA**）。它从将所有数据项视为一个大的聚类开始，并递归地进行划分。在每次迭代中，聚类被分成两个非相似或异质的子聚类：
- en: '![](img/c6980eaf-08c4-40b2-9672-2637b8d1df93.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6980eaf-08c4-40b2-9672-2637b8d1df93.png)'
- en: 'In order to decide which clusters should be grouped or split, we use various
    distances and linkage criteria such as single, complete, average, and centroid
    linkage. These criteria decide the shape of the cluster. Both types of hierarchical
    clustering (agglomerative and divisive hierarchical clustering) require a predefined
    number of clusters or a distance threshold as input to terminate the recursive
    process. It is difficult to decide the distance threshold, so the easiest option
    is to check the number of clusters using a dendrogram. Dendrograms help us to
    understand the process of hierarchical clustering. Let''s see how to create a
    dendrogram using the `scipy` library:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了决定哪些聚类应该被组合或拆分，我们使用各种距离和链接标准，例如单链、全链、平均链和质心链。这些标准决定了聚类的形状。两种类型的层次聚类（聚合型和分裂型层次聚类）都需要预定义的聚类数量或距离阈值作为输入，以终止递归过程。由于很难确定距离阈值，因此最简单的选项是通过树状图检查聚类数量。树状图帮助我们理解层次聚类的过程。让我们来看一下如何使用`scipy`库创建树状图：
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This results in the following output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/277c91a5-02e5-40c7-a414-c1bbadd82e82.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/277c91a5-02e5-40c7-a414-c1bbadd82e82.png)'
- en: 'In the preceding code example, we created the dataset and generated the dendrogram
    using ward linkage. For the dendrograms, we used the `scipy.cluster.hierarchy`
    module. To set the plot title and axis labels, we used `matplotlib`. In order
    to select the number of clusters, we need to draw a horizontal line without intersecting
    the clusters and count the number of vertical lines to find the number of clusters.
    Let''s create a clustering model using agglomerative clustering:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们创建了数据集，并使用ward链接生成了树状图。对于树状图，我们使用了`scipy.cluster.hierarchy`模块。为了设置图表标题和轴标签，我们使用了`matplotlib`。为了选择聚类数量，我们需要画一条横线，使其不与聚类相交，并计算垂直线的数量以找出聚类数。让我们使用聚合型聚类创建一个聚类模型：
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This results in the following output:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/67093ca4-b8e3-4534-a276-57edc0e0d8f2.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67093ca4-b8e3-4534-a276-57edc0e0d8f2.png)'
- en: In the preceding code example, we imported the `AgglomerativeClustering` class
    and created its object or model. This model will fit on the dataset without labels.
    After training, the model is ready to make predictions using the `predict()` method.
    After predicting the results, we plotted the cluster results using a scatter plot.
    In this section, we have seen how hierarchical clustering works and its implementation
    using the `scipy` and scikit-learn libraries. In the next section, we will look
    at density-based clustering.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们导入了`AgglomerativeClustering`类，并创建了它的对象或模型。该模型将适应没有标签的数据集。训练后，模型准备好使用`predict()`方法进行预测。预测结果之后，我们使用散点图绘制了聚类结果。在这一部分中，我们已经看到层次聚类的工作原理及其使用`scipy`和scikit-learn库的实现。在下一部分中，我们将介绍基于密度的聚类。
- en: DBSCAN clustering
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBSCAN 聚类
- en: 'Partitioning clustering methods, such as k-means, and hierarchical clustering
    methods, such as agglomerative clustering, are good for discovering spherical
    or convex clusters. These algorithms are more sensitive to noise or outliers and
    work for well-separated clusters:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 分区聚类方法，如 k-means，和层次聚类方法，如凝聚聚类，适用于发现球形或凸形的簇。这些算法对噪声或离群点较为敏感，并且适用于分离良好的簇：
- en: '![](img/cd625968-9075-4b20-b1b4-cfcb06357e31.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd625968-9075-4b20-b1b4-cfcb06357e31.png)'
- en: Intuitively, we can say that a density-based clustering approach is most similar
    t how we as humans might instinctively group items. In all the preceding figures,
    we can quickly see the number of different groups or clusters due to the density
    of the items.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，我们可以认为基于密度的聚类方法最类似于我们人类可能本能地对物品进行分组的方式。在所有前面的图中，我们可以通过物品的密度快速看到不同组或簇的数量。
- en: '**Density-Based Spatial Clustering of Applications with Noise** (**DBSCAN**)
    is based on the idea of groups and noise. The main idea behind it is that each
    data item of a group or cluster has a minimum number of data items in a given
    radius.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于密度的空间聚类法（带噪声）** (**DBSCAN**) 基于群组和噪声的思想。其核心思想是每个群组或簇中的每个数据项在给定半径内都有最少数量的数据项。'
- en: The main goal of DBSCAN is to discover the dense region that can be computed
    using minimum number of objects (`minPoints`) and given radius (`eps`). DBSCAN
    has the capability to generate random shapes of clusters and deal with noise in
    a dataset. Also, there is no requirement to feed in the number of clusters. DBSCAN
    automatically identifies the number of clusters in the data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 的主要目标是发现密集区域，该区域可以通过最少数量的对象（`minPoints`）和给定半径（`eps`）来计算。DBSCAN 能够生成随机形状的簇，并处理数据集中的噪声。此外，DBSCAN
    不需要输入簇的数量。它会自动识别数据中的簇数。
- en: 'Let''s create a clustering model using DBSCAN clustering in Python:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Python 中的 DBSCAN 聚类方法来创建一个聚类模型：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in the following output:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![](img/923fc029-3684-4ddd-9200-7d7f71d401ad.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/923fc029-3684-4ddd-9200-7d7f71d401ad.png)'
- en: First, we import the `DBSCAN` class and create the moon dataset. After this,
    we create the DBSCAN model and fit it on the dataset. DBSCAN does not need the
    number of clusters. After training, the model is ready to make predictions using
    the `predict()` method. After predicting the results, we plotted the cluster results
    using a scatter plot. In this section, we have seen how DBSCAN clustering works
    and its implementation using the scikit-learn library. In the next section, we
    will see the spectral clustering technique.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入 `DBSCAN` 类并创建月亮数据集。之后，我们创建 DBSCAN 模型并将其拟合到数据集上。DBSCAN 不需要簇的数量。训练完成后，模型可以使用
    `predict()` 方法进行预测。预测结果后，我们通过散点图绘制了聚类结果。在本节中，我们了解了 DBSCAN 聚类的工作原理及其在 scikit-learn
    库中的实现。下一节中，我们将介绍谱聚类技术。
- en: Spectral clustering
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谱聚类
- en: Spectral clustering is a method that employs the spectrum of a similarity matrix.
    The spectrum of a matrix represents the set of its eigenvalues, and a similarity
    matrix consists of similarity scores between each data point. It reduces the dimensionality
    of data before clustering. In other words, we can say that spectral clustering
    creates a graph of data points, and these points are mapped to a lower dimension
    and separated into clusters.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类是一种利用相似度矩阵谱的方法。矩阵的谱代表其特征值的集合，而相似度矩阵由每个数据点之间的相似度分数组成。它在聚类前进行数据的降维。换句话说，谱聚类创建一个数据点图，这些点被映射到较低的维度，并分成不同的簇。
- en: 'A similarity matrix converts data to conquer the lack of convexity in the distribution.
    For any dataset, the data points could be *n*-dimensional, and here could be *m*
    data points. From these *m* points, we can create a graph where the points are
    nodes and the edges are weighted with the similarity between points. A common
    way to define similarity is with a Gaussian kernel, which is a nonlinear function
    of Euclidean distance:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度矩阵将数据转换，以克服分布中缺乏凸性的难题。对于任何数据集，数据点可能是*n*维的，且这里可能有*m*个数据点。从这*m*个点中，我们可以创建一个图，其中点是节点，边则是节点之间的相似度加权。定义相似度的一种常见方式是使用高斯核，它是欧几里得距离的非线性函数：
- en: '![](img/7ee0c840-cc47-4d40-8587-a0d37813e6d8.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ee0c840-cc47-4d40-8587-a0d37813e6d8.png)'
- en: The distance of this function ranges from 0 to 1\. The fact that it's bounded
    between zero and one is a nice property. The absolute distance (it can be anything)
    in Euclidean distance can cause instability and difficulty in modeling. You can
    think of the Gaussian kernel as a normalization function for Euclidean distance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的距离范围从 0 到 1。它被限制在零和一之间是一个很好的性质。欧几里得距离中的绝对距离（它可以是任何值）可能导致不稳定性和建模困难。你可以将高斯核视为欧几里得距离的归一化函数。
- en: 'After getting the graph, create an adjacency matrix and put in each cell of
    the matrix the weight of the edge ![](img/5464e4e3-4ae8-4e0f-89ce-171574d90d18.png).
    This is a symmetric matrix. Let''s call the adjacency matrix A. We can also create
    a "degree" diagonal matrix D, which will have in each ![](img/443cf00c-34b5-4f31-8109-7954d92bbda6.png)
    element the sum of the weights of all edges linked to node *i*. Let''s call this
    matrix D. For a given graph G with *n* vertices, its *n***n* Laplacian matrix
    can be defined as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得图形后，创建一个邻接矩阵，并在矩阵的每个单元格中填入边的权重 ![](img/5464e4e3-4ae8-4e0f-89ce-171574d90d18.png)。这是一个对称矩阵。我们将邻接矩阵称为
    A。我们还可以创建一个“度数”对角矩阵 D，在每个 ![](img/443cf00c-34b5-4f31-8109-7954d92bbda6.png) 元素中填入与节点
    *i* 相连的所有边的权重之和。我们将这个矩阵称为 D。对于一个给定的图 G，假设它有 *n* 个顶点，它的 *n*×*n* 拉普拉斯矩阵可以定义如下：
- en: '![](img/215d43f8-0e28-42b0-8933-f5c90615cf90.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/215d43f8-0e28-42b0-8933-f5c90615cf90.png)'
- en: Here *D* is the degree matrix and *A* is the adjacency matrix of the graph.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *D* 是度数矩阵，*A* 是图的邻接矩阵。
- en: Now we have the Laplacian matrix of the graph (G). We can compute the spectrum
    of a matrix of eigenvectors. If we take *k* least-significant eigenvectors, we
    get a representation in *k* dimensions. The least-significant eigenvectors are
    the ones associated with the smallest eigenvalues. Each eigenvector provides information
    about the connectivity of the graph.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了图（G）的拉普拉斯矩阵。我们可以计算矩阵的特征向量谱。如果我们取 *k* 个最不显著的特征向量，就可以得到一个 *k* 维度的表示。最不显著的特征向量与最小的特征值相关。每个特征向量提供有关图的连通性的信息。
- en: The idea of spectral clustering is to cluster the points using these *k* eigenvectors
    as features. So, you take the *k* least-significant eigenvectors and you have
    your *m* points in *k* dimensions. You run a clustering algorithm, such as k-means,
    and then you have your result. This *k* in spectral clustering is deeply related
    to the Gaussian kernel k-means. You can also think about it as a clustering method
    where your points are projected into a space of infinite dimensions, clustered
    there, and then you use those results as the results of clustering your points.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类的思想是使用这 *k* 个特征向量来对点进行聚类。所以，你取 *k* 个最不显著的特征向量，你就有了 *k* 维度下的 *m* 个点。然后，你运行一个聚类算法，比如
    k-means，最后得到结果。在谱聚类中，这个 *k* 与高斯核 k-means 密切相关。你也可以将其看作是一种聚类方法，其中你的点被投影到一个无限维的空间中，在那里进行聚类，然后你使用这些结果作为聚类点的结果。
- en: 'Spectral clustering is used when k-means works badly because the clusters are
    not linearly distinguishable in their original space. We can also try other clustering
    methods, such as hierarchical clustering or density-based clustering, to solve
    this problem. Let''s create a clustering model using spectral clustering in Python:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当 k-means 聚类效果不好时，通常使用谱聚类，因为原始空间中的簇无法线性区分。我们还可以尝试其他聚类方法，如层次聚类或基于密度的聚类，来解决这个问题。让我们在
    Python 中使用谱聚类创建一个聚类模型：
- en: '[PRE13]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This results in the following output:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![](img/46feaaeb-2ee3-4038-b110-87ccb5642f3f.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46feaaeb-2ee3-4038-b110-87ccb5642f3f.png)'
- en: In the preceding code example, we imported the `SpectralClustering` class and
    created a dummy dataset using pandas. After this, we created the model and fit
    it on the dataset. After training, the model is ready to make predictions using
    the `predict()` method. In this section, we have seen how spectral clustering
    works and its implementation using the scikit-learn library. In the next section,
    we will see how to evaluate a clustering algorithm's performance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们导入了 `SpectralClustering` 类，并使用 pandas 创建了一个虚拟数据集。之后，我们创建了模型并将其拟合到数据集上。训练完成后，模型已经准备好使用
    `predict()` 方法进行预测。在本节中，我们已经了解了谱聚类的工作原理及其在 scikit-learn 库中的实现。在下一节中，我们将学习如何评估聚类算法的性能。
- en: Evaluating clustering performance
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估聚类性能
- en: 'Evaluating clustering performance is an essential step to assess the strength
    of a clustering algorithm for a given dataset. Assessing performance in an unsupervised
    environment is not an easy task, but in the literature, many methods are available.
    We can categorize these methods into two broad categories: internal and external
    performance evaluation. Let''s learn about both of these categories in detail.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 评估聚类性能是评估聚类算法在给定数据集上强度的必要步骤。在无监督环境中评估性能并非易事，但文献中有许多可用的方法。我们可以将这些方法分为两大类：内部评估和外部评估。让我们详细了解这两类评估方法。
- en: Internal performance evaluation
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内部性能评估
- en: In internal performance evaluation, clustering is evaluated based on feature
    data only. This method does not use any target label information. These evaluation
    measures assign better scores to clustering methods that generate well-separated
    clusters. Here, a high score does not guarantee effective clustering results.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部性能评估中，聚类是仅基于特征数据进行评估的。这种方法不使用任何目标标签信息。这些评估指标为生成良好分离聚类的聚类方法分配更高的分数。在这里，高分数并不保证有效的聚类结果。
- en: 'Internal performance evaluation helps us to compare multiple clustering algorithms
    but it does not mean that a better-scoring algorithm will generate better results
    than other algorithms. The following internal performance evaluation measures
    can be utilized to estimate the quality of generated clusters:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 内部性能评估有助于我们比较多个聚类算法，但这并不意味着得分更高的算法会比其他算法生成更好的结果。以下内部性能评估指标可以用来估计生成聚类的质量：
- en: The Davies-Bouldin index
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Davies-Bouldin指数
- en: 'The **Davies-Bouldin index** (**BDI**) is the ratio of intra-cluster distance
    to inter-cluster distance. A lower DBI value means better clustering results.
    This can be calculated as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**Davies-Bouldin指数** (**BDI**) 是聚类内距离与聚类间距离的比值。较低的DBI值意味着更好的聚类结果。计算公式如下：'
- en: '![](img/e75f87e1-1f2d-4dbd-aa1a-fb92b1212df6.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e75f87e1-1f2d-4dbd-aa1a-fb92b1212df6.png)'
- en: 'Here, the following applies:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，以下公式适用：
- en: 'n: The number of clusters'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n：聚类的数量
- en: 'c[i]: The centroid of cluster *i*'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: c[i]：聚类 *i* 的质心
- en: 'σ[i]: The intra-cluster distance or average distance of all cluster items from
    the centroid c[i]'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: σ[i]：聚类内距离或所有聚类项到质心c[i]的平均距离
- en: 'd(c[i], c[j]): The inter-cluster distance between two cluster centroids c[i]
    and c[j]'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: d(c[i], c[j])：两个聚类质心c[i]和c[j]之间的聚类间距离
- en: The silhouette coefficient
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 轮廓系数
- en: 'The silhouette coefficient finds the similarity of an item in a cluster to
    its own cluster items and other nearest clusters. It is also used to find the
    number of clusters, as we have seen elsewhere in this chapter. A high silhouette
    coefficient means better clustering results. This can be calculated as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数用于找出一个项目与其自身聚类项目和其他最近聚类之间的相似度。它也用于确定聚类的数量，如我们在本章其他部分看到的那样。较高的轮廓系数意味着更好的聚类结果。计算公式如下：
- en: '![](img/ebc76612-2f75-4eff-8f4b-2a212a2d9e76.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ebc76612-2f75-4eff-8f4b-2a212a2d9e76.png)'
- en: a[i] is the average distance of the *i*^(th) data point to other points within
    the cluster.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: a[i] 是第 *i* 个数据点与聚类内其他点的平均距离。
- en: b[i] is the average distance of the *i*^(th) data point to other cluster points.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: b[i] 是第 *i* 个数据点与其他聚类点之间的平均距离。
- en: So, we can say that *S(i)* would be between [-1, 1]. So, for *S(i)* to be near
    to 1, a[i] must be very small compared to b[i], that is, a[i] << b[i].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以说 *S(i)* 的值会在[-1, 1]之间。为了让 *S(i)* 接近1，a[i]必须比b[i]小很多，即a[i] << b[i]。
- en: External performance evaluation
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部性能评估
- en: In external performance evaluation, generated clustering is evaluated using
    the actual labels of clusters that are not used in the clustering process. It
    is similar to a supervised learning evaluation process; that is, we can use the
    same confusion matrix here to assess performance. The following external evaluation
    measures are used to evaluate the quality of generated clusters.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在外部性能评估中，生成的聚类是通过与聚类过程中未使用的实际标签进行评估的。这类似于监督学习评估过程；也就是说，我们可以使用相同的混淆矩阵来评估性能。以下外部评估指标用于评估生成聚类的质量。
- en: The Rand score
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Rand得分
- en: 'The Rand score shows how similar a cluster is to the benchmark classification
    and computes the percentage of correctly made decisions. A lower value is preferable
    because this represents distinct clusters. This can be calculated as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Rand得分表示一个聚类与基准分类的相似度，并计算正确决策的百分比。较低的值更可取，因为这表示聚类更加分明。计算公式如下：
- en: '![](img/266b80ed-0d3b-4721-9b47-893f2c7ac724.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/266b80ed-0d3b-4721-9b47-893f2c7ac724.png)'
- en: 'Here, the following applies:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，以下内容适用：
- en: 'TP: Total number of true positives'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TP：真阳性的总数
- en: 'TN: Total number of true negatives'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TN：真阴性的总数
- en: 'FP: Total number of false positives'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP：假阳性的总数
- en: 'FN: Total number of false negatives'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FN：假阴性的总数
- en: The Jaccard score
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jaccard分数
- en: 'The Jaccard score computes the similarity between two datasets. It ranges from
    0 to 1\. 1 means the datasets are identical and 0 means the datasets have no common
    elements. A low value is preferable because it indicates distinct clusters. This
    can be calculated as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard分数计算两个数据集之间的相似性。它的取值范围是0到1，1表示数据集完全相同，0表示数据集没有共同元素。较低的值更可取，因为它表示簇之间的区别。这可以按以下方式计算：
- en: '![](img/106dca43-e3b1-49b8-a4c4-e16f9ae9c12e.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/106dca43-e3b1-49b8-a4c4-e16f9ae9c12e.png)'
- en: Here A and B are two datasets.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这里A和B是两个数据集。
- en: F-Measure or F1-score
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F-度量或F1分数
- en: 'The F-measure is a harmonic mean of precision and recall values. It measures
    both the precision and robustness of clustering algorithms. It also tries to equalize
    the participation of false negatives using the value of β. This can be calculated
    as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: F-度量是精度和召回率值的调和平均数。它衡量了聚类算法的精度和鲁棒性。它还试图通过β的值平衡假阴性的参与。可以按以下方式计算：
- en: '![](img/d4fbdaf9-f90c-4b2d-a3ab-7d501bf65460.png)![](img/03632ba5-602f-432c-804f-a460bd853f5e.png)![](img/d4493844-d68e-46f5-a5fb-f1feee93d11d.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4fbdaf9-f90c-4b2d-a3ab-7d501bf65460.png)![](img/03632ba5-602f-432c-804f-a460bd853f5e.png)![](img/d4493844-d68e-46f5-a5fb-f1feee93d11d.png)'
- en: Here β is the non-negative value. β=1 gives equal weight to precision and recall,
    β = 0.5 gives twice the weight to precision than to recall, and β = 0 gives no
    importance to recall.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这里β是非负值。β=1表示精度和召回率赋予相等的权重，β=0.5表示精度的权重是召回率的两倍，而β=0则表示不考虑召回率。
- en: The Fowlkes-Mallows score
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Fowlkes-Mallows分数
- en: 'The Fowlkes-Mallows score is a geometric mean of precision and recall. A high
    value represents similar clusters. This can be calculated as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Fowlkes-Mallows分数是精度和召回率的几何平均数。较高的值表示簇之间相似。可以按以下方式计算：
- en: '![](img/49c6cf32-9e06-4e2f-910c-2059f933e711.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49c6cf32-9e06-4e2f-910c-2059f933e711.png)'
- en: 'Let''s create a cluster model using k-means clustering and evaluate the performance
    using the internal and external evaluation measures in Python using the Pima Indian
    Diabetes dataset [(https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter11/diabetes.csv](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter11/diabetes.csv)):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用k-means聚类创建一个聚类模型，并使用Python中的内部和外部评估指标评估性能，使用Pima Indian Diabetes数据集[(https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter11/diabetes.csv](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter11/diabetes.csv))：
- en: '[PRE14]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This results in the following output:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到以下输出：
- en: '![](img/d7489eec-d909-4472-bd07-a50b30a765ef.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7489eec-d909-4472-bd07-a50b30a765ef.png)'
- en: 'First, we need to import pandas and read the dataset. In the preceding example,
    we are reading the Pima Indian Diabetes dataset:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入pandas并读取数据集。在前面的例子中，我们读取的是Pima Indian Diabetes数据集：
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After loading the dataset, we need to divide the dataset into dependent/label
    columns (target) and independent/feature columns (`feature_set`). After this,
    the dataset will be broken into train and test sets. Now both dependent and independent
    columns are broken into train and test sets (`feature_train`, `feature_test`,
    `target_train`, and `target_test`) using `train_test_split()`. Let''s split the
    dataset into train and test parts:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载数据集后，我们需要将数据集分为依赖/标签列（目标）和独立/特征列（`feature_set`）。之后，数据集将被拆分为训练集和测试集。现在，依赖列和独立列会通过`train_test_split()`拆分成训练集和测试集（`feature_train`、`feature_test`、`target_train`
    和 `target_test`）。我们来拆分数据集为训练集和测试集：
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, `train_test_split()` takes the dependent and independent DataFrames,
    `test_size` and `random_state`. Here, `test_size` will decide the ratio for the
    train-test split (having a `test_size` value of `0.3` means 30% of the data will
    go to the testing set and the remaining 70% will be for the training set), and
    `random_state` is used as a seed value for reproducing the same data split each
    time. If `random_state` is `None`, then it will split the records in a random
    fashion each time, which will give different performance measures:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`train_test_split()`接收依赖和独立的DataFrame、`test_size`和`random_state`。其中，`test_size`决定了训练集和测试集的比例（`test_size`值为`0.3`意味着30%的数据将分配给测试集，其余70%将作为训练集），而`random_state`则作为种子值，用于每次生成相同的数据分割。如果`random_state`为`None`，那么每次会随机分割记录，这将导致不同的性能评估结果：
- en: '[PRE17]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This results in the following output:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE18]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: First, we imported the `KMeans` and `metrics` modules. We created a k-means
    object or model and fit it on the training dataset (without labels). After training,
    the model makes predictions and these predictions are assessed using internal
    measures, such as the DBI and the silhouette coefficient, and external evaluation
    measures, such as the Rand score, the Jaccard score, the F-Measure, and the Fowlkes-Mallows
    score.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入了`KMeans`和`metrics`模块。我们创建了一个k-means对象或模型，并将其拟合到训练数据集（没有标签）。训练完成后，模型进行预测，并使用内部评估指标（如DBI和轮廓系数）和外部评估指标（如Rand得分、Jaccard得分、F-Measure和Fowlkes-Mallows得分）来评估这些预测。
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have discovered unsupervised learning and its techniques,
    such as dimensionality reduction and clustering. The main focus was on PCA for
    dimensionality reduction and several clustering methods, such as k-means clustering,
    hierarchical clustering, DBSCAN, and spectral clustering. The chapter started
    with dimensionality reduction and PCA. After PCA, our main focus was on clustering
    techniques and how to identify the number of clusters. In later sections, we moved
    on to cluster performance evaluation measures such as the DBI and the silhouette
    coefficient, which are internal measures. After looking at internal clustering
    measures, we looked at external measures such as the Rand score, the Jaccard score,
    the F-measure, and the Fowlkes-Mallows index.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了无监督学习及其技术，如降维和聚类。主要内容包括PCA（主成分分析）降维技术以及几种聚类方法，如k-means聚类、层次聚类、DBSCAN和谱聚类。本章从降维和PCA开始，PCA之后，我们的主要焦点是聚类技术及如何确定聚类的数量。在后续章节中，我们介绍了聚类性能评估指标，如DBI和轮廓系数，这些是内部评估指标。接着，我们又探讨了外部评估指标，如Rand得分、Jaccard得分、F-Measure和Fowlkes-Mallows指数。
- en: The next chapter, [Chapter 12](e04e479d-3b11-4f6a-a2bb-946009c4a70a.xhtml),
    *Analyzing Textual Data*, will focus on text analytics, covering the text preprocessing
    and text classification using NLTK, SpaCy, and scikit-learn. The chapter starts
    by exploring basic operations on textual data such as text normalization using
    tokenization, stopwords removal, stemming and lemmatization, parts-of-speech tagging,
    entity recognition, dependency parsing, and word clouds. In later sections, the
    focus will be on feature engineering approaches such as Bag of Words, term presence,
    TF-IDF, sentiment analysis, text classification, and text similarity.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章，[第12章](e04e479d-3b11-4f6a-a2bb-946009c4a70a.xhtml)，*文本数据分析*，将专注于文本分析，包括使用NLTK、SpaCy和scikit-learn进行文本预处理和文本分类。本章首先探讨文本数据的基本操作，如通过分词、去除停用词、词干提取和词形还原、词性标注、实体识别、依存句法分析以及词云等进行文本标准化。在后续章节中，重点将放在特征工程方法上，如词袋模型、术语存在、TF-IDF、情感分析、文本分类和文本相似度。
