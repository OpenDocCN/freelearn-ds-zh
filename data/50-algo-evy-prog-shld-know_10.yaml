- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Neural Network Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络算法
- en: There is no algorithm for humor.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 幽默没有算法。
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Robert Mankoff
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —罗伯特·曼科夫
- en: Neural networks have been a topic of investigation for over seven decades, but
    their adoption was restricted due to constraints in computational capabilities
    and the dearth of digitized data. Today’s environment is significantly altered
    due to our growing need to solve complex challenges, the explosive growth in data
    production, and advancements such as cloud computing, which provide us with impressive
    computational abilities. These enhancements have opened up the potential for us
    to develop and apply these sophisticated algorithms to solve complex problems
    that were previously deemed impractical. In fact, this is the research area that
    is rapidly evolving and is responsible for most of the major advances claimed
    by leading-edge tech fields such as robotics, edge computing, natural language
    processing, and self-driving cars.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已经成为研究的课题超过七十年，但由于计算能力的限制和数字化数据的匮乏，它们的应用受到了制约。如今，由于我们日益增长的解决复杂挑战的需求、数据生产的爆炸性增长以及如云计算等技术的进步，环境发生了显著变化，赋予了我们强大的计算能力。这些改进为我们提供了开发和应用这些复杂算法的潜力，以解决曾经被认为不切实际的复杂问题。事实上，这是一个迅速发展的研究领域，是机器人技术、边缘计算、自然语言处理和自动驾驶汽车等前沿技术领域大多数重大进展的源泉。
- en: This chapter first introduces the main concepts and components of a typical
    neural network. Then, it presents the various types of neural networks and explains
    the different kinds of activation functions used in these neural networks. Then,
    the backpropagation algorithm is discussed in detail, which is the most widely
    used algorithm for training a neural network. Next, the transfer learning technique
    is explained, which can be used to greatly simplify and partially automate the
    training of models. Finally, how to use deep learning to flag fraudulent documents
    by way of a real-world example application.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先介绍典型神经网络的主要概念和组成部分。接下来，介绍神经网络的不同类型，并解释这些神经网络中使用的各种激活函数。然后，详细讨论反向传播算法，这是训练神经网络中最广泛使用的算法。接下来，解释转移学习技术，它可以极大简化并部分自动化模型的训练。最后，通过一个现实世界的应用示例，说明如何使用深度学习来标记欺诈性文档。
- en: 'The following are the main concepts discussed in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的主要概念如下：
- en: Understanding neural networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解神经网络
- en: The evolution of neural networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的演变
- en: Training a neural network
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: Tools and frameworks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具和框架
- en: Transfer learning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转移学习
- en: 'Case study: using deep learning for fraud detection'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究：使用深度学习进行欺诈检测
- en: Let’s start by looking at the basics of neural networks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从神经网络的基础开始。
- en: The evolution of neural networks
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的演变
- en: A neural network, at its most fundamental level, is composed of individual units
    known as neurons. These neurons serve as the cornerstone of the neural network,
    with each neuron performing its own specific task. The true power of a neural
    network unfolds when these individual neurons are organized into structured layers,
    facilitating complex processing. Each neural network is composed of an intricate
    web of these layers, connected to create an interconnected network.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在其最基本的层面上，由被称为神经元的独立单元组成。这些神经元是神经网络的基石，每个神经元执行各自特定的任务。当这些独立的神经元组织成结构化的层时，神经网络的真正力量得以展现，从而促进复杂的处理过程。每个神经网络都由这些层的错综复杂的网络组成，层与层之间通过连接形成互联网络。
- en: The information or signal is processed step by step as it travels through these
    layers. Each layer modifies the signal, contributing to the overall output. To
    explain, the initial layer receives the input signal, processes it, and then passes
    it to the next layer. This subsequent layer further processes the received signal
    and transfers it onward. This relay continues until the signal reaches the final
    layer, which generates the desired output.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 信息或信号在通过这些层时被一步一步地处理。每一层都会修改信号，最终影响整体输出。具体来说，初始层接收输入信号，对其进行处理后将其传递到下一层。随后的层进一步处理接收到的信号并继续传递。这一传递过程一直持续，直到信号到达最终层，生成所需的输出。
- en: It’s these hidden layers, or intermediate layers, that give neural networks
    their ability to perform deep learning. These layers create a hierarchy of abstract
    representations by transforming the raw input data progressively into a form that
    is more useful. This facilitates the extraction of higher-level features from
    the raw data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正是这些隐藏层或中间层赋予了神经网络进行深度学习的能力。这些层通过逐步将原始输入数据转换为更有用的形式，创建了抽象表示的层次结构。这有助于从原始数据中提取更高层次的特征。
- en: This deep learning capability has a vast array of practical applications, from
    enabling Amazon’s Alexa to understand voice commands to powering Google’s Images
    and organizing Google Photos.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这种深度学习能力具有广泛的实际应用，从使亚马逊的Alexa能够理解语音命令，到支持谷歌的图像和整理谷歌照片。
- en: Historical background
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 历史背景
- en: Inspired by the workings of neurons in the human brain, the concept of neural
    networks was proposed by Frank Rosenblatt in 1957\. To understand the architecture
    fully, it is helpful to briefly look at the layered structure of neurons in the
    human brain. (Refer to *Figure 8.1* to get an idea of how the neurons in the human
    brain are linked together.)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 受到人类大脑中神经元工作的启发，Frank Rosenblatt在1957年提出了神经网络的概念。要完全理解其结构，简要查看人类大脑神经元的分层结构是很有帮助的。（参考*图
    8.1*，了解人类大脑中神经元如何相互连接。）
- en: 'In the human brain, **dendrites** act as sensors that detect a signal. Dendrites
    are integral components of a neuron, serving as the primary sensory apparatus.
    They are responsible for detecting incoming signals. The signal is then passed
    on to an **axon**, which is a long, slender projection of a nerve cell. The function
    of the axon is to transmit this signal to muscles, glands, and other neurons.
    As shown in the following diagram, the signal travels through interconnecting
    tissue called a **synapse** before being passed on to other neurons. Note that
    through this organic pipeline, the signal keeps traveling until it reaches the
    target muscle or gland, where it causes the required action. It typically takes
    seven to eight milliseconds for the signal to pass through the chain of neurons
    and reach its destination:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在人脑中，**树突**充当传感器，检测信号。树突是神经元的组成部分，作为主要感觉器官。它们负责检测传入的信号。然后信号传递给**轴突**，这是神经细胞的一种长而细的突出部分。轴突的功能是将这个信号传输到肌肉、腺体和其他神经元。如下图所示，信号通过称为**突触**的相互连接组织传递，然后传递给其他神经元。请注意，通过这种有机管道，信号一直传播，直到达到目标肌肉或腺体，引起所需的动作。信号通常需要七到八毫秒才能通过神经元链传播并到达目标。
- en: '![Diagram  Description automatically generated](img/B18046_08_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B18046_08_01.png)'
- en: 'Figure 8.1: Neuron chained together in the human brain'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：人脑中连接在一起的神经元
- en: Inspired by this natural architectural masterpiece of signal processing, Frank
    Rosenblatt devised a technique that would mean digital information could be processed
    in layers to solve a complex mathematical problem. His initial attempt at designing
    a neural network was quite simple and looked like a linear regression model. This
    simple neural network did not have any hidden layers and was named a *perceptron*.
    This simple neural network without any layers, the perceptron, became the basic
    unit for neural networks. Essentially, a perceptron is the mathematical analog
    of a biological neuron and hence, serves as the fundamental building block for
    more complex neural networks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 受到这种自然信号处理建筑杰作的启发，Frank Rosenblatt设计了一种技术，使得可以按层处理数字信息以解决复杂的数学问题。他最初设计的神经网络尝试非常简单，看起来像一个线性回归模型。这种简单的神经网络没有任何隐藏层，被命名为*感知器*。这种没有任何层的简单神经网络，感知器，成为了神经网络的基本单元。实质上，感知器是生物神经元的数学模拟，因此是更复杂神经网络的基本构建块。
- en: Now, let us delve into a concise historical account of the evolutionary journey
    of **Artificial Intelligence** (**AI**).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解**人工智能**（**AI**）演化历史的简明史账。
- en: AI winter and the dawn of AI spring
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI寒冬与AI春天的曙光
- en: The initial enthusiasm toward the groundbreaking concept of the perceptron soon
    faded when its significant limitations were discovered. In 1969, Marvin Minsky
    and Seymour Papert conducted an in-depth study that led to the revelation that
    the perceptron was restricted in its learning capabilities. They found that a
    perceptron was incapable of learning and processing complex logical functions,
    even struggling with simple logic functions such as XOR.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对感知器这一突破性概念的最初热情在其重大局限性被发现后迅速消退。1969年，马文·明斯基和西摩·帕珀特进行了深入研究，揭示了感知器在学习能力上的局限性。他们发现，感知器无法学习和处理复杂的逻辑函数，甚至在处理像异或（XOR）这样的简单逻辑函数时也存在困难。
- en: This discovery triggered a significant decline in interest in **Machine Learning**
    (**ML**) and neural networks, commencing an era often referred to as the “AI winter.”
    This was a period when the global research community largely dismissed the potential
    of AI, viewing it as inadequate for tackling complex problems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现引发了对**机器学习**（**ML**）和神经网络兴趣的显著下降，开启了一个通常被称为“人工智能寒冬”的时代。这一时期，全球研究界普遍对人工智能的潜力表示怀疑，认为其不足以解决复杂问题。
- en: On reflection, the “AI winter” was in part a consequence of the restrictive
    hardware capabilities of the time. The hardware either lacked the necessary computing
    power or was prohibitively expensive, which severely hampered advancements in
    AI. This limitation stymied the progress and application of AI, leading to widespread
    disillusionment in its potential.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾起来，“人工智能寒冬”在某种程度上是当时硬件能力受限的结果。那时的硬件要么缺乏必要的计算能力，要么过于昂贵，这极大地阻碍了人工智能的进展。这一限制阻碍了人工智能的应用和发展，导致人们普遍对其潜力感到失望。
- en: Toward the end of the 1990s, there was a tidal shift regarding the image of
    AI and its perceived potential. The catalyst for this change was the advances
    in distributed computing, which provided easily available and affordable infrastructure.
    Seeing the potential, the newly crowned IT giants of that time (like Google) made
    AI the focus of their R&D efforts. The renewed interest in AI resulted in the
    thaw of the so-called AI winter. The thaw reinvigorated research in AI. This eventually
    resulted in turning the current era into an era that can be called the **AI spring**,
    where there is so much interest in AI and neural networks. Also, the digitized
    data was not available.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 到了1990年代末，关于人工智能及其潜力的看法发生了巨大变化。推动这一变化的催化剂是分布式计算的发展，它提供了易于获取和负担得起的基础设施。看到人工智能的潜力，当时新崛起的IT巨头（如谷歌）将人工智能作为其研发的重点。这种对人工智能的重新兴趣导致了所谓“人工智能寒冬”的解冻。这一解冻重新激发了对人工智能的研究，最终使得当前时代成为一个可以称之为**人工智能春天**的时代，大家对人工智能和神经网络充满兴趣。此外，数字化数据当时尚未普及。
- en: Understanding neural networks
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络
- en: First, let us start with the heart of the neural network, the perceptron. You
    can think of a single perceptron as the simplest possible neural network, and
    it forms the basic building block of modern complex multi-layered architectures.
    Let us start by understanding the working of a perceptron.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从神经网络的核心——感知器开始。你可以把一个单独的感知器看作是最简单的神经网络，它是现代复杂多层架构的基本构建模块。让我们从理解感知器的工作原理开始。
- en: Understanding perceptrons
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解感知器
- en: 'A single perceptron has several inputs and a single output that is controlled
    or activated by an activation function. This is shown in *Figure 8.2*:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个单一的感知器有多个输入和一个输出，该输出由激活函数控制或激活。如下图*图8.2*所示：
- en: '![Diagram  Description automatically generated](img/B18046_08_02.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18046_08_02.png)'
- en: 'Figure 8.2: A simple perceptron'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：一个简单的感知器
- en: The perceptron shown in *Figure 8.2* has three input features; *x*[1], *x*[2],
    and *x*[3]. We also add a constant signal called bias. The bias plays a critical
    role in our neural network model, as it allows for flexibility in fitting the
    data. It operates similarly to an intercept added in a linear equation—acting
    as a sort of “shift” of the activation function—thereby allowing us to fit the
    data better when our inputs are equal to zero. The input features and the bias
    get multiplied by weights and are summed up as a weighted sum ![](img/B18046_08_001.png)
    This weighted sum is passed on to the activation function, which generates the
    output y. The ability to use a wide variety of activation functions to formulate
    complex relationships between features and labels is one of the strengths of neural
    networks. A variety of activation functions is selectable through the hyperparameters.
    Some common examples include the sigmoid function, which squashes values between
    0 and 1, making it a good choice for binary classification problems; the tanh
    function, which scales values between -1 and 1, providing a zero-centered output;
    and the **Rectified Linear Unit** (**ReLU**) function, which sets all negative
    values in the vector to zero, effectively removing any negative influence, and
    is commonly used in convolutional neural networks. These activation functions
    are discussed in detail later in the chapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.2*中显示的感知机有三个输入特征；*x*[1]，*x*[2]，和*x*[3]。我们还加入了一个常数信号，称为偏置。偏置在我们的神经网络模型中起着关键作用，因为它允许在拟合数据时具有灵活性。它的作用类似于线性方程中添加的截距——作为激活函数的一种“偏移”——从而使我们在输入为零时能够更好地拟合数据。输入特征和偏置与权重相乘并求和，得到加权和
    ![](img/B18046_08_001.png)。这个加权和会传递给激活函数，产生输出y。能够使用多种激活函数来制定特征与标签之间的复杂关系是神经网络的一个优势。通过超参数可以选择多种激活函数。一些常见的例子包括
    sigmoid 函数，它将值压缩到 0 到 1 之间，是二分类问题的好选择；tanh 函数，它将值缩放到 -1 到 1 之间，提供零中心的输出；以及**修正线性单元**（**ReLU**）函数，它将向量中的所有负值设为零，有效地去除任何负面影响，并且在卷积神经网络中常常被使用。接下来，本章将详细讨论这些激活函数。'
- en: Let us now look into the intuition behind neural networks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来探讨一下神经网络背后的直觉。
- en: Understanding the intuition behind neural networks
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解神经网络背后的直觉
- en: In the last chapter, we discussed some traditional ML algorithms. These traditional
    ML algorithms work great for many important use cases. But they do have limitations
    as well. When the underlying patterns in the training dataset begin to become
    non-linear and multidimensional, it starts to go beyond the capabilities of traditional
    ML algorithms to accurately capture the complex relationships between features
    and labels. These incomprehensive, somewhat simplistic mathematical formulations
    of complex patterns result in suboptimal performance of the trained models for
    these use cases.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了一些传统的机器学习算法。这些传统算法在许多重要的应用场景中表现优异，但它们也有一定的局限性。当训练数据集中的潜在模式开始变得非线性和多维时，传统机器学习算法的能力已经无法准确捕捉特征与标签之间复杂的关系。这些不完备的、相对简化的数学公式化表示复杂模式，导致在这些用例中的训练模型表现不佳。
- en: In real-world scenarios, we often encounter situations where the relationships
    between our features and labels are not linear or straightforward but present
    complex patterns. This is where neural networks shine, offering us a powerful
    tool for modeling such intricacies.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的场景中，我们经常遇到特征与标签之间的关系不是线性或简单的，而是呈现出复杂的模式。这正是神经网络的优势所在，它为我们提供了一个强大的工具，用于建模这些复杂性。
- en: Neural networks are particularly effective when dealing with high-dimensional
    data or when the relationships between features and the outcome are non-linear.
    For instance, they excel in applications like image and speech recognition, where
    the input data (pixels or sound waves) has complex, hierarchical structures. Traditional
    ML algorithms might struggle in these instances, given the high degree of complexity
    and the non-linear relationships between features.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在处理高维数据或特征与结果之间的关系是非线性的情况下特别有效。例如，它们在图像和语音识别等应用中表现出色，其中输入数据（像素或声波）具有复杂的层级结构。传统的机器学习算法可能在这些情况下表现不佳，因为特征之间关系的高度复杂性和非线性。
- en: While neural networks are incredibly powerful tools, it’s crucial to acknowledge
    that they aren’t without their limitations. These restrictions, explored in detail
    later in this chapter, are critical to grasp for the practical and effective use
    of neural networks in tackling real-world dilemmas.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然神经网络是非常强大的工具，但我们必须承认它们并非没有局限性。这些限制将在本章后面详细探讨，对于神经网络在解决现实问题中的有效应用，了解这些限制至关重要。
- en: Now, let’s illustrate some common patterns and their associated challenges when
    simpler ML algorithms like linear regression are employed. Picture this – we’re
    trying to predict a data scientist’s salary based on the “years spent in education.”
    We have collected two different datasets from two separate organizations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们举例说明使用更简单的机器学习算法，如线性回归时常见的模式及其相关挑战。假设我们正在尝试根据“受教育年限”预测数据科学家的工资。我们从两个不同的组织收集了两个数据集。
- en: 'First, let’s introduce you to Dataset 1, illustrated in *Figure 8.3(a)*. It
    depicts a relatively straightforward relationship between the feature (years spent
    in education) and the label (salary), which appears to be linear. However, even
    this simple pattern throws a couple of challenges when we attempt to mathematically
    model it using a linear algorithm:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们介绍数据集 1，如*图 8.3(a)*所示。它描述了特征（受教育年限）与标签（工资）之间的相对简单的关系，看起来是线性的。然而，即使是这个简单的模式，在我们尝试用线性算法进行数学建模时，也会遇到一些挑战：
- en: We know that a salary cannot be negative, meaning that regardless of the years
    spent in education, the salary (`y`) should never be less than zero.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们知道，工资不能为负数，这意味着无论受教育年限如何，工资（`y`）都不应小于零。
- en: There’s at least one junior data scientist who may have just graduated, thus
    spending “`x`[1]” years in education, but currently earns zero salary, perhaps
    as an intern. Hence, for the “`x`" values ranging from zero to “`x`[1],” the salary
    “`y`" remains zero, as depicted in *Figure 8.3(a)*.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少有一位刚毕业的初级数据科学家，可能只用了“`x`[1]”年的教育时间，但目前工资为零，可能是实习生。因此，在“`x`”的取值范围从零到“`x`[1]”时，工资“`y`”保持为零，如*图
    8.3(a)*所示。
- en: Interestingly, we can capture such intricate relationships between the feature
    and label using the Rectified Linear activation function available in neural networks,
    a concept we will explore later.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们可以利用神经网络中可用的修正线性激活函数来捕捉特征与标签之间这种复杂的关系，这是我们后面将探讨的一个概念。
- en: 'Next, we have Dataset 2, showcased in *Figure 8.3(b)*. This dataset represents
    a non-linear relationship between the feature and the label. Here’s how it works:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看数据集 2，如*图 8.3(b)*所示。这个数据集表示特征与标签之间的非线性关系。其工作原理如下：
- en: The salary “`y`" remains at zero while “`x`" (years spent in education) varies
    from zero to “`x`[1].”
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当“`x`” （受教育年限）从零变化到“`x`[1]”时，工资“`y`”保持为零。
- en: The salary increases sharply as “`x`" nears “`x`[2].”
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当“`x`”接近“`x`[2]”时，工资急剧增加。
- en: But once “`y`" exceeds “`x`[2],” the salary plateaus and flattens out.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但一旦“`y`”超过“`x`[2]”，工资将达到平稳状态并趋于平坦。
- en: 'As we will see later in this book, we can model such relationships using the
    sigmoid activation function within a neural network framework. Understanding these
    patterns and knowing which tools to apply is essential to effectively leverage
    the power of neural networks:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本书后面看到的，我们可以在神经网络框架内使用 Sigmoid 激活函数来建模此类关系。理解这些模式并知道何时应用合适的工具，是有效利用神经网络强大功能的关键：
- en: '![Chart, scatter chart  Description automatically generated](img/B18046_08_03.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, scatter chart  Description automatically generated](img/B18046_08_03.png)'
- en: 'Figure 8.3: Salary and years of education'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：工资与受教育年限
- en: '(a) Dataset 1: Linear patterns (b) Dataset 2: Non-linear patterns'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 数据集 1：线性模式 (b) 数据集 2：非线性模式
- en: Understanding layered deep learning architectures
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解分层深度学习架构
- en: 'For more complex problems, researchers have developed a multilayer neural network
    called a **multilayer perceptron**. A multilayer neural network has a few different
    layers, as shown in the following diagram. These layers are as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的问题，研究人员开发了一种多层神经网络，称为**多层感知器**。一个多层神经网络有几个不同的层，如下图所示。这些层如下：
- en: '**Input layer**: The first layer is the input layer. At the input layer, the
    feature values are fed as input to the network.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：第一层是输入层。在输入层，特征值作为输入被馈送到网络中。'
- en: '**Hidden layer(s)**: The input layer is followed by one or more hidden layers.
    Each hidden layers are the arrays of similar activation functions.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：输入层后面跟随一个或多个隐藏层。每个隐藏层都是类似激活函数的数组。'
- en: '**Output layer**: The final layer is called the output layer.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：最后一层称为输出层。'
- en: A simple neural network will have one hidden layer. A deep neural network is
    a neural network with two or more hidden layers. See *Figure 8.4*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的神经网络会有一个隐藏层。一个深度神经网络是一个有两个或更多隐藏层的神经网络。见*图 8.4*。
- en: '![Chart, scatter chart  Description automatically generated](img/B18046_08_04.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 说明自动生成](img/B18046_08_04.png)'
- en: 'Figure 8.4: Simple neural network and deep neural network'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：简单神经网络与深度神经网络
- en: Next, let us try to understand the function of hidden layers.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们尝试理解隐藏层的功能。
- en: Developing an intuition for hidden layers
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 培养对隐藏层的直觉
- en: In a neural network, hidden layers play a key role in interpreting the input
    data. Hidden layers are methodically organized in a hierarchical structure within
    the neural network, where each layer performs a distinct non-linear transformation
    on its input data. This design allows for the extraction of progressively more
    abstract and nuanced features from the input.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，隐藏层在解释输入数据中起着关键作用。隐藏层在神经网络中以层级结构有序组织，每一层对其输入数据执行独特的非线性转换。这种设计允许从输入中提取逐渐更抽象、更细致的特征。
- en: Consider the example of convolutional neural networks, a subtype of neural networks
    specifically engineered for image-processing tasks. In this context, the lower
    hidden layers focus on discerning simple, local features such as edges and corners
    within an image. These features, while fundamental, don’t carry much meaning on
    their own.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以卷积神经网络为例，卷积神经网络是专为图像处理任务设计的神经网络子类型。在这个背景下，较低的隐藏层专注于辨别图像中的简单局部特征，如边缘和角落。这些特征虽然是基础的，但单独来看并没有太大意义。
- en: As we move deeper into the hidden layers, these layers start to connect the
    dots, so to speak. They integrate the basic patterns detected by the lower layers,
    assembling them into more complex, meaningful structures. As a result, an originally
    incoherent scatter of edges and corners transforms into recognizable shapes and
    patterns, granting the network a level of “vision.”
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们深入到隐藏层，这些层开始连接起各个点。可以说，它们将较低层检测到的基本模式整合成更复杂、更有意义的结构。结果，本来杂乱无章的边缘和角落，转变为可识别的形状和模式，从而赋予网络一定的“视觉”。
- en: This progressive transformation process turns unprocessed pixel values into
    an elaborate mapping of features and patterns, enabling advanced applications
    such as fingerprint recognition. Here, the network can pick out the unique arrangement
    of ridges and valleys in a fingerprint, converting this raw visual data into a
    unique identifier. Hence, hidden layers convert raw data and refined it into valuable
    insights.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个逐步转换的过程将未经处理的像素值转化为精细的特征和模式映射，从而实现诸如指纹识别等高级应用。在这里，网络能够识别指纹中脊线和谷线的独特排列，将这些原始的视觉数据转换为独特的身份标识。因此，隐藏层将原始数据转换并提炼成有价值的洞察。
- en: How many hidden layers should be used?
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应该使用多少个隐藏层？
- en: Note that the optimal number of hidden layers will vary from problem to problem.
    For some problems, single-layer neural networks should be used. These problems
    typically exhibit straightforward patterns that can be easily captured and formulated
    by a minimalist network design. For others, we should add multiple layers for
    the best performance. For example, if you’re dealing with a complex problem, such
    as image recognition or natural language processing, a neural network with multiple
    hidden layers and a greater number of nodes in each layer might be necessary.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，隐藏层的最佳数量会根据问题的不同而有所变化。对于某些问题，应该使用单层神经网络。这些问题通常表现出简单的模式，可以通过简洁的网络设计轻松捕捉和表达。对于其他问题，我们应增加多个层以获得最佳性能。例如，如果你正在处理一个复杂的问题，如图像识别或自然语言处理，可能需要一个具有多个隐藏层和每层更多节点的神经网络。
- en: The complexity of your data’s underlying patterns will largely influence your
    network design. For instance, using an excessively complex neural network for
    a simple problem might lead to overfitting, where your model becomes too tailored
    to the training data and performs poorly on new, unseen data. On the other hand,
    a model that’s too simple for a complex problem might result in underfitting,
    where the model fails to capture essential patterns in the data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的潜在模式的复杂性将很大程度上影响你的网络设计。例如，对于简单问题使用过于复杂的神经网络可能导致过拟合，使得你的模型过度拟合训练数据，并且在新的、未见过的数据上表现不佳。另一方面，过于简单的模型可能会导致欠拟合，即模型未能捕捉数据中的关键模式。
- en: Additionally, the choice of activation function plays a critical role. For example,
    if your output needs to be binary (like in a yes/no problem), a sigmoid function
    could be suitable. For multi-class classification problems, a softmax function
    might be better.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，激活函数的选择也起着关键作用。例如，如果你的输出需要是二元的（如是/否问题），则可以使用 sigmoid 函数。对于多分类问题，softmax 函数可能更为合适。
- en: Ultimately, the process of selecting your neural network’s architecture requires
    careful analysis of your problem, coupled with experimentation and fine-tuning.
    This is where developing a baseline experimental model can be beneficial, allowing
    you to iteratively adjust and enhance your network’s design for optimal performance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，选择神经网络架构的过程需要仔细分析你的问题，并进行实验和微调。在这个过程中，开发一个基准实验模型可能会很有帮助，这样你可以通过迭代调整和优化网络设计，以达到最佳性能。
- en: Let us next look into the mathematical basis of a neural network.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看看神经网络的数学基础。
- en: Mathematical basis of neural network
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络的数学基础
- en: 'Understanding the mathematical foundation of neural networks is key to leveraging
    their power. While they may seem complex, the principles are based on familiar
    mathematical concepts such as linear algebra, calculus, and probability. The beauty
    of neural networks lies in their ability to learn from data and improve over time,
    attributes that are rooted in their mathematical structure:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 理解神经网络的数学基础是发挥其能力的关键。虽然它们看起来复杂，但其原理基于熟悉的数学概念，如线性代数、微积分和概率论。神经网络的魅力在于其从数据中学习并随着时间推移不断改进的能力，这些特性源于它们的数学结构：
- en: '![Diagram  Description automatically generated](img/B18046_08_05.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图示描述](img/B18046_08_05.png)'
- en: 'Figure 8.5: A multi-layer perceptron'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5：多层感知机
- en: '*Figure 8.5* shows a 4-layer neural network. In this neural network, an important
    thing to note is that the neuron is the basic unit of this network, and each neuron
    of a layer is connected to all neurons of the next layer. For complex networks,
    the number of these interconnections explodes, and we will explore different ways
    of reducing these interconnections without sacrificing too much quality.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.5* 显示了一个四层神经网络。在这个神经网络中，一个重要的要点是，神经元是该网络的基本单元，并且每一层的神经元都与下一层的所有神经元相连接。对于复杂的网络，这些连接的数量会急剧增加，我们将探索在不牺牲太多质量的情况下减少这些连接的不同方法。'
- en: First, let’s try to formulate the problem we are trying to solve.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们尝试表述我们要解决的问题。
- en: The input is a feature vector, *x*, of dimensions *n*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是一个特征向量 *x*，其维度为 *n*。
- en: We want the neural network to predict values. The predicted values are represented
    by *ý*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望神经网络能够预测值。预测值用 *ý* 表示。
- en: 'Mathematically, we want to determine, given a particular input, the probability
    that a transaction is fraudulent. In other words, given a particular value of
    *x*, what is the probability that *y* = 1? Mathematically, we can represent this
    as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度看，我们想要确定，在给定特定输入的情况下，交易是欺诈的概率。换句话说，给定 *x* 的特定值，*y* = 1 的概率是多少？从数学角度看，我们可以表示为：
- en: '![](img/B18046_08_002.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_08_002.png)'
- en: Note that *x* is an *n*[x]-dimensional vector, where *n*[x] is the number of
    input variables.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*x* 是一个 *n*[x] 维的向量，其中 *n*[x] 是输入变量的数量。
- en: The neural network shown in *Figure 8.6* has four layers. The layers between
    the input and the output are the hidden layers. The number of neurons in the first
    hidden layer is denoted by ![](img/B18046_08_003.png). The links between various
    nodes are multiplied by parameters called *weights*. The process of training a
    neural network is fundamentally centered around determining the optimal values
    for the weights associated with the various connections between the network’s
    neurons. By adjusting these weights, the network can fine-tune its calculations
    and improve its performance over time.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图8.6*所示，神经网络有四层。输入层和输出层之间的层称为隐藏层。第一层隐藏层中的神经元数量用![](img/B18046_08_003.png)表示。各个节点之间的连接由被称为*权重*的参数乘以。训练神经网络的过程本质上是围绕着确定与网络中各个神经元连接相关的权重的最优值。通过调整这些权重，网络可以调整其计算并随着时间的推移提高性能。
- en: Let’s see how we can train a neural network.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何训练一个神经网络。
- en: Training a neural network
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: The process of building a neural network using a given dataset is called training
    a neural network. Let’s look into the anatomy of a typical neural network. When
    we talk about training a neural network, we are talking about calculating the
    best values for the weights. The training is done iteratively by using a set of
    examples in the form of training data. The examples in the training data have
    the expected values of the output for different combinations of input values.
    The training process for neural networks is different from the way traditional
    models are trained (which was discussed in *Chapter 7*, *Traditional Supervised
    Learning Algorithms*).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用给定数据集构建神经网络的过程称为训练神经网络。让我们深入了解典型神经网络的结构。当我们谈论训练神经网络时，我们是在谈论为权重计算最佳值。训练是通过使用一组以训练数据形式呈现的示例进行的。训练数据中的示例为不同输入值组合的输出提供了预期值。神经网络的训练过程与传统模型的训练方式不同（这在*第7章*，*传统监督学习算法*中有讨论）。
- en: Understanding the anatomy of a neural network
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络的结构
- en: 'Let’s see what a neural network consists of:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看神经网络由哪些部分组成：
- en: '**Layers**: Layers are the core building blocks of a neural network. Each layer
    is a data-processing module that acts as a filter. It takes one or more inputs,
    processes them in a certain way, and then produces one or more outputs. Every
    time data passes through a layer, it goes through a processing phase and shows
    patterns that are relevant to the business question we are trying to answer.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层**：层是神经网络的核心构建块。每一层是一个数据处理模块，充当过滤器。它接收一个或多个输入，以某种方式处理这些输入，然后生成一个或多个输出。每次数据通过一层时，它都会经历一个处理阶段，并展示与我们试图回答的业务问题相关的模式。'
- en: '**Loss function**: The loss function provides the feedback signal that is used
    in the various iterations of the learning process. The loss function provides
    the deviation for a single example.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：损失函数提供了在学习过程的各个迭代中使用的反馈信号。损失函数为单个示例提供偏差。'
- en: '**Cost function**: The cost function is the loss function on a complete set
    of examples.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本函数**：成本函数是完整示例集上的损失函数。'
- en: '**Optimizer**: An optimizer determines how the feedback signal provided by
    the loss function will be interpreted.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**：优化器决定如何解释损失函数提供的反馈信号。'
- en: '**Input data**: Input data is the data that is used to train the neural network.
    It specifies the target variable.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入数据**：输入数据是用于训练神经网络的数据。它指定了目标变量。'
- en: '**Weights**: The weights are calculated by training the network. Weights roughly
    correspond to the importance of each of the inputs. For example, if a particular
    input is more important than other inputs, after training, it is given a greater
    weight value, acting as a multiplier. Even a weak signal for that important input
    will gather strength from the large weight value (which acts as a multiplier).
    Thus weight ends up turning each of the inputs according to their importance.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**：权重通过训练网络来计算。权重大致对应于每个输入的重要性。例如，如果某个输入比其他输入更重要，那么在训练后，它将被赋予更大的权重值，作为乘数。即使这个重要输入的信号较弱，它也会因为较大的权重值（作为乘数的作用）而增强。因此，权重最终根据输入的重要性调整每个输入的影响。'
- en: '**Activation function**: The values are multiplied by different weights and
    then aggregated. Exactly how they will be aggregated and how their value will
    be interpreted will be determined by the type of the chosen activation function.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：这些值会被不同的权重乘以，然后汇总。它们如何被汇总以及如何解读其值，将由所选择的激活函数的类型决定。'
- en: Let’s now have a look at a very important aspect of neural network training.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看神经网络训练中一个非常重要的方面。
- en: While training neural networks, we take each of the examples one by one. For
    each of the examples, we generate the output using our under-training model. The
    term “under-training” refers to the model’s learning state, where it is still
    adjusting and learning from data and has not reached its optimal performance yet.
    During this stage, the model parameters, such as weights, are constantly updated
    and adjusted to improve its predictive performance. We calculate the difference
    between the expected output and the predicted output. For each individual example,
    this difference is called the **loss**. Collectively, the loss across the complete
    training dataset is called the **cost**. As we keep on training the model, we
    aim to find the right values of weights that will result in the smallest loss
    value. Throughout the training, we keep on adjusting the values of the weights
    until we find the set of values for the weights that results in the minimum possible
    overall cost. Once we reach the minimum cost, we mark the model as trained.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，我们会逐一处理每一个样本。对于每一个样本，我们使用正在训练的模型生成输出。术语“正在训练”指的是模型的学习状态，此时模型仍在调整并从数据中学习，尚未达到最佳性能。在这个阶段，模型参数，如权重，持续更新和调整，以提高其预测性能。我们计算期望输出与预测输出之间的差异。对于每个单独的样本，这个差异被称为**损失**。所有样本的损失加起来，就是**代价**。随着训练的进行，我们的目标是找到合适的权重值，以使得损失值最小化。在整个训练过程中，我们会不断调整权重值，直到找到一组能使总体代价最小的权重值。一旦我们达到最小代价，就标志着模型已经训练完成。
- en: Defining gradient descent
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义梯度下降
- en: The central goal of training a neural network is to identify the correct values
    for the weights, which act like “dials” or “knobs” that we adjust to minimize
    the difference between the model’s predictions and the actual values.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的核心目标是确定权重的正确值，这些权重像“旋钮”或“调节器”一样，通过调整它们来最小化模型预测与实际值之间的差异。
- en: When training begins, we initiate these weights with random or default values.
    We then progressively adjust them using an optimization algorithm, a popular choice
    being “gradient descent,” to incrementally improve our model’s predictions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练开始时，我们使用随机或默认值初始化这些权重。然后，我们使用优化算法逐步调整它们，常用的优化算法是“梯度下降”，以逐步改进模型的预测结果。
- en: Let’s dive deeper into the gradient descent algorithm. The journey of gradient
    descent starts from the initial random values of weights that we set.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解梯度下降算法。梯度下降的旅程从我们设置的初始随机权重值开始。
- en: From this starting point, we iterate and, at each step, we adjust these weights
    to move us closer to the minimum cost.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个起点开始，我们迭代并在每一步调整这些权重，使得我们更接近最小代价。
- en: To paint a clearer picture, imagine our data features as the input vector **X**.
    The true value of the target variable is **Y**, while the value our model predicts
    is **Y**. We measure the difference, or deviation, between these actual and predicted
    values. This difference gives us our loss.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地说明这一点，假设我们的数据特征是输入向量**X**。目标变量的真实值是**Y**，而我们模型预测的值是**Y**。我们衡量实际值与预测值之间的差异或偏差，这个差异就是我们的损失。
- en: 'We then update our weights, taking into account two key factors: the direction
    to move and the size of the step, also known as the learning rate.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们更新权重，考虑到两个关键因素：移动的方向和步伐的大小，也就是学习率。
- en: The “direction” informs us where to move to find the minimum of the loss function.
    Think of this as descending a hill – we want to go “downhill” where the slope
    is steepest to get to the bottom (our minimum loss) the fastest.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: “方向”告诉我们该朝哪个方向移动，以找到损失函数的最小值。可以把它想象成下坡——我们希望沿着坡度最陡的地方“下坡”，这样可以最快到达底部（即最小损失）。
- en: The “learning rate” determines the size of our step in that chosen direction.
    It’s like deciding whether to walk or run down that hill – a larger learning rate
    means bigger steps (like running), and a smaller one means smaller steps (like
    walking).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: “学习率”决定了我们在选择的方向上的步长大小。就像决定是走下山坡还是跑下去——较大的学习率意味着更大的步伐（像奔跑），而较小的学习率意味着较小的步伐（像走路）。
- en: The goal of this iterative process is to reach a point from which we can’t go
    “downhill”, meaning we have found the minimum cost, indicating our weights are
    now optimal, and our model is well trained.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个迭代过程的目标是达到一个我们无法继续“下坡”的点，意味着我们已经找到了最小的成本，表示我们的权重已经最优，模型也已经很好地训练完成。
- en: 'This simple iterative process is shown in the following diagram:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的迭代过程在下图中展示：
- en: '![A picture containing text, athletic game, sport  Description automatically
    generated](img/B18046_08_06.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![一张图片，包含文字，运动比赛，运动描述自动生成](img/B18046_08_06.png)'
- en: 'Figure 8.6: Gradient Descent Algorithm, finding the minimum'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6：梯度下降算法，寻找最小值
- en: The diagram shows how, by varying the weights, gradient descent tries to find
    the minimum cost. The learning rate and chosen direction will determine the next
    point on the graph to explore.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图示展示了通过调整权重，梯度下降如何尝试找到最小的成本。学习率和选择的方向将决定图表中下一个要探索的点。
- en: Selecting the right value for the learning rate is important. If the learning
    rate is too small, the problem may take a lot of time to converge. If the learning
    rate is too high, the problem will not converge. In the preceding diagram, the
    dot representing our current solution will keep oscillating between the two opposite
    lines of the graph.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的学习率非常重要。如果学习率太小，问题可能需要很长时间才能收敛。如果学习率过高，问题将无法收敛。在前面的图示中，表示当前解的点会在图表的两条对立线之间不断摆动。
- en: 'Now, let’s see how to minimize a gradient. Consider only two variables, *x*
    and *y*. The gradient of *x* and *y* is calculated as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看如何最小化梯度。仅考虑两个变量，*x* 和 *y*。*x* 和 *y* 的梯度计算如下：
- en: '![](img/B18046_08_004.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_08_004.png)'
- en: 'To minimize the gradient, the following approach can be used:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化梯度，可以使用以下方法：
- en: '[PRE0]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This algorithm can also be used to find the optimal or near-optimal values of
    weights for a neural network.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法还可以用于寻找神经网络权重的最优或近似最优值。
- en: Note that the calculation of gradient descent proceeds backward throughout the
    network. We start by calculating the gradient of the final layer first, and then
    the second-to-last one, and then the one before that, until we reach the first
    layer. This is called backpropagation, which was introduced by Hinton, Williams,
    and Rumelhart in 1985.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，梯度下降的计算是从网络的后端开始进行的。我们首先计算最终层的梯度，然后是倒数第二层的梯度，再然后是之前的层，一直到达第一层。这就是所谓的反向传播，它是由Hinton、Williams和Rumelhart于1985年提出的。
- en: Next, let’s look into activation functions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入探讨激活函数。
- en: Activation functions
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: An activation function formulates how the inputs to a particular neuron will
    be processed to generate an output.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数制定了如何处理特定神经元的输入以生成输出的方式。
- en: 'As shown in *Figure 8.7*, each of the neurons in a neural network has an activation
    function that determines how inputs will be processed:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 8.7*所示，神经网络中的每个神经元都有一个激活函数，决定了如何处理输入数据：
- en: '![Diagram  Description automatically generated](img/B18046_08_07.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图示，描述自动生成](img/B18046_08_07.png)'
- en: 'Figure 8.7: Activation function'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7：激活函数
- en: In the preceding diagram, we can see that the results generated by an activation
    function are passed on to the output. The activation function sets the criteria
    that how the values of the inputs are supposed to be interpreted to generate an
    output.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们可以看到由激活函数生成的结果被传递到输出端。激活函数设定了如何解释输入值以生成输出的标准。
- en: For exactly the same input values, different activation functions will produce
    different outputs. Understanding how to select the right activation function is
    important when using neural networks to solve problems.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完全相同的输入值，不同的激活函数将产生不同的输出。理解如何选择正确的激活函数在使用神经网络解决问题时非常重要。
- en: Let’s now look into these activation functions one by one.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐一看看这些激活函数。
- en: Step function
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤函数
- en: 'The simplest possible activation function is the threshold function. The output
    of the threshold function is binary: 0 or 1\. It will generate 1 as the output
    if any of the inputs are greater than 1\. This can be explained in *Figure 8.8*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的激活函数是阈值函数。阈值函数的输出是二值的：0 或 1。如果任何输入大于 1，它将生成 1 作为输出。这可以通过 *图 8.8* 来解释：
- en: '![Diagram  Description automatically generated with medium confidence](img/B18046_08_08.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated with medium confidence](img/B18046_08_08.png)'
- en: 'Figure 8.8: Step function'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8：阶跃函数
- en: Despite its simplicity, the threshold activation function plays an important
    role, especially when we need a clear demarcation between the outputs. With this
    function, as soon as there’s any non-zero value in the weighted sums of inputs,
    the output (*y*) turns to 1\. However, its simplicity has its drawbacks – the
    function is exceedingly sensitive and could be erroneously triggered by the slightest
    signal or noise in the input.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其简单性，阈值激活函数在我们需要输出之间清晰划分时起着重要作用。使用此函数，只要输入的加权和中有任何非零值，输出 (*y*) 就会变为 1。然而，它的简单性也带来了缺点——该函数极其敏感，可能会因为输入中的微弱信号或噪声而被错误触发。
- en: For instance, consider a situation where a neural network uses this function
    to classify emails into “spam” or “not spam.” Here, an output of 1 might represent
    “spam” and 0 might represent “not spam.” The slightest presence of a characteristic
    (like certain key spam words) could trigger the function to classify the email
    as “spam.” Hence, while it’s a valuable tool for certain use cases, its potential
    for over-sensitivity should be considered, especially in applications where noise
    or minor variances in input data are common. Next, let us look into the sigmoid
    function.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一种情况，其中神经网络使用此函数将电子邮件分类为“垃圾邮件”或“非垃圾邮件”。在这里，输出 1 可能表示“垃圾邮件”，而 0 可能表示“非垃圾邮件”。某个特征（如某些关键垃圾邮件词汇）的最轻微出现可能会触发该函数将电子邮件分类为“垃圾邮件”。因此，尽管它在某些应用中是一个有价值的工具，但在输入数据中噪声或轻微变化常见的情况下，应该考虑其过度敏感性的潜力。接下来，让我们深入了解
    Sigmoid 函数。
- en: Sigmoid function
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: 'The sigmoid function can be thought of as an improvement of the threshold function.
    Here, we have control over the sensitivity of the activation function:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数可以视为阈值函数的一种改进。在这里，我们可以控制激活函数的敏感性：
- en: '![Diagram  Description automatically generated with medium confidence](img/B18046_08_09.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated with medium confidence](img/B18046_08_09.png)'
- en: 'Figure 8.9: Sigmoid activation function'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9：Sigmoid 激活函数
- en: 'The sigmoid function, *y*, is defined as follows and shown in *Figure 8.9*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数 *y* 定义如下，并在 *图 8.9* 中显示：
- en: '![](img/B18046_08_005.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_08_005.png)'
- en: 'It can be implemented in Python as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下方式在 Python 中实现：
- en: '[PRE1]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code above demonstrates the sigmoid function using Python. Here, `np.exp(-z)`
    is the exponential operation applied to `-z`, and this term is added to 1 to form
    the denominator of the equation, resulting in a value between 0 and 1.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码使用 Python 演示了 Sigmoid 函数。这里，`np.exp(-z)` 是对 `-z` 应用的指数运算，结果加上 1 构成方程的分母，从而得到一个介于
    0 和 1 之间的值。
- en: The reduction in the activation function’s sensitivity through the sigmoid function
    makes it less susceptible to sudden aberrations or “glitches” in the input. However,
    it’s worth noting that the output remains binary, meaning it can still only be
    0 or 1.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Sigmoid 函数降低激活函数的敏感性，使其不易受到输入中的突变或“故障”影响。然而，值得注意的是，输出仍然是二值的，意味着它只能是 0 或 1。
- en: Sigmoid functions are widely used in binary classification problems where the
    output is expected to be either 0 or 1\. For instance, if you are developing a
    model to predict whether an email is spam (1) or not spam (0), a sigmoid activation
    function would be a suitable choice.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数广泛应用于二分类问题，其中输出预期为 0 或 1。例如，如果你正在开发一个模型来预测电子邮件是否是垃圾邮件（1）或非垃圾邮件（0），则
    Sigmoid 激活函数将是一个合适的选择。
- en: Now, let’s delve into the **ReLU** activation function.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解 **ReLU** 激活函数。
- en: ReLU
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReLU
- en: The output for the first two activation functions presented in this chapter
    was binary. That means that they will take a set of input variables and convert
    them into binary outputs. ReLU is an activation function that takes a set of input
    variables as input and converts them into a single continuous output. In neural
    networks, ReLU is the most popular activation function and is usually used in
    the hidden layers, where we do not want to convert continuous variables into category
    variables.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的前两个激活函数的输出是二进制的。这意味着它们会将一组输入变量转换为二进制输出。ReLU是一种激活函数，它将一组输入变量作为输入，并将其转换为单一的连续输出。在神经网络中，ReLU是最流行的激活函数，通常用于隐藏层，我们不希望将连续变量转换为类别变量。
- en: 'The following diagram summarizes the ReLU activation function:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表总结了ReLU激活函数：
- en: '![Chart, line chart  Description automatically generated](img/B18046_08_10.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 描述自动生成](img/B18046_08_10.png)'
- en: 'Figure 8.10: ReLU'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10：ReLU
- en: 'Note that when *x* ≤ 0, that means *y* = 0\. This means that any signal from
    the input that is zero or less than zero is translated into a zero output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当*x* ≤ 0时，这意味着*y* = 0。这意味着输入中任何为零或小于零的信号都会被转换为零输出：
- en: '![](img/B18046_08_006.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_08_006.png)'
- en: '![](img/B18046_08_007.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_08_007.png)'
- en: As soon as *x* becomes more than zero, it is *x*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦*x*变为大于零，它就是*x*。
- en: 'The ReLU function is one of the most used activation functions in neural networks.
    It can be implemented in Python as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU函数是神经网络中最常用的激活函数之一。它可以在Python中按如下方式实现：
- en: '[PRE2]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now let’s look into Leaky ReLU, which is based on ReLU.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看基于ReLU的Leaky ReLU。
- en: Leaky ReLU
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: 'In ReLU, a negative value for *x* results in a zero value for *y*. This means
    that some information is lost in the process, which makes training cycles longer,
    especially at the start of training. The Leaky ReLU activation function resolves
    this issue. The following applies to Leaky ReLu:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在ReLU中，*x*的负值会导致*y*的值为零。这意味着在过程中丢失了一些信息，这使得训练周期特别在训练初期变得更长。Leaky ReLU激活函数解决了这个问题。以下内容适用于Leaky
    ReLU：
- en: '![](img/B18046_08_008.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_08_008.png)'
- en: '![](img/B18046_08_007.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_08_007.png)'
- en: 'This is shown in the following diagram:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示：
- en: '![Diagram  Description automatically generated](img/B18046_08_11.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18046_08_11.png)'
- en: 'Figure 8.11: Leaky ReLU'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11：Leaky ReLU
- en: Here, ![](img/B18046_07_033.png) is a parameter with a value less than one.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B18046_07_033.png)是一个值小于1的参数。
- en: 'It can be implemented in Python as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以在Python中按如下方式实现：
- en: '[PRE3]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'There are various strategies for assigning a value to ![](img/B18046_08_011.png):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为![](img/B18046_08_011.png)赋值有多种策略：
- en: '**Default value**: We can assign a default value to ![](img/B18046_08_011.png),
    typically `0.01`. This is the most straightforward approach and can be useful
    in scenarios where we want a quick implementation without any intricate tuning.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**默认值**：我们可以为![](img/B18046_08_011.png)指定一个默认值，通常为`0.01`。这是最直接的方法，在我们希望快速实现而不进行复杂调优的情况下非常有用。'
- en: '**Parametric ReLU**: Another approach is to allow ![](img/B18046_08_011.png)
    to be a tunable parameter in our neural network model. In this case, the optimal
    value for ![](img/B18046_08_011.png) is learned during the training process itself.
    This is beneficial in scenarios where we aim to tailor our activation function
    to the specific patterns present in our data.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数化ReLU**：另一种方法是允许![](img/B18046_08_011.png)在我们的神经网络模型中成为一个可调参数。在这种情况下，![](img/B18046_08_011.png)的最佳值是在训练过程中学习到的。这在我们希望将激活函数调整为数据中存在的特定模式时非常有用。'
- en: '**Randomized ReLU**: We could also choose to randomly assign a value to ![](img/B18046_08_015.png).
    This technique, known as randomized ReLU, can act as a form of regularization
    and help prevent overfitting by introducing some randomness into the model. This
    could be helpful in scenarios where we have a large dataset with complex patterns
    and we want to ensure our model doesn’t overfit to the training data.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机ReLU**：我们还可以选择随机赋值给![](img/B18046_08_015.png)。这种技术被称为随机ReLU，可以作为一种正则化方法，通过引入一些随机性来防止过拟合。这在我们有一个包含复杂模式的大型数据集，并且希望确保模型不会过拟合训练数据时非常有用。'
- en: Hyperbolic tangent (tanh)
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双曲正切（tanh）
- en: 'The hyperbolic tangent function, or tanh, is closely related to the sigmoid
    function, with a key distinction: it can output negative values, thereby offering
    a broader output range between `-1` and `1`. This can be useful in situations
    where we want to model phenomena that contain both positive and negative influences.
    *Figure 8.12* illustrates this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数，或称为 tanh，与 sigmoid 函数密切相关，其主要区别在于：它可以输出负值，从而提供一个更广泛的输出范围，介于 `-1` 和 `1`
    之间。这在我们想要建模包含正负影响的现象时非常有用。*图 8.12* 展示了这一点：
- en: '![Chart  Description automatically generated with medium confidence](img/B18046_08_12.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成，信心水平中等](img/B18046_08_12.png)'
- en: 'Figure 8.12: Hyperbolic tangent'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12：双曲正切
- en: 'The *y* function is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* 函数如下：'
- en: '![](img/B18046_08_016.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_08_016.png)'
- en: 'It can be implemented by the following Python code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以通过以下 Python 代码实现：
- en: '[PRE4]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this Python code, we’re using the `numpy` library, indicated by `np`, to
    handle the mathematical operations. The `tanh` function, like the `sigmoid`, is
    an activation function used in neural networks to add non-linearity to the model.
    It is often preferred over the sigmoid function in hidden layers of a neural network
    as it centers the data by making the output mean `0`, which can make learning
    in the next layer easier. However, the choice between `tanh`, `sigmoid`, or any
    other activation function largely depends on the specific needs and complexities
    of the model you’re working with.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段 Python 代码中，我们使用了 `numpy` 库，简称 `np`，来处理数学运算。`tanh` 函数，像 `sigmoid` 一样，是神经网络中的一种激活函数，用于为模型引入非线性。它通常在神经网络的隐藏层中优于
    sigmoid，因为它通过将输出的均值设置为 `0` 来使数据居中，从而使得下一个层的学习更加容易。然而，选择 `tanh`、`sigmoid` 或其他激活函数，主要取决于你正在处理的模型的具体需求和复杂性。
- en: Moving on, let’s now delve into the softmax function.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入探讨一下 softmax 函数。
- en: Softmax
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Softmax
- en: 'Sometimes, we need more than two levels for the output of the activation function.
    Softmax is an activation function that provides us with more than two levels for
    the output. It is best suited to multiclass classification problems. Let’s assume
    that we have *n* classes. We have input values. The input values map the classes
    as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要激活函数的输出有多个层级。Softmax 就是一种激活函数，它为我们提供了超过两个层级的输出。它最适合用于多分类问题。假设我们有 *n*
    个类别。我们有输入值，这些输入值将类别映射如下：
- en: '*x = {x*^((1))*,x*^((2))*,....x*^((n))*}*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*x = {x*^((1))*,x*^((2))*,....x*^((n))*}*'
- en: Softmax operates on probability theory. For binary classifiers, the activation
    function in the final layer will be sigmoid, and for multiclass classifiers, it
    will be softmax. To illustrate, let’s say we’re trying to classify an image of
    a fruit, where the classes are `apple`, `banana`, `cherry`, and `date`. The softmax
    function calculates the probabilities of the image belonging to each of these
    classes. The class with the highest probability is then considered as the prediction.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 操作基于概率理论。对于二分类器，最后一层的激活函数将是 sigmoid，而对于多分类器，则使用 softmax。举个例子，假设我们要对一张水果图片进行分类，类别为
    `apple`（苹果）、`banana`（香蕉）、`cherry`（樱桃）和 `date`（枣）。Softmax 函数会计算这张图片属于每个类别的概率。概率最高的类别会被认为是预测结果。
- en: 'To break this down in terms of Python code and equations, let’s look at the
    following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 Python 代码和公式中解释这一点，让我们来看以下内容：
- en: '[PRE5]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now let us look into various tools and frameworks related to neural networks.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下与神经网络相关的各种工具和框架。
- en: Tools and frameworks
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具和框架
- en: In this section, we will delve into the vast array of tools and frameworks that
    have been developed specifically to facilitate the implementation of neural networks.
    Each of these frameworks has its unique advantages and possible limitations.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将深入探讨一些专门为便于实现神经网络而开发的工具和框架。每个框架都有其独特的优点和可能的局限性。
- en: Among the numerous options available, we’ve chosen to spotlight Keras, a high-level
    neural network API, which is capable of running on top of TensorFlow. Why Keras
    and TensorFlow, you may wonder? Well, these two in combination offer several notable
    benefits that make them a popular choice among practitioners.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在众多可用的选项中，我们选择重点介绍 Keras，它是一个高级神经网络 API，可以在 TensorFlow 之上运行。你可能会问，为什么是 Keras
    和 TensorFlow？这两个结合起来提供了多个显著的优势，成为了业内实践者的热门选择。
- en: Firstly, Keras, with its user-friendly and modular nature, simplifies the process
    of building and designing neural network models, thereby catering to beginners
    as well as experienced users. Secondly, its compatibility with TensorFlow, a powerful
    end-to-end open-source platform for ML, ensures robustness and versatility. TensorFlow’s
    ability to deliver high computational performance is another valuable asset. Together,
    they form a dynamic duo that strikes a balance between usability and functionality,
    making them an excellent choice for the development and deployment of neural network
    models.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Keras因其用户友好且模块化的特点，简化了构建和设计神经网络模型的过程，既适合初学者也适合有经验的用户。其次，它与TensorFlow的兼容性——TensorFlow是一个强大的端到端开源机器学习平台——确保了其健壮性和多功能性。TensorFlow提供的高计算性能是其另一个宝贵资产。两者结合，形成了一个动态组合，在可用性和功能性之间取得了平衡，使其成为神经网络模型开发和部署的绝佳选择。
- en: In the following sections, we’ll explore more about how to use Keras with a
    TensorFlow backend to construct neural networks.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨如何使用具有TensorFlow后端的Keras来构建神经网络。
- en: Keras
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras
- en: Keras ([https://www.tensorflow.org/guide/keras](https://www.tensorflow.org/guide/keras))
    is one of the most popular and easy-to-use neural network libraries and is written
    in Python. It was written with ease of use in mind and provides the fastest way
    to implement deep learning. Keras only provides high-level blocks and is considered
    at the model level.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Keras ([https://www.tensorflow.org/guide/keras](https://www.tensorflow.org/guide/keras))
    是一个非常流行且易于使用的神经网络库，使用Python编写。它的编写目标是易用性，并提供了实现深度学习的最快方式。Keras仅提供高级模块，被视为模型级别的工具。
- en: Now, let’s look into the various backend engines of Keras.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看Keras的各种后端引擎。
- en: Backend engines of Keras
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Keras的后端引擎
- en: Keras needs a lower-level deep learning library to perform tensor-level manipulations.
    This foundational layer is referred to as the “backend engine.”
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Keras需要一个较低级别的深度学习库来执行张量级别的操作。这个基础层被称为“后端引擎”。
- en: 'In simpler terms, tensor-level manipulations involve the computations and transformations
    that are performed on multi-dimensional arrays of data, known as tensors, which
    are the primary data structure used in neural networks. This lower-level deep-learning
    library is called the *backend engine*. Possible backend engines for Keras include
    the following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，张量级别的操作涉及对多维数据数组（即张量）进行计算和转换，张量是神经网络中使用的主要数据结构。这个较低级别的深度学习库被称为*后端引擎*。Keras的后端引擎可能包括以下几种：
- en: '**TensorFlow** ([www.tensorflow.org](http://www.tensorflow.org)): This is the
    most popular framework of its kind and is open sourced by Google.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow** ([www.tensorflow.org](http://www.tensorflow.org)): 这是同类中最受欢迎的框架，由Google开源。'
- en: '**Theano**: This was developed at the MILA lab at Université de Montréal.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Theano**：这是在蒙特利尔大学MILA实验室开发的。'
- en: '**Microsoft Cognitive Toolkit** (**CNTK**) ([https://learn.microsoft.com/en-us/cognitive-toolkit/](https://learn.microsoft.com/en-us/cognitive-toolkit/)):
    This was developed by Microsoft.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Microsoft Cognitive Toolkit** (**CNTK**) ([https://learn.microsoft.com/en-us/cognitive-toolkit/](https://learn.microsoft.com/en-us/cognitive-toolkit/)):
    这是由微软开发的。'
- en: 'The format of this modular deep learning technology stack is shown in the following
    diagram:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模块化深度学习技术栈的格式如下面的图所示：
- en: '![Diagram  Description automatically generated](img/B18046_08_13.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18046_08_13.png)'
- en: 'Figure 8.13: Keras architecture'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13：Keras架构
- en: The advantage of this modular deep learning architecture is that the backend
    of Keras can be changed without rewriting any code. For example, if we find TensorFlow
    better than Theona for a particular task, we can simply change the backend to
    TensorFlow without rewriting any code.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模块化的深度学习架构的优势在于，Keras的后端可以在不重写任何代码的情况下进行更改。例如，如果我们发现TensorFlow在某个特定任务上比Theano更好，我们可以简单地将后端更改为TensorFlow，而无需重写代码。
- en: Next, let us look into the low-level layers of the deep learning stack.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入了解深度学习栈的低级层次。
- en: Low-level layers of the deep learning stack
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习栈的低级层次
- en: The three backend engines we just mentioned can all run both on CPUs and GPUs
    using the low-level layers of the stack. For CPUs, a low-level library of tensor
    operations called **Eigen** is used. For GPUs, TensorFlow uses NVIDIA’s **CUDA
    Deep Neural Network** (**cuDNN**) library. It’s noteworthy to explain why GPUs
    are often preferred in ML.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才提到的三种后端引擎都可以在CPU和GPU上运行，使用堆栈的低级层。对于CPU，使用一个低级的张量操作库**Eigen**。对于GPU，TensorFlow使用NVIDIA的**CUDA深度神经网络**（**cuDNN**）库。值得解释的是，为什么在机器学习中通常更偏爱GPU。
- en: While CPUs are versatile and capable, GPUs are specifically designed to handle
    multiple operations concurrently, which is beneficial when processing large blocks
    of data, a common occurrence in ML tasks. This trait of GPUs, combined with their
    higher memory bandwidth, can significantly expedite ML computations, thereby making
    them a popular choice for such tasks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然CPU具有多功能性和强大性能，但GPU是专门设计用来同时处理多个操作的，这在处理大量数据时尤其有利，而这在机器学习任务中非常常见。GPU的这一特性，加上更高的内存带宽，可以显著加快机器学习计算，因此它们成为这些任务的流行选择。
- en: Next, let us explain the hyperparameters.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们解释一下超参数。
- en: Defining hyperparameters
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义超参数
- en: 'As discussed in *Chapter 6*, *Unsupervised Machine Learning Algorithms*, a
    hyperparameter is a parameter whose value is chosen before the learning process
    starts. We start with common-sense values and then try to optimize them later.
    For neural networks, the important hyperparameters are these:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第6章*《*无监督机器学习算法*》中讨论的那样，超参数是一个在学习过程开始之前选择的参数值。我们通常从常识性的值开始，然后尝试优化它们。对于神经网络，重要的超参数包括：
- en: The activation function
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: The learning rate
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: The number of hidden layers
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的数量
- en: The number of neurons in each hidden layer
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层中的神经元数量
- en: Let’s look into how we can define a model using Keras.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用Keras定义一个模型。
- en: Defining a Keras model
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义一个Keras模型
- en: 'There are three steps involved in defining a complete Keras model:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 定义完整Keras模型涉及三个步骤：
- en: Define the layers
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义层
- en: Define the learning process
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义学习过程
- en: Test the model
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试模型
- en: 'We can build a model using `Keras` in two possible ways:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`Keras`有两种方式来构建模型：
- en: '**The Functional API**: This allows us to architect models for acyclic graphs
    of layers. More complex models can be created using the Functional API.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**函数式API**：这允许我们为无环图的层架构模型。可以使用函数式API创建更复杂的模型。'
- en: '**The Sequential API**: This allows us to architect models for a linear stack
    of layers. It is used for relatively simple models and is the usual choice for
    building models.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序API**：这允许我们为线性堆叠的层架构模型。它适用于相对简单的模型，是构建模型时的常用选择。'
- en: 'First, we take a look at the Sequential way of defining a Keras model:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看一下使用顺序方式定义Keras模型：
- en: 'Let us start with importing the `tensorflow` library:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入`tensorflow`库开始：
- en: '[PRE6]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, load the MNIST dataset from Keras’ datasets:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从Keras的datasets加载MNIST数据集：
- en: '[PRE7]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, split the dataset into training and test sets:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将数据集拆分为训练集和测试集：
- en: '[PRE8]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We normalize the pixel values from a scale out of `255` to a scale out of `1:`
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将像素值从`255`的比例归一化到`1`的比例：
- en: '[PRE9]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we define the structure of the model:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义模型的结构：
- en: '[PRE10]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This script is training a model to classify images from the `MNIST` dataset,
    which is a set of 70,000 small images of digits handwritten by high school students
    and employees of the US Census Bureau.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本正在训练一个模型来分类`MNIST`数据集中的图像，该数据集包含70,000张由高中生和美国人口普查局员工手写的数字小图像。
- en: 'The model is defined using the `Sequential` method in Keras, indicating that
    our model is organized as a linear stack of layers:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 模型使用Keras中的`Sequential`方法定义，表示我们的模型是一个线性堆叠的层：
- en: The first layer is a `Flatten` layer, which transforms the format of the images
    from a two-dimensional array into a one-dimensional array.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一层是一个`Flatten`层，它将图像的格式从二维数组转换为一维数组。
- en: The next layer, a `Dense` layer, is a fully connected neural layer with 128
    nodes (or neurons). The `relu` (ReLU) activation function is used here.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一层是一个`Dense`层，它是一个全连接神经层，包含128个节点（或神经元）。此处使用`relu`（ReLU）激活函数。
- en: The `Dropout` layer randomly sets input units to `0` with a frequency of rate
    at each step during training time, which helps prevent overfitting.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Dropout`层在每次训练时随机将输入单元设置为`0`，其频率由每步的率控制，帮助防止过拟合。'
- en: Another `Dense` layer is included; similar to the previous one, it’s also using
    the `relu` activation function.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个`Dense`层被包含在内，类似于前一个层，它也使用`relu`激活函数。
- en: We again apply a `Dropout` layer with the same rate as before.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们再次应用一个`Dropout`层，使用与之前相同的比率。
- en: The final layer is a 10-node softmax layer—this returns an array of 10 probability
    scores that sums to `1`. Each node contains a score that indicates the probability
    that the current image belongs to one of the 10 digit classes.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一层是一个10节点的softmax层——它返回一个包含10个概率分数的数组，总和为`1`。每个节点包含一个分数，表示当前图像属于10个数字类别中的某一类别的概率。
- en: Note that, here, we have created three layers – the first two layers have the
    `relu` activation function and the third layer has `softmax` as the activation
    function.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这里，我们创建了三层——前两层使用`relu`激活函数，第三层使用`softmax`作为激活函数。
- en: 'Now, let’s take a look at the Functional API way of defining a Keras model:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下使用Functional API定义Keras模型的方式：
- en: 'First, let us import the `tensorflow` library:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入`tensorflow`库：
- en: '[PRE11]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To work with the `MNIST` dataset, we first load it into memory. The dataset
    is conveniently split into training and testing sets, with both images and corresponding
    labels:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使用`MNIST`数据集，我们首先将其加载到内存中。该数据集已经方便地分为训练集和测试集，包含了图像和相应的标签：
- en: '[PRE12]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The images in the `MNIST` dataset are `28x28` pixels in size. When setting
    up a neural network model using TensorFlow, you need to specify the shape of the
    input data, Here, we establish the input tensor for the model:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MNIST`数据集中的图像大小为`28x28`像素。在使用TensorFlow设置神经网络模型时，需要指定输入数据的形状。在这里，我们为模型建立了输入张量：'
- en: '[PRE13]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, the `Flatten` layer is a simple data preprocessing step. It transforms
    the two-dimensional `128x128` pixel input into a one-dimensional array by “flattening”
    it. This prepares the data for the following `Dense` layer:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，`Flatten`层是一个简单的数据预处理步骤。它通过“拉平”输入，将二维的`128x128`像素图像转换为一维数组。这样可以为后续的`Dense`层做准备：
- en: '[PRE14]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then comes the first `Dense` layer, also known as a fully connected layer,
    in which each input node (or neuron) is connected to each output node. The layer
    has 512 output nodes and uses the `relu` activation function. ReLU is a popular
    choice of activation function that outputs the input directly if it is positive;
    otherwise, it outputs zero:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是第一个`Dense`层，也称为全连接层，其中每个输入节点（或神经元）都与每个输出节点相连接。该层有512个输出节点，并使用`relu`激活函数。ReLU是一个流行的激活函数，它在输入为正时直接输出输入值；否则输出零：
- en: '[PRE15]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `Dropout` layer randomly sets a fraction (0.2, or 20% in this case) of
    the input nodes to 0 at each update during training, which helps prevent overfitting:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Dropout`层会在每次训练更新时随机将输入节点的一部分（在本例中为0.2或20%）设置为0，这有助于防止过拟合：'
- en: '[PRE16]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, comes the output layer. It’s another `Dense` layer with 10 output
    nodes (presumably for 10 classes). The `softmax` activation function is applied,
    which outputs a probability distribution over the 10 classes, meaning it will
    output 10 values that sum to 1\. Each value represents the model’s confidence
    that the input image corresponds to a particular class:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后是输出层。它是另一个`Dense`层，包含10个输出节点（假设是10个类别）。应用`softmax`激活函数，该函数输出一个概率分布，覆盖10个类别，这意味着它将输出10个值，总和为1。每个值表示模型对输入图像对应某一特定类别的置信度：
- en: '[PRE17]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that we can define the same neural network using both the Sequential and
    Functional APIs. From the point of view of performance, it does not make any difference
    which approach you take to define the model.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以使用Sequential和Functional两种API定义相同的神经网络。从性能角度来看，选择哪种方式定义模型并没有区别。
- en: 'Let us convert the numerical `train_labels` and `test_labels` into one-hot
    encoded vectors. In the following code each label becomes a binary array of size
    10 with a 1 at its respective digit’s index and 0s elsewhere:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数值型的`train_labels`和`test_labels`转换为one-hot编码向量。在下面的代码中，每个标签都会变成一个大小为10的二进制数组，其中相应数字的索引位置为1，其余位置为0：
- en: '[PRE18]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We should now define the learning process.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在应该定义学习过程。
- en: 'In this step, we define three things:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们定义三项内容：
- en: The optimizer
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器
- en: The `loss` function
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`函数'
- en: 'The metrics that will quantify the quality of the model:'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将量化模型质量的度量标准：
- en: '[PRE19]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that we use the `model.compile` function to define the optimizer, loss
    function, and metrics.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用`model.compile`函数来定义优化器、损失函数和度量标准。
- en: We will now train the model.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将训练模型。
- en: 'Once the architecture is defined, it is time to train the model:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦架构定义完成，就可以开始训练模型：
- en: '[PRE20]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note that parameters such as `batch_size` and `epochs` are configurable parameters,
    making them hyperparameters.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，像`batch_size`和`epochs`这样的参数是可配置的参数，因此它们是超参数。
- en: Next, let us look into how we can choose the sequential or functional model.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入探讨如何选择顺序模型或功能模型。
- en: Choosing a sequential or functional model
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择顺序模型或功能模型
- en: 'When deciding between using a sequential or functional model to construct a
    neural network, the nature of your network’s architecture will guide your choice.
    The sequential model is suited to simple linear stacks of layers. It’s uncomplicated
    and straightforward to implement, making it an ideal choice for beginners or for
    simpler tasks. However, this model comes with a key limitation: each layer can
    be connected to precisely one input tensor and one output tensor.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定使用顺序模型还是功能模型来构建神经网络时，网络架构的性质将指导你的选择。顺序模型适用于简单的线性层堆叠。它实现起来简单直接，是初学者或处理简单任务的理想选择。然而，这种模型有一个关键的限制：每一层只能连接到一个输入张量和一个输出张量。
- en: If the architecture of your network is more complex, such as having multiple
    inputs or outputs at any stage (input, output, or hidden layers), then the sequential
    model falls short. For such complex architectures, the functional model is more
    appropriate. This model provides a higher degree of flexibility, allowing for
    more complex network structures with multiple inputs and outputs at any layer.
    Let us now develop a deeper understanding of TensorFlow.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的网络架构更加复杂，例如在任何阶段（输入层、输出层或隐藏层）有多个输入或输出，那么顺序模型就不再适用。对于这种复杂的架构，功能模型更为合适。该模型提供了更高的灵活性，允许在任何层具有多个输入和输出，从而支持更复杂的网络结构。现在让我们更深入地理解TensorFlow。
- en: Understanding TensorFlow
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解TensorFlow
- en: TensorFlow is one of the most popular libraries for working with neural networks.
    In the preceding section, we saw how we can use it as the backend engine of Keras.
    It is an open-source, high-performance library that can actually be used for any
    numerical computation.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是最流行的神经网络工作库之一。在前面的部分中，我们看到它如何作为Keras的后台引擎使用。它是一个开源的高性能库，实际上可以用于任何数值计算。
- en: If we look at the stack, we can see that we can write TensorFlow code in a high-level
    language such as Python or C++, which gets interpreted by the TensorFlow distributed
    execution engine. This makes it quite useful for and popular with developers.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看看堆栈，我们可以看到，我们可以使用像Python或C++这样的高级语言编写TensorFlow代码，这些代码会被TensorFlow分布式执行引擎解释执行。这使得它对开发者来说非常有用并且广受欢迎。
- en: TensorFlow functions by using a **directed graph** (**DG**) to embody your computations.
    In this graph, nodes are mathematical operations, and the edges connecting these
    nodes signify the input and output of these operations. Moreover, these edges
    symbolize data arrays.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow通过使用**有向图**（**DG**）来体现你的计算。在这个图中，节点是数学运算，连接这些节点的边代表这些运算的输入和输出。此外，这些边还表示数据数组。
- en: Apart from serving as the backend engine for Keras, TensorFlow is broadly used
    in various scenarios. It can help in developing complex ML models, processing
    large datasets, and even deploying AI applications across different platforms.
    Whether you’re creating a recommendation system, image classification model, or
    natural language processing tool, TensorFlow can effectively cater to these tasks
    and more.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 除了作为Keras的后台引擎，TensorFlow还广泛应用于各种场景中。它可以帮助开发复杂的机器学习模型、处理大规模数据集，甚至在不同平台上部署AI应用。无论你是在创建推荐系统、图像分类模型，还是自然语言处理工具，TensorFlow都能有效地满足这些任务以及更多需求。
- en: Presenting TensorFlow’s basic concepts
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍TensorFlow的基本概念
- en: 'Let’s take a brief look at TensorFlow concepts such as scalars, vectors, and
    matrices. We know that a simple number, such as three or five, is called a **scalar**
    in traditional mathematics. Moreover, in physics, a **vector** is something with
    magnitude and direction. In terms of TensorFlow, we use a vector to mean one-dimensional
    arrays. Extending this concept, a two-dimensional array is a **matrix**. For a
    three-dimensional array, we use the term **3D tensor**. We use the term **rank**
    to capture the dimensionality of a data structure. As such, a **scalar** is a
    **rank 0** data structure, a **vector** is a **rank 1** data structure, and a
    **matrix** is a **rank 2** data structure. These multi-dimensional structures
    are known as **tensors** and are shown in the following diagram:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要了解一下 TensorFlow 中的概念，比如标量、向量和矩阵。我们知道，像三或五这样的简单数字，在传统数学中被称为**标量**。此外，在物理学中，**向量**是具有大小和方向的量。在
    TensorFlow 中，我们用向量表示一维数组。扩展这一概念，二维数组即为**矩阵**。对于三维数组，我们使用**3D 张量**这一术语。我们使用**秩**来表示数据结构的维度。因此，**标量**是**秩
    0**的数据结构，**向量**是**秩 1**的数据结构，**矩阵**是**秩 2**的数据结构。这些多维结构被称为**张量**，并在以下图表中展示：
- en: '![Shape, square  Description automatically generated](img/B18046_08_14.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![形状，正方形 描述自动生成](img/B18046_08_14.png)'
- en: 'Figure 8.14: Multi-dimensional structures or tensors'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14：多维结构或张量
- en: As we can see in the preceding diagram, the rank defines the dimensionality
    of a tensor.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前面的图表中看到的，秩定义了张量的维度。
- en: Let’s now look at another parameter, `shape`. `shape` is a tuple of integers
    specifying the length of an array in each dimension.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看另一个参数，`shape`。`shape`是一个整数元组，指定每个维度中数组的长度。
- en: 'The following diagram explains the concept of `shape`:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表解释了`shape`的概念：
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B18046_08_15.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![一张图形用户界面的图片 描述自动生成](img/B18046_08_15.png)'
- en: 'Figure 8.15: Concept of a shape'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15：形状的概念
- en: Using `shape` and ranks, we can specify the details of tensors.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`shape`和秩，我们可以指定张量的详细信息。
- en: Understanding Tensor mathematics
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解张量数学
- en: 'Let’s now look at different mathematical computations using tensors:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下使用张量进行的不同数学运算：
- en: 'Let’s define two scalars and try to add and multiply them using TensorFlow:'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们定义两个标量，并尝试使用 TensorFlow 进行加法和乘法运算：
- en: '[PRE21]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can add and multiply them and display the results:'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以对其进行加法和乘法运算并展示结果：
- en: '[PRE23]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can also create a new scalar tensor by adding the two tensors:'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以通过将两个张量相加来创建一个新的标量张量：
- en: '[PRE25]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can also perform complex tensor functions:'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以执行复杂的张量运算：
- en: '[PRE27]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Understanding the types of neural networks
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络的类型
- en: Neural networks can be designed in various ways, depending on how the neurons
    are interconnected. In a dense, or fully connected, neural network, every single
    neuron in a given layer is linked to each neuron in the next layer. This means
    each input from the preceding layer is fed into every neuron of the subsequent
    layer, maximizing the flow of information.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以根据神经元的互联方式进行不同的设计。在密集型或全连接的神经网络中，给定层中的每个神经元都与下一层的每个神经元相连接。这意味着来自前一层的每个输入都会传递到下一层的每个神经元，从而最大化信息流动。
- en: However, neural networks aren’t always fully connected. Some may have specific
    patterns of connections based on the problem they are designed to solve. For instance,
    in convolutional neural networks used for image processing, each neuron in a layer
    may only be connected to a small region of neurons in the previous layer. This
    mirrors the way neurons in the human visual cortex are organized and helps the
    network efficiently process visual information.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，神经网络并不总是完全连接的。有些网络可能基于其解决问题的需求，具有特定的连接模式。例如，在用于图像处理的卷积神经网络中，某一层的每个神经元可能只与上一层中一小块区域的神经元相连接。这与人类视觉皮层中神经元的组织方式相似，并帮助网络高效处理视觉信息。
- en: Remember, the specific architecture of a neural network – how the neurons are
    interconnected – greatly impacts its functionality and performance.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，神经网络的具体架构——神经元如何互联——对其功能和性能有着极大的影响。
- en: Convolutional neural networks
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '**Convolution neural networks** (**CNNs**) are typically used to analyze multimedia
    data. In order to learn more about how a CNN is used to analyze image-based data,
    we need to have a grasp of the following processes:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）通常用于分析多媒体数据。为了深入了解 CNN 如何分析基于图像的数据，我们需要掌握以下过程：'
- en: Convolution
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积
- en: Pooling
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化
- en: Let’s explore them one by one.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一探索它们。
- en: Convolution
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积
- en: The process of convolution emphasizes a pattern of interest in a particular
    image by processing it with another smaller image called a **filter** (also called
    a **kernel**). For example, if we want to find the edges of objects in an image,
    we can convolve the image with a particular filter to get them. Edge detection
    can help us in object detection, object classification, and other applications.
    So, the process of convolution is about finding characteristics and features in
    an image.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积过程通过使用一个称为**滤波器**（也叫**卷积核**）的小图像处理特定图像，强调图像中的某些模式。例如，如果我们想找到图像中物体的边缘，我们可以将图像与特定的滤波器卷积，从而得到边缘。边缘检测有助于物体检测、物体分类等应用。因此，卷积过程就是在图像中寻找特征和特性。
- en: The approach to finding patterns is based on finding patterns that can be reused
    on different data. The reusable patterns are called filters or kernels.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 查找模式的方法是基于找到可以在不同数据上重用的模式。这些可重用的模式被称为滤波器或卷积核。
- en: Pooling
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 池化
- en: 'An important part of processing multimedia data for the purpose of ML is downsampling
    it. Downsampling is the practice of reducing the resolution of your data, i.e.,
    lessening the data’s complexity or dimensionality. Pooling offers two key advantages:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行机器学习，处理多媒体数据的重要部分是下采样。下采样是减少数据分辨率的过程，即降低数据的复杂性或维度。池化提供了两个主要优点：
- en: By reducing the data’s complexity, we significantly decrease the training time
    for the model, enhancing computational efficiency.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过降低数据的复杂性，我们可以显著减少模型的训练时间，提高计算效率。
- en: Pooling abstracts and aggregates unnecessary details in the multimedia data,
    making it more generalized. This, in turn, enhances the model’s ability to represent
    similar problems.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化抽象并聚合了多媒体数据中的不必要细节，使其更加通用。反过来，这增强了模型表示相似问题的能力。
- en: 'Downsampling is performed as follows:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样过程如下：
- en: '![Diagram  Description automatically generated](img/B18046_08_16.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18046_08_16.png)'
- en: 'Figure 8.16: Downsampling'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16：下采样
- en: In the downsampling process, we essentially condense a group of pixels into
    a single representative pixel. For instance, let’s say we condense a 2x2-pixel
    block into a single pixel, effectively downsampling the original data by a factor
    of four.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在下采样过程中，我们本质上是将一组像素压缩成一个代表性像素。例如，我们可以将一个2x2像素块压缩成一个像素，从而将原始数据的分辨率减少四倍。
- en: The representative value for the new pixel can be chosen in various ways. One
    such method is “max pooling,” where we select the maximum value from the original
    pixel block to represent the new single pixel.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 新像素的代表值可以通过多种方式选择。其中一种方法是“最大池化”，在这种方法中，我们从原始像素块中选择最大值来表示新的单一像素。
- en: On the other hand, if we chose to take the average of the pixel block’s values,
    the process would be termed “average pooling.”
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们选择取像素块值的平均值，这个过程将被称为“平均池化”。
- en: The choice between max pooling and average pooling often depends on the specific
    task at hand. Max pooling is particularly beneficial when we’re interested in
    preserving the most prominent features of the image, as it retains the maximum
    pixel value in a block, thus capturing the most standout or noticeable aspect
    within that section.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化与平均池化的选择通常取决于具体任务。最大池化在我们希望保留图像中最显著特征时特别有用，因为它保留了一个块中的最大像素值，从而捕捉到该部分中最突出或最显眼的特征。
- en: In contrast, average pooling tends to be useful when we want to preserve the
    overall context and reduce noise, as it considers all values within a block and
    calculates their average, creating a more balanced representation that may be
    less sensitive to minor variations or noise in pixel values.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，平均池化在我们希望保留整体背景并减少噪声时通常更有用，因为它考虑了块内的所有值并计算它们的平均值，从而创建一个更平衡的表示，可能对像素值中的细微变化或噪声不太敏感。
- en: Generative Adversarial Networks
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: Generative Adversarial Networks, commonly referred to as GANs, represent a distinct
    class of neural networks capable of generating synthetic data. First introduced
    by Ian Goodfellow and his team in 2014, GANs have been hailed for their innovative
    approach to creating new data resembling the original training samples.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络，通常称为GAN，是一种能够生成合成数据的神经网络类别。它由Ian Goodfellow及其团队于2014年首次提出，因其创新的方法而受到赞誉，能够创建与原始训练样本相似的新数据。
- en: One notable application of GANs is their ability to produce realistic images
    of people who don’t exist in reality, showcasing their remarkable capacity for
    detail generation. However, an even more crucial application lies in their potential
    to generate synthetic data, thereby augmenting existing training datasets, which
    can be extremely beneficial in scenarios where data availability is limited.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）一个显著的应用是能够生成现实中不存在的人的逼真图像，展示了它们在细节生成上的非凡能力。然而，更为关键的应用在于它们能够生成合成数据，从而扩充现有的训练数据集，在数据可用性有限的情况下，这种应用非常有价值。
- en: Despite their potential, GANs are not without limitations. The training process
    of GANs can be quite challenging, often leading to issues such as mode collapse,
    where the generator starts producing limited varieties of samples. Additionally,
    the quality of the generated data is largely dependent on the quality and diversity
    of the input data. Poorly representative or biased data can result in less effective,
    potentially skewed synthetic data.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管生成对抗网络（GAN）具有潜力，但它们并非没有局限性。GAN的训练过程可能相当具有挑战性，常常导致一些问题，如模式崩溃，生成器开始产生有限种类的样本。此外，生成数据的质量在很大程度上取决于输入数据的质量和多样性。数据不具代表性或有偏差时，可能导致合成数据的效果不佳，甚至可能偏向某些特定方向。
- en: In the upcoming section, we will see what transfer learning is.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨什么是迁移学习。
- en: Using transfer learning
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迁移学习
- en: 'Throughout the years, countless organizations, research entities, and contributors
    within the open-source community have meticulously built sophisticated models
    for general use cases. These models, often trained with vast amounts of data,
    have been optimized over years of hard work and are suited for various applications,
    such as:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，无数组织、研究机构和开源社区的贡献者们精心构建了适用于一般用途的复杂模型。这些模型通常通过大量数据进行训练，经过多年的努力优化，适用于各种应用场景，如：
- en: Detecting objects in videos or images
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测视频或图像中的物体
- en: Transcribing audio
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转录音频
- en: Analyzing sentiment in text
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析文本情感
- en: 'When initiating the training of a new ML model, it’s worth questioning, rather
    than starting from a blank slate, whether we can modify an already established,
    pre-trained model to suit our needs. Put simply, could we leverage the learning
    of existing models to tailor a custom model that addresses our specific needs?
    Such an approach, known as transfer learning, can provide several advantages:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动新的机器学习模型训练时，不妨考虑一个问题：与其从零开始，是否可以修改已有的预训练模型来满足我们的需求。简而言之，我们能否利用现有模型的学习成果，定制一个适应我们特定需求的模型？这种方法被称为迁移学习，具有以下几项优势：
- en: It gives a head start to our model training.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为我们的模型训练提供了一个良好的起点。
- en: It potentially enhances the quality of our model by utilizing a pre-validated
    and reliable model.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过利用一个经过预验证和可靠的模型，可能提升我们模型的质量。
- en: In cases where our problem lacks sufficient data, transfer learning using a
    pre-trained model can be of immense help.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的问题缺乏足够数据的情况下，使用预训练模型进行迁移学习可以提供极大的帮助。
- en: 'Consider the following practical examples where transfer learning would be
    beneficial:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下实际例子，在这些场景下，迁移学习将大有裨益：
- en: For training a robot, a neural network model could first be trained using a
    simulation game. In this controlled environment, we can create rare events that
    are difficult to replicate in the real world. Once trained, transfer learning
    can then be applied to adapt the model for real-world scenarios.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练机器人时，可以先通过一个模拟游戏来训练神经网络模型。在这个受控环境中，我们可以创建一些在现实世界中难以复制的稀有事件。训练完成后，可以应用迁移学习来使模型适应现实世界的场景。
- en: Suppose we aim to build a model that distinguishes between Apple and Windows
    laptops in a video feed. Existing, open-source object detection models, known
    for their accuracy in classifying diverse objects in video feeds, could serve
    as an ideal starting point. Using transfer learning, we can first leverage these
    models to identify objects as laptops. Subsequently, we could refine our model
    further to differentiate between Apple and Windows laptops.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们旨在构建一个模型，用于区分视频流中的苹果和Windows笔记本电脑。现有的开源物体检测模型以其在视频流中对不同物体分类的高准确率而著称，这些模型可以作为理想的起点。通过迁移学习，我们可以首先利用这些模型识别物体为笔记本电脑。接着，我们可以进一步优化我们的模型，区分苹果和Windows笔记本电脑。
- en: In our next section, we will implement the principles discussed in this chapter
    to create a neural network for classifying fraudulent documents.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将实现本章讨论的原则，创建一个用于分类欺诈文档的神经网络。
- en: As a visual example, consider a pre-trained model as a well-established tree
    with many branches (layers). Some branches are already ripe with fruits (trained
    to identify features). When applying transfer learning, we “freeze” these fruitful
    branches, preserving their established learning. We then allow new branches to
    grow and bear fruit, which is akin to training the additional layers to understand
    our specific features. This process of freezing some layers and training others
    encapsulates the essence of transfer learning.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个视觉示例，考虑一个预训练的模型作为一棵成熟的大树，树上有许多枝条（层）。一些枝条上已经挂满了果实（训练好以识别特征）。在应用迁移学习时，我们“冻结”这些结实的枝条，保留它们已建立的学习成果。然后，我们允许新的枝条生长并结出果实，这类似于训练额外的层来理解我们的特定特征。冻结某些层并训练其他层的过程概括了迁移学习的本质。
- en: Case study – using deep learning for fraud detection
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 – 使用深度学习进行欺诈检测
- en: Using ML techniques to identify fraudulent documents is an active and challenging
    field of research. Researchers are investigating to what extent the pattern recognition
    power of neural networks can be exploited for this purpose. Instead of manual
    attribute extractors, raw pixels can be used for several deep learning architectural
    structures.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习技术识别欺诈文档是一个活跃且具有挑战性的研究领域。研究人员正在探索神经网络的模式识别能力在多大程度上可以用于此目的。与手动属性提取器不同，可以使用原始像素来构建多种深度学习架构结构。
- en: Methodology
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法论
- en: The technique presented in this section uses a type of neural network architecture
    called **Siamese neural networks**, which features two branches that share identical
    architectures and parameters.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍的技术使用了一种称为**Siamese神经网络**的神经网络架构，该架构具有两个共享相同架构和参数的分支。
- en: 'The use of Siamese neural networks to flag fraudulent documents is shown in
    the following diagram:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Siamese神经网络标记欺诈文档的示意图如下所示：
- en: '![Diagram  Description automatically generated](img/B18046_08_17.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![图示描述自动生成](img/B18046_08_17.png)'
- en: 'Figure 8.17: Siamese neural networks'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17：Siamese神经网络
- en: When a particular document needs to be verified for authenticity, we first classify
    the document based on its layout and type, and then we compare it against its
    expected template and pattern. If it deviates beyond a certain threshold, it is
    flagged as a fake document; otherwise, it is considered an authentic or true document.
    For critical use cases, we can add a manual process for borderline cases where
    the algorithm conclusively classifies a document as authentic or fake.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要验证某一文档的真实性时，我们首先根据其布局和类型对文档进行分类，然后将其与预期的模板和模式进行比较。如果偏离超过某个阈值，则标记为伪造文档；否则，认为它是一个真实文档。对于关键的使用场景，我们可以为边界情况添加人工处理过程，在这些情况下，算法明确将文档分类为真实或伪造。
- en: To compare a document against its expected template, we use two identical CNNs
    in our Siamese architecture. CNNs have the advantage of learning optimal shift-invariant
    local feature detectors and can build representations that are robust to geometric
    distortions of the input image. This is well suited to our problem since we aim
    to pass authentic and test documents through a single network, and then compare
    their outcomes for similarity. To achieve this goal, we implement the following
    steps.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将文档与预期模板进行比较，我们在我们的Siamese架构中使用两个相同的CNN。CNN具有学习最佳平移不变局部特征检测器的优势，并且能够构建对输入图像几何畸变具有鲁棒性的表示。这非常适合我们的问题，因为我们的目标是通过单个网络传递真实文档和测试文档，然后比较它们的输出以确定相似性。为了实现这一目标，我们执行以下步骤。
- en: 'Let’s assume that we want to test a document. For each class of document, we
    perform the following steps:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要测试一个文档。对于每种文档类别，我们执行以下步骤：
- en: Get the stored image of the authentic document. We call it the **true document**.
    The test document should look like the true document.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取存储的真实文档图像。我们称其为**真实文档**。测试文档应该与真实文档相似。
- en: The true document is passed through the neural network layers to create a feature
    vector, which is the mathematical representation of the patterns of the true document.
    We call it **Feature Vector 1**, as shown in the preceding diagram.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真实文档通过神经网络层传递，创建一个特征向量，这是该文档模式的数学表示。我们称其为**特征向量 1**，如前图所示。
- en: The document that needs to be tested is called the **test document**. We pass
    this document through a neural network similar to the one that was used to create
    the feature vector for the true document. The feature vector of the test document
    is called **Feature Vector 2**.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要测试的文档称为 **测试文档**。我们将该文档传递通过一个与用于创建真实文档特征向量的神经网络类似的神经网络。测试文档的特征向量称为 **特征向量
    2**。
- en: We use the Euclidean distance between **Feature Vector 1** and **Feature Vector
    2** to calculate the similarity score between the true document and the test document.
    This similarity score is called the **Measure Of Similarity** (**MOS**). The MOS
    is a number between 0 and 1\. A higher number represents a lower distance between
    the documents and a greater likelihood that the documents are similar.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 **特征向量 1** 和 **特征向量 2** 之间的欧氏距离来计算真实文档与测试文档之间的相似度得分。这个相似度得分被称为 **相似度度量**（**MOS**）。MOS
    是一个介于 0 和 1 之间的数字。数字越大，表示文档之间的距离越小，文档相似的可能性越大。
- en: If the similarity score calculated by the neural network is below a pre-defined
    threshold, we flag the document as fraudulent.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果神经网络计算出的相似度得分低于预定义的阈值，我们将标记该文档为欺诈文档。
- en: Let’s see how we can implement Siamese neural networks using Python.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用 Python 实现双胞胎神经网络。
- en: 'To illustrate how we can implement Siamese neural networks using Python, we’ll
    break down the process into simpler, more manageable blocks. This approach will
    help us follow the PEP8 style guide and keep our code readable and maintainable:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何使用 Python 实现双胞胎神经网络，我们将把这个过程拆解成更简单、易管理的块。这种方法将帮助我们遵循 PEP8 风格指南，保持代码的可读性和可维护性：
- en: 'First, let’s import the Python packages that are required:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入所需的 Python 包：
- en: '[PRE29]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we define the network model that will process each branch of the Siamese
    network. Note that we’ve incorporated a dropout rate of `0.15` to mitigate overfitting:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义将处理双胞胎网络每个分支的网络模型。注意，我们已将丢弃率设置为 `0.15`，以减少过拟合：
- en: '[PRE30]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'For our Siamese networks, we’ll use MNIST images. These images are excellent
    for testing the effectiveness of our Siamese network. We prepare the data such
    that each sample will contain two images and a binary similarity flag indicating
    whether they belong to the same class:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们的双胞胎网络，我们将使用 MNIST 图像。这些图像非常适合测试我们的双胞胎网络的有效性。我们准备数据，使得每个样本将包含两张图像和一个二元相似度标志，指示它们是否属于同一类别：
- en: '[PRE31]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the `prepareData` function, we ensure an equal number of samples across all
    digits. We first create an index of where in our dataset each digit appears, using
    the `np.where` function.
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `prepareData` 函数中，我们确保所有数字的样本数量相等。我们首先使用 `np.where` 函数创建一个索引，表示每个数字在数据集中出现的位置。
- en: 'Then, we prepare our pairs of images and assign labels:'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们准备图像对并分配标签：
- en: '[PRE32]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Subsequently, we’ll prepare our training and testing datasets:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后，我们将准备训练和测试数据集：
- en: '[PRE33]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Lastly, we will implement the MOS, which quantifies the distance between two
    documents that we want to compare:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将实现 MOS，它量化了我们想要比较的两个文档之间的距离：
- en: '[PRE34]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, let’s train the model. We will use 10 epochs to train this model:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练模型。我们将使用 10 个 epoch 来训练该模型：
- en: '[PRE35]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note that we reached an accuracy of `97.49%` using `10` epochs. Increasing the
    number of epochs will further improve the level of accuracy.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用 `10` 个 epoch 我们达到了 `97.49%` 的准确率。增加 epoch 数量将进一步提高准确度。
- en: Summary
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we journeyed through the evolution of neural networks, examining
    different types, key components like activation functions, and the significant
    gradient descent algorithm. We touched upon the concept of transfer learning and
    its practical application in identifying fraudulent documents.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了神经网络的演变，研究了不同类型、关键组件如激活函数，以及重要的梯度下降算法。我们还提到了迁移学习的概念及其在识别欺诈文档中的实际应用。
- en: As we proceed to the next chapter, we’ll delve into natural language processing,
    exploring areas such as word embedding and recurrent networks. We will also learn
    how to implement sentiment analysis. The captivating realm of neural networks
    continues to unfold.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进入下一章，我们将深入探讨自然语言处理，探索诸如词嵌入和递归网络等领域。我们还将学习如何实现情感分析。神经网络的迷人世界仍在展开。
- en: Learn more on Discord
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入本书的 Discord 社区——在这里你可以分享反馈，向作者提问，并了解新版本的发布——请扫描下方二维码：
- en: '[https://packt.link/WHLel](https://packt.link/WHLel)'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/WHLel](https://packt.link/WHLel)'
- en: '![](img/QR_Code1955211820597889031.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1955211820597889031.png)'
