- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: Multi-Step Forecasting
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多步预测
- en: In the previous parts, we covered some basics of forecasting and different types
    of modeling techniques for time series forecasting. However, a complete forecasting
    system is not just the model. There are a few mechanics of time series forecasting
    that make a lot of difference. These topics cannot be called *basics* because
    they require a nuanced understanding of the forecasting paradigm, and that is
    why we didn’t cover these upfront.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们介绍了一些预测的基础知识和时间序列预测的不同建模技术。然而，一个完整的预测系统不仅仅是模型。时间序列预测的某些机制往往会带来很大的差异。这些内容不能被称为*基础*，因为它们需要对预测范式有更为精细的理解，这也是我们没有在一开始就介绍它们的原因。
- en: Now that you have worked on some forecasting models and are familiar with time
    series, it’s time to get more nuanced in our approach. Most of the forecasting
    exercises we have done throughout the book focus on forecasting the next timestep.
    In this chapter, we will look at strategies to generate multi-step forecasting—in
    other words, how to forecast the next *H* timesteps. In most practical applications
    of forecasting, we have to forecast multiple timesteps ahead, and being able to
    handle such cases is an essential skill.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经进行了一些预测模型的训练，并且熟悉了时间序列，现在是时候在我们的方法上更精细一些了。本书中我们进行的大多数预测练习都集中在预测下一个时间步。在本章中，我们将探讨生成多步预测的策略——换句话说，就是如何预测接下来的*H*个时间步。在大多数实际的预测应用中，我们需要预测多个时间步的未来，而能够处理这种情况是一个必备的技能。
- en: 'In this chapter, we will cover these main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Why multi-step forecasting?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择多步预测？
- en: Standard notation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准符号
- en: Recursive strategy
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归策略
- en: Direct strategy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接策略
- en: Joint strategy
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联合策略
- en: Hybrid strategies
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合策略
- en: How to choose a multi-step forecasting strategy
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何选择多步预测策略
- en: Why multi-step forecasting?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择多步预测？
- en: A multi-step forecasting task consists of forecasting the next *H* timesteps,
    *y*[t][+1],…, *y*[t][+][H], of a time series, *y*[1], …, *y*[t], where *H* > 1\.
    Most real-world applications of time series forecasting demand multi-step forecasting,
    whether it is the energy consumption of a household or the sales of a product.
    This is because forecasts are never created to know what will happen in the future
    but, rather, to enable us to take action using the visibility we get.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 多步预测任务包括预测时间序列的下一个*H*个时间步，即*y*[t][+1]，…，*y*[t][+H]，其中*y*[1]，…，*y*[t]为已知的时间序列，且*H*
    > 1。大多数实际的时间序列预测应用都要求进行多步预测，无论是家庭的能源消耗还是产品的销售。这是因为预测的目的从来不是为了知道未来会发生什么，而是为了利用我们获得的可见性来采取行动。
- en: To effectively take any action, we would want to know the forecast a little
    ahead of time. For instance, the dataset we have used throughout the book is about
    the energy consumption of households, logged every half an hour. If the energy
    provider wants to plan its energy production to meet customer demand, the next
    half an hour doesn’t help at all. Similarly, if we look at the retail scenario,
    where we want to forecast the sales of a product, we will want to forecast a few
    days ahead so that we can purchase necessary goods, ship them to the store, and
    so on, in time for the demand.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地采取任何行动，我们希望提前了解预测结果。例如，我们在本书中使用的数据集是关于家庭能源消耗的，每半小时记录一次。如果能源供应商希望计划其能源生产以满足客户需求，那么预测下一个半小时的需求根本没有帮助。同样，如果我们考虑零售场景，假设我们想预测某个产品的销售，我们会希望提前预测几天，以便能够及时采购所需商品，运送到商店等。
- en: Despite being a more prevalent use case, multi-step forecasting has not received
    the attention it deserves. One of the reasons for that is the existence of classical
    statistical models or econometrics models, such as the *ARIMA* and *exponential
    smoothing* methods, which include the multi-step strategy bundled within what
    we call a model; because of that, these models can generate multiple timesteps
    without breaking a sweat (although, as we will see in the chapter, they rely on
    one specific multi-step strategy to generate their forecast). Because these models
    were the most popular models used, practitioners didn’t need to worry about multi-step
    forecasting strategies. However, the advent of **machine learning** (**ML**) and
    **deep learning** (**DL**) methods for time series forecasting has opened up the
    need for a more focused study of multi-step forecasting strategies once again.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管多步预测是一个更为普遍的应用场景，但它并没有受到应有的关注。原因之一是经典统计模型或计量经济学模型的存在，如*ARIMA*和*指数平滑*方法，这些方法将多步策略作为模型的一部分；因此，这些模型可以轻松地生成多个时间步的预测（尽管正如我们将在本章中看到的，它们依赖于一种特定的多步策略来生成预测）。由于这些模型曾是最流行的模型，实践者无需担心多步预测策略。然而，**机器学习**（**ML**）和**深度学习**（**DL**）方法在时间序列预测中的出现，再次推动了对多步预测策略的更深入研究。
- en: Another reason for the lower popularity of multi-step forecasting is that it
    is simply harder than single-step forecasting. This is because the more steps
    we extrapolate into the future, the more uncertainty there is in the predictions,
    due to complex interactions between the different steps ahead. Depending on the
    strategy we choose, we will have to manage the dependencies on previous forecasts,
    the propagation and magnification of errors, and so on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 多步预测较低流行度的另一个原因是它比单步预测要难。这是因为我们向未来推演的步数越多，预测中的不确定性就越大，原因在于不同步之间复杂的交互。根据我们选择的策略，我们将不得不管理对先前预测的依赖、错误的传播和放大等问题。
- en: 'There are many strategies that can be used to generate multi-step forecasting,
    and the following figure summarizes them neatly:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多策略可以用来生成多步预测，以下图表清晰地总结了这些策略：
- en: '![Figure 17.1 – Multi-step forecasting strategies ](img/B22389_18_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图17.1 – 多步预测策略](img/B22389_18_01.png)'
- en: 'Figure 18.1: Multi-step forecasting strategies'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.1：多步预测策略
- en: Each node of the graph in *Figure 18.1* is a strategy, and different strategies
    that have common elements have been linked together with edges in the graph. In
    the rest of the chapter, we will cover each of these nodes (strategies) and explain
    them in detail.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.1*中的每个节点都是一个策略，具有共同元素的不同策略通过图中的边连接在一起。在本章的其余部分，我们将介绍这些节点（策略），并详细解释它们。'
- en: Standard notation
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准符号
- en: Let’s establish a few basic notations to help us understand these strategies.
    We have a time series, *Y*[T], of *T* timesteps, *y*[1], …, *y*[T]. *Y*[T] denotes
    the same series but ending at timestep *t*. We also consider a function, *W*,
    which generates a window of size *k* > 0 from a time series.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们建立一些基本符号，帮助我们理解这些策略。我们有一个时间序列，*Y*[T]，共有*T*个时间步，*y*[1]，…，*y*[T]。*Y*[T]表示同一个序列，但结束于时间步*t*。我们还考虑一个函数，*W*，它从时间序列中生成大小为*k*
    > 0的窗口。
- en: This function is a proxy for how we prepare the input for the different models
    we have seen throughout the book. So if we see *W*(*Y*[t]), it means the function
    will draw a window from *Y*[T] that ends at timestep *t*. We will also consider
    *H* to be the forecast horizon, where *H* > 1\. We will also use ; as an operator,
    which denotes concatenation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数是我们为本书中所见不同模型准备输入的代理。所以，如果我们看到*W*(*Y*[t])，这意味着该函数将从以时间步*t*结束的*Y*[T]中提取一个窗口。我们还将*H*视为预测范围，其中*H*
    > 1。我们还将使用；作为运算符，表示连接。
- en: Now, let’s look at the different strategies (Reference *1* is a good survey
    paper for different strategies). The discussion about merits and where we can
    use each of them is bundled in another upcoming section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下不同的策略（参考*1*是一篇关于不同策略的优秀综述文章）。关于各自优缺点以及在何种场景下使用它们的讨论将在后续章节中总结。
- en: Recursive strategy
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归策略
- en: 'The recursive strategy is the oldest, most intuitive, and most popular technique
    to generate multi-step forecasts. To understand a strategy, there are two major
    regimes we have to understand:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 递归策略是生成多步预测的最古老、最直观、最流行的技术。要理解一种策略，我们需要理解两个主要的领域：
- en: '**Training regime**: How is the training of the models done?'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练方案**：模型的训练是如何进行的？'
- en: '**Forecasting regime**: How are the trained models used to generate forecasts?'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测方案**：训练好的模型如何用于生成预测？'
- en: 'Let’s take the help of a diagram to understand the recursive strategy:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们借助一个图表来理解递归策略：
- en: '![Figure 17.2 – Recursive strategy for multi-step forecasting ](img/B22389_18_02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图17.2 – 递归策略的多步预测](img/B22389_18_02.png)'
- en: 'Figure 18.2: Recursive strategy for multi-step forecasting'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.2：多步预测的递归策略
- en: Let’s discuss these regimes in detail.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论这些方案。
- en: Training regime
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练方案
- en: The recursive strategy involves training a single model to perform a *one-step-ahead*
    forecast. We can see in *Figure 18.2* that we use the window function, *W*(*Y*[t]),
    to draw a window from *Y*[t] and train the model to predict *Y*[t][+1].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 递归策略涉及训练一个单一模型执行*一步预测*。我们可以在*图18.2*中看到，我们使用窗口函数*W*(*Y*[t])，从*Y*[t]中绘制一个窗口，并训练模型预测*Y*[t][+1]。
- en: During training, a loss function (which measures the divergence between the
    output of the model, ![](img/B22389_18_001.png), and the actual value, *Y*[t][+1])
    is used to optimize the parameters of the model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，使用损失函数（衡量模型输出与实际值之间的差异，*Y*[t][+1]）来优化模型的参数。
- en: Forecasting regime
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测方案
- en: We have trained a model to do *one-step-ahead* predictions. Now, we use this
    model in a recursive fashion to generate forecasts *H* timesteps ahead. For the
    first step, we use *W*(*Y*[t]), the window using the latest timestamp in training
    data, and generate the forecast one step ahead, ![](img/B22389_18_002.png). Now,
    this generated forecast is added to the history, and a new window is drawn from
    this history, ![](img/B22389_18_003.png). This window is given as input to the
    same *one-step-ahead* model, and the forecast for the next timestep, ![](img/B22389_18_004.png),
    is generated. This process is repeated until we get forecasts for all *H* timesteps.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练了一个模型来进行*一步预测*。现在，我们以递归方式使用这个模型生成*H*时间步的预测。对于第一步，我们使用*W*(*Y*[t])，使用训练数据中最新的时间戳生成窗口，并预测一步，![](img/B22389_18_002.png)。现在，这个生成的预测被添加到历史记录中，并从这个历史记录中绘制一个新窗口，![](img/B22389_18_003.png)。这个窗口被输入到同一个*一步预测*模型中，并生成下一个时间步的预测，![](img/B22389_18_004.png)。这个过程重复进行，直到我们得到所有*H*时间步的预测。
- en: 'This is the strategy that classical models that have stood the test of time
    (such as *ARIMA* and *exponential smoothing*) use internally when they generate
    multi-step forecasts. In an ML context, this means that we will train a model
    to predict one step ahead (as we have done all through this book) and then do
    a recursive operation, where we forecast one step ahead, use the new forecast
    to recalculate all the features such as lags, rolling windows, and so on, and
    forecast the next step. The pseudocode for the method would be:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是经受时间考验的经典模型（如*ARIMA*和*指数平滑*）在生成多步预测时内部使用的策略。在机器学习的背景下，这意味着我们将训练一个模型来预测一步，然后进行递归操作，其中我们预测一步，使用新的预测重新计算所有特征，如滞后、滚动窗口等，并预测下一步。该方法的伪代码将是：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the context of the DL models, we can think of this as adding the forecast
    to the context window and using the trained model to generate the next step. The
    pseudocode for this would be:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习模型的背景下，我们可以将其视为将预测添加到上下文窗口，并使用训练好的模型生成下一步。该方法的伪代码将是：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Do note that this pseudocode is not ready-to-run code but more like a skeleton
    that you can adapt to your use case. Now, let’s look at another strategy for multi-step
    forecasting.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个伪代码不是可直接运行的代码，而更像是一个您可以根据自己情况调整的框架。现在，让我们看另一种多步预测策略。
- en: Direct strategy
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接策略
- en: 'The **direct strategy**, also called the independent strategy, is a popular
    strategy in forecasting that uses ML. This involves forecasting each horizon independently
    of each other. Let’s look at a diagram first:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**直接策略**，也称为独立策略，是一种在预测中使用机器学习的流行策略。这涉及独立地预测每个时间段。让我们先看一个图表：'
- en: '![Figure 17.3 – Direct strategy for multi-step forecasting ](img/B22389_18_03.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图17.3 – 多步预测的直接策略](img/B22389_18_03.png)'
- en: 'Figure 18.3: Direct strategy for multi-step forecasting'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.3：多步预测的直接策略
- en: Next, let’s discuss the regimes in detail.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们详细讨论这些方案。
- en: Training regime
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练方案
- en: Under the direct strategy (*Figure 18.3*), we train *H* different models, which
    take in the same window function but are trained to predict different timesteps
    in the forecast horizon. Therefore, we learn a separate set of parameters, one
    for each timestep in the horizon, such that all the models combined learn a direct
    and independent mapping from the window, *W*(*Y*[t]), to the forecast horizon,
    *H*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接策略（*图 18.3*）下，我们训练*H*个不同的模型，这些模型接受相同的窗口函数，但被训练以预测预测时间范围内的不同时间步。因此，我们为每个时间步学习一组独立的参数，使得所有模型的组合从窗口*W*(*Y*[t])到预测时间范围*H*之间学习到一个直接且独立的映射。
- en: 'This strategy has gained ground along with the popularity of ML-based time
    series forecasting. From the ML context, we can practically implement it in two
    ways:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于机器学习的时间序列预测的流行，这种策略逐渐获得了关注。从机器学习的角度来看，我们可以通过两种方式来实际实现它：
- en: '**Shifting targets**: Each model in the horizon is trained by shifting the
    target by as many steps as the horizon we train the model to forecast.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平移目标**：每个时间步的模型通过将目标平移与训练预测的时间步数一致来训练。'
- en: '**Eliminating features**: Each model in the horizon is trained by using only
    the allowable features, according to the rules. For instance, when predicting
    *H* = 2, we can’t use lag 1 (because to predict *H* = 2, we would not have actuals
    for *H* = 1).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消除特征**：每个时间步的模型通过只使用允许的特征来训练，符合规则。例如，当预测*H* = 2时，我们不能使用滞后1（因为要预测*H* = 2时，我们无法获得*H*
    = 1的实际值）。'
- en: The two ways mentioned in the preceding list work nicely if we only have lags
    as features. For instance, to eliminate features, we can just drop the offending
    lags and train the model. But in cases where we use rolling features and other
    more sophisticated features, simple dropping doesn’t work because lag 1 is already
    used to calculate the rolling features. This leads to data leakage. In such scenarios,
    we can make a dynamic function that calculates these features, taking in a parameter
    to specify the horizon we create these features for. All the helper methods we
    used in *Chapter 6*, *Feature Engineering for Time Series Forecasting* (`add_rolling_features`,
    `add_seasonal_rolling_features`, and `add_ewma`), have a parameter called `n_shift`,
    which handles this condition. If we train a model for *H* = 2, we need to pass
    `n_shift=2`, and then the method will take care of the rest. Now, while training
    the models, we use this dynamic method to recalculate these features for each
    horizon separately.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两种方法在我们仅使用滞后作为特征时非常有效。例如，为了消除特征，我们可以简单地删除不合适的滞后并训练模型。但在使用滚动特征和其他更复杂特征的情况下，简单的删除方法并不适用，因为滞后1已经用于计算滚动特征，这会导致数据泄露。在这种情况下，我们可以创建一个动态函数来计算这些特征，传入一个参数来指定我们为其创建特征的时间范围。我们在*第六章*《时间序列预测的特征工程》中使用的所有辅助方法（`add_rolling_features`、`add_seasonal_rolling_features`和`add_ewma`）都有一个名为`n_shift`的参数，用于处理这种情况。如果我们为*H*
    = 2训练模型，我们需要传入`n_shift=2`，然后该方法会处理剩余部分。在训练模型时，我们使用这个动态方法分别为每个时间范围重新计算这些特征。
- en: Forecasting regime
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测方案
- en: The forecasting regime is also fairly straightforward. We have the *H*-trained
    models, one for each timestep in the horizon, and we use *W*(*Y*[t]) to forecast
    each of them independently.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 预测方案也相当直接。我们有为每个时间步训练的*H*个模型，并使用*W*(*Y*[t])来独立预测每个模型。
- en: 'For ML models, this requires us to train separate models for each timestep,
    but `MultiOutputRegressor` from `scikit-learn` makes that a bit more manageable.
    Let’s look at some pseudocode:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习模型，这要求我们为每个时间步训练独立的模型，但`scikit-learn`中的`MultiOutputRegressor`使这一过程更加可管理。让我们看一些伪代码：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, it’s time to look at another strategy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候看看另一种策略了。
- en: The Joint strategy
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联合策略
- en: 'The previous two strategies consider a model to have a single output. This
    is the case with most ML models; we formulate the model to predict a single scalar
    value after taking in an array of inputs: **multiple input, single output** (**MISO**).
    But there are some models, such as the DL models, which can be configured to give
    us multiple output. Therefore, the joint strategy, also called **multiple input,
    multiple output** (**MIMO**), aims to learn a single model that produces the entire
    forecasting horizon as output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种策略考虑模型只有单一输出。这是大多数机器学习模型的情况；我们将模型设计为在接收一组输入后预测一个单一的标量值：**多输入单输出**（**MISO**）。但也有一些模型，如深度学习（DL）模型，可以配置为提供多个输出。因此，联合策略，也称为**多输入多输出**（**MIMO**），旨在学习一个单一模型，输出整个预测时间范围：
- en: '![Figure 17.4 – Joint strategy for multi-step forecasting ](img/B22389_18_04.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图17.4 – 多步预测的联合策略](img/B22389_18_04.png)'
- en: 'Figure 18.4: Joint strategy for multi-step forecasting'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.4：多步预测的联合策略
- en: Let’s see how these regimes work.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些方案是如何工作的。
- en: Training regime
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练方案
- en: The joint strategy involves training a single multi-output model to forecast
    all the timesteps in the horizon at once. We can see in *Figure 18.4* that we
    use the window function, *W*(*Y*[t]), to draw a window from *Y*[t] and train the
    model to predict *y*[t][+1],…, *y*[t][+][H]. During training, a loss function
    that measures the divergence between all the output of the model, ![](img/B22389_18_005.png),
    and the actual values, *y*[t][+1],…, *y*[t][+][H], is used to optimize the parameters
    of the model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 联合策略涉及训练一个单一的多输出模型，一次性预测所有时间步的结果。如*图18.4*所示，我们使用窗口函数，*W*(*Y*[t])，从*Y*[t]中提取一个窗口，并训练模型预测*y*[t][+1]，…，*y*[t][+][H]。在训练过程中，使用一个损失函数，衡量模型所有输出与实际值（*y*[t][+1]，…，*y*[t][+][H]）之间的差异，以优化模型的参数。
- en: Forecasting regime
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测方案
- en: The forecasting regime is also very simple. We have a trained model that is
    able to forecast all the timesteps in the horizon, and we use *W*(*Y*[t]) to forecast
    them at once.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 预测方案也非常简单。我们有一个经过训练的模型，可以预测整个时间范围的所有时间步，并且我们使用*W*(*Y*[t])一次性预测所有时间步。
- en: This strategy is typically used in DL models where we configure the last layer
    to output *H* scalars instead of 1.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略通常用于深度学习模型中，我们配置最后一层以输出*H*个标量，而不是1。
- en: 'We have already seen this strategy in action at multiple places in the book:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在书中的多个地方看到过这种策略的应用：
- en: The tabular regression (*Chapter 13*, *Common Modeling Patterns for Time Series*)
    paradigm can easily be extended to output the whole horizon.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格回归（*第13章*，*时间序列的常见建模模式*）范式可以轻松扩展为输出整个预测范围。
- en: We have seen *Sequence-to-Sequence* models with a *fully connected* decoder
    (*Chapter 13*, *Common Modeling Patterns for Time Series*) using this strategy
    for multi-step forecasting.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经看到过使用这种策略进行多步预测的*Sequence-to-Sequence*模型，具有*全连接*解码器（*第13章*，*时间序列的常见建模模式*）。
- en: In *Chapter 14*, *Attention and Transformers for Time Series*, we used this
    strategy to forecast using transformers.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*第14章*，*时间序列的注意力与变换器*中，我们使用该策略通过变换器进行预测。
- en: In *Chapter 16*, *Specialized Deep Learning Architectures for Forecasting*,
    we saw models such as *N-BEATS*, *N-HiTS*, and *Temporal Fusion Transformer*,
    which used this strategy to generate multi-step forecasts.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*第16章*，*用于预测的专用深度学习架构*中，我们看到像*N-BEATS*、*N-HiTS*和*Temporal Fusion Transformer*这样的模型，它们使用该策略生成多步预测。
- en: Hybrid strategies
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合策略
- en: The three strategies we have already covered are the three basic strategies
    for multi-step forecasting, each with its own merits and demerits. Over the years,
    researchers have tried to combine these as hybrid strategies that try to capture
    the good parts of each strategy. Let’s go through a few of them here. This is
    not a comprehensive list because there is none. Anyone with enough creativity
    can come up with alternate strategies, but we will just cover a few that have
    received some attention and deep study from the forecasting community.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论的三种策略是多步预测的三种基本策略，每种策略都有其优缺点。多年来，研究人员尝试将它们结合成混合策略，旨在捕捉每种策略的优点。我们将在这里讨论一些混合策略。这不是一个全面的列表，因为不存在这样的列表。任何有足够创造力的人都可以提出替代策略，但我们将只介绍一些受预测社区关注和深入研究的策略。
- en: DirRec strategy
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DirRec策略
- en: As the name suggests, the **DirRec** strategy is a combination of *direct* and
    *recursive* strategies for multi-step forecasting. One of the disadvantages of
    the direct method is that it forecasts each timestep independently and, therefore,
    loses out on some context when predicting far into the future. To rectify this
    shortcoming, we combine the direct and recursive methods by using the forecast
    generated by the *n*-step-ahead model as a feature in the *n+1*-step-ahead model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名字所示，**DirRec**策略是*直接*和*递归*策略的结合，用于多步预测。直接方法的一个缺点是它独立地预测每个时间步，因此在预测远期时会失去一些上下文。为了解决这个问题，我们通过使用*n*-步预测模型生成的预测作为*n+1*步预测模型的特征，将直接方法和递归方法结合起来。
- en: 'Let’s look at the following diagram and solidify that understanding:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下图示，并加深对这个概念的理解：
- en: '![Figure 17.5 – DirRec strategy for multi-step forecasting ](img/B22389_18_05.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图17.5 – DirRec多步预测策略](img/B22389_18_05.png)'
- en: 'Figure 18.5: DirRec strategy for multi-step forecasting'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.5：DirRec策略用于多步预测
- en: Now, let’s see how these regimes work for the DirRec strategy.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这些机制如何在DirRec策略中运作。
- en: Training regime
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练机制
- en: Similar to the direct strategy, the DirRec strategy (*Figure 18.5*) also has
    *H* models for a forecasting horizon of *H*, but with a twist. We start the process
    by using *W*(*Y*[t]) and train a model to predict one step ahead. In the recursive
    strategy, we used this forecasted timestep in the same model to predict the next
    timestep. But in DirRec, we train a separate model for *H* = 2, using the forecast
    we generated in *H* = 1\. To generalize at timestep *h* < *H*, in addition to
    *W*(*Y*[t]), we include all the forecasts generated by different models at timesteps
    1 to *h*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接策略类似，DirRec策略（*图18.5*）也有*H*个模型来进行*H*步预测，但有所不同。我们从使用*W*(*Y*[t])开始，并训练一个模型来预测一步之后的结果。在递归策略中，我们用这个预测的时间步长在同一个模型中预测下一个时间步。而在DirRec中，我们为*H*
    = 2训练一个独立的模型，使用在*H* = 1时生成的预测结果。为了在时间步*h* < *H*进行泛化，除了*W*(*Y*[t])外，我们还包括了在时间步1到*h*之间不同模型生成的所有预测结果。
- en: Forecasting regime
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测机制
- en: The forecasting regime is just like the training regime, but instead of training
    the models, we use the *H*-trained models to generate the forecasts recursively.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 预测机制就像训练机制一样，但不是训练模型，而是使用*H*训练好的模型递归地生成预测。
- en: 'Let’s take a look at some high-level pseudocode to solidify our understanding:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些高级伪代码来巩固我们的理解：
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, let’s learn about another innovative way of multi-step forecasting.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习另一种创新的多步预测方法。
- en: Iterative block-wise direct strategy
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代块状直接策略
- en: The **iterative block-wise direct** (**IBD**) strategy is also called the **iterative
    multi-SVR strategy**, paying homage to the research paper that suggested this
    (Reference *2*). The direct strategy requires *H* different models to train, and
    that makes it difficult to scale for long-horizon forecasting.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代块状直接**（**IBD**）策略也被称为**迭代多SVR策略**，以致敬提出此策略的研究论文（参考文献 *2*）。直接策略需要训练*H*个不同的模型，这使得它在长时间跨度的预测中难以扩展。'
- en: 'The IBD strategy tries to tackle that shortcoming by using a block-wise iterative
    style of forecasting:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: IBD策略尝试通过使用块状迭代的预测方式来解决这一短板：
- en: '![Figure 17.6 – IBD strategy for multi-step forecasting ](img/B22389_18_06.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图17.6 – 用于多步预测的IBD策略](img/B22389_18_06.png)'
- en: 'Figure 18.6: IBD strategy for multi-step forecasting'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.6：IBD策略用于多步预测
- en: Let’s understand the training and forecasting regimes for this strategy.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解这个策略的训练和预测机制。
- en: Training regime
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练机制
- en: In the IBD strategy, we split the forecast horizon, *H*, into *R* blocks of
    length *L*, such that *H* = *L* x *R*. Instead of training *H* direct models,
    we train *L* direct models.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在IBD策略中，我们将预测跨度*H*分成*R*个长度为*L*的块，使得*H* = *L* x *R*。我们不再训练*H*个直接模型，而是训练*L*个直接模型。
- en: Forecasting regime
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测机制
- en: While forecasting (*Figure 18.6*), we use the *L*-trained models to generate
    the forecast for the first *L* timesteps (*T* + 1 to *T* + *L*) in *H*, using
    the window, *W*(*Y*[T]). Let’s denote this *L* forecast as *Y*[T][+][L]. Now,
    we will use *Y*[T][+][L], along with *Y*[T], in the window function to draw a
    new window, *W*(*Y*[T];*Y*[T][+][L]). This new window is used to generate the
    forecast for the next *L* timesteps (*T* + *L* to *T* + 2*L*). This process is
    repeated many times to complete the full horizon forecast.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行预测时（*图18.6*），我们使用*L*个训练好的模型来生成*H*中前*L*个时间步（*T* + 1到*T* + *L*）的预测，使用窗口*W*(*Y*[T])。我们将这个*L*步的预测结果表示为*Y*[T][+][L]。然后，我们将*Y*[T][+][L]和*Y*[T]一起，用于窗口函数生成一个新的窗口*W*(*Y*[T];*Y*[T][+][L])。这个新窗口用于生成接下来的*L*个时间步（*T*
    + *L*到*T* + 2*L*）的预测。这个过程会重复多次，直到完成整个预测跨度。
- en: 'Let’s also see some high-level pseudocode for this process:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这个过程的一些高级伪代码：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, let’s move on to another creative way to hybridize different strategies.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转向另一种创造性地混合不同策略的方法。
- en: Rectify strategy
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修正策略
- en: The **rectify strategy** is another way we can combine direct and recursive
    strategies. It strikes a middle ground between the two by forming a two-stage
    training and inferencing methodology. We can see this as a model stacking approach
    (*Chapter 9*, *Ensembling and Stacking*) but between different multi-step forecasting
    strategies. In stage 1, we train a one-step-ahead model and generate recursive
    forecasts using that model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**修正策略**是我们可以结合直接和递归策略的另一种方式。它通过形成一个两阶段的训练和推断方法，在两者之间找到一个平衡点。我们可以将其视为一种模型堆叠方法（*第9章*，*集成与堆叠*），但它是应用于不同的多步预测策略。在第一阶段，我们训练一个一步预测模型，并使用该模型生成递归预测。'
- en: Then, in stage 2, we train direct models for the horizon using the original
    window and features, along with the recursive prediction.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在第二阶段，我们使用原始窗口和特征以及递归预测来训练针对预测区间的直接模型。
- en: '![Figure 17.7 – Rectify strategy for multi-step forecasting ](img/B22389_18_07.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图17.7 – 修正策略用于多步预测](img/B22389_18_07.png)'
- en: 'Figure 18.7: Rectify strategy for multi-step forecasting'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.7：多步预测的修正策略
- en: Let’s understand how this strategy works in detail.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解一下这个策略是如何运作的。
- en: Training regime
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练机制
- en: The training happens in two steps. The recursive strategy is applied to the
    horizon, and the forecast for all *H* timesteps is generated. Let’s call this
    ![](img/B22389_18_006.png). Now, we train direct models for each horizon using
    the original history, *Y*[t], and the recursive forecasts, ![](img/B22389_18_006.png),
    as input.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 训练分为两个步骤。递归策略应用于预测区间，并生成所有*H*时间步长的预测。我们称之为![](img/B22389_18_006.png)。接着，我们使用原始历史数据*Y*[t]和递归预测！[](img/B22389_18_006.png)作为输入，训练针对每个预测区间的直接模型。
- en: Forecasting regime
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测机制
- en: The forecasting regime is similar to the training, where the recursive forecasts
    are generated first, and they, along with the original history, are used to generate
    the final forecasts.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 预测机制与训练过程类似，首先生成递归预测，然后将递归预测与原始历史数据一起用于生成最终预测。
- en: 'Let’s see some high-level pseudocode for this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一下这个策略的高级伪代码：
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, let’s move on to the last strategy we will cover here.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入我们将要讨论的最后一种策略。
- en: RecJoint
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RecJoint
- en: True to its name, **RecJoint** is a mashup between the recursive and joint strategies,
    but it is applicable for multi-output models. It aims to balance the benefits
    of both by leveraging recursive forecasting, while also considering dependencies
    between multiple timesteps in the forecast horizon.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 名副其实，**RecJoint**是递归策略和联合策略的结合，但它适用于多输出模型。它通过利用递归预测的同时，考虑预测区间内多个时间步之间的依赖关系，从而平衡两者的优点。
- en: '![Figure 17.8 – RecJoint strategy for multi-step forecasting ](img/B22389_18_08.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图17.8 – RecJoint策略用于多步预测](img/B22389_18_08.png)'
- en: 'Figure 18.8: RecJoint strategy for multi-step forecasting'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.8：RecJoint策略用于多步预测
- en: The following sections detail how this strategy works.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分详细介绍了该策略的工作原理。
- en: Training regime
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练机制
- en: The training regime (*Figure 18.8*) in the RecJoint strategy is very similar
    to the recursive strategy, in the way it trains a single model and recursively
    uses prediction at *t* + 1 as input to train *t* + 2, and so on. But the recursive
    strategy trains the model on just the next timestep, whereas RecJoint generates
    the predictions for the entire horizon and jointly optimizes the entire horizon
    forecasts while training. This forces the model to look at the next *H* timesteps
    and jointly optimize the entire horizon, instead of the myopic one-step-ahead
    objective. We saw this strategy at play when we trained Seq2Seq models using an
    RNN encoder and decoder (*Chapter 13*, *Common Modeling Patterns for Time Series*).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: RecJoint策略中的训练机制（*图18.8*）与递归策略非常相似，都是训练一个模型，并通过递归使用*t* + 1时刻的预测作为输入来训练*t* +
    2时刻的预测，依此类推。但递归策略只在下一个时间步上训练模型，而RecJoint则生成整个预测区间的预测，并在训练过程中共同优化整个区间的预测。这迫使模型查看接下来的*H*个时间步，并共同优化整个预测区间，而不是短视地只关注一步之遥的目标。我们在使用RNN编码器和解码器训练Seq2Seq模型时看到了这个策略的应用（*第13章*，*时间序列的常见建模模式*）。
- en: Forecasting regime
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测机制
- en: The forecasting regime for RecJoint is exactly the same as for the recursive
    strategy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: RecJoint的预测机制与递归策略完全相同。
- en: Now that we understand a few strategies, let’s discuss their merits and demerits.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了几种策略，接下来让我们讨论它们的优点和缺点。
- en: How to choose a multi-step forecasting strategy
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何选择多步预测策略
- en: 'Let’s summarize all the different strategies that we have learned in a table:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在表格中总结一下我们已经学习的所有不同策略：
- en: '![Figure 17.9 – Multi-step forecasting strategies – a summary ](img/B22389_18_09.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图17.9-多步预测策略-摘要](img/B22389_18_09.png)'
- en: 'Figure 18.9: Multi-step forecasting strategies—a summary'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.9：多步预测策略-摘要
- en: 'Here, the following apply:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，以下内容适用：
- en: '*S.O*: Single output'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S.O*：单输出'
- en: '*M.O*: Multi-output'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*M.O*：多输出'
- en: '*T*[SO] and *I*[SO]: Training and inferencing the time of a single-output model'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T*[SO]和*I*[SO]：单输出模型的训练和推理时间'
- en: '*T*[mO] and *I*[mO]: Training and inferencing the time of a multi-output model
    (practically, *T*[mO] is larger than *T*[SO] mostly because multi-output models
    are typically DL models, and their training time is higher than standard ML models)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T*[mO]和*I*[mO]：多输出模型的训练和推理时间（实际上，*T*[mO]大多大于*T*[SO]，因为多输出模型通常是DL模型，其训练时间高于标准ML模型）'
- en: '*H*: The horizon'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*H*：地平线'
- en: '*L* = *H*/*R*, where *R* is the number of blocks in the IBD strategy'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L* = *H*/*R*，其中*R*是IBD策略中的块数'
- en: '![](img/B22389_10_002.png) is some positive real number'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B22389_10_002.png)是一些正实数'
- en: 'The table helps us understand and decide which strategy is better from multiple
    perspectives:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该表格帮助我们从多个角度理解和决定哪种策略更好：
- en: '**Engineering complexity**: *Recursive*, *Joint*, *RecJoint* << *IBD* << *Direct*,
    and *DirRec* << *Rectify*'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工程复杂性**：*递归*、*联合*、*RecJoint* << *IBD* << *直接*、*DirRec* << *校正*'
- en: '**Training time**: *Recursive* << *Joint* (typically *T*[mO] > *T*[SO]) <<
    *RecJoint* << *IBD* << *Direct*, and *DirRec* << *Rectify*'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练时间**：*递归* << *联合*（通常*T*[mO] > *T*[SO]） << *RecJoint* << *IBD* << *直接*、*DirRec*
    << *校正*'
- en: '**Inference time**: *Joint* << *Direct*, *Recursive*, *DirRec*, *IBD*, and
    *RecJoint* << *Rectify*'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理时间**：*联合* << *直接*、*递归*、*DirRec*、*IBD*和*RecJoint* << *校正*'
- en: It also helps us to decide the kind of model we can use for each strategy. For
    instance, a joint strategy can only be implemented with a model that supports
    multi-output, such as a DL model. However, we have yet to discuss how these strategies
    affect accuracies.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 它还帮助我们决定我们可以为每种策略使用哪种模型。例如，联合策略只能使用支持多输出的模型，如DL模型。然而，我们还没有讨论这些策略如何影响准确性。
- en: Although, in ML, the final word goes to empirical evidence, there are ways we
    can analyze the different methods to provide us with some guidelines. *Taieb et
    al.* analyzed the bias and variance of these multi-step forecasting strategies,
    both theoretically and using simulated data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在机器学习中，最终的结论取决于实证证据，但我们可以分析不同的方法以提供一些指导方针。*Taieb等人*从理论上和使用模拟数据分析了这些多步预测策略的偏差和方差。
- en: With this analysis, along with other empirical findings over the years, we have
    an understanding of the strengths and weaknesses of these strategies, and some
    guidelines have emerged from these findings.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种分析，以及多年来的其他实证发现，我们对这些策略的优势和劣势有了一定的了解，并且从这些发现中得出了一些指导方针。
- en: '**Reference check**:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research paper by Taieb et al. is cited in Reference *3*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Taieb等人的研究论文被引用在参考文献*3*中。
- en: '*Taieb et al.* point out several disadvantages of the recursive strategy, contrasting
    with the direct strategy, based on the bias and variance components of error analysis.
    They further corroborated these observations through an empirical study.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*Taieb等人*指出递归策略的几个缺点，与基于误差分析的偏差和方差成分对比，与直接策略形成对比。他们通过实证研究进一步证实了这些观察结果。'
- en: 'The key points that elucidate the difference in performance are as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 阐明性能差异的关键点如下：
- en: For the recursive strategy, the bias and variance components of error in step
    *h* = 1 affect step *h* = 2\. Because of this phenomenon, the errors that a recursive
    model makes tend to accumulate as we move further in the forecast horizon. But
    for the direct strategy, this dependence is not explicit and, therefore, doesn’t
    suffer the same deterioration that we see in the recursive strategy. This was
    also seen in the empirical study, where the recursive strategy was very erratic
    and had the highest variance, which increased significantly as we moved further
    in the horizon.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于递归策略，步骤*h* = 1中的误差的偏差和方差成分会影响步骤*h* = 2。由于这种现象，递归模型的错误会随着预测地平线的进一步移动而累积。但对于直接策略，这种依赖关系不明显，因此不会像递归策略中看到的那样受到相同的恶化。这也在实证研究中看到，递归策略非常不稳定，方差最高，随着地平线的进一步移动而显著增加。
- en: For the direct strategy, the bias and variance components of error in step *h*
    = 1 do not affect *h* = 2\. This is because each horizon, *h*, is forecasted in
    isolation. A downside of this approach is the fact that this strategy can produce
    completely unrelated forecasts across the horizon, leading to unrealistic forecasts.
    The complex dependencies that may exist between the forecast in the horizon are
    not captured in the direct strategy. For instance, a direct strategy on a time
    series with a non-linear trend may result in a broken curve because of the independence
    of each timestep in the horizon.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于直接策略，*h* = 1 步骤中的误差的偏差和方差成分不会影响 *h* = 2。这是因为每个预测期 *h* 都是孤立预测的。这种方法的一个缺点是，它可能会在不同预测期之间生成完全不相关的预测，导致不现实的预测。直接策略无法捕捉预测期之间可能存在的复杂依赖关系。例如，在具有非线性趋势的时间序列上使用直接策略可能会导致曲线断裂，因为预测期内每个时间步的独立性。
- en: Practically, in most cases, a direct strategy produces coherent forecasts.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际上，在大多数情况下，直接策略产生的一致性预测效果较好。
- en: The bias for the recursive strategy is also amplified when the forecasting model
    produces forecasts that have large variations. Highly complex models are known
    to have low bias but a high amount of variations, and these high variations seem
    to amplify the bias for recursive strategy models.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当预测模型产生的预测具有较大变动时，递归策略的偏差也会被放大。已知复杂模型具有低偏差，但有较大的变动，这些较大的变动似乎会放大递归策略模型的偏差。
- en: When we have very large datasets, the bias term of the direct strategy becomes
    zero, but the recursive strategy bias is still non-zero. This was further demonstrated
    in experiments—for long time series, the direct strategy almost always outperformed
    the recursive strategy. From a learning theory perspective, we learn *H* functions
    using the data for the direct strategy, whereas for recursive, we just learn one.
    So with the same amount of data, it is harder to learn *H* true functions than
    one. This is amplified in low-data situations.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们有非常大的数据集时，直接策略的偏差项会变为零，而递归策略的偏差仍然不为零。实验进一步证明了这一点——对于长时间序列，直接策略几乎总是优于递归策略。从学习理论的角度来看，直接策略使用数据学习
    *H* 个函数，而递归策略只学习一个函数。因此，在相同数据量的情况下，学习 *H* 个真实函数比学习一个函数更为困难。这在数据量较少的情况下尤为突出。
- en: 'Although the recursive strategy seems inferior to the direct strategy theoretically
    and empirically, it is not without some advantages:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管递归策略在理论和实证上似乎不如直接策略，但它并非没有一些优势：
- en: For highly non-linear and noisy time series, learning direct functions for all
    the horizons can be hard. In such situations, recursive can work better.
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于高度非线性和噪声较大的时间序列，学习所有预测期的直接函数可能会很困难。在这种情况下，递归策略可能表现得更好。
- en: If the underlying **data-generating process** (**DGP**) is very smooth and can
    be easily approximated, the recursive strategy can work better.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果基础的**数据生成过程**（**DGP**）非常平滑且容易逼近，递归策略可能表现得更好。
- en: When the time series is shorter, the recursive strategy can work better.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当时间序列较短时，递归策略可能表现得更好。
- en: We talked about the direct strategy generating possible unrelated forecasts
    for the horizon, but this is exactly the part that the joint strategy takes care
    of. The joint strategy can be thought of as an extension of the direct strategy,
    but instead of having *H* different models, we have a single model that produces
    *H* output. We learn a single function instead of *H* functions from the given
    data. Therefore, the joint strategy doesn’t have the same weakness as the direct
    strategy in short time series.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们提到过直接策略可能会生成预测期之间不相关的预测，但正是联合策略解决了这一问题。联合策略可以被看作是直接策略的扩展，但它并不是拥有 *H* 个不同的模型，而是一个模型产生
    *H* 个输出。我们从给定数据中学习一个函数，而不是 *H* 个函数。因此，联合策略不会像直接策略那样在短时间序列中表现出同样的弱点。
- en: One of the weaknesses of the joint strategy (and RecJoint) is the high bias
    on very short horizons (such as *H* = 2, *H* = 3, and so on). We learn a model
    that optimizes across all the *H* timesteps in the horizon using a standard loss
    function, such as the mean squared error. But these errors are at different scales.
    The errors that can occur further down the horizon are larger than the immediate
    ones, and this implicitly puts more weight on the longer horizons; thus, the model
    learns a function that is skewed toward getting the longer horizons right.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联合策略（和RecJoint）的一个弱点是对于非常短的预测期（如 *H* = 2, *H* = 3 等）的高偏差。我们学习了一个模型，该模型使用标准损失函数（如均方误差）在整个
    *H* 时间步内进行优化。但这些误差的尺度是不同的。随着时间延续，可能发生的误差会比短期的误差大，这隐式地使得模型更加重视较长的预测期；因此，模型学习到的函数倾向于使较长的预测期更加准确。
- en: The joint and RecJoint strategies are comparable from a variance perspective.
    However, the joint strategy can give us a lower bias because the RecJoint strategy
    learns a recursive function, and it may not be flexible enough to capture the
    pattern. The joint strategy uses the full power of the forecasting model to directly
    forecast the horizon.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联合策略和RecJoint策略从方差的角度来看是可比的。然而，联合策略可以给我们带来更低的偏差，因为RecJoint策略学习的是递归函数，可能没有足够的灵活性来捕捉模式。联合策略充分利用预测模型的全部能力来直接预测未来的时间段。
- en: Hybrid strategies, such as DirRec, IBD, and so on, try to balance the merits
    and demerits of fundamental strategies, such as direct, recursive, and joint.
    With these merits and demerits, we can create an informed experimentation framework
    to come up with the best strategy for the problem at hand.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 混合策略，如DirRec、IBD等，试图平衡基础策略（如直接预测、递归预测和联合预测）的优缺点。通过这些优缺点，我们可以创建一个有根据的实验框架，以便为当前问题找出最佳策略。
- en: Summary
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we touched upon a particular aspect of forecasting that is
    highly relevant for real-world use cases but rarely talked about and studied.
    We saw why we needed multi-step forecasting and then went on to review a few popular
    strategies we can use. We explored the popular and fundamental strategies, such
    as direct, recursive, and joint, and then went on to look at a few hybrid strategies,
    such as DirRec, rectify, and so on. Finally, we looked at the merits and demerits
    of these strategies and discussed a few guidelines for selecting the right strategy
    for your problem.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们探讨了一个与实际应用密切相关但很少被讨论和研究的预测问题。我们了解了为何需要多步预测，并随后回顾了几种可以使用的流行策略。我们探讨了如直接预测、递归预测和联合预测等流行且基础的策略，接着又分析了几种混合策略，如DirRec、rectify等。最后，我们讨论了这些策略的优缺点，并提出了选择适合问题的策略的一些指南。
- en: In the next chapter, we will look at another important aspect of forecasting—evaluation.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨预测的另一个重要方面——评估。
- en: References
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The following is the list of the references that we used throughout the chapter:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在本章中使用的参考文献列表：
- en: 'Taieb, S.B., Bontempi, G., Atiya, A.F., and Sorjamaa, A. (2012). *A review
    and comparison of strategies for multi-step ahead time series forecasting based
    on the NN5 forecasting competition*. Expert Syst. Appl., 39, 7067–7083: [https://arxiv.org/pdf/1108.3259.pdf](https://arxiv.org/pdf/1108.3259.pdf)'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Taieb, S.B., Bontempi, G., Atiya, A.F., and Sorjamaa, A. (2012). *基于NN5预测竞赛的多步时间序列预测策略综述与比较*。专家系统应用，39，7067–7083:
    [https://arxiv.org/pdf/1108.3259.pdf](https://arxiv.org/pdf/1108.3259.pdf)'
- en: 'Li Zhang, Wei-Da Zhou, Pei-Chann Chang, Ji-Wen Yang, and Fan-Zhang Li. (2013).
    *Iterated time series prediction with multiple support vector regression models.*
    Neurocomputing, Volume 99, 2013: [https://www.sciencedirect.com/science/article/pii/S0925231212005863](https://www.sciencedirect.com/science/article/pii/S0925231212005863)'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Li Zhang, Wei-Da Zhou, Pei-Chann Chang, Ji-Wen Yang, 和 Fan-Zhang Li. (2013).
    *基于多个支持向量回归模型的迭代时间序列预测*。神经计算，2013年第99卷: [https://www.sciencedirect.com/science/article/pii/S0925231212005863](https://www.sciencedirect.com/science/article/pii/S0925231212005863)'
- en: 'Taieb, S.B. and Atiya, A.F. (2016). *A Bias and Variance Analysis for Multistep-Ahead
    Time Series Forecasting.* in IEEE Transactions on Neural Networks and Learning
    Systems, vol. 27, no. 1, pp. 62–76, Jan. 2016: [https://ieeexplore.ieee.org/document/7064712](https://ieeexplore.ieee.org/document/7064712)'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Taieb, S.B. and Atiya, A.F. (2016). *多步时间序列预测的偏差与方差分析*。IEEE神经网络与学习系统学报，2016年1月，第27卷，第1期，62–76页:
    [https://ieeexplore.ieee.org/document/7064712](https://ieeexplore.ieee.org/document/7064712)'
- en: Join our community on Discord
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们在Discord上的社区
- en: 'Join our community’s Discord space for discussions with authors and other readers:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mts](https://packt.link/mts)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mts](https://packt.link/mts)'
- en: '![](img/QR_Code15080603222089750.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code15080603222089750.png)'
