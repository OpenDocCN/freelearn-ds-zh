- en: '*Chapter 1*: An Overview of Modern Data Science'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第一章*：现代数据科学概述'
- en: Data science has its roots in the early eighteenth century and has gained tremendous
    popularity during the last couple of decades.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学起源于 18 世纪初，并在过去几十年里获得了巨大的流行。
- en: In this book, you will learn how to run a data science project within Azure,
    the Microsoft public cloud infrastructure. You will gain all skills needed to
    become a certified Azure Data Scientist Associate. You will start with this chapter,
    which gives some foundational terminology used throughout the book. Then, you
    will deep dive into **Azure Machine Learning** (**AzureML**) services. You will
    start by provisioning a workspace. You will then work on the no-code, low-code
    experiences build in the AzureML Studio web interface. Then, you will deep dive
    into the code-first data science experimentation, working with the AzureML **Software
    Development Kit** (**SDK**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，你将学习如何在 Azure 上运行数据科学项目，Azure 是微软的公共云基础设施。你将掌握成为认证 Azure 数据科学家助理所需的所有技能。你将从本章开始，了解书中贯穿的基础术语。然后，你将深入学习**Azure
    机器学习**（**AzureML**）服务。你将首先配置工作区，然后在 AzureML Studio 网页界面中体验无代码、低代码的构建。接着，你将深入学习以代码为主的数据科学实验，使用
    AzureML **软件开发工具包**（**SDK**）进行工作。
- en: In this chapter, you will learn some fundamental data science-related terms
    needed for the DP 100 exam. You will start by understanding the typical life cycle
    of a data science project. You will then read about big data and how Apache Spark
    technology enables you to train machine learning models against them. Then, you
    will explore what the **DevOps** mindset is and how it can help you become a member
    of a highly efficient, multi-disciplinary, agile team that builds machine learning-enhanced
    products.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习一些与数据科学相关的基本术语，这些术语是 DP 100 考试所需的。你将从了解数据科学项目的典型生命周期开始。接着，你将阅读有关大数据的内容，并了解
    Apache Spark 技术如何帮助你训练机器学习模型。然后，你将探索什么是**DevOps**思维方式，以及它如何帮助你成为一个高效、多学科、敏捷团队的成员，这个团队构建了增强机器学习的产品。
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: The evolution of data science
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学的演变
- en: Working on a data science project
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行数据科学项目
- en: Using Spark in data science
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据科学中使用 Spark
- en: Adopting the DevOps mindset
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采纳 DevOps 思维方式
- en: The evolution of data science
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学的演变
- en: If you try to find the roots of the data science practices, you will probably
    end up discovering evidence at the beginning of civilization. In the eighteenth
    century, governments were gathering demographic and financial data for taxation
    purposes, a practice called **statistics**. As years progressed, the use of this
    term was expanded to include the summarization and analysis of the data collected.
    In 1805, Adrien-Marie Legendre, a French mathematician, published a paper describing
    the **least squares** to fit linear equations, although most people credit Carl
    Friedrich Gauss for the complete description he published a couple of years later.
    In 1900, Karl Pearson published in the *Philosophical Magazine* his observations
    on the **chi-square** statistic, a cornerstone in data science for hypothesis
    testing. In 1962, John Tukey, the scientist famous for the **fast Fourier transformation**
    and the **box plot**, published a paper expressing his passion for data analysis
    and how statistics needed to evolve into a new science.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你试图追溯数据科学实践的根源，你可能会发现它的证据可以追溯到文明的初期。18 世纪，政府开始收集人口和财务数据用于征税，这一做法被称为**统计学**。随着时间的推移，这个术语的使用范围扩展到包括对收集的数据进行总结和分析。1805
    年，法国数学家阿德里安-玛丽·勒让德发布了一篇论文，描述了用**最小二乘法**拟合线性方程，尽管大多数人将对该方法的完整描述归功于卡尔·弗里德里希·高斯，他在几年后发表了相关内容。1900
    年，卡尔·皮尔逊在《哲学杂志》上发布了关于**卡方**统计量的观察，这是数据科学中假设检验的基石。1962 年，因**快速傅里叶变换**和**箱型图**而闻名的科学家约翰·图基，发表了一篇论文，表达了他对数据分析的热情，以及统计学如何需要演变成一门新的科学。
- en: On the other hand, with the rise of informatics in the middle of the twentieth
    century, the field of **Artificial Intelligence** (**AI**) was introduced in 1955
    by John McCarthy as the official term for thinking machines. AI is a field of
    computer science that develops systems that can imitate intelligent human behavior.
    Using programming languages such as **Information Processing Language** (**IPL**)
    and **LISt Processor** (**LISP**), developers were writing programs that could
    manipulate lists and various other data structures to solve complex problems.
    In 1955, Arthur Samuel's checkers player was the first piece of software that
    would *learn* from the games it has already played by storing board states and
    the chance of winning if ending up in that state in a cache. This checkers program
    may have been the first example of **machine learning**, a subfield of AI that
    utilizes historical data and the patterns encoded in the data to train models
    and enable systems to mimic human tasks without explicitly coding the entire logic.
    In fact, you can think of machine learning models as software code that is generated
    by training an algorithm against a dataset to recognize certain types of patterns.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，随着二十世纪中叶信息学的兴起，**人工智能**（**AI**）领域由约翰·麦卡锡（John McCarthy）于1955年首次提出，作为思考机器的官方术语。人工智能是计算机科学的一个领域，旨在开发可以模仿人类智能行为的系统。使用诸如**信息处理语言**（**IPL**）和**列表处理器**（**LISP**）等编程语言，开发人员编写了能够操纵列表和其他各种数据结构以解决复杂问题的程序。在1955年，阿瑟·塞缪尔（Arthur
    Samuel）的跳棋程序是第一款能够通过存储棋盘状态并缓存在该状态下获胜的几率来从已经进行过的游戏中*学习*的程序。这个跳棋程序可能是**机器学习**（人工智能的一个子领域）的第一个例子，机器学习利用历史数据和数据中编码的模式来训练模型，使系统能够模仿人类任务，而无需显式编写整个逻辑。事实上，你可以将机器学习模型看作是通过对数据集训练算法以识别特定类型模式所生成的软件代码。
- en: In 2001, William S. Cleveland published the first article in which the term
    **data science** was used in the way we refer to it today, a science at the intersection
    of statistics, data analysis, and informatics that tries to explain phenomena
    based on data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 2001年，威廉·S·克利夫兰（William S. Cleveland）发表了第一篇文章，其中使用了“**数据科学**”这一术语，这一术语至今仍被我们沿用，指的是处于统计学、数据分析和信息学交叉点的科学，旨在通过数据解释现象。
- en: Although most people correlate data science with machine learning, data science
    has a much broader scope, which includes the analysis and preparation of data
    before the actual machine learning model training process, as you will see in
    the next section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数人将数据科学与机器学习相关联，但数据科学的范围要广泛得多，它包括在实际机器学习模型训练过程之前的数据分析和准备工作，正如你将在下一节中看到的那样。
- en: Working on a data science project
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从事数据科学项目
- en: 'A data science project aims to infuse an application with intelligence extracted
    from data. In this section, you will discover the common tasks and key considerations
    needed within such a project. There are quite a few well-established life cycle
    processes, such as **Team Data Science Process** (**TDSP**) and **Cross-Industry
    Standard Process for Data Mining** (**CRISP-DM**), that describe the iterative
    stages executed in a typical project. The most common stages are shown in *Figure
    1.1*:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学项目的目标是将从数据中提取的智能融入应用程序中。在本节中，你将发现该类项目中所需的常见任务和关键考虑事项。有许多成熟的生命周期过程，例如**团队数据科学过程**（**TDSP**）和**跨行业标准数据挖掘过程**（**CRISP-DM**），它们描述了在典型项目中执行的迭代阶段。最常见的阶段如*图
    1.1*所示：
- en: '![Figure 1.1 – The iterative stages of a data science project'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.1 – 数据科学项目的迭代阶段'
- en: '](img/B16777_01_001.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_01_001.jpg)'
- en: Figure 1.1 – The iterative stages of a data science project
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 数据科学项目的迭代阶段
- en: Although the diagram shows some indicative flows between the phases, you are
    free to jump from one phase to any other if needed. Moreover, this approach is
    iterative, and the data science team should go through multiple iterations, improving
    its business understanding and the resulting model until the success criteria
    are met. You will read more about the benefits of an iterative process in this
    chapter's *Adopting the DevOps mindset* section. The data science process starts
    from the business understanding phase, something you will read more about in the
    next section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图表展示了各阶段之间的某些指示性流动，但如果需要，你可以自由地从一个阶段跳转到任何其他阶段。此外，这种方法是迭代的，数据科学团队应当经过多次迭代，不断改进其业务理解和最终模型，直到满足成功标准。在本章的*采用
    DevOps 思维模式*部分，你将进一步了解迭代过程的好处。数据科学过程从业务理解阶段开始，这是你将在下一节中详细阅读的内容。
- en: Understanding of the business problem
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解业务问题
- en: The first stage in a data science project is that of business understanding.
    In this stage, the data science team collaborates with the business stakeholders
    to define a short, straightforward question that machine learning will try to
    answer.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学项目的第一阶段是业务理解阶段。在这个阶段，数据科学团队与业务利益相关者合作，定义一个简短而直接的问题，机器学习将尝试回答。
- en: '*Figure 1.2* shows the five most frequent questions that machine learning can
    answer:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.2* 显示了机器学习可以回答的五个最常见问题：'
- en: '![Figure 1.2 – Five questions machine learning can answer'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.2 – 机器学习可以回答的五个问题'
- en: '](img/B16777_01_002.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_01_002.jpg)'
- en: Figure 1.2 – Five questions machine learning can answer
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 机器学习可以回答的五个问题
- en: 'Behind each of those questions, there is a group of modeling techniques you
    will use:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些问题背后，有一组建模技术你将使用：
- en: '**Regression** models allow you to predict a numeric value based on one or
    more features. For example, in [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*, you will be trying to predict a numeric value
    based on 10 measurements that were taken one year before the value you are trying
    to predict. Training a regression model is a **supervised** machine learning task,
    meaning that you need to provide enough sample data to train the model to predict
    the desired numeric value.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**模型允许你基于一个或多个特征预测一个数值。例如，在[*第8章*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117)《用Python代码进行实验》中，你将尝试基于一年之前采集的10个测量值，预测你希望获得的数值。训练一个回归模型是一个**监督式**的机器学习任务，这意味着你需要提供足够的样本数据来训练模型，以预测期望的数值。'
- en: '**Classification** models allow you to predict a class label for a given set
    of inputs. This label can be as simple as a yes/no label or a blue, green, or
    red color. For example, in [*Chapter 5*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072),
    *Letting the Machines Do the Model Training*, you will be training a classification
    model to detect whether a customer is going to cancel their phone subscription
    or not. Predicting whether a person is going to stop doing something is referred
    to as **churn** or attrition detection. Training a classification model is a supervised
    machine learning task and requires a labeled dataset to train the model. A labeled
    dataset contains both the inputs and the label that you want the model to predict.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**模型允许你根据一组输入预测一个类标签。这个标签可以是简单的“是/否”标签，也可以是蓝色、绿色或红色等颜色标签。例如，在[*第5章*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072)《让机器进行模型训练》中，你将训练一个分类模型，检测客户是否会取消他们的手机订阅。预测一个人是否会停止做某件事，通常被称为**流失**或退订检测。训练一个分类模型是一个监督式机器学习任务，需要一个带标签的数据集来训练模型。带标签的数据集包含了输入和你希望模型预测的标签。'
- en: '**Clustering** is an **unsupervised** machine learning task that groups data.
    In contrast to the previous two model types, clustering doesn''t require any training
    data. It operates on the given dataset and creates the desired number of clusters,
    assigning each data point to the collection it belongs. A common use case of clustering
    models is when you try to identify distinct consumer groups in your customer base
    that you will be targeting with specific marketing campaigns.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**是一个**无监督**的机器学习任务，用于对数据进行分组。与前两种模型类型不同，聚类不需要任何训练数据。它直接作用于给定的数据集，并创建所需数量的集群，将每个数据点分配到相应的集群中。聚类模型的常见应用是当你试图识别你客户基础中的不同消费群体，以便你可以通过特定的营销活动来针对这些群体。'
- en: '**Recommender** systems are designed to recommend the best options based on
    user profiles. Search engines, e-shops, and popular video streaming platforms
    utilize this type of model to produce personalized recommendations on what to
    do next.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐系统**旨在基于用户的个人资料推荐最佳选项。搜索引擎、电子商店和流行的视频流媒体平台利用这种类型的模型来提供个性化的推荐，帮助用户决定接下来该做什么。'
- en: '**Anomaly detection** models can detect outliers from a dataset or within a
    data stream. Outliers are items that don''t belong with the rest of the elements,
    indicating anomalies. For example, if a vibration sensor of a machine starts sending
    abnormal measurements, it may be a good indication that the device is about to
    fail.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测**模型可以从数据集或数据流中检测异常值。异常值是指那些与其他元素不相符的项目，表示存在异常。例如，如果一台机器的振动传感器开始发送异常测量值，这可能是设备即将故障的良好指示。'
- en: During the business understanding phase, you will try to understand the problem
    statement and define the success criteria. Setting up proper expectations of what
    machine learning can and cannot do is key to ensure alignment between teams.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在业务理解阶段，你将尝试理解问题陈述并定义成功标准。设定机器学习能够和不能做到的适当预期是确保团队之间一致性的关键。
- en: Throughout a data science project, it is common to have multiple rounds of business
    understandings. The data science team acquires a lot of insights after exploring
    datasets or training a model. It is helpful to bring those gathered insights to
    the business stakeholders and either verify your team's hypothesis or gain even
    more insights into the problem you are tackling. For example, business stakeholders
    may explain a pattern that you may detect in the data but cannot explain its rationale.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学项目中，通常会有多轮的业务理解。数据科学团队在探索数据集或训练模型后会获得很多洞察。将这些洞察带给业务利益相关者，验证你团队的假设，或深入了解你正在解决的问题是非常有帮助的。例如，业务利益相关者可能会解释你在数据中检测到的模式，但你却无法解释其背后的原因。
- en: Once you get a good grasp of what you are trying to solve, you need to get some
    data, explore it, and even label it, something you will read about in the next
    section.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你掌握了要解决的问题，接下来你需要获取一些数据，进行探索，甚至为其打上标签，更多内容将在下一节中讲解。
- en: Acquiring and exploring the data
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取和探索数据
- en: After understanding the problem you are trying to solve, it's time to gather
    the data to support the machine learning process. Data can have many forms and
    formats. It can be either well-structured tabular data stored in database systems
    or even files, such as images, stored in file shares. Initially, you will not
    know which data to collect, but you must start from somewhere. A typical anecdote
    while looking for data is the belief that there is always an Excel file that will
    contain critical information, and you must keep asking for it until you find it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了你要解决的问题后，接下来是收集支持机器学习过程的数据。数据可以有多种形式和格式。它既可以是存储在数据库系统中的结构化表格数据，也可以是存储在文件共享中的文件，如图像等。起初，你可能不知道要收集哪些数据，但你必须从某个地方开始。寻找数据时，一个典型的轶事是，总觉得总有一个Excel文件包含了关键信息，你必须不断地去请求，直到找到它。
- en: Once you have located the data, you will have to analyze it to understand whether
    the dataset is complete or not. Data is often stored within on-premises systems
    or **Online Transactional Processing** (**OLTP**) databases that you cannot easily
    access. Even if data is accessible, it is not advised to explore it directly within
    the source system, as you may accidentally impact the performance of the underlying
    engine that hosts the data. For example, a complex query on top of a sales table
    may affect the performance of the e-shop solution. In these cases, it is common
    to export the required datasets in a file format, such as the most interoperable
    **Comma-Separated Values** (**CSV**) format or the much more optimized for analytical
    processing **Parquet** format. These files are then uploaded to cheap cloud storage
    and become available for further analysis.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你找到了数据，你就需要分析它，了解数据集是否完整。数据通常存储在本地系统中，或者存储在**在线事务处理**（**OLTP**）数据库中，你无法轻易访问。即使数据可以访问，也不建议直接在源系统中探索它，因为你可能会意外影响承载数据的底层引擎的性能。例如，针对销售表的复杂查询可能会影响电子商店解决方案的性能。在这种情况下，通常会将所需的数据集导出为某种文件格式，比如最具互操作性的**逗号分隔值**（**CSV**）格式，或者更加优化用于分析处理的**Parquet**格式。然后将这些文件上传到便宜的云存储中，供进一步分析使用。
- en: Within Microsoft Azure, the most common target is either a Blob container within
    a storage account or a folder in the filesystem of **Azure Data Lake Gen 2**,
    which offers a far more granular access control mechanism. Copying the data can
    be done in a one-off manner by using tools such as **AzCopy** or **Storage Explorer**.
    If you would like to configure a repeatable process that could pull incrementally
    new data on a schedule, you can use more advanced tools such as the pipelines
    of **Azure Data Factory** or **Azure Synapse Analytics**. In [*Chapter 4*](B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053),
    *Configuring the Workspace*, you will review the components needed to pull data
    from on-premises and the available datastores to which you can connect from within
    the AzureML workspace to access the various datasets. In the *Working with datasets*
    section of [*Chapter 4*](B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053), *Configuring
    the Workspace*, you will read about the dataset types supported by AzureML and
    how you can explore them to gain insights into the info stored within them.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Microsoft Azure 中，最常见的目标要么是存储帐户中的 Blob 容器，要么是 **Azure Data Lake Gen 2** 文件系统中的文件夹，它提供了更加细粒度的访问控制机制。数据复制可以通过使用如
    **AzCopy** 或 **Storage Explorer** 等工具以一次性方式进行。如果你想要配置一个可以定期拉取增量新数据的重复过程，你可以使用更高级的工具，如
    **Azure Data Factory** 或 **Azure Synapse Analytics** 的管道。在 [*第 4 章*](B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053)中，*配置工作区*，你将回顾从本地提取数据所需的组件以及你可以从
    AzureML 工作区中连接的可用数据存储，以便访问各种数据集。在 [*第 4 章*](B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053)的
    *与数据集一起工作* 部分，你将了解 AzureML 支持的数据集类型，以及如何探索它们以深入了解其中存储的信息。
- en: A common task when gathering data is the data cleansing step. You remove duplicate
    records, impute missing values, or fix common data entry issues during this step.
    For example, you could harmonize a country text field by replacing *UK* records
    with *United Kingdom*. Within AzureML, you can perform such cleansing operations
    either in the designer that you will see in [*Chapter 6*](B16777_06_Final_VK_ePub.xhtml#_idTextAnchor084),
    *Visual Model Training and Publishing*, or through the notebooks experience you
    will be working with from [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102),
    *The AzureML Python SDK*, onward. Although you may start doing those cleansing
    operations with AzureML, as the project matures, these cleansing activities tend
    to move within the pipelines of **Azure Data Factory** or **Azure Synapse Analytics**,
    which pulls the data out of the source systems.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 收集数据时，一个常见的任务是数据清理步骤。在这个过程中，你需要删除重复记录、填补缺失值或修复常见的数据输入问题。例如，你可以通过将 *UK* 记录替换为
    *United Kingdom* 来统一国家文本字段。在 AzureML 中，你可以在 [*第 6 章*](B16777_06_Final_VK_ePub.xhtml#_idTextAnchor084)
    中看到的设计器中，或通过你将从 [*第 7 章*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102) 起使用的笔记本体验来执行这些清理操作。尽管你可能会在
    AzureML 中开始进行这些清理操作，但随着项目的推进，这些清理活动通常会移到 **Azure Data Factory** 或 **Azure Synapse
    Analytics** 的管道中，这些管道将数据从源系统中提取出来。
- en: Important note
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: While doing data cleansing, be aware of **yak shaving**. The term *yak shaving*
    was coined in the 90s to describe the situation where, while working on a task,
    you realize that you must do another task, which leads to another one, and so
    on. This chain of tasks may take you away from your original goal. For example,
    you may realize that some records have invalid encoding on the country text field
    example, but you can understand the referenced country. You decide to change the
    export encoding of the CSV file, and you realize that the export tool you were
    using is old and doesn't support UTF-8\. That leads you to a quest to find a system
    administrator to get your software updated. Instead of going down that route,
    make a note of what needs to be done and add it to your backlog. You can fix this
    issue in the next iteration when you will have a better understanding of whether
    you actually need this field or not.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行数据清理时，要注意 **yak shaving**。术语 *yak shaving* 在 90 年代被创造出来，用来描述一种情形：在处理某项任务时，你意识到必须做另一个任务，这又引发了另一个任务，以此类推。这一系列任务可能会让你偏离原定目标。例如，你可能会发现一些记录在国家文本字段中存在无效编码，但你可以理解参考的国家。于是你决定更改
    CSV 文件的导出编码，但你意识到你使用的导出工具已经过时，不支持 UTF-8。这时你开始寻找系统管理员来更新你的软件。与其沿着这个方向走，不如记下需要做的事情，并将其添加到待办事项中。你可以在下次迭代时修复这个问题，那时你会更清楚自己是否真的需要这个字段。
- en: Another common task is labeling the dataset, especially if you will be dealing
    with supervised machine learning models. For example, if you are curating a dataset
    to predict whether a customer will churn or not, you will have to flag the records
    of the customers that canceled their subscriptions. A more complex labeling case
    is when you create a sentiment analysis model for social media messages. In that
    case, you will need to get a feed of messages, go through them, and assign a label
    on whether it is a positive or negative sentiment.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的任务是对数据集进行标注，特别是当您需要处理监督学习模型时。例如，如果您正在整理一个数据集来预测客户是否会流失，您需要标记取消订阅的客户记录。一个更复杂的标注案例是当您为社交媒体消息创建情感分析模型时。在这种情况下，您需要获取一批消息，逐一浏览，并为每条消息标记其情感是正面还是负面。
- en: 'Within AzureML Studio, you can create labeling projects that allow you to scale
    the labeling efforts of datasets. AzureML allows you to define either a text labeling
    or an image labeling task. You then bring in team members to label the data based
    on the given instructions. Once the team has started labeling the data, AzureML
    automatically trains a model relative to your defined task. When the model is
    good enough, it starts providing suggestions to the labelers to improve their
    productivity. *Figure 1.3* shows the labeling project creation wizard and the
    various options available currently in the image labeling task:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AzureML Studio 中，您可以创建标注项目，允许您对数据集的标注工作进行扩展。AzureML 允许您定义文本标注或图像标注任务。然后，您可以邀请团队成员根据给定的指示来标注数据。一旦团队开始标注数据，AzureML
    会自动训练一个相应的模型。当模型足够好时，它开始向标注者提供建议，从而提高生产力。*图 1.3* 显示了标注项目创建向导以及当前图像标注任务中的各种选项：
- en: '![Figure 1.3 – Creating an AzureML labeling project'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.3 – 创建 AzureML 标注项目'
- en: '](img/B16777_01_003.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_01_003.jpg)'
- en: Figure 1.3 – Creating an AzureML labeling project
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 创建 AzureML 标注项目
- en: Through this project phase, you should have discovered the related source systems
    and produced a cleansed dataset ready for the machine learning training. In the
    next section, you will learn how to create additional data features that will
    assist the model training process, a process known as **feature engineering**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过此项目阶段，您应已发现相关的源系统，并生成了一个准备好用于机器学习训练的清理数据集。在下一部分，您将学习如何创建其他数据特征，帮助模型训练过程，这一过程称为
    **特征工程**。
- en: Feature engineering
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: 'During the feature engineering phase, you will be generating new data features
    that will better represent the problem you are trying to solve and help machines
    learn from the dataset. For example, the following code block creates a new feature
    named `product_id` by transforming the `product` column of the sales dataset:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征工程阶段，您将生成新的数据特征，以更好地表示您要解决的问题，并帮助机器从数据集中学习。例如，以下代码块通过转换销售数据集中的 `product`
    列创建一个名为 `product_id` 的新特征：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code block uses the `map` method to convert text into numerical values.
    The `product` column is referred to as being a `orange juice` or `lemonade juice`.
    If you had a 1-to-5 rating feature in the same dataset, that would have been a
    discrete numeric variable with a finite number of values that it can take, in
    this case, only *1*, *2*, *3*, *4*, or *5*. If you had a column that kept how
    many liters or gallons the customer bought, that would have been a **continuous**
    numeric variable that could take any numeric value greater than or equal to zero,
    such as half a liter. Besides numeric values, dates fields are also considered
    as continuous variables.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码块使用 `map` 方法将文本转换为数值。`product` 列被称为 `橙汁` 或 `柠檬水`。如果您在同一数据集中有一个 1 到 5 的评级特征，那将是一个离散的数值变量，能够取的值是有限的，在本例中只有
    *1*、*2*、*3*、*4* 或 *5*。如果您有一个记录客户购买了多少升或多少加仑的列，那么它将是一个 **连续** 数值变量，可以取任何大于或等于零的数值，例如半升。除了数值，日期字段也被视为连续变量。
- en: Important note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Although the `product_id` feature is a **discrete** numeric variable in the
    preceding example, features such as that are commonly treated as a categorical
    variable, as you will see in [*Chapter 5*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072),
    *Letting the Machines Do the Model Training*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在前面的示例中，`product_id` 特征是一个 **离散** 数值变量，但这类特征通常被当作分类变量来处理，正如您将在 [*第 5 章*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072)《让机器来进行模型训练》中看到的那样。
- en: 'There are many featurization techniques available. An indicative list is as
    follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可用的特征化技术。以下是一个示例列表：
- en: '**Scaling of numeric features**: This technique converts all numeric features
    into ranges that can be easily compared. For example, in [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*, you will be training a machine learning model
    on top of medical measurements. Blood glucose measurements range from 80 to 200
    mg/dL, while blood pressure measurements range from 60 to 128 mm Hg. These numeric
    values are scaled down using their mean value, a transformation referred to as
    standardization or **Z-score** normalization. Their values end up within the -1
    to 1 range, which allows machines to extract better insights.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值特征的标准化**：此技术将所有数值特征转换为可以轻松比较的范围。例如，在[*第8章*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117)，*使用
    Python 代码进行实验*中，你将基于医学测量数据训练机器学习模型。血糖测量值范围从 80 到 200 mg/dL，而血压测量值范围从 60 到 128
    mm Hg。这些数值通过它们的均值进行标准化，这种转换称为标准化或**Z-score**归一化。它们的数值最终会在 -1 到 1 的范围内，这样机器就能提取出更好的洞察。'
- en: '**Split**: Splitting a column into two new features is something very common.
    For example, the full name will be split into name and surname for further analysis.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拆分**：将一列拆分为两个新特征是非常常见的做法。例如，全名会被拆分为姓名和姓氏以便进一步分析。'
- en: '**Binning**: This technique groups continuous features into distinct groups
    or bins that may expose important information regarding the problem you are trying
    to solve. For example, if you have the year of birth, you can create bins to group
    the different generations. In this case, folks with a year of birth between 1965
    and 1980 would have been the *generation X* group, and people in the 1981 to 1996
    range would have formed the *millennial* bin. It is common to use the clustering
    models that you saw in the *Understanding of the business problem* section to
    produce cohorts and define those bins.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分箱**：此技术将连续特征分组为不同的组或箱，这可能揭示出有关你正在解决的问题的重要信息。例如，如果你有出生年份数据，可以创建分箱来将不同的代际群体分组。在这种情况下，出生年份在
    1965 到 1980 年之间的人将属于*X 代*群体，而出生在 1981 到 1996 年间的人则属于*千禧一代*分组。通常会使用你在*理解商业问题*部分中看到的聚类模型来生成群体并定义这些分箱。'
- en: '`product`, you performed a label encoding. You converted the categorical variable
    into a numeric one. A typical example for label encoding is t-shirt sizes where
    you convert small to *1*, medium to *2*, and large to *3*. In the `product` example
    though, you accidentally defined the order between `orange juice` (`1`) and `lemonade
    juice` (`2`), which may confuse a machine learning algorithm. In this case, instead
    of the ordinal encoding used in the example that produced the `product_id` feature,
    you could have utilized one-hot encoding. In this case, you would introduce two
    binary features called *orange_juice* and *lemonade_juice*. These features would
    accept either *0* or *1* values, depending on which juice the customer bought.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `product`，你执行了标签编码。你将类别变量转换为数值变量。标签编码的一个典型例子是 T 恤尺码，其中小号对应*1*，中号对应*2*，大号对应*3*。然而，在
    `product` 例子中，你不小心定义了 `橙汁`（`1`）和 `柠檬汁`（`2`）之间的顺序，这可能会让机器学习算法感到困惑。在这种情况下，除了使用示例中产生
    `product_id` 特征的顺序编码，你还可以使用独热编码。在这种情况下，你将引入两个二进制特征，分别为*orange_juice*和*lemonade_juice*。这些特征将根据顾客购买的是哪种果汁，接受*0*或*1*的值。
- en: '**Generate lag features**: If you deal with time-series data, you may need
    to produce features from values from preceding time. For example, if you are trying
    to forecast the temperature 10 minutes from now, you may need to have the current
    temperature and the temperature 30 minutes ago and 1 hour ago. These two additional
    past temperatures are lag features that you will have to engineer.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成滞后特征**：如果你处理的是时间序列数据，可能需要从前一个时间点的值生成特征。例如，如果你尝试预测 10 分钟后的温度，你可能需要当前的温度以及
    30 分钟前和 1 小时前的温度。这两个额外的过去温度就是滞后特征，你需要对它们进行特征工程。'
- en: Important note
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: Making all those transformations in big datasets may require a tremendous amount
    of memory and processing time. This is where technologies such as Spark come into
    play to parallelize the process. You will learn more about Spark in the *Using
    Spark in data science* section of this chapter.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对大数据集进行所有这些转换可能需要大量的内存和处理时间。这时，像 Spark 这样的技术就发挥了作用，通过并行化处理过程。你将在本章的*使用 Spark
    进行数据科学*部分了解更多关于 Spark 的内容。
- en: In [*Chapter 10*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147), *Understanding
    Model Results*, you will use the `MinMaxScaler` method from the `sklearn` library
    to scale numeric features.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第10章*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147)中，*理解模型结果*部分，你将使用来自`sklearn`库的`MinMaxScaler`方法对数值特征进行缩放。
- en: As a last step in the feature engineering stage, you normally remove unnecessary
    or highly correlated features, a process called **feature selection**. You will
    be dropping columns that will not be used to train the machine learning model.
    By dropping those columns, you reduce the memory requirements of the machines
    that will be doing the training, you reduce the computation time needed to train
    the model, and the resulting model will be much smaller in size.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作为特征工程阶段的最后一步，你通常会移除不必要或高度相关的特征，这个过程称为**特征选择**。你将删除那些不会用于训练机器学习模型的列。通过删除这些列，你减少了训练时机器的内存需求，减少了训练模型所需的计算时间，同时生成的模型体积也会更小。
- en: While creating those features, it is logical that you may need to go back to
    the *Acquiring and exploring the data* phase or even to the *Understanding of
    the business problem* stage to get more data and insights. At some point, though,
    your training dataset will be ready to train the model, something you will read
    about in the next section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建这些特征时，你可能需要返回到*数据获取和探索*阶段，甚至回到*理解业务问题*阶段，以获取更多的数据和见解。然而，某一时刻，你的训练数据集将准备好训练模型，这将在下一节中介绍。
- en: Training the model
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'As soon as you have prepared the dataset, the machine learning training process
    can begin. If the model requires supervised learning and you have enough data,
    you split them into a training dataset and validation dataset in a 70% to 30%
    or 80% to 20% ratio. You select the model type you want to train, specify the
    model''s training parameters (called **hyperparameters**), and train the model.
    With the remaining validation dataset, you evaluate the trained model''s performance
    according to a **metric** and you decide whether the model is good enough to move
    to the next stage, or perhaps return to the *Understanding of the business problem*
    stage. The training process of a supervised model is depicted in *Figure 1.4*:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你准备好数据集，机器学习的训练过程就可以开始。如果模型需要监督学习且你有足够的数据，你可以将数据集按70%对30%或80%对20%的比例分为训练集和验证集。你选择要训练的模型类型，指定模型的训练参数（称为**超参数**），并训练模型。使用剩余的验证数据集，你根据**指标**评估训练模型的性能，并决定该模型是否足够好，可以进入下一个阶段，或者是否需要返回到*理解业务问题*阶段。监督模型的训练过程如*图
    1.4*所示：
- en: '![Figure 1.4 – Training a supervised machine learning model'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.4 – 训练一个监督学习模型](img/B16777_01_004.jpg)'
- en: '](img/B16777_01_004.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_01_004.jpg)'
- en: Figure 1.4 – Training a supervised machine learning model
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 训练一个监督学习模型
- en: 'There are a couple of variations to the preceding statement:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤有几种变体：
- en: If the model is in the unsupervised learning category, such as the clustering
    algorithms, you just pass all the data to train the model. You then evaluate whether
    the detected clusters address the business need or not, modify the hyperparameters,
    and try again.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型属于无监督学习类别，例如聚类算法，你只需将所有数据传入训练模型。然后评估检测到的聚类是否满足业务需求，调整超参数，再次尝试。
- en: 'If you have a model that requires supervised learning but don''t have enough
    data, the **k-fold cross validation** technique is commonly used. With k-fold,
    you specify the number of folds you want to split the dataset. AzureML uses **AutoML**
    and performs either 10 folds if the data is less than 1,000 rows or 3 folds if
    the dataset is between 1,000 and 20,000 rows. Once you have those folds, you start
    an iterative process where you do the following:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有一个需要监督学习的模型，但没有足够的数据，通常使用**k折交叉验证**技术。通过k折交叉验证，你可以指定要将数据集拆分成的折数。AzureML使用**AutoML**，如果数据少于1,000行，则执行10折验证；如果数据集在1,000到20,000行之间，则执行3折验证。一旦你有了这些折，你就开始一个迭代过程，具体步骤如下：
- en: Keep a fold away for validation and train with the rest of the folds a new model.
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留一个折用于验证，并使用其余的折来训练一个新的模型。
- en: Evaluate the produced model against the fold that you kept out.
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用保留的折对训练的模型进行评估。
- en: Record the model score and discard the model.
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录模型得分并丢弃模型。
- en: Repeat *step I* by keeping another fold away for validation until all folds
    have been used for validation.
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤I*，将另一个折保留用于验证，直到所有折都用于验证。
- en: Produce the aggregated model's performance.
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成聚合模型的性能。
- en: Important note
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: In the machine learning research literature, there is an approach called **semi-supervised**
    learning. In that approach, a small amount of labeled data is combined with a
    large amount of unlabeled data to train the model.
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在机器学习研究文献中，有一种方法叫做**半监督学习**。这种方法将少量标记数据与大量未标记数据结合起来训练模型。
- en: Instead of training a single model, evaluating the results, and trying again
    with a different set of hyperparameters, you can automate the process and evaluate
    multiple models in parallel. This process is called hyperparameter tuning, something
    you will dive deep into in [*Chapter 9*](B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136),
    *Optimizing the ML Model*. In the same chapter, you will learn how you can even
    automate the model selection, an AzureML capability referred to as AutoML.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 与其训练一个单一的模型，评估结果后再尝试使用不同的超参数集，你可以自动化这个过程，并行评估多个模型。这个过程叫做超参数调优，你将在[*第9章*](B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136)，*优化机器学习模型*中深入了解。在同一章中，你还将学习如何自动化模型选择，这是一种AzureML的功能，称为AutoML。
- en: '**Metrics** help you select the model that minimizes the difference between
    the predicted value and the actual one. They differ depending on the model type
    you are training. In regression models, metrics try to minimize the error between
    the predicted value and the actual one. The most common ones are **Mean Absolute
    Error** (**MAE**), **Root Mean** **Squared Error** (**RMSE**), **Relative Squared
    Error** (**RSE**), **Relative Absolute** **Error** (**RAE**), the **coefficient
    of determination** (**R²**), and **Normalized Root Mean Squared Error** (**NRMSE**),
    which you are going to see in [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**度量标准**帮助你选择使预测值与实际值之间差异最小的模型。它们因训练的模型类型而有所不同。在回归模型中，度量标准试图最小化预测值与实际值之间的误差。最常见的度量标准有**平均绝对误差**（**MAE**）、**均方根误差**（**RMSE**）、**相对平方误差**（**RSE**）、**相对绝对误差**（**RAE**）、**决定系数**（**R²**）和**归一化均方根误差**（**NRMSE**），这些你将在[*第8章*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117)，*使用Python代码进行实验*中看到。'
- en: 'In a classification model, metrics are slightly different, as they have to
    evaluate both how many results it got right and how many it misclassified. For
    example, in the churn binary classification problem, there are four possible results:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类模型中，度量标准略有不同，因为它们不仅要评估模型正确预测的结果数量，还要评估错误分类的数量。例如，在客户流失的二分类问题中，可能有四种结果：
- en: The model predicted that the customer would churn, and the customer churned.
    This is considered a **True Positive** (**TP**).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型预测客户会流失，且客户确实流失了。这被认为是**真阳性**（**TP**）。
- en: The model predicted that the customer would churn, but the customer remained
    loyal. This is considered a **False Positive** (**FP**), since the model was wrong
    about the customer leaving.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型预测客户会流失，但客户保持忠诚。这被认为是**假阳性**（**FP**），因为模型错误地预测了客户会流失。
- en: The model predicted that the customer would not churn, and the customer churned.
    This is considered a **False Negative** (**FN**), since the model was wrong about
    the customer being loyal.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型预测客户不会流失，但客户流失了。这被认为是**假阴性**（**FN**），因为模型错误地判断了客户会忠诚。
- en: The model predicted that the customer would not churn, and the customer remained
    loyal. This is considered a **True Negative** (**TN**).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型预测客户不会流失，且客户保持忠诚。这被认为是**真阴性**（**TN**）。
- en: 'These four states make up the **confusion matrix** that is shown in *Figure
    1.5*:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这四种情况构成了**混淆矩阵**，如*图1.5*所示：
- en: '![Figure 1.5 – The classification model''s evaluation'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.5 – 分类模型的评估'
- en: '](img/B16777_01_005.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_01_005.jpg)'
- en: Figure 1.5 – The classification model's evaluation
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – 分类模型的评估
- en: Through that confusion matrix, you can calculate other metrics, such as **accuracy**,
    which calculates the total number of correct results in the evaluation test (in
    this case, **1132** TP + **2708** TN = 3840 records versus **2708** + **651**
    + **2229** + **1132** = 6720 total records). On the other hand, **precision**
    or **Positive Predictive Value** (**PPV**) evaluates how many true predictions
    are actually true (in this case, **1132** TP versus **1132** + **2229** total
    true predictions). **Recall**, also known as **sensitivity**, measures how many
    actual true values were correctly classified (in this case, **1132** TP versus
    **1132** + **651** total true actuals). Depending on the business problem you
    are trying to solve, you will have to find the balance between the various metrics,
    as one metric may be more helpful than others. For example, during the COVID-19
    pandemic, a model that determines whether someone is infected with recall equal
    to one would identify all infected patients. However, it may have accidentally
    misclassified some of the not-infected ones, which other metrics, such as precision,
    would have caught.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过混淆矩阵，你可以计算其他指标，例如**准确率**，它计算评估测试中所有正确结果的总数（在此案例中，**1132** TP + **2708** TN
    = 3840 条记录与 **2708** + **651** + **2229** + **1132** = 6720 条总记录）。另一方面，**精确度**或**阳性预测值**
    (**PPV**) 评估实际正确的预测有多少（在此案例中，**1132** TP 与 **1132** + **2229** 总的正确预测）。**召回率**，也称为**敏感性**，衡量有多少实际的真实值被正确分类（在此案例中，**1132**
    TP 与 **1132** + **651** 总的实际真实值）。根据你要解决的业务问题，你需要在各种指标之间找到平衡，因为某些指标可能比其他指标更有帮助。例如，在
    COVID-19 大流行期间，一个模型如果通过召回率为 1 来确定某人是否感染了病毒，会识别出所有感染的患者。然而，它也可能错误地分类了一些未感染的患者，而其他指标，如精确度，则能够捕捉到这一点。
- en: Important note
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Be aware when your model fits your data too well. This is something that we
    refer to as **overfitting**, and it may indicate that the model has identified
    a certain pattern within your training dataset that may not exist in real life.
    Such models tend to perform poorly when put into production and make inferences
    on top of unknown data. A common reason for overfitting is a biased training dataset
    that exposes only a subset of real-world examples. Another reason is target leakage,
    which means that somehow the value you are trying to predict is passed as an input
    to the model, perhaps through a feature engineered using the target column. See
    the *Further reading* section for guidance on how to handle overfitting and imbalanced
    data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的模型过于拟合数据时，请留意。这是我们所说的**过拟合**，它可能表示模型在训练数据集中找到了某种在现实生活中不存在的模式。此类模型在投入生产并对未知数据进行推理时，往往表现不佳。过拟合的常见原因是偏倚的训练数据集，它仅暴露了真实世界样本的一部分。另一个原因是目标泄露，即某种方式下，试图预测的值被作为输入传递给模型，可能是通过一个使用目标列工程化的特征。有关如何处理过拟合和不平衡数据的指导，请参阅*进一步阅读*部分。
- en: As you have seen so far, there are many things to consider while training a
    machine learning model, and throughout this book, you will get some hands-on experience
    in training models. In most cases, the first thing you will have to select is
    the type of computer that is going to run the training process. Currently, you
    have two options, **Central Processing Unit** (**CPU**) or **Graphics Processing
    Unit** (**GPU**) compute targets. Both targets have at least a CPU in them, as
    this is the core element of any modern computer. The difference is that the GPU
    compute targets also offer some very powerful graphic cards that can perform massive
    parallel data processing, making training much faster. To take advantage of the
    GPU, the model you are training needs to support GPU-based training. GPU is usually
    used in neural network training with frameworks such as **TensorFlow**, **PyTorch**,
    and **Keras**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你迄今所见，训练机器学习模型时需要考虑许多因素，在本书中，你将获得一些训练模型的实战经验。在大多数情况下，你首先需要选择将要执行训练过程的计算机类型。目前，你有两个选择，**中央处理单元**（**CPU**）或**图形处理单元**（**GPU**）计算目标。这两种目标至少都包含一个
    CPU，因为这是任何现代计算机的核心元素。不同之处在于，GPU 计算目标还提供一些非常强大的显卡，可以执行大规模的并行数据处理，从而加速训练过程。为了利用
    GPU，你训练的模型需要支持基于 GPU 的训练。GPU 通常用于神经网络训练，使用的框架包括**TensorFlow**、**PyTorch** 和**Keras**。
- en: Once you have trained a machine learning model that satisfies the success criteria
    defined during the *Understanding of the business problem* stage of the data science
    project, it is time to operationalize it and start making inferences with it.
    That's what you will read about in the next section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练了一个满足在数据科学项目的*理解业务问题*阶段定义的成功标准的机器学习模型，就该将其操作化，并开始使用它进行推理了。你将在下一节阅读到这一部分内容。
- en: Deploying the model
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署模型
- en: 'When it comes to model operationalization, you have two main approaches:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型操作化方面，你有两种主要方法：
- en: '**Real-time inferences**: The model is always loaded, waiting to make inferences
    on top of incoming data. Typical use cases are web and mobile applications that
    invoke a model to predict based on user input.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时推理**：模型始终处于加载状态，等待对传入数据进行推理。典型的使用场景是网页和移动应用程序，这些应用会调用模型来根据用户输入进行预测。'
- en: '**Batch inferences**: The model is loaded every time the batch process is invoked,
    and it generates predictions on top of the incoming batch of records. For example,
    imagine that you have trained a model to identify your face in pictures and you
    want to label all the images you have on your hard drive. You will configure a
    process to use the model against each image, storing the results in a text or
    CSV file.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理推理**：每次调用批处理过程时，都会加载模型，并基于传入的批记录生成预测。例如，假设你训练了一个模型来识别照片中的你的面部，并且你想要为硬盘上的所有图片打上标签。你将配置一个过程，使用模型对每张图片进行处理，并将结果存储在文本或CSV文件中。'
- en: The main difference between these two is whether you already have the data to
    perform the predictions or not. If you already have the data and they do not change,
    you can make inferences in batch mode. For example, if you are trying to predict
    the football scores for next week's matches, you can run a batch inference and
    store the results in a database. When customers ask for specific predictions,
    you can retrieve the value from the database. During the football match, though,
    the model predicting the end score needs features such as the current number of
    players and how many injuries there are, information that will become available
    in real time. In those situations, you might want to deploy a web service that
    exposes a REST API, where you send in the required information and the model is
    making the real-time inference. You will dive deep into both real-time and batch
    approaches in [*Chapter 12*](B16777_12_Final_VK_ePub.xhtml#_idTextAnchor171),
    *Operationalizing Models with Code*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者之间的主要区别在于你是否已经拥有用于预测的数据。如果你已经拥有数据且这些数据不发生变化，你可以进行批处理推理。例如，如果你想预测下周比赛的足球比分，你可以运行批处理推理并将结果存储在数据库中。当客户请求特定的预测时，你可以从数据库中提取结果。然而，在足球比赛进行时，预测最终比分的模型需要实时获取诸如当前球员数量和受伤情况等特征信息。对于这些情况，你可能想要部署一个暴露REST
    API的Web服务，在该服务中，你发送所需的信息，模型则进行实时推理。你将在[*第12章*](B16777_12_Final_VK_ePub.xhtml#_idTextAnchor171)，*通过代码操作化模型*中深入了解实时和批处理方法。
- en: In this section, you reviewed the project life cycle of a data science project
    and went through all the stages, from understanding what needs to be done all
    the way to operationalizing a model by deploying a batch or real-time service.
    Especially for real-time streaming, you may have heard the term **structured streaming**,
    a scalable processing engine built on Spark to allow developers to perform real-time
    inferences the same way they would perform batch inference on top of static data.
    You will learn more about Spark in the next section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你回顾了数据科学项目的生命周期，并逐步了解了所有阶段，从理解需要做什么到通过部署批处理或实时服务来实现模型操作化。特别是在实时流处理中，你可能听说过**结构化流处理**这一术语，它是基于Spark构建的一个可扩展的处理引擎，允许开发者像在静态数据上执行批处理推理一样执行实时推理。你将在下一节学习更多关于Spark的内容。
- en: Using Spark in data science
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在数据科学中使用Spark
- en: At the beginning of the twenty-first century, the big data problem became a
    reality. Data stored in data centers was growing in volumes and velocity. In 2021,
    we refer to datasets as big data when they reach at least a couple of terabytes
    in size, while it is not uncommon to see even petabytes of data in large organizations.
    These datasets increase at a rapid rate, which can be from a couple of gigabytes
    per day to even per minute, for example, when you are storing user interactions
    with a website in an online store to perform clickstream analysis.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在21世纪初，大数据问题成为现实。数据中心存储的数据在数量和速度上迅速增长。到2021年，我们将数据集称为大数据，当它们的大小达到至少几个TB时，甚至在大型组织中，PB级的数据也并不罕见。这些数据集以非常快的速度增长，可能是每天几GB，甚至是每分钟增长的速度，例如，当你在在线商店中存储用户与网站的交互数据以进行点击流分析时。
- en: In 2009, a research project started at the University of California, Berkeley,
    trying to provide the parallel computing tools needed to handle big data. In 2014,
    the first version of Apache Spark was released from this research project. Members
    from that research team founded the **Databricks** company, one of the most significant
    contributors to the open source Apache Spark project.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 2009年，加利福尼亚大学伯克利分校启动了一个研究项目，旨在提供处理大数据所需的并行计算工具。2014年，Apache Spark的第一个版本从该研究项目中发布。该研究团队的成员创办了**Databricks**公司，这是Apache
    Spark开源项目的重要贡献者之一。
- en: 'Apache Spark provides an easy-to-use scalable solution that allows people to
    perform parallel processing on top of data in a distributed manner. The main idea
    behind the Spark architecture is that a driver node is responsible for executing
    your code. Your code is split into smaller parallel actions that can be performed
    against smaller portions of data. These smaller jobs are scheduled to be executed
    by the worker nodes, as seen in *Figure 1.6*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark提供了一个易于使用的可扩展解决方案，允许人们以分布式的方式对数据进行并行处理。Spark架构的核心思想是，驱动节点负责执行你的代码。你的代码被拆分成多个可以在更小数据部分上执行的并行任务。这些小任务会被调度到工作节点执行，如*图1.6*所示：
- en: '![Figure 1.6 – Parallel processing of big data in a Spark cluster'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.6 – 在 Spark 集群中并行处理大数据'
- en: '](img/B16777_01_006.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_01_006.jpg)'
- en: Figure 1.6 – Parallel processing of big data in a Spark cluster
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 – 在 Spark 集群中并行处理大数据
- en: For example, suppose you wanted to calculate how many products your company
    sold during the last year. In that case, Spark could spin up 12 jobs that would
    produce the monthly aggregates, and then the results would be processed by another
    job that would sum up the totals for all months. If you were tempted to load the
    entire dataset into memory and perform those aggregates directly from there, let's
    examine how much memory you would need within that computer. Let's assume that
    the sales data for a single month is stored in a CSV file that is 1 GB. This file
    will require approximately 10 GB of memory to load. The compressed `pandas.``DataFrame`
    object. As you can understand, loading all 12 files in memory simultaneously is
    an impossible task. You need to parallelize the processing, something Spark can
    do for you automatically.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你想计算公司在去年销售了多少产品。在这种情况下，Spark可以启动12个任务，分别生成每月的汇总数据，然后这些结果将由另一个任务处理，汇总所有月份的总数。如果你倾向于将整个数据集加载到内存中并直接进行汇总，我们来看看你需要多少内存。假设单月的销售数据存储在一个1GB的CSV文件中。加载这个文件大约需要10GB的内存。压缩后的`pandas.``DataFrame`对象。正如你能理解的那样，同时加载所有12个文件到内存中是一个不可能完成的任务。你需要并行处理，而Spark可以自动为你完成这个工作。
- en: Important note
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The Parquet files are stored in a columnar format, which allows you to load
    partially any number of columns you need. In the 1 GB Parquet example, if you
    load only half the columns from the dataset, you will probably need only 20 GB
    of memory. This is one of the reasons why the Parquet format is widely used in
    analytical loads.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件以列式格式存储，这使得你可以仅加载需要的部分列。例如，在1GB的Parquet文件中，如果你只加载数据集的一半列，那么你可能只需要20GB的内存。这也是Parquet格式在分析负载中广泛使用的原因之一。
- en: Spark is written in the Scala programming language. It offers APIs for Scala,
    Python, Java, R, and even C#. Still, the data science community is either working
    on Scala to achieve maximum computational performance and utilizing the Java library
    ecosystem or Python, which is widely adopted by the modern data science community.
    When you are writing Python code to utilize the Spark engine, you are using the
    PySpark tool to perform operations on top of `Spark.DataFrame` objects introduced
    later in Spark Framework. To benefit from the distributed nature of Spark, you
    need to be handling big datasets. This means that Spark may be overkill if you
    deal with only hundreds of thousands of records or even a couple of millions of
    records.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是用 Scala 编程语言编写的。它为 Scala、Python、Java、R，甚至 C# 提供了 API。不过，数据科学社区通常在使用 Scala
    时，致力于达到最大计算性能并利用 Java 库生态系统，或者使用 Python，这也是现代数据科学社区广泛采用的语言。当你编写 Python 代码来利用 Spark
    引擎时，你是在使用 PySpark 工具对后来在 Spark 框架中引入的`Spark.DataFrame`对象进行操作。为了从 Spark 的分布式特性中获益，你需要处理大量数据集。这意味着，如果你处理的记录仅为几十万条甚至几百万条，Spark
    可能会显得过于复杂。
- en: Spark offers two machine learning libraries, the old `Spark.DataFrame` structure,
    a distributed collection of data, and offers similar functionality to the `DataFrame`
    objects used in Python pandas or R. Moreover, the `pandas.DataFrame` manipulations
    to use their existing coding skills on top of Spark.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了两个机器学习库，旧的`Spark.DataFrame`结构，这是一个分布式数据集合，并提供与 Python pandas 或 R 中使用的`DataFrame`对象类似的功能。此外，`pandas.DataFrame`的操作允许你利用现有的编码技能在
    Spark 上进行操作。
- en: AzureML allows you to execute Spark jobs on top of PySpark, either using its
    native compute clusters or by attaching to **Azure Databricks** or **Synapse Spark
    pools**. Although you will not write any PySpark code in this book, in [*Chapter
    12*](B16777_12_Final_VK_ePub.xhtml#_idTextAnchor171), *Operationalizing Models
    with Code*, you will learn how to achieve similar parallelization benefits without
    the need for Spark or a driver node.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: AzureML 允许你在 PySpark 上执行 Spark 任务，既可以使用其原生计算集群，也可以通过连接到**Azure Databricks**或**Synapse
    Spark 池**来实现。尽管本书中你不会编写任何 PySpark 代码，但在 [*第 12 章*](B16777_12_Final_VK_ePub.xhtml#_idTextAnchor171)，《使用代码实现模型操作》中，你将学习如何在不需要
    Spark 或驱动节点的情况下实现类似的并行化效果。
- en: No matter whether you are coding in regular Python, PySpark, R, or Scala, you
    are producing some code artifacts that are probably part of a larger system. In
    the next section, you will explore the DevOps mindset, which emphasizes the communication
    and collaboration of software engineers, data scientists, and system administrators
    to achieve faster release of valuable product features.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是在普通的 Python、PySpark、R 还是 Scala 中编程，你都在生成一些代码工件，这些代码很可能是更大系统的一部分。在下一部分，你将探索
    DevOps 思维模式，该模式强调软件工程师、数据科学家和系统管理员之间的沟通与协作，以实现更快的有价值产品特性的发布。
- en: Adopting the DevOps mindset
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采纳 DevOps 思维模式
- en: DevOps is a team mindset that tries to minimize the silos between developers
    and system operators to shorten the development life cycle of a product. Developers
    are constantly changing a product to introduce new features and modify existing
    behaviors. On the other side, system operators need to keep the production systems
    stable and up and running. In the past, these two groups of people were isolated,
    and developers were *throwing* the new piece of software over to the operations
    team who would try to deploy it in production. As you can imagine, things didn't
    work that well all the time, causing frictions between those two groups. When
    it comes to DevOps, one fundamental practice is that a team needs to be autonomous
    and should contain all required disciplines, both *developers* and *operators*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps 是一种团队思维模式，旨在尽量减少开发人员和系统操作员之间的隔阂，从而缩短产品的开发生命周期。开发人员不断修改产品以引入新特性和修改现有行为。另一方面，系统操作员需要保持生产系统的稳定并确保其正常运行。在过去，这两个团队是孤立的，开发人员常常是将新的软件部分“抛”给运营团队，后者会尝试将其部署到生产环境中。正如你所想，事情并不总是那么顺利，导致这两个团队之间出现摩擦。说到
    DevOps，一个基本的实践是，团队需要具备自主性，并应包含所有所需的学科，包括*开发人员*和*操作员*。
- en: 'When it comes to data science, some people refer to the practice as **MLOps,**
    but the fundamental ideas remain the same. A team should be self-sufficient, capable
    of developing all required components for the overall solution, from the data
    engineering parts that bring in data and the training of the models all the way
    to operationalizing the model in production. These teams usually work in an **agile**
    manner, which embraces an iterative approach, seeking constant improvement based
    on feedback, as seen in *Figure 1.7*:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学领域，一些人将这项实践称为**MLOps**，但其基本理念保持不变。一个团队应该是自给自足的，能够开发出整体解决方案所需的所有组件，从带入数据的数据工程部分、模型的训练，一直到在生产环境中运营化模型。这些团队通常以**敏捷**的方式工作，采用迭代方法，根据反馈寻求持续改进，正如*图1.7*所示：
- en: '![Figure 1.7 – The feedback flow in an agile MLOps team'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.7 – 敏捷MLOps团队中的反馈流程'
- en: '](img/B16777_01_007.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_01_007.jpg)'
- en: Figure 1.7 – The feedback flow in an agile MLOps team
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 – 敏捷MLOps团队中的反馈流程
- en: The MLOps team operates on its backlog and performs the iterative steps you
    saw in the *Working on a data science project* section. Once the model is ready,
    the system administrators, who are part of the team, are aware of what needs to
    be done to take the model into production. The model is monitored closely, and
    if a defect or performance degradation is observed, a backlog item is created
    for the MLOps team to address in their next sprint.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps团队根据其待办事项执行迭代步骤，正如你在*数据科学项目工作*部分看到的那样。一旦模型准备好，作为团队一部分的系统管理员会知道需要做些什么来将模型投入生产。模型被密切监控，如果发现缺陷或性能下降，则为MLOps团队创建一个待办事项，安排在下一个冲刺中处理。
- en: In order to minimize the development and deployment life cycle of new features
    in production, automation needs to be embraced. The goal of a DevOps team is to
    minimize the number of human interventions in the deployment process and automate
    as many repeatable tasks as possible.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化生产中新增特性的开发和部署生命周期，需要拥抱自动化。DevOps团队的目标是尽量减少部署过程中人工干预的次数，并自动化尽可能多的可重复任务。
- en: '*Figure 1.8* shows the most frequently used components while developing real-time
    models using the MLOps mindset:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.8*显示了使用MLOps思维方式开发实时模型时最常用的组件：'
- en: '![Figure 1.8 – Components usually seen in MLOps-driven data science projects'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.8 – MLOps驱动的数据科学项目中常见的组件'
- en: '](img/B16777_01_008.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_01_008.jpg)'
- en: Figure 1.8 – Components usually seen in MLOps-driven data science projects
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 – MLOps驱动的数据科学项目中常见的组件
- en: 'Let''s analyze those components:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析这些组件：
- en: '**ARM templates** allow you to automate the deployment of Azure resources.
    This enables the team to spin up and down development, testing, or even production
    environments in no time. These artifacts are stored within Azure DevOps in a Git
    version-control repository. The deployment of multiple environments is automated
    using Azure DevOps pipelines. You are going to read about ARM templates in [*Chapter
    2*](B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026), *Deploying Azure Machine
    Learning Workspace Resources*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ARM模板**允许你自动化Azure资源的部署。这使得团队可以快速启动和关闭开发、测试，甚至生产环境。这些构件存储在Azure DevOps的Git版本控制仓库中。通过Azure
    DevOps管道，多个环境的部署得以自动化。你将会在[*第2章*](B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026)中阅读关于ARM模板的内容，*部署Azure机器学习工作区资源*。'
- en: Using **Azure Data Factory**, the data science team orchestrates the pulling
    and cleansing of the data from the source systems. The data is copied within a
    data lake, which is accessible from the AzureML workspace. Azure Data Factory
    uses ARM templates to define its orchestration pipelines, templates that are stored
    within the Git repository to track changes and be able to deploy in multiple environments.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Azure Data Factory**，数据科学团队协调从源系统提取和清洗数据。数据被复制到数据湖中，可以从AzureML工作区访问。Azure
    Data Factory使用ARM模板定义其协调管道，这些模板存储在Git仓库中，以便跟踪变更并能够在多个环境中部署。
- en: 'Within the AzureML workspace, data scientists are working on their code. Initially,
    they start working on Jupyter notebooks. Notebooks are a great way to prototype
    some ideas, as you will see in [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102),
    *The AzureML Python SDK*. As the project progresses, the scripts are exported
    from the notebooks and are organized into coding scripts. All those code artifacts
    are version-controlled into Git, using the terminal and commands such as the ones
    seen in *Figure 1.9*:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AzureML 工作区内，数据科学家正在编写他们的代码。最初，他们开始使用 Jupyter 笔记本。笔记本是一个很好的原型设计工具，正如你在[*第7章*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102)，《AzureML
    Python SDK》中所看到的那样。随着项目的进展，脚本会从笔记本中导出，并整理成编码脚本。所有这些代码制品都会通过 Git 进行版本控制，使用终端和如*图1.9*所示的命令：
- en: '![Figure 1.9 – Versioning a notebook and a script file using Git within AzureML'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.9 – 在 AzureML 中使用 Git 对笔记本和脚本文件进行版本控制'
- en: '](img/B16777_01_009.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_01_009.jpg)'
- en: Figure 1.9 – Versioning a notebook and a script file using Git within AzureML
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 – 在 AzureML 中使用 Git 对笔记本和脚本文件进行版本控制
- en: When a model is trained, if it is performing better than the model that is currently
    in production, it is registered within AzureML, and an event is emitted. This
    event is captured by the AzureML DevOps plugin, which triggers the automatic deployment
    of the model in the test environment. The model is tested within that environment,
    and if all tests pass and no errors have been logged in **Application Insights**,
    which is monitoring the deployment, the artifacts can be automatically deployed
    to the next environment, all the way to production.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个模型经过训练后，如果其性能优于当前生产环境中的模型，它会在 AzureML 中注册，并触发一个事件。这个事件会被 AzureML DevOps 插件捕获，从而触发模型在测试环境中的自动部署。模型会在该环境中进行测试，如果所有测试通过且在**Application
    Insights**中未记录任何错误（该工具用于监控部署），则可以将制品自动部署到下一个环境，直到生产环境。
- en: The ability to ensure both code and model quality plays a crucial role in this
    automation process. In Python, you can use various tools, such as Flake8, Bandit,
    and Black, to ensure code quality, check for common security issues, and consistently
    format your code base. You can also use the `pytest` framework to write your functional
    testing, where you will be testing the model results against a golden dataset.
    With `pytest`, you can even perform integration testing to verify that the end-to-end
    system is working as expected.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 确保代码和模型质量的能力在这个自动化过程中起着至关重要的作用。在 Python 中，你可以使用各种工具，如 Flake8、Bandit 和 Black，来确保代码质量、检查常见的安全问题，并保持代码库的一致格式。你还可以使用`pytest`框架编写功能测试，在这些测试中，你将把模型的结果与黄金数据集进行对比。通过`pytest`，你甚至可以执行集成测试，验证端到端系统是否按预期工作。
- en: Adopting DevOps is a never-ending journey. The team will become better every
    time you repeat the process. The trick is to build trust in the end-to-end development
    and deployment process so that everyone is confident to make changes and deploy
    them in production. When the process fails, understand why it failed and learn
    from your mistakes. Create the mechanisms that will prevent future failures and
    move on.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 采用 DevOps 是一项永无止境的旅程。每次重复这个过程，团队都会变得更好。诀窍在于建立对端到端开发和部署过程的信任，让每个人都能自信地做出更改并将其部署到生产环境。当过程失败时，要理解失败的原因，并从错误中吸取教训。创建防止未来失败的机制，并继续前行。
- en: Summary
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about the origins of data science and how it relates
    to machine learning. You then learned about the iterative nature of a data science
    project and discovered the various phases you will be working on. Starting from
    the problem understanding phase, you will then acquire and explore data, create
    new features, train a model, and then deploy to verify your hypothesis. Then,
    you saw how you can scale out the processing of big data files using the Spark
    ecosystem. In the last section, you discovered the DevOps mindset that helps agile
    teams be more efficient, meaning that they develop and deploy new product features
    in short periods of time. You saw the components that are commonly used within
    an MLOps-driven team, and you saw that in the epicenter of that diagram, you find
    AzureML.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了数据科学的起源以及它与机器学习的关系。然后，您学习了数据科学项目的迭代性质，并发现了您将要进行的各个阶段。从问题理解阶段开始，您将获取和探索数据，创建新特征，训练模型，然后进行部署以验证您的假设。接着，您看到如何使用
    Spark 生态系统扩展大数据文件的处理。在最后一节中，您了解了 DevOps 思维方式，它帮助敏捷团队提高效率，即在短时间内开发和部署新产品功能。您看到
    MLOps 驱动的团队中常用的组件，并看到在该图的中心位置，您会找到 AzureML。
- en: In the next chapter, you will learn how to deploy an AzureML workspace and understand
    the Azure resources that you will be using in your data science journey throughout
    this book.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习如何部署 AzureML 工作区，并了解在本书中整个数据科学之旅中将使用的 Azure 资源。
- en: Further reading
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'This section offers a list of helpful web resources that will help you augment
    your knowledge of the topics addressed in this chapter:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了一些有用的网络资源，帮助您扩展对本章中涉及的主题的理解：
- en: 'AzCopy command-line tool to copy blobs and files to a storage account: [http://aka.ms/azcopy](http://aka.ms/azcopy).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AzCopy 命令行工具用于将 blob 和文件复制到存储帐户：[http://aka.ms/azcopy](http://aka.ms/azcopy)。
- en: 'Azure Storage Explorer is a free tool to manage all your Azure cloud storage
    resources: [https://azure.microsoft.com/features/storage-explorer/](https://azure.microsoft.com/features/storage-explorer/).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Storage Explorer 是一款免费的工具，用于管理所有 Azure 云存储资源：[https://azure.microsoft.com/features/storage-explorer/](https://azure.microsoft.com/features/storage-explorer/)。
- en: '*The Hitchhiker''s Guide to the Data Lake* is an extensive guide on key considerations
    and best practices while building a data lake: [https://aka.ms/adls/hitchhikersguide](https://aka.ms/adls/hitchhikersguide).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《数据湖的搭乘指南》* 是一本详细的指南，涵盖了在构建数据湖时需要考虑的关键事项和最佳实践：[https://aka.ms/adls/hitchhikersguide](https://aka.ms/adls/hitchhikersguide)。'
- en: 'Optimizing data processing with AzureML: [https://docs.microsoft.com/azure/machine-learning/concept-optimize-data-processing](https://docs.microsoft.com/azure/machine-learning/concept-optimize-data-processing).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AzureML 优化数据处理：[https://docs.microsoft.com/azure/machine-learning/concept-optimize-data-processing](https://docs.microsoft.com/azure/machine-learning/concept-optimize-data-processing)。
- en: 'The Koalas project: [https://koalas.readthedocs.io](https://koalas.readthedocs.io).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koalas 项目：[https://koalas.readthedocs.io](https://koalas.readthedocs.io)。
- en: 'Guidance to prevent model overfitting and to handle unbalanced data: [https://docs.microsoft.com/azure/machine-learning/concept-manage-ml-pitfalls](https://docs.microsoft.com/azure/machine-learning/concept-manage-ml-pitfalls).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止模型过拟合并处理不平衡数据的指南：[https://docs.microsoft.com/azure/machine-learning/concept-manage-ml-pitfalls](https://docs.microsoft.com/azure/machine-learning/concept-manage-ml-pitfalls)。
- en: 'MLOps guidance for data scientists and app developers working in AzureML and
    Azure DevOps: [https://aka.ms/MLOps](https://aka.ms/MLOps).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对在 AzureML 和 Azure DevOps 中工作的数据科学家和应用程序开发人员的 MLOps 指南：[https://aka.ms/MLOps](https://aka.ms/MLOps)。
