- en: Immutable Design
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不可变设计
- en: In this chapter, we will look at the immutable design of Apache Spark. We will
    delve into the Spark RDD's parent/child chain and use RDD in an immutable way.
    We will then use DataFrame operations for transformations to discuss immutability
    in a highly concurrent environment. By the end of this chapter, we will use the
    Dataset API in an immutable way.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看看Apache Spark的不可变设计。我们将深入研究Spark RDD的父/子链，并以不可变的方式使用RDD。然后，我们将使用DataFrame操作进行转换，以讨论在高度并发的环境中的不可变性。在本章结束时，我们将以不可变的方式使用数据集API。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将涵盖以下主题：
- en: Delving into the Spark RDD's parent/child chain
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入研究Spark RDD的父/子链
- en: Using RDD in an immutable way
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以不可变的方式使用RDD
- en: Using DataFrame operations to transform
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DataFrame操作进行转换
- en: Immutability in the highly concurrent environment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高度并发的环境中的不可变性
- en: Using the Dataset API in an immutable way
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以不可变的方式使用数据集API
- en: Delving into the Spark RDD's parent/child chain
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入研究Spark RDD的父/子链
- en: In this section, we will try to implement our own RDD that inherits the parent
    properties of RDD.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试实现我们自己的RDD，继承RDD的父属性。
- en: 'We will go through the following topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论以下主题：
- en: Extending an RDD
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展RDD
- en: Chaining a new RDD with the parent
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与父RDD链接新的RDD
- en: Testing our custom RDD
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试我们的自定义RDD
- en: Extending an RDD
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展RDD
- en: 'This is a simple test that has a lot of hidden complexity. Let''s start by
    creating a list of the record, as shown in the following code block:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有很多隐藏复杂性的简单测试。让我们从创建记录的列表开始，如下面的代码块所示：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `Record` is just a case class that has an `amount` and `description`, so
    the `amount` is `1` and `d1` is the description.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: “Record”只是一个具有“amount”和“description”的案例类，所以“amount”是1，“d1”是描述。
- en: 'We then created `MultipledRDD` and passed `rdd` to it, and then set the multiplier
    equal to `10`, as shown in the following code:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建了“MultipledRDD”并将“rdd”传递给它，然后将乘数设置为“10”，如下面的代码所示：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We are passing the parent RDD because it has data that was loaded in another
    RDD. In this way, we build the inheritance chain of two RDD's.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递父RDD，因为它包含在另一个RDD中加载的数据。通过这种方式，我们构建了两个RDD的继承链。
- en: Chaining a new RDD with the parent
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与父RDD链接新的RDD
- en: 'We first created a multiple RDD class. In the `MultipliedRDD` class, we have
    two things that pass the parameters:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建了一个多重RDD类。在“MultipliedRDD”类中，我们有两个传递参数的东西：
- en: A brief RDD of the record, that is, `RDD[Record]`
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录的简要RDD，即“RDD[Record]”
- en: A multiplier, that is, `Double`
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 乘数，即“Double”
- en: In our case, there could be a chain of multiple RDD's, which means that there
    could be multiple RDD's inside our RDD. So, this is not always the parent of all
    the directed acyclic graphs. We are just extending the RDD of the type record
    and so we need to pass the RDD that is extended.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，可能会有多个RDD的链，这意味着我们的RDD中可能会有多个RDD。因此，这并不总是所有有向无环图的父级。我们只是扩展了类型为记录的RDD，因此我们需要传递扩展的RDD。
- en: RDD has a lot of methods and we can override any method we want. However, this
    time, we are going with the `compute` method, where we will override the compute
    method to calculate the multiplier. Here, we get a `Partition` split and `TaskContext`.
    These are passed by this part execution engine to our method, so we don't need
    to worry about this. However, we need to return the iterator of the exact same
    type as the type that we pass through the RDD class in the inheritance chain.
    This will be an iterator of the record.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RDD有很多方法，我们可以覆盖任何我们想要的方法。但是，这一次，我们选择了“compute”方法，我们将覆盖计算乘数的方法。在这里，我们获取“Partition”分区和“TaskContext”。这些是执行引擎传递给我们方法的，因此我们不需要担心这一点。但是，我们需要返回与我们通过继承链中的RDD类传递的类型完全相同的迭代器。这将是记录的迭代器。
- en: We then execute the first parent logic, where the first parent is just taking
    that first RDD in our chain. The type here is `Record`, and we are taking an `iterator`
    of `split` and `context`, where the `split` is just a partition that will be executed.
    We know that the Spark RDD is partitioned by the partitioner, but, here, we are
    just getting the specific partition that we need to split. So, the iterator is
    taking the partition and task context, and so it knows which values should be
    returned from that iterative method. For every record in that iterator, which
    is a `salesRecord`, like `amount` and `description`, we are multiplying the `amount`
    by the `multiplier` that was passed to the constructor to get our `Double`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们执行第一个父逻辑，第一个父只是获取我们链中的第一个RDD。这里的类型是“Record”，我们获取“split”和“context”的“iterator”，其中“split”只是将要执行的分区。我们知道Spark
    RDD是由分区器分区的，但是在这里，我们只是获取我们需要拆分的特定分区。因此，迭代器获取分区和任务上下文，因此它知道应该从该迭代方法返回哪些值。对于迭代器中的每条记录，即“salesRecord”，如“amount”和“description”，我们将“amount”乘以传递给构造函数的“multiplier”来获得我们的“Double”。
- en: By doing this, we have multiplied our amount by the multiplier, and we can then
    return the new record which has the new amount. So, we now have an amount of the
    old record multiplied by our `multiplier` and the description of the `salesRecord`.
    For the second filter, what we need to `override` is `getPartitions`, as we want
    to keep the partitioning of the parent RDD. If the previous RDD has 100 partitions,
    for example, we also want our `MultipledRDD` to have 100 partitions. So, we want
    to retain that information about partitions rather than losing it. For the same
    reason, we are just proxying it to the `firstParent`. The `firstParent` of the
    RDD will then just take the previous partitions from that specific RDD.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们已经将我们的金额乘以了乘数，然后我们可以返回具有新金额的新记录。因此，我们现在有了旧记录乘以我们的“乘数”的金额和“salesRecord”的描述。对于第二个过滤器，我们需要“覆盖”的是“getPartitions”，因为我们希望保留父RDD的分区。例如，如果之前的RDD有100个分区，我们也希望我们的“MultipledRDD”有100个分区。因此，我们希望保留关于分区的信息，而不是丢失它。出于同样的原因，我们只是将其代理给“firstParent”。RDD的“firstParent”然后只会从特定RDD中获取先前的分区。
- en: 'In this way, we have created a new `multipliedRDD`, which passes the parent
    and multiplier. For our `extendedRDD`, we need to `collect` it and call `toList`,
    and our list should have `10` and `d1`, as shown in the following example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们创建了一个新的`multipliedRDD`，它传递了父级和乘数。对于我们的`extendedRDD`，我们需要`collect`它并调用`toList`，我们的列表应该包含`10`和`d1`，如下例所示：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Compute was executed automatically when we created the new RDD, and so it is
    always executed without the explicit method call.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建新的RDD时，计算会自动执行，因此它总是在没有显式方法调用的情况下执行。
- en: Testing our custom RDD
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试我们的自定义RDD
- en: 'Let''s start this test to check if this has created our RDD. By doing this,
    we can extend our parent RDD and add behavior to our RDD. This is shown in the
    following screenshot:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始这个测试，以检查这是否已经创建了我们的RDD。通过这样做，我们可以扩展我们的父RDD并向我们的RDD添加行为。这在下面的截图中显示：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the next section, we'll be using RDD in an immutable way.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将以不可变的方式使用RDD。
- en: Using RDD in an immutable way
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以不可变的方式使用RDD
- en: Now that we know how to create a chain of execution using RDD inheritance, let's
    learn how to use RDD in an immutable way.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何使用RDD继承创建执行链，让我们学习如何以不可变的方式使用RDD。
- en: 'In this section, we will go through the following topics:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论以下主题：
- en: Understating DAG immutability
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解DAG的不可变性
- en: Creating two leaves from the one root RDD
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个根RDD创建两个叶子
- en: Examining results from both leaves
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查两个叶子的结果
- en: Let's first understand directed acyclic graph immutability and what it gives
    us. We will then be creating two leaves from one node RDD, and checking if both
    leaves are behaving totally independently if we create a transformation on one
    of the leaf RDD's. We will then examine results from both leaves of our current
    RDD and check if any transformation on any leaf does not change or impact the
    root RDD. It is imperative to work like this because we have found that we will
    not be able to create yet another leaf from the root RDD, because the root RDD
    will be changed, which means it will be mutable. To overcome this, the Spark designers
    created an immutable RDD for us.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解有向无环图的不可变性以及它给我们带来了什么。然后，我们将从一个节点RDD创建两个叶子，并检查如果我们在一个叶子RDD上创建一个转换，那么两个叶子是否完全独立地行为。然后，我们将检查当前RDD的两个叶子的结果，并检查对任何叶子的任何转换是否不会改变或影响根RDD。以这种方式工作是至关重要的，因为我们发现我们将无法从根RDD创建另一个叶子，因为根RDD将被更改，这意味着它将是可变的。为了克服这一点，Spark设计师为我们创建了一个不可变的RDD。
- en: 'There is a simple test to show that the RDD should be immutable. First, we
    will create an RDD from `0 to 5`, which is added to a sequence from the Scala
    branch. `to` is taking the `Int`, and the first parameter is an implicit one,
    which is from the Scala package, as shown in the following example:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个简单的测试来显示RDD应该是不可变的。首先，我们将从`0到5`创建一个RDD，它被添加到来自Scala分支的序列中。`to`获取`Int`，第一个参数是一个隐式参数，来自Scala包，如下例所示：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once we have our RDD data, we can create the first leaf. The first leaf is
    a result (`res`) and we are just mapping every element multiplied by `2`. Let''s
    create a second leaf, but this time it will be marked by `4`, as shown in the
    following example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了RDD数据，我们可以创建第一个叶子。第一个叶子是一个结果（`res`），我们只是将每个元素乘以`2`。让我们创建第二个叶子，但这次它将被标记为`4`，如下例所示：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'So, we have our root RDD and two leaves. First, we will collect the first leaf
    and see that the elements in it are `0, 2, 4, 6, 8, 10`, so everything here is
    multiplied by `2`, as shown in the following example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们有我们的根RDD和两个叶子。首先，我们将收集第一个叶子，并看到其中的元素为`0, 2, 4, 6, 8, 10`，所以这里的一切都乘以`2`，如下例所示：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'However, even though we have that notification on the `res`, the data is still
    exactly the same as it was in the beginning, which is `0, 1, 2, 3, 4, 5`, as shown
    in the following example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使我们在`res`上有了通知，数据仍然与一开始的完全相同，即`0, 1, 2, 3, 4, 5`，如下例所示：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'So, everything is immutable, and executing the transformation of `* 2` didn''t
    change our data. If we create a test for `leaf2`, we will `collect` it and call
    `toList`. We will see that it should `contain` elements like `0, 4, 8, 12, 16,
    20`, as shown in the following example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，一切都是不可变的，执行`* 2`的转换并没有改变我们的数据。如果我们为`leaf2`创建一个测试，我们将`collect`它并调用`toList`。我们会看到它应该包含像`0,
    4, 8, 12, 16, 20`这样的元素，如下例所示：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When we run the test, we will see that every path in our execution, the root,
    that is, data, or the first leaf and second leaf, behave independently from each
    other, as shown in the following code output:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行测试时，我们会看到我们执行中的每条路径，即数据或第一个叶子和第二个叶子，彼此独立地行为，如下面的代码输出所示：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Every mutation is different; we can see that the test passed, which shows us
    that our RDD is immutable.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每次变异都是不同的；我们可以看到测试通过了，这表明我们的RDD是不可变的。
- en: Using DataFrame operations to transform
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DataFrame操作进行转换
- en: The data from the API has an RDD underneath it, and so there is no way that
    the DataFrame could be mutable. In DataFrame, the immutability is even better
    because we can add and subtract columns from it dynamically, without changing
    the source dataset.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: API的数据下面有一个RDD，因此DataFrame是不可变的。在DataFrame中，不可变性甚至更好，因为我们可以动态地添加和减去列，而不改变源数据集。
- en: 'In this section, we will cover the following topics:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将涵盖以下主题：
- en: Understanding DataFrame immutability
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解DataFrame的不可变性
- en: Creating two leaves from the one root DataFrame
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个根DataFrame创建两个叶子
- en: Adding a new column by issuing transformation
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过发出转换来添加新列
- en: We will start by using data from operations to transform our DataFrame. First,
    we need to understand DataFrame immutability and then we will create two leaves,
    but this time from the one root DataFrame. We will then issue a transformation
    that is a bit different than the RDD. This will add a new column to our resulting
    DataFrame because we are manipulating it this way in a DataFrame. If we want to
    map data, then we need to take data from the first column, transform it, and save
    it to another column, and then we'll have two columns. If we are no longer interested,
    we can drop the first column, but the result will be yet another DataFrame.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用操作的数据来转换我们的DataFrame。首先，我们需要了解DataFrame的不可变性，然后我们将从一个根DataFrame创建两个叶子，但这次是。然后，我们将发出一个略有不同于RDD的转换。这将向我们的结果DataFrame添加一个新列，因为我们在DataFrame中是这样操作的。如果我们想要映射数据，那么我们需要从第一列中获取数据，进行转换，并保存到另一列，然后我们将有两列。如果我们不再感兴趣，我们可以删除第一列，但结果将是另一个DataFrame。
- en: So, we'll have the first DataFrame with one column, the second one with result
    and source, and the third one with only one result. Let's look at the code for
    this section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将有第一个DataFrame有一列，第二个有结果和源，第三个只有一个结果。让我们看看这一部分的代码。
- en: 'We will be creating a DataFrame, so we need to call the `toDF()` method. We
    are creating the `UserData` with `"a"` as `"1"`, `"b"` as `"2"`, and `"d"` as `"200"`. The
    `UserData` has `userID` and `data`, two fields that are both `String`, as shown
    in the following example:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个DataFrame，所以我们需要调用`toDF()`方法。我们将使用`"a"`作为`"1"`，`"b"`作为`"2"`，`"d"`作为`"200"`来创建`UserData`。`UserData`有`userID`和`data`两个字段，都是`String`类型，如下例所示：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It is important to create an RDD using a case class in tests because, when
    we are called to the DataFrame, this part will infer the schema and name columns
    accordingly. The following code follows an example of this, where we are filtering
    only a `userID` column from the `userData` that is in `"a"`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试中使用案例类创建RDD是很重要的，因为当我们调用DataFrame时，这部分将推断模式并相应地命名列。以下代码是这方面的一个例子，我们只从`userData`中的`userID`列中进行过滤：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our result should have only one record and so we are dropping two columns,
    but still, the `userData` source that we created will have `3` rows. So, modifying
    it by filtering created yet another DataFrame that we call the `res` without modifying
    the input `userData`, as shown in the following example:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果应该只有一条记录，所以我们要删除两列，但是我们创建的`userData`源将有3行。因此，通过过滤对其进行修改，创建了另一个名为`res`的DataFrame，而不修改输入的`userData`，如下例所示：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'So, let''s start this test and see how immutable data from API behaves, as
    shown in the following screenshot:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始这个测试，看看来自API的不可变数据的行为，如下屏幕截图所示：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As we can see, our test passes, and, from the result (`res`), we know that
    our parent was not modified. So, for example, if we want to map something on `res.map()`,
    we can map the `userData` column, as shown in the following example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们的测试通过了，并且从结果（`res`）中，我们知道我们的父级没有被修改。因此，例如，如果我们想在`res.map()`上映射一些东西，我们可以映射`userData`列，如下例所示：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Another leaf will have an additional column without changing the `userId` source
    code, so that was the immutability of DataFrame.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个叶子将具有一个额外的列，而不更改`userId`源代码，因此这就是DataFrame的不可变性。
- en: Immutability in the highly concurrent environment
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高并发环境中的不可变性
- en: We saw how immutability affects the creation and design of programs, so now
    we will understand how it is useful.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了不可变性如何影响程序的创建和设计，现在我们将了解它的用途。
- en: 'In this section, we will cover the following topics:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖以下主题：
- en: The cons of mutable collections
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可变集合的缺点
- en: Creating two threads that simultaneously modify a mutable collection
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建两个同时修改可变集合的线程
- en: Reasoning about a concurrent program
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理并发程序
- en: 'Let''s first understand the cause of mutable collections. To do this, we will
    be creating two threads that simultaneously modify the mutable collection. We
    will be using this code for our test. First, we will create a `ListBuffer` that
    is a mutable list. Then, we can add and delete links without creating another
    list for any modification. We can then create an `Executors` service with two
    threads. We need two threads to start simultaneously to modify the state. Later,
    we will use a `CountDownLatch` construct from `Java.util.concurrent:`. This is
    shown in the following example:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解可变集合的原因。为此，我们将创建两个同时修改可变集合的线程。我们将使用此代码进行测试。首先，我们将创建一个`ListBuffer`，它是一个可变列表。然后，我们可以添加和删除链接，而无需为任何修改创建另一个列表。然后，我们可以创建一个具有两个线程的`Executors`服务。我们需要两个线程同时开始修改状态。稍后，我们将使用`Java.util.concurrent`中的`CountDownLatch`构造。这在下面的例子中显示：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `CountDownLatch` is a construct that helps us to stop threads from processing
    until we request them to. We need to wait with our logic until both threads start
    executing. We then submit a `Runnable` to the `executors` and our `run()` method
    does the `countDown()` by signaling when it is ready for action and appends `"A"`
    to `listMutable`, as shown in the following example:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountDownLatch`是一种构造，它帮助我们阻止线程处理，直到我们要求它们开始。我们需要等待逻辑，直到两个线程开始执行。然后，我们向`executors`提交一个`Runnable`，我们的`run()`方法通过发出`countDown()`来表示准备好进行操作，并将“A”添加到`listMutable`，如下例所示：'
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, another thread starts, and also uses `countDown` by signaling that it
    is ready to start. But first, it checks whether the list contains `"A"` and, if
    not, it appends that `"A"`, as shown in the following example:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，另一个线程启动，并且也使用`countDown`来表示它已准备好开始。但首先，它会检查列表是否包含“A”，如果没有，就会添加“A”，如下例所示：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We then use `await()` to wait until `countDown` is issued and, when it is issued,
    we are able to progress with the verification of our program, as shown in the
    following example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`await()`等待`countDown`发出，当它发出时，我们可以继续验证我们的程序，如下例所示：
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`listMutable` contains `"A"` or it can have `"A","A"`. `listMutable` checks
    if the list contains `"A"` and, if not, it will not add that element, as shown
    in the following example:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`listMutable`包含`"A"`或可能包含`"A","A"`。`listMutable`检查列表是否包含`"A"`,如果没有，它将不会添加该元素，如下例所示：'
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: But there is a race condition here. There could be a possibility that, after
    the check `if(!listMutable.contains("A"))`, the `run()` thread will add the `"A"`
    element to the list. But we are inside `if`, so we will add another `"A"` by/using
    `listMutable += "A"`. Because of the mutability of the state and the fact that
    it was modified via another thread, we can have `"A"` or `"A","A"`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 但这里存在竞争条件。在检查`if(!listMutable.contains("A"))`之后，`run()`线程可能会将`"A"`元素添加到列表中。但我们在`if`中，所以我们将通过`listMutable
    += "A"`添加另一个`"A"`。由于状态的可变性以及它通过另一个线程进行了修改，我们可能会有`"A"`或`"A","A"`。
- en: We need to be careful while using mutable state since we cannot have such a
    corrupted state. To alleviate this problem, we can use `java.util` collections
    and synchronized lists on it.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用可变状态时需要小心，因为我们不能有这样一个损坏的状态。为了缓解这个问题，我们可以在`java.util`集合上使用同步列表。
- en: 'But if we have the synchronized block, then our program will be very slow because
    we will need to coordinate access to that exclusively. We can also employ `lock`
    from the `java.util.concurrent.locks` package. We can use an implementation, like `ReadLock`
    or `WriteLock`. In the following example, we will use `WriteLock`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们有同步块，那么我们的程序将非常慢，因为我们需要独占地访问它。我们还可以使用`java.util.concurrent.locks`包中的`lock`。我们可以使用`ReadLock`或`WriteLock`等实现。在下面的例子中，我们将使用`WriteLock`：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We also need to `lock` our `lock()` and only then proceed, as shown in the
    following example:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要对我们的`lock()`进行`lock`，然后再进行下一步，如下例所示：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Later, we can use `unlock()`. However, we should also do this in the second
    thread so that our list only has one element, as shown in the following example:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以使用`unlock()`。然而，我们也应该在第二个线程中这样做，这样我们的列表只有一个元素，如下例所示：
- en: '[PRE22]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/9b19108c-26d0-4115-92a9-8191a1900bdb.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b19108c-26d0-4115-92a9-8191a1900bdb.png)'
- en: Locking is a very hard and expensive operation, and so immutability is key to
    performance programs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 锁定是一个非常艰难和昂贵的操作，因此不可变性是性能程序的关键。
- en: Using the Dataset API in an immutable way
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以不可变的方式使用数据集API
- en: 'In this section, we will use the Dataset API in an immutable way. We will cover
    the following topics:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将以不可变的方式使用数据集API。我们将涵盖以下主题：
- en: Dataset immutability
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的不可变性
- en: Creating two leaves from the one root dataset
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个根数据集创建两个叶子
- en: Adding a new column by issuing transformation
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过发出转换添加新列
- en: 'The test case for the dataset is quite similar, but we need to do a `toDS()` for
    our data to be type safe. The type of dataset is `userData`, as shown in the following
    example:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的测试用例非常相似，但我们需要对我们的数据进行`toDS()`以确保类型安全。数据集的类型是`userData`，如下例所示：
- en: '[PRE23]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we will issue a filter of `userData` and specify `isin`, as shown in the
    following example:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将发出对`userData`的过滤，并指定`isin`，如下例所示：
- en: '[PRE24]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It will return the result (`res`), which is a leaf with our `1` element. `userData`
    will still have `3` elements because of this apparent root. Let''s execute this
    program, as shown in the following example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 它将返回结果（`res`），这是一个带有我们的`1`元素的叶子。由于这个明显的根，`userData`仍然有`3`个元素。让我们执行这个程序，如下例所示：
- en: '[PRE25]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can see our test passed, which means that the dataset is also an immutable
    abstraction on top of the DataFrame, and employs the same characteristics. `userData`
    has a something very useful known as a typeset, and, if you use the `show()` method,
    it will infer the schema and know that the `"a"` field is a string or another
    type, as shown in the following example:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的测试通过了，这意味着数据集也是DataFrame之上的不可变抽象，并且具有相同的特性。`userData`有一个非常有用的类型集，如果使用`show()`方法，它将推断模式并知道`"a"`字段是字符串或其他类型，如下例所示：
- en: '[PRE26]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output will be as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE27]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the preceding output, we have both `userID` and `data` fields.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，我们有`userID`和`data`字段。
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we delved into the Spark RDD parent-child chain and created
    a multiplier RDD that was able to calculate everything based on the parent RDD,
    and also based on the partitioning scheme on the parent. We used RDD in an immutable
    way. We saw that the modification of the leaf that was created from the parent didn't
    modify the part. We also learned a better abstraction, that is, a DataFrame, so
    we learned that we can employ transformation there. However, every transformation
    is just adding to another column—it is not modifying anything in place. Next,
    we just set immutability in a highly concurrent environment. We saw how the mutable
    state is bad when accessing multiple threads. Finally, we saw that the Dataset
    API is also created in an immutable type of way and that we can leverage those
    things here.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入研究了Spark RDD的父子链，并创建了一个能够根据父RDD计算一切的乘数RDD，还基于父RDD的分区方案。我们以不可变的方式使用了RDD。我们看到，从父级创建的叶子的修改并没有修改部分。我们还学习了一个更好的抽象，即DataFrame，因此我们学会了可以在那里使用转换。然而，每个转换只是添加到另一列，而不是直接修改任何内容。接下来，我们只需在高度并发的环境中设置不可变性。我们看到了当访问多个线程时，可变状态是不好的。最后，我们看到数据集API也是以不可变的方式创建的，我们可以在这里利用这些特性。
- en: In the next chapter, we'll look at how to avoid shuffle and reduce personal
    expense.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看看如何避免洗牌和减少个人开支。
