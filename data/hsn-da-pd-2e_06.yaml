- en: '*Chapter 4*: Aggregating Pandas DataFrames'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：聚合Pandas DataFrames'
- en: In this chapter, we will continue our discussion of data wrangling from [*Chapter
    3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061), *Data Wrangling with Pandas*,
    by addressing the enrichment and aggregation of data. This includes essential
    skills, such as merging dataframes, creating new columns, performing window calculations,
    and aggregating by group membership. Calculating aggregations and summaries will
    help us draw conclusions about our data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续探讨[*第3章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)中的数据清理内容，*使用Pandas进行数据清理*，通过讨论数据的丰富和聚合。包括一些重要技能，如合并数据框、创建新列、执行窗口计算以及按组进行聚合。计算聚合和总结有助于我们从数据中得出结论。
- en: We will also take a look at the additional functionality `pandas` has for working
    with time series data, beyond the time series slicing we introduced in previous
    chapters, including how we can roll up the data with aggregation and select it
    based on the time of day. Much of the data we will encounter is time series data,
    so being able to effectively work with time series is paramount. Of course, performing
    these operations efficiently is important, so we will also review how to write
    efficient `pandas` code.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨`pandas`在处理时间序列数据时的额外功能，超出我们在前几章中介绍的时间序列切片内容，包括如何通过聚合对数据进行汇总，以及如何根据一天中的时间选择数据。我们将遇到的大部分数据都是时间序列数据，因此有效地处理时间序列数据至关重要。当然，高效地执行这些操作也很重要，因此我们还将回顾如何编写高效的`pandas`代码。
- en: This chapter will get us comfortable with performing analyses using `DataFrame`
    objects. Consequently, these topics are more advanced compared to the prior content
    and may require a few rereads, so be sure to follow along with the notebooks,
    which contain additional examples.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将帮助我们熟练使用`DataFrame`对象进行分析。因此，这些主题相比之前的内容更为高级，可能需要反复阅读几遍，所以请确保跟随带有额外示例的笔记本进行学习。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Performing database-style operations on DataFrames
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`DataFrame`上执行类似数据库的操作
- en: Using DataFrame operations to enrich data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`DataFrame`操作来丰富数据
- en: Aggregating data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合数据
- en: Working with time series data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理时间序列数据
- en: Chapter materials
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章资料
- en: The materials for this chapter can be found on GitHub at [https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_04](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_04).
    There are four notebooks that we will work through, each numbered according to
    when they will be used. The text will prompt you to switch. We will begin with
    the `1-querying_and_merging.ipynb` notebook to learn about querying and merging
    dataframes. Then, we will move on to the `2-dataframe_operations.ipynb` notebook
    to discuss data enrichment through operations such as binning, window functions,
    and pipes. For this section, we will also use the `window_calc.py` Python file,
    which contains a function for performing window calculations using pipes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的资料可以在GitHub上找到，网址为[https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_04](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_04)。我们将通过四个笔记本来学习，每个笔记本按使用顺序编号，文本会提示你切换。我们将从`1-querying_and_merging.ipynb`笔记本开始，学习如何查询和合并数据框。然后，我们将进入`2-dataframe_operations.ipynb`笔记本，讨论通过操作如分箱、窗口函数和管道来丰富数据。在这一部分，我们还将使用`window_calc.py`
    Python文件，该文件包含一个使用管道进行窗口计算的函数。
- en: Tip
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The `understanding_window_calculations.ipynb` notebook contains some interactive
    visualizations for understanding window functions. This may require some additional
    setup, but the instructions are in the notebook.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`understanding_window_calculations.ipynb`笔记本包含一些交互式可视化，用于帮助理解窗口函数。可能需要一些额外的设置，但笔记本中有说明。'
- en: Next, in the `3-aggregations.ipynb` notebook, we will discuss aggregations,
    pivot tables, and crosstabs. Finally, we will focus on additional capabilities
    `pandas` provides when working with time series data in the `4-time_series.ipynb`
    notebook. Note that we will not go over the `0-weather_data_collection.ipynb`
    notebook; however, for those interested, it contains the code that was used to
    collect the data from the **National Centers for Environmental Information** (**NCEI**)
    API, which can be found at [https://www.ncdc.noaa.gov/cdo-web/webservices/v2](https://www.ncdc.noaa.gov/cdo-web/webservices/v2).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在`3-aggregations.ipynb`笔记本中，我们将讨论聚合、透视表和交叉表。最后，我们将重点介绍`pandas`在处理时间序列数据时提供的额外功能，这些内容将在`4-time_series.ipynb`笔记本中讨论。请注意，我们不会讲解`0-weather_data_collection.ipynb`笔记本；然而，对于有兴趣的人，它包含了从**国家环境信息中心**（**NCEI**）API收集数据的代码，API链接可见于[https://www.ncdc.noaa.gov/cdo-web/webservices/v2](https://www.ncdc.noaa.gov/cdo-web/webservices/v2)。
- en: 'Throughout this chapter, we will use a variety of datasets, which can be found
    in the `data/` directory:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用多种数据集，所有数据集均可以在`data/`目录中找到：
- en: '![Figure 4.1 – Datasets used in this chapter'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1 – 本章使用的数据集'
- en: '](img/Figure_4.1_B16834.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.1_B16834.jpg)'
- en: Figure 4.1 – Datasets used in this chapter
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 本章使用的数据集
- en: Note that the `exercises/` directory contains the CSV files that are required
    to complete the end-of-chapter exercises. More information on these datasets can
    be found in the `exercises/README.md` file.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`exercises/`目录包含了完成本章末尾练习所需的CSV文件。更多关于这些数据集的信息可以在`exercises/README.md`文件中找到。
- en: Performing database-style operations on DataFrames
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 DataFrame 上执行数据库风格的操作
- en: '`DataFrame` objects are analogous to tables in a database: each has a name
    we refer to it by, is composed of rows, and contains columns of specific data
    types. Consequently, `pandas` allows us to carry out database-style operations
    on them. Traditionally, databases support a minimum of four operations, called
    **CRUD**: **C**reate, **R**ead, **U**pdate, and **D**elete.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame`对象类似于数据库中的表：每个对象都有一个我们用来引用它的名称，由行组成，并包含特定数据类型的列。因此，`pandas`允许我们在其上执行数据库风格的操作。传统上，数据库支持至少四种操作，称为**CRUD**：**C**reate（创建）、**R**ead（读取）、**U**pdate（更新）和**D**elete（删除）。'
- en: A database query language—most commonly `pandas` operations that will be discussed
    in this section since it may aid the understanding of those familiar with SQL.
    Many data professionals have some familiarity with basic SQL, so consult the *Further
    reading* section for resources that provide a more formal introduction.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一种数据库查询语言——在本节中将讨论的大多数是`pandas`操作，它可能有助于熟悉SQL的人理解。许多数据专业人员都对基础的SQL有所了解，因此请参考*进一步阅读*部分以获取提供更正式介绍的资源。
- en: 'For this section, we will be working in the `1-querying_and_merging.ipynb`
    notebook. We will begin with our imports and read in the NYC weather data CSV
    file:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在`1-querying_and_merging.ipynb`笔记本中进行操作。我们将从导入库并读取纽约市天气数据的CSV文件开始：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is long format data—we have several different weather observations per
    day for various stations covering NYC in 2018:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是长格式数据——我们有多个不同的天气观测数据，涵盖了2018年纽约市各个站点的每日数据：
- en: '![Figure 4.2 – NYC weather data'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2 – 纽约市天气数据'
- en: '](img/Figure_4.2_B16834.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.2_B16834.jpg)'
- en: Figure 4.2 – NYC weather data
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 纽约市天气数据
- en: In [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035), *Working with
    Pandas DataFrames*, we covered how to create dataframes; this was the `pandas`
    equivalent of a `"CREATE TABLE ..."` SQL statement. When we discussed selection
    and filtering in [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035),
    *Working with Pandas DataFrames*, and [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*, we were focusing on reading from dataframes, which
    equated to the `SELECT` (picking columns) and `WHERE` (filtering by Boolean criteria)
    SQL clauses. We carried out update (`UPDATE` in SQL) and delete (`DELETE FROM`
    in SQL) operations when we discussed working with missing data in [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*. In addition to those basic CRUD operations, the
    concept of a `pandas` implementation in this section, along with the idea of querying
    a `DataFrame` object.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 2 章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035)，*使用 Pandas DataFrame*
    中，我们介绍了如何创建 DataFrame；这相当于 SQL 中的 `"CREATE TABLE ..."` 语句。当我们在 [*第 2 章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035)，*使用
    Pandas DataFrame* 和 [*第 3 章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)，*使用
    Pandas 进行数据清洗* 中讨论选择和过滤时，我们主要关注的是从 DataFrame 中读取数据，这等同于 SQL 中的 `SELECT`（选择列）和
    `WHERE`（按布尔条件过滤）子句。我们在讨论处理缺失数据时执行了更新（SQL 中的 `UPDATE`）和删除（SQL 中的 `DELETE FROM`）操作，这些内容出现在
    [*第 3 章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)，*使用 Pandas 进行数据清洗* 中。除了这些基本的
    CRUD 操作外，本节还介绍了 `pandas` 在实现查询 `DataFrame` 对象方面的概念。
- en: Querying DataFrames
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询 DataFrame
- en: 'Pandas provides the `query()` method so that we can easily write complicated
    filters instead of using a Boolean mask. The syntax is similar to the `WHERE`
    clause in a SQL statement. To illustrate this, let''s query the weather data for
    all the rows where the value of the `SNOW` column was greater than zero for stations
    with `US1NY` in their station ID:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 提供了 `query()` 方法，使我们能够轻松编写复杂的过滤器，而无需使用布尔掩码。其语法类似于 SQL 语句中的 `WHERE` 子句。为了说明这一点，让我们查询所有
    `SNOW` 列值大于零且站点 ID 包含 `US1NY` 的站点的天气数据：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Each row is a snow observation for a given combination of date and station.
    Notice that the values vary quite a bit for January 4th—some stations received
    more snow than others:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行都是某个日期和站点组合下的雪观测数据。注意，1 月 4 日的数据差异很大——有些站点的降雪量比其他站点多：
- en: '![Figure 4.3 – Querying the weather data for observations of snow'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3 – 查询雪的天气数据观测值'
- en: '](img/Figure_4.3_B16834.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.3_B16834.jpg)'
- en: Figure 4.3 – Querying the weather data for observations of snow
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 查询雪的天气数据观测值
- en: 'This query is equivalent to the following in SQL. Note that `SELECT *` selects
    all the columns in the table (our dataframe, in this case):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询在 SQL 中等价于以下语句。注意，`SELECT *` 会选择表中的所有列（在这里是我们的 DataFrame）：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035), *Working
    with Pandas DataFrames*, we learned how to use a Boolean mask to get the same
    result:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 2 章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035)，*使用 Pandas DataFrame*
    中，我们学习了如何使用布尔掩码得到相同的结果：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For the most part, which one we use is a matter of preference; however, if we
    have a long name for our dataframe, we will probably prefer the `query()` method.
    In the previous example, we had to type the dataframe's name an additional three
    times in order to use the mask.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分情况下，我们选择使用哪种方式主要取决于个人偏好；然而，如果我们的 DataFrame 名称很长，我们可能会更喜欢使用 `query()` 方法。在前面的例子中，我们不得不额外输入三次
    DataFrame 的名称来使用掩码。
- en: Tip
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: When using Boolean logic with the `query()` method, we can use both logical
    operators (`and`, `or`, `not`) and bitwise operators (`&`, `|`, `~`).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `query()` 方法时，我们可以使用布尔逻辑操作符（`and`、`or`、`not`）和按位操作符（`&`、`|`、`~`）。
- en: Merging DataFrames
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并 DataFrame
- en: When we discussed stacking dataframes one on top of the other with the `pd.concat()`
    function and the `append()` method in [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035),
    *Working with Pandas DataFrames*, we were performing the equivalent of the SQL
    `UNION ALL` statement (or just `UNION`, if we also removed the duplicates, as
    we saw in [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061), *Data
    Wrangling with Pandas*). Merging dataframes deals with how to line them up by
    row.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 [*第 2 章*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035)，*使用 Pandas DataFrame*
    中讨论通过 `pd.concat()` 函数和 `append()` 方法将 DataFrame 堆叠在一起时，我们实际上在执行 SQL 中的 `UNION
    ALL` 语句（如果删除重复项，我们就是在执行 `UNION`，正如我们在 [*第 3 章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)，*使用
    Pandas 进行数据清洗* 中所看到的那样）。合并 DataFrame 涉及如何按行将它们对齐。
- en: 'When referring to databases, merging is traditionally called a **join**. There
    are four types of joins: full (outer), left, right, and inner. These join types
    let us know how the result will be affected by values that are only present on
    one side of the join. This is a concept that''s much more easily understood visually,
    so let''s look at some Venn diagrams and then do some sample joins on the weather
    data. Here, the darker regions represent the data we are left with after performing
    the join:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据库中，合并通常被称为**连接**。连接有四种类型：全连接（外连接）、左连接、右连接和内连接。这些连接类型告诉我们，如何根据连接两边只有一方有的值来影响结果。这是一个更容易通过图示来理解的概念，所以我们来看一些维恩图，并对天气数据进行一些示例连接。在这里，较深的区域表示我们在执行连接后留下的数据：
- en: '![Figure 4.4 – Understanding join types'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.4 – 理解连接类型'
- en: '](img/Figure_4.4_B16834.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.4_B16834.jpg)'
- en: Figure 4.4 – Understanding join types
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 理解连接类型
- en: We have been working with data from numerous weather stations, but we don't
    know anything about them besides their IDs. It would be helpful to know exactly
    where each of the stations is located to better understand discrepancies between
    weather readings for the same day in NYC. When we queried for the snow data, we
    saw quite a bit of variation in the readings for January 4th (see *Figure 4.3*).
    This is most likely due to the location of the station. Stations at higher elevations
    or farther north may record more snow. Depending on how far they actually are
    from NYC, they may have been experiencing a snowstorm that was heavier somewhere
    else, such as Connecticut or Northern New Jersey.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在处理来自众多气象站的数据，但除了它们的ID外，我们对这些站点一无所知。如果能够了解每个气象站的具体位置，将有助于更好地理解同一天纽约市天气读数之间的差异。当我们查询雪量数据时，我们看到1月4日的读数存在相当大的变化（见*图4.3*）。这很可能是由于气象站的位置不同。位于更高海拔或更北方的站点可能会记录更多的降雪。根据它们与纽约市的距离，它们可能正经历某个地方的暴风雪，例如康涅狄格州或北新泽西。
- en: 'The NCEI API''s `stations` endpoint gives us all the information we need for
    the stations. This is in the `weather_stations.csv` file, as well as in the `stations`
    table in the SQLite database. Let''s read this data into a dataframe:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: NCEI API的`stations`端点提供了我们所需的所有气象站信息。这些信息存储在`weather_stations.csv`文件中，并且也存在于SQLite数据库中的`stations`表中。我们可以将这些数据读取到一个数据框中：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For reference, Central Park in NYC is at 40.7829° N, 73.9654° W (latitude 40.7829
    and longitude -73.9654), and NYC has an elevation of 10 meters. The first five
    stations that record NYC data are not in New York. The ones in New Jersey are
    southwest of NYC, while the ones in Connecticut are northeast of NYC:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 供参考，纽约市中央公园的坐标是40.7829° N, 73.9654° W（纬度40.7829， 经度-73.9654），纽约市的海拔为10米。记录纽约市数据的前五个站点不在纽约州。这些位于新泽西州的站点在纽约市的西南，而位于康涅狄格州的站点则位于纽约市的东北：
- en: '![Figure 4.5 – Weather stations dataset'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.5 – 气象站数据集'
- en: '](img/Figure_4.5_B16834.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.5_B16834.jpg)'
- en: Figure 4.5 – Weather stations dataset
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 气象站数据集
- en: 'Joins require us to specify how to match the data up. The only data the `weather`
    dataframe has in common with the `station_info` dataframe is the station ID. However,
    the columns containing this information are not named the same: in the `weather`
    dataframe, this column is called `station`, while in the `station_info` dataframe,
    it is called `id`. Before we join the data, let''s get some information on how
    many distinct stations we have and how many entries are in each dataframe:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 连接要求我们指定如何匹配数据。`weather`数据框架与`station_info`数据框架唯一共有的数据是气象站ID。然而，包含这些信息的列名并不相同：在`weather`数据框架中，这一列被称为`station`，而在`station_info`数据框架中，它被称为`id`。在我们进行连接之前，先获取一些关于有多少个不同气象站以及每个数据框架中有多少条记录的信息：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The difference in the number of unique stations across the dataframes tells
    us they don''t contain all the same stations. Depending on the type of join we
    pick, we may lose some data. Therefore, it''s important to look at the row count
    before and after the join. We can see this in the `describe()`, but we don''t
    need to run that just to get the row count. Instead, we can use the `shape` attribute,
    which gives us a tuple of the form `(number of rows, number of columns)`. To select
    the rows, we just grab the value at index `0` (`1` for columns):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框架中唯一站点数量的差异告诉我们，它们并不包含完全相同的站点。根据我们选择的连接类型，我们可能会丢失一些数据。因此，在连接前后查看行数是很重要的。我们可以在`describe()`中查看这一点，但不需要仅仅为了获取行数而运行它。相反，我们可以使用`shape`属性，它会返回一个元组，格式为（行数，列数）。要选择行，我们只需获取索引为`0`的值（列数为`1`）：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Since we will be checking the row count often, it makes more sense to write
    a function that will give us the row count for any number of dataframes. The `*dfs`
    argument collects all the input to this function in a tuple, which we can iterate
    over in a list comprehension to get the row count:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将频繁检查行数，所以编写一个函数来为任意数量的数据框提供行数更为合适。`*dfs`参数将所有输入收集成一个元组，我们可以通过列表推导式遍历这个元组来获取行数：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we know that we have 78,780 rows of weather data and 279 rows of station
    information data, we can begin looking at the types of joins. We'll begin with
    the inner join, which will result in the least amount of rows (unless the two
    dataframes have all the same values for the column being joined on, in which case
    all the joins will be equivalent). The `weather.station` column and the `station_info.id`
    column, we will only get weather data for stations that are in `station_info`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道，天气数据有78,780行，站点信息数据有279行，我们可以开始查看不同类型的连接。我们将从内连接开始，内连接会产生最少的行数（除非两个数据框在连接列上有完全相同的值，在这种情况下，所有的连接结果都是等价的）。在`weather.station`列和`station_info.id`列上连接，我们将只获得`station_info`中存在的站点的天气数据。
- en: 'We will use the `merge()` method to perform the join (which is an inner join
    by default) by providing the left and right dataframes, along with specifying
    which columns to join on. Since the station ID column is named differently across
    dataframes, we must specify the names with `left_on` and `right_on`. The left
    dataframe is the one we call `merge()` on, while the right one is the dataframe
    that gets passed in as an argument:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`merge()`方法来执行连接（默认是内连接），通过提供左右数据框，并指定要连接的列名。由于站点ID列在不同数据框中命名不同，我们必须使用`left_on`和`right_on`来指定列名。左侧数据框是我们调用`merge()`的方法所在的数据框，而右侧数据框是作为参数传递进来的：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Notice that we have five additional columns, which have been added to the right.
    These came from the `station_info` dataframe. This operation also kept both the
    `station` and `id` columns, which are identical:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们有五个额外的列，它们被添加到了右侧。这些列来自`station_info`数据框。这个操作也保留了`station`和`id`列，它们是完全相同的：
- en: '![Figure 4.6 – Results of an inner join between the weather and stations datasets'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6 – 天气数据集和站点数据集的内连接结果'
- en: '](img/Figure_4.6_B16834.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.6_B16834.jpg)'
- en: Figure 4.6 – Results of an inner join between the weather and stations datasets
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 天气数据集和站点数据集的内连接结果
- en: 'In order to remove the duplicate information in the `station` and `id` columns,
    we can rename one of them before the join. Consequently, we will only have to
    supply a value for the `on` parameter because the columns will share the same
    name:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了去除`station`和`id`列中的重复信息，我们可以在连接前重命名其中一列。因此，我们只需要为`on`参数提供一个值，因为这两列将共享相同的名称：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Since the columns shared the name, we only get one back after joining on them:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两列共享相同的名称，所以在连接时我们只会得到一列数据：
- en: '![Figure 4.7 – Matching the names of the joining column to prevent duplicate
    data in the result'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – 匹配连接列的名称以防止结果中的重复数据'
- en: '](img/Figure_4.7_B16834.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.7_B16834.jpg)'
- en: Figure 4.7 – Matching the names of the joining column to prevent duplicate data
    in the result
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 匹配连接列的名称以防止结果中的重复数据
- en: Tip
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: We can join on multiple columns by passing the list of column names to the `on`
    parameter or to the `left_on` and `right_on` parameters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将列名列表传递给`on`参数，或者传递给`left_on`和`right_on`参数来进行多列连接。
- en: 'Remember that we had 279 unique stations in the `station_info` dataframe, but
    only 110 unique stations for the weather data. When we performed the inner join,
    we lost all the stations that didn''t have weather observations associated with
    them. If we don''t want to lose rows on a particular side of the join, we can
    perform a left or right join instead. A **left join** requires us to list the
    dataframe with the rows that we want to keep (even if they don''t exist in the
    other dataframe) on the left and the other dataframe on the right; a **right join**
    is the inverse:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们在`station_info`数据框中有279个唯一的站点，但在天气数据中只有110个唯一的站点。当我们执行内连接时，所有没有天气观测数据的站点都丢失了。如果我们不想丢失某一侧的数据框的行，可以改为执行左连接或右连接。**左连接**要求我们将希望保留的行所在的数据框（即使它们在另一个数据框中不存在）放在左侧，将另一个数据框放在右侧；**右连接**则是相反的操作：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Wherever the other dataframe contains no data, we will get null values. We
    may want to investigate why we don''t have any weather data associated with these
    stations. Alternatively, our analysis may involve determining the availability
    of data per station, so getting null values isn''t necessarily an issue:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个数据框没有数据的地方，我们会得到null值。我们可能需要调查为什么这些站点没有关联的天气数据。或者，我们的分析可能涉及确定每个站点的数据可用性，所以获得null值不一定是个问题：
- en: '![Figure 4.8 – Null values may be introduced when not using an inner join'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.8 – 不使用内连接时可能引入null值'
- en: '](img/Figure_4.8_B16834.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.8_B16834.jpg)'
- en: Figure 4.8 – Null values may be introduced when not using an inner join
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 – 不使用内连接时可能引入null值
- en: 'Since we placed the `station_info` dataframe on the left for the left join
    and on the right for the right join, the results here are equivalent. In both
    cases, we chose to keep all the stations present in the `station_info` dataframe,
    accepting null values for the weather observations. To prove they are equivalent,
    we need to put the columns in the same order, reset the index, and sort the data:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将`station_info`数据框放在左边用于左连接，右边用于右连接，所以这里的结果是等效的。在两种情况下，我们都选择保留`station_info`数据框中所有的站点，并接受天气观测值为null。为了证明它们是等效的，我们需要将列按相同顺序排列，重置索引，并排序数据：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Note that we have additional rows in the left and right joins because we kept
    all the stations that didn''t have weather observations:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在左连接和右连接中我们有额外的行，因为我们保留了所有没有天气观测值的站点：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The final type of join is a `US1NY` in their station ID because we believed
    that stations measuring NYC weather would have to be labeled as such. This means
    that an inner join would result in losing observations from the stations in Connecticut
    and New Jersey, while a left/right join would result in either lost station information
    or lost weather data. The outer join will preserve all the data. We will also
    pass in `indicator=True` to add an additional column to the resulting dataframe,
    which will indicate which dataframe each row came from:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种连接类型是`US1NY`作为站点ID，因为我们认为测量NYC天气的站点必须标注为此。这意味着，内连接会丢失来自康涅狄格州和新泽西州站点的观测数据，而左连接或右连接则可能导致站点信息或天气数据丢失。外连接将保留所有数据。我们还会传入`indicator=True`，为结果数据框添加一列，指示每一行数据来自哪个数据框：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Indices `US1NY` in their station ID, causing nulls for the station information
    columns. The bottom two rows are stations in New York that aren''t providing weather
    observations for NYC. This join keeps all the data and will often introduce null
    values, unlike inner joins, which won''t:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 站点ID中的`US1NY`导致了站点信息列为null。底部的两行是来自纽约的站点，但没有提供NYC的天气观测数据。这个连接保留了所有数据，通常会引入null值，不同于内连接，内连接不会引入null值：
- en: '![Figure 4.9 – An outer join keeps all the data'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.9 – 外连接保留所有数据'
- en: '](img/Figure_4.9_B16834.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.9_B16834.jpg)'
- en: Figure 4.9 – An outer join keeps all the data
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 – 外连接保留所有数据
- en: 'The aforementioned joins are equivalent to SQL statements of the following
    form, where we simply change `<JOIN_TYPE>` to `(INNER) JOIN`, `LEFT JOIN`, `RIGHT
    JOIN`, or `FULL OUTER JOIN` for the appropriate join:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的连接等同于SQL语句，形式如下，其中我们只需将`<JOIN_TYPE>`替换为`(INNER) JOIN`、`LEFT JOIN`、`RIGHT
    JOIN`或`FULL OUTER JOIN`，以适应所需的连接类型：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Joining dataframes makes working with the dirty data in [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*, easier. Remember, we had data from two distinct
    stations: one had a valid station ID and the other was `?`. The `?` station was
    the only one recording the water equivalent of snow (`WESF`). Now that we know
    about joining dataframes, we can join the data from the valid station ID to the
    data from the `?` station that we are missing by date. First, we will need to
    read in the CSV file, setting the `date` column as the index. We will drop the
    duplicates and the `SNWD` column (snow depth), which we found to be uninformative
    since most of the values were infinite (both in the presence and absence of snow):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 连接数据框使得处理[*第3章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)中脏数据变得更容易，*Pandas数据清洗*也因此变得更简单。记住，我们有来自两个不同站点的数据：一个有有效的站点ID，另一个是`?`。`?`站点是唯一一个记录雪的水当量（`WESF`）的站点。现在我们了解了连接数据框的方式，我们可以通过日期将有效站点ID的数据与我们缺失的`?`站点的数据连接起来。首先，我们需要读取CSV文件，并将`date`列设为索引。然后，我们将删除重复数据和`SNWD`列（雪深），因为我们发现`SNWD`列在大多数情况下没有提供有用信息（无论是有雪还是没有雪，值都是无限大）：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Our starting data looks like this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的起始数据看起来是这样的：
- en: '![Figure 4.10 – Dirty data from the previous chapter'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.10 – 上一章中的脏数据'
- en: '](img/Figure_4.10_B16834.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.10_B16834.jpg)'
- en: Figure 4.10 – Dirty data from the previous chapter
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 上一章中的脏数据
- en: 'Now, we need to create a dataframe for each station. To reduce output, we will
    drop some additional columns:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要为每个站点创建一个数据框。为了减少输出，我们将删除一些额外的列：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This time, the column we want to join on (the date) is actually the index,
    so we will pass in `left_index` to indicate that the column to use from the left
    dataframe is the index, and then `right_index` to indicate the same for the right
    dataframe. We will perform a left join to make sure we don''t lose any rows from
    our valid station, and, where possible, augment them with the observations from
    the `?` station:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们要连接的列（日期）实际上是索引，因此我们将传入`left_index`来表示左侧数据框要使用的列是索引，接着传入`right_index`来表示右侧数据框的相应列也是索引。我们将执行左连接，确保不会丢失任何有效站点的行，并且在可能的情况下，用`?`站点的观测数据补充它们：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'For all the columns that the dataframes had in common, but weren''t part of
    the join, we have two versions now. The versions coming from the left dataframe
    have the `_x` suffix appended to the column names, and those coming from the right
    dataframe have `_y` as the suffix:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据框中共有的所有列，但未参与连接的列，现在我们有了两个版本。来自左侧数据框的版本在列名后添加了`_x`后缀，来自右侧数据框的版本则在列名后添加了`_y`后缀：
- en: '![Figure 4.11 – Merging weather data from different stations'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.11 – 来自不同站点的天气数据合并'
- en: '](img/Figure_4.11_B16834.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.11_B16834.jpg)'
- en: Figure 4.11 – Merging weather data from different stations
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 来自不同站点的天气数据合并
- en: 'We can provide our own suffixes with the `suffixes` parameter. Let''s use a
    suffix for the `?` station only:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`suffixes`参数提供自定义的后缀。让我们只为`?`站点使用一个后缀：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Since we specified an empty string for the left suffix, the columns coming
    from the left dataframe have their original names. However, the right suffix of
    `_?` was added to the names of the columns that came from the right dataframe:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们为左侧后缀指定了空字符串，来自左侧数据框的列保持其原始名称。然而，右侧后缀`_?`被添加到了来自右侧数据框的列名中：
- en: '![Figure 4.12 – Specifying the suffix for shared columns not being used in
    the join'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.12 – 为不参与连接的共享列指定后缀'
- en: '](img/Figure_4.12_B16834.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.12_B16834.jpg)'
- en: Figure 4.12 – Specifying the suffix for shared columns not being used in the
    join
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – 为不参与连接的共享列指定后缀
- en: 'When we are joining on the index, an easier way to do this is to use the `join()`
    method instead of `merge()`. It also defaults to an inner join, but this behavior
    can be changed with the `how` parameter, just like with `merge()`. The `join()`
    method will always use the index of the left dataframe to join, but it can use
    a column in the right dataframe if its name is passed to the `on` parameter. Note
    that suffixes are now specified using `lsuffix` for the left dataframe''s suffix
    and `rsuffix` for the right one. This yields the same result as the previous example
    (*Figure 4.12*):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在索引上进行连接时，一种更简单的方法是使用`join()`方法，而不是`merge()`。它也默认为内连接，但可以通过`how`参数更改此行为，就像`merge()`一样。`join()`方法将始终使用左侧数据框的索引进行连接，但如果传递右侧数据框的列名给`on`参数，它也可以使用右侧数据框中的列。需要注意的是，后缀现在通过`lsuffix`指定左侧数据框的后缀，`rsuffix`指定右侧数据框的后缀。这将产生与之前示例相同的结果（*图
    4.12*）：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: One important thing to keep in mind is that joins can be rather resource-intensive,
    so it is often beneficial to figure out what will happen to the rows before going
    through with it. If we don't already know what type of join we want, this can
    help give us an idea. We can use **set operations** on the index we plan to join
    on to figure this out.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一件重要事情是，连接操作可能相当消耗资源，因此在执行连接之前，弄清楚行会发生什么通常是有益的。如果我们还不知道想要哪种类型的连接，使用这种方式可以帮助我们得到一些思路。我们可以在计划连接的索引上使用**集合操作**来弄清楚这一点。
- en: 'Remember that the mathematical definition of a **set** is a collection of distinct
    objects. By definition, the index is a set. Set operations are often explained
    with Venn diagrams:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，**集合**的数学定义是不同对象的集合。按照定义，索引是一个集合。集合操作通常通过维恩图来解释：
- en: '![Figure 4.13 – Set operations'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.13 – 集合操作'
- en: '](img/Figure_4.13_B16834.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.13_B16834.jpg)'
- en: Figure 4.13 – Set operations
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 – 集合操作
- en: Important note
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that `set` is also a Python type that's available in the standard library.
    A common use of sets is to remove duplicates from a list. More information on
    sets in Python can be found in the documentation at [https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset](https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`set`也是Python标准库中可用的一种类型。集合的一个常见用途是去除列表中的重复项。有关Python中集合的更多信息，请参见文档：[https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset](https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset)。
- en: 'Let''s use the `weather` and `station_info` dataframes to illustrate set operations.
    First, we must set the index to the column(s) that will be used for the join operation:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`weather`和`station_info`数据框来说明集合操作。首先，我们必须将索引设置为用于连接操作的列：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To see what will remain with an inner join, we can take the **intersection**
    of the indices, which shows us the overlapping stations:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看内连接后将保留的内容，我们可以取索引的**交集**，这将显示我们重叠的站点：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As we saw when we ran the inner join, we only got station information for the
    stations with weather observations. This doesn''t tell us what we lost, though;
    for this, we need to find the **set difference**, which will subtract the sets
    and give us the values of the first index that aren''t in the second. With the
    set difference, we can easily see that, when performing an inner join, we don''t
    lose any rows from the weather data, but we lose 169 stations that don''t have
    weather observations:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在执行内连接时看到的那样，我们只得到了有天气观测的站点信息。但这并没有告诉我们丢失了什么；为此，我们需要找到**集合差异**，即减去两个集合，得到第一个索引中不在第二个索引中的值。通过集合差异，我们可以轻松看到，在执行内连接时，我们并没有丢失天气数据中的任何行，但我们丢失了169个没有天气观测的站点：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that this output also tells us how left and right joins will turn out.
    To avoid losing rows, we want to put the `station_info` dataframe on the same
    side as the join (on the left for a left join and on the right for a right join).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个输出还告诉我们左连接和右连接的结果如何。为了避免丢失行，我们希望将`station_info`数据框放在连接的同一侧（左连接时在左边，右连接时在右边）。
- en: Tip
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'We can use the `symmetric_difference()` method on the indices of the dataframes
    involved in the join to see what will be lost from both sides: `index_1.symmetric_difference(index_2)`.
    The result will be the values that are only in one of the indices. An example
    is in the notebook.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`symmetric_difference()`方法对参与连接的数据框的索引进行操作，查看从两侧丢失的内容：`index_1.symmetric_difference(index_2)`。结果将是仅在其中一个索引中存在的值。笔记本中有一个示例。
- en: 'Lastly, we can use the `weather` dataframe contains the stations repeated throughout
    because they provide daily measurements, so we call the `unique()` method before
    taking the union to see the number of stations we will keep:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`weather`数据框，它包含了重复出现的站点信息，因为这些站点提供每日测量值，所以在进行并集操作之前，我们会调用`unique()`方法查看我们将保留的站点数量：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The *Further reading* section at the end of this chapter contains some resources
    on set operations and how `pandas` compares to SQL. For now, let's move on to
    data enrichment.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最后的*进一步阅读*部分包含了一些关于集合操作的资源，以及`pandas`与SQL的对比。目前，让我们继续进行数据丰富化操作。
- en: Using DataFrame operations to enrich data
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DataFrame操作来丰富数据
- en: 'Now that we''ve discussed how to query and merge `DataFrame` objects, let''s
    learn how to perform complex operations on them to create and modify columns and
    rows. For this section, we will be working in the `2-dataframe_operations.ipynb`
    notebook using the weather data, along with Facebook stock''s volume traded and
    opening, high, low, and closing prices daily for 2018\. Let''s import what we
    will need and read in the data:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了如何查询和合并`DataFrame`对象，接下来让我们学习如何在这些对象上执行复杂操作，创建和修改列和行。在本节中，我们将在`2-dataframe_operations.ipynb`笔记本中使用天气数据，以及2018年Facebook股票的交易量和每日开盘价、最高价、最低价和收盘价。让我们导入所需的库并读取数据：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We will begin by reviewing operations that summarize entire rows and columns
    before moving on to binning, applying functions across rows and columns, and window
    calculations, which summarize data along a certain number of observations at a
    time (such as moving averages).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先回顾总结整行和整列的操作，然后再学习分箱、在行和列上应用函数，以及窗口计算，这些操作是在一定数量的观测值上对数据进行汇总（例如，移动平均）。
- en: Arithmetic and statistics
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算术和统计
- en: Pandas has several methods for calculating statistics and performing mathematical
    operations, including comparisons, floor division, and the modulo operation. These
    methods give us more flexibility in how we define the calculation by allowing
    us to specify the axis to perform the calculation on (when performing it on a
    `DataFrame` object). By default, the calculation will be performed along the columns
    (`axis=1` or `axis='columns'`), which generally contain observations of a single
    variable of a single data type; however, we can pass in `axis=0` or `axis='index'`
    to perform the calculation along the rows instead.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas提供了几种计算统计数据和执行数学运算的方法，包括比较、整除和取模运算。这些方法使我们在定义计算时更加灵活，允许我们指定在哪个轴上执行计算（当对`DataFrame`对象进行操作时）。默认情况下，计算将在列上执行（`axis=1`或`axis='columns'`），列通常包含单一变量的单一数据类型的观测值；但是，我们也可以传入`axis=0`或`axis='index'`来沿着行执行计算。
- en: In this section, we are going to use a few of these methods to create new columns
    and modify our data to see how we can use new data to draw some initial conclusions.
    Note that the complete list can be found at [https://pandas.pydata.org/pandas-docs/stable/reference/series.html#binary-operator-functions](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#binary-operator-functions).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用这些方法中的一些来创建新列并修改数据，看看如何利用新数据得出一些初步结论。完整的列表可以在[https://pandas.pydata.org/pandas-docs/stable/reference/series.html#binary-operator-functions](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#binary-operator-functions)找到。
- en: 'To start off, let''s create a column with the Z-score for the volume traded
    in Facebook stock and use it to find the days where the Z-score is greater than
    three in absolute value. These values are more than three standard deviations
    from the mean, which may be abnormal (depending on the data). Remember from our
    discussion of Z-scores in [*Chapter 1*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015),
    *Introduction to Data Analysis*, that we calculate them by subtracting the mean
    and dividing by the standard deviation. Rather than using mathematical operators
    for subtraction and division, we will use the `sub()` and `div()` methods, respectively:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一列Facebook股票交易量的Z分数，并利用它找出Z分数绝对值大于三的日期。这些值距离均值超过三倍标准差，可能是异常值（具体取决于数据）。回想一下我们在[*第1章*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015)《数据分析导论》中讨论的Z分数，我们通过减去均值并除以标准差来计算Z分数。我们将不使用减法和除法的数学运算符，而是分别使用`sub()`和`div()`方法：
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Five days in 2018 had Z-scores for volume traded greater than three in absolute
    value. These dates in particular will come up often in the rest of this chapter
    as they mark some trouble points for Facebook''s stock price:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年有五天的交易量Z分数绝对值大于三。这些日期将会在本章的后续内容中频繁出现，因为它们标志着Facebook股价的一些问题点：
- en: '![Figure 4.14 – Adding a Z-score column'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.14 – 添加Z分数列'
- en: '](img/Figure_4.14_B16834.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.14_B16834.jpg)'
- en: Figure 4.14 – Adding a Z-score column
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – 添加Z分数列
- en: 'Two other very useful methods are `rank()` and `pct_change()`, which let us
    rank the values of a column (and store them in a new column) and calculate the
    percentage change between periods, respectively. By combining these, we can see
    which five days had the largest percentage change of volume traded in Facebook
    stock from the day prior:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两个非常有用的方法是`rank()`和`pct_change()`，它们分别用于对列的值进行排名（并将排名存储在新列中）和计算不同时间段之间的百分比变化。通过将这两者结合使用，我们可以看到Facebook股票在前一天与五天内交易量变化百分比最大的一天：
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The day with the largest percentage change in volume traded was January 12,
    2018, which happens to coincide with one of the many Facebook scandals that shook
    the stock in 2018 ([https://www.cnbc.com/2018/11/20/facebooks-scandals-in-2018-effect-on-stock.html](https://www.cnbc.com/2018/11/20/facebooks-scandals-in-2018-effect-on-stock.html)).
    This was when they announced changes to the news feed to prioritize content from
    a user''s friends over brands they follow. Given that a large component of Facebook''s
    revenue comes from advertising (nearly 89% in 2017, *source*: [https://www.investopedia.com/ask/answers/120114/how-does-facebook-fb-make-money.asp](https://www.investopedia.com/ask/answers/120114/how-does-facebook-fb-make-money.asp)),
    this caused panic as many sold the stock, driving up the volume traded drastically
    and dropping the stock price:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 交易量变化百分比最大的一天是 2018 年 1 月 12 日，这恰好与 2018 年震撼股市的多个 Facebook 丑闻之一重合（[https://www.cnbc.com/2018/11/20/facebooks-scandals-in-2018-effect-on-stock.html](https://www.cnbc.com/2018/11/20/facebooks-scandals-in-2018-effect-on-stock.html)）。当时
    Facebook 公布了新闻源的变化，优先显示来自用户朋友的内容，而非他们所关注的品牌。考虑到 Facebook 的收入大部分来自广告（2017 年约为 89%，*来源*：[https://www.investopedia.com/ask/answers/120114/how-does-facebook-fb-make-money.asp](https://www.investopedia.com/ask/answers/120114/how-does-facebook-fb-make-money.asp)），这引发了恐慌，许多人抛售股票，导致交易量大幅上升，并使股票价格下跌：
- en: '![Figure 4.15 – Ranking trading days by percentage change in volume traded'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.15 – 按交易量变化百分比对交易日进行排名'
- en: '](img/Figure_4.15_B16834.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.15_B16834.jpg)'
- en: Figure 4.15 – Ranking trading days by percentage change in volume traded
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 – 按交易量变化百分比对交易日进行排名
- en: 'We can use slicing to look at the change around this announcement:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用切片方法查看这一公告前后的变化：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Notice how we are able to combine everything we learned in the last few chapters
    to get interesting insights from our data. We were able to sift through a year''s
    worth of stock data and find some days that had large effects on Facebook stock
    (good or bad):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们如何能够将前几章所学的所有内容结合起来，从数据中获取有趣的见解。我们能够筛选出一整年的股票数据，并找到一些对 Facebook 股票产生巨大影响的日期（无论是好是坏）：
- en: '![Figure 4.16 – Facebook stock data before and after announcing changes to
    the news feed'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.16 – 公布新闻源变化前后 Facebook 股票数据'
- en: '](img/Figure_4.16_B16834.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.16_B16834.jpg)'
- en: Figure 4.16 – Facebook stock data before and after announcing changes to the
    news feed
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16 – 公布新闻源变化前后 Facebook 股票数据
- en: 'Lastly, we can inspect the dataframe with aggregated Boolean operations. For
    example, we can see that Facebook stock never had a daily low price greater than
    $215 in 2018 with the `any()` method:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用聚合布尔操作来检查数据框。例如，我们可以使用`any()`方法看到 Facebook 股票在 2018 年内从未有过低于 $215 的日最低价：
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If we want to see if all the rows in a column meet the criteria, we can use
    the `all()` method. This tells us that Facebook has at least one day for the opening,
    high, low, and closing prices with a value less than or equal to $215:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想查看某列中的所有行是否符合标准，可以使用`all()`方法。该方法告诉我们，Facebook 至少有一天的开盘价、最高价、最低价和收盘价小于或等于
    $215：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, let's take a look at how we can use binning to divide up our data rather
    than a specific value, such as $215 in the `any()` and `all()` examples.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用分箱法来划分数据，而不是使用具体的数值，例如在`any()`和`all()`示例中的 $215。
- en: Binning
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分箱法
- en: Sometimes, it's more convenient to work with categories rather than the specific
    values. A common example is working with ages—most likely, we don't want to look
    at the data for each age, such as 25 compared to 26; however, we may very well
    be interested in how the group of individuals aged 25-34 compares to the group
    of those aged 35-44\. This is called **binning** or **discretizing** (going from
    continuous to discrete); we take our data and place the observations into bins
    (or buckets) matching the range they fall into. By doing so, we can drastically
    reduce the number of distinct values our data can take on and make it easier to
    analyze.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，使用类别而不是具体数值进行分析更加方便。一个常见的例子是年龄分析——通常我们不想查看每个年龄的数据，比如 25 岁和 26 岁之间的差异；然而，我们很可能会对
    25-34 岁组与 35-44 岁组之间的比较感兴趣。这就是所谓的**分箱**或**离散化**（从连续数据转为离散数据）；我们将数据按照其所属的范围放入不同的箱（或桶）中。通过这样做，我们可以大幅减少数据中的不同数值，并使分析变得更容易。
- en: Important note
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: While binning our data can make certain parts of the analysis easier, keep in
    mind that it will reduce the information in that field since the granularity is
    reduced.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对数据进行分箱可以使某些分析部分变得更简单，但请记住，它会减少该字段中的信息，因为粒度被降低了。
- en: 'One interesting thing we could do with the volume traded would be to see which
    days had high trade volume and look for news about Facebook on those days or large
    swings in price. Unfortunately, it is highly unlikely that the volume will be
    the same any two days; in fact, we can confirm that, in the data, no two days
    have the same volume traded:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的一件有趣的事情是观察哪些日期的交易量较高，并查看这些日期是否有关于Facebook的新闻，或者是否有股价的大幅波动。不幸的是，几乎不可能有两天的交易量是相同的；事实上，我们可以确认，在数据中，没有两天的交易量是相同的：
- en: '[PRE30]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Remember that `fb.volume.value_counts()` gives us the number of occurrences
    for each unique value for `volume`. We can then create a Boolean mask for whether
    the count is greater than 1 and sum it up (`True` evaluates to `1` and `False`
    evaluates to `0`). Alternatively, we can use `any()` instead of `sum()`, which,
    rather than telling us the number of unique values of `volume` that had more than
    one occurrence, would give us `True` if at least one volume traded amount occurred
    more than once and `False` otherwise.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`fb.volume.value_counts()`会告诉我们每个唯一`volume`值的出现次数。然后，我们可以创建一个布尔掩码，判断该次数是否大于1，并对其进行求和（`True`会被计算为`1`，`False`则为`0`）。另外，我们也可以使用`any()`代替`sum()`，这样做会告诉我们，如果至少有一个交易量发生了多次，返回`True`，否则返回`False`，而不是告诉我们有多少个唯一的`volume`值出现超过一次。
- en: 'Clearly, we will need to create some ranges for the volume traded in order
    to look at the days of high trading volume, but how do we decide which range is
    a good range? One way is to use the `pd.cut()` function for binning based on value.
    First, we should decide how many bins we want to create—three seems like a good
    split, since we can label the bins low, medium, and high. Next, we need to determine
    the width of each bin; `pandas` tries to make this process as painless as possible,
    so if we want equally-sized bins, all we have to do is specify the number of bins
    we want (otherwise, we must specify the upper bound for each bin as a list):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们需要为交易量创建一些区间，以便查看高交易量的日期，但我们如何决定哪个区间是合适的呢？一种方法是使用`pd.cut()`函数基于数值进行分箱。首先，我们应该决定要创建多少个区间——三个位数似乎是一个好的分割，因为我们可以将这些区间分别标记为低、中和高。接下来，我们需要确定每个区间的宽度；`pandas`会尽可能简化这个过程，所以如果我们想要等宽的区间，只需要指定我们想要的区间数（否则，我们必须指定每个区间的上限作为列表）：
- en: '[PRE31]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Tip
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Note that we provided labels for each bin here; if we don't do this, each bin
    will be labeled by the interval of values it includes, which may or may not be
    helpful for us, depending on our application. If we want to both label the values
    and see the bins afterward, we can pass in `retbins=True` when we call `pd.cut()`.
    Then, we can access the binned data as the first element of the tuple that is
    returned, and the bin ranges themselves as the second element.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里为每个区间提供了标签；如果我们不这样做，系统会根据包含的数值区间为每个区间标记标签，这可能对我们有用，也可能没有用，取决于我们的应用。如果我们想同时标记区间的值并在后续查看区间，可以在调用`pd.cut()`时传入`retbins=True`。然后，我们可以通过返回的元组的第一个元素访问分箱数据，第二个元素则是区间范围。
- en: 'It looks like an overwhelming majority of the trading days were in the low-volume
    bin; keep in mind that this is all relative because we evenly divided the range
    between the minimum and maximum trading volumes. Let''s look at the data for the
    three days of high volume:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来绝大多数交易日都属于低交易量区间；请记住，这一切都是相对的，因为我们将最小和最大交易量之间的范围进行了均匀分割。现在让我们看一下这三天高交易量的交易数据：
- en: '[PRE32]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Even among the high-volume days, we can see that July 26, 2018 had a much higher
    trade volume compared to the other two dates in March (nearly 40 million additional
    shares were traded):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在高交易量的日子里，我们也能看到2018年7月26日的交易量远高于3月的其他两个日期（交易量增加了近4000万股）：
- en: '![Figure 4.17 – Facebook stock data on days in the high-volume traded bucket'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.17 – 高交易量区间内的Facebook股票数据'
- en: '](img/Figure_4.17_B16834.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.17_B16834.jpg)'
- en: Figure 4.17 – Facebook stock data on days in the high-volume traded bucket
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17 – 高交易量区间内的Facebook股票数据
- en: 'In fact, querying a search engine for *Facebook stock price July 26, 2018*
    reveals that Facebook had announced their earnings and disappointing user growth
    after market close on July 25th, which was followed by lots of after-hours selling.
    When the market opened the next morning, the stock had dropped from $217.50 at
    close on the 25th to $174.89 at market open on the 26th. Let''s pull out this
    data:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，通过搜索引擎查询 *Facebook 股票价格 2018 年 7 月 26 日* 可以发现，Facebook 在 7 月 25 日股市收盘后宣布了其收益和令人失望的用户增长，随后发生了大量盘后抛售。当第二天股市开盘时，股票从
    25 日收盘的 $217.50 跌至 26 日开盘时的 $174.89。让我们提取这些数据：
- en: '[PRE33]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Not only was there a huge drop in stock price, but the volume traded also skyrocketed,
    increasing by more than 100 million. All of this resulted in a loss of about $120
    billion in Facebook''s market capitalization ([https://www.marketwatch.com/story/facebook-stock-crushed-after-revenue-user-growth-miss-2018-07-25](https://www.marketwatch.com/story/facebook-stock-crushed-after-revenue-user-growth-miss-2018-07-25)):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅股票价格大幅下跌，而且交易量也飙升，增加了超过 1 亿股。所有这些导致了 Facebook 市值约 1200 亿美元的损失（[https://www.marketwatch.com/story/facebook-stock-crushed-after-revenue-user-growth-miss-2018-07-25](https://www.marketwatch.com/story/facebook-stock-crushed-after-revenue-user-growth-miss-2018-07-25)）：
- en: '![Figure 4.18 – Facebook stock data leading up to the day with the highest
    volume traded in 2018'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.18 – 2018 年 Facebook 股票数据，直至最高交易量那天'
- en: '](img/Figure_4.18_B16834.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.18_B16834.jpg)'
- en: Figure 4.18 – Facebook stock data leading up to the day with the highest volume
    traded in 2018
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.18 – 2018 年 Facebook 股票数据，直至最高交易量那天
- en: 'If we look at the other two days marked as high-volume trading days, we will
    find a plethora of information as to why. Both of these days were marked by scandal
    for Facebook. The Cambridge Analytica political data privacy scandal broke on
    Saturday, March 17, 2018, so trading with this information didn''t commence until
    Monday the 19th ([https://www.nytimes.com/2018/03/19/technology/facebook-cambridge-analytica-explained.html](https://www.nytimes.com/2018/03/19/technology/facebook-cambridge-analytica-explained.html)):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看另外两天被标记为高交易量的交易日，会发现有大量的信息说明原因。这两天都与 Facebook 的丑闻有关。剑桥分析公司（Cambridge Analytica）的政治数据隐私丑闻于
    2018 年 3 月 17 日星期六爆发，因此与该信息相关的交易直到 3 月 19 日星期一才开始（[https://www.nytimes.com/2018/03/19/technology/facebook-cambridge-analytica-explained.html](https://www.nytimes.com/2018/03/19/technology/facebook-cambridge-analytica-explained.html)）：
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Things only got worse once more information was revealed in the following days
    with regards to the severity of the incident:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在接下来的几天里披露了关于事件严重性的更多信息，情况变得更糟：
- en: '![Figure 4.19 – Facebook stock data when the Cambridge Analytica scandal broke'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.19 – 剑桥分析丑闻爆发时的 Facebook 股票数据'
- en: '](img/Figure_4.19_B16834.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.19_B16834.jpg)'
- en: Figure 4.19 – Facebook stock data when the Cambridge Analytica scandal broke
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.19 – 剑桥分析丑闻爆发时的 Facebook 股票数据
- en: As for the third day of high trading volume (March 26, 2018), the FTC launched
    an investigation into the Cambridge Analytica scandal, so Facebook's woes continued
    ([https://www.cnbc.com/2018/03/26/ftc-confirms-facebook-data-breach-investigation.html](https://www.cnbc.com/2018/03/26/ftc-confirms-facebook-data-breach-investigation.html)).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 至于第三个高交易量的交易日（2018 年 3 月 26 日），美国联邦贸易委员会（FTC）启动了对剑桥分析丑闻的调查，因此 Facebook 的困境持续下去（[https://www.cnbc.com/2018/03/26/ftc-confirms-facebook-data-breach-investigation.html](https://www.cnbc.com/2018/03/26/ftc-confirms-facebook-data-breach-investigation.html)）。
- en: 'If we look at some of the dates within the medium trading volume group, we
    can see that many are part of the three trading events we just discussed. This
    forces us to reexamine how we created the bins in the first place. Perhaps equal-width
    bins wasn''t the answer? Most days were pretty close in volume traded; however,
    a few days caused the bin width to be rather large, which left us with a large
    imbalance of days per bin:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看中等交易量组内的一些日期，会发现许多都属于我们刚刚讨论的三个交易事件。这迫使我们重新审视最初创建区间的方式。也许等宽区间并不是答案？大多数日期的交易量相对接近；然而，少数几天导致区间宽度较大，这使得每个区间内的日期数量不均衡：
- en: '![Figure 4.20 – Visualizing the equal-width bins'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.20 – 可视化等宽区间'
- en: '](img/Figure_4.20_B16834.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.20_B16834.jpg)'
- en: Figure 4.20 – Visualizing the equal-width bins
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.20 – 可视化等宽区间
- en: 'If we want each bin to have an equal number of observations, we can split the
    bins based on evenly-spaced quantiles using the `pd.qcut()` function. We can bin
    the volumes into quartiles to evenly bucket the observations into bins of varying
    width, giving us the 63 highest trading volume days in the **q4** bin:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望每个区间有相同数量的观测值，可以使用 `pd.qcut()` 函数基于均匀间隔的分位数来划分区间。我们可以将交易量划分为四分位数，从而将观测值均匀分配到宽度不同的区间中，得到**q4**区间中的
    63 个最高交易量的天数：
- en: '[PRE35]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Notice that the bins don''t cover the same range of volume traded anymore:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些区间现在不再覆盖相同的交易量范围：
- en: '![Figure 4.21 – Visualizing the bins based on quartiles'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.21 – 基于四分位数可视化区间](img/Figure_4.21_B16834.jpg)'
- en: '](img/Figure_4.21_B16834.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.21_B16834.jpg)'
- en: Figure 4.21 – Visualizing the bins based on quartiles
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.21 – 基于四分位数可视化区间
- en: Tip
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: In both of these examples, we let `pandas` calculate the bin ranges; however,
    both `pd.cut()` and `pd.qcut()` allow us to specify the upper bounds for each
    bin as a list.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个例子中，我们让 `pandas` 计算区间范围；然而，`pd.cut()` 和 `pd.qcut()` 都允许我们将每个区间的上界指定为列表。
- en: Applying functions
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用函数
- en: So far, most of the actions we have taken on our data have been column-specific.
    When we want to run the same code on all the columns in our dataframe, we can
    use the `apply()` method for more succinct code. Note that this will not be done
    in-place.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对数据进行的大多数操作都是针对单独列进行的。当我们希望在数据框架的所有列上运行相同的代码时，可以使用 `apply()` 方法，使代码更加简洁。请注意，这个操作不会就地执行。
- en: 'Before we get started, let''s isolate the weather observations from the Central
    Park station and pivot the data:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，让我们先隔离中央公园站的天气观测数据，并将数据透视：
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let''s calculate the Z-scores of the `TMIN` (minimum temperature), `TMAX` (maximum
    temperature), and `PRCP` (precipitation) observations in Central Park in October
    2018\. It''s important that we don''t try to take the Z-scores across the full
    year. NYC has four seasons, and what is considered normal weather will depend
    on which season we are looking at. By isolating our calculation to October, we
    can see if October had any days with very different weather:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算 2018 年 10 月中央公园的`TMIN`（最低气温）、`TMAX`（最高气温）和`PRCP`（降水量）观测值的 Z 分数。重要的是，我们不要试图跨全年的数据计算
    Z 分数。纽约市有四个季节，什么是正常的天气取决于我们查看的是哪个季节。通过将计算范围限制在 10 月份，我们可以看看 10 月是否有任何天气与其他天差异很大：
- en: '[PRE37]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`TMIN` and `TMAX` don''t appear to have any values that differ much from the
    rest of October, but `PRCP` does:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`TMIN` 和 `TMAX` 似乎没有任何与 10 月其余时间显著不同的数值，但 `PRCP` 确实有：'
- en: '![Figure 4.22 – Calculating Z-scores for multiple columns at once'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.22 – 一次计算多个列的 Z 分数](img/Figure_4.22_B16834.jpg)'
- en: '](img/Figure_4.22_B16834.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.22_B16834.jpg)'
- en: Figure 4.22 – Calculating Z-scores for multiple columns at once
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.22 – 一次计算多个列的 Z 分数
- en: 'We can use `query()` to extract the value for this date:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `query()` 提取该日期的值：
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If we look at the summary statistics for precipitation in October, we can see
    that this day had much more precipitation than the rest:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看 10 月份降水量的汇总统计数据，我们可以看到，这一天的降水量远远超过其他日子：
- en: '[PRE39]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The `apply()` method lets us run vectorized operations on entire columns or
    rows at once. We can apply pretty much any function we can think of as long as
    those operations are valid on all the columns (or rows) in our data. For example,
    we can use the `pd.cut()` and `pd.qcut()` binning functions we discussed in the
    previous section to divide each column into bins (provided we want the same number
    of bins or value ranges). Note that there is also an `applymap()` method if the
    function we want to apply isn't vectorized. Alternatively, we can use `np.vectorize()`
    to vectorize our functions for use with `apply()`. Consult the notebook for an
    example.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply()` 方法让我们可以对整个列或行一次性运行矢量化操作。我们可以应用几乎任何我们能想到的函数，只要这些操作在数据的所有列（或行）上都是有效的。例如，我们可以使用之前讨论的
    `pd.cut()` 和 `pd.qcut()` 分箱函数，将每一列分成若干区间（前提是我们希望分成相同数量的区间或数值范围）。请注意，如果我们要应用的函数不是矢量化的，还可以使用
    `applymap()` 方法。或者，我们可以使用 `np.vectorize()` 将函数矢量化，以便与 `apply()` 一起使用。有关示例，请参考笔记本。'
- en: 'Pandas does provide some functionality for iterating over the dataframe, including
    the `iteritems()`, `itertuples()`, and `iterrows()` methods; however, we should
    avoid using these unless we absolutely can''t find another solution. Pandas and
    NumPy are designed for vectorized operations, which are much faster because they
    are written in efficient C code; by writing a loop to iterate one element at a
    time, we are making it more computationally intensive due to the way Python implements
    integers and floats. For instance, look at how the time to complete the simple
    operation of adding the number `10` to each value in a series of floats grows
    linearly with the number of rows when using `iteritems()`, but stays near zero,
    regardless of size, when using a vectorized operation:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas确实提供了一些用于迭代数据框的功能，包括`iteritems()`、`itertuples()`和`iterrows()`方法；然而，除非我们完全找不到其他解决方案，否则应避免使用这些方法。Pandas和NumPy是为向量化操作设计的，这些操作要快得多，因为它们是用高效的C代码编写的；通过编写一个循环逐个迭代元素，我们让计算变得更加复杂，因为Python实现整数和浮点数的方式。举个例子，看看完成一个简单操作（将数字`10`加到每个浮点数值上）所需的时间，使用`iteritems()`时，它会随着行数的增加线性增长，而使用向量化操作时，无论行数多大，所需时间几乎保持不变：
- en: '![Figure 4.23 – Vectorized versus iterative operations'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.23 – 向量化与迭代操作'
- en: '](img/Figure_4.23_B16834.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.23_B16834.jpg)'
- en: Figure 4.23 – Vectorized versus iterative operations
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.23 – 向量化与迭代操作
- en: All the functions and methods we have used so far have involved the full row
    or column; however, sometimes, we are more interested in performing window calculations,
    which use a section of the data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用的所有函数和方法都涉及整个行或列；然而，有时我们更关心进行窗口计算，它们使用数据的一部分。
- en: Window calculations
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 窗口计算
- en: Pandas makes it possible to perform calculations over a window or range of rows/columns.
    In this section, we will discuss a few ways of constructing these windows. Depending
    on the type of window, we get a different look at our data.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas使得对窗口或行/列范围进行计算成为可能。在这一部分中，我们将讨论几种构建这些窗口的方法。根据窗口类型的不同，我们可以得到数据的不同视角。
- en: Rolling windows
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滚动窗口
- en: 'When our index is of type `DatetimeIndex`, we can specify the window in day
    parts (such as `2H` for two hours or `3D` for three days); otherwise, we can specify
    the number of periods as an integer. Say we are interested in the amount of rain
    that has fallen in a rolling 3-day window; it would be quite tedious (and probably
    inefficient) to implement this with what we have learned so far. Fortunately,
    we can use the `rolling()` method to get this information easily:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的索引类型为`DatetimeIndex`时，我们可以按日期部分指定窗口（例如，`2H`表示两小时，`3D`表示三天）；否则，我们可以指定期数的整数值。例如，如果我们对滚动3天窗口内的降水量感兴趣；用我们目前所学的知识来实现这一点会显得相当繁琐（而且可能效率低下）。幸运的是，我们可以使用`rolling()`方法轻松获取这些信息：
- en: '[PRE40]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'After performing the rolling 3-day sum, each date will show the sum of that
    day''s and the previous two days'' precipitation:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行滚动3天总和后，每个日期将显示该日期和前两天的降水量总和：
- en: '![Figure 4.24 – Rolling 3-day total precipitation'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.24 – 滚动3天总降水量'
- en: '](img/Figure_4.24_B16834.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.24_B16834.jpg)'
- en: Figure 4.24 – Rolling 3-day total precipitation
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.24 – 滚动3天总降水量
- en: Tip
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: If we want to use dates for the rolling calculation, but don't have dates in
    the index, we can pass the name of our date column to the `on` parameter in the
    call to `rolling()`. Conversely, if we want to use an integer index of row numbers,
    we can simply pass in an integer as the window; for example, `rolling(3)` for
    a 3-row window.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想使用日期进行滚动计算，但索引中没有日期，我们可以将日期列的名称传递给`rolling()`调用中的`on`参数。相反，如果我们想使用行号的整数索引，我们可以直接传递一个整数作为窗口；例如，`rolling(3)`表示一个3行的窗口。
- en: 'To change the aggregation, all we have to do is call a different method on
    the result of `rolling()`; for example, `mean()` for the average and `max()` for
    the maximum. The rolling calculation can also be applied to all the columns at
    once:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改聚合，只需在`rolling()`的结果上调用不同的方法；例如，`mean()`表示平均值，`max()`表示最大值。滚动计算也可以应用于所有列一次：
- en: '[PRE41]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This gives us the 3-day rolling average for all the weather observations from
    Central Park:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这会给我们提供来自中央公园的所有天气观测数据的3天滚动平均值：
- en: '![Figure 4.25 – Rolling 3-day average for all weather observations'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.25 – 所有天气观测数据的滚动3天平均值'
- en: '](img/Figure_4.25_B16834.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.25_B16834.jpg)'
- en: Figure 4.25 – Rolling 3-day average for all weather observations
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.25 – 所有天气观测数据的滚动3天平均值
- en: 'To apply different aggregations across columns, we can use the `agg()` method
    instead; it allows us to specify the aggregations to perform per column as a predefined
    or custom function. We simply pass in a dictionary mapping the columns to the
    aggregation to perform on them. Let''s find the rolling 3-day maximum temperature
    (`TMAX`), minimum temperature (`TMIN`), average wind speed (`AWND`), and total
    precipitation (`PRCP`). Then, we will join it to the original data so that we
    can compare the results:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 若要对不同的列应用不同的聚合操作，我们可以使用`agg()`方法，它允许我们为每列指定预定义的或自定义的聚合函数。我们只需传入一个字典，将列映射到需要执行的聚合操作。让我们来找出滚动的3天最高气温（`TMAX`）、最低气温（`TMIN`）、平均风速（`AWND`）和总降水量（`PRCP`）。然后，我们将其与原始数据进行合并，以便进行比较：
- en: '[PRE42]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Using `agg()`, we were able to calculate different rolling aggregations for
    each column:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`agg()`，我们能够为每列计算不同的滚动聚合操作：
- en: '![Figure 4.26 – Using different rolling calculations per column'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.26 – 对每列使用不同的滚动计算'
- en: '](img/Figure_4.26_B16834.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.26_B16834.jpg)'
- en: Figure 4.26 – Using different rolling calculations per column
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.26 – 对每列使用不同的滚动计算
- en: Tip
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 小提示
- en: 'We can also use variable-width windows with a little extra effort: we can either
    create a subclass of `BaseIndexer` and provide the logic for determining the window
    bounds in the `get_window_bounds()` method (more information can be found at [https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#custom-window-rolling](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#custom-window-rolling)),
    or we can use one of the predefined classes in the `pandas.api.indexers` module.
    The notebook we are currently working in contains an example of using the `VariableOffsetWindowIndexer`
    class to perform a 3-business day rolling calculation.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过额外的努力使用变宽窗口：我们可以创建`BaseIndexer`的子类，并在`get_window_bounds()`方法中提供确定窗口边界的逻辑（更多信息请参考[https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#custom-window-rolling](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#custom-window-rolling)），或者我们可以使用`pandas.api.indexers`模块中的预定义类。我们当前使用的笔记本中包含了使用`VariableOffsetWindowIndexer`类执行3个工作日滚动计算的示例。
- en: With rolling calculations, we have a sliding window over which we calculate
    our functions; however, in some cases, we are more interested in the output of
    a function on all the data up to that point, in which case we use an expanding
    window.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 使用滚动计算时，我们有一个滑动窗口，在这个窗口上我们计算我们的函数；然而，在某些情况下，我们更关心函数在所有数据点上的输出，这时我们使用扩展窗口。
- en: Expanding windows
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展窗口
- en: 'Expanding calculations will give us the cumulative value of our aggregation
    function. We use the `expanding()` method to perform a calculation with an expanding
    window; methods such as `cumsum()` and `cummax()` use expanding windows for their
    calculations. The advantage of using `expanding()` directly is additional flexibility:
    we aren''t limited to predefined aggregations, and we can specify the minimum
    number of periods before the calculation starts with the `min_periods` parameter
    (defaults to 1). With the Central Park weather data, let''s use the `expanding()`
    method to calculate the month-to-date average precipitation:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展计算将为我们提供聚合函数的累积值。我们使用`expanding()`方法进行扩展窗口计算；像`cumsum()`和`cummax()`这样的函数会使用扩展窗口进行计算。直接使用`expanding()`的优点是额外的灵活性：我们不局限于预定义的聚合方法，并且可以通过`min_periods`参数指定计算开始前的最小周期数（默认为1）。使用中央公园的天气数据，让我们使用`expanding()`方法来计算当月至今的平均降水量：
- en: '[PRE43]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Note that while there is no method for the cumulative mean, we are able to
    use the `expanding()` method to calculate it. The values in the `AVG_PRCP` column
    are the values in the `TOTAL_PRCP` column divided by the number of days processed:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然没有计算累积平均值的方法，但我们可以使用`expanding()`方法来计算它。`AVG_PRCP`列中的值是`TOTAL_PRCP`列中的值除以处理的天数：
- en: '![Figure 4.27 – Calculating the month-to-date average precipitation'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.27 – 计算当月至今的平均降水量'
- en: '](img/Figure_4.27_B16834.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.27_B16834.jpg)'
- en: Figure 4.27 – Calculating the month-to-date average precipitation
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.27 – 计算当月至今的平均降水量
- en: 'As we did with `rolling()`, we can provide column-specific aggregations with
    the `agg()` method. Let''s find the expanding maximum temperature, minimum temperature,
    average wind speed, and total precipitation. Note that we can also pass in NumPy
    functions to `agg()`:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在使用`rolling()`时做的那样，我们可以通过`agg()`方法提供列特定的聚合操作。让我们来找出扩展后的最高气温、最低气温、平均风速和总降水量。注意，我们还可以将NumPy函数传递给`agg()`：
- en: '[PRE44]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Once again, we joined the window calculations with the original data for comparison:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次将窗口计算与原始数据结合进行比较：
- en: '![Figure 4.28 – Performing different expanding window calculations per column'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.28 – 对每列执行不同的扩展窗口计算'
- en: '](img/Figure_4.28_B16834.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.28_B16834.jpg)'
- en: Figure 4.28 – Performing different expanding window calculations per column
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.28 – 对每列执行不同的扩展窗口计算
- en: Both rolling and expanding windows equally weight all the observations in the
    window when performing calculations, but sometimes, we want to place more emphasis
    on more recent values. One option is to exponentially weight the observations.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动窗口和扩展窗口在执行计算时，都会平等地权重窗口中的所有观测值，但有时我们希望对较新的值赋予更多的重视。一种选择是对观测值进行指数加权。
- en: Exponentially weighted moving windows
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指数加权移动窗口
- en: 'Pandas also provides the `ewm()` method for exponentially weighted moving calculations.
    As discussed in [*Chapter 1*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015),
    *Introduction to Data Analysis*, we can use the `span` argument to specify the
    number of periods to use for the EWMA calculation:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas还提供了`ewm()`方法，用于进行指数加权移动计算。正如在[*第1章*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015)《数据分析导论》中讨论的那样，我们可以使用`span`参数指定用于EWMA计算的周期数：
- en: '[PRE45]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Unlike the rolling average, the EWMA places higher importance on more recent
    observations, so the jump in temperature on October 7th has a larger effect on
    the EWMA than the rolling average:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 与滚动平均不同，EWMA对最近的观测值赋予更高的权重，因此10月7日的温度突变对EWMA的影响大于对滚动平均的影响：
- en: '![Figure 4.29 – Smoothing the data with moving averages'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.29 – 使用移动平均平滑数据'
- en: '](img/Figure_4.29_B16834.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.29_B16834.jpg)'
- en: Figure 4.29 – Smoothing the data with moving averages
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.29 – 使用移动平均平滑数据
- en: Tip
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Check out the `understanding_window_calculations.ipynb` notebook, which contains
    some interactive visualizations for understanding window functions. This may require
    some additional setup, but the instructions are in the notebook.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`understanding_window_calculations.ipynb`笔记本，其中包含了一些用于理解窗口函数的交互式可视化。这可能需要一些额外的设置，但相关说明已包含在笔记本中。
- en: Pipes
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道
- en: 'Pipes facilitate chaining together operations that expect `pandas` data structures
    as their first argument. By using pipes, we can build up complex workflows without
    needing to write highly nested and hard-to-read code. In general, pipes let us
    turn something like `f(g(h(data), 20), x=True)` into the following, making it
    much easier to read:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 管道使得将多个操作链接在一起变得更加简便，这些操作期望`pandas`数据结构作为它们的第一个参数。通过使用管道，我们可以构建复杂的工作流，而不需要编写高度嵌套且难以阅读的代码。通常，管道让我们能够将像`f(g(h(data),
    20), x=True)`这样的表达式转变为以下形式，使其更易读：
- en: '[PRE46]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Say we wanted to print the dimensions of a subset of the Facebook dataframe
    with some formatting by calling this function:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望通过调用此函数，打印Facebook数据框某个子集的维度，并进行一些格式化：
- en: '[PRE47]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Before we call the function, however, we want to calculate the Z-scores for
    all the columns. One approach is the following:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在调用函数之前，我们需要计算所有列的Z得分。一种方法如下：
- en: '[PRE48]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Alternatively, we could pipe the dataframe after calculating the Z-scores to
    this function:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以在计算完Z得分后，将数据框传递给这个函数：
- en: '[PRE49]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Pipes can also make it easier to write reusable code. In several of the code
    snippets in this book, we have seen the idea of passing a function into another
    function, such as when we pass a NumPy function to `apply()` and it gets executed
    on each column. We can use pipes to extend that functionality to methods of `pandas`
    data structures:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 管道还可以使编写可重用代码变得更容易。在本书中的多个代码片段中，我们看到了将一个函数传递给另一个函数的概念，例如我们将NumPy函数传递给`apply()`，并在每一列上执行它。我们可以使用管道将该功能扩展到`pandas`数据结构的方法：
- en: '[PRE50]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'To illustrate how this can benefit us, let''s look at a function that will
    give us the result of a window calculation of our choice. The function is in the
    `window_calc.py` file. We will import the function and use `??` from IPython to
    view the function definition:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这如何为我们带来好处，让我们看一个函数，它将给出我们选择的窗口计算结果。该函数位于`window_calc.py`文件中。我们将导入该函数，并使用`??`从IPython查看函数定义：
- en: '[PRE51]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Our `window_calc()` function takes the dataframe, the function to execute (as
    long as it takes a dataframe as its first argument), and information on how to
    aggregate the result, along with any optional parameters, and gives us back a
    new dataframe with the results of the window calculations. Let''s use this function
    to find the expanding median of the Facebook stock data:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`window_calc()`函数接受数据框架、需要执行的函数（只要它的第一个参数是数据框架），以及如何聚合结果的信息，还可以传递任何可选的参数，然后返回一个包含窗口计算结果的新数据框架。让我们使用这个函数来计算Facebook股票数据的扩展中位数：
- en: '[PRE52]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Note that the `expanding()` method doesn''t require us to specify any parameters,
    so all we had to do was pass in `pd.DataFrame.expanding` (no parentheses), along
    with the aggregation to perform as the window calculation on the dataframe:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`expanding()`方法不需要我们指定任何参数，因此我们只需要传入`pd.DataFrame.expanding`（不带括号），并附带需要进行的聚合操作，作为数据框架上的窗口计算：
- en: '![Figure 4.30 – Using pipes to perform expanding window calculations'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.30 – 使用管道进行扩展窗口计算'
- en: '](img/Figure_4.30_B16834.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.30_B16834.jpg)'
- en: Figure 4.30 – Using pipes to perform expanding window calculations
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.30 – 使用管道进行扩展窗口计算
- en: 'The `window_calc()` function also takes `*args` and `**kwargs`; these are optional
    parameters that, if supplied, will be collected by Python into `kwargs` when they
    are passed by name (such as `span=20`) and into `args` if not (passed by position).
    These can then be `*` for `args` and `**` for `kwargs`. We need this behavior
    in order to use the `ewm()` method for the EWMA of the closing price of Facebook
    stock:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '`window_calc()`函数还接受`*args`和`**kwargs`；这些是可选参数，如果提供，它们会在按名称传递时被 Python 收集到`kwargs`中（例如`span=20`），如果未提供（按位置传递），则会收集到`args`中。然后可以使用`*`表示`args`，`**`表示`kwargs`。我们需要这种行为，以便使用`ewm()`方法计算Facebook股票收盘价的指数加权移动平均（EWMA）：'
- en: '[PRE53]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In the previous example, we had to use `**kwargs` because the `span` argument
    is not the first argument that `ewm()` receives, and we didn''t want to pass the
    ones before it:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们不得不使用`**kwargs`，因为`span`参数并不是`ewm()`接收的第一个参数，我们不想传递它前面的那些参数：
- en: '![Figure 4.31 – Using pipes to perform exponentially weighted window calculations'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.31 – 使用管道进行指数加权窗口计算'
- en: '](img/Figure_4.31_B16834.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.31_B16834.jpg)'
- en: Figure 4.31 – Using pipes to perform exponentially weighted window calculations
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.31 – 使用管道进行指数加权窗口计算
- en: 'To calculate the rolling 3-day weather aggregations for Central Park, we take
    advantage of `*args` since we know that the window is the first argument to `rolling()`:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算中央公园的3天滚动天气聚合，我们利用了`*args`，因为我们知道窗口是`rolling()`的第一个参数：
- en: '[PRE54]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We were able to aggregate each of the columns differently since we passed in
    a dictionary instead of a single value:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够对每一列进行不同的聚合，因为我们传入了一个字典，而不是一个单一的值：
- en: '![Figure 4.32 – Using pipes to perform rolling window calculations'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.32 – 使用管道进行滚动窗口计算'
- en: '](img/Figure_4.32_B16834.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.32_B16834.jpg)'
- en: Figure 4.32 – Using pipes to perform rolling window calculations
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.32 – 使用管道进行滚动窗口计算
- en: Notice how we were able to create a consistent API for the window calculations,
    without the caller needing to figure out which aggregation method to call after
    the window function. This hides some of the implementation details, while making
    it easier to use. We will be using this function as the base for some of the functionality
    in the `StockVisualizer` class we will build in [*Chapter 7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146),
    *Financial Analysis – Bitcoin and the Stock Market*.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们是如何能够为窗口计算创建一致的 API，而调用者无需弄清楚在窗口函数之后需要调用哪个聚合方法。这隐藏了一些实现细节，同时使得使用更加方便。我们将在[*第7章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146)中构建的`StockVisualizer`类的某些功能中使用这个函数，*金融分析
    - 比特币与股市*。
- en: Aggregating data
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合数据
- en: 'We already got a sneak peek at aggregation when we discussed window calculations
    and pipes in the previous section. Here, we will focus on summarizing the dataframe
    through aggregation, which will change the shape of our dataframe (often through
    row reduction). We also saw how easy it is to take advantage of vectorized NumPy
    functions on `pandas` data structures, especially to perform aggregations. This
    is what NumPy does best: it performs computationally efficient mathematical operations
    on numeric arrays.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论窗口计算和管道操作的上一节中，已经对聚合有了初步了解。在这里，我们将专注于通过聚合来汇总数据帧，这将改变我们数据帧的形状（通常是通过减少行数）。我们还看到，利用NumPy的矢量化函数在`pandas`数据结构上进行聚合是多么容易。这正是NumPy最擅长的：在数值数组上执行高效的数学计算。
- en: NumPy pairs well with aggregating dataframes since it gives us an easy way to
    summarize data with different pre-written functions; often, when aggregating,
    we just need the NumPy function, since most of what we would want to write ourselves
    has previously been built. We have already seen some NumPy functions commonly
    used for aggregations, such as `np.sum()`, `np.mean()`, `np.min()`, and `np.max()`;
    however, we aren't limited to numeric operations—we can use things such as `np.unique()`
    on strings. Always check whether NumPy already has a function before implementing
    one yourself.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy与聚合数据框非常搭配，因为它为我们提供了一种通过不同的预写函数汇总数据的简便方法；通常，在进行聚合时，我们只需要使用NumPy函数，因为大多数我们自己编写的函数已经由NumPy实现。我们已经看到了一些常用的NumPy聚合函数，比如`np.sum()`、`np.mean()`、`np.min()`和`np.max()`；然而，我们不仅限于数值操作——我们还可以在字符串上使用诸如`np.unique()`之类的函数。总是检查NumPy是否已经提供了所需的函数，再决定是否自己实现。
- en: 'For this section, we will be working in the `3-aggregations.ipynb` notebook.
    Let''s import `pandas` and `numpy` and read in the data we will be working with:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将在`3-aggregations.ipynb`笔记本中进行操作。让我们导入`pandas`和`numpy`，并读取我们将要处理的数据：
- en: '[PRE55]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Note that the weather data for this section has been merged with some of the
    station data:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本节的天气数据已经与部分站点数据合并：
- en: '![Figure 4.33 – Merged weather and station data for this section'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.33 – 本节合并的天气与站点数据'
- en: '](img/Figure_4.33_B16834.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.33_B16834.jpg)'
- en: Figure 4.33 – Merged weather and station data for this section
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.33 – 本节合并的天气与站点数据
- en: 'Before we dive into any calculations, let''s make sure that our data won''t
    be displayed in scientific notation. We will modify how floats are formatted for
    displaying. The format we will apply is `.2f`, which will provide the float with
    two digits after the decimal point:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行任何计算之前，我们先确保数据不会以科学计数法显示。我们将修改浮动数值的显示格式。我们将应用的格式是`.2f`，该格式会提供一个小数点后两位的浮动数值：
- en: '[PRE56]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: First, we will take a look at summarizing the full dataset before moving on
    to summarizing by groups and building pivot tables and crosstabs.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看如何汇总完整数据集，然后再进行按组汇总，并构建数据透视表和交叉表。
- en: Summarizing DataFrames
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 汇总数据帧
- en: 'When we discussed window calculations, we saw that we could run the `agg()`
    method on the result of `rolling()`, `expanding()`, or `ewm()`; however, we can
    also call it directly on the dataframe in the same fashion. The only difference
    is that the aggregations done this way will be performed on all the data, meaning
    that we will only get a series back that contains the overall result. Let''s aggregate
    the Facebook stock data the same way we did with the window calculations. Note
    that we won''t get anything back for the `trading_volume` column, which contains
    the volume traded bins from `pd.cut()`; this is because we aren''t specifying
    an aggregation to run on that column:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论窗口计算时，我们看到可以在`rolling()`、`expanding()`或`ewm()`的结果上运行`agg()`方法；然而，我们也可以像对待数据框一样直接调用它。唯一的区别是，通过这种方式执行的聚合将在所有数据上进行，这意味着我们将只得到一个包含整体结果的系列。让我们像在窗口计算中一样对Facebook的股票数据进行聚合。请注意，对于`trading_volume`列（它包含`pd.cut()`生成的交易量区间），我们不会得到任何结果；这是因为我们没有为该列指定聚合操作：
- en: '[PRE57]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We can use aggregations to easily find the total snowfall and precipitation
    for 2018 in Central Park. In this case, since we will be performing the sum on
    both, we can either use `agg(''sum'')` or call `sum()` directly:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用聚合函数轻松找到2018年中央公园的总降雪量和降水量。在这种情况下，由于我们将在两者上执行求和操作，我们可以使用`agg('sum')`或直接调用`sum()`：
- en: '[PRE58]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Additionally, we can provide multiple functions to run on each of the columns
    we want to aggregate. As we have already seen, we get a `Series` object when each
    column has a single aggregation. To distinguish between the aggregations in the
    case of multiple ones per column, `pandas` will return a `DataFrame` object instead.
    The index of this dataframe will tell us which metric is being calculated for
    which column:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以为每个要聚合的列提供多个函数。正如我们之前所见，当每列只有一个聚合函数时，我们会得到一个`Series`对象。若每列有多个聚合函数，`pandas`将返回一个`DataFrame`对象。这个数据框的索引会告诉我们正在为哪个列计算哪种指标：
- en: '[PRE59]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This results in a dataframe where the rows indicate the aggregation function
    being applied to the data columns. Note that we get nulls for any combination
    of aggregation and column that we didn''t explicitly ask for:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生一个数据框，其中行表示应用于数据列的聚合函数。请注意，对于我们没有明确要求的任何聚合与列的组合，结果会是空值：
- en: '![Figure 4.34 – Performing multiple aggregations per column'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.34 – 每列执行多个聚合'
- en: '](img/Figure_4.34_B16834.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.34_B16834.jpg)'
- en: Figure 4.34 – Performing multiple aggregations per column
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.34 – 每列执行多个聚合
- en: So far, we have learned how to aggregate over specific windows and over the
    entire dataframe; however, the real power comes with the ability to aggregate
    by group membership. This lets us calculate things such as the total precipitation
    per month, per station and average OHLC stock prices for each volume traded bin
    we've created.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学会了如何在特定窗口和整个数据框上进行聚合；然而，真正的强大之处在于按组别进行聚合。这使我们能够计算如每月、每个站点的总降水量，以及我们为每个交易量区间创建的股票OHLC平均价格等内容。
- en: Aggregating by group
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 按组聚合
- en: 'To calculate the aggregations per group, we must first call the `groupby()`
    method on the dataframe and provide the column(s) we want to use to determine
    distinct groups. Let''s look at the average of our stock data points for each
    of the volume traded bins we created with `pd.cut()`; remember, these are three
    equal-width bins:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 要按组计算聚合，我们必须首先在数据框上调用`groupby()`方法，并提供我们想用来确定不同组的列。让我们来看一下我们用`pd.cut()`创建的每个交易量区间的股票数据点的平均值；记住，这些是三个等宽区间：
- en: '[PRE60]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The average OHLC prices are smaller for larger trading volumes, which was to
    be expected given that the three dates in the high-volume traded bin were selloffs:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 较大交易量的OHLC平均价格较小，这是预期之中的，因为高交易量区间的三个日期是抛售日：
- en: '![Figure 4.35 – Aggregating by group'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.35 – 按组聚合'
- en: '](img/Figure_4.35_B16834.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.35_B16834.jpg)'
- en: Figure 4.35 – Aggregating by group
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.35 – 按组聚合
- en: 'After running `groupby()`, we can also select specific columns for aggregation:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行`groupby()`后，我们还可以选择特定的列进行聚合：
- en: '[PRE61]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This gives us the aggregations for the closing price in each volume traded
    bucket:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了每个交易量区间的收盘价聚合：
- en: '![Figure 4.36 – Aggregating a specific column per group'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.36 – 按组聚合特定列'
- en: '](img/Figure_4.36_B16834.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.36_B16834.jpg)'
- en: Figure 4.36 – Aggregating a specific column per group
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.36 – 按组聚合特定列
- en: 'If we need more fine-tuned control over how each column gets aggregated, we
    use the `agg()` method again with a dictionary that maps the columns to their
    aggregation function. As we did previously, we can provide lists of functions
    per column; the result, however, will look a little different:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要更精细地控制每个列的聚合方式，我们可以再次使用`agg()`方法，并提供一个将列映射到聚合函数的字典。如同之前一样，我们可以为每一列提供多个函数；不过结果会看起来有些不同：
- en: '[PRE62]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We now have a hierarchical index in the columns. Remember, this means that
    if we want to select the minimum low price for the medium volume traded bucket,
    we need to use `fb_agg.loc[''med'', ''low''][''min'']`:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的列有了层级索引。记住，这意味着，如果我们想选择中等交易量区间的最低低价，我们需要使用`fb_agg.loc['med', 'low']['min']`：
- en: '![Figure 4.37 – Performing multiple aggregations per column with groups'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.37 – 按组进行每列的多个聚合'
- en: '](img/Figure_4.37_B16834.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.37_B16834.jpg)'
- en: Figure 4.37 – Performing multiple aggregations per column with groups
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.37 – 按组进行每列的多个聚合
- en: 'The columns are stored in a `MultiIndex` object:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 列存储在一个`MultiIndex`对象中：
- en: '[PRE63]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We can use a list comprehension to remove this hierarchy and instead have our
    column names in the form of `<column>_<agg>`. At each iteration, we will get a
    tuple of the levels from the `MultiIndex` object, which we can combine into a
    single string to remove the hierarchy:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用列表推导来移除这个层级，而是将列名转换为`<column>_<agg>`的形式。在每次迭代中，我们将从`MultiIndex`对象中获取一个元组的层级，这些层级可以组合成一个单一的字符串来去除层级：
- en: '[PRE64]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'This replaces the hierarchy in the columns with a single level:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把列中的层级替换为单一层级：
- en: '![Figure 4.38 – Flattening out the hierarchical index'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.38 – 扁平化层级索引'
- en: '](img/Figure_4.38_B16834.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.38_B16834.jpg)'
- en: Figure 4.38 – Flattening out the hierarchical index
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.38 – 扁平化层级索引
- en: 'Say we want to see the average observed precipitation across all the stations
    per day. We would need to group by the date, but it is in the index. In this case,
    we have a few options:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想查看所有站点每天的平均降水量。我们需要按日期进行分组，但日期在索引中。在这种情况下，我们有几种选择：
- en: Resampling, which we will cover in the *Working with time series data* section,
    later in this chapter.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重采样，我们将在本章后面的*处理时间序列数据*部分讲解。
- en: Resetting the index and using the date column that gets created from the index.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重置索引，并使用从索引中创建的日期列。
- en: Passing `level=0` to `groupby()` to indicate that the grouping should be performed
    on the outermost level of the index.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`level=0`传递给`groupby()`，表示分组应在索引的最外层进行。
- en: Using a `Grouper` object.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`Grouper`对象。
- en: 'Here, we will pass `level=0` to `groupby()`, but note that we can also pass
    in `level=''date''` because our index is named. This gives us the average precipitation
    observations across the stations, which may give us a better idea of the weather
    than simply picking a station to look at. Since the result is a single-column
    `DataFrame` object, we call `squeeze()` to turn it into a `Series` object:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将`level=0`传递给`groupby()`，但请注意，我们也可以传入`level='date'`，因为我们的索引已经命名。这将给我们一个跨所有站点的平均降水量观察结果，这可能比仅查看某个站点的数据更能帮助我们了解天气。由于结果是一个单列的`DataFrame`对象，我们调用`squeeze()`将其转换为`Series`对象：
- en: '[PRE65]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We can also group by many categories at once. Let''s find the quarterly total
    recorded precipitation per station. Here, rather than pass in `level=0` to `groupby()`,
    we need to use a `Grouper` object to aggregate from daily to quarterly frequency.
    Since this will create a multi-level index, we will also use `unstack()` to put
    the inner level (the quarter) along the columns after the aggregation is performed:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以一次性按多个类别进行分组。让我们找出每个站点每季度的降水总量。在这里，我们需要使用`Grouper`对象将频率从日度聚合到季度，而不是将`level=0`传递给`groupby()`。由于这将创建多层级索引，我们还将使用`unstack()`在聚合完成后将内层级（季度）放置在列中：
- en: '[PRE66]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'There are many possible follow-ups for this result. We could look at which
    stations receive the most/least precipitation. We could go back to the location
    and elevation information we had for each station to see if that affects precipitation.
    We could also see which quarter has the most/least precipitation across the stations:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个结果，有许多可能的后续分析。我们可以查看哪些站点接收到的降水最多/最少。我们还可以回到每个站点的位置信息和海拔数据，看看这些是否影响降水。我们还可以查看哪个季度在所有站点中降水最多/最少：
- en: '![Figure 4.39 – Aggregating by a column with dates in the index'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.39 – 按包含日期的列进行聚合'
- en: '](img/Figure_4.39_B16834.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.39_B16834.jpg)'
- en: Figure 4.39 – Aggregating by a column with dates in the index
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.39 – 按包含日期的列进行聚合
- en: Tip
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The `DataFrameGroupBy` objects returned by the `groupby()` method have a `filter()`
    method, which allows us to filter groups. We can use this to exclude certain groups
    from the aggregation. Simply pass a function that returns a Boolean for each group's
    subset of the dataframe (`True` to include the group and `False` to exclude it).
    An example is in the notebook.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupby()`方法返回的`DataFrameGroupBy`对象具有一个`filter()`方法，允许我们过滤组。我们可以使用它来从聚合中排除某些组。只需传入一个返回布尔值的函数，针对每个组的子集（`True`表示包含该组，`False`表示排除该组）。示例可以在笔记本中查看。'
- en: 'Let''s see which months have the most precipitation. First, we need to group
    by day and average the precipitation across the stations. Then, we can group by
    month and sum the resulting precipitation. Finally, we will use `nlargest()` to
    get the five months with the most precipitation:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看哪个月份降水最多。首先，我们需要按天进行分组，并对各个站点的降水量求平均。然后，我们可以按月分组并求和。最后，我们将使用`nlargest()`来获取降水量最多的前五个月：
- en: '[PRE67]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Perhaps the previous result was surprising. The saying goes *April showers bring
    May flowers*; however, April wasn't in the top five (neither was May, for that
    matter). Snow will count toward precipitation, but that doesn't explain why summer
    months are higher than April. Let's look for days that accounted for a large percentage
    of the precipitation in a given month to see if April shows up there.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 也许前面的结果让人感到惊讶。俗话说，*四月的阵雨带来五月的花*；然而，四月并未出现在前五名中（五月也没有）。雪也会计入降水量，但这并不能解释为何夏季的降水量会高于四月。让我们查找在某月降水量占比较大的天数，看看四月是否会出现在那里。
- en: 'To do so, we need to calculate the average daily precipitation across stations
    and then find the total per month; this will be the denominator. However, in order
    to divide the daily values by the total for their month, we will need a `Series`
    object of equal dimensions. This means that we will need to use the `transform()`
    method, which will perform the specified calculation on the data while always
    returning an object of equal dimensions to what we started with. Therefore, we
    can call it on a `Series` object and always get a `Series` object back, regardless
    of what the aggregation function itself would return:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要计算每个气象站的日均降水量，并计算每月的总降水量；这将作为分母。然而，为了将每日的值除以其所在月份的总值，我们需要一个维度相等的`Series`对象。这意味着我们需要使用`transform()`方法，它会对数据执行指定的计算，并始终返回一个与起始对象维度相同的对象。因此，我们可以在`Series`对象上调用它，并始终返回一个`Series`对象，无论聚合函数本身返回的是什么：
- en: '[PRE68]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Rather than getting a single sum for January and another for February, notice
    that we have the same value being repeated for the January entries and a different
    one for the February ones. Note that the value for February is the value we found
    in the previous result:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 与其对一月和二月分别得到一个总和，不如注意到我们在一月的条目中得到了相同的值，而在二月的条目中得到了不同的值。注意，二月的值是我们在前面结果中找到的那个值：
- en: '![Figure 4.40 – The denominator for calculating the percentage of monthly precipitation'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.40 – 用于计算月度降水百分比的分母](img/Figure_4.40_B16834.jpg)'
- en: '](img/Figure_4.40_B16834.jpg)'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.40_B16834.jpg)'
- en: Figure 4.40 – The denominator for calculating the percentage of monthly precipitation
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.40 – 用于计算月度降水百分比的分母
- en: 'We can make this a column in our dataframe to easily calculate the percentage
    of the monthly precipitation that occurred each day. Then, we can use `nlargest()`
    to pull out the largest values:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在数据框中创建这一列，以便轻松计算每天发生的月度降水百分比。然后，我们可以使用`nlargest()`方法提取最大的值：
- en: '[PRE69]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Together, the 4th- and 5th-place days in terms of the amount of monthly precipitation
    they accounted for make up more than 50% of the rain in April. They were also
    consecutive days:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在按月降水量排序的前四和第五天中，它们加起来占了四月降水量的50%以上。这两天也是连续的：
- en: '![Figure 4.41 – Calculating the percentage of monthly precipitation that occurred
    each day'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.41 – 计算每天发生的月度降水百分比](img/Figure_4.41_B16834.jpg)'
- en: '](img/Figure_4.41_B16834.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.41_B16834.jpg)'
- en: Figure 4.41 – Calculating the percentage of monthly precipitation that occurred
    each day
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.41 – 计算每天发生的月度降水百分比
- en: Important note
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The `transform()` method also works on `DataFrame` objects, in which case it
    will return a `DataFrame` object. We can use it to easily standardize all the
    columns at once. An example is in the notebook.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '`transform()`方法同样适用于`DataFrame`对象，在这种情况下它将返回一个`DataFrame`对象。我们可以使用它一次性轻松地标准化所有列。示例可以在笔记本中找到。'
- en: Pivot tables and crosstabs
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据透视表和交叉表
- en: To wrap up this section, we will discuss some `pandas` functions that will aggregate
    our data into some common formats. The aggregation methods we discussed previously
    will give us the highest level of customization; however, `pandas` provides some
    functions to quickly generate a pivot table and a crosstab in a common format.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的最后，我们将讨论一些`pandas`函数，这些函数能够将我们的数据聚合为一些常见格式。我们之前讨论的聚合方法会给我们最大的自定义空间；然而，`pandas`还提供了一些函数，可以快速生成数据透视表和交叉表，格式上符合常见标准。
- en: 'In order to generate a pivot table, we must specify what to group on and, optionally,
    which subset of columns we want to aggregate and/or how to aggregate (average,
    by default). Let''s create a pivot table of averaged OHLC data for Facebook per
    volume traded bin:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成数据透视表，我们必须指定按什么进行分组，并可选择指定要聚合的列子集和/或聚合方式（默认情况下为平均）。让我们创建一个Facebook按交易量分组的平均OHLC数据的透视表：
- en: '[PRE70]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Since we passed in `columns=''trading_volume''`, the distinct values in the
    `trading_volume` column were placed along the columns. The columns from the original
    dataframe then went to the index. Notice that the index for the columns has a
    name (**trading_volume**):'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们传入了`columns='trading_volume'`，`trading_volume`列中的不同值被放置在了列中。原始数据框中的列则转到了索引。请注意，列的索引有一个名称（**trading_volume**）：
- en: '![Figure 4.42 – Pivot table of column averages per volume traded bin'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.42 – 每个交易量区间的列平均值透视表'
- en: '](img/Figure_4.42_B16834.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.42_B16834.jpg)'
- en: Figure 4.42 – Pivot table of column averages per volume traded bin
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.42 – 每个交易量区间的列平均值透视表
- en: Tip
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: If we pass `trading_volume` as the `index` argument instead, we get the transpose
    of *Figure 4.42*, which is also the exact same output as *Figure 4.35* when we
    used `groupby()`.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将`trading_volume`作为`index`参数传入，得到的将是*图 4.42*的转置，这也是使用`groupby()`时得到的与*图
    4.35*完全相同的输出。
- en: 'With the `pivot()` method, we weren''t able to handle multi-level indices or
    indices with repeated values. For this reason, we haven''t been able to put the
    weather data in wide format. The `pivot_table()` method solves this issue. To
    do so, we need to put the `date` and `station` information in the index and the
    distinct values of the `datatype` column along the columns. The values will come
    from the `value` column. We will use the median to aggregate any overlapping combinations
    (if any):'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pivot()`方法时，我们无法处理多级索引或具有重复值的索引。正因为如此，我们无法将天气数据放入宽格式。`pivot_table()`方法解决了这个问题。为此，我们需要将`date`和`station`信息放入索引中，将`datatype`列的不同值放入列中。值将来自`value`列。对于任何重叠的组合（如果有），我们将使用中位数进行聚合：
- en: '[PRE71]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'After resetting the index, we have our data in wide format. One final step
    would be to rename the index:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在重置索引后，我们得到了宽格式的数据。最后一步是重命名索引：
- en: '![Figure 4.43 – Pivot table with median values per datatype, station, and date'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.43 – 每种数据类型、站点和日期的中位数值透视表'
- en: '](img/Figure_4.43_B16834.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.43_B16834.jpg)'
- en: Figure 4.43 – Pivot table with median values per datatype, station, and date
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.43 – 每种数据类型、站点和日期的中位数值透视表
- en: 'We can use the `pd.crosstab()` function to create a frequency table. For example,
    if we want to see how many low-, medium-, and high-volume trading days Facebook
    stock had each month, we can use a crosstab. The syntax is pretty straightforward;
    we pass the row and column labels to the `index` and `columns` parameters, respectively.
    By default, the values in the cells will be the count:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`pd.crosstab()`函数来创建一个频率表。例如，如果我们想查看每个月Facebook股票的低、中、高交易量交易天数，可以使用交叉表。语法非常简单；我们将行和列标签分别传递给`index`和`columns`参数。默认情况下，单元格中的值将是计数：
- en: '[PRE72]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This makes it easy to see the months when high volumes of Facebook stock were
    traded:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以方便地查看Facebook股票在不同月份的高交易量：
- en: '![Figure 4.44 – Crosstab showing the number of trading days per month, per
    volume traded bin'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.44 – 显示每个月、每个交易量区间的交易天数的交叉表'
- en: '](img/Figure_4.44_B16834.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.44_B16834.jpg)'
- en: Figure 4.44 – Crosstab showing the number of trading days per month, per volume
    traded bin
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.44 – 显示每个月、每个交易量区间的交易天数的交叉表
- en: Tip
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: We can normalize the output to percentages of the row/column totals by passing
    in `normalize='rows'`/`normalize='columns'`. An example is in the notebook.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过传入`normalize='rows'`/`normalize='columns'`来将输出标准化为行/列总计的百分比。示例见笔记本。
- en: 'To change the aggregation function, we can provide an argument to `values`
    and then specify `aggfunc`. To illustrate this, let''s find the average closing
    price of each trading volume bucket per month instead of the count in the previous
    example:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改聚合函数，我们可以为`values`提供一个参数，然后指定`aggfunc`。为了说明这一点，让我们计算每个月每个交易量区间的平均收盘价，而不是上一个示例中的计数：
- en: '[PRE73]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We now get the average closing price per month, per volume traded bin, with
    null values when that combination wasn''t present in the data:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了每个月、每个交易量区间的平均收盘价，数据中没有该组合时则显示为null值：
- en: '![Figure 4.45 – Crosstab using averages instead of counts'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.45 – 使用平均值而非计数的交叉表'
- en: '](img/Figure_4.45_B16834.jpg)'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.45_B16834.jpg)'
- en: Figure 4.45 – Crosstab using averages instead of counts
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.45 – 使用平均值而非计数的交叉表
- en: 'We can also get row and column subtotals with the `margins` parameter. Let''s
    count the number of times each station recorded snow per month and include the
    subtotals:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`margins`参数来获取行和列的小计。让我们统计每个月每个站点记录到雪的次数，并包括小计：
- en: '[PRE74]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Along the bottom row, we have the total snow observations per month, while
    down the rightmost column, we have the total snow observations in 2018 per station:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在底部的行中，我们展示了每月的总降雪观测数据，而在最右边的列中，我们列出了 2018 年每个站点的总降雪观测数据：
- en: '![Figure 4.46 – Crosstab counting the number of days with snow per month, per
    station'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.46 – 统计每个月每个站点降雪天数的交叉表](img/Figure_4.46_B16834.jpg)'
- en: '](img/Figure_4.46_B16834.jpg)'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.46_B16834.jpg)'
- en: Figure 4.46 – Crosstab counting the number of days with snow per month, per
    station
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.46 – 统计每个月每个站点降雪天数的交叉表
- en: Just by looking at a few stations, we can see that, despite all of them supplying
    weather information for NYC, they don't share every facet of the weather. Depending
    on which stations we choose to look at, we could be adding/subtracting snow from
    what really happened in NYC.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看少数几个站点，我们可以发现，尽管它们都提供了纽约市的天气信息，但它们并不共享天气的每个方面。根据我们选择查看的站点，我们可能会添加或减少与纽约市实际发生的雪量。
- en: Working with time series data
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用时间序列数据
- en: 'With time series data, we have some additional operations we can use, for anything
    from selection and filtering to aggregation. We will be exploring some of this
    functionality in the `4-time_series.ipynb` notebook. Let''s start off by reading
    in the Facebook data from the previous sections:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时间序列数据时，我们可以进行一些额外的操作，涵盖从选择和筛选到聚合的各个方面。我们将在 `4-time_series.ipynb` 笔记本中探索一些这种功能。我们先从读取前面章节中的
    Facebook 数据开始：
- en: '[PRE75]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: We will begin this section by discussing the selection and filtering of time
    series data before moving on to shifting, differencing, resampling, and finally
    merging based on time. Note that it's important to set the index to our date (or
    datetime) column, which will allow us to take advantage of the additional functionality
    we will be discussing. Some operations may work without doing this, but for a
    smooth process throughout our analysis, using an index of type `DatetimeIndex`
    is recommended.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将从讨论时间序列数据的选择和筛选开始，接着讲解数据的平移、差分、重采样，最后讲解基于时间的数据合并。请注意，设置日期（或日期时间）列作为索引非常重要，这将使我们能够利用接下来要讲解的额外功能。某些操作无需设置索引也能工作，但为了确保分析过程的顺利进行，建议使用
    `DatetimeIndex` 类型的索引。
- en: Time-based selection and filtering
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于时间的选择和筛选
- en: 'Let''s start with a quick recap of datetime slicing and indexing. We can easily
    isolate data for the year by indexing on it: `fb.loc[''2018'']`. In the case of
    our stock data, the full dataframe would be returned because we only have 2018
    data; however, we can filter to a month (`fb.loc[''2018-10'']`) or to a range
    of dates. Note that using `loc[]` is optional with ranges:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先快速回顾一下日期时间切片和索引的内容。我们可以通过索引轻松提取特定年份的数据：`fb.loc['2018']`。对于我们的股票数据，由于只有 2018
    年的数据，因此会返回整个数据框；不过，我们也可以过滤出某个月的数据（例如 `fb.loc['2018-10']）` 或者某个日期范围的数据。请注意，对于范围的选择，使用
    `loc[]` 是可选的：
- en: '[PRE76]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We only get three days back because the stock market is closed on the weekends:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅能获得三天的数据，因为股市在周末休市：
- en: '![Figure 4.47 – Selecting data based on a date range'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.47 – 基于日期范围选择数据](img/Figure_4.46_B16834.jpg)'
- en: '](img/Figure_4.47_B16834.jpg)'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.47_B16834.jpg)'
- en: Figure 4.47 – Selecting data based on a date range
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.47 – 基于日期范围选择数据
- en: 'Keep in mind that the date range can also be supplied using other frequencies,
    such as month or the quarter of the year:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，日期范围也可以使用其他频率提供，例如按月或按季度：
- en: '[PRE77]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'When targeting the beginning or end of a date range, `pandas` has some additional
    methods for selecting the first or last rows within a specified unit of time.
    We can select the first week of stock prices in 2018 using the `first()` method
    and an offset of `1W`:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标是日期范围的开始或结束时，`pandas` 提供了一些额外的方法来选择指定时间单位内的第一行或最后一行数据。我们可以使用 `first()` 方法和
    `1W` 偏移量来选择 2018 年的第一周股价数据：
- en: '[PRE78]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'January 1, 2018 was a holiday, meaning that the market was closed. It was also
    a Monday, so the week here is only four days long:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 2018 年 1 月 1 日是节假日，股市休市。而且那天是周一，因此这一周只有四天：
- en: '![Figure 4.48 – Facebook stock data during the first week of trading in 2018'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.48 – 2018 年首周 Facebook 股票数据](img/Figure_4.47_B16834.jpg)'
- en: '](img/Figure_4.48_B16834.jpg)'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.48_B16834.jpg)'
- en: Figure 4.48 – Facebook stock data during the first week of trading in 2018
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.48 – 2018 年首周 Facebook 股票数据
- en: 'We can perform a similar operation for the most recent dates as well. Selecting
    the last week in the data is as simple as switching the `first()` method with
    the `last()` method:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以对最近的日期执行类似的操作。选择数据中的最后一周，只需将 `first()` 方法替换为 `last()` 方法即可：
- en: '[PRE79]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Since December 31, 2018 was a Monday, the last week only consists of one day:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 2018 年 12 月 31 日是星期一，最后一周只包含一天：
- en: '![Figure 4.49 – Facebook stock data during the last week of trading in 2018'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.49 – 2018 年最后一周的 Facebook 股票数据'
- en: '](img/Figure_4.49_B16834.jpg)'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.49_B16834.jpg)'
- en: Figure 4.49 – Facebook stock data during the last week of trading in 2018
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.49 – 2018 年最后一周的 Facebook 股票数据
- en: 'When working with daily stock data, we only have data for the dates the stock
    market was open. Suppose that we reindexed the data to include rows for each day
    of the year:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理每日股票数据时，我们只有股市开放日期的数据。假设我们将数据重新索引，以便为每一天添加一行：
- en: '[PRE80]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The reindexed data would have all nulls for January 1st and any other days
    the market was closed. We can combine the `first()`, `isna()`, and `all()` methods
    to confirm this. Here, we will also use the `squeeze()` method to turn the 1-row
    `DataFrame` object resulting from the call to `first(''1D'').isna()` into a `Series`
    object so that calling `all()` yields a single value:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 重新索引的数据将在 1 月 1 日和其他市场关闭的日期上显示所有空值。我们可以结合使用 `first()`、`isna()` 和 `all()` 方法来确认这一点。这里，我们还将使用
    `squeeze()` 方法将通过调用 `first('1D').isna()` 得到的 1 行 `DataFrame` 对象转换为 `Series` 对象，以便调用
    `all()` 时返回单一值：
- en: '[PRE81]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'We can use the `first_valid_index()` method to obtain the index of the first
    non-null entry in our data, which will be the first day of trading in the data.
    To obtain the last day of trading, we can use the `last_valid_index()` method.
    For the first quarter of 2018, the first day of trading was January 2nd and the
    last was March 29th:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `first_valid_index()` 方法获取数据中第一个非空条目的索引，它将是数据中的第一个交易日。要获取最后一个交易日，我们可以使用
    `last_valid_index()` 方法。对于 2018 年第一季度，交易的第一天是 1 月 2 日，最后一天是 3 月 29 日：
- en: '[PRE82]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'If we wanted to know what Facebook''s stock price looked like as of March 31,
    2018, our initial idea may be to use indexing to retrieve it. However, if we try
    to do so with `loc[]` (`fb_reindexed.loc[''2018-03-31'']`), we will get null values
    because the stock market wasn''t open that day. If we use the `asof()` method
    instead, it will give us the closest non-null data that precedes the date we ask
    for, which in this case is March 29th. Therefore, if we wanted to see how Facebook
    performed on the last day in each month, we could use `asof()`, and avoid having
    to first check if the market was open that day:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想知道 2018 年 3 月 31 日 Facebook 的股票价格，最初的想法可能是使用索引来获取它。然而，如果我们尝试通过 `loc[]`
    (`fb_reindexed.loc['2018-03-31']`) 来获取，我们将得到空值，因为那天股市并未开放。如果我们改用 `asof()` 方法，它将返回在我们要求的日期之前的最近非空数据，在本例中是
    3 月 29 日。因此，如果我们想查看 Facebook 在每个月的最后一天的表现，可以使用 `asof()`，而无需先检查当天股市是否开放：
- en: '[PRE83]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'For the next few examples, we will need time information in addition to the
    date. The datasets we have been working with thus far lack a time component, so
    we will switch to the Facebook stock data by the minute from May 20, 2019 through
    May 24, 2019 from Nasdaq.com. In order to properly parse the datetimes, we need
    to pass in a lambda function as the `date_parser` argument since they are not
    in a standard format (for instance, May 20, 2019 at 9:30 AM is represented as
    `2019-05-20 09-30`); the lambda function will specify how to convert the data
    in the `date` field into datetimes:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个例子中，我们除了日期之外还需要时间信息。到目前为止，我们处理的数据集缺少时间组件，因此我们将切换到来自 Nasdaq.com 的 2019
    年 5 月 20 日至 2019 年 5 月 24 日的按分钟划分的 Facebook 股票数据。为了正确解析日期时间，我们需要将一个 lambda 函数作为
    `date_parser` 参数传入，因为这些数据并非标准格式（例如，2019 年 5 月 20 日 9:30 AM 表示为 `2019-05-20 09-30`）；该
    lambda 函数将指定如何将 `date` 字段中的数据转换为日期时间：
- en: '[PRE84]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'We have the OHLC data per minute, along with the volume traded per minute:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有每分钟的 OHLC 数据，以及每分钟的成交量数据：
- en: '![Figure 4.50 – Facebook stock data by the minute'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.50 – Facebook 股票数据按分钟划分'
- en: '](img/Figure_4.50_B16834.jpg)'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.50_B16834.jpg)'
- en: Figure 4.50 – Facebook stock data by the minute
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.50 – Facebook 股票数据按分钟划分
- en: Important note
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: In order to properly parse datetimes in a non-standard format, we need to specify
    the format it is in. For a reference on the available codes, consult the Python
    documentation at [https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确解析非标准格式的日期时间，我们需要指定其格式。有关可用代码的参考，请查阅 Python 文档：[https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior)。
- en: 'We can use `first()` and `last()` with `agg()` to bring this data to a daily
    granularity. To get the true open value, we need to take the first observation
    per day; conversely, for the true closing value, we need to take the last observation
    per day. The high and low will be the maximum and minimum of their respective
    columns per day. Volume traded will be the daily sum:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`first()`和`last()`结合`agg()`将这些数据转换为每日的粒度。为了获取真实的开盘价，我们需要每天获取第一个观察值；相反，对于真实的收盘价，我们需要每天获取最后一个观察值。高点和低点将分别是它们每天各自列的最大和最小值。成交量将是每日的总和：
- en: '[PRE85]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'This rolls the data up to a daily frequency:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 这将数据升级到每日频率：
- en: '![Figure 4.51 – Rolling up the data from the minute level to the daily level'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 4.51 – 将数据从分钟级别升级到日级别'
- en: '](img/Figure_4.51_B16834.jpg)'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.51_B16834.jpg)'
- en: Figure 4.51 – Rolling up the data from the minute level to the daily level
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 4.51 – 将数据从分钟级别升级到日级别
- en: 'The next two methods we will discuss help us select data based on the time
    part of the datetime. The `at_time()` method allows us to isolate rows where the
    time part of the datetime is the time we specify. By running `at_time(''9:30'')`,
    we can grab all the market open prices (the stock market opens at 9:30 AM):'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将讨论的两种方法帮助我们基于日期时间的时间部分选择数据。`at_time()`方法允许我们隔离日期时间的时间部分是我们指定的时间的行。通过运行`at_time('9:30')`，我们可以抓取所有开盘价格（股票市场在上午9:30开盘）：
- en: '[PRE86]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'This tells us what the stock data looked like at the opening bell each day:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们每天开盘铃响时的股票数据是怎样的：
- en: '![Figure 4.52 – Grabbing the stock data at market open each day'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 4.52 – 每天市场开盘时的股票数据'
- en: '](img/Figure_4.52_B16834.jpg)'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.52_B16834.jpg)'
- en: Figure 4.52 – Grabbing the stock data at market open each day
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 4.52 – 每天市场开盘时的股票数据
- en: 'We can use the `between_time()` method to grab all the rows where the time
    portion of the datetime is between two times (inclusive of the endpoints by default).
    This method can be very useful if we want to look at data within a certain time
    range, day over day. Let''s grab all the rows within the last two minutes of trading
    each day (15:59 - 16:00):'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`between_time()`方法来抓取时间部分位于两个时间之间（默认包含端点）的所有行。如果我们想要逐日查看某个时间范围内的数据，这种方法非常有用。让我们抓取每天交易的最后两分钟内的所有行（15:59
    - 16:00）：
- en: '[PRE87]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'It looks like the last minute (16:00) has significantly more volume traded
    each day compared to the previous minute (15:59). Perhaps people rush to make
    trades before close:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来最后一分钟（16:00）每天的交易量显著高于前一分钟（15:59）。也许人们会在闭市前赶紧进行交易：
- en: '![Figure 4.53 – Stock data in the last two minutes of trading per day'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 4.53 – 每天交易最后两分钟的股票数据'
- en: '](img/Figure_4.53_B16834.jpg)'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.53_B16834.jpg)'
- en: Figure 4.53 – Stock data in the last two minutes of trading per day
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 4.53 – 每天交易最后两分钟的股票数据
- en: 'We may wonder if this also happens in the first two minutes. Do people put
    their trades in the night before, and they execute when the market opens? It is
    trivial to change the previous code to answer that question. Instead, let''s see
    if, on average, more shares were traded within the first 30 minutes of trading
    or in the last 30 minutes for the week in question. We can combine `between_time()`
    with `groupby()` to answer this question. In addition, we need to use `filter()`
    to exclude groups from the aggregation. The excluded groups are times that aren''t
    in the time range we want:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会想知道这是否也发生在前两分钟内。人们是否在前一天晚上进行交易，然后在市场开盘时执行？更改前面的代码以回答这个问题是微不足道的。相反，让我们看看在讨论的这周中，平均而言，更多的股票是在开盘后的前30分钟内交易还是在最后30分钟内交易。我们可以结合`between_time()`和`groupby()`来回答这个问题。此外，我们需要使用`filter()`来排除聚合中的组。被排除的组是不在我们想要的时间范围内的时间：
- en: '[PRE88]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'For the week in question, there were 18,593 more trades on average around the
    opening time than the closing time:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论的这周中，开盘时间周围的平均交易量比收盘时间多了18,593笔：
- en: '[PRE89]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Tip
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: We can use the `normalize()` method on `DatetimeIndex` objects or after first
    accessing the `dt` attribute of a `Series` object to normalize all the datetimes
    to midnight. This is helpful when the time isn't adding value to our data. There
    are examples of this in the notebook.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对`DatetimeIndex`对象使用`normalize()`方法或在首次访问`Series`对象的`dt`属性后使用它，以将所有日期时间规范化为午夜。当时间对我们的数据没有添加价值时，这非常有用。笔记本中有此类示例。
- en: With the stock data, we have a snapshot of the price for each minute or day
    (depending on the granularity), but we may be interested in seeing the change
    between time periods as a time series rather than aggregating the data. For this,
    we need to learn how to create lagged data.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 通过股票数据，我们可以获得每分钟或每天的价格快照（具体取决于数据粒度），但我们可能更关心将时间段之间的变化作为时间序列显示，而不是聚合数据。为此，我们需要学习如何创建滞后数据。
- en: Shifting for lagged data
  id: totrans-495
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滞后数据的偏移
- en: 'We can use the `shift()` method to create lagged data. By default, the shift
    will be by one period, but this can be any integer (positive or negative). Let''s
    use `shift()` to create a new column that indicates the previous day''s closing
    price for the daily Facebook stock data. From this new column, we can calculate
    the price change due to after-hours trading (after the market close one day right
    up to the market open the following day):'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`shift()`方法创建滞后数据。默认情况下，偏移量为一个周期，但它可以是任何整数（正数或负数）。我们来使用`shift()`方法创建一个新列，表示每日Facebook股票的前一日收盘价。通过这个新列，我们可以计算由于盘后交易导致的价格变化（即一天市场收盘后至下一天市场开盘前的交易）：
- en: '[PRE90]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'This gives us the days that were most affected by after-hours trading:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们展示了受盘后交易影响最大的日期：
- en: '![Figure 4.54 – Using lagged data to calculate after-hours changes in stock
    price'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.54 – 使用滞后数据计算盘后股价变化'
- en: '](img/Figure_4.54_B16834.jpg)'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.54_B16834.jpg)'
- en: Figure 4.54 – Using lagged data to calculate after-hours changes in stock price
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.54 – 使用滞后数据计算盘后股价变化
- en: Tip
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: To add/subtract time from the datetimes in the index, consider using `Timedelta`
    objects instead. There is an example of this in the notebook.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 若要从索引中的日期时间添加/减去时间，可以考虑使用`Timedelta`对象。笔记本中有相关示例。
- en: In the previous example, we used the shifted data to calculate the change across
    columns. However, if, rather than the after-hours trading, we were interested
    in the change in Facebook's stock price each day, we would calculate the difference
    between the closing price and the shifted closing price. Pandas makes it a little
    easier than this, as we will see next.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的例子中，我们使用了偏移后的数据来计算跨列的变化。然而，如果我们感兴趣的不是盘后交易，而是Facebook股价的每日变化，我们将计算收盘价与偏移后收盘价之间的差异。Pandas使这变得比这更简单，我们稍后会看到。
- en: Differenced data
  id: totrans-505
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 差分数据
- en: 'We''ve already discussed creating lagged data with the `shift()` method. However,
    often, we are interested in how the values change from one time period to the
    next. For this, `pandas` has the `diff()` method. By default, this will calculate
    the change from time period *t-1* to time period *t*:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过如何使用`shift()`方法创建滞后数据。然而，通常我们关心的是值从一个时间周期到下一个时间周期的变化。为此，`pandas`提供了`diff()`方法。默认情况下，它会计算从时间周期*t-1*到时间周期*t*的变化：
- en: '![](img/Formula_04_001.jpg)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_001.jpg)'
- en: 'Note that this is equivalent to subtracting the result of `shift()` from the
    original data:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这相当于从原始数据中减去`shift()`的结果：
- en: '[PRE91]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'We can use `diff()` to easily calculate the day-over-day change in the Facebook
    stock data:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`diff()`轻松计算Facebook股票数据的逐日变化：
- en: '[PRE92]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'For the first few trading days of the year, we can see that the stock price
    increased, and that the volume traded decreased daily:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 对于年的前几个交易日，我们可以看到股价上涨，而成交量每天减少：
- en: '![Figure 4.55 – Calculating day-over-day changes'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.55 – 计算逐日变化'
- en: '](img/Figure_4.55_B16834.jpg)'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.55_B16834.jpg)'
- en: Figure 4.55 – Calculating day-over-day changes
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.55 – 计算逐日变化
- en: Tip
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: To specify the number of periods that are used for the difference, simply pass
    in an integer to `diff()`. Note that this number can be negative. An example of
    this is in the notebook.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 要指定用于计算差异的周期数，只需向`diff()`传递一个整数。请注意，这个数字可以是负数。笔记本中有相关示例。
- en: Resampling
  id: totrans-518
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重采样
- en: 'Sometimes, the data is at a granularity that isn''t conducive to our analysis.
    Consider the case where we have data per minute for the full year of 2018\. The
    level of granularity and nature of the data may render plotting useless. Therefore,
    we will need to aggregate the data to a less granular frequency:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，数据的粒度不适合我们的分析。假设我们有2018年全年的每分钟数据，粒度和数据的性质可能使得绘图无用。因此，我们需要将数据汇总到一个较低粒度的频率：
- en: '![Figure 4.56 – Resampling can be used to roll up granular data'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.56 – 重采样可以用于汇总细粒度数据'
- en: '](img/Figure_4.56_B16834.jpg)'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.56_B16834.jpg)'
- en: Figure 4.56 – Resampling can be used to roll up granular data
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.56 – 重采样可以用于汇总细粒度数据
- en: 'Suppose we had a full year of the data in *Figure 4.50* (Facebook stock data
    by the minute). It''s possible that this level of granularity is beyond what is
    useful for us, in which case we can use the `resample()` method to aggregate our
    time series data to a different granularity. To use `resample()`, all we have
    to do is say how we want to roll up the data and tack on an optional call to an
    aggregation method. For example, we can resample this minute-by-minute data to
    a daily frequency and specify how to aggregate each column:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们拥有*图 4.50*中的一整年的数据（Facebook 股票按分钟显示）。这种粒度可能超出了我们的需求，在这种情况下，我们可以使用 `resample()`
    方法将时间序列数据聚合为不同的粒度。使用 `resample()` 时，我们只需要告诉它如何汇总数据，并可选地调用聚合方法。例如，我们可以将这些按分钟的股票数据重采样为每日频率，并指定如何聚合每一列：
- en: '[PRE93]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'This is equivalent to the result we got back in the *Time-based selection and
    filtering* section (*Figure 4.51*):'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在*基于时间的选择和过滤*部分得到的结果相当（*图 4.51*）：
- en: '![Figure 4.57 – Per-minute data resampled into daily data'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.57 – 每分钟的数据重采样为日数据](img/Figure_4.57_B16834.jpg)'
- en: '](img/Figure_4.57_B16834.jpg)'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.57_B16834.jpg)'
- en: Figure 4.57 – Per-minute data resampled into daily data
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.57 – 将每分钟的数据重采样为日数据
- en: 'We can resample to any frequency supported by `pandas` (more information can
    be found in the documentation at [http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html](http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html)).
    Let''s resample the daily Facebook stock data to the quarterly average:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重采样为 `pandas` 支持的任何频率（更多信息可以参考文档：[http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html](http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html)）。让我们将每日的
    Facebook 股票数据重采样为季度平均值：
- en: '[PRE94]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'This gives us the average quarterly performance of the stock. The fourth quarter
    of 2018 was clearly troublesome:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了股票的季度平均表现。2018年第四季度显然很糟糕：
- en: '![Figure 4.58 – Resampling to quarterly averages'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.58 – 重采样为季度平均值](img/Figure_4.59_B16834.jpg)'
- en: '](img/Figure_4.58_B16834.jpg)'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.58_B16834.jpg)'
- en: Figure 4.58 – Resampling to quarterly averages
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.58 – 重采样为季度平均值
- en: 'To look further into this, we can use the `apply()` method to look at the difference
    between how the quarter began and how it ended. We will also need the `first()`
    and `last()` methods from the *Time-based selection and filtering* section:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步分析，我们可以使用 `apply()` 方法查看季度开始和结束时的差异。我们还需要在*基于时间的选择和过滤*部分使用 `first()` 和
    `last()` 方法：
- en: '[PRE95]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Facebook''s stock price declined in all but the second quarter:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook 的股票价格在除第二季度外的所有季度都出现了下降：
- en: '![Figure 4.59 – Summarizing Facebook stock''s performance per quarter in 2018'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.59 – 总结 Facebook 2018年每个季度的股票表现](img/Figure_4.59_B16834.jpg)'
- en: '](img/Figure_4.59_B16834.jpg)'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.57_B16834.jpg)'
- en: Figure 4.59 – Summarizing Facebook stock's performance per quarter in 2018
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.59 – 总结 Facebook 2018年每个季度的股票表现
- en: 'Consider the melted minute-by-minute stock data in `melted_stock_data.csv`:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑 `melted_stock_data.csv` 中按分钟融化的股票数据：
- en: '[PRE96]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'The OHLC format makes it easy to analyze the stock data, but a single column
    is trickier:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: OHLC 格式使得分析股票数据变得更加容易，但如果数据只有一列，则会变得更加复杂：
- en: '![Figure 4.60 – Stock prices by the minute'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.60 – 按分钟显示股票价格](img/Figure_4.61_B16834.jpg)'
- en: '](img/Figure_4.60_B16834.jpg)'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.60_B16834.jpg)'
- en: Figure 4.60 – Stock prices by the minute
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.60 – 按分钟显示股票价格
- en: 'The `Resampler` object we get back after calling `resample()` has an `ohlc()`
    method, which we can use to retrieve the OHLC data we are used to seeing:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在调用 `resample()` 后得到的 `Resampler` 对象有一个 `ohlc()` 方法，我们可以使用它来获取我们习惯看到的 OHLC
    数据：
- en: '[PRE97]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Since the column in the original data was called `price`, we select it after
    calling `ohlc()`, which is pivoting our data. Otherwise, we will have a hierarchical
    index in the columns:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 由于原始数据中的列叫做 `price`，我们在调用 `ohlc()` 后选择它，这样我们就能对数据进行透视处理。否则，我们会在列中得到一个层次化索引：
- en: '![Figure 4.61 – Resampling the stock prices per minute to form daily OHLC data'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.61 – 将按分钟的股票价格重采样以形成每日 OHLC 数据](img/Figure_4.59_B16834.jpg)'
- en: '](img/Figure_4.61_B16834.jpg)'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.61_B16834.jpg)'
- en: Figure 4.61 – Resampling the stock prices per minute to form daily OHLC data
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.61 – 将按分钟的股票价格重采样以形成每日 OHLC 数据
- en: 'In the previous examples, we `asfreq()` after to not aggregate the result:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们使用 `asfreq()` 来避免对结果进行聚合：
- en: '[PRE98]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Note that when we resample at a granularity that''s finer than the data we
    have, it will introduce `NaN` values:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们在比已有数据更细粒度的情况下进行重采样时，它将引入 `NaN` 值：
- en: '![Figure 4.62 – Upsampling increases the granularity of the data and will introduce
    null values'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.62 – 向上采样会增加数据的粒度，并且会引入空值](img/Figure_4.62_B16834.jpg)'
- en: '](img/Figure_4.62_B16834.jpg)'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.58_B16834.jpg)'
- en: Figure 4.62 – Upsampling increases the granularity of the data and will introduce
    null values
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.62 – 向上采样增加了数据的粒度，并会引入空值
- en: 'The following are a few ways we can handle the `NaN` values. In the interest
    of brevity, examples of these are in the notebook:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是处理`NaN`值的一些方法。为了简洁起见，示例在笔记本中：
- en: Use `pad()` after `resample()` to forward fill.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`resample()`之后使用`pad()`进行前向填充。
- en: Call `fillna()` after `resample()`, as we saw in [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*, when we handled missing values.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`resample()`之后调用`fillna()`，正如我们在[*第3章*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061)《使用Pandas进行数据清洗》中看到的，当我们处理缺失值时。
- en: Use `asfreq()` followed by `assign()` to handle each column individually.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`asfreq()`后接`assign()`来单独处理每一列。
- en: So far, we have been working with time series data stored in a single `DataFrame`
    object, but we may want to combine time series. While the techniques discussed
    in the *Merging DataFrames* section will work for time series, `pandas` provides
    additional functionality for merging time series so that we can merge on close
    matches rather than requiring an exact match. We will discuss these next.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理存储在单个`DataFrame`对象中的时间序列数据，但我们可能希望合并多个时间序列。虽然在*合并DataFrame*部分讨论的技术适用于时间序列，`pandas`提供了更多的功能来合并时间序列，这样我们就可以基于接近的匹配进行合并，而不需要完全匹配。接下来我们将讨论这些内容。
- en: Merging time series
  id: totrans-564
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并时间序列
- en: Time series often go down to the second or are even more granular, meaning that
    it can be difficult to merge if the entries don't have the same datetime. Pandas
    solves this problem with two additional merging functions. When we want to pair
    up observations that are close in time, we can use `pd.merge_asof()` to match
    on nearby keys rather than on equal keys, like we did with joins. On the other
    hand, if we want to match up the equal keys and interleave the keys without matches,
    we can use `pd.merge_ordered()`.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列通常精确到秒，甚至更为细致，这意味着如果条目没有相同的日期时间，合并可能会非常困难。Pandas通过两个附加的合并函数解决了这个问题。当我们想要配对接近时间的观测值时，可以使用`pd.merge_asof()`，根据附近的键进行匹配，而不是像我们在连接中那样使用相等的键。另一方面，如果我们想匹配相等的键并交错没有匹配项的键，可以使用`pd.merge_ordered()`。
- en: 'To illustrate how these work, we are going to use the `fb_prices` and `aapl_prices`
    tables in the `stocks.db` SQLite database. These contain the prices of Facebook
    and Apple stock, respectively, along with a timestamp of when the price was recorded.
    Note that the Apple data was collected before the stock split in August 2020 ([https://www.marketwatch.com/story/3-things-to-know-about-apples-stock-split-2020-08-28](https://www.marketwatch.com/story/3-things-to-know-about-apples-stock-split-2020-08-28)).
    Let''s read these tables from the database:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这些方法的工作原理，我们将使用`stocks.db` SQLite数据库中的`fb_prices`和`aapl_prices`表。这些表分别包含Facebook和Apple股票的价格，以及记录价格时的时间戳。请注意，Apple数据是在2020年8月股票拆分之前收集的([https://www.marketwatch.com/story/3-things-to-know-about-apples-stock-split-2020-08-28](https://www.marketwatch.com/story/3-things-to-know-about-apples-stock-split-2020-08-28))。让我们从数据库中读取这些表：
- en: '[PRE99]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'The Facebook data is at the minute granularity; however, we have (fictitious)
    seconds for the Apple data:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook的数据是按分钟粒度的；然而，Apple的数据是按（虚构的）秒粒度的：
- en: '[PRE100]'
  id: totrans-569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'If we use `merge()` or `join()`, we will only have values for both Apple and
    Facebook when the Apple price was at the top of the minute. Instead, to try and
    line these up, we can perform an *as of* merge. In order to handle the mismatch,
    we will specify to merge with the nearest minute (`direction=''nearest''`) and
    require that a match can only occur between times that are within 30 seconds of
    each other (`tolerance`). This will place the Apple data with the minute that
    it is closest to, so `9:31:52` will go with `9:32` and `9:37:07` will go with
    `9:37`. Since the times are on the index, we pass in `left_index` and `right_index`,
    just like we did with `merge()`:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`merge()`或`join()`，只有当Apple的价格位于整分钟时，我们才会得到两者的匹配值。相反，为了对齐这些数据，我们可以执行*as
    of*合并。为了处理这种不匹配情况，我们将指定合并时使用最近的分钟(`direction='nearest'`)，并要求匹配只能发生在相差不超过30秒的时间之间(`tolerance`)。这将把Apple数据与最接近的分钟对齐，因此`9:31:52`将与`9:32`匹配，`9:37:07`将与`9:37`匹配。由于时间位于索引中，我们像在`merge()`中一样，传入`left_index`和`right_index`：
- en: '[PRE101]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'This is similar to a left join; however, we are more lenient when matching
    the keys. Note that in the case where multiple entries in the Apple data match
    the same minute, this function will only keep the closest one. We get a null value
    for `9:31` because the entry for Apple at `9:31` was `9:31:52`, which gets placed
    at `9:32` when using `nearest`:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于左连接；然而，在匹配键时我们更加宽松。需要注意的是，如果多个苹果数据条目匹配相同的分钟，这个函数只会保留最接近的一个。我们在`9:31`处得到一个空值，因为苹果在`9:31`的条目是`9:31:52`，当使用`nearest`时它会被放置到`9:32`：
- en: '![Figure 4.63 – Merging time series data with a 30-second tolerance'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.63 – 在30秒容忍度下合并时间序列数据'
- en: '](img/Figure_4.63_B16834.jpg)'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.63_B16834.jpg)'
- en: Figure 4.63 – Merging time series data with a 30-second tolerance
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.63 – 在30秒容忍度下合并时间序列数据
- en: 'If we don''t want the behavior of a left join, we can use the `pd.merge_ordered()`
    function instead. This will allow us to specify our join type, which will be `''outer''`
    by default. We will have to reset our index to be able to join on the datetimes,
    however:'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不希望使用左连接的行为，可以改用`pd.merge_ordered()`函数。这将允许我们指定连接类型，默认情况下为`'outer'`。然而，我们需要重置索引才能在日期时间上进行连接：
- en: '[PRE102]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'This strategy will give us null values whenever the times don''t match exactly,
    but it will at least sort them for us:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略会在时间不完全匹配时给我们空值，但至少会对它们进行排序：
- en: '![Figure 4.64 – Performing a strict merge on time series data and ordering
    it'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.64 – 对时间序列数据执行严格的合并并对其进行排序'
- en: '](img/Figure_4.64_B16834.jpg)'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.64_B16834.jpg)'
- en: Figure 4.64 – Performing a strict merge on time series data and ordering it
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.64 – 对时间序列数据执行严格的合并并对其进行排序
- en: Tip
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: We can pass `fill_method='ffill'` to `pd.merge_ordered()` to forward-fill the
    first `NaN` after a value, but it does not propagate beyond that; alternatively,
    we can chain a call to `fillna()`. There is an example of this in the notebook.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`fill_method='ffill'`传递给`pd.merge_ordered()`，以在一个值后填充第一个`NaN`，但它不会继续传播；另外，我们可以链式调用`fillna()`。笔记本中有一个示例。
- en: The `pd.merge_ordered()` function also makes it possible to perform a group-wise
    merge, so be sure to check out the documentation for more information.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.merge_ordered()`函数还使得按组合并成为可能，因此请务必查看文档以获取更多信息。'
- en: Summary
  id: totrans-585
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we discussed how to join dataframes, how to determine the data
    we will lose for each type of join using set operations, and how to query dataframes
    as we would a database. We then went over some more involved transformations on
    our columns, such as binning and ranking, and how to do so efficiently with the
    `apply()` method. We also learned the importance of vectorized operations in writing
    efficient `pandas` code. Then, we explored window calculations and using pipes
    for cleaner code. Our discussion of window calculations served as a primer for
    aggregating across whole dataframes and by groups. We also went over how to generate
    pivot tables and crosstabs. Finally, we looked at some time series-specific functionality
    in `pandas` for everything from selection and aggregation to merging.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何连接数据框，如何使用集合操作确定每种连接类型丢失的数据，以及如何像查询数据库一样查询数据框。接着我们讲解了一些更复杂的列变换，例如分箱和排名，以及如何使用`apply()`方法高效地进行这些操作。我们还学习了在编写高效`pandas`代码时矢量化操作的重要性。随后，我们探索了窗口计算和使用管道来使代码更简洁。关于窗口计算的讨论为聚合整个数据框和按组聚合奠定了基础。我们还讨论了如何生成透视表和交叉表。最后，我们介绍了`pandas`中针对时间序列的特定功能，涵盖了从选择、聚合到合并等各个方面。
- en: In the next chapter, we will cover visualization, which `pandas` implements
    by providing a wrapper around `matplotlib`. Data wrangling will play a key role
    in prepping our data for visualization, so be sure to complete the exercises that
    are provided in the following section before moving on.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论可视化，`pandas`通过提供一个封装器来实现这一功能，封装了`matplotlib`。数据清洗将在准备数据以进行可视化时发挥关键作用，因此在继续之前，一定要完成下一节提供的练习。
- en: Exercises
  id: totrans-588
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Using the CSV files in the `exercises/` folder and what we have learned so
    far in this book, complete the following exercises:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`exercises/`文件夹中的CSV文件以及我们到目前为止在本书中学到的内容，完成以下练习：
- en: With the `earthquakes.csv` file, select all the earthquakes in Japan with a
    magnitude of 4.9 or greater using the `mb` magnitude type.
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`earthquakes.csv`文件，选择所有日本的地震，并使用`mb`震级类型筛选震级为4.9或更大的地震。
- en: Create bins for each full number of earthquake magnitude (for instance, the
    first bin is (0, 1], the second is (1, 2], and so on) with the `ml` magnitude
    type and count how many are in each bin.
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建每个地震震级完整数字的箱子（例如，第一个箱子是(0, 1]，第二个箱子是(1, 2]，以此类推），使用`ml`震级类型并计算每个箱子中的数量。
- en: 'Using the `faang.csv` file, group by the ticker and resample to monthly frequency.
    Make the following aggregations:'
  id: totrans-592
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`faang.csv`文件，按股票代码分组并重采样为月频率。进行以下聚合：
- en: a) Mean of the opening price
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 开盘价的均值
- en: b) Maximum of the high price
  id: totrans-594
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 最高价的最大值
- en: c) Minimum of the low price
  id: totrans-595
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 最低价的最小值
- en: d) Mean of the closing price
  id: totrans-596
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 收盘价的均值
- en: e) Sum of the volume traded
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 成交量总和
- en: Build a crosstab with the earthquake data between the `tsunami` column and the
    `magType` column. Rather than showing the frequency count, show the maximum magnitude
    that was observed for each combination. Put the magnitude type along the columns.
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个交叉表，展示地震数据中`tsunami`列和`magType`列之间的关系。不要显示频次计数，而是显示每个组合观察到的最大震级。将震级类型放在列中。
- en: Calculate the rolling 60-day aggregations of the OHLC data by ticker for the
    FAANG data. Use the same aggregations as exercise *3*.
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算FAANG数据中每个股票的OHLC数据的滚动60天聚合值。使用与练习*3*相同的聚合方法。
- en: Create a pivot table of the FAANG data that compares the stocks. Put the ticker
    in the rows and show the averages of the OHLC and volume traded data.
  id: totrans-600
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个FAANG数据的透视表，比较股票。将股票代码放入行中，显示OHLC和成交量数据的平均值。
- en: Calculate the Z-scores for each numeric column of Amazon's data (`ticker` is
    AMZN) in Q4 2018 using `apply()`.
  id: totrans-601
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`apply()`方法计算2018年第四季度亚马逊数据（`ticker`为AMZN）每个数值列的Z分数。
- en: 'Add event descriptions:'
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加事件描述：
- en: 'a) Create a dataframe with the following three columns: `ticker`, `date`, and
    `event`. The columns should have the following values:'
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 创建一个数据框，包含以下三列：`ticker`，`date`和`event`。这些列应包含以下值：
- en: 'i) `ticker`: `''FB''`'
  id: totrans-604
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'i) `ticker`: `''FB''`'
- en: 'ii) `date`: `[''2018-07-25'', ''2018-03-19'', ''2018-03-20'']`'
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ii) `date`: `[''2018-07-25'', ''2018-03-19'', ''2018-03-20'']`'
- en: 'iii) `event`: `[''Disappointing user growth announced after close.'', ''Cambridge
    Analytica story'', ''FTC investigation'']`'
  id: totrans-606
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'iii) `event`: `[''公布财报后用户增长令人失望。'', ''剑桥分析丑闻'', ''FTC调查'']`'
- en: b) Set the index to `['date', 'ticker']`.
  id: totrans-607
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 将索引设置为`['date', 'ticker']`。
- en: c) Merge this data with the FAANG data using an outer join.
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 使用外连接将此数据与FAANG数据合并。
- en: Use the `transform()` method on the FAANG data to represent all the values in
    terms of the first date in the data. To do so, divide all the values for each
    ticker by the values for the first date in the data for that ticker. This is referred
    to as an `transform()` can take a function name.
  id: totrans-609
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对FAANG数据使用`transform()`方法，将所有的值表示为数据中第一个日期的值。为此，将每个股票的所有值除以该股票在数据中第一个日期的值。这被称为`transform()`可以接受一个函数名称。
- en: The `covid19_cases.csv` file.
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`covid19_cases.csv` 文件。'
- en: ii) Create a `date` column by parsing the `dateRep` column into a datetime.
  id: totrans-611
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ii) 通过解析`dateRep`列为datetime格式来创建一个`date`列。
- en: iii) Set the `date` column as the index.
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: iii) 将`date`列设置为索引。
- en: iv) Use the `replace()` method to update all occurrences of `United_States_of_America`
    and `United_Kingdom` to `USA` and `UK`, respectively.
  id: totrans-613
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: iv) 使用`replace()`方法将所有`United_States_of_America`和`United_Kingdom`替换为`USA`和`UK`。
- en: v) Sort the index.
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: v) 排序索引。
- en: b) For the five countries with the most cases (cumulative), find the day with
    the largest number of cases.
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 对于病例最多的五个国家（累计），找出病例数最多的那一天。
- en: c) Find the 7-day average change in COVID-19 cases for the last week in the
    data for the five countries with the most cases.
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 找出数据中最后一周五个疫情病例最多的国家的COVID-19病例7天平均变化。
- en: d) Find the first date that each country other than China had cases.
  id: totrans-617
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 找出中国以外的每个国家首次出现病例的日期。
- en: e) Rank the countries by cumulative cases using percentiles.
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 按累计病例数使用百分位数对国家进行排名。
- en: Further reading
  id: totrans-619
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Check out the following resources for more information on the topics that were
    covered in this chapter:'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下资源，了解本章中涉及的主题：
- en: '*Intro to SQL: Querying and managing data*: [https://www.khanacademy.org/computing/computer-programming/sql](https://www.khanacademy.org/computing/computer-programming/sql)'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SQL入门：查询和管理数据*：[https://www.khanacademy.org/computing/computer-programming/sql](https://www.khanacademy.org/computing/computer-programming/sql)'
- en: '*(Pandas) Comparison with SQL*: [https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html)'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*(Pandas)与SQL的比较*：[https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html)'
- en: '*Set Operations*: [https://www.probabilitycourse.com/chapter1/1_2_2_set_operations.php](https://www.probabilitycourse.com/chapter1/1_2_2_set_operations.php)'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集合运算*：[https://www.probabilitycourse.com/chapter1/1_2_2_set_operations.php](https://www.probabilitycourse.com/chapter1/1_2_2_set_operations.php)'
- en: '**args and **kwargs in Python explained*: [https://pythontips.com/2013/08/04/args-and-kwargs-in-python-explained/](https://yasoob.me/2013/08/04/args-and-kwargs-in-python-explained/)'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python中的**args和**kwargs解释*：[https://pythontips.com/2013/08/04/args-and-kwargs-in-python-explained/](https://yasoob.me/2013/08/04/args-and-kwargs-in-python-explained/)'
