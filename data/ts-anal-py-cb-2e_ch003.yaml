- en: 2 Reading Time Series Data from Files
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 从文件读取时间序列数据
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 书籍社区
- en: '![](img/file0.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/file0.png)'
- en: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
- en: In this chapter, we will use **pandas**, a popular **Python** library with a
    rich set of I/O tools, data wrangling, and date/time functionality to streamline
    working with **time series data**. In addition, you will explore several reader
    functions available in pandas to ingest data from different file types, such as
    **Comma-Separated Value** (**CSV**), Excel, and SAS. You will explore reading
    from files, whether stored locally on your drive or remotely on the cloud, such
    as an **AWS S3 bucket**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 **pandas**，一个流行的 **Python** 库，具有丰富的 I/O 工具、数据处理和日期/时间功能，用于简化处理 **时间序列数据**。此外，您还将探索
    pandas 中可用的多个读取函数，来导入不同文件类型的数据，如 **逗号分隔值** (**CSV**)、Excel 和 SAS。您将探索如何从文件中读取数据，无论这些文件是存储在本地驱动器上，还是远程存储在云端，如
    **AWS S3 桶**。
- en: Time series data is complex and can be in different shapes and formats. Conveniently,
    the pandas reader functions offer a vast number of arguments (parameters) to help
    handle such variety in the data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据是复杂的，可能有不同的形状和格式。幸运的是，pandas 的读取函数提供了大量的参数（选项），以帮助处理数据的多样性。
- en: The **pandas** library provides two fundamental data structures, Series and
    DataFrame, implemented as classes. The DataFrame class is a distinct data structure
    for working with tabular data (think rows and columns in a spreadsheet). The main
    difference between the two data structures is that a Series is one-dimensional
    (single column), and a DataFrame is two-dimensional (multiple columns). The relationship
    between the two is that you get a Series when you slice out a column from a DataFrame.
    You can think of a DataFrame as a side-by-side concatenation of two or more Series
    objects.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**pandas** 库提供了两个基本的数据结构：Series 和 DataFrame，它们作为类实现。DataFrame 类是一个用于处理表格数据（类似于电子表格中的行和列）的独特数据结构。它们之间的主要区别在于，Series
    是一维的（单列），而 DataFrame 是二维的（多列）。它们之间的关系是，当你从 DataFrame 中切片出一列时，你得到的是一个 Series。你可以将
    DataFrame 想象成是两个或多个 Series 对象的并排拼接。'
- en: A particular feature of the Series and DataFrames data structures is that they
    both have a labeled axis called an index. A specific type of index that you will
    often see with time series data is the `DatetimeIndex` , which you will explore
    further in this chapter. Generally, the index makes slicing and dicing operations
    very intuitive. For example, to make a DataFrame ready for time series analysis,
    you will learn how to create DataFrames with an index of the `DatetimeIndex` type.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Series 和 DataFrame 数据结构的一个特性是，它们都具有一个叫做索引的标签轴。你在时间序列数据中常见的索引类型是 `DatetimeIndex`，你将在本章中进一步了解。通常，索引使切片和切割操作变得非常直观。例如，为了使
    DataFrame 准备好进行时间序列分析，你将学习如何创建具有 `DatetimeIndex` 类型索引的 DataFrame。
- en: 'We will cover the following recipes on how to ingest data into a pandas DataFrame:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下将数据导入 pandas DataFrame 的方法：
- en: Reading data from CSVs and other delimited files
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 CSV 和其他分隔文件读取数据
- en: Reading data from an Excel file
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Excel 文件读取数据
- en: Reading data from URLs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 URL 读取数据
- en: Reading data from Parquet filesWorking with large data files
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Parquet 文件读取数据处理大型数据文件
- en: WHY DATETIMEINDEX?
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么选择 DATETIMEINDEX？
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A pandas DataFrame with an index of the `DatetimeIndex` type unlocks a large
    set of features and useful functions needed when working with time series data.
    You can think of it as adding a layer of intelligence or awareness to pandas to
    treat the DataFrame as a time series DataFrame.
  id: totrans-15
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个具有 `DatetimeIndex` 类型索引的 pandas DataFrame 解锁了在处理时间序列数据时所需的大量功能和有用的函数。你可以将其视为为
    pandas 增加了一层智能或感知，使其能够将 DataFrame 视为时间序列 DataFrame。
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter and forward, we will extensively use pandas 2.2.0 (released
    January 20, 2024).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章及后续章节中，我们将广泛使用 pandas 2.2.0（2024年1月20日发布）。
- en: Throughout our journey, you will be installing additional Python libraries to
    use in conjunction with pandas. You can download the Jupyter notebooks from the
    GitHub repository ([https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch2/Chapter%202.ipynb](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch2/Chapter%202.ipynb))
    to follow along.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的整个过程中，你将安装其他Python库，以便与pandas一起使用。你可以从GitHub仓库下载Jupyter笔记本（[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch2/Chapter%202.ipynb](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch2/Chapter%202.ipynb)）来跟着做。
- en: 'You can download the datasets used in this chapter from the GitHub repository
    using this link: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch2](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch2).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下链接从GitHub仓库下载本章使用的数据集：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch2](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch2)。
- en: Reading data from CSVs and other delimited files
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从CSV文件和其他分隔符文件读取数据
- en: In this recipe, you will use the `pandas.read_csv()` function, which offers
    a large set of parameters that you will explore to ensure the data is properly
    read into a time series DataFrame. In addition, you will learn how to specify
    an index column, parse the index to be of the type `DatetimeIndex`, and parse
    string columns that contain dates into `datetime` objects.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，你将使用`pandas.read_csv()`函数，它提供了一个庞大的参数集，你将探索这些参数以确保数据正确读取到时间序列DataFrame中。此外，你将学习如何指定索引列，将索引解析为`DatetimeIndex`类型，并将包含日期的字符串列解析为`datetime`对象。
- en: Generally, using Python, data read from a CSV file will be in string format
    (text). When using the `read_csv` method in pandas, it will try to infer the appropriate
    data types (dtype), and, in most cases, it does a great job at that. However,
    there are situations where you will need to explicitly indicate which columns
    to cast to a specific data type. For example, you will specify which column(s)
    to parse as dates using the `parse_dates` parameter in this recipe.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用Python从CSV文件读取的数据会是字符串格式（文本）。当使用`read_csv`方法在pandas中读取时，它会尝试推断适当的数据类型（dtype），在大多数情况下，它做得非常好。然而，也有一些情况需要你明确指示哪些列应转换为特定的数据类型。例如，你将使用`parse_dates`参数指定要解析为日期的列。
- en: Getting ready
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will read a CSV file containing hypothetical box office numbers for a movie.
    The file is provided in the GitHub repository for this book. The data file is
    in `datasets/Ch2/movieboxoffice.csv`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你将读取一个包含假设电影票房数据的CSV文件。该文件已提供在本书的GitHub仓库中。数据文件位于`datasets/Ch2/movieboxoffice.csv`。
- en: How to do it…
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'You will ingest our CSV file using pandas and leverage some of the available
    parameters in `read_csv`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用pandas读取我们的CSV文件，并利用`read_csv`中的一些可用参数：
- en: 'First, let''s load the libraries:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，加载所需的库：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a `Path` object for the file location:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为文件位置创建一个`Path`对象：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Read the CSV file into a DataFrame using the `read_csv` function and pass the
    `filepath` with additional parameters.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`read_csv`函数将CSV文件读取到DataFrame中，并传递包含额外参数的`filepath`。
- en: 'The first column in the CSV file contains movie release dates, and it needs
    to be set as an index of the `DatetimeIndex` (`index_col=0` and `parse_dates=[''Date'']`)
    types. Specify which columns you want to include by providing a list of column
    names to `usecols`. The default behavior is that the first row includes the header
    (`header=0`):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: CSV文件的第一列包含电影发布日期，需要将其设置为`DatetimeIndex`类型的索引（`index_col=0`和`parse_dates=['Date']`）。通过提供列名列表给`usecols`来指定你希望包含的列。默认行为是第一行包含表头（`header=0`）：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will output the following first five rows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下前五行：
- en: '![Figure 2.1: The first five rows of the ts DataFrame in JupyterLab](img/file16.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1：JupyterLab中ts DataFrame的前五行](img/file16.png)'
- en: 'Figure 2.1: The first five rows of the ts DataFrame in JupyterLab'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：JupyterLab中ts DataFrame的前五行
- en: 'Print a summary of the DataFrame to check the index and column data types:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印DataFrame的摘要以检查索引和列的数据类型：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that the `Date` column is now an index (not a column) of the type `DatetimeIndex`.
    Additionally, both the `Daily` and `Forecast` columns have the wrong dtype inference.
    You would expect them to be of the `float` type. The issue is due to the source
    CSV file containing dollar signs (`$`) and thousand separators (`,`) in both columns.
    The presence of non-numeric characters will cause the columns to be interpreted
    as strings. A column with the `dtype` object indicates either a string column
    or a column with mixed dtypes (not homogeneous).
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，`Date` 列现在是一个索引（而非列），类型为 `DatetimeIndex`。另外，`Daily` 和 `Forecast` 列的 dtype
    推断错误。你本来期望它们是 `float` 类型。问题在于源 CSV 文件中的这两列包含了美元符号 (`$`) 和千位分隔符（`,`）。这些非数字字符会导致列被解释为字符串。具有
    `dtype` 为 `object` 的列表示该列包含字符串或混合类型的数据（不是同质的）。
- en: 'To fix this, you need to remove both the dollar sign (`$`) and thousand separators
    (`,`) or any other non-numeric character. You can accomplish this using `str.replace()`,
    which can take a regular expression to remove all non-numeric characters but exclude
    the period (`.`) for the decimal place. Removing these characters does not convert
    the dtype, so you will need to cast those two columns as a float dtype using `.astype(float)`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，你需要去除美元符号 (`$`) 和千位分隔符（`,`）或任何其他非数字字符。你可以使用 `str.replace()` 来完成此操作，它可以接受正则表达式来移除所有非数字字符，但排除小数点（`.`）。移除这些字符不会转换
    dtype，因此你需要使用 `.astype(float)` 将这两列转换为 `float` 类型：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Print a summary of the updated DataFrame:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 打印更新后的 DataFrame 摘要：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, you have a DataFrame with `DatetimeIndex` and both `Daily` and `Forecast`
    columns are of the `float64` dtype (numeric fields).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你拥有一个 `DatetimeIndex` 的 DataFrame，并且 `Daily` 和 `Forecast` 列的 dtype 都是 `float64`（数字类型）。
- en: How it works…
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Using pandas for data transformation is fast since it loads the data into memory.
    For example, the `read_csv` method reads and loads the entire data into a DataFrame
    in memory. When requesting a DataFrame summary with the `info()` method, the output
    will display memory usage for the entire DataFrame in addition to column and index
    data types. To get the exact memory usage for each column, including the index,
    you can use the `memory_usage()` method:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 进行数据转换非常快速，因为它将数据加载到内存中。例如，`read_csv` 方法会读取并将整个数据加载到内存中的 DataFrame
    中。当使用 `info()` 方法请求 DataFrame 的摘要时，输出除了显示列和索引的数据类型外，还会显示整个 DataFrame 的内存使用情况。要获取每个列的确切内存使用情况，包括索引，你可以使用
    `memory_usage()` 方法：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The total will match what was provided in the DataFrame summary:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 总计将与 DataFrame 摘要中提供的内容匹配：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So far, you have used a few of the available parameters when reading a CSV file
    using `read_csv`. The more familiar you become with the different options available
    in any pandas reader functions, the more upfront preprocessing you can do during
    data ingestion (reading).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你在使用 `read_csv` 读取 CSV 文件时，已经使用了一些可用的参数。你对 pandas 阅读函数中不同选项越熟悉，你在数据读取（导入）过程中就能做更多的前期预处理工作。
- en: You leveraged the built-in `parse_dates` argument, which takes in a list of
    columns (either specified by name or position). The combination of `index_col=0`
    and `parse_dates=[0]` produced a DataFrame with an index of the `DatetimeIndex`
    type.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用了内建的 `parse_dates` 参数，它接收一个列名（或位置）列表。将 `index_col=0` 和 `parse_dates=[0]`
    组合在一起，生成了一个具有 `DatetimeIndex` 类型的索引的 DataFrame。
- en: 'Let''s inspect the parameters used in this recipe as defined in the official
    `pandas.read_csv()` documentation ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看官方 `pandas.read_csv()` 文档中定义的本示例中使用的参数（[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)）：
- en: '`filepath_or_buffer`: This is the first positional argument and the only required
    field needed (at a minimum) to read a CSV file. Here, you passed the Python path
    object named `filepath`. This can also be a string that represents a valid file
    path such as `''../../datasets/Ch2/movieboxoffice.csv''` or a URL that points
    to a remote file location, such as an AWS S3 bucket (we will examine this later
    in the *Reading data from URLs* recipe in this chapter).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filepath_or_buffer`：这是第一个位置参数，也是读取 CSV 文件时所需的唯一必填字段。这里，你传递了一个名为 `filepath`
    的 Python 路径对象。它也可以是一个表示有效文件路径的字符串，例如 `''../../datasets/Ch2/movieboxoffice.csv''`，或者指向远程文件位置的
    URL，例如 AWS S3 存储桶（我们将在本章的 *从 URL 读取数据* 示例中进一步探讨）。'
- en: '`sep`: This takes a string to specify which delimiter to use. The default is
    a comma delimiter (`,`) which assumes a CSV file. If the file is separated by
    another delimiter, such as a pipe (`|`) or semicolon (`;`), then the argument
    can be updated, such as `sep="|" or sep=";"`.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep`：该参数用于指定分隔符的字符串。默认分隔符是逗号（`,`），假设是一个CSV文件。如果文件使用其他分隔符，例如管道符号（`|`）或分号（`;`），可以更新该参数，例如`sep="|"`
    或 `sep=";"`。'
- en: Another alias to `sep` is `delimiter`, which can be used as well as a parameter
    name.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep`的另一个别名是`delimiter`，也可以作为参数名使用。'
- en: '`header`: In this case, you specified that the first `row` (`0`) value contains
    the header information. The default value is `infer`, which usually works as-is
    in most cases. If the CSV does not contain a header, then you specify `header=None`.
    If the CSV has a header but you prefer to supply custom column names, then you
    need to specify `header=0` and overwrite it by providing a list of new column
    names to the `names` argument.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`header`：在这种情况下，你指定了第一行（`0`）包含表头信息。默认值是`infer`，通常在大多数情况下可以直接使用。如果CSV文件没有表头，则需要指定`header=None`。如果CSV文件有表头，但你希望提供自定义的列名，则需要指定`header=0`并通过`names`参数提供新的列名列表来覆盖它。'
- en: '`parse_dates`: In the recipe, you provided a list of column positions using
    `[0]`, which specified only the first column (by position) should be parsed. The
    `parse_dates` argument can take a list of column names, such as `["Date"]`, or
    a list of column positions, such as `[0, 3]`, indicating the first and the fourth
    columns. If you only intend to parse the index column(s) specified in the `index_col`
    parameter, you only need to pass `True` (Boolean).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse_dates`：在本示例中，你提供了列位置的列表`[0]`，这表示仅解析第一列（按位置）。`parse_dates`参数可以接受列名的列表，例如`["Date"]`，或者列位置的列表，例如`[0,
    3]`，表示第一列和第四列。如果你仅打算解析`index_col`参数中指定的索引列，只需传递`True`（布尔值）。'
- en: '`index_col`: You specified that the first column by position (`index_col=0`)
    will be used as the DataFrame index. Alternatively, you could provide the column
    name as a string (`index_col=''Date''`). The parameter also takes in a list of
    integers (positional indices) or strings (column names), which would create a
    `MultiIndex` object.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index_col`：你指定了第一列的位置（`index_col=0`）作为DataFrame的索引。或者，你也可以提供列名作为字符串（`index_col=''Date''`）。该参数还可以接受一个整数列表（位置索引）或字符串列表（列名），这将创建一个`MultiIndex`对象。'
- en: '`usecols`: The default value is `None`, which includes all the columns in the
    dataset. Limiting the number of columns to only those that are required results
    in faster parsing and overall lower memory usage, since you only bring in what
    is needed. The `usecols` arguments can take a list of *column names*, such as
    `[''Date'', ''DOW'', ''Daily'', ''Percent Diff'', ''Forecast'']` or a list of
    *positional indices*, such as `[0, 1, 3, 7, 6]`, which would produce the same
    result.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`usecols`：默认值为`None`，表示包含数据集中的所有列。限制列的数量，仅保留必要的列可以加快解析速度，并减少内存使用，因为只引入了需要的数据。`usecols`参数可以接受一个*列名*的列表，例如`[''Date'',
    ''DOW'', ''Daily'', ''Percent Diff'', ''Forecast'']`，或者一个*位置索引*的列表，例如`[0, 1, 3,
    7, 6]`，两者会产生相同的结果。'
- en: Recall that you specified which columns to include by passing a list of column
    names to the **usecols** parameter. These names are based on the file header (the
    first row of the CSV file).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，你通过将列名列表传递给**usecols**参数，指定了要包含的列。这些列名是基于文件头部（CSV文件的第一行）。
- en: 'If you decide to provide custom header names, you cannot reference the original
    names in the **usecols** parameter; this will produce the following error: `ValueError:
    Usecols do not match columns.`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你决定提供自定义的列名，则无法在**usecols**参数中引用原始列名；这会导致以下错误：`ValueError: Usecols do not
    match columns.`。'
- en: There's more…
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: There are situations where `parse_dates` may not work (it just cannot parse
    the date). In such cases, the column(s) will be returned unchanged, and no error
    will be thrown. This is where the `date_format` parameter can be helpful.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有些情况下，`parse_dates`可能无法正常工作（即无法解析日期）。在这种情况下，相关列将保持原样，并且不会抛出错误。这时，`date_format`参数可以派上用场。
- en: 'The following code shows how `date_format` can be used:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用`date_format`：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The preceding code will print out the first five rows of the `ts` DataFrame,
    displaying a correctly parsed `Date` index.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印出`ts` DataFrame的前五行，正确地展示解析后的`Date`索引。
- en: '![Figure 2.2: The first five rows of the ts DataFrame using JupyterLab](img/file17.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2：使用JupyterLab展示的ts DataFrame的前五行](img/file17.jpg)'
- en: 'Figure 2.2: The first five rows of the ts DataFrame using JupyterLab'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：使用 JupyterLab 查看 ts DataFrame 的前五行
- en: 'Let''s break it down. In the preceding code, since the date is stored as a
    string in the form *26-Apr-2021*, you passed `"%d-%b-%Y"` to reflect that:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下。在上面的代码中，由于日期以 `26-Apr-2021` 这样的字符串形式存储，你传递了 `"%d-%b-%Y"` 来反映这一点：
- en: '`%d` represents the day of the month, such as `01` or `02`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%d` 表示月份中的日期，例如 `01` 或 `02`。'
- en: '`%b` represents the abbreviated month name, such as `Apr` or `May`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%b` 表示缩写的月份名称，例如 `Apr` 或 `May`。'
- en: '`%Y` represents the year as a four-digit number, such as `2020` or `2021`.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%Y` 表示四位数的年份，例如 `2020` 或 `2021`。'
- en: 'Other common string codes include the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其他常见的字符串代码包括以下内容：
- en: '`%y` represents a two-digit year, such as `19` or `20`.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%y` 表示两位数的年份，例如 `19` 或 `20`。'
- en: '`%B` represents the month''s full name, such as `January` or `February`.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%B` 表示月份的全名，例如 `January` 或 `February`。'
- en: '`%m` represents the month as a two-digit number, such as `01` or `02`.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%m` 表示月份，作为两位数，例如 `01` 或 `02`。'
- en: For more information on Python's string formats for representing dates, visit
    [https://strftime.org](https://strftime.org).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Python 字符串格式用于表示日期的更多信息，请访问 [https://strftime.org](https://strftime.org)。
- en: See also
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: When dealing with more complex date formats an alternate options is to use the
    `to_datetime()` function. The `to_datetime()` function is used to convert a string,
    integer, or float into a datetime object.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 处理更复杂的日期格式时，另一个选项是使用 `to_datetime()` 函数。`to_datetime()` 函数用于将字符串、整数或浮动数值转换为日期时间对象。
- en: 'Initially, you will read the CSV data as is, then apply the `to_datetime()`
    function to parse the specific column(s) as desired. This is demonstrated in the
    following code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，你将按原样读取 CSV 数据，然后应用 `to_datetime()` 函数将特定列解析为所需的日期时间格式。以下代码展示了这一过程：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The last line, `ts.index = pd.to_datetime(ts.index, format="%d-%b-%Y"),` converts
    the index of the `ts` DataFrame into a `DatetimeIndex` object. Notice how we specified
    the data string format similar to what we did with the `date_format` parameter
    in the `read_csv()` function in the *There’s more…* section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行，`ts.index = pd.to_datetime(ts.index, format="%d-%b-%Y"),` 将 `ts` 数据框的索引转换为
    `DatetimeIndex` 对象。请注意，我们如何指定数据字符串格式，类似于在 *还有更多…* 部分的 `read_csv()` 函数中使用 `date_format`
    参数的方式。
- en: Reading data from an Excel file
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 Excel 文件中读取数据
- en: To read data from an Excel file, you will need to use a different reader function
    from pandas. Generally, working with Excel files can be challenging since the
    file can contain formatted multi-line headers, merged header cells, and images.
    They may also contain multiple worksheets with custom names (labels). Therefore,
    it is vital that you always inspect the Excel file first. The most common scenario
    is reading from an Excel file that contains data partitioned into multiple sheets,
    which is the focus of this recipe.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 Excel 文件中读取数据，你需要使用 pandas 提供的不同读取函数。一般来说，处理 Excel 文件可能会有些挑战，因为文件可能包含格式化的多行标题、合并的标题单元格以及图片。它们还可能包含多个工作表，每个工作表都有自定义的名称（标签）。因此，在操作
    Excel 文件之前，务必先检查文件。最常见的场景是读取包含多个工作表的 Excel 文件，这也是本教程的重点。
- en: In this recipe, you will be using the `pandas.read_excel()` function and will
    examine the various parameters available to ensure the data is read properly as
    a DataFrame with a `DatetimeIndex` for time series analysis. In addition, you
    will explore different options to read Excel files with multiple sheets.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，你将使用 `pandas.read_excel()` 函数，并检查可用的各种参数，以确保数据作为具有 `DatetimeIndex` 的
    DataFrame 正确读取，用于时间序列分析。此外，你还将探索读取包含多个工作表的 Excel 文件的不同选项。
- en: Getting ready
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: To use `pandas.read_excel()`, you will need to install an additional library
    for reading and writing Excel files. In the `read_excel()` function, you will
    use the engine parameter to specify which library (engine) to use for processing
    an Excel file. Depending on the Excel file extension you are working with (for
    example, `.xls` or `.xlsx`), you may need to specify a different engine that may
    require installing an additional library.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `pandas.read_excel()`，你需要安装额外的库来读取和写入 Excel 文件。在 `read_excel()` 函数中，你将使用
    `engine` 参数指定处理 Excel 文件所需的库（引擎）。根据你所处理的 Excel 文件扩展名（例如 `.xls` 或 `.xlsx`），你可能需要指定不同的引擎，这可能需要安装额外的库。
- en: The supported libraries (engines) for reading and writing Excel include `xlrd`,
    `openpyxl`, `odf`, and `pyxlsb`. When working with Excel files, the two most common
    libraries are usually `xlrd` and `openpyxl`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 支持读取和写入 Excel 的库（引擎）包括 `xlrd`、`openpyxl`、`odf` 和 `pyxlsb`。处理 Excel 文件时，最常用的两个库通常是
    `xlrd` 和 `openpyxl`。
- en: The `xlrd` library only supports `.xls` files. So, if you are working with an
    older Excel format, such as `.xls`, then `xlrd` will do just fine. For newer Excel
    formats, such as `.xlsx`, we will need a different engine, and in this case, `openpyxl`
    would be the recommendation to go with.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`xlrd` 库只支持 `.xls` 文件。因此，如果你正在处理较旧的 Excel 格式，例如 `.xls`，那么 `xlrd` 就能很好地工作。对于更新的
    Excel 格式，例如 `.xlsx`，我们需要使用不同的引擎，在这种情况下，推荐使用 `openpyxl`。'
- en: 'To install `openpyxl` using `conda`, run the following command in the terminal:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `conda` 安装 `openpyxl`，请在终端运行以下命令：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To install using `pip`, run the following command:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `pip` 安装，请运行以下命令：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We will be using the `sales_trx_data.xlsx` file, which you can download from
    the book's GitHub repository. See the *Technical requirements* section of this
    chapter. The file contains sales data split by year into two sheets (`2017` and
    `2018`), respectively.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `sales_trx_data.xlsx` 文件，你可以从本书的 GitHub 仓库下载。请参阅本章的 *技术要求* 部分。该文件包含按年份拆分的销售数据，分别存在两个工作表中（`2017`
    和 `2018`）。
- en: How to do it…
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'You will ingest the Excel file (`.xlsx`) using pandas and `openpyxl`, and leverage
    some of the available parameters in `read_excel()`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用 pandas 和 `openpyxl` 导入 Excel 文件（`.xlsx`），并利用 `read_excel()` 中的一些可用参数：
- en: 'Import the libraries for this recipe:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入此配方所需的库：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Read the Excel (`.xlxs`) file using the `read_excel()`function. By default,
    pandas will only read from the first sheet. This is specified under the `sheet_name`
    parameter, which is set to `0` as the default value. Before passing a new argument,
    you can use `pandas.ExcelFile` first to inspect the file and determine the number
    of sheets available. The `ExcelFile` class will provide additional methods and
    properties, such as `sheet_name`, which returns a list of sheet names:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `read_excel()` 函数读取 Excel（`.xlxs`）文件。默认情况下，pandas 只读取第一个工作表。这个参数在 `sheet_name`
    中指定，默认值设置为 `0`。在传递新的参数之前，你可以先使用 `pandas.ExcelFile` 来检查文件并确定可用工作表的数量。`ExcelFile`
    类将提供额外的方法和属性，例如 `sheet_name`，它返回一个工作表名称的列表：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If you have multiple sheets, you can specify which sheets you want to ingest
    by passing a list to the `sheet_name` parameter in `read_excel`. The list can
    either be positional arguments, such as first, second, and fifth sheets with `[0,
    1, 4]`, sheet names with `["Sheet1", "Sheet2", "Sheet5"]`, or a combination of
    both, such as first sheet, second sheet, and a sheet named `"Revenue"` `[0, 1,
    "Revenue"]`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有多个工作表，可以通过将一个列表传递给 `read_excel` 中的 `sheet_name` 参数来指定要导入的工作表。该列表可以是位置参数，如第一个、第二个和第五个工作表
    `[0, 1, 4]`，工作表名称 `["Sheet1", "Sheet2", "Sheet5"]`，或两者的组合，例如第一个工作表、第二个工作表和一个名为
    `"Revenue"` 的工作表 `[0, 1, "Revenue"]`。
- en: 'In the following code, you will use sheet positions to read both the first
    and second sheets (`0` and `1` indexes). This will return a Python `dictionary`
    object with two DataFrames. Note that the returned dictionary (key-value pair)
    has numeric keys (`0` and `1`) representing the first and second sheets (positional
    index), respectively:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，你将使用工作表位置来读取第一个和第二个工作表（`0` 和 `1` 索引）。这将返回一个 Python `dictionary` 对象，包含两个
    DataFrame。请注意，返回的字典（键值对）具有数字键（`0` 和 `1`），分别表示第一个和第二个工作表（位置索引）：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Alternatively, you can pass a list of sheet names. Notice that the returned
    dictionary keys are now strings and represent the sheet names as shown in the
    following code:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，你可以传递一个工作表名称的列表。请注意，返回的字典键现在是字符串，表示工作表名称，如以下代码所示：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you want to read from all the available sheets, you will pass `None` instead.
    The keys for the dictionary, in this case, will represent sheet names:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你想从所有可用工作表中读取数据，可以传递 `None`。在这种情况下，字典的键将表示工作表名称：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The two DataFrames within the dictionary are identical (homogeneous-typed) in
    terms of their schema (column names and data types). You can inspect each DataFrame
    with `ts['2017'].info()` and `ts['2018'].info()`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 字典中的两个 DataFrame 在它们的架构（列名和数据类型）上是相同的（同类型）。你可以通过 `ts['2017'].info()` 和 `ts['2018'].info()`
    来检查每个 DataFrame。
- en: They both have a `DatetimeIndex` object, which you specified in the `index_col`
    parameter. The 2017 DataFrame consists of 36,764 rows and the 2018 DataFrame consists
    of 37,360\. In this scenario, you want to stack (combine) the two (think `UNION`
    in SQL) into a single DataFrame that contains all 74,124 rows and a `DatetimeIndex`
    that spans from `2017-01-01` to `2018-12-31`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 它们都有一个 `DatetimeIndex` 对象，你在 `index_col` 参数中指定了该对象。2017 年的 DataFrame 包含 36,764
    行，2018 年的 DataFrame 包含 37,360 行。在这种情况下，你希望将两个 DataFrame 堆叠（合并）（类似于 SQL 中的 `UNION`），得到一个包含所有
    74,124 行且 `DatetimeIndex` 从 `2017-01-01` 到 `2018-12-31` 的单一 DataFrame。
- en: 'To combine the two DataFrames along the index axis (stacked one on top of the
    other), you will use the `pandas.concat()` function. The default behavior of the
    `concat()` function is to concatenate along the index axis (`axis=0`). In the
    following code, you will explicitly specify which DataFrames to concatenate:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要沿着索引轴（一个接一个堆叠）将两个 DataFrame 合并，你将使用 `pandas.concat()` 函数。`concat()` 函数的默认行为是沿着索引轴连接（`axis=0`）。在以下代码中，你将明确指定要连接哪些
    DataFrame：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'When you have multiple DataFrames returned (think multiple sheets), you can
    use the `concat()` function on the returned dictionary. In other words, you can
    combine the `concat()` and `read_excel()` functions in one statement. In this
    case, you will end up with a `MultiIndex` DataFrame where the first level is the
    sheet name (or number) and the second level is the `DatetimeIndex`. For example,
    using the `ts` dictionary, you will get a two-level index: `MultiIndex([(''2017'',
    ''2017-01-01''), ..., (''2018'', ''2018-12-31'')], names=[None, ''Date''], length=74124)`.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当返回多个 DataFrame 时（比如多个工作表），你可以对返回的字典使用 `concat()` 函数。换句话说，你可以在一个语句中将 `concat()`
    和 `read_excel()` 函数结合使用。在这种情况下，最终你会得到一个 `MultiIndex` DataFrame，其中第一级是工作表名称（或编号），第二级是
    `DatetimeIndex`。例如，使用 `ts` 字典，你会得到一个两级索引：`MultiIndex([('2017', '2017-01-01'),
    ..., ('2018', '2018-12-31')], names=[None, 'Date'], length=74124)`。
- en: 'To reduce the number of levels, you can use the `droplevel(level=0)` method
    to drop the first level after pandas `.concat()` shown as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要减少级别数，你可以使用 `droplevel(level=0)` 方法，在 pandas `.concat()` 之后删除第一级，示例如下：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you are only reading one sheet, the behavior is slightly different. By default,
    `sheet_name` is set to `0`, which means it reads the first sheet. You can modify
    this and pass a different value (single value), either the sheet name (string)
    or sheet position (integer). When passing a single value, the returned object
    will be a pandas DataFrame and not a dictionary:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你只读取一个工作表，行为会略有不同。默认情况下，`sheet_name` 被设置为 `0`，这意味着它读取第一个工作表。你可以修改这个设置并传递一个不同的值（单一值），无论是工作表名称（字符串）还是工作表位置（整数）。当传递单一值时，返回的对象将是一个
    pandas DataFrame，而不是字典：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Do note though that if you pass a single value inside two brackets (`[1]`),
    then pandas will interpret this differently and the returned object will be a
    dictionary that contains one DataFrame.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意，如果你在两个括号内传递一个单一值（`[1]`），那么 pandas 会以不同的方式解释它，返回的对象将是一个包含一个 DataFrame 的字典。
- en: Lastly, note that you did not need to specify the engine in the last example.
    The `read_csv` function will determine which engine to use based on the file extension.
    So, for example, suppose the library for that engine is not installed. In that
    case, it will throw an `ImportError` message, indicating that the library (dependency)
    is missing.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，在最后一个示例中你不需要指定引擎。`read_csv` 函数将根据文件扩展名确定使用哪个引擎。所以，假设该引擎的库没有安装，在这种情况下，它会抛出一个
    `ImportError` 消息，指出缺少该库（依赖项）。
- en: How it works…
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作原理……
- en: The `pandas.read_excel()` function has many common parameters with the `pandas.read_csv()`
    function that you used earlier. The `read_excel` function can either return a
    DataFrame object or a dictionary of DataFrames. The dependency here is whether
    you are passing a single value (scalar) or a list to `sheet_name`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas.read_excel()` 函数与之前使用过的 `pandas.read_csv()` 函数有许多相同的常见参数。`read_excel`
    函数可以返回一个 DataFrame 对象或一个包含 DataFrame 的字典。这里的依赖关系在于你是传递一个单一值（标量）还是一个列表给 `sheet_name`。'
- en: In the `sales_trx_data.xlsx` file, both sheets had the same schema (homogeneous-
    typed). The sales data was partitioned (split) by year, where each sheet contained
    sales for a particular year. In this case, concatenating the two DataFrames was
    a natural choice. The `pandas.concat()` function is like the `DataFrame.append()`
    function, in which the second DataFrame was added (appended) to the end of the
    first DataFrame. This should be similar in behavior to the `UNION` clause for
    those coming from a SQL background.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在` sales_trx_data.xlsx`文件中，两个工作表具有相同的架构（同质类型）。销售数据按年份进行分区（拆分），每个工作表包含特定年份的销售数据。在这种情况下，连接这两个DataFrame是一个自然的选择。`pandas.concat()`函数类似于`DataFrame.append()`函数，其中第二个DataFrame被添加（附加）到第一个DataFrame的末尾。对于来自SQL背景的用户来说，这应该类似于`UNION`子句的行为。
- en: There's more…
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多…
- en: An alternative method to reading an Excel file is with the `pandas.ExcelFile()`
    class, which returns a pandas `ExcelFile` object. Earlier in this recipe, you
    used `ExcelFile()` to inspect the number of sheets in the Excel file through the
    `sheet_name` property.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种读取Excel文件的方法是使用`pandas.ExcelFile()`类，它返回一个pandas `ExcelFile`对象。在本食谱的早些时候，您使用`ExcelFile()`通过`sheet_name`属性检查Excel文件中的工作表数量。
- en: The `ExcelFile` class has several useful methods, including the `parse()` method
    to parse the Excel file into a DataFrame, similar to the `pandas.read_excel()`
    function.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExcelFile`类具有多个有用的方法，包括`parse()`方法，用于将Excel文件解析为DataFrame，类似于`pandas.read_excel()`函数。'
- en: 'In the following example, you will use the `ExcelFile` class to parse the first
    sheet, assign the first column as an index, and print the first five rows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，您将使用`ExcelFile`类解析第一个工作表，将第一列作为索引，并打印前五行：
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should see similar results for the first five rows of the DataFrame:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到类似的结果，显示数据框（DataFrame）的前五行：
- en: '![Figure 2.3: The first five rows of the DataFrame using JupyterLab](img/file18.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3：使用JupyterLab显示数据框的前五行](img/file18.jpg)'
- en: 'Figure 2.3: The first five rows of the DataFrame using JupyterLab'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：使用JupyterLab显示数据框的前五行
- en: From *Figure 2.3*, it should become clear that `ExcelFile.parse()` is *equivalent*
    to `pandas.read_excel()`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图2.3*中，应该能清楚地看出，`ExcelFile.parse()`*相当于*`pandas.read_excel()`。
- en: See also
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'For more information on `pandas.read_excel()` and `pandas.ExcelFile()`, please
    refer to the official documentation:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`pandas.read_excel()`和`pandas.ExcelFile()`的更多信息，请参考官方文档：
- en: '`pandas.read_excel`: [https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas.read_excel`: [https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html)'
- en: '`pandas.ExcelFile.parse`: [https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.parse.html](https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.parse.html)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas.ExcelFile.parse`: [https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.parse.html](https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.parse.html)'
- en: Reading data from URLs
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从URL读取数据
- en: Files can be downloaded and stored locally on your machine, or stored on a remote
    server or cloud location. In the earlier two recipes, *Reading from CSVs and other
    delimited files*, and *Reading data from an Excel file*, both files were stored
    locally.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 文件可以下载并存储在本地计算机上，或存储在远程服务器或云端位置。在前两个示例中，*从CSV和其他分隔文件读取*和*从Excel文件读取数据*，两个文件都存储在本地。
- en: Many of the pandas reader functions can read data from remote locations by passing
    a URL path. For example, `read_csv()` and `read_excel()` can take a URL to read
    a file accessible via the internet. In this recipe, you will read a CSV file using
    `pandas.read_csv()` and Excel files using `pandas.read_excel()` from remote locations,
    such as GitHub and AWS S3 (private and public buckets). You will also read data
    directly from an HTML page into a pandas DataFrame.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: pandas的许多读取函数可以通过传递URL路径从远程位置读取数据。例如，`read_csv()`和`read_excel()`可以接受一个URL来读取通过互联网访问的文件。在本例中，您将使用`pandas.read_csv()`读取CSV文件，使用`pandas.read_excel()`读取Excel文件，数据源来自远程位置，如GitHub和AWS
    S3（私有和公共桶）。您还将直接从HTML页面读取数据并导入到pandas DataFrame中。
- en: Getting ready
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to install the **AWS SDK for Python** (**Boto3**) for reading
    files from S3 buckets. Additionally, you will learn how to use the `storage_options`
    parameter available in many of the reader functions in pandas to read from S3
    without the Boto3 library.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装**AWS SDK for Python**（**Boto3**），以便从S3桶读取文件。此外，您还将学习如何使用`storage_options`参数，它在pandas中的许多读取函数中可用，用于在没有Boto3库的情况下从S3读取数据。
- en: To use an S3 URL (for example, `s3://bucket_name/path-to-file`) in pandas, you
    will need to install the `s3fs` library. You will also need to install an HTML
    parser for when we use `read_html()`. For example, for the parsing engine (the
    HTML parser), you can install either `lxml` or `html5lib`; pandas will pick whichever
    is installed (it will first look for `lxml`, and if that fails, then for `html5lib`).
    If you plan to use `html5lib` you will need to install Beautiful Soup (`beautifulsoup4`).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 pandas 中使用 S3 URL（例如，`s3://bucket_name/path-to-file`），您需要安装 `s3fs` 库。您还需要安装一个
    HTML 解析器，当我们使用 `read_html()` 时。比如，解析引擎（HTML 解析器）可以选择安装 `lxml` 或 `html5lib`；pandas
    会选择安装的解析器（它会首先查找 `lxml`，如果失败，则查找 `html5lib`）。如果您计划使用 `html5lib`，则需要安装 Beautiful
    Soup（`beautifulsoup4`）。
- en: 'To install using pip, you can use the following command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pip 安装，您可以使用以下命令：
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To install using Conda, you can use:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Conda 安装，您可以使用：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How to do it…
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'This recipe will present you with different scenarios when reading data from
    online (remote) sources. Let''s import pandas upfront since you will be using
    it throughout this recipe:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将向您展示从在线（远程）源读取数据时的不同场景。让我们先导入 pandas，因为在整个本节中都会使用它：
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Reading data from GitHub
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从 GitHub 读取数据
- en: 'Sometimes, you may find useful public data on GitHub that you want to use and
    read directly (without downloading). One of the most common file formats on GitHub
    are CSV files. Let''s start with the following steps:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您可能会在 GitHub 上找到有用的公共数据，希望直接使用并读取（而不是下载）。GitHub 上最常见的文件格式之一是 CSV 文件。让我们从以下步骤开始：
- en: 'To read a CSV file from GitHub, you will need the URL to the raw content. If
    you copy the file''s GitHub URL from the browser and use it as the file path,
    you will get a URL that looks like this: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/datasets/Ch2/AirQualityUCI.csv](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/datasets/Ch2/AirQualityUCI.csv).
    This URL is a pointer to the web page in GitHub and not the data itself; hence
    when using `pd.read_csv()`, it will throw an error:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要从 GitHub 读取 CSV 文件，您需要获取原始内容的 URL。如果您从浏览器复制文件的 GitHub URL 并将其作为文件路径使用，您将得到一个如下所示的
    URL：[https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/datasets/Ch2/AirQualityUCI.csv](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/datasets/Ch2/AirQualityUCI.csv)。此
    URL 是指向 GitHub 网页，而不是数据本身；因此，当使用 `pd.read_csv()` 时，它会抛出错误：
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Instead, you will need the raw content, which will give you a URL that looks
    like this: [https://raw.githubusercontent.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./main/datasets/Ch2/AirQualityUCI.csv](https://raw.githubusercontent.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./main/datasets/Ch2/AirQualityUCI.csv):'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相反，您需要原始内容，这会给您一个如下所示的 URL：[https://raw.githubusercontent.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./main/datasets/Ch2/AirQualityUCI.csv](https://raw.githubusercontent.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./main/datasets/Ch2/AirQualityUCI.csv)：
- en: '![Figure 2.4: The GitHub page for the CSV file. Note the View raw button](img/file19.jpg)'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.4：CSV 文件的 GitHub 页面。注意查看原始按钮](img/file19.jpg)'
- en: 'Figure 2.4: The GitHub page for the CSV file. Note the View raw button'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.4：CSV 文件的 GitHub 页面。注意查看原始按钮
- en: In *Figure 2.4*, notice that the values are not comma-separated (not a comma-delimited
    file); instead, the file uses semicolon (`;`) to separate the values.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *图 2.4* 中，请注意值没有用逗号分隔（不是逗号分隔文件）；相反，文件使用分号（`;`）来分隔值。
- en: The first column in the file is the `Date` column. You will need to parse (using
    the `parse_date` parameter) and convert it to `DatetimeIndex` (`index_col` parameter).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中的第一列是 `Date` 列。您需要解析（使用 `parse_date` 参数）并将其转换为 `DatetimeIndex`（`index_col`
    参数）。
- en: 'Pass the new URL to `pandas.read_csv()`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 将新的 URL 传递给 `pandas.read_csv()`：
- en: '[PRE25]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We successfully ingested the data from the CSV file in GitHub into a DataFrame
    and printed the first three rows of select columns.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地将 GitHub 上的 CSV 文件数据导入到 DataFrame 中，并打印了选定列的前三行数据。
- en: Reading data from a public S3 bucket
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从公共 S3 存储桶读取数据
- en: 'AWS supports **virtual-hosted-style** URLs such as `https://bucket-name.s3.Region.amazonaws.com/keyname`,
    **path-style** URLs such as `https://s3.Region.amazonaws.com/bucket-name/keyname`,
    and using `S3://bucket/keyname`. Here are examples of how these different URLs
    may look for our file:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 支持 **虚拟主机风格** 的 URL，如 `https://bucket-name.s3.Region.amazonaws.com/keyname`，**路径风格**
    的 URL，如 `https://s3.Region.amazonaws.com/bucket-name/keyname`，以及使用 `S3://bucket/keyname`。以下是这些不同
    URL 在我们文件中的示例：
- en: 'A virtual hosted-style URL or an object URL: [https://tscookbook.s3.us-east-1.amazonaws.com/AirQualityUCI.xlsx](https://tscookbook.s3.us-east-1.amazonaws.com/AirQualityUCI.xlsx)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个虚拟托管样式的 URL 或对象 URL：[https://tscookbook.s3.us-east-1.amazonaws.com/AirQualityUCI.xlsx](https://tscookbook.s3.us-east-1.amazonaws.com/AirQualityUCI.xlsx)
- en: 'A path-style URL: [https://s3.us-east-1.amazonaws.com/tscookbook/AirQualityUCI.xlsx](https://s3.us-east-1.amazonaws.com/tscookbook/AirQualityUCI.xlsx)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个路径样式的 URL：[https://s3.us-east-1.amazonaws.com/tscookbook/AirQualityUCI.xlsx](https://s3.us-east-1.amazonaws.com/tscookbook/AirQualityUCI.xlsx)
- en: 'An S3 protocol: `s3://tscookbook/AirQualityUCI.csv`'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 S3 协议：`s3://tscookbook/AirQualityUCI.csv`
- en: In this example, you will be reading the `AirQualityUCI.xlsx` file, which has
    only one sheet. It contains the same data as `AirQualityUCI.csv`, which we read
    earlier from GitHub.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，您将读取 `AirQualityUCI.xlsx` 文件，该文件只有一个工作表，包含与先前从 GitHub 读取的 `AirQualityUCI.csv`
    相同的数据。
- en: 'Note that in the URL, you do not need to specify the region as `us-east-1`.
    The `us-east-1` region, which represents US East (North Virginia), is an **exception.**
    This is not the case for other regions:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 URL 中，您不需要指定 `us-east-1` 区域。`us-east-1` 区域代表美国东部（北弗吉尼亚），是一个 **例外**。其他区域则不是这种情况：
- en: '[PRE27]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Read the same file using the `S3://` URL:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `S3://` URL 读取相同的文件：
- en: '[PRE28]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You may get an error such as the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会遇到如下错误：
- en: '[PRE29]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This indicates that either you do not have the `s3fs` library installed or possibly
    you are not using the right Python/Conda environment.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明您要么没有安装 `s3fs` 库，要么可能没有使用正确的 Python/Conda 环境。
- en: Reading data from a private S3 bucket
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从私有 S3 存储桶读取数据
- en: When reading files from a private S3 bucket, you will need to pass your credentials
    to authenticate. A convenient parameter in many of the I/O functions in pandas
    is `storage_options`, which allows you to send additional content with the request,
    such as a custom header or required credentials to a cloud service.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 从私有 S3 存储桶读取文件时，您需要传递凭证以进行身份验证。pandas 中许多 I/O 函数中的一个便捷参数是 `storage_options`，它允许您在请求中发送额外的内容，例如自定义头部或所需的云服务凭证。
- en: 'You will need to pass a dictionary (key-value pair) to provide the additional
    information along with the request, such as username, password, access keys, and
    secret keys to `storage_options` as in `{"username": username, "password": password}`.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '您需要传递一个字典（键值对），以便与请求一起提供额外的信息，例如用户名、密码、访问密钥和密钥访问密钥，传递给 `storage_options`，如
    `{"username": username, "password": password}`。'
- en: 'Now, you will read the `AirQualityUCI.csv` file, located in a private S3 bucket:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将读取位于私有 S3 存储桶中的 `AirQualityUCI.csv` 文件：
- en: 'You will start by storing your AWS credentials in a config `.cfg` file outside
    your Python script. Then, use `configparser` to read the values and store them
    in Python variables. You do not want your credentials exposed or hardcoded in
    your code:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将从将您的 AWS 凭证存储在 Python 脚本之外的 `.cfg` 配置文件开始。然后，使用 `configparser` 读取这些值，并将其存储在
    Python 变量中。您不希望凭证暴露或硬编码在代码中：
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You can load the `aws.cfg` file using `config.read()`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `config.read()` 加载 `aws.cfg` 文件：
- en: '[PRE31]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The *AWS Access Key ID* and *Secret Access Key* are now stored in A`WS_ACCESS_KEY`
    and `AWS_SECRET_KEY.` Use `pandas.read_csv()` to read the CSV file and update
    the `storage_options` parameter by passing your credentials, as shown in the following
    code:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*AWS 访问密钥 ID* 和 *密钥访问密钥* 现在存储在 `AWS_ACCESS_KEY` 和 `AWS_SECRET_KEY` 中。使用 `pandas.read_csv()`
    读取 CSV 文件，并通过传递您的凭证来更新 `storage_options` 参数，如下代码所示：'
- en: '[PRE32]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Alternatively, you can use the AWS SDK for Python (Boto3) to achieve similar
    results. The `boto3` Python library gives you more control and additional capabilities
    (beyond just reading and writing to S3). You will pass the same credentials stored
    earlier in `AWS_ACCESS_KEY` and `AWS_SECRET_KEY` and pass them to AWS, using `boto3`
    to authenticate:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，您可以使用 AWS 的 Python SDK（Boto3）来实现类似的功能。`boto3` Python 库为您提供了更多的控制和额外的功能（不仅仅是读取和写入
    S3）。您将传递之前存储在 `AWS_ACCESS_KEY` 和 `AWS_SECRET_KEY` 中的凭证，并通过 `boto3` 进行身份验证：
- en: '[PRE33]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, the `client` object has access to many methods specific to the AWS S3
    service for creating, deleting, and retrieving bucket information, and more. In
    addition, Boto3 offers two levels of APIs: client and resource. In the preceding
    example, you used the client API.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`client` 对象可以访问许多特定于 AWS S3 服务的方法，用于创建、删除和检索存储桶信息等。此外，Boto3 提供了两种级别的 API：客户端和资源。在前面的示例中，您使用了客户端
    API。
- en: The client is a low-level service access interface that gives you more granular
    control, for example, `boto3.client("s3")`. The resource is a higher-level object-oriented
    interface (an abstraction layer), for example, `boto3.resource("s3")`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端是一个低级服务访问接口，提供更精细的控制，例如，`boto3.client("s3")`。资源是一个更高级的面向对象接口（抽象层），例如，`boto3.resource("s3")`。
- en: In *Chapter 4*, *Persisting Time Series Data to Files*, you will explore the
    **resource** API interface when writing to S3\. For now, you will use the client
    interface.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 4 章*，*将时间序列数据持久化到文件* 中，您将探索写入 S3 时的 **resource** API 接口。目前，您将使用客户端接口。
- en: 'You will use the `get_object` method to retrieve the data. Just provide the
    bucket name and a key. The key here is the actual filename:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将使用 `get_object` 方法来检索数据。只需提供存储桶名称和密钥。这里的密钥是实际的文件名：
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'When calling the `client.get_object()` method, a dictionary (key-value pair)
    is returned, as shown in the following example:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `client.get_object()` 方法时，将返回一个字典（键值对），如以下示例所示：
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The content you are interested in is in the response body under the `Body` key.
    You passed `data['Body']` to the `read_csv()` function, which loads the response
    stream (`StreamingBody`) into a DataFrame.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你感兴趣的内容在响应体中的 `Body` 键下。你将 `data['Body']` 传递给 `read_csv()` 函数，它会将响应流（`StreamingBody`）加载到
    DataFrame 中。
- en: Reading data from HTML
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从 HTML 中读取数据
- en: 'pandas offers an elegant way to read HTML tables and convert the content into
    a pandas DataFrame using the `pandas.read_html()` function:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 提供了一种优雅的方式来读取 HTML 表格并使用 `pandas.read_html()` 函数将内容转换为 pandas DataFrame：
- en: 'In the following recipe, we will extract HTML tables from Wikipedia for COVID-19
    pandemic tracking cases by country and by territory ([https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory](https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory)):'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将从 Wikipedia 提取 HTML 表格，用于按国家和地区跟踪 COVID-19 大流行病例（[https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory](https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory)）：
- en: '[PRE36]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`pandas.read_html()` returned a list of DataFrames, one for each HTML table
    found in the URL. Keep in mind that the website''s content is dynamic and gets
    updated regularly, and the results may vary. In our case, it returned 69 DataFrames.
    The DataFrame at index `15` contains summary on COVID-19 cases and deaths by region.
    Grab the DataFrame (at index `15`) and assign it to the `df` variable, and print
    the returned columns:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pandas.read_html()` 返回一个包含 DataFrame 的列表，每个 HTML 表格对应一个 DataFrame，位于 URL 中找到的每个
    HTML 表格。请注意，网站内容是动态的，且会定期更新，因此结果可能会有所不同。在我们的例子中，返回了 69 个 DataFrame。索引为 `15` 的
    DataFrame 包含按地区划分的 COVID-19 病例和死亡情况的汇总。获取该 DataFrame（位于索引 `15`）并将其分配给 `df` 变量，接着打印返回的列：'
- en: '[PRE37]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Display the first five rows for `Total cases`, `Total deaths`, and the `Cases
    per million` columns:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示 `Total cases`、`Total deaths` 和 `Cases per million` 列的前五行：
- en: '[PRE38]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: How it works…
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Most of the pandas reader functions accept a URL as a path. Examples include
    the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 pandas 读取器函数都接受 URL 作为路径。以下是一些示例：
- en: '`pandas.read_csv()`'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas.read_csv()`'
- en: '`pandas.read_excel()`'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas.read_excel()`'
- en: '`pandas.read_parquet()`'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas.read_parquet()`'
- en: '`pandas.read_table()`'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas.read_table()`'
- en: '`pandas.read_pickle()`'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas.read_pickle()`'
- en: '`pandas.read_orc()`'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas.read_orc()`'
- en: '`pandas.read_stata()`'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas.read_stata()`'
- en: '`pandas.read_sas()`'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas.read_sas()`'
- en: '`pandas.read_json()`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas.read_json()`'
- en: The URL needs to be one of the valid URL schemes that pandas supports, which
    includes `http` and `https`, `ftp`, `s3`, `gs`, or the `file` protocol.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: URL 需要是 pandas 支持的有效 URL 方案之一，包括 `http` 和 `https`、`ftp`、`s3`、`gs`，或者 `file`
    协议。
- en: The `read_html()` function is great for scraping websites that contain data
    in HTML tables. It inspects the HTML and searches for all the `<table>` elements
    within the HTML. In HTML, table rows are defined with the `<tr> </tr>` tags and
    headers with the `<th></th>` tags. The actual data (cell) is contained within
    the `<td> </td>` tags. The `read_html()` function looks for `<table>`, `<tr>`,
    `<th>`, and `<td>` tags and, converts the content into a DataFrame, and assigns
    the columns and rows as they were defined in the HTML. If an HTML page contains
    more than one `<table></table>` tag, `read_html` will return them all and you
    will get a list of DataFrames.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_html()` 函数非常适合抓取包含 HTML 表格数据的网站。它检查 HTML 并搜索其中的所有 `<table>` 元素。在 HTML
    中，表格行使用 `<tr> </tr>` 标签定义，表头使用 `<th></th>` 标签定义。实际数据（单元格）包含在 `<td> </td>` 标签中。`read_html()`
    函数查找 `<table>`、`<tr>`、`<th>` 和 `<td>` 标签，并将内容转换为 DataFrame，并根据 HTML 中的定义分配列和行。如果一个
    HTML 页面包含多个 `<table></table>` 标签，`read_html` 会返回所有的表格，并且你将得到一个 DataFrame 列表。'
- en: 'The following code demonstrates how `pandas.read_html()`works:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了`pandas.read_html()`的工作原理：
- en: '[PRE39]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Passing HTML literal strings**'
  id: totrans-217
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**传递HTML字面字符串**'
- en: ''
  id: totrans-218
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As of pandas version 2.1.0\. you will need to wrap HTML code in io.StringIO.
    The `StringIO(<HTML CODE>)` creates an in-memory file-like object from the HTML
    string that can be passed directly to `read_html()` function.
  id: totrans-219
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从pandas版本2.1.0开始，你需要将HTML代码包装在`io.StringIO`中。`StringIO(<HTML CODE>)`将从HTML字符串创建一个内存中的类文件对象，可以直接传递给`read_html()`函数。
- en: 'In the preceding code, the `read_html()` function reads the HTML content from
    the file-like object and converts an HTML table, represented between “`<table>
    … </table>`“ tags, into a pandas DataFrame. The headers between the `<th>` and
    `</th>` tags represent the column names of the DataFrame, and the content between
    the `<tr><td>` and `</td></tr>` tags represent the row data of the DataFrame.
    Note that if you go ahead and delete the `<table>` and `</table>` table tags,
    you will get the `ValueError: No tables found` error.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '在前面的代码中，`read_html()`函数从类文件对象读取HTML内容，并将用“`<table> … </table>`”标签表示的HTML表格转换为pandas数据框。`<th>`和`</th>`标签之间的内容表示数据框的列名，`<tr><td>`和`</td></tr>`标签之间的内容表示数据框的行数据。请注意，如果你删除`<table>`和`</table>`标签，你将遇到`ValueError:
    No tables found`错误。'
- en: There's more…
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多…
- en: The `read_html()` function has an optional `attr` argument, which takes a dictionary
    of valid HTML `<table>` attributes, such as `id` or `class`. For example, you
    can use the `attr` parameter to narrow down the tables returned to those that
    match the `class` attribute `sortable` as in `<table class="sortable">`. The `read_html`
    function will inspect the entire HTML page to ensure you target the right set
    of attributes.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_html()`函数有一个可选的`attr`参数，它接受一个包含有效HTML`<table>`属性的字典，例如`id`或`class`。例如，你可以使用`attr`参数来缩小返回的表格范围，仅限那些匹配`class`属性`sortable`的表格，如`<table
    class="sortable">`。`read_html`函数将检查整个HTML页面，确保你定位到正确的属性集。'
- en: In the previous exercise, you used the `read_html` function on the COVID-19
    Wikipedia page, and it returned 71 tables (DataFrames). The number of tables will
    probably increase as time goes by as Wikipedia gets updated. You can narrow down
    the result set and guarantee some consistency by using the `attr` option. First,
    start by inspecting the HTML code using your browser. You will see that several
    of the `<table>` elements have multiple classes listed, such as `sortable`. You
    can look for other unique identifiers.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个练习中，你使用了`read_html`函数在COVID-19维基百科页面上，它返回了71个表格（数据框）。随着维基百科的更新，表格数量可能会随着时间的推移增加。你可以通过使用`attr`选项来缩小结果集并保证一定的一致性。首先，使用浏览器检查HTML代码。你会看到一些`<table>`元素有多个类，如`sortable`。你可以寻找其他独特的标识符。
- en: '[PRE40]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note, if you get the error `html5lib not found`, please install it you will
    need to install both `html5lib` and `beautifulSoup4`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你收到`html5lib not found`的错误，请安装它，你需要同时安装`html5lib`和`beautifulSoup4`。
- en: 'To install using `conda`, use the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`conda`安装，使用以下命令：
- en: '[PRE41]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To install using `pip`, use the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pip`安装，使用以下命令：
- en: '[PRE42]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, let''s use the `sortable` class and request the data again:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`sortable`类并重新请求数据：
- en: '[PRE43]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The list returned a smaller subset of tables (from `71` down to `7`).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的列表是一个较小的表格子集（从`71`减少到`7`）。
- en: See also
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: 'For more information, please refer to the official `pandas.read_html` documentation:
    [https://pandas.pydata.org/docs/reference/api/pandas.read_html.html](https://pandas.pydata.org/docs/reference/api/pandas.read_html.html).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请参考官方的`pandas.read_html`文档：[https://pandas.pydata.org/docs/reference/api/pandas.read_html.html](https://pandas.pydata.org/docs/reference/api/pandas.read_html.html)。
- en: Reading data from Parquet files
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Parquet文件读取数据
- en: '**Parquet** files have emerged as a popular choice for storing and processing
    large datasets efficiently in the world of data engineering and big data analytics.
    Initially developed by Twitter and Cloudera, Parquet was later contributed to
    the **Apache Foundation** as an open-source columnar file format. The focus of
    Parquet is to prioritize fast data retrieval and efficient compression. Its design
    specifically caters to analytical workloads and serves as an excellent option
    for partitioning data, which you will explore in this recipe and again in Chapter
    4, *Persisting Time Series Data to Files*. As a result, Parquet has become the
    de facto standard for modern data architectures and cloud storage solutions.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**Parquet**文件已成为在数据工程和大数据分析领域中高效存储和处理大数据集的热门选择。最初由Twitter和Cloudera开发，Parquet后来作为开源列式文件格式贡献给了**Apache基金会**。Parquet的重点是优先考虑快速的数据检索和高效的压缩。它的设计专门针对分析型工作负载，并且作为数据分区的优秀选择，你将在本食谱中以及第四章*将时间序列数据持久化到文件*中再次探索它。因此，Parquet已成为现代数据架构和云存储解决方案的事实标准。'
- en: In this recipe you learn how to read parquet files using pandas and learn how
    to query a specific partition for efficient data retrieval.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，你将学习如何使用pandas读取parquet文件，并学习如何查询特定分区以实现高效的数据检索。
- en: Getting ready
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will be reading parquet files that contain weather data from National Oceanic
    And Atmospheric Administration from Los Angeles Airport stations which you can
    find in `datasets/Ch2/ LA_weather.parquet` folder. This contains weather readings
    from 2010-2023 and partitioned by year (14 subfolders).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你将读取包含来自美国国家海洋和大气管理局的洛杉矶机场站点天气数据的parquet文件，这些文件可以在`datasets/Ch2/ LA_weather.parquet`文件夹中找到。它包含2010-2023年的天气读数，并按年份进行分区（14个子文件夹）。
- en: You will use the `pandas.read_parquet()` function which requires you to install
    a Parquet engine to process the files. You can install either **fastparquet**
    or **PyArrow**, with the latter being the default choice for pandas.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用`pandas.read_parquet()`函数，这需要你安装一个Parquet引擎来处理文件。你可以安装**fastparquet**或**PyArrow**，后者是pandas的默认选择。
- en: 'To install PyArrow using **conda** run the following command:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**conda**安装PyArrow，运行以下命令：
- en: '[PRE44]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'To install PyArrow using **pip** run the following command:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**pip**安装PyArrow，运行以下命令：
- en: '[PRE45]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: How to do it…
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作…
- en: The **PyArrow** library allows you to pass additional arguments `(**kwargs`)
    to the `pandas.read_parquet()` function, thereby providing more options when reading
    files, as you will explore.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyArrow**库允许你将额外的参数`(**kwargs)`传递给`pandas.read_parquet()`函数，从而在读取文件时提供更多的选项，正如你将要探索的那样。'
- en: Reading all partitions
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 读取所有分区
- en: 'The following steps are for reading all the partitions in the `LA_weather.parquet`
    folder in one go:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤用于一次性读取`LA_weather.parquet`文件夹中的所有分区：
- en: Create a path to reference the Parquet folder, which contains the partitions,
    and pass it to the `read_parquet` function.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个路径来引用包含分区的Parquet文件夹，并将其传递给`read_parquet`函数。
- en: '[PRE46]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: You can validate and check the schema using the `.info()` method
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用`.info()`方法验证和检查模式
- en: '[PRE47]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This should produce the following output:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生以下输出：
- en: '[PRE48]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Reading specific partitions
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 读取特定分区
- en: 'The following steps explain how to read a specific partition or set of partitions
    using the `filters` argument and specify columns using the `columns` argument
    from the PyArrow library:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤说明如何使用`filters`参数读取特定分区或分区集，并使用PyArrow库中的`columns`参数指定列：
- en: 'As the data is partitioned by year, you can utilize the `filters` argument
    to specify a particular partition. In the following, you will only read the partition
    for the year 2012:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于数据按年份进行分区，你可以使用`filters`参数来指定特定的分区。在下面的示例中，你将只读取2012年的分区：
- en: '[PRE49]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'To read a set of partitions, such as for the years 2021, 2022, and 2023, you
    can utilize any of the following options, which will produce similar results:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要读取一组分区，例如2021年、2022年和2023年的数据，你可以使用以下任意选项，这些选项会产生类似的结果：
- en: '[PRE50]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'After defining the `filters` object, you can assign it to the `filters` argument
    within the `read_parquet()` function as demonstrated below:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义`filters`对象后，你可以将其分配给`read_parquet()`函数中的`filters`参数，如下所示：
- en: '[PRE51]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Another useful argument is `columns` which allows you to specify the column
    names you want to retrieve as a Python list.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个有用的参数是`columns`，它允许你指定要检索的列名，作为Python列表传递。
- en: '[PRE52]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: In the preceding code, the `read_parquet()` function will only retrieve the
    specified columns (‘*Date’*, ‘*year’*, and ‘*TMAX’*) from the Parquet file, using
    the defined filters. You can validate the results by running `df.head()` and `df.info()`
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，`read_parquet()` 函数将仅检索 Parquet 文件中的指定列（‘*Date*’、‘*year*’ 和 ‘*TMAX*’），并使用定义的过滤器。你可以通过运行
    `df.head()` 和 `df.info()` 来验证结果。
- en: '[PRE53]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Notice how the memory usage has significantly reduced when narrowing your selection
    to only the necessary data by specifying the partitions and columns for your data
    analysis.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，通过指定数据分析所需的分区和列，缩小数据选择范围后，内存使用显著减少。
- en: How it works…
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何运作…
- en: There are several advantages to working with the Parquet file format, especially
    when dealing with large data files. The columnar-oriented format of Parquet offers
    faster data retrieval and efficient compression, making it ideal for cloud storage
    and reducing storage costs. Parquet employs advanced techniques and algorithms
    for data encoding, leading to improved compression ratios.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Parquet 文件格式有多个优势，尤其是在处理大型数据文件时。Parquet 的列式格式提供了更快的数据检索和高效的压缩，使其非常适合云存储，并有助于减少存储成本。Parquet
    采用了先进的数据编码技术和算法，从而提高了压缩率。
- en: Figure 2.5 below shows an example of a folder structure for a dataset stored
    as a Parquet file partitioned by year. Each year has its own subfolder, and within
    each subfolder, there are individual files.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图 2.5 显示了一个按年份划分的 Parquet 数据集文件夹结构示例。每个年份都有自己的子文件夹，在每个子文件夹内，包含有单独的文件。
- en: '![Figure 2.5: Example of a folder structure for a Parquet dataset partitioned
    by year](img/file20.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5：按年份划分的 Parquet 数据集文件夹结构示例](img/file20.png)'
- en: 'Figure 2.5: Example of a folder structure for a Parquet dataset partitioned
    by year'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：按年份划分的 Parquet 数据集文件夹结构示例
- en: Parquet files are referred to as “**self-described**” since each file contains
    the encoded data and additional metadata in the footer section. The metadata includes
    the version of the Parquet format, data schema and structure (such as column types),
    and other statistical information such as the minimum and maximum values for the
    columns. Consequently, when writing and reading Parquet datasets using pandas,
    you will notice that the DataFrame schema is preserved.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件被称为“**自描述**”文件，因为每个文件包含编码的数据和位于页脚部分的附加元数据。元数据包括 Parquet 格式的版本、数据模式和结构（例如列类型），以及其他统计信息，如列的最小值和最大值。因此，在使用
    pandas 读写 Parquet 数据集时，你会注意到 DataFrame 的模式得以保留。
- en: 'There are some key parameters you need to be familiar with based on the official
    pandas documentation for the `read_parquet()` reader function which you can find
    in their official page here ([https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html)).
    You already used some of these parameters in the previous *How to do it…* section.
    The following shows the key parameters you already used:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 根据官方 pandas 文档，你需要熟悉一些关键参数，这些文档可在其官网页面中找到（[https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html)）。你已经在之前的
    *如何操作…* 部分使用了其中一些参数。以下是你已经使用过的关键参数：
- en: '[PRE54]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '`path`: This is the first positional argument and the only required field needed
    (at a minimum) to read a Parquet file. In our example, you passed the Python path
    object named `file` as the argument. You can also pass a valid URL that points
    to a remote Parquet file location, such as an AWS S3 bucket.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path`：这是第一个位置参数，也是读取 Parquet 文件时所需的唯一必填字段（至少需要此项）。在我们的示例中，你传递了名为 `file` 的
    Python 路径对象作为参数。你也可以传递一个有效的 URL，指向远程 Parquet 文件的位置，例如 AWS S3 存储桶。'
- en: '`engine`: The default value is “auto” if you do not pass any arguments to the
    engine parameter. The other two valid options are “pyarrow” and “fastparquet”
    depending on which engine you installed. In our example, you installed the **PyArrow**
    library (see the *Getting ready* section). The “auto” option will first attempt
    to load the **PyArrow**, and then fallback to **fastparquet** if it’s not available.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`engine`：如果你没有向引擎参数传递任何参数，则默认值为“auto”。另两个有效选项是“pyarrow”和“fastparquet”，具体取决于你安装的引擎。在我们的示例中，你安装了
    **PyArrow** 库（参见 *准备工作* 部分）。“auto”选项将首先尝试加载 **PyArrow**，如果不可用，则回退到 **fastparquet**。'
- en: '`columns`: Here you can specify the columns you would like to limit when reading.
    You will pass this as a Python list even if you select just one column. In our
    example, you defined a columns variable as `columns = [''DATE'', ''year'', ''TMAX'']`
    and then passed it as an argument.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`columns`：在这里，您可以指定在读取时希望限制的列。即使只选择一列，您也将其作为Python列表传递。在我们的示例中，您将列变量定义为`columns
    = [''DATE'', ''year'', ''TMAX'']`，然后将其作为参数传递。'
- en: '`**kwargs`: Indicates that additional arguments can be passed to the engine.
    In our case, we used the **PyArrow** library; hence, the `read_parquet()` pandas
    function will pass these arguments to the `pyarrow.parquet.read_table()` function
    from the PyArrow library. In the previous examples, we passed a list with filtering
    criteria to the `filters` parameter in the `pyarrow.parquet.read_table()`. For
    additional parameters that you can utilize, you can check the official documentation
    for `pyarrow.parquet.read_table()` here [https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**kwargs`：表示可以将额外的参数传递给引擎。在我们的案例中，我们使用了**PyArrow**库；因此，`read_parquet()` pandas函数将这些参数传递给PyArrow库中的`pyarrow.parquet.read_table()`函数。在之前的示例中，我们将包含过滤条件的列表传递给了`pyarrow.parquet.read_table()`中的`filters`参数。有关可以使用的其他参数，您可以在这里查看`pyarrow.parquet.read_table()`的官方文档：[https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html)'
- en: There’s more…
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Recall in the *Getting Ready* section that you installed the PyArrow library
    as the backend engine for working with Parquet files with pandas. When you used
    the `read_parquet()` reader function in pandas, the `pyarrow` engine was the default.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回想一下在*准备工作*部分中，您已将PyArrow库安装为处理Parquet文件的后端引擎。当您在pandas中使用`read_parquet()`读取函数时，`pyarrow`引擎是默认的。
- en: 'Since you have already installed the library, you can utilize it directly to
    work with Parquet files in a similar manner as you did using pandas. Instead of
    the `pandas.read_parquet()` function, you will use `pyarrow.parquet.read_table()`
    function as shown in the following:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 既然您已经安装了该库，您可以直接利用它像使用pandas一样处理Parquet文件。您将使用`pyarrow.parquet.read_table()`函数，而不是`pandas.read_parquet()`函数，如下所示：
- en: '[PRE55]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The `table` object is an instance of the `pyarrow.Table` class. You can validate
    this using the following code:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`table`对象是`pyarrow.Table`类的一个实例。您可以使用以下代码验证这一点：'
- en: '[PRE56]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The `table` object contains many useful methods including the `.to_pandas()`
    method to convert the object into a pandas DataFrame:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`table`对象包含许多有用的方法，包括`.to_pandas()`方法，用于将该对象转换为pandas DataFrame：'
- en: '[PRE57]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The following illustrates this further to show the similarities between the
    `read_table()` and `read_parquet()` functions:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容进一步说明了`read_table()`和`read_parquet()`函数之间的相似性：
- en: '[PRE58]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Both `df_pa` and `df_pd` are equivalent.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`df_pa`和`df_pd`是等价的。'
- en: The PyArrow library provides a low-level interface, while pandas provides a
    high-level interface that is built on top of PyArrow. The same applies if you
    decide to install the `fastparquet` library instead. Note that PyArrow is the
    Python implementation of **Apache Arrow**, an open-source project for in-memory
    data (columnar memory). While Apache Parquet specifies the columnar file format
    for efficient storage and retrieval, Apache Arrow allows us to process such large
    columnar datasets in memory efficiently as well.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: PyArrow库提供了低级接口，而pandas提供了建立在PyArrow之上的高级接口。如果您决定安装`fastparquet`库，情况也是如此。请注意，PyArrow是**Apache
    Arrow**的Python实现，Apache Arrow是一个用于内存数据（列式内存）的开源项目。虽然Apache Parquet指定了用于高效存储和检索的列式文件格式，Apache
    Arrow则使我们能够高效地在内存中处理如此庞大的列式数据集。
- en: See also
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: '[PRE59]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Working with large data files
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理大数据文件
- en: One of the advantages of using pandas is that it provides data structures for
    in-memory analysis, which results in a performance advantage when working with
    data. However, this advantage can also become a constraint when working with large
    datasets, as the amount of data you can load is limited by the available memory.
    When datasets exceed the available memory, it can lead to performance degradation,
    especially when pandas creates intermediate copies of the data for certain operations.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pandas的一个优点是，它提供了内存分析的数据结构，这使得处理数据时具有性能优势。然而，当处理大数据集时，这一优势也可能变成约束，因为您可以加载的数据量受可用内存的限制。当数据集超过可用内存时，可能会导致性能下降，尤其是在pandas为某些操作创建数据的中间副本时。
- en: 'In real-world scenarios, there are general best practices to mitigate these
    limitations, including:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实场景中，有一些通用的最佳实践可以缓解这些限制，包括：
- en: '**Sampling or loading a small number of rows for your Exploratory Data Analysis
    (EDA):** Before applying your data analysis strategy to the entire dataset, it
    is a good practice to sample or load a small number of rows. This allows you to
    get a better understanding of your data, gain some intuition, and identify unnecessary
    columns that can be eliminated, thus reducing the overall dataset size.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样或加载少量行进行探索性数据分析（EDA）**：在将数据分析策略应用于整个数据集之前，采样或加载少量行是一种良好的实践。这可以帮助您更好地了解数据，获得直觉，并识别可以删除的不必要列，从而减小整体数据集的大小。'
- en: '**Reduce the Number of Columns:** Keeping only the columns necessary for your
    analysis can significantly reduce the memory footprint of the dataset.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少列数**：仅保留分析所需的列可以显著减少数据集的内存占用。'
- en: '**Chunking**: Utilizing the `chunksize` parameter available in many of the
    reader functions in pandas allows you to process the data in smaller, manageable
    chunks. This technique helps in handling large datasets by processing them piece
    by piece.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分块处理**：利用 pandas 中许多读取函数提供的 `chunksize` 参数，您可以将数据分成较小、易于管理的块进行处理。此技巧有助于通过逐块处理来应对大规模数据集。'
- en: '**User Other Libraries for Large Datasets**: There are alternative libraries
    specifically designed for working with large datasets that offer a similar API
    to pandas, such as **Dask** **Polars, and Modin**.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用其他大数据集库**：有一些专门为处理大数据集设计的替代库，它们提供类似于 pandas 的 API，例如 **Dask**、**Polars**
    和 **Modin**。'
- en: 'In this recipe, you will learn about techniques within pandas to handle large
    datasets, such as *chunking*. Afterward, you will explore three new libraries:
    **Dask**, **Polars**, and **Modin.** These libraries serve as alternatives to
    pandas and can be particularly useful when dealing with large datasets.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将学习 pandas 中处理大数据集的技巧，如 *分块处理*。随后，您将探索三个新库：**Dask**、**Polars** 和 **Modin**。这些库是
    pandas 的替代方案，特别适用于处理大规模数据集。
- en: Getting ready
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, you will install Dask and Polars libraries.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将安装 Dask 和 Polars 库。
- en: 'To install using pip, you can use the following command:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pip 安装，您可以使用以下命令：
- en: '[PRE60]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'To install using Conda, you can use:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Conda 安装，您可以使用：
- en: '[PRE61]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: In this recipe, you will be working with the New York Taxi data set from ([https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page))
    and we will be working with Yellow Taxi Trip Records for 2023 (covering January
    to May). In the GitHub repository of the book I have provided the `run_once()`
    function that you will need to execute once. It will combine all five months of
    datasets (five parquet files), and produce one large CSV data set (around 1.72
    GB).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将使用来自 [https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
    的纽约出租车数据集，并且我们将使用 2023 年的 Yellow Taxi 旅行记录（涵盖 1 月至 5 月）。在本书的 GitHub 仓库中，我提供了 `run_once()`
    函数，您需要执行一次。它将结合五个月的数据集（五个 parquet 文件），并生成一个大型 CSV 数据集（约 1.72 GB）。
- en: 'Below is the script as a reference:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是脚本作为参考：
- en: '[PRE62]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: How to do it…
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'In this recipe, you will explore four different methods for handling large
    datasets for ingestion purposes. These methods include:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将探索四种不同的方法来处理大型数据集以进行摄取。这些方法包括：
- en: Using the `chuncksize` parameter, which is available in many of the reader functions
    in pandas.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `chunksize` 参数，该参数在 pandas 的许多读取函数中都有提供。
- en: Using the Dask library
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Dask 库
- en: Using the Polars library
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Polars 库
- en: 'The `memory_profiler` library will be utilized for illustration purposes to
    show memory consumption. You can install the library using `pip`:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory_profiler` 库将用于演示内存消耗。您可以使用 `pip` 安装该库：'
- en: '[PRE63]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'To use memory_profiler in Jupyter Notebook, you will need to run the following
    once:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Jupyter Notebook 中使用 memory_profiler，您需要先运行以下命令一次：
- en: '[PRE64]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Once loaded, you can use it inside any code cell. You just need to start the
    cell with the `%memit` or `%%memit in a Jupyter code cell. A typical output will
    show peak memory size and increment size.`
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 加载后，您可以在任何代码单元格中使用它。只需在 Jupyter 代码单元格中以 `%memit` 或 `%%memit` 开头即可。典型的输出将显示峰值内存大小和增量大小。
- en: '**Peak memory** represents the maximum memory usage during the execution of
    a specific line of code.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**峰值内存**表示在执行特定代码行时的最大内存使用量。'
- en: '**Increment** represents the difference in memory usage between the current
    line and the previous line.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增量**表示当前行与上一行之间的内存使用差异。'
- en: Using Chunksize
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用Chunksize
- en: Several reader functions in pandas support chunking through the `chunksize`
    parameter. This approach is convenient when you have a large dataset that you
    need to ingest, but it may not be suitable if you need to perform complex logic
    on each chunk, which requires coordination between the chunks.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: pandas中有多个读取函数支持通过`chunksize`参数进行分块。当你需要导入一个大型数据集时，这种方法非常方便，但如果你需要对每个数据块执行复杂逻辑（这需要各个数据块之间的协调），它可能不适用。
- en: 'Some of the reader functions in pandas that support the `chunksize` parameter
    include: `pandas.read_csv()`, `pandas.read_table()`, `pandas.read_sql()`, `pandas.read_sql_query()`,
    `pandas.read_sql_table()`, `pandas.read_json()`, `pandas.read_fwf()`, `pandas.read_sas()`,
    `pandas.read_spss()`, and `pandas.read_stata()`.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: pandas中支持`chunksize`参数的一些读取函数包括：`pandas.read_csv()`，`pandas.read_table()`，`pandas.read_sql()`，`pandas.read_sql_query()`，`pandas.read_sql_table()`，`pandas.read_json()`，`pandas.read_fwf()`，`pandas.read_sas()`，`pandas.read_spss()`，以及`pandas.read_stata()`。
- en: 'First, let’s start by reading this large tile using the traditional approach
    with `read_csv` without chunking:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们开始使用传统方法，通过`read_csv`读取这个大型文件，而不进行数据块分割：
- en: '[PRE65]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Given we have two magic commands `%%time` and `%%memit` the output will display
    memory usage and CPU time as shown below:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有两个魔法命令`%%time`和`%%memit`，输出将显示内存使用情况和CPU时间，如下所示：
- en: '[PRE66]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Using the same `read_csv` function, you utilize the chunksize parameter, which
    represents the number of lines to read per chunk. In this example, you will use
    `chunksize=10000,` which will create a chunk every 10000 rows.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相同的`read_csv`函数，你可以利用chunksize参数，它表示每个数据块读取的行数。在这个例子中，你将使用`chunksize=10000,`，这将每10000行创建一个数据块。
- en: '[PRE67]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: This will produce the following output
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出
- en: '[PRE68]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The reason the execution occurred so fast is that what has been returned is
    an **iterator** object of type `TextFileReader` as shown:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 执行如此快速的原因是返回的是一个**迭代器**对象，类型为`TextFileReader`，如下所示：
- en: '[PRE69]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'To retrieve the data in each chunk you can use the `get_chunk()` method to
    retrieve one chunk at a time or use a loop to retrieve all chunks or simply use
    the `pandas.concat()` function:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要检索每个数据块中的数据，你可以使用`get_chunk()`方法一次检索一个数据块，或者使用循环检索所有数据块，或者简单地使用`pandas.concat()`函数：
- en: 'Option 1: Using the `get_chunk()` method or Python `next()` function. This
    will retrieve one chunk at a time, at 10000 records per chunk. Every time you
    run one get_chunk() or next() you will get the next chunk.'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选项 1：使用`get_chunk()`方法或Python的`next()`函数。这将一次检索一个数据块，每个数据块包含10000条记录。每次运行一个get_chunk()或next()时，你将获得下一个数据块。
- en: '[PRE70]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Option 2: Looping through the chunks. This is useful if you want to perform
    simple operations on each chunk before combining each chunk:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选项 2：遍历数据块。这在你希望对每个数据块执行简单操作后再合并各个数据块时非常有用：
- en: '[PRE71]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Option 3: using pd.concat() to retrieve all the chunks at once in one operation.
    This may not be as useful in terms of overall performance:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选项 3：使用pd.concat()一次性检索所有数据块。这在整体性能上可能不如其他方法：
- en: '[PRE72]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The memory and CPU time measurements are added for illustration purposes. As
    you can observe, looping through the chunks and appending each chunk can be a
    time-consuming process.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 内存和CPU时间的测量是为了说明问题。如你所见，遍历数据块并逐个追加每个数据块可能是一个耗时的过程。
- en: Next, you will learn about how to use Polars which provides a very similar API
    to that of pandas making the transition to learn Polars a simpler task.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将学习如何使用Polars，它提供了与pandas非常相似的API，使得学习Polars的过渡变得更简单。
- en: Using Polars
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用Polars
- en: Similar to pandas the Polars library is designed to be used on a single machine
    but offers higher performance than pandas when working with large datasets. Unlike
    pandas which is single threaded and cannot leverage multiple cores on a single
    machine, Polars can utilize all available cores on a single machine for efficient
    parallel processing. In terms of memory usage, pandas provides in-memory data
    structures hence its popularity for in-memory analytics. This also means that
    when you load your CSV file, the entire dataset is loaded into memory, hence working
    with datasets beyond your memory’s capacity can be problematic. Polars on the
    other hand requires less memory than pandas for similar operations.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 pandas，Polars 库的设计也主要用于单台机器，但在处理大型数据集时，Polars 的性能优于 pandas。与单线程且无法利用单台机器多个核心的
    pandas 不同，Polars 可以充分利用单台机器上所有可用的核心进行高效的并行处理。在内存使用方面，pandas 提供了内存数据结构，因此它在内存分析中非常流行。这也意味着，当你加载
    CSV 文件时，整个数据集会被加载到内存中，因此，处理超出内存容量的数据集可能会遇到问题。另一方面，Polars 在执行类似操作时所需的内存比 pandas
    少。
- en: Polars is written in Rust, a programming language that offers similar performance
    to C and C++ and is becoming a very popular programming language in the land of
    Machine Learning Operations (MLOps) due to its performance compared to Python.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Polars 是用 Rust 编写的，这是一种性能与 C 和 C++ 相似的编程语言，由于其与 Python 相比的优越性能，Rust 在机器学习操作（MLOps）领域变得非常流行。
- en: In this recipe you will explore the basics of Polars, primarily reading a large
    CSV file using the `read_csv()` reader function. The
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将探索 Polars 的基础知识，主要是使用 `read_csv()` 读取大型 CSV 文件。
- en: Start by importing the Polars library
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入 Polars 库：
- en: '[PRE73]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'You can now read the CSV file using the `read_csv` function similar to how
    you have done it using pandas. Notice the use of the Jupyter magic commands `%%time`
    and `%%memit` for illustration purposes:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你可以使用 `read_csv` 函数来读取 CSV 文件，类似于你使用 pandas 时的做法。注意为了演示目的，使用了 Jupyter 魔法命令
    `%%time` 和 `%%memit`：
- en: '[PRE74]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: You can use the `.head()` method to print out the first 5 records of the Polars
    DataFrame similar to pandas.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用 `.head()` 方法打印出 Polars DataFrame 的前 5 条记录，这与 pandas 类似。
- en: '[PRE75]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'To get the total number of rows and columns of the Polars DataFrame you can
    use the `.shape` property similar to how pandas:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取 Polars DataFrame 的总行数和列数，你可以使用 `.shape` 属性，类似于 pandas 的用法：
- en: '[PRE76]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'If you decided to use Polars for processing large datasets but later decided
    to output the results back as a pandas DataFrame, you can do so using the `.to_pandas()`
    method:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你决定使用 Polars 来处理大型数据集，但后来决定将结果输出为 pandas DataFrame，可以使用 `.to_pandas()` 方法来实现：
- en: '[PRE77]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Just from these simple code runs it becomes clear how fast Polars is. This can
    be even more obvious from the memory and CPU metrics when comparing `read_csv`
    in the Polars library to the `read_csv` in pandas.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些简单的代码运行中可以清楚地看出 Polars 的速度有多快。当将 Polars 库中的`read_csv`与 pandas 中的`read_csv`进行比较时，从内存和
    CPU 指标上可以更加明显地看出这一点。
- en: Using Dask
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 Dask
- en: Another popular library for working with large datasets is Dask. It has a similar
    API to pandas but differs in its distributed computing capabilities allowing it
    to scale beyond a single machine. Dask integrates pretty well with other popular
    libraries such as pandas, Scikit-Learn, NumPy, and XGBoost.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个处理大型数据集的流行库是 Dask。它与 pandas 的 API 相似，但在分布式计算能力上有所不同，允许它超越单台机器进行扩展。Dask 与其他流行的库如
    pandas、Scikit-Learn、NumPy 和 XGBoost 等集成得很好。
- en: Additionally, you can install Dask-ML, an add-on library that provides scalable
    machine learning alongside popular Python ML libraries such as Scikit-Learn, XGBoot,
    PyTorch, and TensorFlow/Keras.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以安装 Dask-ML，这是一款附加库，提供可扩展的机器学习功能，并与流行的 Python ML 库如 Scikit-Learn、XGBoot、PyTorch
    和 TensorFlow/Keras 等兼容。
- en: In this recipe, you will explore the basics of Polars, primarily reading a large
    CSV file using the `read_csv()` reader function.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将探索 Polars 的基础知识，主要是使用 `read_csv()` 读取大型 CSV 文件。
- en: 'Start by importing the `dataframe` module from the `dask` library:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从 `dask` 库中导入 `dataframe` 模块：
- en: '[PRE78]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'You can now read the CSV file using the `read_csv` function similar to how
    you have done it using pandas. Notice the use of the Jupyter magic commands `%%time`
    and `%%memit` for illustration purposes:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你可以使用 `read_csv` 函数来读取 CSV 文件，类似于你使用 pandas 时的做法。注意为了演示目的，使用了 Jupyter 魔法命令
    `%%time` 和 `%%memit`：
- en: '[PRE79]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Interesting output in terms of memory and CPU utilization. One would assume
    nothing was read. Let’s run a few tests to understand what is happening.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 以内存和 CPU 利用率方面的有趣输出为例。一个人可能会以为什么都没有被读取。让我们运行一些测试来了解发生了什么。
- en: 'You will explore the df_dk DataFrame using familiar techniques you would normally
    do in pandas such as checking the size of the DataFrame:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将使用在pandas中常用的技术来探索df_dk DataFrame，例如检查DataFrame的大小：
- en: '[PRE80]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Notice that we get insights into the number of columns, and their data types,
    but no information on the total number of records (rows). Additionally, notice
    the Delayed object in Dask. We will get back to this shortly.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以看到列的数量和它们的数据类型，但没有有关总记录数（行数）的信息。此外，注意Dask中的Delayed对象。我们稍后会再次提到它。
- en: 'Lastly, try to output the DataFrame using the print function:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，尝试使用print函数输出DataFrame：
- en: '[PRE81]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The output is very interesting; pretty much all that is shown is the structure
    or layout of the DataFrame, but no data. To simplify the explanation, Dask utilizes
    a strategy called lazy loading or lazy evaluation. In other words, most workloads
    in Dask are lazy; they do not get executed immediately until you trigger them
    with a specific action, for example, using the `compute()` method. This feature
    enables Dask to handle large datasets and distributed computing by delaying the
    actual computation until it is explicit. Instead, Dask constructs a task graph
    or execution logic behind the scenes almost instantaneously, but the task graph
    or execution logic is not triggered.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 输出非常有趣；显示的几乎完全是DataFrame的结构或布局，但没有数据。为了简化解释，Dask采用了一种称为懒加载（lazy loading）或懒评估（lazy
    evaluation）策略。换句话说，Dask中的大多数工作负载是懒惰的；它们不会立即执行，直到你通过特定操作触发它们，例如使用`compute()`方法。这个特性使得Dask能够通过延迟实际计算，直到它变得明确，从而处理大数据集和分布式计算。相反，Dask几乎瞬间构建了一个任务图或执行逻辑，但任务图或执行逻辑并未被触发。
- en: When using the `read_csv`, Dask does not load the entire dataset “yet”. It only
    reads the data when you perform specific operations or functions. For example,
    using the `head()` method will retrieve only the first 5 records, and that’s it.
    Thus saving memory and improving performance.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`read_csv`时，Dask并不会立即加载整个数据集“”。它仅在你执行特定操作或函数时才读取数据。例如，使用`head()`方法将只检索前5条记录，仅此而已。这样可以节省内存并提高性能。
- en: 'Print the first 5 records of the Dask DataFrame using the head method:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用head方法打印Dask DataFrame的前五条记录：
- en: '[PRE82]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: You will notice that the first five (5) records are printed out similar to how
    you would expect when using pandas.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，前五（5）条记录的打印结果与使用pandas时的预期类似。
- en: 'To get the total number of records in the dataset, you can use the compute
    method, which will force evaluation:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取数据集中的总记录数，你可以使用`compute()`方法，这将强制进行评估：
- en: '[PRE83]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Lastly, if you were able to reduce the size of the DataFrame to be easier to
    load in pandas, you can convert from Dask DataFrame to a pandas DataFrame using
    the `compute()` method:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，如果你能够将DataFrame的大小缩小到便于pandas加载的程度，可以使用`compute()`方法将Dask DataFrame转换为pandas
    DataFrame：
- en: '[PRE84]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: The Dask library offers different APIs. You only explored the DataFrame API,
    which is similar to the pandas library. Dask offers many optimization capabilities
    and has a learning curve for those working with very large datasets and needing
    to scale their current workflows, whether it be from NumPy, Scikit-Learn, or pandas.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: Dask库提供了不同的API。你只探讨了类似于pandas库的DataFrame API。Dask提供了许多优化功能，对于处理非常大数据集并需要扩展当前工作流程的人来说，它有一定的学习曲线，无论是从NumPy、Scikit-Learn还是pandas。
- en: How it works…
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Due to pandas popularity, many libraries such as Polars and Dasks were inspired
    by pandas simplicity and API. This is because these libraries are designed to
    target pandas users to provide a solution to one of pandas most significant limitations:
    lack of ability to scale and work with very large datasets that cannot fit into
    memory.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 由于pandas的流行，许多库如Polars和Dask受到pandas简洁性和API的启发。因为这些库的设计目标是针对pandas用户，提供解决方案，解决pandas最重要的限制之一：缺乏扩展能力，无法处理无法放入内存的非常大数据集。
- en: There's more…
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有更多…
- en: So far, you have been introduced to better options when working with large files
    than using pandas, especially if you have memory constraints and cannot fit all
    the data into the memory available. The pandas library is a single-core framework
    and does not offer parallel computing capabilities. Instead, there are specialized
    libraries and frameworks for parallel processing designed to work with big data.
    Such frameworks do not rely on loading everything into memory and instead can
    utilize multiple CPU cores, disk usage, or expand into multiple worker nodes (think
    multiple machines). Earlier, you explored **Dask,** which chunks your data, creates
    a computation graph, and parallelizes the smaller tasks (chunks) behind the scenes,
    thus speeding the overall processing time and reducing memory overhead.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解了在处理大文件时，比使用pandas更好的选择，特别是当你有内存限制，无法将所有数据加载到可用内存中时。pandas是一个单核框架，无法提供并行计算功能。相反，有一些专门的库和框架可以进行并行处理，旨在处理大数据。这些框架不依赖于将所有数据加载到内存中，而是可以利用多个CPU核心、磁盘使用，或扩展到多个工作节点（也就是多台机器）。之前，你探讨了**Dask**，它将数据分块，创建计算图，并在后台将较小的任务（分块）并行化，从而加速整体处理时间并减少内存开销。
- en: These frameworks are great but will require you to spend time learning the framework
    and may necessitate that you rewrite the original code to leverage these capabilities.
    So, there might be a learning curve initially. Luckily, this is where the **Modin**
    project comes into play. The Modin library acts as a wrapper or, more specifically,
    an abstraction on top of **Dask** or **Ray** that uses an API similar to that
    of pandas. Modin makes optimizing your pandas' code much more straightforward
    without learning another framework; all it takes is a single line of code.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这些框架虽然很好，但需要你花时间学习框架，可能还需要重写原始代码以利用这些功能。所以，最开始可能会有学习曲线。幸运的是，**Modin**项目正是在这一过程中发挥作用。Modin库作为**Dask**或**Ray**的封装器，或者更具体地说，是其上面的抽象，使用与pandas类似的API。Modin让优化pandas代码变得更加简单，无需学习另一个框架；只需要一行代码。
- en: 'Start by importing the necessary libraries:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 首先导入必要的库：
- en: '[PRE85]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Notice a few things here, first we specified the engine to be used. In this
    case we opted to use Dask. Modin supports other engines including Ray and MPI.
    Second, notice the import modin.pandas as pd statement, this single line is all
    that is needed to scale your existing pandas code. Keep in mind that the Modin
    project is in active development, which means that as pandas mature and adds additional
    features and functionalities, Modin may still be catching up.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 注意几点，首先我们指定了要使用的引擎。在这种情况下，我们选择使用Dask。Modin支持其他引擎，包括Ray和MPI。其次，请注意`import modin.pandas
    as pd`语句，这一行代码就足够将你的现有pandas代码进行扩展。请记住，Modin项目仍在积极开发中，这意味着随着pandas的成熟和新功能的增加，Modin可能仍在追赶。
- en: 'Let’s read our CSV file and compare the metrics in terms of CPU and memory
    utilization:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取CSV文件并比较CPU和内存利用率：
- en: '[PRE86]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Your data is loaded fast and you can run other pandas functions to further
    inspect your data such as `df_pd.head(),` `df_pd.info()`, `df_pd.head()` and you
    will notice how fast the results appear:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据加载很快，你可以运行其他pandas函数进一步检查数据，例如`df_pd.head(),` `df_pd.info()`，`df_pd.head()`，你会注意到结果出现的速度非常快：
- en: '[PRE87]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Using Moding allows you to utilize your existing pandas code, skillset and experience
    with the library without having to learn a new framework. This includes access
    to the pandas I/O functions (reader and writer functions) and all the parameters
    you would expect from the pandas library.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Modin可以让你在不学习新框架的情况下，利用现有的pandas代码、技能和库经验。这包括访问pandas的I/O函数（读写函数）以及你从pandas库中期望的所有参数。
- en: See also
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另见
- en: Other Python projects are dedicated to making working with large datasets more
    scalable and performant and, in some cases, better options than pandas.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 其他Python项目致力于使大数据集的处理更具可扩展性和性能，在某些情况下，它们提供了比pandas更好的选择。
- en: 'Dask: [https://dask.org/](https://dask.org/)'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dask: [https://dask.org/](https://dask.org/)'
- en: 'Modin: [https://modin.readthedocs.io/en/latest/](https://modin.readthedocs.io/en/latest/)'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Modin: [https://modin.readthedocs.io/en/latest/](https://modin.readthedocs.io/en/latest/)'
- en: 'Polars: [https://pola.rs](https://pola.rs)'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Polars: [https://pola.rs](https://pola.rs)'
- en: 'Ray: [https://ray.io/](ch003.xhtml)'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ray: [https://ray.io/](ch003.xhtml)'
- en: 'Vaex: [https://vaex.io/](https://vaex.io/)'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vaex: [https://vaex.io/](https://vaex.io/)'
