- en: 'Chapter 14: The Data Lakehouse'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章：数据湖仓
- en: 'Throughout this book, you have encountered two primary data analytics use cases:
    descriptive analytics, which includes BI and SQL analytics, and advanced analytics,
    which includes data science and machine learning. You learned how Apache Spark,
    as a unified data analytics platform, can cater to all these use cases. Apache
    Spark, being a computational platform, is data storage-agnostic and can work with
    any traditional storage mechanisms, such as databases and data warehouses, and
    modern distributed data storage systems, such as data lakes. However, traditional
    descriptive analytics tools, such as BI tools, are designed around data warehouses
    and expect data to be presented in a certain way. Modern advanced analytics and
    data science tools are geared toward working with large amounts of data that can
    easily be accessed on data lakes. It is also not practical or cost-effective to
    store redundant data in separate storage to be able to cater to these individual
    use cases.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，您已经接触到了两种主要的数据分析用例：描述性分析（包括BI和SQL分析）和高级分析（包括数据科学和机器学习）。您了解到，作为统一数据分析平台的Apache
    Spark，可以满足所有这些用例。由于Apache Spark是一个计算平台，它与数据存储无关，能够与任何传统存储机制（如数据库和数据仓库）以及现代分布式数据存储系统（如数据湖）协同工作。然而，传统的描述性分析工具，如BI工具，通常是围绕数据仓库设计的，且期望数据以特定方式呈现。现代的高级分析和数据科学工具则倾向于处理可以轻松访问的数据湖中的大量数据。将冗余数据存储在单独的存储系统中以满足这些独立用例，不仅不实际，而且成本效益较低。
- en: This chapter will present a new paradigm called the **data lakehouse**, which
    tries to overcome the limitations of data warehouses and data lakes and bridge
    the gap by combining the best elements of both.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍一种新的范式——**数据湖仓**，它试图克服数据仓库和数据湖的局限性，通过结合两者的最佳元素来弥合这两者之间的差距。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Moving from BI to AI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从BI到AI的转变
- en: The data lakehouse paradigm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖仓范式
- en: Advantages of data lakehouses
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖仓的优势
- en: By the end of this chapter, you will have learned about the key challenges of
    existing data storage architectures, such as data warehouses and data lakes, and
    how a data lakehouse helps bridge this gap. You will gain an understanding of
    the core requirements of a data lakehouse and its reference architecture, as well
    as explore a few existing and commercially available data lakehouses and their
    limitations. Finally, you will learn about the reference architecture for the
    data lakehouse, which makes use of Apache Spark and Delta Lake, as well as some
    of their advantages.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，您将了解现有数据存储架构（如数据仓库和数据湖）的关键挑战，以及数据湖仓如何帮助弥合这一差距。您将理解数据湖仓的核心要求和参考架构，并探讨一些现有的商业化数据湖仓及其局限性。最后，您将了解数据湖仓的参考架构，它利用Apache
    Spark和Delta Lake，并学习它们的一些优势。
- en: Moving from BI to AI
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从BI到AI的转变
- en: '**Business intelligence** (**BI**) remains the staple of data analytics. In
    BI, organizations collect raw transactional from a myriad of data sources and
    ETL it into a format that is conducive for building operational reports and enterprise
    dashboards, which depict the overall enterprise over a past period. This also
    helps business executives make informed decisions on the future strategy of an
    organization. However, if the amount of transactional data that''s been generated
    has increased by several orders of magnitude, it is difficult (if not impossible)
    to surface relevant and timely insights that can help businesses make decisions.
    Moreover, it is also not sufficient to just rely on structured transactional data
    for business decision-making. Instead, new types of unstructured data, such as
    customer feedback in the form of natural language, voice transcripts from a customer
    service center, and videos and images of products and customer reviews need to
    be considered if you wish to understand the current state of a business, the state
    of the market, and customer and social trends for businesses to stay relevant
    and profitable. Thus, you must move on from traditional BI and decision support
    systems and supplement the operational reports and executive dashboards with predictive
    analytics, if not completely replace **BI** with **artificial intelligence** (**AI**).
    Traditional BI and data warehouse tools completely fall apart when catering to
    AI use cases. This will be explored in more detail in the next few sections.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**商业智能**（**BI**）仍然是数据分析的核心。在BI中，组织从各种数据源收集原始事务数据，并通过ETL将其转换成一种有利于生成运营报告和企业仪表板的格式，这些报告和仪表板展示了过去一段时间内整个企业的运营情况。这也帮助企业高层做出有关未来战略的明智决策。然而，如果生成的事务数据量增加了几个数量级，那么很难（如果不是不可能的话）从中提取出相关且及时的洞察，帮助企业做出决策。此外，仅仅依赖结构化的事务数据进行业务决策也不再足够。如果你希望了解企业的当前状态、市场状况、客户和社会趋势以保持企业的相关性和盈利性，那么就需要考虑新的非结构化数据类型，例如以自然语言形式的客户反馈、客户服务中心的语音记录，以及产品和客户评论的视频和图像。因此，你必须从传统的BI和决策支持系统转变，并且用预测分析来补充运营报告和高层仪表板，甚至完全用**人工智能**（**AI**）替代**BI**。传统的BI和数据仓库工具在应对AI用例时完全失效。这个问题将在接下来的几节中详细探讨。'
- en: Challenges with data warehouses
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据仓库的挑战
- en: 'Traditionally, data warehouses have been the primary data sources of BI tools.
    Data warehouses expect data to be transformed and stored in a predefined schema
    that makes it easy for BI tools to query it. BI tools have evolved to take advantage
    of data warehouses, which makes the process very efficient and performant. The
    following diagram represents the typical reference architecture of a **BI and
    DW** system:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，数据仓库一直是BI工具的主要数据来源。数据仓库期望数据按照预定义的架构进行转化和存储，这使得BI工具可以轻松地查询数据。BI工具已经发展成能够利用数据仓库的优势，这使得处理过程变得非常高效且具有较好的性能。下图表示了**BI和DW**系统的典型参考架构：
- en: '![Figure 14.1 – BI and DW architecture'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.1 – 商业智能（BI）和数据仓库（DW）架构'
- en: '](img/B16736_14_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_14_01.jpg)'
- en: Figure 14.1 – BI and DW architecture
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1 – 商业智能（BI）和数据仓库（DW）架构
- en: 'As shown in the preceding diagram, a **BI and DW** system extracts raw transactional
    data from transactional systems, transforms the data according to a schema that
    has been defined by the data warehouse, and then loads the data into the data
    warehouse. This entire process is scheduled to be run periodically, typically
    nightly. Modern ETL and data warehouse systems have also evolved to support data
    loads more frequently, such as hourly. However, there are a few key drawbacks
    to this approach that limit these systems from truly supporting modern AI use
    cases. They are as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图所示，**BI和DW**系统从事务系统中提取原始事务数据，根据数据仓库定义的架构转化数据，然后将数据加载到数据仓库中。这个整个过程通常会定期执行，通常是每晚一次。现代的ETL和数据仓库系统已经发展到支持更频繁的数据加载，比如每小时一次。然而，这种方法存在一些关键的缺点，限制了这些系统真正支持现代AI用例。具体来说，以下几点：
- en: The compute and storage of traditional data warehouses are typically located
    on-premises on a single server machine. Their capacity is generally planned for
    peak workloads as the storage and compute capacity of such databases is tied to
    the machine or server they are running on. They cannot be scaled easily, if at
    all. This means that traditional on-premises data warehouses have their data capacity
    set and cannot handle the rapid influx of data that big data typically brings
    on. This makes their architecture rigid and not future-proof.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统数据仓库的计算和存储通常位于本地的单台服务器上。它们的容量通常为峰值负载进行规划，因为这些数据库的存储和计算能力与其运行的机器或服务器紧密相关。这使得它们的扩展性差，甚至无法扩展。这意味着传统的本地数据仓库的数据容量是固定的，无法处理大数据带来的快速数据涌入。这使得它们的架构显得僵化，无法适应未来的发展。
- en: Data warehouses were designed to only be loaded periodically, and almost all
    traditional data warehouses were not designed to handle real-time data ingestion.
    This means data analysts and business executives working off such data warehouses
    usually only get to work on stale data, delaying their decision-making process.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据仓库的设计初衷是仅在特定时间间隔加载数据，几乎所有传统的数据仓库都没有设计成能够处理实时数据摄取。这意味着，基于这些数据仓库的分析师和企业高管通常只能处理过时的数据，从而延迟了决策过程。
- en: Finally, data warehouses are based on relational databases, which, in turn,
    cannot handle unstructured data such as video, audio, or natural language. This
    makes them incapable of expanding to cater to advanced analytics use cases such
    as data science, machine learning, or AI.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，数据仓库基于关系型数据库，而关系型数据库无法处理视频、音频或自然语言等非结构化数据。这使得数据仓库无法扩展以支持数据科学、机器学习或人工智能等高级分析应用场景。
- en: To overcome the aforementioned shortcomings of data warehouses, especially the
    inability to separate compute and storage, and thus the inability to scale on-demand
    and their shortcomings with dealing with real-time and unstructured data, enterprises
    have moved toward data lake architectures. These were first introduced by the
    **Hadoop** ecosystem. We will look at this in more detail in the following sections.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服数据仓库上述的缺点，尤其是无法分离计算和存储，因此无法按需扩展，且处理实时和非结构化数据的能力较弱，企业转向了数据湖架构。这些架构最早由**Hadoop**生态系统引入。我们将在以下章节中详细探讨这一点。
- en: Challenges with data lakes
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据湖的挑战
- en: Data lakes are low-cost storage systems with filesystem-like APIs that can hold
    any form of data, whether it's structured or unstructured, such as the **Hadoop
    Distributed File System** (**HDFS**). Enterprises adopted the data lake paradigm
    to solve the scalability and segregation of compute and storage. With the advent
    of big data and Hadoop, the first HDFS was adopted as data lakes were being stored
    in generic and open file formats such as Apache Parquet and ORC. With the advent
    of the cloud, object stores such as Amazon S3, Microsoft Azure ADLS, and Google
    Cloud Storage were adopted as data lakes. These are very inexpensive and allow
    us to automatically archive data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖是低成本的存储系统，具有类似文件系统的API，可以容纳任何形式的数据，无论是结构化还是非结构化数据，例如**Hadoop分布式文件系统**（**HDFS**）。企业采用数据湖范式来解决计算与存储分离和可扩展性的问题。随着大数据和Hadoop的出现，第一个HDFS被采用，数据湖开始存储在像Apache
    Parquet和ORC这样的通用开放文件格式中。随着云计算的到来，像Amazon S3、Microsoft Azure ADLS和Google Cloud
    Storage这样的对象存储被采纳为数据湖。这些存储非常便宜，并且能够自动归档数据。
- en: 'While data lakes are highly scalable, relatively inexpensive, and can support
    a myriad of data and file types, they do not conform to the strict schema-on-write
    requirements of BI tools. Thus, the cloud-based data lake architecture was supplemented
    with an additional layer of cloud-based data warehouses to specifically cater
    to BI use cases. The architecture of a typical decision support system in the
    cloud is shown in the following diagram:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据湖具有高度的可扩展性，成本较低，并且能够支持多种数据和文件类型，但它们不符合BI工具严格的“写时模式”要求。因此，基于云的数据湖架构被补充了额外的一层云数据仓库，以专门支持BI应用场景。下图展示了云中典型决策支持系统的架构：
- en: '![Figure 14.2 – Data lake architecture in the cloud'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.2 – 云中的数据湖架构'
- en: '](img/B16736_14_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_14_02.jpg)'
- en: Figure 14.2 – Data lake architecture in the cloud
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 – 云中的数据湖架构
- en: In the previous diagram, we can see a typical data lake architecture in the
    cloud. First, raw data is ingested into the data lake as is, without any transformations
    applied, in a streaming or fashionable manner. The raw data is then ELTed and
    put back into the data lake for consumption by downstream use cases such as data
    science, machine learning, and AI. Part of the data that's required for BI and
    operational reporting is cleaned, integrated, and loaded into a cloud-based data
    warehouse to be consumed by BI tools. This architecture solves all the issues
    of a traditional data warehouse. Data lakes are infinitely scalable and completely
    independent of any compute. This is because compute is only required while either
    ingesting, transforming, or consuming data. Data lakes can also handle different
    data types, ranging from structured and semi-structured data to unstructured data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以看到一个典型的云端数据湖架构。首先，原始数据以原样、无任何转换的方式通过流式处理或其他时尚方式输入到数据湖中。然后，原始数据会被 ETL
    处理，并重新放回数据湖，供下游的使用场景，如数据科学、机器学习和人工智能使用。对于 BI 和运营报告所需的部分数据，会进行清洗、整合并加载到基于云的数据仓库中，以供
    BI 工具使用。这种架构解决了传统数据仓库的所有问题。数据湖具有无限的可扩展性，且完全独立于任何计算资源。这是因为计算资源仅在进行数据摄取、转换或消费时才需要。数据湖还可以处理各种数据类型，从结构化和半结构化数据到非结构化数据。
- en: 'However, the data lake architecture does present a few key challenges:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据湖架构确实存在一些关键挑战：
- en: Data lakes do not possess any built-in transactional controls or data quality
    checks, so data engineers need to build additional code into their data processing
    pipelines to perform transactional control. This helps ensure the data is consistent
    and of the right quality for downstream consumption.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖不具备内建的事务控制或数据质量检查功能，因此数据工程师需要在数据处理管道中编写额外的代码来执行事务控制。这有助于确保数据一致性，并保证数据具有适合下游使用的质量。
- en: Data lakes can store data in structured files such as Parquet and ORC; however,
    traditional BI tools may not be capable of reading data in such files, so another
    layer of a data warehouse must be introduced to cater to these use cases. This
    introduces an additional layer of complexity as two separate data processing pipelines
    need to exist – one for ELTing the data to be used by advanced analytics use cases,
    and another for ETLing the data into the data warehouse. This also increases the
    operational costs as data storage almost doubles and two separate data storage
    systems need to be managed.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖可以存储如 Parquet 和 ORC 这样的结构化文件数据；然而，传统的 BI 工具可能无法读取这些文件中的数据，因此必须引入另一个数据仓库层来满足这些用例。这增加了额外的复杂性，因为需要存在两条独立的数据处理管道——一条用于将数据
    ELT 到用于高级分析的场景，另一条用于将数据 ETL 到数据仓库。这也增加了运营成本，因为数据存储几乎翻倍，并且需要管理两个独立的数据存储系统。
- en: While raw data could be streamed into the warehouse, it may not be readily available
    for downstream business analytics systems until the raw data has been ETLed into
    the data warehouse, presenting stale data to business users and data analysts,
    thereby delaying their decision-making process.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然原始数据可以流入数据仓库，但在原始数据经过 ETL（提取、转换、加载）处理并加载到数据仓库之前，它可能无法直接供下游的业务分析系统使用，从而导致业务用户和数据分析师看到的是陈旧的数据，延缓了他们的决策过程。
- en: The data lakehouse promises to overcome such challenges that are faced by traditional
    data warehouses, as well as modern data lakes, and help bring the best elements
    of both to end users. We will explore the data lakehouse paradigm in detail in
    the following section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖仓承诺克服传统数据仓库和现代数据湖所面临的挑战，帮助将两者的最佳特性带给最终用户。我们将在接下来的部分详细探讨数据湖仓模式。
- en: The data lakehouse paradigm
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据湖仓模式
- en: The data lakehouse paradigm combines the best aspects of the data warehouse
    with those of the data lake. A data lakehouse is based on open standards and implements
    data structures and data management features such as data warehouses. This paradigm
    also uses data lakes for its cost-effective and scalable data storage. By combining
    the best of both data warehousing and data lakes, data lakehouses cater to data
    analysts and data scientists simultaneously, without having to maintain multiple
    systems or having to maintain redundant copies of data. Data lakehouses help accelerate
    data projects as teams access data in a single place, without needing to access
    multiple systems. Data lakehouses also provide access to the freshest data, which
    is complete and up to date so that it can be used in BI, data science, machine
    learning, and AI projects. Though data lakehouses are based on data lakes such
    as cloud-based object stores, they need to adhere to certain requirements, as
    described in the following section.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖仓范式结合了数据仓库和数据湖的最佳方面。数据湖仓基于开放标准，实现了数据仓库的数据结构和数据管理特性。这种范式还利用数据湖的成本效益和可扩展的数据存储。通过结合数据仓库和数据湖的优点，数据湖仓同时满足数据分析师和数据科学家的需求，无需维护多个系统或冗余数据副本。数据湖仓帮助加速数据项目，团队可以在一个地方访问数据，无需访问多个系统。数据湖仓还提供访问最新数据的机会，这些数据完整且及时更新，可以用于商业智能、数据科学、机器学习和人工智能项目。虽然数据湖仓基于云端对象存储等数据湖，但它们需要遵守特定的要求，如下一节所述。
- en: Key requirements of a data lakehouse
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据湖仓的关键要求
- en: 'A data lakehouse needs to satisfy a few key requirements for it to be able
    to provide the structure and data management capabilities of a data warehouse,
    as well as the scalability and ability to work with the unstructured data of a
    data lake. The following are some key requirements that must be taken into consideration:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖仓需要满足一些关键要求，以便提供数据仓库的结构和数据管理能力，以及数据湖的可伸缩性和处理非结构化数据的能力。以下是必须考虑的一些关键要求：
- en: A data lakehouse needs to be able to support **ACID** transactions to ensure
    data reads for SQL queries. In a data lakehouse, multiple data pipelines could
    be writing and reading data from the same dataset, and support for transactions
    guarantees that data readers and writers never have an inconsistent view of data.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖仓需要支持**ACID**事务，以确保SQL查询的数据读取。在数据湖仓中，多个数据管道可以同时写入和读取相同的数据集，事务支持保证数据读取者和写入者永远不会看到不一致的数据视图。
- en: A data lakehouse should be able to decouple compute from storage to make sure
    that one can scale independently of the other. Not only does it make the data
    lakehouse more economical but it also helps support concurrent users using multiple
    clusters and very large datasets.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖仓应该能够将计算与存储解耦，确保它们可以独立扩展。这不仅使数据湖仓更经济实惠，还有助于支持使用多个集群和非常大的数据集的并发用户。
- en: A data lakehouse needs to be based on open standards. This allows a variety
    of tools, APIs, and libraries to be able to access the data lakehouse directly
    and prevents any sort of expensive vendor or data lock-ins.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖仓需要基于开放标准。这允许各种工具、API和库直接访问数据湖仓，并防止任何昂贵的供应商或数据锁定。
- en: To be able to support structured data and data models such as **Star/Snowflake**
    schemas from the data warehousing world, a data lakehouse must be able to support
    schema enforcement and evolution. The data lakehouse should support mechanisms
    for managing data integrity, governance, and auditing.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要支持结构化数据和数据模型，如数据仓库世界中的**Star/Snowflake**模式，数据湖仓必须支持模式强制执行和演进。数据湖仓应支持管理数据完整性、治理和审计的机制。
- en: Support for a variety of data types, including structured and unstructured data
    types, is required as a data lakehouse can be used to store, analyze, and process
    data, ranging from text, transactions, IoT data, and natural language to audio
    transcripts and video files.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖仓需要支持多种数据类型，包括结构化和非结构化数据类型，因为数据湖仓可用于存储、分析和处理各种数据，从文本、交易、物联网数据、自然语言到音频转录和视频文件。
- en: Support for traditional structured data analysis such as BI and SQL analytics,
    as well as advanced analytics workloads including data science, machine learning,
    and AI, is required. A data lakehouse should be able to directly support BI and
    data discovery by supporting SQL standard connectivity over **JDBC/ODBC**.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要支持传统的结构化数据分析，如商业智能（BI）和 SQL 分析，以及包括数据科学、机器学习和人工智能（AI）在内的高级分析工作负载。数据湖屋应能够通过支持**JDBC/ODBC**标准连接，直接支持
    BI 和数据发现。
- en: A data lakehouse should be able to support end-to-end stream processing, starting
    from the ability to ingest real-time data directly into the data lakehouse, to
    real-time ELT of data and real-time business analytics. The data lakehouse should
    also support real-time machine learning and low-latency machine learning inference
    in real time.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖屋应能够支持端到端的流处理，从能够将实时数据直接摄取到数据湖屋，到数据的实时 ELT 和实时商业分析。数据湖屋还应支持实时机器学习和低延迟的机器学习推理。
- en: Now that you understand the key requirements of a data lakehouse, let's try
    to understand its core components and reference architecture.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了数据湖屋的关键需求，让我们试着理解它的核心组件和参考架构。
- en: Data lakehouse architecture
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据湖屋架构
- en: Data lakehouse features such as scalability, the ability to handle unstructured
    data, and being able to segregate storage and compute are afforded by the underlying
    data lakes that are used for persistent storage. However, a few core components
    are required to provide data warehouse-like functionality, such as **ACID** transactions,
    indexes, data governance and audits, and other data-level optimizations. A scalable
    metadata layer is one of the core components. A metadata layer sits on top of
    open file formats such as Apache Parquet and helps keep track of file and table
    versions and features such as ACID transactions. The metadata layer also enables
    features such as streaming data ingestion, schema enforcement, and evolution,
    and enforcing data validation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖屋的功能，如可扩展性、处理非结构化数据的能力以及能够将存储和计算分离，得益于用于持久化存储的底层数据湖。然而，为了提供类似数据仓库的功能，仍然需要一些核心组件，比如**ACID**事务、索引、数据治理和审计以及其他数据级优化。可扩展的元数据层是其中一个核心组件。元数据层位于开放文件格式（如
    Apache Parquet）之上，帮助跟踪文件和表格的版本以及像**ACID**事务这样的功能。元数据层还支持流数据摄取、架构强制和演化以及数据验证等功能。
- en: 'Based on these core components, a reference data lakehouse architecture has
    been produced, as shown in the following diagram:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些核心组件，已经产生了一个参考数据湖屋架构，如下图所示：
- en: '![Figure 14.3 – Data lakehouse architecture'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.3 – 数据湖屋架构'
- en: '](img/B16736_14_03.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_14_03.jpg)'
- en: Figure 14.3 – Data lakehouse architecture
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3 – 数据湖屋架构
- en: Data lakehouses are built on top of inexpensive cloud-based storage, which does
    not offer very high throughput data access. To be able to cater to low latency
    and highly concurrent use cases, a data lakehouse needs to be able to provide
    speedy data access via features such as data skipping indexes, folder- and file-level
    pruning, and the ability to collect and store table and file statistics to help
    the query execution engine derive an optimal query execution plan. The data lakehouse
    should possess a high-speed data caching layer to speed up data access for frequently
    accessed data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖屋是建立在廉价的云存储基础上的，这些存储提供的吞吐量数据访问并不特别高。为了能够满足低延迟和高度并发的使用场景，数据湖屋需要通过数据跳跃索引、文件夹和文件级修剪等功能，提供快速的数据访问能力，并能够收集和存储表格和文件统计信息，帮助查询执行引擎推导出最优的查询执行计划。数据湖屋应具备高速数据缓存层，以加快对频繁访问数据的访问。
- en: Examples of existing lakehouse architectures
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现有数据湖屋架构示例
- en: A few commercially available cloud-based offerings do satisfy the requirements
    of a data lakehouse to some extent, if not completely. Some of them are listed
    in this section.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一些市面上商业化的云服务产品在一定程度上（如果不是完全）满足了数据湖屋的需求。部分产品在本节中有列出。
- en: '**Amazon Athena** is an interactive query service provided by AWS as part of
    its managed services. AWS is backed by the open source scalable query engine Presto
    and lets you query data residing in S3 buckets. It supports a metadata layer that''s
    backed by Hive and lets you create table schema definitions. However, query engines
    such as Athena cannot solve all the problems of data lakes and data warehouses.
    They still lack basic data management features such as ACID transactions and performance
    improvement features such as indexes and caching.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon Athena** 是 AWS 提供的一项交互式查询服务，作为其托管服务的一部分。AWS 基于开源的可扩展查询引擎 Presto，允许你查询存储在
    S3 桶中的数据。它支持由 Hive 支持的元数据层，并允许你创建表的模式定义。然而，像 Athena 这样的查询引擎无法解决数据湖和数据仓库的所有问题。它们仍然缺乏基本的数据管理功能，如
    ACID 事务，以及性能提升功能，如索引和缓存。'
- en: The cloud-based commercial data warehouses known as Snowflake come a close second
    as it offers all the features of a traditional data warehouse, along with more
    advanced features that support artificial intelligence, machine learning, and
    data science. It combines data warehouses, subject-specific data marts, and data
    lakes into a single version of the truth that can power multiple workloads, including
    traditional analytics and advanced analytics. However, Snowflake does not provide
    data management features; data is stored within its storage system and it doesn't
    provide the same features for data stored in data lakes. Snowflake might also
    not be a good fit for large-scale ML projects as the data would need to be streamed
    into a data lake.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基于云的商业数据仓库，如 Snowflake，紧随其后，因为它提供了传统数据仓库的所有功能，以及支持人工智能、机器学习和数据科学的更多高级功能。它将数据仓库、特定主题的数据集市和数据湖结合成一个单一的真实版本，能够支持多种工作负载，包括传统分析和高级分析。然而，Snowflake
    不提供数据管理功能；数据存储在其存储系统中，并且对于存储在数据湖中的数据，它没有提供相同的功能。对于大规模机器学习项目，Snowflake 可能也不适用，因为数据需要被流式传输到数据湖中。
- en: '**Google BigQuery**, the petabyte-scale, real-time data warehousing solution,
    offers almost all the features that a data lakehouse does. It supports simultaneous
    batch and streaming workloads, as well as ML workloads using SQL- and query-like
    language via its BigQuery ML offering, which even supports AutoML. However, even
    BigQuery requires data to be stored in its internal format, and it doesn''t provide
    all the query performance-boosting features for external data stored in data lakes.
    In the following section, we will explore how we can leverage Apache Spark, along
    with Delta Lake and cloud-based data lakes, as a data lakehouse.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Google BigQuery**，这款按 PB 级扩展的实时数据仓库解决方案，提供了几乎所有数据湖仓所需的功能。它支持同时的批处理和流处理工作负载，以及通过其
    BigQuery ML 提供的 SQL 和类查询语言进行的机器学习工作负载，甚至支持 AutoML。然而，BigQuery 仍然要求数据以其内部格式存储，并且对于存储在数据湖中的外部数据，它没有提供所有增强查询性能的功能。在接下来的部分中，我们将探讨如何利用
    Apache Spark、Delta Lake 和基于云的数据湖作为数据湖仓。'
- en: Apache Spark-based data lakehouse architecture
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于 Apache Spark 的数据湖仓架构
- en: 'Apache Spark, when combined with Delta Lake and cloud-based data lakes, satisfies
    almost all the requirements of a data lakehouse. We will explore this, along with
    an Apache Spark-based reference architecture, in this section. Let''s get started:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Apache Spark 与 Delta Lake 和基于云的数据湖结合时，几乎可以满足数据湖仓的所有需求。在本节中，我们将探讨这一点，并展示一个基于
    Apache Spark 的参考架构。让我们开始吧：
- en: Delta Lake, via its transaction logs, fully supports ACID transactions, similar
    to a traditional data warehouse, to ensure data that's written to Delta Lake is
    consistent and that any downstream readers never read any dirty data. This also
    allows multiple reads to occur and writes data to the same dataset from multiple
    Spark clusters, without compromising the integrity of the dataset.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta Lake 通过其事务日志，完全支持 ACID 事务，类似于传统数据仓库，以确保写入 Delta Lake 的数据是一致的，并且任何下游读取者永远不会读取脏数据。这也允许多个读取操作发生，并从多个
    Spark 集群将数据写入同一数据集，而不会影响数据集的完整性。
- en: Apache Spark has always been data storage-agnostic and can read data from a
    myriad of data sources, This includes reading data into memory so that it can
    be processed and then writing to the results to persistent storage. Thus, Apache
    Spark, when coupled with a distributed, persistent storage system such as a cloud-based
    data lake, fully supports decoupling storage and compute.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 一直以来都不依赖于特定的数据存储，可以从各种数据源中读取数据，包括将数据读取到内存中以便处理，然后将结果写入持久存储。因此，当
    Apache Spark 与分布式持久存储系统（如基于云的数据湖）结合时，完全支持存储和计算解耦。
- en: Apache Spark supports multiple ways of accessing data stored within Delta Lake,
    including direct access using Spark's Java, Scala, PySpark, and SparkR APIs. Apache
    Spark also supports JDBC/ODBC connectivity via Spark ThriftServer for BI tool
    connectivity. Delta Lake also supports plain Java APIs for connectivity outside
    of Apache Spark.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 支持多种方式访问存储在 Delta Lake 中的数据，包括使用 Spark 的 Java、Scala、PySpark 和 SparkR
    API 进行直接访问。Apache Spark 还支持通过 Spark ThriftServer 进行 JDBC/ODBC 连接，便于与 BI 工具连接。Delta
    Lake 还支持用于 Apache Spark 外部连接的纯 Java API。
- en: Delta Lake supports its own built-in metadata layer via its transaction log.
    Transaction logs provide Delta Lake with version control, an audit trail of table
    changes, and Time Travel to be able to traverse between various versions of a
    table, as well as the ability to restore any snapshot of the table at a given
    point in time.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta Lake 通过其事务日志支持内置的元数据层。事务日志为 Delta Lake 提供了版本控制、表更改的审计跟踪和时间旅行功能，能够在不同版本的表之间进行切换，并能在特定时间点恢复表的任何快照。
- en: Both Apache Spark and Delta Lake support all types of structured and unstructured
    data. Moreover, Delta Lake supports schema enforcement, along with support and
    schema evolution.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 和 Delta Lake 都支持所有类型的结构化和非结构化数据。此外，Delta Lake 支持模式强制执行，并支持模式演化。
- en: Apache Spark has support for real-time analytics via Structured Streaming, and
    Delta Lake fully supports simultaneous batch and streaming.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 支持通过结构化流处理进行实时分析，而 Delta Lake 完全支持批处理和流处理的并发执行。
- en: Thus, Apache Spark, along with Delta Lake coupled with cloud-based data lakes,
    supports in-memory data caching and performance improvement features such as data-skipping
    indexes and collecting table and file statistics. This combination makes for a
    great candidate for a data lakehouse.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Apache Spark 与 Delta Lake 配合使用，同时依托云数据湖，支持内存数据缓存和性能提升特性，如数据跳跃索引和收集表与文件统计信息。这种组合使得数据湖仓成为一个很好的候选方案。
- en: Note
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Spark Structured Streaming only supports micro-batch-based streaming and doesn't
    support event processing. Also, Apache Spark and Delta Lake together make SQL
    query performance very fast. However, Spark's inherent JVM scheduling delays still
    introduce a considerable delta in query processing times, making it unsuitable
    for ultra-low latency queries. Moreover, Apache Spark can support concurrent users
    via multiple clusters and by tuning certain Spark cluster parameters, though this
    complexity needs to be managed by the user. These reasons make Apache Spark, despite
    being used with Delta Lake, not suitable for very high concurrency, ultra-low
    latency use cases. Databricks has developed a next-generation query processing
    engine named Photon that can overcome these issues of open source Spark. However,
    Databricks has not released Photon for open source Apache Spark at the time of
    writing.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 结构化流处理仅支持基于微批次的流处理，不支持事件处理。此外，Apache Spark 和 Delta Lake 共同使 SQL 查询性能非常快。然而，Spark
    固有的 JVM 调度延迟仍会引入相当大的查询处理时间差，这使其不适用于超低延迟查询。此外，Apache Spark 可以通过多个集群和调整某些 Spark
    集群参数来支持并发用户，尽管这种复杂性需要用户进行管理。这些原因使得即使与 Delta Lake 一起使用，Apache Spark 也不适合用于非常高并发、超低延迟的应用场景。Databricks
    开发了一个名为 Photon 的下一代查询处理引擎，可以克服开源 Spark 的这些问题。然而，Databricks 在撰写本文时尚未将 Photon 发布到开源
    Apache Spark 中。
- en: 'Now that you have seen how Apache Spark and Delta Lake can work together as
    a data lakehouse, let''s see what that reference architecture looks like:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到 Apache Spark 和 Delta Lake 如何作为数据湖仓一起工作，接下来让我们看看这个参考架构的样子：
- en: '![Figure 14.4 – Data lakehouse powered by Apache Spark and Delta Lake'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.4 – 由 Apache Spark 和 Delta Lake 提供支持的数据湖仓'
- en: '](img/B16736_14_04.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_14_04.jpg)'
- en: Figure 14.4 – Data lakehouse powered by Apache Spark and Delta Lake
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4 – 由 Apache Spark 和 Delta Lake 提供支持的数据湖仓
- en: The preceding diagram shows a data lakehouse architecture using Apache Spark
    and Delta Lake. Here, data ingestion can be done in real time or batch using Structured
    Streaming, via regular Spark batch jobs, or using some third-party data integration
    tool. Raw data from various sources, such as transactional databases, IoT data,
    clickstream data, server logs, and so on, is directly streamed into the data lakehouse
    and stored in Delta file format. Then, that raw data is transformed using Apache
    Spark DataFrame APIs or using Spark SQL. Again, this can be accomplished in either
    batch or streaming fashion using Structured Streaming. Table metadata, indexes,
    and statistics all are handled by Delta Lake transaction logs, and Hive can be
    used as the metastore. BI and SQL analytics tools can consume the data in the
    data lakehouse directly using JDBC/ODBC connections, and advanced analytics tools
    and libraries can directly interface with the data in the lakehouse using Spark
    SQL or DataFrame APIs vis the Spark clusters. This way, the combination of Apache
    Spark and Delta Lake within the cloud, with object stores as data lakes, can be
    used to implement the data lakehouse paradigm. Now that we have implemented a
    reference Data Lakehouse architecture, let's understand some of its advantages.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示展示了使用 Apache Spark 和 Delta Lake 的数据湖屋架构。在这里，数据摄取可以通过结构化流处理（Structured Streaming）以实时或批量方式完成，也可以通过常规的
    Spark 批处理作业，或使用一些第三方数据集成工具完成。来自各种来源的原始数据，如事务性数据库、物联网数据、点击流数据、服务器日志等，直接流入数据湖屋并以
    Delta 文件格式存储。接着，这些原始数据会使用 Apache Spark DataFrame API 或 Spark SQL 进行转换。同样，这可以通过结构化流处理以批量或流式方式完成。表格元数据、索引和统计信息都由
    Delta Lake 事务日志处理，Hive 可以作为元存储。商业智能（BI）和 SQL 分析工具可以通过 JDBC/ODBC 连接直接访问数据湖屋中的数据，高级分析工具和库也可以通过
    Spark 集群使用 Spark SQL 或 DataFrame API 直接与湖中的数据交互。通过这种方式，Apache Spark 和 Delta Lake
    在云中与对象存储作为数据湖结合，可以用于实现数据湖屋范式。现在我们已经实现了一个参考数据湖屋架构，接下来让我们了解其一些优势。
- en: Advantages of data lakehouses
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据湖屋的优势
- en: Data lakehouses address most of the challenges of using data warehouses and
    data lakes. Some advantages of using data lakehouses are that they reduce data
    redundancies, which are caused by two-tier systems such as a data lake along with
    a data warehouse in the cloud. This translates to reduced storage costs and simplified
    maintenance and data governance as any data governance features, such as access
    control and audit logging, can be implemented in a single place. This eliminates
    the operational overhead of managing data governance on multiple tools.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖屋解决了使用数据仓库和数据湖时的多数挑战。使用数据湖屋的一些优势包括：减少由云中数据湖和数据仓库等双层系统引起的数据冗余。这意味着减少存储成本，并简化维护和数据治理，因为任何数据治理功能，如访问控制和审计日志，都可以在一个地方实现。这消除了在多个工具上管理数据治理的操作开销。
- en: You should have all the data in a single storage system so that you have simplified
    data processing and ETL architectures, which also means easier to maintain and
    manage pipelines. Data engineers do not need to maintain separate code bases for
    disparate systems, and this greatly helps in reducing errors in data pipelines.
    It also makes it easier to track data lineage and fix data issues when they are
    identified.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该将所有数据存储在一个单一的存储系统中，这样可以简化数据处理和 ETL 架构，这也意味着更容易维护和管理数据管道。数据工程师无需为不同的系统维护独立的代码库，这极大地减少了数据管道中的错误。此外，当数据问题被识别时，也更容易追踪数据的血缘关系并进行修复。
- en: Data lakehouses provide data analysts, business executives, and data scientists
    with direct access to the most recent data in the lakehouse. This reduces their
    dependence on IT teams for data access and helps them with timely and informed
    decision-making. Data lakehouses ultimately reduce the total cost of ownership
    as they eliminate data redundancy, reduce operational overhead, and provide performant
    data processing and storage systems at a fraction of the cost compared to certain
    commercially available specialist data warehouses.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖屋为数据分析师、企业高管和数据科学家提供了直接访问湖屋中最新数据的权限。这减少了他们对 IT 团队在数据访问方面的依赖，帮助他们做出及时且明智的决策。数据湖屋最终降低了总体拥有成本，因为它们消除了数据冗余、减少了操作开销，并且提供了具有高性能的数据处理和存储系统，相比于某些商业化的专业数据仓库，其成本大大降低。
- en: Despite all the advantages offered by data lakehouses, the technology is still
    nascent, so it might lag behind certain purpose-built products that have had decades
    of research and development behind them. As the technology matures, data lakehouses
    will become more performant and offer connectivity to more common workflows and
    tools, while still being simple to use and cost-effective.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据湖屋提供了诸多优势，但该技术仍处于初期阶段，因此可能会落后于一些已经有数十年研发背景的专用产品。随着技术的成熟，数据湖屋将变得更加高效，并能与更多常见的工作流和工具连接，同时保持简单易用且具有成本效益。
- en: Summary
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you saw the challenges that are faced by data warehouses and
    data lakes in designing and implementing large-scale data processing systems that
    deal with large-scale data. We also looked at the need for businesses to move
    from advanced analytics to simple descriptive analytics and how the existing systems
    cannot solve both problems simultaneously. Then, the data lakehouse paradigm was
    introduced, which solves the challenges of both data warehouses and data lakes
    and how it bridges the gap of both systems by combining the best elements from
    both. The reference architecture for data lakehouses was presented and a few data
    lakehouse candidates were presented from existing commercially available, large-scale
    data processing systems, along with their drawbacks. Next, an Apache Spark-based
    data lakehouse architecture was presented that made use of the Delta Lake and
    cloud-based data lakes. Finally, some advantages of data lakehouses were presented,
    along with a few of their shortcomings.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了数据仓库和数据湖在设计和实施处理大规模数据的系统时所面临的挑战。我们还探讨了企业从高级分析转向简单描述性分析的需求，以及现有系统如何无法同时解决这两者的问题。接着，介绍了数据湖屋的概念，它解决了数据仓库和数据湖的挑战，并通过结合两者的最佳元素弥合了这两种系统的差距。随后，展示了数据湖屋的参考架构，并介绍了几种现有商业化的大规模数据处理系统中可用的数据湖屋候选方案及其缺点。接下来，展示了基于Apache
    Spark的数据湖屋架构，它利用了Delta Lake和基于云的数据湖。最后，介绍了数据湖屋的一些优势，同时也提到了一些不足之处。
