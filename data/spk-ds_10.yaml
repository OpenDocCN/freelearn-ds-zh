- en: Chapter 10.  Putting It All Together
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章：整合所有内容
- en: Big data analytics is revolutionizing the way businesses are run and has paved
    the way for several hitherto unimagined opportunities. Almost every enterprise,
    individual researcher, or investigative journalist has lots of data to process.
    We need a concise approach to start from raw data and arrive at meaningful insights
    based on the questions at hand.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据分析正在革新企业运营方式，并为许多前所未有的机会铺平道路。几乎每个企业、个人研究人员或调查记者都有大量数据需要处理。我们需要一种简洁的方法，从原始数据出发，根据当前的问题得出有意义的洞察。
- en: We have covered various aspects of data science using Apache Spark in previous
    chapters. We started off discussing big data analytics requirements and how Apache
    spark fits in. Gradually, we looked into the Spark programming model, RDDs, and
    DataFrame abstractions and learnt how unified data access is enabled by Spark
    datasets along with the streaming aspect of continuous applications. Then we covered
    the entire breadth of the data analysis life cycle using Apache Spark followed
    by machine learning. We learnt structured and unstructured data analytics on Spark
    and explored the visualization aspects for data engineers and scientists, as well
    as business users.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们已经讨论了使用 Apache Spark 进行数据科学的各个方面。我们从讨论大数据分析需求以及 Apache Spark 如何适应这些需求开始。逐步地，我们探讨了
    Spark 编程模型、RDD 和 DataFrame 抽象，并学习了 Spark 数据集如何通过连续应用的流式处理方面实现统一数据访问。接着，我们覆盖了使用
    Apache Spark 进行数据分析生命周期的全貌，随后是机器学习的内容。我们学习了在 Spark 上进行结构化和非结构化数据分析，并探索了面向数据工程师、科学家以及业务用户的可视化方面。
- en: 'All the previously discussed chapters helped us understand one concise aspect
    per chapter. We are now equipped to traverse the entire data science life cycle.
    In this chapter, we shall take up an end-to-end case study and apply all that
    we have learned so far. We will not introduce any new concepts; this will help
    apply the knowledge gained so far and strengthen our understanding. However, we
    have reiterated some concepts without going into too much detail, to make this
    chapter self-contained. The topics covered in this chapter are roughly the same
    as the steps in the data analytics life cycle:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所有之前讨论的章节帮助我们理解了每个章节中的一个简洁方面。现在我们已经具备了穿越整个数据科学生命周期的能力。在这一章节中，我们将通过一个端到端的案例研究，应用我们迄今为止学到的所有内容。我们不会引入任何新的概念；这将帮助我们应用已获得的知识并加深理解。然而，我们会重复一些概念，避免过多细节，使这一章节能够自成一体。本章所覆盖的主题大致与数据分析生命周期中的步骤相同：
- en: A quick recap
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速回顾
- en: Introducing a case study
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入案例研究
- en: Framing the business problem
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建业务问题
- en: Data acquisition and data cleansing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据获取与数据清洗
- en: Developing the hypothesis
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出假设
- en: Data exploration
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据探索
- en: Data preparation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Model building
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型构建
- en: Data visualization
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Communicating the results to business users
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向业务用户传达结果
- en: Summary
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: A quick recap
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速回顾
- en: 'We already discussed in detail the various steps involved in a typical data
    science project separately in different chapters. Let us quickly glance through
    what we have covered already and touch upon some important aspects. A high-level
    overview of the steps involved may appear as in the following figure:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在不同章节中详细讨论了典型数据科学项目中的各个步骤。让我们快速回顾一下我们已经覆盖的内容，并简要提及一些重要方面。以下图表展示了这些步骤的高层次概述：
- en: '![A quick recap](img/image_10_001.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![快速回顾](img/image_10_001.jpg)'
- en: In the preceding pictorial representation, we have tried to explain the steps
    involved in a data science project at a higher level, mostly generic to many data
    science assignments. Many more substeps are actually present at every stage, but
    may differ from project to project.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们尝试从更高层次解释数据科学项目中的步骤，这些步骤通常适用于许多数据科学任务。每个阶段实际上都包含更多的子步骤，但可能因项目不同而有所差异。
- en: It is very difficult for data scientists to find the best approach and steps
    to follow in the beginning. Generally, data science projects do not have a well-defined
    life cycle such as the **Software Development Life Cycle** (**SDLC**). It is usually
    the case that data science projects get tramped into delivery delays with repeated
    hold-ups, as most of the steps in the life cycle are iterative. Also, there could
    be cyclic dependencies across teams that add to the complexity and cause delay
    in execution. However, while working on big data analytics projects, it is important
    as well as advantageous for data scientists to follow a well-defined data science
    workflow, irrespective of different business cases. This not only helps in an
    organized execution, but also helps us stay focused on the objective, as data
    science projects are inherently agile in most cases. Also, it is recommended that
    you plan for some level of research on data, domain, and algorithms for any given
    project.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据科学家来说，一开始找到最佳方法和步骤是非常困难的。通常，数据科学项目没有像**软件开发生命周期**（**SDLC**）那样明确的生命周期。数据科学项目通常会因为周期性步骤而导致交付延迟，而且这些步骤是反复迭代的。此外，跨团队的循环依赖也增加了复杂性并导致执行延迟。然而，在处理大数据分析项目时，数据科学家遵循一个明确的数据科学工作流程，无论业务案例如何，都显得尤为重要且有利。这不仅有助于组织执行，还能帮助我们保持专注于目标，因为数据科学项目在大多数情况下天生是敏捷的。同时，建议你为任何项目规划一些关于数据、领域和算法的研究。
- en: In this chapter, we may not be able to accommodate all the granular steps in
    a single flow, but will address the important areas to give you a heads-up. We
    will try to look at some different coding examples that we have not covered in
    the previous chapters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们可能无法将所有细节步骤放在一个流程中，但我们会涉及到一些重要的部分，为你提供一个概览。我们将尝试看一些之前章节未涉及的编码示例。
- en: Introducing a case study
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入案例研究
- en: We will be exploring Academy Awards demographics in this chapter. You can download
    the data from the GitHub repository at [https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv](https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索奥斯卡奖的受众人口统计信息。你可以从GitHub仓库下载数据：[https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv](https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv)。
- en: This dataset is based on the data provided at [http://www.crowdflower.com/data-for-everyone](http://www.crowdflower.com/data-for-everyone).
    It contains demographic details such as race, birthplace, and age. Rows are around
    400 and it can be easily processed on a simple home computer, so you can do a
    **Proof of Concept** (**POC**) on executing a data science project on Spark.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集基于[http://www.crowdflower.com/data-for-everyone](http://www.crowdflower.com/data-for-everyone)提供的数据。它包含了种族、出生地和年龄等人口统计信息。数据行大约有400条，可以在简单的家庭计算机上轻松处理，因此你可以在Spark上执行一个**概念验证**（**POC**）来进行数据科学项目的尝试。
- en: Just start by downloading the file and inspecting the data. The data may look
    fine but as you take a closer look, you will notice that it is not "clean". For
    example, the date of birth column does not follow the same format. Some years
    are in two-digit format whereas some are in four-digit format. Birthplace does
    not have country for locations within the USA.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 只需下载文件并检查数据。数据看起来可能没问题，但当你仔细查看时，你会发现它并不“干净”。例如，出生日期这一列没有统一的格式。有些年份是两位数字格式，而有些则是四位数字格式。出生地列在美国境内的地点没有包含国家信息。
- en: Likewise, you will also notice that the data looks skewed, with more "white"
    race people from the USA. But you might have felt that the trend has changed toward
    later years. You have not used any tools or techniques so far, just had a quick
    glance at the data. In the real world of data science, this seemingly trivial
    activity can be quite helpful further down the life cycle. You get to develop
    a feel for the data at hand and simultaneously hypothesize about the data. This
    brings you to the very first step in the workflow.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你会注意到数据看起来有偏差，来自美国的“白人”种族人数较多。但你可能会觉得，趋势在近几年发生了变化。到目前为止，你还没有使用任何工具或技术，只是对数据进行了快速浏览。在数据科学的实际工作中，这种看似微不足道的活动可能在整个生命周期中非常有帮助。你能够逐步对手头的数据形成感觉，并同时对数据提出假设。这将带你进入工作流程的第一步。
- en: The business problem
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业问题
- en: As iterated before, the most important aspect of any data science project is
    the question at hand. Having a clear understanding on *what problem are we trying
    to solve?* This is critical to the success of the project. It also drives what
    is considered as relevant data and what is not. For example, in the current case
    study, if what we want to look at is the demographics, then movie name and person
    name are irrelevant. At times, there is no specific question at hand! *What then?*
    Even when there is no specific question, the business may still have some objective,
    or data scientists and domain experts can work together to find the area of business
    to work on. To understand the business, functions, problem statement, or data,
    the data scientists start with "Questioning". It not only helps in defining the
    workflow, but helps in sourcing the right data to work on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，任何数据科学项目中最重要的方面是手头的问题。清楚地理解*我们要解决什么问题？*对项目的成功至关重要。它还决定了什么数据被视为相关，什么数据不相关。例如，在当前的案例研究中，如果我们要关注的是人口统计信息，那么电影名称和人物名称就是不相关的。有时，手头没有具体问题！*那怎么办？*即使没有具体问题，商业可能仍然有一些目标，或者数据科学家和领域专家可以合作，找到需要解决的商业领域。为了理解商业、职能、问题陈述或数据，数据科学家首先会进行“提问”。这不仅有助于定义工作流程，还能帮助寻找正确的数据来源。
- en: 'As an example, if the business focus is on demographics information, a formal
    business problem statement can be defined as:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，如果商业关注点是人口统计信息，那么可以定义一个正式的商业问题陈述：
- en: '*What is the impact of the race and country of origin among Oscar award winners?*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*种族和原籍地对奥斯卡奖获得者的影响是什么？*'
- en: In real-world, scenarios this step will not be this straightforward. Framing
    the right question is the collective responsibility of the data scientist, strategy
    team, domain experts, and the project owner. Since the whole exercise is futile
    if it does not serve the purpose, a data scientist has to consult all stakeholders
    and try to elicit as much information as possible from them. However, they may
    end up getting invaluable insights or "hunches". All of these combined form the
    core of the initial hypothesis and also help the data scientist to understand
    what exactly they should look for.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实场景中，这一步骤不会如此简单。提出正确的问题是数据科学家、战略团队、领域专家和项目负责人共同的责任。因为如果不服务于目的，整个过程都是徒劳的，所以数据科学家必须咨询所有相关方，并尽可能从他们那里获取信息。然而，他们最终可能会得到一些宝贵的见解或“直觉”。所有这些信息共同构成了初步假设的核心，并帮助数据科学家理解他们应该寻找什么。
- en: The situations where there is no specific question at hand that the business
    is trying to find an answer for are even more interesting to deal with, but can
    be complex in executing!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有具体问题的情况下，商业试图找出答案的情形更为有趣，但执行起来可能更复杂！
- en: Data acquisition and data cleansing
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据采集与数据清洗
- en: '**Data acquisition** is the logical next step. It may be as simple as selecting
    data from a single spreadsheet or it may be an elaborate several months project
    in itself. A data scientist has to collect as much relevant data as possible.
    ''Relevant'' is the keyword here. Remember, more relevant data beats clever algorithms.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据采集**是逻辑上的下一步。它可能仅仅是从一个电子表格中选择数据，也可能是一个复杂的、持续几个月的项目。数据科学家必须尽可能多地收集相关数据。这里的“相关”是关键词。记住，更多相关的数据胜过聪明的算法。'
- en: We have already covered how to source data from heterogeneous data sources and
    consolidate it to form a single data matrix, so we will not iterate the same fundamentals
    here. Instead, we source our data from a single source and extract a subset of
    it.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了如何从异构数据源中获取数据并将其整合成一个单一的数据矩阵，因此这里不再重复这些基础知识。相反，我们从一个单一来源获取数据，并提取其中的一个子集。
- en: 'Now it is time to view the data and start cleansing it. The scripts presented
    in this chapter tend to be longer than the previous examples but still are no
    means of production quality. Real-world work requires a lot more exception checks
    and performance tuning:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候查看数据并开始清洗它了。本章中呈现的脚本通常比之前的示例更长，但仍然不能算作生产级别的质量。实际工作中需要更多的异常检查和性能调优：
- en: '**Scala**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Python**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For the dataset at hand, you might also have noticed that `date_of_birth` and
    `birthplace` require a lot of cleaning. The following code shows two **user-defined
    functions** (**UDFs**) that clean `date_of_birth` and `birthplace` respectively.
    These UDFs work on a single data element at a time and they are just ordinary
    Scala/Python functions. These user defined functions should be registered so that
    they can be used from within a SQL statement. The final step is to create a cleaned
    data frame that will participate in further analysis.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于手头的数据集，你可能也注意到 `date_of_birth` 和 `birthplace` 需要大量清理。以下代码展示了两个 **用户定义函数**（**UDFs**），分别清理
    `date_of_birth` 和 `birthplace`。这些 UDF 每次处理单个数据元素，它们只是普通的 Scala/Python 函数。为了能够在
    SQL 语句中使用，这些用户定义的函数应该被注册。最后一步是创建一个清理后的数据框，参与进一步的分析。
- en: 'Notice the following logic for cleaning `birthplace.` It is a weak logic because
    we are assuming that any string ending with two characters is an American state.
    We have to compare them against a list of valid abbreviations. Similarly, assuming
    two-digit years are always from the twentieth century is another error-prone assumption.
    Depending on the use case, a data scientist/data engineer has to take a call whether
    retaining more rows is important or only quality data should be included. All
    such decisions should be neatly documented for reference:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意下面的清理 `birthplace` 的逻辑。它是一个比较简单的逻辑，因为我们假设任何以两个字符结尾的字符串都是美国的一个州。我们需要将其与有效的州缩写列表进行比较。同样，假设两位数的年份总是来自二十世纪也是一个容易出错的假设。根据实际情况，数据科学家/数据工程师需要决定是否保留更多行，或者只包括质量更高的数据。所有这些决策应该清晰地记录下来，供后续参考：
- en: '**Scala:**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Python:**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The UDF to clean date accepts a hyphenated date string and splits it. If the
    last component, which is the year, is two digits long, then it is assumed to be
    a twentieth-century date and 1900 is added to bring it to four-digit format.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 清理日期的 UDF 接受一个带有连字符的日期字符串并将其拆分。如果最后一个组件（即年份）是两位数，那么假设它是二十世纪的日期，并加上 1900 将其转换为四位数格式。
- en: 'The following UDF appends the country as USA if the country string is either
    New York City or the last component is two characters long, where it is assumed
    to be a state in the USA:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 UDF 会将国家设置为美国（USA），如果国家字符串是“纽约市”或者最后一个组件是两个字符长，这时假设它是美国的一个州：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Python:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 'Python:'
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The UDFs should be registered if you want to access them from SELECT strings:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果想通过 SELECT 字符串访问 UDF，应该注册 UDF：
- en: '**Scala:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Python:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Clean the data frame using the UDFs. Perform the following cleanup operations:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 UDF 清理数据框。执行以下清理操作：
- en: Call UDFs `fncleanDate` and `fncleanBirthplace` to fix birthplace and country.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 UDF `fncleanDate` 和 `fncleanBirthplace` 来修正出生地和国家。
- en: Subtract birth year from `award_year` to get `age` at the time of receiving
    the award.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `award_year` 中减去出生年份以获取获奖时的 `age`。
- en: Retain `race` and `award` as they are.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留 `race` 和 `award` 原样。
- en: '**Scala:**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Python:**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The last line requires some explanation. The UDFs are used similar to SQL functions
    and the expressions are aliased to meaningful names. We have added a computed
    column `age` because we would like to validate the impact of age also. The `substring_index`
    function  searches the first argument for the second argument. `-1` indicates
    to look for the first occurrence from the right.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行需要一些解释。UDF 的使用类似于 SQL 函数，且表达式被别名为有意义的名称。我们添加了一个计算列 `age`，因为我们希望验证年龄的影响。`substring_index`
    函数搜索第一个参数中的第二个参数。`-1` 表示从右侧查找第一个出现的值。
- en: Developing the hypothesis
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 制定假设
- en: A hypothesis is your best guess about what the outcome will be. You form your
    initial hypothesis based on the question, conversations with stakeholders, and
    also by looking at the data. You may form one or more hypotheses for a given problem.
    This initial hypothesis serves as a roadmap that guides you through the exploratory
    analysis. Developing a hypothesis is very important to statistically approve or
    not approve a statement, and not just by looking at the data as a data matrix
    or even through visuals. This is because our perception built by just looking
    at the data may be incorrect and rather deceptive at times.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 假设是你关于结果的最佳猜测。你根据问题、与利益相关者的对话以及查看数据来形成初步假设。你可能会为给定的问题形成一个或多个假设。这个初步假设作为一张路线图，引导你进行探索性分析。制定假设对于统计学上批准或不批准一个声明非常重要，而不仅仅是通过查看数据矩阵或视觉效果。这是因为仅凭数据可能会导致错误的认知，甚至可能具有误导性。
- en: 'Now you know that your final result may or may not prove the hypothesis to
    be correct. Coming to the case study we have considered for this lesson, we arrive
    at the following initial hypotheses:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道，最终结果可能会证明假设是正确的，也可能证明是假设错误的。对于我们本课考虑的案例研究，我们得出了以下初步假设：
- en: Award winners are mostly white
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获奖者大多是白人
- en: Most of the award winners are from the USA
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数获奖者来自美国
- en: Best actors and actresses tend to be younger than best directors
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳演员和最佳女演员往往比最佳导演年轻
- en: Now that we have formalized our hypotheses, we are all set to move forward with
    the next steps in the life cycle..
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经正式化了假设，准备好进行生命周期中的下一步。
- en: Data exploration
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: Now that we have a clean data frame with relevant data and the initial hypothesis,
    it is time to really explore what we have. The DataFrames abstraction provides
    functions such as `group by` out of the box for you to look around. You may register
    the cleaned data frame as a table and run the time-tested SQL statements to do
    just the same.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个干净的数据框，其中包含相关数据和初步假设，是时候真正探索我们拥有的内容了。DataFrames抽象提供了像`group by`这样的函数，帮助你进行探索。你也可以将清理过的数据框注册为表格，并运行经过时间验证的SQL语句来完成相同的操作。
- en: This is also the time to plot a few graphs. This phase of visualization is the
    exploratory analysis mentioned in the data visualization chapter. The objectives
    of this exploration are greatly influenced by the initial information you garner
    from the business stakeholders and the hypothesis. In other words, your discussions
    with the stakeholders help you know what to look for.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是绘制一些图表的时机。这一可视化阶段是数据可视化章节中提到的探索性分析。这个探索的目标在很大程度上受你从业务利益相关者那里获得的初步信息和假设的影响。换句话说，你与利益相关者的讨论帮助你了解要寻找什么。
- en: 'There are some general guidelines that are applicable for almost all data science
    assignments, but again subjective to different use cases. Let us look at some
    generic ones:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些通用的指南适用于几乎所有的数据科学任务，但也会根据不同的使用场景而有所不同。我们来看一些通用的指南：
- en: Look for missing data and treat it. We have already discussed various ways to
    do this in [Chapter 5](ch05.xhtml "Chapter 5. Data Analysis on Spark"), *Data
    Analysis on Spark*.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找缺失数据并进行处理。我们在[第5章](ch05.xhtml "Chapter 5. Data Analysis on Spark")，*在Spark上进行数据分析*中已经讨论过各种处理方法。
- en: Find the outliers in the dataset and treat them. We have discussed this aspect
    as well. Please note that there are cases where what we think of as outliers and
    normal data points may change depending on the use case.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找数据集中的离群值并进行处理。我们也讨论过这一方面。请注意，有些情况下，我们认为的离群值和正常数据点可能会根据使用场景而变化。
- en: Perform univariate analysis, wherein you explore each variable in the dataset
    separately. Frequency distribution or percentile distribution are quite common.
    Perhaps plot some graphs to get a better idea. This will also help you prepare
    your data before getting into data modeling.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行单变量分析，在这个过程中，你会分别探索数据集中的每一个变量。频率分布或百分位分布是非常常见的。也许你可以绘制一些图表，以获得更清晰的理解。这还将帮助你在进行数据建模之前准备数据。
- en: Validate your initial hypothesis.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证你的初步假设。
- en: Check minimum and maximum values of numerical data. If the variation is too
    high in any column, that could be a candidate for data normalization or scaling.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查数值数据的最小值和最大值。如果某一列的变化范围过大，可能需要进行数据标准化或缩放处理。
- en: Check distinct values in categorical data (string values such as city names)
    and their frequencies. If there are too many distinct values (aka levels) in any
    column, you may have to look for ways to reduce the number of levels. If one level
    is occurring almost always, then this column is not helping the model to differentiate
    between the possible outcomes. Such columns are likely candidates for removal.
    At the exploration stage, you just figure out such candidate columns and let the
    data preparation phase take care of the actual action.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查分类数据中的不同值（如城市名等字符串值）及其频率。如果某一列的不同值（也就是层级）过多，可能需要寻找减少层级数量的方法。如果某一层级几乎总是出现，那么该列对于模型区分可能的结果没有帮助，这样的列很可能是移除的候选。在探索阶段，你只需找出这些候选列，真正的操作可以留给数据准备阶段来处理。
- en: 'In our current dataset, we do not have any missing data and we do not have
    any numerical data that might create any challenge. However, some missing values
    might creep in when invalid dates are processed. So, the following code covers
    the remaining action items. This code assumes that `cleaned_df` is already created:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的数据集中，没有缺失数据，也没有可能带来挑战的数值数据。然而，在处理无效日期时，可能会有一些缺失值出现。因此，以下代码涵盖了剩余的操作项。假设`cleaned_df`已经创建：
- en: '**Scala/Python:**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala/Python:**'
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following visualizations correspond to the initial hypotheses. Note that
    two of our hypotheses were found to be correct but the third one was not. These
    visualizations are created using zeppelin:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下可视化图表对应于最初的假设。请注意，我们的两个假设是正确的，但第三个假设是错误的。这些可视化图表是使用zeppelins创建的：
- en: '![Data exploration](img/image_10_002.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![数据探索](img/image_10_002.jpg)'
- en: Note here that the all hypotheses cannot just be validated through visuals,
    as they can be deceptive at times. So proper statistical tests such as t-tests,
    ANOVA, Chi-squared tests, correlation tests, and so on need to be performed as
    applicable. We will not get into the details in this section. Please refer to
    [Chapter 5](ch05.xhtml "Chapter 5. Data Analysis on Spark"), *Data Analysis on
    Spark*, for further details.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，并非所有假设都可以仅通过可视化来验证，因为可视化有时可能会具有误导性。因此，需要进行适当的统计检验，例如t检验、方差分析（ANOVA）、卡方检验、相关性检验等，根据实际情况进行。我们将在本节中不详细讨论这些内容。请参阅[第5章](ch05.xhtml
    "第5章。Spark上的数据分析")，*Spark上的数据分析*，了解更多详情。
- en: Data preparation
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: The data exploration stage helped us identify all the issues that needed to
    be fixed before proceeding to the modeling stage. Each individual issue requires
    careful thought and deliberation to choose the best fix. Here are some common
    issues and the possible fixes. The best fix is dependent on the problem at hand
    and/or the business context.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索阶段帮助我们识别了在进入建模阶段之前需要修复的所有问题。每个问题都需要仔细思考和讨论，以选择最佳的修复方法。以下是一些常见问题和可能的解决方法。最佳修复方法取决于当前的问题和/或业务背景。
- en: Too many levels in a categorical variable
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类别变量中的层级过多
- en: 'This is one of the most common issues we face. The treatment of this issue
    is dependent on multiple factors:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们面临的最常见问题之一。解决这个问题的方法取决于多个因素：
- en: If the column is almost always unique, for example, it is a transaction ID or
    timestamp, then it does not participate in modeling unless you are deriving new
    features from it. You may safely drop the column without losing any information
    content. You usually drop it during the data cleansing stage itself.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果某列几乎总是唯一的，例如，它是一个交易ID或时间戳，那么除非你从中衍生出新的特征，否则它不会参与建模。你可以安全地删除该列而不会丢失任何信息内容。通常，你会在数据清理阶段就删除它。
- en: If it is possible to replace the levels with coarser-grained levels (for example,
    state or country instead of city) that make sense in the current context, then
    usually that is the best way to fix this issue.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果可以用较粗粒度的层级（例如，使用州或国家代替城市）来替代当前层级，并且在当前上下文中是合理的，那么通常这是解决此问题的最佳方法。
- en: You may want to add dummy columns with 0 or 1 values for each distinct level.
    For example, if you have 100 levels in a single column, you add 100 columns instead.
    At most, one column will have 1 at any observation (row). This is called **one-hot
    encoding** and Spark provides this out of the box through the `ml.features` package.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能需要为每个不同的层级添加一个虚拟列，值为0或1。例如，如果你在单个列中有100个层级，你可以添加100列。每次观察（行）中最多只有一个列会为1。这就是**独热编码（one-hot
    encoding）**，Spark通过`ml.features`包默认提供此功能。
- en: Another option is to retain the most frequent levels. You may even attach each
    of these levels to one of the dominant levels that is somehow considered "nearer"
    to this level. Also, you may bundle up the remaining into a single bucket, say,
    `Others`.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种选择是保留最频繁的层级。你甚至可以将每个这些层级与一个“较近”的主层级进行关联。此外，你可以将其余的层级归为一个单独的桶，例如`Others`。
- en: There is no hard and fast rule for an absolute limit to the number of levels.
    It depends on what granularity you require in each individual feature and the
    performance constraints.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于层级数量的绝对限制并没有硬性规定。这取决于每个特性所需的粒度以及性能限制。
- en: 'The current dataset has too many levels in the categorical variable `country`.
    We chose to retain the most frequent levels and bundle the remaining into `Others`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当前数据集中，类别变量`country`有太多的层级。我们选择保留最频繁的层级，并将其余的层级归为`Others`：
- en: '**Scala:**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Python:**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Numerical variables with too much variation
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数值变量的变化过大
- en: Sometimes numerical data values may vary by several orders of magnitude. For
    example, if you are looking at the annual income of individuals, it may vary a
    lot. Z-score normalization (standardization) and min-max scaling are two popular
    choices to deal with such data. Spark includes both of these transformations out
    of the box in the `ml.features` package.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有时数值数据的变化可能跨越几个数量级。例如，如果你查看个人的年收入，它可能会有很大差异。Z-score标准化（标准化处理）和最小最大值缩放是两种常用的数据处理方法。Spark在`ml.features`包中已经包含了这两种转换方法。
- en: Our current dataset does not have any such variable. The only numerical variable
    we have is age and its value is uniformly two digits. That's one less issue to
    fix.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的数据集中没有这样的变量。我们唯一的数值型变量是年龄，其值始终为两位数。这样就少了一个需要解决的问题。
- en: Please note that it is not always necessary to normalize such data. If you are
    comparing two variables that are in two different scales, or if you are using
    a clustering algorithm or SVM classifier, or any other scenario where there is
    really a need to normalize the data, you may normalize the data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，并非所有数据都需要进行标准化。如果你正在比较两个尺度不同的变量，或者使用聚类算法、SVM分类器，或者其他真正需要标准化数据的场景时，你可以对数据进行标准化处理。
- en: Missing data
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺失数据
- en: This is a major area of concern. Any observations where the target itself is
    missing should be removed from the training data. The remaining observations may
    be retained with some imputed values or removed as per the requirements. You should
    be very careful in imputing the missing values; it may lead to misleading output
    otherwise! It may seem very easy to just go ahead and substitute average values
    in the blank cells of a continuous variable, but this may not be the right approach.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个主要的关注点。任何目标值本身缺失的观测数据应该从训练数据中移除。其余的观测数据可以保留，并填补一些缺失值，或者根据需求移除。在填补缺失值时，你需要非常小心；否则可能导致误导性输出！直接在连续变量的空白单元格中填入平均值似乎很简单，但这可能不是正确的方法。
- en: Our current case study does not have any missing data so there is no scope for
    treating it. However, let us look at an example.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的案例研究没有缺失数据，因此没有需要处理的情况。然而，让我们看一个例子。
- en: Let's assume you have a student's dataset that you are dealing with, and it
    has data from class-1 to class-5\. If there are some missing `Age` values and
    you just find the average of the whole column and substitute, then that would
    rather become an outlier and could lead to vague results. You may choose to find
    the average of only the class that the student is in, and then impute that value.
    This is at least a better approach, but may not be a perfect one. In most of the
    cases, you will have to give weightage to other variables as well. If you do so,
    you may end up building a predictive model to find the missing values and this
    can be a great approach!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在处理一个学生数据集，其中包含了从1班到5班的数据。如果有一些缺失的`年龄`值，而你仅仅通过求整列的平均值来填补，那么这就会成为一个离群点，并且可能导致模糊的结果。你可以选择仅计算学生所在班级的平均值，并用该值填补。这至少是一个更好的方法，但可能不是完美的。在大多数情况下，你还需要对其他变量给予一定的权重。如果这样做，你可能最终会构建一个预测模型来寻找缺失的值，这也是一个很好的方法！
- en: Continuous data
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续数据
- en: 'Numerical data is often continuous and must be discretized because it is a
    prerequisite to some of the algorithms. It is usually split into different buckets
    or ranges of values. However, there could be cases where you may not just uniformly
    bucket based on the range of your data, you may have to consider the variance
    or standard deviation or any other applicable reason to bucket properly. Now,
    deciding the number of buckets is also at the discretion of the data scientist,
    but that too needs careful analysis. Too few buckets reduces granularity and too
    many buckets is just about the same as having too many categorical levels. In
    our case study, `age` is an example of such data and we need to discretize it.
    We split it into different buckets. For example, look at this pipeline stage,
    which converts `age` to 10 buckets:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数值数据通常是连续的，必须进行离散化，因为这是某些算法的前提条件。它通常被拆分成不同的区间或值范围。然而，也可能存在一些情况，你不仅仅是根据数据的范围均匀分桶，可能还需要考虑方差、标准差或任何其他适用的原因来正确地分桶。现在，决定桶的数量也是数据科学家的自由裁量权，但这也需要仔细分析。桶太少会降低粒度，桶太多则和类别级别太多差不多。在我们的案例研究中，`age`就是这种数据的一个例子，我们需要将其离散化。我们将其拆分成不同的区间。例如，看看这个管道阶段，它将`age`转换为10个桶：
- en: '**Scala:**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Python:**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Categorical data
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别数据
- en: We have discussed the need for discretizing continuous data and converting it
    to categories or buckets. We have also discussed the introduction of dummy variables,
    one for each distinct value of a categorical variable. There is one more common
    data preparation practice where we convert categorical levels to numerical (discrete)
    data. This is required because many machine learning algorithms work with numerical
    data, integers, and real-valued numbers, or some other situation may demand it.
    So, we need to convert categorical data into numerical data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了将连续数据离散化并转换为类别或区间的必要性。我们还讨论了引入虚拟变量，每个类别变量的不同值都有一个虚拟变量。还有一种常见的数据准备做法是将类别级别转换为数值（离散）数据。这是必要的，因为许多机器学习算法需要处理数值数据、整数或实数，或者某些其他情况可能要求这样做。因此，我们需要将类别数据转换为数值数据。
- en: There can be downsides to this approach. Introducing an order into inherently
    unordered data may not be logical at times. For example, assigning numbers such
    as 0, 1, 2, 3 to the colors "red", "green", "blue", and "black", respectively,
    does not make sense. This is because we cannot say that red is one unit distant
    from "green" and so is "green" from "blue"! If applicable, introducing dummy variables
    makes more sense in many such cases.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能会有一些缺点。将固有的无序数据引入顺序有时可能不合逻辑。例如，将数字0、1、2、3分别赋给颜色“红色”、“绿色”、“蓝色”和“黑色”是没有意义的。因为我们不能说“红色”距离“绿色”一单位远，“绿色”距离“蓝色”也一样远！在许多此类情况下，若适用，引入虚拟变量更有意义。
- en: Preparing the data
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'Having discussed the common issues and possible fixes, let us see how to prepare
    our current dataset. We have already covered the too many levels issue related
    code fix. The following example shows the rest. It converts all the features into
    a single features column. It also sets aside some data for testing the models.
    This code heavily relies on the `ml.features` package, which was designed to support
    the data preparation phase. Note that this piece of code is just defining what
    needs to be done. The transformations are not carried out as yet. These will become
    stages in subsequently defined pipelines. Execution is deferred as late as possible,
    until the actual model is built. The Catalyst optimizer finds the optimal route
    to implement the pipeline:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了常见问题和可能的解决方法之后，让我们看看如何准备我们当前的数据集。我们已经涵盖了与太多类别级别相关的代码修复。下面的示例展示了其余部分。它将所有特征转换为单个特征列。它还为测试模型预留了一些数据。这段代码重度依赖于`ml.features`包，该包旨在支持数据准备阶段。请注意，这段代码只是定义了需要做的工作。转换尚未执行，这些将在后续定义的管道中成为阶段。执行被推迟到尽可能晚，直到实际模型构建时才会执行。Catalyst优化器会找到实现管道的最佳路径：
- en: '**Scala:**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Python:**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After carrying out all data preparation activity, you will end up with a completely
    numeric data with no missing values and with manageable levels in each attribute.
    You may have already dropped any attributes that may not add much value to the
    analysis on hand. This is what we call the **final data matrix**. You are all
    set now to start modeling your data. So, first you split your source data into
    train data and test data. Models are "trained" using train data and "tested" using
    test data. Note that the split is random and you may end up with different train
    and test partitions if you redo the split.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成所有数据准备工作后，你将得到一个完全由数字组成的、没有缺失值且每个属性的值都处于可管理水平的数据集。你可能已经删除了那些对当前分析贡献不大的属性。这就是我们所说的**最终数据矩阵**。现在，你已经准备好开始建模数据了。首先，你将源数据分成训练数据和测试数据。模型使用训练数据“训练”，然后使用测试数据“测试”。请注意，数据的划分是随机的，如果你重新划分，可能会得到不同的训练集和测试集。
- en: Model building
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型构建
- en: A model is a representation of things, a rendering or description of reality.
    Just like a model of a physical building, data science models attempt to make
    sense of the reality; in this case, the reality is the underlying relationships
    between the features and the predicted variable. They may not be 100 percent accurate,
    but still very useful to give some deep insights into our business space based
    on the data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是事物的表现形式，是对现实的渲染或描述。就像一座物理建筑的模型一样，数据科学模型试图理解现实；在这种情况下，现实是特征与预测变量之间的潜在关系。它们可能不是100%准确，但仍然非常有用，能够基于数据为我们的业务领域提供深刻的见解。
- en: There are several machine learning algorithms that help us model data and Spark
    provides many of them out of the box. However, which model to build is still a
    million dollar question. It depends on various factors, such as interpretability-accuracy
    trade-off, how much data you have at hand, categorical or numerical variables,
    time and memory constraints, and so on. In the following code example, we have
    just trained a few models at random to show you how it can be done.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种机器学习算法帮助我们进行数据建模，Spark提供了其中的许多算法。然而，选择构建哪个模型依然是一个价值百万美元的问题。这取决于多个因素，比如可解释性与准确度之间的权衡、手头的数据量、分类或数值型变量、时间和内存的限制等等。在下面的代码示例中，我们随机训练了几个模型，向你展示如何进行。
- en: 'We''ll be predicting the award type based on race, age, and country. We''ll
    be using the DecisionTreeClassifier, RandomForestClassifier, and OneVsRest algorithms.
    These three are chosen arbitrarily. All of them work with multiclass labels and
    are simple to understand. We have used the following evaluation metrics provided
    by the `ml` package:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将根据种族、年龄和国家预测奖项类型。我们将使用DecisionTreeClassifier、RandomForestClassifier和OneVsRest算法。这三种算法是随意选择的，它们都能处理多类别标签，并且容易理解。我们使用了`ml`包提供的以下评估指标：
- en: '**Accuracy**: The ratio of correctly predicted observations.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：正确预测的观测值所占比例。'
- en: '**Weighted Precision**: Precision is the ratio of correct positive observations
    to all positive observations. Weighted precision takes the frequency of individual
    classes into account.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权精度**：精度是正确正类观测值与所有正类观测值的比率。加权精度考虑了各类的频率。'
- en: '**Weighted Recall**: Recall is the ratio of positives to actual positives.
    Actual positives are the sum of true positives and false negatives. Weighted Recall
    takes the frequency of individual classes into account.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权召回率**：召回率是正类与实际正类的比率。实际正类是指真实正类和假阴性的总和。加权召回率考虑了各类的频率。'
- en: '**F1**: The default evaluation measure. This is the weighted average of Precision
    and Recall.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1**：默认的评估度量。它是精度和召回率的加权平均值。'
- en: '**Scala:**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Python:**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'So far, we have tried a few models and found that they gives us roughly the
    same performance. There are various other ways to validate the model performance.
    This again depends on the algorithm you have used, the business context, and the
    outcome produced. Let us look at some metrics that are offered out of the box
    in the `spark.ml.evaluation` package:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经尝试了几种模型，发现它们的表现大致相同。验证模型性能有许多其他方法，这取决于你使用的算法、业务背景以及所产生的结果。我们来看一下`
    spark.ml.evaluation`包中提供的一些评估指标：
- en: '**Scala:**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE19]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Python:**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Output:**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出:**'
- en: '|  | **Decision tree** | **Random Forest** | **OneVsRest** |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | **决策树** | **随机森林** | **OneVsRest** |'
- en: '| F1 | 0.29579 | 0.26451 | 0.25649 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| F1 | 0.29579 | 0.26451 | 0.25649 |'
- en: '| WeightedPrecision | 0.32654 | 0.26451 | 0.25295 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 加权精度 | 0.32654 | 0.26451 | 0.25295 |'
- en: '| WeightedRecall | 0.30827 | 0.29323 | 0.32330 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 加权召回率 | 0.30827 | 0.29323 | 0.32330 |'
- en: Upon validating the model performance, you will have to tune the model as much
    as possible. Now, tuning can happen both ways, at the data level and at the algorithm
    level. Feeding the right data that an algorithm expects is very important. The
    problem is that whatever data you feed in, the algorithm may still give some output
    - it never complains! So, apart from cleaning the data properly by treating missing
    values, treating univariate and multivariate outliers, and so on, you can create
    many more relevant features. This feature engineering is usually treated as the
    most important aspect of data science. Having decent domain expertise helps to
    engineer better features. Now, coming to the algorithmic aspect of tuning, there
    is always scope for working on optimizing the parameters that we pass to an algorithm.
    You may choose to use grid search to find the optimal parameters. Also, data scientists
    should question themselves on which loss function to use and why, and, out of
    GD, SGD, L-BFGS, and so on, which algorithm to use to optimize the loss function
    and why.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证模型性能后，你将需要尽可能地调整模型。调整可以在数据层面和算法层面进行。提供算法所期望的正确数据非常重要。问题在于，无论你输入什么数据，算法可能仍然会给出某些输出——它从不抱怨！因此，除了通过处理缺失值、处理单变量和多变量异常值等方式对数据进行清理外，你还可以创建更多相关的特征。这种特征工程通常被视为数据科学中最重要的部分。拥有一定的领域专业知识有助于更好地进行特征工程。至于算法层面的调整，总是有机会优化我们传递给算法的参数。你可以选择使用网格搜索来寻找最佳参数。此外，数据科学家应当自问，应该使用哪个损失函数及其原因，以及在
    GD、SGD、L-BFGS 等算法中，应该选择哪种算法来优化损失函数及其原因。
- en: Please note that the preceding approach is intended just to demonstrate how
    to perform the steps on Spark. Selecting one algorithm over the other by just
    looking at the accuracy level may not be the best way. Selecting an algorithm
    depends on the type of data you are dealing with, the outcome variable, the business
    problem/requirement, computational challenges, interpretability, and many others.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前述方法仅用于演示如何在 Spark 上执行步骤。仅通过查看准确率选择某个算法可能不是最佳方法。选择算法取决于你所处理的数据类型、结果变量、业务问题/需求、计算挑战、可解释性等多种因素。
- en: Data visualization
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可视化
- en: '**Data visualization** is something which is needed every now and then from
    the time you take on a data science assignment. Before building any model, preferably,
    you will have to visualize each variable to see their distributions to understand
    their characteristics and also find outliers so you can treat them. Simple tools
    such as scatterplot, box plot, bar chart, and so on are a few versatile, handy
    tools for such purposes. Also, you will have to use the visuals in most of the
    steps to ensure you are heading in the right direction.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据可视化** 是从开始处理数据科学任务时就需要时常使用的工具。在构建任何模型之前，最好先对每个变量进行可视化，以查看它们的分布，了解它们的特征，并找出异常值以便处理。诸如散点图、箱形图、条形图等简单工具是此类目的的多功能且便捷的工具。此外，在大多数步骤中你还需要使用可视化工具，以确保你正在朝正确的方向前进。'
- en: Every time you want to collaborate with business users or stakeholders, it is
    always a good practice to convey your analysis through visuals. Visuals can accommodate
    more data in them in a more meaningful way and are inherently intuitive in nature.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你与业务用户或利益相关者合作时，通过可视化来传达分析结果总是一个好习惯。可视化可以以更有意义的方式容纳更多的数据，并且本质上具有直观性。
- en: Please note that most data science assignment outcomes are preferably represented
    through visuals and dashboards to business users. We already have a dedicated
    chapter on this topic, so we won't go deeper into it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，大多数数据科学任务的结果通常通过可视化和仪表板呈现给业务用户。我们已经有专门的章节讲解这一主题，因此这里就不再深入探讨。
- en: Communicating the results to business users
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向业务用户传达结果
- en: In real-life scenarios, it is mostly the case that you have to keep communicating
    with the business intermittently. You might have to build several models before
    concluding on a final production-ready model and communicate the results to the
    business.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，通常需要你间歇性地与业务方进行沟通。在得出最终的可投入生产的模型之前，你可能需要建立多个模型，并将结果传达给业务方。
- en: 'An implementable model does not always depend on accuracy; you might have to
    bring in other measures such as sensitivity, specificity, or an ROC curve, and
    also represent your results through visuals such as a Gain/Lift chart or an output
    of a K-S test with statistical significance. Note that these techniques require
    business users'' input. This input often guides the way you build the models or
    set thresholds. Let us look at a few examples to better understand how it works:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可实施的模型并不总是依赖于准确性；你可能需要引入其他指标，如灵敏度、特异性，或者ROC曲线，还可以通过可视化图表（如增益/提升图表）或具有统计显著性的K-S检验输出展示结果。需要注意的是，这些技术需要业务用户的输入。这些输入通常会指导你如何构建模型或设置阈值。让我们通过几个例子更好地理解它是如何工作的：
- en: If a regressor predicts the probability of an event occurring, then blindly
    setting the threshold to 0.5 and assuming anything above 0.5 is 1 and less than
    0.5 is 0 may not be the best way! You may use an ROC curve and take a rather more
    scientific or logical decision.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个回归模型预测某个事件发生的概率，那么盲目将阈值设置为0.5，并假设大于0.5的是1，小于0.5的是0，可能并不是最好的方法！你可以使用ROC曲线并做出更科学或更有逻辑性的决策。
- en: False-negative predictions for diagnosis of a cancer test may not be desirable
    at all! This is an extreme case of life risk.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对癌症检测的假阴性预测可能是完全不可取的！这是一个极端的生命风险案例。
- en: E-mail campaigning is cheaper compared to delivery of hard copies. So the business
    may decide to send e-mails to the recipients who are predicted with less than
    0.5 (say 0.35) probability.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比于发送纸质副本，电子邮件营销更便宜。因此，业务方可能决定向那些预测概率低于0.5（例如0.35）的收件人发送电子邮件。
- en: Notice that the preceding decisions are influenced heavily by business users
    or the problem owners, and data scientists work closely with them to take a call
    on such cases.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前述决策受到业务用户或问题所有者的强烈影响，数据科学家与他们密切合作，以决定此类案例。
- en: Again, as discussed already, the right visuals are the most preferred way to
    communicate the results to the business.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，如前所述，正确的可视化是与业务沟通结果的最优方式。
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have taken up a case study and completed the data analytics
    life cycle end to end. During the course of building a data product, we have applied
    the knowledge gained so far in the previous chapters. We have stated a business
    problem, formed an initial hypothesis, acquired data, and prepared it for model
    building. We have tried building multiple models and found a suitable model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们进行了一个案例研究，并完成了数据分析生命周期的整个过程。在构建数据产品的过程中，我们应用了前几章中获得的知识。我们提出了一个业务问题，形成了初步假设，获取了数据，并为建模做了准备。我们尝试了多种模型，最终找到了合适的模型。
- en: In the next chapter, which is also the final chapter, we will discuss building
    real-world applications using Spark.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，也是最后一章，我们将讨论如何使用Spark构建实际应用。
- en: References
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[http://www2.sas.com/proceedings/forum2007/073-2007.pdf](http://www2.sas.com/proceedings/forum2007/073-2007.pdf).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www2.sas.com/proceedings/forum2007/073-2007.pdf](http://www2.sas.com/proceedings/forum2007/073-2007.pdf)。'
- en: '[https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/](https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/](https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/)。'
- en: '[http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf](http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf](http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf)。'
