- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Monitoring Data Pipelines in Production
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产环境中的数据管道监控
- en: In the previous chapters, we learned how to build, configure, and deploy data
    pipelines using the Databricks Data Intelligence Platform. To round off managing
    data pipelines for the lakehouse, in this final chapter of the book, we’ll dive
    into the crucial task of monitoring data pipelines in production. We’ll learn
    how to leverage comprehensive monitoring techniques directly from the Databricks
    Data Intelligence Platform to track pipeline health, pipeline performance, and
    data quality, to name a few. We will also implement a few real-world examples
    through hands-on exercises. Lastly, we’ll look at the best practices for ensuring
    that your data pipelines run smoothly, enabling timely issue detection and resolution,
    and ensuring the delivery of reliable and accurate data for your analytics and
    business needs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了如何使用 Databricks 数据智能平台构建、配置和部署数据管道。为了完善湖仓的数据管道管理，本书的最后一章将深入探讨生产环境中数据管道监控的关键任务。我们将学习如何直接利用
    Databricks 数据智能平台的全面监控技术，追踪管道健康状况、管道性能和数据质量等。我们还将通过实际操作练习实现一些现实世界的示例。最后，我们将讨论确保数据管道平稳运行的最佳实践，及时检测和解决问题，并确保为您的分析和业务需求提供可靠、准确的数据。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Introduction to data pipeline monitoring
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管道监控简介
- en: Pipeline health and performance monitoring
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道健康和性能监控
- en: Data quality monitoring
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量监控
- en: Best practices for production failure resolution
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产环境故障解决的最佳实践
- en: Hands-on exercise – setting up a webhook alert when a job runs longer than expected
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实操练习 – 设置一个 Webhook 警报，当作业运行时间超过预期时触发
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To follow along with the examples provided in this chapter, you’ll need Databricks
    workspace permissions to create and start an all-purpose cluster so that you can
    import and execute the chapter’s accompanying notebooks. It’s also recommended
    that your Databricks user be elevated to a workspace administrator so that you
    can create and edit alert destinations. All code samples can be downloaded from
    this chapter’s GitHub repository at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10)
    . This chapter will create and run several new notebooks, estimated to consume
    around 10-15 **Databricks** **Units** ( **DBUs** ).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本章中的示例，您需要拥有 Databricks 工作空间权限，以创建和启动通用集群，便于导入和执行本章附带的笔记本。还建议将您的 Databricks
    用户提升为工作空间管理员，这样您才能创建和编辑警报目标。所有代码示例可从本章的 GitHub 仓库下载：[https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10)。本章将创建并运行多个新笔记本，预计消耗约
    10-15 **Databricks** **Units** (**DBUs**)。
- en: Introduction to data pipeline monitoring
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据管道监控简介
- en: As data teams deploy data pipelines into production environments, being able
    to detect processing errors, delays, or data quality issues as soon as they happen
    can make a huge impact on catching and correcting issues before they have a chance
    to cascade to downstream systems and processes. As such, the environment that
    data teams build and deploy their pipelines in should be able to monitor them
    and alert them when problems arise.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据团队将数据管道部署到生产环境中，能够在发生处理错误、延迟或数据质量问题时立即检测到，这对于在问题扩展到下游系统和流程之前进行捕捉和纠正，能产生巨大的影响。因此，数据团队构建和部署管道的环境应该能够监控它们，并在出现问题时发出警报。
- en: Exploring ways to monitor data pipelines
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索监控数据管道的方法
- en: 'There are several ways that data teams can monitor their data pipelines in
    production from within the Databricks Data Intelligence Platform. For example,
    data teams can manually observe updates regarding their data pipeline by doing
    the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据团队可以通过 Databricks 数据智能平台在生产环境中监控数据管道。比如，数据团队可以通过以下方式手动观察数据管道的更新：
- en: Viewing pipeli ne status from the **Delta Live Tables** ( **DLT** ) UI
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 **Delta Live Tables** (**DLT**) UI 查看管道状态
- en: Querying pipeline information from the DLT event log
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 DLT 事件日志中查询管道信息
- en: 'While these manual means provide a way to quickly view the latest status of
    a data pipeline in an ad hoc manner, it’s certainly not a scalable solution, particularly
    as your data team adds more and more pipelines. Instead, organizations turn to
    more automated mechanisms. For instance, many organizations choose to leverage
    the built-in notification system in the Databricks Data Intelligence Platform.
    Notifications are prevalent in many objects within the platform. For example,
    data administrators can configure notifications in the following scenarios to
    alert data teams about a change in status pertaining to a particular Databricks
    resource:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些手动方式提供了一种快速查看数据管道最新状态的方式，但它们显然不是一个可扩展的解决方案，特别是当你的数据团队增加越来越多的管道时。相反，组织会转向更自动化的机制。例如，许多组织选择利用
    Databricks 数据智能平台内置的通知系统。通知在平台内的许多对象中都很常见。例如，数据管理员可以在以下场景中配置通知，提醒数据团队有关特定 Databricks
    资源状态的变化：
- en: DLT pipeline (either on update or on flow)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DLT 管道（无论是更新还是流）
- en: Databricks workflow (at the top-most job level)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 工作流（在最上层的作业级别）
- en: Databricks workflow task (finer-grained notification than the preceding option)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 工作流任务（比前述选项更细粒度的通知）
- en: While these notifications can be helpful to alert teams about events or status
    changes during data processing, data teams also need mechanisms for alerting each
    other about issues in the contents of the data landing into the enterprise l akehouse.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些通知有助于在数据处理过程中提醒团队有关事件或状态变化，数据团队还需要机制来提醒彼此有关进入企业湖仓的数据内容中的问题。
- en: Using DBSQL Alerts to notify data validity
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 DBSQL 警报通知数据有效性
- en: The Databricks Data Intelligence Platform can create alert notifications driven
    by a query within the DBSQL portion of the platform called **DBSQL Alerts** .
    DBSQL Alerts can be a useful tool to alert data teams about the data landing in
    their enterprise lakehouse. DBSQL Alerts operate by specifying a particular query
    outcome condition that must be met for the data to be considered valid. However,
    if a particular condition in an Alert is violated, such as an order amount crossing
    above a certain dollar amount threshold, for example, then the system will trigger
    a notification to send to an alert destination. The following diagram depicts
    a DBSQL Alert that notifies recipients via email when there are sales orders exceeding
    a specific dollar amount – in this case, that’s $10,000. In this example, the
    query is a max aggregation, the triggering condition is when the max aggregation
    exceeds $10,000, and the alert destination is an email address.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 数据智能平台可以通过平台内的 DBSQL 部分创建警报通知，称为**DBSQL 警报**。DBSQL 警报是一个有用的工具，可以提醒数据团队关于数据进入他们的企业湖仓的信息。DBSQL
    警报通过指定一个特定的查询结果条件来操作，只有当条件被满足时，数据才会被认为是有效的。然而，如果警报中的某个条件被违反，比如订单金额超过某个美元阈值，那么系统就会触发通知并发送到警报目标。下图描述了一个
    DBSQL 警报，当销售订单超过特定金额时通过电子邮件通知收件人——在这个例子中，金额阈值是 10,000 美元。在此示例中，查询是一个最大值聚合，触发条件是当最大值聚合超过
    10,000 美元时，警报目标是一个电子邮件地址。
- en: '![Figure 10.1 – Configuration of a DBSQL Alert notifying recipients via email](img/B22011_10_001.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – 配置 DBSQL 警报通过电子邮件通知收件人](img/B22011_10_001.jpg)'
- en: Figure 10.1 – Configuration of a DBSQL Alert notifying recipients via email
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 配置 DBSQL 警报通过电子邮件通知收件人
- en: Furthermore, DBSQL Alerts can be scheduled to execute on a repeated schedule,
    for example once every hour. This is an excellent way to automate *data validation*
    checks on the contents of your datasets using the built-in mechanism from within
    the Databricks Data Intelligence Platform. The following screenshot is an example
    of how alerts can be used to schedule a data validation query on a repeated schedule
    and notify data teams when a particular condition or set of conditions has been
    violated.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DBSQL 警报可以设置为按重复计划执行，例如每小时执行一次。这是一种极好的方式，通过 Databricks 数据智能平台内置的机制，自动化进行*数据验证*检查，确保数据集内容的准确性。下图是一个示例，展示了如何使用警报在重复的时间间隔内调度数据验证查询，并在特定条件或条件集被违反时通知数据团队。
- en: '![Figure 10.2 – Configuration of an Alert triggering condition](img/B22011_10_002.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2 – 警报触发条件的配置](img/B22011_10_002.jpg)'
- en: Figure 10.2 – Configuration of an Alert triggering condition
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 警报触发条件的配置
- en: Another mechanism for monitoring data pipelines in production is through workflow
    notifications. Within the Databricks Data Intelligence Platform, notification
    messages can be delivered to enterprise messaging platforms, such as Slack or
    Microsoft Teams, or to incident management systems such as PagerDuty. Later in
    the chapter, we’ll explore how to implement an HTTP webhook-based delivery destination,
    which is popular in web services architecture environments.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 监控生产环境中的数据管道的另一种机制是通过工作流通知。在 Databricks 数据智能平台中，通知消息可以发送到企业消息平台，如 Slack 或 Microsoft
    Teams，或发送到事件管理系统，如 PagerDuty。本章稍后我们将探讨如何实现基于 HTTP Webhook 的交付目标，这在 Web 服务架构环境中非常流行。
- en: There are two types of notifications that can be sent from within a particular
    workflow – job status and task status. Job status notifications are high-level
    statuses about the overall success or failure of a particular workflow. However,
    you can also configure notifications to be sent to monitoring destinations at
    the task level, such as if you’d like to monitor when tasks within a workflow
    are retried.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从特定工作流中发送两种类型的通知——作业状态和任务状态。作业状态通知是关于特定工作流整体成功或失败的高级状态。然而，您还可以配置通知在任务级别发送到监控目标，例如，如果您希望监控工作流中的任务何时被重试。
- en: '![Figure 10.3 – Configuring job- and task-level notifications](img/B22011_10_003.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – 配置作业级别和任务级别的通知](img/B22011_10_003.jpg)'
- en: Figure 10.3 – Configuring job- and task-level notifications
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 配置作业级别和任务级别的通知
- en: While alert notifications are a great way to automate the notification of team
    members when problems arise, data teams also need to monitor the health of data
    pipelines in a periodic and ad hoc manner. We will discuss this in the next section.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然警报通知是自动通知团队成员处理问题的好方法，但数据团队还需要定期和临时地监控数据管道的健康状况。我们将在下一节讨论这一点。
- en: Pipeline health and performance monitoring
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道健康状况和性能监控
- en: 'The Databricks Data Intelligence Platform provides a location for data teams
    to query the status of data pipelines called the event log. The event log contains
    a history of all events that pertain to a particular DLT pipeline. In particular,
    the event log will contain an event feed with a list of event objects with recorded
    metadata about the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 数据智能平台提供了一个供数据团队查询数据管道状态的地方，称为事件日志。事件日志包含与特定 DLT 管道相关的所有事件的历史记录。特别地，事件日志将包含一个事件流，其中列出包含以下录制元数据的事件对象：
- en: What type of event occurred
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发生的事件类型
- en: A unique identifier of the event
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件的唯一标识符
- en: Timestamps of when the event occurred
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件发生的时间戳
- en: A high-level description of the event
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件的高级描述
- en: Fine-grained details about the event
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于事件的细粒度详情
- en: An event-level indication ( **INFO** , **WARN** , **ERROR** , or **METRICS**
    )
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件级别指示（**INFO**、**WARN**、**ERROR** 或 **METRICS**）
- en: The origin source of the event
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件的来源
- en: 'Unlike scalar functions, which return a single value, **Table Valued Functions**
    ( **TVFs** ) are functions that return a table as the result. For DLT pipelines
    that publish to a catalog and schema within Unity Catalog, the Databricks Data
    Intelligence Platform offers a special TVF called **event_log()** to query comprehensive
    information regarding a given DLT pipeline. The **event_log()** function can take
    one of two arguments as input: a fully qualified table name of a pipeline dataset
    or a pipeline ID as an argument.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与标量函数不同，标量函数返回单一值，**表值函数**（**TVF**）则是返回一个表作为结果的函数。对于发布到 Unity Catalog 中目录和模式的
    DLT 管道，Databricks 数据智能平台提供了一种特殊的 TVF，名为 **event_log()**，用于查询有关特定 DLT 管道的全面信息。**event_log()**
    函数可以接受两个参数之一作为输入：管道数据集的完全限定表名或管道 ID 作为参数。
- en: '![Figure 10.4 – The event_log() TVF returns a list of events that occurred](img/B22011_10_004.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4 – **event_log()** TVF 返回发生的事件列表](img/B22011_10_004.jpg)'
- en: Figure 10.4 – The event_log() TVF returns a list of events that occurred
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – **event_log()** TVF 返回发生的事件列表
- en: 'The **event_log()** function will retrieve information about a given DLT pipeline,
    including the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**event_log()** 函数将检索有关给定 DLT 管道的信息，包括以下内容：'
- en: Outcomes of data quality checks (expectations)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量检查（预期结果）的结果
- en: Auditing information
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审计信息
- en: Pipeline update status
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道更新状态
- en: Data lineage information
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据血统信息
- en: 'A common approach to make it easier for data stewards to query events for a
    particular DLT pipeline is to register a view alongside the datasets for a particular
    pipeline. This allows users to conveniently reference the event log results in
    subsequent queries. The following SQL **Data Definition Language** ( **DDL** )
    statement will create a view that retrieves the event log for a DLT pipeline with
    the **my_dlt_pipeline_id** ID:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的方法是，数据管理员可以通过注册一个视图与特定管道的数据集一起，使查询特定 DLT 管道事件变得更加容易。这允许用户在后续查询中方便地引用事件日志结果。以下
    SQL **数据定义语言** (**DDL**) 语句将创建一个视图，用于检索 ID 为 **my_dlt_pipeline_id** 的 DLT 管道的事件日志：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Sometimes, the event log for a particular pipeline can grow too large, making
    it difficult for data stewards to quickly summarize the latest status updates.
    Instead, data teams can narrow the event log feed even further to a particular
    dataset within a DLT pipeline. For example, data teams can create a view on top
    o f a specific dataset to capture all the events using the **table()** function
    and provide a fully-qualified table name as an argument to the function. The following
    SQL DDL statement will create a view that retrieves the event log for a dataset
    called **my_gold_table** :'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，特定管道的事件日志可能会增长得太大，使得数据管理员很难快速总结最新的状态更新。相反，数据团队可以进一步缩小事件日志的范围，聚焦于 DLT 管道中的特定数据集。例如，数据团队可以在某个特定数据集上创建视图，通过
    **table()** 函数捕获所有事件，并提供完全限定的表名作为函数的参数。以下 SQL DDL 语句将创建一个视图，用于检索名为 **my_gold_table**
    的数据集的事件日志：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The **event_log()** TVF function provides data teams with great visibility into
    the actions performed on a particular DLT pipeline and dataset making it easy
    to implement end-to-end observability and auditability.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**event_log()** TVF 函数为数据团队提供了对特定 DLT 管道和数据集上执行的操作的良好可见性，使得实现端到端的可观察性和可审计性变得更加容易。'
- en: Important note
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Presently, if a DLT pipeline is configured to publish output datasets to Unity
    Catalog, then only the owner of a particular DLT pipeline can query the views.
    To share access to the event logs, the pipeline owner must save a copy of the
    event log feed to another table within Unity Catalog and grant access to other
    users or groups.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，如果一个 DLT 管道被配置为将输出数据集发布到 Unity Catalog，那么只有特定 DLT 管道的所有者可以查询这些视图。为了共享事件日志的访问权限，管道所有者必须将事件日志馈送的副本保存到
    Unity Catalog 中的另一个表中，并授予其他用户或组访问权限。
- en: Let’s look at how we might leverage the **event_log()** function to query the
    data quality events for a particular DLT pipeline.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何利用 **event_log()** 函数查询特定 DLT 管道的数据质量事件。
- en: Hands-on exercise – querying data quality events for a dataset
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践练习 – 查询数据集的数据质量事件
- en: Important note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For the following exercise, you will need to use a shared, all-purpose cluster
    or a Databricks SQL warehouse to query the event log. Furthermore, the event log
    is only available to query DLT pipelines that have been configured to store datasets
    in Unity Catalog. The event log will not be found for DLT pipelines that have
    been configured to store datasets in the legacy Hive Metastore.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下练习，您需要使用共享的通用集群或 Databricks SQL 仓库来查询事件日志。此外，事件日志仅适用于查询已配置为在 Unity Catalog
    中存储数据集的 DLT 管道。对于已配置为在传统 Hive Metastore 中存储数据集的 DLT 管道，将找不到事件日志。
- en: Data quality metrics are stored in the event log as a serialized JSON string.
    We’ll need to parse the JSON string into a different data structure so that we
    can easily query data quality events from the event log. Let’s use the **from_json()**
    SQL function to parse the serialized JSON string for our data quality expectations.
    We’ll need to specify a schema as an argument to instruct Spark how to parse the
    JSON string into a deserialized data structure – specifically, an array of structs
    that contain information about the expectation name, dataset name, number of passing
    records, and number of failing records. Lastly, we’ll use the **explode()** SQL
    function to transform the array of expectation structs into a new row for each
    expectation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量指标作为序列化的 JSON 字符串存储在事件日志中。我们需要将 JSON 字符串解析成不同的数据结构，以便能够方便地查询事件日志中的数据质量事件。我们将使用
    **from_json()** SQL 函数解析序列化的 JSON 字符串，以满足我们的数据质量预期。我们需要指定一个 schema 作为参数，指示 Spark
    如何将 JSON 字符串解析为反序列化的数据结构——具体来说，是一个包含期望名称、数据集名称、通过记录数和失败记录数的结构体数组。最后，我们将使用 **explode()**
    SQL 函数，将期望结构体数组转换为每个期望的新行。
- en: 'We can leverage the previously defined views to monitor the ongoing data quality
    of the datasets within our DLT pipeline. Let’s create another view pertaining
    to the data quality of our DLT pipeline:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用之前定义的视图来监控我们 DLT 管道中数据集的持续数据质量。让我们创建另一个与 DLT 管道数据质量相关的视图：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Several common questions that are posed by data teams include: “How many records
    were processed?”, “How many records failed data quality validation?”, or “What
    was the percentage of passing records versus failing records?”. Let’s take the
    previous example a step further and summarize the high-level data quality metrics
    per dataset in our pipeline. Let’s count the total number of rows having an expectation
    applied, as well as the percentage of passing records versus failing records for
    each of the datasets in our DLT pipeline:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据团队常常提出一些常见问题，比如：“处理了多少条记录？”，“有多少条记录未通过数据质量验证？”或者“通过记录与未通过记录的比例是多少？”。让我们进一步分析前面的例子，并总结一下我们管道中每个数据集的高层数据质量指标。我们来统计一下应用了预期值的行数，以及每个数据集中通过记录与未通过记录的百分比：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We get the following output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![Figure 10.5 – Events captured in the DLT event log](img/B22011_10_005.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5 – DLT 事件日志中捕获的事件](img/B22011_10_005.jpg)'
- en: Figure 10.5 – Events captured in the DLT event log
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – DLT 事件日志中捕获的事件
- en: As you can see, the **event_log()** function makes it simple for data teams
    to query comprehensive information regarding a given DLT pipeline. Not only can
    data teams query the status of a pipeline update but they can also query the status
    of the quality data landing into their lakehouse. Still, data teams need a way
    to automate the notification of failing data quality checks at runtime, as is
    the scenario when the data accuracy of downstream reports is critical to the business.
    Let’s look closer at this in the following section.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，**event_log()** 函数使数据团队能够轻松查询有关给定 DLT 管道的全面信息。数据团队不仅可以查询管道更新的状态，还可以查询质量数据是否已成功地导入到湖仓。然而，数据团队仍然需要一种方式，在运行时自动通知数据质量检查失败的情况，尤其是在下游报告的数据准确性对业务至关重要时。我们将在接下来的部分中更详细地探讨这个问题。
- en: Data quality monitoring
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据质量监控
- en: Ongoing monitoring of the data quality of datasets within your lakehouse is
    critical for the success of business-critical data applications deployed to production.
    Take, for example, the impact that a sudden ingestion of null values on a joined
    column might have on downstream reports that rely on joining together upstream
    datasets. Suddenly, **business intelligence** ( **BI** ) reports might refresh,
    but the data may appear stale or outdated. By automatically detecting data quality
    issues as soon as they arise, your data team can be alerted of potential issues
    and take immediate action to intervene and correct possible data corruption or
    even data loss.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 持续监控你湖仓中数据集的数据质量，对于成功部署到生产环境的业务关键型数据应用至关重要。例如，假设某个关联列突然注入了空值，这可能会影响到依赖于上游数据集连接的下游报告。突然间，**商业智能**（**BI**）报告可能会刷新，但数据可能显得过时或不准确。通过在问题出现时自动检测数据质量问题，数据团队可以及时收到潜在问题的警报，并立即采取措施干预，修正可能的数据损坏，甚至数据丢失。
- en: '![Figure 10.6 – Detecting issues early is important to ensure the quality of
    downstream processes](img/B22011_10_006.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – 及早检测问题对确保下游流程的质量至关重要](img/B22011_10_006.jpg)'
- en: Figure 10.6 – Detecting issues early is important to ensure the quality of downstream
    processes
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 及早检测问题对确保下游流程的质量至关重要
- en: Introducing Lakehouse Monitoring
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入湖仓监控
- en: Lakehouse Monitoring, a recent feature of the Databricks Data Intelligence Platform,
    gives data teams the ability to track and monitor the data quality of data and
    other assets in the lakehouse. Data teams can automatically measure the statistical
    distribution of data across columns, number of null values, minimum, maximum,
    median column values, and other statistical properties. With Lakehouse Monitoring,
    data teams can automatically detect major problems in datasets such as data skews
    or missing values, and alert team members of issues so that they can take appropriate
    action.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Lakehouse Monitoring，作为 Databricks 数据智能平台的一个新功能，赋予数据团队跟踪和监控湖仓中数据及其他资产数据质量的能力。数据团队可以自动测量列之间的数据统计分布、空值数量、最小值、最大值、中位数值以及其他统计属性。通过
    Lakehouse Monitoring，数据团队能够自动检测数据集中的重大问题，如数据偏斜或缺失值，并提醒团队成员关注问题，以便他们采取适当措施。
- en: Lakehouse Monitoring is most useful when used to monitor the data quality of
    Delta tables, views, materialized views, and streaming tables. It can even be
    used in **Machine Learning** ( **ML** ) pipelines, measuring the statistical summaries
    of datasets and triggering alert notifications as soon as data drift is detected.
    Furthermore, Lakehouse Monitoring can be customized to be fine- or coarse-grained
    in the monitoring metrics.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 湖仓监控在监控 Delta 表、视图、物化视图和流表的数据质量时最为有效。它甚至可以在**机器学习**（**ML**）管道中使用，测量数据集的统计摘要，并在检测到数据漂移时触发警报通知。此外，湖仓监控可以根据监控度量的粒度需求进行精细或粗粒度定制。
- en: Lakehouse Monitoring begins with the creation of a monitor object, which is
    then attached to a data asset such as a Delta table in your lakehouse. Behind
    the scenes, the monitor object will create two additional tables inside your lakehouse
    to capture statistical measures of the corresponding Delta table or other data
    assets.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 湖仓监控从创建一个监控对象开始，监控对象随后将附加到湖仓中的一个数据资产（例如 Delta 表）。在后台，监控对象将创建两个额外的表，用于捕捉对应 Delta
    表或其他数据资产的统计度量。
- en: The monitoring tables are then used to power a Dashboard, which can be used
    by data teams and other stakeholders to get a view into the real-time data insights
    of the quality of your data in the l akehouse.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，监控表将被用于驱动仪表板，数据团队和其他相关方可以使用该仪表板查看湖仓中数据质量的实时数据洞察。
- en: '![Figure 10.7 – A lakehouse monitor will create two metrics tables for the
    monitored data asset](img/B22011_10_007.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7 – 一个湖仓监控器将为监控的数据资产创建两个度量表](img/B22011_10_007.jpg)'
- en: Figure 10.7 – A lakehouse monitor will create two metrics tables for the monitored
    data asset
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 一个湖仓监控器将为监控的数据资产创建两个度量表。
- en: 'A lakehouse monitor can be configured to measure different aspects of a data
    asset, which is also referred to as a profile type. There are three *monitor profile
    types* that can be created:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 湖仓监控器可以配置为衡量数据资产的不同方面，这也被称为配置文件类型。可以创建三种*监控配置文件类型*：
- en: '**Snapshot** : This is a generic, yet robust monitor. It’s useful to monitor
    data quality and other metrics of a table.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快照**：这是一个通用但强大的监控工具。它用于监控表的数据质量和其他度量指标。'
- en: '**Time series** : It’s useful for time series datasets. It’s used to monitor
    the data quality over time period windows.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列**：它适用于时间序列数据集。它用于监控数据在时间段窗口内的质量变化。'
- en: '**Inference** : It’s useful to compare the quality of an ML model inference
    versus the input over a window of time periods.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推断**：将机器学习模型推断的质量与输入在一段时间内的变化进行比较是非常有用的。'
- en: In this chapter, we’ll only be covering the time series and snapshot types.
    Discussing inference is out of the scope of this book, but you are encouraged
    to explore how Lakehouse Monitoring can be helpful for ML use cases ( [https://docs.databricks.com/en/lakehouse-monitoring/fairness-bias.html](https://docs.databricks.com/en/lakehouse-monitoring/fairness-bias.html)
    ).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本章仅覆盖时间序列和快照类型。推断的讨论超出了本书的范围，但我们鼓励你探索湖仓监控如何对机器学习用例有所帮助（[https://docs.databricks.com/en/lakehouse-monitoring/fairness-bias.html](https://docs.databricks.com/en/lakehouse-monitoring/fairness-bias.html)）。
- en: Monitors can also be created that compare the statistical metrics of a table
    versus a baseline table. This can be useful in scenarios such as comparing the
    relative humidity of smart thermostat devices for this week as compared to last
    week, or comparing the number of recorded sales in a particular dataset for a
    monthly sales report versus last month’s report, for example.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以创建比较表的统计指标与基准表的监控器。例如，可以用于比较智能恒温器设备本周的相对湿度与上周的湿度，或者将某个数据集的录入销售数量与上月的销售报告进行比较。
- en: Let’s look at a practical example of using a lakehouse monitor in a l akehouse.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个实际的例子，看看如何在湖仓中使用湖仓监控器。
- en: Hands-on exercise – creating a lakehouse monitor
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实操练习 – 创建一个湖仓监控器
- en: In this hands-on exercise, we’re going to create a lakehouse monitor for measuring
    the data quality of a target Delta table. Although our Delta table does contain
    timestamp information, we’ll choose a *snapshot profile* to monitor the data quality
    of a target Delta table in our lakehouse. Recall that the snapshot profile is
    a generic lakehouse monitor that also proves to be quite versatile, as mentioned
    earlier. The snapshot profiler will allow us to measure standard summary metrics
    about our dataset or insert custom business calculations around the data quality.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个动手练习中，我们将创建一个 lakehouse 监控工具，用于衡量目标 Delta 表的数据质量。虽然我们的 Delta 表确实包含时间戳信息，但我们将选择一个
    *快照配置文件* 来监控我们 lakehouse 中目标 Delta 表的数据质量。回想一下，快照配置文件是一个通用的 lakehouse 监控工具，正如前面所提到的，它还具有相当大的灵活性。快照分析器将允许我们衡量数据集的标准总结性指标，或者围绕数据质量插入自定义的业务计算。
- en: Like many resources in the Databricks Data Intelligence Platform, there are
    a variety of ways that you can create a new lakehouse monitor. For example, you
    can use the Databricks UI, the Databricks REST API, the Databricks CLI (covered
    in [*Chapter 9*](B22011_09.xhtml#_idTextAnchor222) ), or automation tools such
    as Terraform, to name a few. Perhaps the simplest mechanism for creating a new
    monitor is through the UI. In this hands-on exercise, we’re going to use the Databricks
    UI to create the lakehouse monitor. This is a great way to get started experimenting
    with Lakehouse Monitoring and with different data quality metrics to measure your
    datasets. However, it’s recommended in production scenarios to migrate your lakehouse
    monitors to an automated build tool such as **Databricks Asset Bundles** ( **DABs**
    ) (covered in [*Chapter 9*](B22011_09.xhtml#_idTextAnchor222) ) or Terraform (covered
    in [*Chapter 8*](B22011_08.xhtml#_idTextAnchor185) ).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Databricks 数据智能平台中的许多资源一样，您可以通过多种方式创建新的 lakehouse 监控工具。例如，您可以使用 Databricks
    UI、Databricks REST API、Databricks CLI（详见 [*第9章*](B22011_09.xhtml#_idTextAnchor222)），或者像
    Terraform 这样的自动化工具，等等。也许最简单的创建新监控工具的方式是通过 UI。在这个动手练习中，我们将使用 Databricks UI 来创建
    lakehouse 监控工具。这是开始实验 Lakehouse 监控和不同数据质量度量的绝佳方法，用于评估数据集。然而，建议在生产环境中将您的 lakehouse
    监控工具迁移到自动化构建工具，如 **Databricks 资产包（DABs）**（详见 [*第9章*](B22011_09.xhtml#_idTextAnchor222)）或
    Terraform（详见 [*第8章*](B22011_08.xhtml#_idTextAnchor185)）。
- en: If you haven’t done so already, you can clone the accompanying code resources
    for this chapter at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10)
    .
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，可以在 [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10)
    下载并克隆本章的相关代码资源。
- en: The first step is to generate a target Delta table, which we would like to monitor
    the data quality. Clone or import the data generator notebook or create a new
    notebook with the following code generator source code.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是生成一个目标 Delta 表，我们希望监控其数据质量。克隆或导入数据生成器笔记本，或创建一个新的笔记本并使用以下代码生成器源代码。
- en: 'In the first cell of the notebook, we’ll leverage the **%pip** magic command
    to download and install the **dbldatagen** Python library, which is used to generate
    sample data:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本的第一个单元格中，我们将利用 **%pip** 魔法命令来下载并安装 **dbldatagen** Python 库，用于生成样本数据：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we’ll define a helper function for generating a synthetic dataset containing
    smart thermostat readings over time:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个辅助函数，用于生成一个包含智能恒温器读数的合成数据集，记录随时间变化的情况：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we’ll save the newly created dataset as a Delta table in Unity Catalog:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将把新创建的数据集保存为 Unity Catalog 中的 Delta 表：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that our Delta table has been created in our lakehouse, let’s use the UI
    in the Catalog Explo rer to create a new monitor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们的 Delta 表已经在 lakehouse 中创建完成，让我们在 Catalog Explorer 中使用 UI 来创建一个新的监控工具。
- en: From the left-side navigation bar, click on the Catalog Explorer icon. Next,
    navigate to the catalog created for this chapter by expanding the list of catalogs
    or using the **Search** field to filter the results. Click on the schema that
    was created for this chapter. Finally, click on the Delta table that was created
    earlier by our data generator notebook. Click on the data quality tab that is
    appropriately titled **Quality** .
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧导航栏中，点击“目录浏览器”图标。接下来，通过展开目录列表或使用**搜索**字段过滤结果，导航到为本章创建的目录。点击为本章创建的架构。最后，点击之前由我们的数据生成笔记本创建的
    Delta 表。点击标题为**质量**的数据质量标签。
- en: '![Figure 10.8 – A new monitor can be created directly from the Databricks UI](img/B22011_10_008.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.8 – 可以直接从 Databricks 用户界面创建新的监控器](img/B22011_10_008.jpg)'
- en: Figure 10.8 – A new monitor can be created directly from the Databricks UI
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – 可以直接从 Databricks 用户界面创建新的监控器
- en: Next, click on the **Get started** button to begin creating a new monitor. A
    pop-up dialog will open, prompting you to select the profile type for the monitor,
    as well as advanced configuration options such as the schedule, notification delivery,
    and workspace directory for storing the generated dashboard.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，点击**开始使用**按钮，开始创建新的监控器。一个弹出对话框将打开，提示您选择监控器的配置文件类型，以及一些高级配置选项，如调度、通知传递和存储生成的仪表板的工作区目录。
- en: Click the dropdown for the profile type and select the option for generating
    a snapshot profile.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 点击下拉菜单选择配置文件类型，并选择生成快照配置文件的选项。
- en: Next, click on the **Advanced Options** section to expand the dialog form. The
    UI will allow users to capture dataset metrics, either manually or by defining
    a cron schedule for executing the metrics calculations on a repeated schedule.
    You’ll notice that the dialog provides the flexibility to define the schedule
    using a traditional cron syntax, or by selecting the date and time drop-down menus
    in the dialog form. For this hands-on exercise, we’ll choose the former option
    and refresh the monitoring metrics manually through the click of a button.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，点击**高级选项**部分以展开对话框表单。用户界面将允许用户捕获数据集指标，可以选择手动执行或定义一个 cron 调度来定期执行指标计算。您会注意到，该对话框提供了使用传统
    cron 语法定义调度的灵活性，或者通过选择对话框表单中的日期和时间下拉菜单来定义调度。对于本次实践，我们将选择前一种方式，并通过点击按钮手动刷新监控指标。
- en: Optionally, you can choose to have notifications about the success or failure
    of monitoring metrics calculations sent via email to a list of email recipients.
    You can add up to five email addresses for notifications to be delivered to. Ensure
    that your user email address is listed in the **Notifications** section and that
    the checkbox is checked to receive a notification for failures during the metrics
    collection.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，您可以选择将关于监控指标计算成功或失败的通知通过电子邮件发送给一组电子邮件接收者。您最多可以添加五个电子邮件地址，以便通知能够送达。确保您的用户电子邮件地址列在**通知**部分，并且勾选复选框以接收关于指标收集失败的通知。
- en: If you recall from earlier, a lakehouse monitor will create two metrics tables.
    We’ll need to provide a location in Unity Catalog to store these metrics tables.
    Under the **Metrics** section, add the catalog and schema name created for this
    chapter’s hands-on exercise. For example, enter **chp10.monitor_demo** .
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得之前提到的，湖仓监控器将创建两个指标表。我们需要在 Unity Catalog 中提供存储这些指标表的位置。在**指标**部分，添加为本章实践创建的目录和架构名称。例如，输入**chp10.monitor_demo**。
- en: The last item that we need to specify is a workspace location for storing the
    generated dashboard for our lakehouse monitor. By default, the generated assets
    will be stored under the user’s home directory, for example, **/Users/<user_email_address>/databricks_lakehouse_monitoring**
    . For this hands-on exercise, we’ll accept the default location.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要指定的最后一项是存储生成的湖仓监控仪表板的工作区位置。默认情况下，生成的资源将存储在用户的主目录下，例如**/Users/<user_email_address>/databricks_lakehouse_monitoring**。在本次实践中，我们将接受默认位置。
- en: We’re ready to create our monitor! Click the **Create** button to create the
    lakehouse monitor for our Delta table.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好创建监控器了！点击**创建**按钮，以为我们的 Delta 表创建湖仓监控器。
- en: Since we haven’t configured a schedule for our lakehouse monitor, we’ll need
    to manually execute a metrics collection. Back in the Catalog Explorer, under
    the **Quality** tab of our Delta table, click on the **Refresh metrics** button
    to manually trigger a metrics collection.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们尚未为湖仓监控配置调度，因此我们需要手动执行度量收集。在 Catalog Explorer 中，在我们 Delta 表的**质量**标签下，点击**刷新度量**按钮以手动触发度量收集。
- en: '![Figure 10.9 – Monitoring metrics can be manually triggered from the Catalog
    Explorer UI](img/B22011_10_009.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.9 – 监控度量可以通过 Catalog Explorer UI 手动触发](img/B22011_10_009.jpg)'
- en: Figure 10.9 – Monitoring metrics can be manually triggered from the Catalog
    Explorer UI
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 – 监控度量可以通过 Catalog Explorer UI 手动触发
- en: An update of the table metrics will be triggered to execute and will take up
    to a few minutes to complete. Once the update has completed, click the **View
    dashboard** button to view the metrics captured. Congratulations! You’ve created
    your first lakehouse monitor and you’re well on your way to implementing robust
    and automated data quality observability for your data team.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表格度量的更新将会被触发并执行，可能需要几分钟才能完成。更新完成后，点击**查看仪表板**按钮查看捕获的度量数据。恭喜你！你已经创建了第一个湖仓监控，并且你正在稳步推进，为数据团队实现强大且自动化的数据质量可观察性。
- en: Now that we have an idea of how to alert our team members when production issues
    arise, let’s turn our attention to a few approaches to resolve failures in production
    deployments.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何在生产问题出现时提醒我们的团队成员，让我们关注一些解决生产部署中失败问题的方法。
- en: Best practices for production failure resolution
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产故障解决的最佳实践
- en: 'The DLT framework was designed with failure resolution in mind. For example,
    DLT will automatically respond to three types of common pipeline failures:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: DLT 框架是针对故障解决设计的。例如，DLT 会自动响应三种常见的管道故障类型：
- en: Databricks Runtime regressions (covered in [*Chapter 2*](B22011_02.xhtml#_idTextAnchor052)
    )
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks Runtime 回归（详见 [*第 2 章*](B22011_02.xhtml#_idTextAnchor052)）
- en: Update processing failures
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新处理失败
- en: Data transaction failure
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据事务失败
- en: Let’s look at update failures and data transaction failures in greater detail.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下更新失败和数据事务失败。
- en: Handling pipeline update failures
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理管道更新失败
- en: The DLT framework was designed with robust error handling in mind. During a
    pipeline update, the framework will attempt to apply the most recent updates to
    tables defined in the dataflow graph. If a processing error occurs, the framework
    will classify the error as either a retriable error or a non-retriable error.
    A retriable error means that the framework has classified the runtime error as
    likely an issue caused by the current set of conditions. For example, a system
    error would not be considered a retriable error, since it relates to the runtime
    environment that execution retries will not solve. However, a network timeout
    would be a retriable error, since it could be impacted by the temporary set of
    network environment conditions. By default, the DLT framework retries a pipeline
    update twice if it detects a *retriable* error.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: DLT 框架是针对强大的错误处理设计的。在管道更新期间，框架会尝试将最新的更新应用到数据流图中定义的表。如果发生处理错误，框架会将错误分类为可重试错误或不可重试错误。可重试错误意味着框架将运行时错误分类为可能由当前条件集引起的问题。例如，系统错误不会被认为是可重试错误，因为它与运行时环境有关，而执行重试无法解决。然而，网络超时会被视为可重试错误，因为它可能受到临时网络环境条件的影响。默认情况下，如果检测到*可重试*错误，DLT
    框架会对管道更新进行两次重试。
- en: Recovering from table transaction failure
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从表事务失败中恢复
- en: Due to the nature of the Delta Lake transaction log, changes to a dataset are
    atomic, meaning that they can only happen if a table transaction (such as a **Data
    Manipulation Language** ( **DML** ) statement) is committed to the transaction
    log. As a result, if a transaction fails in the middle of its execution, then
    the entire transaction is abandoned, thereby preventing the dataset from entering
    a non-deterministic state requiring data teams to intervene and manually reverse
    the data changes.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Delta Lake 事务日志的特性，数据集的更改是原子性的，这意味着它们只能在表事务（如**数据操作语言**（**DML**）语句）提交到事务日志时发生。因此，如果一个事务在执行过程中失败，则整个事务会被放弃，从而防止数据集进入需要数据团队介入并手动撤销数据更改的非确定性状态。
- en: Now that we understand how to handle pipeline failures in production, let’s
    cement the topics from this chapter through a real-world example.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了如何处理生产环境中的管道故障，让我们通过一个真实世界的示例来巩固本章的主题。
- en: Hands-on exercise – setting up a webhook alert when a job runs longer than expected
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践练习 – 设置Webhook警报，当作业运行时间超过预期时进行通知
- en: In this hands-on exercise, we’ll be creating a custom HTTP webhook that will
    notify an HTTP endpoint about the timeout status of a scheduled job in Databricks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实践练习中，我们将创建一个自定义HTTP Webhook，当Databricks中的定时作业超时时，会向HTTP端点发送通知。
- en: A webhook alert is a notification mechanism in the Databricks Data Intelligence
    Platform that enables data teams to monitor their data pipeline by automatically
    publishing the outcome of a particular job execution run. For example, you can
    receive notifications about the successful run, execution state, and run failures.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Webhook警报是Databricks数据智能平台中的一种通知机制，它使数据团队能够通过自动发布特定作业执行结果，来监控其数据管道。例如，您可以收到关于作业成功运行、执行状态和运行失败的通知。
- en: Why are we using a workflow rather than a DLT pipeline directly?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们使用工作流而不是直接使用DLT管道？
- en: In practice, a DLT pipeline will often be just one of many dependencies in a
    complete data product. Databricks workflows are a popular orchestration tool that
    can prepare dependencies, run one or more DLT pipelines, and execute downstream
    tasks as well. In this exercise, we’ll be configuring notifications from a Databricks
    workflow, as opposed to notifications directly from a DLT pipeline, to simulate
    a typical production scenario.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，DLT管道通常只是一个完整数据产品中的许多依赖项之一。Databricks工作流是一个流行的编排工具，能够准备依赖项、运行一个或多个DLT管道，并执行下游任务。在本次练习中，我们将配置来自Databricks工作流的通知，而不是直接从DLT管道中获取通知，以模拟典型的生产场景。
- en: Let’s start by navigating to your Databricks workspace and logging into your
    workspace. Next, let’s create a new workflow. We’ll start by navigating to the
    Workflow UI by clicking on the workflows icon from the workspace navigation bar
    on the left-hand side. Give the workflow a meaningful name, such as **Production**
    **Monitoring Demo** .
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先进入您的Databricks工作区并登录。接下来，创建一个新的工作流。我们可以通过点击工作区导航栏左侧的工作流图标，进入工作流UI。为工作流命名时，可以选择一个有意义的名称，例如**生产**
    **监控演示**。
- en: If you haven’t done so already, you can download the sample notebooks for this
    chapter’s exercise at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10)
    . We’ll be using the IoT device data generator notebook, titled **04a-IoT Device
    Data Generator.py** , and the IoT Device DLT pipeline definition notebook, which
    is titled **04b-IoT Device** **Data Pipeline.py** .
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有下载，可以在[https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10)下载本章练习的示例笔记本。我们将使用名为**04a-IoT
    设备数据生成器.py**的物联网设备数据生成器笔记本和名为**04b-IoT 设备数据管道.py**的物联网设备DLT管道定义笔记本。
- en: In the Workflow UI, create a new workflow with two tasks. The first task will
    prepare an input dataset using the **04a-IoT Device Data Generator.py** notebook;
    the second task will execute a DLT pipeline that reads the generated data using
    the **04b-IoT Device Data** **Pipeline.py** notebook.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作流UI中，创建一个包含两个任务的新工作流。第一个任务将使用**04a-IoT 设备数据生成器.py**笔记本准备输入数据集；第二个任务将使用**04b-IoT
    设备数据管道.py**笔记本执行读取生成数据的DLT管道。
- en: "![Figure 10.10 – The \uFEFFworkflow will generate IoT device data and execute\
    \ a DLT pipeline update](img/B22011_10_010.jpg)"
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.10 – 工作流将生成物联网设备数据并执行DLT管道更新](img/B22011_10_010.jpg)'
- en: Figure 10.10 – The workflow will generate IoT device data and execute a DLT
    pipeline update
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 – 工作流将生成物联网设备数据并执行DLT管道更新
- en: Now that our workflow has been created, let’s imagine, for example, that our
    pipeline is taking longer than expected to execute. Wouldn’t it be helpful to
    be notified if there are potential processing delays, so that your data team can
    investigate immediately or prevent a long-running job from running up a large
    cloud bill due to a processing mishap?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的工作流已经创建完成，假设我们的管道执行时间比预期的要长。若出现潜在的处理延迟，是否能及时收到通知，以便您的数据团队立即进行调查，或者防止由于处理错误导致长时间运行的任务产生巨额云计算费用，这不就非常有帮助吗？
- en: Fortunately, the Databricks Data Intelligence Platform makes it simple to configure
    this type of notification. Let’s create a timeout threshold for our workflow.
    This will automatically notify our HTTP webhook endpoint that our workflow is
    taking longer than expected to execute. Once our workflow has exceeded this timeout
    threshold, the current execution run is stopped, and the run is marked as *failed*
    . We would like to be notified of this type of failure scenario.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Databricks数据智能平台使得配置这种类型的通知变得简单。让我们为工作流创建一个超时阈值。这将自动通知我们的HTTP webhook端点，告知我们的工作流执行时间超过了预期。一旦工作流超过了这个超时阈值，当前的执行将被停止，并且该执行被标记为*失败*。我们希望在这种失败场景发生时收到通知。
- en: From the Workflow UI, click on the newly created workflow, **Production Monitoring
    Demo** , to reveal the details. Under the **Job notifications** section, click
    on the **Add metric thresholds** button to add a new run duration threshold. Let’s
    add a maximum duration of 120 minutes to the maximum threshold. Click the **Save**
    button. Next, click on the **+ Add notification** button to add a new notification.
    Expand the **Destination** drop-down menu to reveal the choices and select **+
    Add new system destination** . A new browser tab will open, presenting the workspace
    administration settings for your Databricks workspace. Under the **Notifications**
    section, click on the **Manage** button. Click the **Add destination** button.
    Select **Webhook** for the destination type, provide a meaningful name for the
    destination, enter the endpoint URL for which the notifications should be sent,
    and enter the username and password information if your endpoint uses basic HTTP
    authentication. Click the **Create** button to create the Webhook destination.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作流UI中，点击新创建的工作流**生产监控演示**，以查看详细信息。在**作业通知**部分，点击**添加度量阈值**按钮，添加一个新的运行时长阈值。让我们将最大时长设置为120分钟，然后点击**保存**按钮。接下来，点击**+
    添加通知**按钮，添加一个新的通知。展开**目标**下拉菜单，选择**+ 添加新系统目标**。一个新的浏览器标签页将打开，显示Databricks工作区的工作区管理设置。在**通知**部分，点击**管理**按钮。点击**添加目标**按钮。选择**Webhook**作为目标类型，为目标提供一个有意义的名称，输入通知应发送的端点URL，并在端点使用基本HTTP认证时输入用户名和密码信息。点击**创建**按钮以创建Webhook目标。
- en: '![Figure 10.11 – Creating a new Webhook destination](img/B22011_10_011.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图10.11 – 创建一个新的Webhook目标](img/B22011_10_011.jpg)'
- en: Figure 10.11 – Creating a new Webhook destination
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11 – 创建一个新的Webhook目标
- en: Finally, click the **Save** button to finalize the metric threshold notification.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，点击**保存**按钮以最终确定度量阈值通知。
- en: "![Figure 10.12 – Execution duration thresholds can be set on a \uFEFFworkflow’s\
    \ tasks](img/B22011_10_012.jpg)"
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图10.12 – 可以为工作流的任务设置执行时长阈值](img/B22011_10_012.jpg)'
- en: Figure 10.12 – Execution duration thresholds can be set on a workflow’s tasks
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12 – 可以为工作流的任务设置执行时长阈值
- en: Now that we’ve established a run duration threshold, whenever our workflow runs
    for longer than 120 minutes, our workflow will be stopped and a notification message
    with a status message of **Timed Out** will be sent to our HTTP webhook destination.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设定了运行时长阈值，每当我们的工作流运行超过120分钟时，工作流将会停止，并且会向我们的HTTP webhook目标发送一个状态为**超时**的通知消息。
- en: Congratulations! You’ve now automated the monitoring of your data pipelines
    in production, allowing your team to be automatically notified whenever failure
    conditions arise. This means that your teams can step in and correct data processing
    issues as soon as they happen and minimize potential downtime.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！现在你已经自动化了生产环境中数据管道的监控，当出现故障条件时，团队将会自动收到通知。这意味着你的团队可以在问题发生时立即介入并修正数据处理问题，从而最小化潜在的停机时间。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered several techniques for implementing pipeline and
    data quality observability so that data teams can react as soon as problems arise
    and thwart major downstream disruptions. One of the major keys to becoming a successful
    data team is being able to react to issues quickly. We saw how alert notifications
    are built into many aspects of the Databricks Data Intelligence Platform and how
    we can configure different types of alert destinations to send notifications when
    conditions are not met.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了几种实现管道和数据质量可观测性的技术，以便数据团队能够在问题出现时迅速做出反应，避免下游大规模的中断。成为一个成功的数据团队的关键之一就是能够快速应对问题。我们看到了Databricks数据智能平台在许多方面内置了警报通知，并且我们可以配置不同类型的警报目标，以在条件不满足时发送通知。
- en: We covered monitoring capabilities built into the Databricks platform, such
    as the pipeline event log that makes it easy for pipeline owners to query the
    data pipeline’s health, auditability, and performance, as well as data quality,
    in real time. We also saw how Lakehouse Monitoring is a robust and versatile feature
    that allows data teams to automatically monitor the statistical metrics of datasets
    and notify team members when thresholds have been crossed. We also covered techniques
    to evaluate data quality throughout the pipeline, preventing downstream errors
    and inaccuracies.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了Databricks平台内置的监控功能，例如管道事件日志，它使得管道拥有者可以轻松查询数据管道的健康状况、可审计性、性能，以及实时的数据质量。我们还看到，湖仓监控是一个强大且多功能的功能，它允许数据团队自动监控数据集的统计指标，并在阈值被突破时通知团队成员。我们还介绍了评估数据质量的技术，以确保整个管道中没有下游错误和不准确的情况。
- en: Lastly, we concluded the chapter with a real-world exercise for automatically
    alerting data teams in the event of a real and all-too-common problem – when a
    scheduled job runs for longer than expected.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过一个实际案例来结束这一章，演示了如何在遇到一个现实且非常常见的问题时，自动提醒数据团队——当一个计划任务运行超时。
- en: Congratulations on reaching the end of this book! Thank you for taking this
    journey with me through each chapter. We’ve covered a lot of topics, but you should
    feel proud of your accomplishments thus far. By now, you should have a well-rounded
    foundation of the lakehouse on which you can continue to build. In fact, I hope
    that this book has filled you with inspiration to continue your lakehouse journey
    and to build modern data applications that do great things. I wish you the best
    of luck and encourage you to keep learning!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了本书的阅读！感谢你与我一起通过每一章进行这段旅程。我们已经涵盖了许多话题，但到目前为止，你应该为自己的成就感到骄傲。到现在为止，你应该已经建立了一个扎实的湖仓基础，接下来你可以继续在其上构建。实际上，我希望这本书能给你带来灵感，继续你的湖仓之旅，并构建出具有重大意义的现代数据应用。祝你好运，鼓励你保持学习！
