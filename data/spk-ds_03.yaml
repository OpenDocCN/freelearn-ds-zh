- en: Chapter 3.  Introduction to DataFrames
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章：DataFrame 介绍
- en: To solve any real-world big data analytics problem, access to an efficient and
    scalable computing system is definitely mandatory. However, if the computing power
    is not accessible to the target users in a way that's easy and familiar to them,
    it will barely make any sense. Interactive data analysis gets easier with datasets
    that can be represented as named columns, which was not the case with plain RDDs.
    So, the need for a schema-based approach to represent data in a standardized way
    was the inspiration behind DataFrames.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决任何实际的“大数据分析”问题，访问一个高效且可扩展的计算系统是绝对必要的。然而，如果计算能力对目标用户而言不易获取且不熟悉，那它几乎毫无意义。当数据集可以表示为命名列时，交互式数据分析会变得更简单，而这在普通的
    RDD 中并不适用。因此，采用基于架构的方式来标准化数据表示的需求，正是 DataFrame 的灵感来源。
- en: The previous chapter outlined some design aspects of Spark. We learnt how Spark
    enabled distributed data processing on distributed collections of data (RDDs)
    through in-memory computation. It covered most of the points that revealed Spark
    as a fast, efficient, and scalable computing platform. In this chapter, we will
    see how Spark introduced the DataFrame API to make data scientists feel at home
    to carry out their usual data analysis activities with ease.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章概述了 Spark 的一些设计方面。我们了解了 Spark 如何通过内存计算在分布式数据集合（RDD）上实现分布式数据处理。它涵盖了大部分揭示 Spark
    作为一个快速、高效和可扩展计算平台的要点。在本章中，我们将看到 Spark 如何引入 DataFrame API，使数据科学家能够轻松地进行他们日常的数据分析工作。
- en: 'This topic is going to serve as a foundation for many upcoming chapters and
    we strongly recommend you to understand the concepts covered in here very well.
    As a prerequisite for this chapter, a basic understanding of SQL and Spark is
    needed. The topics covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容将作为接下来许多章节的基础，我们强烈建议你充分理解这里涵盖的概念。作为本章的前提，需具备 SQL 和 Spark 的基本理解。本章涵盖的主题如下：
- en: Why DataFrames?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择 DataFrame？
- en: Spark SQL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Catalyst optimizer
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Catalyst 优化器
- en: DataFrame API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame API
- en: DataFrame basics
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame 基础
- en: RDD versus DataFrame
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD 与 DataFrame
- en: Creating DataFrames
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 DataFrame
- en: From RDDs
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 RDD
- en: From JSON
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 JSON
- en: From JDBC sources
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 JDBC 数据源
- en: From other data sources
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自其他数据源
- en: Manipulating DataFrames
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作 DataFrame
- en: Why DataFrames?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择 DataFrame？
- en: Apart from massive, scalable computing capability, big data applications also
    need a mix of a few more features, such as support for a relational system for
    interactive data analysis (simple SQL style), heterogeneous data sources, and
    different storage formats along with different processing techniques.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了强大的、可扩展的计算能力外，大数据应用还需要结合一些额外的特性，例如支持交互式数据分析的关系系统（简单的 SQL 风格）、异构数据源以及不同的存储格式和处理技术。
- en: 'Though Spark provided a functional programming API to manipulate distributed
    collections of data, it ended up with tuples (_1, _2, ...). Coding to operate
    on tuples was a little complicated and messy, and was slow at times. So, a standardized
    layer was needed, with the following characteristics:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Spark 提供了一个函数式编程 API 来操作分布式数据集合，但最终结果还是元组（_1、_2、...）。在元组上编写代码稍显复杂且凌乱，且有时速度较慢。因此，迫切需要一个标准化的层，具备以下特点：
- en: Named columns with a schema (higher-level abstraction than tuples) so that manipulating
    and tracking them would be easy
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用带有架构的命名列（比元组更高层次的抽象），使得对它们进行操作和追踪变得容易
- en: Functionality to consolidate data from various data sources such as Hive, Parquet,
    SQL Server, PostgreSQL, JSON, and also Spark's native RDDs, and unify them to
    a common format
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供将来自不同数据源的数据（如 Hive、Parquet、SQL Server、PostgreSQL、JSON，以及 Spark 原生的 RDD）整合并统一为一个公共格式的功能
- en: Ability to take advantage of built-in schemas in special file formats such as
    Avro, CSV, JSON, and so on.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够利用 Avro、CSV、JSON 等特殊文件格式中的内置架构。
- en: Support for simple relational as well as complex logical operations
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持简单的关系运算以及复杂的逻辑操作
- en: Elimination of the need to define column objects based on domain-specific tasks
    for the ML algorithms to work on, and to serve as a common data layer for all
    algorithms in MLlib
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消除了基于领域特定任务为机器学习算法定义列对象的需求，作为机器学习库（MLlib）中所有算法的公共数据层
- en: A language-independent entity that can be passed between functions of different
    languages
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个语言无关的实体，可以在不同语言的函数之间传递
- en: To address the above requirements, the DataFrame API was built as one more level
    of abstraction on top of Spark SQL.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足上述需求，DataFrame API 被构建为在 Spark SQL 之上的又一抽象层。
- en: Spark SQL
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Executing SQL queries for basic business needs is very common and almost every
    business does it using some kind of database. So Spark SQL also supports the execution
    of SQL queries written using either a basic SQL syntax or HiveQL. Spark SQL can
    also be used to read data from an existing Hive installation. Apart from these
    plain SQL operations, Spark SQL also addresses some tough problems. Designing
    complex logic through relational queries was cumbersome and almost impossible
    at times. So, Spark SQL was designed to integrate the capabilities of relational
    processing and functional programming so that complex logics can be implemented,
    optimized, and scaled on a distributed computing setup. There are basically three
    ways to interact with Spark SQL, including SQL, the DataFrame API, and the Dataset
    API. The Dataset API is an experimental layer added in Spark 1.6 at the time of
    writing this book so we will limit our discussions to DataFrames only.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 SQL 查询以满足基本业务需求是非常常见的，几乎每个企业都通过某种数据库进行此操作。所以 Spark SQL 也支持执行使用基本 SQL 语法或
    HiveQL 编写的 SQL 查询。Spark SQL 还可以用来从现有的 Hive 安装中读取数据。除了这些简单的 SQL 操作外，Spark SQL 还解决了一些棘手的问题。通过关系查询设计复杂逻辑曾经是繁琐的，甚至在某些时候几乎是不可能的。因此，Spark
    SQL 旨在将关系处理和函数式编程的能力结合起来，从而实现、优化和在分布式计算环境中扩展复杂逻辑。基本上有三种方式与 Spark SQL 交互，包括 SQL、DataFrame
    API 和 Dataset API。Dataset API 是在写本书时 Spark 1.6 中加入的实验性层，因此我们将仅限于讨论 DataFrame。
- en: Spark SQL exposes DataFrames as a higher-level API and takes care of all the
    complexities involved and also performs all the background tasks. Through the
    declarative syntax, users can focus on what the program should accomplish and
    not bother about the control flow, which will be taken care of by the Catalyst
    optimizer, built inside Spark SQL.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 将 DataFrame 显示为更高级的 API，处理所有涉及的复杂性并执行所有后台任务。通过声明式语法，用户可以专注于程序应该完成什么，而不必担心控制流，因为这将由内置于
    Spark SQL 中的 Catalyst 优化器处理。
- en: The Catalyst optimizer
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Catalyst 优化器
- en: 'The Catalyst optimizer is the fulcrum of Spark SQL and DataFrame. It is built
    with the functional programming constructs of Scala and has the following features:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst 优化器是 Spark SQL 和 DataFrame 的支点。它通过 Scala 的函数式编程构造构建，具有以下特点：
- en: 'Schema inference from various data formats:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从各种数据格式中推断模式：
- en: Spark has built-in support for JSON schema inference. Users can just create
    a table out of any JSON file by registering it as a table and simply query it
    with SQL syntaxes.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 内置支持 JSON 模式推断。用户只需将任何 JSON 文件注册为表，并通过 SQL 语法进行查询，即可创建该表。
- en: RDDs that are Scala objects; the type information is extracted from Scala's
    type system, that is, **case classes**, if they contain case classes.
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD 是 Scala 对象；类型信息通过 Scala 的类型系统提取，也就是**案例类**，如果它们包含案例类的话。
- en: RDDs that are Python objects; the type information is extracted with a different
    approach. Since Python is not statically typed and follows a dynamic type system,
    the RDD can contain multiple types. So, Spark SQL samples the dataset and infers
    the schema using an algorithm similar to JSON schema inference.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD 是 Python 对象；类型信息通过不同的方式提取。由于 Python 不是静态类型的，遵循动态类型系统，因此 RDD 可以包含多种类型。因此，Spark
    SQL 会对数据集进行采样，并使用类似于 JSON 模式推断的算法推断模式。
- en: In future, built-in support for CSV, XML, and other formats will be provided.
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来将提供对 CSV、XML 和其他格式的内置支持。
- en: 'Built-in support for a wide range of data sources and query federation for
    efficient data import:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内置支持多种数据源和查询联合，以高效导入数据：
- en: Spark has a built-in mechanism to fetch data from some external data sources
    (for example, JSON, JDBC, Parquet, MySQL, Hive, PostgreSQL, HDFS, S3, and so on)
    through query federation. It can accurately model the sourced data by using out-of-the-box
    SQL data types and other complex data types such as Struct, Union, Array, and
    so on.
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 具有内置机制，可以通过查询联合从某些外部数据源（例如 JSON、JDBC、Parquet、MySQL、Hive、PostgreSQL、HDFS、S3
    等）获取数据。它可以通过使用开箱即用的 SQL 数据类型和其他复杂数据类型，如 Struct、Union、Array 等，精确建模源数据。
- en: It also allows users to source data using the **Data Source API** from the data
    sources that are not supported out of the box (for example, CSV, Avro HBase, Cassandra,
    and so on).
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还允许用户使用**数据源 API**从 Spark 原生不支持的数据源中获取数据（例如 CSV、Avro、HBase、Cassandra 等）。
- en: Spark uses predicate pushdown (pushes filtering or aggregation into external
    storage systems) to optimize data sourcing from external systems and combine them
    to form the data pipeline.
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 使用谓词下推（将过滤或聚合操作推送到外部存储系统）来优化从外部系统获取数据，并将它们结合形成数据管道。
- en: 'Control and optimization of code generation:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码生成的控制与优化：
- en: Optimization actually happens very late in the entire execution pipeline.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化实际上发生在整个执行管道的最后阶段。
- en: 'Catalyst is designed to optimize all phases of query execution: analysis, logical
    optimization, physical planning, and code generation to compile parts of queries
    to Java bytecode.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Catalyst 被设计用来优化查询执行的所有阶段：分析、逻辑优化、物理规划和代码生成，将查询的部分内容编译为 Java 字节码。
- en: The DataFrame API
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame API
- en: Excel spreadsheets like data representation, or output from a database projection
    (select statement's output), the data representation closest to human being had
    always been a set of uniform columns with multiple rows. Such a two-dimensional
    data structure that usually has labelled rows and columns is called a DataFrame
    in some realms, such as R DataFrames and Python's Pandas DataFrames. In a DataFrame,
    typically, a single column has the same kind of data, and rows describe data points
    about that column that mean something together, be it data about a person, a purchase,
    or a baseball game outcome. You can think of it as a matrix, or a spreadsheet,
    or an RDBMS table.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Excel 表格的数据表示，或来自数据库投影（select 语句的输出），数据表示最接近人类理解的始终是由多个行和统一列组成的集合。这样的二维数据结构，通常具有带标签的行和列，在一些领域被称为
    DataFrame，例如 R 的 DataFrame 和 Python 的 Pandas DataFrame。在 DataFrame 中，通常单列包含相同类型的数据，行描述与该列相关的数据点，这些数据点一起表达某些含义，无论是关于一个人的信息、一次购买，还是一场棒球比赛的结果。你可以将它视为矩阵、电子表格或关系型数据库表。
- en: DataFrames in R and Pandas are very handy in slicing, reshaping, and analyzing
    data -essential operations in any data wrangling and data analysis workflow. This
    inspired the development of a similar concept on Spark, called DataFrames.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 和 Pandas 中，DataFrame 在切片、重塑和分析数据方面非常方便——这些是任何数据清洗和数据分析工作流中的基本操作。这启发了在 Spark
    中开发类似的概念，称为 DataFrame。
- en: DataFrame basics
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame 基础
- en: The DataFrame API was first introduced in Spark 1.3.0, released in March 2015\.
    It is a programming abstraction of Spark SQL for structured and semi-structured
    data processing. It enables developers to harness the power of the DataFrames,
    data structure through Python, Java, Scala, and R. Like RDDs, a Spark DataFrame
    is a distributed collection of records organized into named columns, similar to
    an RDBMS table or the DataFrames of R or Pandas. Unlike RDDs, however, they keep
    track of schemas and facilitate relational operations as well as procedural operations
    such as `map`. Internally, DataFrames store data in columnar format, but construct
    row objects on the fly when required by the procedural functions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API 首次在 Spark 1.3.0 版本中引入，发布于 2015 年 3 月。它是 Spark SQL 对结构化和半结构化数据处理的编程抽象。它使开发人员能够通过
    Python、Java、Scala 和 R 等语言利用 DataFrame 数据结构的强大功能。与 RDD 相似，Spark DataFrame 是一组分布式记录，按命名列组织，类似于关系型数据库管理系统（RDBMS）中的表或
    R 或 Pandas 中的 DataFrame。与 RDD 不同的是，DataFrame 会跟踪模式（schema），并促进关系型操作以及程序化操作，如 `map`。在内部，DataFrame
    以列式格式存储数据，但在需要程序化函数时，会动态构建行对象。
- en: 'The DataFrame API brings two features with it:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API 带来了两个特点：
- en: Built-in support for a variety of data formats such as Parquet, Hive, and JSON.
    Nonetheless, through Spark SQL's external data sources API, DataFrames can access
    a wide array of third-party data sources such as databases and NoSQL stores.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内置对多种数据格式的支持，如 Parquet、Hive 和 JSON。然而，通过 Spark SQL 的外部数据源 API，DataFrame 可以访问各种第三方数据源，如数据库和
    NoSQL 存储。
- en: 'A more robust and feature-rich DSL with functions designed for common tasks
    such as:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个更强大、功能丰富的 DSL，内置了许多用于常见任务的函数，如：
- en: Metadata
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据
- en: Sampling
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽样
- en: Relational data processing - project, filter, aggregation, join
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系型数据处理 - 投影、过滤、聚合、连接
- en: UDFs
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户定义函数（UDFs）
- en: The DataFrame API builds on the Spark SQL query optimizer to automatically execute
    code efficiently on a cluster of machines.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API 基于 Spark SQL 查询优化器，自动在集群的机器上高效执行代码。
- en: RDDs versus DataFrames
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDDs 与 DataFrames 的区别
- en: RDDs and DataFrames are two different types of fault-tolerant and distributed
    data abstractions provided by Spark. They are similar to an extent but greatly
    differ when it comes to implementation. Developers need to have a clear understanding
    of their differences to be able to match their requirements to the right abstraction.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 和 DataFrame 是 Spark 提供的两种不同类型的容错分布式数据抽象。它们在某些方面相似，但在实现上有很大区别。开发者需要清楚理解它们之间的差异，以便能够将需求与合适的抽象匹配。
- en: Similarities
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相似性
- en: 'The following are the similarities between RDDs and DataFrames:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 RDD 和 DataFrame 之间的相似性：
- en: Both are fault-tolerant, partitioned data abstractions in Spark
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者都是 Spark 中容错的、分区的数据抽象
- en: Both can handle disparate data sources
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者都能处理不同的数据源
- en: Both are lazily evaluated (execution happens when an output operation is performed
    on them), thereby having the ability to take the most optimized execution plan
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者都是惰性求值的（执行发生在对它们进行输出操作时），从而能够采用最优化的执行计划。
- en: 'Both APIs are available in all four languages: Scala, Python, Java, and R'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个 API 在四种语言中都可用：Scala、Python、Java 和 R。
- en: Differences
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 区别
- en: 'The following are the differences between RDDs and DataFrames:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 RDD 和 DataFrame 之间的区别：
- en: DataFrames are a higher-level abstraction than RDDs.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame 是比 RDD 更高级的抽象。
- en: The definition of RDD implies defining a **Directed Acyclic Graph** (**DAG**)
    whereas defining a DataFrame leads to the creation of an **Abstract Syntax Tree**
    (**AST**). An AST will be utilized and optimized by the Spark SQL catalyst engine.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义 RDD 意味着定义一个 **有向无环图** (**DAG**)，而定义 DataFrame 则会创建一个 **抽象语法树** (**AST**)。AST
    将由 Spark SQL catalyst 引擎使用并优化。
- en: RDD is a general data structure abstraction whereas a DataFrame is a specialized
    data structure to deal with two-dimensional, table-like data.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD 是一种通用的数据结构抽象，而 DataFrame 是专门用于处理二维表格数据的结构。
- en: The DataFrame API is actually SchemaRDD-renamed. The renaming was to signify
    that it is no longer inherited from RDD and to comfort data scientists with a
    familiar name and concept.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API 实际上是 SchemaRDD 的重命名。重命名的目的是为了表明它不再继承自 RDD，并且为了让数据科学家对这个熟悉的名称和概念感到安心。
- en: Creating DataFrames
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 DataFrame
- en: 'Spark DataFrame creation is similar to RDD creation. To get access to the DataFrame
    API, you need SQLContext or HiveContext as an entry point. In this section, we
    are going to demonstrate how to create DataFrames from various data sources, starting
    from basic code examples with in-memory collections:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrame 的创建方式类似于 RDD 的创建方式。要访问 DataFrame API，你需要 SQLContext 或 HiveContext
    作为入口点。在本节中，我们将演示如何从各种数据源创建 DataFrame，从基本的内存集合代码示例开始：
- en: '![Creating DataFrames](img/image_03_001.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![创建 DataFrame](img/image_03_001.jpg)'
- en: Creating DataFrames from RDDs
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 RDD 创建 DataFrame
- en: 'The following code creates an RDD from a list of colors followed by a collection
    of tuples containing the color name and its length. It creates a DataFrame using
    the `toDF` method to convert the RDD into a DataFrame. The `toDF` method takes
    a list of column labels as an optional argument:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码从一个包含颜色的列表创建一个 RDD，然后是包含颜色名称及其长度的元组集合。它使用 `toDF` 方法将 RDD 转换为 DataFrame。`toDF`
    方法接受一个可选的列标签列表作为参数：
- en: '**Python**:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Scala**:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see from the preceding example, the creation of a DataFrame is similar
    to that of an RDD from a developer's perspective. We created an RDD here and then
    transformed that to tuples which are then sent to the `toDF` method. Note that
    `toDF` takes a list of tuples instead of scalar elements. You need to pass tuples
    even to create single-column DataFrames. Each tuple is akin to a row. You can
    optionally label the columns; otherwise, Spark creates obscure names such as `_1`,
    `_2`. Type inference of the columns happens implicitly.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的例子可以看出，从开发者的角度来看，创建 DataFrame 与创建 RDD 类似。我们在这里创建了一个 RDD，然后将其转换为元组，这些元组接着被传递到
    `toDF` 方法中。请注意，`toDF` 接受的是元组列表，而不是标量元素。即使是创建单列的 DataFrame，你也需要传递元组。每个元组类似于一行。你可以选择为列命名，否则
    Spark 会创建一些模糊的名称，如 `_1`、`_2`。列的类型推断是隐式进行的。
- en: 'If you already have the data as RDDs, Spark SQL supports two different methods
    for converting existing RDDs into DataFrames:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经拥有作为 RDD 的数据，Spark SQL 支持两种不同的方法将现有的 RDD 转换为 DataFrame：
- en: The first method uses reflection to infer the schema of an RDD that contains
    specific types of object, which means you are aware of the schema.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种方法通过反射推断 RDD 的模式，这些 RDD 包含特定类型的对象，意味着你已经知道该模式。
- en: The second method is through a programmatic interface that lets you construct
    a schema and then apply it to an existing RDD. While this method is more verbose,
    it allows you to construct DataFrames when the column types are not known until
    runtime.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种方法是通过编程接口，它让你可以构建一个模式，并将其应用到现有的 RDD 上。虽然这种方法更加冗长，但它允许在运行时列类型未知时构建 DataFrame。
- en: Creating DataFrames from JSON
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 JSON 创建 DataFrame
- en: 'JavaScript Object Notation, or JSON, is a language-independent, self-describing,
    lightweight data-exchange format. JSON has become a popular data exchange format
    and has become ubiquitous. In addition to JavaScript and RESTful interfaces, databases
    such as MySQL have accepted JSON as a data type and MongoDB stores all data as
    JSON documents in binary form. Conversion of data to and from JSON is essential
    for any modern data analysis workflow. The Spark DataFrame API lets developers
    convert JSON objects into DataFrames and vice versa. Let''s have a close look
    at the following examples for a better understanding:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript 对象表示法（JSON）是一种独立于语言的、自描述的、轻量级的数据交换格式。JSON 已经成为一种流行的数据交换格式，并且无处不在。除了
    JavaScript 和 RESTful 接口之外，像 MySQL 这样的数据库已经接受 JSON 作为数据类型，MongoDB 将所有数据作为二进制格式的
    JSON 文档存储。数据与 JSON 之间的转换对于任何现代数据分析工作流来说都是至关重要的。Spark 的 DataFrame API 让开发人员可以将
    JSON 对象转换为 DataFrame，反之亦然。让我们通过以下示例深入了解，帮助更好地理解：
- en: '**Python**:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Scala**:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Spark infers schemas automatically from the keys and creates a DataFrame accordingly.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 会自动推断模式并根据键创建相应的 DataFrame。
- en: Creating DataFrames from databases using JDBC
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 JDBC 从数据库创建 DataFrame
- en: Spark allows developers to create DataFrames from other databases using JDBC,
    provided you ensure that the JDBC driver for the intended database is accessible.
    A JDBC driver is a software component that allows a Java application to interact
    with a database. Different databases require different drivers. Usually, database
    providers such as MySQL supply these driver components to access their databases.
    You have to ensure that you have the right driver for the database you want to
    work with.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 允许开发人员通过 JDBC 从其他数据库创建 DataFrame，前提是你确保目标数据库的 JDBC 驱动程序可以访问。JDBC 驱动程序是一个软件组件，它允许
    Java 应用程序与数据库进行交互。不同的数据库需要不同的驱动程序。通常，像 MySQL 这样的数据库提供商会提供这些驱动程序组件来访问他们的数据库。你必须确保你拥有适用于目标数据库的正确驱动程序。
- en: 'The following example assumes that you already have a MySQL database running
    at the given URL, a table called `people` in the database called `test` with some
    data in it, and valid credentials to log in. There is an additional step of relaunching
    the REPL shell with the appropriate JAR file:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例假设你已经在给定的 URL 上运行了 MySQL 数据库，并且数据库 `test` 中有一个名为 `people` 的表，并且其中有一些数据，此外，你也有有效的凭证用于登录。还有一个额外的步骤是使用适当的
    JAR 文件重新启动 REPL shell：
- en: Note
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you do not already have the JAR file in your system, download it from the
    MySQL site at the following link: [https://dev.mysql.com/downloads/connector/j/](https://dev.mysql.com/downloads/connector/j/).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你系统中尚未拥有 JAR 文件，可以从 MySQL 网站通过以下链接下载：[https://dev.mysql.com/downloads/connector/j/](https://dev.mysql.com/downloads/connector/j/)。
- en: '**Python**:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Scala**:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Creating DataFrames from Apache Parquet
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 Apache Parquet 创建 DataFrame
- en: Apache Parquet is an efficient, compressed columnar data representation available
    to any project in the Hadoop ecosystem. Columnar data representations store data
    by column, as opposed to the traditional approach of storing data row by row.
    Use cases that require frequent querying of two to three columns from several
    columns benefit greatly from such an arrangement because columns are stored contiguously
    on the disk and you do not have to read unwanted columns in row-oriented storage.
    Another advantage is in compression. Data in a single column belongs to a single
    type. The values tend to be similar, and sometimes identical. These qualities
    greatly enhance compression and encoding efficiency. Parquet allows compression
    schemes to be specified on a per-column level and allows adding more encodings
    as they are invented and implemented.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet 是一种高效的、压缩的列式数据表示方式，任何 Hadoop 生态系统中的项目都可以使用它。列式数据表示通过列来存储数据，而不是传统的按行存储数据。需要频繁查询多列（通常是两三列）的使用场景特别适合这种存储方式，因为列在磁盘上是连续存储的，而不像行存储那样需要读取不需要的列。另一个优点是在压缩方面。单列中的数据属于同一类型，值通常相似，有时甚至完全相同。这些特性极大地提高了压缩和编码的效率。Parquet
    允许在列级别指定压缩方案，并允许随着新压缩编码的发明与实现而添加更多的编码方式。
- en: 'Apache Spark provides support for both reading and writing Parquet files that
    automatically preserves the schema of the original data. The following example
    writes the people data loaded into a DataFrame in the previous example into Parquet
    format and then re-reads it into an RDD:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark提供对Parquet文件的读取和写入支持，这些操作会自动保留原始数据的模式。以下示例将前面例子中加载到DataFrame中的人员数据写入Parquet格式，然后重新读取到RDD中：
- en: '**Python**:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Scala**:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Creating DataFrames from other data sources
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从其他数据源创建DataFrame
- en: Spark provides built-in support for multiple data sources such as JSON, JDBC,
    HDFS, Parquet, MYSQL, Amazon S3, and so on. In addition, it provides a Data Source
    API that provides a pluggable mechanism for accessing structured data through
    Spark SQL. There are several libraries built on top of this pluggable component,
    for example, CSV, Avro, Cassandra, and MongoDB, to name a few. These libraries
    are not part of the Spark code base. These are built for individual data sources
    and hosted on a community site, Spark packages.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Spark为多个数据源（如JSON、JDBC、HDFS、Parquet、MYSQL、Amazon S3等）提供内建支持。此外，它还提供了一个数据源API，提供了一种可插拔的机制，通过Spark
    SQL访问结构化数据。基于这个可插拔组件，构建了多个库，例如CSV、Avro、Cassandra和MongoDB等。这些库不属于Spark代码库，它们是为特定数据源构建的，并托管在社区网站Spark
    Packages上。
- en: DataFrame operations
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame操作
- en: 'In the previous section of this chapter, we learnt many different ways of creating
    DataFrames. In this section, we will focus on various operations that can be performed
    on DataFrames. Developers chain multiple operations to filter, transform, aggregate,
    and sort data in the DataFrames. The underlying Catalyst optimizer ensures efficient
    execution of these operations. These functions you find here are similar to those
    you commonly find in SQL operations on tables:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的上一节中，我们学习了许多不同的方法来创建DataFrame。在本节中，我们将重点介绍可以在DataFrame上执行的各种操作。开发者通过链式调用多个操作来过滤、转换、聚合和排序DataFrame中的数据。底层的Catalyst优化器确保这些操作的高效执行。这些函数类似于你在SQL操作中对表常见的操作：
- en: '**Python**:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Scala**:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Under the hood
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 底层原理
- en: You already know by now that the DataFrame API is empowered by Spark SQL and
    that the Spark SQL's Catalyst optimizer plays a crucial role in optimizing the
    performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经知道，DataFrame API由Spark SQL赋能，而Spark SQL的Catalyst优化器在优化性能中起着至关重要的作用。
- en: Though the query is executed lazily, it uses the *catalog* component of Catalyst
    to identify whether the column names used in the program or expressions exist
    in the table being used and the data types are proper, and also takes many other
    such precautionary actions. The advantage to this approach is that, instead of
    waiting till program execution, an error pops up as soon as the user types an
    invalid expression.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管查询是惰性执行的，但它使用了Catalyst的*catalog*组件来识别程序或表达式中使用的列名是否存在于正在使用的表中，且数据类型是否正确，并采取了许多其他预防性措施。采用这种方法的优点是，用户一输入无效表达式，错误就会立刻弹出，而不是等到程序执行时才发现。
- en: Summary
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explained the motivation behind the development of the DataFrame
    API in Spark and how development in Spark has become easier than ever. We briefly
    covered the design aspect of the DataFrame API and how it is built on top of Spark
    SQL. We discussed various ways of creating DataFrames from different data sources
    such as RDDs, JSON, Parquet, and JDBC. At the end of this chapter, we just gave
    you a heads-up on how to perform operations on DataFrames. We will discuss DataFrame
    operations in the context of data science and machine learning in more detail
    in the upcoming chapters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们解释了开发Spark DataFrame API的动机，以及开发Spark使得编程变得比以往更加容易。我们简要介绍了DataFrame API的设计理念，以及它如何建立在Spark
    SQL之上。我们讨论了从不同数据源（如RDD、JSON、Parquet和JDBC）创建DataFrame的多种方式。在本章结束时，我们简单提到了如何在DataFrame上执行操作。我们将在接下来的章节中，结合数据科学和机器学习，更详细地讨论DataFrame操作。
- en: In the next chapter, we will learn how Spark supports unified data access and
    discuss on Dataset and Structured Stream  components in details.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习Spark如何支持统一的数据访问，并详细讨论Dataset和Structured Stream组件。
- en: References
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'DataFrame reference on the SQL programming guide of Apache Spark official resource:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark官方文档中的DataFrame参考：
- en: '[https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes)'
- en: 'Databricks: Introducing DataFrames in Apache Spark for Large Scale Data Science:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks：在Apache Spark中引入DataFrames，用于大规模数据科学：
- en: '[https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html](https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html](https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html)'
- en: 'Databricks: From Pandas to Apache Spark''s DataFrame:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks：从Pandas到Apache Spark的DataFrame：
- en: '[https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html](https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html](https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html)'
- en: 'API reference guide on Scala for Spark DataFrames:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Scala API参考指南，适用于Spark DataFrames：
- en: '[https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/sql/DataFrame.html](https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/sql/DataFrame.html)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/sql/DataFrame.html](https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/sql/DataFrame.html)'
- en: 'A Cloudera blogpost on Parquet - an efficient general-purpose columnar file
    format for Apache Hadoop:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Cloudera博客文章：Parquet——一种高效的通用列式存储格式，适用于Apache Hadoop：
- en: '[http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/](http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/](http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/)'
