- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Getting to Know NumPy, pandas, Arrow, and Matplotlib
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 NumPy、pandas、Arrow 和 Matplotlib
- en: One of Python’s biggest strengths is its profusion of high-quality science and
    data processing libraries. At the core of all of them is **NumPy**, which provides
    efficient array and matrix support. On top of NumPy, we can find almost all of
    the scientific libraries. For example, in our field, there’s **Biopython**. But
    other generic data analysis libraries can also be used in our field. For example,
    **pandas** is the *de facto* standard for processing tabled data. More recently,
    **Apache Arrow** provides efficient implementations of some of pandas’ functionality,
    along with language interoperability. Finally, **Matplotlib** is the most common
    plotting library in the Python space and is appropriate for scientific computing.
    While these are general libraries with wide applicability, they are fundamental
    for bioinformatics processing, so we will study them in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的最大优势之一是其丰富的高质量科学和数据处理库。所有这些库的核心是**NumPy**，它提供了高效的数组和矩阵支持。在 NumPy 之上，我们几乎可以找到所有的科学库。例如，在我们这一领域，有**Biopython**。但其他通用的数据分析库也可以在我们这一领域使用。例如，**pandas**
    是处理表格数据的*事实标准*。最近，**Apache Arrow** 提供了一些 pandas 功能的高效实现，并且支持语言互操作性。最后，**Matplotlib**
    是 Python 领域中最常见的绘图库，适用于科学计算。虽然这些库都是广泛应用的通用库，但它们对生物信息学处理至关重要，因此我们将在本章中学习它们。
- en: We will start by looking at pandas as it provides a high-level library with
    very broad practical applicability. Then, we’ll introduce Arrow, which we will
    use only in the scope of supporting pandas. After that, we’ll discuss NumPy, the
    workhorse behind almost everything we do. Finally, we’ll introduce Matplotlib.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 pandas 开始，因为它提供了一个高层次的库，具有非常广泛的实际应用性。然后，我们将介绍 Arrow，我们只在支持 pandas 的范围内使用它。接下来，我们将讨论
    NumPy，这是几乎所有工作背后的驱动力。最后，我们将介绍 Matplotlib。
- en: Our recipes are very introductory – each of these libraries could easily occupy
    a full book, but the recipes should be enough to help you through this book. If
    you are using Docker, and because all these libraries are fundamental for data
    analysis, they can be found in the `tiagoantao/bioinformatics_base` Docker image
    from [*Chapter 1*](B17942_01.xhtml#_idTextAnchor020).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的教程非常基础——这些库中的每一个都可以轻松占据一本完整的书，但这些教程应该足够帮助你完成本书的内容。如果你使用 Docker，并且由于所有这些库对于数据分析至关重要，它们可以在来自[*第
    1 章*](B17942_01.xhtml#_idTextAnchor020)的 `tiagoantao/bioinformatics_base` Docker
    镜像中找到。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下教程：
- en: Using pandas to process vaccine-adverse events
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pandas 处理疫苗不良事件
- en: Dealing with the pitfalls of joining pandas DataFrames
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理 pandas DataFrame 合并的陷阱
- en: Reducing the memory usage of pandas DataFrames
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低 pandas DataFrame 的内存使用
- en: Accelerating pandas processing with Apache Arrow
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Apache Arrow 加速 pandas 处理
- en: Understanding NumPy as the engine behind Python data science and bioinformatics
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 NumPy 作为 Python 数据科学和生物信息学的引擎
- en: Introducing Matplotlib for chart generation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Matplotlib 用于图表生成
- en: Using pandas to process vaccine-adverse events
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 pandas 处理疫苗不良事件
- en: 'We will be introducing pandas with a concrete bioinformatics data analysis
    example: we will be studying data from the **Vaccine Adverse Event Reporting System**
    (**VAERS**, [https://vaers.hhs.gov/](https://vaers.hhs.gov/)). VAERS, which is
    maintained by the US Department of Health and Human Services, includes a database
    of vaccine-adverse events going back to 1990.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个具体的生物信息学数据分析示例来介绍 pandas：我们将研究来自**疫苗不良事件报告系统**（**VAERS**，[https://vaers.hhs.gov/](https://vaers.hhs.gov/)）的数据。VAERS
    由美国卫生与公共服务部维护，包含自 1990 年以来的疫苗不良事件数据库。
- en: VAERS makes data available in **comma-separated values** (**CSV**) format. The
    CSV format is quite simple and can even be opened with a simple text editor (be
    careful with very large file sizes as they may crash your editor) or a spreadsheet
    such as Excel. pandas can work very easily with this format.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: VAERS 提供的数据是**逗号分隔值**（**CSV**）格式。CSV 格式非常简单，甚至可以用简单的文本编辑器打开（请注意，文件过大会导致编辑器崩溃），或者使用类似
    Excel 的电子表格程序打开。pandas 可以非常轻松地处理这种格式。
- en: Getting ready
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'First, we need to download the data. It is available at [https://vaers.hhs.gov/data/datasets.xhtml](https://vaers.hhs.gov/data/datasets.xhtml).
    Please download the ZIP file: we will be using the 2021 file; do not download
    a single CSV file only. After downloading the file, unzip it, and then recompress
    all the files individually with `gzip –9 *csv` to save disk space.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要下载数据。可以在[https://vaers.hhs.gov/data/datasets.xhtml](https://vaers.hhs.gov/data/datasets.xhtml)下载。请下载ZIP文件：我们将使用2021年文件，不要仅下载单个CSV文件。下载文件后，解压缩它，然后使用`gzip
    –9 *csv`将所有文件单独重新压缩，以节省磁盘空间。
- en: Feel free to have a look at the files with a text editor, or preferably with
    a tool such as `less` (`zless` for compressed files). You can find documentation
    for the content of the files at [https://vaers.hhs.gov/docs/VAERSDataUseGuide_en_September2021.pdf](https://vaers.hhs.gov/docs/VAERSDataUseGuide_en_September2021.pdf).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随时可以使用文本编辑器查看文件，或者最好使用诸如`less`（压缩文件用`zless`）的工具。您可以在[https://vaers.hhs.gov/docs/VAERSDataUseGuide_en_September2021.pdf](https://vaers.hhs.gov/docs/VAERSDataUseGuide_en_September2021.pdf)找到文件内容的文档。
- en: If you are using the Notebooks, code is provided at the beginning of them so
    that you can take care of the necessary processing. If you are using Docker, the
    base image is enough.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是笔记本，代码已在开头提供，您可以处理所需的处理步骤。如果您使用的是Docker，基础镜像已足够。
- en: The code can be found in `Chapter02/Pandas_Basic.py`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可以在`Chapter02/Pandas_Basic.py`中找到。
- en: How to do it...
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Follow these steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照以下步骤操作：
- en: 'Let’s start by loading the main data file and gathering the basic statistics:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从加载主要数据文件并收集基本统计信息开始：
- en: '[PRE0]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We start by loading the data. In most cases, there is no need to worry about
    the text encoding as the default, UTF-8, will work, but in this case, the text
    encoding is `legacy iso-8859-1`. Then, we print the column names, which start
    with `VAERS_ID`, `RECVDATE`, `STATE`, `AGE_YRS`, and so on. They include 35 entries
    corresponding to each of the columns. Then, we print the types of each column.
    Here are the first few entries:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载数据。在大多数情况下，默认的UTF-8编码就可以正常工作，但在这种情况下，文本编码是`legacy iso-8859-1`。接下来，我们打印列名，列名以`VAERS_ID`、`RECVDATE`、`STATE`、`AGE_YRS`等开头，共有35个条目，分别对应每一列。然后，我们打印每列的类型。以下是前几个条目：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'By doing this, we get the shape of the data: `(654986, 35)`. This means 654,986
    rows and 35 columns. You can use any of the preceding strategies to get the information
    you need regarding the metadata of the table.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们获得了数据的形状：`(654986, 35)`。这意味着有654,986行和35列。您可以使用上述任何一种策略来获取有关表格元数据的信息。
- en: 'Now, let’s explore the data:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们探索数据：
- en: '[PRE2]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are many ways we can look at the data. We will start by inspecting the
    first row, based on location. Here is an abridged version:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过多种方式查看数据。我们将从根据位置检查第一行开始。以下是简化版本：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After we index by `VAERS_ID`, we can use one ID to get a row. We can use 916600
    (which is the ID from the preceding record) and get the same result.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在按`VAERS_ID`索引后，我们可以使用一个ID来获取一行。我们可以使用916600（这是前一条记录的ID）并获得相同的结果。
- en: 'Then, we retrieve the first three rows. Notice the two different ways we can
    do so:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们提取前三行。注意我们可以通过两种不同的方式做到这一点：
- en: Using the `head` method
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`head`方法
- en: Using the more general array specification; that is, `iloc[:3]`
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更通用的数组规范；也就是`iloc[:3]`
- en: 'Finally, we retrieve the first five rows, but only the second and third columns
    –`iloc[:5, 2:4]`. Here is the output:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们提取前五行，但仅提取第二和第三列——`iloc[:5, 2:4]`。以下是输出：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s do some basic computations now, namely computing the maximum age in the
    dataset:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们做一些基本的计算，即计算数据集中最大年龄：
- en: '[PRE5]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The maximum value is 119 years. More importantly than the result, notice the
    two dialects for accessing `AGE_YRS` (as a dictionary key and as an object field)
    for the access columns.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最大值为119岁。比结果更重要的是，注意两种访问`AGE_YRS`的方式（作为字典键和作为对象字段）来访问列。
- en: 'Now, let’s plot the ages involved:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们绘制涉及的年龄分布：
- en: '[PRE6]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This generates two plots (a condensed version is shown in the following step).
    We use pandas plotting machinery here, which uses Matplotib underneath.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成两个图表（以下步骤显示的是简化版本）。我们在这里使用的是pandas的绘图工具，它底层使用Matplotlib。
- en: 'While we have a full recipe for charting with Matplotlib (*Introducing Matplotlib
    for chart generation*), let’s have a sneak peek here by using it directly:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然我们已经有完整的Matplotlib绘图食谱（*引入Matplotlib进行图表生成*），但让我们在此先通过直接使用它来一窥究竟：
- en: '[PRE7]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This includes both figures from the previous steps. Here is the output:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括前一步的两个图表。以下是输出：
- en: '![Figure 2.1 – Left – the age for each observation of adverse effect;  right
    – a histogram showing the distribution of ages  ](img/B17942_02_001.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 左侧 – 每个不良反应观察的年龄；右侧 – 显示年龄分布的直方图](img/B17942_02_001.jpg)'
- en: Figure 2.1 – Left – the age for each observation of adverse effect; right –
    a histogram showing the distribution of ages
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 左侧 – 每个不良反应观察的年龄；右侧 – 显示年龄分布的直方图
- en: 'We can also take a non-graphical, more analytical approach, such as counting
    the events per year:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以采取一种非图形的、更分析性的方法，比如按年计数事件：
- en: '[PRE8]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output will be as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, let’s see how many people died:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看有多少人死亡：
- en: '[PRE10]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of the count is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 计数结果如下：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that the type of `DIED` is *not* a Boolean. It’s more declarative to have
    a Boolean representation of a Boolean characteristic, so we create `is_dead` for
    it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`DIED` 的类型*不是*布尔值。使用布尔值表示布尔特性更具声明性，因此我们为它创建了 `is_dead`。
- en: Tip
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Here, we are assuming that NaN is to be interpreted as `False`. In general,
    we must be careful with the interpretation of NaN. It may mean `False` or it may
    simply mean – as in most cases – a lack of data. If that were the case, it should
    not be converted into `False`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设 NaN 应该被解释为 `False`。一般来说，我们必须小心解读 NaN。它可能表示 `False`，或者像大多数情况一样，仅表示数据缺失。如果是这种情况，它不应该被转换为
    `False`。
- en: 'Now, let’s associate the individual data about deaths with the type of vaccine
    involved:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将死亡的个人数据与所涉及的疫苗类型进行关联：
- en: '[PRE12]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After we get a DataFrame containing just deaths, we must read the data that
    contains vaccine information. First, we must do some exploratory analysis of the
    types of vaccines and their adverse events. Here is the abridged output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 获取仅包含死亡数据的 DataFrame 后，我们需要读取包含疫苗信息的数据。首先，我们需要进行一些关于疫苗类型及其不良反应的探索性分析。以下是简化后的输出：
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: After that, we must choose just the COVID-related vaccines and join them with
    individual data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们必须选择与 COVID 相关的疫苗，并将其与个人数据进行合并。
- en: 'Finally, let’s see the top 10 COVID vaccine lots that are overrepresented in
    terms of deaths and how many US states were affected by each lot:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们看看前 10 个在死亡数量上过度代表的 COVID 疫苗批次，以及每个批次影响的美国州数：
- en: '[PRE14]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: That concludes this recipe!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本节到此结束！
- en: There’s more...
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: The preceding data about vaccines and lots is not completely correct; we will
    cover some data analysis pitfalls in the next recipe.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 关于疫苗和批次的前述数据并不完全正确；我们将在下一个食谱中讨论一些数据分析中的陷阱。
- en: In the *Introducing Matplotlib for chart generation* recipe, we will introduce
    Matplotlib, a chart library that provides the backend for pandas plotting. It
    is a fundamental component of Python’s data analysis ecosystem.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *引入 Matplotlib 用于图表生成* 的食谱中，我们将介绍 Matplotlib，一个为 pandas 绘图提供后端支持的图表库。它是 Python
    数据分析生态系统的一个基础组件。
- en: See also
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'The following is some extra information that may be useful:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可能有用的额外信息：
- en: While the first three recipes of this chapter are enough to support you throughout
    this book, there is plenty of content available on the web to help you understand
    pandas. You can start with the main user guide, which is available at [https://pandas.pydata.org/docs/user_guide/index.xhtml](https://pandas.pydata.org/docs/user_guide/index.xhtml).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然本章的前三个食谱足以支持你阅读本书，但网络上有很多资源可以帮助你理解 pandas。你可以从主要的用户指南开始，网址为 [https://pandas.pydata.org/docs/user_guide/index.xhtml](https://pandas.pydata.org/docs/user_guide/index.xhtml)。
- en: 'If you need to plot data, do not forget to check the visualization part of
    the guide since it is especially helpful: [https://pandas.pydata.org/docs/user_guide/visualization.xhtml](https://pandas.pydata.org/docs/user_guide/visualization.xhtml).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你需要绘制数据图表，不要忘记查看指南中的可视化部分，因为它特别有帮助：[https://pandas.pydata.org/docs/user_guide/visualization.xhtml](https://pandas.pydata.org/docs/user_guide/visualization.xhtml)。
- en: Dealing with the pitfalls of joining pandas DataFrames
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理连接 pandas DataFrame 时的陷阱
- en: The previous recipe was a whirlwind tour that introduced pandas and exposed
    most of the features that we will use in this book. While an exhaustive discussion
    about pandas would require a complete book, in this recipe – and in the next one
    – we are going to discuss topics that impact data analysis and are seldom discussed
    in the literature but are very important.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个食谱是对 pandas 的快速介绍，涵盖了我们在本书中将使用的大部分功能。尽管关于 pandas 的详细讨论需要一本完整的书，但在本食谱（以及下一个食谱）中，我们将讨论一些对数据分析有影响的主题，这些主题在文献中很少讨论，但却非常重要。
- en: 'In this recipe, we are going to discuss some pitfalls that deal with relating
    DataFrames through joins: it turns out that many data analysis errors are introduced
    by carelessly joining data. We will introduce techniques to reduce such problems
    here.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将讨论通过连接（joins）关联 DataFrame 时的一些陷阱：事实证明，许多数据分析错误是由于不小心连接数据所引入的。我们将在这里介绍一些减少此类问题的技巧。
- en: Getting ready
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will be using the same data as in the previous recipe, but we will jumble
    it a bit so that we can discuss typical data analysis pitfalls. Once again, we
    will be joining the main adverse events table with the vaccination table, but
    we will randomly sample 90% of the data from each. This mimics, for example, the
    scenario where you only have incomplete information. This is one of the many examples
    where joins between tables do not have intuitively obvious results.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与上一食谱相同的数据，但会稍微打乱它，以便讨论典型的数据分析陷阱。我们将再次将主要的不良事件表与疫苗表连接，但会从每个表中随机采样 90% 的数据。这模拟了例如你只有不完整信息的场景。这是很多情况下，表之间的连接结果并不直观明显的一个例子。
- en: 'Use the following code to prepare our files by randomly sampling 90% of the
    data:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码通过随机采样 90% 的数据来准备我们的文件：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Because this code involves random sampling, the results that you will get will
    be different from the ones reported here. If you want to get the same results,
    I have provided the files that I used in the `Chapter02` directory. The code for
    this recipe can be found in `Chapter02/Pandas_Join.py`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此代码涉及随机采样，因此你将得到与此处报告的结果不同的结果。如果你想得到相同的结果，我已经提供了我在 `Chapter02` 目录中使用的文件。此食谱的代码可以在
    `Chapter02/Pandas_Join.py` 中找到。
- en: How to do it...
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Follow these steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照以下步骤操作：
- en: 'Let’s start by doing an inner join of the individual and vaccine tables:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先对个体数据和疫苗数据表做一个内连接：
- en: '[PRE17]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `len` output for this code is 589,487 for the individual data, 620,361 for
    the vaccination data, and 558,220 for the join. This suggests that some individual
    and vaccine data was not captured.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码的 `len` 输出结果是：个体数据为 589,487，疫苗数据为 620,361，连接结果为 558,220。这表明一些个体数据和疫苗数据没有被捕获。
- en: 'Let’s find the data that was not captured with the following join:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过以下连接查找未被捕获的数据：
- en: '[PRE18]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You will see that 56,524 rows of individual data aren’t joined and that there
    are 62,141 rows of vaccinated data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到 56,524 行个体数据没有被连接，并且有 62,141 行疫苗数据。
- en: 'There are other ways to join data. The default way is by performing a left
    outer join:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还有其他方式可以连接数据。默认的方法是执行左外连接：
- en: '[PRE19]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: A left outer join assures that all the rows on the left table are always represented.
    If there are no rows on the right, then all the right columns will be filled with
    `None` values.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 左外连接确保左表中的所有行始终被表示。如果右表没有匹配的行，则所有右侧的列将被填充为 `None` 值。
- en: Warning
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: There is a caveat that you should be careful with. Remember that the left table
    – `vdata` – had one entry per `VAERS_ID`. When you left join, you may end up with
    a case where the left-hand side is repeated several times. For example, the `groupby`
    operation that we did previously shows that `VAERS_ID` of 962303 has 11 entries.
    This is correct, but it’s not uncommon to have the incorrect expectation that
    you will still have a single row on the output per row on the left-hand side.
    This is because the left join returns 1 or more left entries, whereas the inner
    join above returns 0 or 1 entries, where sometimes, we would like to have precisely
    1 entry. Be sure to always test the output for what you want in terms of the number
    of entries.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个警告需要小心。请记住，左表 - `vdata` - 每个 `VAERS_ID` 都有一条记录。当你进行左连接时，可能会遇到左侧数据被重复多次的情况。例如，我们之前做的
    `groupby` 操作显示，`VAERS_ID` 为 962303 的数据有 11 条记录。这是正确的，但也不罕见的是，很多人错误地期望左侧的每一行在输出中仍然是单独一行。这是因为左连接会返回一个或多个左侧条目，而上述的内连接返回的是
    0 或 1 条记录，而有时我们希望每个左侧的行都精确对应一条记录。务必始终测试输出结果，确保记录的数量符合预期。
- en: 'There is a right join as well. Let’s right join COVID vaccines – the left table
    – with death events – the right table:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 也有右连接。让我们将 COVID 疫苗数据（左表）与死亡事件数据（右表）做右连接：
- en: '[PRE20]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you may expect, a right join will ensure that all the rows on the right table
    are represented. So, we end up with 583,817 COVID entries, 7,670 dead entries,
    and a right join of 8,624 entries.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所料，右连接将确保右表中的所有行都会被表示出来。因此，我们最终得到了 583,817 个 COVID 记录，7,670 个死亡记录，以及一个 8,624
    条记录的右连接。
- en: We also check the number of duplicated entries on the joined table and we get
    954\. If we subtract the length of the dead table from the joined table, we also
    get, as expected, 954\. Make sure you have checks like this when you’re making
    joins.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还检查了连接表中的重复条目数量，结果是954。如果我们从连接表中减去死表的长度，结果也是954。做连接时，确保进行这样的检查。
- en: 'Finally, we are going to revisit the problematic COVID lot calculations since
    we now understand that we might be overcounting lots:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将重新审视有问题的COVID批次计算，因为我们现在知道我们可能在过度计算批次：
- en: '[PRE21]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Note that the strategies that we’ve used here ensure that we don’t get repeats:
    first, we limit the number of columns to the ones we will be using, then we remove
    repeated indexes and empty `VAERS_ID`. This ensures no repetition of the `VAERS_ID`,
    `VAX_LOT` pair, and that no lots are associated with no IDs.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我们在这里使用的策略确保了没有重复项：首先，我们限制了将要使用的列的数量，然后移除重复的索引和空的`VAERS_ID`。这样就能确保`VAERS_ID`和`VAX_LOT`的组合不重复，并且不会有没有ID关联的批次。
- en: There’s more...
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There are other types of joins other than left, inner, and right. Most notably,
    there is the outer join, which assures all entries from both tables have representation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了左连接、内连接和右连接外，还有其他类型的连接。最值得注意的是外连接，它确保两个表中的所有条目都有表示。
- en: 'Make sure you have tests and assertions for your joins: a very common bug is
    having the wrong expectations for how joins behave. You should also make sure
    that there are no empty values on the columns where you are joining, as they can
    produce a lot of excess tuples.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你对连接操作有测试和断言：一个非常常见的bug是对连接行为的预期错误。你还应该确保在连接的列上没有空值，因为空值可能会产生大量多余的元组。
- en: Reducing the memory usage of pandas DataFrames
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少pandas DataFrame的内存使用
- en: When you are dealing with lots of information – for example, when analyzing
    whole genome sequencing data – memory usage may become a limitation for your analysis.
    It turns out that naïve pandas is not very efficient from a memory perspective,
    and we can substantially reduce its consumption.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处理大量信息时——例如在分析全基因组测序数据时——内存使用可能会成为分析的限制因素。事实证明，天真的pandas在内存方面并不是很高效，我们可以大幅减少它的内存消耗。
- en: 'In this recipe, we are going to revisit our VAERS data and look at several
    ways to reduce pandas memory usage. The impact of these changes can be massive:
    in many cases, reducing memory consumption may mean the difference between being
    able to use pandas or requiring a more alternative and complex approach, such
    as Dask or Spark.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们将重新审视我们的VAERS数据，并探讨几种减少pandas内存使用的方法。这些变化的影响可能是巨大的：在许多情况下，减少内存消耗可能意味着能否使用pandas，或者需要采用更复杂的替代方法，如Dask或Spark。
- en: Getting ready
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: We will be using the data from the first recipe. If you have run it, you are
    all set; if not, please follow the steps discussed there. You can find this code
    in `Chapter02/Pandas_Memory.py`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用第一个方案中的数据。如果你已经运行过它，你就可以开始了；如果没有，请按照其中讨论的步骤操作。你可以在`Chapter02/Pandas_Memory.py`找到这段代码。
- en: How to do it…
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Follow these steps:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些步骤操作：
- en: 'First, let’s load the data and inspect the size of the DataFrame:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们加载数据并检查DataFrame的大小：
- en: '[PRE22]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is an abridged version of the output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出的简化版本：
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here, we have information about the number of rows and the type and non-null
    values of each row. Finally, we can see that the DataFrame requires a whopping
    1.3 GB.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有关于行数、每行类型和非空值的相关信息。最后，我们可以看到，DataFrame需要高达1.3 GB的内存。
- en: 'We can also inspect the size of each column:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以检查每一列的大小：
- en: '[PRE24]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here is an abridged version of the output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出的简化版本：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`SYMPTOM_TEXT` occupies 442 MB, so 1/3 of our entire table.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`SYMPTOM_TEXT`占用了442 MB，相当于我们整个表的1/3。'
- en: Now, let’s look at the `DIED` column. Can we find a more efficient representation?
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看`DIED`这一列。我们能找到更高效的表示方式吗？
- en: '[PRE26]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The original column takes 21,181,488 bytes, whereas our compact representation
    takes 656,986 bytes. That’s 32 times less!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 原始列占用了21,181,488字节，而我们的压缩表示仅占用656,986字节。也就是说减少了32倍！
- en: What about the `STATE` column? Can we do better?
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 那么`STATE`这一列呢？我们能做得更好吗？
- en: '[PRE27]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here, we convert the `STATE` column, which is text, into `encoded_state`, which
    is a number. This number is the position of the state’s name in the list state.
    We use this number to look up the list of states. The original column takes around
    36 MB, whereas the encoded column takes 0.6 MB.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将 `STATE` 列（文本类型）转换为 `encoded_state`（数字类型）。这个数字是州名在州列表中的位置。我们使用这个数字来查找州的列表。原始列大约占用
    36 MB，而编码后的列只占用 0.6 MB。
- en: As an alternative to this approach, you can look at categorical variables in
    pandas. I prefer to use them as they have wider applications.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这种方法的替代方案，您可以查看 pandas 中的分类变量。我更喜欢使用它们，因为它们有更广泛的应用。
- en: 'We can apply most of these optimizations when we *load* the data, so let’s
    prepare for that. But now, we have a chicken-and-egg problem: to be able to know
    the content of the state table, we have to do a first pass to get the list of
    states, like so:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在 *加载* 数据时应用大多数这些优化，因此让我们为此做好准备。但现在，我们遇到了一个先有鸡还是先有蛋的问题：为了能够了解州表的内容，我们必须进行第一次遍历，获取州列表，如下所示：
- en: '[PRE28]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We have a converter that simply returns the uppercase version of the state.
    We only return the `STATE` column to save memory and processing time. Finally,
    we get the `STATE` column from the DataFrame (which has only a single column).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个转换器，简单地返回州的全大写版本。我们只返回 `STATE` 列，以节省内存和处理时间。最后，我们从 DataFrame 中获取 `STATE`
    列（该列只有一个字段）。
- en: 'The ultimate optimization is *not* to load the data. Imagine that we don’t
    need `SYMPTOM_TEXT` – that is around 1/3 of the data. In that case, we can just
    skip it. Here is the final version:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的优化是 *不* 加载数据。假设我们不需要 `SYMPTOM_TEXT` —— 这大约占数据的三分之一。在这种情况下，我们可以跳过它。以下是最终版本：
- en: '[PRE29]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We are now at 714 MB, which is a bit over half of the original. This could be
    still substantially reduced by applying the methods we used for `STATE` and `DIED`
    to all other columns.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的内存占用为 714 MB，稍微超过原始数据的一半。通过将我们对 `STATE` 和 `DIED` 列使用的方法应用于所有其他列，这个数字还可以大大减少。
- en: See also
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'The following is some extra information that may be useful:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可能有用的额外信息：
- en: If you are willing to use a support library to help with Python processing,
    check the next recipe on Apache Arrow, which will allow you to have extra memory
    savings for more memory efficiency.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您愿意使用支持库来帮助 Python 处理，请查看下一个关于 Apache Arrow 的食谱，它将帮助您在更多内存效率上节省额外的内存。
- en: If you end up with DataFrames that take more memory than you have available
    on a single machine, then you must step up your game and use chunking - which
    we will not cover in the Pandas context - or something that can deal with large
    data automatically. Dask, which we’ll cover in [*Chapter 11*](B17942_11.xhtml#_idTextAnchor272),
    *Parallel Processing with Dask and Zarr*, allows you to work with larger-than-memory
    datasets with, among others, a pandas-like interface.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果最终得到的 DataFrame 占用了比单台机器可用内存更多的内存，那么您必须提高处理能力并使用分块处理——我们在 Pandas 上下文中不会涉及——或者使用可以自动处理大数据的工具。Dask（我们将在
    [*第 11 章*](B17942_11.xhtml#_idTextAnchor272) “*使用 Dask 和 Zarr 进行并行处理” 中讨论）允许您使用类似
    pandas 的接口处理超大内存数据集。
- en: "Accelerating pandas processing with \LApache Arrow"
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Arrow 加速 pandas 处理
- en: When dealing with large amounts of data, such as in whole genome sequencing,
    pandas is both slow and memory-consuming. Apache Arrow provides faster and more
    memory-efficient implementations of several pandas operations and can interoperate
    with it.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理大量数据时，例如全基因组测序，pandas 的速度较慢且占用内存较大。Apache Arrow 提供了几种 pandas 操作的更快且更节省内存的实现，并且可以与
    pandas 进行互操作。
- en: 'Apache Arrow is a project co-founded by Wes McKinney, the founder of pandas,
    and it has several objectives, including working with tabular data in a language-agnostic
    way, which allows for language interoperability while providing a memory- and
    computation-efficient implementation. Here, we will only be concerned with the
    second part: getting more efficiency for large-data processing. We will do this
    in an integrated way with pandas.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Arrow 是由 pandas 的创始人 Wes McKinney 共同创立的一个项目，它有多个目标，包括以与语言无关的方式处理表格数据，这样可以实现语言间的互操作性，同时提供高效的内存和计算实现。在这里，我们将只关注第二部分：提高大数据处理的效率。我们将与
    pandas 一起以集成的方式实现这一点。
- en: Here, we will once again use VAERS data and show how Apache Arrow can be used
    to accelerate pandas data loading and reduce memory consumption.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将再次使用 VAERS 数据，并展示如何使用 Apache Arrow 加速 pandas 数据加载并减少内存消耗。
- en: Getting ready
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Again, we will be using data from the first recipe. Be sure you download and
    prepare it, as explained in the *Getting ready* section of the *Using pandas to
    process vaccine-adverse events* recipe. The code is available in `Chapter02/Arrow.py`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将使用第一个食谱中的数据。确保你已经按照 *准备工作* 部分中解释的方式下载并准备好它，代码可以在 `Chapter02/Arrow.py`
    中找到。
- en: How to do it...
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Follow these steps:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤进行：
- en: 'Let’s start by loading the data using both pandas and Arrow:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始使用 pandas 和 Arrow 加载数据：
- en: '[PRE30]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'pandas requires 1.3 GB, whereas Arrow requires 614 MB: less than half the memory.
    For large files like this, this may mean the difference between being able to
    process data in memory or needing to find another solution, such as Dask. While
    some functions in Arrow have similar names to pandas (for example, `read_csv`),
    that is not the most common occurrence. For example, note the way we compute the
    total size of the DataFrame: by getting the size of each column and performing
    a sum, which is a different approach from pandas.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 需要 1.3 GB，而 Arrow 只需 614 MB：不到一半的内存。对于像这样的超大文件，这可能意味着能否将数据加载到内存中进行处理，或者需要寻找其他解决方案，比如
    Dask。虽然 Arrow 中某些函数与 pandas 的名称相似（例如，`read_csv`），但这并不是最常见的情况。例如，注意我们计算 DataFrame
    总大小的方法：通过获取每一列的大小并求和，这与 pandas 的方法不同。
- en: 'Let’s do a side-by-side comparison of the inferred types:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们并排比较推断出的类型：
- en: '[PRE31]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here is an abridged version of the output:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出的简化版本：
- en: '[PRE32]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you can see, Arrow is generally more specific with type inference and is
    one of the main reasons why memory usage is substantially lower.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Arrow 在类型推断方面通常更为具体，这也是其内存使用显著更低的主要原因之一。
- en: 'Now, let’s do a time performance comparison:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们做一个时间性能比较：
- en: '[PRE33]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'On my computer, the results are as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的计算机上，结果如下：
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Arrow’s implementation is three times faster. The results on your computer will
    vary as this is dependent on the hardware.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Arrow 的实现速度是三倍。由于这取决于硬件，因此你电脑上的结果可能有所不同。
- en: 'Let’s repeat the memory occupation comparison while not loading the `SYMPTOM_TEXT`
    column. This is a fairer comparison as most numerical datasets do not tend to
    have a very large text column:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在不加载 `SYMPTOM_TEXT` 列的情况下重复内存占用比较。这是一个更公平的比较，因为大多数数值数据集通常没有非常大的文本列：
- en: '[PRE35]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'pandas requires 847 MB, whereas Arrow requires 205 MB: four times less.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 需要 847 MB，而 Arrow 只需 205 MB：少了四倍。
- en: 'Our objective is to use Arrow to load data into pandas. For that, we need to
    convert the data structure:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的目标是使用 Arrow 将数据加载到 pandas 中。为此，我们需要转换数据结构：
- en: '[PRE36]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'There are two very important points to be made here: the pandas representation
    created by Arrow uses only 1 GB, whereas the pandas representation, from its native
    `read_csv`, is 1.3 GB. This means that even if you use pandas to process data,
    Arrow can create a more compact representation to start with.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两点非常重要：Arrow 创建的 pandas 表示只用了 1 GB，而 pandas 从其本地的 `read_csv` 创建的表示则需要 1.3
    GB。这意味着即使你使用 pandas 处理数据，Arrow 也可以首先创建一个更紧凑的表示。
- en: 'The preceding code has one problem regarding memory consumption: when the converter
    is running, it will require memory to hold *both* the pandas and the Arrow representations,
    hence defeating the purpose of using less memory. Arrow can self-destruct its
    representation while creating the pandas version, hence resolving the problem.
    The line for this is `vdata = vdata_arrow.to_pandas(self_destruct=True)`.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码存在一个内存消耗问题：当转换器运行时，它将需要内存来存储 *pandas* 和 *Arrow* 的两个表示，从而违背了使用更少内存的初衷。Arrow
    可以在创建 pandas 版本的同时自我销毁其表示，从而解决这个问题。相关代码行是 `vdata = vdata_arrow.to_pandas(self_destruct=True)`。
- en: There’s more...
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: If you have a very large DataFrame that cannot be processed by pandas, even
    after it’s been loaded by Arrow, then maybe Arrow can do all the processing as
    it has a computing engine as well. That being said, Arrow’s engine is, at the
    time of writing, substantially less complete in terms of functionality than pandas.
    Remember that Arrow has many other features, such as language interoperability,
    but we will not be making use of those in this book.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个非常大的 DataFrame，甚至在 Arrow 加载后也无法由 pandas 处理，那么或许 Arrow 可以完成所有处理，因为它也有计算引擎。尽管如此，Arrow
    的计算引擎在写作时，功能上远不如 pandas 完善。记住，Arrow 还有许多其他功能，例如语言互操作性，但我们在本书中不会使用到这些。
- en: Understanding NumPy as the engine behind Python data science and bioinformatics
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 NumPy 作为 Python 数据科学和生物信息学的引擎
- en: Most of your analysis will make use of NumPy, even if you don’t use it explicitly.
    NumPy is an array manipulation library that is behind libraries such as pandas,
    Matplotlib, Biopython, and scikit-learn, among many others. While much of your
    bioinformatics work may not require explicit direct use of NumPy, you should be
    aware of its existence as it underpins almost everything you do, even if only
    indirectly via the other libraries.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你没有显式使用 NumPy，大多数分析都将使用 NumPy。NumPy 是一个数组操作库，它是 pandas、Matplotlib、Biopython、scikit-learn
    等许多库背后的基础。虽然你在生物信息学工作中可能不需要直接使用 NumPy，但你应该了解它的存在，因为它几乎支持了你所做的一切，即使是通过其他库间接使用。
- en: 'In this recipe, we will use VAERS data to demonstrate how NumPy is behind many
    of the core libraries that we use. This is a very light introduction to the library
    so that you are aware that it exists and that it is behind almost everything.
    Our example will extract the number of cases from the five US states with more
    adverse effects, splitting them into age bins: 0 to 19 years, 20 to 39, up to
    100 to 119.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用 VAERS 数据演示 NumPy 如何在我们使用的许多核心库中发挥作用。这是一个非常简要的 NumPy 入门介绍，目的是让你了解它的存在，并知道它几乎在所有东西背后。我们的示例将从五个美国州中提取不良反应案例的数量，并按年龄分组：0至19岁、20至39岁，直到100至119岁。
- en: Getting ready
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Once again, we will be using the data from the first recipe, so make sure it’s
    available. The code for it can be found in `Chapter02/NumPy.py`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，我们将使用第一个例子中的数据，因此请确保数据可用。相关的代码可以在 `Chapter02/NumPy.py` 找到。
- en: How to do it…
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Follow these steps:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: 'Let’s start by loading the data with pandas and reducing the data so that it’s
    related to the top five US states only:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先通过 pandas 加载数据，并将数据减少到仅与前五个美国州相关：
- en: '[PRE37]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The top states are as follows. This rank will be used later to construct a
    NumPy matrix:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 前五个州如下。这个排名将在后续用来构建 NumPy 矩阵：
- en: '![Figure 2.2 – US states with largest numbers of adverse effects ](img/B17942_02_002.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 美国具有最大不良反应数量的州](img/B17942_02_002.jpg)'
- en: Figure 2.2 – US states with largest numbers of adverse effects
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 美国具有最大不良反应数量的州
- en: 'Now, let’s extract the two NumPy arrays that contain age and state data:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们提取包含年龄和州数据的两个 NumPy 数组：
- en: '[PRE38]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Note that the data that underlies pandas is NumPy data (the `values` call for
    both Series returns NumPy types). Also, you may recall that pandas has properties
    such as `.shape` or `.dtype`: these were inspired by NumPy and behave the same.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，pandas 背后的数据是 NumPy 数据（对于 Series，`values` 调用返回的是 NumPy 类型）。此外，你可能还记得 pandas
    有 `.shape` 或 `.dtype` 等属性：这些属性的设计灵感来源于 NumPy，且行为相同。
- en: 'Now, let’s create a NumPy matrix from scratch (a 2D array), where each row
    is a state and each column represents an age group:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们从头开始创建一个 NumPy 矩阵（一个二维数组），其中每一行代表一个州，每一列代表一个年龄组：
- en: '[PRE39]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The array has five rows – one for each state – and six columns – one for each
    age group. All the cells in the array must have the same type.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数组有五行——每行代表一个州——和六列——每列代表一个年龄组。数组中的所有单元格必须具有相同的类型。
- en: 'We initialize the array with zeros. There are many ways to initialize arrays,
    but if you have a very large array, initializing it may take a lot of time. Sometimes,
    depending on your task, it might be OK that the array is empty at the beginning
    (meaning it was initialized with random trash). In that case, using `np.empty`
    will be much faster. We use pandas iteration here: this is not the best way to
    do things from a pandas perspective, but we want to make the NumPy part very explicit.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用零来初始化数组。虽然初始化数组有很多方法，但如果你的数组非常大，初始化可能会花费很长时间。有时，根据你的任务，数组一开始为空（意味着它被初始化为随机垃圾）也是可以接受的。在这种情况下，使用
    `np.empty` 会快得多。我们在这里使用 pandas 的迭代：从 pandas 的角度来看，这不是最好的做法，但我们希望让 NumPy 的部分非常明确。
- en: 'We can extract a single row – in our case, the data for a state – very easily.
    The same applies to a column. Let’s take California data and then the 0-19 age
    group:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以非常轻松地提取单一的行——在我们这个例子中，是某个州的数据。对列同样适用。我们来看看加利福尼亚州的数据，然后是0-19岁的年龄组：
- en: '[PRE40]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note the syntax to extract a row or a column. It should be familiar to you,
    given that pandas copied the syntax from NumPy and we encountered it in previous
    recipes.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意提取行或列的语法。鉴于 pandas 复制了 NumPy 的语法，且我们在之前的例子中也遇到过，它应该对你来说很熟悉。
- en: 'Now, let’s compute a new matrix where we have the fraction of cases per age
    group:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们计算一个新的矩阵，矩阵中的每个元素表示每个年龄组的案例比例：
- en: '[PRE41]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The last line applies the `compute_frac` function to all rows. `compute_frac`
    takes a single row and returns a new row where all the elements are divided by
    the total sum.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行对所有行应用了 `compute_frac` 函数。`compute_frac` 接受一行数据并返回一个新行，其中所有元素都被总和除以。
- en: 'Now, let’s create a new matrix that acts as a percentage instead of a fraction
    – simply because it reads better:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个新的矩阵，表示百分比而不是比例——这样看起来更清晰：
- en: '[PRE42]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The first line simply multiplies all the elements of the 2D array by 100\. Matplotlib
    is smart enough to traverse different array structures. That line will work if
    it’s presented with an array with any dimensions and would do exactly what is
    expected.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行代码仅仅是将 2D 数组的所有元素乘以 100。Matplotlib 足够聪明，能够处理不同的数组结构。只要传递给它任何维度的数组，这行代码都能正常工作并按预期进行操作。
- en: 'Here is the result:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![Figure 2.3 – A matrix representing the distribution of vaccine-adverse effects  in
    the five US states with the most cases ](img/B17942_02_003.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 一个矩阵，表示美国五个病例最多的州中疫苗不良反应的分布](img/B17942_02_003.jpg)'
- en: Figure 2.3 – A matrix representing the distribution of vaccine-adverse effects
    in the five US states with the most cases
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 一个矩阵，表示美国五个病例最多的州中疫苗不良反应的分布
- en: 'Finally, let’s create a graphical representation of the matrix using Matplotlib:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们使用 Matplotlib 创建该矩阵的图形表示：
- en: '[PRE43]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Do not dwell too much on the Matplotlib code – we are going to discuss it in
    the next recipe. The fundamental point here is that you can pass NumPy data structures
    to Matplotlib. Matplotlib, like pandas, is based on NumPy.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 不要过多纠结于 Matplotlib 的代码——我们将在下一个示例中讨论它。这里的关键点是，你可以将 NumPy 数据结构传递给 Matplotlib。Matplotlib
    就像 pandas 一样，是基于 NumPy 的。
- en: See also
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The following is some extra information that may be useful:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可能有用的额外信息：
- en: 'NumPy has many more features than the ones we’ve discussed here. There are
    plenty of books and tutorials on them. The official documentation is a good place
    to start: [https://numpy.org/doc/stable/](https://numpy.org/doc/stable/).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 有许多功能超出了我们在这里讨论的范围。市面上有许多书籍和教程可以帮助学习这些功能。官方文档是一个很好的起点：[https://numpy.org/doc/stable/](https://numpy.org/doc/stable/)。
- en: 'There are many important issues to discover with NumPy, but probably one of
    the most important is broadcasting: NumPy’s ability to take arrays of different
    structures and get the operations right. For details, go to [https://numpy.org/doc/stable/user/theory.broadcasting.xhtml](https://numpy.org/doc/stable/user/theory.broadcasting.xhtml).'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 有许多重要的功能值得探索，但其中最重要的可能是广播：NumPy 能够处理不同结构的数组，并正确地进行操作。详细信息请参见 [https://numpy.org/doc/stable/user/theory.broadcasting.xhtml](https://numpy.org/doc/stable/user/theory.broadcasting.xhtml)。
- en: Introducing Matplotlib for chart generation
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍用于图表生成的 Matplotlib
- en: Matplotlib is the most common Python library for generating charts. There are
    more modern alternatives, such as **Bokeh**, which is web-centered, but the advantage
    of Matplotlib is not only that it is the most widely available and widely documented
    chart library but also, in the computational biology world, we want a chart library
    that is both web- and paper-centric. This is because many of our charts will be
    submitted to scientific journals, which are equally concerned with both formats.
    Matplotlib can handle this for us.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib 是最常用的 Python 图表生成库。虽然也有一些更现代的替代库，如**Bokeh**，它是以 web 为中心的，但 Matplotlib
    的优势不仅在于它是最广泛可用且文档最为丰富的图表库，还因为在计算生物学领域，我们需要一个既适用于 web 又适用于纸质文档的图表库。这是因为我们许多图表将会提交给科学期刊，这些期刊同样关注这两种格式。Matplotlib
    能够为我们处理这一需求。
- en: Many of the examples in this recipe could also be done directly with pandas
    (hence indirectly with Matplotlib), but the point here is to exercise Matplotlib.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例中的许多例子也可以直接使用 pandas（间接使用 Matplotlib）完成，但这里的重点是练习使用 Matplotlib。
- en: Once again, we are going to use VAERS data to plot some information about the
    DataFrame’s metadata and summarize the epidemiological data.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用 VAERS 数据来绘制有关 DataFrame 元数据的信息，并总结流行病学数据。
- en: Getting ready
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Again, we will be using the data from the first recipe. The code can be found
    in `Chapter02/Matplotlib.py`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将使用第一个示例中的数据。代码可以在 `Chapter02/Matplotlib.py` 中找到。
- en: How to do it...
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'Follow these steps:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: 'The first thing that we will do is plot the fraction of nulls per column:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是绘制每列空值的比例：
- en: '[PRE44]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`labels` is the column names that we are analyzing, `bar_values` is the fraction
    of null values, and `x_positions` is the location of the bars on the bar chart
    that we are going to plot next.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`labels` 是我们正在分析的列名，`bar_values` 是空值的比例，`x_positions` 是接下来要绘制的条形图上条形的位置。'
- en: 'Here is the code for the first version of the bar plot:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是第一个版本的条形图代码：
- en: '[PRE45]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We start by creating a figure object with a title. The figure will have a subplot
    that will contain the bar chart. We also set several labels and only used defaults.
    Here is the sad result:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个带有标题的图形对象。该图形将包含一个子图，用于显示条形图。我们还设置了几个标签，并且仅使用了默认设置。以下是令人沮丧的结果：
- en: '![Figure 2.4 – Our first chart attempt, just using the defaults ](img/B17942_02_004.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 我们的第一次图表尝试，使用默认设置](img/B17942_02_004.jpg)'
- en: Figure 2.4 – Our first chart attempt, just using the defaults
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 我们的第一次图表尝试，使用默认设置
- en: 'Surely, we can do better. Let’s format the chart substantially more:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当然，我们可以做得更好。让我们对图表进行更大幅度的格式化：
- en: '[PRE46]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The first thing that we do is set up a bigger figure for Matplotlib to provide
    a tighter layout. We rotate the *x*-axis tick labels 45 degrees so that they fit
    better. We also put the values on the bars. Finally, we do not have a standard
    *x*-axis label as it would be on top of the tick labels. Instead, we write the
    text explicitly. Note that the coordinate system of the figure can be completely
    different from the coordinate system of the subplot – for example, compare the
    coordinates of `ax.text` and `fig.text`. Here is the result:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做的第一件事是为 Matplotlib 设置一个更大的图形，以提供更紧凑的布局。我们将 *x* 轴的刻度标签旋转 45 度，使其更合适地显示。我们还在条形上标注了数值。最后，我们没有使用标准的
    *x* 轴标签，因为它会遮挡住刻度标签。相反，我们明确写出了文本。请注意，图形的坐标系与子图的坐标系可能完全不同——例如，比较 `ax.text` 和 `fig.text`
    的坐标。以下是结果：
- en: '![Figure 2.5 – Our second chart attempt, while taking care of the layout ](img/B17942_02_005.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5 – 我们的第二次图表尝试，已考虑布局问题](img/B17942_02_005.jpg)'
- en: Figure 2.5 – Our second chart attempt, while taking care of the layout
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 我们的第二次图表尝试，已考虑布局问题
- en: 'Now, we are going to do some summary analysis of our data based on four plots
    on a single figure. We will chart the vaccines involved in deaths, the days between
    administration and death, the deaths over time, and the sex of people who have
    died for the top 10 states in terms of their quantity:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将根据一个图形上的四个子图来对数据进行一些汇总分析。我们将展示与死亡相关的疫苗、接种与死亡之间的天数、随时间变化的死亡情况以及十大州的死亡者性别：
- en: '[PRE47]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The preceding code is strictly pandas-based and was made in preparation for
    the plotting activity.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码完全基于 pandas，并为绘图活动做了准备。
- en: 'The following code plots all the information simultaneously. We are going to
    have four subplots organized in 2 by 2 format:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码同时绘制所有信息。我们将有四个子图，按 2x2 格式排列：
- en: '[PRE48]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We start by creating a figure with 2x2 subplots. The `subplots` function returns,
    along with the figure object, four axes objects that we can use to create our
    charts. Note that the legend is positioned in the pie chart, we have used a twin
    axis on the time distance plot, and we have a way to compute stacked bars on the
    death per state chart. Here is the result:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个 2x2 的子图图形。`subplots` 函数返回图形对象及四个坐标轴对象，我们可以使用这些坐标轴对象来创建我们的图表。请注意，图例位于饼图中，我们在时间距离图上使用了双坐标轴，并且在每个州的死亡率图上计算了堆积条形图。以下是结果：
- en: '![Figure 2.6 – Four combined charts summarizing the vaccine data ](img/B17942_02_006.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 汇总疫苗数据的四个合并图表](img/B17942_02_006.jpg)'
- en: Figure 2.6 – Four combined charts summarizing the vaccine data
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 汇总疫苗数据的四个合并图表
- en: There’s more...
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Matplotlib has two interfaces you can use – an older interface, designed to
    be similar to MATLAB, and a more powerful `matplotlib.pyplot` module. To make
    things confusing, the entry points for the OO interface are in that module – that
    is, `matplotlib.pyplot.figure` and `matplotlib.pyplot.subplots`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib 有两个接口可供使用——一个较旧的接口，设计上类似于 MATLAB，另一个是功能更强大的 `matplotlib.pyplot` 模块。为了增加混淆，面向对象接口的入口点就在这个模块中——即
    `matplotlib.pyplot.figure` 和 `matplotlib.pyplot.subplots`。
- en: See also
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The following is some extra information that may be useful:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可能有用的额外信息：
- en: The documentation for Matplolib is really, really good. For example, there’s
    a gallery of visual samples with links to the code for generating each sample.
    This can be found at [https://matplotlib.org/stable/gallery/index.xhtml](https://matplotlib.org/stable/gallery/index.xhtml).
    The API documentation is generally very complete.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib 的文档真的非常出色。例如，它提供了一个包含每个示例代码链接的可视化样本画廊。你可以在[https://matplotlib.org/stable/gallery/index.xhtml](https://matplotlib.org/stable/gallery/index.xhtml)找到这个画廊。API
    文档通常非常完整。
- en: Another way to improve the looks of Matplotlib charts is to use the Seaborn
    library. Seaborn’s main purpose is to add statistical visualization artifacts,
    but as a side effect, when imported, it changes the defaults of Matplotlib to
    something more palatable. We will be using Seaborn throughout this book; check
    out the plots provided in the next chapter.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善 Matplotlib 图表外观的另一种方法是使用 Seaborn 库。Seaborn 的主要目的是添加统计可视化工具，但作为副作用，它在导入时会将
    Matplotlib 的默认设置更改为更易接受的样式。我们将在本书中始终使用 Seaborn；请查看下一章中提供的图表。
