- en: Chapter 2. Apache Spark MLlib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。Apache Spark MLlib
- en: 'MLlib is the machine learning library that is provided with Apache Spark, the
    in memory cluster based open source data processing system. In this chapter, I
    will examine the functionality, provided within the MLlib library in terms of
    areas such as regression, classification, and neural processing. I will examine
    the theory behind each algorithm before providing working examples that tackle
    real problems. The example code and documentation on the web can be sparse and
    confusing. I will take a step-by-step approach in describing how the following
    algorithms can be used, and what they are capable of doing:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib是Apache Spark提供的机器学习库，它是基于内存的开源数据处理系统。在本章中，我将研究MLlib库提供的回归、分类和神经处理等领域的功能。我将在提供解决实际问题的工作示例之前，先研究每个算法背后的理论。网络上的示例代码和文档可能稀少且令人困惑。我将采用逐步的方法来描述以下算法的用法和能力。
- en: Classification with Naïve Bayes
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类
- en: Clustering with K-Means
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-Means聚类
- en: Neural processing with ANN
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ANN神经处理
- en: Having decided to learn about Apache Spark, I am assuming that you are familiar
    with Hadoop. Before I proceed, I will explain a little about my environment. My
    Hadoop cluster is installed on a set of Centos 6.5 Linux 64 bit servers. The following
    section will describe the architecture in detail.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定学习Apache Spark之前，我假设你对Hadoop很熟悉。在继续之前，我将简要介绍一下我的环境。我的Hadoop集群安装在一组Centos
    6.5 Linux 64位服务器上。接下来的部分将详细描述架构。
- en: The environment configuration
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境配置
- en: 'Before delving into the Apache Spark modules, I wanted to explain the structure
    and version of Hadoop and Spark clusters that I will use in this book. I will
    be using the Cloudera CDH 5.1.3 version of Hadoop for storage and I will be using
    two versions of Spark: 1.0 and 1.3 in this chapter.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究Apache Spark模块之前，我想解释一下我在本书中将使用的Hadoop和Spark集群的结构和版本。我将在本章中使用Cloudera CDH
    5.1.3版本的Hadoop进行存储，并且我将使用两个版本的Spark：1.0和1.3。
- en: The earlier version is compatible with Cloudera software, and has been tested
    and packaged by them. It is installed as a set of Linux services from the Cloudera
    repository using the yum command. Because I want to examine the Neural Net technology
    that has not been released yet, I will also download and run the development version
    of Spark 1.3 from GitHub. This will be explained later in the chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 早期版本与Cloudera软件兼容，并经过了他们的测试和打包。它是作为一组Linux服务从Cloudera仓库使用yum命令安装的。因为我想要研究尚未发布的神经网络技术，我还将从GitHub下载并运行Spark
    1.3的开发版本。这将在本章后面进行解释。
- en: Architecture
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: 'The following diagram explains the structure of the small Hadoop cluster that
    I will use in this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表解释了我将在本章中使用的小型Hadoop集群的结构：
- en: '![Architecture](img/B01989_02_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![架构](img/B01989_02_01.jpg)'
- en: The previous diagram shows a five-node Hadoop cluster with a NameNode called
    **hc2nn**, and DataNodes **hc2r1m1** to **hc2r1m4**. It also shows an Apache Spark
    cluster with a master node and four slave nodes. The Hadoop cluster provides the
    physical Centos 6 Linux machines while the Spark cluster runs on the same hosts.
    For instance, the Spark master server runs on the Hadoop Name Node machine **hc2nn**,
    whereas the Spark **slave1** worker runs on the host **hc2r1m1**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了一个包含NameNode（称为hc2nn）和DataNodes（hc2r1m1到hc2r1m4）的五节点Hadoop集群。它还显示了一个包含一个主节点和四个从节点的Apache
    Spark集群。Hadoop集群提供了物理Centos 6 Linux机器，而Spark集群运行在同一台主机上。例如，Spark主服务器运行在Hadoop
    NameNode机器hc2nn上，而Spark从节点1运行在主机hc2r1m1上。
- en: The Linux server naming standard used higher up should be explained. For instance
    the Hadoop NameNode server is called hc2nn. The **h** in this server name means
    Hadoop, the **c** means cluster, and the **nn** means NameNode. So, hc2nn means
    Hadoop cluster 2 NameNode. Similarly, for the server hc2r1m1, the h means Hadoop
    the **c** means cluster the **r** means rack and the **m** means machine. So,
    the name stands for Hadoop cluster 2 rack 1 machine 1\. In a large Hadoop cluster,
    the machines will be organized into racks, so this naming standard means that
    the servers will be easy to locate.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Linux服务器命名标准应该解释得更清楚。例如，Hadoop NameNode服务器被称为hc2nn。这个服务器名字中的h代表Hadoop，c代表集群，nn代表NameNode。因此，hc2nn代表Hadoop集群2的NameNode。同样，对于服务器hc2r1m1，h代表Hadoop，c代表集群，r代表机架，m代表机器。因此，这个名字代表Hadoop集群2的机架1的机器1。在一个大型的Hadoop集群中，机器会被组织成机架，因此这种命名标准意味着服务器很容易被定位。
- en: You can arrange your Spark and Hadoop clusters as you see fit, they don't need
    to be on the same hosts. For the purpose of writing this book, I have limited
    machines available so it makes sense to co-locate the Hadoop and Spark clusters.
    You can use entirely separate machines for each cluster, as long as Spark is able
    to access Hadoop (if you want to use it for distributed storage).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据自己的需要安排Spark和Hadoop集群，它们不需要在同一台主机上。为了撰写本书，我只有有限的机器可用，因此将Hadoop和Spark集群放在同一台主机上是有意义的。你可以为每个集群使用完全独立的机器，只要Spark能够访问Hadoop（如果你想用它来进行分布式存储）。
- en: Remember that although Spark is used for the speed of its in-memory distributed
    processing, it doesn't provide storage. You can use the Host file system to read
    and write your data, but if your data volumes are big enough to be described as
    big data, then it makes sense to use a distributed storage system like Hadoop.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，尽管Spark用于其内存分布式处理的速度，但它并不提供存储。你可以使用主机文件系统来读写数据，但如果你的数据量足够大，可以被描述为大数据，那么使用像Hadoop这样的分布式存储系统是有意义的。
- en: Remember also that Apache Spark may only be the processing step in your **ETL**
    (**Extract**, **Transform**, **Load**) chain. It doesn't provide the rich tool
    set that the Hadoop ecosystem contains. You may still need Nutch/Gora/Solr for
    data acquisition; Sqoop and Flume for moving data; Oozie for scheduling; and HBase,
    or Hive for storage. The point that I am making is that although Apache Spark
    is a very powerful processing system, it should be considered a part of the wider
    Hadoop ecosystem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 还要记住，Apache Spark可能只是**ETL**（**提取**，**转换**，**加载**）链中的处理步骤。它并不提供Hadoop生态系统所包含的丰富工具集。您可能仍然需要Nutch/Gora/Solr进行数据采集；Sqoop和Flume用于数据传输；Oozie用于调度；HBase或Hive用于存储。我要说明的是，尽管Apache
    Spark是一个非常强大的处理系统，但它应被视为更广泛的Hadoop生态系统的一部分。
- en: Having described the environment that will be used in this chapter, I will move
    on to describe the functionality of the Apache Spark **MLlib** (**Machine Learning
    library**).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述了本章将使用的环境之后，我将继续描述Apache Spark **MLlib**（**机器学习库**）的功能。
- en: The development environment
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发环境
- en: 'The Scala language will be used for coding samples in this book. This is because
    as a scripting language, it produces less code than Java. It can also be used
    for the Spark shell, as well as compiled with Apache Spark applications. I will
    be using the sbt tool to compile the Scala code, which I have installed as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的编码示例将使用Scala语言。这是因为作为一种脚本语言，它产生的代码比Java少。它也可以用于Spark shell，并与Apache Spark应用程序一起编译。我将使用sbt工具来编译Scala代码，安装方法如下：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For convenience while writing this book, I have used the generic Linux account
    called **hadoop** on the Hadoop NameNode server `hc2nn`. As the previous commands
    show that I need to install `sbt` as the root account, which I have accessed via
    `su` (switch user). I have then downloaded the `sbt.rpm` file, to the `/tmp` directory,
    from the web-based server called `repo.scala-sbt.org` using `wget`. Finally, I
    have installed the `rpm` file using the `rpm` command with the options `i` for
    install, `v` for verify, and `h` to print the hash marks while the package is
    being installed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便撰写本书，我在Hadoop NameNode服务器`hc2nn`上使用了名为**hadoop**的通用Linux帐户。由于前面的命令表明我需要以root帐户安装`sbt`，因此我通过`su`（切换用户）访问了它。然后，我使用`wget`从名为`repo.scala-sbt.org`的基于Web的服务器下载了`sbt.rpm`文件到`/tmp`目录。最后，我使用`rpm`命令安装了`rpm`文件，选项为`i`表示安装，`v`表示验证，`h`表示在安装包时打印哈希标记。
- en: Tip
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Downloading the example code**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载示例代码**'
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[http://www.packtpub.com](http://www.packtpub.com)的帐户中下载您购买的所有Packt Publishing图书的示例代码文件。如果您在其他地方购买了这本书，可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便直接通过电子邮件接收文件。
- en: 'I have developed all of the Scala code for Apache Spark, in this chapter, on
    the Linux server `hc2nn`, using the Linux hadoop account. I have placed each set
    of code within a sub directory under `/home/hadoop/spark`. For instance, the following
    sbt structure diagram shows that the MLlib Naïve Bayes code is stored within a
    subdirectory called `nbayes`, under the `spark` directory. What the diagram also
    shows is that the Scala code is developed within a subdirectory structure named
    `src/main/scala`, under the `nbayes` directory. The files called `bayes1.scala`
    and `convert.scala` contain the Naïve Bayes code that will be used in the next
    section:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我在Linux服务器`hc2nn`上使用Linux hadoop帐户为Apache Spark开发了所有Scala代码。我将每组代码放在`/home/hadoop/spark`目录下的子目录中。例如，以下sbt结构图显示了MLlib朴素贝叶斯代码存储在`spark`目录下名为`nbayes`的子目录中。图表还显示了Scala代码是在名为`src/main/scala`的子目录结构下开发的。文件`bayes1.scala`和`convert.scala`包含了下一节将使用的朴素贝叶斯代码：
- en: '![The development environment](img/B01989_02_02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![开发环境](img/B01989_02_02.jpg)'
- en: The `bayes.sbt` file is a configuration file used by the sbt tool, which describes
    how to compile the Scala files within the `Scala` directory (also note that if
    you were developing in Java, you would use a path of the form `nbayes/src/main/java`).
    The contents of the `bayes.sbt` file are shown next. The `pwd` and `cat` Linux
    commands remind you of the file location, and they also remind you to dump the
    file contents.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`bayes.sbt`文件是sbt工具使用的配置文件，描述了如何编译`Scala`目录中的Scala文件（还要注意，如果您在Java中开发，您将使用形式为`nbayes/src/main/java`的路径）。下面显示了`bayes.sbt`文件的内容。`pwd`和`cat`
    Linux命令提醒您文件位置，并提醒您转储文件内容。'
- en: 'The name, version, and `scalaVersion` options set the details of the project,
    and the version of Scala to be used. The `libraryDependencies` options define
    where the Hadoop and Spark libraries can be located. In this case, CDH5 has been
    installed using the Cloudera parcels, and the packages libraries can be located
    in the standard locations, that is, `/usr/lib/hadoop` for Hadoop and `/usr/lib/spark`
    for Spark. The resolver''s option specifies the location for the Cloudera repository
    for other dependencies:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 名称、版本和`scalaVersion`选项设置了项目的详细信息，以及要使用的Scala版本。`libraryDependencies`选项定义了Hadoop和Spark库的位置。在这种情况下，使用Cloudera
    parcels安装了CDH5，并且包库可以在标准位置找到，即Hadoop的`/usr/lib/hadoop`和Spark的`/usr/lib/spark`。解析器选项指定了Cloudera存储库的位置以获取其他依赖项：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The Scala nbayes project code can be compiled from the `nbayes` sub directory
    using this command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Scala nbayes项目代码可以使用以下命令从`nbayes`子目录编译：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `sbt compile` command is used to compile the code into classes. The classes
    are then placed in the `nbayes/target/scala-2.10/classes` directory. The compiled
    classes can be packaged into a JAR file with this command:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用` sbt compile`命令将代码编译成类。然后将类放置在`nbayes/target/scala-2.10/classes`目录中。可以使用以下命令将编译后的类打包成JAR文件：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `sbt package` command will create a JAR file under the directory `nbayes/target/scala-2.10`.
    As the example in the *sbt structure diagram* shows the JAR file named `naive-bayes_2.10-1.0.jar`
    has been created after a successful compile and package. This JAR file, and the
    classes that it contains, can then be used in a `spark-submit` command. This will
    be described later as the functionality in the Apache Spark MLlib module is explored.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sbt package`命令将在目录`nbayes/target/scala-2.10`下创建一个JAR文件。正如*sbt结构图*中的示例所示，成功编译和打包后，创建了名为`naive-bayes_2.10-1.0.jar`的JAR文件。然后，可以在`spark-submit`命令中使用此JAR文件及其包含的类。随后将在探索Apache
    Spark MLlib模块中描述此功能。'
- en: Installing Spark
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Spark
- en: Finally, when describing the environment used for this book, I wanted to touch
    on the approach to installing and running Apache Spark. I won't elaborate on the
    Hadoop CDH5 install, except to say that I installed it using the Cloudera parcels.
    However, I manually installed version 1.0 of Apache Spark from the Cloudera repository,
    using the Linux `yum` commands. I installed the service-based packages, because
    I wanted the flexibility that would enable me to install multiple versions of
    Spark as services from Cloudera, as I needed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当描述用于本书的环境时，我想谈谈安装和运行Apache Spark的方法。我不会详细说明Hadoop CDH5的安装，只是说我使用Cloudera
    parcels进行了安装。但是，我手动从Cloudera存储库安装了Apache Spark的1.0版本，使用了Linux的`yum`命令。我安装了基于服务的软件包，因为我希望能够灵活安装Cloudera的多个版本的Spark作为服务，根据需要进行安装。
- en: When preparing a CDH Hadoop release, Cloudera takes the code that has been developed
    by the Apache Spark team, and the code released by the Apache Bigtop project.
    They perform an integration test so that it is guaranteed to work as a code stack.
    They also reorganize the code and binaries into services and parcels. This means
    that libraries, logs, and binaries can be located in defined locations under Linux,
    that is, `/var/log/spark`, `/usr/lib/spark`. It also means that, in the case of
    services, the components can be installed using the Linux `yum` command, and managed
    via the Linux `service` command.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备CDH Hadoop版本时，Cloudera使用Apache Spark团队开发的代码和Apache Bigtop项目发布的代码。他们进行集成测试，以确保作为代码堆栈工作。他们还将代码和二进制文件重新组织为服务和包。这意味着库、日志和二进制文件可以位于Linux下的定义位置，即`/var/log/spark`、`/usr/lib/spark`。这也意味着，在服务的情况下，可以使用Linux的`yum`命令安装组件，并通过Linux的`service`命令进行管理。
- en: 'Although, in the case of the Neural Network code described later in this chapter,
    a different approach was used. This is how Apache Spark 1.0 was installed for
    use with Hadoop CDH5:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在本章后面描述的神经网络代码的情况下，使用了不同的方法。这是如何安装Apache Spark 1.0以与Hadoop CDH5一起使用的：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The first step is to ensure that a Cloudera repository file exists under the
    `/etc/yum.repos.d` directory, on the server `hc2nn` and all of the other Hadoop
    cluster servers. The file is called `cloudera-cdh5.repo`, and specifies where
    the yum command can locate software for the Hadoop CDH5 cluster. On all the Hadoop
    cluster nodes, I use the Linux yum command, as root, to install the Apache Spark
    components core, master, worker, history-server, and python:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是确保在服务器`hc2nn`和所有其他Hadoop集群服务器的`/etc/yum.repos.d`目录下存在Cloudera存储库文件。该文件名为`cloudera-cdh5.repo`，并指定yum命令可以定位Hadoop
    CDH5集群软件的位置。在所有Hadoop集群节点上，我使用Linux的yum命令，以root身份，安装Apache Spark组件核心、主、工作、历史服务器和Python：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This gives me the flexibility to configure Spark in any way that I want in
    the future. Note that I have installed the master component on all the nodes,
    even though I only plan to use it from the Name Node at this time. Now, the Spark
    install needs to be configured on all the nodes. The configuration files are stored
    under `/etc/spark/conf`. The first thing to do, will be to set up a `slaves` file,
    which specifies on which hosts Spark will run it''s worker components:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我能够在将来以任何我想要的方式配置Spark。请注意，我已经在所有节点上安装了主组件，尽管我目前只打算从Name Node上使用它。现在，需要在所有节点上配置Spark安装。配置文件存储在`/etc/spark/conf`下。首先要做的事情是设置一个`slaves`文件，指定Spark将在哪些主机上运行其工作组件：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see from the contents of the `slaves` file above Spark, it will
    run four workers on the Hadoop CDH5 cluster, Data Nodes, from `hc2r1m1` to `hc2r1m4`.
    Next, it will alter the contents of the `spark-env.sh` file to specify the Spark
    environment options. The `SPARK_MASTER_IP` values are defined as the full server
    name:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的`slaves`文件的内容可以看出，Spark将在Hadoop CDH5集群的四个工作节点Data Nodes上运行，从`hc2r1m1`到`hc2r1m4`。接下来，将更改`spark-env.sh`文件的内容以指定Spark环境选项。`SPARK_MASTER_IP`的值被定义为完整的服务器名称：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The web user interface port numbers are specified for the master and worker
    processes, as well as the operational port numbers. The Spark service can then
    be started as root from the Name Node server. I use the following script:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 主和工作进程的Web用户界面端口号已经指定，以及操作端口号。然后，Spark服务可以从Name Node服务器以root身份启动。我使用以下脚本：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This starts the Spark worker service on all of the slaves, and the master and
    history server on the Name Node `hc2nn`. So now, the Spark user interface can
    be accessed using the `http://hc2nn:18080` URL.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在所有从节点上启动Spark工作服务，并在Name Node `hc2nn`上启动主和历史服务器。因此，现在可以使用`http://hc2nn:18080`
    URL访问Spark用户界面。
- en: The following figure shows an example of the Spark 1.0 master web user interface.
    It shows details about the Spark install, the workers, and the applications that
    are running or completed. The statuses of the master and workers are given. In
    this case, all are alive. Memory used and availability is given in total and by
    worker. Although, there are no applications running at the moment, each worker
    link can be selected to view the executor processes' running on each worker node,
    as the work volume for each application run is spread across the spark cluster.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了Spark 1.0主节点Web用户界面的示例。它显示了有关Spark安装、工作节点和正在运行或已完成的应用程序的详细信息。给出了主节点和工作节点的状态。在这种情况下，所有节点都是活动的。显示了总内存使用情况和可用情况，以及按工作节点的情况。尽管目前没有应用程序在运行，但可以选择每个工作节点链接以查看在每个工作节点上运行的执行器进程，因为每个应用程序运行的工作量都分布在Spark集群中。
- en: Note also the Spark URL, `spark://hc2nn.semtech-solutions.co.nz:7077`, will
    be used when running the Spark applications like `spark-shell` and `spark-submit`.
    Using this URL, it is possible to ensure that the shell or application is run
    against this Spark cluster.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意Spark URL，`spark://hc2nn.semtech-solutions.co.nz:7077`，在运行Spark应用程序（如`spark-shell`和`spark-submit`）时将被使用。使用此URL，可以确保对该Spark集群运行shell或应用程序。
- en: '![Installing Spark](img/B01989_02_03.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![安装Spark](img/B01989_02_03.jpg)'
- en: This gives a quick overview of the Apache Spark installation using services,
    its configuration, how to start it, and how to monitor it. Now, it is time to
    tackle the first of the MLlib functional areas, which is classification using
    the Naïve Bayes algorithm. The use of Spark will become clearer as Scala scripts
    are developed, and the resulting applications are monitored.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这快速概述了使用服务的Apache Spark安装、其配置、如何启动以及如何监视。现在，是时候着手处理MLlib功能领域中的第一个部分，即使用朴素贝叶斯算法进行分类。随着Scala脚本的开发和生成的应用程序的监视，Spark的使用将变得更加清晰。
- en: Classification with Naïve Bayes
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类
- en: This section will provide a working example of the Apache Spark MLlib Naïve
    Bayes algorithm. It will describe the theory behind the algorithm, and will provide
    a step-by-step example in Scala to show how the algorithm may be used.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将提供Apache Spark MLlib朴素贝叶斯算法的工作示例。它将描述算法背后的理论，并提供一个Scala的逐步示例，以展示如何使用该算法。
- en: Theory
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论
- en: 'In order to use the Naïve Bayes algorithm to classify a data set, the data
    must be linearly divisible, that is, the classes within the data must be linearly
    divisible by class boundaries. The following figure visually explains this with
    three data sets, and two class boundaries shown via the dotted lines:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用朴素贝叶斯算法对数据集进行分类，数据必须是线性可分的，也就是说，数据中的类必须能够通过类边界进行线性划分。以下图形通过三个数据集和两个虚线所示的类边界来直观解释这一点：
- en: '![Theory](img/B01989_02_04.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![理论](img/B01989_02_04.jpg)'
- en: 'Naïve Bayes assumes that the features (or dimensions) within a data set are
    independent of one another, that is, they have no effect on each other. An example
    for Naïve Bayes is supplied with the help of Hernan Amiune at [http://hernan.amiune.com/](http://hernan.amiune.com/).
    The following example considers the classification of emails as spam. If you have
    100 e-mails then perform the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯假设数据集中的特征（或维度）彼此独立，即它们互不影响。Hernan Amiune在[http://hernan.amiune.com/](http://hernan.amiune.com/)提供了朴素贝叶斯的一个例子。以下例子考虑将电子邮件分类为垃圾邮件。如果你有100封电子邮件，那么执行以下操作：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Thus, convert this example into probabilities, so that a Naïve Bayes equation
    can be created.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将这个例子转换为概率，以便创建一个朴素贝叶斯方程。
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So, what is the probability that an e-mail that contains the word buy is spam?
    Well, this would be written as **P (Spam|Buy)**. Naïve Bayes says that it is described
    by the equation in the following figure:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，包含单词“buy”的电子邮件是垃圾邮件的概率是多少？这将被写成**P(垃圾邮件|Buy)**。朴素贝叶斯表示它由以下图中的方程描述：
- en: '![Theory](img/B01989_02_05.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![理论](img/B01989_02_05.jpg)'
- en: 'So, using the previous percentage figures, we get the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用先前的百分比数据，我们得到以下结果：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This means that it is 92 percent more likely that an e-mail that contains the
    word buy is spam. That was a look at the theory; now, it's time to try a real
    world example using the Apache Spark MLlib Naïve Bayes algorithm.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着包含单词“buy”的电子邮件更有可能是垃圾邮件，概率高达92%。这是对理论的一瞥；现在，是时候尝试使用Apache Spark MLlib朴素贝叶斯算法进行一个真实世界的例子了。
- en: Naïve Bayes in practice
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中的朴素贝叶斯
- en: 'The first step is to choose some data that will be used for classification.
    I have chosen some data from the UK government data web site, available at: [http://data.gov.uk/dataset/road-accidents-safety-data](http://data.gov.uk/dataset/road-accidents-safety-data).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是选择一些用于分类的数据。我选择了来自英国政府数据网站的一些数据，可在[http://data.gov.uk/dataset/road-accidents-safety-data](http://data.gov.uk/dataset/road-accidents-safety-data)上找到。
- en: 'The data set is called "Road Safety - Digital Breath Test Data 2013," which
    downloads a zipped text file called `DigitalBreathTestData2013.txt`. This file
    contains around half a million rows. The data looks like this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集名为“道路安全-2013年数字酒精测试数据”，下载一个名为`DigitalBreathTestData2013.txt`的压缩文本文件。该文件包含大约50万行数据。数据如下所示：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In order to classify the data, I have modified both the column layout, and
    the number of columns. I have simply used Excel to give the data volume. However,
    if my data size had been in the big data range, I would have had to use Scala,
    or perhaps a tool like Apache Pig. As the following commands show, the data now
    resides on HDFS, in the directory named `/data/spark/nbayes`. The file name is
    called `DigitalBreathTestData2013- MALE2.csv`. Also, the line count from the Linux
    `wc` command shows that there are 467,000 rows. Finally, the following data sample
    shows that I have selected the columns: Gender, Reason, WeekType, TimeBand, BreathAlcohol,
    and AgeBand to classify. I will try and classify on the Gender column using the
    other columns as features:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对数据进行分类，我修改了列布局和列数。我只是使用Excel来给出数据量。但是，如果我的数据量达到了大数据范围，我可能需要使用Scala，或者像Apache
    Pig这样的工具。如下命令所示，数据现在存储在HDFS上，目录名为`/data/spark/nbayes`。文件名为`DigitalBreathTestData2013-
    MALE2.csv`。此外，来自Linux `wc`命令的行数显示有467,000行。最后，以下数据样本显示我已经选择了列：Gender, Reason,
    WeekType, TimeBand, BreathAlcohol和AgeBand进行分类。我将尝试使用其他列作为特征对Gender列进行分类：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The Apache Spark MLlib classification functions use a data structure called
    `LabeledPoint`, which is a general purpose data representation defined at: [http://spark.apache.org/docs/1.0.0/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint](http://spark.apache.org/docs/1.0.0/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark MLlib分类函数使用一个名为`LabeledPoint`的数据结构，这是一个通用的数据表示，定义在：[http://spark.apache.org/docs/1.0.0/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint](http://spark.apache.org/docs/1.0.0/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint)。
- en: This structure only accepts Double values, which means the text values in the
    previous data need to be classified numerically. Luckily, all of the columns in
    the data will convert to numeric categories, and I have provided two programs
    in the software package with this book, under the directory `chapter2\naive bayes`
    to do just that. The first is called `convTestData.pl`, and is a Perl script to
    convert the previous text file into Linux. The second file, which will be examined
    here is called `convert.scala`. It takes the contents of the `DigitalBreathTestData2013-
    MALE2.csv` file and converts each record into a Double vector.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结构只接受Double值，这意味着前面数据中的文本值需要被分类为数字。幸运的是，数据中的所有列都将转换为数字类别，我已经在本书的软件包中提供了两个程序，在`chapter2\naive
    bayes`目录下。第一个叫做`convTestData.pl`，是一个Perl脚本，用于将以前的文本文件转换为Linux。第二个文件，将在这里进行检查，名为`convert.scala`。它将`DigitalBreathTestData2013-
    MALE2.csv`文件的内容转换为Double向量。
- en: 'The directory structure and files for an sbt Scala-based development environment
    have already been described earlier. I am developing my Scala code on the Linux
    server `hc2nn` using the Linux account hadoop. Next, the Linux `pwd` and `ls`
    commands show my top level `nbayes` development directory with the `bayes.sbt`
    configuration file, whose contents have already been examined:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于sbt Scala的开发环境的目录结构和文件已经在前面进行了描述。我正在使用Linux账户hadoop在Linux服务器`hc2nn`上开发我的Scala代码。接下来，Linux的`pwd`和`ls`命令显示了我的顶级`nbayes`开发目录，其中包含`bayes.sbt`配置文件，其内容已经被检查过：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The Scala code to run the Naïve Bayes example is shown next, in the `src/main/scala`
    subdirectory, under the `nbayes` directory:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来显示了运行朴素贝叶斯示例的Scala代码，在`src/main/scala`子目录下的`nbayes`目录中：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will examine the `bayes1.scala` file later, but first, the text-based data
    on HDFS must be converted into the numeric Double values. This is where the `convert.scala`
    file is used. The code looks like this:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后将检查`bayes1.scala`文件，但首先，HDFS上的基于文本的数据必须转换为数值Double值。这就是`convert.scala`文件的用途。代码如下：
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'These lines import classes for Spark context, the connection to the Apache
    Spark cluster, and the Spark configuration. The object that is being created is
    called `convert1`. It is an application, as it extends the class `App`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行导入了Spark上下文的类，连接到Apache Spark集群的类，以及Spark配置。正在创建的对象名为`convert1`。它是一个应用程序，因为它扩展了类`App`：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The next line creates a function called `enumerateCsvRecord`. It has a parameter
    called `colData`, which is an array of strings, and returns a string:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行创建了一个名为`enumerateCsvRecord`的函数。它有一个名为`colData`的参数，它是一个字符串数组，并返回一个字符串：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The function then enumerates the text values in each column, so for an instance,
    `Male` becomes `0`. These numeric values are stored in values like `colVal1`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，该函数枚举每一列中的文本值，例如，`Male` 变成 `0`。这些数值存储在像 `colVal1` 这样的值中：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'A comma separated string called `lineString` is created from the numeric column
    values, and is then returned. The function closes with the final brace character`}`.
    Note that the data line created next starts with a label value at column one,
    and is followed by a vector, which represents the data. The vector is space separated
    while the label is separated from the vector by a comma. Using these two separator
    types allows me to process both: the label and the vector in two simple steps
    later:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从数值列值创建一个逗号分隔的字符串`lineString`，然后返回它。函数以最终的大括号字符`}`结束。请注意，下一个创建的数据行从第一列的标签值开始，然后是一个代表数据的向量。向量是以空格分隔的，而标签与向量之间用逗号分隔。使用这两种分隔符类型可以让我稍后以两个简单的步骤处理标签和向量：
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The main script defines the HDFS server name and path. It defines the input
    file, and the output path in terms of these values. It uses the Spark URL and
    application name to create a new configuration. It then creates a new context
    or connection to Spark using these details:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 主脚本定义了HDFS服务器名称和路径。它定义了输入文件和输出路径，使用这些值。它使用Spark URL和应用程序名称创建一个新的配置。然后使用这些详细信息创建一个新的Spark上下文或连接：
- en: '[PRE21]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The CSV-based raw data file is loaded from HDFS using the Spark context `textFile`
    method. Then, a data row count is printed:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark上下文的`textFile`方法从HDFS加载基于CSV的原始数据文件。然后打印数据行数：
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The CSV raw data is passed line by line to the `enumerateCsvRecord` function.
    The returned string-based numeric data is stored in the `enumRddData` variable:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: CSV原始数据逐行传递给`enumerateCsvRecord`函数。返回的基于字符串的数字数据存储在`enumRddData`变量中：
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, the number of records in the `enumRddData` variable is printed, and
    the enumerated data is saved to HDFS:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，打印`enumRddData`变量中的记录数，并将枚举数据保存到HDFS中：
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In order to run this script as an application against Spark, it must be compiled.
    This is carried out with the `sbt` package command, which also compiles the code.
    The following command was run from the `nbayes` directory:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将此脚本作为Spark应用程序运行，必须对其进行编译。这是通过`package`命令来完成的，该命令还会编译代码。以下命令是从`nbayes`目录运行的：
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This causes the compiled classes that are created to be packaged into a JAR
    library, as shown here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致创建的编译类被打包成一个JAR库，如下所示：
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The application `convert1` can now be run against Spark using the application
    name, the Spark URL, and the full path to the JAR file that was created. Some
    extra parameters specify memory and maximum cores that are supposed to be used:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以使用应用程序名称、Spark URL和创建的JAR文件的完整路径来运行应用程序`convert1`。一些额外的参数指定了应该使用的内存和最大核心：
- en: '[PRE27]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This creates a data directory on HDFS called the `/data/spark/nbayes/` followed
    by the result, which contains part files, containing the processed data:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这在HDFS上创建了一个名为`/data/spark/nbayes/`的数据目录，随后是包含处理过的数据的部分文件：
- en: '[PRE28]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the following HDFS `cat` command, I have concatenated the part file data
    into a file called `DigitalBreathTestData2013-MALE2a.csv`. I have then examined
    the top five lines of the file using the `head` command to show that it is numeric.
    Finally, I have loaded it into HDFS with the `put` command:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下HDFS `cat`命令中，我已经将部分文件数据连接成一个名为`DigitalBreathTestData2013-MALE2a.csv`的文件。然后，我使用`head`命令检查了文件的前五行，以显示它是数字的。最后，我使用`put`命令将其加载到HDFS中：
- en: '[PRE29]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following HDFS `ls` command now shows the numeric data file stored on HDFS,
    in the `nbayes` directory:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下HDFS `ls`命令现在显示了存储在HDFS上的数字数据文件，位于`nbayes`目录中：
- en: '[PRE30]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that the data has been converted into a numeric form, it can be processed
    with the MLlib Naïve Bayes algorithm; this is what the Scala file `bayes1.scala`
    does. This file imports the same configuration and context classes as before.
    It also imports MLlib classes for Naïve Bayes, vectors, and the LabeledPoint structure.
    The application class that is created this time is called `bayes1`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已转换为数字形式，可以使用MLlib朴素贝叶斯算法进行处理；这就是Scala文件`bayes1.scala`的作用。该文件导入了与之前相同的配置和上下文类。它还导入了朴素贝叶斯、向量和LabeledPoint结构的MLlib类。这次创建的应用程序类名为`bayes1`：
- en: '[PRE31]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Again, the HDFS data file is defined, and a Spark context is created as before:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 再次定义HDFS数据文件，并像以前一样创建一个Spark上下文：
- en: '[PRE32]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The raw CSV data is loaded and split by the separator characters. The first
    column becomes the label (`Male/Female`) that the data will be classified upon.
    The final columns separated by spaces become the classification features:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 原始CSV数据被加载并按分隔符字符拆分。第一列成为数据将被分类的标签（`男/女`）。最后由空格分隔的列成为分类特征：
- en: '[PRE33]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The data is then randomly divided into training (70%) and testing (30%) data
    sets:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，数据被随机分成训练（70%）和测试（30%）数据集：
- en: '[PRE34]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The Naïve Bayes MLlib function can now be trained using the previous training
    set. The trained Naïve Bayes model, held in the variable `nbTrained`, can then
    be used to predict the `Male/Female` result labels against the testing data:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以使用先前的训练集来训练朴素贝叶斯MLlib函数。训练后的朴素贝叶斯模型存储在变量`nbTrained`中，然后可以用于预测测试数据的`男/女`结果标签：
- en: '[PRE35]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Given that all of the data already contained labels, the original and predicted
    labels for the test data can be compared. An accuracy figure can then be computed
    to determine how accurate the predictions were, by comparing the original labels
    with the prediction values:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于所有数据已经包含标签，可以比较测试数据的原始和预测标签。然后可以计算准确度，以确定预测与原始标签的匹配程度：
- en: '[PRE36]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'So this explains the Scala Naïve Bayes code example. It''s now time to run
    the compiled `bayes1` application using `spark-submit`, and to determine the classification
    accuracy. The parameters are the same. It''s just the class name that has changed:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了Scala朴素贝叶斯代码示例。现在是时候使用`spark-submit`运行编译后的`bayes1`应用程序，并确定分类准确度。参数是相同的。只是类名已经改变：
- en: '[PRE37]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The resulting accuracy given by the Spark cluster is just `43` percent, which
    seems to imply that this data is not suitable for Naïve Bayes:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Spark集群给出的准确度只有`43`％，这似乎意味着这些数据不适合朴素贝叶斯：
- en: '[PRE38]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In the next example, I will use K-Means to try and determine what clusters
    exist within the data. Remember, Naïve Bayes needs the data classes to be linearly
    divisible along the class boundaries. With K-Means, it will be possible to determine
    both: the membership and centroid location of the clusters within the data.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我将使用K-Means来尝试确定数据中存在的聚类。请记住，朴素贝叶斯需要数据类沿着类边界线性可分。使用K-Means，将能够确定数据中的成员资格和聚类的中心位置。
- en: Clustering with K-Means
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用K-Means进行聚类
- en: This example will use the same test data from the previous example, but will
    attempt to find clusters in the data using the MLlib K-Means algorithm.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例将使用前一个示例中的相同测试数据，但将尝试使用MLlib K-Means算法在数据中找到聚类。
- en: Theory
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论
- en: The K-Means algorithm iteratively attempts to determine clusters within the
    test data by minimizing the distance between the mean value of cluster center
    vectors, and the new candidate cluster member vectors. The following equation
    assumes data set members that range from **X1** to **Xn**; it also assumes **K**
    cluster sets that range from **S1** to **Sk** where **K <= n**.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means算法通过迭代尝试确定测试数据中的聚类，方法是最小化聚类中心向量的平均值与新候选聚类成员向量之间的距离。以下方程假设数据集成员的范围从**X1**到**Xn**；它还假设了从**S1**到**Sk**的**K**个聚类集，其中**K
    <= n**。
- en: '![Theory](img/B01989_02_06.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![Theory](img/B01989_02_06.jpg)'
- en: K-Means in practice
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际中的K-Means
- en: Again, the K-Means MLlib functionality uses the LabeledPoint structure to process
    its data and so, it needs numeric input data. As the same data from the last section
    is being reused, I will not re-explain the data conversion. The only change that
    has been made in data terms, in this section, is that processing under HDFS will
    now take place under the `/data/spark/kmeans/` directory**.** Also, the conversion
    Scala script for the K-Means example produces a record that is all comma separated.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，K-Means MLlib功能使用LabeledPoint结构来处理其数据，因此需要数值输入数据。由于本节重复使用了上一节的相同数据，我不会重新解释数据转换。在本节中在数据方面唯一的变化是，HDFS下的处理现在将在`/data/spark/kmeans/`目录下进行。此外，K-Means示例的Scala脚本转换产生的记录是逗号分隔的。
- en: 'The development and processing for the K-Means example has taken place under
    the `/home/hadoop/spark/kmeans` directory, to separate the work from other development.
    The sbt configuration file is now called `kmeans.sbt`, and is identical to the
    last example, except for the project name:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means示例的开发和处理已经在`/home/hadoop/spark/kmeans`目录下进行，以便将工作与其他开发分开。sbt配置文件现在称为`kmeans.sbt`，与上一个示例相同，只是项目名称不同：
- en: '[PRE39]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The code for this section can be found in the software package under `chapter2\K-Means`.
    So, looking at the code for `kmeans1.scala`, which is stored under `kmeans/src/main/scala`,
    some similar actions occur. The import statements refer to Spark context and configuration.
    This time, however, the K-Means functionality is also being imported from MLlib.
    Also, the application class name has been changed for this example to `kmeans1`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分的代码可以在软件包的`chapter2\K-Means`目录下找到。因此，查看存储在`kmeans/src/main/scala`下的`kmeans1.scala`的代码，会发生一些类似的操作。导入语句涉及到Spark上下文和配置。然而，这次K-Means功能也被从MLlib中导入。此外，本示例的应用程序类名已更改为`kmeans1`：
- en: '[PRE40]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The same actions are being taken as the last example to define the data file—define
    the Spark configuration and create a Spark context:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一个示例一样，正在采取相同的操作来定义数据文件——定义Spark配置并创建Spark上下文：
- en: '[PRE41]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, the CSV data is loaded from the data file, and is split by comma characters
    into the variable `VectorData`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从数据文件加载了CSV数据，并按逗号字符分割为变量`VectorData`：
- en: '[PRE42]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'A K-Means object is initialized, and the parameters are set to define the number
    of clusters, and the maximum number of iterations to determine them:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化了一个K-Means对象，并设置了参数来定义簇的数量和确定它们的最大迭代次数：
- en: '[PRE43]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Some default values are defined for initialization mode, the number of runs,
    and Epsilon, which I needed for the K-Means call, but did not vary for processing.
    Finally, these parameters were set against the K-Means object:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为初始化模式、运行次数和Epsilon定义了一些默认值，这些值我需要用于K-Means调用，但在处理中没有变化。最后，这些参数被设置到K-Means对象中：
- en: '[PRE44]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'I cached the training vector data to improve the performance, and trained the
    K-Means object using the Vector Data to create a trained K-Means model:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我缓存了训练向量数据以提高性能，并使用向量数据训练了K-Means对象以创建训练过的K-Means模型：
- en: '[PRE45]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'I have computed the K-Means cost, the number of input data rows, and output
    the results via print line statements. The cost value indicates how tightly the
    clusters are packed, and how separated clusters are:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我计算了K-Means成本、输入数据行数，并通过打印行语句输出了结果。成本值表示簇有多紧密地打包在一起，以及簇之间有多分离：
- en: '[PRE46]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, I have used the K-Means Model to print the cluster centers as vectors
    for each of the three clusters that were computed:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我使用了K-Means模型来打印计算出的三个簇的簇中心作为向量：
- en: '[PRE47]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Finally, I have used the K-Means Model `predict` function to create a list
    of cluster membership predictions. I have then counted these predictions by value
    to give a count of the data points in each cluster. This shows which clusters
    are bigger, and if there really are three clusters:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我使用了K-Means模型的`predict`函数来创建簇成员预测列表。然后，我通过值来计算这些预测，以给出每个簇中数据点的计数。这显示了哪些簇更大，以及是否真的有三个簇：
- en: '[PRE48]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'So, in order to run this application, it must be compiled and packaged from
    the `kmeans` subdirectory as the Linux `pwd` command shows here:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了运行这个应用程序，必须从`kmeans`子目录编译和打包，如Linux的`pwd`命令所示：
- en: '[PRE49]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Once this packaging is successful, I check HDFS to ensure that the test data
    is ready. As in the last example, I converted my data to numeric form using the
    `convert.scala` file, provided in the software package. I will process the data
    file `DigitalBreathTestData2013-MALE2a.csv` in the HDFS directory /`data/spark/kmeans`
    shown here:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这个打包成功，我检查HDFS以确保测试数据已准备就绪。与上一个示例一样，我使用软件包中提供的`convert.scala`文件将我的数据转换为数值形式。我将在HDFS目录`/data/spark/kmeans`中处理数据文件`DigitalBreathTestData2013-MALE2a.csv`：
- en: '[PRE50]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The `spark-submit` tool is used to run the K-Means application. The only change
    in this command, as shown here, is that the class is now `kmeans1`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark-submit`工具用于运行K-Means应用程序。在这个命令中唯一的变化是，类现在是`kmeans1`：'
- en: '[PRE51]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output from the Spark cluster run is shown to be as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Spark集群运行的输出如下所示：
- en: '[PRE52]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The previous output shows the input data volume, which looks correct, plus
    it also shows the K-Means cost value. Next comes the three vectors, which describe
    the data cluster centers with the correct number of dimensions. Remember that
    these cluster centroid vectors will have the same number of columns as the original
    vector data:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的输出显示了输入数据量，看起来是正确的，还显示了K-Means成本值。接下来是三个向量，描述了具有正确维数的数据簇中心。请记住，这些簇中心向量将具有与原始向量数据相同的列数：
- en: '[PRE53]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Finally, cluster membership is given for clusters 1 to 3 with cluster 1 (index
    0) having the largest membership at `407,539` member vectors.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对1到3号簇的簇成员资格进行了给出，其中1号簇（索引0）的成员数量最多，为`407,539`个成员向量。
- en: '[PRE54]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: So, these two examples show how data can be classified and clustered using Naïve
    Bayes and K-Means. But what if I want to classify images or more complex patterns,
    and use a black box approach to classification? The next section examines Spark-based
    classification using **ANN's**, or **Artificial Neural Network's**. In order to
    do this, I need to download the latest Spark code, and build a server for Spark
    1.3, as it has not yet been formally released (at the time of writing).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这两个例子展示了如何使用朴素贝叶斯和K均值对数据进行分类和聚类。但是，如果我想对图像或更复杂的模式进行分类，并使用黑盒方法进行分类呢？下一节将介绍使用ANN（人工神经网络）进行基于Spark的分类。为了做到这一点，我需要下载最新的Spark代码，并为Spark
    1.3构建服务器，因为它在撰写本文时尚未正式发布。
- en: ANN – Artificial Neural Networks
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ANN - 人工神经网络
- en: In order to examine the **ANN** (Artificial Neural Network) functionality in
    Apache Spark, I will need to obtain the latest source code from the GitHub website.
    The ANN functionality has been developed by Bert Greevenbosch ([http://www.bertgreevenbosch.nl/](http://www.bertgreevenbosch.nl/)),
    and is set to be released in Apache Spark 1.3\. At the time of writing the current
    Spark release is 1.2.1, and CDH 5.x ships with Spark 1.0\. So, in order to examine
    this unreleased ANN functionality, the source code will need to be sourced and
    built into a Spark server. This is what I will do after explaining a little on
    the theory behind ANN.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究Apache Spark中的**ANN**（人工神经网络）功能，我需要从GitHub网站获取最新的源代码。**ANN**功能由Bert Greevenbosch
    ([http://www.bertgreevenbosch.nl/](http://www.bertgreevenbosch.nl/)) 开发，并计划在Apache
    Spark 1.3中发布。撰写本文时，当前的Spark版本是1.2.1，CDH 5.x附带的Spark版本是1.0。因此，为了研究这个未发布的**ANN**功能，需要获取源代码并构建成Spark服务器。这是我在解释一些**ANN**背后的理论之后将要做的事情。
- en: Theory
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论
- en: 'The following figure shows a simple biological neuron to the left. The neuron
    has dendrites that receive signals from other neurons. A cell body controls activation,
    and an axon carries an electrical impulse to the dendrites of other neurons. The
    artificial neuron to the right has a series of weighted inputs: a summing function
    that groups the inputs, and a firing mechanism (**F(Net)**), which decides whether
    the inputs have reached a threshold, and if so, the neuron will fire:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了左侧的一个简单的生物神经元。神经元有树突接收其他神经元的信号。细胞体控制激活，轴突将电脉冲传递给其他神经元的树突。右侧的人工神经元具有一系列加权输入：将输入分组的求和函数，以及一个触发机制（**F(Net)**），它决定输入是否达到阈值，如果是，神经元将发射：
- en: '![Theory](img/B01989_02_07.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![理论](img/B01989_02_07.jpg)'
- en: Neural networks are tolerant of noisy images and distortion, and so are useful
    when a black box classification method is needed for potentially degraded images.
    The next area to consider is the summation function for the neuron inputs. The
    following diagram shows the summation function called **Net** for neuron **i**.
    The connections between the neurons that have the weighting values, contain the
    stored knowledge of the network. Generally, a network will have an input layer,
    an output layer, and a number of hidden layers. A neuron will fire if the sum
    of its inputs exceeds a threshold.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络对嘈杂的图像和失真具有容忍性，因此在需要对潜在受损图像进行黑盒分类时非常有用。接下来要考虑的是神经元输入的总和函数。下图显示了神经元i的总和函数**Net**。具有加权值的神经元之间的连接包含网络的存储知识。通常，网络会有一个输入层，一个输出层和若干隐藏层。如果神经元的输入总和超过阈值，神经元将发射。
- en: '![Theory](img/B01989_02_08.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![理论](img/B01989_02_08.jpg)'
- en: In the previous equation, the diagram and the key show that the input values
    from a pattern **P** are passed to neurons in the input layer of a network. These
    values become the input layer neuron activation values; they are a special case.
    The inputs to neuron **i** are the sum of the weighting value for neuron connection
    **i-j**, multiplied by the activation from neuron **j**. The activation at neuron
    **j** (if it is not an input layer neuron) is given by **F(Net)**, the squashing
    function, which will be described next.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述方程中，图表和关键显示了来自模式**P**的输入值被传递到网络的输入层神经元。这些值成为输入层神经元的激活值；它们是一个特例。神经元**i**的输入是神经元连接**i-j**的加权值的总和，乘以神经元**j**的激活。神经元**j**的激活（如果它不是输入层神经元）由**F(Net)**，即压缩函数给出，接下来将对其进行描述。
- en: 'A simulated neuron needs a firing mechanism, which decides whether the inputs
    to the neuron have reached a threshold. And then, it fires to create the activation
    value for that neuron. This firing or squashing function can be described by the
    generalized sigmoid function shown in the following figure:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模拟神经元需要一个触发机制，决定神经元的输入是否达到了阈值。然后，它会发射以创建该神经元的激活值。这种发射或压缩功能可以用下图所示的广义S形函数来描述：
- en: '![Theory](img/B01989_02_09.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![理论](img/B01989_02_09.jpg)'
- en: 'This function has two constants: **A** and **B**; **B** affects the shape of
    the activation curve as shown in the previous graph. The bigger the value, the
    more similar a function becomes to an on/off step. The value of **A** sets a minimum
    for the returned activation. In the previous graph it is zero.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数有两个常数：**A**和**B**；**B**影响激活曲线的形状，如前图所示。数值越大，函数越类似于开/关步骤。**A**的值设置了返回激活的最小值。在前图中为零。
- en: So, this provides a mechanism for simulating a neuron, creating weighting matrices
    as the neuron connections, and managing the neuron activation. But how are the
    networks organized? The next diagram shows a suggested neuron architecture—the
    neural network has an input layer of neurons, an output layer, and one or more
    hidden layers. All neurons in each layer are connected to each neuron in the adjacent
    layers.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这提供了模拟神经元、创建权重矩阵作为神经元连接以及管理神经元激活的机制。但是网络是如何组织的呢？下图显示了一个建议的神经元架构 - 神经网络具有一个输入层的神经元，一个输出层和一个或多个隐藏层。每层中的所有神经元都与相邻层中的每个神经元相连。
- en: '![Theory](img/B01989_02_10.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![理论](img/B01989_02_10.jpg)'
- en: During the training, activation passes from the input layer through the network
    to the output layer. Then, the error or difference between the expected or actual
    output causes error deltas to be passed back through the network, altering the
    weighting matrix values. Once the desired output layer vector is achieved, then
    the knowledge is stored in the weighting matrices, and the network can be further
    trained or used for classification.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，激活从输入层通过网络传递到输出层。然后，期望的或实际输出之间的错误或差异导致错误增量通过网络传递回来，改变权重矩阵的值。一旦达到期望的输出层向量，知识就存储在权重矩阵中，网络可以进一步训练或用于分类。
- en: So, the theory behind neural networks has been described in terms of back propagation.
    Now is the time to obtain the development version of the Apache Spark code, and
    build the Spark server, so that the ANN Scala code can be run.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络背后的理论已经以反向传播的方式描述。现在是时候获取Apache Spark代码的开发版本，并构建Spark服务器，以便运行ANN Scala代码。
- en: Building the Spark server
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建Spark服务器
- en: 'I would not normally advise that Apache Spark code be downloaded and used before
    it has been released by Spark, or packaged by Cloudera (for use with CDH), but
    the desire to examine ANN functionality, along with the time scale allowed for
    this book, mean that I need to do so. I extracted the full Spark code tree from
    this path:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常不建议在Spark发布之前下载和使用Apache Spark代码，或者在Cloudera（用于CDH）打包，但是对ANN功能进行检查的愿望，以及本书允许的时间范围，意味着我需要这样做。我从这个路径提取了完整的Spark代码树：
- en: '[PRE55]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'I stored this code on the Linux server `hc2nn`, under the directory `/home/hadoop/spark/spark`.
    I then obtained the ANN code from Bert Greevenbosch''s GitHub development area:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我将这段代码存储在Linux服务器`hc2nn`的目录`/home/hadoop/spark/spark`下。然后我从Bert Greevenbosch的GitHub开发区域获取了ANN代码：
- en: '[PRE56]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The `ANNClassifier.scala` file contains the public functions that will be called.
    The `ArtificialNeuralNetwork.scala` file contains the private MLlib ANN functions
    that `ANNClassifier.scala` calls. I already have Java open JDK installed on my
    server, so the next step is to set up the `spark-env.sh` environment configuration
    file under `/home/hadoop/spark/spark/conf`. My file looks like this:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`ANNClassifier.scala`文件包含将被调用的公共函数。`ArtificialNeuralNetwork.scala`文件包含`ANNClassifier.scala`调用的私有MLlib
    ANN函数。我已经在服务器上安装了Java open JDK，所以下一步是在`/home/hadoop/spark/spark/conf`路径下设置`spark-env.sh`环境配置文件。我的文件如下：'
- en: '[PRE57]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The `SPARK_MASTER_IP` variable tells the cluster which server is the master.
    The port variables define the master, the worker web, and the operating port values.
    There are some log and JAR file paths defined, as well as `JAVA_HOME` and the
    local server IP address. Details for building Spark with Apache Maven can be found
    at:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`SPARK_MASTER_IP`变量告诉集群哪个服务器是主服务器。端口变量定义了主服务器、工作服务器web和操作端口值。还定义了一些日志和JAR文件路径，以及`JAVA_HOME`和本地服务器IP地址。有关使用Apache
    Maven构建Spark的详细信息，请参阅：'
- en: '[PRE58]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The slaves file in the same directory will be set up as before with the names
    of the four workers servers from `hc2r1m1` to `hc2r1m4`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同目录中的slaves文件将像以前一样设置为四个工作服务器的名称，从`hc2r1m1`到`hc2r1m4`。
- en: 'In order to build using Apache Maven, I had to install `mvn` on to my Linux
    server `hc2nn`, where I will run the Spark build. I did this as the root user,
    obtaining a Maven repository file by first using `wget`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用Apache Maven构建，我必须在我的Linux服务器`hc2nn`上安装`mvn`，我将在那里运行Spark构建。我以root用户的身份进行了这个操作，首先使用`wget`获取了一个Maven存储库文件：
- en: '[PRE59]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Then, checking that the new repository file is in place with `ls` long listing.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用`ls`长列表检查新的存储库文件是否就位。
- en: '[PRE60]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Then Maven can be installed using the Linux `yum` command, the examples below
    show the install command and a check via `ls` that the `mvn` command exists.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以使用Linux的`yum`命令安装Maven，下面的示例展示了安装命令以及通过`ls`检查`mvn`命令是否存在。
- en: '[PRE61]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The commands that I have used to build the Spark source tree are shown here
    along with the successful output. First, the environment is set up, and then the
    build is started with the `mvn` command. Options are added to build for Hadoop
    2.3/yarn, and the tests are skipped. The build uses the `clean` and `package`
    options to remove the old build files each time, and then create JAR files. Finally,
    the build output is copied via the `tee` command to a file named `build.log`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我用来构建Spark源代码树的命令以及成功的输出如下所示。首先设置环境，然后使用`mvn`命令启动构建。添加选项以构建Hadoop 2.3/yarn，并跳过测试。构建使用`clean`和`package`选项每次删除旧的构建文件，然后创建JAR文件。最后，构建输出通过`tee`命令复制到一个名为`build.log`的文件中：
- en: '[PRE62]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The actual build command that you use will depend upon whether you have Hadoop,
    and the version of it. Check the previous *building spark* for details, the build
    takes around 40 minutes on my servers.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 您使用的实际构建命令将取决于您是否安装了Hadoop以及其版本。有关详细信息，请查看之前的*构建Spark*，在我的服务器上构建大约需要40分钟。
- en: 'Given that this build will be packaged and copied to the other servers in the
    Spark cluster, it is important that all the servers use the same version of Java,
    else errors such as these will occur:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这个构建将被打包并复制到Spark集群中的其他服务器，很重要的一点是所有服务器使用相同版本的Java，否则会出现诸如以下错误：
- en: '[PRE63]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Given that the source tree has been built, it now needs to be bundled up and
    released to each of the servers in the Spark cluster. Given that these servers
    are also the members of the CDH cluster, and have password-less SSH access set
    up, I can use the `scp` command to release the built software. The following commands
    show the spark directory under the `/home/hadoop/spark` path being packaged into
    a tar file called `spark_bld.tar`. The Linux `scp` command is then used to copy
    the tar file to each slave server; the following example shows `hc2r1m1`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于源代码树已经构建完成，现在需要将其捆绑并发布到Spark集群中的每台服务器上。考虑到这些服务器也是CDH集群的成员，并且已经设置了无密码SSH访问，我可以使用`scp`命令来发布构建好的软件。以下命令展示了将`/home/hadoop/spark`路径下的spark目录打包成名为`spark_bld.tar`的tar文件。然后使用Linux的`scp`命令将tar文件复制到每个从服务器；以下示例展示了`hc2r1m1`：
- en: '[PRE64]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now that the tarred Spark build is on the slave node, it needs to be unpacked.
    The following command shows the process for the server `hc2r1m1`. The tar file
    is unpacked to the same directory as the build server `hc2nn`, that is, `/home/hadoop/spark`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，打包的Spark构建已经在从节点上，需要进行解压。以下命令显示了服务器`hc2r1m1`的过程。tar文件解压到与构建服务器`hc2nn`相同的目录，即`/home/hadoop/spark`：
- en: '[PRE65]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Once the build has been run successfully, and the built code has been released
    to the slave servers, the built version of Spark can be started from the master
    server **hc2nn**. Note that I have chosen different port numbers from the Spark
    version 1.0, installed on these servers. Also note that I will start Spark as
    root, because the Spark 1.0 install is managed as Linux services under the root
    account. As the two installs will share facilities like logging and `.pid` file
    locations, root user will ensure access. This is the script that I have used to
    start Apache Spark 1.3:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建成功运行，并且构建的代码已经发布到从服务器，Spark的构建版本可以从主服务器**hc2nn**启动。请注意，我已经选择了与这些服务器上安装的Spark版本1.0不同的端口号。还要注意，我将以root身份启动Spark，因为Spark
    1.0安装是在root帐户下管理的Linux服务。由于两个安装将共享日志记录和`.pid`文件位置等设施，root用户将确保访问。这是我用来启动Apache
    Spark 1.3的脚本：
- en: '[PRE66]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'It executes the `spark-env.sh` file to set up the environment, and then uses
    the scripts in the Spark `sbin` directory to start the services. It starts the
    master and the history server first on `hc2nn`, and then it starts the slaves.
    I added a delay before starting the slaves, as I found that they were trying to
    connect to the master before it was ready. The Spark 1.3 web user interface can
    now be accessed via this URL:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 它执行`spark-env.sh`文件来设置环境，然后使用Spark `sbin`目录中的脚本来启动服务。首先在`hc2nn`上启动主服务器和历史服务器，然后启动从服务器。在启动从服务器之前，我添加了延迟，因为我发现它们在主服务器准备好之前就尝试连接到主服务器。现在可以通过此URL访问Spark
    1.3 Web用户界面：
- en: '[PRE67]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The Spark URL, which allows applications to connect to Spark is this:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Spark URL允许应用程序连接到Spark是这样的：
- en: '[PRE68]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: As defined by the port numbers in the spark environment configuration file,
    Spark is now available to be used with ANN functionality. The next section will
    present the ANN Scala scripts and data to show how this Spark-based functionality
    can be used.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 根据spark环境配置文件中的端口号，Spark现在可以与ANN功能一起使用。下一节将展示ANN Scala脚本和数据，以展示如何使用基于Spark的功能。
- en: ANN in practice
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ANN实践
- en: 'In order to begin ANN training, test data is needed. Given that this type of
    classification method is supposed to be good at classifying distorted or noisy
    images, I have decided to attempt to classify the images here:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始ANN训练，需要测试数据。鉴于这种分类方法应该擅长分类扭曲或嘈杂的图像，我决定尝试在这里对图像进行分类：
- en: '![ANN in practice](img/B01989_02_11.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![ANN in practice](img/B01989_02_11.jpg)'
- en: 'They are hand-crafted text files that contain shaped blocks, created from the
    characters 1 and 0\. When they are stored on HDFS, the carriage return characters
    are removed, so that the image is presented as a single line vector. So, the ANN
    will be classifying a series of shape images, and then it will be tested against
    the same images with noise added to determine whether the classification will
    still work. There are six training images, and they will each be given an arbitrary
    training label from 0.1 to 0.6\. So, if the ANN is presented with a closed square,
    it should return a label of 0.1\. The following image shows an example of a testing
    image with noise added. The noise, created by adding extra zero (0) characters
    within the image, has been highlighted:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是手工制作的文本文件，包含由字符1和0创建的形状块。当它们存储在HDFS上时，回车字符被移除，因此图像呈现为单行向量。因此，ANN将对一系列形状图像进行分类，然后将针对添加噪声的相同图像进行测试，以确定分类是否仍然有效。有六个训练图像，它们将分别被赋予从0.1到0.6的任意训练标签。因此，如果ANN被呈现为封闭的正方形，它应该返回一个标签0.1。以下图像显示了添加噪声的测试图像的示例。通过在图像中添加额外的零（0）字符创建的噪声已经被突出显示：
- en: '![ANN in practice](img/B01989_02_12.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![ANN in practice](img/B01989_02_12.jpg)'
- en: 'Because the Apache Spark server has changed from the previous examples, and
    the Spark library locations have also changed, the `sbt` configuration file used
    for compiling the example ANN Scala code must also be changed. As before, the
    ANN code is being developed using the Linux hadoop account in a subdirectory called
    `spark/ann`. The `ann.sbt` file exists within the `ann` directory:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Apache Spark服务器已经从之前的示例中更改，并且Spark库的位置也已更改，用于编译示例ANN Scala代码的`sbt`配置文件也必须更改。与以前一样，ANN代码是在Linux
    hadoop帐户中的一个名为`spark/ann`的子目录中开发的。`ann.sbt`文件存在于`ann`目录中：
- en: '[PRE69]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The contents of the `ann.sbt` file have been changed to use full paths of JAR
    library files for the Spark dependencies. This is because the new Apache Spark
    code for build 1.3 now resides under `/home/hadoop/spark/spark`. Also, the project
    name has been changed to `A N N`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`ann.sbt`文件的内容已更改为使用Spark依赖项的JAR库文件的完整路径。这是因为新的Apache Spark构建1.3现在位于`/home/hadoop/spark/spark`下。此外，项目名称已更改为`A
    N N`：'
- en: '[PRE70]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'As in the previous examples, the actual Scala code to be compiled exists in
    a subdirectory named `src/main/scala` as shown next. I have created two Scala
    programs. The first trains using the input data, and then tests the ANN model
    with the same input data. The second tests the trained model with noisy data,
    to the test distorted data classification:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前的示例一样，要编译的实际Scala代码存在于名为`src/main/scala`的子目录中，如下所示。我创建了两个Scala程序。第一个使用输入数据进行训练，然后用相同的输入数据测试ANN模型。第二个使用嘈杂的数据测试训练模型，以测试扭曲数据的分类：
- en: '[PRE71]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'I will examine the first Scala file entirely, and then I will just show the
    extra features of the second file, as the two examples are very similar up to
    the point of training the ANN. The code examples shown here can be found in the
    software package provided with this book, under the path `chapter2\ANN`. So, to
    examine the first Scala example, the import statements are similar to the previous
    examples. The Spark context, configuration, vectors, and `LabeledPoint` are being
    imported. The RDD class for RDD processing is being imported this time, along
    with the new ANN class `ANNClassifier`. Note that the `MLlib/classification` routines
    widely use the `LabeledPoint` structure for input data, which will contain the
    features and labels that are supposed to be trained against:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我将完全检查第一个Scala文件，然后只展示第二个文件的额外特性，因为这两个示例在训练ANN的时候非常相似。这里展示的代码示例可以在本书提供的软件包中找到，路径为`chapter2\ANN`。因此，要检查第一个Scala示例，导入语句与之前的示例类似。导入了Spark上下文、配置、向量和`LabeledPoint`。这次还导入了RDD类用于RDD处理，以及新的ANN类`ANNClassifier`。请注意，`MLlib/classification`例程广泛使用`LabeledPoint`结构作为输入数据，其中包含了应该被训练的特征和标签：
- en: '[PRE72]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The application class in this example has been called `testann1`. The HDFS
    files to be processed have been defined in terms of the HDFS server, path, and
    file name:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，应用程序类被称为`testann1`。要处理的HDFS文件已经根据HDFS服务器、路径和文件名进行了定义：
- en: '[PRE73]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The Spark context has been created with the URL for the Spark instance, which
    now has a different port number—`8077`. The application name is `ANN 1`. This
    will appear on the Spark web UI when the application is run:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Spark上下文已经创建，使用了Spark实例的URL，现在端口号不同了——`8077`。应用程序名称是`ANN 1`。当应用程序运行时，这将出现在Spark
    Web UI上：
- en: '[PRE74]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The HDFS-based input training and test data files are loaded. The values on
    each line are split by space characters, and the numeric values have been converted
    into Doubles. The variables that contain this data are then stored in an array
    called inputs. At the same time, an array called outputs is created, containing
    the labels from 0.1 to 0.6\. These values will be used to classify the input patterns:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 加载基于HDFS的输入训练和测试数据文件。每行的值都被空格字符分割，并且数值已经转换为双精度。包含这些数据的变量然后存储在一个名为inputs的数组中。同时，创建了一个名为outputs的数组，其中包含了从0.1到0.6的标签。这些值将用于对输入模式进行分类：
- en: '[PRE75]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The input and output data, representing the input data features and labels,
    are then combined and converted into a `LabeledPoint` structure. Finally, the
    data is parallelized in order to partition it for the optimal parallel processing:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出数据，表示输入数据特征和标签，然后被合并并转换成`LabeledPoint`结构。最后，数据被并行化以便对其进行最佳并行处理：
- en: '[PRE76]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Variables are created to define the hidden layer topology of the ANN. In this
    case, I have chosen to have two hidden layers, each with 100 neurons. The maximum
    numbers of iterations are defined, as well as a batch size (six patterns) and
    convergence tolerance. The tolerance refers to how big the training error can
    get before we can consider training to have worked. Then, an ANN model is created
    using these configuration parameters and the input data:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 创建变量来定义ANN的隐藏层拓扑。在这种情况下，我选择了有两个隐藏层，每个隐藏层有100个神经元。定义了最大迭代次数，以及批处理大小（六个模式）和收敛容限。容限是指在我们可以考虑训练已经完成之前，训练误差可以达到多大。然后，使用这些配置参数和输入数据创建了一个ANN模型：
- en: '[PRE77]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'In order to test the trained ANN model, the same input training data is used
    as testing data used to obtain prediction labels. First, an input data variable
    is created called `rPredictData`. Then, the data is partitioned and finally, the
    predictions are obtained using the trained ANN model. For this model to work,
    it must output the labels 0.1 to 0.6:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试训练好的ANN模型，相同的输入训练数据被用作测试数据来获取预测标签。首先创建一个名为`rPredictData`的输入数据变量。然后，对数据进行分区，最后使用训练好的ANN模型获取预测。对于这个模型工作，必须输出标签0.1到0.6：
- en: '[PRE78]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The label predictions are printed, and the script closes with a closing bracket:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 打印标签预测，并以一个闭合括号结束脚本：
- en: '[PRE79]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'So, in order to run this code sample, it must first be compiled and packaged.
    By now, you must be familiar with the `sbt` command, executed from the `ann` sub
    directory:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了运行这个代码示例，必须首先编译和打包。到目前为止，您一定熟悉`ann`子目录中执行的`sbt`命令：
- en: '[PRE80]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The `spark-submit` command is then used from within the new `spark/spark` path
    using the new Spark-based URL at port 8077 to run the application `testann1`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用`spark-submit`命令从新的`spark/spark`路径使用新的基于Spark的URL在端口8077上运行应用程序`testann1`：
- en: '[PRE81]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'By checking the Apache Spark web URL at `http://hc2nn.semtech-solutions.co.nz:19080/`,
    it is now possible to see the application running. The following figure shows
    the application **ANN 1** running, as well as the previous completed executions:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查`http://hc2nn.semtech-solutions.co.nz:19080/`上的Apache Spark Web URL，现在可以看到应用程序正在运行。下图显示了应用程序**ANN
    1**正在运行，以及之前完成的执行：
- en: '![ANN in practice](img/B01989_02_13.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![实践中的ANN](img/B01989_02_13.jpg)'
- en: 'By selecting one of the cluster host worker instances, it is possible to see
    a list of executors that actually carry out cluster processing for that worker:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择集群主机工作实例中的一个，可以看到实际执行该工作的执行程序列表：
- en: '![ANN in practice](img/B01989_02_14.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![实践中的ANN](img/B01989_02_14.jpg)'
- en: Finally, by selecting one of the executors, it is possible to see its history
    and configuration, as well as the links to the log file, and error information.
    At this level, with the log information provided, debugging is possible. These
    log files can be checked for processing error messages.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过选择一个执行程序，可以查看其历史和配置，以及日志文件和错误信息的链接。在这个级别上，通过提供的日志信息，可以进行调试。可以检查这些日志文件以获取处理错误消息。
- en: '![ANN in practice](img/B01989_02_15.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![实践中的ANN](img/B01989_02_15.jpg)'
- en: 'The **ANN 1** application provides the following output to show that it has
    reclassified the same input data correctly. The reclassification has been successful,
    as each of the input patterns has been given the same label as it was trained
    with:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**ANN 1**应用程序提供以下输出，以显示它已经正确地重新分类了相同的输入数据。重新分类是成功的，因为每个输入模式都被赋予了与训练时相同的标签。'
- en: '[PRE82]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'So, this shows that ANN training and test prediction will work with the same
    data. Now, I will train with the same data, but test with distorted or noisy data,
    an example of which I already demonstrated. This example can be found in the file
    called `test_ann2.scala`, in your software package. It is very similar to the
    first example, so I will just demonstrate the changed code. The application is
    now called `testann2`:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这表明ANN训练和测试预测将使用相同的数据。现在，我将使用相同的数据进行训练，但使用扭曲或嘈杂的数据进行测试，这是我已经演示过的一个例子。您可以在软件包中的名为`test_ann2.scala`的文件中找到这个例子。它与第一个例子非常相似，所以我只会演示修改后的代码。该应用程序现在称为`testann2`。
- en: '[PRE83]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'An extra set of testing data is created, after the ANN model has been created
    using the training data. This testing data contains noise:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用训练数据创建ANN模型后，会创建额外的一组测试数据。这些测试数据包含噪音。
- en: '[PRE84]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'This data is processed into input arrays, and is partitioned for cluster processing:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据被处理成输入数组，并被分区进行集群处理。
- en: '[PRE85]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'It is then used to generate label predictions in the same way as the first
    example. If the model classifies the data correctly, then the same label values
    should be printed from 0.1 to 0.6:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它被用来以与第一个示例相同的方式生成标签预测。如果模型正确分类数据，则应该从0.1到0.6打印相同的标签值。
- en: '[PRE86]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The code has already been compiled, so it can be run using the `spark-submit`
    command:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 代码已经被编译，因此可以使用`spark-submit`命令运行。
- en: '[PRE87]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Here is the cluster output from this script, which shows a successful classification
    using a trained ANN model, and some noisy test data. The noisy data has been classified
    correctly. For instance, if the trained model had become confused, it might have
    given a value of `0.15` for the noisy `close_square_test.img` test image in position
    one, instead of returning `0.1` as it did:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这是脚本的集群输出，显示了使用训练过的ANN模型进行成功分类以及一些嘈杂的测试数据。嘈杂的数据已经被正确分类。例如，如果训练模型混淆了，它可能会在位置一的嘈杂的`close_square_test.img`测试图像中给出`0.15`的值，而不是返回`0.1`。
- en: '[PRE88]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter has attempted to provide you with an overview of some of the functionality
    available within the Apache Spark MLlib module. It has also shown the functionality
    that will soon be available in terms of ANN, or artificial neural networks, which
    is intended for release in Spark 1.3\. It has not been possible to cover all the
    areas of MLlib, due to the time and space allowed for this chapter.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 本章试图为您提供Apache Spark MLlib模块中一些功能的概述。它还展示了即将在Spark 1.3版本中推出的ANN（人工神经网络）的功能。由于本章的时间和空间限制，无法涵盖MLlib的所有领域。
- en: You have been shown how to develop Scala-based examples for Naïve Bayes classification,
    K-Means clustering, and ANN or artificial neural networks. You have been shown
    how to prepare test data for these Spark MLlib routines. You have also been shown
    that they all accept the LabeledPoint structure, which contains features and labels.
    Also, each approach takes a training and prediction approach to training and testing
    a model using different data sets. Using the approach shown in this chapter, you
    can now investigate the remaining functionality in the MLlib library. You should
    refer to the [http://spark.apache.org/](http://spark.apache.org/) website, and
    ensure that when checking documentation that you refer to the correct version,
    that is, [http://spark.apache.org/docs/1.0.0/](http://spark.apache.org/docs/1.0.0/)
    for version 1.0.0.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经学会了如何为朴素贝叶斯分类、K均值聚类和ANN或人工神经网络开发基于Scala的示例。您已经学会了如何为这些Spark MLlib例程准备测试数据。您还了解到它们都接受包含特征和标签的LabeledPoint结构。此外，每种方法都采用了训练和预测的方法，使用不同的数据集来训练和测试模型。使用本章展示的方法，您现在可以研究MLlib库中剩余的功能。您应该参考[http://spark.apache.org/](http://spark.apache.org/)网站，并确保在查看文档时参考正确的版本，即[http://spark.apache.org/docs/1.0.0/](http://spark.apache.org/docs/1.0.0/)，用于1.0.0版本。
- en: Having examined the Apache Spark MLlib machine learning library, in this chapter,
    it is now time to consider Apache Spark's stream processing capability. The next
    chapter will examine stream processing using the Spark and Scala-based example
    code.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经研究了Apache Spark MLlib机器学习库，现在是时候考虑Apache Spark的流处理能力了。下一章将使用基于Spark和Scala的示例代码来研究流处理。
