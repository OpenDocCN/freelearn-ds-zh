- en: 'Chapter 1: Distributed Computing Primer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章：分布式计算入门
- en: This chapter introduces you to the **Distributed Computing** paradigm and shows
    you how Distributed Computing can help you to easily process very large amounts
    of data. You will learn about the concept of **Data Parallel Processing** using
    the **MapReduce** paradigm and, finally, learn how Data Parallel Processing can
    be made more efficient by using an in-memory, unified data processing engine such
    as Apache Spark.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了**分布式计算**范式，并展示了分布式计算如何帮助你轻松处理大量数据。你将学习如何使用**MapReduce**范式实现**数据并行处理**，并最终了解如何通过使用内存中的统一数据处理引擎（如Apache
    Spark）来提高数据并行处理的效率。
- en: Then, you will dive deeper into the architecture and components of Apache Spark
    along with code examples. Finally, you will get an overview of what's new with
    the latest 3.0 release of Apache Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将深入了解Apache Spark的架构和组件，并结合代码示例进行讲解。最后，你将概览Apache Spark最新的3.0版本中新增的功能。
- en: In this chapter, the key skills that you will acquire include an understanding
    of the basics of the Distributed Computing paradigm and a few different implementations
    of the Distributed Computing paradigm such as MapReduce and Apache Spark. You
    will learn about the fundamentals of Apache Spark along with its architecture
    and core components, such as the Driver, Executor, and Cluster Manager, and how
    they come together as a single unit to perform a Distributed Computing task. You
    will learn about Spark's **Resilient Distributed Dataset** (**RDD**) API along
    with higher-order functions and lambdas. You will also gain an understanding of
    the Spark SQL Engine and its DataFrame and SQL APIs. Additionally, you will implement
    working code examples. You will also learn about the various components of an
    Apache Spark data processing program, including transformations and actions, and
    you will learn about the concept of **Lazy Evaluation**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将掌握的关键技能包括理解分布式计算范式的基础知识，以及分布式计算范式的几种不同实现方法，如MapReduce和Apache Spark。你将学习Apache
    Spark的基本原理及其架构和核心组件，例如Driver、Executor和Cluster Manager，并了解它们如何作为一个整体协同工作以执行分布式计算任务。你将学习Spark的**弹性分布式数据集**（**RDD**）API，及其高阶函数和lambda表达式。你还将了解Spark
    SQL引擎及其DataFrame和SQL API。此外，你将实现可运行的代码示例。你还将学习Apache Spark数据处理程序的各个组件，包括转换和动作，并学习**惰性求值**的概念。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Introduction Distributed Computing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 分布式计算
- en: Distributed Computing with Apache Spark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行分布式计算
- en: Big data processing with Spark SQL and DataFrames
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark SQL和DataFrames进行大数据处理
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will be using the Databricks Community Edition to run our
    code. This can be found at [https://community.cloud.databricks.com](https://community.cloud.databricks.com).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Databricks Community Edition来运行代码。你可以在[https://community.cloud.databricks.com](https://community.cloud.databricks.com)找到该平台。
- en: Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注册说明可在[https://databricks.com/try-databricks](https://databricks.com/try-databricks)找到。
- en: The code used in this chapter can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter01](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter01).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的代码可以从[https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter01](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter01)下载。
- en: The datasets used in this chapter can be found at [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的数据集可以在[https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data)找到。
- en: 'The original datasets can be taken from their sources, as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集可以从以下来源获取：
- en: 'Online Retail: [https://archive.ics.uci.edu/ml/datasets/Online+Retail+II](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线零售：[https://archive.ics.uci.edu/ml/datasets/Online+Retail+II](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II)
- en: 'Image Data: [https://archive.ics.uci.edu/ml/datasets/Rice+Leaf+Diseases](https://archive.ics.uci.edu/ml/datasets/Rice+Leaf+Diseases)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像数据：[https://archive.ics.uci.edu/ml/datasets/Rice+Leaf+Diseases](https://archive.ics.uci.edu/ml/datasets/Rice+Leaf+Diseases)
- en: 'Census Data: [https://archive.ics.uci.edu/ml/datasets/Census+Income](https://archive.ics.uci.edu/ml/datasets/Census+Income)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人口普查数据：[https://archive.ics.uci.edu/ml/datasets/Census+Income](https://archive.ics.uci.edu/ml/datasets/Census+Income)
- en: 'Country Data: [https://public.opendatasoft.com/explore/dataset/countries-codes/information/](https://public.opendatasoft.com/explore/dataset/countries-codes/information/)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国家数据：[https://public.opendatasoft.com/explore/dataset/countries-codes/information/](https://public.opendatasoft.com/explore/dataset/countries-codes/information/)
- en: Distributed Computing
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式计算
- en: In this section, you will learn about Distributed Computing, the need for it,
    and how you can use it to process very large amounts of data in a quick and efficient
    manner.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解分布式计算，它的需求，以及如何利用它以快速且高效的方式处理大量数据。
- en: Introduction to Distributed Computing
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式计算简介
- en: '**Distributed Computing** is a class of computing techniques where we use a
    group of computers as a single unit to solve a computational problem instead of
    just using a single machine.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**分布式计算**是一类计算技术，我们通过使用一组计算机作为一个整体来解决计算问题，而不是仅仅依赖单台机器。'
- en: In data analytics, when the amount of data becomes too large to fit in a single
    machine, we can either split the data into smaller chunks and process it on a
    single machine iteratively, or we can process the chunks of data on several machines
    in parallel. While the former gets the job done, it might take longer to process
    the entire dataset iteratively; the latter technique gets the job completed in
    a shorter period of time by using multiple machines at once.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析中，当数据量变得太大，无法在单台机器中处理时，我们可以将数据拆分成小块并在单台机器上迭代处理，或者可以在多台机器上并行处理数据块。虽然前者可以完成任务，但可能需要更长时间来迭代处理整个数据集；后者通过同时使用多台机器，能够在更短的时间内完成任务。
- en: There are different kinds of Distributed Computing techniques; however, for
    data analytics, one popular technique is **Data Parallel Processing**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种分布式计算技术；然而，在数据分析中，一种流行的技术是 **数据并行处理**。
- en: Data Parallel Processing
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据并行处理
- en: '**Data Parallel Processing** involves two main parts:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据并行处理**包含两个主要部分：'
- en: The actual data that needs to be processed
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要处理的实际数据。
- en: The piece of code or business logic that needs to be applied to the data in
    order to process it
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要应用于数据的代码片段或业务逻辑，以便进行处理。
- en: 'We can process large amounts of data by splitting it into smaller chunks and
    processing them in parallel on several machines. This can be done in two ways:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将大量数据拆分成小块，并在多台机器上并行处理它们，从而处理大规模数据。这可以通过两种方式实现：
- en: First, bring the data to the machine where our code is running.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，将数据带到运行代码的机器上。
- en: Second, take our code to where our data is actually stored.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二步，将我们的代码带到数据实际存储的位置。
- en: One drawback of the first technique is that as our data sizes become larger,
    the amount of time it takes to move data also increases proportionally. Therefore,
    we end up spending more time moving data from one system to another and, in turn,
    negating any efficiency gained by our parallel processing system. We also find
    ourselves creating multiple copies of data during data replication.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种技术的一个缺点是，随着数据量的增大，数据移动所需的时间也会成比例增加。因此，我们最终会花费更多的时间将数据从一个系统移动到另一个系统，从而抵消了并行处理系统带来的任何效率提升。同时，我们还会在数据复制过程中产生多个数据副本。
- en: The second technique is far more efficient because instead of moving large amounts
    of data, we can easily move a few lines of code to where our data actually resides.
    This technique of moving code to where the data resides is referred to as Data
    Parallel Processing. This Data Parallel Processing technique is very fast and
    efficient, as we save the amount of time that was needed earlier to move and copy
    data across different systems. One such Data Parallel Processing technique is
    called the **MapReduce paradigm**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种技术要高效得多，因为我们不再移动大量数据，而是可以将少量代码移到数据实际存放的位置。这种将代码移到数据所在位置的技术称为数据并行处理。数据并行处理技术非常快速和高效，因为它节省了之前在不同系统之间移动和复制数据所需的时间。这样的数据并行处理技术之一被称为
    **MapReduce 模式**。
- en: Data Parallel Processing using the MapReduce paradigm
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 MapReduce 模式进行数据并行处理
- en: 'The MapReduce paradigm breaks down a Data Parallel Processing problem into
    three main stages:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 模式将数据并行处理问题分解为三个主要阶段：
- en: The Map stage
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Map 阶段
- en: The Shuffle stage
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shuffle 阶段
- en: The Reduce stage
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reduce 阶段
- en: The `(key, value)` pairs, applies some processing on the pairs, and transforms
    them into another set of `(key, value)` pairs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`(key, value)`对，应用一些处理，将它们转换为另一组`(key, value)`对。'
- en: The `(key, value)` pairs from the Map stage and shuffles/sorts them so that
    pairs with the same *key* end up together.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从Map阶段获取的`(key, value)`对，将其洗牌/排序，使得具有相同*key*的对最终聚集在一起。
- en: The `(key, value)` pairs from the Shuffle stage and reduces or aggregates the
    pairs to produce the final result.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从Shuffle阶段获取的`(key, value)`对，经过归约或聚合，产生最终结果。
- en: There can be multiple **Map** stages followed by multiple **Reduce** stages.
    However, a **Reduce** stage only starts after all of the **Map** stages have been
    completed.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 可以有多个**Map**阶段，后跟多个**Reduce**阶段。然而，**Reduce**阶段仅在所有**Map**阶段完成后开始。
- en: Let's take a look at an example where we want to calculate the counts of all
    the different words in a text document and apply the **MapReduce** paradigm to
    it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，假设我们想要计算文本文件中所有不同单词的计数，并应用**MapReduce**范式。
- en: 'The following diagram shows how the MapReduce paradigm works in general:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了MapReduce范式的一般工作原理：
- en: '![Figure 1.1 – Calculating the word count using MapReduce'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1 – 使用MapReduce计算单词计数'
- en: '](img/B16736_01_01.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_01_01.jpg)'
- en: Figure 1.1 – Calculating the word count using MapReduce
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 使用MapReduce计算单词计数
- en: 'The previous example works in the following manner:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的例子按照以下方式工作：
- en: In *Figure 1.1*, we have a cluster of three nodes, labeled **M1**, **M2**, and
    **M3**. Each machine includes a few text files containing several sentences in
    plain text. Here, our goal is to use MapReduce to count all of the words in the
    text files.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*图1.1*中，我们有一个包含三台节点的集群，标记为**M1**、**M2**和**M3**。每台机器上都有几个文本文件，包含若干句子的纯文本。在这里，我们的目标是使用MapReduce来计算文本文件中所有单词的数量。
- en: We load all the text documents onto the cluster; each machine loads the documents
    that are local to it.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将所有文本文件加载到集群中；每台机器加载本地的文档。
- en: The `(word, count)` pair.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`(word, count)`对。'
- en: The `(word, count)` pairs from the **Map stage** and shuffles/sorts them so
    that word pairs with the same keyword end up together.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**Map阶段**获取的`(word, count)`对，进行洗牌/排序，使得具有相同关键词的单词对聚集在一起。
- en: The **Reduce Stage** groups all keywords together and sums their counts to produce
    the final count of each individual word.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Reduce阶段**将所有关键字分组，并对其计数进行汇总，得到每个单独单词的最终计数。'
- en: The MapReduce paradigm was popularized by the **Hadoop** framework and was pretty
    popular for processing big data workloads. However, the MapReduce paradigm offers
    a very low-level API for transforming data and requires users to have proficient
    knowledge of programming languages such as Java. Expressing a data analytics problem
    using Map and Reduce is not very intuitive or flexible.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce范式由**Hadoop**框架推广，曾在大数据工作负载处理领域非常流行。然而，MapReduce范式提供的是非常低级别的API来转换数据，且要求用户具备如Java等编程语言的熟练知识。使用Map和Reduce来表达数据分析问题既不直观也不灵活。
- en: MapReduce was designed to run on commodity hardware, and since commodity hardware
    was prone to failures, resiliency to hardware failures was a necessity. MapReduce
    achieves resiliency to hardware failures by saving the results of every stage
    to disk. This round-trip to disk after every stage makes MapReduce relatively
    slow at processing data because of the slow I/O performance of physical disks
    in general. To overcome this limitation, the next generation of the MapReduce
    paradigm was created, which made use of much faster system memory, as opposed
    to disks, to process data and offered much more flexible APIs to express data
    transformations. This new framework is called Apache Spark, and you will learn
    about it in the next section and throughout the remainder of this book.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce被设计成可以在普通硬件上运行，由于普通硬件容易发生故障，因此对于硬件故障的容错能力至关重要。MapReduce通过将每个阶段的结果保存到磁盘上来实现容错。每个阶段结束后必须往返磁盘，这使得MapReduce在处理数据时相对较慢，因为物理磁盘的I/O性能普遍较低。为了克服这个限制，下一代MapReduce范式应运而生，它利用比磁盘更快的系统内存来处理数据，并提供了更灵活的API来表达数据转换。这个新框架叫做Apache
    Spark，接下来的章节和本书其余部分你将学习到它。
- en: Important note
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In Distributed Computing, you will often encounter the term **cluster**. A cluster
    is a group of computers all working together as a single unit to solve a computing
    problem. The primary machine of a cluster is, typically, termed the **Master Node**,
    which takes care of the orchestration and management of the cluster, and secondary
    machines that actually carry out task execution are called **Worker Nodes**. A
    cluster is a key component of any Distributed Computing system, and you will encounter
    these terms throughout this book.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式计算中，你会经常遇到**集群**这个术语。集群是由一组计算机组成的，它们共同作为一个单一的单元来解决计算问题。集群的主要计算机通常被称为**主节点**，负责集群的协调和管理，而实际执行任务的次要计算机则被称为**工作节点**。集群是任何分布式计算系统的关键组成部分，在本书中，你将会遇到这些术语。
- en: Distributed Computing with Apache Spark
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行分布式计算
- en: Over the last decade, Apache Spark has grown to be the de facto standard for
    big data processing. Indeed, it is an indispensable tool in the hands of anyone
    involved with data analytics.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，Apache Spark已发展成为大数据处理的事实标准。事实上，它是任何从事数据分析工作的人手中不可或缺的工具。
- en: Here, we will begin with the basics of Apache Spark, including its architecture
    and components. Then, we will get started with the PySpark programming API to
    actually implement the previously illustrated word count problem. Finally, we
    will take a look at what's new with the latest 3.0 release of Apache Spark.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从Apache Spark的基础知识开始，包括其架构和组件。接着，我们将开始使用PySpark编程API来实际实现之前提到的单词计数问题。最后，我们将看看最新的Apache
    Spark 3.0版本有什么新功能。
- en: Introduction to Apache Spark
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Spark介绍
- en: Apache Spark is an in-memory, unified data analytics engine that is relatively
    fast compared to other distributed data processing frameworks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个内存中的统一数据分析引擎，相较于其他分布式数据处理框架，它的速度相对较快。
- en: It is a unified data analytics framework because it can process different types
    of big data workloads with a single engine. The different workloads include the
    following
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个统一的数据分析框架，因为它可以使用单一引擎处理不同类型的大数据工作负载。这些工作负载包括以下内容：
- en: Batch data processing
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量数据处理
- en: Real-time data processing
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时数据处理
- en: Machine learning and data science
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习和数据科学
- en: Typically, data analytics involves all or a combination of the previously mentioned
    workloads to solve a single business problem. Before Apache Spark, there was no
    single framework that could accommodate all three workloads simultaneously. With
    Apache Spark, various teams involved in data analytics can all use a single framework
    to solve a single business problem, thus improving communication and collaboration
    among teams and drastically reducing their learning curve.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据分析涉及到之前提到的所有或部分工作负载来解决单一的业务问题。在Apache Spark出现之前，没有一个单一的框架能够同时容纳所有三种工作负载。通过Apache
    Spark，参与数据分析的各个团队可以使用同一个框架来解决单一的业务问题，从而提高团队之间的沟通与协作，并显著缩短学习曲线。
- en: We will explore each of the preceding workloads, in depth, in [*Chapter 2*](B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032)*,*
    *Data Ingestion* through to [*Chapter 8*](B16736_08_Final_JM_ePub.xhtml#_idTextAnchor150)*,
    Unsupervised Machine Learning,* of this book.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书中深入探索每一个前述的工作负载，从[*第二章*](B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032)*，数据摄取*，到[*第八章*](B16736_08_Final_JM_ePub.xhtml#_idTextAnchor150)*，无监督机器学习*。
- en: 'Further, Apache Spark is fast in two aspects:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Apache Spark在两个方面都非常快：
- en: It is fast in terms of data processing speed.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在数据处理速度方面非常快。
- en: It is fast in terms of development speed.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在开发速度方面很快。
- en: Apache Spark has fast job/query execution speeds because it does all of its
    data processing in memory, and it has other optimizations techniques built-in
    such as **Lazy Evaluation**, **Predicate Pushdown**, and Partition Pruning to
    name a few. We will go over Spark's optimization techniques in the coming chapters.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark具有快速的作业/查询执行速度，因为它将所有数据处理都在内存中进行，并且还内置了一些优化技术，如**惰性求值**、**谓词下推**和**分区修剪**，仅举几例。我们将在接下来的章节中详细介绍Spark的优化技术。
- en: Secondly, Apache Spark provides developers with very high-level APIs to perform
    basic data processing operations such as *filtering*, *grouping*, *sorting*, *joining*,
    and *aggregating*. By using these high-level programming constructs, developers
    can very easily express their data processing logic, making their development
    many times faster.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，Apache Spark 为开发者提供了非常高级的 API，用于执行基本的数据处理操作，例如 *过滤*、*分组*、*排序*、*连接* 和 *聚合*。通过使用这些高级编程结构，开发者能够非常轻松地表达数据处理逻辑，使开发速度大大提高。
- en: The core abstraction of Apache Spark, which makes it fast and very expressive
    for data analytics, is called an RDD. We will cover this in the next section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 的核心抽象，正是使其在数据分析中既快速又富有表现力的，是 RDD。我们将在下一节中介绍这个概念。
- en: Data Parallel Processing with RDDs
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 RDD 进行数据并行处理
- en: An RDD is the core abstraction of the Apache Spark framework. Think of an RDD
    as any kind of immutable data structure that is typically found in a programming
    language but one that resides in the memory of several machines instead of just
    one. An RDD consists of partitions, which are logical divisions of an RDD, with
    a few of them residing on each machine.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 是 Apache Spark 框架的核心抽象。可以将 RDD 看作是任何一种不可变数据结构，它通常存在于编程语言中，但与一般情况下只驻留在单台机器的不同，RDD
    是分布式的，驻留在多台机器的内存中。一个 RDD 由多个分区组成，分区是 RDD 的逻辑划分，每个机器上可能会有一些分区。
- en: 'The following diagram helps explain the concepts of an RDD and its partitions:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下图帮助解释 RDD 及其分区的概念：
- en: '![Figure 1.2 – An RDD'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.2 – 一个 RDD'
- en: '](img/B16736_01_02.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_01_02.jpg)'
- en: Figure 1.2 – An RDD
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 一个 RDD
- en: In the previous diagram, we have a cluster of three machines or nodes. There
    are three RDDs on the cluster, and each RDD is divided into partitions. Each node
    of the cluster contains a few partitions of an individual RDD, and each RDD is
    distributed among several nodes of the cluster by means of partitions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示意图中，我们有一个由三台机器或节点组成的集群。集群中有三个 RDD，每个 RDD 被分成多个分区。每个节点包含一个或多个分区，并且每个 RDD
    通过分区在集群的多个节点之间分布。
- en: The RDD abstractions are accompanied by a set of high-level functions that can
    operate on the RRDs in order to manipulate the data stored within the partitions.
    These functions are called **higher-order functions**, and you will learn about
    them in the following section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 抽象伴随着一组可以操作 RDD 的高阶函数，用于操作存储在分区中的数据。这些函数被称为 **高阶函数**，你将在接下来的章节中了解它们。
- en: Higher-order functions
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高阶函数
- en: Higher-order functions manipulate RDDs and help us write business logic to transform
    data stored within the partitions. Higher-order functions accept other functions
    as parameters, and these inner functions help us define the actual business logic
    that transforms data and is applied to each partition of the RDD in parallel.
    These inner functions passed as parameters to the higher-order functions are called
    **lambda functions** or **lambdas**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 高阶函数操作 RDD，并帮助我们编写业务逻辑来转换存储在分区中的数据。高阶函数接受其他函数作为参数，这些内部函数帮助我们定义实际的业务逻辑，转换数据并并行应用于每个
    RDD 的分区。传递给高阶函数的内部函数称为 **lambda 函数** 或 **lambda 表达式**。
- en: Apache Spark comes with several higher-order functions such as `map`, `flatMap`,
    `reduce`, `fold`, `filter`, `reduceByKey`, `join`, and `union` to name a few.
    These functions are high-level functions and help us express our data manipulation
    logic very easily.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 提供了多个高阶函数，例如 `map`、`flatMap`、`reduce`、`fold`、`filter`、`reduceByKey`、`join`
    和 `union` 等。这些函数是高级函数，帮助我们非常轻松地表达数据操作逻辑。
- en: 'For example, consider our previously illustrated word count example. Let''s
    say you wanted to read a text file as an RDD and split each word based on a delimiter
    such as a whitespace. This is what code expressed in terms of an RDD and higher-order
    function would look like:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑我们之前展示的字数统计示例。假设你想将一个文本文件作为 RDD 读取，并根据分隔符（例如空格）拆分每个单词。用 RDD 和高阶函数表示的代码可能如下所示：
- en: '[PRE0]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the previous code snippet, the following occurs:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，发生了以下情况：
- en: We are loading a text file using the built-in `sc.textFile()` method, which
    loads all text files at the specified location into the cluster memory, splits
    them into individual lines, and returns an RDD of lines or strings.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用内置的 `sc.textFile()` 方法加载文本文件，该方法会将指定位置的所有文本文件加载到集群内存中，将它们按行拆分，并返回一个包含行或字符串的
    RDD。
- en: We then apply the `flatMap()` higher-order function to the new RDD of lines
    and supply it with a function that instructs it to take each line and split it
    based on a white space. The lambda function that we pass to `flatMap()` is simply
    an anonymous function that takes one parameter, an individual line of `StringType`,
    and returns a list of words. Through the `flatMap()` and `lambda()` functions,
    we are able to transform an RDD of lines into an RDD of words.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对新的行 RDD 应用 `flatMap()` 高阶函数，并提供一个函数，指示它将每一行按空格分开。我们传递给 `flatMap()` 的 Lambda
    函数只是一个匿名函数，它接受一个参数，即单个 `StringType` 行，并返回一个单词列表。通过 `flatMap()` 和 `lambda()` 函数，我们能够将一个行
    RDD 转换为一个单词 RDD。
- en: Finally, we use the `map()` function to assign a count of `1` to every individual
    word. This is pretty easy and definitely more intuitive compared to developing
    a MapReduce application using the Java programming language.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用 `map()` 函数为每个单词分配 `1` 的计数。这相对简单，且比使用 Java 编程语言开发 MapReduce 应用程序直观得多。
- en: To summarize what you have learned, the primary construct of the Apache Spark
    framework is an RDD. An RDD consists of *partitions* distributed across individual
    nodes of a cluster. We use special functions called higher-order functions to
    operate on the RDDs and transform the RDDs according to our business logic. This
    business logic is passed along to the Worker Nodes via higher-order functions
    in the form of lambdas or anonymous functions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 总结你所学的内容，Apache Spark 框架的主要构建块是 RDD。一个 RDD 由分布在集群各个节点上的 *分区* 组成。我们使用一种叫做高阶函数的特殊函数来操作
    RDD，并根据我们的业务逻辑转化 RDD。这些业务逻辑通过高阶函数以 Lambda 或匿名函数的形式传递到 Worker 节点。
- en: Before we dig deeper into the inner workings of higher-order functions and lambda
    functions, we need to understand the architecture of the Apache Spark framework
    and the components of a typical Spark Cluster. We will do this in the following
    section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨高阶函数和 Lambda 函数的内部工作原理之前，我们需要了解 Apache Spark 框架的架构以及一个典型 Spark 集群的组件。我们将在接下来的章节中进行介绍。
- en: Note
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The *Resilient* part of an RDD comes from the fact that every RDD knows its
    lineage. At any given point in time, an RDD has information of all the individual
    operations performed on it, going back all the way to the data source itself.
    Thus, if any Executors are lost due to any failures and one or more of its partitions
    are lost, it can easily recreate those partitions from the source data making
    use of the lineage information, thus making it *Resilient* to failures.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 的 *Resilient* 特性来源于每个 RDD 知道它的血统。任何时候，一个 RDD 都拥有它上面执行的所有操作的信息，追溯到数据源本身。因此，如果由于某些故障丢失了
    Executors，且其某些分区丢失，它可以轻松地通过利用血统信息从源数据重新创建这些分区，从而使其对故障具有 *Resilient*（韧性）。
- en: Apache Spark cluster architecture
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Spark 集群架构
- en: 'A typical Apache Spark cluster consists of three major components, namely,
    the Driver, a few Executors, and the Cluster Manager:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的 Apache Spark 集群由三个主要组件组成，即 Driver、若干 Executors 和 Cluster Manager：
- en: '![Figure 1.3 – Apache Spark Cluster components'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.3 – Apache Spark 集群组件'
- en: '](img/B16736_01_03.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_01_03.jpg)'
- en: Figure 1.3 – Apache Spark Cluster components
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – Apache Spark 集群组件
- en: Let's examine each of these components a little closer.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看这些组件。
- en: Driver – the heart of a Spark application
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Driver – Spark 应用程序的核心
- en: The Spark Driver is a Java Virtual Machine process and is the core part of a
    Spark application. It is responsible for user application code declarations, along
    with the creation of RDDs, DataFrames, and datasets. It is also responsible for
    coordinating with and running code on the Executors and creating and scheduling
    tasks on the Executors. It is even responsible for relaunching Executors after
    a failure and finally returning any data requested back to the client or the user.
    Think of a Spark Driver as the `main()` program of any Spark application.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Driver 是一个 Java 虚拟机进程，是 Spark 应用程序的核心部分。它负责用户应用程序代码的声明，同时创建 RDD、DataFrame
    和数据集。它还负责与 Executors 协调并在 Executors 上运行代码，创建并调度任务。它甚至负责在失败后重新启动 Executors，并最终将请求的数据返回给客户端或用户。可以把
    Spark Driver 想象成任何 Spark 应用程序的 `main()` 程序。
- en: Important note
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The Driver is the single point of failure for a Spark cluster, and the entire
    Spark application fails if the driver fails; therefore, different Cluster Managers
    implement different strategies to make the Driver highly available.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Driver 是 Spark 集群的单点故障，如果 Driver 失败，整个 Spark 应用程序也会失败；因此，不同的集群管理器实现了不同的策略以确保
    Driver 高可用。
- en: Executors – the actual workers
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行器 – 实际的工作节点
- en: Spark Executors are also Java Virtual Machine processes, and they are responsible
    for running operations on RDDs that actually transform data. They can cache data
    partitions locally and return the processed data back to the Driver or write to
    persistent storage. Each Executor runs operations on a set of partitions of an
    RDD in parallel.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 执行器也是 Java 虚拟机进程，它们负责在 RDD 上运行实际的转换数据操作。它们可以在本地缓存数据分区，并将处理后的数据返回给 Driver，或写入持久化存储。每个执行器会并行运行一组
    RDD 分区的操作。
- en: Cluster Manager – coordinates and manages cluster resources
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群管理器 – 协调和管理集群资源
- en: The **Cluster Manager** is a process that runs centrally on the cluster and
    is responsible for providing resources requested by the Driver. It also monitors
    the Executors regarding task progress and their status. Apache Spark comes with
    its own Cluster Manager, which is referred to as the Standalone Cluster Manager,
    but it also supports other popular Cluster Managers such as YARN or Mesos. Throughout
    this book, we will be using Spark's built-in Standalone Cluster Manager.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**集群管理器**是一个在集群上集中运行的进程，负责为 Driver 提供所请求的资源。它还监控执行器的任务进度和状态。Apache Spark 自带集群管理器，称为
    Standalone 集群管理器，但它也支持其他流行的集群管理器，如 YARN 或 Mesos。在本书中，我们将使用 Spark 自带的 Standalone
    集群管理器。'
- en: Getting started with Spark
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用 Spark
- en: So far, we have learnt about Apache Spark's core data structure, called RDD,
    the functions used to manipulate RDDs, called higher-order functions, and the
    components of an Apache Spark cluster. You have also seen a few code snippets
    on how to use higher-order functions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了 Apache Spark 的核心数据结构 RDD、用于操作 RDD 的函数（即高阶函数）以及 Apache Spark 集群的各个组件。你还见识了一些如何使用高阶函数的代码片段。
- en: 'In this section, you will put your knowledge to practical use and write your
    very first Apache Spark program, where you will use Spark''s Python API called
    **PySpark** to create a word count application. However, first, we need a few
    things to get started:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你将把所学知识付诸实践，编写你的第一个 Apache Spark 程序，你将使用 Spark 的 Python API —— **PySpark**
    来创建一个词频统计应用程序。然而，在开始之前，我们需要准备以下几样东西：
- en: An Apache Spark cluster
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Apache Spark 集群
- en: Datasets
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集
- en: Actual code for the word count application
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词频统计应用程序的实际代码
- en: We will use the free **Community Edition of Databricks** to create our Spark
    cluster. The code used can be found via the GitHub link that was mentioned at
    the beginning of this chapter. The links for the required resources can be found
    in the *Technical requirements* section toward the beginning of the chapter.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用免费的 **Databricks 社区版** 来创建我们的 Spark 集群。所使用的代码可以通过本章开头提到的 GitHub 链接找到。所需资源的链接可以在本章开头的
    *技术要求* 部分找到。
- en: Note
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although we are using Databricks Spark Clusters in this book, the provided code
    can be executed on any Spark cluster running Spark 3.0, or higher, as long as
    data is provided at a location accessible by your Spark cluster.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书中使用的是 Databricks Spark 集群，但提供的代码可以在任何运行 Spark 3.0 或更高版本的 Spark 集群上执行，只要数据位于
    Spark 集群可以访问的位置。
- en: 'Now that you have gained an understanding of Spark''s core concepts such as
    RDDs, higher-order functions, lambdas, and Spark''s architecture, let''s implement
    your very first Spark application using the following code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了 Spark 的核心概念，如 RDD、高阶函数、Lambda 表达式以及 Spark 架构，让我们通过以下代码实现你的第一个 Spark
    应用程序：
- en: '[PRE1]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the previous code snippet, the following takes place:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个代码片段中，发生了以下操作：
- en: We load a text file using the built-in `sc.textFile()` method, which reads all
    of the text files at the specified location, splits them into individual lines,
    and returns an RDD of lines or strings.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用内建的 `sc.textFile()` 方法加载文本文件，该方法会读取指定位置的所有文本文件，将其拆分为单独的行，并返回一个包含行或字符串的 RDD。
- en: Then, we apply the `flatMap()` higher-order function to the RDD of lines and
    supply it with a function that instructs it to take each line and split it based
    on a white space. The lambda function that we pass to `flatMap()` is simply an
    anonymous function that takes one parameter, a line, and returns individual words
    as a list. By means of the `flatMap()` and `lambda()` functions, we are able to
    transform an RDD of lines into an RDD of words.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们对 RDD 中的行应用 `flatMap()` 高阶函数，并传入一个函数，该函数指示它根据空格将每一行拆分开来。我们传给 `flatMap()`
    的 lambda 函数只是一个匿名函数，它接受一个参数——一行文本，并将每个单词作为列表返回。通过 `flatMap()` 和 `lambda()` 函数，我们能够将行的
    RDD 转换成单词的 RDD。
- en: Then, we use the `map()` function to assign a count of `1` to every individual
    word.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用 `map()` 函数为每个单独的单词分配一个 `1` 的计数。
- en: Finally, we use the `reduceByKey()` higher-order function to sum up the count
    of similar words occurring multiple times.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用 `reduceByKey()` 高阶函数对多次出现的相似单词进行计数求和。
- en: Once the counts have been calculated, we make use of the `take()` function to
    display a sample of the final word counts.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦计算出单词的计数，我们使用 `take()` 函数展示最终单词计数的一个示例。
- en: Although displaying a sample result set is usually helpful in determining the
    correctness of our code, in a big data setting, it is not practical to display
    all the results on to the console. So, we make use of the `saveAsTextFile()` function
    to persist our finals results in persistent storage.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管展示一个示例结果集通常有助于确定我们代码的正确性，但在大数据环境下，将所有结果展示到控制台上并不现实。因此，我们使用 `saveAsTextFile()`
    函数将最终结果持久化到持久存储中。
- en: Important note
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is not recommended that you display the entire result set onto the console
    using commands such as `take()` or `collect()`. It could even be outright dangerous
    to try and display all the data in a big data setting, as it could try to bring
    way too much data back to the driver and cause the driver to fail with an `OutOfMemoryError`,
    which, in turn, causes the entire application to fail.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不推荐使用 `take()` 或 `collect()` 等命令将整个结果集展示到控制台。这在大数据环境下甚至可能是危险的，因为它可能试图将过多的数据带回驱动程序，从而导致驱动程序因
    `OutOfMemoryError` 失败，进而使整个应用程序失败。
- en: Therefore, it is recommended that you use `take()` with a very small result
    set, and use `collect()` only when you are confident that the amount of data returned
    is, indeed, very small.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，建议你在结果集非常小的情况下使用 `take()`，而仅在确信返回的数据量确实非常小时，才使用 `collect()`。
- en: 'Let''s dive deeper into the following line of code in order to understand the
    inner workings of lambdas and how they implement Data Parallel Processing along
    with higher-order functions:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨下面这一行代码，了解 lambda 的内部工作原理，以及它们如何通过高阶函数实现数据并行处理：
- en: '[PRE2]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the previous code snippet, the `flatMmap()` higher-order function bundles
    the code present in the lambda and sends it over a network to the Worker Nodes,
    using a process called *serialization*. This *serialized lambda* is then sent
    out to every executor, and each executor, in turn, applies this lambda to individual
    RDD partitions in parallel.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，`flatMmap()` 高阶函数将 lambda 中的代码打包，并通过一种叫做 *序列化* 的过程将其发送到网络上的 Worker
    节点。这个 *序列化的 lambda* 随后会被发送到每个执行器，每个执行器则将此 lambda 并行应用到各个 RDD 分区上。
- en: Important note
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Since higher-order functions need to be able to serialize the lambdas in order
    to send your code to the Executors. The lambda functions need to be `serializable`,
    and failing this, you might encounter a *Task not serializable* error.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 由于高阶函数需要能够序列化 lambda，以便将你的代码发送到执行器。因此，lambda 函数需要是 `可序列化` 的，如果不满足这一要求，你可能会遇到
    *任务不可序列化* 的错误。
- en: In summary, higher-order functions are, essentially, transferring your data
    transformation code in the form of serialized lambdas to your data in RDD partitions.
    Therefore, instead of moving data to where the code is, we are actually moving
    our code to where data is situated, which is the exact definition of Data Parallel
    Processing, as we learned earlier in this chapter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，高阶函数本质上是将你的数据转换代码以序列化 lambda 的形式传递到 RDD 分区中的数据上。因此，我们并不是将数据移动到代码所在的位置，而是将代码移动到数据所在的位置，这正是数据并行处理的定义，正如我们在本章前面所学的那样。
- en: Thus, Apache Spark along with its RDDs and higher-order functions implements
    an in-memory version of the Data Parallel Processing paradigm. This makes Apache
    Spark fast and efficient at big data processing in a Distributed Computing setting.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Apache Spark 及其 RDD 和高阶函数实现了内存中版本的数据并行处理范式。这使得 Apache Spark 在分布式计算环境中，进行大数据处理时既快速又高效。
- en: The RDD abstraction of Apache Spark definitely offers a higher level of programming
    API compared to MapReduce, but it still requires some level of comprehension of
    the functional programming style to be able to express even the most common types
    of data transformations. To overcome this challenge, Spark's already existing
    SQL engine was expanded, and another abstraction, called the DataFrame, was added
    on top of RDDs. This makes data processing much easier and more familiar for data
    scientists and data analysts. The following section will explore the DataFrame
    and SQL API of the Spark SQL engine.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 的 RDD 抽象相较于 MapReduce 确实提供了一个更高级的编程 API，但它仍然需要一定程度的函数式编程理解，才能表达最常见的数据转换类型。为了克服这个挑战，Spark
    扩展了已有的 SQL 引擎，并在 RDD 上增加了一个叫做 DataFrame 的抽象。这使得数据处理对于数据科学家和数据分析师来说更加容易和熟悉。接下来的部分将探索
    Spark SQL 引擎的 DataFrame 和 SQL API。
- en: Big data processing with Spark SQL and DataFrames
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark SQL 和 DataFrame 处理大数据
- en: The Spark SQL engine supports two types of APIs, namely, DataFrame and Spark
    SQL. Being higher-level abstractions than RDDs, these are far more intuitive and
    even more expressive. They come with many more data transformation functions and
    utilities that you might already be familiar with as a data engineer, data analyst,
    or a data scientist.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 引擎支持两种类型的 API，分别是 DataFrame 和 Spark SQL。作为比 RDD 更高层次的抽象，它们更加直观，甚至更具表现力。它们带有更多的数据转换函数和工具，你作为数据工程师、数据分析师或数据科学家，可能已经熟悉这些内容。
- en: Spark SQL and DataFrame APIs offer a low barrier to entry into big data processing.
    They allow you to use your existing knowledge and skills of data analytics and
    allow you to easily get started with Distributed Computing. They help you get
    started with processing data at scale, without having to deal with any of the
    complexities that typically come along with Distributed Computing frameworks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 和 DataFrame API 提供了一个低门槛进入大数据处理的途径。它们让你可以使用现有的数据分析知识和技能，轻松开始分布式计算。它们帮助你开始进行大规模数据处理，而无需处理分布式计算框架通常带来的复杂性。
- en: In this section, you will learn how to use both DataFrame and Spark SQL APIs
    to get started with your scalable data processing journey. Notably, the concepts
    learned here will be useful and are required throughout this book.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容将教你如何使用 DataFrame 和 Spark SQL API 来开始你的可扩展数据处理之旅。值得注意的是，这里学到的概念在本书中会非常有用，并且是必须掌握的。
- en: Transforming data with Spark DataFrames
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Spark DataFrame 转换数据
- en: Starting with Apache Spark 1.3, the Spark SQL engine was added as a layer on
    top of the RDD API and expanded to every component of Spark, to offer an even
    easier to use and familiar API for developers. Over the years, the Spark SQL engine
    and its DataFrame and SQL APIs have grown to be even more robust and have become
    the de facto and recommended standard for using Spark in general. Throughout this
    book, you will be exclusively using either DataFrame operations or Spark SQL statements
    for all your data processing needs, and you will rarely ever use the RDD API.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Apache Spark 1.3 开始，Spark SQL 引擎被作为一层添加到 RDD API 之上，并扩展到 Spark 的每个组件，以提供一个更易于使用且熟悉的
    API 给开发者。多年来，Spark SQL 引擎及其 DataFrame 和 SQL API 变得更加稳健，并成为了使用 Spark 的事实标准和推荐标准。在本书中，你将专门使用
    DataFrame 操作或 Spark SQL 语句来进行所有数据处理需求，而很少使用 RDD API。
- en: 'Think of a Spark DataFrame as a Pandas DataFrame or a relational database table
    with rows and named columns. The only difference is that a Spark DataFrame resides
    in the memory of several machines instead of a single machine. The following diagram
    shows a Spark DataFrame with three columns distributed across three worker machines:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 可以把 Spark DataFrame 想象成一个 Pandas DataFrame 或一个具有行和命名列的关系型数据库表。唯一的区别是，Spark DataFrame
    存储在多台机器的内存中，而不是单台机器的内存中。下图展示了一个具有三列的 Spark DataFrame，分布在三台工作机器上：
- en: '![Figure 1.4 – A distributed DataFrame'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.4 – 分布式 DataFrame'
- en: '](img/B16736_01_04.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_01_04.jpg)'
- en: Figure 1.4 – A distributed DataFrame
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 分布式 DataFrame
- en: A Spark DataFrame is also an immutable data structure such as an RDD, consisting
    of rows and named columns, where each individual column can be of any type. Additionally,
    DataFrames come with operations that allow you to manipulate data, and we generally
    refer to these set of operations as **Domain Specific Language** (**DSL**). Spark
    DataFrame operations can be grouped into two main categories, namely, transformations
    and actions, which we will explore in the following sections.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrame 也是一种不可变的数据结构，类似于 RDD，包含行和命名列，其中每一列可以是任何类型。此外，DataFrame 提供了可以操作数据的功能，我们通常将这些操作集合称为
    **领域特定语言** (**DSL**)。Spark DataFrame 操作可以分为两大类，即转换（transformations）和动作（actions），我们将在接下来的章节中进行探讨。
- en: One advantage of using DataFrames or Spark SQL over the RDD API is that the
    Spark SQL engine comes with a built-in query optimizer called **Catalyst**. This
    Catalyst optimizer analyzes user code, along with any available statistics on
    the data, to generate the best possible execution plan for the query. This query
    plan is further converted into Java bytecode, which runs natively inside the Executor's
    Java JVM. This happens irrespective of the programming language used, thus making
    any code processed via the Spark SQL engine equally performant in most cases,
    whether it be written using Scala, Java, Python, R, or SQL.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DataFrame 或 Spark SQL 相较于 RDD API 的一个优势是，Spark SQL 引擎内置了一个查询优化器，名为 **Catalyst**。这个
    Catalyst 优化器分析用户代码，以及任何可用的数据统计信息，以生成查询的最佳执行计划。这个查询计划随后会被转换为 Java 字节码，原生运行在 Executor
    的 Java JVM 内部。无论使用哪种编程语言，这个过程都会发生，因此在大多数情况下，使用 Spark SQL 引擎处理的任何代码都能保持一致的性能表现，不论是使用
    Scala、Java、Python、R 还是 SQL 编写的代码。
- en: Transformations
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换
- en: '`read`, `select`, `where`, `filter`, `join`, and `groupBy`.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`read`、`select`、`where`、`filter`、`join` 和 `groupBy`。'
- en: Actions
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作
- en: '`write`, `count`, and `show`.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`write`、`count` 和 `show`。'
- en: Lazy evaluation
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 懒评估
- en: Spark transformations are lazily evaluated, which means that transformations
    are not evaluated immediately as they are declared, and data is not manifested
    in memory until an action is called. This has a few advantages, as it gives the
    Spark optimizer an opportunity to evaluate all of your transformations until an
    action is called and generate the most optimal plan of execution to get the best
    performance and efficiency out of your code.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 转换是懒评估的，这意味着转换不会在声明时立即评估，数据也不会在内存中展现，直到调用一个动作。这样做有几个优点，因为它给 Spark 优化器提供了评估所有转换的机会，直到调用一个动作，并生成最优化的执行计划，以便从代码中获得最佳性能和效率。
- en: The advantage of Lazy Evaluation coupled with Spark's Catalyst optimizer is
    that you can solely focus on expressing your data transformation logic and not
    worry too much about arranging your transformations in a specific order to get
    the best performance and efficiency out of your code. This helps you be more productive
    at your tasks and not become perplexed by the complexities of a new framework.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 懒评估与 Spark 的 Catalyst 优化器相结合的优势在于，你可以专注于表达数据转换逻辑，而不必过多担心按特定顺序安排转换，以便从代码中获得最佳性能和效率。这可以帮助你在任务中更高效，不至于被新框架的复杂性弄得困惑。
- en: Important note
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Compared to Pandas DataFrames, Spark DataFrames are not manifested in memory
    as soon as they are declared. They are only manifested in memory when an action
    is called. Similarly, DataFrame operations don't necessarily run in the order
    you specified them to, as Spark's Catalyst optimizer generates the best possible
    execution plan for you, sometimes even combining a few operations into a single
    unit.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Pandas DataFrame 相比，Spark DataFrame 在声明时不会立即加载到内存中。它们只有在调用动作时才会被加载到内存中。同样，DataFrame
    操作不一定按你指定的顺序执行，因为 Spark 的 Catalyst 优化器会为你生成最佳的执行计划，有时甚至会将多个操作合并成一个单元。
- en: 'Let''s take the word count example that we previously implemented using the
    RDD API and try to implement it using the DataFrame DSL:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以之前使用 RDD API 实现的词频统计为例，尝试使用 DataFrame DSL 来实现。
- en: '[PRE3]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the previous code snippet, the following occurs:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，发生了以下情况：
- en: First, we import a few functions from the PySpark SQL function library, namely,
    split and explode.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从 PySpark SQL 函数库中导入几个函数，分别是 split 和 explode。
- en: Then, we read text using the `SparkSession` `read.text()` method, which creates
    a DataFrame of lines of `StringType`.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用 `SparkSession` 的 `read.text()` 方法读取文本，这会创建一个由 `StringType` 类型的行组成的 DataFrame。
- en: We then use the `split()` function to separate out every line into its individual
    words; the result is a DataFrame with a single column, named `value`, which is
    actually a list of words.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们使用`split()`函数将每一行拆分为独立的单词；结果是一个包含单一列的DataFrame，列名为`value`，实际上是一个单词列表。
- en: Then, we use the `explode()` function to separate the list of words in each
    row out to every word on a separate row; the result is a DataFrame with a column
    labeled `word`.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用`explode()`函数将每一行中的单词列表分解为每个单词独立成行；结果是一个带有`word`列的DataFrame。
- en: Now we are finally ready to count our words, so we group our words by the `word`
    column and count individual occurrences of each word. The final result is a DataFrame
    of two columns, that is, the actual `word` and its `count`.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们终于准备好计算单词数量了，因此我们通过`word`列对单词进行分组，并统计每个单词的出现次数。最终结果是一个包含两列的DataFrame，即实际的`word`和它的`count`。
- en: We can view a sample of the result using the `show()` function, and, finally,
    save our results in persistent storage using the `write()` function.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`show()`函数查看结果样本，最后，使用`write()`函数将结果保存到持久化存储中。
- en: Can you guess which operations are actions? If you guessed `show()` or `write()`,
    then you are correct. Every other function, including `select()` and `groupBy()`,
    are transformations and will not induce the Spark job into action.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你能猜出哪些操作是动作操作吗？如果你猜测是`show()`或`write()`，那你是对的。其他所有函数，包括`select()`和`groupBy()`，都是转换操作，不会触发Spark任务的执行。
- en: Note
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the `read()` function is a transformation, sometimes, you will notice
    that it will actually execute a Spark job. The reason for this is that with certain
    structured and semi-structured data formats, Spark will try and infer the schema
    information from the underlying files and will process a small subset of the actual
    files to do this.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`read()`函数是一种转换操作，但有时你会注意到它实际上会执行一个Spark任务。之所以这样，是因为对于某些结构化和半结构化的数据格式，Spark会尝试从底层文件中推断模式信息，并处理实际文件的小部分以完成此操作。
- en: Using SQL on Spark
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark上的SQL
- en: SQL is an expressive language for ad hoc data exploration and business intelligence
    types of queries. Because it is a very high-level declarative programming language,
    the user can simply focus on the input and output and what needs to be done to
    the data and not worry too much about the programming complexities of how to actually
    implement the logic. Apache Spark's SQL engine also has a SQL language API along
    with the DataFrame and Dataset APIs.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: SQL是一种用于临时数据探索和商业智能查询的表达性语言。因为它是一种非常高级的声明式编程语言，用户只需关注输入、输出以及需要对数据执行的操作，而无需过多担心实际实现逻辑的编程复杂性。Apache
    Spark的SQL引擎也提供了SQL语言API，并与DataFrame和Dataset APIs一起使用。
- en: With Spark 3.0, Spark SQL is now compliant with ANSI standards, so if you are
    a data analyst who is familiar with another SQL-based platform, you should be
    able to get started with Spark SQL with minimal effort.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark 3.0，Spark SQL现在符合ANSI标准，因此，如果你是熟悉其他基于SQL的平台的数据分析师，你应该可以毫不费力地开始使用Spark
    SQL。
- en: Since DataFrames and Spark SQL utilize the same underlying Spark SQL engine,
    they are completely interchangeable, and it is often the case that users intermix
    DataFrame DSL with Spark SQL statements for parts of the code that are expressed
    easily with SQL.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DataFrame和Spark SQL使用相同的底层Spark SQL引擎，它们是完全可互换的，通常情况下，用户会将DataFrame DSL与Spark
    SQL语句混合使用，尤其是在代码的某些部分，用SQL表达更加简洁。
- en: 'Now, let''s rewrite our word count program using Spark SQL. First, we create
    a table specifying our text file to be a CSV file with a white space as the delimiter,
    a neat trick to read each line of the text file, and also split each file into
    individual words all at once:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用Spark SQL重写我们的单词计数程序。首先，我们创建一个表，指定我们的文本文件为CSV文件，并以空格作为分隔符，这是一个巧妙的技巧，可以读取文本文件的每一行，并且同时将每个文件拆分成单独的单词：
- en: '[PRE4]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we have a table of a single column of words, we just need to `GROUP
    BY` the `word` column and do a `COUNT()` operation to get our word counts:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个包含单列单词的表格，我们只需要对`word`列进行`GROUP BY`操作，并执行`COUNT()`操作来获得单词计数：
- en: '[PRE5]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, you can observe that solving the same business problem became progressively
    easier from using MapReduce to RRDs, to DataFrames and Spark SQL. With each new
    release, Apache Spark has been adding many higher-level programming abstractions,
    data transformation and utility functions, and other optimizations. The goal has
    been to enable data engineers, data scientists, and data analysts to focus their
    time and energy on solving the actual business problem at hand and not worry about
    complex programming abstractions or system architectures.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以观察到，解决相同的业务问题变得越来越容易，从使用 MapReduce 到 RRDs，再到 DataFrames 和 Spark SQL。每一个新版本发布时，Apache
    Spark 都在增加更多的高级编程抽象、数据转换和实用函数，以及其他优化。目标是让数据工程师、数据科学家和数据分析师能够将时间和精力集中在解决实际的业务问题上，而无需担心复杂的编程抽象或系统架构。
- en: Apache Spark's latest major release of version 3 has many such enhancements
    that make the life of a data analytics professional much easier. We will discuss
    the most prominent of these enhancements in the following section.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 最新版本 3 的主要发布包含了许多增强功能，使数据分析专业人员的工作变得更加轻松。我们将在接下来的章节中讨论这些增强功能中最突出的部分。
- en: What's new in Apache Spark 3.0?
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Spark 3.0 有什么新特性？
- en: 'There are many new and notable features in Apache Spark 3.0; however, only
    a few are mentioned here, which you will find very useful during the beginning
    phases of your data analytics journey:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Apache Spark 3.0 中有许多新的显著特性；不过这里只提到了其中一些，你会发现它们在数据分析初期阶段非常有用：
- en: '**Speed**: Apache Spark 3.0 is orders of magnitude faster than its predecessors.
    Third-party benchmarks have put Spark 3.0 to be anywhere from 2 to 17 times faster
    for certain types of workloads.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：Apache Spark 3.0 比其前版本快了几个数量级。第三方基准测试表明，Spark 3.0 在某些类型的工作负载下速度是前版本的
    2 到 17 倍。'
- en: '**Adaptive Query Execution**: The Spark SQL engine generates a few logical
    and physical query execution plans based on user code and any previously collected
    statistics on the source data. Then, it tries to choose the most optimal execution
    plan. However, sometimes, Spark is not able to generate the best possible execution
    plan either because the statistics are either stale or non-existent, leading to
    suboptimal performance. With adaptive query execution, Spark is able to dynamically
    adjust the execution plan during runtime and give the best possible query performance.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应查询执行**：Spark SQL 引擎根据用户代码和之前收集的源数据统计信息生成一些逻辑和物理查询执行计划。然后，它会尝试选择最优的执行计划。然而，有时由于统计信息过时或不存在，Spark
    可能无法生成最佳的执行计划，导致性能不佳。通过自适应查询执行，Spark 能够在运行时动态调整执行计划，从而提供最佳的查询性能。'
- en: '**Dynamic Partition Pruning**: Business intelligence systems and data warehouses
    follow a data modeling technique called **Dimensional Modeling**, where data is
    stored in a central fact table surrounded by a few dimensional tables. Business
    intelligence types of queries utilizing these dimensional models involve queries
    with multiple joins between the dimension and fact tables, along with various
    filter conditions on the dimension tables. With dynamic partition pruning, Spark
    is able to filter out any fact table partitions based on the filters applied on
    these dimensions, resulting in less data being read into the memory, which, in
    turn, results in better query performance.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态分区修剪**：商业智能系统和数据仓库采用了一种名为 **维度建模** 的数据建模技术，其中数据存储在一个中央事实表中，周围是几个维度表。利用这些维度模型的商业智能类型查询涉及多个维度表和事实表之间的连接，并带有各种维度表的筛选条件。通过动态分区修剪，Spark
    能够根据应用于这些维度的筛选条件，过滤掉任何事实表分区，从而减少内存中读取的数据量，进而提高查询性能。'
- en: '**Kubernetes Support**: Earlier, we learned that Spark comes with its own Standalone
    Cluster Manager and can also work with other popular resource managers such as
    YARN and Mesos. Now Spark 3.0 natively supports **Kubernetes**, which is a popular
    open source framework for running and managing parallel container services.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes 支持**：之前，我们了解到 Spark 提供了自己的独立集群管理器，并且还可以与其他流行的资源管理器如 YARN 和 Mesos
    配合使用。现在，Spark 3.0 原生支持 **Kubernetes**，这是一个流行的开源框架，用于运行和管理并行容器服务。'
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned the concept of Distributed Computing. We discovered
    why Distributed Computing has become very important, as the amount of data being
    generated is growing rapidly, and it is not practical or feasible to process all
    your data using a single specialist system.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了分布式计算的概念。我们发现，分布式计算变得非常重要，因为生成的数据量正在快速增长，使用单一专用系统处理所有数据既不切实际也不现实。
- en: You then learned about the concept of Data Parallel Processing and reviewed
    a practical example of its implementation by means of the MapReduce paradigm.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你学习了数据并行处理的概念，并通过 MapReduce 范式回顾了它的实际实现示例。
- en: Then, you were introduced to an in-memory, unified analytics engine called Apache
    Spark, and learned how fast and efficient it is for data processing. Additionally,
    you learned it is very intuitive and easy to get started for developing data processing
    applications. You also got to understand the architecture and components of Apache
    Spark and how they come together as a framework.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，你了解了一个名为 Apache Spark 的内存中统一分析引擎，并学习了它在数据处理方面的速度和高效性。此外，你还了解到，它非常直观且容易上手，适合开发数据处理应用程序。你还了解了
    Apache Spark 的架构及其组件，并理解了它们如何作为一个框架结合在一起。
- en: Next, you came to understand RDDs, which are the core abstraction of Apache
    Spark, how they store data on a cluster of machines in a distributed manner, and
    how you can leverage higher-order functions along with lambda functions to implement
    Data Parallel Processing via RDDs.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你学习了 RDD（弹性分布式数据集）的概念，它是 Apache Spark 的核心抽象，了解了它如何在一群机器上以分布式方式存储数据，以及如何通过高阶函数和
    lambda 函数结合使用 RDD 实现数据并行处理。
- en: You also learned about the Spark SQL engine component of Apache Spark, how it
    provides a higher level of abstraction than RRDs, and that it has several built-in
    functions that you might already be familiar with. You learned to leverage the
    DataFrame DSL to implement your data processing business logic in an easier and
    more familiar way. You also learned about Spark's SQL API, how it is ANSI SQL
    standards-compliant, and how it allows you to seamlessly perform SQL analytics
    on large amounts of data efficiently.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习了 Apache Spark 中的 Spark SQL 引擎组件，了解了它提供了比 RDD 更高层次的抽象，并且它有一些你可能已经熟悉的内置函数。你学会了利用
    DataFrame 的 DSL 以更简单且更熟悉的方式实现数据处理的业务逻辑。你还了解了 Spark 的 SQL API，它符合 ANSI SQL 标准，并且允许你高效地在大量数据上执行
    SQL 分析。
- en: You also came to know some of the prominent improvements in Apache Spark 3.0,
    such as adaptive query execution and dynamic partition pruning, which help make
    Spark 3.0 much faster in performance than its predecessors.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你还了解了 Apache Spark 3.0 中一些显著的改进，例如自适应查询执行和动态分区修剪，这些都大大提高了 Spark 3.0 的性能，使其比前几版本更快。
- en: Now that you have learned the basics of big data processing with Apache Spark,
    you are ready to embark on a data analytics journey using Spark. A typical data
    analytics journey starts with acquiring raw data from various source systems,
    ingesting it into a historical storage component such as a data warehouse or a
    data lake, then transforming the raw data by cleansing, integrating, and transforming
    it to get a single source of truth. Finally, you can gain actionable business
    insights through clean and integrated data, leveraging descriptive and predictive
    analytics. We will cover each of these aspects in the subsequent chapters of this
    book, starting with the process of data cleansing and ingestion in the following
    chapter.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学习了使用 Apache Spark 进行大数据处理的基础知识，接下来你可以开始利用 Spark 踏上数据分析之旅。一个典型的数据分析旅程从从各种源系统获取原始数据开始，将其导入到历史存储组件中，如数据仓库或数据湖，然后通过清洗、集成和转换来处理原始数据，以获得一个统一的真实数据源。最后，你可以通过干净且集成的数据，利用描述性和预测性分析获得可操作的业务洞察。我们将在本书的后续章节中讨论这些方面，首先从下一章的数据清洗和数据导入过程开始。
