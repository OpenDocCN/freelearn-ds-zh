- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Derivatives and Gradients
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 导数与梯度
- en: Now that we understand why multivariate functions and high-dimensional spaces
    are more complex than the single-variable case we studied earlier, it’s time to
    see how to do things in the general case.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了为什么多变量函数和高维空间比之前学习的单变量情况更复杂，是时候看看如何在一般情况下处理这些问题了。
- en: To recap quickly, our goal in machine learning is to optimize functions with
    millions of variables. For instance, think about a neural network N(x,w) trained
    for binary classification, where
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 快速回顾一下，我们在机器学习中的目标是优化拥有数百万个变量的函数。例如，考虑一个为二元分类训练的神经网络N(x,w)，其中
- en: x ∈ℝ^n is the input data,
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x ∈ℝ^n是输入数据，
- en: w ∈ℝ^m is the vector compressing all of the weight parameters,
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: w ∈ℝ^m是压缩所有权重参数的向量，
- en: and N(x,w) ∈ [0,1] is the prediction, representing the probability of belonging
    to the positive class.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而N(x,w) ∈ [0,1]是预测值，表示属于正类的概率。
- en: In the case of, say, binary cross-entropy loss, we have the loss function
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在比如二元交叉熵损失的情况下，我们有损失函数
- en: '![ d L(w ) = − ∑ y log N (x ,w ), i i k=1 ](img/file1457.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![ d L(w ) = − ∑ y log N (x ,w ), i i k=1 ](img/file1457.png)'
- en: where x[i] is the i-th data point with ground truth y[i] ∈{0,1}. See, I told
    you that we have to write much more in multivariable calculus. (We’ll talk about
    binary cross-entropy loss in Chapter [20](ch032.xhtml#the-expected-value).)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 其中x[i]是第i个数据点，真实值y[i] ∈{0,1}。看，我告诉过你，我们在多变量微积分中要写更多内容。（我们将在第[20](ch032.xhtml#the-expected-value)章讨论二元交叉熵损失。）
- en: 'Training the neural network is the same as finding a global minimum of L(w),
    if it exists. We have already seen how we can do optimization in a single variable:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络与寻找L(w)的全局最小值是一样的，如果它存在的话。我们已经看到如何在单变量情况下进行优化：
- en: figure out the direction of increase by calculating the derivative,
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过计算导数来找出增加的方向，
- en: take a small step,
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采取一个小步骤，
- en: then iterate.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后进行迭代。
- en: 'For this to work in multiple variables, we need to generalize the concept of
    the derivative. We can quickly discover the issue: since division with a vector
    is not defined, the difference quotient'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这在多变量情况下有效，我们需要推广导数的概念。我们可以迅速发现问题：因为向量的除法没有定义，所以差商
- en: '![f(x)−-f-(y-) x− y ](img/file1458.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![f(x)−-f-(y-) x− y ](img/file1458.png)'
- en: 'makes no sense when f : ℝ^n →ℝ is a function of n variables and x,y ∈ℝ^n are
    n-dimensional vectors.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '当f : ℝ^n →ℝ是一个n变量的函数且x,y ∈ℝ^n是n维向量时，这个公式没有意义。'
- en: How can we make sense of it, then? This is what we’ll learn in the following
    chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们该如何理解它呢？这就是我们将在下一章学习的内容。
- en: 16.1 Partial and total derivatives
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.1 偏导数与全导数
- en: 'Let’s take a look at multivariable functions more closely! For the sake of
    simplicity, let f : ℝ² →ℝ be our function of two variables. To emphasize the dependence
    on the individual variables, we often write'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们更仔细地看看多变量函数！为了简化起见，设f : ℝ² →ℝ为我们的二元函数。为了强调对单个变量的依赖，我们通常写成'
- en: '![f(x1,x2), x1,x2 ∈ ℝ. ](img/file1459.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![f(x1,x2), x1,x2 ∈ ℝ. ](img/file1459.png)'
- en: 'Here’s the trick: by fixing one of the variables, we obtain the two single-variable
    functions! That is, if x[1] ∈ℝ² is fixed, we have x→f(x[1],x), and if x[2] ∈ℝ²
    is fixed, we have x→f(x,x[2]), both of which are well-defined univariate functions.
    Think about this as slicing the function graph with a plane parallel to the x−z
    or the y −z axes, as illustrated by Figure [16.1](#). The part cut out by the
    plane is a single-variable function.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有个技巧：通过固定其中一个变量，我们就能得到两个单变量函数！也就是说，如果固定x[1] ∈ℝ²，我们就得到x→f(x[1],x)，如果固定x[2]
    ∈ℝ²，我们就得到x→f(x,x[2])，这两者都是定义良好的单变量函数。把这个看作是通过平行于x−z或y−z轴的平面来切割函数图像，就像图16.1所示。被平面切割出来的部分是一个单变量函数。
- en: '![PIC](img/file1462.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1462.png)'
- en: 'Figure 16.1: Slicing the surface with the x −z plane'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1：用x−z平面切割曲面
- en: 'We can define the derivative of these functions by the limit of difference
    quotients. These are called the partial derivatives:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过差商的极限来定义这些函数的导数。这些被称为偏导数：
- en: '![∂f-- f(x,x2)-−-f(x1,x2) ∂x1 (x1,x2 ) = xli→mx1 x − x1 , ∂f f(x ,x) − f(x
    ,x ) ----(x1,x2 ) = lim ---1---------1--2-. ∂x2 x→x2 x − x2 ](img/file1463.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![∂f-- f(x,x2)-−-f(x1,x2) ∂x1 (x1,x2 ) = xli→mx1 x − x1 , ∂f f(x ,x) − f(x
    ,x ) ----(x1,x2 ) = lim ---1---------1--2-. ∂x2 x→x2 x − x2 ](img/file1463.png)'
- en: (Keep in mind that x[1] signifies the variable in ![∂f- ∂x1](img/file1464.png),
    but an actual scalar value in the argument of ![∂f- ∂x1](img/file1465.png)(x[1],x[2]).
    This can be quite confusing, but you’ll soon learn to make sense of it.)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: （请记住，x[1]表示![∂f- ∂x1](img/file1464.png)中的变量，但在![∂f- ∂x1](img/file1465.png)(x[1],x[2])的参数中是一个实际的标量值。这可能会让人感到困惑，但你很快就能理解它。）
- en: 'The definition is similar for general multivariable functions; we just have
    to write much more. There, the partial derivative of f : ℝ^n →ℝ at the point x
    = (x[1],…,x[n]) with respect to the i-th variable is defined by'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '对于一般的多变量函数，定义是类似的；我们只需要写得更多。在那里，f : ℝ^n →ℝ 在点x = (x[1],…,x[n])处关于第i个变量的偏导数通过以下方式定义：'
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(21).png)(16.1)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(21).png)(16.1)'
- en: 'One of the biggest challenges in multivariable calculus is to manage the ever-growing
    notational complexity. Just take a look at the difference quotient above:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在多变量微积分中，最大的挑战之一就是管理不断增加的符号复杂性。只要看看上面的差商：
- en: '![f(x1,...,x,...,xn)-−-f(x1,...,xi,...,xn). x − xi ](img/file1468.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![f(x1,...,x,...,xn)-−-f(x1,...,xi,...,xn). x − xi ](img/file1468.png)'
- en: This is not the prettiest to look at, and this kind of notational complexity
    can pile up fast. Fortunately, linear algebra comes to the rescue! Not only can
    we compact the variables into the vector x = (x[1],…,x[n]), we can use the standard
    basis
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是最美观的表示方式，并且这种符号复杂性可能会迅速堆积。幸运的是，线性代数来解救我们！我们不仅可以将变量压缩成向量x = (x[1],…,x[n])，还可以使用标准基
- en: '![ei = (0,...,0, 1 ,0,...,0) ◟◝◜◞ i- th component ](img/file1469.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![ei = (0,...,0, 1 ,0,...,0) ◟◝◜◞ i- th component ](img/file1469.png)'
- en: to write the difference quotients as
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 将差商写成
- en: '![f(x+-hei)-−-f(x), h ∈ ℝ. h ](img/file1470.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![f(x+-hei)-−-f(x), h ∈ ℝ. h ](img/file1470.png)'
- en: Thus, ([19.1](#)) can be compacted. With this newly found form, we are ready
    to make a concise and formal definition for partial derivatives.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，([19.1](#)) 可以简化。通过这种新发现的形式，我们准备好为偏导数做出简洁和正式的定义。
- en: Definition 66\. (Partial derivatives)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 66\.（偏导数）
- en: 'Let f : ℝ^n →ℝ be a function of n variables. The partial derivative of f at
    the point x = (x[1],…,x[n]) with respect to the i-th variable is defined by'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '设f : ℝ^n →ℝ 是一个n变量的函数。f在点x = (x[1],…,x[n])处关于第i个变量的偏导数通过以下方式定义：'
- en: '![-∂f-(x ) = lim f(x-+-hei)−-f-(x-). ∂xi h→0 h ](img/file1471.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![-∂f-(x ) = lim f(x-+-hei)−-f-(x-). ∂xi h→0 h ](img/file1471.png)'
- en: If the above limit exists, we say that f is partially differentiable with respect
    to the i-th variable x[i].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果上述极限存在，我们称f在第i个变量x[i]处是部分可微的。
- en: The partial derivative is again a vector-scalar function. Because of this, it
    is often written as ![-∂- ∂xi](img/file1472.png)f, reflecting on the fact that
    the symbol ![∂-- ∂xi](img/file1473.png) can be thought of as a function that maps
    functions to functions. I know, this is a bit abstract, but you’ll get used to
    it quickly.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 偏导数再次是一个向量-标量函数。因为这个原因，它通常被写作 ![-∂- ∂xi](img/file1472.png)f，反映出符号 ![∂-- ∂xi](img/file1473.png)
    可以被看作一个将函数映射到函数的函数。我知道，这有点抽象，但你很快就会习惯的。
- en: As usual, there are several alternative notations for the partial derivatives.
    Among others, the symbols
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，偏导数有几种替代符号。包括符号
- en: f[x[i]](x),
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: f[x[i]](x),
- en: D[i]f(x),
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D[i]f(x),
- en: ∂[i]f(x)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ∂[i]f(x)
- en: denote the i-th partial derivative of f at x. For simplicity, we’ll use the
    old-school ![-∂f ∂xi](img/file1474.png)(x).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表示f在x处的第i个偏导数。为了简化，我们将使用老式符号 ![-∂f ∂xi](img/file1474.png)(x)。
- en: It’s best to start with a few examples to illustrate the concept of partial
    derivatives.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最好从几个例子开始，来说明偏导数的概念。
- en: Example 1\. Let
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 1\. 让
- en: '![f(x1,x2) = x21 + x22\. ](img/file1475.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![f(x1,x2) = x21 + x22\. ](img/file1475.png)'
- en: To calculate, say, ∂f∕∂x[1], we fix the second variable and treat x[2] as a
    constant. Formally, we obtain the single-variable function
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算，例如，∂f∕∂x[1]，我们固定第二个变量，并将x[2]视为常数。形式上，我们得到单变量函数
- en: '![ 1 2 2 f (x) := x + x2, x2 ∈ ℝ, ](img/file1476.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 2 2 f (x) := x + x2, x2 ∈ ℝ, ](img/file1476.png)'
- en: 'whose derivative gives the first partial derivative:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其导数给出了第一个偏导数：
- en: '![ 1 ∂f-(x ,x ) = df-(x ) = 2x . ∂x1 1 2 dx 1 1 ](img/file1477.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 ∂f-(x ,x ) = df-(x ) = 2x . ∂x1 1 2 dx 1 1 ](img/file1477.png)'
- en: Similarly, we get that
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们得到
- en: '![∂f ∂x-(x1,x2) = 2x2\. 2 ](img/file1478.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![∂f ∂x-(x1,x2) = 2x2\. 2 ](img/file1478.png)'
- en: Once you are comfortable with the mental gymnastics of fixing variables, you’ll
    be able to perform partial differentiation without writing out all the intermediate
    steps.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你习惯了固定变量的思维方式，就可以在不写出所有中间步骤的情况下执行偏微分。
- en: Example 2\. Let’s see a more complicated example. Define
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2\. 让我们来看一个更复杂的例子。定义
- en: '![f(x1,x2) = sin(x21 + x2). ](img/file1479.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![f(x1,x2) = sin(x21 + x2). ](img/file1479.png)'
- en: 'By fixing x[2], we obtain a composite function. Thus the chain rule is used
    to calculate the first partial derivative:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过固定x[2]，我们得到一个复合函数。因此，链式法则用于计算第一个偏导数：
- en: '![ ∂f ----(x1,x2 ) = 2x1 cos(x21 + x2). ∂x1 ](img/file1480.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![ ∂f ----(x1,x2 ) = 2x1 cos(x21 + x2). ∂x1 ](img/file1480.png)'
- en: Similarly, we obtain that
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们得到
- en: '![∂f-- 2 ∂x2(x1,x2) = cos(x1 + x2). ](img/file1481.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![∂f-- 2 ∂x2(x1,x2) = cos(x1 + x2). ](img/file1481.png)'
- en: (I highly advise you to carry out the above calculations step by step as an
    exercise, even if you understand all the intermediate steps.)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: （我强烈建议您逐步进行上述计算，即使您理解所有中间步骤，也要作为练习完成。）
- en: Example 3\. Finally, let’s see a function that is partially differentiable in
    one variable but not in the other. Define the function
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 3\. 最后，让我们看一个在一个变量上部分可微但在另一个变量上不可微的函数。定义函数
- en: '![ ( |{ − 1 if x2 <0, f(x1,x2) = |( 1 otherwise. ](img/file1482.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![ ( |{ − 1 if x2 <0, f(x1,x2) = |( 1 otherwise. ](img/file1482.png)'
- en: As f(x[1],x[2]) does not depend on x[1], we can see that by fixing x[2], the
    resulting function is constant. Thus,
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 f(x[1],x[2]) 不依赖于 x[1]，我们可以看到，通过固定 x[2]，结果函数是常数。因此，
- en: '![-∂f-(x ,x ) = 0 ∂x1 1 2 ](img/file1483.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![-∂f-(x ,x ) = 0 ∂x1 1 2 ](img/file1483.png)'
- en: holds everywhere. However, in x[2], there is a discontinuity at 0; thus, ![∂f-
    ∂x2](img/file1484.png) is undefined there.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有地方成立。然而，在 x[2] 中，0 处存在不连续性；因此，![-∂f ∂x2](img/file1484.png) 在该处未定义。
- en: 16.1.1 The gradient
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.1.1 梯度
- en: If a function is partially differentiable in every variable, we can compact
    the derivatives together in a single vector to form the gradient.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个函数在每个变量上都部分可微，我们可以将所有的导数合并成一个单一的向量来形成梯度。
- en: Definition 67\. (The gradient)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 67\. （梯度）
- en: 'Let f : ℝ^n → ℝ be a function that is partially differentiable in all of its
    variables. Then, its gradient is defined by the (column) vector'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n → ℝ 是一个在其所有变量上部分可微的函数。那么，其梯度由（列）向量定义为'
- en: '![ ⌊ ⌋ ∂∂x1f (x ) ||-∂-f (x )|| ∇f(x ) := ||∂x2 . || ∈ ℝn ×1\. |⌈ .. |⌉ -∂-
    ∂xnf (x ) ](img/file1485.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ∂∂x1f (x ) ||-∂-f (x )|| ∇f(x ) := ||∂x2 . || ∈ ℝn ×1\. |⌈ .. |⌉ -∂-
    ∂xnf (x ) ](img/file1485.png)'
- en: A few remarks are in order. First, the symbol ∇ is called nabla, a symbol that
    was conceived to denote gradients.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 需要做一些说明。首先，符号 ∇ 被称为 nabla，是一个用于表示梯度的符号。
- en: Second, the gradient can be thought of as a vector-vector function. To see that,
    consider the already familiar function f(x[1],x[2]) = x[1]² + x[2]². The gradient
    of f is
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，梯度可以看作是一个向量-向量函数。为了理解这一点，考虑已经熟悉的函数 f(x[1],x[2]) = x[1]² + x[2]²。f 的梯度是
- en: '![ ⌊ ⌋ 2x1 ∇f (x1,x2) = ⌈ ⌉ , 2x2 ](img/file1486.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ 2x1 ∇f (x1,x2) = ⌈ ⌉ , 2x2 ](img/file1486.png)'
- en: or
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '![∇f (x ) = 2x ](img/file1487.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![∇f (x ) = 2x ](img/file1487.png)'
- en: in vectorized form. We can visualize this by drawing the vector ∇f(x[1],x[2])
    at each point (x[1],x[2]) ∈ℝ².
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以向量化形式表示。我们可以通过在每个点 (x[1],x[2]) ∈ ℝ² 画出向量 ∇f(x[1],x[2]) 来可视化这一点。
- en: '![PIC](img/file1488.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1488.png)'
- en: 'Figure 16.2: The vector field given by the gradient of x[1]² + x[2]²'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.2：由 x[1]² + x[2]² 的梯度给出的向量场
- en: 'Thus, you can think about ∇f as a vector-vector function ∇f : ℝ^n →ℝ^n. The
    gradient at a given point x is obtained by evaluating this function, yielding
    (∇f)(x).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，您可以将 ∇f 看作是一个向量-向量函数 ∇f : ℝ^n → ℝ^n。给定点 x 处的梯度是通过评估该函数得到的，得到 (∇f)(x)。'
- en: For clarity, the parentheses are omitted, arriving at the all familiar notation
    ∇f(x).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，省略了括号，得到了大家熟悉的符号 ∇f(x)。
- en: 16.1.2 Higher order partial derivatives
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.1.2 高阶偏导数
- en: 'The partial derivatives of a vector-scalar function f : ℝ^n →ℝ are vector-scalar
    functions themselves. Thus, we can perform partial differentiation one more time!'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '向量-标量函数 f : ℝ^n → ℝ 的偏导数本身也是向量-标量函数。因此，我们可以再进行一次偏微分！'
- en: If they exist, the second order partial derivatives are defined by
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它们存在，二阶偏导数由以下公式定义：
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(22).png)(16.2)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![L(U,V ) = {f : U → V | f 是线性函数}](img/equation_(22).png)(16.2)'
- en: where a ∈ℝ^n is an arbitrary vector. (When the second partial differentiation
    takes place with respect to the same variable, ([16.2](ch026.xhtml#higher-order-partial-derivatives))
    is abbreviated by ![∂2f ∂x2i](img/file1494.png)(a).)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 a ∈ ℝ^n 是任意向量。（当第二次偏导数是关于同一变量的偏导时，([16.2](ch026.xhtml#higher-order-partial-derivatives))
    由 ![∂2f ∂x2i](img/file1494.png)(a) 简写表示。）
- en: 'The definition begs the question: is the order of differentiation interchangeable?
    That is, does'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义引出了一个问题：求导的顺序是否可以互换？也就是说，是否有
- en: '![ 2 2 --∂-f--(a) = -∂-f--(a) ∂xi∂xj ∂xj∂xi ](img/file1495.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 2 --∂-f--(a) = -∂-f--(a) ∂xi∂xj ∂xj∂xi ](img/file1495.png)'
- en: 'hold? The answer is quite surprising: the order is interchangeable under some
    mild assumptions, but not in the general case. There is a famous theorem about
    it which we won’t prove, but it’s essential to know.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案相当令人惊讶：在一些温和的假设下，求导顺序是可以互换的，但在一般情况下不是。关于这一点有一个著名的定理，我们不会证明，但它是非常重要的。
- en: Theorem 98\.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 98\.
- en: 'Let f : ℝ^n → ℝ be an arbitrary vector-scalar function and let a ∈ ℝ^n. If
    there is a small ball B(𝜀,a) ⊆ℝ^n centered at a such that f has continuous second-order
    partial derivatives at all points of B(𝜀,a), then'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '设f : ℝ^n → ℝ为任意的向量-标量函数，且a ∈ ℝ^n。如果存在一个以a为中心的球B(𝜖,a) ⊆ ℝ^n，使得f在B(𝜖,a)的所有点处具有连续的二阶偏导数，则'
- en: '![--∂2f-- -∂2f--- ∂xi ∂xj(a) = ∂xj∂xi(a) ](img/file1496.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![--∂2f-- -∂2f--- ∂xi ∂xj(a) = ∂xj∂xi(a)](img/file1496.png)'
- en: holds for all i = 1,…,n.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有i = 1,…,n成立。
- en: Theorem [98](ch026.xhtml#x1-456002r98) is known as either Schwarz’s theorem,
    Clairaut’s theorem, or Young’s theorem.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 定理[98](ch026.xhtml#x1-456002r98)被称为施瓦茨定理、克莱罗定理或杨氏定理。
- en: 16.1.3 The total derivative
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.1.3 总导数
- en: Partial derivatives seem to generalize the notion of differentiability for multivariable
    functions. However, something is missing. Let’s revisit the single-variable case
    for a moment.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 偏导数似乎将可微性的概念推广到多变量函数。然而，似乎还有什么缺失。让我们稍微回顾一下单变量的情况。
- en: 'Recall that according to Theorem [77](ch020.xhtml#x1-199002r77), the differentiability
    of a single-variable function f : ℝ →ℝ at a given point a is equivalent to a local
    approximation of f by the linear function'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '回想一下，根据定理[77](ch020.xhtml#x1-199002r77)，单变量函数f : ℝ → ℝ在给定点a处的可微性等价于由线性函数进行的局部近似'
- en: '![l(x ) = f(a) + f′(a)(x − a). ](img/file1497.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![l(x ) = f(a) + f′(a)(x − a).](img/file1497.png)'
- en: If x is close to a, l(x) is also close to f(x). Moreover, this is the best linear
    approximation we can do around a. In a single variable, this is equivalent to
    differentiation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果x靠近a，l(x)也将接近f(x)。此外，这是我们在a附近所能做的最佳线性近似。在单变量中，这相当于求导。
- en: 'This gives us an idea: even though difference quotients like ![f(x)−f(y) x−y](img/file1498.png)
    do not exist in multiple variables, the best local approximation with a multivariable
    linear function does!'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们一个想法：尽管像![f(x)−f(y) x−y](img/file1498.png)这样的差商在多变量中不存在，但多变量线性函数的最佳局部近似却存在！
- en: Thus, the notion of total differentiability is born.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总可微性的概念就此诞生。
- en: Definition 68\. (Total differentiability)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 68.（总可微性）
- en: 'Let f : ℝ^n →ℝ be a function of n variables. We say that f is totally differentiable
    (or sometimes just differentiable for short) at a ∈ℝ^n if there exists a row vector
    D[f](a) ∈ℝ^(1×n) such that'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '设f : ℝ^n → ℝ为一个n变量的函数。如果f在a ∈ ℝ^n处完全可微（或简称为可微），则存在一个行向量D[f](a) ∈ ℝ^(1×n)，使得'
- en: f(x) = f(a) + D[f](a)(x − a) + o(∥x − a∥) (16.3)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = f(a) + D[f](a)(x − a) + o(∥x − a∥) (16.3)
- en: holds for all x ∈B(𝜀,a), where 𝜀/span>0 and B(𝜀,a) is defined by
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有x ∈ B(𝜖,a)，其中𝜖/span>0，B(𝜖,a)由下式定义
- en: '![B(𝜀,a) = {x ∈ ℝn : ∥x − a∥} <𝜀. ](img/file1499.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![B(𝜖,a) = {x ∈ ℝn : ∥x − a∥} <𝜖.](img/file1499.png)'
- en: (In other words, B(𝜀,a) is a ball of radius 𝜀/span>0 around a.) When exists,
    the vector D[f](a) is called the total derivative of f at a.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: （换句话说，B(𝜖,a)是以a为中心，半径为𝜖/span>0的球。）当存在时，向量D[f](a)称为f在a处的总导数。
- en: Recall that when it is not stated explicitly, we use column vectors, because
    we want to write our linear transformations in the form Ax, where A ∈ℝ^(m×n) and
    x ∈ℝ^(n×1). Thus, the “dimensionology” of the formula
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '回想一下，当没有明确说明时，我们使用列向量，因为我们希望将线性变换写成Ax的形式，其中A ∈ ℝ^(m×n)且x ∈ ℝ^(n×1)。因此，公式的“维度学” '
- en: '![ f(x) = f(a) + Df (a )(x− a )+o(∥x − a∥) ∈ ℝ1×1 ◟◝1◜×◞1 ◟◝◜1◞×1 ◟-◝1◜× ◞n
    ◟-◝◜n×◞1 ∈ℝ ∈ℝ ∈ℝ ∈ℝ ](img/file1500.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![ f(x) = f(a) + Df (a )(x− a )+o(∥x − a∥) ∈ ℝ1×1 ◟◝1◜×◞1 ◟◝◜1◞×1 ◟-◝1◜× ◞n
    ◟-◝◜n×◞1 ∈ℝ ∈ℝ ∈ℝ ∈ℝ ](img/file1500.png)'
- en: works out. (Don’t be fooled, ℝ^(1×1) is a scalar.)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 计算得出。（别被愚弄了，ℝ^(1×1)是一个标量。）
- en: Let’s unravel the notion of total differentiability. The form ([16.3](ch026.xhtml#x1-257003r68))
    implies that a totally differentiable function f equals to the linear part f(a)
    + D[f](a)(x −a) plus a small error.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解开总可微性的概念。形式（[16.3](ch026.xhtml#x1-257003r68)）意味着一个完全可微的函数f等于线性部分f(a) + D[f](a)(x
    − a)加上一个小误差。
- en: The surface given by the linear part is called the tangent plane. We can visualize
    it for functions of two variables.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由线性部分给出的表面称为切平面。我们可以为二变量函数可视化它。
- en: '![PIC](img/file1501.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1501.png)'
- en: 'Figure 16.3: The tangent plane'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3：切平面
- en: Unsurprisingly, the partial and total derivatives share an intimate connection.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，偏导数和总导数有着密切的关系。
- en: Theorem 99\. (Total derivative and the partial derivatives)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 定理99.（总导数与偏导数）
- en: 'Let f : ℝ^n →ℝ be a function that is totally differentiable at a ∈ℝ^n. Then,
    all of its partial derivatives exist at a and'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '设f : ℝ^n → ℝ为一个在a ∈ ℝ^n处完全可微的函数。则，它的所有偏导数在a处都存在，并且'
- en: f(x) = f(a) + ∇f(a)^T (x − a) + o(∥x − a∥) (16.4)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = f(a) + ∇f(a)^T (x − a) + o(∥x − a∥) (16.4)
- en: holds for all a in some B(𝜀,a), 𝜀/span>0\. (That is, D[f](a) = ∇f(a)^T .)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 a ∈ B(𝜀,a)，𝜀/span>0，成立。（即，D[f](a) = ∇f(a)^T 。）
- en: In other words, the equation ([16.4](ch026.xhtml#x1-257006r99)) gives that the
    coefficients of the best linear approximation are equal to the partial derivatives.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，方程 ([16.4](ch026.xhtml#x1-257006r99)) 表明最佳线性近似的系数等于偏导数。
- en: Proof. Because f is totally differentiable at a, the definition gives that f
    can be written in the form
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。因为 f 在 a 处是完全可微的，定义告诉我们 f 可以写成如下形式
- en: '![f(x) = f(a)+ Df (a)(x− a)+ o(∥x − a∥), ](img/file1502.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![f(x) = f(a)+ Df (a)(x− a)+ o(∥x − a∥), ](img/file1502.png)'
- en: where D[f](a) = (d[1],…,d[n]) is the vector that describes the coefficients
    of the linear part.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 D[f](a) = (d[1],…,d[n]) 是描述线性部分系数的向量。
- en: Our goal is to show that
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是证明
- en: '![ f-(a-+-hei)−-f-(a) hli→m0 h = di, ](img/file1503.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![ f-(a-+-hei)−-f-(a) hli→m0 h = di, ](img/file1503.png)'
- en: where e[i] is the unit (column) vector whose i-th component is 1, while the
    others are 0.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 e[i] 是单位（列）向量，其第 i 个分量为 1，其余分量为 0。
- en: Let’s do a quick calculation! Based on what we know, we have
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速做个计算！根据我们所知道的，我们有
- en: '![f(a-+-hei)−-f(a)-= Df-(a)hei +-o(∥hei∥) h h = Df (a)ei + o(1) = di + o(1),
    ](img/file1504.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![f(a-+-hei)−-f(a)-= Df-(a)hei +-o(∥hei∥) h h = Df (a)ei + o(1) = di + o(1),
    ](img/file1504.png)'
- en: thus confirming that lim[h→0]![f(a+hei)−-f(a)- h](img/file1505.png) = d[i],
    which is what we had to show.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从而确认 lim[h→0]![f(a+hei)−-f(a)- h](img/file1505.png) = d[i]，这正是我们需要证明的。
- en: What’s all the hassle with total differentiation, then? Theorem [99](ch026.xhtml#x1-257006r99)
    tells us that total differentiability is a stronger condition than partial differentiability.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，总微分的麻烦在哪里呢？定理 [99](ch026.xhtml#x1-257006r99) 告诉我们，总微分是比偏微分更强的条件。
- en: 'Surprisingly, the other direction is not true: the existence of partial derivatives
    does not imply total differentiability, as the example'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 出人意料的是，反方向不成立：偏导数的存在并不意味着总微分性，如示例所示。
- en: '![ ( |{ f (x, y) = 1 if x = 0 or y = 0, |( 0 otherwise ](img/file1506.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![ ( |{ f (x, y) = 1 如果 x = 0 或 y = 0, |( 0 否则 ](img/file1506.png)'
- en: illustrates. This function has all its partial derivatives at 0, yet the total
    derivative does not exist. (You can convince yourself by either drawing a figure,
    or noting that the function 1 −d^T x can never be o(∥x∥), regardless of the choice
    of d.)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示。这个函数在 0 处有所有的偏导数，但总导数不存在。（你可以通过画图或注意到函数 1 −d^T x 永远不可能是 o(∥x∥) 来证明这一点，无论
    d 的选择如何。）
- en: Remark 11\. (The total derivative as an operator)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 备注 11\. （总导数作为算子）
- en: 'Just like for single-variable functions, the total derivative of f : ℝ^n →ℝ
    is a function D[f] : ℝ^n →ℝ^n.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '就像单变量函数一样，f : ℝ^n →ℝ 的总导数是一个函数 D[f] : ℝ^n →ℝ^n。'
- en: 'At the highest level of abstraction, we can think about the total derivative
    as an operator that maps a vector-scalar function to a vector-vector function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高层次的抽象中，我们可以将总导数看作是一个算子，它将一个向量标量函数映射到一个向量向量函数：
- en: '![ n ℝ n ℝn D : (ℝ ) → (ℝ ) , D : f ↦→ D , f ](img/file1507.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![ n ℝ n ℝn D : (ℝ ) → (ℝ ) , D : f ↦→ D , f ](img/file1507.png)'
- en: where A^B denotes the set of all functions mapping A to B.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 A^B 表示将 A 映射到 B 的所有函数的集合。
- en: You are not required to understand this at all, but trust me, the more abstract
    your thinking is, the more powerful you’ll be.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要完全理解这一点，但相信我，你的思维越抽象，你就会越强大。
- en: 16.1.4 Directional derivatives
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.1.4 方向导数
- en: 'So far, we have talked about two kinds of derivatives: partial derivatives
    that describe the rate of change along a fixed axis, and total derivatives that
    give the best linear approximation of the function at a given point.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们谈了两种导数：描述沿固定轴变化速率的偏导数，以及给出给定点函数最佳线性近似的总导数。
- en: Partial derivatives are only concerned with a few particular directions. However,
    this is not the end of the story in multiple variables. With the standard orthonormal
    basis vectors e[i], the partial derivatives are defined by
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 偏导数只关注几个特定的方向。然而，在多变量中，这不是故事的全部。通过标准的正交归一基向量 e[i]，偏导数的定义为
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(23).png)(16.5)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![L(U,V ) = {f : U → V | f 是线性的}](img/equation_(23).png)(16.5)'
- en: As we saw earlier, these describe the rate of change along the dimensions. However,
    the standard orthonormal vectors are just a few special directions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，这些描述了沿各维度的变化速率。然而，标准的正交归一向量仅是一些特定的方向。
- en: What about an arbitrary direction v? Can we define the derivative along these?
    Sure! There is nothing stopping us from replacing e[i] with v in ([16.5](ch026.xhtml#directional-derivatives)).
    Thus, directional derivatives are born.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 那么在任意方向 v 下呢？我们可以定义沿这些方向的导数吗？当然！没有任何东西阻止我们在 ([16.5](ch026.xhtml#directional-derivatives))
    中用 v 替换 e[i]。因此，方向导数应运而生。
- en: Definition 69\. (Directional derivatives)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 69.（方向导数）
- en: 'Let f : ℝ^n →ℝ be a function of n variables and let v ∈ℝ^n be an arbitrary
    vector. The directional derivative of f along v is defined by the limit'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n →ℝ 是一个 n 个变量的函数，v ∈ℝ^n 是一个任意向量。f 在 v 方向上的方向导数由极限定义：'
- en: '![∂f f(a + hv) − f(a) ∂v-:= lhim→0 -------h--------. ](img/file1510.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![∂f f(a + hv) − f(a) ∂v-:= lhim→0 -------h--------. ](img/file1510.png)'
- en: 'Good news: the directional derivatives can be described in terms of the gradient!'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息：方向导数可以通过梯度来描述！
- en: Theorem 100\.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 100.
- en: 'Let f : ℝ^n →ℝ be a function of n variables. If f is totally differentiable
    at a ∈ℝ^n, then its directional derivatives exist in all directions, and'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n →ℝ 是一个 n 个变量的函数。如果 f 在 a ∈ℝ^n 处全微分，那么它在所有方向上的方向导数都存在，并且'
- en: '![∂f(a) = ∇f (a)Tv. ∂v ](img/file1511.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![∂f(a) = ∇f (a)Tv. ∂v ](img/file1511.png)'
- en: Proof. Because of the total differentiability, Theorem [103](ch026.xhtml#x1-263004r103)
    gives that
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。由于全微分性，定理 [103](ch026.xhtml#x1-263004r103) 表明：
- en: '![f (x ) = f (a )+ ∇f (a)T(x − a)+ o(∥x − a∥) ](img/file1512.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![f (x ) = f (a )+ ∇f (a)T(x − a)+ o(∥x − a∥) ](img/file1512.png)'
- en: around a. Thus,
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在 a 附近。因此，
- en: '![f(a+ hv )− f (a ) h∇f (a)Tv + o(h) ----------------= ---------------- h h
    = ∇f (a)Tv + o(1), ](img/file1513.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![f(a+ hv )− f (a ) h∇f (a)Tv + o(h) ----------------= ---------------- h h
    = ∇f (a)Tv + o(1), ](img/file1513.png)'
- en: giving that
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 给出：
- en: '![∂f-(a) = lim f-(a-+-hv-)−-f(a) ∂v h→0 h = lim ∇f (a)Tv + o(1) h→0 = ∇f (a)T
    v, ](img/file1514.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![∂f-(a) = lim f-(a-+-hv-)−-f(a) ∂v h→0 h = lim ∇f (a)Tv + o(1) h→0 = ∇f (a)T
    v, ](img/file1514.png)'
- en: as we needed to show.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所需的那样证明。
- en: 'In other words, Theorem [100](ch026.xhtml#x1-258003r100) gives that no matter
    the direction v, the directional derivative can be written in terms of the gradient
    and v. If you think about this for a minute, this is quite amazing: the rates
    of change along n special directions determine the rate of change in any other
    direction.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，定理 [100](ch026.xhtml#x1-258003r100) 表示，无论方向 v 如何，方向导数都可以用梯度和 v 来表示。如果你仔细想一想，这真的很惊人：沿着
    n 个特定方向的变化率决定了其他任何方向的变化率。
- en: 16.1.5 Properties of the gradient
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.1.5 梯度的性质
- en: In one variable, we have learned that if the derivative of f is positive at
    some a, then f is increasing around a. (If the derivative is negative, f is decreasing.)
    If we think about the derivative f^′(a) as a one-dimensional vector, then the
    derivative points towards the direction of increase.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在一维情况下，我们已经学到，如果 f 在某个 a 处的导数为正，那么 f 在 a 附近是增加的。（如果导数为负，则 f 是减少的。）如果我们将 f'(a)
    看作一个一维向量，那么导数指向增大的方向。
- en: Is this true in higher dimensions? Yes, and this is what makes gradient descent
    work.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这在高维空间中也成立吗？是的，这就是梯度下降法有效的原因。
- en: Theorem 101\. (The gradient determines the direction of the increase)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 101.（梯度决定增加的方向）
- en: 'Let f : ℝ^n →ℝ be a function of n variables, and suppose that f is totally
    differentiable at a ∈ℝ^n.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n →ℝ 是一个 n 个变量的函数，假设 f 在 a ∈ℝ^n 处全微分。'
- en: Then
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(24).png)(16.6)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(24).png)(16.6)'
- en: I know, ([16.6](ch026.xhtml#x1-259002r101)) is pretty overloaded, so let’s unpack
    it. First, let’s start with the mysterious argmax. For a given function f,
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道，([16.6](ch026.xhtml#x1-259002r101)) 很复杂，所以让我们来详细解析一下。首先，从神秘的 argmax 开始。对于给定的函数
    f，
- en: '![argmaxx ∈Sf(x) ](img/file1517.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![argmaxx ∈Sf(x) ](img/file1517.png)'
- en: denotes the values that maximize f on the set S. As the maximum may not be unique,
    argmax can yield a set. (The definition of argmin is the same, but with minimum
    instead of maximum.)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表示在集合 S 上最大化 f 的值。由于最大值可能不唯一，argmax 可能会得到一个集合。（argmin 的定义相同，只是取最小值而不是最大值。）
- en: Thus, in English, ([16.6](ch026.xhtml#x1-259002r101)) states that the unit direction
    that maximizes the directional derivative at a ∈ℝ^n is the normalized gradient.
    Now we are ready to see the proof!
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，英文中，([16.6](ch026.xhtml#x1-259002r101)) 表明，在 a ∈ℝ^n 处，最大化方向导数的单位方向是归一化的梯度。现在我们准备好看到证明了！
- en: Proof. Do you remember the Cauchy-Schwarz inequality (Theorem [8](ch008.xhtml#x1-43003r8))?
    It was a long time ago, so let’s recall! In the vector space ℝ^n, the Cauchy-Schwarz
    inequality tells us that for any x,y ∈ℝ^n,
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。你还记得柯西-施瓦茨不等式（定理 [8](ch008.xhtml#x1-43003r8)）吗？那是很久以前的事了，让我们来回顾一下！在向量空间 ℝ^n
    中，柯西-施瓦茨不等式告诉我们，对于任何 x, y ∈ℝ^n，
- en: '![xT y ≤ ∥x∥∥y∥. ](img/file1518.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![xT y ≤ ∥x∥∥y∥. ](img/file1518.png)'
- en: Now, as Theorem [100](ch026.xhtml#x1-258003r100) implies, the directional derivatives
    can be written as
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，正如定理[100](ch026.xhtml#x1-258003r100)所暗示的，方向导数可以写作
- en: '![∂f- T ∂v(a) = ∇f (a) v. ](img/file1519.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![∂f- T ∂v(a) = ∇f (a) v. ](img/file1519.png)'
- en: Combined with the Cauchy-Schwarz inequality, we get that
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 结合柯西-施瓦茨不等式，我们得到
- en: '![∂f ---(a) = ∇f (a)T v ∂v ≤ ∥ ∇f (a )∥∥v ∥. ](img/file1520.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![∂f ---(a) = ∇f (a)T v ∂v ≤ ∥ ∇f (a )∥∥v ∥. ](img/file1520.png)'
- en: By restricting the directions to unit vectors,
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将方向限制为单位向量，
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(25).png)(16.7)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![L(U,V ) = {f : U → V | f 是线性的}](img/equation_(25).png)(16.7)'
- en: follows. Thus, the directional derivatives must be less than or equal to the
    gradient’s norm. (At least, along a direction vector with unit length.)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如此，方向导数必须小于或等于梯度的范数。（至少，在单位长度的方向向量上。）
- en: However, by letting v[0] = ∇f(a)∕∥∇f(a)∥, we obtain that
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过让 v[0] = ∇f(a)∕∥∇f(a)∥，我们得到
- en: '![∂f--(a ) = ∇f (a)Tv0 ∂v0 ∇f (a)T∇f (a) = ---∥∇f-(a)∥-- 2 = ∥∇f-(a)∥- ∥∇f
    (a)∥ = ∥∇f (a)∥. ](img/file1522.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![∂f--(a ) = ∇f (a)Tv0 ∂v0 ∇f (a)T∇f (a) = ---∥∇f-(a)∥-- 2 = ∥∇f-(a)∥- ∥∇f
    (a)∥ = ∥∇f (a)∥. ](img/file1522.png)'
- en: Thus, with the choice v[0] = ![-∇f(a)-](img/file1523.png), equality can be attained
    in ([16.7](#)). This means that ![](img/file1524.png) maximizes the directional
    derivative at a, which is what we had to prove.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过选择 v[0] = ![-∇f(a)-](img/file1523.png)，可以在([16.7](#))中达到等式。这意味着 ![](img/file1524.png)
    在 a 点最大化了方向导数，这就是我们需要证明的。
- en: With that, we have the basics of differentiation in multiple variables under
    our belt. To sum up, we have learned that the difference quotient definition of
    the derivative does not generalize directly for multiple variables, but we can
    fix all but one variables to make the difference quotient work, thus obtaining
    partial derivatives.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 到此，我们已经掌握了多变量的微分基础。总结一下，我们已经学到，导数的差商定义不能直接推广到多变量，但我们可以将除一个变量外的其他变量固定，从而使差商成立，从而得到偏导数。
- en: On the other hand, the linear approximation definition works in multiple dimensions,
    but instead of
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，线性近似定义在多维空间中也适用，但不是
- en: '![ ′ f(a)+ f (a)(x− a), f : ℝ → ℝ, x,a ∈ ℝ, ](img/file1525.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![ ′ f(a)+ f (a)(x− a), f : ℝ → ℝ, x,a ∈ ℝ, ](img/file1525.png)'
- en: like we had in one variable, we obtain
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在单变量时那样，我们得到
- en: '![f (a )+ ∇f (a)T(x− a), f : ℝn → ℝ, x,a ∈ ℝn, ](img/file1526.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![f (a )+ ∇f (a)T(x− a), f : ℝn → ℝ, x,a ∈ ℝn, ](img/file1526.png)'
- en: where the analogue of the derivative is the gradient vector ∇f(a) ∈ℝ^n.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，导数的类比是梯度向量 ∇f(a) ∈ℝ^n。
- en: Even when we were studying differentiation in one variable for the first time,
    I told you that the local linear approximation definition would be useful someday.
    That time is now, and we are reaping the benefits. Soon, we’ll see gradient descent
    in its full glory.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们第一次学习单变量微分时，我也曾告诉你，局部线性近似定义总有一天会派上用场。那个时候就是现在，我们正在收获成果。很快，我们将全面了解梯度下降。
- en: 16.2 Derivatives of vector-valued functions
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.2 向量值函数的导数
- en: 'In a single variable, defining higher-order derivatives is easy. We simply
    have to keep repeating differentiation:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在单变量情况下，定义高阶导数是很简单的。我们只需要不断地进行求导：
- en: '![ ′′ ′ ′ f (x) = (f (x)), f′′′(x) = (f′′(x))′, ](img/file1527.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![ ′′ ′ ′ f (x) = (f (x)), f′′′(x) = (f′′(x))′, ](img/file1527.png)'
- en: and so on. However, this is not that straightforward with multivariable functions.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。然而，对于多变量函数来说，这并不是那么简单。
- en: So far, we have only talked about gradients, the generalization of the derivative
    for vector-scalar functions.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了梯度，这是向量-标量函数的导数的推广。
- en: 'As ∇f(a) is a column vector, the gradient is a vector-vector function ∇ : ℝ^n
    →ℝ^n. We only know how to compute the derivative of vector-scalar functions. It’s
    time to change that!'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '由于 ∇f(a) 是列向量，梯度是一个向量-向量函数 ∇ : ℝ^n →ℝ^n。我们只知道如何计算向量-标量函数的导数。是时候改变这一点了！'
- en: 16.2.1 The derivatives of curves
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.1 曲线的导数
- en: Curves, often describing the solutions of dynamical systems, are one of the
    most important objects in mathematics. We don’t explicitly use them in machine
    learning, but they are underneath algorithms such as gradient descent. (Where
    we traverse a discretized curve leading to a local minimum.)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线，通常描述动态系统的解，是数学中最重要的对象之一。虽然我们在机器学习中并不显式使用它们，但它们在诸如梯度下降的算法中潜在地起作用。（在这里，我们遍历一个离散化的曲线，最终到达局部最小值。）
- en: Formally, a curve – that is, a scalar-vector function – is given by a function
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，曲线——即标量-向量函数——由一个函数给出
- en: '![ ⌊γ (t)⌋ | 1 | n ||γ2(t)|| n(×1) γ : ℝ → ℝ , γ (t) = || .. || ∈ ℝ , ⌈ . ⌉
    γn(t) ](img/file1528.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊γ (t)⌋ | 1 | n ||γ2(t)|| n(×1) γ : ℝ → ℝ , γ (t) = || .. || ∈ ℝ , ⌈ . ⌉
    γn(t) ](img/file1528.png)'
- en: 'where the γ[i] : ℝ →ℝ functions are good old single-variable scalar-scalar
    functions. As the independent variable often represents time, it is customary
    to denote it with t.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 γ[i] : ℝ →ℝ 函数是经典的一元标量-标量函数。由于自变量通常表示时间，因此习惯上用 t 来表示它。'
- en: 'We can differentiate γ componentwise:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以逐分量地对 γ 求导：
- en: '![ ⌊ ⌋ γ′1(t) ||γ′(t)|| γ′(t) := || 2\. || ∈ ℝn(×1). |⌈ .. |⌉ ′ γn(t) ](img/file1529.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ γ′1(t) ||γ′(t)|| γ′(t) := || 2\. || ∈ ℝn(×1). |⌈ .. |⌉ ′ γn(t) ](img/file1529.png)'
- en: If we indeed imagine γ(t) as a trajectory in space, γ^′(t) is the tangent vector
    to γ at t. Since the differentiation is componentwise, Theorem [77](ch020.xhtml#x1-199002r77)
    implies that if γ is differentiable at some a ∈ℝ,
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们确实将 γ(t) 想象为空间中的一条轨迹，则 γ′(t) 是 γ 在 t 处的切向量。由于微分是逐分量进行的，定理 [77](ch020.xhtml#x1-199002r77)
    说明，如果 γ 在某个 a ∈ℝ 处可微，
- en: γ(t) = γ(a) + γ′(t)^T (t − a) + o(|t − a|) (16.8)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: γ(t) = γ(a) + γ′(t)^T (t − a) + o(|t − a|) （16.8）
- en: 'there. The equation ([16.8](ch026.xhtml#the-derivatives-of-curves)) is a true
    vectorized formula: some components are vectors, and some are scalars. Yet, this
    is simple and makes perfect sense to us. Hiding the complexities of vectors and
    matrices is the true power of linear algebra.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在那里。方程式（[16.8](ch026.xhtml#the-derivatives-of-curves)）是一个真正的向量化公式：一些分量是向量，一些是标量。然而，这很简单，并且对我们来说非常有意义。隐藏向量和矩阵的复杂性是线性代数的真正力量。
- en: 'It is easy to see that for any two curves γ,η : ℝ →ℝ^n, differentiation is
    additive, as (γ + η)^′ = γ^′ + η^′. What happens when we compose a scalar-vector
    function with a vector-scalar one?'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '很容易看出，对于任意两条曲线 γ, η : ℝ →ℝ^n，微分是可加的，即 (γ + η)^′ = γ^′ + η^′。当我们将标量-向量函数与向量-标量函数复合时，会发生什么呢？'
- en: 'This situation is commonplace in machine learning. If, say, L : ℝ^n →ℝ describes
    the loss function and γ : ℝ →ℝ^n is our trajectory in the parameter space ℝ^n,
    the composite function f(γ(t)) describes the model loss at time t. Thus, to compute
    (f ∘γ)^′, we have to generalize the chain rule.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '这种情况在机器学习中非常常见。比如，假设 L : ℝ^n →ℝ 描述的是损失函数，而 γ : ℝ →ℝ^n 是我们在参数空间 ℝ^n 中的轨迹，复合函数
    f(γ(t)) 描述了时间 t 时刻的模型损失。因此，为了计算 (f ∘γ)^′，我们必须推广链式法则。'
- en: Theorem 102\. (The chain rule for composing scalar-vector and vector-scalar
    functions)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 102.（标量-向量函数和向量-标量函数复合的链式法则）
- en: 'Let γ : ℝ →ℝ^n and f : ℝ^n →ℝ be arbitrary functions. If γ is differentiable
    at some a ∈ℝ and f is differentiable at γ(a), then f ∘γ : ℝ →ℝ is also differentiable
    at a and'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '设 γ : ℝ →ℝ^n 和 f : ℝ^n →ℝ 为任意函数。如果 γ 在某个 a ∈ℝ 处可微，且 f 在 γ(a) 处可微，则 f ∘γ : ℝ
    →ℝ 也在 a 处可微，且'
- en: '![(f ∘γ)′(a) = ∇f (γ(a))Tγ′(a ) ](img/file1530.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![(f ∘γ)′(a) = ∇f (γ(a))Tγ′(a ) ](img/file1530.png)'
- en: there.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在那里。
- en: Proof. As f is differentiable at γ(a), Theorem [99](ch026.xhtml#x1-257006r99)
    gives
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。由于 f 在 γ(a) 处可微，定理 [99](ch026.xhtml#x1-257006r99) 给出：
- en: '![f(γ(t)) = f (γ (a )) + ∇f (γ(a))T (γ (t)− γ (a )) + o(∥γ(t) − γ(a)∥). ](img/file1531.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![f(γ(t)) = f (γ (a )) + ∇f (γ(a))T (γ (t)− γ (a )) + o(∥γ(t) − γ(a)∥). ](img/file1531.png)'
- en: Thus,
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '![(f ∘γ )′(a) = lim f(γ(t))−-f(γ(a)) t→a t− a T γ(t)−-γ(a)- = ∇f (γ(a)) lit→ma
    [ t− a + o(1)] T ′ = ∇f (γ(a)) γ (a), ](img/file1532.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![(f ∘γ )′(a) = lim f(γ(t))−f(γ(a)) t→a t− a T γ(t)−γ(a)- = ∇f (γ(a)) lit→ma
    [ t− a + o(1)] T ′ = ∇f (γ(a)) γ (a), ](img/file1532.png)'
- en: which is what we had to prove.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们要证明的。
- en: 16.2.2 The Jacobian and Hessian matrices
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.2 雅可比矩阵和海森矩阵
- en: 'Now, our task is to extend the derivative for vector-vector functions, so let
    f : ℝ^n →ℝ^m be one. By writing out the output of f explicitly, we can decompose
    it into multiple components:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，我们的任务是将导数扩展到向量-向量函数。设 f : ℝ^n →ℝ^m 为一个这样的函数。通过显式地写出 f 的输出，我们可以将其分解为多个分量：'
- en: '![ ⌊ ⌋ | f1(x )| f(x) = |⌈ ... |⌉ ∈ ℝm (×1) fm(x ) ](img/file1533.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ | f1(x )| f(x) = |⌈ ... |⌉ ∈ ℝm (×1) fm(x ) ](img/file1533.png)'
- en: 'where f[i] : ℝ^n →ℝ are vector-scalar functions.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 f[i] : ℝ^n →ℝ 是向量-标量函数。'
- en: The natural idea is to compute the partial derivatives for f[i], compacting
    them into a matrix. And so we shall!
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 自然的想法是计算 f[i] 的偏导数，将它们压缩成一个矩阵。我们就这么做！
- en: Definition 70\. (The Jacobian matrix)
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 70.（雅可比矩阵）
- en: 'Let f : ℝ^n →ℝ^m be an arbitrary vector-vector function, and suppose that'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n →ℝ^m 为任意的向量-向量函数，假设'
- en: '![f(x) = (f (x ),...,f (x)), 1 m ](img/file1534.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![f(x) = (f (x ),...,f (x)), 1 m ](img/file1534.png)'
- en: 'where all f[i] : ℝ^n → ℝ are (partially) differentiable at some a ∈ ℝ^n. The
    matrix'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '其中所有的 f[i] : ℝ^n → ℝ 都是在某个 a ∈ ℝ^n 处（部分）可微的。矩阵为'
- en: '![ ⌊ ∂f1 ∂f1 ∂f1 ⌋ | ∂x1(a) ∂x2(a) ... ∂xn(a)| | ∂f2(a) ∂f2(a) ... ∂f2(a)|
    Jf(a) := || ∂x1\. ∂x2\. . ∂xn. || ∈ ℝm ×n |⌈ .. .. .. .. |⌉ ∂fm- ∂fm- ∂fm-- ∂x1
    (a) ∂x2 (a) ... ∂xn(a) ](img/file1535.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ∂f1 ∂f1 ∂f1 ⌋ | ∂x1(a) ∂x2(a) ... ∂xn(a)| | ∂f2(a) ∂f2(a) ... ∂f2(a)|
    Jf(a) := || ∂x1\. ∂x2\. . ∂xn. || ∈ ℝm ×n |⌈ .. .. .. .. |⌉ ∂fm- ∂fm- ∂fm-- ∂x1
    (a) ∂x2 (a) ... ∂xn(a) ](img/file1535.png)'
- en: is called the Jacobian of f at a.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为 f 在 a 处的 Jacobian。
- en: 'In other words, the rows of the Jacobian are the gradients of f[i]:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，Jacobian 的行是 f[i] 的梯度：
- en: '![ ⌊ T ⌋ | ∇f1(a) | || ∇f2(a)T || || . || Jf(a) = | .. |. ||∇f (a )T || ⌈ m
    ⌉ ](img/file1536.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ T ⌋ | ∇f1(a) | || ∇f2(a)T || || . || Jf(a) = | .. |. ||∇f (a )T || ⌈ m
    ⌉ ](img/file1536.png)'
- en: 'I have good news: the best local linear approximation of f around a is given
    by'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我有个好消息：f 在 a 附近的最佳局部线性近似由下式给出
- en: '![f(x ) = f(a)+ J (a)(x − a)+ o(∥x − a∥), f ](img/file1537.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![f(x ) = f(a)+ J (a)(x − a)+ o(∥x − a∥), f ](img/file1537.png)'
- en: if the best local linear approximation exists. Thus, the Jacobian is a proper
    generalization of the gradient.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最佳局部线性近似存在。于是，Jacobian 成为梯度的适当概括。
- en: 'We can use the Jacobian to generalize the notion of second-order derivatives
    for vector-scalar functions: by computing the Jacobian of the gradient, we obtain
    a special matrix, the analogue of the second derivative.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用 Jacobian 来概括向量-标量函数的二阶导数的概念：通过计算梯度的 Jacobian，我们得到一个特殊的矩阵，它是二阶导数的类比。
- en: Definition 71\. (The Hessian matrix)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 71.（Hessian 矩阵）
- en: 'Let f : ℝ^n →ℝ be an arbitrary vector-scalar function, and suppose that all
    of its second-order partial derivatives exist at a ∈ℝ^n.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n →ℝ 是一个任意的向量-标量函数，并假设其所有二阶偏导数在 a ∈ℝ^n 处存在。'
- en: The matrix
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵
- en: '![ ⌊ ⌋ ∂2f2(a) -∂2f-(a) ... -∂2f-(a) || ∂x21 ∂x1∂2x2 ∂x1∂2xn || || ∂∂x2f∂x1(a
    ) ∂∂xf2(a) ... ∂x∂2∂fxn(a)|| n×n Hf (a) := | .. 2.. .. .. | ∈ ℝ |⌈ . . . . |⌉
    -∂2f--(a ) -∂2f-(a) ... ∂2f2(a) ∂xn∂x1 ∂xn∂x2 ∂xn ](img/file1538.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ∂2f2(a) -∂2f-(a) ... -∂2f-(a) || ∂x21 ∂x1∂2x2 ∂x1∂2xn || || ∂∂x2f∂x1(a
    ) ∂∂xf2(a) ... ∂x∂2∂fxn(a)|| n×n Hf (a) := | .. 2.. .. .. | ∈ ℝ |⌈ . . . . |⌉
    -∂2f--(a ) -∂2f-(a) ... ∂2f2(a) ∂xn∂x1 ∂xn∂x2 ∂xn ](img/file1538.png)'
- en: is called the Hessian of f at a.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为 f 在 a 处的 Hessian。
- en: In other words,
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，
- en: '![Hf (a) = J ∇f(a)T ](img/file1539.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![Hf (a) = J ∇f(a)T ](img/file1539.png)'
- en: holds by definition. Moreover, if f behaves nicely (for instance, all second-order
    partial derivatives exist and are continuous), Theorem [98](ch026.xhtml#x1-456002r98)
    implies that the Hessian is symmetric; that is, H[f](a) = H[f](a)^T .
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 通过定义得以成立。此外，如果 f 行为良好（例如，所有二阶偏导数都存在且连续），定理[98](ch026.xhtml#x1-456002r98)表明 Hessian
    是对称的；也就是说，H[f](a) = H[f](a)^T。
- en: 16.2.3 The total derivative for vector-vector functions
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.3 向量-向量函数的总导数
- en: One last generalization. (I promise.) Recall that the existence of the gradient
    (that is, partial differentiability) doesn’t imply total differentiability for
    vector-scalar functions, as the example
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个概括。（我保证。）回忆一下，梯度的存在（即，部分可微性）并不意味着向量-标量函数的整体可微性，如该例所示
- en: '![ ( |{ f (x, y) = 1 if x = 0 or y = 0, |( 0 otherwise ](img/file1540.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![ ( |{ f (x, y) = 1 if x = 0 or y = 0, |( 0 otherwise ](img/file1540.png)'
- en: shows at zero.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 显示在零处。
- en: This is true for vector-vector functions as well, as the Jacobian is the generalization
    of the gradient, not the total derivative.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对于向量-向量函数同样成立，因为 Jacobian 是梯度的概括，而不是总导数。
- en: It is best to rip the band-aid off quickly and define the total derivative for
    vector-vector functions. The definition will be a bit abstract, but trust me,
    the investment will pay off when talking about the chain rule. (Which is the foundation
    of backpropagation, the algorithm that makes gradient descent computationally
    feasible.)
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 最好快速“撕下创可贴”，并为向量-向量函数定义总导数。这个定义可能有点抽象，但相信我，这项投资将在讨论链式法则时得到回报。（链式法则是反向传播算法的基础，而反向传播使得梯度下降的计算成为可能。）
- en: Definition 72\. (Total differentiability of vector-vector functions)
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 72.（向量-向量函数的整体可微性）
- en: 'Let f : ℝ^n →ℝ^m be an arbitrary vector-vector function. We say that f is totally
    differentiable (or sometimes just differentiable in short) at a ∈ℝ^n if there
    exists a matrixD[f](a) ∈ℝ^(m×n) such that'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '设 f : ℝ^n →ℝ^m 是一个任意的向量-向量函数。我们说 f 在 a ∈ℝ^n 处是整体可微的（或简略地称为可微的），如果存在矩阵 D[f](a)
    ∈ℝ^(m×n)，使得'
- en: f(x) = f(a) + D[f](a)(x − a) + o(∥x − a∥) (16.9)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = f(a) + D[f](a)(x − a) + o(∥x − a∥)（16.9）
- en: holds for all x ∈B(𝜀,a), where 𝜀/span>0 and B(𝜀,a) is defined by
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有 x ∈B(𝜖,a) 都成立，其中 𝜖/span>0 且 B(𝜖,a) 定义为
- en: '![B(𝜀,a) = {x ∈ ℝn : ∥x − a∥ <𝜀}. ](img/file1541.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![B(𝜖,a) = {x ∈ ℝn : ∥x − a∥ <𝜖}. ](img/file1541.png)'
- en: (In other words, B(𝜀,a) is a ball of radius 𝜀/span>0 around a.) When exists,
    the matrix D[f](a) is called the total derivative of f at a.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: （换句话说，B(𝜖,a) 是以 a 为中心、半径为 𝜖/span>0 的球体。）当存在时，矩阵 D[f](a) 被称为 f 在 a 处的总导数。
- en: Notice that Definition [72](ch026.xhtml#x1-263002r72) is almost verbatim to
    Definition [68](ch026.xhtml#x1-257003r68), except that the “derivative” is a matrix
    this time.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到定义[72](ch026.xhtml#x1-263002r72)几乎与定义[68](ch026.xhtml#x1-257003r68)完全相同，唯一不同的是这次“导数”是一个矩阵。
- en: You are probably not surprised to hear that its relation with the Jacobian is
    the same as the gradient and the total derivative in the vector-scalar case.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不惊讶地听到，它与雅可比矩阵的关系与向量-标量情况下的梯度和全导数相同。
- en: Theorem 103\. (Total derivative and the partial derivatives)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 定理103.（全导数与偏导数）
- en: 'Let f : ℝ^n →ℝ^m be a function that is totally differentiable at a ∈ℝ^n. Then,
    all of its partial derivatives exist at a and'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '设f : ℝ^n →ℝ^m是一个在a ∈ℝ^n处完全可微的函数。那么，它的所有偏导数在a处存在，并且'
- en: '![Df (a) = Jf(a ). ](img/file1542.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![Df (a) = Jf(a ). ](img/file1542.png)'
- en: The proof is almost identical to the one of Theorem [99](ch026.xhtml#x1-257006r99),
    with more complex notations. I strongly recommend you work it out line by line,
    as this kind of mental gymnastics helps significantly to get used to matrices
    in practice.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 证明几乎与定理[99](ch026.xhtml#x1-257006r99)的证明相同，只是符号更复杂。我强烈建议你逐行推导，因为这种脑力训练有助于你更好地适应矩阵的实际应用。
- en: Componentwise, the total derivative can be written as
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 按分量来看，全导数可以写成
- en: '![ ⌊ ∂f1 ∂f1 ∂f1- ⌋ | ∂x1(a) ∂x2(a) ... ∂xn (a )| | ∂f2(a) ∂f2(a) ... ∂f2-(a
    )| Df(a) = || ∂x1\. ∂x2\. . ∂xn. ||∈ ℝm ×n. |⌈ .. .. .. .. |⌉ ∂fm-(a) ∂fm(a) ...
    ∂fm-(a) ∂x1 ∂x2 ∂xn ](img/file1543.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ∂f1 ∂f1 ∂f1- ⌋ | ∂x1(a) ∂x2(a) ... ∂xn (a )| | ∂f2(a) ∂f2(a) ... ∂f2-(a
    )| Df(a) = || ∂x1\. ∂x2\. . ∂xn. ||∈ ℝm ×n. |⌈ .. .. .. .. |⌉ ∂fm-(a) ∂fm(a) ...
    ∂fm-(a) ∂x1 ∂x2 ∂xn ](img/file1543.png)'
- en: By introducing the notation
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入记号
- en: '![ ⌊ ⌋ ∂∂fx1(a ) || ∂fi2 || || ∂xi(a )|| -∂f(a) = | ... | ∈ ℝm ×1, ∂xi || ∂f
    || |⌈ ∂mxi (a)|⌉ ](img/file1544.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ∂∂fx1(a ) || ∂fi2 || || ∂xi(a )|| -∂f(a) = | ... | ∈ ℝm ×1, ∂xi || ∂f
    || |⌈ ∂mxi (a)|⌉ ](img/file1544.png)'
- en: the total derivative D[f](a) can be written in the block-forms
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 全导数D[f](a)可以写成块状形式。
- en: '![ [-∂f -∂f -∂f ] Df (a ) = ∂x1(a) ∂x2(a) ... ∂xn(a) ](img/file1545.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![ [-∂f -∂f -∂f ] Df (a ) = ∂x1(a) ∂x2(a) ... ∂xn(a) ](img/file1545.png)'
- en: and
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![ ⌊ ⌋ ∇f1 (a )T || T || D (a ) = || ∇f2 (a ) || . f | ... | ⌈ ⌉ ∇fm (a)T ](img/file1546.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ∇f1 (a )T || T || D (a ) = || ∇f2 (a ) || . f | ... | ⌈ ⌉ ∇fm (a)T ](img/file1546.png)'
- en: 16.2.4 Derivatives and function operations
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.4 导数与函数运算
- en: 'We have generalized the notion of derivatives as far as possible for us. Now
    it’s time to study their relations with the two essential function operations:
    addition and composition. (As there is no vector multiplication in higher dimensional
    spaces, the product and ratio of vector-vector functions are undefined.)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将导数的概念推广到尽可能广泛的程度。现在是时候研究它们与两个基本的函数运算——加法和复合——之间的关系了。（由于高维空间中没有向量乘法，向量-向量函数的乘积和比值是未定义的。）
- en: 'Let’s start with the simpler one: addition.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从更简单的开始：加法。
- en: Theorem 104\. (Linearity of the total derivative)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 定理104.（全导数的线性性质）
- en: 'Let f,g : ℝ^n →ℝ^m be two vector-vector functions that are differentiable at
    some a ∈ℝ^n, and let α,β ∈ℝ be two arbitrary scalars.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '设f,g : ℝ^n →ℝ^m是两个在某个a ∈ℝ^n处可微的向量-向量函数，且设α,β ∈ℝ为两个任意标量。'
- en: Then, αf + βg is also differentiable at a and
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，αf + βg在a处也是可微的，并且
- en: '![D αf+ βg(a) = αDf (a)+ βDg (a ) ](img/file1547.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![D αf+ βg(a) = αDf (a)+ βDg (a ) ](img/file1547.png)'
- en: there.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 那里。
- en: Proof. Because of the total differentiability, ([16.9](ch026.xhtml#x1-263002r72))
    implies that
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。由于全微分，（[16.9](ch026.xhtml#x1-263002r72)）意味着
- en: '![αf(x) + βg(x) = αf(a) + βg(a) + (αD (a )+ βD (a))(x − a) f g + o(∥x− a∥),
    ](img/file1548.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![αf(x) + βg(x) = αf(a) + βg(a) + (αD (a )+ βD (a))(x − a) f g + o(∥x− a∥),
    ](img/file1548.png)'
- en: which implies
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着
- en: '![Dαf+βg(a) = αDf (a)+ βDg (a). ](img/file1549.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![Dαf+βg(a) = αDf (a)+ βDg (a). ](img/file1549.png)'
- en: This is what we had to show.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们需要展示的内容。
- en: Linearity is always nice, but what we need is the ultimate generalization of
    the chain rule. We previously saw the special case of composing a scalar-vector
    and a vector-vector function (see Theorem [102](ch026.xhtml#x1-261003r102)), but
    we need to go one step further.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 线性是始终可取的，但我们需要的是链式法则的终极推广。我们之前看到过标量-向量和向量-向量函数的特殊情况（见定理[102](ch026.xhtml#x1-261003r102)），但我们需要更进一步。
- en: The multivariable chain rule is extremely important in machine learning. A neural
    network is a composite function, with layers acting as components. During gradient
    descent, we use the chain rule to calculate the derivative of this composition.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量链式法则在机器学习中极其重要。神经网络是一个复合函数，层次结构充当了其组成部分。在梯度下降过程中，我们使用链式法则来计算这个复合函数的导数。
- en: Theorem 105\. (Multivariable chain rule)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 定理105.（多变量链式法则）
- en: 'Let f : ℝ^m →ℝ^l and g : ℝ^n →ℝ^m be two vector-vector functions. If g is totally
    differentiable at a ∈ℝ^n and f is totally differentiable at g(a), then f ∘g is
    also totally differentiable at a and'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '设f : ℝ^m →ℝ^l，g : ℝ^n →ℝ^m是两个向量-向量函数。如果g在a ∈ ℝ^n处完全可微，且f在g(a)处完全可微，则f ∘g在a处也完全可微，并且'
- en: D[f∘g](a) = D[f](g(a)) D[g](a) (16.10)
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: D[f∘g](a) = D[f](g(a)) D[g](a) (16.10)
- en: holds.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 成立。
- en: To our advantage, the derivative of a composed function ([16.10](ch026.xhtml#x1-264004r105))
    is given by the product of two matrices. Since matrix multiplication can be done
    lightning fast, this is good news.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们有利的是，复合函数的导数（[16.10](ch026.xhtml#x1-264004r105)）由两个矩阵的乘积给出。由于矩阵乘法可以快速进行，这是个好消息。
- en: We will see two proofs for Theorem [105](ch026.xhtml#x1-264004r105). One is
    done with a faster-than-light engine, while the other shows much more by reducing
    the general case to Theorem [102](ch026.xhtml#x1-261003r102). Both provide a ton
    of insight. Let’s start with the heavy machinery.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到定理[105](ch026.xhtml#x1-264004r105)的两个证明。一个使用超光速引擎，另一个通过将一般情况简化为定理[102](ch026.xhtml#x1-261003r102)展示了更多内容。两者都提供了丰富的见解。让我们从重型机械开始。
- en: Proof. (First method.)
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。（第一种方法。）
- en: As f is totally differentiable at g(a), the equation ([16.9](ch026.xhtml#x1-263002r72))
    implies
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 由于f在g(a)处完全可微，方程（[16.9](ch026.xhtml#x1-263002r72)）意味着
- en: '![f(g(x)) = f(g(a))+ D (g (a ))(g(x) − g(a))+ o(∥g(x)− g(a)∥). f ](img/file1550.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![f(g(x)) = f(g(a)) + D(g(a))(g(x) − g(a)) + o(∥g(x) − g(a)∥). f ](img/file1550.png)'
- en: In turn, again because of the total differentiability of g at a, we have
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，由于g在a处完全可微，我们有
- en: '![g(x)− g(a) = Dg (a)(x− a)+ o(∥x − a∥). ](img/file1551.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![g(x)− g(a) = Dg (a)(x− a)+ o(∥x − a∥). ](img/file1551.png)'
- en: Thus, we can continue our calculation by
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以继续计算
- en: '![f(g (x )) = f(g(a))+ Df (g(a))(g(x )− g(a))+ o(∥g(x) − g(a)∥) = f(g(a))+
    D (g(a))D (a)(x − a) f g + Df (g (a ))[o(∥x− a ∥)+ o(∥g(x)− g(a)∥)], ◟-----------------◝◜----------------◞
    =o (∥x−a∥) ](img/file1552.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![f(g (x )) = f(g(a)) + Df (g(a))(g(x )− g(a)) + o(∥g(x) − g(a)∥) = f(g(a))
    + D (g(a))D (a)(x − a) f g + Df (g (a ))[o(∥x− a ∥)+ o(∥g(x)− g(a)∥)], ◟-----------------◝◜----------------◞
    =o (∥x−a∥) ](img/file1552.png)'
- en: showing that f ∘g is totally differentiable at a with total derivative
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 证明f ∘g在a处完全可微，且具有全导数
- en: '![Df∘g(a) = Df(g (a ))Dg (a), ](img/file1553.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![Df∘g(a) = Df(g(a))Dg(a), ](img/file1553.png)'
- en: which is what we needed to show.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们需要展示的内容。
- en: Now, about that second proof.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于第二个证明。
- en: Proof. (Second method.)
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。（第二种方法。）
- en: Let’s unpack D[f∘g](a) a bit. Writing out the components of f ∘g, we have
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微分析一下D[f∘g](a)。写出f∘g的各个组成部分，我们得到
- en: '![ ⌊ ⌋ | (f ∘g )1(x )| | (f ∘g )2(x )| (f ∘ g)(x) = || . || ∈ ℝl, x ∈ ℝn. |⌈
    .. |⌉ (f ∘ g)(x) l ](img/file1554.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ | (f ∘g )1(x )| | (f ∘g )2(x )| (f ∘ g)(x) = || . || ∈ ℝl, x ∈ ℝn. |⌈
    .. |⌉ (f ∘ g)(x) l ](img/file1554.png)'
- en: By definition, the i-th row and j-th column of D[f∘g](a) is
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，D[f∘g](a)的第i行第j列是
- en: '![ ∂ (f ∘g )i (Df∘g(a))i,j = ---∂x---(a). j ](img/file1555.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![ ∂ (f ∘g )i (Df∘g(a))i,j = ---∂x---(a). j ](img/file1555.png)'
- en: If you look at it long enough, you’ll realize that ![∂(f∘g) --∂xji](img/file1556.png)(a)
    is the derivative of a single variable function. Indeed, the function to be differentiated
    is the composition of the curve
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你足够长时间看它，你会意识到![∂(f∘g) --∂xji](img/file1556.png)(a)是单变量函数的导数。事实上，要求导的函数是曲线的组合
- en: '![γ : t ↦→ g(a1,...,aj−1,t,aj+1,...,an) ](img/file1557.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![γ : t ↦→ g(a1,...,aj−1,t,aj+1,...,an) ](img/file1557.png)'
- en: 'and the vector-scalar function f[i] : ℝ^m → ℝ. Thus, the chain rule for the
    composition of scalar-vector and vector-scalar functions (given by Theorem [102](ch026.xhtml#x1-261003r102))
    can be applied:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '以及向量-标量函数f[i] : ℝ^m → ℝ。因此，标量-向量和向量-标量函数的链式法则（由定理[102](ch026.xhtml#x1-261003r102)给出）可以应用：'
- en: '![∂(f ∘g)i T ∂ --∂x----(a) = ∇fi(g(a)) ∂x--g(a), j j ](img/file1558.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![∂(f ∘g)i T ∂ --∂x----(a) = ∇fi(g(a)) ∂x--g(a), j j ](img/file1558.png)'
- en: where ![∂∂xj](img/file1559.png)g(a) is the componentwise derivative
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![∂∂xj](img/file1559.png)g(a)是按分量计算的导数
- en: '![ ⌊ ⌋ ∂g1(a) || ∂∂g2xj(a)|| -∂-- || -∂xj--|| ∂xj g(a) = | .. | . |⌈ . |⌉ ∂gm∂(xa)
    j ](img/file1560.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ∂g1(a) || ∂∂g2xj(a)|| -∂-- || -∂xj--|| ∂xj g(a) = | .. | . |⌈ . |⌉ ∂gm∂(xa)
    j ](img/file1560.png)'
- en: To sum up, we have
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们得到
- en: '![ ](img/file1561.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![ ](img/file1561.png)'
- en: This is the element in the i-th row and j-th column of the matrix product D[f](g(a))D[g](a),
    hence
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这是矩阵乘积D[f](g(a))D[g](a)中第i行第j列的元素，因此
- en: '![ ](img/file1562.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![ ](img/file1562.png)'
- en: which is what we had to show.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们必须展示的内容。
- en: With the concept of total derivatives for vector-vector functions and the general
    chain rule under our belt, we are ready to actually do things with multivariable
    functions. Thus, our next stop lays the foundations of optimization.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 通过掌握向量-向量函数的全导数概念和一般的链式法则，我们准备好实际处理多变量函数了。因此，我们的下一站是奠定优化的基础。
- en: 16.3 Summary
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.3 总结
- en: 'You know by now: half the success in mathematics is picking the right representations
    and notations. Although multivariable calculus can seem insanely complex, it’s
    a cakewalk if we have a good understanding of linear algebra. This is why we started
    our entire journey with vectors and matrices! Going from f(x[1],…,x[n]) to f(x)
    is a big deal.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该知道：数学成功的一半在于选择正确的表示法和符号。尽管多变量微积分看起来异常复杂，但如果我们对线性代数有很好的理解，它就变得轻松了。这也是我们为何从向量和矩阵开始的原因！从
    f(x[1],…,x[n]) 到 f(x) 是一项重要的进展。
- en: In this chapter, we have learned that differentiation in multiple dimensions
    is slightly more complicated than in the single-variable case. First, we have
    the partial derivatives defined by
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学到了多维度的微分比单变量情况稍微复杂一些。首先，我们定义了偏导数：
- en: '![∂f-(a) = lim f(a+-hei)-−-f(a), a ∈ ℝn, ∂xi h→0 h ](img/file1563.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![∂f-(a) = lim f(a+-hei)-−-f(a), a ∈ ℝn, ∂xi h→0 h ](img/file1563.png)'
- en: 'where e[i] is the vector whose i-th component is one, while the others are
    zero. We can think about ![∂∂fx- i](img/file1564.png) as the derivative of the
    single-variable function obtained by fixing all but the i-th variable of f. Together,
    the partial derivatives form the gradient:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，e[i] 是一个向量，其第 i 个分量为 1，其他分量为 0。我们可以将 ![∂∂fx- i](img/file1564.png) 视为通过固定
    f 的除第 i 个变量外所有变量所得到的单变量函数的导数。所有的偏导数组成梯度：
- en: '![ ⌊-∂- ⌋ |∂x1f (a )| ||∂∂x2f (a )|| n×1 ∇f (a ) := || .. || ∈ ℝ . ⌈ . ⌉ -∂-f
    (a ) ∂xn ](img/file1565.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊-∂- ⌋ |∂x1f (a )| ||∂∂x2f (a )|| n×1 ∇f (a ) := || .. || ∈ ℝ . ⌈ . ⌉ -∂-f
    (a ) ∂xn ](img/file1565.png)'
- en: However, the partial derivatives are not exactly the perfect analogue of the
    univariate derivatives. There, we learned that the derivative is the best local
    linear approximation, and this is the version that can be generalized to multiple
    variables. Thus, we say that f is totally differentiable at a ∈ℝ^n if it can be
    written in the form
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，偏导数并不完全是单变量导数的完美类比。在单变量的情况下，我们知道导数是最好的局部线性近似，而这正是可以推广到多变量的版本。因此，我们说 f 在 a
    ∈ ℝ^n 处是全微分的，如果它可以写成以下形式：
- en: '![f(x) = f(a) + ∇f (a )T (x − a) + o(∥x− a ∥). ](img/file1566.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![f(x) = f(a) + ∇f (a )T (x − a) + o(∥x− a ∥). ](img/file1566.png)'
- en: In machine learning, one of the most essential tools is the multivariable chain
    rule
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，最重要的工具之一就是多变量链式法则。
- en: '![Df∘g(a) = Df(g (a))Dg (a), ](img/file1567.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![Df∘g(a) = Df(g (a))Dg (a), ](img/file1567.png)'
- en: which is used to compute the derivatives in practice. Without the chain rule,
    we wouldn’t have any effective method to compute the gradient. In turn, as the
    name suggests, the gradient is the cornerstone of gradient descent. We already
    understand the single-variable version, so it’s time to dive deep into the general
    one. See you in the next chapter!
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是用来实际计算导数的工具。如果没有链式法则，我们就没有有效的方法来计算梯度。因此，正如名字所示，梯度是梯度下降法的基石。我们已经理解了单变量版本，现在是时候深入研究一般的多变量版本了。下一章见！
- en: 16.4 Problems
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4 问题
- en: Problem 1\. Compute the partial derivatives and the Hessian matrix of the following
    functions.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 1. 计算以下函数的偏导数和 Hessian 矩阵。
- en: (a) f(x[1],x[2]) = x[1]^(3x[2]²) + 2x[1]x[2] + x[2]³ (b) f(x[1],x[2]) = e^(x[1]²−x[2])
    + sin(x[1]x[2]) (c) f(x[1],x[2]) = ln(x[1]² + x[2]²) + x[1]e^(x[2]) (d) f(x[1],x[2])
    = cos(x[1]x[2]) + x[1]² sin(x[2]) (e) f(x[1],x[2]) = f(x[1],x[2]) = ![x2+x2 x11−x22](img/file1568.png)
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: (a) f(x[1],x[2]) = x[1]^(3x[2]²) + 2x[1]x[2] + x[2]³ (b) f(x[1],x[2]) = e^(x[1]²−x[2])
    + sin(x[1]x[2]) (c) f(x[1],x[2]) = ln(x[1]² + x[2]²) + x[1]e^(x[2]) (d) f(x[1],x[2])
    = cos(x[1]x[2]) + x[1]² sin(x[2]) (e) f(x[1],x[2]) = f(x[1],x[2]) = ![x2+x2 x11−x22](img/file1568.png)
- en: Problem 2\. Compute the Jacobian matrix of the following functions.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 2. 计算以下函数的 Jacobian 矩阵。
- en: (a)
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![ ⌊ ⌋ x21x2 + ex2 f(x1,x2) = ⌈ x2⌉ sin(x1x2)+ x1e ](img/file1569.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ x21x2 + ex2 f(x1,x2) = ⌈ x2⌉ sin(x1x2)+ x1e ](img/file1569.png)'
- en: (b)
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![ ⌊ ⌋ ln(x21 + x22) + x1x2 f(x1,x2) = ⌈ 2 x1 ⌉ cos(x1)+ x 2e ](img/file1570.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ln(x21 + x22) + x1x2 f(x1,x2) = ⌈ 2 x1 ⌉ cos(x1)+ x 2e ](img/file1570.png)'
- en: (c)
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: '![ ⌊ ⌋ x3 − x2 f(x1,x2) = ⌈ 1 2 ⌉ ex1x2 + x1 cos(x2 ) ](img/file1571.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ x3 − x2 f(x1,x2) = ⌈ 1 2 ⌉ ex1x2 + x1 cos(x2 ) ](img/file1571.png)'
- en: (d)
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: (d)
- en: '![ ⌊ ⌋ ⌈ tan (x1x2 )+ x32 ⌉ f(x1,x2 ) = ∘x2-+-x2-+ sin(x ) 1 2 1 ](img/file1572.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ ⌈ tan (x1x2 )+ x32 ⌉ f(x1,x2 ) = ∘x2-+-x2-+ sin(x ) 1 2 1 ](img/file1572.png)'
- en: (e)
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: (e)
- en: '![ ⌊ ⌋ x ex2 − ln(1 + x2) f(x1,x2) = ⌈ 1 1 ⌉ x22cos(x1)+ x1x2 ](img/file1573.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊ ⌋ x ex2 − ln(1 + x2) f(x1,x2) = ⌈ 1 1 ⌉ x22cos(x1)+ x1x2 ](img/file1573.png)'
- en: Problem 3\. Let f(x[1],x[2]) = x[1]![∘ |x2|-](img/file1574.png). Show that f
    is partially differentiable but not totally differentiable at (0,0).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 3. 设 f(x[1],x[2]) = x[1]![∘ |x2|-](img/file1574.png)。证明 f 在 (0,0) 处是部分可微的，但不是全微的。
- en: Join our community on Discord
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他用户、机器学习专家以及作者本人一起阅读本书。提问、为其他读者提供解决方案、通过问我任何问题环节与作者交流，等等。扫描二维码或访问链接加入社区。[https://packt.link/math](https://packt.link/math)
- en: '![PIC](img/file1.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1.png)'
