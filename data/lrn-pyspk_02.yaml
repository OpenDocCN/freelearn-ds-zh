- en: Chapter 2. Resilient Distributed Datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 章。弹性分布式数据集
- en: Resilient Distributed Datasets (RDDs) are a distributed collection of immutable
    JVM objects that allow you to perform calculations very quickly, and they are
    the *backbone* of Apache Spark.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性分布式数据集（RDDs）是一组不可变的 JVM 对象的分布式集合，允许您非常快速地进行计算，它们是 Apache Spark 的 *骨干*。
- en: As the name suggests, the dataset is distributed; it is split into chunks based
    on some key and distributed to executor nodes. Doing so allows for running calculations
    against such datasets very quickly. Also, as already mentioned in [Chapter 1](ch01.html
    "Chapter 1. Understanding Spark"), *Understanding Spark*, RDDs keep track (log)
    of all the transformations applied to each chunk to speed up the computations
    and provide a fallback if things go wrong and that portion of the data is lost;
    in such cases, RDDs can recompute the data. This data lineage is another line
    of defense against data loss, a complement to data replication.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，数据集是分布式的；它根据某些键分割成块，并分布到执行节点。这样做可以非常快速地运行针对此类数据集的计算。此外，如[第 1 章](ch01.html
    "第 1 章。理解 Spark")中已提到的，“理解 Spark”，RDD 会跟踪（记录）对每个块应用的所有转换，以加快计算并提供回退机制，以防出现错误且该部分数据丢失；在这种情况下，RDD
    可以重新计算数据。这种数据血缘关系是防止数据丢失的另一道防线，是数据复制的补充。
- en: 'The following topics are covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Internal workings of an RDD
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD 的内部工作原理
- en: Creating RDDs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 RDD
- en: Global versus local scopes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局与局部作用域
- en: Transformations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换
- en: Actions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作
- en: Internal workings of an RDD
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD 的内部工作原理
- en: 'RDDs operate in parallel. This is the strongest advantage of working in Spark:
    Each transformation is executed in parallel for enormous increase in speed.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs 以并行方式运行。这是在 Spark 中工作的最大优势：每个转换都是并行执行的，这极大地提高了速度。
- en: 'The transformations to the dataset are lazy. This means that any transformation
    is only executed when an action on a dataset is called. This helps Spark to optimize
    the execution. For instance, consider the following very common steps that an
    analyst would normally do to get familiar with a dataset:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据集的转换是惰性的。这意味着只有当对数据集调用操作时，任何转换才会执行。这有助于 Spark 优化执行。例如，考虑以下分析师通常为了熟悉数据集而执行的非常常见的步骤：
- en: Count the occurrence of distinct values in a certain column.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算某个列中 distinct 值的出现次数。
- en: Select those that start with an `A`.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择以 `A` 开头的记录。
- en: Print the results to the screen.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果打印到屏幕上。
- en: As simple as the previously mentioned steps sound, if only items that start
    with the letter `A` are of interest, there is no point in counting distinct values
    for all the other items. Thus, instead of following the execution as outlined
    in the preceding points, Spark could only count the items that start with `A`,
    and then print the results to the screen.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面提到的步骤听起来很简单，但如果只对以字母 `A` 开头的项感兴趣，就没有必要对所有其他项的 distinct 值进行计数。因此，而不是按照前面几点所述的执行流程，Spark
    只能计数以 `A` 开头的项，然后将结果打印到屏幕上。
- en: 'Let''s break this example down in code. First, we order Spark to map the values
    of `A` using the `.map(lambda v: (v, 1))` method, and then select those records
    that start with an `''A''` (using the `.filter(lambda val: val.startswith(''A''))`
    method). If we call the `.reduceByKey(operator.add)` method it will reduce the
    dataset and *add* (in this example, count) the number of occurrences of each key.
    All of these steps **transform** the dataset.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们用代码来分解这个例子。首先，我们使用 `.map(lambda v: (v, 1))` 方法让 Spark 映射 `A` 的值，然后选择以 `''A''`
    开头的记录（使用 `.filter(lambda val: val.startswith(''A''))` 方法）。如果我们调用 `.reduceByKey(operator.add)`
    方法，它将减少数据集并 *添加*（在这个例子中，是计数）每个键的出现次数。所有这些步骤 **转换** 数据集。'
- en: Second, we call the `.collect()` method to execute the steps. This step is an
    **action** on our dataset - it finally counts the distinct elements of the dataset.
    In effect, the action might reverse the order of transformations and filter the
    data first before mapping, resulting in a smaller dataset being passed to the
    reducer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们调用 `.collect()` 方法来执行步骤。这一步是对我们数据集的 **操作** - 它最终计算数据集的 distinct 元素。实际上，操作可能会颠倒转换的顺序，先过滤数据然后再映射，从而将较小的数据集传递给归约器。
- en: Note
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Do not worry if you do not understand the previous commands yet - we will explain
    them in detail later in this chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您目前还不理解前面的命令，请不要担心 - 我们将在本章后面详细解释它们。
- en: Creating RDDs
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 RDD
- en: 'There are two ways to create an RDD in PySpark: you can either `.parallelize(...)`
    a collection (`list` or an `array` of some elements):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 中创建 RDD 有两种方式：你可以 `.parallelize(...)` 一个集合（`list` 或某些元素的 `array`）：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Or you can reference a file (or files) located either locally or somewhere
    externally:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 或者你可以引用位于本地或外部位置的文件（或文件）：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'We downloaded the Mortality dataset `VS14MORT.txt` file from (accessed on July
    31, 2016) [ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/DVS/mortality/mort2014us.zip](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/DVS/mortality/mort2014us.zip);
    the record schema is explained in this document [http://www.cdc.gov/nchs/data/dvs/Record_Layout_2014.pdf](http://www.cdc.gov/nchs/data/dvs/Record_Layout_2014.pdf).
    We selected this dataset on purpose: The encoding of the records will help us
    to explain how to use UDFs to transform your data later in this chapter. For your
    convenience, we also host the file here: [http://tomdrabas.com/data/VS14MORT.txt.gz](http://tomdrabas.com/data/VS14MORT.txt.gz)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从（于 2016 年 7 月 31 日访问）[ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/DVS/mortality/mort2014us.zip](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/DVS/mortality/mort2014us.zip)
    下载了死亡率数据集 `VS14MORT.txt` 文件；记录模式在本文档中解释 [http://www.cdc.gov/nchs/data/dvs/Record_Layout_2014.pdf](http://www.cdc.gov/nchs/data/dvs/Record_Layout_2014.pdf)。我们故意选择这个数据集：记录的编码将帮助我们解释如何在本章后面使用
    UDFs 来转换你的数据。为了你的方便，我们还在这里托管了文件：[http://tomdrabas.com/data/VS14MORT.txt.gz](http://tomdrabas.com/data/VS14MORT.txt.gz)
- en: The last parameter in `sc.textFile(..., n)` specifies the number of partitions
    the dataset is divided into.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`sc.textFile(..., n)` 的最后一个参数指定了数据集被分割成的分区数。'
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: A rule of thumb would be to break your dataset into two-four partitions for
    each in your cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经验法则是将你的数据集分成两到四个分区，每个分区在你的集群中。
- en: 'Spark can read from a multitude of filesystems: Local ones such as NTFS, FAT,
    or Mac OS Extended (HFS+), or distributed filesystems such as HDFS, S3, Cassandra,
    among many others.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 可以从多种文件系统中读取：本地文件系统，如 NTFS、FAT 或 Mac OS 扩展（HFS+），或分布式文件系统，如 HDFS、S3、Cassandra
    等。
- en: Tip
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Be wary where your datasets are read from or saved to: The path cannot contain
    special characters `[]`. Note, that this also applies to paths stored on Amazon
    S3 or Microsoft Azure Data Storage.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意你的数据集是从哪里读取或保存的：路径不能包含特殊字符 `[]`。注意，这也适用于存储在 Amazon S3 或 Microsoft Azure Data
    Storage 上的路径。
- en: 'Multiple data formats are supported: Text, parquet, JSON, Hive tables, and
    data from relational databases can be read using a JDBC driver. Note that Spark
    can automatically work with compressed datasets (like the Gzipped one in our preceding
    example).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 支持多种数据格式：文本、parquet、JSON、Hive 表以及来自关系数据库的数据可以使用 JDBC 驱动程序读取。请注意，Spark 可以自动处理压缩数据集（如我们前面的例子中的
    Gzipped 数据集）。
- en: Depending on how the data is read, the object holding it will be represented
    slightly differently. The data read from a file is represented as `MapPartitionsRDD`
    instead of `ParallelCollectionRDD` when we `.paralellize(...)` a collection.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据的读取方式，持有数据的对象将略有不同。当我们 `.paralellize(...)` 一个集合时，从文件读取的数据将表示为 `MapPartitionsRDD`
    而不是 `ParallelCollectionRDD`。
- en: Schema
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式
- en: 'RDDs are *schema-less* data structures (unlike DataFrames, which we will discuss
    in the next chapter). Thus, parallelizing a dataset, such as in the following
    code snippet, is perfectly fine with Spark when using RDDs:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs 是无模式的（与我们在下一章中将要讨论的 DataFrames 不同）。因此，当使用 RDDs 时，Spark 在并行化数据集（如下代码片段所示）时是完全可以接受的：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'So, we can mix almost anything: a `tuple`, a `dict`, or a `list` and Spark
    will not complain.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以混合几乎所有东西：一个 `tuple`、一个 `dict` 或一个 `list`，Spark 不会抱怨。
- en: 'Once you `.collect()` the dataset (that is, run an action to bring it back
    to the driver) you can access the data in the object as you would normally do
    in Python:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你 `.collect()` 收集数据集（即运行一个操作将其返回到驱动程序），你就可以像在 Python 中正常那样访问对象中的数据：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It will produce the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它将产生以下输出：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `.collect()` method returns all the elements of the RDD to the driver where
    it is serialized as a list.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`.collect()` 方法将 RDD 的所有元素返回到驱动程序，其中它被序列化为一个列表。'
- en: Note
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We will talk more about the caveats of using `.collect()` later in this chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面更详细地讨论使用 `.collect()` 的注意事项。
- en: Reading from files
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从文件读取
- en: When you read from a text file, each row from the file forms an element of an
    RDD.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当你从文本文件中读取时，文件中的每一行形成一个 RDD 的元素。
- en: 'The `data_from_file.take(1)` command will produce the following (somewhat unreadable)
    output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`data_from_file.take(1)` 命令将产生以下（有些难以阅读）输出：'
- en: '![Reading from files](img/B05793_02_01.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![从文件读取](img/B05793_02_01.jpg)'
- en: To make it more readable, let's create a list of elements so each line is represented
    as a list of values.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其更易于阅读，让我们创建一个元素列表，这样每行都表示为一个值列表。
- en: Lambda expressions
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lambda 表达式
- en: In this example, we will extract the useful information from the cryptic looking
    record of `data_from_file`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将从 `data_from_file` 的神秘记录中提取有用的信息。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Please refer to our GitHub repository for this book for the details of this
    method. Here, due to space constraints, we will only present an abbreviated version
    of the full method, especially where we create the Regex pattern. The code can
    be found here: [https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/LearningPySpark_Chapter03.ipynb](https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/LearningPySpark_Chapter03.ipynb).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅我们 GitHub 仓库中这本书的详细信息，关于此方法的细节。在这里，由于空间限制，我们只展示完整方法的简略版，特别是我们创建正则表达式模式的部分。代码可以在以下位置找到：[https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/LearningPySpark_Chapter03.ipynb](https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/LearningPySpark_Chapter03.ipynb)。
- en: 'First, let''s define the method with the help of the following code, which
    will parse the unreadable row into something that we can use:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在以下代码的帮助下定义该方法，该代码将解析不可读的行，使其变得可使用：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Tip
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: A word of caution here is necessary. Defining pure Python methods can slow down
    your application as Spark needs to continuously switch back and forth between
    the Python interpreter and JVM. Whenever you can, you should use built-in Spark
    functions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要提醒的是，定义纯 Python 方法可能会减慢你的应用程序，因为 Spark 需要不断地在 Python 解释器和 JVM 之间切换。只要可能，你应该使用内置的
    Spark 函数。
- en: 'Next, we import the necessary modules: The `re` module as we will use regular
    expressions to parse the record, and `NumPy` for ease of selecting multiple elements
    at once.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们导入必要的模块：`re` 模块，因为我们将会使用正则表达式来解析记录，以及 `NumPy` 以便于一次性选择多个元素。
- en: Finally, we create a `Regex` object to extract the information as specified
    and parse the row through it.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建一个 `Regex` 对象来提取所需的信息，并通过它解析行。
- en: Note
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We will not be delving into details here describing Regular Expressions. A good
    compendium on the topic can be found here [https://www.packtpub.com/application-development/mastering-python-regular-expressions](https://www.packtpub.com/application-development/mastering-python-regular-expressions).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入描述正则表达式。关于这个主题的良善汇编可以在以下位置找到：[https://www.packtpub.com/application-development/mastering-python-regular-expressions](https://www.packtpub.com/application-development/mastering-python-regular-expressions)。
- en: Once the record is parsed, we try to convert the list into a `NumPy` array and
    return it; if this fails we return a list of default values `-99` so we know this
    record did not parse properly.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦解析了记录，我们尝试将列表转换为 `NumPy` 数组并返回它；如果失败，我们返回一个包含默认值 `-99` 的列表，这样我们知道这个记录没有正确解析。
- en: Tip
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'We could implicitly filter out the malformed records by using `.flatMap(...)`
    and return an empty list `[]` instead of `-99` values. Check this for details:
    [http://stackoverflow.com/questions/34090624/remove-elements-from-spark-rdd](http://stackoverflow.com/questions/34090624/remove-elements-from-spark-rdd)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 `.flatMap(...)` 隐式过滤掉格式不正确的记录，并返回一个空列表 `[]` 而不是 `-99` 值。有关详细信息，请参阅：[http://stackoverflow.com/questions/34090624/remove-elements-from-spark-rdd](http://stackoverflow.com/questions/34090624/remove-elements-from-spark-rdd)
- en: 'Now, we will use the `extractInformation(...)` method to split and convert
    our dataset. Note that we pass only the method signature to `.map(...)`: the method
    will *hand over* one element of the RDD to the `extractInformation(...)` method
    at a time in each partition:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 `extractInformation(...)` 方法来分割和转换我们的数据集。请注意，我们只传递方法签名到 `.map(...)`：该方法将每次在每个分区中
    `hand over` 一个 RDD 元素到 `extractInformation(...)` 方法：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Running `data_from_file_conv.take(1)` will produce the following result (abbreviated):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `data_from_file_conv.take(1)` 将产生以下结果（简略）：
- en: '![Lambda expressions](img/B05793_02_02.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![Lambda 表达式](img/B05793_02_02.jpg)'
- en: Global versus local scope
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全局与局部作用域
- en: One of the things that you, as a prospective PySpark user, need to get used
    to is the inherent parallelism of Spark. Even if you are proficient in Python,
    executing scripts in PySpark requires shifting your thinking a bit.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作为潜在的 PySpark 用户，你需要习惯 Spark 的固有并行性。即使你精通 Python，在 PySpark 中执行脚本也需要稍微转变一下思维方式。
- en: 'Spark can be run in two modes: Local and cluster. When you run Spark locally
    your code might not differ to what you are currently used to with running Python:
    Changes would most likely be more syntactic than anything else but with an added
    twist that data and code can be copied between separate worker processes.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以以两种模式运行：本地和集群。当您在本地运行Spark时，您的代码可能与您目前习惯的Python运行方式不同：更改可能更多的是语法上的，但有一个额外的变化，即数据和代码可以在不同的工作进程之间复制。
- en: However, taking the same code and deploying it to a cluster might cause a lot
    of head-scratching if you are not careful. This requires understanding how Spark
    executes a job on the cluster.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您不小心，将相同的代码部署到集群中可能会让您感到困惑。这需要理解Spark如何在集群上执行作业。
- en: In the cluster mode, when a job is submitted for execution, the job is sent
    to the driver (or a master) node. The driver node creates a DAG (see [Chapter
    1](ch01.html "Chapter 1. Understanding Spark"), *Understanding Spark*) for a job
    and decides which executor (or worker) nodes will run specific tasks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群模式下，当提交作业以执行时，作业被发送到驱动程序节点（或主节点）。驱动程序节点为作业创建一个DAG（见[第一章](ch01.html "第一章.
    理解Spark")，*理解Spark*），并决定哪些执行器（或工作节点）将运行特定任务。
- en: 'The driver then instructs the workers to execute their tasks and return the
    results to the driver when done. Before that happens, however, the driver prepares
    each task''s closure: A set of variables and methods present on the driver for
    the worker to execute its task on the RDD.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，驱动程序指示工作进程执行其任务，并在完成后将结果返回给驱动程序。然而，在发生之前，驱动程序会为每个任务准备闭包：一组变量和方法，这些变量和方法在驱动程序上存在，以便工作进程可以在RDD上执行其任务。
- en: This set of variables and methods is inherently *static* within the executors'
    context, that is, each executor gets a *copy* of the variables and methods from
    the driver. If, when running the task, the executor alters these variables or
    overwrites the methods, it does so **without** affecting either other executors'
    copies or the variables and methods of the driver. This might lead to some unexpected
    behavior and runtime bugs that can sometimes be really hard to track down.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这组变量和方法在执行器上下文中本质上是*静态的*，也就是说，每个执行器都会从驱动程序获取变量和方法的一个*副本*。如果在运行任务时，执行器更改这些变量或覆盖方法，它将这样做**而不**影响其他执行器的副本或驱动程序的变量和方法。这可能会导致一些意外的行为和运行时错误，有时很难追踪。
- en: Note
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Check out this discussion in PySpark''s documentation for a more hands-on example:
    [http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes](http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 查看PySpark文档中的这个讨论以获取更实际的示例：[http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes](http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes)。
- en: Transformations
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换
- en: Transformations shape your dataset. These include mapping, filtering, joining,
    and transcoding the values in your dataset. In this section, we will showcase
    some of the transformations available on RDDs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 转换塑造了您的数据集。这包括映射、过滤、连接和转码数据集中的值。在本节中，我们将展示RDD上可用的某些转换。
- en: Note
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Due to space constraints we include only the most often used transformations
    and actions here. For a full set of methods available we suggest you check PySpark's
    documentation on RDDs [http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于空间限制，我们在此仅包含最常用的转换和操作。对于完整的方法集，我们建议您查看PySpark关于RDD的文档[http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)。
- en: Since RDDs are schema-less, in this section we assume you know the schema of
    the produced dataset. If you cannot remember the positions of information in the
    parsed dataset we suggest you refer to the definition of the `extractInformation(...)`
    method on GitHub, code for `Chapter 03`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RDD是无模式的，在本节中我们假设您知道生成的数据集的模式。如果您无法记住解析的数据集中信息的位置，我们建议您参考GitHub上`extractInformation(...)`方法的定义，代码在`第三章`。
- en: The .map(...) transformation
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: .map(...)转换
- en: 'It can be argued that you will use the `.map(...)` transformation most often.
    The method is applied to each element of the RDD: In the case of the `data_from_file_conv`
    dataset, you can think of this as a transformation of each row.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，您将最常使用`.map(...)`转换。该方法应用于RDD的每个元素：在`data_from_file_conv`数据集的情况下，您可以将其视为对每行的转换。
- en: 'In this example, we will create a new dataset that will convert year of death
    into a numeric value:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将创建一个新的数据集，将死亡年份转换为数值：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Running `data_2014.take(10)` will yield the following result:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`data_2014.take(10)`将产生以下结果：
- en: '![The .map(...) transformation](img/B05793_02_03.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![The .map(...) transformation](img/B05793_02_03.jpg)'
- en: Note
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If you are not familiar with `lambda` expressions, please refer to this resource:
    [https://pythonconquerstheuniverse.wordpress.com/2011/08/29/lambda_tutorial/](https://pythonconquerstheuniverse.wordpress.com/2011/08/29/lambda_tutorial/).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不太熟悉`lambda`表达式，请参阅此资源：[https://pythonconquerstheuniverse.wordpress.com/2011/08/29/lambda_tutorial/](https://pythonconquerstheuniverse.wordpress.com/2011/08/29/lambda_tutorial/)。
- en: 'You can of course bring more columns over, but you would have to package them
    into a `tuple`, `dict,` or a `list`. Let''s also include the 17th element of the
    row along so that we can confirm our `.map(...)` works as intended:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你当然可以引入更多的列，但你需要将它们打包成一个`tuple`、`dict`或`list`。让我们也包含行中的第17个元素，以便我们可以确认我们的`.map(...)`按预期工作：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code will produce the following result:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将产生以下结果：
- en: '![The .map(...) transformation](img/B05793_02_04.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![The .map(...) transformation](img/B05793_02_04.jpg)'
- en: The .filter(...) transformation
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: The .filter(...) transformation
- en: 'Another most often used transformation is the `.filter(...)` method, which
    allows you to select elements from your dataset that fit specified criteria. As
    an example, from the `data_from_file_conv` dataset, let''s count how many people
    died in an accident in 2014:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个最常使用的转换方法是`.filter(...)`方法，它允许你从数据集中选择符合特定标准的元素。作为一个例子，从`data_from_file_conv`数据集中，让我们计算有多少人在2014年发生了事故死亡：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Tip
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Note that the preceding command might take a while depending on how fast your
    computer is. For us, it took a little over two minutes to return a result.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前面的命令可能需要一段时间，具体取决于你的电脑有多快。对我们来说，它花了点超过两分钟的时间才返回结果。
- en: The .flatMap(...) transformation
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: The .flatMap(...) transformation
- en: 'The `.flatMap(...)` method works similarly to `.map(...)`, but it returns a
    flattened result instead of a list. If we execute the following code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`.flatMap(...)`方法与`.map(...)`类似，但它返回一个扁平化的结果而不是列表。如果我们执行以下代码：'
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It will yield the following output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 它将产生以下输出：
- en: '![The .flatMap(...) transformation](img/B05793_02_05.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![The .flatMap(...) transformation](img/B05793_02_05.jpg)'
- en: You can compare this result with the results of the command that generated `data_2014_2`
    previously. Note, also, as mentioned earlier, that the `.flatMap(...)` method
    can be used to filter out some malformed records when you need to parse your input.
    Under the hood, the `.flatMap(...)` method treats each row as a list and then
    simply *adds* all the records together; by passing an empty list the malformed
    records is dropped.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这个结果与之前生成`data_2014_2`的命令的结果进行比较。注意，正如之前提到的，`.flatMap(...)`方法可以在你需要解析输入时用来过滤掉一些格式不正确的记录。在底层，`.flatMap(...)`方法将每一行视为一个列表，然后简单地*添加*所有记录；通过传递一个空列表，格式不正确的记录将被丢弃。
- en: The .distinct(...) transformation
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: The .distinct(...) transformation
- en: 'This method returns a list of distinct values in a specified column. It is
    extremely useful if you want to get to know your dataset or validate it. Let''s
    check if the `gender` column contains only males and females; that would verify
    that we parsed the dataset properly. Let''s run the following code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法返回指定列中的唯一值列表。如果你想要了解你的数据集或验证它，这个方法非常有用。让我们检查`gender`列是否只包含男性和女性；这将验证我们是否正确解析了数据集。让我们运行以下代码：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This code will produce the following output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将产生以下结果：
- en: '![The .distinct(...) transformation](img/B05793_02_06.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![The .distinct(...) transformation](img/B05793_02_06.jpg)'
- en: First, we extract only the column that contains the gender. Next, we use the
    `.distinct()` method to select only the distinct values in the list. Lastly, we
    use the `.collect()` method to return the print of the values on the screen.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们只提取包含性别的列。接下来，我们使用`.distinct()`方法来选择列表中的唯一值。最后，我们使用`.collect()`方法来在屏幕上打印这些值。
- en: Tip
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Note that this is an expensive method and should be used sparingly and only
    when necessary as it shuffles the data around.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是一个昂贵的操作，应该谨慎使用，并且仅在必要时使用，因为它会在数据周围进行洗牌。
- en: The .sample(...) transformation
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: The .sample(...) transformation
- en: 'The `.sample(...)` method returns a randomized sample from the dataset. The
    first parameter specifies whether the sampling should be with a replacement, the
    second parameter defines the fraction of the data to return, and the third is
    seed to the pseudo-random numbers generator:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`.sample(...)` 方法从数据集中返回一个随机样本。第一个参数指定采样是否带替换，第二个参数定义要返回的数据的分数，第三个是伪随机数生成器的种子：'
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this example, we selected a randomized sample of 10% from the original dataset.
    To confirm this, let''s print the sizes of the datasets:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们从原始数据集中选择了 10% 的随机样本。为了确认这一点，让我们打印数据集的大小：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding command produces the following output:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令产生了以下输出：
- en: '![The .sample(...) transformation](img/B05793_02_07.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![.sample(...) 转换](img/B05793_02_07.jpg)'
- en: We use the `.count()` action that counts all the records in the corresponding
    RDDs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `.count()` 操作来计算相应 RDD 中的所有记录数。
- en: The .leftOuterJoin(...) transformation
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: .leftOuterJoin(...) 转换
- en: '`.leftOuterJoin(...)`, just like in the SQL world, joins two RDDs based on
    the values found in both datasets, and returns records from the left RDD with
    records from the right one appended in places where the two RDDs match:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`.leftOuterJoin(...)`，就像在 SQL 世界中一样，基于两个数据集中找到的值将两个 RDD 连接起来，并返回来自左 RDD 的记录，在两个
    RDD 匹配的地方附加来自右 RDD 的记录：'
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Running `.collect(...)` on the `rdd3` will produce the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `rdd3` 上运行 `.collect(...)` 将产生以下结果：
- en: '![The .leftOuterJoin(...) transformation](img/B05793_02_08.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![.leftOuterJoin(...) 转换](img/B05793_02_08.jpg)'
- en: Tip
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: This is another expensive method and should be used sparingly and only when
    necessary as it shuffles the data around causing a performance hit.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种成本较高的方法，应该谨慎使用，并且仅在必要时使用，因为它会在数据周围进行洗牌，从而影响性能。
- en: 'What you can see here are all the elements from RDD `rdd1` and their corresponding
    values from RDD `rdd2`. As you can see, the value `''a''` shows up two times in
    `rdd3` and `''a''` appears twice in the RDD `rdd2`. The value `b` from the `rdd1`
    shows up only once and is joined with the value `''6''` from the `rdd2`. There
    are two things *missing*: Value `''c''` from `rdd1` does not have a corresponding
    key in the `rdd2` so the value in the returned tuple shows as `None`, and, since
    we were performing a left outer join, the value `''d''` from the `rdd2` disappeared
    as expected.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这里看到的是来自 RDD `rdd1` 的所有元素及其来自 RDD `rdd2` 的对应值。正如你所见，值 `'a'` 在 `rdd3` 中出现了两次，并且
    `'a'` 在 RDD `rdd2` 中也出现了两次。`rdd1` 中的值 `b` 只出现一次，并与来自 `rdd2` 的值 `'6'` 相连接。有两件事*缺失*：`rdd1`
    中的值 `'c'` 在 `rdd2` 中没有对应的键，因此在返回的元组中的值显示为 `None`，并且，由于我们执行的是左外连接，`rdd2` 中的值 `'d'`
    如预期那样消失了。
- en: 'If we used the `.join(...)` method instead we would have got only the values
    for `''a''` and `''b''` as these two values intersect between these two RDDs.
    Run the following code:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 `.join(...)` 方法，我们只会得到 `'a'` 和 `'b'` 的值，因为这两个值在这两个 RDD 之间相交。运行以下代码：
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It will result in the following output:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 它将产生以下输出：
- en: '![The .leftOuterJoin(...) transformation](img/B05793_02_09.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![.leftOuterJoin(...) 转换](img/B05793_02_09.jpg)'
- en: 'Another useful method is `.intersection(...)`, which returns the records that
    are equal in both RDDs. Execute the following code:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的方法是 `.intersection(...)`，它返回在两个 RDD 中相等的记录。执行以下代码：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![The .leftOuterJoin(...) transformation](img/B05793_02_10.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![.leftOuterJoin(...) 转换](img/B05793_02_10.jpg)'
- en: The .repartition(...) transformation
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: .repartition(...) 转换
- en: 'Repartitioning the dataset changes the number of partitions that the dataset
    is divided into. This functionality should be used sparingly and only when really
    necessary as it shuffles the data around, which in effect results in a significant
    hit in terms of performance:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 重新分区数据集会改变数据集被分割成的分区数量。这个功能应该谨慎使用，并且仅在真正必要时使用，因为它会在数据周围进行洗牌，这在实际上会导致性能的显著下降：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The preceding code prints out `4` as the new number of partitions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码打印出 `4` 作为新的分区数量。
- en: The `.glom()` method, in contrast to `.collect()`, produces a list where each
    element is another list of all elements of the dataset present in a specified
    partition; the main list returned has as many elements as the number of partitions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `.collect()` 相比，`.glom()` 方法产生一个列表，其中每个元素是另一个列表，包含在指定分区中存在的数据集的所有元素；返回的主要列表具有与分区数量相同的元素数量。
- en: Actions
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作
- en: Actions, in contrast to transformations, execute the scheduled task on the dataset;
    once you have finished transforming your data you can execute your transformations.
    This might contain no transformations (for example, `.take(n)` will just return
    `n` records from an RDD even if you did not do any transformations to it) or execute
    the whole chain of transformations.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 与转换不同，动作在数据集上执行计划的任务；一旦你完成了数据的转换，你就可以执行转换。这可能不包含任何转换（例如，`.take(n)` 将只从RDD返回
    `n` 条记录，即使你没有对它进行任何转换）或执行整个转换链。
- en: The .take(...) method
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: .take(...) 方法
- en: 'This is most arguably the most useful (and used, such as the `.map(...)` method).
    The method is preferred to `.collect(...)` as it only returns the `n` top rows
    from a single data partition in contrast to `.collect(...)`, which returns the
    whole RDD. This is especially important when you deal with large datasets:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是最有用（并且使用最频繁，例如 `.map(...)` 方法）。该方法比 `.collect(...)` 更受欢迎，因为它只从单个数据分区返回 `n`
    个顶部行，而 `.collect(...)` 则返回整个RDD。这在处理大型数据集时尤为重要：
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you want somewhat randomized records you can use `.takeSample(...)` instead,
    which takes three arguments: First whether the sampling should be with replacement,
    the second specifies the number of records to return, and the third is a seed
    to the pseudo-random numbers generator:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要一些随机的记录，可以使用 `.takeSample(...)` 代替，它接受三个参数：第一个参数指定采样是否带替换，第二个参数指定要返回的记录数，第三个参数是伪随机数生成器的种子：
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The .collect(...) method
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: .collect(...) 方法
- en: This method returns all the elements of the RDD to the driver. As we have just
    provided a caution about it, we will not repeat ourselves here.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将RDD的所有元素返回给驱动器。正如我们刚刚已经对此提出了警告，我们在这里不再重复。
- en: The .reduce(...) method
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: .reduce(...) 方法
- en: The `.reduce(...)` method reduces the elements of an RDD using a specified method.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`.reduce(...)` 方法使用指定的方法对RDD的元素进行归约。'
- en: 'You can use it to sum the elements of your RDD:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用它来对RDD的元素进行求和：
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This will produce the sum of `15`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生`15`的总和。
- en: We first create a list of all the values of the `rdd1` using the `.map(...)`
    transformation, and then use the `.reduce(...)` method to process the results.
    The `reduce(...)` method, on each partition, runs the summation method (here expressed
    as a `lambda`) and returns the sum to the driver node where the final aggregation
    takes place.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用 `.map(...)` 转换创建 `rdd1` 所有值的列表，然后使用 `.reduce(...)` 方法处理结果。`reduce(...)`
    方法在每个分区上运行求和函数（这里表示为 `lambda`），并将求和结果返回给驱动节点，在那里进行最终聚合。
- en: Note
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A word of caution is necessary here. The functions passed as a reducer need
    to be **associative**, that is, when the order of elements is changed the result
    does not, and **commutative**, that is, changing the order of operands does not
    change the result either.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要提醒一句。作为reducer传递的函数需要是**结合律**，也就是说，当元素的顺序改变时，结果不会改变，并且**交换律**，也就是说，改变操作数的顺序也不会改变结果。
- en: The example of the associativity rule is *(5 + 2) + 3 = 5 + (2 + 3)*, and of
    the commutative is *5 + 2 + 3 = 3 + 2 + 5*. Thus, you need to be careful about
    what functions you pass to the reducer.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 结合律的例子是 *(5 + 2) + 3 = 5 + (2 + 3)*，交换律的例子是 *5 + 2 + 3 = 3 + 2 + 5*。因此，你需要小心传递给reducer的函数。
- en: 'If you ignore the preceding rule, you might run into trouble (assuming your
    code runs at all). For example, let''s assume we have the following RDD (with
    one partition only!):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你忽略了前面的规则，你可能会遇到麻烦（假设你的代码能正常运行的话）。例如，假设我们有一个以下RDD（只有一个分区！）：
- en: '[PRE21]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If we were to reduce the data in a manner that we would like to divide the
    current result by the subsequent one, we would expect a value of `10`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以我们想要将当前结果除以下一个结果的方式来减少数据，我们期望得到`10`的值：
- en: '[PRE22]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'However, if you were to partition the data into three partitions, the result
    will be wrong:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你将数据分区成三个分区，结果将会是错误的：
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: It will produce `0.004`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 它将产生 `0.004`。
- en: 'The `.reduceByKey(...)` method works in a similar way to the `.reduce(...)`
    method, but it performs a reduction on a key-by-key basis:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`.reduceByKey(...)` 方法的工作方式与 `.reduce(...)` 方法类似，但它是在键键基础上进行归约：'
- en: '[PRE24]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The preceding code produces the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会产生以下结果：
- en: '![The .reduce(...) method](img/B05793_02_11.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![.reduce(...) 方法](img/B05793_02_11.jpg)'
- en: The .count(...) method
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: .count(...) 方法
- en: 'The `.count(...)` method counts the number of elements in the RDD. Use the
    following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`.count(...)` 方法计算RDD中元素的数量。使用以下代码：'
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This code will produce `6`, the exact number of elements in the `data_reduce`
    RDD.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将产生 `6`，这是 `data_reduce` RDD 中元素的确切数量。
- en: 'The `.count(...)` method produces the same result as the following method,
    but it does not require moving the whole dataset to the driver:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`.count(...)` 方法产生的结果与以下方法相同，但它不需要将整个数据集移动到驱动程序：'
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If your dataset is in a key-value form, you can use the `.countByKey()` method
    to get the counts of distinct keys. Run the following code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据集是键值形式，您可以使用 `.countByKey()` 方法来获取不同键的计数。运行以下代码：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This code will produce the following output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将产生以下输出：
- en: '![The .count(...) method](img/B05793_02_12.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![.count(...) 方法](img/B05793_02_12.jpg)'
- en: The .saveAsTextFile(...) method
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`.saveAsTextFile(...)` 方法'
- en: 'As the name suggests, the `.saveAsTextFile(...)` the RDD and saves it to text
    files: Each partition to a separate file:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，`.saveAsTextFile(...)` 方法将 RDD 保存为文本文件：每个分区保存到一个单独的文件中：
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To read it back, you need to parse it back as all the rows are treated as strings:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取它，你需要将其解析回字符串，因为所有行都被视为字符串：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The list of keys read matches what we had initially:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 读取的键列表与我们最初的有匹配：
- en: '![The .saveAsTextFile(...) method](img/B05793_02_13.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![.saveAsTextFile(...) 方法](img/B05793_02_13.jpg)'
- en: The .foreach(...) method
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`.foreach(...)` 方法'
- en: This is a method that applies the same function to each element of the RDD in
    an iterative way; in contrast to `.map(..)`, the `.foreach(...)` method applies
    a defined function to each record in a one-by-one fashion. It is useful when you
    want to save the data to a database that is not natively supported by PySpark.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个将相同的函数以迭代方式应用于 RDD 中每个元素的方法；与 `.map(..)` 相比，`.foreach(...)` 方法以一对一的方式对每条记录应用定义的函数。当您想将数据保存到
    PySpark 本地不支持的数据库时，它非常有用。
- en: 'Here, we''ll use it to print (to CLI - not the Jupyter Notebook) all the records
    that are stored in `data_key` RDD:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用它来打印（到 CLI - 而不是 Jupyter Notebook）存储在 `data_key` RDD 中的所有记录：
- en: '[PRE30]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If you now navigate to CLI you should see all the records printed out. Note,
    that every time the order will most likely be different.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在导航到 CLI，你应该看到所有记录被打印出来。注意，每次的顺序很可能是不同的。
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: RDDs are the backbone of Spark; these schema-less data structures are the most
    fundamental data structures that we will deal with within Spark.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 是 Spark 的骨架；这些无模式的数据库结构是我们将在 Spark 中处理的最基本的数据结构。
- en: In this chapter, we presented ways to create RDDs from text files, by means
    of the `.parallelize(...)` method as well as by reading data from text files.
    Also, some ways of processing unstructured data were shown.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了通过 `.parallelize(...)` 方法以及从文本文件中读取数据来创建 RDD 的方法。还展示了处理非结构化数据的一些方法。
- en: Transformations in Spark are lazy - they are only applied when an action is
    called. In this chapter, we discussed and presented the most commonly used transformations
    and actions; the PySpark documentation contains many more [http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中的转换是惰性的 - 只有在调用操作时才会应用。在本章中，我们讨论并介绍了最常用的转换和操作；PySpark 文档包含更多内容[http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)。
- en: 'One major distinction between Scala and Python RDDs is speed: Python RDDs can
    be much slower than their Scala counterparts.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 和 Python RDD 之间的一个主要区别是速度：Python RDD 可能比它们的 Scala 对应物慢得多。
- en: In the next chapter we will walk you through a data structure that made PySpark
    applications perform *on par* with those written in Scala - the DataFrames.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将向您介绍一种数据结构，它使 PySpark 应用程序的性能与 Scala 编写的应用程序相当 - 数据帧。
