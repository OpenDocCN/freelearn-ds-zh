- en: Chapter 7.  Extending Spark with SparkR
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章：使用 SparkR 扩展 Spark
- en: Statisticians and data scientists have been using R to solve challenging problems
    in almost every field, ranging from bioinformatics to election campaigns. They
    prefer R due to its powerful visualization capabilities, strong community, and
    rich package ecosystem for statistics and machine learning. Many academic institutions
    around the world teach data science and statistics using the R language.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学家和数据科学家一直在使用 R 来解决几乎所有领域的挑战性问题，从生物信息学到选举活动。他们偏爱 R 是因为它强大的可视化功能、强大的社区以及丰富的统计学和机器学习包生态系统。全球许多学术机构都使用
    R 语言教授数据科学和统计学。
- en: R was originally created by and for statisticians in around the mid-1990s with
    a goal to deliver a better and more user-friendly way to perform data analysis.
    R was initially used in academics and research. As businesses became increasingly
    aware of the role of data science in their business growth, the number of data
    analysts using R in the corporate sector started growing as well. The R language
    user base is considered to be more than two million strong, after being in existence
    for two decades.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: R 最初是在 1990 年代中期，由统计学家为统计学家创建的，目的是提供一种更好、更用户友好的方式来进行数据分析。R 最初用于学术和研究领域。随着企业越来越意识到数据科学在业务增长中的作用，使用
    R 进行数据分析的企业数据分析师数量也开始增长。经过二十年的发展，R 语言的用户基础已被认为超过两百万。
- en: One of the driving factors behind all this success is the fact that R is designed
    to make the life of the analyst easier but not that of the computer. R is inherently
    single-threaded and it can only process datasets that completely fit in a single
    machine's memory. But nowadays, R users are working with increasingly larger datasets.
    Seamless integration of modern-day distributed processing power underneath the
    well-established R language allows data scientists to leverage the best of both
    worlds. They can keep up with their ever-increasing business demands and continue
    to benefit from the flexibility of their favorite R language.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切成功背后的驱动力之一是 R 的设计目的是让分析师的工作更轻松，而不是让计算机的工作更轻松。R 本质上是单线程的，它只能处理完全适合单台机器内存的数据集。然而，现如今，R
    用户正在处理越来越大的数据集。将现代分布式处理能力无缝集成到这个已经建立的 R 语言下，使数据科学家能够兼得二者的优势。这样，他们可以跟上日益增长的业务需求，同时继续享受自己喜爱的
    R 语言的灵活性。
- en: 'This chapter introduces SparkR, an R API to Spark for R programmers so that
    they can harness the power of Spark, without learning a new language. Since prior
    knowledge of R, R Studio, and data analysis skills are already assumed, this chapter
    does not attempt to introduce R. A very brief overview of the Spark compute engine
    is provided as a quick recap. The reader should go through the first three chapters
    of this book to gain a deeper understanding of the Spark programming model and
    DataFrames. This knowledge is extremely important because the developer has to
    understand which part of his code is executing in the local R environment and
    which part is being handled by the Spark compute engine. The topics covered in
    this chapter are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 SparkR，这是一个为 R 程序员设计的 Spark API，使他们能够利用 Spark 的强大功能，而无需学习新的语言。由于假设读者已具备
    R、R Studio 及数据分析技能，本章不再介绍 R 基础知识。提供了一个非常简短的 Spark 计算引擎概述作为快速回顾。读者应阅读本书的前三章，以深入理解
    Spark 编程模型和 DataFrame。这个知识非常重要，因为开发人员需要理解他编写的代码中哪些部分是在本地 R 环境中执行的，哪些部分是由 Spark
    计算引擎处理的。本章涉及的主题如下：
- en: SparkR basics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkR 基础知识
- en: Advantages of R with Spark and its limitations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 与 Spark 的优势及其局限性
- en: Programming with SparkR
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SparkR 编程
- en: SparkR DataFrames
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkR DataFrame
- en: Machine learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习
- en: SparkR basics
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkR 基础知识
- en: R is a language and environment for statistical computing and graphics. SparkR
    is an R package that provides a lightweight frontend to enable Apache Spark access
    from R. The goal of SparkR is to combine the flexibility and ease of use provided
    by the R environment and the scalability and fault tolerance provided by the Spark
    compute engine. Let us recap the Spark architecture before discussing how SparkR
    realizes its goal.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: R 是一种用于统计计算和图形的语言和环境。SparkR 是一个 R 包，它提供了一个轻量级的前端，使得可以从 R 访问 Apache Spark。SparkR
    的目标是将 R 环境提供的灵活性和易用性与 Spark 计算引擎提供的可扩展性和容错性相结合。在讨论 SparkR 如何实现其目标之前，我们先回顾一下 Spark
    的架构。
- en: Apache Spark is a fast, general-purpose, fault-tolerant framework for interactive
    and iterative computations on large, distributed datasets. It supports a wide
    variety of data sources as well as storage layers. It provides unified data access
    to combine different data formats, streaming data and defining complex operations
    using high-level, composable operators. You can develop your applications interactively
    using Scala, Python, or R shell (or Java without a shell). You can deploy it on
    your home desktop or you can run it on large clusters of thousands of nodes crunching
    petabytes of data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个快速的、通用的、容错的框架，用于对大规模分布式数据集进行交互式和迭代计算。它支持多种数据源和存储层。它提供了统一的数据访问，能够结合不同的数据格式、流数据，并使用高级可组合操作符定义复杂的操作。你可以使用
    Scala、Python 或 R shell（或者 Java，无需 shell）交互式地开发应用。你可以在家用桌面上部署它，或者将它部署在大规模集群中，处理数
    PB 的数据。
- en: Note
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: SparkR originated in the AMPLab ([https://amplab.cs.berkeley.edu/](https://amplab.cs.berkeley.edu/))
    to explore different techniques to integrate the usability of R with the scalability
    of Spark. It was released as an alpha component in Apache Spark 1.4, which was
    released in June 2015\. The Spark 1.5 release had improved R usability and introduced
    the MLlib machine learning package with **Generalized Linear Models** (**GLMs**).
    The Spark 1.6 release that happened in January 2016 added some more features,
    such as model summary and feature interactions. The Spark 2.0 release that happened
    in July 2016 brought several important features, such as UDF, improved model coverage,
    DataFrames Window functions API, and so on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 起源于 AMPLab ([https://amplab.cs.berkeley.edu/](https://amplab.cs.berkeley.edu/))，旨在探索将
    R 的可用性与 Spark 的可扩展性相结合的不同技术。它作为 Apache Spark 1.4 的一个 alpha 组件发布，该版本于 2015 年 6
    月发布。Spark 1.5 版本提高了 R 的可用性，并引入了带有**广义线性模型**（**GLMs**）的 MLlib 机器学习包。2016 年 1 月发布的
    Spark 1.6 版本增加了一些新特性，如模型摘要和特征交互。2016 年 7 月发布的 Spark 2.0 版本带来了多个重要特性，如 UDF、改进的模型覆盖、DataFrames
    窗口函数 API 等等。
- en: Accessing SparkR from the R environment
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 R 环境访问 SparkR
- en: 'You can start SparkR from R shell or R Studio. The entry point to SparkR is
    the SparkSession object, which represents the connection to the Spark cluster.
    The node on which R is running becomes the driver. Any objects created by the
    R program reside on this driver. Any objects created via SparkSession are created
    on the worker nodes in the cluster. The following diagram depicts the runtime
    view of R interaction with Spark running on a cluster. Note that R interpreter
    exists on every worker node in the cluster. The following figure does not show
    the cluster manager and it does not show the storage layer either. You could use
    any cluster manager (for example, Yarn or Mesos) and any storage option, such
    as HDFS, Cassandra, or Amazon S3:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 R shell 或 R Studio 启动 SparkR。SparkR 的入口点是 SparkSession 对象，它代表了与 Spark 集群的连接。R
    所运行的节点成为驱动程序。由 R 程序创建的任何对象都驻留在此驱动程序上。通过 SparkSession 创建的任何对象都在集群中的工作节点上创建。下图展示了
    R 与在集群上运行的 Spark 交互的运行时视图。请注意，R 解释器存在于集群中的每个工作节点上。下图没有显示集群管理器，也没有显示存储层。你可以使用任何集群管理器（例如
    Yarn 或 Mesos）和任何存储选项，如 HDFS、Cassandra 或 Amazon S3：
- en: '![Accessing SparkR from the R environment](img/image_07_001.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![从 R 环境访问 SparkR](img/image_07_001.jpg)'
- en: 'Source: http://www.slideshare.net/Hadoop_Summit/w-145p210-avenkataraman.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：http://www.slideshare.net/Hadoop_Summit/w-145p210-avenkataraman.
- en: A SparkSession object is created by passing information such as application
    name, memory, number of cores, and the cluster manager to connect to. Any interaction
    with the Spark engine is initiated via this SparkSession object. A SparkSession
    object is already created for you if you use SparkR shell. You have to explicitly
    create it otherwise. This object replaces SparkContext and SQLContext objects
    that existed in Spark 1.x releases. These objects still exist for backward compatibility.
    Even the preceding figure depicts SparkContext, which you should treat as SparkSession
    post Spark 2.0.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: SparkSession 对象通过传递应用名称、内存、核心数和要连接的集群管理器等信息来创建。所有与 Spark 引擎的交互都通过这个 SparkSession
    对象发起。如果使用 SparkR shell，则会自动为你创建一个 SparkSession 对象；否则，你需要显式创建它。这个对象取代了 Spark 1.x
    版本中的 SparkContext 和 SQLContext 对象。为了向后兼容，这些对象仍然存在。即使前面的图展示的是 SparkContext，你也应当将其视为
    Spark 2.0 后的 SparkSession。
- en: Now that we have understood how to access Spark from the R environment, let
    us examine the core data abstractions provided by the Spark engine.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了如何从 R 环境访问 Spark，接下来让我们来看看 Spark 引擎提供的核心数据抽象。
- en: RDDs and DataFrames
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD 和 DataFrame
- en: At the core of the Spark engine is its main data abstraction, called a **Resilient
    Distributed Dataset** (**RDD**). An RDD is composed of one or more data sources
    and is defined by the user as a series of transformations (aka lineage) on one
    or more stable (concrete) data sources. Every RDD or RDD partition knows how to
    recreate itself on failure using the lineage graph, thereby providing fault tolerance.
    RDD is an immutable data structure, implying that it is sharable between threads
    without synchronization overheads and hence amenable for parallelization. Operations
    on RDDs are either transformations or actions. Transformations are individual
    steps in the lineage. In other words, they are operations that create RDDs because
    every transformation is getting data from a stable data source or transforming
    an immutable RDD and creating another RDD. Transformations are simply declarations;
    they are not evaluated until an *action* operation is applied on that RDD. Actions
    are the operations that utilize the RDDs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 引擎的核心是其主要的数据抽象，称为**弹性分布式数据集（Resilient Distributed Dataset，简称RDD）**。一个
    RDD 由一个或多个数据源组成，并由用户定义为一系列在一个或多个稳定（具体的）数据源上的转换（也叫血统）。每个 RDD 或 RDD 分区都知道如何使用血统图在失败时重新创建自己，从而提供容错功能。RDD
    是不可变的数据结构，这意味着它可以在不需要同步开销的情况下在线程间共享，因此适合并行化。对 RDD 的操作要么是转换，要么是动作。转换是血统中的单独步骤。换句话说，转换是创建
    RDD 的操作，因为每个转换都是从一个稳定的数据源获取数据或转换一个不可变的 RDD，进而创建另一个 RDD。转换只是声明；它们在对 RDD 执行*动作*操作之前不会被评估。动作是利用
    RDD 的操作。
- en: Spark optimizes RDD computation based on the action on hand. For example, if
    the action is to read the first line, only one partition is computed, skipping
    the rest. It automatically performs in-memory computation with graceful degradation
    (spills it to disk when memory is insufficient) and distributes processing across
    all the cores. You may cache an RDD if it is frequently accessed in your program
    logic, thereby avoiding recomputing overhead.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 会根据当前的操作优化 RDD 的计算。例如，如果操作是读取第一行，则只计算一个分区，跳过其余部分。当内存不足时，它会自动执行内存计算，并优雅地退化（当内存不足时，溢出到磁盘），并将处理分布到所有核心。你可以缓存一个
    RDD，如果它在你的程序逻辑中频繁被访问，从而避免重新计算的开销。
- en: The R language provides a two-dimensional data structure called a *DataFrame*
    which makes data manipulation convenient. Apache Spark comes with its own DataFrames
    that are inspired by the DataFrame in R and Python (through Pandas). A Spark DataFrame
    is a specialized data structure that is built on top of the RDD data structure
    abstraction. It provides distributed DataFrame implementation that looks very
    similar to R DataFrame from the developer perspective and at the same time can
    support very large datasets. The Spark dataset API adds structure to DataFrames
    and this structure provides information for more optimization under the hood.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: R 语言提供了一种二维数据结构，叫做*数据框（DataFrame）*，使得数据操作变得方便。Apache Spark 有自己的 DataFrame，这些
    DataFrame 灵感来自于 R 和 Python（通过 Pandas）中的 DataFrame。Spark 的 DataFrame 是建立在 RDD 数据结构抽象之上的一种专用数据结构。它提供了分布式的
    DataFrame 实现，从开发者的角度看，它与 R DataFrame 非常相似，同时还能支持非常大的数据集。Spark 数据集 API 为 DataFrame
    增加了结构，而这一结构为底层优化提供了信息。
- en: Getting started
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用
- en: 'Now that we have understood the underlying data structures and the runtime
    view, it is time to run a few commands. In this section, we assume that you already
    have R and Spark successfully installed and added to the path. We also assume
    that the `SPARK_HOME` environment variable is set. Let us see how to access SparkR
    from R shell or R Studio:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了底层数据结构和运行时视图，是时候运行一些命令了。在这一部分，我们假设你已经成功安装了 R 和 Spark，并将其添加到了路径中。我们还假设已经设置了
    `SPARK_HOME` 环境变量。接下来我们看看如何通过 R shell 或 R Studio 访问 SparkR：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is all you need to do to access the power of Spark DataFrames from within
    the R environment.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你在 R 环境中访问 Spark DataFrame 的所有操作。
- en: Advantages and limitations
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势与限制
- en: 'The R language has long been the lingua franca of data scientists. Its simple-to-understand
    DataFrame abstraction, expressive APIs, and vibrant package ecosystem are exactly
    what the analysts needed. The main challenge was with the scalability. SparkR
    bridges that gap by providing distributed in-memory DataFrames without leaving
    the R eco-system. Such a symbiotic relationship allows users to gain the following
    benefits:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: R 语言长期以来一直是数据科学家的通用语言。其易于理解的数据框架抽象、富有表现力的 API 和充满活力的包生态系统正是分析师所需要的。主要的挑战在于可扩展性。SparkR
    通过提供分布式内存中的数据框架，同时保持在 R 生态系统内，弥补了这一缺陷。这样的共生关系使得用户能够获得以下好处：
- en: There is no need for the analyst to learn a new language
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析师无需学习新语言
- en: The SparkR APIs are similar to R APIs
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkR 的 API 与 R 的 API 类似
- en: You can access SparkR from R studio, along with the autocomplete feature
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过 R Studio 访问 SparkR，并且可以使用自动补全功能。
- en: Performing interactive, exploratory analysis of a very large dataset is no longer
    hindered by memory limitations or long turnaround times
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行对非常大数据集的交互式、探索性分析不再受到内存限制或长时间等待的困扰。
- en: Accessing data from different types of data sources becomes a lot easier. Most
    of the tasks which were imperative before have become declarative. Check [Chapter
    4](http://Chapter%204), *Unified Data Access*, to learn more
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从不同类型的数据源访问数据变得更加容易。大多数之前必须显式操作的任务，现在已经转变为声明式任务。请参阅[第 4 章](http://Chapter%204)，*统一数据访问*，了解更多信息。
- en: You can freely mix dplyr such as Spark functions, SQL, and R libraries that
    are still not available in Spark
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以自由混合使用如 Spark 函数、SQL 和仍未在 Spark 中可用的 R 库等 dplyr。
- en: 'In spite of all the exciting advantages of combining the best of both worlds,
    there are still some limitations with this combination. These limitations may
    not impact every use case, but we need to be aware of them anyway:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结合两者优点带来了许多令人兴奋的优势，但这种结合仍然存在一些局限性。这些局限性可能不会影响每个用例，但我们还是需要意识到它们：
- en: The inherent dynamic nature of R limits the information available for the catalyst
    optimizer. We may not get the full advantage of optimizations such as predicate
    pushback when compared to statically typed languages such as Scala.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 的固有动态特性限制了 Catalyst 优化器能够获取的信息。与静态类型语言（如 Scala）相比，我们可能无法充分利用像谓词下推等优化。
- en: SparkR does not have support for all the machine learning algorithms that are
    already available in other APIs such as the Scala API.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkR 不支持 Scala API 等其他 API 中已经提供的所有机器学习算法。
- en: In summary, using Spark for data preprocessing and using R for analysis and
    visualization seems to be the best approach in the near future.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，使用 Spark 进行数据预处理，使用 R 进行分析和可视化似乎是未来的最佳方案。
- en: Programming with SparkR
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SparkR 编程
- en: 'So far, we have understood the runtime model of SparkR and the basic data abstractions
    that provide the fault tolerance and scalability. We have understood how to access
    the Spark API from R shell or R studio. It''s time to try out some basic and familiar
    operations:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经理解了 SparkR 的运行时模型以及提供容错性和可扩展性的基本数据抽象。我们也了解了如何从 R shell 或 R Studio
    访问 Spark API。现在是时候尝试一些基本且熟悉的操作了：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The operations look very similar to R DataFrame functions because spark DataFrames
    are modeled based on R DataFrames and Python (Pandas) DataFrames. But the similarity
    may create confusion if you are not careful. You may accidentally end up choking
    your local machine by running a compute-intensive function on an R `data.frame`,
    thinking that the load will be distributed. For example, the intersect function
    has the same signature in both packages. You need to pay attention to whether
    the object is of class `SparkDataFrame` (Spark DataFrame) or `data.frame` (R DataFrame).
    You also need to minimize back and forth conversions between local R `data.frame`
    objects and Spark DataFrame objects. Let us get a feel for this distinction by
    trying out some examples:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作看起来与 R DataFrame 函数非常相似，因为 Spark DataFrame 的模型是基于 R DataFrame 和 Python（Pandas）DataFrame
    的。但这种相似性可能会造成混淆，如果不小心，你可能会误以为负载会被分配，从而在 R `data.frame` 上运行计算密集型函数，导致本地计算机崩溃。例如，intersect
    函数在两个包中的签名相同。你需要注意对象是否是 `SparkDataFrame`（Spark DataFrame）类，还是 `data.frame`（R DataFrame）类。你还需要尽量减少本地
    R `data.frame` 对象与 Spark DataFrame 对象之间的反复转换。让我们通过尝试一些例子来感受这种区别：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Function name masking
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 函数名称遮蔽
- en: 'Now that we have tried some basic operations, let us digress a little bit.
    We have to understand what happens when a loaded library has overlapping function
    names with the base package or some other package that was already loaded. This
    is sometimes referred to as function name overlapping, function masking, or name
    conflict. You might have noticed the messages mentioning the objects masked when
    the SparkR package is loaded. This is common for any package loaded into the R
    environment, and is not specific to SparkR alone. If the R environment already
    contains any function that has the same name as a function in the package being
    loaded, then any subsequent calls to that function exhibit the behavior of the
    function in the latest package loaded. If you want to access the previous function
    instead of the `SparkR` function, you need to explicitly prefix that function
    with its package name, as shown:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经尝试了一些基本操作，接下来让我们稍微跑题一下。我们必须了解当加载的库与基础包或已加载的其他包存在函数名重叠时会发生什么。这有时被称为函数名重叠、函数屏蔽或命名冲突。你可能注意到当加载
    SparkR 包时，消息中提到了被屏蔽的对象。这对于任何加载到 R 环境中的包都是常见的，并不仅仅是 SparkR 特有的。如果 R 环境中已经存在一个与正在加载的包中函数同名的函数，那么随后的对该函数的调用将表现出最新加载的包中函数的行为。如果你希望访问以前的函数而不是
    `SparkR` 函数，你需要显式地在该函数名前加上包名，如下所示：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Subsetting data
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子集数据
- en: 'Subsetting operations on R DataFrames are quite flexible and SparkR tries to
    retain these operations with the same or similar equivalents. We have already
    seen some operations in the preceding examples but this section presents them
    in an ordered fashion:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对 R 数据框的子集操作非常灵活，SparkR 尝试保留这些操作，并提供相同或类似的等效功能。我们已经在前面的示例中看到了一些操作，但本节将这些操作按顺序呈现：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: At the time of writing this book (Apache Spark 2.o release), row index based
    slicing is not available. You will not be able to get a specific row or range
    of rows using the `df[n,]` or `df[m:n,]` syntax.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时（Apache Spark 2.o 发布版），基于行索引的切片操作尚不可用。你将无法使用 `df[n,]` 或 `df[m:n,]` 语法获取特定的行或行范围。
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Column functions
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列函数
- en: 'You will have already noticed the column functions `between` in the subsetting
    data section. These functions operate on the `Column` class. As the name suggests,
    these functions operate on a single column at a time and are usually used in subsetting
    DataFrames. There are several other handy column functions for common operations
    such as sorting, casting, and formatting. In addition to working on the values
    within a column, you can append columns to a DataFrame or drop one or more columns
    from a DataFrame. Negative column subscripts may be used to omit columns, similar
    to R. The following examples show the use of `Column` class functions in subset
    operations followed by adding and dropping columns:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经注意到在子集数据部分有列函数 `between`。这些函数作用于 `Column` 类。如其名称所示，这些函数一次处理单个列，通常用于子集化
    DataFrame。还有其他一些常用的列函数，用于排序、类型转换和格式化等常见操作。除了处理列中的值外，你还可以向 DataFrame 添加列或删除一个或多个列。可以使用负的列下标来省略列，类似于
    R。以下示例展示了在子集操作中使用 `Column` 类函数，随后进行添加和删除列的操作：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Grouped data
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分组数据
- en: 'DataFrame data can be subgrouped using the `group_by` function similar to SQL.
    There are multiple ways of performing such operations. We introduce a slightly
    complex example in this section. Moreover, we use `%>%`, aka the forward pipe
    operator, provided by the `magrittr` library, which provides a mechanism for chaining
    commands:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用类似 SQL 的 `group_by` 函数对 DataFrame 数据进行子分组。有多种方式可以执行这样的操作。本节介绍了一个稍微复杂的示例。此外，我们使用了由
    `magrittr` 库提供的 `%>%`，即前向管道操作符，它提供了一个链式命令的机制：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can keep chaining the operations using the forward pipe operator. Look at
    the column renamed part of the code carefully. The column name argument is the
    output of previous operations, which would have completed before commencement
    of this operation and thus you can safely assume that the `avg(sepal_len)` column
    already exists. The `format_number` works as expected, and this is yet another
    handy `Column` operation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续使用前向管道操作符来链式调用操作。仔细观察代码中的列重命名部分。列名参数是先前操作的输出，这些操作在此操作开始之前已经完成，因此你可以放心假设
    `avg(sepal_len)` 列已经存在。`format_number` 按预期工作，这又是一个便捷的 `Column` 操作。
- en: The next section has another similar example with `GroupedData` and its equivalent
    implementation using `dplyr`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节有另一个类似的示例，使用 `GroupedData` 及其等效的 `dplyr` 实现。
- en: SparkR DataFrames
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkR DataFrame
- en: 'In this section, we try out some useful, commonly used operations. First, we
    try out the traditional R/`dplyr` operations and then show equivalent operations
    using the SparkR API:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们尝试了一些有用的、常用的操作。首先，我们尝试了传统的 R/`dplyr` 操作，然后展示了使用 SparkR API 的等效操作：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This operation is very similar to the SQL group and is followed by order. Its
    equivalent implementation in SparkR is also very similar to the `dplyr` example.
    Look at the following example. Pay attention to the method names and compare their
    positioning with respect to the preceding `dplyr` example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作与 SQL 中的 `GROUP BY` 很相似，后面跟着排序。它在 SparkR 中的等效实现也与 `dplyr` 示例非常相似。请看下面的示例。注意方法名称，并将它们的位置与前面的
    `dplyr` 示例进行比较：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: SparkR is intended to be as close to the existing R API as possible. So, the
    method names look very similar to `dplyr` methods. For example, look at the example
    which has `groupBy` whereas `dplyr` has `group_by`. SparkR supports redundant
    function names. For example, it has `group_by` as well as `groupBy` to cater to
    developers coming from different programming environments. The method names in
    `dplyr` and SparkR are again very close to the SQL keyword `GROUP BY`. But the
    sequence of these method calls is not the same. The example also showed an additional
    step of converting a Spark DataFrame to an R `data.frame` using `collect`. The
    methods are arranged inside out, in the sense that first the data is grouped,
    then summarized, and then arranged. This is understandable because in SparkR,
    the DataFrame created in the innermost method becomes the argument for its immediate
    predecessor and so on.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 旨在尽可能接近现有的 R API。因此，方法名称与 `dplyr` 方法非常相似。例如，查看这个示例，其中使用了 `groupBy`，而
    `dplyr` 使用的是 `group_by`。SparkR 支持冗余的函数名称。例如，它同时提供 `group_by` 和 `groupBy`，以适应来自不同编程环境的开发者。`dplyr`
    和 SparkR 中的方法名称再次与 SQL 关键字 `GROUP BY` 非常接近。但这些方法调用的顺序并不相同。示例还展示了一个额外的步骤：使用 `collect`
    将 Spark DataFrame 转换为 R 的 `data.frame`。这些方法的顺序是由内而外的，意味着首先对数据进行分组，然后进行汇总，最后进行排序。这是可以理解的，因为在
    SparkR 中，最内层方法中创建的 DataFrame 成为其直接前驱方法的参数，以此类推。
- en: SQL operations
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SQL 操作
- en: 'If you are not very happy with the syntax in the preceding example, you may
    want to try writing an SQL string as shown, which does exactly the same as the
    preceding but uses the good old SQL syntax:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对前面的示例中的语法不太满意，你可以尝试如下编写 SQL 字符串，它和前面的操作完全相同，但使用了老式的 SQL 语法：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding example looks like the most natural way of implementing the operation
    on hand, if you are used to fetching data from RDBMS tables. But how are we doing
    this? The first statement tells Spark to register a temporary table (or, as the
    name suggests, a view, a logical abstraction of a table). This is not exactly
    the same as a database table. It is temporary in the sense that it is destroyed
    when the SparkSession object is destroyed. You are not explicitly writing data
    into any RDBMS datastore (you have to use `SaveAsTable` for that). But when once
    you register a Spark DataFrame as a temporary table, you are free to use SQL syntax
    to operate on that DataFrame. The next statement is a basic `SELECT` statement
    that displays column names followed by five rows, as dictated by the `LIMIT` keyword.
    The next SQL statement created a Spark DataFrame containing a Species column followed
    by two average columns sorted on the average sepal length. This DataFrame is in
    turn collected as an R `data.frame` by using collect. The final result is exactly
    the same as the preceding example. You are free to use either syntax. For more
    information and examples, check out the SQL section in[Chapter 4](http://chapter%204),
    *Unified Data Access*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例看起来是实现当前操作最自然的方式，特别是如果你习惯从 RDBMS 表中提取数据的话。但我们是怎么做的呢？第一条语句告诉 Spark 注册一个临时表（或者，顾名思义，它是一个视图，是表的逻辑抽象）。这与数据库表并不完全相同。它是临时的，因为它会在
    SparkSession 对象被销毁时被销毁。你并没有显式地将数据写入任何 RDBMS 数据存储（如果要这样做，你必须使用 `SaveAsTable`）。但是，一旦你将
    Spark DataFrame 注册为临时表，你就可以自由使用 SQL 语法对该 DataFrame 进行操作。接下来的语句是一个基本的 `SELECT`
    语句，显示了列名，后面跟着 `LIMIT` 关键字指定的五行数据。接下来的 SQL 语句创建了一个包含 Species 列的 Spark DataFrame，后面跟着两个平均值列，并按照平均花萼长度排序。这个
    DataFrame 又通过 `collect` 被收集为 R 的 `data.frame`。最终结果与前面的示例完全相同。你可以自由选择使用任何一种语法。欲了解更多信息和示例，请查看[第
    4 章](http://chapter%204)，*统一数据访问*。
- en: Set operations
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集合操作
- en: 'The usual set operations, such as `union`, `intersection`, and `minus`, are
    available out of the box in SparkR. In fact, when SparkR is loaded, the warning
    message shows `intersect` as one of the masked functions. The following examples
    are based on `beaver` datasets:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR中提供了常用的集合操作，例如`union`、`intersection`和`minus`，这些操作开箱即用。事实上，当加载SparkR时，警告信息显示`intersect`是被屏蔽的函数之一。以下示例基于`beaver`数据集：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Merging DataFrames
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并DataFrames
- en: 'The next example illustrates the joining of two DataFrames using the `merge`
    command. The first part of the example shows the R implementation and the next
    part shows the SparkR implementation:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例演示了如何使用`merge`命令连接两个DataFrame。示例的第一部分展示了R实现，接下来的部分展示了SparkR实现：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding piece of code completely relies on R''s base package. We have
    used the same names for join columns in both DataFrames for simplicity. The next
    piece of code demonstrates the same example using SparkR. It looks similar to
    the preceding code so look carefully for the differences:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码完全依赖于R的基础包。为了简便起见，我们在两个DataFrame中使用了相同的连接列名称。接下来的代码演示了使用SparkR的相同示例。它看起来与前面的代码相似，因此需要仔细观察其中的差异：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You may want to play with different types of joins, such as left outer join
    and right outer join, or different column names to get a better understanding
    of this function.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想尝试不同类型的连接，如左外连接和右外连接，或使用不同的列名称，以更好地理解这个函数。
- en: Machine learning
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: SparkR provides wrappers on existing MLLib functions. R formulas are implemented
    as MLLib feature transformers. A transformer is an ML pipeline (`spark.ml`) stage
    that takes a DataFrame as input and produces another DataFrame as output, which
    generally contains some appended columns. Feature transformers are a type of transformers
    that convert input columns to feature vectors and these feature vectors are appended
    to the source DataFrame. For example, in linear regression, string input columns
    are one-hot encoded and numeric values are converted to doubles. A label column
    will be appended (if not there in the data frame already) as a replica of the
    response variable.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR提供了对现有MLLib函数的封装。R公式被实现为MLLib特征转换器。转换器是ML管道（`spark.ml`）的一个阶段，它接受DataFrame作为输入，并产生另一个DataFrame作为输出，通常包含一些附加列。特征转换器是一种转换器，将输入列转换为特征向量，这些特征向量会附加到源DataFrame上。例如，在线性回归中，字符串输入列被独热编码，数值被转换为双精度。一个标签列将会被附加（如果在数据框中尚未存在的话），作为响应变量的副本。
- en: In this section, we cover example code for the Naive Bayes and Gaussian GLM
    models. We do not explain the models as such or the summaries they produce. Instead,
    we go straight away to how it can be done using SparkR.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们提供了朴素贝叶斯和高斯GLM模型的示例代码。我们不会详细解释模型本身或它们所产生的总结，而是直接演示如何使用SparkR来实现。
- en: The Naive Bayes model
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型
- en: The NaÃ¯ve Bayes model is an intuitively simple model that works with categorical
    data. We'll be training a sample dataset using the NaÃ¯ve Bayes model. We will
    not explain how the model works but move straight away to training the model using
    SparkR. If you want more information, please refer to [Chapter 6](ch06.xhtml "Chapter 6. 
    Machine Learning"), *Machine Learning*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型是一个直观简单的模型，适用于分类数据。我们将使用朴素贝叶斯模型训练一个样本数据集。我们不会解释模型的工作原理，而是直接使用SparkR来训练模型。如果你想了解更多信息，请参考[第6章](ch06.xhtml
    "第6章 机器学习")，*机器学习*。
- en: This example takes a dataset with the average marks and attendance of twenty
    students. In fact, this dataset has already been introduced in [Chapter 6](http://Chapter%206),
    *Machine Learning*, for training ensembles. However, let us revisit its contents.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例使用一个包含二十名学生的平均成绩和出勤数据的数据集。实际上，这个数据集已经在[第6章](http://Chapter%206)中介绍过，*机器学习*，用于训练集成方法。然而，让我们重新回顾一下它的内容。
- en: 'The students are awarded `Pass` or `Fail` based on a set of well-defined rules.
    Two students with IDs `1009` and `1020` are granted `Pass`, even though they would
    have failed otherwise. Even though we do not provide the actual rules to the model,
    we expect the model to predict these two students'' result as `Fail`. Here are
    the `Pass` / `Fail` criteria:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 学生们根据一组明确定义的规则被授予`Pass`或`Fail`。两个学生，ID为`1009`和`1020`，尽管本应不及格，但被授予了`Pass`。即使我们没有为模型提供实际的规则，我们仍期望模型预测这两名学生的结果为`Fail`。以下是`Pass`
    / `Fail`标准：
- en: Marks < 40 => Fail
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marks < 40 => Fail
- en: Poor attendance => Fail
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出勤不佳 => Fail
- en: Marks above 40 and attendance Full => Pass
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成绩高于40且出勤为全勤 => Pass
- en: 'Marks > 60 and attendance at least Enough => PassThe following is an example
    to train Naive Bayes model:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成绩 > 60 且出勤至少足够 => 通过。以下是训练朴素贝叶斯模型的示例：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The Gaussian GLM model
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯 GLM 模型
- en: 'In this example, we try to predict temperature based on the values of ozone,
    solar radiation, and wind:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们尝试基于臭氧、太阳辐射和风速的值预测温度：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Summary
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: To date, SparkR does not support all algorithms available in Spark, but active
    development is happening to bridge the gap. The Spark 2.0 release has improved
    algorithm coverage, including NaÃ¯ve Bayes, k-means clustering, and survival regression.
    Check out the latest documentation for the supported algorithms. More work is
    underway in bringing out a CRAN release of SparkR, with better integration with
    R packages and Spark packages, and better RFormula support.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，SparkR 尚未支持 Spark 中的所有算法，但正在积极开发中以弥补这一差距。Spark 2.0 版本已经改善了算法覆盖，包括朴素贝叶斯、k-均值聚类和生存回归等。请查看最新文档了解支持的算法。更多工作正在进行，旨在推出
    SparkR 的 CRAN 版本，并与 R 包和 Spark 包更好地集成，同时提供更好的 RFormula 支持。
- en: References
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*SparkR: The Past, Present and Future* by *Shivaram Venkataraman: *[http://shivaram.org/talks/sparkr-summit-2015.pdf](http://shivaram.org/talks/sparkr-summit-2015.pdf)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SparkR：过去、现在与未来*，作者：*Shivaram Venkataraman*：[http://shivaram.org/talks/sparkr-summit-2015.pdf](http://shivaram.org/talks/sparkr-summit-2015.pdf)'
- en: '*Enabling Exploratory Data Science with Spark and R* by *Shivaram Venkataraman*
    and *Hossein Falaki:*[http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r](http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过 Spark 和 R 实现探索性数据科学*，作者：*Shivaram Venkataraman* 和 *Hossein Falaki*：[http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r](http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r)'
- en: '*SparkR: Scaling R Programs with Spark* by *Shivaram Venkataraman* and others:
    [http://shivaram.org/publications/sparkr-sigmod.pdf](http://shivaram.org/publications/sparkr-sigmod.pdf)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SparkR：使用 Spark 扩展 R 程序*，作者：*Shivaram Venkataraman* 等：[http://shivaram.org/publications/sparkr-sigmod.pdf](http://shivaram.org/publications/sparkr-sigmod.pdf)'
- en: '*Recent Developments in SparkR for Advanced Analytics* by *Xiangrui Meng*:
    [http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf](http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SparkR 中的最新进展：面向高级分析*，作者：*Xiangrui Meng*：[http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf](http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf)'
- en: 'To understand RFormula, try out the following links:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要理解 RFormula，请尝试以下链接：
- en: '[https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html)'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html)'
- en: '[http://spark.apache.org/docs/latest/ml-features.html#rformula](http://spark.apache.org/docs/latest/ml-features.html#rformula)'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/ml-features.html#rformula](http://spark.apache.org/docs/latest/ml-features.html#rformula)'
