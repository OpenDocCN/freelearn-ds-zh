- en: Chapter 3.  Introduction to DataFrames
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。DataFrames简介
- en: To solve any real-world big data analytics problem, access to an efficient and
    scalable computing system is definitely mandatory. However, if the computing power
    is not accessible to the target users in a way that's easy and familiar to them,
    it will barely make any sense. Interactive data analysis gets easier with datasets
    that can be represented as named columns, which was not the case with plain RDDs.
    So, the need for a schema-based approach to represent data in a standardized way
    was the inspiration behind DataFrames.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决任何真实的大数据分析问题，绝对需要访问一个高效且可扩展的计算系统。然而，如果计算能力对目标用户不易于访问，那么这几乎没有任何意义。交互式数据分析通过可以表示为命名列的数据集变得更加容易，而这在普通的RDDs中是不可能的。因此，需要一种基于模式的方法来以标准化的方式表示数据，这就是DataFrames背后的灵感来源。
- en: The previous chapter outlined some design aspects of Spark. We learnt how Spark
    enabled distributed data processing on distributed collections of data (RDDs)
    through in-memory computation. It covered most of the points that revealed Spark
    as a fast, efficient, and scalable computing platform. In this chapter, we will
    see how Spark introduced the DataFrame API to make data scientists feel at home
    to carry out their usual data analysis activities with ease.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章概述了Spark的一些设计方面。我们了解到Spark如何通过内存计算在分布式数据集（RDDs）上进行分布式数据处理。它涵盖了大部分内容，揭示了Spark作为一个快速、高效和可扩展的计算平台。在本章中，我们将看到Spark如何引入DataFrame
    API，使数据科学家能够轻松进行他们通常的数据分析活动。
- en: 'This topic is going to serve as a foundation for many upcoming chapters and
    we strongly recommend you to understand the concepts covered in here very well.
    As a prerequisite for this chapter, a basic understanding of SQL and Spark is
    needed. The topics covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主题将作为许多即将到来的章节的基础，并且我们强烈建议您非常了解这里涵盖的概念。作为本章的先决条件，需要对SQL和Spark有基本的了解。本章涵盖的主题如下：
- en: Why DataFrames?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要使用DataFrames？
- en: Spark SQL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Catalyst optimizer
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Catalyst优化器
- en: DataFrame API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame API
- en: DataFrame basics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame基础知识
- en: RDD versus DataFrame
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD与DataFrame
- en: Creating DataFrames
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建DataFrames
- en: From RDDs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从RDDs
- en: From JSON
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从JSON
- en: From JDBC sources
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从JDBC数据源
- en: From other data sources
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从其他数据源
- en: Manipulating DataFrames
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作DataFrames
- en: Why DataFrames?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要使用DataFrames？
- en: Apart from massive, scalable computing capability, big data applications also
    need a mix of a few more features, such as support for a relational system for
    interactive data analysis (simple SQL style), heterogeneous data sources, and
    different storage formats along with different processing techniques.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了大规模、可扩展的计算能力外，大数据应用还需要一些其他特性的混合，例如支持交互式数据分析的关系系统（简单的SQL风格）、异构数据源以及不同的存储格式以及不同的处理技术。
- en: 'Though Spark provided a functional programming API to manipulate distributed
    collections of data, it ended up with tuples (_1, _2, ...). Coding to operate
    on tuples was a little complicated and messy, and was slow at times. So, a standardized
    layer was needed, with the following characteristics:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Spark提供了一个用于操作分布式数据集的函数式编程API，但最终却以元组（_1、_2等）结束。对元组进行操作的编码有时会有些复杂和混乱，有时还会很慢。因此，需要一个标准化的层，具有以下特点：
- en: Named columns with a schema (higher-level abstraction than tuples) so that manipulating
    and tracking them would be easy
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有模式的命名列（比元组更高级的抽象），使得操作和跟踪它们变得容易
- en: Functionality to consolidate data from various data sources such as Hive, Parquet,
    SQL Server, PostgreSQL, JSON, and also Spark's native RDDs, and unify them to
    a common format
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从各种数据源（如Hive、Parquet、SQL Server、PostgreSQL、JSON以及Spark的本地RDDs）整合数据的功能，并将它们统一到一个通用格式中
- en: Ability to take advantage of built-in schemas in special file formats such as
    Avro, CSV, JSON, and so on.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用特殊文件格式（如Avro、CSV、JSON等）中的内置模式的能力
- en: Support for simple relational as well as complex logical operations
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持简单的关系操作以及复杂的逻辑操作
- en: Elimination of the need to define column objects based on domain-specific tasks
    for the ML algorithms to work on, and to serve as a common data layer for all
    algorithms in MLlib
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消除了基于特定领域任务定义列对象的需求，以便ML算法能够正常工作，并为MLlib中的所有算法提供一个通用的数据层
- en: A language-independent entity that can be passed between functions of different
    languages
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可以在不同语言的函数之间传递的与语言无关的实体
- en: To address the above requirements, the DataFrame API was built as one more level
    of abstraction on top of Spark SQL.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足上述要求，DataFrame API被构建为在Spark SQL之上的另一层抽象。
- en: Spark SQL
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Executing SQL queries for basic business needs is very common and almost every
    business does it using some kind of database. So Spark SQL also supports the execution
    of SQL queries written using either a basic SQL syntax or HiveQL. Spark SQL can
    also be used to read data from an existing Hive installation. Apart from these
    plain SQL operations, Spark SQL also addresses some tough problems. Designing
    complex logic through relational queries was cumbersome and almost impossible
    at times. So, Spark SQL was designed to integrate the capabilities of relational
    processing and functional programming so that complex logics can be implemented,
    optimized, and scaled on a distributed computing setup. There are basically three
    ways to interact with Spark SQL, including SQL, the DataFrame API, and the Dataset
    API. The Dataset API is an experimental layer added in Spark 1.6 at the time of
    writing this book so we will limit our discussions to DataFrames only.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 执行基本业务需求的SQL查询非常常见，几乎每个企业都会使用某种数据库进行操作。因此，Spark SQL也支持使用基本SQL语法或HiveQL编写的SQL查询。Spark
    SQL还可以用于从现有的Hive安装中读取数据。除了这些普通的SQL操作，Spark SQL还解决了一些棘手的问题。通过关系查询设计复杂的逻辑有时很麻烦，几乎不可能。因此，Spark
    SQL被设计为整合关系处理和函数式编程的能力，以便在分布式计算环境中实现、优化和扩展复杂的逻辑。与Spark SQL交互的基本上有三种方式，包括SQL、DataFrame
    API和Dataset API。Dataset API是在撰写本书时添加到Spark 1.6中的一个实验性层，因此我们将限制我们的讨论只涉及DataFrames。
- en: Spark SQL exposes DataFrames as a higher-level API and takes care of all the
    complexities involved and also performs all the background tasks. Through the
    declarative syntax, users can focus on what the program should accomplish and
    not bother about the control flow, which will be taken care of by the Catalyst
    optimizer, built inside Spark SQL.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL将DataFrames公开为更高级别的API，并处理所有涉及的复杂性，并执行所有后台任务。通过声明性语法，用户可以专注于程序应该完成的任务，而不必担心由Spark
    SQL内置的Catalyst优化器处理的控制流。
- en: The Catalyst optimizer
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Catalyst优化器
- en: 'The Catalyst optimizer is the fulcrum of Spark SQL and DataFrame. It is built
    with the functional programming constructs of Scala and has the following features:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器是Spark SQL和DataFrame的支点。它是使用Scala的函数式编程构造构建的，并具有以下功能：
- en: 'Schema inference from various data formats:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自各种数据格式的模式推断：
- en: Spark has built-in support for JSON schema inference. Users can just create
    a table out of any JSON file by registering it as a table and simply query it
    with SQL syntaxes.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark内置支持JSON模式推断。用户只需将任何JSON文件注册为表，并使用SQL语法简单查询即可创建表格。
- en: RDDs that are Scala objects; the type information is extracted from Scala's
    type system, that is, **case classes**, if they contain case classes.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDDs是Scala对象；类型信息从Scala的类型系统中提取，即**case classes**，如果它们包含case classes。
- en: RDDs that are Python objects; the type information is extracted with a different
    approach. Since Python is not statically typed and follows a dynamic type system,
    the RDD can contain multiple types. So, Spark SQL samples the dataset and infers
    the schema using an algorithm similar to JSON schema inference.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDDs是Python对象；类型信息是使用不同的方法提取的。由于Python不是静态类型的，并遵循动态类型系统，RDD可以包含多种类型。因此，Spark
    SQL对数据集进行抽样，并使用类似于JSON模式推断的算法推断模式。
- en: In future, built-in support for CSV, XML, and other formats will be provided.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来，将提供对CSV、XML和其他格式的内置支持。
- en: 'Built-in support for a wide range of data sources and query federation for
    efficient data import:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内置支持广泛的数据源和查询联合以实现高效的数据导入：
- en: Spark has a built-in mechanism to fetch data from some external data sources
    (for example, JSON, JDBC, Parquet, MySQL, Hive, PostgreSQL, HDFS, S3, and so on)
    through query federation. It can accurately model the sourced data by using out-of-the-box
    SQL data types and other complex data types such as Struct, Union, Array, and
    so on.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark具有内置机制，可以通过查询联合从一些外部数据源（例如JSON、JDBC、Parquet、MySQL、Hive、PostgreSQL、HDFS、S3等）中获取数据。它可以使用开箱即用的SQL数据类型和其他复杂数据类型（如Struct、Union、Array等）准确地对数据进行建模。
- en: It also allows users to source data using the **Data Source API** from the data
    sources that are not supported out of the box (for example, CSV, Avro HBase, Cassandra,
    and so on).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还允许用户使用**Data Source API**从不受支持的数据源（例如CSV、Avro HBase、Cassandra等）中获取数据。
- en: Spark uses predicate pushdown (pushes filtering or aggregation into external
    storage systems) to optimize data sourcing from external systems and combine them
    to form the data pipeline.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark使用谓词下推（将过滤或聚合推入外部存储系统）来优化从外部系统获取数据并将它们组合成数据管道。
- en: 'Control and optimization of code generation:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制和优化代码生成：
- en: Optimization actually happens very late in the entire execution pipeline.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化实际上发生在整个执行管道的非常晚期。
- en: 'Catalyst is designed to optimize all phases of query execution: analysis, logical
    optimization, physical planning, and code generation to compile parts of queries
    to Java bytecode.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Catalyst旨在优化查询执行的所有阶段：分析、逻辑优化、物理规划和代码生成，以将查询的部分编译为Java字节码。
- en: The DataFrame API
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame API
- en: Excel spreadsheets like data representation, or output from a database projection
    (select statement's output), the data representation closest to human being had
    always been a set of uniform columns with multiple rows. Such a two-dimensional
    data structure that usually has labelled rows and columns is called a DataFrame
    in some realms, such as R DataFrames and Python's Pandas DataFrames. In a DataFrame,
    typically, a single column has the same kind of data, and rows describe data points
    about that column that mean something together, be it data about a person, a purchase,
    or a baseball game outcome. You can think of it as a matrix, or a spreadsheet,
    or an RDBMS table.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 类似Excel电子表格的数据表示，或者来自数据库投影的输出（select语句的输出），最接近人类的数据表示始终是一组具有多行统一列的数据。这种通常具有标记行和列的二维数据结构在某些领域被称为DataFrame，例如R
    DataFrames和Python的Pandas DataFrames。在DataFrame中，通常单个列具有相同类型的数据，并且行描述了关于该列的数据点，这些数据点一起表示某种含义，无论是关于一个人、一次购买还是一场棒球比赛的结果。您可以将其视为矩阵、电子表格或RDBMS表。
- en: DataFrames in R and Pandas are very handy in slicing, reshaping, and analyzing
    data -essential operations in any data wrangling and data analysis workflow. This
    inspired the development of a similar concept on Spark, called DataFrames.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: R和Pandas中的DataFrames非常方便地对数据进行切片、重塑和分析-这是任何数据整理和数据分析工作流程中必不可少的操作。这启发了在Spark上开发类似概念的DataFrames。
- en: DataFrame basics
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame基础知识
- en: The DataFrame API was first introduced in Spark 1.3.0, released in March 2015\.
    It is a programming abstraction of Spark SQL for structured and semi-structured
    data processing. It enables developers to harness the power of the DataFrames,
    data structure through Python, Java, Scala, and R. Like RDDs, a Spark DataFrame
    is a distributed collection of records organized into named columns, similar to
    an RDBMS table or the DataFrames of R or Pandas. Unlike RDDs, however, they keep
    track of schemas and facilitate relational operations as well as procedural operations
    such as `map`. Internally, DataFrames store data in columnar format, but construct
    row objects on the fly when required by the procedural functions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API首次在2015年3月发布的Spark 1.3.0中引入。它是Spark SQL的编程抽象，用于结构化和半结构化数据处理。它使开发人员能够通过Python，Java，Scala和R利用DataFrame数据结构的强大功能。与RDD类似，Spark
    DataFrame是一个分布式记录集合，组织成命名列，类似于RDBMS表或R或Pandas的DataFrame。但是，与RDD不同的是，它们跟踪模式并促进关系操作以及`map`等过程操作。在内部，DataFrame以列格式存储数据，但在需要时通过过程函数构造行对象。
- en: 'The DataFrame API brings two features with it:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API带来了两个特性：
- en: Built-in support for a variety of data formats such as Parquet, Hive, and JSON.
    Nonetheless, through Spark SQL's external data sources API, DataFrames can access
    a wide array of third-party data sources such as databases and NoSQL stores.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内置支持各种数据格式，如Parquet，Hive和JSON。尽管如此，通过Spark SQL的外部数据源API，DataFrame可以访问各种第三方数据源，如数据库和NoSQL存储。
- en: 'A more robust and feature-rich DSL with functions designed for common tasks
    such as:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有为常见任务设计的函数的更健壮和功能丰富的DSL，例如：
- en: Metadata
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据
- en: Sampling
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽样
- en: Relational data processing - project, filter, aggregation, join
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系数据处理 - 项目，过滤，聚合，连接
- en: UDFs
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UDFs
- en: The DataFrame API builds on the Spark SQL query optimizer to automatically execute
    code efficiently on a cluster of machines.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API建立在Spark SQL查询优化器之上，可以在机器集群上自动高效地执行代码。
- en: RDDs versus DataFrames
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD与DataFrame
- en: RDDs and DataFrames are two different types of fault-tolerant and distributed
    data abstractions provided by Spark. They are similar to an extent but greatly
    differ when it comes to implementation. Developers need to have a clear understanding
    of their differences to be able to match their requirements to the right abstraction.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: RDD和DataFrame是Spark提供的两种不同类型的容错和分布式数据抽象。它们在某种程度上相似，但在实现时有很大的不同。开发人员需要清楚地了解它们的差异，以便能够将其需求与正确的抽象匹配。
- en: Similarities
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相似之处
- en: 'The following are the similarities between RDDs and DataFrames:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是RDD和DataFrame之间的相似之处：
- en: Both are fault-tolerant, partitioned data abstractions in Spark
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者都是Spark中的容错，分区数据抽象
- en: Both can handle disparate data sources
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者都可以处理不同的数据源
- en: Both are lazily evaluated (execution happens when an output operation is performed
    on them), thereby having the ability to take the most optimized execution plan
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者都是惰性评估的（在它们上执行输出操作时发生执行），因此具有最优化的执行计划的能力
- en: 'Both APIs are available in all four languages: Scala, Python, Java, and R'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个API在Scala，Python，Java和R中都可用
- en: Differences
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 差异
- en: 'The following are the differences between RDDs and DataFrames:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是RDD和DataFrame之间的区别：
- en: DataFrames are a higher-level abstraction than RDDs.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框架比RDDs更高级的抽象。
- en: The definition of RDD implies defining a **Directed Acyclic Graph** (**DAG**)
    whereas defining a DataFrame leads to the creation of an **Abstract Syntax Tree**
    (**AST**). An AST will be utilized and optimized by the Spark SQL catalyst engine.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD的定义意味着定义一个**有向无环图**（**DAG**），而定义DataFrame会导致创建一个**抽象语法树**（**AST**）。 AST将由Spark
    SQL catalyst引擎利用和优化。
- en: RDD is a general data structure abstraction whereas a DataFrame is a specialized
    data structure to deal with two-dimensional, table-like data.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD是一种通用的数据结构抽象，而DataFrame是一种专门处理二维表格数据的数据结构。
- en: The DataFrame API is actually SchemaRDD-renamed. The renaming was to signify
    that it is no longer inherited from RDD and to comfort data scientists with a
    familiar name and concept.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API实际上是SchemaRDD重命名。重命名是为了表示它不再继承自RDD，并且以熟悉的名称和概念安慰数据科学家。
- en: Creating DataFrames
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据框架
- en: 'Spark DataFrame creation is similar to RDD creation. To get access to the DataFrame
    API, you need SQLContext or HiveContext as an entry point. In this section, we
    are going to demonstrate how to create DataFrames from various data sources, starting
    from basic code examples with in-memory collections:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrame的创建类似于RDD的创建。要访问DataFrame API，您需要SQLContext或HiveContext作为入口点。在本节中，我们将演示如何从各种数据源创建数据框架，从基本的代码示例开始，使用内存集合：
- en: '![Creating DataFrames](img/image_03_001.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![创建数据框架](img/image_03_001.jpg)'
- en: Creating DataFrames from RDDs
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从RDD创建数据框架
- en: 'The following code creates an RDD from a list of colors followed by a collection
    of tuples containing the color name and its length. It creates a DataFrame using
    the `toDF` method to convert the RDD into a DataFrame. The `toDF` method takes
    a list of column labels as an optional argument:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码从颜色列表创建了一个RDD，然后是一个包含颜色名称及其长度的元组集合。它使用`toDF`方法将RDD转换为DataFrame。`toDF`方法将列标签列表作为可选参数：
- en: '**Python**:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Scala**:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see from the preceding example, the creation of a DataFrame is similar
    to that of an RDD from a developer's perspective. We created an RDD here and then
    transformed that to tuples which are then sent to the `toDF` method. Note that
    `toDF` takes a list of tuples instead of scalar elements. You need to pass tuples
    even to create single-column DataFrames. Each tuple is akin to a row. You can
    optionally label the columns; otherwise, Spark creates obscure names such as `_1`,
    `_2`. Type inference of the columns happens implicitly.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的示例中可以看出，从开发人员的角度来看，创建DataFrame与从RDD创建类似。我们在这里创建了一个RDD，然后将其转换为元组，然后将其发送到`toDF`方法。请注意，`toDF`接受元组列表而不是标量元素。即使要创建单列DataFrame，您也需要传递元组。每个元组类似于一行。您可以选择标记列；否则，Spark会创建类似`_1`、`_2`的模糊名称。列的类型推断隐式发生。
- en: 'If you already have the data as RDDs, Spark SQL supports two different methods
    for converting existing RDDs into DataFrames:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经将数据作为RDDs，Spark SQL支持将现有RDDs转换为DataFrame的两种不同方法：
- en: The first method uses reflection to infer the schema of an RDD that contains
    specific types of object, which means you are aware of the schema.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种方法使用反射来推断包含特定类型对象的RDD的模式，这意味着您了解模式。
- en: The second method is through a programmatic interface that lets you construct
    a schema and then apply it to an existing RDD. While this method is more verbose,
    it allows you to construct DataFrames when the column types are not known until
    runtime.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种方法是通过编程接口，允许您构建模式，然后将其应用于现有的RDD。虽然这种方法更冗长，但它允许您在运行时构建DataFrame，当列类型直到运行时才知道时。
- en: Creating DataFrames from JSON
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从JSON创建DataFrame
- en: 'JavaScript Object Notation, or JSON, is a language-independent, self-describing,
    lightweight data-exchange format. JSON has become a popular data exchange format
    and has become ubiquitous. In addition to JavaScript and RESTful interfaces, databases
    such as MySQL have accepted JSON as a data type and MongoDB stores all data as
    JSON documents in binary form. Conversion of data to and from JSON is essential
    for any modern data analysis workflow. The Spark DataFrame API lets developers
    convert JSON objects into DataFrames and vice versa. Let''s have a close look
    at the following examples for a better understanding:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript对象表示法（JSON）是一种与语言无关、自描述、轻量级的数据交换格式。JSON已经成为一种流行的数据交换格式，并且变得无处不在。除了JavaScript和RESTful接口之外，诸如MySQL之类的数据库已经接受JSON作为一种数据类型，而MongoDB以二进制形式将所有数据存储为JSON文档。数据与JSON之间的转换对于任何现代数据分析工作流程都是必不可少的。Spark
    DataFrame API允许开发人员将JSON对象转换为DataFrame，反之亦然。让我们仔细看一下以下示例，以便更好地理解：
- en: '**Python**:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Python：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Scala**:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Scala：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Spark infers schemas automatically from the keys and creates a DataFrame accordingly.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Spark会自动从键中推断模式并相应地创建DataFrame。
- en: Creating DataFrames from databases using JDBC
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用JDBC从数据库创建DataFrame
- en: Spark allows developers to create DataFrames from other databases using JDBC,
    provided you ensure that the JDBC driver for the intended database is accessible.
    A JDBC driver is a software component that allows a Java application to interact
    with a database. Different databases require different drivers. Usually, database
    providers such as MySQL supply these driver components to access their databases.
    You have to ensure that you have the right driver for the database you want to
    work with.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Spark允许开发人员使用JDBC从其他数据库创建DataFrame，只要确保所需数据库的JDBC驱动程序可访问。JDBC驱动程序是一种软件组件，允许Java应用程序与数据库交互。不同的数据库需要不同的驱动程序。通常，诸如MySQL之类的数据库提供商会提供这些驱动程序组件以访问他们的数据库。您必须确保您拥有要使用的数据库的正确驱动程序。
- en: 'The following example assumes that you already have a MySQL database running
    at the given URL, a table called `people` in the database called `test` with some
    data in it, and valid credentials to log in. There is an additional step of relaunching
    the REPL shell with the appropriate JAR file:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例假设您已经在给定的URL上运行了MySQL数据库，在名为`test`的数据库中有一个名为`people`的表，并且有有效的凭据登录。还有一个额外的步骤是使用适当的JAR文件重新启动REPL
    shell：
- en: Note
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you do not already have the JAR file in your system, download it from the
    MySQL site at the following link: [https://dev.mysql.com/downloads/connector/j/](https://dev.mysql.com/downloads/connector/j/).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的系统中尚未有JAR文件，请从MySQL网站下载：[https://dev.mysql.com/downloads/connector/j/](https://dev.mysql.com/downloads/connector/j/)。
- en: '**Python**:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Python：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Scala**:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Scala：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Creating DataFrames from Apache Parquet
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Apache Parquet创建DataFrame
- en: Apache Parquet is an efficient, compressed columnar data representation available
    to any project in the Hadoop ecosystem. Columnar data representations store data
    by column, as opposed to the traditional approach of storing data row by row.
    Use cases that require frequent querying of two to three columns from several
    columns benefit greatly from such an arrangement because columns are stored contiguously
    on the disk and you do not have to read unwanted columns in row-oriented storage.
    Another advantage is in compression. Data in a single column belongs to a single
    type. The values tend to be similar, and sometimes identical. These qualities
    greatly enhance compression and encoding efficiency. Parquet allows compression
    schemes to be specified on a per-column level and allows adding more encodings
    as they are invented and implemented.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet是Hadoop生态系统中任何项目都可以使用的高效的、压缩的列式数据表示。列式数据表示按列存储数据，而不是传统的按行存储数据的方法。需要频繁查询多个列中的两到三列的用例因此受益于这样的安排，因为列在磁盘上是连续存储的，您不必读取不需要的列在面向行的存储中。另一个优势在于压缩。单个列中的数据属于单一类型。这些值往往是相似的，有时是相同的。这些特性极大地增强了压缩和编码的效率。Parquet允许在每列级别指定压缩方案，并允许在发明和实现更多编码时添加更多编码。
- en: 'Apache Spark provides support for both reading and writing Parquet files that
    automatically preserves the schema of the original data. The following example
    writes the people data loaded into a DataFrame in the previous example into Parquet
    format and then re-reads it into an RDD:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark提供了对Parquet文件的读取和写入支持，可以自动保留原始数据的模式。以下示例将在上一个示例中加载到DataFrame中的people数据写入Parquet格式，然后重新读取到RDD中：
- en: '**Python**:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Scala**:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Creating DataFrames from other data sources
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从其他数据源创建数据框架
- en: Spark provides built-in support for multiple data sources such as JSON, JDBC,
    HDFS, Parquet, MYSQL, Amazon S3, and so on. In addition, it provides a Data Source
    API that provides a pluggable mechanism for accessing structured data through
    Spark SQL. There are several libraries built on top of this pluggable component,
    for example, CSV, Avro, Cassandra, and MongoDB, to name a few. These libraries
    are not part of the Spark code base. These are built for individual data sources
    and hosted on a community site, Spark packages.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了对多种数据源的内置支持，例如JSON、JDBC、HDFS、Parquet、MYSQL、Amazon S3等。此外，它还提供了一个数据源API，通过Spark
    SQL提供了一种可插拔的机制来访问结构化数据。基于这个可插拔组件构建了几个库，例如CSV、Avro、Cassandra和MongoDB等。这些库不是Spark代码库的一部分，它们是为个别数据源构建的，并托管在一个名为Spark
    packages的社区网站上。
- en: DataFrame operations
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame操作
- en: 'In the previous section of this chapter, we learnt many different ways of creating
    DataFrames. In this section, we will focus on various operations that can be performed
    on DataFrames. Developers chain multiple operations to filter, transform, aggregate,
    and sort data in the DataFrames. The underlying Catalyst optimizer ensures efficient
    execution of these operations. These functions you find here are similar to those
    you commonly find in SQL operations on tables:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前一部分，我们学习了创建数据框架的许多不同方法。在本节中，我们将重点关注可以在数据框架上执行的各种操作。开发人员可以链接多个操作来过滤、转换、聚合和排序数据框架中的数据。底层的Catalyst优化器确保这些操作的高效执行。这里的函数与通常在表上进行的SQL操作中常见的函数相似：
- en: '**Python**:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**：'
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Scala**:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**：'
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Under the hood
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 底层
- en: You already know by now that the DataFrame API is empowered by Spark SQL and
    that the Spark SQL's Catalyst optimizer plays a crucial role in optimizing the
    performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经知道DataFrame API是由Spark SQL支持的，并且Spark SQL的Catalyst优化器在优化性能方面起着关键作用。
- en: Though the query is executed lazily, it uses the *catalog* component of Catalyst
    to identify whether the column names used in the program or expressions exist
    in the table being used and the data types are proper, and also takes many other
    such precautionary actions. The advantage to this approach is that, instead of
    waiting till program execution, an error pops up as soon as the user types an
    invalid expression.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管查询是惰性执行的，但它使用Catalyst的*catalog*组件来识别程序中使用的列名或表达式是否存在于正在使用的表中，数据类型是否正确，以及采取许多其他预防性措施。这种方法的优势在于，用户一输入无效表达式，就会立即弹出错误，而不是等到程序执行。
- en: Summary
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explained the motivation behind the development of the DataFrame
    API in Spark and how development in Spark has become easier than ever. We briefly
    covered the design aspect of the DataFrame API and how it is built on top of Spark
    SQL. We discussed various ways of creating DataFrames from different data sources
    such as RDDs, JSON, Parquet, and JDBC. At the end of this chapter, we just gave
    you a heads-up on how to perform operations on DataFrames. We will discuss DataFrame
    operations in the context of data science and machine learning in more detail
    in the upcoming chapters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们解释了开发Spark数据框架API背后的动机，以及Spark开发如何变得比以往更容易。我们简要介绍了数据框架API的设计方面，以及它是如何构建在Spark
    SQL之上的。我们讨论了从不同数据源（如RDD、JSON、Parquet和JDBC）创建数据框架的各种方法。在本章末尾，我们简要介绍了如何对数据框架执行操作。在接下来的章节中，我们将更详细地讨论数据科学和机器学习中的DataFrame操作。
- en: In the next chapter, we will learn how Spark supports unified data access and
    discuss on Dataset and Structured Stream  components in details.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习Spark如何支持统一数据访问，并详细讨论数据集和结构化流组件。
- en: References
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'DataFrame reference on the SQL programming guide of Apache Spark official resource:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark官方资源的SQL编程指南上的DataFrame参考：
- en: '[https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes)'
- en: 'Databricks: Introducing DataFrames in Apache Spark for Large Scale Data Science:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks：介绍Apache Spark用于大规模数据科学的数据框架：
- en: '[https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html](https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html](https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html)'
- en: 'Databricks: From Pandas to Apache Spark''s DataFrame:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks：从Pandas到Apache Spark的DataFrame：
- en: '[https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html](https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html](https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html)'
- en: 'API reference guide on Scala for Spark DataFrames:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Scala中Spark数据框架的API参考指南：
- en: '[https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/sql/DataFrame.html](https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/sql/DataFrame.html)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/sql/DataFrame.html](https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/sql/DataFrame.html)'
- en: 'A Cloudera blogpost on Parquet - an efficient general-purpose columnar file
    format for Apache Hadoop:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Cloudera博客关于Parquet - 一种高效的通用列式文件格式，用于Apache Hadoop：
- en: '[http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/](http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/](http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/)'
