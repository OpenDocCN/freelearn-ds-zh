- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Specialized Deep Learning Architectures for Forecasting
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专用深度学习架构用于预测
- en: Our journey through the world of **deep learning** (**DL**) is coming to an
    end. In the previous chapter, we were introduced to the global paradigm of forecasting
    and saw how we can make a simple model such as a **Recurrent Neural Network**
    (**RNN**) perform close to the high benchmark set by global machine learning models.
    In this chapter, we are going to review a few popular DL architectures that were
    designed specifically for time series forecasting. With these more sophisticated
    model architectures, we will be better equipped to handle problems in the wild
    that call for more powerful models than vanilla RNNs and LSTMs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在**深度学习**（**DL**）世界中的旅程即将结束。在上一章中，我们介绍了全球预测范式，并看到如何使一个简单的模型，如**递归神经网络**（**RNN**），表现接近全球机器学习模型设定的高基准。在本章中，我们将回顾一些专门为时间序列预测设计的流行深度学习架构。借助这些更复杂的模型架构，我们将更好地应对现实世界中需要比普通RNN和LSTM更强大模型的问题。
- en: 'In this chapter, we will be covering these main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: The need for specialized architectures
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对专用架构的需求
- en: Introduction to NeuralForecast
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NeuralForecast简介
- en: Neural Basis Expansion Analysis for Interpretable Time Series Forecasting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释的时间序列预测的神经基础扩展分析
- en: Neural Basis Expansion Analysis for Interpretable Time Series Forecasting with
    Exogenous Variables
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带外生变量的可解释的时间序列预测的神经基础扩展分析
- en: Neural Hierarchical Interpolation for Time Series Forecasting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于时间序列预测的神经层次插值
- en: Autoformer
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Autoformer
- en: LTSF-Linear family
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LTSF-Linear家族
- en: Patch Time Series Forecasting
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 补丁时间序列预测
- en: iTransformer
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iTransformer
- en: Temporal Fusion Transformer
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时序融合变换器
- en: TSMixer
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TSMixer
- en: Time Series Dense Encoder
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列密集编码器
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need to set up an Anaconda environment by following the instructions
    in the *Preface* to get a working environment with all the packages and datasets
    required for the code in this book.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要按照*前言*中的说明设置Anaconda环境，以获取包含本书代码所需的所有包和数据集的工作环境。
- en: The code associated with this chapter can be found at [https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python/tree/main/notebooks/Chapter16](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python/tree/main/notebooks/Chapter16).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章相关的代码可以在[https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python/tree/main/notebooks/Chapter16](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python/tree/main/notebooks/Chapter16)找到。
- en: 'You will need to run the following notebooks for this chapter:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要运行以下笔记本文件来进行本章学习：
- en: '`02-Preprocessing_London_Smart_Meter_Dataset.ipynb` in `Chapter02`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02-Preprocessing_London_Smart_Meter_Dataset.ipynb` 在 `Chapter02`'
- en: '`01-Setting_up_Experiment_Harness.ipynb` in `Chapter04`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01-Setting_up_Experiment_Harness.ipynb` 在 `Chapter04`'
- en: '`01-Feature_Engineering.ipynb` in `Chapter06`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01-Feature_Engineering.ipynb` 在 `Chapter06`'
- en: The need for specialized architectures
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对专用架构的需求
- en: Inductive bias, or learning bias, refers to a set of assumptions a learning
    algorithm makes to generalize the function it learns on training data to unseen
    data. Inductive bias is not inherently a bad thing and is different from “bias”
    in the context of bias and variance in learning theory. We use and design inductive
    bias either through model architectures or through feature engineering. For instance,
    a **Convolutional Neural Network** (**CNN**) works better on images than a standard
    **Feed Forward Network** (**FFN**) on pure pixel input because the CNN has the
    locality and spatial bias that FFNs do not have. Although the FFN is theoretically
    a universal approximator, we can learn better models with the inductive bias the
    CNN has.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 偏倚归纳（或学习偏倚）指的是学习算法在将其在训练数据上学到的函数推广到未见数据时所做的一系列假设。偏倚归纳本身并不是一件坏事，它不同于学习理论中的“偏倚”和“方差”中的“偏倚”。我们通过模型架构或特征工程来使用和设计偏倚归纳。例如，**卷积神经网络**（**CNN**）在图像上表现优于标准的**前馈神经网络**（**FFN**）在纯像素输入上的表现，因为CNN具有FFN所没有的局部性和空间偏倚。虽然FFN理论上是一个通用逼近器，但我们可以利用CNN所具有的偏倚归纳来学习更好的模型。
- en: Deep learning is thought to be a completely data-driven approach where the feature
    engineering and final task are learned end to end, thus avoiding the inductive
    bias that the modelers bake in while designing the features. But that view is
    not entirely correct. These inductive biases, which used to be put in through
    the features, now make their way through the design of architecture. Every DL
    architecture has its own inductive bias, which is why some types of models perform
    better on some types of data. For instance, a CNN works well on images, but not
    as much on sequences because the spatial inductive bias and translational equivariance
    that the CNN brings to the table are most effective on images.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习被认为是一种完全数据驱动的方法，特征工程和最终任务都是端到端学习的，从而避免了模型设计者在特征设计时所植入的归纳偏差。但这种看法并不完全正确。过去通过特征引入的归纳偏差，如今通过架构设计的方式融入其中。每种深度学习架构都有自己的归纳偏差，这就是为什么某些类型的模型在某些类型的数据上表现更好。例如，卷积神经网络（CNN）在图像上表现良好，但在序列数据上的表现则不如图像，因为CNN所带来的空间归纳偏差和平移等价性对图像最为有效。
- en: In an ideal world, we would have an infinite supply of good, annotated data
    and we would be able to learn entirely data-driven networks with no strong inductive
    bias. But sadly, in the real world, we will never have enough data to learn such
    complex functions. This is where designing the right kind of inductive bias makes
    or breaks the DL system. We used to heavily rely on RNNs for sequences and they
    had a strong auto-regressive inductive bias baked into them. But later, Transformers,
    which have a much weaker inductive bias for sequences, came in, and with large
    amounts of data, they were able to learn better functions for sequences. Therefore,
    this decision about how strong an inductive bias we bake into models is an important
    question in designing DL architectures.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的世界里，我们会有无限的好数据，并且能够学习完全数据驱动的网络，没有强烈的归纳偏差。但遗憾的是，在现实世界中，我们永远不会有足够的数据来学习如此复杂的函数。这就是设计正确类型的归纳偏差成败的关键。我们曾经在序列建模中大量依赖RNN，它们内置了强烈的自回归归纳偏差。但后来，Transformer的出现改变了这一局面，尽管它对序列的归纳偏差较弱，但在大数据的支持下，它们能够更好地学习序列的函数。因此，如何决定将多强的归纳偏差融入模型，是设计深度学习架构时的重要问题。
- en: Over the years, many DL architectures have been proposed specifically for time
    series forecasting and each of them has its own inductive bias attached to it.
    We’ll not be able to review every single one of those models, but we will cover
    the major ones that made a lasting impact on the field. We will also look at how
    we can use a few open-source libraries to train those models on our data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，许多深度学习架构专门用于时间序列预测，每种架构都有其独特的归纳偏差。我们无法一一回顾所有这些模型，但我们会介绍一些对该领域产生深远影响的主要模型。我们还将探讨如何使用一些开源库在我们的数据上训练这些模型。
- en: We will exclusively focus on models that can handle the global modeling paradigm,
    directly or indirectly. This is because of the infeasibility of training separate
    models for each time series when we are forecasting at scale.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于能够处理全局建模范式的模型，无论是直接还是间接。这是因为在大规模预测时，为每个时间序列训练独立模型是不现实的。
- en: We are going to look at a few popular architectures developed for time series
    forecasting. One of the major factors influencing the inclusion of a model is
    also the availability of stable open-source frameworks that support these models.
    This is in no way a complete list because there are many architectures we are
    not covering here. I’ll try and share a few links in the *Further reading* section
    to get you started on your journey of exploration.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍几种用于时间序列预测的流行架构。影响模型选择的一个主要因素是是否有支持这些模型的稳定开源框架。这里列出的并不是完整的架构清单，因为有许多架构我们没有涵盖。我会在*进一步阅读*部分尝试分享一些链接，帮助你开始探索之旅。
- en: Before we get into the meat of the chapter, let’s understand the library we
    are going to use for it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入本章的核心内容之前，让我们先了解一下我们将要使用的库。
- en: Introduction to NeuralForecast
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NeuralForecast简介
- en: NeuralForecast is yet another library from the wonderful folks at NIXTLA. You
    might recall the name from *Chapter 4*, *Setting a Strong Baseline Forecast*,
    where we used `statsforecast` for classical time series models like ARIMA, ETS,
    and so on. They have a whole suite of open-source libraries for time series forecasting
    (`mlforecast` for machine learning based forecasts, `hierarchicalforecast` for
    reconciling forecasts for hierarchical data, `utilsforecast` with some utilities
    for forecasting, `datasetsforecast` with some ready-to-use datasets, and `TimeGPT`,
    their foundational model for time series).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: NeuralForecast是NIXTLA团队的又一款库。你可能还记得在*第4章*，*设置强基准预测*中，我们使用了`statsforecast`来处理经典的时间序列模型，如ARIMA、ETS等。他们有一整套用于时间序列预测的开源库（如`mlforecast`用于基于机器学习的预测，`hierarchicalforecast`用于层级数据的预测整合，`utilsforecast`包含一些预测工具，`datasetsforecast`提供一些现成的数据集，以及`TimeGPT`，他们的时间序列基础模型）。
- en: Since we have learned how to use `statsforecast`, extending that to `neuralforecast`
    is going to be easy because both libraries maintain similar APIs, structure, and
    ways of working. `neuralforecast` offers both classic and cutting-edge deep learning
    models in an easy-to-use API, which makes it perfect for the practical side of
    the chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经学习了如何使用`statsforecast`，因此将其扩展到`neuralforecast`将变得非常容易，因为这两个库在API、结构和工作方式上非常相似。`neuralforecast`提供了经典和前沿的深度学习模型，并且具有易于使用的API，非常适合本章的实际应用。
- en: NeuralForecast is structured to offer an intuitive and flexible API that integrates
    seamlessly with modern data science workflows. The package includes implementations
    of several prominent models, each catering to different aspects of time series
    forecasting.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: NeuralForecast的结构旨在提供一个直观且灵活的API，能够与现代数据科学工作流无缝集成。该包包括多个著名模型的实现，每个模型都针对时间序列预测的不同方面。
- en: Common parameters and configurations
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见的参数和配置
- en: 'Similar to `statsforecast`, `neuralforecast` also expects the input data to
    be in a particular form:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`statsforecast`，`neuralforecast`也期望输入数据具有特定的格式：
- en: '`ds`: This column should have the time index. It can either be a datetime column
    or an integer column which represents time.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ds`：此列应包含时间索引。它可以是一个日期时间列，也可以是表示时间的整数列。'
- en: '`y`: This column should have the time series we are trying to forecast.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y`：此列应包含我们要预测的时间序列。'
- en: '`unique_id`: This column lets us differentiate different time series with a
    unique ID that we choose. It can be the household ID in our data or any other
    uniquely identifying ID that we give.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unique_id`：此列使我们能够通过选择的唯一ID来区分不同的时间序列。它可以是我们数据中的家庭ID，或者是我们指定的任何其他唯一标识符。'
- en: 'Most models in the `neuralforecast` package share a set of common parameters
    that control aspects like:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralforecast`包中的大多数模型共享一组控制各种方面的通用参数，例如：'
- en: '`stat_exog_list`: This is a list of static continuous columns.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stat_exog_list`：这是一个列出静态连续列的列表。'
- en: '`hist_exog_list`: This is a list of temporal exogenous features for which the
    history is available.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hist_exog_list`：这是一个列出历史可用的外生特征的列表。'
- en: '`futr_exog_list`: This is a list of temporal exogenous features for which the
    future is available.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`futr_exog_list`：这是一个列出未来可用的外生特征的列表。'
- en: '`learning_rate`: This dictates the speed at which a model learns. A higher
    rate might converge faster but can overshoot optimal weights, while a lower rate
    ensures more stable convergence at the cost of speed.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：这是决定模型学习速度的参数。较高的学习率可能更快收敛，但可能会超过最优权重，而较低的学习率则能保证更稳定的收敛，但速度较慢。'
- en: '`batch_size`: This influences the amount of data fed into the model at each
    training step, affecting both memory usage and training dynamics.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`：此参数影响每次训练步骤中输入模型的数据量，进而影响内存使用和训练动态。'
- en: '`max_steps`: This defines the maximum number of epochs – the number of times
    the entire dataset is passed forward and backward through the neural network.
    It is max because we can also add early stopping and, in that case, the number
    of epochs can be lower than this as well.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_steps`：此参数定义了最大训练轮数——即整个数据集通过神经网络正向和反向传递的次数。它是最大值，因为我们还可以添加早停机制，在这种情况下，轮数可能会少于这个值。'
- en: '`loss`: This is the metric used to gauge the difference between predicted values
    and actual values, guiding the optimization process. For a list of loss functions
    included, refer to the NIXTLA documentation: [https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`：这是用于衡量预测值与实际值之间差异的指标，指导优化过程。有关包含的损失函数的列表，请参阅NIXTLA文档：[https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html](https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html)'
- en: '`scaler_type`: This is a string indicating the type of temporal normalization
    used. Temporal normalization does the scaling for each instance of the batch separately
    at the window level. Some examples include `[''minmax,'' ''robust,'' ''standard'']`.
    This is only applicable for window-based models like NBEATS and TimesNet and not
    recurrent models like RNNs. For a full list of scalers, check the NIXTLA temporal
    scalers: [https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html](https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaler_type`：这是一个字符串，表示使用的时间归一化类型。时间归一化会在每个批次的窗口级别对每个实例分别进行缩放。常见的类型包括`[''minmax,''
    ''robust,'' ''standard'']`。此参数仅适用于基于窗口的模型，如NBEATS和TimesNet，而不适用于递归模型如RNN。如果您想查看所有归一化器，请查阅NIXTLA时间归一化器：[https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html](https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html)。'
- en: '`early_stop_patience_steps`: If defined, this sets the number of steps we will
    wait without any improvement in validation scores before stopping training.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stop_patience_steps`：如果定义了该参数，它设置了在验证得分没有改进的情况下，我们将在多少步后停止训练。'
- en: '`random_seed`: This defines the random seed, which is essential for reproducibility.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_seed`：此参数定义了随机种子，它对于结果的可复现性至关重要。'
- en: '**Practitioner’s tip**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**实践者小贴士**：'
- en: 'The parameters `stat_exog_list`, `hist_exog_list`, and `futr_exog_list` are
    only available to models which support them. Do check the documentation of the
    model you are going to use to see if it supports these parameters. Some models
    support all three, some only support `futr_exog_list`, and so on. The entire list
    of models and what is available can be found here: [https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html](https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`stat_exog_list`、`hist_exog_list`和`futr_exog_list`仅对支持这些参数的模型可用。在使用的模型文档中检查，查看该模型是否支持这些参数。有些模型支持所有三个，有些模型只支持`futr_exog_list`，等等。有关支持的模型和可用项的完整列表，请参阅：[https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html](https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html)。
- en: Also, if you have some features for which you only have historical data, they
    go in `hist_exog_list`. If you have some features for which you have both historical
    and future data, they go in both `hist_exog_list` and `futr_exog_list`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某些特征只有历史数据，那么它们应该放在`hist_exog_list`中。如果某些特征既有历史数据又有未来数据，它们应同时放在`hist_exog_list`和`futr_exog_list`中。
- en: 'Apart from these common model parameters, `neuralforecast` also has a core
    class, `NeuralForecast`, which orchestrates the training (just like we have `StatsForecast`
    in `statsforecast`). Similar to `statsforecast`, this is where we define the list
    of models we need to forecast and so on. Let’s look at a few parameters in this
    class as well:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些常见的模型参数外，`neuralforecast`还具有一个核心类`NeuralForecast`，它协调训练过程（就像我们在`statsforecast`中使用`StatsForecast`一样）。与`statsforecast`类似，这里定义了我们需要预测的模型列表等参数。让我们也来看一下这个类中的一些参数：
- en: '`models`: This defines a list of models that we need to fit or predict.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`models`：该参数定义了我们需要拟合或预测的模型列表。'
- en: '`freq`: This sets the frequency of the time series we want to forecast. It
    can either be a string (a valid pandas or polars offset alias) or an integer.
    This is used for generating future dataframes for prediction and should be defined
    according to the data you have.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`freq`：此参数设置我们想要预测的时间序列的频率。它可以是一个字符串（一个有效的pandas或polars偏移别名）或一个整数。此参数用于生成用于预测的未来数据框，并应根据您所拥有的数据进行定义。'
- en: If `ds` is a datetime column, then `freq` can be a string indicating the frequency
    of repetition (like ‘D’ for days, ‘H’ for hours, etc.) or an integer indicating
    a multiplier of the default unit (usually days) to determine the interval between
    each date. And if `ds` is a numerical column, then `freq` should also be a numerical
    column indicating a fixed numerical increment between values.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`ds`是一个日期时间列，那么`freq`可以是一个字符串，表示重复的频率（例如‘D’代表天，‘H’代表小时等），或者是一个整数，表示默认单位（通常为天）的倍数，用于确定每个日期之间的间隔。如果`ds`是数值列，那么`freq`也应是一个数值列，表示值之间的固定数值增量。
- en: '`local_scaler_type`: This is an alternate way to scale the time series. While
    `scaler_type` in Windows-based models scales the time series for each window,
    this scales each time series as a pre-processing step. For each `unique_id`, this
    step scales the time series separately and stores the scalers so that the inverse
    transformations can be applied while predicting.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_scaler_type`：这是另一种缩放时间序列的方式。在基于 Windows 的模型中，`scaler_type` 在每个窗口中缩放时间序列，而此方法在预处理步骤中缩放每个时间序列。对于每个
    `unique_id`，此步骤会单独缩放时间序列并存储缩放器，以便在预测时可以应用逆变换。'
- en: 'A typical workflow involving `neuralforecast` looks as below:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `neuralforecast` 的典型工作流程如下所示：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: “Auto” models
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “自动”模型
- en: 'One of the standout features of the `neuralforecast` package is the inclusion
    of “auto” models. These models automate the process of hyperparameter tuning and
    model selection, simplifying the workflow for users. By utilizing techniques from
    **automated machine learning** (**AutoML**), these models can adapt their architecture
    and settings based on the dataset, significantly reducing the manual effort involved
    in the model configuration. They have intelligent default ranges defined so that,
    even if you don’t declare any ranges to tune, they will take the default ranges
    and tune the models. Additional information can be found here: [https://nixtlaverse.nixtla.io/neuralforecast/models.html](https://nixtlaverse.nixtla.io/neuralforecast/models.html).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralforecast` 包的一个突出特点是包含了“自动”模型。这些模型自动化了超参数调优和模型选择的过程，简化了用户的工作流程。通过利用**自动化机器学习**（**AutoML**）技术，这些模型能够根据数据集自适应其架构和设置，大大减少了模型配置中的手动工作量。它们定义了智能的默认范围，因此即使你没有声明要调优的范围，它们也会采用默认范围并进行调优。更多信息请见：[https://nixtlaverse.nixtla.io/neuralforecast/models.html](https://nixtlaverse.nixtla.io/neuralforecast/models.html)。'
- en: Exogenous features
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部特征
- en: NeuralForecast can also easily incorporate exogenous variables into the forecasting
    process (depending on the capability of the model). Exogenous features, which
    are external influences that can affect the target variable, are crucial for improving
    forecasting accuracy, especially when these external factors significantly impact
    the outcome. Many models within the `neuralforecast` package can integrate such
    features to refine predictions by accounting for additional information that may
    not be present in the time series data itself.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: NeuralForecast 还可以轻松地将外部变量纳入预测过程中（具体取决于模型的能力）。外部特征是指能够影响目标变量的外部因素，对于提高预测准确性至关重要，尤其是当这些外部因素对结果产生显著影响时。`neuralforecast`
    包中的许多模型可以整合这些特征，通过考虑时间序列数据中可能不存在的附加信息来优化预测。
- en: 'For instance, the inclusion of holiday effects, weather conditions, or economic
    indicators as exogenous variables can provide critical insights that pure historical
    data cannot. This feature is especially useful in models like NBEATSx, NHITS,
    and TSMixerx within the package, which can model complex interactions between
    both the historical and future exogenous inputs. By handling exogenous features
    effectively, NeuralForecast enhances the models’ ability to forecast accurately
    in real-world scenarios where external factors play a pivotal role. To check which
    models can handle exogenous information, refer to the documentation on the website:
    [https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html](https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，将假期效应、天气状况或经济指标作为外部变量纳入，可以提供纯历史数据无法提供的重要洞察。这个特性在像 NBEATSx、NHITS 和 TSMixerx
    这样的模型中尤为有用，这些模型能够建模历史和未来外部输入之间的复杂交互。通过有效处理外部特征，NeuralForecast 提高了模型在现实场景中预测准确性的能力，因为外部因素在这些场景中起着关键作用。要查看哪些模型可以处理外部信息，请参考网站上的文档：[https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html](https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html)
- en: Now, without further ado, let’s get started on the first model on the list.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们不再拖延，开始介绍列表中的第一个模型。
- en: Neural Basis Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释性时间序列预测的神经基础扩展分析（N-BEATS）
- en: The first model that used some components from DL (we can’t call it DL because
    it is essentially a mix of DL and classical statistics) and made a splash in the
    field was a model that won the M4 competition (univariate) in 2018\. This was
    a model by Slawek Smyl from Uber (at the time) and was a Frankenstein-style mix
    of exponential smoothing and an RNN, dubbed **ES-RNN** (*Further reading* has
    links to a newer and faster implementation of the model that uses GPU acceleration).
    This led to Makridakis et al. putting forward an argument that “*hybrid approaches
    and combinations of methods are the way forward*.” The creators of the **N-BEATS**
    model aspired to challenge this conclusion by designing a pure DL architecture
    for time series forecasting. They succeeded in this when they created a model
    that beat all other methods in the M4 competition (although they didn’t publish
    it in time to participate in the competition). It is a very unique architecture,
    taking a lot of inspiration from signal processing. Let’s take a deeper look and
    understand the architecture.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个使用深度学习（DL）一些组件的模型（我们不能称其为DL，因为它本质上是DL与经典统计学的混合）并在该领域引起轰动的是在2018年赢得M4竞赛（单变量）的一个模型。这是由Slawek
    Smyl（当时在Uber）设计的模型，它是指数平滑和RNN的“怪物”式混合，名为**ES-RNN**（*进一步阅读*部分提供了使用GPU加速的该模型更新版的链接）。这促使Makridakis等人提出“*混合方法和方法组合是未来的方向*”这一观点。**N-BEATS**模型的创造者希望通过设计一个纯DL架构来挑战这一结论，用于时间序列预测。当他们创建出一个在M4竞赛中击败所有其他方法的模型时（尽管他们没有及时发布以参与竞赛），他们成功地实现了这一目标。这是一个非常独特的架构，深受信号处理的启发。让我们更深入地了解并理解这一架构。
- en: '**Reference check**:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research paper by Makridakis et al. and the blog post by Slawek Smyl are
    cited in the *References* section as *1* and *2*, respectively.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Makridakis等人的研究论文以及Slawek Smyl的博客文章在*参考文献*部分分别被引用为*1*和*2*。
- en: We need to establish a bit of context and terminology before moving ahead with
    the explanation. The core problem that they are solving is univariate forecasting,
    which means it is similar to classical methods such as exponential smoothing and
    ARIMA in the sense that it takes only the history of the time series to generate
    a forecast. There is no provision to include other covariates in the model. The
    model is shown a window from the history and is asked to predict the next few
    timesteps. The window of history is referred to as the **lookback period** and
    the future timesteps are the **forecast period**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续解释之前，我们需要先建立一些背景和术语。它们所解决的核心问题是单变量预测，这意味着它类似于经典方法，如指数平滑和ARIMA，因为它只使用时间序列的历史数据来生成预测。模型中没有包含其他协变量的机制。模型展示一个历史窗口，并要求预测接下来的几个时间步。历史窗口被称为**回溯期**，而未来的时间步则是**预测期**。
- en: The architecture of N-BEATS
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: N-BEATS的架构
- en: 'The N-BEATS architecture was different from the existing architectures (at
    the time) in a few aspects:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATS架构在几个方面不同于当时的现有架构：
- en: Instead of the common encoder-decoder (or sequence-to-sequence) formulation,
    N-BEATS formulates the problem as a multivariate regression problem.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与常见的编码器-解码器（或序列到序列）形式不同，N-BEATS将问题表述为多元回归问题。
- en: Most of the other architectures at the time were relatively shallow (~5 LSTM
    layers). However, N-BEATS used the residual principle to stack many basic blocks
    (we will explain this shortly) and the paper has shown that we can stack up to
    150 layers and still facilitate efficient learning.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当时大多数其他架构相对较浅（大约5层LSTM）。然而，N-BEATS使用残差原则堆叠了许多基础块（我们稍后会解释这一点），而且论文表明我们可以堆叠多达150层，仍然能够实现高效学习。
- en: The model lets us extend it to human-interpretable output, still in a principled
    way.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型让我们将其扩展为可人类解释的输出，仍然是以一种有原则的方式进行的。
- en: 'Let’s look at the architecture and go deeper:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看架构，并深入分析：
- en: '![Figure 16.1 – N-BEATS architecture ](img/B22389_16_01.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图16.1 – N-BEATS架构](img/B22389_16_01.png)'
- en: 'Figure 16.1: N-BEATS architecture'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1：N-BEATS架构
- en: We can see three columns of *rectangular blocks*, each one an exploded view
    of another. Let’s start at the leftmost (which is the most granular view) and
    then go up step by step, building up to the architecture. At the top, there is
    a representative time series, which has a lookback window and a forecast period.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到三个*矩形块*的列，每一个都是另一个的爆炸视图。从最左侧开始（那是最细节的视图），然后逐步向上构建，直到完整的架构。在最顶部，有一个代表性的时间序列，它包含一个回溯窗口和一个预测期。
- en: Blocks
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 块
- en: 'The fundamental learning unit in N-BEATS is a `block`. Each block, *l*, takes
    in an input, (*x*[l]), of the size of the lookback period and generates two outputs:
    a forecast, (![](img/B22389_16_001.png)), and a backcast, (![](img/B22389_16_002.png)).
    The backcast is the block’s own best prediction of the lookback period. It is
    synonymous with fitted values in the classical sense; they tell us how the stack
    would have predicted the lookback window using the function it has learned. The
    block input is first processed by a stack of four standard, fully connected layers
    (complete with a bias term and non-linear activation), transforming the input
    into a hidden representation, *h*[l]. Now, this hidden representation is transformed
    by two separate linear layers (no bias or non-linear activation) to something
    the paper calls expansion coefficients for the backcast and forecast, ![](img/B22389_16_003.png)
    and ![](img/B22389_16_004.png), respectively.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATS中的基本学习单元是`块`。每个块，*l*，接受一个输入，(*x*[l])，其大小为回看期，并生成两个输出：一个预测，(![](img/B22389_16_001.png))，和一个反向预测，(![](img/B22389_16_002.png))。反向预测是块对回看期的最佳预测。它在经典意义上与拟合值同义；它们告诉我们堆栈如何使用它所学到的函数预测回看窗口。块的输入首先通过堆叠的四个标准完全连接层进行处理（包含偏置项和非线性激活），将输入转换为隐藏表示，*h*[l]。现在，这个隐藏表示通过两个独立的线性层（没有偏置或非线性激活）转换为论文中称之为扩展系数的内容，分别是反向预测和预测的系数，![](img/B22389_16_003.png)和![](img/B22389_16_004.png)。
- en: The last part of the block takes these expansion coefficients and maps them
    to the output using a set of basis layers (![](img/B22389_16_005.png) and ![](img/B22389_16_006.png)).
    We will talk about the basis layers in a bit more detail later, but for now, just
    understand that they take the expansion coefficients and transform them into the
    desired outputs (![](img/B22389_16_001.png) and![](img/B22389_16_002.png)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 块的最后部分将这些扩展系数映射到输出，使用一组基础层（![](img/B22389_16_005.png)和![](img/B22389_16_006.png)）。稍后我们会详细讲解基础层，但现在，先理解它们将扩展系数转换为所需的输出（![](img/B22389_16_001.png)和![](img/B22389_16_002.png)）。
- en: Stacks
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆栈
- en: 'Now, let’s move one layer up the abstraction to the middle column of *Figure
    16.1*. It shows how different blocks are arranged in a `stack`, *s*. All the blocks
    in a stack share the same kind of basis layers and therefore are grouped as a
    stack. As we saw earlier, each block has two outputs, ![](img/B22389_16_001.png)
    and ![](img/B22389_16_002.png). The blocks are arranged in a residual manner,
    each block processing and cleaning the time series step by step. The input to
    a block, *l*, is ![](img/B22389_16_011.png) . At each step, the backcast generated
    by the block is subtracted from the input to that block before it’s passed on
    to the next layer. All the forecast outputs of all the blocks in a stack are added
    up to make the *stack forecast*:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们向上一层抽象，转到*图16.1*的中间列。它展示了不同块在一个`堆栈`（*s*）中的排列方式。堆栈中的所有块共享相同类型的基础层，因此它们被归为一个堆栈。如我们之前所见，每个块有两个输出，![](img/B22389_16_001.png)和![](img/B22389_16_002.png)。这些块按残差方式排列，每个块一步步处理和清洗时间序列。块的输入，*l*，是![](img/B22389_16_011.png)。在每个步骤中，块生成的反向预测会从输入中减去，然后再传递到下一层。堆栈中所有块的预测输出会被加起来形成*堆栈预测*：
- en: '![](img/B22389_16_012.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_012.png)'
- en: The residual backcast from the last block in a stack is the *stack residual*
    (*x*^s).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 堆栈中最后一个块的残差反向预测是*堆栈残差*（*x*^s）。
- en: The overall architecture
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 整体架构
- en: 'With that, we can move to the rightmost column of *Figure 16.1*, which shows
    the top-level view of the architecture. We saw that each stack has two outputs—a
    stack forecast (*y*^s) and a stack residual (*x*^s). There can be *N* stacks that
    make up the N-BEATS model. Each stack is chained together so that for any stack
    (*s*), the stack residual out of the previous stack (*x*^(s-1)) is the input and
    the stack generates two outputs: the stack forecast (*y*^s) and the stack residual
    (*x*^s). Finally, the N-BEATS forecast, ![](img/B22389_05_001.png), is the additive
    sum of all the stack forecasts:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以转到*图16.1*的最右侧列，该列展示了架构的顶层视图。我们看到每个堆栈有两个输出——一个是堆栈预测（*y*^s），另一个是堆栈残差（*x*^s）。可以有*N*个堆栈组成N-BEATS模型。每个堆栈是串联在一起的，因此对于任何堆栈（*s*），前一个堆栈的堆栈残差（*x*^(s-1)）是输入，堆栈会生成两个输出：堆栈预测（*y*^s）和堆栈残差（*x*^s）。最后，N-BEATS预测，![](img/B22389_05_001.png)，是所有堆栈预测的加和：
- en: '![](img/B22389_16_014.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_014.png)'
- en: Now that we have understood what the model is doing, we need to come back to
    one point that we left for later—**basis functions**.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了模型在做什么，我们需要回到一个之前留到后面讲的点——**基函数**。
- en: '**Disclaimer**:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明**：'
- en: The explanation here is to mostly aid intuition, so we might be hand-waving
    over a few mathematical concepts. For a more rigorous treatment of the subject,
    you should refer to mathematical books/articles that cover the topic. For example,
    *Functions as Vector Spaces* from the *Further reading* section and *Function
    Spaces* ([https://cns.gatech.edu/~predrag/courses/PHYS-6124-12/StGoChap2.pdf](https://cns.gatech.edu/~predrag/courses/PHYS-6124-12/StGoChap2.pdf)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的解释主要是为了帮助理解，因此我们可能略过了一些数学概念。如果想深入理解这一主题，建议参考涉及该主题的数学书籍或文章。例如，《函数作为向量空间》在*进一步阅读*部分和*函数空间*（[https://cns.gatech.edu/~predrag/courses/PHYS-6124-12/StGoChap2.pdf](https://cns.gatech.edu/~predrag/courses/PHYS-6124-12/StGoChap2.pdf)）。
- en: Basis functions and interpretability
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基函数与可解释性
- en: To understand what basis functions are, we need to understand a concept from
    linear algebra. We talked about vector spaces in *Chapter 11*, *Introduction to
    Deep Learning*, and gave you a geometric interpretation of vectors and vector
    spaces. We talked about how a vector is a point in the *n*-dimensional vector
    space. We had that discussion regarding regular Euclidean space (*R*^n), which
    is intended to represent physical space. Euclidean spaces are defined with an
    origin and an orthonormal basis. An orthonormal basis is a unit vector (magnitude=1)
    and they are orthogonal (in simple intuition, at 90 degrees) to each other. Therefore,
    a vector, ![](img/B22389_16_015.png), can be written as ![](img/B22389_16_016.png),
    where ![](img/B22389_16_017.png) and ![](img/B22389_16_018.png) are the orthonormal
    basis. You may remember this from high school.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解什么是基函数，我们需要了解线性代数中的一个概念。我们在*第11章*《深度学习导论》中讨论过向量空间，并给出了向量和向量空间的几何解释。我们谈到了向量是*
    n *维向量空间中的一个点。我们曾讨论过常规的欧几里得空间（*R*^n），它用于表示物理空间。欧几里得空间是通过一个原点和一个正交归一基来定义的。正交归一基是单位向量（大小=1），且它们彼此正交（直观理解为90度）。因此，一个向量，![](img/B22389_16_015.png)，可以写成！[](img/B22389_16_016.png)，其中![](img/B22389_16_017.png)和![](img/B22389_16_018.png)是正交归一基。你可能还记得这部分内容是高中学过的。
- en: Now, there is a branch of mathematics that views a function as a point in a
    vector space (at which point, we call it a functional space). This comes from
    the fact that all the mathematical conditions that need to be satisfied for a
    vector space (things such as additivity, associativity, and so on) are valid if
    we consider functions instead of points. To better drive that intuition, let’s
    consider a function, *f*(*x*) = 2*x* + 4*x*². We can consider this function as
    a vector in the function space with basis *x* and *x*². Now, the coefficients,
    2 and 4, can be changed to give us different functions; this can be any real number
    from -![](img/B22389_11_014.png) to +![](img/B22389_11_014.png). This space of
    all functions that can have a basis of *x* and *x*² is the functional space, and
    every function in the function space can be defined as a linear combination of
    the basis functions. We can have the basis of any arbitrary function, which gives
    us a lot of flexibility. From a machine learning perspective, searching for the
    best function in this functional space automatically means that we are restricting
    the function search so that we have some properties defined by the basis functions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有一个数学分支将函数视为向量空间中的一个点（此时我们称其为泛函空间）。这一观点源自于这样一个事实：向量空间需要满足的所有数学条件（如加法性、结合性等）在我们考虑函数而非点时依然有效。为了更好地理解这一点，让我们考虑一个函数，*f*(*x*)
    = 2*x* + 4*x*²。我们可以将这个函数视为在具有基函数*x*和*x*²的函数空间中的一个向量。现在，系数2和4可以变化，给我们不同的函数；它们可以是从-![](img/B22389_11_014.png)到+![](img/B22389_11_014.png)的任何实数。所有能够以*x*和*x*²为基的函数的集合就是泛函空间，泛函空间中的每个函数都可以表示为基函数的线性组合。我们可以选择任意函数作为基函数，这给我们提供了极大的灵活性。从机器学习的角度来看，在这个泛函空间中寻找最佳函数，实际上意味着我们在限制函数的搜索范围，使其具备基函数所定义的某些性质。
- en: Coming back to N-BEATS, we talked about the expansion coefficients, ![](img/B22389_16_021.png)and
    ![](img/B22389_16_022.png), which are mapped to the output using a set of basis
    layers (![](img/B22389_16_005.png) and ![](img/B22389_16_006.png)). A basis layer
    can also be thought of as a basis function because we know that a layer is nothing
    but a function that maps its inputs to its outputs. Therefore, by learning the
    expansion coefficients, we are essentially searching for the best function that
    can represent the output but is constrained by the basis functions we choose.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 N-BEATS，我们讨论了扩展系数，![](img/B22389_16_021.png) 和 ![](img/B22389_16_022.png)，这些系数通过一组基础层（![](img/B22389_16_005.png)
    和 ![](img/B22389_16_006.png)）映射到输出。一个基础层也可以被看作是一个基础函数，因为我们知道一个层仅仅是一个函数，它将输入映射到输出。因此，通过学习扩展系数，我们实际上是在寻找一个最佳的函数，这个函数能够表示输出，但受到我们选择的基础函数的限制。
- en: 'There are two modes in which N-BEATS operates: *generic* and *interpretable*.
    The N-BEATS paper shows that under both modes, N-BEATS managed to beat the best
    in the M4 competition. Generic mode is where we do not have any basis function
    constraining the function search. We can also think of this as setting the basis
    function to be the identity function. So, in this mode, we are leaving the function
    completely learned by the model through a linear projection of the basis coefficients.
    This mode lacks human interpretability because we don’t have any idea how the
    different functions are learned and what each stack signifies.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATS 有两种操作模式：*通用*模式和*可解释*模式。N-BEATS 论文表明，在这两种模式下，N-BEATS 都能够在 M4 比赛中超越最优秀的模型。通用模式是指我们没有任何基础函数来限制函数搜索。我们也可以将此看作是将基础函数设置为恒等函数。因此，在这种模式下，我们让模型通过线性投影的方式，完全学习函数。这种模式缺乏人类可解释性，因为我们无法了解不同的函数是如何学习的，也无法理解每一层的意义。
- en: But if we have fixed basis functions that constrain the function space, we can
    bring in more interpretability. For instance, if we have a basis function that
    constrains the output to represent the trends for all the blocks in a stack, we
    can say that the forecast output of that stack represents the trend component.
    Similarly, if we have another basis function that constrains the output to represent
    the seasonality for all the blocks in a stack, we can say that the forecast output
    of the stack represents seasonality.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们有固定的基础函数来限制函数空间，我们就可以引入更多的可解释性。例如，如果我们有一个基础函数来限制输出表示堆栈中所有模块的趋势，那么我们可以说该堆栈的预测输出代表趋势成分。同样，如果我们有另一个基础函数来限制输出表示堆栈中所有模块的季节性，那么我们可以说该堆栈的预测输出代表季节性。
- en: This is exactly what the paper has proposed as well. They have defined specific
    basis functions that capture trend and seasonality, and including such blocks
    makes the final forecast more interpretable by giving us a decomposition. The
    trend basis function is a polynomial of a small degree, *p*. So, as long as *p*
    is low, such as 1, 2, or 3, it forces the forecast output to mimic the trend component.
    For the seasonality basis function, the authors chose a Fourier basis (similar
    to the one we saw in *Chapter 6*, *Feature Engineering for Time Series Forecasting*).
    This forces the forecast output to be functions of these sinusoidal basis functions
    that mimic seasonality. In other words, the model learns to combine these sinusoidal
    waves with different coefficients to reconstruct the seasonality pattern as best
    as possible.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是论文中提出的内容。他们定义了特定的基础函数来捕捉趋势和季节性，包含这些模块可以通过给出分解来使最终的预测更具可解释性。趋势基础函数是一个低阶多项式，*p*。因此，只要
    *p* 较低，比如 1、2 或 3，就会迫使预测输出模仿趋势成分。对于季节性基础函数，作者选择了傅里叶基础（类似于我们在*第六章*，*时间序列预测的特征工程*中看到的）。这迫使预测输出成为这些正弦基础函数的组合，模仿季节性。换句话说，模型学习将这些正弦波与不同的系数组合，以尽可能好地重构季节性模式。
- en: For a deeper understanding of these basis functions and how they are structured,
    I have linked to a *Kaggle notebook* in the *Further reading* section that provides
    a clear explanation of the trend and seasonality basis functions. The associated
    notebook also has an additional section that visualizes the first few basis functions
    of seasonality. Along with the original paper, these additional readings will
    help you solidify your understanding.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入理解这些基础函数及其结构，我在*进一步阅读*部分链接了一篇*Kaggle笔记本*，其中提供了关于趋势和季节性基础函数的清晰解释。相关笔记本还包含一个额外的部分，展示了季节性基础函数的前几个可视化图像。结合原始论文，这些附加阅读材料将有助于加深你的理解。
- en: N-BEATS wasn’t designed to be a global model, but it does well in the global
    setting. The M4 competition was a collection of unrelated time series and the
    N-BEATS model was trained so that the model was exposed to all those series and
    learned a common function to forecast each time series in the dataset. This, along
    with ensembling multiple N-BEATS models with different lookback windows, was the
    success formula for the M4 competition.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATS并非专为全球模型设计，但在全球环境中表现良好。M4竞赛是一个包含无关时间序列的集合，N-BEATS模型的训练方式使得模型能够接触到所有这些序列，并学习一种共同的函数来预测数据集中的每个时间序列。这种方法，再加上使用不同回溯窗口的多个N-BEATS模型的集成，构成了M4竞赛的成功公式。
- en: '**Reference check**:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research paper by Boris Oreshkin et al. (N-BEATS) is cited in the *References*
    section as *3*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Boris Oreshkin等人的研究论文（N-BEATS）在*参考文献*部分被标注为*3*。
- en: Forecasting with N-BEATS
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用N-BEATS进行预测
- en: N-BEATS, along with many other specialized architectures we will explore in
    this chapter, are implemented in NIXTLA’s NeuralForecast packages. First, let’s
    look at the initialization parameters of the implementation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATS以及我们将在本章中探讨的许多其他专用架构，都已在NIXTLA的NeuralForecast包中实现。首先，我们来看一下该实现的初始化参数。
- en: 'The `NBEATS` class in NeuralForecast has lots of parameters, but here are the
    most important ones:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: NeuralForecast中的`NBEATS`类有很多参数，以下是最重要的一些：
- en: '`stack_types`: This defines the number of stacks that we need to have in the
    N-BEATS model. This should be a list of strings (*generic*, *trend*, or *seasonality*)
    denoting the number and type of stacks. Examples include `["trend", "seasonality"]`,
    `["trend", "seasonality", "generic"]`, and `["generic", "generic", "generic"]`.
    However, if the entire network is generic, we can just have a single generic stack
    with more blocks as well.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stack_types`：这定义了我们在N-BEATS模型中需要的堆叠数量。它应该是一个包含字符串的列表（*generic*、*trend*或*seasonality*），表示堆叠的数量和类型。示例包括`["trend",
    "seasonality"]`、`["trend", "seasonality", "generic"]`和`["generic", "generic",
    "generic"]`。不过，如果整个网络是通用的，我们也可以只使用一个通用堆叠，并添加更多的块。'
- en: '`n_blocks`: This is a list of integers signifying the number of blocks in each
    stack that we have defined. If we had defined `stack_types` as `["trend", "seasonality"]`,
    and we want three blocks each, we can set `n_blocks` to `[3,3]`.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_blocks`：这是一个整数列表，表示我们已定义的每个堆叠中块的数量。如果我们已将`stack_types`定义为`["trend", "seasonality"]`，并希望每个堆叠有三个块，那么我们可以将`n_blocks`设置为`[3,3]`。'
- en: '`input_size`: This is an integer which contains the autoregressive units (lags)
    to be tested.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_size`：这是一个整数，表示要测试的自回归单位（滞后）。'
- en: '`shared_weights`: This is a list of Booleans signifying whether the weights
    generating the expansion coefficients are shared with other blocks in a stack.
    It is recommended to share the weights in the interpretable stacks and not share
    them in the identity stacks.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shared_weights`：这是一个布尔值列表，表示生成扩展系数的权重是否与堆叠中的其他块共享。建议在可解释的堆叠中共享权重，而在身份堆叠中则不共享。'
- en: There are several other parameters, but these are not as important. A full list
    of parameters and their descriptions can be found at [https://nixtlaverse.nixtla.io/neuralforecast/models.nbeats.html](https://nixtlaverse.nixtla.io/neuralforecast/models.nbeats.html).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他参数，但这些参数不如上述重要。参数及其描述的完整列表可以在[https://nixtlaverse.nixtla.io/neuralforecast/models.nbeats.html](https://nixtlaverse.nixtla.io/neuralforecast/models.nbeats.html)找到。
- en: Since the strength of the model is in forecasting slightly longer durations,
    we can do a single-shot 48-step horizon simply by setting the forecast horizon
    parameter `h = 48`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该模型的优势在于预测较长时间的跨度，我们只需设置预测时长参数`h = 48`，即可进行一次性的48步预测。
- en: '**Notebook alert**:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提示**：'
- en: The complete code for training N-BEATS can be found in the `01-NBEATS_NeuralForecast.ipynb`
    notebook in the `Chapter16` folder.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练N-BEATS的完整代码可以在`Chapter16`文件夹中的`01-NBEATS_NeuralForecast.ipynb`笔记本中找到。
- en: Interpreting N-BEATS forecasting
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释N-BEATS预测结果
- en: 'N-BEATS, if we are running it in the interpretable model, also gives us more
    interpretability by separating the forecast into trend and seasonality. To get
    the interpretable output, we can call the `decompose` function. We must ensure
    that, in our initial parameters, we include the stack type for the trend and seasonal
    components: `stack_types = [''trend'',''seasonality'']`.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATS，如果我们在可解释模型中运行它，也通过将预测分解为趋势和季节性，提供了更多的可解释性。要获得可解释的输出，我们可以调用`decompose`函数。我们必须确保在初始参数中包括趋势和季节性组件的堆叠类型：`stack_types
    = ['trend','seasonality']`。
- en: '[PRE1]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will return us an `array` from which trend and seasonality can be accessed,
    like `y_hat =[0,1]`. The order of trend or seasonality depends on how you include
    it in stack_types, though the default is `['seasonality','trend']`, meaning seasonality
    is `y_hat =[0,1]` and trend is `y_hat =[0,1]`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个`array`，从中可以访问趋势和季节性，比如`y_hat =[0,1]`。趋势或季节性的顺序取决于你在`stack_types`中如何包含它，但默认顺序是`['seasonality','trend']`，意味着季节性是`y_hat
    =[0,1]`，趋势是`y_hat =[0,1]`。
- en: 'Let’s see how one of the household predictions decomposed:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看其中一个家庭预测是如何分解的：
- en: '![Figure 16.2 – Decomposed predictions from N-BEATS (interpretable) ](img/B22389_16_02.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图16.2 – N-BEATS的分解预测（可解释）](img/B22389_16_02.png)'
- en: 'Figure 16.2: Decomposed predictions from N-BEATS (interpretable)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：N-BEATS的分解预测（可解释）
- en: With all its success, N-BEATS was still a univariate model. It was not able
    to take in any external information, apart from its history. This was fine for
    the M4 competition, where all the time series in question were also univariate.
    However, many real-world time series problems come with additional explanatory
    variables (or exogenous variables). Let’s look at a slight modification that was
    made to N-BEATS that enabled exogenous variables.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管N-BEATS取得了巨大成功，但它仍然是一个单变量模型。它不能接受任何外部信息，除了它的历史数据。这在M4竞赛中是可以的，因为所有相关的时间序列都是单变量的。然而，许多实际世界中的时间序列问题会带有额外的解释变量（或外生变量）。让我们看看对N-BEATS做的一个小改动，如何使其能够处理外生变量。
- en: Neural Basis Expansion Analysis for Interpretable Time Series Forecasting with
    Exogenous Variables (N-BEATSx)
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带外生变量的可解释时间序列预测的神经基础扩展分析（N-BEATSx）
- en: Olivares et al. proposed an extension of the N-BEATS model by making it compatible
    with exogenous variables. The overall structure is the same (with blocks, stacks,
    and residual connections) as N-BEATS (*Figure 16.1*), so we will only be focusing
    on the key differences and additions that the **N-BEATSx** model puts forward.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Olivares等人提出了N-BEATS模型的扩展，通过使其与外生变量兼容。整体结构与N-BEATS（*图16.1*）相同（包含块、堆叠和残差连接），因此我们将仅关注**N-BEATSx**模型提出的关键差异和新增内容。
- en: '**Reference check**:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research paper by Olivares et al. (N-BEATSx) is cited in the *References*
    section as *4*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Olivares等人的研究论文（N-BEATSx）在*参考文献*部分被引用为*4*。
- en: Handling exogenous variables
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理外生变量
- en: 'In N-BEATS, the input to a block was the lookback window, *y*^b. But here,
    the input to a block is both the lookback window, *y*^b, and the array of exogenous
    variables, *x*. These exogenous variables can be of two types: time-varying and
    static. The static variables are encoded using a static feature encoder. This
    is nothing but a single-layer FC that encodes the static information into a dimension
    specified by the user. Now, the encoded static information, the time-varying exogenous
    variables, and the lookback window are concatenated to form the input for a block
    so that the hidden state representation, *h*[l], of block *l* is not *FC*(*y*^b)
    like in N-BEATS, but *FC*([*y*^b;*x*]), where [;] represents concatenation. This
    way, the exogenous information is part of the input to every block as it is concatenated
    with the residual at each step.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在N-BEATS中，一个块的输入是回溯窗口，*y*^b。但在这里，块的输入是回溯窗口，*y*^b，以及外生变量数组，*x*。这些外生变量可以分为两种类型：时间变化型和静态型。静态变量使用静态特征编码器进行编码。这个编码器实际上是一个单层全连接网络（FC），将静态信息编码为用户指定的维度。现在，编码后的静态信息、时间变化的外生变量和回溯窗口被连接在一起，形成一个块的输入，从而使得块*l*的隐藏状态表示，*h*[l]，不再是像N-BEATS中的*FC*(*y*^b)，而是*FC*([*y*^b;*x*])，其中[;]表示连接。这样，外生信息作为输入的一部分，与残差在每一步都被连接到每个块中。
- en: Exogenous blocks
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外生块
- en: 'In addition to this, the paper also proposes a new kind of block—an *exogenous
    block*. The exogenous block takes in the concatenated lookback window and exogenous
    variables (just like any other block) as input and produces a backcast and forecast:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，论文还提出了一种新型的模块——*外生模块*。外生模块接收串联的回顾窗口和外生变量（与任何其他模块相同）作为输入，并生成反向预测和预测：
- en: '![](img/B22389_16_025.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_025.png)'
- en: Here, *N*[x] is the number of exogenous features.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*N*[x]表示外生特征的数量。
- en: Here, we can see that the exogenous forecast is the linear combination of the
    exogenous variables and that the weights for this linear combination are learned
    by the expansion coefficients, ![](img/B22389_16_026.png). The paper refers to
    this configuration as the interpretable exogenous block because, by using the
    expansion weights, we can define the importance of each exogenous variable and
    even figure out the exact part of the forecast, which is because of a particular
    exogenous variable.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到外生预测是外生变量的线性组合，并且这个线性组合的权重由扩展系数![](img/B22389_16_026.png)学习。该论文将此配置称为可解释的外生模块，因为通过使用扩展权重，我们可以定义每个外生变量的重要性，甚至找出由特定外生变量引起的预测的确切部分。
- en: 'N-BEATSx also has a generic version (which is not interpretable) of the exogenous
    block. In this block, the exogenous variables are passed through an encoder that
    learns a context vector, *C*[l], and the forecast is generated using the following
    formula:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATSx还具有外生模块的通用版本（不可解释）。在此模块中，外生变量通过一个学习上下文向量*C*[l]的编码器传递，并且使用以下公式生成预测：
- en: '![](img/B22389_16_027.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_027.png)'
- en: 'They proposed two encoders: a **Temporal Convolutional Network** (**TCN**)
    and **WaveNet** (a network similar to the TCN, but with dilation to expand the
    receptive field). The *Further reading* section contains resources if you wish
    to learn more about WaveNet, an architecture that originated in the sound domain.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出了两种编码器：**时间卷积网络**（TCN）和**WaveNet**（类似于TCN的网络，但通过扩张来扩展感受野）。*进一步阅读*部分包含了更多关于WaveNet的资源，这是一种起源于声音领域的架构。
- en: N-BEATSx is also implemented in NIXTLA `neuralforecast`, however, at the time
    of writing, it cannot yet handle categorical data. Thus, we will need to encode
    the categorical features into numerical representations (like we did in *Chapter
    10*, *Global Forecasting Models*) before using `neuralforecast`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATSx也在NIXTLA的`neuralforecast`中实现，然而在撰写本文时，它尚不能处理分类数据。因此，我们需要将分类特征编码为数值表示（就像我们在*第10章*
    *全球预测模型*中所做的）后再使用`neuralforecast`。
- en: The research paper also showed that N-BEATSx outperformed N-BEATS, ES-RNN, and
    other benchmarks on electricity price forecasting considerably.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 研究论文还表明，N-BEATSx在电力价格预测方面明显优于N-BEATS、ES-RNN和其他基准模型。
- en: Continuing with the legacy of N-BEATS, we will now talk about another modification
    to the architecture that makes it suitable for long-term forecasting.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在延续N-BEATS的基础上，我们现在将讨论另一种修改架构的方法，使其适用于长期预测。
- en: Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经层次插值用于时间序列预测（N-HiTS）
- en: 'Although there has been a good amount of work from DL to tackle time series
    forecasting, very little focus has been on long-horizon forecasting. Despite recent
    progress, long-horizon forecasting remains a challenge for two reasons:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在处理时间序列预测方面取得了相当多的成果，但对长期预测的关注仍然很少。尽管最近取得了进展，但由于两个原因，长期预测仍然是一个挑战：
- en: The expressiveness required to truly capture the variation
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真正捕捉变化所需的表达能力
- en: The computational complexity
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算复杂度
- en: Attention-based methods (Transformers) and N-BEATS-like methods scale quadratically
    in memory and the computational cost concerning the forecasting horizon.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的方法（Transformers）和类似N-BEATS的方法在内存和与预测时间范围相关的计算成本上呈二次扩展。
- en: The authors claim that N-HiTS drastically cuts long-forecasting compute costs
    while simultaneously showing 25% accuracy improvements compared to existing Transformer-based
    architectures across a large array of multi-variate forecasting datasets.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 作者声称，与现有基于Transformer的架构相比，N-HiTS大大降低了长期预测的计算成本，同时在大量多变量预测数据集上显示出25%的准确率改进。
- en: '**Reference check**:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research paper by Challu et al. on N-HiTS is cited in the *References* section
    as *5*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Challu等人在*N-HiTS*上的研究论文被引用为*5*，详见*参考文献*部分。
- en: The Architecture of N-HiTS
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: N-HiTS的架构
- en: N-HiTS can be considered as an alteration to N-BEATS because the two share a
    large part of their architectures. *Figure 16.1*, which shows the N-BEATS architecture,
    is still valid for N-HiTS. N-HiTS also has stacks of blocks arranged in a residual
    manner; it differs only in the kind of blocks it uses. For instance, there is
    no provision for interpretable blocks. All the blocks in N-HiTS are generic. While
    N-BEATS tries to decompose the signal into different patterns (trend, seasonality,
    and so on), N-HiTS tries to decompose the signal into multiple frequencies and
    forecast them separately.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: N-HiTS可以被视为对N-BEATS的修改，因为它们两者在架构上有很大一部分相同。 *图16.1* 展示了N-BEATS的架构，对于N-HiTS仍然有效。N-HiTS也具有以残差方式排列的块堆叠；它们的不同之处仅在于所使用的块的种类。例如，N-HiTS没有为可解释块提供规定。所有N-HiTS中的块都是通用的。虽然N-BEATS试图将信号分解为不同的模式（趋势、季节性等），但N-HiTS试图将信号分解为多个频率并分别进行预测。
- en: 'To enable this, a few key improvements have been proposed:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，提出了一些关键的改进：
- en: Multi-rate data sampling
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多速率数据采样
- en: Hierarchical interpolation
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层插值
- en: Synchronizing the rate of input sampling with a scale of output interpolation
    across the blocks
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入采样率与各个块之间的输出插值尺度同步
- en: Multi-rate data sampling
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多速率数据采样
- en: N-HiTS incorporates sub-sampling layers before the fully connected blocks so
    that the resolution of the input to each block is different. This is similar to
    smoothing the signal with different resolutions so that each block is looking
    at a pattern that occurs at different resolutions—for instance, if one block looks
    at the input every day, another block looks at the output every week, and so on.
    This way, when arranged with different blocks looking at different resolutions,
    the model will be able to predict patterns that occur in those resolutions. This
    significantly reduces the memory footprint and the computation required as well,
    because instead of looking at all *H* steps of the lookback window, we are looking
    at smaller series (such as H/2, H/4, and so on).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: N-HiTS在完全连接块之前加入了子采样层，以便每个块的输入分辨率不同。这类似于用不同分辨率平滑信号，以便每个块查看发生在不同分辨率的模式，例如，如果一个块每天查看输入，另一个块每周查看输出等。这样，当用不同块查看不同分辨率时，模型将能够预测在这些分辨率下发生的模式。这显著减少了内存占用和所需的计算，因为我们不是查看所有*H*步的回顾窗口，而是查看较小的系列（例如H/2、H/4等）。
- en: 'N-HiTS accomplishes this using a Max Pooling or Average Pooling layer of kernel
    size *k*[l] on the lookback window. A pooling operation is similar to a convolution
    operation, but the function that is used is non-learnable. In *Chapter 12*, *Building
    Blocks of Deep Learning for Time Series*, we learned about convolutions, kernels,
    stride, and so on. While a convolution uses weights that are learned from data
    while training, a pooling operation uses a non-learnable and fixed function to
    aggregate the data in the receptive field of a kernel. Common examples of these
    functions are the maximum, average, sum, and so on. N-HiTS uses `MaxPool1d` or
    `AvgPool1d` (in `PyTorch` terminology) with different kernel sizes for different
    blocks. Each pooling operation also has a stride equal to the kernel, resulting
    in non-overlapping windows over which we do the aggregation operation. To refresh
    our memory, let’s see what max pooling with `kernel=2` and `stride=2` looks like:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: N-HiTS使用了一个最大池化或平均池化层，内核大小为*k*[l]，在回顾窗口上进行池化操作。池化操作类似于卷积操作，但使用的函数是非可学习的。在*第12章*，*时间序列深度学习的基本构建块*中，我们学习了卷积、内核、步长等。而卷积使用从数据中学习的权重进行训练，池化操作使用非可学习和固定的函数来聚合内核接收场中的数据。这些函数的常见示例包括最大值、平均值、求和等。N-HiTS在不同块中使用`MaxPool1d`或`AvgPool1d`（以`PyTorch`术语表示），每个池化操作的步长也等于内核，导致非重叠窗口上进行聚合操作。为了刷新我们的记忆，让我们看看`kernel=2`和`stride=2`的最大池化是什么样子：
- en: '![Figure 16.3 – Max pooling on one dimension – kernel=2, stride=2 ](img/B22389_16_03.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图16.3 - 一维最大池化 - 内核=2，步长=2](img/B22389_16_03.png)'
- en: 'Figure 16.3: Max pooling on one dimension—kernel = 2, stride = 2'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3：一维最大池化 - 内核 = 2，步长 = 2
- en: Therefore, a larger kernel size will tend to cut more high-frequency (or small-timescale)
    components from the input. This way, the block is forced to focus on larger-scale
    patterns. The paper calls this **multi-rate signal sampling**.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，较大的内核大小会倾向于削减输入中更多的高频（或小时间尺度）成分。这样，块被迫专注于更大尺度的模式。该论文将此称为**多速率信号采样**。
- en: Hierarchical interpolation
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分层插值
- en: In a standard multi-step forecasting setting, the model must forecast *H* timesteps.
    As *H* becomes larger, the compute requirements increase and lead to an explosion
    of expressive power the model needs to have.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的多步预测设置中，模型必须预测*H*个时间步长。随着*H*的增大，计算要求增加，并导致模型所需的表现力爆炸性增长。
- en: Training a model with such a large expressive power, without overfitting, is
    a challenge in itself. To combat these issues, N-HiTS proposes a technique called
    **temporal interpolatio****n** `(`not the simple interpolation between two known
    points in time, but something specific to the architecture).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个具有如此大表现力的模型而不发生过拟合本身就是一个挑战。为了应对这些问题，N-HiTS提出了一种名为**时间插值**的技术（`(`不是在两个已知时间点之间的简单插值，而是与架构特定的插值方法)`）。
- en: The pooled input (which we saw in the previous section) goes into the block
    along with the usual mechanism to generate expansion coefficients and finally
    gets converted into forecast output. But here, instead of setting the dimension
    of the expansion coefficients as *H*, N-HiTS sets them as *r*[l] X *H*, where
    *r*[l] is the **expressiveness ratio**. This parameter essentially reduces the
    forecast output dimension and thus controls the issues we discussed in the previous
    paragraph. To recover the original sampling rate and predict all the *H* points
    in the forecast horizon, we can use an interpolation function. There are many
    options for the interpolation functions—linear, nearest neighbor, cubic, and so
    on. All these options can easily be implemented in `PyTorch` using the `interpolate`
    function.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 汇聚的输入（我们在前一节中看到的）与常规机制一起进入区块，生成扩展系数，并最终转换为预测输出。但是在这里，N-HiTS将扩展系数的维度设置为*r*[l]
    X *H*，而不是*H*，其中*r*[l]是**表现力比率**。这个参数本质上减少了预测输出的维度，从而控制了我们在前一段中讨论的问题。为了恢复原始采样率并预测预测范围内的所有*H*点，我们可以使用插值函数。插值函数有很多选择——线性插值、最近邻插值、三次插值等。所有这些选项都可以通过`PyTorch`中的`interpolate`函数轻松实现。
- en: Synchronizing the input sampling and output interpolation
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 同步输入采样和输出插值
- en: 'In addition to proposing the input sampling through pooling and output interpolation,
    N-HiTS also proposes arranging them in different blocks in a particular way. The
    authors argue that hierarchical interpolation can only happen the right way if
    the expressiveness ratios are distributed across blocks in a manner that is synchronized
    with the multi-rate sampling. Blocks closer to the input should have a smaller
    expressiveness ratio, *r*[l], and larger kernel sizes, *k*[l]. This means that
    the blocks closer to the input will generate larger resolution patterns (because
    of aggressive interpolation) while being forced to look at aggressively subsampled
    input signals. The paper proposes exponentially increasing expressiveness ratios
    as we move from the initial block to the last block to handle a wide range of
    frequency bands. The official N-HiTS implementation uses the following formula
    to set the expressiveness ratios and pooling kernels:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过池化和输出插值提出输入采样外，N-HiTS还提出了以特定方式将它们排列在不同区块中的方法。作者认为，层次化插值只有在表现力比率以与多速率采样同步的方式分布在区块之间时，才能正确进行。离输入较近的区块应该具有较小的表现力比率*r*[l]，以及较大的卷积核大小*k*[l]。这意味着离输入较近的区块将生成更高分辨率的模式（因为进行更为积极的插值），同时被迫查看大幅度下采样的输入信号。论文提出了随着从初始区块到最后一个区块的移动，表现力比率呈指数增长，以处理广泛的频率带。官方的N-HiTS实现使用以下公式来设置表现力比率和池化卷积核：
- en: '[PRE2]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can also provide explicit `pooling_sizes` and `downsampling_fequencies` to
    reflect known cycles of the time series (weekly seasonality, monthly seasonality,
    and so on). The core principle of N-BEATS (one block removing the effect it captures
    from the signal and passing it on to the next block) is used here as well so that,
    at each level, the patterns or frequencies that a block captures are removed from
    the input signal before being passed on to the next block. In the end, the final
    forecast is the sum of all such individual block forecasts.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以提供显式的`pooling_sizes`和`downsampling_fequencies`来反映已知的时间序列周期（例如每周季节性、每月季节性等）。N-BEATS的核心原理（一个区块从信号中去除其捕捉的影响并传递给下一个区块）在这里也得到了应用，因此在每一层中，区块捕捉到的模式或频率会在传递给下一个区块之前从输入信号中去除。最终，最终的预测是所有这些单独区块预测的总和。
- en: Forecasting with N-HiTS
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用N-HiTS进行预测
- en: '*N-HiTS* is implemented in NIXTLA forecasting. We can use the same framework
    we were working with for NBEATS and extend it to train *N-HiTS* on our data. What’s
    even better is that the implementation supports exogenous variables, the same
    way N-BEATSx handles exogenous variables (although without the exogenous block).
    First, let’s look at the initialization parameters of the implementation.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*N-HiTS* 已在 NIXTLA 预测中实现。我们可以使用与 N-BEATS 相同的框架，并扩展其以便在我们的数据上训练 *N-HiTS*。更棒的是，该实现支持外生变量，就像
    N-BEATSx 处理外生变量一样（尽管没有外生模块）。首先，让我们看看实现的初始化参数。'
- en: 'The `NHITS` class in `neuralforecast` has the following parameters:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralforecast` 中的 `NHITS` 类具有以下参数：'
- en: '`n_blocks`: This is a list of integers signifying the number of blocks to be
    used in each stack. For instance, `[1,1,1]` means there will be three stacks with
    one block each.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_blocks`：这是一个整数列表，表示每个堆叠中使用的块的数量。例如，`[1,1,1]` 表示将有三个堆叠，每个堆叠中有一个块。'
- en: '`n_pool_kernel_size`: This is a list of integers that defines the pooling size
    (*k*[l]) for each stack. This is an optional parameter, and if provided, we can
    have more control over how the pooling happens in the different stacks. Using
    an ordering of higher to lower improves results.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_pool_kernel_size`：这是一个整数列表，用于定义每个堆叠的池化大小（*k*[l]）。这是一个可选参数，如果提供，我们可以更好地控制不同堆叠中池化的方式。使用从高到低的排序可以提高结果。'
- en: '`pooling_mode`: This defines the kind of pooling to be used. It should be either
    `''MaxPool1d''` or `''AvgPool1d''`.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooling_mode`：定义要使用的池化类型。它应该是 `''MaxPool1d''` 或 `''AvgPool1d''`。'
- en: '`n_freq_downsample`: This is a list of integers that defines the expressiveness
    ratios (*r*[l]) for each stack. This is an optional parameter, and if provided,
    we can have more control over how the interpolation happens in the different stacks.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_freq_downsample`：这是一个整数列表，用于定义每个堆叠的表现力比率（*r*[l]）。这是一个可选参数，如果提供，我们可以更好地控制不同堆叠中插值的方式。'
- en: '**Notebook alert**:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提示**：'
- en: The complete code for training N-HiTS can be found in the `02-NHiTS_NeuralForecast.ipynb`
    notebook in the `Chapter16` folder.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 N-HiTS 的完整代码可以在 `Chapter16` 文件夹中的 `02-NHiTS_NeuralForecast.ipynb` 笔记本中找到。
- en: Now, let’s shift our focus and look at a few modifications of the Transformer
    model to make it better for time series forecasting.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们把注意力转向 Transformer 模型的一些修改，使其更适合时间序列预测。
- en: Autoformer
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Autoformer
- en: Recently, Transformer models have shown superior performance in capturing long-term
    patterns than standard RNNs. One of the major factors of that is the fact that
    self-attention, which powers Transformers, can reduce the length that the relevant
    sequence information has to be held on to before it can be used for prediction.
    In other words, in an RNN, if the timestep 12 steps before holds important information,
    that information has to be stored in the RNN through 12 updates before it can
    be used for prediction. But with self-attention in Transformers, the model is
    free to create a shortcut between lag 12 and the current step directly because
    of the lack of recurrence in the structure.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Transformer 模型在捕捉长期模式方面表现出比标准 RNN 更强的性能。其主要原因之一是自注意力机制，Transformer 正是依靠这一机制来减少相关序列信息在被用于预测之前必须保存的长度。换句话说，在
    RNN 中，如果第 12 步之前的时间步包含重要信息，那么这些信息必须通过 12 次更新存储在 RNN 中，才能用于预测。但是，在 Transformer
    中，由于结构中没有递归，模型可以自由地在滞后 12 步和当前步骤之间创建一个快捷通道。
- en: 'But the same self-attention is also the reason why we can’t scale vanilla Transformers
    to long sequences. In the previous section, we discussed how long-term forecasting
    is a challenge because of two reasons: the expressiveness required to truly capture
    the variation and computational complexity. Self-attention, with its quadratic
    computational complexity, contributes to the second reason.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 但同样的自注意力机制也是我们无法将原生 Transformer 扩展到长序列的原因。在上一节中，我们讨论了长期预测面临的挑战，主要有两个原因：捕捉变化所需的表现力和计算复杂度。自注意力的二次计算复杂度就是第二个原因。
- en: 'The research community has recognized this challenge and has put a lot of effort
    into devising efficient transformers through many techniques, such as downsampling,
    low-rank approximations, sparse attention, and so on. For a detailed account of
    such techniques, refer to the link for *Efficient Transformers: A Survey* in the
    *Further reading* section.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 研究界已经意识到这个挑战，并投入了大量努力，通过许多技术，如下采样、低秩近似、稀疏注意力等，来设计高效的变换器。有关这些技术的详细介绍，请参阅*进一步阅读*部分中的*高效变换器：调查*链接。
- en: '`Autoformer` is another model that is designed for long-term forecasting. Autoformer
    invents a new kind of attention and couples it with aspects from time series decomposition.
    Let’s take a look at what makes Autoformer special.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`Autoformer`是另一种为长期预测设计的模型。Autoformer发明了一种新的注意力机制，并将其与时间序列分解的各个方面结合。让我们来看一下是什么使得Autoformer如此特别。'
- en: The architecture of the Autoformer model
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Autoformer模型的架构
- en: 'The Autoformer model is a modification of Transformers. The following are its
    major contributions:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Autoformer模型是变换器的改进版。以下是其主要贡献：
- en: '**Uniform Input Representation**: A methodical way to include the history of
    the series along with other information, which will help in capturing long-term
    signals such as the week, month, holidays, and so on'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性输入表示**：一种系统化的方法，将序列的历史信息与其他信息结合，有助于捕获长期信号，如周、月、节假日等。'
- en: '**Generative-style decoder**: Used to generate the long-term horizon in a single
    forward pass instead of via dynamic recurrence'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成式解码器**：用于在一次前向传播中生成长期预测，而不是通过动态递归生成。'
- en: '**AutoCorrelation mechanism**: An alternative to standard dot product attention,
    which takes into account sub-series similarity rather than point-to-point similarity'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自相关机制**：一种替代标准点积注意力的方法，考虑的是子序列相似性，而非点对点相似性。'
- en: '**Decomposition architecture**: A specially designed architecture that separates
    seasonality, trend, and residual in a time series while modeling it'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分解架构**：一种专门设计的架构，在建模时间序列时将季节性、趋势和残差分离。'
- en: '**Reference check**:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research paper by Wu et al. on Autoformer is cited in the *References* section
    as *9*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 吴等人在Autoformer上的研究论文在*参考文献*部分被引用为*9*。
- en: Uniform Input Representation
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一致性输入表示
- en: RNNs capture time series patterns with their recurrent structure, so they only
    need the sequence; they don’t need information about the timestamp to extract
    the patterns. However, the self-attention in Transformers is done via point-wise
    operations that are performed in sets (the order doesn’t matter in a set). Typically,
    we include positional encodings to capture the order of the sequence. Instead
    of using positional encodings, we can use richer information, such as hierarchical
    timestamp information (such as weeks, months, years, and so on). This is what
    the authors proposed through **Uniform Input Representation**.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: RNN通过其递归结构捕获时间序列模式，因此只需要序列；它们不需要时间戳信息来提取模式。然而，变换器中的自注意力是通过点对点操作来完成的，这些操作在一组内执行（顺序在一组内不重要）。通常，我们会包含位置编码来捕获序列的顺序。我们可以不使用位置编码，而是使用更丰富的信息，如层次化时间戳信息（例如周、月、年等）。这正是作者通过**一致性输入表示**提出的。
- en: Uniform Input Representation uses three types of embeddings to capture the history
    of the time series, the sequence of values in the time series, and the global
    timestamp information. The sequence of values in the time series is captured by
    the standard positional embedding of the `d_model` dimension.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性输入表示使用三种类型的嵌入来捕获时间序列的历史、时间序列中的值序列和全局时间戳信息。时间序列中的值序列通过`d_model`维度的标准位置嵌入来捕获。
- en: Uniform Input Representation uses a one-dimensional convolutional layer with
    `kernel=3` and `stride=1` to project the history (which is scalar or one-dimensional)
    into an embedding of `d_model` dimensions. This is referred to as **value embedding**.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性输入表示使用一个一维卷积层，`kernel=3`，`stride=1`，将历史数据（它是标量或一维的）投影到`d_model`维度的嵌入中。这被称为**值嵌入**。
- en: The global timestamp information is embedded by a learnable embedding of `d_model`
    dimensions with limited vocabulary in a mechanism that is identical to embedding
    categorical variables into fixed-size vectors (*Chapter 15*, *Strategies for Global
    Deep Learning Forecasting Models*). This is referred to as **temporal embedding**.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 全球时间戳信息通过一个可学习的嵌入（`d_model`维度，有限词汇表）嵌入机制来实现，这个机制与将类别变量嵌入到固定大小的向量中的方法相同（*第15章*，*全球深度学习预测模型策略*）。这被称为**时间嵌入**。
- en: Now that we have three embeddings of the same dimension, `d_model`, all we need
    to do is add them together to get the Uniform Input Representation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有三个相同维度的嵌入，`d_model`，我们需要做的就是将它们加在一起，得到统一输入表示。
- en: Generative-style decoder
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成式解码器
- en: The standard way of inferencing a Transformer model is by decoding one token
    at a time. This autoregressive process is time-consuming and repeats a lot of
    calculations for each step. To alleviate this problem, the Autoformer model adopts
    a more generative fashion where the entire forecasting horizon is generated in
    a single forward pass.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 推理Transformer模型的标准方式是一次解码一个标记。这个自回归过程非常耗时，并且每一步都会重复很多计算。为了解决这个问题，Autoformer模型采用了更具生成性的方式，在一次前向传播中生成整个预测区间。
- en: In NLP, it is a popular technique to use a special token (START) to start the
    dynamic decoding process. Instead of choosing a special token for this purpose,
    the Autoformer model chooses a sample from the input sequence, such as an earlier
    slice before the output window. For instance, if we say the input window is *t*[1]
    to *t*[w], we will sample a sequence of length *C* from the input, *t*[w-c] to
    *t*[w], and include this sequence as the starting sequence of the decoder. To
    make the model predict the entire horizon in a single forward pass, we can extend
    the decoder input tensor so that its length is *C* + *H*, where *H* is the length
    of the prediction horizon. The initial *C* tokens are filled with the sample sequence
    from the input, and the rest are filled as zeros—that is, ![](img/B22389_16_028.png).
    This is just the target. Although ![](img/B22389_16_029.png) has zeros filled
    in for the prediction horizon, this is just for the target. The other information,
    such as the global timestamps, is included in ![](img/B22389_16_029.png). Sufficient
    masking of the attention matrix is also employed so that each position does not
    attend to future positions, thus maintaining the autoregressive nature of the
    prediction.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，使用特殊标记（START）来开始动态解码过程是一种流行的技术。Autoformer模型没有选择为此目的使用特殊标记，而是从输入序列中选择一个样本，例如输出窗口之前的一个较早的切片。例如，假设输入窗口是*t*[1]到*t*[w]，我们将从输入中采样一个长度为*C*的序列，*t*[w-c]到*t*[w]，并将该序列作为解码器的起始序列。为了使模型在一次前向传播中预测整个预测区间，我们可以扩展解码器输入张量，使其长度为*C*
    + *H*，其中*H*是预测区间的长度。初始的*C*个标记由输入的样本序列填充，剩余部分填充为零——即，![](img/B22389_16_028.png)。这只是目标。尽管![](img/B22389_16_029.png)的预测区间填充了零，但这仅仅是目标。其他信息，例如全球时间戳，包含在![](img/B22389_16_029.png)中。还采用了充分的注意力矩阵遮蔽，使得每个位置不会关注未来的位置，从而保持预测的自回归特性。
- en: Now, let’s look at the time series decomposition architecture.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下时间序列分解架构。
- en: Decomposition architecture
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分解架构
- en: 'We saw this idea of decomposition back in *Chapter 3*, *Analyzing and Visualizing
    Time Series Data*, and even in this chapter (N-BEATS). Autoformer successfully
    renovated the Transformer architecture into a deep-decomposition architecture:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第3章*，*时间序列数据的分析与可视化*中曾看到过分解的思想，甚至在本章（N-BEATS）中也有所提及。Autoformer成功地将Transformer架构改造成了一种深度分解架构：
- en: '![Figure 16.5 – Autoformer architecture ](img/B22389_16_04.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图16.5 – Autoformer架构](img/B22389_16_04.png)'
- en: 'Figure 16.4: Autoformer architecture'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：Autoformer架构
- en: It is easier to understand the overall architecture first and then dive deeper
    into the details. In *Figure 16.4*, there are boxes labeled **Auto-Correlation**
    and **Series Decomp**. For now, just know that auto-correlation is a type of attention
    and that series decomposition is a particular block that decomposes the signal
    into trend-cyclical and seasonal components.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 首先理解整体架构，然后再深入了解细节会更容易。在*图16.4*中，有标记为**自相关**和**系列分解**的框。现在，只需知道自相关是一种注意力机制，而系列分解是一个将信号分解为趋势-周期性和季节性分量的特定模块。
- en: Encoder
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编码器
- en: 'With the level of abstraction discussed in the preceding section, let’s understand
    what is happening in the encoder:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面章节讨论的抽象级别，让我们理解编码器中发生了什么：
- en: The uniform representation of the time series, *x*[en], is the input to the
    encoder. The input is passed through an **Auto-Correlation** block (for self-attention)
    whose output is *x*[ac].
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 时间序列的均匀表示*x*[en]是编码器的输入。输入通过一个**自相关**块（用于自注意力），其输出是*x*[ac]。
- en: The uniform representation, *x*[en], is added back to *x*[ac] as a residual
    connection, *x*[ac] = *x*[ac] + *x*[en].
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 均匀表示*x*[en]作为残差连接加回*x*[ac]，*x*[ac] = *x*[ac] + *x*[en]。
- en: Now, *x*[ac] is passed through a **Series Decomp** block, which decomposes the
    signal into a trend-cyclical component (*x*[T]) and a seasonal component, *x*[seas].
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，*x*[ac]通过一个**系列分解**块，该块将信号分解为趋势-周期性成分（*x*[T]）和季节性成分*x*[seas]。
- en: We discard *x*[T] and pass *x*[seas] to a Feed Forward network, which gives
    *x*[FF] as an output.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们丢弃*x*[T]并将*x*[seas]传递给前馈网络，该网络输出*x*[FF]。
- en: '*x*[seas] is again added to *x*[FF] as a residual connection, *x*[seas] = *x*[FF]
    + *x*[seas].'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*x*[seas]再次作为残差连接加回*x*[FF]，*x*[seas] = *x*[FF] + *x*[seas]。'
- en: Finally, this *x*[seas] is passed through another **Series Decomp** layer, which
    again decomposes the signal into the trend, ![](img/B22389_16_031.png), and a
    seasonal component, ![](img/B22389_16_032.png).
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，这个*x*[seas]通过另一个**系列分解**层，该层再次将信号分解为趋势！[](img/B22389_16_031.png)和季节性成分！[](img/B22389_16_032.png)。
- en: We discard ![](img/B22389_16_031.png), and pass on ![](img/B22389_16_032.png)
    as the final output from one block of the encoder.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们丢弃！[](img/B22389_16_031.png)，并将！[](img/B22389_16_032.png)作为一个编码器块的最终输出传递。
- en: There may be *N* blocks of encoders stacked together, one taking in the output
    of the previous encoder as input.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能会有*N*个编码器块堆叠在一起，每个编码器块都将前一个编码器的输出作为输入。
- en: Now, let’s shift our attention to the decoder block.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向解码器块。
- en: Decoder
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解码器
- en: 'The Autoformer model uses a START token-like mechanism by including a sampled
    window from the input sequence. But instead of just taking the sequence, Autoformer
    does a bit of special processing on it. Autoformer uses the bulk of its learning
    power to learn seasonality. The output of the transformer is also just the seasonality.
    Therefore, instead of including the complete window from the input sequence, Autoformer
    decomposes the signal and only includes the seasonal component in the START token.
    Let’s look at this process step by step:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Autoformer模型通过包含来自输入序列的采样窗口，使用类似START标记的机制。但Autoformer不仅仅取序列，而是对其进行了一些特殊处理。Autoformer将其学习能力的重点放在了季节性上。变换器的输出也是季节性。因此，Autoformer不包含完整的输入序列窗口，而是将信号分解，并仅在START标记中包含季节性成分。让我们一步步地来看这个过程：
- en: If the input (the context window) is *x*, we decompose it with the **Series
    Decomp** block into ![](img/B22389_16_035.png) and ![](img/B22389_16_036.png).
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果输入（上下文窗口）是*x*，我们通过**系列分解**块将其分解为！[](img/B22389_16_035.png)和！[](img/B22389_16_036.png)。
- en: Now, we sample *C* timesteps from the end of ![](img/B22389_16_036.png) and
    append *H* zeros, where *H* is the forecast horizon, and construct *x*[ds].
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们从![](img/B22389_16_036.png)的末尾采样*C*个时间步，并添加*H*个零，其中*H*是预测视野，构造*x*[ds]。
- en: This *x*[ds] is then used to create a uniform representation, *x*[dec].
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个*x*[ds]接着用于创建一个均匀表示*x*[dec]。
- en: Meanwhile, we sample *C* timesteps from the end of ![](img/B22389_16_035.png)
    and append *H* timesteps with the series mean (*mean*(*x*)), where *H* is the
    forecast horizon, and construct *x*[dt].
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与此同时，我们从![](img/B22389_16_035.png)的末尾采样*C*个时间步，并附加*H*个时间步与序列的均值（*mean*(*x*))，其中*H*是预测视野，构造*x*[dt]。
- en: 'This *x*[dec] is then used as the input for the decoder. This is what happens
    in the decoder:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这个*x*[dec]作为输入用于解码器。这就是解码器中发生的事情：
- en: The input, *x*[dec] , is first passed through an `Auto-Correlation` (for self-attention)
    block whose output is *x*[dac].
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入*x*[dec]首先通过一个`自相关`（用于自注意力）块，其输出是*x*[dac]。
- en: The uniform representation, *x*[dec] , is added back to *x*[dac] as a residual
    connection, *x*[dac] = *x*[dac] + *x*[dec].
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 均匀表示*x*[dec]作为残差连接加回*x*[dac]，*x*[dac] = *x*[dac] + *x*[dec]。
- en: Now, *x*[dac] is passed through a **Series Decomp** block that decomposes the
    signal into a trend-cyclical component (*x*[dT][1]) and a seasonal component,
    *x*[dseas].
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，*x*[dac]通过一个**系列分解**块，该块将信号分解为趋势-周期性成分（*x*[dT][1]）和季节性成分*x*[dseas]。
- en: In the decoder, we do not discard the trend component; instead, we save it.
    This is because we will be adding all the trend components with the trend in it
    (*x*[dt]) to come up with the overall trend part (*T*).
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在解码器中，我们不会丢弃趋势组件；相反，我们会将其保存。这是因为我们将会将所有包含趋势的趋势组件（*x*[dt]）加在一起，形成整体的趋势部分（*T*）。
- en: The seasonal output from the **Series Decomp** block (*x*[dseas]), along with
    the output from the encoder (![](img/B22389_16_032.png)), is then passed into
    another **Auto-Correlation** block where cross-attention between the decoder sequence
    and encoder sequence is calculated. Let the output of this block be *x*[cross].
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**序列分解**块的季节性输出（*x*[dseas]），以及来自编码器的输出（![](img/B22389_16_032.png)），然后传入另一个**自相关**块，在这里计算解码器序列和编码器序列之间的交叉注意力。让这个块的输出是*x*[cross]。'
- en: Now, *x*[dseas] is added back to *x*[cross] as a residual connection, *x*[cross]
    = *x*[cross] + *x*[dseas].
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，*x*[dseas]作为残差连接被加回到*x*[cross]上，*x*[cross] = *x*[cross] + *x*[dseas]。
- en: '*x*[cross] is again passed through a **Series Decomp** block, which splits
    *x*[cross] into two components— *x*[dT2] and *x*[dseas2].'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*x*[cross]再次通过**序列分解**块，该块将*x*[cross]分解为两个组件—*x*[dT2]和*x*[dseas2]。'
- en: '*x*[dseas] is then transformed using a **Feed Forward** network into *x*[dff]
    and *x*[dseas] is added to it in a residual connection, *x*[dff]= *x*[dff] + *x*[dseas].'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*x*[dseas]随后通过**前馈网络**转换为*x*[dff]，并通过残差连接将*x*[dseas]加到其中，*x*[dff] = *x*[dff]
    + *x*[dseas]。'
- en: Finally, *x*[dff] is passed through yet another **Series Decomp** block, which
    decomposes it into two components— *x*[dT3] and *x*[dseas3]. *x*[dseas3] is the
    final output of the decoder, which captures seasonality.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，*x*[dff]通过另一个**序列分解**块，该块将其分解为两个组件—*x*[dT3]和*x*[dseas3]。*x*[dseas3]是解码器的最终输出，捕捉季节性。
- en: 'Another output is the residual trend, ![](img/B22389_16_040.png), which is
    a projection of the summation of all the trend components extracted in the decoder’s
    **Series Decomp** blocks. The projection layer is a **Conv1d** layer, which projects
    the extracted trend to the desired output dimension: ![](img/B22389_16_041.png).'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个输出是残差趋势，![](img/B22389_16_040.png)，它是通过解码器的**序列分解**块提取的所有趋势组件的总和的投影。投影层是一个**Conv1d**层，它将提取的趋势投射到所需的输出维度：![](img/B22389_16_041.png)。
- en: '*M* such decoder layers are stacked on top of each other, each one feeding
    its output as the input to the next one.'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*M*个这样的解码器层被依次堆叠，每个层将其输出作为下一个层的输入。'
- en: The residual trend, ![](img/B22389_16_040.png), of each decoder layer gets added
    to the trend init, *x*[dt], to model the overall trend component (*T*).
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个解码器层的残差趋势，![](img/B22389_16_040.png)，会加到趋势初始化部分*x*[dt]上，以建模整体的趋势组件（*T*）。
- en: The *x*[dseas3] of the final decoder layer is considered to be the overall seasonality
    component and is projected to the desired output dimension (*S*) using a linear
    layer.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终解码器层的*x*[dseas3]被认为是整体的季节性组件，并通过一个线性层投射到所需的输出维度（*S*）。
- en: Finally, the prediction or the forecast *X*[out]= *T* + *S*.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，预测或预测结果 *X*[out] = *T* + *S*。
- en: The whole architecture is cleverly designed so that the relatively stable and
    easy-to-predict part of the time series (the trend-cyclical) is removed and the
    difficult-to-capture seasonality can be modeled well.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 整个架构巧妙地设计，以便将时间序列中相对稳定且易于预测的部分（趋势-周期性）移除，从而能够很好地建模难以捕捉的季节性。
- en: 'Now, how does the **Series Decomp** block decompose the series? The mechanism
    may be familiar to you already: `AvgPool1d` with some padding so that it maintains
    the same size as the input. This acts like a moving average over the specified
    kernel width.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，**序列分解**块是如何分解序列的呢？这个机制你可能已经熟悉了：`AvgPool1d`配合一定的填充，这样它可以保持与输入相同的大小。这就像是在指定的核宽度上做一个移动平均。
- en: We have been talking about the **Auto-Correlation** block throughout this explanation.
    Now, let’s understand the ingenuity of the **Auto-Correlation** block.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在整个解释过程中都在讨论**自相关**块。现在，让我们来理解一下**自相关**块的巧妙之处。
- en: Auto-correlation mechanism
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自相关机制
- en: 'Autoformer uses an auto-correlation mechanism in place of standard scaled dot
    product attention. This discovers sub-series similarity based on periodicity and
    uses this similarity to aggregate similar sub-series. This clever mechanism breaks
    the information bottleneck by expanding the point-wise operation of the scaled
    dot product attention to a sub-series level operation. The initial part of the
    overall mechanism is similar to the standard attention procedure, where we project
    the query, key, and values into the same dimension using weight matrices. The
    key difference is the attention weight calculation and how they are used to calculate
    the values. This mechanism achieves this by using two salient sub-mechanisms:
    discovering period-based dependencies and time delay aggregation.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Autoformer 使用自相关机制替代标准的缩放点积注意力机制。该机制基于周期性发现子序列的相似性，并利用这种相似性聚合相似的子序列。这个巧妙的机制通过将标准缩放点积注意力的逐点操作扩展到子序列级操作，从而打破了信息瓶颈。整体机制的初始部分类似于标准的注意力过程，其中我们使用权重矩阵将查询、键和值投影到相同的维度。关键区别在于注意力权重的计算方式以及它们如何被用来计算值。这个机制通过使用两个显著的子机制来实现这一点：发现基于周期的依赖关系和时间延迟聚合。
- en: Period-based dependencies
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于周期的依赖关系
- en: Autoformer uses autocorrelation as the key measure of similarity. Auto-correlation,
    as we know, represents the similarity between a given time series, *X*[t], and
    its lagged series. For instance, ![](img/B22389_16_043.png) is the autocorrelation
    between the time series *X*[t] and ![](img/B22389_16_044.png). Autoformer considers
    this autocorrelation as the unnormalized confidence of the particular lag. Therefore,
    from the list of all ![](img/B22389_16_045.png), we choose *k* most possible lags
    and use *softmax* to convert these unnormalized confidences into probabilities.
    We use these probabilities as weights to aggregate relevant sub-series (we will
    talk about this in the next section).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Autoformer 使用自相关作为相似性度量。正如我们所知，自相关表示给定时间序列 *X*[t] 与其滞后序列之间的相似性。例如，![](img/B22389_16_043.png)
    是时间序列 *X*[t] 与 ![](img/B22389_16_044.png) 之间的自相关。Autoformer 将这个自相关视为特定滞后的未归一化置信度。因此，从所有
    ![](img/B22389_16_045.png) 中，我们选择 *k* 个最可能的滞后并使用 *softmax* 将这些未归一化的置信度转换为概率。我们使用这些概率作为权重来聚合相关子序列（我们将在下一节讨论这一点）。
- en: 'The autocorrelation calculation is not the most efficient operation and Autoformer
    suggests an alternative to make the calculation faster. Based on the **Wiener–Khinchin
    theorem** in **Stochastic Processes** (this is outside the scope of the book,
    but for those who are interested, I have included a link in the *Further reading*
    section), autocorrelation can also be calculated using **Fast Fourier Transform**
    (**FFT**). The process can be seen as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 自相关计算并不是最高效的操作，Autoformer 提出了一个替代方案，使得计算更快。根据 **Wiener–Khinchin 定理**（该内容在**随机过程**中涉及，超出了本书的范围，但对于感兴趣的读者，我在*进一步阅读*部分提供了链接），自相关也可以通过
    **快速傅里叶变换** (**FFT**) 进行计算。过程如下所示：
- en: '![](img/B22389_16_046.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_046.png)'
- en: Here, ![](img/B22389_16_047.png) denotes the FFT and ![](img/B22389_16_048.png)
    denotes the conjugate operation (the conjugate of a complex number is the number
    with the same real part and an imaginary part, which is equal in magnitude but
    with the sign reversed. The mathematics around this is outside the scope of this
    book).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B22389_16_047.png) 表示 FFT，![](img/B22389_16_048.png) 表示共轭操作（复数的共轭是具有相同实部和虚部符号相反的数值，大小相等。有关这一部分的数学推导超出了本书的范围）。
- en: 'This can easily be written in PyTorch as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以很容易地用 PyTorch 编写如下：
- en: '[PRE3]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, ![](img/B22389_16_049.png) is in the spectral domain. To bring it back
    to the real domain, we need to do an inverse FFT:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，![](img/B22389_16_049.png) 处于频谱域。为了将其带回实数域，我们需要做一个逆 FFT：
- en: '![](img/B22389_16_050.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_050.png)'
- en: 'Here, ![](img/B22389_16_051.png) denotes the inverse FFT. In PyTorch, we can
    do this easily:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B22389_16_051.png) 表示逆 FFT。在 PyTorch 中，我们可以轻松实现：
- en: '[PRE4]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When the query and key are the same, this calculates self-attention; when they
    are different, they calculate cross-attention.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 当查询和键相同，计算的是自注意力；当它们不同，计算的是交叉注意力。
- en: Now, all we need to do is take the top-k values from `corr` and use them to
    aggregate the sub-series.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们所需要做的就是从 `corr` 中取出前 *k* 个值，并用它们来聚合子序列。
- en: Time delay aggregation
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 时间延迟聚合
- en: We have identified the major lags that are auto-correlated using the FFT and
    inverse-FFT. For a more concrete example, the dataset we have been working on
    (*London Smart Meter Dataset*) has a half-hourly frequency and has strong daily
    and weekly seasonality. Therefore, the auto-correlation identification may have
    picked out 48 and 48*7 as the two most important lags. In the standard attention
    mechanism, we use the calculated probability as weights to aggregate the value.
    Autoformer also does something similar, but instead of applying the weights to
    points, it applies them to sub-series.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 FFT 和逆 FFT 确定了自相关的主要滞后。例如，我们使用的数据集（*伦敦智能电表数据集*）具有半小时的频率，并且具有强烈的日常和每周季节性。因此，自相关识别可能已将
    48 和 48*7 选为最重要的两个滞后。在标准的注意力机制中，我们使用计算出的概率作为权重来聚合值。Autoformer 也做了类似的事情，但它不是将权重应用于单个点，而是将它们应用于子序列。
- en: 'Autoformer does this by shifting the time series by the lag, ![](img/B22389_16_045.png),
    and then using the lag’s weight to aggregate them:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Autoformer 通过将时间序列按滞后进行平移来实现此功能，![](img/B22389_16_045.png)，然后使用滞后权重对其进行聚合：
- en: '![](img/B22389_16_053.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_053.png)'
- en: Here, ![](img/B22389_16_054.png) is the *softmax*-ed probabilities on the *top-k*
    autocorrelations.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B22389_16_054.png) 是 *softmax* 归一化后的 *top-k* 自相关概率。
- en: In our example, we can think of this as shifting the series by 48 timesteps
    so that the previous day’s timesteps are aligned with the current day and then
    using the weight of the 48 lag to scale it. Then, we can move on to the 48*7 lag,
    align the previous week’s timesteps with the current week, and then use the weight
    of the 48*7 lag to scale it. So, in the end, we will get a weighted mixture of
    the seasonality patterns that we can observe daily and weekly. Since these weights
    are learned by the model, we can hypothesize that different blocks learn to focus
    on different seasonalities, and thus as a whole, the blocks learn the overall
    pattern in the time series.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们可以将其理解为将序列平移 48 个时间步长，以使前一天的时间步长与当前日对齐，然后使用 48 个滞后的权重进行缩放。然后，我们可以转到
    48*7 的滞后，将前一周的时间步长与当前周对齐，再使用 48*7 滞后的权重进行缩放。因此，最终我们将获得一个加权的季节性模式混合，这些模式可以在日常和每周中观察到。由于这些权重是由模型学习的，我们可以假设不同的模块学习专注于不同的季节性，因此整体上，模块学习了时间序列中的总体模式。
- en: Forecasting with Autoformer
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Autoformer 进行预测
- en: '`Autoformer` is implemented in NIXTLA forecasting. We can use the same framework
    we were working with for NBEATS and extend it to train `Autoformer` on our data.
    First, let’s look at the initialization parameters of the implementation.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`Autoformer` 实现在 NIXTLA 预测中。我们可以使用之前用于 NBEATS 的相同框架，并扩展它以在我们的数据上训练 `Autoformer`。首先，让我们看看实现中的初始化参数。'
- en: We have to keep in mind that the Autoformer model does not support exogenous
    variables. The only additional information it officially supports is global timestamp
    information such as the week, month, and so on, along with holiday information.
    We can technically extend this to any categorical feature (static or dynamic),
    but no real-valued information is currently supported.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须记住，Autoformer 模型不支持外生变量。它官方支持的唯一附加信息是全局时间戳信息，如周、月等，以及假期信息。我们可以从技术上将其扩展到任何类别特征（静态或动态），但目前不支持任何实值信息。
- en: Let’s look at the initialization parameters of the implementation.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看实现中的初始化参数。
- en: 'The `Autoformer` class has the following major parameters:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`Autoformer` 类具有以下主要参数：'
- en: '`distil`: This is a Boolean flag for turning the attention distillation off
    and on.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distil`：这是一个布尔标志，用于开启或关闭注意力蒸馏。'
- en: '`encoder_layers`: This is an integer representing the number of encoder layers.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layers`：这是一个整数，表示编码器层的数量。'
- en: '`decoder_layers`: This is an integer representing the number of decoder layers.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layers`：这是一个整数，表示解码器层的数量。'
- en: '`n_head`: This is an integer representing the number of attention heads.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_head`：这是一个整数，表示注意力头的数量。'
- en: '`conv_hidden_size`: This is an integer parameter that specifies the channels
    of the convolutional encoder, which can be thought of similarly to controlling
    the number of kernels or filters in the convolutional layers. The number of channels
    effectively determines how many different filters are applied to the input data,
    each capturing different features.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_hidden_size`：这是一个整数参数，指定卷积编码器的通道数，可以类似于控制卷积层中内核或滤波器的数量。通道的数量有效地决定了应用于输入数据的不同滤波器的数量，每个滤波器捕捉不同的特征。'
- en: '`activation`: This is a string that takes in one of two values—`relu` or `gelu`.
    This is the activation to be used in the encoder and decoder layers.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation`：这是一个字符串值，可以是 `relu` 或 `gelu` 中的一个。它是用于编码器和解码器层的激活函数。'
- en: '`factor`: This is an int value that helps us control the top-k values that
    will be selected in the Auto Correlation mechanism we discussed. `top_k = int(self.factor
    * math.log(length))` is the exact formula used, but we can treat *k* as a factor
    to control the top *K* selection.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`factor`：这是一个整数值，帮助我们控制在我们讨论的自相关机制中将要选择的 top-k 值。`top_k = int(self.factor *
    math.log(length))` 是使用的准确公式，但我们可以将 *k* 看作一个因子来控制前 *K* 的选择。'
- en: '`dropout`: This is a float between 0 and 1, which determines the strength of
    the dropout in the network.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout`：这是一个介于 0 和 1 之间的浮动值，决定网络中 dropout 的强度。'
- en: '**Notebook alert**:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本警告**：'
- en: The complete code for training the Autoformer model can be found in the `03-Autoformer_NeuralForecast.ipynb`
    notebook in the `Chapter16` folder.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 Autoformer 模型的完整代码可以在 `Chapter16` 文件夹中的 `03-Autoformer_NeuralForecast.ipynb`
    笔记本中找到。
- en: Let’s switch tracks and look at a family of simple linear models that were proposed
    to challenge Transformers in **Long-Term Time Series Forecasting** (**LTSF**).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们换个话题，来看一下提出的用于挑战 Transformer 的一类简单线性模型，用于**长期时间序列预测**（**LTSF**）。
- en: LTSF-Linear family of models
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LTSF-线性系列模型
- en: There has been a lot of debate on whether Transformers are right for forecasting
    problems, how popular Transformer papers haven’t used strong baselines to show
    their superiority, how the order-agnostic attention mechanism may not be the best
    way to approach strongly ordered time series, and so on. The criticism was more
    pronounced for Long-Term Time Series Forecasting as it relies more on the extraction
    of strong trends and seasonalities. In 2023, Ailing Zeng et al. decided to put
    the Transformer models to the test and conducted a wide study using 5 multivariate
    datasets, pitting five Transformer models (FEDFormer, Autoformer, Informer, Pyraformer,
    and LogTrans) against a set of simple linear models that they proposed. Surprisingly,
    the simple linear models they proposed beat all the Transformer models comfortably.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Transformer 是否适合用于预测问题，许多论文讨论了 Transformer 论文普遍没有使用强基线来展示其优越性、顺序不敏感的注意力机制可能不是处理强序列时间序列的最佳方法等等。对于长期时间序列预测的批评更为明显，因为它更多依赖于提取强趋势和季节性。2023年，曾爱玲等人决定对
    Transformer 模型进行测试，并通过使用 5 个多元数据集进行广泛研究，将五种 Transformer 模型（FEDFormer、Autoformer、Informer、Pyraformer
    和 LogTrans）与他们提出的一组简单线性模型进行对比。令人惊讶的是，他们提出的简单线性模型轻松超过了所有 Transformer 模型。
- en: '**Reference check**:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research papers by Ailing Zeng et al. and the different Transformer models,
    FEDFormer, Autoformer, Informer, Pyraformer, and LogTrans, are cited in the *References*
    section as *14*, *16*, *9*, *8*, *15*, and *17* respectively.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 曾爱玲等人的研究论文和不同的 Transformer 模型，FEDFormer、Autoformer、Informer、Pyraformer 和 LogTrans，分别在
    *参考文献* 部分被引用为 *14*、*16*、*9*、*8*、*15* 和 *17*。
- en: 'There are three models in the family of LTSF models that the authors proposed:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LTSF 模型系列中，作者提出了三种模型：
- en: Linear
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性
- en: D-Linear
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D-线性
- en: N-Linear
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: N-线性
- en: These models are so simple that it’s almost embarrassing that they outperformed
    the Transformer models. But once you get to know them a bit more, you might appreciate
    the simple but effective inductive biases that have been built into the model.
    Let’s look at them one by one.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型非常简单，以至于它们能超越 Transformer 模型简直有些让人羞愧。但一旦你更了解它们，你可能会欣赏到这些模型内建的简单但有效的归纳偏置。让我们一一来看。
- en: Linear
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性
- en: 'Just as the name suggests, this is a simple linear model. It takes the context
    window and applies a linear layer to predict the forecast horizon. It also considers
    different time series as separate channels and applies different linear layers
    to each of them. In `PyTorch`, all we need to have is an `nn.Linear` layer for
    each of the channels:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名字所示，这是一个简单的线性模型。它取上下文窗口并应用一个线性层来预测预测视野。它还将不同的时间序列视为独立通道，并对每个通道应用不同的线性层。在
    `PyTorch` 中，我们只需要为每个通道拥有一个 `nn.Linear` 层：
- en: '[PRE5]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This embarrassingly simple model was able to outperform a few Transformer models
    like the Informer, LogTrans, and so on.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这个令人尴尬的简单模型能够超越一些 Transformer 模型，如 Informer、LogTrans 等等。
- en: D-Linear
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D-线性
- en: 'D-Linear took the simple linear model and injected a decomposition prior into
    it. We saw in *Chapter 3* how we can decompose a time series into trend, seasonality,
    and residual. D-Linear does exactly that and uses a moving average (the window
    or the kernel size is a hyperparameter) and separates the input time series, *x*,
    into trend, *t* (the moving average), and the rest, *r* (seasonality + residual).
    Now, it proceeds to apply separate linear layers to *t* and *r* separately, and
    finally add them back together for the final forecast. Let’s look at a simplified
    `PyTorch` implementation:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: D-Linear将简单的线性模型与一个分解先验结合起来。我们在*第3章*中已经看到如何将时间序列分解为趋势、季节性和残差。D-Linear正是这样做的，并使用移动平均（窗口或核大小是一个超参数）将输入时间序列*x*分解为趋势*t*（移动平均）和剩余部分*r*（季节性+残差）。接下来，它对*t*和*r*分别应用独立的线性层，最后将它们加在一起得到最终的预测。让我们来看一下简化的`PyTorch`实现：
- en: '[PRE6]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The decomposition prior in the model helps it perform better than a simple linear
    model consistently and it also outperforms all the Transformer models in the study
    in almost all the datasets used.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中的分解先验使其比简单的线性模型表现得更好，并且它在研究中几乎所有数据集上都超过了所有Transformer模型。
- en: N-Linear
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: N-Linear
- en: 'The authors also proposed another model, which added another very simple modification
    to the linear model. This modification was to handle the distributional shifts
    in data that are inherent in time series data. In N-Linear, we just extract the
    last value in the input context and subtract it from the entire series (in a sort
    of normalization) and then use the linear layer for prediction. Now, once the
    output from the linear layer is available, we add back the last value that we
    subtracted earlier. In PyTorch, a simple implementation would look like this:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还提出了另一种模型，向线性模型添加了另一个非常简单的修改。这个修改是用来处理时间序列数据中固有的分布性变化。在N-Linear中，我们只需提取输入上下文中的最后一个值，并将其从整个序列中减去（进行一种规范化处理），然后使用线性层进行预测。现在，一旦线性层的输出可用，我们再将之前减去的最后一个值加回去。在PyTorch中，一个简单的实现可能是这样的：
- en: '[PRE7]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: N-Linear models also perform quite well in comparison to the other Transfomer
    models in the study. In most of the datasets that were part of the study, N-Linear
    or D-Linear came out to be the top-performing model, which is quite telling.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 与研究中的其他Transformer模型相比，N-Linear模型表现得也相当优秀。在研究中大多数数据集里，N-Linear或D-Linear都成为了表现最好的模型，这一点非常值得注意。
- en: This paper exposed some major flaws in the way we were using Transformer models
    for time series forecasting, especially for multivariate time series problems.
    A typical input to a transformer is of the form (*Batch* x *Time steps* x *Embedding*).
    The most common way to forecast multivariate time series is to pass in all the
    time series or other features in a time step as the embedding. This results in
    seemingly unrelated values being embedded in a single token and mixed together
    in the attention mechanism (which itself isn’t strongly ordered). This leads to
    a “muddled’ representation and thereby Transformers might be struggling to wean
    out the real patterns from the data.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 本文揭示了我们在使用Transformer模型进行时间序列预测时存在的一些重大缺陷，特别是在多变量时间序列问题上。Transformer的典型输入形式是（*Batch*
    x *Time steps* x *Embedding*）。预测多变量时间序列的最常见方法是将所有时间序列或其他特征作为embedding传入每个时间步。这会导致看似不相关的值被嵌入到一个单一的token中，并在注意力机制中混合在一起（而注意力机制本身并不强制按顺序排列）。这就导致了“混乱”的表示，进而使得Transformer模型可能在从数据中提取真实模式时遇到困难。
- en: This paper had such an impact that many newer models, including PatchTST and
    iTransformer, which we will be seeing later in the chapter, have used these models
    as benchmarks and showed that they perform better than them. This underlines the
    need for strong and simple methods to be reserved as strong baselines so that
    we aren’t misled by the “coolness” of any algorithm.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 本文产生了如此大的影响，以至于许多新模型，包括PatchTST和iTransformer（我们将在本章后面看到这些模型），都将这些模型作为基准，并且表现优于它们。这强调了需要保留强大而简单的方法作为可靠的基准，以避免被任何算法的“炫酷性”所误导。
- en: Now let’s see how we can also use these simple linear models and get good long-term
    forecasts.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用这些简单的线性模型并获得良好的长期预测。
- en: Forecasting with the LTSF-Linear family
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LTSF-Linear家族进行预测
- en: NLinear and DLinear are implemented in NIXTLA forecasting with the same framework
    we have seen in the prior models.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: NLinear和DLinear在NIXTLA预测中实现，采用了我们在前述模型中看到的相同框架。
- en: Let’s look at the initialization parameters of the implementation.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看实现中的初始化参数。
- en: 'The `DLinear` class has similar parameters to many of the other models. Some
    callouts are the following major parameters:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '`DLinear` 类与许多其他模型具有类似的参数。以下是一些主要参数：'
- en: '`moving_avg_window`: This is an integer value of the window size used for trend-seasonality
    decomposition. This value should be an odd integer.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`moving_avg_window`：这是一个整数值，表示用于趋势季节性分解的窗口大小。此值应该是一个奇数。'
- en: '`exclude_insample_y`: This is a boolean value to skip the autoregressive features.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exclude_insample_y`：这是一个布尔值，用于跳过自回归特征。'
- en: The `NLinear` class has no additional parameters because it is just an input
    window to the output window map.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`NLinear` 类没有额外的参数，因为它只是一个输入窗口到输出窗口的映射。'
- en: '**Notebook alert**:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒**：'
- en: The complete code for training the D-Linear model can be found in the `04-DLinear_NeuralForecast.ipynb`
    notebook, and for the N-Linear model, in the `05-NLinear_NeuralForecast.ipynb`
    notebook in the `Chapter16` folder.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 训练D-Linear模型的完整代码可以在`04-DLinear_NeuralForecast.ipynb`笔记本中找到，N-Linear模型的代码则位于`05-NLinear_NeuralForecast.ipynb`笔记本中的`Chapter16`文件夹里。
- en: '**Practitioner’s tip**:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '**从业者提示**：'
- en: The jury is out on this debate as Transformers are modified more and more to
    suit time series forecasting. There might always be datasets where using a Transformer-based
    model gives you better performance than some other class of models. As practitioners,
    we should be able to suspend disbelief and try different classes of models to
    see which one fits well for our use case. After all, we only care about the dataset
    we are trying to forecast.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这个辩论尚无定论，因为Transformer模型在不断修改以适应时间序列预测。可能总有某些数据集，在这些数据集上使用基于Transformer的模型能够比其他类型的模型获得更好的性能。作为从业者，我们应该能够怀疑一切，并尝试不同类别的模型，看看哪种模型适合我们的应用场景。毕竟，我们关心的只是我们正在预测的数据集。
- en: Now, let’s look at a modification of how Transformers can be used for time series
    that learned from the insights of the LTSF-Linear paper and showed that it can
    outperform the simple linear models we just saw.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何根据LTSF-Linear论文的见解修改Transformer来用于时间序列，并展示它如何优于我们刚才看到的简单线性模型。
- en: Patch Time Series Transformer (PatchTST)
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Patch时间序列Transformer（PatchTST）
- en: In 2021, Alexey Dosovitskiy et al. proposed Vision Transformer, which introduced
    the Transformer architecture which was widely successful in Natural Language Processing
    to Vision. Although not the first to introduce patching, they applied it in a
    way that works really well for vision. The design broke up an image into patches
    and fed the transformer each patch in sequence.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年，Alexey Dosovitskiy等人提出了Vision Transformer，该架构将Transformer架构广泛应用于自然语言处理领域，并取得了巨大成功，随后也被引入到计算机视觉中。尽管不是第一个引入patch技术的模型，但他们在视觉任务中的应用方式非常成功。该设计将图像分成多个patch，并依次将每个patch输入到Transformer中。
- en: '**Reference check**:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research paper by Alexey Dosovitskiy et al. on Vision Tranformers and Yuqi
    Nie et al. on PatchTST are cited in the *References* section as *12* and *13*,
    respectively.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Alexey Dosovitskiy等人关于Vision Transformers的研究论文和Yuqi Nie等人关于PatchTST的研究论文分别在*参考文献*部分中标记为*12*和*13*。
- en: Fast-forward to 2023, and we have the same patching design applied to time series
    forecasting. Yuqi Nie et al. proposed **Patch Time Series Transformer** (**PatchTST**)
    by adopting the patching design for time series. They were motivated by the apparent
    ineffectiveness of more complicated Transformer designs (like Autoformer and Informer)
    on time series forecasting.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 快进到2023年，我们看到同样的patch设计被应用于时间序列预测。Yuqi Nie等人提出了**Patch时间序列Transformer**（**PatchTST**），通过为时间序列采用patch设计。它们的动机是更复杂的Transformer设计（如Autoformer和Informer）在时间序列预测中的效果并不明显。
- en: In 2023, Zheng et al. showed up many Transformer models by comparing them with
    a simple linear model which outperformed most of the Transformer models on common
    benchmarks. One of the key insights from the paper was that the point-wise application
    of time series to Transformer architecture doesn’t capture the locality information
    and strong ordering in time series data. Therefore, the authors proposed a simpler
    alternative that performs better than the linear models and solves the problem
    of including long context windows to Transformers without blowing up the memory
    and compute requirements.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年，Zheng等人通过将多个Transformer模型与一个简单的线性模型进行比较，展示了该线性模型在常见基准测试中优于大多数Transformer模型。论文的一个关键洞察是，时间序列点对点地应用到Transformer架构无法捕捉时间序列数据中的局部信息和强排序性。因此，作者提出了一种更简单的替代方案，它比线性模型表现更好，并解决了在不增加内存和计算需求的情况下将长时间窗口引入Transformer的问题。
- en: The architecture of the PatchTST model
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PatchTST模型的架构
- en: 'The PatchTST model is a modification of Transformers. The following are its
    major contributions:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTST模型是对Transformers的修改。以下是其主要贡献：
- en: '**Patching**: A methodical way to include the history of the series along with
    other information, which will help in capturing long-term signals such as the
    week, month, holidays, and so on.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分片**：一种有条理的方法，用于将序列的历史信息与其他信息结合，这有助于捕捉长期信号，如周、月、节假日等。'
- en: '**Channel-independence**: A conceptual way to process multi-variate time series
    as separate, independent time series. Although I wouldn’t call this a major contribution,
    this is indeed something we need to be aware of.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通道独立性**：一种将多变量时间序列作为独立时间序列进行处理的概念方式。虽然我不认为这是一项重大贡献，但这确实是我们需要注意的地方。'
- en: Let’s take a look at these in a bit more detail.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些参数。
- en: Patching
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分片
- en: We saw some adaptations of Transformers for time series forecasting earlier
    in the chapter. All of them focused on making attention mechanisms adapt to time
    series forecasting and longer context windows. But all of them used attention
    in a pointwise manner. Let’s use a diagram to make the point clearer and introduce
    patching.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章早些时候看到了一些针对时间序列预测的Transformer改进方法。它们都集中在使注意力机制适应时间序列预测和更长的上下文窗口。但它们都以逐点的方式使用注意力。让我们通过一个图示来澄清这一点，并引入分片的概念。
- en: '![](img/B22389_16_05.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_05.png)'
- en: 'Figure 16.5: Patched vs non-patched time series inputs to Transformers'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5：输入Transformer的分片与非分片时间序列
- en: In *Figure 16.5*, we are considering a time series with 8 time steps as an example.
    On the left-hand side, we can see how all the other transformer architectures
    we have discussed handle the time series. They use some mechanism, like the Uniform
    Representation in AutoFormer, to convert a time series point into a k-dimensional
    embedding and then feed it to the Transformer architecture point by point. The
    attention mechanism for each point is calculated by looking at all the other points
    in the context window.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图16.5*中，我们以一个有8个时间步长的时间序列为例。在左侧，我们可以看到我们讨论过的所有其他Transformer架构如何处理时间序列。它们使用一些机制，如AutoFormer中的统一表示，将时间序列的每个点转换为k维嵌入，然后将其逐点输入到Transformer架构中。每个点的注意力机制是通过查看上下文窗口中所有其他点来计算的。
- en: The PatchTST paper claims that this kind of point-wise attention for time series
    doesn’t capture the locality effectively and proposes converting the time series
    into patches and feeding those patches to the Transformer instead. Patching is
    nothing but making the time series into shorter time series in a process very
    similar (or almost identical) to the sliding window operation we saw earlier in
    the book. The major difference is that this patching is done after we have already
    sampled a window from the larger time series.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTST论文声称，这种点对点的注意力机制并不能有效捕捉时间序列的局部性，提出将时间序列转换为片段，并将这些片段馈送到Transformer中。分片不过是将时间序列变成更短的时间序列，这一过程与我们之前在书中看到的滑动窗口操作非常相似（或者几乎相同）。主要的不同之处在于，这种分片是在我们已经从更大的时间序列中采样一个窗口后进行的。
- en: 'Patching is typically defined by a couple of parameters:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 分片通常由几个参数定义：
- en: Patch Length (*P*) is the length of each sub-time series or patch.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 片段长度（*P*）是每个子时间序列或片段的长度。
- en: Stride (*S*) is the length of the non-overlapping region between two consecutive
    patches. More intuitively, this is the number of time steps we move in each iteration
    of patching. This holds the exact same meaning as stride in convolutions.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步幅（*S*）是两个连续片段之间不重叠区域的长度。更直观地说，这就是每次分片迭代时我们移动的时间步数。这与卷积中的步幅具有完全相同的意义。
- en: With these two parameters fixed, a time series of length *L* would result in
    ![](img/B22389_16_055.png) patches. Here, we also pad repeated numbers of the
    last value to the end of the original sequence to ensure each patch is of the
    same size.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 固定这两个参数后，一个长度为*L*的时间序列将会产生 ![](img/B22389_16_055.png) 个片段。在这里，我们还将最后一个值的重复数字填充到原始序列的末尾，以确保每个片段的大小相同。
- en: In *Figure 16.5*, we can see that we have illustrated the patching process of
    a time series with length ![](img/B22389_16_056.png), with ![](img/B22389_16_057.png),
    and ![](img/B22389_16_058.png). Using the formula we saw just now, we can calculate
    ![](img/B22389_16_059.png). We can also see that the last value, 8, has been repeated
    at the end as a padding to make the last patch length also 4.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 16.5*中，我们可以看到我们已经说明了一个时间序列补丁处理的过程，长度为！[](img/B22389_16_056.png)，包含！[](img/B22389_16_057.png)，和！[](img/B22389_16_058.png)。使用刚才看到的公式，我们可以计算出！[](img/B22389_16_059.png)。我们还可以看到，最后一个值
    8 已经在末尾重复，作为填充，使得最后一个补丁的长度也为 4。
- en: Now, each of these patches is considered as the embedding, of sorts, and passed
    into and processed by the Transformer architecture. With this kind of input patching,
    for a given context of ![](img/B22389_16_060.png), the number of input tokens
    to the Transformer can be reduced to, approximately, ![](img/B22389_16_061.png).
    This means that the computational complexity and memory usage are also reduced
    by a factor of ![](img/B22389_16_062.png). This enables the model to process longer
    context windows with the same hardware constraints, thus possibly enhancing the
    forecasting performance of the model.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑到每个补丁可以看作一种嵌入，并被传入并由 Transformer 架构处理。通过这种输入补丁，对于给定的上下文！[](img/B22389_16_060.png)，Transformer
    的输入令牌数量大约可以减少到！[](img/B22389_16_061.png)。这意味着计算复杂度和内存使用也减少了约！[](img/B22389_16_062.png)。这使得模型在相同的硬件限制下能够处理更长的上下文窗口，从而可能提高模型的预测性能。
- en: Now, let’s look at channel independence.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下通道独立性。
- en: Channel independence
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通道独立性
- en: A multivariate time series can be thought of as a multi-channel signal. Transformer
    inputs can either be a single channel or multiple. Most of the other Transformer-based
    models capable of multivariate forecasting take the approach where the channels
    are mixed together and processed. Or, in other words, input tokens take in information
    from all time series and project it to a shared embedding space, mixing information.
    But other simpler approaches process each channel separately, and the authors
    of PatchTST bring that independence to Transformers.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量时间序列可以看作是一个多通道信号。Transformer 输入可以是单个通道或多个通道。大多数基于 Transformer 的多变量预测模型采用的是将通道混合在一起并处理的方法。换句话说，输入令牌从所有时间序列中获取信息，并将其投影到共享的嵌入空间，混合信息。而其他更简单的方法则分别处理每个通道，PatchTST
    的作者将这种独立性引入到了 Transformer 中。
- en: In practice, this is very simple. Let’s try to understand it with an example.
    Consider a dataset where there are ![](img/B22389_16_063.png) time series, making
    it a multi-variate time series. So, the input to the PatchTST would be ![](img/B22389_16_064.png),
    where ![](img/B22389_16_065.png) is the batch size and ![](img/B22389_16_066.png)
    is the length of the context window. After patching, it becomes ![](img/B22389_16_067.png),
    where ![](img/B22389_16_068.png) is the number of patches and ![](img/B22389_16_069.png)
    is the patch length. Now, to process this multi-variate signal in a channel-independent
    way, we just reshape the tensor such that each of the M time series becomes another
    sample in the batch, i.e., ![](img/B22389_16_070.png), where ![](img/B22389_16_071.png).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这非常简单。让我们通过一个例子来理解。考虑一个数据集，其中有！[](img/B22389_16_063.png)个时间序列，形成一个多变量时间序列。因此，PatchTST
    的输入将是！[](img/B22389_16_064.png)，其中！[](img/B22389_16_065.png)是批量大小，！[](img/B22389_16_066.png)是上下文窗口的长度。补丁处理后，它变成了！[](img/B22389_16_067.png)，其中！[](img/B22389_16_068.png)是补丁的数量，！[](img/B22389_16_069.png)是补丁的长度。现在，为了以通道独立的方式处理这个多变量信号，我们只需要将张量重塑，使得每个
    M 个时间序列变成批次中的一个样本，即！[](img/B22389_16_070.png)，其中！[](img/B22389_16_071.png)。
- en: While this independence brings some desirable properties to the model, it also
    means that any interaction between different time series is ignored as they are
    treated as completely independent. The model is still trained in a global model
    paradigm and will benefit from cross-learning, but any explicit interaction between
    different time series (like two time series varying together) is not captured.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种独立性为模型带来了一些理想的特性，但这也意味着不同时间序列之间的任何交互都被忽略，因为它们被视为完全独立。模型仍然是在全局模型范式下训练的，并将从跨领域学习中受益，但不同时间序列之间的任何显式交互（如两个时间序列一起变化）并未被捕捉。
- en: Apart from these major components, the architecture is pretty similar to vanilla
    Transformer architecture. Now, let’s look at how we can practically forecast using
    PatchTST.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些主要组件外，架构与标准 Transformer 架构非常相似。现在，让我们来看看如何使用 PatchTST 进行实际的预测。
- en: Forecasting with PatchTST
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PatchTST 进行预测
- en: PatchTST is implemented in NIXTLA forecasting. The same framework as used previously
    can be used here with PatchTST as well.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTST 在 NIXTLA 预测中得到了实现。与之前使用的框架相同，这里也可以使用相同的框架来实现 PatchTST。
- en: Let’s look at the initialization parameters of the implementation.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下实现的初始化参数。
- en: 'The `PatchTST` class has the following major parameters:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`PatchTST` 类具有以下主要参数：'
- en: '`encoder_layers`: This is an integer representing the number of encoder layers.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layers`：这是一个整数，表示编码器层的数量。'
- en: '`hidden_size`: This parameter sets the size of the embeddings and the encoders,
    directly influencing the model’s capacity and its ability to capture information
    from the data. This is the activation to be used in the encoder and decoder layers.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`：该参数设置嵌入和编码器的大小，直接影响模型的能力及其从数据中捕获信息的能力。这是编码器和解码器层使用的激活函数。'
- en: '`patch_len` & `stride`: These parameters define how the input sequence is divided
    into patches, which affects how the model perceives temporal dependencies. `patch_len`
    controls the length of each segment, while stride affects the overlap between
    these segments.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_len` 和 `stride`：这些参数定义了输入序列如何被划分成补丁，进而影响模型如何感知时间依赖性。`patch_len` 控制每个段的长度，而
    `stride` 则影响这些段之间的重叠部分。'
- en: '`stride`: This parameter sets the size of the embeddings and the encoders,
    directly influencing the model’s capacity and its ability to capture information
    from the data. This is the activation to be used in the encoder and decoder layers.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`：该参数设置嵌入和编码器的大小，直接影响模型的能力及其从数据中捕获信息的能力。这是编码器和解码器层使用的激活函数。'
- en: 'Regularization parameters:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化参数：
- en: '`dropout`: This is a float between 0 and 1, which determines the strength of
    the dropout in the network.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout`：这是一个介于 0 和 1 之间的浮动值，用于确定网络中的丢弃强度。'
- en: '`fc_dropout`: This is a float value that is the linear layer dropout.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fc_dropout`：这是一个浮动值，表示线性层的丢弃率。'
- en: '`head_dropout`: This is a float value that is the flatten layer dropout.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_dropout`：这是一个浮动值，表示扁平化层的丢弃率。'
- en: '`attn_dropout`: This is a float value that is the attention layer dropout.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attn_dropout`：这是一个浮动值，表示注意力层的丢弃率。'
- en: '**Notebook alert**:'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**笔记本提示**：'
- en: The complete code for training the PatchTST model can be found in the `06-PatchTST_NeuralForecast.ipynb`
    notebook in the `Chapter16` folder.
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练 PatchTST 模型的完整代码可以在 `Chapter16` 文件夹中的 `06-PatchTST_NeuralForecast.ipynb`
    笔记本中找到。
- en: Now, let’s look at another Transformer-based model that took the innovation
    from PatchTST and turned it on its head for good effect, outperforming the LTSF-Linear
    models.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一个基于 Transformer 的模型，它从 PatchTST 中获得创新，并反过来加以改进，取得了良好的效果，超越了 LTSF-Linear
    模型。
- en: iTransformer
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: iTransformer
- en: We have already talked at length about the inadequacies of Transformer architectures
    in handling multivariate time series, namely the inefficient capture of locality,
    the order-agnostic attention mechanism muddling up information across time steps,
    and so on. In 2024, Yong Liu et al. took a slightly different view of this problem
    and, in their own words, “an extreme case of patching.”
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经详细讨论了 Transformer 架构在处理多变量时间序列时的不足之处，即无法高效捕获局部性，基于顺序无关的注意力机制使得跨时间步的信息混乱等问题。在
    2024 年，Yong Liu 等人对这个问题持有稍微不同的看法，并用他们自己的话说，“一种极端的补丁处理方式。”
- en: The architecture of iTransformer
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: iTransformer 的架构
- en: They proposed that it is not that the Transformer architecture is ineffective
    for time series forecasting, but rather it is improperly used. The authors suggested
    that we flip the inputs to the Transformer architecture so that the attention
    isn’t applied across time steps but rather across variates or different series/features
    on the time series. *Figure 16.6* shows the difference clearly.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出，问题不在于 Transformer 架构在时间序列预测中无效，而是其使用不当。作者建议，我们可以翻转输入到 Transformer 架构，使得注意力机制不再跨时间步应用，而是跨不同的变量或时间序列的不同特征进行应用。*图
    16.6* 清楚地展示了这种差异。
- en: '![](img/B22389_16_06.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_06.png)'
- en: 'Figure 16.6: Transformers vs iTransformers—the difference'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.6：Transformers 与 iTransformers 的差异
- en: In vanilla Transformers, we use the input as (*Batch* x *Time steps* x *Embeddings*
    *(features)*), the attention gets applied across the time steps, and eventually,
    the Position-Wise Feed Forward Network mixes the different features into a Variate-Mixed
    Representation. But when you flip the input to (*Batch* x *Embeddings* *(features)*
    x *Timesteps*), the attention gets calculated across the variables and the Position-Wise
    Feed Forward Network mixes the time leaving variates separate in a Variate-Unmixed
    Representation.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始Transformer中，我们使用输入为（*批次* x *时间步* x *嵌入*（*特征*）），注意力机制在时间步之间应用，最终，按位置的前馈网络将不同的特征混合成一个变元混合表示。但当你将输入反转为（*批次*
    x *嵌入*（*特征*） x *时间步*）时，注意力机制会在变量之间进行计算，按位置的前馈网络会混合时间并保持变元在变元未混合表示中的独立性。
- en: This “flipping” comes with some more benefits. Now that attention isn’t calculated
    across time, we can include very large context windows with minimal computational
    and memory constraints (remember that the computational and memory complexity
    comes from the O(N²) of the attention mechanism). In fact, the paper suggests
    including the entire time series history as the context window. On the other hand,
    we need to be mindful of the number of features or concurrent time series we include
    in the model.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“反转”带来了一些额外的好处。由于注意力机制不再跨时间进行计算，我们可以在计算和内存约束较小的情况下包含非常大的上下文窗口（记住，计算和内存复杂度来自注意力机制的O(N²)）。事实上，论文建议将整个时间序列历史作为上下文窗口。另一方面，我们需要注意包含在模型中的特征数或并发时间序列的数量。
- en: 'A typical Transformer architecture has these major components:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的Transformer架构包含以下主要组件：
- en: Attention mechanism
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制
- en: Feed forward network
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络
- en: Layer normalization
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层归一化
- en: In the inverted version, we already saw that attention is applied across variates
    and the **Feed Forward Network** (**FFN**) learns generalizable representations
    of the lookback window for the final prediction of the forecast. The layer normalization
    also works out well in the inverted version. In standard Transformers, layer normalization
    is typically used to normalize the multivariate representation of each time step.
    But in the inverted version, we normalize each variate separately across time.
    This is similar to the normalization we were doing in the N-Linear model and has
    been proven to work well on non-stationary time series problems.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在倒转版本中，我们已经看到，注意力机制是跨变元应用的，**前馈网络**（**FFN**）学习了用于最终预测的回溯窗口的可泛化表示。层归一化在倒转版本中也表现得很好。在标准Transformer中，层归一化通常用于归一化每个时间步的多变量表示。但在倒转版本中，我们对每个变元在时间维度上分别进行归一化。这类似于我们在N-Linear模型中做的归一化，并且已被证明对非平稳时间序列问题有效。
- en: '**Reference check**:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research paper by Yong Liu et al. on iTransformers is cited in the *References*
    section as *18*.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: Yong Liu等人关于iTransformer的研究论文在*参考文献*部分被引用为*18*。
- en: Forecasting with iTransformer
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用iTransformer进行预测
- en: iTransformer is implemented in NIXTLA forecasting. The same framework as was
    used previously can be used here with iTransformer as well.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: iTransformer在NIXTLA预测中实现。可以使用与之前相同的框架，也可以在这里使用iTransformer。
- en: 'The `iTransformer` class has the following major parameters:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '`iTransformer`类有以下主要参数：'
- en: '`n_series`: This is an integer representing the number of time series.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_series`：这是一个整数，表示时间序列的数量。'
- en: '`e_layers`: This is an integer representing the number of encoder layers.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`e_layers`：这是一个整数，表示编码器层的数量。'
- en: '`d_layers`: This is an integer representing the number of decoder layers.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_layers`：这是一个整数，表示解码器层的数量。'
- en: '`d_ff`: This is an integer representing the number of kernels in the 1-dimensional
    convolutional layers used in the encoder and decoder layers.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_ff`：这是一个整数，表示在编码器和解码器层中使用的1维卷积层的核数。'
- en: '**Notebook alert**:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒**：'
- en: The complete code for training the iTransformer model can be found in the `07-iTransformer_NeuralForecast.ipynb`
    notebook in the `Chapter16` folder.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 训练iTransformer模型的完整代码可以在`Chapter16`文件夹中的`07-iTransformer_NeuralForecast.ipynb`笔记本中找到。
- en: Now, let’s look at one more, very successful, architecture that is well-designed
    to utilize all kinds of information in a global context.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下另一种非常成功的架构，它设计得很好，能够在全局上下文中利用各种信息。
- en: Temporal Fusion Transformer (TFT)
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间融合Transformer（TFT）
- en: TFT is a model that is thoughtfully designed from the ground up to make the
    most efficient use of all the different kinds of information in a global modeling
    context—static and dynamic variables. TFT also has interpretability at the heart
    of all design decisions. The result is a high-performing, interpretable, and global
    DL model.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: TFT是一种从全局建模角度精心设计的模型，以最有效地利用各种静态和动态变量信息为特点。TFT在所有设计决策中都注重解释性。其结果是一个高性能、可解释和全局的深度学习模型。
- en: '**Reference check**:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research paper by Lim et al. on TFT is cited in the *References* section
    as *10*.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: Lim等人关于TFT的研究论文在*参考文献*部分被引用为*10*。
- en: At first glance, the model architecture looks complicated and daunting. But
    once you peel the onion, it is quite simple and ingenious. We will take this one
    level of abstraction at a time to ease you into the full model. Along the way,
    there will be many black boxes I’m going to ask you to take for granted, but don’t
    worry—we will open every one of them as we dive deeper.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，模型架构看起来复杂且令人望而生畏。但一旦你剥开这层外皮，它其实相当简单和巧妙。我们将一级一级地深入模型，以帮助您更好地理解。在这个过程中，会有很多黑箱我会请您默认接受，但不用担心——我们会逐一打开它们，深入探讨。
- en: The architecture of TFT
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFT的架构
- en: 'Let’s establish some notations and a setting before we start. We have a dataset
    with *I* unique time series and each entity, *i*, has some static variables (*s*[i]).
    The collection of all static variables of all entities can be represented by *S*.
    We also have the context window of length *k*. Along with this, we have the time-varying
    variables, which have one distinction—for some variables, we do not have the future
    data (unknown), and for other variables, we know the future (known). Let’s denote
    all the time-varying information (the context window, known, and unknown time-varying
    variables) from the context window’s input, *x*[t-k]…*x*[t]. The known time-varying
    variables for the future are denoted using ![](img/B22389_16_072.png), where ![](img/B22389_16_045.png)
    is the forecast horizon. With these notations, we are ready to look at the first
    level of abstraction:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们先建立一些符号和设置。我们有一个包含*I*个唯一时间序列的数据集，每个实体*i*都有一些静态变量(*s*[i])。所有实体的所有静态变量的集合可以用*S*表示。我们还有长度为*k*的上下文窗口。除此之外，我们还有时间变化的变量，这些变量有一个区别——对于某些变量，我们没有未来数据（未知），而对于其他变量，我们知道未来（已知）。让我们用输入的上下文窗口*x*[t-k]…*x*[t]来表示所有时间变化信息（上下文窗口、已知和未知的时间变化变量）。未来的已知时间变化变量用![](img/B22389_16_072.png)表示，其中![](img/B22389_16_045.png)是预测的时间跨度。有了这些符号，我们准备好来看第一层抽象化了：
- en: '![Figure 16.6 – TFT – a high-level overview ](img/B22389_16_07.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![图16.6 – TFT – 高层概述](img/B22389_16_07.png)'
- en: 'Figure 16.7: TFT—a high-level overview'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7：TFT—高层概述
- en: There is a lot to unpack here. Let’s start with the static variables, *S*. First,
    the static variables are passed through a **Variable Selection Network** (**VSN**).
    The VSN does instance-wise feature selection and performs some non-linear processing
    on the inputs. This processed input is fed into a bunch of **Static Covariate
    Encoders** (**SEs**). The SE block is designed to integrate the static metadata
    in a principled way.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多内容需要分析。让我们从静态变量*S*开始。首先，静态变量通过**变量选择网络**（**VSN**）传递。VSN对实例级特征进行选择，并对输入进行一些非线性处理。处理后的输入被馈送到一组**静态协变量编码器**（**SEs**）。SE块被设计为以一种原则性的方式整合静态元数据。
- en: If you follow the arrows from the SE block in *Figure 16.6*, you will see that
    the static covariates are used in three (four distinct outputs) different places
    in the architecture. We will see how these are used in each of these places when
    we talk about them. But all these different places may be looking at different
    aspects of the static information. To allow the model this flexibility, the processed
    and variable-selected output is fed into four different **Gated Residual Networks**
    (**GRNs**), which, in turn, generate four outputs— *c*[s], *c*[e], *c*[c], and
    *c*[h]. We will explain what a GRN is later, but for now, just understand that
    it is a block capable of non-linear processing, along with a residual connection,
    which enables it to bypass the non-linear processing if needed.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你按照*图16.6*中SE模块的箭头，你会看到静态协变量在架构中三个（四个不同输出）不同的地方被使用。我们将在讨论这些模块时看到它们是如何在这些地方使用的。但这些不同的地方可能在关注静态信息的不同方面。为了让模型具有这种灵活性，处理过并经过变量选择的输出被输入到四个不同的**门控残差网络**（**GRNs**）中，这四个GRN会分别生成四个输出——*c*[s]、*c*[e]、*c*[c]和*c*[h]。我们稍后会解释什么是GRN，但现在只需要理解它是一个可以进行非线性处理的模块，并带有残差连接，这使得它在需要时可以跳过非线性处理。
- en: The past inputs, *x*[t-k]…*x*[t], and the future known inputs, ![](img/B22389_16_072.png),
    are also passed through separate VSNs and these processed outputs are fed into
    a **Locality Enhancement** (**LE**) Seq2Seq layer. We can think of LE as a way
    to encode the local context and temporal ordering into the embeddings of each
    timestep. This is similar to the positional embeddings in vanilla Transformers.
    We can also see similar attempts in the `Conv1d` layers that were used to encode
    the history in the uniform representation in the Autoformer models. We will see
    what is happening inside the LE later, but for now, just understand it captures
    the local context conditioned on other observed variables and static information.
    Let’s call the output of the block **Locality Encoded Context Vectors** ( ![](img/B22389_16_075.png),
    and ![](img/B22389_16_076.png)).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 过去的输入，*x*[t-k]…*x*[t]，以及未来已知的输入，![](img/B22389_16_072.png)，也会通过各自的VSN，并且这些处理后的输出会被输入到**局部增强**（**LE**）的Seq2Seq层。我们可以把LE看作是一种将局部上下文和时间顺序编码到每个时间步的嵌入中的方式。这类似于普通Transformer中的位置编码。我们还可以看到在`Conv1d`层中也有类似的尝试，这些层用于在Autoformer模型中对历史进行编码。我们稍后会探讨LE内部的工作原理，但现在只需要理解它捕捉了基于其他观测变量和静态信息的局部上下文。我们将这个模块的输出称为**局部编码上下文向量**（![](img/B22389_16_075.png)
    和 ![](img/B22389_16_076.png)）。
- en: The terminology, notation, and grouping of major blocks are not the same as
    in the original paper. I have changed these to make them more accessible and understandable.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 术语、符号和主要模块的分组与原始论文中不同。我对这些进行了修改，以使它们更易于理解。
- en: Now, these LE context vectors are fed into a **Temporal Fusion Decoder** (**TFD**).
    The TFD applies a slight variation of multi-head self-attention in a Transfomer
    model-like manner and produces the **Decoded Representation** (![](img/B22389_16_077.png)).
    Finally, this decoded representation is passed through a **Gated Linear Unit**
    (**GLU**) and an **Add & Norm** block that adds the LE context vectors as a residual
    connection.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些LE上下文向量被输入到**时间融合解码器**（**TFD**）中。TFD以Transformer模型中类似的方式应用了多头自注意力的微小变体，并生成**解码表示**（![](img/B22389_16_077.png)）。最后，这个解码表示通过**门控线性单元**（**GLU**）和一个**Add
    & Norm**模块，后者将LE上下文向量作为残差连接添加进去。
- en: 'A GLU is a unit that helps the model decide how much information it needs to
    allow to flow through. We can think of it as a learned information throttle that
    is widely used in **Natural Language Processing** (**NLP**) architectures. The
    formula is really simple:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: GLU是一个帮助模型决定需要允许多少信息流过的单元。我们可以将其视为一个学习的信息节流装置，它在**自然语言处理**（**NLP**）架构中广泛应用。公式非常简单：
- en: '![](img/B22389_16_078.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_078.png)'
- en: Here, *W* and *V* are learnable weight matrices, *b* and *c* are learnable biases,
    ![](img/B22389_03_004.png) is an activation function, and ![](img/B22389_16_080.png)
    is the Hadamard product operator (element-wise multiplication).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W*和*V*是可学习的权重矩阵，*b*和*c*是可学习的偏置，![](img/B22389_03_004.png)是激活函数，![](img/B22389_16_080.png)是哈达玛积运算符（元素逐个相乘）。
- en: The **Add & Norm** block is the same as in the vanilla Transformer; we discussed
    this back in *Chapter 14*, *Attention and Transformers for Time Series*.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '**Add & Norm**模块与普通Transformer中的相同；我们在*第14章*，*时间序列的注意力和Transformer*中讨论过这一点。'
- en: Now, to top it all off, we have a `Dense` layer (linear layer with bias) that
    projects the output of the `Add & Norm` block to the desired output dimensions.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了完美收尾，我们有一个`Dense`层（带偏置的线性层），它将`Add & Norm`模块的输出投影到所需的输出维度。
- en: And with that, it is time for us to step one level down in our abstraction.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是我们在抽象层次上进一步下沉的时候了。
- en: Locality Enhancement Seq2Seq layer
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局部增强Seq2Seq层
- en: 'Let’s peel back the onion and see what’s happening inside the LE Seq2Seq layer.
    Let’s start with a figure:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们剥开洋葱，看看LE Seq2Seq层内部发生了什么。让我们从一张图开始：
- en: '![Figure 16.7 – TFT – LE Seq2Seq layer ](img/B22389_16_08.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![图16.7 – TFT – LE Seq2Seq层](img/B22389_16_08.png)'
- en: 'Figure 16.8: TFT—LE Seq2Seq layer'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8：TFT—LE Seq2Seq层
- en: The LE uses a Seq2Seq architecture to capture the local context. The process
    starts with the processed past inputs. The LSTM encoder takes in these past inputs,
    *x*[t-k]…*x*[t] . *c*[h] *c*[c] from the static covariate encoder acts as the
    initial hidden states of the LSTM. The encoder processes each timestep at a time,
    producing hidden states at each time step, *H*[t-k]…*H*[t]. The last hidden states
    (context vector) are now passed on to the LSTM decoder, which processes the known
    future inputs, ![](img/B22389_16_072.png), and produces the hidden states at each
    of the future timesteps, ![](img/B22389_16_082.png). Finally, all these hidden
    states are passed through a **GLU + AddNorm** block with the residual connection
    from before the LSTM processing. The outputs are the LE context vectors ( ![](img/B22389_16_075.png)
    and ![](img/B22389_16_076.png)).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: LE使用Seq2Seq架构来捕获局部上下文。处理过程从过去的输入开始。LSTM编码器接收这些过去的输入，*x*[t-k]…*x*[t]。*c*[h] *c*[c]来自静态协变量编码器，作为LSTM的初始隐藏状态。编码器逐步处理每个时间步，产生每个时间步的隐藏状态，*H*[t-k]…*H*[t]。最后的隐藏状态（上下文向量）被传递给LSTM解码器，解码器处理已知的未来输入，![](img/B22389_16_072.png)，并在每个未来时间步生成隐藏状态，![](img/B22389_16_082.png)。最后，所有这些隐藏状态都通过一个**GLU
    + AddNorm**模块，带有来自LSTM处理前的残差连接。输出是LE上下文向量（![](img/B22389_16_075.png)和![](img/B22389_16_076.png)）。
- en: 'Now, let’s look at the next block: the TFD.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看下一个模块：TFD。
- en: Temporal fusion decoder
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间融合解码器
- en: 'Let’s start this discussion with another figure:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从另一张图开始讨论：
- en: '![Figure 16.8 – Temporal Fusion Transformer – Temporal Fusion Decoder ](img/B22389_16_09.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![图16.8 – 时间融合变换器 – 时间融合解码器](img/B22389_16_09.png)'
- en: 'Figure 16.9: Temporal Fusion Transformer—Temporal Fusion Decoder'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9：时间融合变换器—时间融合解码器
- en: The LE context vectors from both the past input and known future input are concatenated
    into a single LE context vector. Now, this can be thought of as the position-encoded
    tokens in the Transformer paradigm. The first thing the TFD does is enrich these
    encodings with static information, *c*[e], that was created from the static covariate
    encoder. This was concatenated with the embeddings. A position-wise GRN is used
    to enrich the embeddings. These enriched embeddings are now used as the query,
    key, and values for the **Masked Interpretable Multi-Head Attention** block.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 来自过去输入和已知未来输入的LE上下文向量被连接成一个单一的LE上下文向量。现在，这可以被视为在Transformer范式中的位置编码标记。TFD首先做的是用来自静态协变量编码器创建的静态信息*c*[e]来丰富这些编码。它与嵌入一起连接。一个逐位置的GRN用于丰富嵌入。这些增强的嵌入现在作为**掩码可解释多头注意力**模块的查询、键和值。
- en: The paper posits that the **Masked Interpretable Multi-Head Attention** block
    learns long-term dependencies across time steps. The local dependencies are already
    captured by the LE Seq2Seq layer in the embeddings, but the point-wise long-term
    dependencies are captured by **Masked Interpretable Multi-Head Attention**. This
    block also enhances the interpretability of the architecture. The attention weights
    that are generated in the process give us some indication of the major timesteps
    involved in the process. However, the multi-head attention has one drawback from
    the interpretability perspective.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 论文提出，**掩码可解释多头注意力**模块学习了跨时间步的长期依赖关系。局部依赖关系已经通过LE Seq2Seq层在嵌入中捕获，但逐点的长期依赖关系则通过**掩码可解释多头注意力**捕获。该模块还增强了架构的可解释性。生成的注意力权重为我们提供了一些关于过程涉及的主要时间步的信息。然而，从可解释性的角度来看，多头注意力有一个缺点。
- en: In vanilla multi-head attention, we use separate projection weights for the
    values, which means that the values for each head are different and hence the
    attention weights are not straightforward to interpret.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的多头注意力中，我们为值使用了独立的投影权重，这意味着每个头的值是不同的，因此注意力权重并不容易解释。
- en: TFT gets over this limitation by employing a *single shared weight matrix* for
    projecting the values into the attention dimension. Even with the shared value
    projection weights, because of the individual query and key projection weights,
    each head can learn different temporal patterns. In addition to this, TFT also
    employs masking to make sure information from the future is not used in operations.
    We discussed this type of causal masking in *Chapter 14*, *Attention and Transformers
    for Time Series*. With these two modifications, TFT names this layer **Masked
    Interpretable Multi-Head Attention**.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: TFT通过使用一个*共享权重矩阵*将值投影到注意力维度上，克服了这一限制。即使有共享的值投影权重，由于每个查询和键的投影权重不同，每个头部仍然可以学习不同的时间模式。除了这一点，TFT还使用了掩蔽机制，确保在操作中不会使用来自未来的信息。我们在*第14章*，*时间序列的注意力和变换器*中讨论了这种因果掩蔽。通过这两项修改，TFT将这一层命名为**掩蔽可解释的多头注意力**。
- en: And with that, it’s time to open the last and most granular level of abstraction
    we have been using.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 到这里，我们可以打开我们所使用的最后一个也是最精细的抽象层级了。
- en: Gated residual networks
  id: totrans-420
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 门控残差网络
- en: We have been talking about GRNs for some time now; so far, we have just taken
    them at face value. Let’s understand what is happening inside a GRN—one of the
    most basic building blocks of a TFT.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论GRN有一段时间了；到目前为止，我们只是从表面上看待它们。现在让我们来理解一下GRN内部发生了什么——这是TFT中最基础的构建块之一。
- en: 'Let’s look at a schematic diagram of a GRN to understand it better:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个GRN的示意图，以更好地理解它：
- en: '![Figure 16.9 – TFT – GRN (left) and VSN (right) ](img/B22389_16_10.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![图16.9 – TFT – GRN（左）和VSN（右）](img/B22389_16_10.png)'
- en: 'Figure 16.10: TFT—GRN (left) and VSN (right)'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10：TFT—GRN（左）和VSN（右）
- en: 'The GRN takes in two inputs: the primary input, *a*, and the external context,
    *c*. The context, *c*, is an optional input and is treated as zero if it’s not
    present. First, both the inputs, *a* and *c*, are transformed by separate dense
    layers and a subsequent activation function—the **Exponential Linear Unit** (**ELU**)
    ([https://pytorch.org/docs/stable/generated/torch.nn.ELU.html](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html)).'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: GRN接收两个输入：主输入*a*和外部上下文*c*。上下文*c*是一个可选输入，如果不存在，则视为零。首先，两个输入*a*和*c*通过单独的密集层和随后的激活函数进行变换——**指数线性单元**（**ELU**）（[https://pytorch.org/docs/stable/generated/torch.nn.ELU.html](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html)）。
- en: Now, the transformed *a* and *c* inputs are added together and then transformed
    again using another `Dense` layer. Finally, this is passed through a **GLU+Add
    & Norm** layer with residual connections from the original *a*. This structure
    bakes in enough non-linearity to learn complex interactions between the inputs,
    but at the same time lets the model ignore those non-linearities through a residual
    connection. Therefore, such a block allows the model to scale the computation
    required up or down based on the data.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，变换后的*a*和*c*输入被加在一起，然后通过另一个`Dense`层再次进行变换。最后，这通过一个带有来自原始*a*的残差连接的**GLU+加法与归一化**层进行处理。这个结构结合了足够的非线性，能够学习输入之间的复杂交互，但同时通过残差连接让模型能够忽略这些非线性。因此，这样的模块可以让模型根据数据规模调整所需的计算量。
- en: Variable selection networks
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变量选择网络
- en: The last building block of the TFT is the VSN. VSNs enable TFT to do instance-wise
    variable selection. Most real-world time series datasets have many variables that
    do not have a lot of predictive power, so being able to select the ones that do
    have predictive power automatically will help the model pick out relevant patterns.
    *Figure 16.9* (right) shows this VSN.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: TFT的最后一个构建块是VSN。VSN使得TFT能够进行实例级的变量选择。大多数真实世界的时间序列数据集包含许多预测能力较弱的变量，因此能够自动选择那些具有预测能力的变量，将有助于模型挑选出相关的模式。*图16.9*（右）展示了这个VSN。
- en: These additional variables can be categorical or continuous. TFT uses entity
    embeddings to convert the categorical features into numerical vectors of the dimension
    that we desire (*d*[model]). We talked about this in *Chapter 15*, *Strategies
    for Global Deep Learning Forecasting Models*. The continuous features are linearly
    transformed (independently) into the same dimension, *d*[model]. This gives us
    the transformed inputs, ![](img/B22389_16_085.png), where *m* is the number of
    features and *t* is the timestep. We can concatenate all these embeddings (flatten
    them) and that flattened representation can be represented as ![](img/B22389_16_086.png).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 这些附加变量可以是分类的也可以是连续的。TFT使用实体嵌入将分类特征转换为我们所需的数值向量维度（*d*[model]）。我们在*第15章*《全球深度学习预测模型策略》中讨论过这个问题。连续特征则被线性转换（独立地）为相同维度的向量，*d*[model]。这为我们提供了变换后的输入，![](img/B22389_16_085.png)，其中*m*是特征数量，*t*是时间步长。我们可以将所有这些嵌入（扁平化）拼接在一起，得到的扁平化表示可以表示为！[](img/B22389_16_086.png)。
- en: 'Now, there are two parallel streams in which these embeddings are processed—one
    for non-linear processing of the embeddings and another to do feature selection.
    Each of these embeddings is processed by separate GRNs (but shared for all timesteps)
    to give us the non-linearly processed ones, ![](img/B22389_16_085.png). In another
    stream, the VSN processes the flattened representation, ![](img/B22389_16_086.png),
    along with optional context information, *c*, and processes it through a GRN with
    a softmax activation. This gives us a weight, *v*[t], which is a vector of length
    *m*. This *v*[t] is now used in a weighted sum of all the non-linearly processed
    feature embeddings, ![](img/B22389_16_085.png), which is calculated as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些嵌入被处理的方式有两个平行流：一个用于对嵌入进行非线性处理，另一个用于特征选择。每个嵌入通过独立的GRN进行处理（但所有时间步共享），以得到非线性处理后的嵌入，![](img/B22389_16_085.png)。在另一个流中，VSN处理扁平化表示，![](img/B22389_16_086.png)，以及可选的上下文信息*c*，并通过带有softmax激活函数的GRN进行处理。这为我们提供了一个权重向量*v*[t]，它的长度为*m*。现在，*v*[t]被用于对所有非线性处理后的特征嵌入进行加权求和，![](img/B22389_16_085.png)，该加权和计算如下：
- en: '![](img/B22389_16_090.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_090.png)'
- en: Forecasting with TFT
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TFT进行预测
- en: '*TFT* is implemented in NIXTLA forecasting. We can use the same framework we
    were working with for NBEATS and extend it to train *TFT* on our data. Additionally,
    NIXTLA supports exogenous variables, the same way N-BEATSx handles exogenous variables.
    First, let’s look at the initialization parameters of the implementation.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '*TFT*在NIXTLA预报中得到了实现。我们可以使用与NBEATS相同的框架，并将其扩展以在我们的数据上训练*TFT*。此外，NIXTLA支持外生变量，就像N-BEATSx处理外生变量一样。首先，让我们看一下实现的初始化参数。'
- en: 'The `TFT` class in NIXTLA has the following major parameters:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: NIXTLA中的`TFT`类有以下主要参数：
- en: '`hidden_size`: This is an integer representing the hidden dimension across
    the model. This is the dimension in which all the GRNs work, the VSN, the LSTM
    hidden sizes, the self-attention hidden sizes, and so on. Arguably, this is the
    most important hyperparameter in the model.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`：这是一个整数，表示模型中的隐藏维度。在这个维度中，所有的GRN、VSN、LSTM隐藏层、self-attention隐藏层等都会进行计算。可以说，这是模型中最重要的超参数。'
- en: '`n_head`: This is an integer representing the number of attention heads.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_head`：这是一个整数，表示注意力头的数量。'
- en: '`dropout`: This is a float between 0 and 1, which determines the strength of
    the dropout in the Variable Selection Networks.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout`：这是一个介于0和1之间的浮动值，决定变量选择网络中的dropout强度。'
- en: '`attn_dropout`: This is a float between 0 and 1, which determines the strength
    of the dropout in the decoder’s attention layer.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attn_dropout`：这是一个介于0和1之间的浮动值，决定解码器注意力层中dropout的强度。'
- en: '**Notebook alert**:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒**：'
- en: The complete code for training TFT can be found in the `08-TFT_NeuralForecast.ipynb`
    notebook in the `Chapter16` folder.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的TFT训练代码可以在`Chapter16`文件夹中的`08-TFT_NeuralForecast.ipynb`笔记本中找到。
- en: Interpreting TFT
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释TFT
- en: TFT approaches interpretability from a slightly different perspective than N-BEATS.
    While N-BEATS gives us a decomposed output for interpretability, TFT gives us
    visibility into how the model has interpreted the variables it has used. On account
    of the VSNs, we have ready access to feature weights. Like the feature importance
    we get from tree-based models, TFT gives us access to similar scores. Because
    of the self-attention layer, the attention weights can also be interpreted to
    help us understand which time steps hold a large enough weightage in the attention
    mechanism.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: TFT 从一个稍微不同的角度来探讨可解释性，与 N-BEATS 不同。N-BEATS 为我们提供了解构输出以实现可解释性，而 TFT 则让我们看清模型是如何解释其使用的变量的。由于
    VSN，我们可以轻松访问特征权重。类似于树模型中的特征重要性，TFT 也能提供类似的评分。由于自注意力层，注意力权重也可以被解释，帮助我们理解哪些时间步骤在注意力机制中占据较大权重。
- en: 'PyTorch Forecasting makes this possible by performing a few steps. First, we
    get the raw predictions using `mode="raw"` in the `predict` function. Then, we
    use those raw predictions in the `interpret_output` function. There is a parameter
    called `reduction` in the `interpret_output` function that decides how to aggregate
    the weights across different instances. We know that TFT does instance-wise feature
    selection in VSNs and attention is also done instance-wise. `''mean''` is a good
    option for looking at the global interpretability:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Forecasting 通过执行几个步骤使这一切成为可能。首先，我们通过 `predict` 函数中的 `mode="raw"` 获取原始预测。然后，我们将这些原始预测用于
    `interpret_output` 函数。`interpret_output` 函数中有一个名为 `reduction` 的参数，用于决定如何聚合不同实例的权重。我们知道
    TFT 在 VSN 中进行实例级的特征选择，且注意力机制也是按实例进行的。`'mean'` 是查看全局可解释性的一个不错选择：
- en: '[PRE8]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This `interpretation` variable is a dictionary with weights for different aspects
    of the model, such as `attention`, `static_variables`, `encoder_variables`, and
    `decoder_variables`. PyTorch Forecasting also provides us with an easy way to
    visualize this importance:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `interpretation` 变量是一个字典，包含模型中不同方面的权重，例如 `attention`、`static_variables`、`encoder_variables`
    和 `decoder_variables`。PyTorch Forecasting 还为我们提供了一个简单的方式来可视化这些重要性：
- en: '[PRE9]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This generates four plots:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成四个图表：
- en: '![Figure 16.10 – Interpreting TFT ](img/B22389_16_11.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.10 – 解释 TFT](img/B22389_16_11.png)'
- en: 'Figure 16.11: Interpreting TFT'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.11：解释 TFT
- en: We can also look at each instance and plot similar visualizations for each prediction
    we make. All we need to do is use `reduction="none"` and then plot it ourselves.
    The accompanying notebook explores how to do that and more.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看每个实例，并为每个预测绘制类似的可视化图表。我们只需要使用 `reduction="none"`，然后自行绘制即可。附带的笔记本探讨了如何实现这一点及更多内容。
- en: Now, let’s switch tracks and look at some models that proved that simple MLPs
    are also more than capable of matching or beating Transformer-based models.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们换个角度，看看一些模型，这些模型证明了简单的 MLP 同样能够匹敌甚至超越基于 Transformer 的模型。
- en: TSMixer
  id: totrans-452
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TSMixer
- en: While the Transformer-based models were forging ahead with steam, a parallel
    track of research started by using **Multi-Layer Perceptrons** (**MLPs**) instead
    of Transformers as the key learning unit. The trend kicked off in 2021 when MLP-Mixer
    showed that one can attain state-of-the-art performance in vision problems by
    using just MLPs, replacing Convolutional Neural Networks. And so, similar mixer
    architectures using MLPs as the key learning component started popping up in all
    domains. In 2023, Si-An Chen et al. from Google brought mixing MLPs into time
    series forecasting.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 当基于 Transformer 的模型如火如荼地推进时，一条平行的研究轨迹开始使用**多层感知机**（**MLPs**）代替 Transformer 作为关键学习单元。这一趋势始于
    2021 年，当时 MLP-Mixer 展示了仅使用 MLP 就能在视觉问题上达到最先进的性能，取代了卷积神经网络。于是，类似的 MLP 混合架构开始在各个领域出现。2023
    年，Google 的 Si-An Chen 等人将 MLP 混合引入了时间序列预测。
- en: '**Reference check**:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research paper by Si-An et al. on TSMixer is cited in the *References* section
    as *19*.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: Si-An 等人关于 TSMixer 的研究论文在*参考文献*部分被引用为*19*。
- en: The architecture of the TSMixer model
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TSMixer 模型的架构
- en: TSMixer really took inspiration from the Transformer model but tried to replicate
    similar processes with an MLP. Let’s use *Figure 16.10* to understand the similarities
    and differences.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: TSMixer 确实从 Transformer 模型中汲取了灵感，但它试图通过 MLP 模拟类似的过程。让我们通过*图 16.10*来理解其相似性与差异。
- en: '![](img/B22389_16_12.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_12.png)'
- en: 'Figure 16.12: Transformer vs TSMixer'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.12：Transformer 与 TSMixer 对比
- en: If you look at the Transformer block, we can see that there is a Multi-Head
    Attention that looks across timesteps, and “mixes” them together using attention.
    Then those outputs are passed on to the Position-Wise Feed Forward networks, which
    “mix” the different features together. Drawing inspiration from these, the TSMixer
    also has a Time-Mixing component and a Feature-Mixing component in a Mixer block.
    There is a Temporal Projection that takes the output from the Mixer block and
    projects it to the output space.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观察Transformer块，我们可以看到有一个多头注意力机制，它会跨时间步长进行观察，并利用注意力机制“混合”它们。然后，这些输出会传递给位置逐步前馈网络，它们将不同的特征混合在一起。从这些灵感中，TSMixer也在混合器块中包含了一个时间混合组件和一个特征混合组件。时间投影层将混合器块的输出投影到输出空间。
- en: Let’s take this one level of explanation at a time. *Figure 16.11* shows the
    entire architecture.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐层解释。*图 16.11*展示了整个架构。
- en: '![](img/B22389_16_13.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_13.png)'
- en: 'Figure 16.13: TSMixer Architecture'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.13：TSMixer架构
- en: The input, which is a multivariate time series is fed into N mixer layers, which
    process it sequentially and the output from the final mixer layer is fed into
    the temporal projection layer, which converts the learned representation into
    the actual forecast. Although the figure and the paper refer to “features,” they
    aren’t features in the way we have been discussing in this book. Here, “features”
    means other time series in a multi-variate setting.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是一个多变量时间序列，它被送入N个混合器层，逐个处理，最终混合器层的输出被送入时间投影层，时间投影层将学到的表示转换为实际的预测。尽管图表和论文中提到了“特征”，但它们并不是我们在本书中讨论的特征。这里的“特征”指的是多变量设置中的其他时间序列。
- en: Now, let’s double-click on Mixer Layer and see the Time Mixing and Feature Mixing
    inside a block.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入研究混合器层，看看时间混合和特征混合是如何在一个块内工作的。
- en: Mixer Layer
  id: totrans-466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合器层
- en: '![](img/B22389_16_14.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_14.png)'
- en: 'Figure 16.14: TSMixer—Mixer block'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.14：TSMixer—混合器块
- en: The input is of the form (*Batch Size* x *Features* x *Time Steps*) and is first
    passed through the Time Mixing block. The input is first transposed into the form
    (*Batch Size* x *Time Steps* x *Features*) such that the weights in the Time Mixing
    MLP are mixing timesteps. Now, this “time-mixed” output is passed to the Feature
    Mixing MLP, which uses its weights to mix the different features to give the final
    learned representation. Batch Normalization layers and residual connections are
    added in between to make the model more robust and learn deeper and smoother connections.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的形式为（*批量大小* x *特征* x *时间步长*），首先通过时间混合块。输入首先被转置为（*批量大小* x *时间步长* x *特征*）的形式，这样，时间混合MLP中的权重就能混合时间步长。现在，这个“时间混合”的输出被传递到特征混合MLP，后者使用其权重来混合不同的特征，得到最终的学习表示。批量归一化层和残差连接被加入其中，以使模型更加健壮，并学习更深层次和更平滑的连接。
- en: 'Given an input matrix ![](img/B22389_16_091.png), Time Mixing can be represented
    mathematically as:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入矩阵 ![](img/B22389_16_091.png)，时间混合可以用数学公式表示为：
- en: '![](img/B22389_16_092.png)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_092.png)'
- en: Feature mixing is actually a two-layer MLP, one projecting to a hidden dimension,
    *H*[inner], and the next projecting from *H*[inner] to the output dimension, *H*.
    If not specified explicitly, this defaults to the original number of features
    (or number of time series), *C*.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 特征混合实际上是一个两层的MLP，其中一层将数据投影到隐藏维度*H*[inner]，下一层从*H*[inner]投影到输出维度*H*。如果没有明确指定，这将默认使用原始特征数量（或时间序列数量）*C*。
- en: '![](img/B22389_16_093.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_093.png)'
- en: '![](img/B22389_16_094.png)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_094.png)'
- en: 'Therefore the entire Mixer layer can be represented as:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，整个混合器层可以表示为：
- en: '![](img/B22389_16_095.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_095.png)'
- en: Now, this output is passed through the Temporal Projection layer to get the
    forecast.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个输出被传递到时间投影层以得到预测。
- en: Temporal Projection Layer
  id: totrans-478
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间投影层
- en: '![](img/B22389_16_15.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_15.png)'
- en: 'Figure 16.15: TSMixer—Temporal Projection Layer'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.15：TSMixer—时间投影层
- en: The temporal projection layer is nothing but a fully connected layer applied
    to the time domain. This is identical to the simple linear model we saw earlier,
    where we apply a fully connected layer to the input context to get the forecast.
    Instead of applying the layer to the input, TSMixer applies this layer to the
    “mixed” output from the Mixer Layer.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 时间投影层只是一个应用于时间域的全连接层。这与我们之前看到的简单线性模型相同，在这个模型中，我们将全连接层应用于输入上下文以得到预测。TSMixer并不是将该层应用于输入，而是将其应用于来自混合器层的“混合”输出。
- en: The output from the previous layer is in the form (*Batch Size* x *Time Steps*
    x *Features*). This is transposed to (*Batch Size* x *Features* x *Time Steps*)
    and then passed through a fully connected layer, which projects the input into
    (*Batch Size* x *Features* x *Forecast Horizon*) to get the final forecast.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 前一层的输出形式为（*批量大小* x *时间步长* x *特征*）。它被转置为（*批量大小* x *特征* x *时间步长*），然后通过一个全连接层，该层将输入投影到（*批量大小*
    x *特征* x *预测视野*），从而得到最终的预测结果。
- en: 'Given ![](img/B22389_16_096.png) as the output of the *k*-th Mixer Layer and
    forecast horizon, *T*:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 给定![](img/B22389_16_096.png)作为第*k*个Mixer Layer的输出和预测视野，*T*：
- en: '![](img/B22389_16_097.png)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_097.png)'
- en: But how do we include additional features? Many time series problems have static
    features and dynamic (future-looking) features, which adds quite a bit of information
    to the problem. The architecture we have discussed so far doesn’t let you include
    them. For this reason, the authors proposed a slight tweak to include this additional
    information, TSMixerx.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何加入额外的特征呢？许多时间序列问题有静态特征和动态（面向未来的）特征，这为问题增加了相当多的信息。到目前为止，我们讨论的架构并未让你包含这些信息。为此，作者提出了一个小的调整来包括这些额外信息，即TSMixerx。
- en: TSMixerx—TSMixer with auxiliary information
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TSMixerx—带辅助信息的TSMixer
- en: Following the same notation as before, consider we have the input time series
    (or collection of time series), ![](img/B22389_16_098.png). Now, we would have
    a few historical features, ![](img/B22389_16_099.png), some future-looking features,
    ![](img/B22389_16_100.png), and some static features, ![](img/B22389_16_101.png).
    To effectively include all this additional information, the authors defined another
    unit of learning called the Conditional Feature Mixing layer and then used it
    in a way that assimilates all the information.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 按照之前的符号约定，考虑我们有输入的时间序列（或时间序列集合），![](img/B22389_16_098.png)。现在，我们将有一些历史特征，![](img/B22389_16_099.png)，一些面向未来的特征，![](img/B22389_16_100.png)，以及一些静态特征，![](img/B22389_16_101.png)。为了有效地包含所有这些额外信息，作者定义了另一个学习单元，称为条件特征混合层（Conditional
    Feature Mixing layer），并以一种能够同化所有信息的方式使用它。
- en: The **Conditional Feature Mixing** (**CFM**) layer is almost identical to the
    Feature Mixing layer, except for an additional layer to process the static information
    along with the features. The static information is first repeated across time
    steps and projected into the output dimension using a linear layer. This is then
    concatenated with the input features and the concatenated input is then “mixed”
    together and projected to the output dimension.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '**条件特征混合**（**CFM**）层几乎与特征混合层相同，不同之处在于增加了一个处理静态信息和特征的额外层。静态信息首先被重复跨越时间步长，并通过线性层投影到输出维度。然后，它与输入特征连接，连接后的输入再被“混合”在一起，并投影到输出维度。'
- en: 'Mathematically, it can be represented as:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，它可以表示为：
- en: '![](img/B22389_16_102.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_102.png)'
- en: '![](img/B22389_16_103.png)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_103.png)'
- en: where ![](img/B22389_16_104.png) means concatenation and *Expand* means repeating
    the static information for all the time steps.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/B22389_16_104.png)表示连接操作，而*Expand*表示将静态信息重复到所有时间步长中。
- en: Now, let’s see how the CFM layer is used in the overall TSMixerx architecture.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看CFM层如何在整体TSMixerx架构中使用。
- en: '![](img/B22389_16_16.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_16.png)'
- en: 'Figure 16.16: TSMixerx—architecture using exogenous variables'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.16：TSMixerx—使用外生变量的架构
- en: 'First off, we have *X* and ![](img/B22389_16_105.png), which have *L* timesteps,
    which is the length of the context window. Therefore, we concatenate both and
    use a simple temporal projection layer to project the combined tensor into ![](img/B22389_16_106.png),
    where *T* is the length of the forecast horizon. This is also the length of the
    future-looking features, *Z*. Now, we combine this with the static information
    using a CFM layer, which projects them into a hidden dimension, *H*. Formally,
    this step is represented as:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有*X*和![](img/B22389_16_105.png)，它们有*L*个时间步长，即上下文窗口的长度。因此，我们将它们连接，并使用一个简单的时间投影层将组合张量投影到![](img/B22389_16_106.png)，其中*T*是预测视野的长度，这也是面向未来特征*Z*的长度。接着，我们使用CFM层将其与静态信息结合，并将它们投影到一个隐藏维度，*H*。形式上，这一步表示为：
- en: '![](img/B22389_16_107.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_107.png)'
- en: Now, we want to conditionally mix the future-looking features, *Z*, as well
    with the static information, *S*. Therefore, we use a CFM layer to do that and
    project this combined information into a hidden dimension, *H*.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们希望有条件地混合面向未来的特征，*Z*，以及静态信息，*S*。因此，我们使用CFM层来实现这一点，并将这些组合信息投影到一个隐藏维度，*H*。
- en: '![](img/B22389_16_108.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_108.png)'
- en: At this point, we have ![](img/B22389_16_109.png) and ![](img/B22389_16_110.png),
    which are both in ![](img/B22389_16_111.png) dimensions. So, we use another CFM
    layer to mix these features further conditioned on ![](img/B22389_16_062.png).
    This gives us the first feature mixed latent representation, ![](img/B22389_16_113.png).
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们有！[](img/B22389_16_109.png) 和 ![](img/B22389_16_110.png)，它们都在！[](img/B22389_16_111.png)维度下。因此，我们使用另一个CFM层进一步混合这些特征，并根据！[](img/B22389_16_062.png)进行条件处理。这给了我们第一个混合特征的潜在表示，！[](img/B22389_16_113.png)。
- en: '![](img/B22389_16_114.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_114.png)'
- en: Now, this latent representation is passed through ![](img/B22389_16_115.png)
    subsequent CFMs (similar to regular TSMixer architecture), where ![](img/B22389_16_116.png)
    is the total number of Mixer layers, to give us ![](img/B22389_16_117.png), the
    final latent representation. There are ![](img/B22389_16_115.png) layers because
    the first Mixer layer is already defined and is different from the rest in just
    the input dimensions.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个潜在表示通过！[](img/B22389_16_115.png)后续的CFM（类似于常规的TSMixer架构），其中！[](img/B22389_16_116.png)是总的Mixer层数，最终得到！[](img/B22389_16_117.png)，即最终的潜在表示。共有！[](img/B22389_16_115.png)层，因为第一个Mixer层已经定义并且在输入维度上与其他层不同。
- en: '![](img/B22389_16_119.png)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_119.png)'
- en: Now, we can use a simple linear layer to project this output into the desired
    output dimension. If it is a point prediction for a single time series, we can
    project it to ![](img/B22389_16_120.png). In case we are predicting the M time
    series, then we can project it to ![](img/B22389_16_121.png).
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用一个简单的线性层将这个输出投影到所需的输出维度。如果它是单一时间序列的点预测，我们可以将其投影到！[](img/B22389_16_120.png)。如果我们预测的是M个时间序列，那么我们可以将其投影到！[](img/B22389_16_121.png)。
- en: Forecasting with TSMixer and TSMixerx
  id: totrans-505
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TSMixer和TSMixerx进行预测
- en: TSMixer is implemented in NIXTLA forecasting with the same framework we have
    seen in the prior models.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: TSMixer在NIXTLA预测中实现，使用的是我们在之前模型中看到的相同框架。
- en: Let’s look at the initialization parameters of the implementation.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下实现的初始化参数。
- en: 'The `TSMixer` class has the following major parameters:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '`TSMixer`类具有以下主要参数：'
- en: '`n_series`: This is an integer value indicating the number of time series.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_series`：这是一个整数值，表示时间序列的数量。'
- en: '`n_block`: This is an integer value indicating the number of mixing layers
    used in the model.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_block`：这是一个整数值，表示模型中使用的混合层数量。'
- en: '`ff_dim`: This is an integer value indicating the number of units to use for
    the second feed forward layer.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ff_dim`：这是一个整数值，表示第二个前馈层中要使用的单元数量。'
- en: '`revin`: This is a Boolean value that, if True, uses Reversible instance Normalization
    to process inputs and outputs (ICLR 2022 paper: [https://openreview.net/forum?id=cGDAkQo1C0p](https://openreview.net/forum?id=cGDAkQo1C0p)).'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revin`：这是一个布尔值，如果为True，则使用可逆实例归一化来处理输入和输出（ICLR 2022论文：[https://openreview.net/forum?id=cGDAkQo1C0p](https://openreview.net/forum?id=cGDAkQo1C0p)）。'
- en: 'Similar to NBEATX, there is a `TSMixerx` class that can take exogenous information.
    To forecast with exogenous information, you would add appropriate information
    into the parameters below:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于NBEATX，`TSMixerx`类可以处理外生信息。要使用外生信息进行预测，您需要将适当的信息添加到下面的参数中：
- en: '`futr_exog_list`: This takes a list of future exogenous columns.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`futr_exog_list`：这是一个未来外生列的列表。'
- en: '`hist_exog_list`: This takes a list of historical exogenous columns.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hist_exog_list`：这是一个历史外生列的列表。'
- en: '`stag_exog_list`: This is a list of exogenous columns.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stag_exog_list`：这是一个外生列的列表。'
- en: '**Notebook alert**:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒**：'
- en: The complete code for training the TSMixer model can be found in the `09-TSMixer_NeuralForecast.ipynb`
    notebook in the `Chapter16` folder.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练TSMixer模型的完整代码可以在`Chapter16`文件夹中的`09-TSMixer_NeuralForecast.ipynb`笔记本中找到。
- en: Let’s now look at one more MLP-based architecture which has shown that it performs
    better than PatchTST and the Linear family of models we saw earlier.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下另一种基于MLP的架构，它已经证明比我们之前看到的PatchTST和线性系列模型表现得更好。
- en: Time Series Dense Encoder (TiDE)
  id: totrans-520
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列稠密编码器（TiDE）
- en: We saw earlier in the chapter that a linear family of models outperformed quite
    a lot of Transformer models. In 2023, Das et al. from Google proposed a model
    that extends that idea into non-linearity. They argued that the linear models
    will fall short where there are inherent non-linearities in the dependence between
    the future and the past. The inclusion of covariates compounds this problem.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章早些时候看到，线性模型家族的表现超越了许多Transformer模型。2023年，Google的Das等人提出了一种将这一思想扩展到非线性的模型。他们认为，当未来与过去之间存在内在的非线性依赖关系时，线性模型会失效。而协变量的加入加剧了这个问题。
- en: '**Reference check**:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research paper by Das et al. on TiDE is cited in the *References* section
    as *20*.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: Das等人关于TiDE的研究论文在*参考文献*部分被引用为*20*。
- en: Therefore, they introduced a simple and efficient **Multi-Layer Perceptron**
    (**MLP**) based architecture for long-term time series forecasting. The model
    essentially encodes the past of the time series, along with the covariates using
    dense MLPs, and then decodes this latent representation into a forecast. The model
    assumes channel independence (similar to PatchTST) and considers different related
    time series in a multivariate problem as separate time series.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，他们提出了一种简单高效的基于**多层感知机**（**MLP**）的架构，用于长期时间序列预测。该模型本质上通过密集的MLP编码时间序列的过去及其协变量，然后将这个潜在表示解码为一个预测。该模型假设通道独立性（类似于PatchTST），并将多元问题中不同相关的时间序列视为单独的时间序列。
- en: The architecture of the TiDE model
  id: totrans-525
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TiDE模型的架构
- en: The architecture has two main components—an encoder and a decoder. But all through
    the architecture, one learning component they call a Residual block is reused.
    Let’s take a look at the Residual block first.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构有两个主要组件——编码器和解码器。但在整个架构中，他们称之为残差块的一个学习组件被重复使用。让我们先看看残差块。
- en: Residual block
  id: totrans-527
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 残差块
- en: The residual block is an MLP with a ReLU and a subsequent linear projection
    enabling a residual connection. *Figure 16.14* shows a residual block.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 残差块是一个带有ReLU和后续线性投影的MLP，允许残差连接。*图16.14*显示了一个残差块。
- en: '![](img/B22389_16_17.png)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_17.png)'
- en: 'Figure 16.17: TiDE: residual block'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.17：TiDE：残差块
- en: We define the layer by setting a hidden dimension and an output dimension. The
    first Dense layer transforms the input to the hidden dimension and then ReLU non-linearity
    is applied to the output. This output is then linearly projected to the output
    dimension and a dropout layer is stacked on top of that. The residual connection
    is then added to the output by projecting the input into the output dimension
    using another linear projection. And to top it all off, the output is passed through
    Layer Normalization.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过设置隐藏维度和输出维度来定义该层。第一层Dense将输入转换为隐藏维度，然后对输出应用ReLU非线性。然后，该输出被线性投影到输出维度，并在其上堆叠一个丢弃层。残差连接随后通过将输入投影到输出维度并使用另一个线性投影来添加到输出中。最后，输出通过层归一化进行处理。
- en: 'Let ![](img/B22389_16_122.png) be the input to the block, *h* be the hidden
    dimension, and *o* be the output dimension. Then, the Residual block can be represented
    as:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 设![](img/B22389_16_122.png)为该块的输入，*h*为隐藏维度，*o*为输出维度。那么，残差块可以表示为：
- en: '![](img/B22389_16_123.png)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_123.png)'
- en: Let’s establish some notation to help us with the rest of the explanation. There
    are *N* time series in the dataset, *L* is the length of the lookback window,
    and *H* is the length of the forecast. So, the lookback of the i^(th) time series
    can be represented as ![](img/B22389_16_124.png), and its forecast is ![](img/B22389_16_125.png).
    The r-dimensional dynamic covariates at time ![](img/B22389_16_126.png) are represented
    by ![](img/B22389_16_127.png). Static features of the i^(th) time series are ![](img/B22389_16_128.png).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们建立一些符号，以帮助我们理解接下来的解释。数据集中有*N*个时间序列，*L*是回溯窗口的长度，*H*是预测的长度。因此，第i^(th)个时间序列的回溯可以表示为![](img/B22389_16_124.png)，而它的预测是![](img/B22389_16_125.png)。在时间![](img/B22389_16_126.png)的r维动态协变量表示为![](img/B22389_16_127.png)。第i^(th)个时间序列的静态特征是![](img/B22389_16_128.png)。
- en: Now, let’s look at the larger architecture in *Figure 16.15*.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下*图16.15*中的更大架构。
- en: '![](img/B22389_16_18.png)'
  id: totrans-536
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_18.png)'
- en: 'Figure 16.18: TiDE: overall architecture'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.18：TiDE：整体架构
- en: Encoder
  id: totrans-538
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器
- en: The encoder is tasked with mapping the lookback window and the corresponding
    covariates into a dense latent representation. The first step is a **Linear Projection**
    of the dynamic covariates, ![](img/B22389_16_129.png), into ![](img/B22389_16_130.png),
    where ![](img/B22389_16_131.png), called *temporal width*, is much smaller than
    *r*. We use the Residual block for this projection.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的任务是将回溯窗口及其相应的协变量映射到一个稠密的潜在表示。第一步是对动态协变量进行**线性投影**，![](img/B22389_16_129.png)，映射到![](img/B22389_16_130.png)，其中![](img/B22389_16_131.png)，称为*时间宽度*，远小于*r*。我们使用残差块来完成这个投影。
- en: '![](img/B22389_16_132.png)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_132.png)'
- en: From a programmatic perspective (where *B* is the batch size), if the input
    dimension of the dynamic covariates is ![](img/B22389_16_133.png), we project
    it to ![](img/B22389_16_134.png).
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 从程序的角度来看（其中*B*是批量大小），如果动态协变量的输入维度是![](img/B22389_16_133.png)，我们将其投影到![](img/B22389_16_134.png)。
- en: 'This is done so that when we flatten the time series and its covariates before
    feeding it through the encoder, the dimension of the resulting tensor doesn’t
    explode. That brings us to the next step, which is the flattening of the tensors
    and concatenating them. The flattening and concatenation operation looks something
    like this:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是为了当我们将时间序列及其协变量展平后输入编码器时，结果张量的维度不会爆炸。这就引出了下一步，即张量的展平和拼接操作。展平和拼接操作类似于这样：
- en: 'Lookback window: ![](img/B22389_16_135.png)'
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回溯窗口：![](img/B22389_16_135.png)
- en: 'Dynamic covariates: ![](img/B22389_16_136.png)'
  id: totrans-544
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动态协变量：![](img/B22389_16_136.png)
- en: 'Static information: ![](img/B22389_16_137.png)'
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 静态信息：![](img/B22389_16_137.png)
- en: 'Concatenated representation: ![](img/B22389_16_138.png)'
  id: totrans-546
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拼接表示：![](img/B22389_16_138.png)
- en: Now, this concatenated representation is passed through a stack of *n*[e] Residual
    blocks to encode them into a dense latent representation.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个拼接后的表示会通过一组*n*[e]个残差块进行编码，转换成稠密的潜在表示。
- en: '![](img/B22389_16_139.png)'
  id: totrans-548
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_139.png)'
- en: In the programmatic perspective, the dimensions get transformed from ![](img/B22389_16_138.png)
    to ![](img/B22389_16_141.png), where *H* is the hidden size of the latent representation.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 从程序的角度来看，维度从![](img/B22389_16_138.png)转换到![](img/B22389_16_141.png)，其中*H*是潜在表示的隐藏层大小。
- en: Now that we have the latent representation, let’s look at how we can decode
    the forecast from this.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了潜在表示，让我们来看一下如何从中解码预测结果。
- en: Decoder
  id: totrans-551
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器
- en: Just like the encoder, the decoder also has two separate steps. In the first
    step, we use a stack of *n*[d] of Residual Blocks to decode the latent representation
    into a decoded vector of dimension, ![](img/B22389_16_142.png), where ![](img/B22389_16_143.png)
    is the *decoder output dimension*. This decoded vector is reshaped into a ![](img/B22389_16_144.png)
    dimensional vector.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 和编码器一样，解码器也有两个独立的步骤。在第一步中，我们使用一组*n*[d]个残差块将潜在表示解码成维度为![](img/B22389_16_142.png)的解码向量，其中![](img/B22389_16_143.png)是*解码器输出维度*。这个解码向量被重新形状为![](img/B22389_16_144.png)维的向量。
- en: '![](img/B22389_16_145.png)'
  id: totrans-553
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_145.png)'
- en: '![](img/B22389_16_146.png)'
  id: totrans-554
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_146.png)'
- en: Now, we use a **Temporal Decoder** to convert this decoded vector into predictions.
    The temporal decoder is just a Residual block that takes in the concatenated decoded
    vector, ![](img/B22389_16_147.png) and the encoded future exogenous vector, ![](img/B22389_16_148.png).
    The authors argue that this residual connection allows some future covariates
    to affect the forecast in a stronger way. For instance, if one of the future covariates
    is holidays in a retail forecasting problem, then you want that variable to have
    a strong influence on the forecast.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用**时间解码器**将这个解码向量转换成预测。时间解码器仅是一个残差块，它接受拼接后的解码向量![](img/B22389_16_147.png)和编码后的未来外生向量![](img/B22389_16_148.png)。作者认为，这个残差连接允许某些未来的协变量在预测中产生更强的影响。例如，如果在零售预测问题中，未来的协变量之一是节假日，那么你希望这个变量对预测有很强的影响。
- en: This residual connection helps the model enable that “highway” if needed.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 这个残差连接帮助模型在需要时启用“高速公路”功能。
- en: '![](img/B22389_16_149.png)'
  id: totrans-557
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_16_149.png)'
- en: Finally, we add a Global Residual Connection that linearly maps the lookback
    window to the prediction ![](img/B22389_16_150.png), after a linear mapping to
    the right dimension. This ensures that the linear model that we saw earlier in
    the chapter becomes a subclass of the TiDE model.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加一个全局残差连接，将回溯窗口线性映射到预测结果![](img/B22389_16_150.png)，经过线性映射到正确的维度。这确保了我们在本章前面看到的线性模型变成了TiDE模型的一个子类。
- en: Forecasting with TiDE
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TiDE进行预测
- en: '*TiDE* is implemented in NIXTLA forecasting with the same framework we have
    seen in the prior models.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '*TiDE* 在 NIXTLA 预测中实现，采用了我们在之前的模型中看到的相同框架。'
- en: Let’s look at the initialization parameters of the implementation.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下实现的初始化参数。
- en: 'The `TIDE` class has the following major parameters:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '`TIDE` 类有以下主要参数：'
- en: '`decoder_output_dim`: An integer that controls the number of units in the output
    of the decoder ![](img/B22389_16_151.png). It directly impacts the dimensionality
    of the decoded sequence and can influence the model’s ability to reconstruct the
    target series.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_output_dim`：一个整数，控制解码器输出中的单元数量 ![](img/B22389_16_151.png)。它直接影响解码序列的维度，并且可能会影响模型重建目标序列的能力。'
- en: '`temporal_decoder_dim`: An integer that defines the output size of the temporal
    decoder. Although we discussed that the output of the temporal decoder is the
    final forecast, `NeuralfForecast` has a uniform map from network output to desired
    output dimension. Therefore, `temporal_decoder_dim` denotes the dimension of the
    penultimate layer, which will finally be transformed into the final output. The
    size of the dimension determines how much information you are allowing to pass
    on to the final forecasting layer.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temporal_decoder_dim`：一个整数，定义了时间解码器的输出大小。尽管我们讨论过时间解码器的输出是最终预测结果，`NeuralfForecast`
    通过网络输出到期望输出维度的统一映射。因此，`temporal_decoder_dim` 表示倒数第二层的维度，最终将被转换为最终输出。该维度的大小决定了你允许传递到最终预测层的信息量。'
- en: '`num_encoder_layers`: The number of encoder layers stacked on top of each other.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_encoder_layers`：堆叠在一起的编码器层的数量。'
- en: '`num_decoder_layers`: The number of decoder layers stacked on top of each other'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_decoder_layers`：堆叠在一起的解码器层的数量'
- en: '`temporal_width`: An integer that affects the lower temporal projected dimension
    ![](img/B22389_16_152.png), influencing how exogenous data is projected and processed.
    It plays a role in how the model incorporates and learns from exogenous information.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temporal_width`：一个整数，影响较低时间投影维度 ![](img/B22389_16_152.png)，影响外生数据如何被投影和处理。它在模型如何整合和学习外生信息方面起着重要作用。'
- en: '`layernorm`: This Boolean flag determines whether Layer Normalization is applied.
    Layer Normalization can stabilize and accelerate training, which might lead to
    better performance, especially in deeper networks.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layernorm`：该布尔标志决定是否应用层归一化。层归一化可以稳定并加速训练，可能会提高性能，特别是在更深的网络中。'
- en: 'Additionally, TIDE can handle exogenous information, which can be included
    in the below parameters:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，TIDE 可以处理外生信息，这些信息可以包含在以下参数中：
- en: '`futr_exog_list`: This takes a list of future exogenous columns'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`futr_exog_list`：接受一个未来外生列的列表'
- en: '`hist_exog_list`: This takes a list of historical exogenous columns'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hist_exog_list`：接受一个历史外生列的列表'
- en: '`stag_exog_list`: This is a list of exogenous columns'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stag_exog_list`：这是一个外生列的列表'
- en: '**Notebook alert**:'
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**笔记本提醒**：'
- en: The complete code for training the TIDE model can be found in the `10-TIDE_NeuralForecast.ipynb`
    notebook in the `Chapter16` folder.
  id: totrans-574
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用于训练 TIDE 模型的完整代码可以在 `Chapter16` 文件夹中的 `10-TIDE_NeuralForecast.ipynb` 笔记本中找到。
- en: We have covered a few popular specialized architectures for time series forecasting,
    but this is in no way a complete list. There are so many model architectures and
    techniques out there. I have included a few in the *Further reading* section to
    get you started.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了几种常见的时间序列预测专用架构，但这绝不是完整的列表。外面有很多模型架构和技术。我在*进一步阅读*部分列出了一些，供你参考。
- en: Congratulations on making it through probably one of the toughest and densest
    chapters in this book. Give yourself a pat on the back, sit back, and relax.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你成功完成了本书中可能最具挑战性和最密集的章节之一。给自己拍拍背，坐下来放松一下吧。
- en: Summary
  id: totrans-577
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Our journey with deep learning for time series has finally reached a conclusion
    with us reviewing a few specialized architectures for time series forecasting.
    We got an understanding of why it makes sense to have specialized architectures
    for time series and forecasting and went on to understand how different models
    such as *N-BEATS*, *N-BEATSx*, *N-HiTS*, *Autoformer*, *TFT*, *PatchTST*, *TiDE*,
    and *TSMixer* work. In addition to covering the architecture and theory behind
    it, we also looked at how we can use these models on real datasets using `neuralforecast`
    by NIXTLA. We never know which model will work better with our dataset, but as
    practitioners, we need to develop intuition that will guide us in the experimentation.
    Knowing how these models work behind the scenes is essential in developing that
    intuition and will help us experiment more efficiently.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关于时间序列的深度学习之旅终于迎来了结尾，我们回顾了一些时间序列预测的专门架构。我们理解了为何在时间序列和预测中采用专门架构是合理的，并且进一步了解了如何使不同的模型，如
    *N-BEATS*、*N-BEATSx*、*N-HiTS*、*Autoformer*、*TFT*、*PatchTST*、*TiDE* 和 *TSMixer*
    等工作。除了介绍这些架构背后的理论，我们还探讨了如何使用 `neuralforecast`（由NIXTLA开发）在实际数据集上应用这些模型。我们无法预知哪个模型在我们的数据集上效果最好，但作为从业者，我们需要培养出能够指导实验的直觉。了解这些模型的工作原理对于培养这种直觉至关重要，也将帮助我们更高效地进行实验。
- en: This brings this part of this book to a close. At this point, you should be
    much more comfortable with using DL for time series forecasting problems.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的这一部分到此结束。到目前为止，你应该已经更熟悉如何使用深度学习来解决时间序列预测问题。
- en: In the next chapter, let’s look at another important topic in forecasting—probabilistic
    forecasting.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论另一个在预测中非常重要的主题——概率预测。
- en: References
  id: totrans-581
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The following is a list of the references that we used throughout this chapter:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在本章中使用的参考文献列表：
- en: 'Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. (2020).
    *The M4 Competition: 100,000 time series and 61 forecasting methods*. International
    Journal of Forecasting, Volume 36, Issue 1\. Pages 54–74\. [https://doi.org/10.1016/j.ijforecast.2019.04.014](https://doi.org/10.1016/j.ijforecast.2019.04.014).'
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spyros Makridakis, Evangelos Spiliotis, 和 Vassilios Assimakopoulos. (2020).
    *M4竞赛：100,000个时间序列和61种预测方法*. 《国际预测期刊》，第36卷，第1期。页码 54–74。[https://doi.org/10.1016/j.ijforecast.2019.04.014](https://doi.org/10.1016/j.ijforecast.2019.04.014)。
- en: 'Slawek Smyl. (2018). *M4 Forecasting Competition: Introducing a New Hybrid
    ES-RNN Model*. [https://www.uber.com/blog/m4-forecasting-competition/](https://www.uber.com/blog/m4-forecasting-competition/).'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Slawek Smyl. (2018). *M4预测竞赛：引入一种新的混合ES-RNN模型*。[https://www.uber.com/blog/m4-forecasting-competition/](https://www.uber.com/blog/m4-forecasting-competition/)。
- en: 'Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. (2020).
    *N-BEATS: Neural basis expansion analysis for interpretable time series forecasting*.
    8^(th) International Conference on Learning Representations, (ICLR). [https://openreview.net/forum?id=r1ecqn4YwB](https://openreview.net/forum?id=r1ecqn4YwB).'
  id: totrans-585
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, 和 Yoshua Bengio. (2020).
    *N-BEATS: 可解释时间序列预测的神经基础扩展分析*. 第8届国际学习表示大会（ICLR）。[https://openreview.net/forum?id=r1ecqn4YwB](https://openreview.net/forum?id=r1ecqn4YwB)。'
- en: 'Kin G. Olivares and Cristian Challu and Grzegorz Marcjasz and R. Weron and
    A. Dubrawski. (2022). *Neural basis expansion analysis with exogenous variables:
    Forecasting electricity prices with NBEATSx*. International Journal of Forecasting,
    2022\. [https://www.sciencedirect.com/science/article/pii/S0169207022000413](https://www.sciencedirect.com/science/article/pii/S0169207022000413).'
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kin G. Olivares, Cristian Challu, Grzegorz Marcjasz, R. Weron, 和 A. Dubrawski.
    (2022). *带外生变量的神经基础扩展分析：使用NBEATSx预测电力价格*. 《国际预测期刊》，2022年。[https://www.sciencedirect.com/science/article/pii/S0169207022000413](https://www.sciencedirect.com/science/article/pii/S0169207022000413)。
- en: 'Cristian Challu and Kin G. Olivares and Boris N. Oreshkin and Federico Garza
    and Max Mergenthaler-Canseco and Artur Dubrawski. (2022). *N-HiTS: Neural Hierarchical
    Interpolation for Time Series Forecasting*. arXiv preprint arXiv: Arxiv-2201.12886\.
    [https://arxiv.org/abs/2201.12886](https://arxiv.org/abs/2201.12886).'
  id: totrans-587
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco,
    和 Artur Dubrawski. (2022). *N-HiTS: 用于时间序列预测的神经层次插值*. arXiv预印本 Arxiv: Arxiv-2201.12886。[https://arxiv.org/abs/2201.12886](https://arxiv.org/abs/2201.12886)。'
- en: Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion,
    Gomez, Aidan N, Kaiser, Lukasz, and Polosukhin, Illia. (2017). *Attention is All
    you Need.* Advances in Neural Information Processing Systems. [https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion,
    Gomez, Aidan N, Kaiser, Lukasz, 和 Polosukhin, Illia. (2017). *注意力即你所需要的*。神经信息处理系统进展.
    [https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).
- en: 'Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
    and Wancai Zhang. (2021). *Informer: Beyond Efficient Transformer for Long Sequence
    Time-Series Forecasting*. Thirty-Fifth {AAAI} Conference on Artificial Intelligence,
    {AAAI} 2021\. [https://ojs.aaai.org/index.php/AAAI/article/view/17325](https://ojs.aaai.org/index.php/AAAI/article/view/17325).'
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
    和 Wancai Zhang. (2021). *Informer: 超越高效的Transformer用于长序列时间序列预测*。第三十五届{AAAI}人工智能会议，{AAAI}
    2021\. [https://ojs.aaai.org/index.php/AAAI/article/view/17325](https://ojs.aaai.org/index.php/AAAI/article/view/17325).'
- en: 'Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. (2021). *Autoformer:
    Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting*.
    Advances in Neural Information Processing Systems 34: Annual Conference on Neural
    Information Processing Systems 2021, NeurIPS 2021, December 6–14, 2021\. [https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html).'
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Haixu Wu, Jiehui Xu, Jianmin Wang, 和 Mingsheng Long. (2021). *Autoformer: 带有自动相关性的分解Transformer用于长期序列预测*。神经信息处理系统进展
    34: 2021年神经信息处理系统年会，NeurIPS 2021, 2021年12月6日至14日\. [https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html).'
- en: Bryan Lim, Sercan Ö. Arik, Nicolas Loeff, and Tomas Pfister. (2019). *Temporal
    Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting*.
    International Journal of Forecasting, Volume 37, Issue 4, 2021, Pages 1,748–1,764\.
    [https://www.sciencedirect.com/science/article/pii/S0169207021000637](https://www.sciencedirect.com/science/article/pii/S0169207021000637).
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bryan Lim, Sercan Ö. Arik, Nicolas Loeff, 和 Tomas Pfister. (2019). *时间融合Transformer用于可解释的多视角时间序列预测*。国际预测期刊，第37卷，第4期，2021年，第1,748–1,764页\.
    [https://www.sciencedirect.com/science/article/pii/S0169207021000637](https://www.sciencedirect.com/science/article/pii/S0169207021000637).
- en: 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua
    Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. (2021). *An Image is Worth 16x16
    Words: Transformers for Image Recognition at Scale*. 9th International Conference
    on Learning Representations, ICLR 2021\. [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy).'
  id: totrans-592
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua
    Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, Jakob Uszkoreit, 和 Neil Houlsby. (2021). *一张图片值16x16个词：用于大规模图像识别的Transformer*。第九届国际学习表征大会，ICLR
    2021\. [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy).
- en: 'Yuqi Nie, Nam H. Nguyen, and Phanwadee Sinthong and J. Kalagnanam. (2022).
    *A Time Series is Worth 64 Words: Long-term Forecasting with Transformers*. 10th
    International Conference on Learning Representations, ICLR 2022\. [https://openreview.net/forum?id=Jbdc0vTOcol](https://openreview.net/forum?id=Jbdc0vTOcol).'
  id: totrans-593
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yuqi Nie, Nam H. Nguyen, 和 Phanwadee Sinthong 和 J. Kalagnanam. (2022). *一条时间序列值64个词：使用Transformer的长期预测*。第十届国际学习表征大会，ICLR
    2022\. [https://openreview.net/forum?id=Jbdc0vTOcol](https://openreview.net/forum?id=Jbdc0vTOcol).
- en: Ailing Zeng and Mu-Hwa Chen, L. Zhang, and Qiang Xu. (2023). *Are Transformers
    Effective for Time Series Forecasting?* AAAI Conference on Artificial Intelligence.
    [https://ojs.aaai.org/index.php/AAAI/article/view/26317](https://ojs.aaai.org/index.php/AAAI/article/view/26317).
  id: totrans-594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ailing Zeng 和 Mu-Hwa Chen, L. Zhang, 和 Qiang Xu. (2023). *Transformer是否对时间序列预测有效？*
    AAAI人工智能会议. [https://ojs.aaai.org/index.php/AAAI/article/view/26317](https://ojs.aaai.org/index.php/AAAI/article/view/26317).
- en: 'Liu, Shizhan and Yu, Hang and Liao, Cong and Li, Jianguo and Lin, Weiyao and
    Liu, Alex X and Dustdar, Schahram. (2022). *Pyraformer: Low-Complexity Pyramidal
    Attention for Long-Range Time Series Modeling and Forecasting*. International
    Conference on Learning Representations. [https://openreview.net/pdf?id=0EXmFzUn5I](https://openreview.net/pdf?id=0EXmFzUn5I).'
  id: totrans-595
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Liu, Shizhan 和 Yu, Hang 和 Liao, Cong 和 Li, Jianguo 和 Lin, Weiyao 和 Liu, Alex
    X 和 Dustdar, Schahram. (2022). *Pyraformer: 低复杂度的金字塔注意力机制用于长时间序列建模与预测*. 国际学习表征会议
    (ICLR)。[https://openreview.net/pdf?id=0EXmFzUn5I](https://openreview.net/pdf?id=0EXmFzUn5I)'
- en: 'Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and
    Jin, Rong. (2022). *{FEDformer}: Frequency enhanced decomposed transformer for
    long-term series forecasting*. Proc. 39^(th) International Conference on Machine
    Learning (ICML 2022). [https://proceedings.mlr.press/v162/zhou22g.html](https://proceedings.mlr.press/v162/zhou22g.html).'
  id: totrans-596
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Zhou, Tian 和 Ma, Ziqing 和 Wen, Qingsong 和 Wang, Xue 和 Sun, Liang 和 Jin, Rong.
    (2022). *{FEDformer}: 增强频率的分解变换器用于长期时间序列预测*。第39届国际机器学习会议 (ICML 2022)。[https://proceedings.mlr.press/v162/zhou22g.html](https://proceedings.mlr.press/v162/zhou22g.html)'
- en: Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and
    Xifeng Yan. (2019). *Enhancing the locality and breaking the memory bottle neck
    of transformer on time series forecasting*. Advances in Neural Information Processing
    Systems. [https://proceedings.neurips.cc/paper_files/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf).
  id: totrans-597
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang 和
    Xifeng Yan. (2019). *增强局部性并打破变换器在时间序列预测中的记忆瓶颈*. 神经信息处理系统进展（Advances in Neural
    Information Processing Systems）。[https://proceedings.neurips.cc/paper_files/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf)
- en: 'Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng
    Long. (2024). *iTransformer: Inverted Transformers Are Effective for Time Series
    Forecasting*.12^(th) International Conference on Learning Representations, ICLR
    2024\. [https://openreview.net/forum?id=JePfAI8fah](https://openreview.net/forum?id=JePfAI8fah).'
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma 和 Mingsheng
    Long. (2024). *iTransformer：倒转变换器在时间序列预测中的有效性*。第12届国际学习表征会议 (ICLR 2024)。[https://openreview.net/forum?id=JePfAI8fah](https://openreview.net/forum?id=JePfAI8fah)
- en: 'Si-An Chen and Chun-Liang Li and Sercan O Arik and Nathanael Christian Yoder
    and Tomas Pfister. (2023). *TSMixer: An All-MLP Architecture for Time Series Forecasting*.
    Transactions on Machine Learning Research. [https://openreview.net/forum?id=wbpxTuXgm0](https://openreview.net/forum?id=wbpxTuXgm0).'
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Si-An Chen, Chun-Liang Li, Sercan O Arik, Nathanael Christian Yoder 和 Tomas
    Pfister. (2023). *TSMixer: 一种全MLP架构的时间序列预测模型*。机器学习研究期刊（Transactions on Machine
    Learning Research）。[https://openreview.net/forum?id=wbpxTuXgm0](https://openreview.net/forum?id=wbpxTuXgm0)'
- en: 'Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, and Rose
    Yu. (2023). *Long-term Forecasting with TiDE: Time-series Dense Encoder*. Transactions
    on Machine Learning Research. [https://openreview.net/forum?id=pCbC3aQB5W](https://openreview.net/forum?id=pCbC3aQB5W).'
  id: totrans-600
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen 和 Rose Yu.
    (2023). *使用TiDE进行长期预测：时间序列稠密编码器*. 机器学习研究期刊（Transactions on Machine Learning Research）。[https://openreview.net/forum?id=pCbC3aQB5W](https://openreview.net/forum?id=pCbC3aQB5W)
- en: Further reading
  id: totrans-601
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can check out the following resources for further reading:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查阅以下资源进行进一步阅读：
- en: '*Fast ES-RNN: A GPU Implementation of the ES-RNN Algorithm*: [https://arxiv.org/abs/1907.03329](https://arxiv.org/abs/1907.03329)
    and [https://github.com/damitkwr/ESRNN-GPU](https://github.com/damitkwr/ESRNN-GPU)'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Fast ES-RNN: ES-RNN算法的GPU实现*：[https://arxiv.org/abs/1907.03329](https://arxiv.org/abs/1907.03329)
    和 [https://github.com/damitkwr/ESRNN-GPU](https://github.com/damitkwr/ESRNN-GPU)'
- en: '*Functions as Vector Spaces*: [https://www.youtube.com/watch?v=NvEZol2Q8rs](https://www.youtube.com/watch?v=NvEZol2Q8rs)'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*作为向量空间的函数*：[https://www.youtube.com/watch?v=NvEZol2Q8rs](https://www.youtube.com/watch?v=NvEZol2Q8rs)'
- en: '*Forecast with N-BEATS*, by Gaetan Dubuc: [https://www.kaggle.com/code/gatandubuc/forecast-with-n-beats-interpretable-model/notebook](https://www.kaggle.com/code/gatandubuc/forecast-with-n-beats-interpretable-model/notebook)'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用N-BEATS进行预测*，由Gaetan Dubuc提供：[https://www.kaggle.com/code/gatandubuc/forecast-with-n-beats-interpretable-model/notebook](https://www.kaggle.com/code/gatandubuc/forecast-with-n-beats-interpretable-model/notebook)'
- en: '*WaveNet: A Generative Model for Audio*, by DeepMind: [https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio](https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio)'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*WaveNet: 一种用于音频生成的模型*，由DeepMind提供：[https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio](https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio)'
- en: '*What is Residual Connection?*, by Wanshun Wong: [https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55](https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55)'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*什么是残差连接？*，由Wanshun Wong撰写：[https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55](https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55)'
- en: '*Efficient Transformers: A Survey*, by Tay et al.: [https://arxiv.org/abs/2009.06732](https://arxiv.org/abs/2009.06732)'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高效的Transformer：一项综述*，由Tay等人撰写：[https://arxiv.org/abs/2009.06732](https://arxiv.org/abs/2009.06732)'
- en: '*Autocorrelation and the Wiener-Khinchin theorem*: [https://www.itp.tu-berlin.de/fileadmin/a3233/grk/pototskyLectures2012/pototsky_lectures_part1.pdf](https://www.itp.tu-berlin.de/fileadmin/a3233/grk/pototskyLectures2012/pototsky_lectures_part1.pdf)'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自相关与Wiener-Khinchin定理*：[https://www.itp.tu-berlin.de/fileadmin/a3233/grk/pototskyLectures2012/pototsky_lectures_part1.pdf](https://www.itp.tu-berlin.de/fileadmin/a3233/grk/pototskyLectures2012/pototsky_lectures_part1.pdf)'
- en: '*Modelling Long- and Short-Term Temporal Patterns with Deep Neural Networks*,
    by Lai et al.: [https://dl.acm.org/doi/10.1145/3209978.3210006](https://dl.acm.org/doi/10.1145/3209978.3210006)
    and [https://github.com/cure-lab/SCINet](https://github.com/cure-lab/SCINet)'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用深度神经网络建模长短期时间模式*，由Lai等人撰写：[https://dl.acm.org/doi/10.1145/3209978.3210006](https://dl.acm.org/doi/10.1145/3209978.3210006)
    和 [https://github.com/cure-lab/SCINet](https://github.com/cure-lab/SCINet)'
- en: '*Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional
    Time Series Forecasting*, by Sen et al.: [https://proceedings.neurips.cc/paper/2019/hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html)
    and [https://github.com/rajatsen91/deepglo](https://github.com/rajatsen91/deepglo)'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全球思考，本地行动：一种基于深度神经网络的高维时间序列预测方法*，由Sen等人撰写：[https://proceedings.neurips.cc/paper/2019/hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html)
    和 [https://github.com/rajatsen91/deepglo](https://github.com/rajatsen91/deepglo)'
- en: Join our community on Discord
  id: totrans-612
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord
- en: 'Join our community’s Discord space for discussions with authors and other readers:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mts](https://packt.link/mts)'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mts](https://packt.link/mts)'
- en: '![](img/QR_Code15080603222089750.png)'
  id: totrans-615
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code15080603222089750.png)'
