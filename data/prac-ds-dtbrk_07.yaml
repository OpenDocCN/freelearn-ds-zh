- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Model Deployment Approaches
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型部署方法
- en: In the previous chapter, we looked at how we can utilize Databricks MLflow Model
    Registry to manage our ML model versioning and life cycle. We also learned how
    we could use the integrated access control to manage access to the models registered
    in Model Registry. We also understood how we could use the available webhook support
    with Model Registry to trigger automatic Slack notifications or jobs to validate
    the registered model in the registry.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了如何利用 Databricks MLflow 模型注册表来管理我们的机器学习模型版本和生命周期。我们还学习了如何使用集成的访问控制来管理对模型注册表中已注册模型的访问。我们还了解了如何使用模型注册表中可用的
    Webhook 支持，触发自动的 Slack 通知或作业，以验证注册表中的已注册模型。
- en: In this chapter, we will take the registered models from Model Registry and
    understand how to deploy them using the various model deployment options available
    in Databricks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从模型注册表中获取已注册的模型，并了解如何使用 Databricks 中可用的各种模型部署选项进行部署。
- en: 'We will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Understanding ML deployments and paradigms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解机器学习部署和范式
- en: Deploying ML models for batch and streaming inference
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署机器学习模型进行批处理和流处理推理
- en: Deploying ML models for real-time inference
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署机器学习模型进行实时推理
- en: Incorporating custom Python libraries into MLflow models for Databricks deployment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将自定义 Python 库集成到 MLflow 模型中以进行 Databricks 部署
- en: Deploying custom models with MLflow and Model Serving
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MLflow 和模型服务部署自定义模型
- en: Packaging dependencies with MLflow models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将依赖项打包到 MLflow 模型中
- en: Let’s go through the technical requirements for this chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看本章的技术要求。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll need the following before diving into this chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入本章之前，我们需要以下内容：
- en: Access to a Databricks workspace
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问 Databricks 工作区
- en: A running cluster with **Databricks Runtime for Machine Learning** (**Databricks
    Runtime ML**) with a version of 13 or above
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行一个包含**Databricks 机器学习运行时**（**Databricks Runtime ML**）的集群，版本为13或更高
- en: All the previous notebooks, executed as described
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有前面的笔记本，如上所述执行
- en: A basic knowledge of Apache Spark, including DataFrames and SparkUDF
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础的 Apache Spark 知识，包括 DataFrame 和 SparkUDF
- en: Let’s take a look at what exactly ML deployment is.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下机器学习部署到底是什么。
- en: Understanding ML deployments and paradigms
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习部署和范式
- en: '**Data science** is not the same as **data engineering**. Data science is more
    geared toward taking a business problem that we convert into data problems using
    scientific methods. We develop mathematical models and then optimize their performance.
    Data engineers are mainly concerned with the reliability of the data in the data
    lake. They are more focused on the tools to make the data pipelines scalable and
    maintainable while meeting the **service-level** **agreements** (**SLAs**).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据科学**与**数据工程**不同。数据科学更侧重于将商业问题转化为数据问题，并利用科学方法进行解决。我们开发数学模型，然后优化其性能。而数据工程师主要关注数据湖中数据的可靠性。他们更关注用于使数据管道可扩展和可维护的工具，同时确保符合**服务水平**
    **协议**（**SLA**）。'
- en: When we talk about ML deployments, we want to bridge the gap between data science
    and data engineering.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论机器学习部署时，我们希望弥合数据科学和数据工程之间的差距。
- en: 'The following figure visualizes the entire process of ML deployment:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 下图可视化了整个机器学习部署过程：
- en: '![Figure 7.1 – Displaying the ML deployment process](img/B17875_07_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 展示机器学习部署过程](img/B17875_07_01.jpg)'
- en: Figure 7.1 – Displaying the ML deployment process
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 展示机器学习部署过程
- en: On the right-hand side, we have the process of data science, which is very interactive
    and iterative. We understand the business problem and discover the datasets that
    can add value to our analysis. Then, we build data pipelines to wrangle the data
    and analyze it. We develop our models, and the chain continues.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧，我们展示了数据科学的过程，这是一个非常互动和迭代的过程。我们理解商业问题，并发现可以为分析增值的数据集。然后，我们构建数据管道来处理数据并进行分析。我们开发我们的模型，链条继续。
- en: 'The left-hand side of this diagram showcases the integration of the best practices
    from the software development world into the data science world. It’s mostly automated.
    Once our candidate model is ready, we do the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该图的左侧展示了将软件开发领域的最佳实践与数据科学领域的整合。这大多是自动化的。一旦我们的候选模型准备就绪，我们会执行以下步骤：
- en: First, we register it with the Model Registry.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将其注册到模型注册表。
- en: Next, we integrate the model with our applications.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将模型与我们的应用程序集成。
- en: Then, we test the integrated model with our application.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将用我们的应用程序测试集成后的模型。
- en: Finally, we deploy it to production, where we monitor the model’s performance
    and improve it.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将其部署到生产环境中，在那里我们监控模型的性能并加以改进。
- en: Some of these processes may look very similar to DevOps, but there are some
    critical differences between **DevOps** and **ModelOps**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些过程可能看起来与DevOps非常相似，但**DevOps**和**ModelOps**之间存在一些关键的区别。
- en: DevOps, in essence, combines software development and IT operations such as
    **continuous integration** (**CI**), **continuous deployment** (**CD**), updating
    or rolling back features, and pushing a patch.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，DevOps结合了软件开发和IT运营，如**持续集成**（**CI**）、**持续部署**（**CD**）、更新或回滚功能以及推送补丁。
- en: ModelOps combines the principles of DevOps, such as CI/CD, with specific requirements
    tailored to the world of ML. It introduces the need for continuous training and
    monitoring of ML models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ModelOps结合了DevOps的原则，如CI/CD，并结合了机器学习领域的特定需求。它引入了持续训练和监控ML模型的需求。
- en: Continuous training is a vital aspect of ModelOps. Unlike traditional software,
    where once a module is deployed, it rarely changes, ML models require ongoing
    updates. With the influx of new data, models must be periodically retrained to
    ensure their accuracy and relevance. This means that even if the core model code
    remains unchanged, the model itself evolves to adapt to the changing data landscape.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 持续训练是ModelOps中的一个重要方面。与传统软件不同，传统软件模块一旦部署后很少发生变化，ML模型需要不断更新。随着新数据的涌入，模型必须定期重新训练，以确保其准确性和相关性。这意味着即使核心模型代码保持不变，模型本身也会随着数据环境的变化而进化。
- en: 'Continuous monitoring in ModelOps encompasses two key areas: model performance
    monitoring and infrastructure monitoring. Model performance monitoring involves
    tracking how well the model is performing in real-world scenarios. This includes
    metrics such as accuracy, precision, and recall, among others. Infrastructure
    monitoring, on the other hand, focuses on the health and performance of the computing
    infrastructure supporting the model. This dual monitoring approach ensures that
    both the model and the underlying systems are operating optimally.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ModelOps中的持续监控涵盖了两个关键领域：模型性能监控和基础设施监控。模型性能监控涉及追踪模型在现实场景中的表现，包括准确率、精度、召回率等指标。另一方面，基础设施监控关注支持模型的计算基础设施的健康状况和性能。这种双重监控方法确保了模型和底层系统的最佳运行状态。
- en: This approach differs from traditional software engineering, where once a software
    module is tested and deployed to production, it typically remains stable without
    the need for continuous monitoring and adaptation. In ModelOps, the ever-evolving
    nature of data and the importance of maintaining model performance make continuous
    training and monitoring integral components of the process.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与传统软件工程有所不同，在传统软件工程中，一旦软件模块经过测试并部署到生产环境，它通常保持稳定，无需持续监控和适应。而在ModelOps中，数据的不断变化和维护模型性能的重要性使得持续训练和监控成为过程的核心部分。
- en: In the initial days of MLOps, most companies used Java and custom-built in-house
    tools for managing ML deployments, continuous training, and monitoring. However,
    today, most of the tools and frameworks have become open source, and we have seen
    Python is the de facto standard when implementing the entire model development
    life cycle in production.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLOps的初期，大多数公司使用Java和定制的内部工具来管理ML部署、持续训练和监控。然而，现在大多数工具和框架已经开源，并且我们已经看到Python成为了实施整个模型开发生命周期的事实标准。
- en: 'Let’s take a look at the most common ML deployment paradigms. Most ML use cases
    can be categorized into four buckets:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下最常见的ML部署模式。大多数ML用例可以分为四类：
- en: '**Batch deployments** (run ad hoc or at a scheduled time):'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量部署**（可即时运行或按计划执行）：'
- en: These are the most common deployments and are relatively easy to implement and
    are most efficient in terms of cost and productionization effort.
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些是最常见的部署方式，实施相对简单，且在成本和生产化工作量方面最为高效。
- en: Models make predictions that are stored in fast-access data repositories such
    as DynamoDB, Cassandra, Cosmos DB, or Delta tables within data lakehouses. These
    storage solutions are chosen for their efficiency in serving predictions. However,
    it’s important to note that these choices are tailored to use cases with low-latency
    retrieval requirements, and batch use cases with less stringent retrieval time
    constraints may have different considerations. Additionally, Databricks SQL offers
    a serverless, high-performance data warehousing solution that seamlessly integrates
    with data lakehouses, simplifying data management and analytics for enhanced productivity
    and reliability in leveraging predictive models. It’s worth mentioning that Delta
    tables also incorporate write optimizations, ensuring efficient data storage and
    processing.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型进行预测，并将预测结果存储在快速访问的数据仓库中，如 DynamoDB、Cassandra、Cosmos DB 或数据湖仓中的 Delta 表。这些存储解决方案因其高效的预测服务能力而被选择。然而，需要注意的是，这些选择是为具有低延迟检索要求的使用场景量身定制的，而批处理使用场景则可能有不同的考虑因素。此外，Databricks
    SQL 提供了一种无服务器的高性能数据仓储解决方案，它与数据湖仓无缝集成，简化了数据管理和分析，提高了利用预测模型的生产力和可靠性。值得一提的是，Delta
    表还结合了写入优化，确保了高效的数据存储和处理。
- en: '**Streaming deployments** (run continuously on the data):'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式部署**（持续运行在数据上）：'
- en: These deployments become essential when you don’t have access to your entire
    dataset before the inference starts, and you need to process new data relatively
    quickly as soon as it arrives.
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你在推理开始之前无法访问整个数据集，并且需要在新数据到达时相对快速地处理它们时，这些部署变得至关重要。
- en: Spark Structured Streaming is excellent for processing streaming data. It also
    has an inbuilt queuing mechanism, making it very useful when processing extensive
    image data.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Structured Streaming 非常适合处理流数据。它还内置了排队机制，在处理大量图像数据时非常有用。
- en: '**Real time** (REST endpoint):'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时**（REST 端点）：'
- en: These deployments become important when the use cases require near real-time
    requests and responses from a model deployed as part of an application.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用场景需要模型作为应用的一部分进行近实时的请求和响应时，这些部署变得尤为重要。
- en: At the time of writing this book, Databricks boasts a production-grade model
    serving offering that’s seamlessly integrated into its platform. This offering
    harnesses the power of serverless computing for optimal performance. Although
    delving into exhaustive details about the multitude of deployment architectures
    is not within the purview of this book, you can access comprehensive information
    on this subject in the Databricks documentation ([https://docs.databricks.com/en/serverless-compute/index.html](https://docs.databricks.com/en/serverless-compute/index.html)).
    Alternatively, you can seamlessly deploy your ML models as REST endpoints following
    their development and testing phases on Databricks with various cloud services
    such as Azure ML (leveraging Azure Kubernetes Service), AWS Sagemaker, and Google
    Vertex AI. The ML model is packaged into a container image and subsequently registered
    with the managed services offered by the respective cloud providers.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编写本书时，Databricks 提供了一项与其平台无缝集成的生产级模型服务。该服务利用无服务器计算的强大功能以实现最佳性能。尽管深入探讨各种部署架构的细节超出了本书的范围，但你可以在
    Databricks 文档中访问有关此主题的详细信息（[https://docs.databricks.com/en/serverless-compute/index.html](https://docs.databricks.com/en/serverless-compute/index.html)）。另外，你还可以在
    Databricks 上通过 Azure ML（利用 Azure Kubernetes 服务）、AWS Sagemaker 和 Google Vertex
    AI 等各种云服务，轻松地将你的机器学习模型部署为 REST 端点。机器学习模型被打包成容器镜像，然后与相应云提供商提供的托管服务进行注册。
- en: You can also use your own Kubernetes clusters for model deployments using the
    same paradigm.
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还可以使用自己的 Kubernetes 集群，使用相同的范式进行模型部署。
- en: '**On-device** (edge):'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备端**（边缘）：'
- en: These are very specific use cases in which we want to deploy models on devices
    such as Raspberry Pis or other IoT use cases. We will not be covering these in
    this chapter.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些是非常特定的使用场景，我们希望将模型部署到诸如 Raspberry Pi 或其他物联网设备上。我们将在本章中不涉及这些内容。
- en: As a best practice, it’s advisable to initially consider batch deployment as
    your go-to ML deployment paradigm. Transition to alternative paradigms only after
    thoroughly validating that batch deployment is inadequate for your specific use
    case. Keep in mind that the long-term maintenance costs associated with a real-time
    ML deployment system are generally higher than those for a batch system.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是，初步考虑将批量部署作为你的主要机器学习部署范式。仅在彻底验证批量部署不足以满足你的特定用例时，才过渡到其他范式。请记住，实时机器学习部署系统的长期维护成本通常高于批量系统。
- en: 'It’s also crucial to factor in response latency requirements when selecting
    the most appropriate ML deployment paradigm:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择最合适的机器学习部署范式时，考虑响应延迟要求同样至关重要：
- en: '**Batch deployment**: Ideally suited for scenarios where the expected response
    time for inference ranges from hours to days:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量部署**：最适合那些推理响应时间预期为几小时到几天的场景：'
- en: '**Use case recommendation**: This is particularly useful for data analytics
    and reporting tasks that are not time-sensitive, such as generating monthly sales
    forecasts or risk assessments.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用例推荐**：对于那些不受时间限制的数据分析和报告任务，尤其是生成每月销售预测或风险评估，这种部署方法特别有用。'
- en: '**Structured streaming deployment**: Optimal for use cases requiring inference
    on new data within a time frame of a few minutes up to an hour:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化流式部署**：最适合那些需要在几分钟到一小时内对新数据进行推理的用例：'
- en: '**Use case recommendation**: Real-time analytics or fraud detection systems
    often benefit from this deployment type, where the data stream needs to be analyzed
    continuously but an instant response is not critical.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用例推荐**：实时分析或欺诈检测系统通常会从这种部署类型中受益，这些系统需要持续分析数据流，但即时响应并不是关键。'
- en: '**Near real-time or REST endpoint deployments**: These are suitable when the
    expected latency lies between hundreds of milliseconds to a minute:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**近实时或 REST 端点部署**：当预期延迟在几百毫秒到一分钟之间时，这些部署方式最为合适：'
- en: '**Use case recommendation**: This deployment paradigm is best suited for applications
    such as real-time recommendation systems or automated customer service bots, which
    require fairly quick responses but not immediate action.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用例推荐**：这种部署范式最适合应用于实时推荐系统或自动化客户服务机器人等需要较快响应但不要求立即反应的应用。'
- en: '**Edge deployments**: These are geared toward scenarios demanding sub-100 ms
    SLAs:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边缘部署**：这些部署适用于那些要求小于 100 毫秒 SLA 的场景：'
- en: '**Use case recommendation**: This is crucial for **Internet of Things** (**IoT**)
    applications, autonomous vehicles, or any use case that requires lightning-fast
    decision-making capabilities.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用例推荐**：对于**物联网**（**IoT**）应用、自动驾驶车辆或任何需要闪电般决策能力的用例，这一点至关重要。'
- en: 'There are just broad guidelines. The following figure summarizes all the points
    we discussed here:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是一些大致的指导原则。以下图表总结了我们在这里讨论的所有要点：
- en: '![Figure 7.2 – The response latency requirements for various ML deployments](img/B17875_07_02.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 各种机器学习部署的响应延迟要求](img/B17875_07_02.jpg)'
- en: Figure 7.2 – The response latency requirements for various ML deployments
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 各种机器学习部署的响应延迟要求
- en: Now, let’s look at the various deployment options when using Databricks. Apart
    from the deployment approaches discussed here, some open source projects may interest
    you for serving models as REST. The links to these can be found in the *Further
    reading* section at the end of this chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下使用 Databricks 时的各种部署选项。除了这里讨论的部署方法，一些开源项目可能会对你有兴趣，尤其是将模型作为 REST 服务进行部署。这些项目的链接可以在本章末尾的*进一步阅读*部分找到。
- en: Deploying ML models for batch and streaming inference
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署机器学习模型进行批量和流式推理
- en: This section will cover examples of deploying ML models in a batch and streaming
    manner using Databricks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍使用 Databricks 以批量和流式方式部署机器学习模型的示例。
- en: In both **batch** and **streaming** inference deployments, we use the model
    to make the predictions and then store them at a location for later use. The final
    storage area for the prediction results can be a database with low latency read
    access, cloud storage such as S3 to be exported to another system, or even a Delta
    table that can easily be queried by business analysts.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在**批量**和**流式**推理部署中，我们使用模型进行预测，然后将预测结果存储在某个位置以供以后使用。预测结果的最终存储区域可以是一个具有低延迟读取访问的数据库、用于导出到其他系统的云存储（如
    S3），甚至是一个可以被业务分析师轻松查询的 Delta 表。
- en: When working with large amounts of data, Spark offers an efficient framework
    for processing and analyzing it, making it an ideal candidate to leverage our
    trained machine learning models.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大量数据时，Spark提供了一个高效的框架来处理和分析数据，这使得它成为了一个理想的候选框架来利用我们训练的机器学习模型。
- en: Note
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: One important note to remember is that we can use any non-distributed ML library
    to train our models. So long as it uses the MLflow model abstractions, you can
    utilize all the benefits of MLflow’s Model Registry and the code presented in
    this chapter.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一个重要提示是，我们可以使用任何非分布式的机器学习库来训练模型。只要它使用了MLflow的模型抽象，就可以利用MLflow模型注册表的所有好处，并且可以使用本章中呈现的代码。
- en: 'We should always consider the access pattern of the results generated by the
    model. Depending on where we store our prediction results, we can perform the
    following optimizations:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该始终考虑模型生成的结果的访问模式。根据我们存储预测结果的位置，我们可以进行以下优化：
- en: Partitioning, which can speed up data reads if your data is stored as static
    files or in a data warehouse
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区可以加速数据读取，尤其是当你的数据以静态文件形式存储或保存在数据仓库中时。
- en: Building indexes in databases on the relevant query, which generally improves
    performance
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据库中为相关查询构建索引，通常会提高性能。
- en: Let’s look at an example of how to perform batch and stream inference deployment
    using the Databricks environment.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个如何在Databricks环境中执行批量和流式推理部署的示例。
- en: Batch inference on Databricks
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Databricks上进行批量推理
- en: Batch inference is the most common type of model deployment paradigm. Running
    inference in batch infers running predictions using a model and storing them for
    later use.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 批量推理是最常见的模型部署方式。批量推理的意思是使用模型进行预测并将结果存储以供后续使用。
- en: 'For this, we will use the model available to us in MLflow’s Model Registry.
    We must ensure that we have at least one model version in staging for the notebook
    provided as part of this chapter to execute it:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用在MLflow的模型注册表中可用的模型。我们必须确保在此章节提供的笔记本中至少有一个模型版本处于Staging阶段，以便执行：
- en: 'Go into the **Models** tab and select the **Churn Prediction Bank** registered
    model. There should be a model version that is in the **Staging** state:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入**Models**选项卡，选择**Churn Prediction Bank**已注册模型。应该会有一个版本处于**Staging**状态：
- en: '![Figure 7.3 – The registered model in the Staging stage of Model Registry](img/B17875_07_003.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 模型注册表中Staging阶段的已注册模型](img/B17875_07_003.jpg)'
- en: Figure 7.3 – The registered model in the Staging stage of Model Registry
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 模型注册表中Staging阶段的已注册模型
- en: 'Open the notebook associated with `Chapter-07` named *Batch and Streaming*.
    We will simply load the model from the registry as a Python function, as shown
    in the following code block:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开与`Chapter-07`相关联的笔记本，名为*Batch and Streaming*。我们将简单地将模型从注册表加载为Python函数，如下所示的代码块：
- en: '[PRE0]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The rest of the notebook reads the same `raw_data` that we used to train our
    model in a Spark DataFrame and then after selecting the columns that we used to
    train our classification model using AutoML:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该笔记本的其余部分读取与我们用于训练模型的相同`raw_data`，以Spark DataFrame的形式进行读取，然后在选择用于训练分类模型的列之后，使用AutoML进行训练：
- en: '[PRE1]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s take a look at how we can utilize the same model loaded as a Spark UDF
    in a streaming inference deployment.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何利用相同的模型加载为Spark UDF进行流式推理部署。
- en: 'We won’t get into the details about how Structured Streaming in Spark works
    in this chapter as it is a large topic in itself. *Spark: The Definitive Guide:
    Big Data Processing Made Simple* is a great book for learning in-depth about Apache
    Spark and Structured Streaming. A streaming DataFrame can be conceptualized as
    an unbounded table that continuously updates as new data arrives. Links have been
    provided in the *Further reading* section to different resources for you to learn
    more about Structured Streaming.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '本章不详细讨论Spark中的结构化流式处理，因为它本身是一个庞大的话题。《*Spark: The Definitive Guide: Big Data
    Processing Made Simple*》是一本深入学习Apache Spark和结构化流式处理的好书。流式DataFrame可以被看作是一个不断更新的无界表格，随着新数据的到来持续更新。在*进一步阅读*部分中，提供了关于结构化流式处理的更多资源链接，供你深入学习。'
- en: Streaming inference on Databricks
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Databricks上进行流式推理
- en: 'Let’s look at the sample code to demonstrate how you can deploy the model we
    used in the previous section to perform streaming inference:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下示例代码，展示如何将上一节中使用的模型部署以进行流式推理：
- en: 'In `Cmd 15`, we must define `raw_data` from the Delta table to be read as a
    stream instead of a batch:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Cmd 15`中，我们必须将来自Delta表的`raw_data`定义为流式读取而非批量读取：
- en: '[PRE2]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The rest of the flow will look similar to batch inference.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 剩余的流程将与批量推理类似。
- en: 'Once we have defined our streaming Dataframe, we call upon the same model that
    we loaded from the model registry that is available in the staging environment:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们定义了我们的流式 Dataframe，我们调用在暂存环境中可用的模型注册表中加载的相同模型：
- en: '[PRE3]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once we have predictions ready, we can write the data out as a Delta table or
    format that is efficient for our use case.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦我们准备好预测结果，就可以将数据写入 Delta 表或适合我们使用场景的格式。
- en: Now, let’s take a look at how easy it is to use the same model if we want to
    perform real-time inference.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看如果我们想要执行实时推理，使用相同模型有多么简单。
- en: Deploying ML models for real-time inference
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 ML 模型进行实时推理
- en: Real-time inferences include generating predictions on a small number of records
    using a model deployed as a REST endpoint. The expectation is to receive the predictions
    in a few milliseconds.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 实时推理包括使用作为 REST 端点部署的模型对少量记录生成预测。期望在几毫秒内收到预测结果。
- en: Real-time deployments are needed in use cases when the features are only available
    when serving the model and cannot be pre-computed. These deployments are more
    complex to manage than batch or streaming deployments.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征仅在提供模型时可用且无法预先计算的使用场景中，需要进行实时部署。这些部署比批量或流式部署更复杂，需要更多的管理。
- en: 'Databricks offers integrated model serving endpoints, enabling you to prototype,
    develop, and deploy real-time inference models on production-grade, fully managed
    infrastructure within the Databricks environment. At the time of writing this
    book, there are two additional methods you can utilize to deploy your models for
    real-time inference:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 提供集成的模型服务端点，使您能够在 Databricks 环境中，在生产级别的完全托管基础设施上原型设计、开发和部署实时推理模型。在撰写本书时，您可以利用以下两种额外方法将模型部署到实时推理：
- en: 'Managed solutions provided by the following cloud providers:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由以下云服务提供商提供的托管解决方案：
- en: '**Azure ML**'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure ML**'
- en: '**AWS SageMaker**'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS SageMaker**'
- en: '**GCP VertexAI**'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GCP VertexAI**'
- en: Custom solutions that use Docker and Kubernetes or a similar set of technologies
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker 和 Kubernetes 或类似技术集的定制解决方案
- en: 'If you’re considering a robust solution for deploying and managing ML models
    in a production setting, Databricks Model Serving offers a host of compelling
    features:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在考虑为生产环境中部署和管理 ML 模型提供强大的解决方案，Databricks 模型服务提供了一系列引人注目的功能：
- en: '**Effortless endpoint creation**: With just a click, Databricks takes care
    of setting up a fully equipped environment suitable for your model, complete with
    options for serverless computing.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轻松创建端点**：只需点击一下，Databricks 就会处理设置一个完全配备的环境，适合您的模型，并提供无服务器计算选项。'
- en: '**Adaptive scalability and reliability**: Built for the rigors of production,
    Databricks Model Serving is engineered to manage a high throughput of over 25,000
    queries every second. The service dynamically scales to meet fluctuating demand
    and even allows the accommodation of multiple models on a single access point.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应可扩展性和可靠性**：Databricks 模型服务为生产环境的高要求而设计，能够处理每秒超过 25,000 个查询的高吞吐量。该服务能够动态扩展以满足波动的需求，甚至允许多个模型在单一访问点上运行。'
- en: '**Robust security measures**: Every deployed model operates within a secure
    digital perimeter and is allocated dedicated computing resources that are decommissioned
    once the model is no longer in use.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强大的安全措施**：每个部署的模型都在安全的数字边界内运行，并分配专用计算资源，模型不再使用时会撤销这些资源。'
- en: '**Smooth integration with MLflow**: The platform easily hooks into MLflow’s
    Model Registry, streamlining the deployment process of your ML models.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与 MLflow 的顺畅集成**：该平台轻松连接到 MLflow 的模型注册表，简化了 ML 模型的部署过程。'
- en: '**Comprehensive monitoring and debugging**: Databricks captures all request
    and response interactions in a specialized Delta table, facilitating real-time
    monitoring. Metrics such as query speed, latency, and error metrics are updated
    dynamically and are exportable to your choice of monitoring solution.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全面的监控和调试功能**：Databricks 会捕捉所有请求和响应交互，并将其记录到专门的 Delta 表中，以便进行实时监控。查询速度、延迟和错误等指标会动态更新，并可以导出到您选择的监控解决方案中。'
- en: '**Real-time feature incorporation**: If you’ve trained your model using Databricks’
    Feature Store, those features are seamlessly bundled with the model. Furthermore,
    these can be updated in real time if you’ve configured your online feature store.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时特征集成**：如果您使用Databricks的特征存储训练了您的模型，这些特征会与模型无缝打包在一起。此外，如果您配置了在线特征存储，您还可以实时更新这些特征。'
- en: Let’s understand some of the important technical details around the model serving
    endpoint feature grouped into various categories.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解一些与模型服务端点功能相关的重要技术细节，按类别分组。
- en: In-depth analysis of the constraints and capabilities of Databricks Model Serving
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Databricks模型服务的约束和能力的深入分析
- en: In this section, we will provide a comprehensive overview of the key technical
    aspects surrounding the use of Databricks Model Serving. From the payload size
    and query throughput limitations to latency and concurrency metrics, this section
    aims to equip you with essential insights that will guide your utilization of
    Databricks Model Serving effectively. Additionally, we will delve into system
    resource allocation details and discuss compliance and regional limitations that
    may impact your operations. Finally, we will touch upon miscellaneous factors
    and operational insights that could influence your decision-making when deploying
    ML models on this platform.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将全面概述与使用Databricks模型服务相关的关键技术要点。从负载大小和查询吞吐量限制，到延迟和并发性指标，本节旨在为您提供基本的见解，帮助您有效地使用Databricks模型服务。此外，我们还将深入探讨系统资源分配细节，并讨论可能影响您操作的合规性和区域限制。最后，我们将触及可能影响您在该平台上部署机器学习模型决策的其他因素和操作见解。
- en: '**Payload constraints and** **query throughput**:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载限制和** **查询吞吐量**：'
- en: '**Payload size**: It’s worth noting that the payload size for each request
    is capped at 16 MB. For most standard use cases, this is sufficient, but for more
    complex models, optimizations may be required.'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载大小**：值得注意的是，每个请求的负载大小限制为16 MB。对于大多数标准用例来说，这已经足够，但对于更复杂的模型，可能需要进行优化。'
- en: '**Queries per second** (**QPS**): The system comes with a default limit of
    200 QPS per workspace. Although adequate for experimentation and low-traffic services,
    this can be scaled up to 25,000 QPS for high-demand scenarios by consulting Databricks
    support.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每秒查询数**（**QPS**）：系统为每个工作空间设定了200 QPS的默认限制。尽管对于实验和低流量服务来说已经足够，但对于高需求场景，可以通过咨询Databricks支持将其扩展至25,000
    QPS。'
- en: '**Latency and** **concurrency metrics**:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟和** **并发性指标**：'
- en: '**Evaluation latency**: Those of us who work with computationally intensive
    models need to be mindful that Databricks imposes a 120-second upper limit for
    evaluation latency.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估延迟**：那些使用计算密集型模型的用户需要注意，Databricks对评估延迟设置了120秒的上限。'
- en: '**Concurrent requests**: Concurrency is capped at 200 queries per second across
    all serving endpoints in a workspace.. While this is often more than adequate,
    custom adjustments can be made through Databricks support for higher-demand services.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并发请求**：在一个工作空间中，所有服务端点的并发限制为每秒200个查询。虽然这一限制通常足够，但对于需求更高的服务，可以通过Databricks支持进行自定义调整。'
- en: '**System resources** **and overhead**:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统资源** **和开销**：'
- en: '**Memory**: The environment allocates a default of 4 GB per model. This is
    generally sufficient for most traditional ML models, but deep learning models
    may require an extension of up to 16 GB.'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存**：该环境为每个模型分配默认的4 GB内存。对于大多数传统的机器学习模型来说，这通常是足够的，但深度学习模型可能需要扩展至16 GB。'
- en: '**Latency overhead**: The architecture aims for a sub-50 ms additional latency,
    which is a best-effort approximation rather than a guarantee.'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟开销**：该架构的目标是将额外延迟保持在50毫秒以下，但这是一个尽力而为的近似值，而非保证。'
- en: '**Compliance and** **regional restrictions**:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规性和** **区域限制**：'
- en: '**HIPAA compliance**: For those in the healthcare domain, it’s critical to
    note that Databricks Model Serving isn’t currently HIPAA compliant.'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HIPAA合规性**：对于医疗领域的用户，需要注意的是，Databricks模型服务目前不符合HIPAA合规性要求。'
- en: '**Regional limitations**: There are instances where workspace location can
    disrupt Model Serving capabilities. This is an essential factor to consider during
    the planning stage. For a list of supported regions, go to [https://docs.databricks.com/en/resources/supported-regions.html](https://docs.databricks.com/en/resources/supported-regions.html).'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**区域限制**：某些情况下，工作区位置可能会干扰模型服务功能。这是规划阶段需要考虑的一个重要因素。要查看支持的区域列表，请访问 [https://docs.databricks.com/en/resources/supported-regions.html](https://docs.databricks.com/en/resources/supported-regions.html)。'
- en: '`pandas.core.indexes.numeric`''" can occur due to incompatible pandas versions.
    To fix it:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas.core.indexes.numeric`” 可能是由于 pandas 版本不兼容导致的。解决方法如下：'
- en: Run '`add-pandas-dependency.py`' ([https://learn.microsoft.com/en-us/azure/databricks/_extras/documents/add-pandas-dependency.py](https://learn.microsoft.com/en-us/azure/databricks/_extras/documents/add-pandas-dependency.py))
    script with the MLflow `run_id`.
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 '`add-pandas-dependency.py`' ([https://learn.microsoft.com/en-us/azure/databricks/_extras/documents/add-pandas-dependency.py](https://learn.microsoft.com/en-us/azure/databricks/_extras/documents/add-pandas-dependency.py))
    脚本，并使用 MLflow `run_id`。
- en: Re-register the model in the MLflow model registry.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 MLflow 模型注册表中重新注册模型。
- en: Serve the updated MLflow model.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供更新后的 MLflow 模型。
- en: '**Operational insights**:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运营洞察**：'
- en: '**Endpoint creation time**: The time it takes to provision a new model endpoint
    is around 10 minutes.'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端点创建时间**：创建新模型端点的时间大约为 10 分钟。'
- en: '**Zero-downtime updates**: The system is designed to perform endpoint updates
    with zero downtime, minimizing operational risk.'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零停机时间更新**：该系统旨在执行端点更新时实现零停机时间，最小化运营风险。'
- en: '**Dynamic scaling**: Databricks Model Serving employs intelligent scaling algorithms
    that adapt to fluctuating traffic patterns and provisioned concurrency, ensuring
    optimal resource allocation.'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态扩展**：Databricks 模型服务采用智能扩展算法，能够根据流量模式和预配置的并发性进行调整，确保资源的最优分配。'
- en: 'Let’s take a look at an example of how you can use Databricks’ inbuilt Model
    Serving endpoints to develop, prototype, and deploy models to generate real-time
    inference:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个如何使用 Databricks 内建的模型服务端点来开发、原型设计并部署模型以生成实时推理的示例：
- en: 'Go to the **Models** section in your workspace and select the **Churn Prediction**
    **Bank** model:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到工作区中的**模型**部分并选择**客户流失预测** **银行**模型：
- en: '![Figure 7.4 – The registered model in the Staging stage of Model Registry](img/B17875_07_004.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – 在模型注册表的 Staging 阶段注册的模型](img/B17875_07_004.jpg)'
- en: Figure 7.4 – The registered model in the Staging stage of Model Registry
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 在模型注册表的 Staging 阶段注册的模型
- en: 'Click on the **Use model for** **inference** button:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**用于推理的模型**按钮：
- en: '![Figure 7.5 – The Use model for inference button, which gives you the option
    to either use the model for batch/streaming inference or as a real-time REST endpoint](img/B17875_07_05.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 用于推理的模型按钮，您可以选择使用模型进行批量/流推理或作为实时 REST 端点](img/B17875_07_05.jpg)'
- en: Figure 7.5 – The Use model for inference button, which gives you the option
    to either use the model for batch/streaming inference or as a real-time REST endpoint
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 用于推理的模型按钮，您可以选择使用模型进行批量/流推理或作为实时 REST 端点
- en: 'Select **Real-time** and click on **Enable Serving**. Here, we can select what
    model version we want to serve and also the name of the serving endpoint. There
    are also options to automatically generate code for batch and streaming inference
    from the UI:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**实时**并点击**启用服务**。在这里，我们可以选择要提供的模型版本以及服务端点的名称。还可以选择自动生成批量和流推理的代码：
- en: '![Figure 7.6 – How to enable real-time serving from the UI](img/B17875_07_06.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 如何从 UI 启用实时服务](img/B17875_07_06.jpg)'
- en: Figure 7.6 – How to enable real-time serving from the UI
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 如何从 UI 启用实时服务
- en: 'You can also specify the type of compute resources you’d like to allocate for
    your model deployment. This is determined by the volume of concurrent requests
    that your endpoint is expected to handle. For our example, we will select **Small**:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以指定希望为模型部署分配的计算资源类型。这取决于端点预计要处理的并发请求量。在我们的示例中，我们将选择**小型**：
- en: '![Figure 7.7 – The various compute options](img/B17875_07_07.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 各种计算选项](img/B17875_07_07.jpg)'
- en: Figure 7.7 – The various compute options
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 各种计算选项
- en: Lastly, you can also select the **Scale to zero** option to make sure that your
    endpoint is not costing you when there is no load on it. Now, click **Create Endpoint**.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您还可以选择**扩展到零**选项，以确保在没有负载时您的端点不会产生费用。现在，点击**创建端点**。
- en: 'You will be redirected to the **Status** page, where you can see the current
    state of your model deployment, including what versions of the models are being
    deployed:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将被重定向到**状态**页面，在这里你可以看到当前模型部署的状态，包括正在部署的模型版本：
- en: '![Figure 7.8 – The status page of the deployed Model Serving endpoint](img/B17875_07_008.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 部署的模型服务端点的状态页面](img/B17875_07_008.jpg)'
- en: Figure 7.8 – The status page of the deployed Model Serving endpoint
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 部署的模型服务端点的状态页面
- en: 'You can also check the events associated with the model deployments:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以查看与模型部署相关的事件：
- en: '![Figure 7.9 – The Status page of the deployed Model Serving endpoint](img/B17875_07_009.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 部署的模型服务端点的状态页面](img/B17875_07_009.jpg)'
- en: Figure 7.9 – The Status page of the deployed Model Serving endpoint
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 部署的模型服务端点的状态页面
- en: 'You can do the same for the metrics associated with the endpoint:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以对与端点相关的指标执行相同的操作：
- en: '![Figure 7.10 – The metrics associated with the Model Serving endpoint](img/B17875_07_010.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – 与模型服务端点相关的指标](img/B17875_07_010.jpg)'
- en: Figure 7.10 – The metrics associated with the Model Serving endpoint
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 与模型服务端点相关的指标
- en: 'Another important thing to note here is that access to the REST endpoint is
    inherited from the permissions you set in Model Registry:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这里另一个需要注意的重要事项是，访问 REST 端点的权限是继承自你在模型注册表中设置的权限：
- en: '![Figure 7.11 – The permissions inherited by the Model Serving endpoint](img/B17875_07_11.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.11 – 模型服务端点继承的权限](img/B17875_07_11.jpg)'
- en: Figure 7.11 – The permissions inherited by the Model Serving endpoint
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 模型服务端点继承的权限
- en: 'Now, let’s take a look at how you can query your model. In the UI, once you
    see your model endpoint in the **Ready** state, you can click the **Query endpoint**
    button at the top-right corner of the serving endpoint status page:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下如何查询你的模型。在用户界面中，一旦你看到模型端点处于**就绪**状态，你可以点击服务端点状态页面右上角的**查询端点**按钮：
- en: '![Figure 7.12 – The Query endpoint button](img/B17875_07_12.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – 查询端点按钮](img/B17875_07_12.jpg)'
- en: Figure 7.12 – The Query endpoint button
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 查询端点按钮
- en: 'There are code snippets that explain how to query a particular version of your
    deployed model either in Python, cURL, or SQL. There is another option to mimic
    a browser request and the following steps describe how you can utilize it:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些代码片段可以解释如何在 Python、cURL 或 SQL 中查询你部署的特定版本的模型。还有一个选项可以模拟浏览器请求，以下步骤描述了如何使用它：
- en: 'Click on the **Show Example** button. This will only work when we have input
    examples logged in MLflow alongside the model:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**显示示例**按钮。只有当我们在 MLflow 中记录了模型的输入示例时，这才有效：
- en: '![Figure 7.13 – The automatically logged sample input records from AutoML](img/B17875_07_13.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 来自 AutoML 的自动记录的示例输入记录](img/B17875_07_13.jpg)'
- en: Figure 7.13 – The automatically logged sample input records from AutoML
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 来自 AutoML 的自动记录的示例输入记录
- en: 'To send the JSON request to our model for real-time inference, simply click
    **Send request**:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要向我们的模型发送 JSON 请求以进行实时推理，只需点击**发送请求**：
- en: '![Figure 7.14 – The response that was received from the deployed model](img/B17875_07_14.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.14 – 从部署的模型接收到的响应](img/B17875_07_14.jpg)'
- en: Figure 7.14 – The response that was received from the deployed model
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 从部署的模型接收到的响应
- en: When we trained our Churn prediction model, AutoML logged example inputs that
    our model expects when deployed as a REST endpoint. If you are not using AutoML
    and training the model yourself, the MLflow API can be used to log sample inputs
    to your model at the time of a model run.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练我们的客户流失预测模型时，AutoML 记录了我们的模型在作为 REST 端点部署时期望的示例输入。如果你没有使用 AutoML 而是自己训练模型，可以使用
    MLflow API 在模型运行时记录模型的示例输入。
- en: 'Let’s look at how we can use Python to query the model endpoints with the help
    of the example notebook:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何借助示例笔记本使用 Python 查询模型端点：
- en: Open the `Real-Time` notebook in the `Chapter-07` folder.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter-07`文件夹中的`Real-Time`笔记本。
- en: 'To query the model endpoint, we need each REST call to be accompanied by a
    Databricks `Cmd 4`, we must extract the PAT token from our notebook instance and
    programmatically extract our workspace domain name. This helps keep our code workspace-agnostic:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查询模型端点，我们需要每个 REST 调用都伴随一个 Databricks `Cmd 4`，我们必须从我们的笔记本实例中提取 PAT token，并以编程方式提取我们的工作区域名。这有助于保持代码与工作区无关：
- en: '[PRE4]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Cmd 6` contains a method score that takes as input sample records for inference
    as a Python dictionary, converts it into JSON, and sends a request to the deployed
    model. The model then responds with the predictions that are returned in JSON
    format:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Cmd 6` 包含一种方法评分，该方法将作为输入的样本记录转换为 Python 字典，转化为 JSON 格式，并向已部署的模型发送请求。模型随后以
    JSON 格式返回预测结果：'
- en: '[PRE5]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To engage with the serving endpoint APIs effectively, you should assemble your
    JSON request payload according to one of the recognized formats. Each format offers
    distinct advantages and limitations. In our specific scenario, our ML model anticipates
    input in the form of a pandas DataFrame. Therefore, we have two optimal orientation
    options to structure our API query to the endpoint:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了有效地与服务端 API 进行交互，您应该根据已识别的格式之一组装 JSON 请求负载。每种格式都有独特的优点和限制。在我们的特定场景中，我们的 ML
    模型期望以 pandas DataFrame 形式接收输入。因此，我们有两种最佳的结构化 API 查询端点的方向选项：
- en: '`dataframe_split` method, serialized in JSON in a split orientation. This format
    is more bandwidth-efficient compared to records orientation but is a bit harder
    to read:'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`dataframe_split` 方法，采用 JSON 格式并以分割方向序列化。与记录方向相比，这种格式更节省带宽，但阅读起来稍微复杂：'
- en: '[PRE6]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`records` layout is another available choice for representing DataFrame data.
    It comes as a JSON object with each entry presenting a row in the DataFrame. This
    record is easy to read and is human-friendly, but it consumes more bandwidth as
    the column names are repeated for each record:'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`records` 布局是表示 DataFrame 数据的另一种选择。它以 JSON 对象的形式呈现，每个条目代表 DataFrame 中的一行。此记录易于阅读且对人类友好，但由于列名在每条记录中都会重复，因此会消耗更多带宽：'
- en: '[PRE7]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Lastly, you can simply call inference on these records:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以直接对这些记录进行推断调用：
- en: '[PRE8]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When dealing with ML models such as those built in TensorFlow or PyTorch, which
    expect tensor inputs, you generally have two primary formatting options to consider
    for API requests: instances and input. Both the instances and input formats offer
    unique advantages and limitations that can significantly impact the design and
    performance of your ML solution.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理如 TensorFlow 或 PyTorch 等 ML 模型时，这些模型通常期望张量输入，您通常有两种主要的格式选项可供考虑：实例格式和输入格式。实例格式和输入格式各自具有独特的优缺点，这些优缺点会显著影响您
    ML 解决方案的设计和性能。
- en: 'Let’s delve into each format’s specifics to better understand how they can
    be optimally utilized:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解每种格式的细节，以便更好地理解如何最优化地使用它们：
- en: '**Instances format for tensors**: The **instances** format is tailored for
    tensor data, accommodating tensors in a row-wise manner. This is an ideal choice
    when all input tensors share the same dimension at index 0\. Essentially, each
    tensor in an instances list can be conceptually combined with other tensors with
    the same name across the list to form the complete input tensor for the model.
    This merging is only seamless if all tensors conform to the same 0-th dimension:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量的实例格式**：**实例**格式专为张量数据设计，以行优先的方式处理张量。当所有输入张量在索引0的维度上共享相同的大小时，这种格式是理想选择。本质上，实例列表中的每个张量可以与列表中具有相同名称的其他张量进行概念上的合并，从而形成模型的完整输入张量。只有当所有张量在0维度上保持一致时，这种合并才是无缝的：'
- en: 'Single tensor:'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一张量：
- en: '[PRE9]'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Multiple named tensors:'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个命名张量：
- en: '[PRE10]'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Input format for tensors**: The **input** format is another option that structures
    tensor data in a column-oriented manner. This format differs from instances in
    a crucial way: it allows for varying tensor instances across different tensor
    types. This is in contrast to the instances format, which requires a consistent
    number of tensor instances for each type.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量的输入格式**：**输入**格式是另一种将张量数据按列的方式结构化的选项。这种格式与实例格式在一个关键点上有所不同：它允许不同张量类型的实例具有不同的数量。这与实例格式相对，后者要求每种类型的张量实例数量一致。'
- en: Databricks’ serving functionality provides the flexibility to deploy multiple
    models behind a single endpoint, a feature that’s particularly useful for conducting
    A/B tests. Furthermore, you can allocate a specific percentage of total traffic
    among the various models housed behind the same endpoint. For more details on
    this, you can consult the official documentation ([https://dpe-azure.docs.databricks.com/machine-learning/model-serving/serve-multiple-models-to-serving-endpoint.html#serve-multiple-models-to-a-model-serving-endpoint](https://dpe-azure.docs.databricks.com/machine-learning/model-serving/serve-multiple-models-to-serving-endpoint.html#serve-multiple-models-to-a-model-serving-endpoint)).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks的服务功能提供了在单个端点后部署多个模型的灵活性，这对于进行A/B测试特别有用。此外，你还可以在同一个端点后将总流量的特定百分比分配给各个模型。有关更多详细信息，请参考官方文档([https://dpe-azure.docs.databricks.com/machine-learning/model-serving/serve-multiple-models-to-serving-endpoint.html#serve-multiple-models-to-a-model-serving-endpoint](https://dpe-azure.docs.databricks.com/machine-learning/model-serving/serve-multiple-models-to-serving-endpoint.html#serve-multiple-models-to-a-model-serving-endpoint))。
- en: 'Adding another model to an existing endpoint is a straightforward process via
    the user interface. Simply navigate to the **Edit Configuration** section and
    select the **Add served model** option. From there, you’ll be able to choose which
    model from the registry to deploy, specify its version, define the compute resources,
    and set the desired traffic allocation:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过用户界面将另一个模型添加到现有端点是一个直接的过程。只需导航到**编辑配置**部分，选择**添加已服务模型**选项。在那里，你可以选择要部署的注册表中的模型，指定其版本，定义计算资源，并设置所需的流量分配：
- en: '![Figure 7.15 – How to add multiple models behind the same endpoint](img/B17875_07_15.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.15 – 如何将多个模型添加到同一个端点后](img/B17875_07_15.jpg)'
- en: Figure 7.15 – How to add multiple models behind the same endpoint
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – 如何将多个模型添加到同一个端点后
- en: There is a notebook in the `Chapter-07` folder called `real-time-additional`
    that contains code that demonstrates how we can set these endpoints using the
    API using Python programmatically. You can go through it at your own pace.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Chapter-07`文件夹中，有一个名为`real-time-additional`的笔记本，里面包含了演示如何使用Python编程通过API设置这些端点的代码。你可以按自己的节奏浏览它。
- en: Now, let’s delve into other prevalent scenarios related to model deployment.
    First on the list is incorporating custom user-defined functions and libraries
    when deploying models with MLflow.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨与模型部署相关的其他常见场景。首先要介绍的是，在使用MLflow部署模型时，如何集成自定义用户定义的函数和库。
- en: Incorporating custom Python libraries into MLflow models for Databricks deployment
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Databricks部署中将自定义Python库集成到MLflow模型中
- en: 'If your projects necessitate the integration of bespoke Python libraries or
    packages hosted on a secure private repository, MLflow provides a useful utility
    function, `add_libraries_to_model`. This feature allows you to seamlessly incorporate
    these custom dependencies into your models during the logging process, before
    deploying them via Databricks Model Serving. While the subsequent code examples
    demonstrate this functionality using scikit-learn models, the same methodology
    can be applied to any model type supported by MLflow:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的项目需要集成托管在安全私有仓库中的定制Python库或包，MLflow提供了一个有用的实用函数`add_libraries_to_model`。这个功能可以在日志记录过程中无缝地将这些自定义依赖项集成到你的模型中，然后通过Databricks模型服务进行部署。虽然以下代码示例使用scikit-learn模型演示了这一功能，但相同的方法可以应用于MLflow支持的任何模型类型：
- en: '**Upload dependencies and install them in the notebook**: The recommended location
    for uploading dependency files is **Databricks File** **System** (**DBFS**):'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**上传依赖项并在笔记本中安装它们**：推荐的上传依赖项文件位置是**Databricks文件系统**（**DBFS**）：'
- en: '[PRE11]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`mlflow.sklearn.log_model()` with the `pip_requirements` or `conda_env` parameters
    to specify your dependencies:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`mlflow.sklearn.log_model()`和`pip_requirements`或`conda_env`参数来指定依赖项：
- en: '[PRE12]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`add_libraries_to_model()` function for embedding custom libraries alongside
    the model to ensure consistent environments:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`add_libraries_to_model()`函数用于将自定义库与模型一起嵌入，以确保环境一致：'
- en: '[PRE13]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Model deployment**: Once the new model version, including the custom libraries,
    has been registered, you can proceed to deploy it with Databricks Model Serving.'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型部署**：一旦新的模型版本，包括自定义库，已经注册，你就可以继续使用Databricks模型服务进行部署。'
- en: You can read more about this on the MLflow website ([https://www.mlflow.org/docs/latest/python_api/mlflow.models.html?highlight=add_libraries#mlflow.models.add_libraries_to_model](https://www.mlflow.org/docs/latest/python_api/mlflow.models.html?highlight=add_libraries#mlflow.models.add_libraries_to_model)).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 MLflow 网站上阅读更多相关内容（[https://www.mlflow.org/docs/latest/python_api/mlflow.models.html?highlight=add_libraries#mlflow.models.add_libraries_to_model](https://www.mlflow.org/docs/latest/python_api/mlflow.models.html?highlight=add_libraries#mlflow.models.add_libraries_to_model)）。
- en: 'Here is another end-to-end example. You can find the entire code in the `custom-python-libraries`
    notebook in the `Chapter-07` folder:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个端到端的示例，你可以在 `Chapter-07` 文件夹中的 `custom-python-libraries` 笔记本中找到完整的代码：
- en: '[PRE14]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Moving on, in the following section, we’ll delve into the intricacies of custom
    model development, exploring how specialized algorithms, unique data processing
    techniques, and enterprise-specific requirements can be seamlessly integrated
    into your MLflow deployments for enhanced performance and compliance.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在以下章节中，我们将深入探讨自定义模型开发的细节，探索如何将专用算法、独特的数据处理技术和企业特定需求无缝集成到 MLflow 部署中，以提高性能和合规性。
- en: Deploying custom models with MLflow and Model Serving
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 MLflow 和 Model Serving 部署自定义模型
- en: Deploying ML models often requires more than just making predictions. Many use
    cases demand additional capabilities, such as preprocessing inputs, post-processing
    outputs, or even executing custom logic for each request. Custom models in MLflow
    offer this level of flexibility, making it possible to integrate specialized logic
    directly alongside your models. This section will walk you through how to deploy
    such custom models with Model Serving.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 部署机器学习模型通常不仅仅是进行预测。许多用例要求额外的功能，例如预处理输入、后处理输出，甚至为每个请求执行自定义逻辑。MLflow 中的自定义模型提供了这种灵活性，使得可以将专门的逻辑直接与模型集成。本节将带你了解如何使用模型服务（Model
    Serving）部署这些自定义模型。
- en: 'MLflow custom models are particularly beneficial in the following scenarios:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow 自定义模型在以下场景中特别有用：
- en: '**Preprocessing needs**: When your model requires specific preprocessing steps
    before inputs can be fed into the prediction function.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理需求**：当你的模型在将输入传递给预测函数之前需要特定的预处理步骤时。'
- en: '**Post-processing requirements**: When the raw outputs of your model need to
    be transformed or formatted for end user consumption.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后处理需求**：当模型的原始输出需要转换或格式化以供最终用户使用时。'
- en: '**Conditional logic**: If the model itself has per-request branching logic,
    such as choosing between different models or algorithms based on the input.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条件逻辑**：如果模型本身有基于请求的分支逻辑，例如根据输入选择不同的模型或算法。'
- en: '**Fully custom code**: When you need to deploy an entirely custom code base
    alongside your model.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全自定义代码**：当你需要在模型旁边部署完全自定义的代码库时。'
- en: 'To create a custom model in MLflow, you need to write a `PythonModel` class
    that implements two essential functions:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 MLflow 中创建自定义模型，你需要编写一个实现两个核心函数的 `PythonModel` 类：
- en: '`load_context`: The `load_context` method is responsible for initializing components
    like model parameters or third-party modules that are crucial for the model but
    only need to be loaded once. This step enhances the performance during the model''s
    prediction phase.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_context`：`load_context` 方法负责初始化诸如模型参数或第三方模块等关键组件，这些组件对于模型至关重要，但只需加载一次。这一步骤有助于提升模型预测阶段的性能。'
- en: '`predict`: This function contains all the logic that executes each time an
    input request is made.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict`：该函数包含了每次收到输入请求时执行的所有逻辑。'
- en: 'Here is some example code that defines a custom MLflow model class called `CustomModel`
    that was built using the `PythonModel` base class:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例代码，定义了一个名为 `CustomModel` 的自定义 MLflow 模型类，该类使用 `PythonModel` 基类构建：
- en: '[PRE15]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let’s understand this code in more detail as it can easily be modified in the
    future for your use cases.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解这段代码，因为它可以根据你的用例在未来轻松修改。
- en: '`load_context(self, context)`: The load_context method initializes essential
    resources for our model to execute. The resources are loaded only once to optimize
    the inference phase. Let''s understand the code inside this method in more detail.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_context(self, context)`：load_context 方法初始化执行模型所需的基本资源。这些资源只加载一次，以优化推理阶段。让我们更详细地了解这个方法中的代码。'
- en: '`self.model = torch.load(context.artifacts["model-weights"])`: This line loads
    a PyTorch model from the artifacts and assigns it to the `self.model` attribute.
    The model weights are expected to be part of the artifacts under the `model-weights`
    key.'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.model = torch.load(context.artifacts["model-weights"])`：这行代码从工件中加载一个PyTorch模型，并将其分配给`self.model`属性。模型的权重应该是工件中的一部分，位于`model-weights`键下。'
- en: '`from preprocessing_utils.my_custom_tokenizer import CustomTokenizer`: This
    line imports a custom tokenizer class.'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from preprocessing_utils.my_custom_tokenizer import CustomTokenizer`：这行代码导入一个自定义的分词器类。'
- en: '`self.tokenizer = CustomTokenizer(context.artifacts["tokenizer_cache"])`: This
    line creates an instance of the imported `CustomTokenizer` class and initializes
    it using an artifact labeled `tokenizer_cache`. It is stored in the `self.tokenizer`
    attribute.'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.tokenizer = CustomTokenizer(context.artifacts["tokenizer_cache"])`：这行代码创建了一个导入的`CustomTokenizer`类的实例，并使用标记为`tokenizer_cache`的工件进行初始化。它被存储在`self.tokenizer`属性中。'
- en: '`format_inputs(self, model_input)`: This method is designed to handle the formatting
    or preprocessing of model inputs. As of now, this function''s code has not been
    implemented and is indicated by pass.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`format_inputs(self, model_input)`：这个方法用于处理模型输入的格式化或预处理。到目前为止，这个函数的代码尚未实现，表示为`pass`。'
- en: As of now, this function's code has not been implemented and is indicated by
    pass.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到目前为止，这个函数的代码尚未实现，表示为`pass`。
- en: '`format_outputs(self, outputs)`: This function is responsible for post-processing
    or formatting the raw outputs from the model.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`format_outputs(self, outputs)`：这个函数负责对模型的原始输出进行后处理或格式化。'
- en: '`predictions = (torch.sigmoid(outputs)).data.numpy()`: This line applies the
    sigmoid activation function to the raw outputs and then converts the resulting
    tensor into a NumPy array'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predictions = (torch.sigmoid(outputs)).data.numpy()`：这行代码将sigmoid激活函数应用于原始输出，并将结果张量转换为NumPy数组。'
- en: This function formats or post-processes the model’s raw outputs
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个函数用于格式化或后处理模型的原始输出。
- en: '`predict(self, context, model_input)`: Finally, we have the predict method
    that performs the following steps:'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict(self, context, model_input)`：最后，我们有`predict`方法，执行以下步骤：'
- en: '`model_input = self.format_inputs(model_input)`: This line calls the `format_inputs`
    function to format or preprocess the inputs'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_input = self.format_inputs(model_input)`：这行代码调用`format_inputs`函数来格式化或预处理输入。'
- en: '`outputs = self.model.predict(model_input)`: This line uses the pre-loaded
    PyTorch model to generate predictions'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs = self.model.predict(model_input)`：这行代码使用预加载的PyTorch模型来生成预测结果。'
- en: '`return self.format_outputs(outputs)`: This line calls `format_outputs` to
    post-process the raw outputs before returning them'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return self.format_outputs(outputs)`：这行代码调用`format_outputs`函数，在返回之前对原始输出进行后处理。'
- en: 'MLflow allows you to log custom models, complete with shared code modules from
    your organization. For instance, you can use the `code_path` parameter to log
    entire code bases that the model requires:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow允许你记录自定义模型，并附带你组织中共享的代码模块。例如，你可以使用`code_path`参数来记录模型所需的整个代码库：
- en: '[PRE16]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `mlflow.pyfunc.log_model(CustomModel(), "model", code_path = ["preprocessing_utils/"])`
    line uses MLflow’s `log_model` method to log a custom Python model for later use,
    such as serving or sharing it with team members. Let’s break down the function
    arguments:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlflow.pyfunc.log_model(CustomModel(), "model", code_path = ["preprocessing_utils/"])`这一行使用MLflow的`log_model`方法来记录一个自定义的Python模型，以便以后使用，比如提供服务或与团队成员共享。我们来分解一下函数参数：'
- en: '`CustomModel()`: This is an instance of the custom Python model class you’ve
    defined (such as the `CustomModel` class we saw earlier). This model will be logged
    and can be later retrieved from MLflow’s Model Registry.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CustomModel()`：这是你定义的自定义Python模型类的实例（例如我们之前看到的`CustomModel`类）。这个模型将被记录，并可以以后从MLflow的模型注册表中检索。'
- en: '`"model"`: This is the name you are giving to the logged model. It serves as
    an identifier that can be used when you are referring to this model in MLflow.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"model"`：这是你为记录的模型指定的名称。它作为一个标识符，用于在MLflow中引用该模型。'
- en: '`code_path = ["preprocessing_utils/"]`: This is a list of local file paths
    to Python files that the custom model depends on. In this case, it indicates that
    the code in the `preprocessing_utils` folder is necessary for the custom model
    to function correctly. This is especially useful when you want to include some
    preprocessing or utility code that is required to run the model. When you log
    the model, the code in this directory will be packaged alongside it. This ensures
    that you’ll have all the necessary code when you load the model later.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`code_path = ["preprocessing_utils/"]`：这是一个本地文件路径列表，指向自定义模型所依赖的Python文件。在此案例中，它表示`preprocessing_utils`文件夹中的代码是自定义模型正常运行所必需的。这样做特别有用，当你希望包含一些必需的预处理或工具代码来运行模型时。在记录模型时，该目录中的代码将与模型一起打包。这确保了在后续加载模型时，你拥有所有必需的代码。'
- en: So, when this function is executed, it logs your `CustomModel` class instance
    as a model with the name “model” in MLflow. It also packages any dependent code
    located in the `preprocessing_utils/` directory along with it. The resulting artifact
    can then be loaded and executed anywhere MLflow is available, and it will include
    both the model and its dependencies.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当执行此函数时，它会将你的`CustomModel`类实例作为名为“model”的模型记录到MLflow中。它还会将位于`preprocessing_utils/`目录中的所有依赖代码一起打包。生成的工件可以在任何支持MLflow的地方加载和执行，且包含模型及其依赖项。
- en: Once you log your custom model, it can be registered with MLflow Model Registry
    and then deployed to a Model Serving endpoint, just like any other model.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你记录了自定义模型，它可以注册到MLflow模型注册表中，并像其他模型一样部署到模型服务端点。
- en: Let’s look at an end-to-end example showcasing the use of custom models. The
    code uses the wine dataset, which is a classic and straightforward multi-class
    classification problem. Specifically, the dataset contains 178 wine samples from
    three different cultivars (types of grapes) in Italy. Each sample has 13 different
    features, such as Alcohol, Malic acid, and so on.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个完整的示例，展示自定义模型的使用。代码使用了葡萄酒数据集，这是一个经典且简单的多类别分类问题。具体来说，该数据集包含来自意大利三种不同葡萄品种的178个葡萄酒样本。每个样本有13个不同的特征，例如酒精含量、苹果酸等。
- en: The aim is to predict which cultivar a particular wine sample belongs to based
    on these 13 features. In other words, given a new wine sample, the model will
    predict whether it belongs to `class_0`, `class_1`, or `class_2`, each representing
    one of the three cultivars. It also provides the probabilities of the sample belonging
    to each of these classes.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是根据这13个特征预测某个葡萄酒样本属于哪种葡萄品种。换句话说，给定一个新的葡萄酒样本，模型将预测它属于`class_0`、`class_1`或`class_2`，每个类别代表三种葡萄品种之一。它还会提供该样本属于这些类别的概率。
- en: The code utilizes a decision tree classifier trained on a subset of the wine
    dataset (the training set). Once the model has been trained, it’s wrapped in a
    custom Python class (`CustomModelWrapper`) to facilitate logging via MLflow.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码使用决策树分类器，该分类器在葡萄酒数据集的一个子集（训练集）上进行了训练。一旦模型训练完成，它就会被包装在一个自定义的Python类（`CustomModelWrapper`）中，以便通过MLflow进行日志记录。
- en: 'Finally, the model is used to make predictions on new, unseen data (the test
    set). This code is available in the `custom-model` notebook in the `Chapter-07`
    folder:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型用于对新的、未见过的数据（测试集）进行预测。此代码可在`Chapter-07`文件夹中的`custom-model`笔记本中找到：
- en: '[PRE17]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code defines a `CustomModelWrapper` class that inherits from
    `mlflow.pyfunc.PythonModel`. This class serves as a wrapper for a given classifier
    model. The `__init__` method initializes the classifier, while the `predict` method
    computes probabilities and class predictions. These are then returned as a pandas
    DataFrame, which includes both the probability scores for each class and the final
    predicted labels:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码定义了一个`CustomModelWrapper`类，该类继承自`mlflow.pyfunc.PythonModel`。此类作为给定分类器模型的封装器。`__init__`方法初始化分类器，而`predict`方法计算概率和类别预测。然后，这些结果作为pandas
    DataFrame返回，其中包括每个类别的概率得分和最终预测的标签：
- en: '[PRE18]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Continuing from the custom model wrapper, this code takes additional steps to
    prepare for model deployment. First, it loads the wine dataset and divides it
    into training and test sets. `DecisionTreeClassifier` is then initialized and
    trained on the training set. Subsequently, an instance of `CustomModelWrapper`
    is created to encompass the trained classifier, adding an extra layer for output
    formatting.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 从自定义模型包装器继续，代码采取额外步骤来准备模型的部署。首先，它加载葡萄酒数据集，并将其分为训练集和测试集。然后初始化并训练 `DecisionTreeClassifier`，并在训练集上进行训练。接着，创建一个
    `CustomModelWrapper` 实例，将训练好的分类器包装其中，添加了额外的输出格式化层。
- en: The next phase involves defining the input and output schemas by specifying
    the data types and names of the features and target variables. These schemas serve
    as a blueprint for the model’s expected input and output, which is crucial for
    later deployment stages. An example input is also crafted using a single row from
    the training set to illustrate how the model will receive data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 下一阶段涉及定义输入和输出架构，通过指定特征和目标变量的数据类型及名称。这些架构作为模型预期输入和输出的蓝图，对于后续部署阶段至关重要。我们还使用训练集中的一行数据来构建示例输入，展示模型如何接收数据。
- en: Finally, the model is logged into MLflow, incorporating not just the custom
    wrapper, but also the input example and the predefined schemas. This comprehensive
    logging ensures that the model is ready for future tracking and deployment with
    all its nuances intact.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型被记录到 MLflow 中，不仅包括自定义包装器，还包括输入示例和预定义的架构。这种全面的记录确保模型在未来跟踪和部署时保持所有细节不变。
- en: In an ML deployment pipeline, ensuring that all model dependencies are correctly
    packaged is critical for stable, scalable, and efficient operation. The following
    section elaborates on the best practices for packaging these dependencies alongside
    your model using MLflow.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ML 部署管道中，确保所有模型依赖项正确打包对于稳定、可扩展和高效的运行至关重要。以下部分将详细说明使用 MLflow 将这些依赖项与模型一起打包的最佳实践。
- en: Packaging dependencies with MLflow models
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 MLflow 模型一起打包依赖项
- en: In a Databricks environment, files commonly reside in DBFS. However, for enhanced
    performance, it’s recommended to bundle these artifacts directly within the model
    artifact. This ensures that all dependencies are statically captured at deployment
    time.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks 环境中，文件通常存储在 DBFS 中。然而，为了提高性能，建议将这些工件直接打包进模型工件中。这可以确保所有依赖项在部署时被静态捕获。
- en: 'The `log_model()` method allows you to not only log the model but also its
    dependent files and artifacts. This function takes an `artifacts` parameter where
    you can specify paths to these additional files:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`log_model()` 方法不仅可以记录模型，还可以记录其依赖文件和工件。此函数接受一个 `artifacts` 参数，您可以在其中指定这些附加文件的路径：'
- en: '[PRE19]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In custom Python models logged with MLflow, you can access these dependencies
    within the model’s code using the `context.artifacts` attribute:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过 MLflow 记录的自定义 Python 模型中，您可以使用 `context.artifacts` 属性在模型的代码中访问这些依赖项：
- en: '[PRE20]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: At the time of serving the custom models from model endpoints, all the artifacts
    are copied over to the deployment container. They can be accessed as shown in
    the example using the context object.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过模型端点提供自定义模型时，所有的工件都会被复制到部署容器中。它们可以像示例中所示，通过上下文对象进行访问。
- en: MLflow allows you to specify a Conda environment for your model. You can provide
    a `conda.yaml` file that lists all the dependencies required by your model. When
    you serve the model, MLflow uses this Conda environment to ensure that all dependencies
    are correctly installed. This file is created automatically if you don’t specify
    it manually at the time of logging the model.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow 允许您为模型指定 Conda 环境。您可以提供一个列出所有模型所需依赖项的 `conda.yaml` 文件。当您提供模型时，MLflow
    会使用此 Conda 环境来确保所有依赖项正确安装。如果在记录模型时没有手动指定，该文件会自动创建。
- en: 'Here’s an example of how to specify a Conda environment in Python:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个在 Python 中指定 Conda 环境的示例：
- en: '[PRE21]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This brings us to the end of this chapter. Let’s summarize what we’ve learned.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容到此结束。让我们总结一下我们所学的内容。
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter covered the various deployment options in Databricks for your ML
    models. We also learned about the multiple deployment paradigms and how you can
    implement them using the Databricks workspace. The book’s subsequent editions
    will detail the many new features that Databricks is working on to simplify the
    MLOps journey for its users.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 Databricks 中的各种 ML 模型部署选项。我们还了解了多种部署范式，以及如何使用 Databricks 工作区实现这些范式。本书的后续版本将详细介绍
    Databricks 正在开发的许多新功能，这些功能旨在简化 MLOps 旅程，为用户提供更好的支持。
- en: In the next chapter, we will dive deeper into Databricks Workflows to schedule
    and automate ML workflows. We will go over how to set up ML training using the
    Jobs API. We will also take a look at the Jobs API’s integration with webhooks
    to trigger automated testing for your models when a model is transitioned from
    one registry stage to another.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将深入探讨 Databricks 工作流，学习如何调度和自动化 ML 工作流。我们将介绍如何使用 Jobs API 设置 ML 训练。我们还将查看
    Jobs API 如何与 webhooks 集成，以便在模型从一个注册表阶段转移到另一个阶段时，触发自动化测试。
- en: Further reading
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Here are some more links for further reading:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些进一步阅读的链接：
- en: '*MLeap* ([https://combust.github.io/mleap-docs)](https://combust.github.io/mleap-docs)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MLeap* ([https://combust.github.io/mleap-docs)](https://combust.github.io/mleap-docs))'
- en: '*Databricks*, *Introduction to DataFrames –* *Python* ([https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html))'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks*，*DataFrames 简介 –* *Python* ([https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html))'
- en: '*Structured Streaming Programming* *Guide* ([https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html))'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结构化流处理编程* *指南* ([https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html))'
- en: Docker ([https://docs.docker.com/](https://docs.docker.com/))
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker ([https://docs.docker.com/](https://docs.docker.com/))
- en: '*Kubernetes* ([https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/))'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kubernetes* ([https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/))'
- en: '*Pickle – Python object* *serialization* ([https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html))'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pickle – Python 对象* *序列化* ([https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html))'
