- en: 4\. The Bias-Variance Trade-Off
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 偏差-方差权衡
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we'll cover the remaining elements of logistic regression,
    including what happens when you call `.fit` to train the model, and the statistical
    assumptions you should be aware of when using this modeling technique. You will
    learn how to use L1 and L2 regularization with logistic regression to prevent
    overfitting and how to use the practice of cross-validation to decide the regularization
    strength. After reading this chapter, you will be able to use logistic regression
    in your work and employ regularization in the model fitting process to take advantage
    of the bias-variance trade-off and improve model performance on unseen data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍逻辑回归的剩余部分，包括调用`.fit`训练模型时发生的事情，以及在使用此建模技术时应该注意的统计假设。你将学习如何在逻辑回归中使用 L1 和
    L2 正则化来防止过拟合，并了解如何使用交叉验证实践来决定正则化的强度。阅读本章后，你将能够在工作中使用逻辑回归，并在模型拟合过程中使用正则化，以利用偏差-方差权衡并提高模型在未见数据上的表现。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In this chapter, we will introduce the remaining details of logistic regression
    left over from the previous chapter. In addition to being able to use scikit-learn
    to fit logistic regression models, you will gain insight into the gradient descent
    procedure, which is similar to the processes that are used "under the hood" (invisible
    to the user) to accomplish model fitting in scikit-learn. Finally, we'll complete
    our discussion of the logistic regression model by familiarizing ourselves with
    the formal statistical assumptions of this method.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍上一章中剩余的逻辑回归细节。除了能够使用 scikit-learn 拟合逻辑回归模型外，你还将深入了解梯度下降过程，这与 scikit-learn
    中用于完成模型拟合的“幕后”过程类似。最后，我们将通过熟悉这种方法的正式统计假设，完成对逻辑回归模型的讨论。
- en: 'We begin our exploration of the foundational machine learning concepts of overfitting,
    underfitting, and the bias-variance trade-off by examining how the logistic regression
    model can be extended to address the overfitting problem. After reviewing the
    mathematical details of the regularization methods that are used to alleviate
    overfitting, you will learn a useful practice for tuning the hyperparameters of
    regularization: cross-validation. Through the methods of regularization and some
    simple feature engineering, you will gain an understanding of how to improve both
    overfitted and underfitted models.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过探讨如何扩展逻辑回归模型以解决过拟合问题，开始探索机器学习中基础概念——过拟合、欠拟合和偏差-方差权衡。在回顾用于缓解过拟合的正则化方法的数学细节后，你将学到一种调优正则化超参数的实用方法：交叉验证。通过正则化方法和一些简单的特征工程，你将理解如何改进过拟合和欠拟合的模型。
- en: Although we are focusing on logistic regression in this chapter, the concepts
    of overfitting, underfitting, regularization, and the bias-variance trade-off
    are relevant to nearly all supervised modeling techniques in machine learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章我们主要关注逻辑回归，但过拟合、欠拟合、正则化以及偏差-方差权衡的概念几乎适用于机器学习中所有监督学习建模技术。
- en: Estimating the Coefficients and Intercepts of Logistic Regression
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计逻辑回归的系数和截距
- en: 'In the previous chapter, we learned that the coefficients of a logistic regression
    model (each of which goes with a particular feature), as well as the intercept,
    are determined using the training data when the `.fit` method is called on a logistic
    regression model in scikit-learn. These numbers are called the **parameters**
    of the model, and the process of finding the best values for them is called parameter
    **estimation**. Once the parameters are found, the logistic regression model is
    essentially a finished product: with just these numbers, we can use a logistic
    regression model in any environment where we can perform common mathematical functions.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们学习了逻辑回归模型的系数（每个系数对应一个特定的特征）以及截距，这些值是在调用 scikit-learn 中逻辑回归模型的`.fit`方法时，使用训练数据来确定的。这些数值被称为模型的**参数**，而找到最佳参数值的过程称为参数**估计**。一旦参数确定，逻辑回归模型就基本完成了：只需要这些数值，我们就可以在任何可以执行常见数学函数的环境中使用逻辑回归模型。
- en: It is clear that the process of parameter estimation is important, since this
    is how we can make a predictive model from our data. So, how does parameter estimation
    work? To understand this, the first step is to familiarize ourselves with the
    concept of a **cost function**. A cost function is a way of telling how far away
    the model predictions are from perfectly describing the data. The larger the difference
    between the model predictions and the actual data, then the larger the "cost"
    returned by the cost function.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，参数估计过程是非常重要的，因为正是通过这个过程，我们能够从数据中构建预测模型。那么，参数估计是如何工作的呢？要理解这一点，第一步是熟悉**代价函数**的概念。代价函数是一种衡量模型预测与数据完美描述之间距离的方式。模型预测与实际数据之间的差异越大，代价函数返回的“代价”就越大。
- en: 'This is a straightforward concept for regression problems: the difference between
    predictions and true values can be used for the cost, after going through a transformation
    (such as absolute value or squaring) to make the value of the cost positive, and
    then averaging this over all the training samples.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，这是一个直观的概念：预测值与真实值之间的差异可以用作代价，通过某种变换（例如取绝对值或平方）将代价值转换为正数，再对所有训练样本进行平均。
- en: 'For classification problems, especially in fitting logistic regression models,
    a typical cost function is the **log-loss** function, also called cross-entropy
    loss. This is the cost function that scikit-learn uses, in a modified form, to
    fit logistic regression:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，特别是在拟合逻辑回归模型时，一个典型的代价函数是**对数损失**函数，也叫交叉熵损失。这是 scikit-learn 在拟合逻辑回归时使用的代价函数，经过修改：
- en: '![Figure 4.1: The log-loss function'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1：对数损失函数'
- en: '](img/B16925_4_1.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_1.jpg)'
- en: 'Figure 4.1: The log-loss function'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：对数损失函数
- en: 'Here, there are *n* training samples, *y*i is the true label (0 or 1) of the
    *i*th sample, *p*i is the predicted probability that the label of the *i*th sample
    equals 1, and log is the natural logarithm. The summation notation (that is, the
    uppercase Greek letter, sigma) over all the training samples and division by *n*
    serve to take the average of this cost function over all training samples. With
    this in mind, take a look at the following graph of the natural logarithm function
    and consider what the interpretation of this cost function is:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有*n*个训练样本，*y*i 是第 *i* 个样本的真实标签（0 或 1），*p*i 是第 *i* 个样本标签为 1 的预测概率，log 是自然对数。对所有训练样本求和的符号（即大写的希腊字母
    sigma）和除以 *n*，用于对所有训练样本的代价函数进行平均。考虑到这一点，看看下面的自然对数函数图像，并思考这个代价函数的解释：
- en: '![Figure 4.2: Natural logarithm on the interval (0, 1)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2：区间 (0, 1) 上的自然对数'
- en: '](img/B16925_4_2.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_2.jpg)'
- en: 'Figure 4.2: Natural logarithm on the interval (0, 1)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：区间 (0, 1) 上的自然对数
- en: To see how the log-loss cost function works, consider its value for a sample
    where the true label is 1, which is *y = 1* in this case, so the second part of
    the cost function, *(1 - y*i*)log(1 - p*i*)*, will be exactly equal to 0 and will
    not affect the value. Then the value of the cost function is *-y*i*log(p*i*) =
    -log(p*i*)* since *y*i *= 1*. So, the cost for this sample is simply the negative
    of the natural logarithm of the predicted probability. Now since the true label
    for the sample is 1, consider how the cost function should behave. We expect that
    for predicted probabilities that are close to 1, the cost function will be small,
    representing a small error for predictions that are closer to the true value.
    For predictions that are closer to 0, it will be larger, since the cost function
    is supposed to take on larger values the more "wrong" the prediction is.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解对数损失代价函数是如何工作的，考虑一个样本，其中真实标签为 1，即 *y = 1*，因此代价函数的第二部分，*(1 - y*i*)log(1 -
    p*i*)*，将完全等于 0，不会影响结果。此时，代价函数的值为 *-y*i*log(p*i*) = -log(p*i*)*，因为 *y*i *= 1*。因此，该样本的代价就是预测概率的自然对数的负值。现在，由于该样本的真实标签为
    1，考虑代价函数应该如何表现。我们期望，对于接近 1 的预测概率，代价函数会很小，表示预测值与真实值接近时的误差很小。对于接近 0 的预测，代价会更大，因为代价函数应当随着预测错误的增大而增大。
- en: From the graph of the natural logarithm in *Figure 4.2* we can see that for
    values of *p* that are closer to 0, the natural logarithm takes on increasingly
    negative values. This means the cost function will take on increasingly positive
    values, so that the cost of classifying a positive sample with a very low probability
    is relatively high, as it should be. Conversely, if the predicted probability
    is closer to 1, then the graph indicates the cost will be closer to 0 – again,
    this is as expected for a prediction that is "more correct." Therefore, the cost
    function behaves as expected for a positive sample. A similar observation can
    be made for samples where the true label is 0.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 4.2*中的自然对数图中，我们可以看到，对于更接近 0 的*p*值，自然对数的值越来越负。这意味着成本函数将变得越来越大，因此，分类一个具有非常低概率的正样本的成本相对较高，这正是我们所期望的。相反，如果预测的概率更接近
    1，则图形表明成本将接近 0——再次，这与一个“更正确”预测的期望一致。因此，成本函数在正样本的情况下表现如预期。对于标签为 0 的样本，也可以做类似的观察。
- en: Now we understand how the log-loss cost function works for logistic regression.
    But what does this have to do with how the coefficients and the intercept are
    determined? We will learn in the next section.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了对数损失成本函数在逻辑回归中的工作原理。但这与系数和截距的确定有什么关系呢？我们将在下一节学习。
- en: Note
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The code for generating the plots presented in this section can be found here:
    [https://packt.link/NeF8P](https://packt.link/NeF8P).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 生成本节中展示的图表的代码可以在这里找到：[https://packt.link/NeF8P](https://packt.link/NeF8P)。
- en: Gradient Descent to Find Optimal Parameter Values
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降寻找最优参数值
- en: The problem of finding the parameter values (coefficients and intercept) for
    a logistic regression model using a log-loss cost boils down to a problem of `.fit`
    method of the logistic regression model in scikit-learn. There are different solution
    techniques for finding the set of parameters with the lowest cost, and you can
    choose which one you would like to use with the `solver` keyword when you are
    instantiating the model class. All of these methods work somewhat differently.
    However, they are all based on the concept of **gradient descent**.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对数损失成本找到逻辑回归模型的参数值（系数和截距）的问题，归结为 scikit-learn 中逻辑回归模型的`.fit`方法的问题。找到具有最低成本的参数集有不同的解决技术，您可以在实例化模型类时使用`solver`关键字选择您想要使用的技术。所有这些方法都略有不同，但它们都基于**梯度下降**的概念。
- en: The gradient descent process starts with an `solver` keyword. However, for more
    advanced machine learning algorithms such as deep neural networks, selection of
    the initial guesses for parameters requires more attention.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降过程从`solver`关键字开始。然而，对于像深度神经网络这样的更高级机器学习算法，选择参数的初始猜测需要更多的关注。
- en: For the sake of illustration, we will consider a problem where there is only
    one parameter to estimate. We'll look at the value of a hypothetical cost function
    (*y = f(x) = x*2 *– 2x*) and devise a gradient descent procedure to find the value
    of the parameter, *x*, for which the cost, *y*, is the lowest. Here, we choose
    some *x* values, create a function that returns the value of the cost function,
    and look at the value of the cost function over this range of parameters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明问题，我们考虑一个只需要估计一个参数的情况。我们将观察一个假设的成本函数（*y = f(x) = x*2 *– 2x*）的值，并设计一个梯度下降过程来找到使成本*y*最小的参数值*x*。在这里，我们选择一些*x*值，创建一个返回成本函数值的函数，并观察在这个参数范围内成本函数的值。
- en: 'The code to do this is as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作的代码如下：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is the output of the print statement:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是打印语句的输出：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The remaining code snippet is as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的代码片段如下：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The resulting plot should appear as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图应如下所示：
- en: '![Figure 4.3: A cost function plot'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3：成本函数图'
- en: '](img/B16925_4_3.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_3.jpg)'
- en: 'Figure 4.3: A cost function plot'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：成本函数图
- en: Note
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'In the preceding code snippets, we assume that you would have imported the
    necessary libraries. You can refer to the following notebook for the complete
    code for the chapter including the import statement for the preceding snippets:
    [https://packt.link/A4VyF](https://packt.link/A4VyF).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，我们假设您已经导入了必要的库。您可以参考以下笔记本，获取包括前述代码片段导入语句的完整代码：[https://packt.link/A4VyF](https://packt.link/A4VyF)。
- en: 'Looking at the **error surface** in *Figure 4.3*, which is the plot of the
    cost function over a range of parameter values, it''s pretty evident what parameter
    value will result in the lowest value of the cost function: *x = 1*. In fact,
    with some calculus, you could easily confirm this by setting the derivative to
    zero and then solving for *x*, confirming that *x = 1* is the minimum. However,
    generally speaking, it is not always feasible to solve the problem so simply.
    In cases where it is necessary to use gradient descent, we don''t always know
    how the entire error surface looks. Rather, after we''ve chosen the initial guess
    for the parameter, all we''re able to know is the direction of the error surface
    in the immediate vicinity of that point.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 **误差面**（在 *图 4.3* 中），这是代价函数在一系列参数值上的图像，显而易见，哪个参数值将导致代价函数的最低值：*x = 1*。实际上，利用一些微积分，你可以通过将导数设置为零并求解
    *x*，轻松确认 *x = 1* 是最小值。然而，通常来说，并不是所有问题都能如此简单地解决。在需要使用梯度下降的情况下，我们并不总是知道整个误差面的形状。相反，在我们选择了参数的初始猜测值之后，我们只能知道在该点周围区域内误差面的方向。
- en: '**Gradient descent** is an iterative algorithm; starting from the initial guess,
    we try to find a new guess that lowers the cost function and continue with this
    until we''ve found a good solution. We are trying to move "downhill" on the error
    surface, but we only know which direction to move in and how far to move in that
    direction, based on the shape of the error surface in the immediate neighborhood
    of our current guess. In mathematical terms, we only know the value of the **derivative**
    (which is called the **gradient** in more than one dimension) at the parameter
    value of the current guess. If you have not studied calculus, you can think of
    the gradient as telling you which direction is downhill, and how steep the hill
    is from where you''re standing. We use this information to "take a step" in the
    direction of decreasing error. How big a step we decide to take depends on the
    **learning rate**. Since the gradient declines toward the direction of decreasing
    error, we want to take a step in the direction that is the negative of the gradient.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降**是一种迭代算法；从初始猜测值开始，我们尝试找到一个新的猜测值，使代价函数降低，并继续进行，直到找到一个好的解决方案。我们试图在误差面上“下坡”，但我们只能根据当前猜测值附近的误差面形状知道该朝哪个方向移动以及在该方向上走多远。从数学角度看，我们只知道当前猜测值的参数处的
    **导数**（在多维情况下称为 **梯度**）。如果你没有学习过微积分，可以把梯度理解为告诉你哪个方向是下坡，以及从你站立的地方山坡有多陡。我们利用这些信息在减少误差的方向上“迈出一步”。我们决定走多大的步伐取决于
    **学习率**。由于梯度朝着误差减少的方向减小，我们希望朝梯度的负方向迈步。'
- en: 'These notions can be formalized in the following equation. To get to the new
    guess, *x*new, from the current guess, *x*old, where *f''(x*old*)* is the derivative
    (that is, the gradient) of the cost function at the current guess:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念可以通过以下方程进行形式化。为了从当前猜测值 *x*old 获得新猜测值 *x*new，其中 *f'(x*old*)* 是当前猜测值处代价函数的导数（即梯度）：
- en: '![Figure 4.4: Equation to obtain the new guess from the current guess'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4：从当前猜测值获取新猜测值的方程'
- en: '](img/B16925_4_4.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_4.jpg)'
- en: 'Figure 4.4: Equation to obtain the new guess from the current guess'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：从当前猜测值获取新猜测值的方程
- en: 'In the following graph, we can see the results of starting a gradient descent
    procedure from *x = 4.5*, with a learning rate of 0.75, and then optimizing *x*
    to attain the lowest value of the cost function:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到从 *x = 4.5* 开始进行梯度下降过程的结果，学习率为 0.75，然后通过优化 *x* 使代价函数达到最小值：
- en: '![Figure 4.5: The gradient descent path'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5：梯度下降路径'
- en: '](img/B16925_4_5.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_5.jpg)'
- en: 'Figure 4.5: The gradient descent path'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：梯度下降路径
- en: Gradient descent also works in higher-dimensional spaces; in other words, with
    more than one parameter. However, you can only visualize up to a two-dimensional
    error surface (that is, two parameters at a time on a three-dimensional plot)
    on a single graph.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降也适用于更高维的空间；换句话说，适用于多个参数。然而，你只能在单一图表中可视化最多二维的误差面（即在三维图中同时展示两个参数）。
- en: Having described the workings of gradient descent, let's perform an exercise
    to implement the gradient descent algorithm, expanding on the example of this
    section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述了梯度下降的工作原理后，让我们进行一个练习，实现梯度下降算法，并扩展本节的例子。
- en: Note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The code for generating the plots presented in this section can be found here:
    [https://packt.link/NeF8P](https://packt.link/NeF8P). If you''re reading the print
    version of this book, you can download and browse the color versions of some of
    the images in this chapter by visiting the following link: [https://packt.link/FAXBM](https://packt.link/FAXBM)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成本节所呈现图表的代码可以在这里找到：[https://packt.link/NeF8P](https://packt.link/NeF8P)。如果你正在阅读本书的印刷版，你可以通过访问以下链接下载并浏览本章一些图像的彩色版本：[https://packt.link/FAXBM](https://packt.link/FAXBM)
- en: 'Exercise 4.01: Using Gradient Descent to Minimize a Cost Function'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.01：使用梯度下降最小化代价函数
- en: 'In this exercise, our task is to find the best set of parameters in order to
    minimize the following hypothetical cost function: *y = f(x) = x*2 *– 2x*. To
    do this, we will employ gradient descent, which was described in the preceding
    section. Perform the following steps to complete the exercise:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们的任务是找到一组最佳参数，以最小化以下假设的代价函数：*y = f(x) = x*2 *– 2x*。为此，我们将采用前面部分描述的梯度下降方法。执行以下步骤以完成练习：
- en: Note
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before you begin this exercise, please make sure you have executed the prerequisite
    steps of importing the necessary libraries and loading the cleaned dataframe.
    These steps along with the code for this exercise can be found at [https://packt.link/NeF8P](https://packt.link/NeF8P).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本练习之前，请确保你已执行了导入必要库和加载清理后的数据框架的先决步骤。有关这些步骤以及本练习的代码，你可以在[https://packt.link/NeF8P](https://packt.link/NeF8P)找到。
- en: 'Create a function that returns the value of the cost function and look at the
    value of the cost function over a range of parameters. You can use the following
    code to do this (note that this repeats code from the preceding section):'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个返回代价函数值的函数，并查看在一系列参数下代价函数的值。你可以使用以下代码来做到这一点（注意，这部分代码重复了前面的部分）：
- en: '[PRE3]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will obtain the following plot of the cost function:'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下的代价函数图：
- en: '![Figure 4.6: A cost function plot'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.6：代价函数图'
- en: '](img/B16925_4_6.jpg)'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_4_6.jpg)'
- en: 'Figure 4.6: A cost function plot'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.6：代价函数图
- en: 'Create a function for the value of the gradient. This is the analytical derivative
    of the cost function. Use this function to evaluate the gradient at the point
    *x = 4.5*, and then use this in combination with the learning rate to find the
    next step of the gradient descent process:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来求梯度值。这是代价函数的解析导数。使用此函数来计算在 *x = 4.5* 时的梯度，然后将其与学习率结合，找到梯度下降过程的下一步：
- en: '[PRE4]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is the next gradient descent step after *x = 4.5*.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是 *x = 4.5* 后的下一个梯度下降步骤。
- en: 'Plot the gradient descent path, from the starting point to the next point,
    using the following code:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码绘制梯度下降路径，从起点到下一个点：
- en: '[PRE5]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You will obtain the following output:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '![Figure 4.7: The first gradient descent path step'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.7：第一次梯度下降路径步骤'
- en: '](img/B16925_4_7.jpg)'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_4_7.jpg)'
- en: 'Figure 4.7: The first gradient descent path step'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.7：第一次梯度下降路径步骤
- en: Here, it appears as though we've taken a step in the right direction. However,
    it's clear that we've overshot where we want to be. It may be that our learning
    rate is too large, and consequently, we are taking steps that are too big. While
    tuning the learning rate will be a good way to converge toward an optimal solution
    more quickly, in this example, we can just continue illustrating the remainder
    of the process. Here, it looks like we may need to take a few more steps. In practice,
    gradient descent continues until the size of the steps become very small, or the
    change in the cost function becomes very small (you can specify how small by using
    the `tol` argument in the scikit-learn logistic regression), indicating that we're
    close enough to a good solution – that is, a `max_iter`).
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，看起来我们似乎朝着正确的方向迈出了第一步。然而，很明显我们已经越过了我们想要到达的位置。可能是我们的学习率过大，因此我们采取了过大的步伐。虽然调节学习率是加速收敛到最优解的好方法，但在这个例子中，我们可以继续演示过程的其余部分。这里看起来我们可能还需要再迈几步。实际上，梯度下降会一直进行，直到步伐变得非常小，或者代价函数的变化变得非常小（你可以通过使用`tol`参数在scikit-learn的逻辑回归中指定多小），这表示我们已经接近一个好的解——也就是`max_iter`。
- en: 'Perform 14 iterations to converge toward the local minimum of the cost function
    by using the following code snippet (note that `iterations = 15`, but the endpoint
    is not included in the call to `range()`):'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用以下代码片段执行14次迭代，以便向代价函数的局部最小值收敛（请注意，`iterations = 15`，但在调用 `range()` 时不包括终点）：
- en: '[PRE6]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You will obtain the following output:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '[PRE7]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This `for` loop stores the successive estimates in the `x_path` array, using
    the current estimate to calculate the derivative and find the next estimate. From
    the resulting values of the gradient descent process, it looks like we've gotten
    very close (`1.00021362`) to the optimal solution of 1.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个`for`循环将连续的估计值存储在`x_path`数组中，使用当前估计值计算导数并找到下一个估计值。从梯度下降过程的结果值来看，我们似乎已经非常接近（`1.00021362`）最优解1。
- en: 'Plot the gradient descent path using the following code:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码绘制梯度下降路径：
- en: '[PRE8]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You will obtain the following output:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '![Figure 4.8: The gradient descent path'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.8：梯度下降路径'
- en: '](img/B16925_4_8.jpg)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_4_8.jpg)'
- en: 'Figure 4.8: The gradient descent path'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：梯度下降路径
- en: We encourage you to repeat the previous procedure with different learning rates
    in order to see how they affect the gradient descent path. With the right learning
    rate, it's possible to converge on a highly accurate solution very quickly. While
    the choice of learning rate can be important in different machine learning applications,
    for logistic regression, the problem is usually pretty easy to solve and you don't
    need to select a learning rate in scikit-learn.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励你重复之前的过程，尝试不同的学习率，看看它们如何影响梯度下降路径。选择合适的学习率，可以非常快速地收敛到一个高度准确的解。虽然在不同的机器学习应用中，学习率的选择很重要，但对于逻辑回归来说，这个问题通常比较容易解决，在scikit-learn中你不需要特别选择学习率。
- en: As you experimented with different learning rates, did you notice what happened
    when the learning rate was greater than one? In this case, the step that we take
    in the direction of the decreasing error is too large and we actually wind up
    with a higher error. This problem can compound itself and actually lead the gradient
    descent process away from the region of minimum error. On the other hand, if the
    step size is too small, it can take a very long time to find the desired solution.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试不同的学习率时，是否注意到当学习率大于1时发生了什么？在这种情况下，我们朝着减少误差的方向迈出的步伐过大，实际上会导致更高的误差。这个问题可能会自我加剧，甚至导致梯度下降过程远离最小误差区域。另一方面，如果步长太小，找到理想的解可能需要非常长的时间。
- en: Assumptions of Logistic Regression
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归的假设
- en: Since it is a classical statistical model, similar to the F-test and Pearson
    correlation we already examined, logistic regression makes certain assumptions
    about the data. While it's not necessary to follow every one of these assumptions
    in the strictest possible sense, it's good to be aware of them. That way, if a
    logistic regression model is not performing very well, you can try to investigate
    and figure out why, using your knowledge of the ideal situation that logistic
    regression is intended for. You may find slightly different lists of the specific
    assumptions from different resources. However, those that are listed here are
    widely accepted.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它是一个经典的统计模型，类似于我们已经考察过的F检验和皮尔逊相关性，逻辑回归对数据有一些假设。虽然不必严格遵循每一个假设，但了解它们是很有帮助的。这样，如果逻辑回归模型表现不佳，你可以尝试调查并找出原因，利用你对逻辑回归所期望的理想情况的理解。你可能会在不同的资源中看到略有不同的假设列表，然而这里列出的假设是被广泛接受的。
- en: '**Features Are Linear in the Log Odds**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征在对数几率中是线性的**'
- en: We learned about this assumption in the previous chapter, *Chapter 3*, *Details
    of Logistic Regression and Feature Exploration*. Logistic regression is a linear
    model, so it will only work well as long as the features are effective at describing
    a linear trend in the log odds. In particular, logistic regression won't capture
    interactions, polynomial features, or the discretization of features, on its own.
    You can, however, specify all of these as "new features" – even though they may
    be engineered from existing features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章*第3章*中学习了这个假设，*逻辑回归与特征探索的详细信息*。逻辑回归是一个线性模型，所以只要特征能够有效描述对数几率中的线性趋势，它就能很好地工作。特别地，逻辑回归无法捕捉特征之间的交互作用、多项式特征或特征的离散化。你可以将这些指定为“新特征”——即使它们可能是由现有特征衍生出来的。
- en: Remember from the previous chapter that the most important feature from univariate
    feature exploration, `PAY_1`, was not found to be linear in the log odds.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 记住上一章提到的，从单变量特征探索中，`PAY_1`特征在对数几率中并不是线性的。
- en: '**No Multicollinearity of Features**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征之间没有多重共线性**'
- en: 'Multicollinearity means that features are correlated with each other. The worst
    violation of this assumption is when features are perfectly correlated with each
    other, such as one feature being identical to another, or when one feature equals
    another multiplied by a constant. We can investigate the correlation of features
    using the correlation plot that we''re already familiar with from univariate feature
    selection. Here is the correlation plot from the previous chapter:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 多重共线性意味着特征之间存在相关性。这个假设最严重的违反情况是特征之间完全相关，例如一个特征与另一个特征完全相同，或者一个特征等于另一个特征乘以常数。我们可以使用我们已经熟悉的相关性图来调查特征的相关性，这个图也在单变量特征选择中出现过。以下是上一章的相关性图：
- en: '![Figure 4.9: A correlation plot of features and the response'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.9: 特征与响应的相关性图'
- en: '](img/B16925_4_9.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_9.jpg)'
- en: 'Figure 4.9: A correlation plot of features and the response'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4.9: 特征与响应的相关性图'
- en: 'We can see from the correlation plot what perfect correlation looks like: since
    every feature and the response variable has a correlation of 1 with itself, we
    can see that a correlation of 1 is a light, cream color. From the color bar, which
    doesn''t include -1, we know there are no correlations with that value.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从相关性图中看到完美相关的样子：由于每个特征和响应变量与其自身的相关性为1，我们可以看到1的相关性是浅色的奶油色。从颜色条中，我们可以知道没有-1的相关性。
- en: Note
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Jupyter notebook containing the code and the corresponding plots presented
    in this section can be found here: [https://packt.link/UOEMp](https://packt.link/UOEMp).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 包含本节中代码和相应图表的 Jupyter 笔记本可以在此找到：[https://packt.link/UOEMp](https://packt.link/UOEMp)。
- en: The clearest examples of correlated predictors in our case study data are the
    `BILL_AMT` features. It makes intuitive sense that bills might be similar from
    month to month for a given account. For instance, there may be an account that
    typically carries a balance of zero, or an account that has a large balance that
    is taking a while to pay off. Are any of the `BILL_AMT` features perfectly correlated?
    From *Figure 4.9*, it does not look like it. So, while these features may not
    contribute much independent information, we won't remove them at this point out
    of concern for multicollinearity.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例研究数据中，最明显的相关预测变量是`BILL_AMT`特征。直观来看，账单在同一个账户的每个月可能会相似。例如，可能有一个账户通常保持零余额，或者有一个账户存在大量余额，且需要较长时间才能还清。`BILL_AMT`特征之间是否存在完全相关？从*图
    4.9*来看，似乎没有。所以，虽然这些特征可能没有提供太多独立的信息，但我们目前不会出于担心多重共线性的原因而删除它们。
- en: '**The Independence of Observations**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**观察值的独立性**'
- en: This is a common assumption in classical statistical models, including linear
    regression. Here, the observations (or samples) are assumed to be independent.
    Does this make sense with the case study data? We'd want to confirm with our client
    whether the same individual can hold multiple credit accounts across the dataset
    and consider what to do depending on how common it was. Let's assume we've been
    told that in our data each credit account belongs to a unique person, so we may
    assume independence of observations in this respect.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这是经典统计模型中的一个常见假设，包括线性回归。在这里，假设观察值（或样本）是独立的。这个假设在案例研究数据中是否合理？我们需要与客户确认，数据集中的同一个人是否可以拥有多个信用账户，并根据这种情况的普遍性来决定如何处理。假设我们已经被告知，在我们的数据中，每个信用账户都属于唯一的人，因此我们可以假设在这一点上观察值是独立的。
- en: 'Across different domains of data, some common violations of independence of
    observations are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的数据领域中，观察值独立性的一些常见违反情况如下：
- en: '**Spatial autocorrelation** of observations; for example, in natural phenomena
    such as soil types, where observations that are geographically close to each other
    may be similar to each other.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空间自相关**的观察值；例如，在自然现象中，如土壤类型，其中地理上彼此接近的观察值可能相似。'
- en: '**Temporal autocorrelation** of observations, which may occur in time series
    data. Observations at the current point in time are usually assumed to be correlated
    to the most recent point(s) in time.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间自相关**的观察值，通常出现在时间序列数据中。在时间序列数据中，通常假设当前时刻的观察值与最近的时刻（们）相关。'
- en: However, these issues are not relevant to our case study data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些问题与我们的案例研究数据无关。
- en: '**No Outliers**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**无异常值**'
- en: Outliers are observations where the value of the feature(s) or response are
    very far from most of the data or are different in some other way. A more appropriate
    term for an outlier observation of a feature value is a high leverage point, as
    the term "outlier" is usually applied to the response variable. However, in our
    binary classification problem, it's not possible to have an outlier value of the
    response variable, since it can only take on the values 0 and 1\. In practice,
    you may see both of these terms used to refer to features.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是指特征（或响应）的值与大多数数据的差异非常大，或者在其他方面有所不同。对于特征值的异常值，更恰当的术语是高杠杆点，因为“异常值”通常用于描述响应变量。然而，在我们的二分类问题中，不可能有响应变量的异常值，因为它只能取值0或1。在实际应用中，您可能会看到这两个术语都用于描述特征。
- en: 'To see why these kinds of points can have an adverse effect on linear models
    in general, take a look at this synthetic linear data with 100 points and the
    line of best fit that results from linear regression:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么这些类型的点通常会对线性模型产生不利影响，请看这个包含100个点的合成线性数据以及由线性回归得到的最佳拟合线：
- en: '![Figure 4.10: “Well-behaved” linear data and a regression fit'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.10：“表现良好”的线性数据和回归拟合'
- en: '](img/B16925_4_10.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_10.jpg)'
- en: 'Figure 4.10: "Well-behaved" linear data and a regression fit'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10：“表现良好”的线性数据和回归拟合
- en: 'Here, the model intuitively appears to be a good fit for the data. However,
    what if an outlier feature value is added? To illustrate this, we add a point
    with an x value that is very different from most of the observations and a y value
    that is in a similar range to the other observations. We then show the resulting
    regression line:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，模型直观上看似与数据拟合得很好。然而，如果加入一个异常值特征值会怎样呢？为了说明这一点，我们添加了一个点，其x值与大多数观测值非常不同，而y值与其他观测值处于相似范围。然后，我们展示了结果回归线：
- en: '![Figure 4.11: A plot showing what happens when an outlier is included'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.11：显示当包含异常值时会发生什么的图表'
- en: '](img/B16925_4_11.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_11.jpg)'
- en: 'Figure 4.11: A plot showing what happens when an outlier is included'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11：显示当包含异常值时会发生什么的图表
- en: Due to the presence of a single high leverage point, the regression model fit
    for all the data is no longer a very good representation of much of the data.
    This shows the potential effect of just a single data point on linear models,
    especially if that point doesn't appear to follow the same trend as the rest of
    the data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在一个高杠杆点，所有数据的回归模型拟合不再很好地代表大部分数据。这展示了单个数据点对线性模型的潜在影响，特别是当该点似乎与其余数据的趋势不一致时。
- en: There are methods for dealing with outliers. But a more fundamental question
    to ask is "Is data like this realistic?". If the data doesn't seem right, it is
    a good idea to ask the client whether the outliers are believable. If not, they
    should be excluded. However, if they do represent valid data, then non-linear
    models or other methods should be used.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 处理异常值有很多方法。但一个更根本的问题是：“这样的数据现实吗？”如果数据看起来不太对，可以询问客户这些异常值是否可信。如果不可信，应该将它们排除。但如果它们代表有效的数据，则应使用非线性模型或其他方法。
- en: With our case study data, we did not observe outliers in the histograms that
    we plotted during feature exploration. Therefore, we don't have this concern.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例研究数据中，在特征探索过程中绘制的直方图中并没有观察到异常值。因此，我们没有这个顾虑。
- en: '**How Many Features Should You Include?**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**你应该包含多少个特征？**'
- en: This is not so much an assumption as it is guidance on model building. There
    is no clear-cut law that states how many features to include in a logistic regression
    model. However, a common rule of thumb is the "rule of 10," which states that
    for every 10 occurrences of the rarest outcome class, 1 feature may be added to
    the model. So, for example, in a binary logistic regression problem with 100 samples,
    if the class balance has 20% positive outcomes and 80% negative outcomes, then
    there are only 20 positive outcomes in total, and so only 2 features should be
    used in the model. A "rule of 20" has also been suggested, which would be a more
    stringent limit on the number of features to include (1 feature in our example).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这不完全是一个假设，更像是构建模型的指导原则。没有明确的定律说明在逻辑回归模型中应该包含多少个特征。然而，一个常见的经验法则是“10的法则”，即每出现10次最稀有的结果类别，就可以在模型中添加1个特征。例如，在一个包含100个样本的二分类逻辑回归问题中，如果类别平衡是20%的正样本和80%的负样本，那么正样本总数只有20个，因此模型中应该仅使用2个特征。此外，还建议采用“20的法则”，它对包含的特征数量设定了更严格的限制（在我们的例子中为1个特征）。
- en: Another point to consider in the case of binary features, such as those that
    result from one-hot encoding, is how many samples will have a positive value for
    that feature. If the feature is very imbalanced, in other words, with very few
    samples containing either a 1 or a 0, it may not make sense to include it in the
    model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的点是，对于二进制特征（例如由独热编码产生的特征），即该特征有多少样本会有正值。如果该特征非常不平衡，换句话说，包含1或0的样本非常少，那么将其纳入模型可能没有意义。
- en: For the case study data, we are fortunate to have a relatively large number
    of samples and relatively balanced features, so these are not concerns.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于案例研究数据，我们很幸运拥有相对较多的样本和较为平衡的特征，因此这些问题并不显著。
- en: Note
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The code for generating the plots presented in this section can be found here:
    [https://packt.link/SnX3y](https://packt.link/SnX3y).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中呈现的绘图代码可以在此处找到：[https://packt.link/SnX3y](https://packt.link/SnX3y)。
- en: 'The Motivation for Regularization: The Bias-Variance Trade-Off'
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化的动机：偏差-方差权衡
- en: We can extend the basic logistic regression model that we have learned about
    by using a powerful concept known as **shrinkage** or **regularization**. In fact,
    every logistic regression that you have fit so far in scikit-learn has used some
    amount of regularization. That is because it is a default option in the logistic
    regression model object. However, until now, we have ignored it.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用一种强大的概念——**收缩**或**正则化**，来扩展我们所学的基本逻辑回归模型。实际上，到目前为止，您在scikit-learn中拟合的每一个逻辑回归模型都使用了一定量的正则化。这是因为正则化是逻辑回归模型对象中的默认选项。不过，直到现在，我们一直忽视了它。
- en: 'As you learn about these concepts in greater depth, you will also become familiar
    with a few foundational concepts in machine learning: **overfitting**, **underfitting**,
    and the **bias-variance trade-off**. A model is said to overfit the training data
    if the performance of the model on the training data (for example, the ROC AUC)
    is substantially better than the performance on a held-out test set. In other
    words, good performance on the training set does not generalize to the unseen
    test set. We started to discuss these concepts in *Chapter 2*, *Introduction to
    Scikit-Learn and Model Evaluation*, when we distinguished between model training
    and test scores.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当你对这些概念有更深入的了解时，你还会熟悉一些机器学习中的基础概念：**过拟合**、**欠拟合**和**偏差-方差权衡**。如果一个模型在训练数据上的表现（例如，ROC
    AUC）远远好于在保留的测试集上的表现，那么这个模型被认为是对训练数据进行了过拟合。换句话说，在训练集上的良好表现并不能推广到未见过的测试集。我们在*第2章*，*Scikit-Learn简介与模型评估*中开始讨论这些概念，当时我们区分了模型训练分数和测试分数。
- en: 'When a model is overfitted to the training data, it is said to have high **variance**.
    In other words, whatever variability exists in the training data, the model has
    learned this very well – in fact, too well. This will be reflected in a high model
    training score. However, when such a model is used to make predictions on new
    and unseen data, the performance is lower. Overfitting is more likely in the following
    circumstances:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型对训练数据发生过拟合时，它被认为具有较高的**方差**。换句话说，训练数据中存在的任何变异性，模型都学得非常好——实际上，学得太好了。这将在较高的训练得分中得到体现。然而，当这样的模型用于对新的、未见过的数据进行预测时，其表现较差。以下情况下，过拟合的可能性更大：
- en: There are a large number of features available in relation to the number of
    samples. In particular, there may be so many possible features that it is cumbersome
    to directly inspect all of them, like we were able to do with the case study data.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用的特征数量相较于样本数量非常庞大。尤其是，可能存在如此多的特征，以至于直接检查所有特征变得繁琐，就像我们在案例研究数据中能够做到的那样。
- en: A complex model, that is, more complex than logistic regression, is used. These
    include models such as gradient boosting ensembles or neural networks.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用了更复杂的模型，即比逻辑回归更复杂的模型。这些包括梯度提升集成模型或神经网络等。
- en: Under these circumstances, the model has an opportunity develop more complex
    **hypotheses** about the relationships between features and the response variable
    in the training data during model fitting, making overfitting more likely.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型有机会在模型拟合过程中开发出关于特征与响应变量之间关系的更复杂的**假设**，从而使过拟合的可能性增加。
- en: In contrast, if a model is not fitting the training data very well, this is
    known as underfitting, and the model is said to have high **bias**.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果一个模型无法很好地拟合训练数据，这就是所谓的欠拟合，模型被认为具有较高的**偏差**。
- en: 'We can examine the differences between underfitting, overfitting, and the ideal
    that sits in between, by fitting polynomial models on some hypothetical data:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在一些假设数据上拟合多项式模型，来检查欠拟合、过拟合和理想模型之间的区别：
- en: '![Figure 4.12: Quadratic data with underfit, overfit, and ideal models'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.12：包含欠拟合、过拟合和理想模型的二次数据'
- en: '](img/B16925_4_12.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_12.jpg)'
- en: 'Figure 4.12: Quadratic data with underfit, overfit, and ideal models'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12：包含欠拟合、过拟合和理想模型的二次数据
- en: In *Figure 4.12*, we can see that including too few features, in this case,
    a linear model of *y* with just two features, a slope and an intercept, is clearly
    not a good representation of the data. This is known as an underfit model. However,
    if we include too many features, that is, many high-degree polynomial terms, such
    as *x*2, *x*3, *x*4,… *x*10, we can fit the training data almost perfectly. However,
    this is not necessarily a good thing. When we look at the results of the overfitted
    model in between the training data points, where new predictions may need to be
    made, we can see that the model is unstable and may not provide reliable predictions
    for data that was not in the training set. We can tell this just based on an intuitive
    understanding of the relationship between the features and the response variable,
    which we can get from visualizing the data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 4.12*中，我们可以看到，包含过少特征（在这种情况下，是仅有两个特征的*y*线性模型，一个斜率和一个截距）显然不是对数据的良好表示。这被称为欠拟合模型。然而，如果我们包含过多特征，即许多高次多项式项，比如*x*²、*x*³、*x*⁴、……
    *x*¹⁰，虽然可以几乎完美地拟合训练数据，但这不一定是好事。当我们观察过拟合模型在训练数据点之间的结果时，尤其是在可能需要进行新预测的地方，我们可以看到模型不稳定，并且可能无法为未出现在训练集中的数据提供可靠的预测。我们仅凭对特征与响应变量之间关系的直观理解，就能看出这一点，这种理解来自于对数据的可视化。
- en: Note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The code for generating the plots presented in this section can be found here:
    [https://packt.link/SnX3y](https://packt.link/SnX3y).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 生成本节中展示的图表的代码可以在此找到：[https://packt.link/SnX3y](https://packt.link/SnX3y)。
- en: The synthetic data for this example was generated by a second-degree (that is,
    quadratic) polynomial. Knowing this, we could easily find the ideal model by fitting
    a second-degree polynomial to the training data, as shown in *Figure 4.12*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本例中的合成数据是通过二次（即二次方）多项式生成的。知道这一点后，我们可以通过将二次多项式拟合到训练数据上，轻松找到理想模型，如*图 4.12*所示。
- en: In general, however, we won't know what the ideal model formulation is ahead
    of time. For this reason, we need to compare training and test scores to assess
    whether a model may be overfitting or underfitting.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常情况下，我们无法提前知道理想模型的公式。因此，我们需要通过比较训练和测试得分，来评估模型是否存在过拟合或欠拟合的情况。
- en: In some cases, it may be desirable to introduce some bias into the model training
    process, especially if this decreases overfitting and increases model performance
    on new, unseen data. In this way, it may be possible to leverage the bias-variance
    trade-off to improve a model. We can use **regularization** methods to accomplish
    this. Additionally, we may also be able to use these methods for **variable selection**
    as part of the modeling process. Using a predictive model to select variables
    is an alternative to the univariate feature selection methods that we've already
    explored. We begin to experiment with these concepts in the following exercise.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，引入一些偏差到模型训练过程中是可取的，特别是当这样做可以减少过拟合，并提高模型在新数据（即未见过的数据）上的表现时。通过这种方式，可能可以利用偏差-方差权衡来改善模型。我们可以使用**正则化**方法来实现这一点。此外，我们也可以将这些方法用于**变量选择**，作为建模过程的一部分。使用预测模型来选择变量，是我们之前探讨的单变量特征选择方法的替代方案。在接下来的练习中，我们将开始实验这些概念。
- en: 'Exercise 4.02: Generating and Modeling Synthetic Classification Data'
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.02：生成和建模合成分类数据
- en: 'In this exercise, we''ll observe overfitting in practice by using a synthetic
    dataset. Consider yourself in the situation of having been given a binary classification
    dataset with many candidate features (200), where you don''t have time to look
    through all of them individually. It''s possible that some of these features are
    highly correlated or related in some other way. However, with this many variables,
    it can be difficult to effectively explore all of them. Additionally, the dataset
    has relatively few samples: only 1,000\. We are going to generate this challenging
    dataset by using a feature of scikit-learn that allows you to create synthetic
    datasets for making conceptual explorations such as this. Perform the following
    steps to complete the exercise:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将通过使用合成数据集来观察过拟合现象。假设你现在面临一个二分类数据集，包含许多候选特征（200个），而你没有时间逐一检查它们。可能其中一些特征是高度相关的，或者以其他方式相互关联。然而，特征的数量如此之多，可能会使得有效地探索每个特征变得困难。此外，数据集的样本数量相对较少：只有
    1,000 个样本。我们将通过使用 scikit-learn 提供的一个功能来生成这个具有挑战性的数据集，该功能允许你创建合成数据集，用于进行此类概念性探索。请按照以下步骤完成练习：
- en: Note
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before you begin this exercise, please make sure you have executed the prerequisite
    steps of importing the necessary libraries. These steps along with the code for
    this exercise can be found at [https://packt.link/mIMsT](https://packt.link/mIMsT).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本练习之前，请确保你已经执行了导入必要库的前提步骤。这些步骤及本练习的代码可以在 [https://packt.link/mIMsT](https://packt.link/mIMsT)
    找到。
- en: 'Import the `make_classification`, `train_test_split`, `LogisticRegression`,
    and `roc_auc_score` classes using the following code:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码导入 `make_classification`、`train_test_split`、`LogisticRegression` 和 `roc_auc_score`
    类：
- en: '[PRE9]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Notice that we''ve imported several familiar classes from scikit-learn, in
    addition to a new one that we haven''t seen before: `make_classification`. This
    class does just what its name indicates – it makes data for a classification problem.
    Using the various keyword arguments, you can specify how many samples and features
    to include, and how many classes the response variable will have. There is also
    a range of other options that effectively control how "easy" the problem will
    be to solve.'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们从 scikit-learn 导入了几个熟悉的类，另外还导入了一个我们之前没有见过的新类：`make_classification`。这个类的功能正如其名所示——它用于生成分类问题的数据。通过使用各种关键字参数，你可以指定要包含多少样本和特征，以及响应变量将有多少个类别。还有一系列其他选项，可以有效控制问题的“难易程度”。
- en: Note
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: For more information, refer to [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html).
    Suffice to say that we've selected options here that make a reasonably easy-to-solve
    problem, with some curveballs thrown in. In other words, we expect high model
    performance, but we'll have to work a little bit to get it.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更多信息，请参考[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)。简单来说，我们在这里选择了使问题相对容易解决的选项，但也加入了一些复杂因素。换句话说，我们期望模型表现良好，但我们需要付出一点努力才能实现这一点。
- en: 'Generate a dataset with two variables, `x_synthetic` and `y_synthetic`. `x_synthetic`
    has the 200 candidate features, and `y_synthetic` the response variable, each
    for 1,000 samples. Use the following code:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个包含两个变量的数据集，`x_synthetic` 和 `y_synthetic`。`x_synthetic` 包含 200 个候选特征，`y_synthetic`
    包含响应变量，每个包含 1,000 个样本。使用以下代码：
- en: '[PRE10]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Examine the shape of the dataset and the class fraction of the response variable
    using the following code:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码检查数据集的形状以及响应变量的类别比例：
- en: '[PRE11]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You will obtain the following output:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '[PRE12]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After checking the shape of the output, note that we''ve generated an almost
    perfectly balanced dataset: close to a 50/50 class balance. It is also important
    to note that we''ve generated all the features so that they have the same `shift`
    and `scale` – that is, a mean of 0 with a standard deviation of 1\. Making sure
    that the features are on the same scale, or have roughly the same range of values,
    is a key point for using regularization methods – and we''ll see why later. If
    the features in a raw dataset are on widely different scales, it is advisable
    to normalize them so that they are on the same scale. Scikit-learn has the functionality
    to make this easy, which we''ll learn about in the activity at the end of this
    chapter.'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检查输出形状后，注意到我们生成了一个几乎完美平衡的数据集：类别平衡接近50/50。还需要注意的是，我们已生成所有特征，使它们具有相同的`shift`和`scale`——即均值为0，标准差为1。确保特征在相同的尺度上，或者说具有大致相同的取值范围，是使用正则化方法的关键点——稍后我们将看到为什么。如果原始数据集中的特征尺度差异较大，建议对其进行归一化，以确保它们处于相同的尺度上。Scikit-learn提供了简便的方法来实现这一点，我们将在本章末的活动中学习。
- en: 'Plot the first few features as histograms to show that the range of values
    is the same using the following code:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将前几个特征绘制为直方图，以显示它们的取值范围相同：
- en: '[PRE13]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You will obtain the following output:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您将得到以下输出：
- en: '![Figure 4.13: Histograms for the first 4 of 200 synthetic features'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.13：200个合成特征中的前4个特征的直方图'
- en: '](img/B16925_4_13.jpg)'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_4_13.jpg)'
- en: 'Figure 4.13: Histograms for the first 4 of 200 synthetic features'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.13：200个合成特征中的前4个特征的直方图
- en: Because we generated this dataset, we don't need to directly examine all 200
    features to make sure that they're on the same scale. So, what are the possible
    concerns with this dataset? The data is balanced in terms of the class fractions
    of the response variable, so we don't need to undersample, oversample, or use
    other methods that are helpful for imbalanced data. What about relationships among
    the features themselves, and the features and response variable? There are a lot
    of these relationships and it is a challenge to investigate them all directly.
    Based on our rule of thumb (that is, 1 feature allowed for every 10 samples of
    the rarest class), 200 features is too many. We have 500 observations in the rarest
    class, so by that rule, we shouldn't have more than 50 features. It's possible
    that with so many features, the model training procedure will overfit. We will
    now start to learn how to use options in the scikit-learn logistic regression
    to prevent this.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们生成了这个数据集，因此无需直接检查所有200个特征来确保它们在相同的尺度上。那么，这个数据集可能存在哪些问题呢？由于响应变量的类别比例已经平衡，因此我们无需进行欠采样、过采样或使用其他对不平衡数据有帮助的方法。那么特征之间以及特征与响应变量之间的关系呢？这些关系有很多，直接调查它们是一个挑战。根据我们的经验法则（即每10个稀有类别样本对应1个特征），200个特征过多。我们在最稀有类别中有500个观察值，所以根据这个规则，我们不应该有超过50个特征。特征数量过多可能会导致模型训练过程过拟合。接下来，我们将开始学习如何在scikit-learn的逻辑回归中使用选项来防止这种情况发生。
- en: 'Split the data into training and test sets using an 80/20 split, and then instantiate
    a logistic regression model object using the following code:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用80/20的比例将数据拆分为训练集和测试集，然后使用以下代码实例化一个逻辑回归模型对象：
- en: '[PRE14]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice here that we are specifying some new options in the logistic regression
    model, which, so far, we have not paid attention to. First, we specify the `penalty`
    argument to be `l1`. This means we are going to use `C` parameter to be equal
    to 1,000\. `C` is the "inverse of regularization strength," according to the scikit-learn
    documentation ([https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)).
    This means that higher values of `C` correspond to less regularization. By choosing
    a relatively large number, such as 1,000, we are using relatively little regularization.
    The default value of `C` is 1\. So, we are not really using much regularization
    here, rather, we are simply becoming familiar with the options to do so. Finally,
    we are using the `liblinear` solver, which we have used in the past.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们在逻辑回归模型中指定了一些新的选项，这是我们之前未关注的。首先，我们将`penalty`参数设置为`l1`。这意味着我们将使用`C`参数，值为
    1,000。根据 scikit-learn 文档（[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)），`C`是“正则化强度的倒数”。这意味着较大的`C`值对应较少的正则化。通过选择相对较大的数值，如
    1,000，我们使用的是相对较少的正则化。`C`的默认值是 1。所以，我们这里实际上并没有使用太多正则化，而只是熟悉如何使用这些选项。最后，我们使用`liblinear`求解器，这是我们以前使用过的。
- en: Although we happen to be using scaled data here (all features have a mean of
    0 and standard deviation of 1), it's worth noting at this point that among the
    various options we have available for solvers, `liblinear` is "robust to unscaled
    data." Also note that `liblinear` is one of only two solver options that support
    the L1 penalty – the other option being `saga`.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管我们这里使用的是经过缩放的数据（所有特征的均值为0，标准差为1），但值得注意的是，在我们可用的各种求解器选项中，`liblinear`是“对未缩放数据具有鲁棒性的”。另外要注意的是，`liblinear`是唯一支持L1惩罚的两个求解器选项之一，另一个选项是`saga`。
- en: Note
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You can find out more information on available solvers at [https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression).
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以在[https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)上了解更多关于可用求解器的信息。
- en: 'Fit the logistic regression model on the training data using the following
    code:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码在训练数据上拟合逻辑回归模型：
- en: '[PRE15]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is the output:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE16]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Calculate the training score using this code by first getting predicted probabilities
    and then obtaining the ROC AUC:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码计算训练得分，首先获取预测概率，然后得到 ROC AUC：
- en: '[PRE17]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output should be as follows:'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE18]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Calculate the test score similar to how the training score was obtained:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与计算训练得分相似的方法计算测试得分：
- en: '[PRE19]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output should be as follows:'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE20]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: From these results, it's apparent that the logistic regression model has overfit
    the data. That is, the ROC AUC score on the training data is substantially higher
    than that of the test data.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从这些结果来看，很明显，逻辑回归模型已经过拟合数据。也就是说，训练数据上的 ROC AUC 得分远高于测试数据上的得分。
- en: Lasso (L1) and Ridge (L2) Regularization
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lasso (L1) 和 Ridge (L2) 正则化
- en: Before applying regularization to a logistic regression model, let's take a
    moment to understand what regularization is and how it works. The two ways of
    regularizing logistic regression models in scikit-learn are called `penalty =
    'l1'` or `'l2'`. These are called "penalties" because the effect of regularization
    is to add a penalty, or a cost, for having larger values of the coefficients in
    a fitted logistic regression model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在将正则化应用于逻辑回归模型之前，我们先花点时间理解什么是正则化以及它是如何工作的。在 scikit-learn 中，正则化逻辑回归模型的两种方式分别叫做`penalty
    = 'l1'`或`'l2'`。它们被称为“惩罚”，因为正则化的作用是增加惩罚或成本，以防止逻辑回归模型中系数的值过大。
- en: 'As we''ve already learned, coefficients in a logistic regression model describe
    the relationship between the log odds of the response and each of the features.
    Therefore, if a coefficient value is particularly large, then a small change in
    that feature will have a large effect on the prediction. When a model is being
    fit and is learning the relationship between features and the response variable,
    the model can start to learn the noise in the data. We saw this previously in
    *Figure 4.12*: if there are many features available when fitting a model, and
    there are no guardrails on the values that their coefficients can take, then the
    model fitting process may try to discover relationships between the features and
    the response variable that won''t generalize to new data. In this way, the model
    becomes tuned to the unpredictable, random noise that accompanies real-world,
    imperfect data. Unfortunately, this only serves to increase the model''s skill
    at predicting the training data, which is not our ultimate goal. Therefore, we
    should seek to root out such spurious relationships from the model.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经学到的，逻辑回归模型中的系数描述了响应变量的对数几率与每个特征之间的关系。因此，如果某个系数值特别大，那么该特征的微小变化将在预测中产生较大的影响。当模型正在拟合并学习特征与响应变量之间的关系时，模型可能开始学习数据中的噪声。我们之前在*图
    4.12*中看到过这一点：如果在拟合模型时可用的特征很多，并且没有对它们系数值施加限制，那么模型拟合过程可能会试图发现特征与响应变量之间的关系，这些关系无法推广到新数据。这样，模型就会变得更适应现实世界中不完美数据中的不可预测的随机噪声。不幸的是，这只会提高模型对训练数据的预测能力，而这并不是我们的最终目标。因此，我们应该努力从模型中剔除这些虚假的关系。
- en: 'Lasso and ridge regularization use different mathematical formulations to accomplish
    this goal. These methods work by making changes to the cost function that is used
    for model fitting, which we introduced previously as the log-loss function. Lasso
    regularization uses what is called the **1-norm** (hence the term L1):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso 和岭回归正则化使用不同的数学公式来实现这一目标。这些方法通过对模型拟合时使用的成本函数进行修改来工作，我们之前介绍过这个函数是对数损失函数。Lasso正则化使用的是所谓的**1-范数**（因此也叫L1）：
- en: '![Figure 4.14: Log-loss equation with lasso penalty'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.14：带Lasso惩罚的对数损失方程'
- en: '](img/B16925_4_14.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_14.jpg)'
- en: 'Figure 4.14: Log-loss equation with lasso penalty'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14：带Lasso惩罚的对数损失方程
- en: The 1-norm, which is the first term in the equation in *Figure 4.14*, is just
    the sum of the absolute values of the coefficients of the *m* different features.
    The absolute value is used because having a coefficient that's large in either
    the positive or negative directions can contribute to overfitting. So, what else
    is different about this cost function compared to the log-loss function that we
    saw earlier? Well, now there is a *C* factor that is multiplied by the fraction
    in front of the sum of the log-loss function.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 1-范数，即*图 4.14*中方程的第一项，实际上是* m *个不同特征系数绝对值的和。使用绝对值是因为无论系数是正向还是负向过大，都可能导致过拟合。那么，这个成本函数与我们之前看到的对数损失函数相比，有什么不同呢？嗯，现在有一个*C*因子，它乘以了对数损失函数前面分数的部分。
- en: This is the "inverse of regularization strength," as described in the scikit-learn
    documentation ([https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)).
    Since this factor is in front of the term of the cost function that calculates
    the prediction error, as opposed to the term that does regularization, then making
    it larger makes the prediction error more important in the cost function, while
    regularization is made less important. In short, *larger values of C lead to less
    regularization* in the scikit-learn implementation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这是“正则化强度的倒数”，正如scikit-learn文档中所描述的（[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)）。由于这个因子位于计算预测误差的成本函数项前面，而不是正则化项前面，因此增大它会让预测误差在成本函数中变得更加重要，而正则化则变得不那么重要。简而言之，*在scikit-learn实现中，C值越大，正则化越少*。
- en: 'L2, or ridge regularization, is similar to L1, except that instead of the sum
    of absolute values of coefficients, ridge uses the sum of their squares, called
    the **2-norm**:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: L2 或岭回归正则化类似于L1正则化，不同之处在于，岭回归使用的是系数的平方和，而不是绝对值之和，这个平方和被称为**2-范数**：
- en: '![Figure 4.15: Log-loss equation with ridge penalty'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.15：带岭回归惩罚的对数损失方程'
- en: '](img/B16925_4_15.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_15.jpg)'
- en: 'Figure 4.15: Log-loss equation with ridge penalty'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15：带脊回归惩罚的对数损失方程
- en: Note that if you look at the cost functions for logistic regression in the scikit-learn
    documentation, the specific form is different than what is used here, but the
    overall idea is similar. Additionally, after you become comfortable with the concepts
    of lasso and ridge penalties, you should be aware that there is an additional
    regularization method called **elastic-net**, which is a combination lasso and
    ridge.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你查看 scikit-learn 文档中的逻辑回归成本函数，具体形式与这里使用的不同，但总体思路是相似的。此外，在你熟悉了套索（lasso）和脊回归（ridge）惩罚的概念之后，你应该知道还有一种叫做
    **弹性网（elastic-net）** 的额外正则化方法，它是套索和脊回归的结合。
- en: '**Why Are There Two Different Formulations of Regularization?**'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么正则化有两种不同的公式？**'
- en: 'It may be that one or the other will provide better out-of-sample performance,
    so you may wish to test them both. There is another key difference in these methods:
    the L1 penalty also performs feature selection, in addition to regularization.
    It does this by setting some coefficient values to exactly zero during the regularization
    process, effectively removing features from the model. L2 regularization makes
    the coefficient values smaller but does not completely eliminate them. Not all
    solver options in scikit-learn support both L1 and L2 regularization, so you will
    need to select an appropriate solver for the regularization technique you want
    to use.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 可能其中一个方法会提供更好的样本外表现，因此你可能希望同时测试这两种方法。这些方法之间还有另一个关键差异：L1 惩罚除了执行正则化外，还进行特征选择。它通过在正则化过程中将某些系数值设置为零，从而有效地从模型中去除这些特征。L2
    正则化则是将系数值变小，但不会完全消除它们。并非所有的求解器选项都支持 L1 和 L2 正则化，因此你需要为你想使用的正则化技术选择合适的求解器。
- en: Note
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The mathematical details of why L1 regularization removes features but L2 doesn't
    are beyond the scope of this book. However, for a more thorough explanation of
    this topic and further reading in general, we recommend the very readable (and
    free) resource, *An Introduction to Statistical Learning by Gareth James*, et
    al. In particular, see *page 222* of the corrected 7th printing, for a helpful
    graphic on the difference between L1 and L2 regularization.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么 L1 正则化会去除特征，而 L2 不会的数学原理超出了本书的范围。然而，关于这个话题的更深入解释以及进一步的阅读，我们推荐一本非常易读（且免费的）资源——*Gareth
    James 等人编著的《统计学习导论》*。特别是，参见修订版第七印刷的 *第 222 页*，上面有一幅有助于理解 L1 和 L2 正则化差异的图示。
- en: '**Intercepts and Regularization**'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**截距和正则化**'
- en: We have not discussed intercepts very much, other than to note that we have
    been estimating them with our linear models, along with the coefficients that
    go with each feature. So, should you use an intercept? The answer is probably
    yes, until you've developed an advanced understanding of linear models and are
    certain that in a specific case you should not. However, such cases do exist,
    for example, in a linear regression where the features and the response variable
    have all been normalized to have a mean of zero.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并没有过多讨论截距，除了提到我们已经通过线性模型估计了它们，以及与每个特征相关的系数。那么，应该使用截距吗？答案可能是肯定的，直到你对线性模型有了更深入的理解，并确信在特定情况下不需要使用它。然而，确实存在这样的情况，例如在一个特征和响应变量都已归一化为零均值的线性回归模型中。
- en: Intercepts don't go with any particular feature. Therefore, it doesn't make
    much sense to regularize them, as they shouldn't contribute to overfitting. Notice
    that in the regularization penalty term for L1, the summation starts with *j =
    1*, and similarly for L2, we have skipped *σ*0, which is the intercept term.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 截距与任何特定特征无关。因此，对其进行正则化没有太大意义，因为它不应该有助于过拟合。请注意，在 L1 的正则化惩罚项中，求和从 *j = 1* 开始，同样在
    L2 中，我们跳过了 *σ*0，这就是截距项。
- en: 'This is the ideal situation: not regularizing the intercept. However, some
    of the solvers in scikit-learn, such as `liblinear`, actually do regularize the
    intercept. There is an `intercept_scaling` option that you can supply to the model
    class to counteract this effect. We have not illustrated this here as, although
    it is theoretically incorrect, regularizing the intercept often does not have
    much effect on the model''s predictive quality in practice.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这是理想的情况：不对截距进行正则化。然而，scikit-learn 中的一些求解器，如 `liblinear`，实际上会对截距进行正则化。你可以通过提供一个
    `intercept_scaling` 选项来对抗这一效应。我们在这里没有展示这一点，因为虽然从理论上讲，这样做是不正确的，但在实践中，正则化截距通常对模型的预测质量影响不大。
- en: '**Scaling and Regularization**'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**缩放与正则化**'
- en: As noted in the previous exercise, it is best practice to `LIMIT_BAL` in our
    dataset, is much larger than other features, such as `PAY_1`, it may, in fact,
    be desirable to have a larger value for the coefficient of `PAY_1` and a smaller
    value for that of `LIMIT_BAL` in order to put their effects on the same scale
    in the linear combination of features and coefficients that are used for model
    prediction. Normalizing all the features before using regularization avoids complications
    such as this that arise simply from differences in scale.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个练习所述，最佳实践是`LIMIT_BAL`在我们的数据集中远大于其他特征，比如`PAY_1`，实际上，可能希望为`PAY_1`的系数赋予较大的值，而为`LIMIT_BAL`的系数赋予较小的值，从而使它们在特征和系数的线性组合中对模型预测的影响处于相同的尺度。通过在使用正则化之前对所有特征进行标准化，可以避免因尺度差异而引发的此类复杂问题。
- en: In fact, scaling your data may also be necessary, depending on which solver
    you are using. The different variations on the gradient descent process available
    in scikit-learn may or may not be able to work effectively with unscaled data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，缩放数据可能也是必要的，这取决于你使用的求解器。scikit-learn中可用的不同梯度下降变体可能无法有效处理未缩放的数据。
- en: '**The Importance of Selecting the Right Solver**'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**选择合适求解器的重要性**'
- en: 'As we''ve come to learn, the different solvers available for logistic regression
    in scikit-learn have different behaviors regarding the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所了解的，scikit-learn中可用的不同逻辑回归求解器在以下方面有不同的表现：
- en: Whether they support both L1 and L2 regularization
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是否支持L1和L2正则化
- en: How they treat the intercept during regularization
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们如何在正则化过程中处理截距
- en: How they deal with unscaled data
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们如何处理未缩放的数据
- en: Note
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: There are other differences as well. A helpful table comparing these and other
    traits is available at [https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression).
    You can use this table to decide which solver is appropriate for your problem.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 还有其他的区别。一个有用的表格比较了这些和其他特性，可以参考[https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)。你可以使用这个表格来决定哪个求解器最适合你的问题。
- en: To summarize this section, we have learned the mathematical foundations of lasso
    and ridge regularization. *These methods work by shrinking the coefficient values
    toward 0, and in the case of the lasso, setting some coefficients to exactly 0
    and thus performing feature selection*. You can imagine that in our example of
    overfitting in *Figure 4.12*, if the complex, overfitted model had some coefficients
    shrunk toward 0, it would look more like the ideal model, which has fewer coefficients.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这一部分内容，我们学习了lasso和ridge正则化的数学基础。*这些方法通过将系数值收缩到接近0来工作，在lasso的情况下，还会将某些系数精确地设为0，从而执行特征选择*。你可以想象，在我们*图4.12*中的过拟合例子中，如果复杂的过拟合模型将一些系数收缩到接近0，它将更像理想模型，而理想模型的系数较少。
- en: 'Here is a plot of a regularized regression model, using the same high-degree
    polynomial features as the overfitted model, but with a ridge penalty:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个正则化回归模型的图示，使用与过拟合模型相同的高阶多项式特征，但加上了脊岭惩罚：
- en: '![Figure 4.16: An overfit model and regularized model using the same features'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.16：一个过拟合模型和使用相同特征的正则化模型'
- en: '](img/B16925_4_16.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_16.jpg)'
- en: 'Figure 4.16: An overfit model and regularized model using the same features'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16：一个过拟合模型和使用相同特征的正则化模型
- en: The regularized model looks similar to the ideal model, demonstrating the ability
    of regularization to correct overfitting. Note, however, that the regularized
    model should not be recommended for extrapolation. Here, we can see that the regularized
    model starts to increase toward the right side of *Figure 4.16*. This increase
    should be viewed with suspicion, as there is nothing in the training data that
    makes it clear that this would be expected. This is an example of the general
    view that the *extrapolation of model predictions outside the range of training
    data is not recommended*. However, it is clear from *Figure 4.16* that even if
    we didn't have knowledge of the model that was used to generate this synthetic
    data (as we typically don't have knowledge of the data-generating process in real-world
    predictive modeling work), we can still use regularization to reduce the effect
    of overfitting when a large number of candidate features are available.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化后的模型看起来类似于理想模型，展示了正则化纠正过拟合的能力。然而，需要注意的是，正则化模型不应推荐用于外推。在此，我们可以看到正则化模型在*图4.16*的右侧开始增加。这个增加应该被视为可疑，因为训练数据中没有任何迹象表明这是可以预期的。这是*不推荐对超出训练数据范围的模型预测进行外推*的一般观点的一个例子。然而，从*图4.16*可以清楚地看到，即使我们没有关于生成这些合成数据的模型的知识（因为在现实世界的预测建模工作中，我们通常没有数据生成过程的知识），我们仍然可以使用正则化来减少在有大量候选特征时的过拟合影响。
- en: '**Model and Feature Selection**'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型与特征选择**'
- en: 'L1 regularization is one way to use a model, such as logistic regression, to
    perform feature selection. Other methods include forward or backward **stepwise
    selection** from the pool of candidate features. Here is the high-level idea behind
    these methods: in the case of **forward selection**, features are added to the
    model one at a time, and the out-of-sample performance is observed along the way.
    At each iteration, the addition of all possible features from the candidate pool
    is considered, and the one resulting in the greatest increase in the out-of-sample
    performance is chosen. When adding additional features ceases to improve the model''s
    performance, no more features need to be added from the candidates. In the case
    of **backward selection**, you first start with all the features in the model
    and determine which one you should remove: the one resulting in the smallest decrease
    in the out-of-sample performance. You can continue removing features in this way
    until the performance begins to decrease appreciably.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: L1正则化是一种使用模型（如逻辑回归）进行特征选择的方法。其他方法包括从候选特征池中进行前向或后向**逐步选择**。这些方法背后的高层次思想如下：在**前向选择**的情况下，特征一个一个地添加到模型中，并观察外样本性能的变化。在每次迭代时，都会考虑将所有候选池中的特征添加到模型中，并选择能够最大化外样本性能提升的特征。当添加更多特征不再改善模型性能时，就不需要再从候选特征中添加更多特征。在**后向选择**的情况下，首先从模型中开始使用所有特征，并确定应该删除哪个特征：删除后对外样本性能影响最小的特征。你可以继续按这种方式删除特征，直到性能开始显著下降。
- en: Note
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The code for generating the plots presented in this section can be found here:
    [https://packt.link/aUBMb](https://packt.link/aUBMb).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中展示的生成图表的代码可以在此找到：[https://packt.link/aUBMb](https://packt.link/aUBMb)。
- en: 'Cross-Validation: Choosing the Regularization Parameter'
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证：选择正则化参数
- en: By now, you may suspect that we could use regularization in order to decrease
    the overfitting we observed when we tried to model the synthetic data in *Exercise
    4.02*, *Generating and Modeling Synthetic Classification Data*. The question is,
    how do we choose the regularization parameter, *C*? *C* is an example of a model
    **hyperparameter**. Hyperparameters are different from the parameters that are
    estimated when a model is trained, such as the coefficients and the intercept
    of a logistic regression. Rather than being estimated by an automated procedure
    like the parameters are, hyperparameters are input directly by the user as keyword
    arguments, typically when instantiating the model class. So, how do we know what
    values to choose?
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能会怀疑我们是否可以使用正则化来减少在尝试对*练习4.02*中的合成数据建模时观察到的过拟合现象，*生成与建模合成分类数据*。问题是，我们该如何选择正则化参数*C*呢？*C*是一个模型的**超参数**示例。超参数与在训练模型时估计的参数不同，例如逻辑回归的系数和截距。超参数不像参数那样通过自动化程序估计，而是由用户直接输入作为关键字参数，通常在实例化模型类时进行输入。那么，我们如何知道应该选择什么值呢？
- en: Hyperparameters are more difficult to estimate than parameters. This is because
    it is up to the data scientist to determine what the best value is, as opposed
    to letting an optimization algorithm find it. However, it is possible to programmatically
    choose hyperparameter values, which could be viewed as an optimization procedure
    in its own right. Practically speaking, in the case of the regularization parameter
    *C*, this is most commonly done by fitting the model on one set of data with a
    particular value of *C*, determining model training performance, and then assessing
    the out-of-sample performance on another set of data.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数比参数更难估算。这是因为数据科学家需要决定最佳值，而不是让优化算法来寻找它。然而，程序化选择超参数值是可能的，这可以被视为一种优化过程。从实际角度看，在正则化参数*C*的情况下，最常见的做法是，使用特定的*C*值在一组数据上拟合模型，确定模型的训练性能，然后在另一组数据上评估*out-of-sample*性能。
- en: We are already familiar with the concept of using model training and test sets.
    However, there is a key difference here; for instance, what would happen if we
    were to use the test set multiple times in order to see the effect of different
    values of *C*?
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经熟悉使用模型训练集和测试集的概念。然而，这里有一个关键的区别；例如，如果我们多次使用测试集，以查看不同*C*值的效果，会发生什么？
- en: It may occur to you that after the first time you use the unseen test set to
    assess the out-of-sample performance for a particular value of *C*, it is no longer
    an "unseen" test set. While only the training data was used for estimating the
    model parameters (that is, the coefficients and the intercept), now the test data
    is being used to estimate the hyperparameter *C*. Effectively, the test data has
    now become additional training data in the sense that it is being used to find
    a good value for the hyperparameter.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想到，在第一次使用未见过的测试集来评估特定值的*out-of-sample*性能后，它就不再是“未见过”的测试集了。虽然在估算模型参数（即系数和截距）时仅使用了训练数据，但现在测试数据被用来估算超参数*C*。实际上，测试数据已经变成了额外的训练数据，因为它用于寻找超参数的最佳值。
- en: 'For this reason, it is common to divide the data into three parts: a training
    set, a test set, and a **validation set**. The validation set serves multiple
    purposes:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通常将数据分为三部分：训练集、测试集和**验证集**。验证集有多个用途：
- en: '**Estimating Hyperparameters**'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**估算超参数**'
- en: The validation set can be repeatedly used to assess the out-of-sample performance
    with different hyperparameter values to select hyperparameters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集可以反复使用，以评估不同超参数值的*out-of-sample*性能，从而选择超参数。
- en: '**A Comparison of Different Models**'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**不同模型的比较**'
- en: In addition to finding hyperparameter values for a model, the validation set
    can be used to estimate the out-of-sample performance of different models; for
    example, if we wanted to compare logistic regression to random forest.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为模型找到超参数值外，验证集还可以用来估算不同模型的*out-of-sample*性能；例如，如果我们想将逻辑回归与随机森林进行比较。
- en: Note
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**Data Management Best Practices**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据管理最佳实践**'
- en: As a data scientist, it's up to you to figure out how to divide up your data
    for different predictive modeling tasks. In the ideal case, you should reserve
    a portion of your data for the very end of the process, after you've already selected
    model hyperparameters and also selected the best model. This **unseen test set**
    is reserved for the last step, when it can be used to assess the endpoint of your
    model-building efforts, to see how the final model generalizes to new unseen data.
    When reserving the test set, it is good practice to make sure that the features
    and responses have similar characteristics to the rest of the data. In other words,
    the class fraction should be the same, and the distribution of features should
    be similar. This way, the test data should be representative of the data you built
    the model with.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，如何划分数据以进行不同的预测建模任务是你的责任。在理想情况下，你应该保留一部分数据用于流程的最后阶段，即在你已经选择了模型超参数并确定了最佳模型之后。这**未见过的测试集**被保留到最后一步，可以用来评估你模型构建工作的最终结果，查看最终模型如何泛化到新的未见数据。在保留测试集时，最好确保特征和响应的特性与其余数据相似。换句话说，类别比例应该相同，特征的分布应该相似。这样，测试数据就能代表你用来构建模型的数据。
- en: While model validation is a good practice, it raises the question of whether
    the particular split we choose for the training, validation, and test data has
    any effect on the outcomes that we are tracking. For example, perhaps the relationship
    between the features and the response variable is slightly different in the unseen
    test set that we have reserved, or in the validation set, versus the training
    set. It is likely impossible to eliminate all such variability, but we can use
    the method of **cross-validation** to avoid placing too much faith in one particular
    split of the data.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型验证是一个好习惯，但它引发了一个问题：我们为训练集、验证集和测试集选择的特定拆分，是否对我们跟踪的结果有任何影响。例如，也许特征和响应变量之间的关系在我们保留的未见测试集或验证集与训练集之间略有不同。要消除所有此类变异几乎是不可能的，但我们可以使用**交叉验证**的方法，以避免对某一特定数据拆分过度依赖。
- en: 'Scikit-learn provides convenient functions to facilitate cross-validation analyses.
    These functions play a similar role to `train_test_split`, which we have already
    been using, although the default behavior is somewhat different. Let''s get familiar
    with them now. First, import these two classes:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 提供了便捷的函数来促进交叉验证分析。这些函数与我们已经使用的 `train_test_split` 起到类似的作用，尽管默认行为有些不同。现在让我们来熟悉它们。首先，导入这两个类：
- en: '[PRE21]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Similar to `train_test_split`, we need to specify what proportion of the dataset
    we would like to use for training versus testing. However, with cross-validation
    (specifically the **k-fold cross-validation** that was implemented in the classes
    we just imported), rather than specifying a proportion directly, we simply indicate
    how many folds we would like – that is, the "**k folds**." The idea here is that
    the data will be divided into **k** equal proportions. For example, if we specify
    4 folds, then each fold will have 25% of the data. These folds will be the test
    data in four separate instances of model training, while the remaining 75% from
    each fold will be used to train the model. In this procedure, each data point
    gets used as training data a total of *k - 1* times, and as test data only once.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 `train_test_split`，我们需要指定数据集用于训练和测试的比例。然而，在交叉验证中（特别是我们刚刚导入的类中实现的**k折交叉验证**），我们不直接指定比例，而是简单地指明我们希望有多少个折叠——即“**k折**”。这里的想法是，数据将被分成**k**个相等的部分。例如，如果我们指定4个折叠，那么每个折叠将包含25%的数据。这些折叠将在四个单独的模型训练实例中作为测试数据，而每个折叠的其余75%将用于训练模型。在此过程中，每个数据点总共会作为训练数据使用
    *k - 1* 次，而仅作为测试数据使用一次。
- en: 'When instantiating the class, we indicate the number of folds, whether or not
    to shuffle the data before splitting, and a random seed if we want repeatable
    results across different runs:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化该类时，我们指定了折数、是否在拆分数据前进行洗牌，以及是否设置随机种子，以确保在不同运行中得到可重复的结果：
- en: '[PRE22]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here, we''ve instantiated an object with four folds and no shuffling. The way
    in which we use the object that is returned, which we''ve called `k_folds`, is
    by passing the features and response data that we wish to use for cross-validation,
    to the `.split` method of this object. This outputs an `X_syn_train` and `y_syn_train`,
    we could loop through the splits like this:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们实例化了一个具有四个折叠且没有洗牌的对象。我们使用返回的对象（我们称之为 `k_folds`）的方法是将我们希望用于交叉验证的特征数据和响应数据传递给该对象的
    `.split` 方法。这会输出 `X_syn_train` 和 `y_syn_train`，我们可以像这样遍历这些拆分：
- en: '[PRE23]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The iterator will return the row indices of `X_syn_train` and `y_syn_train`,
    which we can use to index the data. Inside this `for` loop, we can write code
    to use these indices to select data for repeatedly training and testing a model
    object with different subsets of the data. In this way, we can get a robust indication
    of the out-of-sample performance when using one particular hyperparameter value,
    and then repeat the whole process using another hyperparameter value. Consequently,
    the cross-validation loop may sit **nested** inside an outer loop over different
    hyperparameter values. We'll illustrate this in the following exercise.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代器将返回 `X_syn_train` 和 `y_syn_train` 的行索引，我们可以用这些索引来获取数据。在这个 `for` 循环内部，我们可以编写代码，使用这些索引反复选择数据进行模型训练和测试，使用不同的数据子集。通过这种方式，我们可以获得一个稳健的模型外表现指标，当使用某一特定超参数值时，然后重复整个过程，使用另一个超参数值。因此，交叉验证循环可能会**嵌套**在一个外部的超参数值循环中。我们将在下面的练习中演示这一点。
- en: 'First though, what do these splits look like? If we were to simply plot the
    indices from `train_index` and `test_index` as different colors, we would get
    something that looks like this:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，首先，这些拆分看起来是什么样子的？如果我们只是将`train_index`和`test_index`的索引用不同的颜色绘制出来，我们将得到如下图所示的效果：
- en: '![Figure 4.17: Training/test splits for k-folds with four folds and no shuffling'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.17：没有打乱的四折k折训练/测试拆分'
- en: '](img/B16925_4_17.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_17.jpg)'
- en: 'Figure 4.17: Training/test splits for k-folds with four folds and no shuffling'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17：没有打乱的四折k折训练/测试拆分
- en: 'Here, we see that with the options we''ve indicated for the `KFold` class,
    the procedure has simply taken the first 25% of the data, according to the order
    of rows, as the first test fold, then the next 25% of data for the second fold,
    and so on. But what if we wanted stratified folds? In other words, what if we
    wanted to ensure that the class fractions of the response variable were equal
    in every fold? While `train_test_split` allows this option as a keyword argument,
    there is a separate `StratifiedKFold` class that implements this for cross-validation.
    We can illustrate how the stratified splits will appear as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，按照我们为`KFold`类指定的选项，程序简单地将数据的前25%（按行的顺序）作为第一个测试折，然后将下一个25%的数据作为第二个折，依此类推。但如果我们想要分层抽样折呢？换句话说，如果我们希望确保每个折中的响应变量的类别比例相等呢？虽然`train_test_split`允许通过关键字参数实现这个选项，但有一个独立的`StratifiedKFold`类，它为交叉验证实现了这个功能。我们可以通过以下方式来说明分层拆分的效果：
- en: '[PRE24]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Figure 4.18: Training/test splits for stratified k-folds'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.18：分层k折训练/测试拆分'
- en: '](img/B16925_4_18.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_18.jpg)'
- en: 'Figure 4.18: Training/test splits for stratified k-folds'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18：分层k折训练/测试拆分
- en: In *Figure 4.18*, we can see that there has been some amount of "shuffling"
    between the different folds. The procedure has moved samples between folds as
    necessary to ensure that the class fractions in each fold are equal.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4.18*中，我们可以看到不同折之间已经进行了一定程度的“打乱”。程序根据需要在折之间移动样本，以确保每个折中的类别比例相等。
- en: Now, what if we want to shuffle the data to choose samples from throughout the
    range of indices for each test fold? First, why might we want to do this? Well,
    with the synthetic data that we've created for our problem, we can be certain
    that the data is in no particular order. However, in many real-world situations,
    the data we receive may be sorted in some way.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们想要将数据打乱，以便从每个测试折中选择整个索引范围的样本，该怎么办呢？首先，为什么我们想这么做？嗯，对于我们为这个问题创建的合成数据，我们可以确定数据是没有特定顺序的。然而，在许多实际情况下，我们收到的数据可能以某种方式进行了排序。
- en: For instance, perhaps the rows of the data have been ordered by the date an
    account was created, or by some other logic. Therefore, it can be a good idea
    to shuffle the data before splitting. This way, any traits that might have been
    used for sorting can be expected to be consistent throughout the folds. Otherwise,
    the data in different folds may have different characteristics, possibly leading
    to different relationships between features and response.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，数据的行可能是按账户创建日期排序的，或按其他逻辑排序的。因此，在拆分数据之前先打乱数据可能是个好主意。这样，任何可能被用于排序的特征，应该在每个折中都能保持一致。否则，不同折中的数据可能会有不同的特征，可能导致特征与响应之间的关系不同。
- en: 'This can lead to a situation where model performance is uneven between the
    folds. In order to "mix up" the folds throughout all the row indices of a dataset,
    all we need to do is set the `shuffle` parameter to `True`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能导致模型在不同折之间的表现不均衡。为了在数据集的所有行索引中“打乱”折，只需要将`shuffle`参数设置为`True`：
- en: '[PRE25]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Figure 4.19: Training/test splits for stratified k-folds with shuffling'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.19：带有打乱的分层k折训练/测试拆分'
- en: '](img/B16925_4_19.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_19.jpg)'
- en: 'Figure 4.19: Training/test splits for stratified k-folds with shuffling'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.19：带有打乱的分层k折训练/测试拆分
- en: With shuffling, the test folds are spread out randomly, and fairly evenly, across
    the indices of the input data.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 通过打乱，测试折会随机且均匀地分布在输入数据的索引上。
- en: K-fold cross-validation is a widely used method in data science. However, the
    choice of how many folds to use depends on the particular dataset at hand. Using
    a smaller number of folds means that the amount of training data in each fold
    will be relatively small. Therefore, this increases the chances that the model
    will underfit, as models generally work better when trained on more data. It's
    a good idea to try a few different numbers of folds and see how the mean and the
    variability of the k-fold test score changes. Common numbers of folds can range
    anywhere from 4 or 5 to 10.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: K折交叉验证是数据科学中广泛使用的一种方法。然而，选择使用多少折数取决于手头的特定数据集。使用较小的折数意味着每个折中的训练数据量相对较小。因此，这增加了模型过拟合的机会，因为模型通常在更多数据的训练下效果更好。建议尝试几种不同的折数，看看k折测试分数的均值和变异性如何变化。常见的折数范围通常是从4或5到10。
- en: In the event of a very small dataset, it may be necessary to use as much data
    as possible for training in the cross-validation folds. In this scenario, you
    can use a method called **leave-one-out cross-validation** (**LOOCV**). In LOOCV,
    the test set for each fold consists of a single sample. In other words, there
    will be as many folds as there are samples in the training data. For each iteration,
    the model is trained on all but one sample, and a prediction is made for that
    sample. The accuracy, or other performance metric, can then be constructed using
    these predictions.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集非常小的情况下，可能需要在交叉验证折中尽可能多地使用数据进行训练。在这种情况下，可以使用一种叫做**留一法交叉验证**（**LOOCV**）的方法。在LOOCV中，每个折的测试集由一个单一的样本组成。换句话说，折数将与训练数据中的样本数量相同。在每次迭代中，模型会在除一个样本外的所有样本上进行训练，并对该样本进行预测。然后，可以根据这些预测来构建准确度或其他性能指标。
- en: Other concerns that relate to the creation of a test set, such as choosing an
    out-of-time test set for problems where observations from the past must be used
    to predict future events, also apply to cross-validation.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 与创建测试集相关的其他问题，如为那些需要使用过去的观测值来预测未来事件的问题选择超时测试集，也同样适用于交叉验证。
- en: In *Exercise 4.02*, *Generating and Modeling Synthetic Classification Data*,
    we saw that fitting a logistic regression on our training data led to overfitting.
    Indeed, the test score (*ROC AUC = 0.81*) was substantially lower than the training
    score (*ROC AUC = 0.94*). We had essentially used very little or no regularization
    by setting the regularization parameter *C* to a relatively large value (1,000).
    Now we will see what happens when we vary *C* through a wide range of values.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习 4.02*，*生成和建模合成分类数据*中，我们看到对训练数据拟合逻辑回归导致了过拟合。事实上，测试分数（*ROC AUC = 0.81*）明显低于训练分数（*ROC
    AUC = 0.94*）。我们实际上使用了非常少或没有正则化，因为我们将正则化参数*C*设置为一个相对较大的值（1,000）。现在我们将看到当我们在一个较宽的范围内调整*C*时会发生什么。
- en: Note
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The code for generating the plots presented in this section can be found here:
    [https://packt.link/37Zks](https://packt.link/37Zks).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中呈现的生成图形的代码可以在这里找到：[https://packt.link/37Zks](https://packt.link/37Zks)。
- en: 'Exercise 4.03: Reducing Overfitting on the Synthetic Data Classification Problem'
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.03：减少合成数据分类问题中的过拟合
- en: 'This exercise is a continuation of *Exercise 4.02*, *Generating and Modeling
    Synthetic Classification Data*. Here, we will use a cross-validation procedure
    in order to find a good value for the hyperparameter *C*. We will do this by using
    only the training data, reserving the test data for after model building is complete.
    Be prepared – this is a long exercise – but it will illustrate a general procedure
    that you will be able to use with many different kinds of machine learning models,
    so it is worth the time spent here. Perform the following steps to complete the
    exercise:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习是*练习 4.02*，*生成和建模合成分类数据*的延续。在这里，我们将使用交叉验证程序来找到超参数*C*的一个合适值。我们将通过仅使用训练数据来完成此任务，将测试数据保留到模型构建完成后再使用。请做好准备——这将是一个较长的练习——但它将展示一个通用过程，您可以将其应用于许多不同类型的机器学习模型，因此，花费时间完成它是非常值得的。按照以下步骤完成此练习：
- en: Note
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Before you begin this exercise, you need to execute some prerequisite steps
    that can be found in the following notebook along with the code for this exercise:
    [https://packt.link/JqbsW](https://packt.link/JqbsW).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始此练习之前，您需要执行一些先决步骤，这些步骤可以在以下笔记本中找到，并附有此练习的代码：[https://packt.link/JqbsW](https://packt.link/JqbsW)。
- en: Vary the value of the regularization parameter, *C*, to range from *C = 1000*
    to *C = 0.001*. You can use the following snippets to do this.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整正则化参数 *C* 的值，使其范围从 *C = 1000* 到 *C = 0.001*。你可以使用以下代码片段来实现这一点。
- en: 'First, define exponents, which will be powers of 10, as follows:'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，定义指数，它们将是 10 的幂次方，如下所示：
- en: '[PRE26]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here is the output of the preceding code:'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '[PRE27]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, vary *C* by the powers of 10, as follows:'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，按 10 的幂次方调整 *C* 值，如下所示：
- en: '[PRE28]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is the output of the preceding code:'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '[PRE29]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: It's generally a good idea to vary the regularization parameter by powers of
    10, or by using a similar strategy, as training models can take a substantial
    amount of time, especially when using k-fold cross-validation. This gives you
    a good idea of how a wide range of *C* values impacts the bias-variance trade-off,
    without needing to train a very large number of models. In addition to the integer
    powers of 10, we also include points on the log10 scale that are about halfway
    between. If it seems like there is some interesting behavior in between these
    relatively widely spaced values, you can add more granular values for *C* in a
    smaller part of the range of possible values.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通常，最好通过 10 的幂次方来调整正则化参数，或者使用类似的策略，因为训练模型可能需要大量时间，特别是在使用 k 折交叉验证时。这能让你更好地了解不同的*C*值如何影响偏差-方差权衡，而无需训练大量模型。除了
    10 的整数次方，我们还包括 log10 坐标轴上大约位于中间的点。如果在这些相对间隔较大的值之间似乎有一些有趣的行为，你可以在可能值的较小范围内添加更多细化的
    *C* 值。
- en: 'Import the `roc_curve` class:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `roc_curve` 类：
- en: '[PRE30]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We'll continue to use the ROC AUC score for assessing, training, and testing
    performance. Now that we have several values of *C* to try and several folds (in
    this case four) for the cross-validation, we will want to store the training and
    test scores for each fold and for each value of *C*.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将继续使用 ROC AUC 分数来评估、训练和测试性能。现在，我们有几个不同的 *C* 值要尝试，并且有多个折（在这个例子中是四个）进行交叉验证，我们将需要存储每个折和每个
    *C* 值对应的训练和测试分数。
- en: 'Define a function that takes the `k_folds` cross-validation splitter, the array
    of *C* values (`C_vals`), the model object (`model`), and the features and response
    variable (`X` and `Y`, respectively) as inputs, to explore different amounts of
    regularization with k-fold cross-validation. Use the following code:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数接受 `k_folds` 交叉验证分割器、*C* 值数组（`C_vals`）、模型对象（`model`）、特征和响应变量（`X` 和
    `Y`）作为输入，以通过 k 折交叉验证探索不同的正则化量。使用以下代码：
- en: '[PRE31]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The function we started in this step will return the ROC AUCs and ROC curve
    data. The return block will be written during a later step in the exercise. For
    now, you can simply write the preceding code as is, because we will be defining
    `k_folds`, `C_vals`, `model`, `X`, and `Y` as we progress in the exercise.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在此步骤中开始的函数将返回 ROC AUC 和 ROC 曲线数据。返回块将在后续步骤中编写。现在，你可以按照原样编写上述代码，因为我们将在练习过程中定义
    `k_folds`、`C_vals`、`model`、`X` 和 `Y`。
- en: 'Within this function block, create a NumPy array to hold model performance
    data, with dimensions `n_folds` by `len(C_vals)`:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个函数块内，创建一个 NumPy 数组来保存模型性能数据，数组的维度为 `n_folds` × `len(C_vals)`：
- en: '[PRE32]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Next, we'll store the arrays of true and false positive rates and thresholds
    that go along with each of the test ROC AUC scores in a **list of lists**.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将把与每个测试 ROC AUC 分数相关联的真正阳性率、假阳性率和阈值存储在一个**列表的列表**中。
- en: Note
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'This is a convenient way to store all this model performance information, as
    a list in Python can contain any kind of data, including another list. Here, each
    item of the inner lists in the **list of lists** will be a tuple holding the arrays
    of TPR, FPR, and the thresholds for each of the folds, for each of the C values.
    Tuples are an ordered collection data type in Python, similar to lists, but unlike
    lists they are immutable: the items in a tuple can''t be changed after the tuple
    is created. When a function returns multiple values, like the roc_curve function
    of scikit-learn, these values can be output to a single variable, which will be
    a tuple of those values. This way of storing results should be more obvious when
    we access these arrays later in order to examine them.'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是存储所有模型性能信息的一种方便方式，因为Python中的列表可以包含任何类型的数据，包括另一个列表。在这里，**列表的列表**中的每个内层列表项将是一个元组，包含每个折叠的TPR、FPR和阈值数组，对于每个*C*值。元组是Python中的有序集合数据类型，类似于列表，但与列表不同的是它们是不可变的：一旦元组创建，元组中的项不能更改。当一个函数返回多个值时，像scikit-learn的roc_curve函数，这些值可以输出到一个单一的变量中，这个变量将是一个包含这些值的元组。这种存储结果的方式，在我们稍后访问这些数组以进行检查时，应该更为明显。
- en: 'Create a list of empty lists using `[[]]` and `*len(C_vals)` as follows:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `[[]]` 和 `*len(C_vals)` 创建一个空列表，如下所示：
- en: '[PRE33]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Using `*len(C_vals)` indicates that there should be a list of tuples of metrics
    (TPR, FPR, thresholds) for each value of *C*.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 `*len(C_vals)` 表示每个*C*值应该有一个包含指标（TPR、FPR、阈值）元组的列表。
- en: We have learned how to loop through the different folds for cross-validation
    in the preceding section. What we need to do now is write an outer loop in which
    we will nest the cross-validation loop.
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经在前一节中学习了如何在交叉验证中遍历不同的折叠。接下来我们需要做的是编写一个外部循环，其中嵌套交叉验证循环。
- en: 'Create an outer loop for training and testing each of the k-folds for each
    value of *C*:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个*C*值创建一个外部循环来训练和测试每个k折：
- en: '[PRE34]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can reuse the same model object that we have already, and simply set a new
    value of *C* within each run of the loop. Inside the loop of *C* values, we run
    the cross-validation loop. We begin by yielding the training and test data row
    indices for each split.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以重用已经有的相同模型对象，并在每次循环中设置一个新的*C*值。在*C*值的循环中，我们运行交叉验证循环。我们从为每个拆分生成训练和测试数据的行索引开始。
- en: 'Obtain the training and test indices for each fold:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取每个折叠的训练和测试索引：
- en: '[PRE35]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Index the features and response variable to obtain the training and test data
    for this fold using the following code:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码索引特征和响应变量，以获取该折叠的训练和测试数据：
- en: '[PRE36]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The training data for the current fold is then used to train the model.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后使用当前折叠的训练数据来训练模型。
- en: 'Fit the model on the training data, as follows:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上拟合模型，如下所示：
- en: '[PRE37]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This will effectively "reset" the model from whatever the previous coefficients
    and intercept were to reflect the training on this new data.
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将有效地“重置”模型，从之前的系数和截距中恢复，反映出在这组新数据上的训练。
- en: The training and test ROC AUC scores are then obtained, as well as the arrays
    of TPRs, FPRs, and thresholds that go along with the test data.
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后获得训练和测试的ROC AUC分数，以及与测试数据相关的TPR、FPR和阈值数组。
- en: 'Obtain the training ROC AUC score:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取训练ROC AUC分数：
- en: '[PRE38]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Obtain the test ROC AUC score:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取测试ROC AUC分数：
- en: '[PRE39]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Obtain the test ROC curves for each fold using the following code:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码获取每个折叠的测试ROC曲线：
- en: '[PRE40]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We will use a fold counter to keep track of the folds that are incremented,
    and once outside the cross-validation loop, we print a status update to standard
    output. Whenever performing long computational procedures, it's a good idea to
    periodically print the status of the job so that you can monitor its progress
    and confirm that things are still working correctly. This cross-validation procedure
    will likely take only a few seconds on your laptop, but for longer jobs this can
    be especially reassuring.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将使用一个折叠计数器来跟踪递增的折叠，在交叉验证循环之外，打印状态更新到标准输出。每当执行长时间的计算过程时，定期打印作业的状态是个好主意，这样你可以监控进展并确认一切正常工作。这个交叉验证过程在你的笔记本电脑上可能只需要几秒钟，但对于较长的任务，这样做尤其令人放心。
- en: 'Increment the fold counter using the following code:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码递增折叠计数器：
- en: '[PRE41]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Write the following code to indicate the progress of execution for each value
    of *C*:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写以下代码以显示每个*C*值的执行进度：
- en: '[PRE42]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Write the code to return the ROC AUCs and ROC curve data and finish the function:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码以返回ROC AUC和ROC曲线数据并完成函数：
- en: '[PRE43]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that we will continue to use the split into four folds that we illustrated
    previously, but you are encouraged to try this procedure with different numbers
    of folds to compare the effect.
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们将继续使用之前展示的四折拆分，但鼓励你尝试使用不同数量的折来比较效果。
- en: We have covered a lot of material in the preceding steps. You may want to take
    a few moments to review this with your classmates in order to make sure that you
    understand each part. Running the function is comparatively simple. That is the
    beauty of a well-designed function – all the complicated parts get abstracted
    away, allowing you to concentrate on usage.
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在前面的步骤中已经覆盖了很多内容。你可能想花几分钟时间和你的同学一起复习一下，以确保你理解每个部分。运行这个函数相对简单。这就是设计良好的函数的魅力——所有复杂的部分都被抽象化了，允许你专注于如何使用它。
- en: 'Run the function we''ve designed to examine cross-validation performance, with
    the *C* values that we previously defined, and by using the model and data we
    were working with in the previous exercise. Use the following code:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行我们设计的函数来检查交叉验证的性能，使用我们之前定义的*C*值，并使用我们在上一个练习中使用的模型和数据。使用以下代码：
- en: '[PRE44]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'When you run this code, you should see the following output populate below
    the code cell as the cross-validation is completed for each value of *C*:'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你运行此代码时，你应该会看到以下输出，随着每个*C*值的交叉验证完成，输出会出现在代码单元格下方：
- en: '[PRE45]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: So, what do the results of the cross-validation look like? There are a few ways
    to examine this. It is useful to look at the performance of each fold individually,
    so that you can see how variable the results are.
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 那么，交叉验证的结果是什么样的呢？有几种方法可以查看这个结果。单独查看每一折的性能是很有用的，这样你可以看到结果的变化程度。
- en: This tells you how different subsets of your data perform as test sets, leading
    to a general idea of the range of performance you can expect from the unseen test
    set. What we're interested in here is whether or not we are able to use regularization
    to alleviate the overfitting that we saw. We know that using *C = 1,000* led to
    overfitting – we know this from comparing the training and test scores. But what
    about the other *C* values that we've tried? A good way to visualize this will
    be to plot the training and test scores on the *y-axis* and the values of *C*
    on the *x-axis*.
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这告诉你数据的不同子集作为测试集的表现，从而大致了解你可以从未见过的测试集期望的表现范围。我们在这里感兴趣的是，是否能够通过正则化来缓解我们所看到的过拟合问题。我们知道使用*C
    = 1,000*导致了过拟合——我们通过比较训练和测试分数得知这一点。但对于我们尝试的其他*C*值呢？一个很好的可视化方法是将训练和测试分数绘制在*y轴*上，将*C*值绘制在*x轴*上。
- en: 'Loop over each of the folds to view their results individually by using the
    following code:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码，循环遍历每一折，以单独查看它们的结果：
- en: '[PRE46]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You will obtain the following output:'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '![Figure 4.20: The training and test scores for each fold and C-value'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.20：每一折和C值的训练和测试得分'
- en: '](img/B16925_4_20.jpg)'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_4_20.jpg)'
- en: 'Figure 4.20: The training and test scores for each fold and C-value'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.20：每一折和C值的训练和测试得分
- en: We can see that for each fold of the cross-validation, as *C* decreases, the
    training performance also decreases. However, at the same time, the test performance
    increases. For some folds and values of *C*, the test ROC AUC score actually exceeds
    that of the training data, while for others, these two metrics simply come closer
    together. In all cases, we can say that the *C* values of 10-1.5 and 10-2 appear
    to have a similar test performance, which is substantially higher than the test
    performance of *C = 10*3\. So, it appears that regularization has successfully
    addressed our overfitting problem.
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，对于交叉验证的每一折，随着*C*值的减小，训练性能也在下降。然而，与此同时，测试性能却在增加。对于某些折和*C*的值，测试的ROC AUC分数实际上超过了训练数据的分数，而对于其他情况，这两个指标则趋向于接近。在所有情况下，我们可以说，10^-1.5
    和 10^-2 的*C*值在测试性能上表现相似，明显高于*C = 10^3*的测试性能。因此，似乎正则化成功解决了我们的过拟合问题。
- en: But what about the lower values of *C*? For values that are lower than 10-2,
    the ROC AUC metric suddenly drops to 0.5\. As you know, this value means that
    the classification model is essentially useless, performing no better than a coin
    flip. You are encouraged to check on this later when exploring how regularization
    affects the coefficient values; however, this is what happens when so much L1
    regularization is applied that all model coefficients shrink to 0\. Obviously,
    such models are not useful to us, as they encode no information about the relationship
    between the features and response variable.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 那么*C*的较低值呢？对于低于10-2的值，ROC AUC指标突然下降到0.5。正如您所知，这个值意味着分类模型基本上是无用的，性能不比抛硬币好。当探索正则化如何影响系数值时，鼓励您稍后检查这一点；然而，当应用了如此多的L1正则化以至于所有模型系数都收缩到0时，就会发生这种情况。显然，这样的模型对我们没有用，因为它们不包含关于特征和响应变量之间关系的任何信息。
- en: Looking at the training and test performance of each k-fold split is helpful
    for gaining insights into the variability of model performance that may be expected
    when the model is scored on new, unseen data. But in order to summarize the results
    of the k-folds procedure, a common approach is to average the performance metric
    over the folds, for each value of the hyperparameter being considered. We'll perform
    this in the next step.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看每个k折分割的训练和测试性能有助于了解当模型在新的未见数据上得分时可能预期的模型性能的变化。但为了总结k折过程的结果，一个常见的方法是对每个正在考虑的超参数值的性能指标进行折叠平均。我们将在下一步中执行此操作。
- en: 'Plot the mean of training and test ROC AUC scores for each *C* value using
    the following code:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码绘制每个*C*值的训练和测试ROC AUC分数的平均值：
- en: '[PRE47]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![Figure 4.21: The average training and test scores across cross-validation
    folds'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.21：跨交叉验证折叠的平均训练和测试分数'
- en: '](img/B16925_4_21.jpg)'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_4_21.jpg)'
- en: 'Figure 4.21: The average training and test scores across cross-validation folds'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.21：跨交叉验证折叠的平均训练和测试分数
- en: From this plot, it's clear that *C = 10*-1.5 and *10*-2 are the best values
    of *C*. There is little or no overfitting here, as the average training and test
    scores are nearly the same. You could search a finer grid of *C* values (that
    is *C = 10*-1.1*,* *10*-1.2, and so on) in order to more precisely locate a *C*
    value. However, from our graph, we can see that either *C = 10*-1.5 or *C = 10*-2
    will likely be good solutions. We will move forward with *C = 10*-1.5.
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从这个图中可以看出，*C = 10*-1.5和*10*-2是最佳的*C*值。这里几乎没有过拟合，因为平均训练和测试分数几乎相同。您可以搜索更精细的*C*值网格（即*C
    = 10*-1.1*、*10*-1.2等），以更精确地定位*C*值。然而，从我们的图表中，我们可以看到*C = 10*-1.5或*C = 10*-2可能是很好的解决方案。我们将继续使用*C
    = 10*-1.5。
- en: Examining the summary metric of ROC AUC is a good way to get a quick idea of
    how models will perform. However, for any real-world business application, you
    will often need to choose a specific threshold, which goes along with specific
    true and false positive rates. These will be needed to use the classifier to make
    the required "yes" or "no" decision, which, in our case study, is a prediction
    of whether an account will default. For this reason, it is useful to look at the
    ROC curves across the different folds of the cross-validation. To facilitate this,
    the preceding function has been designed to return the true and false positive
    rates, and thresholds, for each test fold and value of *C*, in the `cv_test_roc`
    list of lists. First, we need to find the index of the outer list that corresponds
    to the *C* value that we've chosen, *10*-1.5.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检查ROC AUC的摘要指标是了解模型性能的快速方法。然而，对于任何真实的业务应用程序，您通常需要选择一个特定的阈值，该阈值与特定的真正和假正率相对应。这些将需要使用分类器来做出所需的“是”或“否”决定，在我们的案例研究中，这是关于账户是否会违约的预测。因此，查看交叉验证的不同折叠中的ROC曲线是有用的。为了方便起见，前面的函数已经被设计为返回每个测试折叠和*C*值的真正和假正率以及阈值，在`cv_test_roc`列表的列表中。首先，我们需要找到对应于我们选择的*C*值*10*-1.5的外部列表的索引。
- en: To accomplish this, we could simply look at our list of *C* values and count
    by hand, but it's safer to do this programmatically by finding the index of the
    non-zero element of a Boolean array, as is shown in the next step.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要实现这一点，我们可以简单地查看我们的*C*值列表并手动计数，但最好通过编程方式找到布尔数组的非零元素的索引来进行操作，如下一步所示。
- en: 'Use a Boolean array to find the index where *C = 10*-1.5 and convert it to
    an integer data type with the following code:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用布尔数组找到*C = 10*-1.5的索引，并使用以下代码将其转换为整数数据类型：
- en: '[PRE48]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Here is the output of the preceding code:'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '[PRE49]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Convert the integer version of the Boolean array into a single integer index
    using the `nonzero` function with this code:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nonzero`函数将布尔数组的整数版本转换为单个整数索引，代码如下：
- en: '[PRE50]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here is the output of the preceding code:'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '[PRE51]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We have now successfully located the *C* value that we wish to use.
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们现在已经成功找到了我们希望使用的*C*值。
- en: 'Access the true and false positive rates in order to plot the ROC curves for
    each fold:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问真正阳性和假阳性率，以绘制每个折叠的ROC曲线：
- en: '[PRE52]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'You will obtain the following output:'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '![Figure 4.22: ROC curves for each fold'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.22：每个折叠的ROC曲线'
- en: '](img/B16925_4_22.jpg)'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_4_22.jpg)'
- en: 'Figure 4.22: ROC curves for each fold'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.22：每个折叠的ROC曲线
- en: It appears that there is a fair amount of variability in the ROC curves. For
    example, if, for some reason, we want to limit the false positive rate to 40%,
    then from the plot it appears that we may be able to achieve a true positive rate
    of anywhere from approximately 60% to 80%. You can find the exact values by examining
    the arrays that we have plotted. This gives you an idea of how much variability
    in performance can be expected when deploying the model on new data. Generally,
    the more training data that is available, then the less variability there will
    be between the folds of cross-validation, so this could also be a sign that it
    would be a good idea to collect additional data, especially if the variability
    between training folds seems unacceptably high. You also may wish to try different
    numbers of folds with this procedure so as to see the effect on the variability
    of results between folds.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 看起来ROC曲线存在相当大的变异性。例如，如果出于某种原因，我们想将假阳性率限制在40%，那么从图中可以看出，我们可能能够实现的真正阳性率大约在60%到80%之间。你可以通过检查我们绘制的数组来找到精确值。这给你一个关于在新数据上部署模型时，性能波动的预期情况。通常，训练数据越多，交叉验证的折叠之间的变异性就越小，因此这也可能是一个收集更多数据的好主意，尤其是当训练折叠之间的变异性似乎不可接受地高时。你可能还希望尝试使用不同数量的折叠进行此过程，以查看结果变异性对折叠之间的影响。
- en: While normally we would try other models on our synthetic data problem, such
    as a random forest or support vector machine, if we imagine that in cross-validation,
    logistic regression proved to be the best model, we would decide to make this
    our final choice. When the final model is selected, all the training data can
    be used to fit the model, using the hyperparameters chosen with cross-validation.
    It's best to use as much data as possible in model fitting, as models typically
    work better when trained on more data.
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然通常我们会尝试在我们的合成数据问题上使用其他模型，例如随机森林或支持向量机，但如果我们假设在交叉验证中，逻辑回归证明是最好的模型，我们将决定将其作为最终选择。当最终模型被选定后，可以使用所有训练数据来拟合该模型，使用在交叉验证中选择的超参数。最好在模型拟合时使用尽可能多的数据，因为通常情况下，模型在训练时使用更多的数据效果更好。
- en: Train the logistic regression on all the training data from our synthetic problem
    and compare the training and test scores, using the held-out test set as shown
    in the following steps.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的合成问题中，使用所有训练数据训练逻辑回归，并比较训练和测试分数，使用以下步骤所示的保留测试集。
- en: Note
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: This is the final step in the model selection process. You should only use the
    unseen test set after your choice of model and hyperparameters are considered
    finished, otherwise it will not be "unseen."
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是模型选择过程中的最后一步。只有在你选择好模型和超参数之后，才能使用未见过的测试集，否则它就不再是“未见过”的。
- en: 'Set the *C* value and train the model on all the training data with this code:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置*C*值，并使用以下代码在所有训练数据上训练模型：
- en: '[PRE53]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Here is the output of the preceding code:'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '[PRE54]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Obtain predicted probabilities and the ROC AUC score for the training data
    with this code:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码获取训练数据的预测概率和ROC AUC分数：
- en: '[PRE55]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here is the output of the preceding code:'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '[PRE56]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Obtain predicted probabilities and the ROC AUC score for the test data with
    this code:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码获取测试数据的预测概率和ROC AUC分数：
- en: '[PRE57]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Here is the output of the preceding code:'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '[PRE58]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Here, we can see that by using regularization, the model training and test scores
    are similar, indicating that the overfitting problem has been greatly reduced.
    The training score is lower since we have introduced bias into the model at the
    expense of variance. However, this is OK, since the test score, which is the most
    important part, is higher. The out-of-sample test score is what matters for predictive
    capability. You are encouraged to check that these training and test scores are
    similar to those from the cross-validation procedure by printing the values from
    the arrays that we plotted previously; you should find that they are.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，通过使用正则化，模型的训练分数和测试分数相似，表明过拟合问题已大大减轻。训练分数较低，因为我们在模型中引入了偏差，牺牲了方差。然而，这没关系，因为最重要的测试分数较高。样本外测试分数才是预测能力的关键。建议您通过打印我们之前绘制的数组中的值，检查这些训练分数和测试分数是否与交叉验证过程中的结果相似；您应该发现它们是相似的。
- en: Note
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: In a real-world project, before delivering this model to your client for production
    use, you may wish to train the model on all the data that you were given, including
    the unseen test set. This follows the idea that the more data a model has seen,
    the better it is likely to perform in practice. However, some practitioners prefer
    to only use models that have been tested, meaning you would deliver the model
    trained only on the training data, not including the test set.
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在一个实际项目中，在将这个模型交付给客户用于生产使用之前，您可能希望在所有提供的数据上训练模型，包括未见过的测试集。这遵循了一个想法，即模型看到的数据越多，实际表现可能越好。然而，一些从业者更喜欢只使用经过测试的模型，这意味着您只会交付在训练数据上训练的模型，而不包括测试集。
- en: We know that L1 regularization works by decreasing the magnitude (that is, absolute
    value) of coefficients of the logistic regression. It can also set some coefficients
    to zero, therefore performing feature selection. In the next step, we will determine
    how many coefficients were set to zero.
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们知道，L1正则化通过减少逻辑回归系数的大小（即绝对值）来工作。它还可以将一些系数设置为零，从而执行特征选择。在下一步，我们将确定有多少个系数被设置为零。
- en: 'Access the coefficients of the trained model and determine how many do not
    equal zero (`!= 0`) with this code:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码访问训练模型的系数，并确定有多少个系数不等于零（`!= 0`）：
- en: '[PRE59]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output should be as follows:'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE60]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This code takes the sum of a Boolean array indicating the locations of non-zero
    coefficients, so it shows how many coefficients in the model did not get set to
    zero by L1 regularization. Only 2 of the 200 features were selected!
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码对一个布尔数组求和，表示非零系数的位置，因此显示模型中有多少个系数没有被L1正则化设置为零。在200个特征中，只有2个被选择了！
- en: 'Examine the value of the intercept using this code:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码检查截距的值：
- en: '[PRE61]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output should be as follows:'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE62]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: This shows that the intercept was regularized to 0.
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这表明截距被正则化为0。
- en: In this exercise, we accomplished several goals. We used the k-fold cross-validation
    procedure to tune the regularization hyperparameter. We saw the power of regularization
    for reducing overfitting, and in the case of L1 regularization in logistic regression,
    selecting features.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们完成了几个目标。我们使用了k折交叉验证过程来调整正则化超参数。我们看到了正则化在减少过拟合方面的强大作用，并且在逻辑回归中的L1正则化情况下，还能进行特征选择。
- en: Many machine learning algorithms offer some type of feature selection capability.
    Many also require the tuning of hyperparameters. The function here that loops
    over hyperparameters, and performs cross-validation, is a powerful concept that
    generalizes to other models. Scikit-learn offers functionality to make this process
    easier; in particular, the `sklearn.model_selection.GridSearchCV` procedure, which
    applies cross-validation to a grid search over hyperparameters. A **grid search**
    can be helpful when there are multiple hyperparameters to tune, by looking at
    all combinations of the ranges of different hyperparameters that you specify.
    A **randomized grid search** can speed up this process by randomly choosing a
    smaller number of combinations when an exhaustive grid search would take too long.
    Once you are comfortable with the concepts illustrated here, you are encouraged
    to streamline your workflow with convenient functions like these.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法提供某种类型的特征选择功能。许多算法还需要调整超参数。这里的函数通过循环超参数并执行交叉验证，提供了一个强大的概念，可以推广到其他模型。Scikit-learn
    提供了简化这个过程的功能；特别是，`sklearn.model_selection.GridSearchCV`过程，它对超参数进行网格搜索并应用交叉验证。当需要调整多个超参数时，**网格搜索**非常有帮助，因为它可以查看您指定的不同超参数范围的所有组合。**随机网格搜索**可以通过随机选择较少的组合来加速这一过程，尤其是在全面的网格搜索过于耗时的情况下。一旦您熟悉了这里展示的概念，建议您通过使用像这些方便的函数来简化工作流程。
- en: Options for Logistic Regression in Scikit-Learn
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scikit-Learn 中逻辑回归的选项
- en: 'We have used and discussed most of the options that you may supply to scikit-learn
    when instantiating or tuning the hyperparameters of a `LogisticRegression` model
    class. Here, we list them all and provide some general advice on their usage:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用并讨论了在实例化或调整`LogisticRegression`模型类的超参数时，您可能提供的大部分选项。在这里，我们列出了所有选项，并提供了一些关于它们使用的通用建议：
- en: '![Figure 4.23: A complete list of options for the logistic regression model
    in scikit-learn'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.23：Scikit-learn 中逻辑回归模型的完整选项列表'
- en: '](img/B16925_4_23.jpg)'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_23.jpg)'
- en: 'Figure 4.23: A complete list of options for the logistic regression model in
    scikit-learn'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.23：Scikit-learn 中逻辑回归模型的完整选项列表
- en: If you are in doubt regarding which option to use for logistic regression, we
    recommend you consult the scikit-learn documentation for further guidance ([https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)).
    Some options, such as the regularization parameter *C*, or the choice of a penalty
    for regularization, will need to be explored through the cross-validation process.
    Here, as with many choices to be made in data science, there is no universal approach
    that will apply to all datasets. The best way to see which options to use with
    a given dataset is to try several of them and see which gives the best out-of-sample
    performance. Cross-validation offers you a robust way to do this.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对使用哪个选项进行逻辑回归感到疑惑，我们建议您参考 scikit-learn 文档以获取进一步的指导（[https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)）。一些选项，例如正则化参数*C*，或正则化惩罚的选择，需要通过交叉验证过程来探索。在这里，正如许多数据科学决策一样，没有一种通用的方法适用于所有数据集。查看使用哪些选项最适合给定数据集的最佳方法是尝试其中的几个，并查看哪一个在样本外表现最好。交叉验证为您提供了一种稳健的方式来做到这一点。
- en: Scaling Data, Pipelines, and Interaction Features in Scikit-Learn
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Scikit-Learn 中的缩放数据、管道和交互特征
- en: '**Scaling Data**'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '**缩放数据**'
- en: 'Compared to the synthetic data we were just working with, the case study data
    is relatively large. If we want to use L1 regularization, then according to the
    official documentation ([https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)),
    we ought to use the `saga` solver. However, this solver is not robust to unscaled
    datasets. Hence, we need to be sure to scale the data. This is also a good idea
    whenever doing regularization, so all the features are on the same scale and are
    equally penalized by the regularization process. A simple way to make sure that
    all the features have the same scale is to put them all through the transformation
    of subtracting the minimum and dividing by the range from minimum to maximum.
    This transforms each feature so that it will have a minimum of 0 and a maximum
    of 1\. To instantiate the `MinMaxScaler` scaler that does this, we can use the
    following code:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前处理的合成数据相比，案例研究数据相对较大。如果我们想使用 L1 正则化，那么根据官方文档（[https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)），我们应该使用
    `saga` 解算器。然而，这个解算器对未缩放的数据集不具备鲁棒性。因此，我们需要确保对数据进行缩放。每当进行正则化时，这也是一个好主意，这样所有特征就处于相同的尺度，并且在正则化过程中会受到同等的惩罚。确保所有特征具有相同尺度的一个简单方法是将它们都通过一个变换过程，即减去最小值并除以最小值到最大值的范围。这将把每个特征转换为使其最小值为
    0，最大值为 1。为了实例化一个执行这一过程的 `MinMaxScaler` 缩放器，我们可以使用以下代码：
- en: '[PRE63]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '**Pipelines**'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '**管道**'
- en: 'Previously, we used a logistic regression model in the cross-validation loop.
    However, now that we''re scaling data, what new considerations are there? The
    scaling is effectively "learned" from the minimum and maximum values of the training
    data. After this, a logistic regression model would be trained on data scaled
    by the extremes of the model training data. However, we won''t know the minimum
    and maximum values of the new, unseen data. So, following the philosophy of making
    cross-validation an effective indicator of model performance on unseen data, we
    need to use the minimum and maximum values of the training data in each cross-validation
    fold in order to scale the test data in that fold, before making predictions on
    the test data. Scikit-learn has the functionality to facilitate the combination
    of several training and test steps for situations such as this: the `Pipeline`.
    Our pipeline will consist of two steps: the scaler and the logistic regression
    model. These can both be fit on the training data and then used to make predictions
    on the test data. The process of fitting a pipeline is executed as a single step
    in the code, so all the parts of the pipeline are fit at once in this sense. Here
    is how a `Pipeline` is instantiated:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们在交叉验证循环中使用了逻辑回归模型。然而，现在我们对数据进行了缩放，新的考虑因素是什么？缩放实际上是通过训练数据的最小值和最大值来“学习”的。之后，逻辑回归模型将基于由模型训练数据的极值缩放过的数据进行训练。然而，我们无法知道新数据（未见数据）的最小值和最大值。因此，按照使交叉验证成为评估未见数据模型性能的有效指标的理念，我们需要在每个交叉验证折叠中使用训练数据的最小值和最大值，以便在该折叠中对测试数据进行缩放，然后再对测试数据进行预测。Scikit-learn
    提供了便捷的功能来结合多个训练和测试步骤，以应对这种情况：`Pipeline`。我们的管道将包括两个步骤：缩放器和逻辑回归模型。这两个步骤可以都在训练数据上进行拟合，然后用于对测试数据进行预测。拟合管道的过程在代码中作为一个单一步骤执行，因此从这个角度看，管道的所有部分都是一次性拟合的。以下是如何实例化一个`Pipeline`：
- en: '[PRE64]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '**Interaction Features**'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '**交互特征**'
- en: Considering the case study data, do you think a logistic regression model with
    all possible features would be overfit or underfit? You can think about this from
    the perspective of rules of thumb, such as the "rule of 10," and the number of
    features (17) versus samples (26,664) that we have. Alternatively, you can consider
    all the work we've done so far with this data. For instance, we've had a chance
    to visualize all the features and ensure they make sense. Since there are relatively
    few features, and we have relatively high confidence that they are high quality
    because of our data exploration work, we are in a different situation than with
    the synthetic data exercises in this chapter, where we had a large number of features
    about which we knew relatively little. So, it may be that overfitting will be
    less of an issue with our case study at this point, and the benefits of regularization
    may not be significant.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到案例研究数据，你认为一个包含所有可能特征的逻辑回归模型会过拟合还是欠拟合？你可以从经验法则的角度来考虑，例如“10法则”，以及我们拥有的特征数（17个）与样本数（26,664个）之间的关系。或者，你也可以回顾我们迄今为止在这个数据上所做的所有工作。例如，我们已经有机会对所有特征进行可视化，并确保它们是合理的。由于特征相对较少，并且由于我们通过数据探索工作对它们的质量有较高的信心，我们的情况与本章中使用合成数据的练习不同，后者有大量特征，但我们对其了解较少。因此，可能目前我们的案例研究数据过拟合问题不太明显，正则化的好处可能也不会显著。
- en: 'In fact, it may be that we will underfit the model using only the 17 features
    that came with the data. One strategy to deal with this is to engineer new features.
    Some simple feature engineering techniques we''ve discussed include interaction
    and polynomial features. Polynomials may not make sense given the way in which
    some of the data has been encoded; for example, *-1*2 *= 1*, which may not be
    sensible for `PAY_1`. However, we may wish to try creating interaction features
    to capture the relationships between features. `PolynomialFeatures` can be used
    to create interaction features only, without polynomial features. The example
    code is as follows:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，使用仅有的17个特征，我们可能会出现欠拟合。应对这种情况的一种策略是进行特征工程。我们讨论过的一些简单特征工程技术包括交互特征和多项式特征。考虑到某些数据的编码方式，多项式特征可能没有意义；例如，*-1*2
    = 1*，这对于`PAY_1`可能并不合理。然而，我们可能希望尝试创建交互特征，以捕捉特征之间的关系。`PolynomialFeatures`可以用来仅创建交互特征，而不包括多项式特征。示例代码如下：
- en: '[PRE65]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Here, `degree` represents the degree of the polynomial features, `interaction_only`
    takes a Boolean value (setting it to `True` indicates that only interaction features
    will be created), and so does `include_bias`, which adds an intercept to the model
    (the default value is `False`, which is correct here as the logistic regression
    model will add an intercept).
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`degree`表示多项式特征的阶数，`interaction_only`是布尔值（将其设置为`True`表示仅创建交互特征），`include_bias`也是布尔值，它会向模型添加截距项（默认值为`False`，这里是正确的，因为逻辑回归模型会自动添加截距）。
- en: 'Activity 4.01: Cross-Validation and Feature Engineering with the Case Study
    Data'
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 4.01：使用案例研究数据进行交叉验证和特征工程
- en: 'In this activity, we''ll apply the knowledge of cross-validation and regularization
    that we''ve learned in this chapter to the case study data. We''ll perform basic
    feature engineering. In order to estimate parameters for the regularized logistic
    regression model for the case study data, which is larger in size than the synthetic
    data that we''ve worked with, we''ll use the `saga` solver. In order to use this
    solver, and for the purpose of regularization, we''ll need to `Pipeline` class
    in scikit-learn. Once you have completed the activity, you should obtain an improved
    cross-validation test performance with the use of interaction features, as shown
    in the following diagram:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将应用本章中学到的交叉验证和正则化知识到案例研究数据中。我们将进行基础的特征工程。为了为案例研究数据的正则化逻辑回归模型估计参数，由于该数据集比我们之前使用的合成数据集大，因此我们将使用`saga`求解器。为了使用此求解器，并出于正则化的目的，我们需要使用scikit-learn中的`Pipeline`类。完成活动后，你应当能够得到使用交互特征的改进版交叉验证测试表现，具体如下图所示：
- en: '![Figure 4.24: Improved model test performance'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.24：改进的模型测试表现'
- en: '](img/B16925_4_24.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_24.jpg)'
- en: 'Figure 4.24: Improved model test performance'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.24：改进的模型测试表现
- en: 'Perform the following steps to complete the activity:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成活动：
- en: Select the features from the DataFrame of the case study data.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从案例研究数据的数据框中选择特征。
- en: You can use the list of feature names that we've already created in this chapter,
    but be sure not to include the response variable, which would be a very good (but
    entirely inappropriate) feature!
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用我们在本章中已经创建的特征名称列表，但一定要确保不包括响应变量，因为它是一个非常好的（但完全不适当的）特征！
- en: Make a training/test split using a random seed of 24.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机种子24进行训练/测试集划分。
- en: We'll use this going forward and reserve this test data as the unseen test set.
    By specifying the random seed, we can easily create separate notebooks with other
    modeling approaches using the same training data.
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将继续使用这个并将此测试数据保留为未见过的测试集。通过指定随机种子，我们可以轻松创建包含其他建模方法的独立笔记本，并使用相同的训练数据。
- en: Instantiate `MinMaxScaler` to scale the data.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`MinMaxScaler`来缩放数据。
- en: Instantiate a logistic regression model with the `saga` solver, L1 penalty,
    and set `max_iter` to `1000` as we want the solver to have enough iterations to
    find a good solution.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`saga`求解器、L1惩罚并将`max_iter`设置为`1000`来实例化一个逻辑回归模型，因为我们希望求解器有足够的迭代次数来找到一个良好的解。
- en: Import the `Pipeline` class and create a pipeline with the scaler and the logistic
    regression model, using the names `'scaler'` and `'model'` for the steps, respectively.
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`Pipeline`类，并使用`'scaler'`和`'model'`作为步骤名称，分别创建一个包含缩放器和逻辑回归模型的流水线。
- en: Use the `get_params` and `set_params` methods to see how to view the parameters
    from each stage of the pipeline and change them.
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`get_params`和`set_params`方法查看每个流水线阶段的参数，并进行更改。
- en: Create a smaller range of *C* values to test with cross-validation, as these
    models will take longer to train and test with more data than our previous exercise;
    we recommend *C = [10*2*, 10, 1, 10*-1*, 10*-2*, 10*-3*]*.
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个较小范围的*C*值以进行交叉验证测试，因为这些模型在使用比我们之前练习更多数据时，训练和测试将花费更长时间；我们推荐的*C*值为 *C = [10*2*,
    10, 1, 10*-1*, 10*-2*, 10*-3*]。
- en: Make a new version of the `cross_val_C_search` function called `cross_val_C_search_pipe`.
    Instead of the `model` argument, this function will take a `pipeline` argument.
    The changes inside the function will be to set the *C* value using `set_params(model__C
    = <value you want to test>)` on the pipeline, replacing the model with the pipeline
    for the `fit` and `predict_proba` methods, and accessing the *C* value using `pipeline.get_params()['model__C']`
    for the printed status update.
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`cross_val_C_search`函数版本，名为`cross_val_C_search_pipe`。这个函数将不再使用`model`参数，而是接受一个`pipeline`参数。函数内部的更改将是通过在流水线中使用`set_params(model__C
    = <value you want to test>)`来设置*C*值，替换`fit`和`predict_proba`方法中的模型为流水线，并通过`pipeline.get_params()['model__C']`访问*C*值，以打印状态更新。
- en: Run this function as in the previous exercise, but using the new range of *C*
    values, the pipeline you created, and the features and response variable from
    the training split of the case study data.
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像之前的练习一样运行这个函数，但使用新的*C*值范围、你创建的流水线，以及来自案例研究数据训练集的特征和响应变量。
- en: You may see warnings here, or in later steps, regarding the non-convergence
    of the solver; you could experiment with the `tol` or `max_iter` options to try
    and achieve convergence, although the results you obtain with `max_iter = 1000`
    are likely to be sufficient.
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可能会看到关于求解器不收敛的警告，可能出现在此处或后续步骤中；你可以尝试使用`tol`或`max_iter`选项来实现收敛，尽管使用`max_iter
    = 1000`获得的结果可能已经足够。
- en: Plot the average training and test ROC AUC across folds for each *C* value.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制每个*C*值在各折交叉验证中的平均训练和测试ROC AUC。
- en: Create interaction features for the case study data and confirm that the number
    of new features makes sense.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为案例研究数据创建交互特征，并确认新特征的数量是合理的。
- en: Repeat the cross-validation procedure and observe the model performance when
    using interaction features.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复交叉验证过程，并观察在使用交互特征时模型的表现。
- en: Note that this will take substantially more time, due to the larger number of
    features, but it will probably take less than 10 minutes. So, does the average
    cross-validation test performance improve with the interaction features? Is regularization
    useful?
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，由于特征数量较多，这将需要更多时间，但可能不超过10分钟。那么，交互特征是否改善了平均交叉验证测试性能？正则化有用吗？
- en: Note
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The Jupyter notebook containing the Python code for this activity can be found
    at [https://packt.link/ohGgX](https://packt.link/ohGgX). Detailed step-wise solution
    to this activity can be found via [this link](B16925_Solution_ePub.xhtml#_idTextAnchor155).
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含此活动的Python代码的Jupyter notebook可以在[https://packt.link/ohGgX](https://packt.link/ohGgX)找到。此活动的详细逐步解决方案可以通过[此链接](B16925_Solution_ePub.xhtml#_idTextAnchor155)查看。
- en: Summary
  id: totrans-444
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the final details of logistic regression and
    continued to understand how to use scikit-learn to fit logistic regression models.
    We gained more visibility into how the model fitting process works by learning
    about the concept of a cost function, which is minimized by the gradient descent
    procedure to estimate parameters during model fitting.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了逻辑回归的最终细节，并继续学习如何使用`scikit-learn`拟合逻辑回归模型。通过了解代价函数的概念，我们对模型拟合过程有了更多的了解，代价函数通过梯度下降过程来最小化，从而在模型拟合过程中估计参数。
- en: We also learned of the need for regularization by introducing the concepts of
    underfitting and overfitting. In order to reduce overfitting, we saw how to adjust
    the cost function to regularize the coefficients of a logistic regression model
    using an L1 or L2 penalty. We used cross-validation to select the amount of regularization
    by tuning the regularization hyperparameter. To reduce underfitting, we saw how
    to do some simple feature engineering with interaction features for the case study
    data.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过引入欠拟合和过拟合的概念，了解到正则化的必要性。为了减少过拟合，我们了解了如何调整代价函数，通过L1或L2惩罚对逻辑回归模型的系数进行正则化。我们使用交叉验证来选择正则化的程度，通过调整正则化超参数来进行选择。为了减少欠拟合，我们还学习了如何通过交互特征进行一些简单的特征工程，来处理案例研究数据。
- en: 'We are now familiar with some of the most important concepts in machine learning.
    We have, so far, only used a very basic classification model: logistic regression.
    However, as you increase your toolbox of models that you know how to use, you
    will find that the concepts of overfitting and underfitting, the bias-variance
    trade-off, and hyperparameter tuning will come up again and again. These ideas,
    as well as convenient scikit-learn implementations of the cross-validation functions
    that we wrote in this chapter, will help us through our exploration of more advanced
    prediction methods.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经熟悉了一些机器学习中最重要的概念。到目前为止，我们仅使用了一个非常基础的分类模型：逻辑回归。然而，随着你逐步扩展所掌握的模型工具箱，你会发现过拟合和欠拟合的概念、偏差-方差权衡以及超参数调优将一次又一次地出现。这些概念，以及我们在本章中编写的交叉验证函数的便捷`scikit-learn`实现，将帮助我们在探索更先进的预测方法时提供支持。
- en: In the next chapter, we will learn about decision trees, an entirely different
    type of predictive model than logistic regression, and the random forests that
    are based on them. However, we will use the same concepts that we learned here,
    cross-validation and hyperparameter search, to tune these models.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习决策树，这是一种完全不同于逻辑回归的预测模型类型，以及基于决策树的随机森林。然而，我们将使用在本章中学到的相同概念——交叉验证和超参数搜索——来调优这些模型。
