- en: Chapter 7. Tuning Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。调整机器学习模型
- en: 'Tuning an algorithm or a machine learning application is simply a process that
    one goes through in order to enable the algorithm to perform optimally (in terms
    of runtime and memory usage) when optimizing the parameters that impact the model.
    This chapter aims to guide the reader through model tuning. It will cover the
    main techniques used to optimize an ML algorithm''s performance. Techniques will
    be explained both from the MLlib and Spark ML perspective. In [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples* and [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*, we described how to develop some complete machine
    learning applications and pipelines from data collection to model evaluation.
    In this chapter, we will try to reuse some of those applications to improve the
    performance by tuning several parameters such as Hyperparameter tuning, Grid search
    parameter tuning with MLlib and Spark ML, random search parameter tuning and cross-validation.
    The hypothesis ,which is also an important statistical test, will be discussed.
    In summary, the following topics will be covered in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 调整算法或机器学习应用程序只是一个过程，通过这个过程，可以使算法在优化影响模型的参数时以最佳方式运行（以运行时间和内存使用方面）。本章旨在指导读者进行模型调整。它将涵盖用于优化ML算法性能的主要技术。技术将从MLlib和Spark
    ML的角度进行解释。在[第5章](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "第5章。通过示例进行监督和无监督学习")和[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "第6章。构建可扩展的机器学习管道")中，我们描述了如何从数据收集到模型评估开发一些完整的机器学习应用程序和管道。在本章中，我们将尝试重用其中一些应用程序，通过调整一些参数（如超参数调整、MLlib和Spark
    ML的网格搜索参数调整、随机搜索参数调整和交叉验证）来提高性能。还将讨论假设，这也是一个重要的统计测试。总之，本章将涵盖以下主题：
- en: Details about machine learning model tuning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关机器学习模型调整的详细信息
- en: Typical challenges in model tuning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型调整中的典型挑战
- en: Evaluating machine learning models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估机器学习模型
- en: Validation and evaluation techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证和评估技术
- en: Parameter tuning for machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的参数调整
- en: Hypothesis testing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设检验
- en: Machine learning model selection
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型选择
- en: Details about machine learning model tuning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有关机器学习模型调整的详细信息
- en: Fundamentally, one can argue that the ultimate goal of **machine learning**
    (**ML**) is to make a machine that can automatically build models from data without
    requiring tedious and time-consuming human involvement. You will see that one
    of the difficulties in ML is that learning algorithms are like decision trees,
    random forests, and clustering techniques that require you to set parameters before
    you use the models for practical purposes. Alternatively, you need to set some
    constraints on those parameters.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，可以说机器学习（ML）的最终目标是制造一台可以自动从数据中构建模型的机器，而不需要繁琐和耗时的人类参与。你会发现机器学习中的一个困难是学习算法，如决策树、随机森林和聚类技术，需要在使用模型之前设置参数。或者，您需要对这些参数设置一些约束。
- en: How you set those parameters depends on a set of factors and the specification.
    Your goal in this regard is usually to set those parameters to the optimal values
    that enable you to complete a learning task in the best possible way. Thus, tuning
    an algorithm or ML technique can be simply thought of as a process where one goes
    through a series of steps in which they optimize the parameters that impact the
    model's performance in order to enable the algorithm to perform in the best way.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如何设置这些参数取决于一系列因素和规格。在这方面，您的目标通常是将这些参数设置为最佳值，以使您以最佳方式完成学习任务。因此，调整算法或ML技术可以简单地被认为是一个过程，通过这个过程，您可以优化影响模型性能的参数，以使算法以最佳方式运行。
- en: In [Chapter 3](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 3. Understanding the Problem by Understanding the Data"), *Understanding
    the Problem by Understanding the Data*, and [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples*, we discussed some techniques to choose the
    best algorithm based on your data and discuss the most widely used algorithms.
    To get the best result out of your model, you have to first define what the *best*
    actually is. We will discuss tuning in both abstract and concrete ways.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d "第3章。通过了解数据来理解问题")，“通过了解数据来理解问题”，和[第5章](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "第5章。通过示例进行监督和无监督学习")，“通过示例进行监督和无监督学习”，我们讨论了一些根据数据选择最佳算法的技术，并讨论了最广泛使用的算法。为了从模型中获得最佳结果，您必须首先定义“最佳”是什么。我们将以抽象和具体的方式讨论调整。
- en: In the abstract sense of machine learning, tuning involves working with variables
    or based on parameters that have been identified to affect system performance
    as evaluated by some appropriate metric. Hence, the improved performance reveals
    which parameter settings are more favorable (that is, tuned) or less favorable
    (that is, un-tuned). In common sense terms, tuning is essentially selecting the
    best parameters for an algorithm to optimize its performance, given the working
    environment of hardware, specific workloads, and so on And tuning in machine learning
    is an automated process for doing this.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的抽象意义上，调整涉及使用已经确定影响系统性能的变量或基于参数，通过一些适当的度量来评估。因此，改进的性能揭示了哪些参数设置更有利（即调整）或不利（即未调整）。在常识层面上，调整实质上是选择算法的最佳参数，以优化其性能，考虑到硬件的工作环境、特定工作负载等。机器学习中的调整是一个自动化的过程。
- en: Well, let's get to the point and make the discussion more concrete through some
    examples. If you take an ML algorithm for clustering like KNN or K-Means, as a
    developer/data scientist/data engineer you need to specify the number of K in
    your model or centroids.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，让我们通过一些例子更具体地讨论这个问题。如果你选择一个用于聚类的ML算法，比如KNN或K-Means，作为开发者/数据科学家/数据工程师，你需要指定模型或质心中的K的数量。
- en: So the question is 'how can you do this?' Technically, there is no shortcut
    around the need to tune the model. Computationally a naïve approach would be to
    try with different values of K as a model and of course observing how it goes
    to inter and intra-group error as you vary the number of K in your model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，问题是“你如何做到这一点？”从技术上讲，没有绕过调整模型的必要。计算上，一个天真的方法是尝试不同的K值作为模型，当然观察当你改变模型中的K的数量时，它如何转向组内和组间误差。
- en: The second example could be by using the **Support Vector Machine** (**SVM**)
    for classification tasks. As you know, an SVM classification requires an initial
    learning phase in which the training data are used to adjust the classification
    parameters. This really denotes an initial parameter-tuning phase where you might
    try to tune the models in order to achieve high-quality results.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个例子可能是使用**支持向量机**（**SVM**）进行分类任务。正如你所知，SVM分类需要一个初始学习阶段，其中训练数据用于调整分类参数。这实际上表示一个初始的参数调整阶段，你可能会尝试调整模型以获得高质量的结果。
- en: 'The third practical example suggest that there is no such thing as a perfect
    set of optimizations for all deployments of an Apache web server. A sysadmin learns
    from the data on the job, so to speak, and optimizes it''s own Apache web server
    configuration as appropriate for its specific environment. Now imagine an automated
    process for doing those three things, that is, a system that can learn from data
    on its own; the definition of machine learning. A system that tunes its own parameters
    in such a data-based fashion would be an instance of tuning in machine learning.
    Now, let us summarize the main points of why we evaluate the predictive performance
    of a model:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个实际例子表明，没有一套完美的优化适用于所有Apache Web服务器的部署。系统管理员从工作中的数据中学习，适当地优化其特定环境的Apache Web服务器配置。现在想象一个自动化过程来完成这三件事，也就是说，一个可以自行从数据中学习的系统；这就是机器学习的定义。一个以这种基于数据的方式调整自己参数的系统将是机器学习中调整的一个实例。现在，让我们总结为什么我们评估模型的预测性能的主要原因：
- en: We want to estimate the generalization error, the predictive performance of
    our model on future (unseen) data.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望估计泛化误差，即我们的模型在未来（未见过的）数据上的预测性能。
- en: We want to increase the predictive performance by tweaking the learning algorithm
    and selecting the best-performing model from a given hypothesis space.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望通过调整学习算法并从给定的假设空间中选择表现最佳的模型来提高预测性能。
- en: We want to identify the machine learning algorithm that is best-suited for the
    problem at hand; thus, we want to compare different algorithms, selecting the
    best-performing one as well as the best-performing model from the algorithm's
    hypothesis space.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要确定最适合手头问题的机器学习算法；因此，我们想要比较不同的算法，选择表现最佳的算法以及该算法假设空间中表现最佳的模型。
- en: 'In a nutshell, there are four steps in the process of finding the best parameter
    set:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在找到最佳参数集的过程中有四个步骤：
- en: '**Define the parametric space**: First, we need to decide the exact parameter
    values we would like to consider for the algorithm.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义参数空间**：首先，我们需要决定我们想要考虑的算法的确切参数值。'
- en: '**Define the cross-validation settings**: Secondly we need to decide how to
    choose the optimal cross-validation folds for the data (to be discussed later
    in this chapter).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义交叉验证设置**：其次，我们需要决定如何为数据选择最佳的交叉验证折叠（将在本章后面讨论）。'
- en: '**Define the metric**: Thirdly, we need to decide which metric to use for determining
    the best combination of parameters. For example, accuracy, root means squared
    error, precision, recall, or f-score, and so on.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义度量标准**：第三，我们需要决定使用哪种度量标准来确定最佳参数组合。例如，准确度、均方根误差、精确度、召回率或F分数等等。'
- en: '**Train, evaluate and compare**: Fourthly, for each unique combination of the
    parameter values, cross-validation is carried out and based on the error metric
    defined by the user in the third step, the best-performing model can be chosen.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练、评估和比较**：第四，对于每个参数值的唯一组合，进行交叉验证，并根据用户在第三步中定义的错误度量，可以选择表现最佳的模型。'
- en: There are many techniques and algorithms available for model tunings like hyperparameter
    optimization or model selection, hyperparameter tuning, grid search parameter
    tuning, random search parameter tuning and **Cross Validation** (**CV**). Unfortunately,
    the current implementation of Spark has developed only a few of them including
    Cross Validator and TrainValidationSplit.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 模型调整有许多可用的技术和算法，比如超参数优化或模型选择、超参数调整、网格搜索参数调整、随机搜索参数调整和**交叉验证**（**CV**）。不幸的是，Spark的当前实现只开发了其中的一部分，包括交叉验证器和训练验证拆分。
- en: Therefore, we will try to use these two hyperparameters to tune different models
    including Random Forest, Liner Regression, and Logistic Regression. Some applications
    from [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples* and [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines* will be re-used without providing many details again
    to make the model tuning easier.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将尝试使用这两个超参数来调整不同的模型，包括随机森林、线性回归和逻辑回归。一些来自[第5章](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "第5章。通过示例进行监督和无监督学习")和[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "第6章。构建可扩展的机器学习管道")的应用将被重新使用，而不再提供许多细节，以使模型调整更容易。
- en: Typical challenges in model tuning
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型调优中的典型挑战
- en: After the following discussion, you might be thinking that this process is difficult,
    and you'd be right. In fact, because of the difficulty in determining what optimal
    model parameters are, often some more complex learning algorithms are used before
    experimenting effectively with simpler options with better-tuned parameters. As
    we've already discussed, machine learning involves a lot of experimentation. And
    the tuning of the internal knobs of a learning algorithm, commonly referred to
    as hyperparameters, are equally important from model building to prediction and
    before deployment.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下讨论之后，您可能会认为这个过程很困难，您是对的。事实上，由于确定最佳模型参数的困难，通常在有效地尝试更简单的选项之前会使用一些更复杂的学习算法，这些选项具有更好调整的参数。正如我们已经讨论过的，机器学习涉及大量的实验。调整学习算法的内部旋钮，通常称为超参数，从模型构建到预测以及部署之前同样重要。
- en: Technically, running a learning algorithm over a training dataset with different
    hyperparameter settings will result in different models, and of course different
    performance parameters. According to Oracle developers, it is not recommended
    to begin tuning without having first established clear objectives, since you cannot
    succeed if there is any definition of success.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，在训练数据集上使用不同超参数设置运行学习算法将导致不同的模型，当然也会有不同的性能参数。据Oracle开发人员称，不建议在没有首先确立清晰目标的情况下开始调优，因为如果没有成功的定义，就不可能成功。
- en: Subsequently, we are typically interested in selecting the best-performing model
    from the training data set; we need to find a way to estimate their respective
    performances in order to rank them against each other. Going one step beyond mere
    algorithm fine-tuning, we are usually not only experimenting with the one single
    algorithm that we think would be the best solution under the given circumstances.
    More often than not, we want to compare different algorithms to each other, often
    in terms of predictive and computational performance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们通常有兴趣从训练数据集中选择表现最佳的模型；我们需要找到一种方法来估计它们各自的性能，以便将它们彼此排名。在超越纯粹的算法微调之后，我们通常不仅仅是在尝试我们认为在给定情况下是最佳解决方案的单一算法。往往我们想要将不同的算法相互比较，通常是在预测和计算性能方面。
- en: 'Often there is a very basic question regarding parameter tuning using grid
    search and random search. Typically, some machine learning methods have parameters
    that need to be tuned using either of them. For example, according to Wei Ch.
    et al., (*A General Formulation for Support Vector Machine, proceedings of the
    9th International Conference on Neural Information Processing (ICONIP, 02), V-5,
    18-22 Nov. 2002*) the standard formulation of SVMs is as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常有一个非常基本的问题，即使用网格搜索和随机搜索进行参数调优。通常，一些机器学习方法需要使用其中之一来调整参数。例如，根据Wei Ch.等人的说法，(*支持向量机的一般公式，第9届国际神经信息处理会议论文集（ICONIP，02），V-5，2002年11月18-22日*)
    SVM的标准公式如下：
- en: '![Typical challenges in model tuning](img/00086.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![模型调优中的典型挑战](img/00086.jpeg)'
- en: 'Figure 1: The standard formula for SVM'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：SVM的标准公式
- en: Now, suppose we need to tune model parameter C, and we need to do it with ease.
    It's clearly seen from the equation that, tuning C also involves other parameters
    like *xi*, *i* and *w*; where, the regularization parameter C > 0, is the norm
    of w. In the RHS it is the stabilizer and ![Typical challenges in model tuning](img/00100.jpeg)
    is the empirical loss term depending upon the target function f(xi). In the standard
    SVMs (either linear SVM or other variants), the regularized functionalities can
    be minimized further by solving the convex quadratic optimization problem. Once
    the problem is solved, it guarantees the unique global minimum solution to gain
    the best predictive performance. Therefore, the whole process is more or less
    an optimization problem.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们需要调整模型参数C，并且需要轻松地进行调整。从方程中可以清楚地看到，调整C还涉及其他参数，如*xi*，*i*和*w*；其中，正则化参数C
    > 0，是w的范数。在RHS中，它是稳定器，![模型调优中的典型挑战](img/00100.jpeg)是依赖于目标函数f(xi)的经验损失项。在标准SVM（线性SVM或其他变体）中，可以通过解决凸二次优化问题进一步最小化正则化功能。一旦问题解决了，它就保证了获得最佳预测性能的唯一全局最小解。因此，整个过程多多少少是一个优化问题。
- en: 'In summary, *Figure 2, The model tuning process, consideration, and workflow*
    shows the tuning process and its consideration as a workflow:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，*图2，模型调优过程、考虑和工作流程*显示了调优过程及其考虑作为工作流程：
- en: '![Typical challenges in model tuning](img/00061.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![模型调优中的典型挑战](img/00061.jpeg)'
- en: 'Figure 2: The model tuning process, consideration, and workflow'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：模型调优过程、考虑和工作流程
- en: Now, if you are given the raw dataset, you will most probably do the pre-processing
    and split the dataset into training and test sets. Therefore, to tune hyperparameter
    C, you need to first split the training set into a validation training set and
    a validation test set.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您获得了原始数据集，您很可能会进行预处理并将数据集分成训练集和测试集。因此，要调整超参数C，您需要首先将训练集分成验证训练集和验证测试集。
- en: After that, you might try tuning the parameters using the validation training
    set and validation test set. Then use the best parameters you've got and retrain
    the model on the complete training set. Now you can perform the testing on the
    test set as the final step.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，您可以尝试使用验证训练集和验证测试集来调整参数。然后使用您得到的最佳参数在完整的训练集上重新训练模型。现在您可以在测试集上执行测试作为最后一步。
- en: Up to this point your approach seems to be okay, but which of the following
    two options do you think is better, on average?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您的方法似乎还可以，但以下两个选项中哪一个平均更好呢？
- en: Would it be better to use the final model from validation, which was trained
    on a validation training set for the final testing?
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用在验证训练集上训练的最终模型进行最终测试会更好吗？
- en: Or would it better to use the entire training set and retrain the model with
    the best parameters from the grid or random search? Although the parameters were
    not optimised for this set, we have final training data in this case.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者最好使用整个训练集，并使用网格或随机搜索的最佳参数重新训练模型吗？尽管这些参数未针对此集进行优化，但在这种情况下，我们有最终的训练数据。
- en: Are you intending to go for option 1, because the parameters were already optimized
    on this training (that is, the validation training set) set? Or are you intending
    to go for option 2 because, although the parameters were not optimized for the
    training set, you have the final training data in this case? We suggest you go
    for option 2, but only if you trust your validation setup in option 2\. The reason
    is that you have performed the **Cross Validation** (**CV**) to identify the most
    general parameters setup or else model selection or whatever you're trying to
    optimize. These findings should be applied to the entire training set and tested
    once on the test set.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您打算选择选项1吗，因为参数已经在此训练（即验证训练集）集上进行了优化？还是您打算选择选项2，因为尽管这些参数未针对训练集进行优化，但在这种情况下，您有最终的训练数据？我们建议您选择选项2，但前提是您信任选项2中的验证设置。原因是您已执行了**交叉验证**（**CV**）以确定最通用的参数设置，否则模型选择或您尝试优化的任何其他内容。这些发现应该应用于整个训练集，并在测试集上进行一次测试。
- en: Well, suppose you went for option 2; now the second challenge is evolving. How
    do we estimate the performance of a machine learning model? Technically, you might
    argue that we should nourish the training data to our learning algorithm to learn
    the optimal model. Then we could predict the labels based on the test labels.
    Thirdly, we count the number of wrong predictions on the test dataset to compute
    the model's error rate.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，假设您选择了选项2；现在第二个挑战正在发展。我们如何估计机器学习模型的性能？从技术上讲，您可能会认为我们应该将训练数据提供给我们的学习算法，以学习最佳模型。然后，我们可以基于测试标签预测标签。其次，我们计算测试数据集上的错误预测数量，以计算模型的错误率。
- en: 'That''s it? Not so fast my friend! Be contingent on our goal, unfortunately
    guesstimating the performance of that model is not that insignificant. Maybe we
    should address the previous question from another angle: why do we care about
    performance estimation at all? Well, ideally, the estimated performance of a model
    tells how well it performs on unobserved data - making predictions on future data
    is often the main problem we want to solve in applications of machine learning
    or the development of novel algorithms.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样？朋友，不要那么着急！根据我们的目标，不幸的是，估计该模型的性能并不那么微不足道。也许我们应该从另一个角度来解决前面的问题：我们为什么要关心性能估计？理想情况下，模型的估计性能告诉我们它在未观察到的数据上的表现如何-在应用机器学习或开发新算法的应用中，对未来数据进行预测通常是我们想要解决的主要问题。
- en: Finally, there are several other challenges depending upon the data structure,
    problem type, problem domain and appropriate use cases that need to be addressed
    that you will come across when you start practicing a lot.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有其他几个挑战，取决于数据结构、问题类型、问题领域和适当的用例，当您开始大量练习时，您将遇到这些挑战。
- en: Evaluating machine learning models
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估机器学习模型
- en: In this section, we will discuss how to evaluate a machine learning model because
    you should always evaluate a model to determine if it is ready to perform well
    consistently, predicting the target for new and future data. Obviously future
    data might have many unknown target values. Therefore, you need to check performance-related
    metrics such as the accuracy metric of the ML model on the data. In this regard,
    you need to provide a dataset containing scores generated from a trained model
    and then evaluate the model to compute a set of industry-standard evaluation metrics.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论如何评估机器学习模型，因为您应该始终评估模型，以确定它是否准备好始终表现良好，预测新数据和未来数据的目标。显然，未来的数据可能有许多未知的目标值。因此，您需要检查性能相关的指标，如数据上的ML模型的准确度指标。在这方面，您需要提供一个包含从训练模型生成的分数的数据集，然后评估模型以计算一组行业标准的评估指标。
- en: To evaluate a model appropriately, you need to present a sample of data that
    has been labeled with the target and this data will be used as the ground truth
    or facts dataset from the training data source. As we have already discussed,
    evaluating the predictive accuracy of an ML model with the same training dataset
    might not be useful.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要适当评估模型，您需要提供一个已标记目标的数据样本，这些数据将用作来自训练数据源的地面真相或事实数据集。正如我们已经讨论过的那样，使用相同的训练数据集评估ML模型的预测准确度可能没有用。
- en: The reason is that the model itself can remember the training data based on
    the rewards it receives instead of generalizing from it. Thus, when the ML model
    training has finished, you can get to know the target values to be predicted from
    the presented observations in the model. After that you can compare the predicted
    values that are returned by the ML model you have trained against the known target
    value.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是模型本身可以根据其接收的奖励记住训练数据，而不是从中概括。因此，当ML模型训练完成时，您可以从模型中呈现的观察中了解要预测的目标值。之后，您可以将您训练的ML模型返回的预测值与已知的目标值进行比较。
- en: Finally, you might be interested in computing the summary metric that shows
    the performance metrics to indicate how well the predicted and true values match
    accuracy parameters such as precision, recall, weighted true positive, weighted
    true negative, lift, and so on. In this section, however, we will particularly
    discuss how we can evaluate regression, classification (that is, binary classification,
    multiclass classification) and clustering model in the first place.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可能对计算总结指标感兴趣，该指标显示性能指标，以指示预测值和真实值匹配的准确度参数，如精确度、召回率、加权真正例、加权真负例、提升等。然而，在这一部分，我们将特别讨论如何首先评估回归、分类（即二元分类、多类分类）和聚类模型。
- en: Evaluating a regression model
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估回归模型
- en: Assume you that you are an online currency trader and you work on Forex or Fortrade.
    Right now you have two currency pairs in mind to buy or sell, for example, GBP/USD
    and USD/JPY pairs. If you look at these two pairs carefully, you'll see that USD
    is common in both pairs. Now, if you observe the historical prices of USD, GBP
    or JPY you can predict the future outcome of whether you should open the trade
    in buy or sell.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您是一名在线货币交易员，您在外汇或Fortrade上工作。现在您心中有两个货币对要买入或卖出，例如GBP/USD和USD/JPY。如果您仔细观察这两个货币对，您会发现USD在这两个货币对中都是共同的。现在，如果您观察USD、GBP或JPY的历史价格，您可以预测未来的结果，即您应该开仓买入还是卖出。
- en: This kind of problem can be treated as the typical regression problem. Here,
    the target variable (price in this case) is a continuous numeric value changing
    over time, based on the market opening time. Therefore, for making predictions
    on the prices, based on the given feature values of a certain currency (that is,
    USD, GBP or JPY in this example), we can fit a simple linear regression or logistic
    regression model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种问题可以被视为典型的回归问题。在这里，目标变量（在这种情况下是价格）是一个随时间变化的连续数值，基于市场开盘时间。因此，为了根据给定的某种货币（例如本例中的USD、GBP或JPY）的特征值做出价格预测，我们可以拟合一个简单的线性回归或逻辑回归模型。
- en: In this case, the feature values could be the historical prices and some external
    factors that drift the value of a certain currency or currency pair. In this way,
    the trained build model can predict the price of a certain currency.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，特征值可以是历史价格和一些外部因素，这些因素会使某种货币或货币对的价值发生变化。通过这种方式，训练好的模型可以预测某种货币的价格。
- en: The regression models (that is, linear, logistic or generalized liner regression
    models) can be used for finding or calculating the score of the same dataset we
    trained, now that the predicted prices of all of the currencies or the historical
    prices of these three currencies are available. We can further evaluate the performance
    of the model by analyzing, on average, how much the predicted prices deviate compared
    to the actual prices. In this way, people can guess whether the predicted price
    will go up or down and can earn money from online currency websites such as Forex
    or Fortrade and so on.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型（即线性、逻辑或广义线性回归模型）可用于找到或计算我们训练的相同数据集的分数，现在可以使用所有货币的预测价格或这三种货币的历史价格。我们可以通过分析预测价格与实际价格相比平均偏离多少来进一步评估模型的性能。通过这种方式，人们可以猜测预测价格是上涨还是下跌，并可以从外汇或Fortrade等在线货币网站赚钱。
- en: Evaluating a binary classification model
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估二元分类模型
- en: 'As we have already discussed in the binary classification scenario, there are
    only two possible outcomes for the target variable. For example: {0, 1}, {spam,
    hap}, {B, N}, {false, true} and {negative, positive} and so on. Now assume that
    you are given a dataset comprising researchers around the world with demographic,
    socio-economic and employment variables and you would like to predict the Ph.D.
    scholarship amount (that is, salary level) as a binary variable with the values
    of, say, {<=1.5K$, >=4.5K$}.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在二元分类场景中已经讨论过的，目标变量只有两种可能的结果。例如：{0, 1}，{垃圾邮件，快乐}，{B，N}，{false，true}和{负面，正面}等。现在假设您获得了一个包含世界各地研究人员的人口统计学、社会经济和就业变量的数据集，并且您想要预测博士奖学金金额（即薪水水平）作为一个二元变量，其值为，比如{<=1.5K$，>=4.5K$}。
- en: In this particular example, the negative class would represent the researcher
    whose salary or scholarship is less than or equal to $1,500 monthly. Consequently,
    the positive class, on the other hand, represents all other researchers whose
    salary is more than or equal to $4,500.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的例子中，负类将代表薪水或奖学金每月低于或等于1500美元的研究人员。因此，另一方面，正类代表薪水高于或等于4500美元的所有其他研究人员。
- en: Now, from the problem scenario, it is clear that it is also a regression problem.
    As a result, you would train a model, score the data, and evaluate the results
    and see how much it deviates with actual labels. Therefore, in this type of problem,
    you would perform an experiment to evaluate the performance of a two-class (that
    is, binary class) logistic regression model, which is one of the most commonly
    used binary classifier in the area of ML.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从问题场景中可以清楚地看出，这也是一个回归问题。因此，您将训练模型，对数据进行评分，并评估结果，看看它与实际标签相差多少。因此，在这种类型的问题中，您将进行实验，评估两类（即二元类）逻辑回归模型的性能，这是ML领域中最常用的二元分类器之一。
- en: Evaluating a multiclass classification model
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估多类分类模型
- en: In [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*, we developed several applications and pipelines,
    and you might remember that we also developed a multiclass classification problem
    for the OCR dataset using logistic regression and showed the result using Multiclass
    Metrics. In that example, there were 26 classes for 26 characters (that is, from
    A to Z). We had to predict the class or label of a certain character and whether
    it really fell under the correct class or labels. This kind of regression problem
    can be resolved using the multiclass classification method.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d"第6章。构建可扩展的机器学习管道")中，*构建可扩展的机器学习管道*，我们开发了几个应用程序和管道，您可能还记得我们还使用逻辑回归为OCR数据集开发了一个多类分类问题，并使用多类指标显示了结果。在该示例中，有26个类别对应26个字符（即从A到Z）。我们必须预测某个字符的类别或标签，以及它是否真的属于正确的类别或标签。这种回归问题可以使用多类分类方法解决。
- en: Therefore, in this type of problem, you would perform an experiment to evaluate
    the performance of a multiclass (that is, more than two class) logistic regression
    model too.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种类型的问题中，您还将进行实验，评估多类（即两类以上）逻辑回归模型的性能。
- en: Evaluating a clustering model
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估聚类模型
- en: Since the clustering models differ expressively from classification and regression
    models in many different aspects, if you evaluate a clustering model you will
    find a different set of statistics and performance related metrics for clustering
    models. The performance metrics that were returned in the clustering model evaluation
    technique, describe how many data points were assigned to each cluster, the amount
    of separation between clusters, and how tightly the data points are bunched within
    each cluster.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于聚类模型在许多不同方面与分类和回归模型有显著差异，如果您评估一个聚类模型，您将找到一组不同的统计数据和与聚类模型相关的性能指标。在聚类模型评估技术中返回的性能指标描述了每个簇分配了多少数据点，簇之间的分离程度以及数据点在每个簇内的紧密程度。
- en: 'For example, if you recall, in [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples*, you found a clustering problem that we discussed
    and resolved using Spark ML and MLlib in the *Unsupervised Learning with Spark:
    An example section*. In that particular example, we showed K-Means clustering
    of the neighborhood using the Saratoga NY Homes dataset, and showed an exploratory
    analysis based on price and lot size features for possible neighborhoods of houses
    located in the same area. This kind of problem can be resolved and evaluated using
    a clustering model. However, the current implementation of Spark does not yet
    provide any developed algorithm for model evaluation.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您回忆一下，在[第5章](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "第5章。 通过示例进行监督和无监督学习")中，*通过示例进行监督和无监督学习*，您会发现我们讨论并使用Spark ML和MLlib解决了一个聚类问题，在*使用Spark进行无监督学习：示例部分*。在这个特定的例子中，我们展示了使用Saratoga
    NY Homes数据集的K-Means聚类，并基于价格和地块大小特征对位于同一地区的房屋可能的邻域进行了探索性分析。这种问题可以使用聚类模型解决和评估。然而，Spark的当前实现尚未为模型评估提供任何发展的算法。
- en: Validation and evaluation techniques
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证和评估技术
- en: 'There are several widely used terms in machine learning application development
    that can be a bit tricky and confusing, so let''s talk through them and sort them
    out. These terms include model, target function, hypothesis, confusion matrix,
    model deployment, induction algorithm, classifier, learning algorithms, cross-validation,
    and parameters:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习应用开发中有一些广泛使用的术语可能有点棘手和令人困惑，所以让我们一起讨论并梳理一下。这些术语包括模型、目标函数、假设、混淆矩阵、模型部署、归纳算法、分类器、学习算法、交叉验证和参数：
- en: '**Target function**: In reinforcement learning or predictive modeling, let''s
    say we focus on modeling an object. The ultimate target is to learn or approximate
    a specific and unknown but targeted function. The target function is denoted as
    *f(x) = y*; where *x* and *y* both are variable and *f(x)* is the true function
    that we want to model and it also signifies that we are trying to maximize or
    achieve the target value *y*.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标函数**：在强化学习或预测建模中，假设我们专注于对一个对象进行建模。最终目标是学习或逼近一个特定且未知但有针对性的函数。目标函数表示为*f(x)
    = y*；其中*x*和*y*都是变量，*f(x)*是我们想要建模的真实函数，它也表示我们试图最大化或实现目标值*y*。'
- en: '**Hypothesis**: A statistical hypothesis (don''t confuse this with the research
    hypothesis proposed by the researcher) is a generalized function that is testable
    on the basis of observing a process. The process is similar to the true function
    that is modeled through a set of random variables from the training dataset. The
    objective behind the hypothesis testing is that a hypothesis is proposed for measuring
    the statistical relationship between the two data sets for the example training
    set and test set, where, both the datasets have to be statistically significant
    from an idealized (remember not a randomized or normalized model) model.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假设**：统计假设（不要将其与研究者提出的研究假设混淆）是一个可通过观察过程进行测试的广义函数。该过程类似于通过训练数据集中的一组随机变量对建模的真实函数进行测试。假设检验的目标是提出一个假设，以测量两个数据集之间的统计关系，例如训练集和测试集，其中，这两个数据集都必须与理想化模型（记住不是随机化或标准化模型）具有统计显著性。'
- en: '**Learning algorithm**: As already stated, the ultimate goal of a machine learning
    application is to find or approximate the target function. In this continuous
    process, the learning algorithm is a set of instructions that models the target
    function using the training dataset. Technically, a learning algorithm often comes
    with a hypothesis space and formulates the final hypothesis.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习算法**：如前所述，机器学习应用的最终目标是找到或逼近目标函数。在这个持续的过程中，学习算法是一组指令，它使用训练数据集对目标函数进行建模。从技术上讲，学习算法通常伴随着一个假设空间，并制定最终的假设。'
- en: '**Model**: A statistical model is a mathematical model that exemplifies a set
    of assumptions while generating sample and similar data from a larger population.
    Finally, the model often represents a significantly idealized form for the data-generating
    process. Also, in the machine learning area, the terms hypothesis and model are
    often used interchangeably.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：统计模型是一个数学模型，它在生成样本和类似数据时体现了一组假设。最终，该模型通常代表了数据生成过程的一个显著理想化形式。此外，在机器学习领域，假设和模型这两个术语经常可以互换使用。'
- en: '**Induction algorithm**: An induction algorithm takes input specific instances
    to produce a generalized model beyond these input instances.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归纳算法**：归纳算法接受特定实例的输入，以产生超出这些输入实例的广义模型。'
- en: '**Model deployment**: Model deployment usually denotes applying an already
    built and developed model to the real data in order to make a prediction for an
    example.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型部署**：模型部署通常表示将已构建和开发的模型应用于实际数据，以对一个示例进行预测。'
- en: '**Cross-validation**: this is a method for estimating accuracy in terms of
    an error out of a machine learning model by dividing the data into K mutually
    exclusive subsets or folds of approximately equal size. The model then is trained
    and tested K times in iteration, each time on the available dataset excluding
    a fold and then tested on that fold.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证：这是一种通过将数据分成大约相等大小的 K 个互斥子集或折叠来估计机器学习模型的准确性的方法。然后，模型在迭代中进行 K 次训练和测试，每次在可用数据集上排除一个折叠，然后在该折叠上进行测试。
- en: '**Classifier**: A classifier is a special case of hypothesis or discrete-valued
    function that is used to assign the most categorical class labels to particular
    data points such as the label point.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器：分类器是假设或离散值函数的特殊情况，用于为特定数据点分配最分类类别标签。
- en: '**Regressor**: A regressor is also a special case of hypothesis that does the
    mapping from unlabeled features to a value within a predefined metric space.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归器：回归器也是假设的一种特殊情况，它将未标记的特征映射到预定义度量空间内的值。
- en: '**Hyperparameters**: Hyperparameters are the tuning parameters of a machine
    learning algorithm to which a learning algorithm fits the training data.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数：超参数是机器学习算法的调整参数，学习算法将训练数据拟合到这些参数上。
- en: 'According to Pedro D. et al., *A Few Useful Things to Know about Machine Learning*
    at [http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf),
    we also need a few more things outlined, as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Pedro D. 等人在[http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)中的
    *关于机器学习的一些有用的东西*，我们还需要概述一些其他事项，如下所示：
- en: '**Representation**: a classifier or regressor must be represented in some formal
    language that a computer can process by creating proper hypothesis spaces.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示：分类器或回归器必须用计算机可以处理的某种形式语言表示，从而创建适当的假设空间。
- en: '**Evaluation**: An evaluation function (that is, objective function or scoring
    function) is needed to distinguish a good vs bad classifier r regressor, which
    is used internally by the algorithm by which the model has been build or trained.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估：需要一个评估函数（即目标函数或评分函数）来区分好的分类器或回归器与坏的分类器或回归器，该函数由算法内部使用，用于构建或训练模型。
- en: '**Optimization**: We also need to have a method for searching among the classifiers
    or regressor, aiming for the highest scoring one. Thus the optimization is the
    key to the efficiency of the learner that helps to determine the optimal parameters.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化：我们还需要一种在分类器或回归器中搜索的方法，以寻求最高得分。因此，优化是决定学习者效率的关键，有助于确定最佳参数。
- en: 'In a nutshell, the key formula in learning in an ML algorithm is:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在机器学习算法中学习的关键公式是：
- en: '*Learning = Representation + Evaluation + Optimization*'
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*学习 = 表示 + 评估 + 优化*'
- en: Consequently, to validate and evaluate the trained model you need to understand
    the above terms very clearly so that you can conceptualize the ML problem and
    the proper uses of the Spark ML and MLlib APIs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了验证和评估训练好的模型，您需要非常清楚地理解上述术语，以便能够概念化机器学习问题以及 Spark ML 和 MLlib API 的正确用法。
- en: Parameter tuning for machine learning models
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的参数调整
- en: In this section, we will discuss tuning parameters and technique for machine
    learning models such as hyperparameter tuning, random search parameter tuning,
    and grid search parameter tuning and cross-validation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论调整参数和技术，例如超参数调整、随机搜索参数调整、网格搜索参数调整和交叉验证等机器学习模型的调整参数和技术。
- en: Hyperparameter tuning
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Hyperparameter tuning is a technique for choosing the right combination of parameters
    based on the performance of presented data. It is one of the fundamental requirements
    to obtain meaningful and accurate results from machine learning algorithms in
    practice. For example, suppose we have two hyperparameters to tune for a pipeline
    presented in *Figure 3*, a Spark ML pipeline model using a logistic regression
    estimator (dash lines only happen during pipeline fitting).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整是一种根据所呈现数据的性能选择正确参数组合的技术。这是从实践中获得机器学习算法的有意义和准确结果的基本要求之一。例如，假设我们有两个要调整的超参数，用于
    *图 3* 中呈现的管道，即使用逻辑回归估计器的 Spark ML 管道模型（虚线仅在管道拟合期间发生）。
- en: 'We can see that we have put three candidate values for each. Therefore, there
    would be nine combinations in total. However, only four are shown in the diagram,
    namely **Tokenizer**, **HashingTF**, **Transformer** and **Logistic Regression**
    (**LR**). Now we want to find the one that will lead to the model with the best
    evaluation result eventually. As we have already discussed in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines,* the fitted model consists of the tokenizer, the hashing
    TF feature extractor, and the fitted logistic regression model:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们为每个参数放入了三个候选值。因此，总共会有九种组合。然而，图中只显示了四种，即 **Tokenizer**、**HashingTF**、**Transformer**
    和 **Logistic Regression**（**LR**）。现在我们要找到最终会导致最佳评估结果的模型。正如我们在[第 6 章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "第 6 章。构建可扩展的机器学习管道")中已经讨论的那样，拟合模型由分词器、哈希 TF 特征提取器和拟合的逻辑回归模型组成：
- en: '![Hyperparameter tuning](img/00148.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: 超参数调整
- en: 'Figure 3: Spark ML pipeline model using logistic regression estimator (dash
    lines only happen during pipeline fitting)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用逻辑回归估计器的 Spark ML 管道模型（虚线仅在管道拟合期间发生）
- en: '*Figure 3* shows the typical workflow of the previously mentioned pipeline.
    The dash line however happens only during the pipeline fitting.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3* 显示了先前提到的管道的典型工作流程。然而，虚线仅在管道拟合期间发生。'
- en: As mentioned earlier, the fitted pipeline model is a transformer. The transformer
    can be used for prediction, model validation, and model inspection. In addition,
    we also argued that one ill-fated distinguishing characteristic of the ML algorithms
    is that typically they have many hyperparameters that need to be tuned for better
    performance.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，拟合的管道模型是一个转换器。转换器可用于预测、模型验证和模型检查。此外，我们还提到ML算法的一个不幸的特点是，它们通常有许多需要调整以获得更好性能的超参数。
- en: For example, the degree of regularizations in these hyperparameters is distinctive
    from the model parameters optimized by the Spark MLlib. As a consequence, it is
    really hard to guess or measure the best combination of hyperparameters without
    expert knowledge of the data and the algorithm to use. Since the complex dataset
    is based on the ML problem type, the size of the pipeline and the number of hyperparameters
    may grow exponentially (or linearly), the hyperparameter tuning becomes cumbersome
    even for an ML expert, not to mention that the result of the tuning parameters
    may become unreliable
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这些超参数中的正则化程度与Spark MLlib优化的模型参数不同。因此，如果没有对数据和要使用的算法的专业知识，很难猜测或衡量最佳的超参数组合。由于复杂数据集是基于ML问题类型的，管道的大小和超参数的数量可能呈指数级增长（或线性增长），即使对于ML专家来说，超参数调整也变得繁琐，更不用说调整参数的结果可能变得不可靠。
- en: 'According to Spark API documentation provided at [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html),
    a unique and uniform API is used for specifying Spark ML Estimators and Transformers.
    A `ParamMap` is a set of (parameter, value) pairs with a `Param` as a named parameter
    with self-contained documentation provided by Spark. Technically, there are two
    ways for passing the parameters to an algorithm as specified in the following
    options:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)提供的Spark
    API文档，用于指定Spark ML估计器和转换器的是一个独特且统一的API。`ParamMap`是一组具有`Param`作为命名参数的（参数，值）对，由Spark提供自包含文档。从技术上讲，有两种方式可以传递参数给算法，如下所示：
- en: 'Setting parameters. For example, if an LR is an instance of `LogisticRegression`
    (that is, Estimator), you can call the `setMaxIter()` method as follows: `LR.setMaxIter(5)`.
    It essentially fits the model pointing the regression instance as follows: `LR.fit()`.
    In this particular example, there would be at most five iterations.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置参数。例如，如果LR是`LogisticRegression`的一个实例（即估计器），您可以调用`setMaxIter()`方法如下：`LR.setMaxIter(5)`。这本质上是将回归实例指向模型的拟合：`LR.fit()`。在这个特定的例子中，最多会有五次迭代。
- en: The second option involves passing a `ParamMaps` to `fit()` or `transform()`
    (refer *Figure 1* for details). In this circumstance, any parameters will be overridden
    by the `ParamMaps` previously specified via setter methods in the ML application-specific
    codes or algorithms.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个选项涉及将`ParamMaps`传递给`fit()`或`transform()`（有关详细信息，请参阅*图1*）。在这种情况下，任何参数都将被先前通过ML应用程序特定代码或算法中的setter方法指定的`ParamMaps`覆盖。
- en: Tip
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'An excellent way to create a shortlist of well-performing algorithms for your
    dataset is to use the Caret package in R since tuning in Spark is not that robust.
    Caret is a package in R created and maintained by Max Kuhn from Pfizer. Development
    started in 2005 and was later made open source and uploaded to CRAN which is actually
    an acronym which stands for **Classification And Regression Training** (**CARET**).
    It was initially developed out of the need to run multiple different algorithms
    for a given problem. Interested readers can have a look at that package for theoretical
    and practical consideration by visiting: [http://topepo.github.io/caret/index.html](http://topepo.github.io/caret/index.html).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为您的数据集创建一个表现良好的算法的简短列表的一个很好的方法是使用R中的Caret包，因为Spark中的调整不够健壮。Caret是由辉瑞公司的Max Kuhn创建和维护的R中的一个包。开发始于2005年，后来被开源并上传到CRAN，实际上CRAN是**分类和回归训练**（**CARET**）的缩写。最初开发是出于运行给定问题的多个不同算法的需求。有兴趣的读者可以访问[http://topepo.github.io/caret/index.html](http://topepo.github.io/caret/index.html)查看该包的理论和实际考虑。
- en: Grid search parameter tuning
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格搜索参数调整
- en: 'Suppose you have selected your hyperparameters, and by applying tuning, you
    now also need to find the features. In this regard, a full grid search of the
    space of hyperparameters and features is computationally too intensive. Therefore,
    you need to perform a fold of the K-fold cross-validation instead of a full grid
    search:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经选择了您的超参数，并通过调整应用了调整，现在您还需要找到特征。在这方面，对超参数和特征空间进行完整的网格搜索计算量太大。因此，您需要执行K折交叉验证的一次折叠，而不是进行完整的网格搜索：
- en: Tune the required hyperparameters using cross-validation on the training set
    of the fold, using all the available features
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用折叠的训练集在交叉验证上调整所需的超参数，使用所有可用的特征。
- en: Select the required features using those hyperparameters
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这些超参数选择所需的特征
- en: Repeat the computation for each fold in K
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对K中的每个折叠重复计算
- en: The final model is constructed on all the data using the N most prevalent features
    that were selected from each fold of CV
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终模型是在使用从CV的每个折叠中选择的N个最常见特征的所有数据上构建的
- en: The interesting thing is that the hyperparameters would also be tuned again
    using all the data in a cross-validation loop. Would there be a large downside
    from this method as compared to a full grid search? In essence, I am doing a line
    search in each dimension of free parameters (finding the best value in one dimension,
    holding that constant then finding the best in the next dimension), rather than
    every single combination of parameter settings.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，超参数也将在交叉验证循环中再次使用所有数据进行调整。与完整的网格搜索相比，这种方法会有很大的缺点吗？实质上，我在每个自由参数的维度上进行线性搜索（找到一个维度上的最佳值，将其保持不变，然后找到下一个维度上的最佳值），而不是每个参数设置的每个组合。
- en: The most important downside for searching along single parameters instead of
    optimizing them altogether is that you ignore interactions. It is quite common
    that, for instance, more than one parameter influences model complexity. In that
    case, you need to look at their interaction in order to successfully optimize
    the hyperparameters. Depending on how large your data set is and how many models
    you compare, optimization strategies that return the maximum observed performance
    may run into trouble (this is true for both grid search and your strategy).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着单个参数搜索而不是一起优化的最重要的缺点是忽略了相互作用。很常见的情况是，例如，不止一个参数影响模型复杂性。在这种情况下，您需要查看它们的相互作用，以成功地优化超参数。根据您的数据集有多大以及您比较了多少模型，返回最大观察性能的优化策略可能会遇到麻烦（这对于网格搜索和您的策略都是如此）。
- en: 'The reason is that searching through a large number of performance estimates
    for the maximum skims the variance of the performance estimate: you may just end
    up with a model and training/test split combination that accidentally happens
    to look good. Even worse, you may get several perfect-looking combinations, and
    the optimization then cannot know which model to choose and thus becomes unstable.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是在大量性能估计中搜索最大值会忽略性能估计的方差：您可能最终得到一个看起来不错的模型和训练/测试拆分组合。更糟糕的是，您可能会得到几个看起来完美的组合，然后优化就无法知道选择哪个模型，因此变得不稳定。
- en: Random search parameter tuning
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机搜索参数调整
- en: The default method for optimizing tuning parameters in the train is to use a
    grid search. This approach is usually effective but, in cases when there are many
    tuning parameters, it can be inefficient. There are a number of models where this
    can be beneficial in finding reasonable values of the tuning parameters in a relatively
    short time. However, there are some models where the efficiency in a small search
    field can cancel out other optimizations. Unfortunately, the current implementation
    in Spark for hyperparameter tuning does not provide any technique for random search
    tuning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中优化调整参数的默认方法是使用网格搜索。这种方法通常是有效的，但是在存在许多调整参数的情况下，可能效率低下。有一些模型可以在相对较短的时间内找到调整参数的合理值，这是有益的。然而，有一些模型在小范围搜索中的效率可能会抵消其他优化。不幸的是，Spark目前对超参数调整的实现没有提供任何随机搜索调整的技术。
- en: Tip
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In contrast, for example, a number of models in CARET utilize the *sub-model
    trick* where M tuning parameter combinations are evaluated; potentially far fewer
    than M model fits are required. This approach is best leveraged when a simple
    grid search is used. For this reason, it may be inefficient to use a random search.
    Finally, many of the models wrapped by train have a small number of parameters.
    The average number of parameters is 1.7\. To use a random search, another option
    is available in **trainControl** called search. Possible values of this argument
    are `grid` and `random`. The built-in models contained in CARET contain code to
    generate random tuning parameter combinations. The total number of unique combinations
    is specified by the **tuneLength** option to train.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，例如，CARET中的许多模型利用*子模型技巧*，其中评估了M个调整参数组合；可能远少于M个模型拟合所需的数量。当使用简单的网格搜索时，这种方法最有效。因此，使用随机搜索可能效率低下。最后，训练包装的许多模型具有少量参数。平均参数数量为1.7。要使用随机搜索，**trainControl**中还有一个选项叫做search。该参数的可能值是`grid`和`random`。CARET中包含的内置模型包含生成随机调整参数组合的代码。唯一组合的总数由train的**tuneLength**选项指定。
- en: Cross-validation
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Cross-validation (also called the **Rotation Estimation** (**RE**)) is a model
    validation technique for assessing the quality of the statistical analysis and
    results. The target is to make the model generalize towards an independent test
    set.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证（也称为**旋转估计**（**RE**））是一种评估统计分析和结果质量的模型验证技术。目标是使模型向独立测试集泛化。
- en: One of the perfect uses of the cross-validation technique is making a prediction
    from a machine learning model. Technically, it will help if you want to estimate
    how a predictive model will perform accurately in practice when you deploy it
    as an ML application.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证技术的一个完美用途是从机器学习模型中进行预测。从技术上讲，如果您想要估计预测模型在实践中的准确性，当您将其部署为ML应用程序时，它将会有所帮助。
- en: During the cross-validation process, a model is usually trained with a dataset
    of a known type. Conversely, it is tested using a dataset of unknown type. In
    this regard, cross-validation help describes a dataset to test the model in the
    training phase using the validation set.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证过程中，模型通常使用已知类型的数据集进行训练。相反，它使用未知类型的数据集进行测试。在这方面，交叉验证有助于使用验证集在训练阶段描述数据集以测试模型。
- en: However, in order to minimize the flaws in the machine learning model such as
    overfitting and underfitting, the cross-validation technique provides insights
    into how the model will generalize to an independent set.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了最小化机器学习模型中的缺陷，如过拟合和欠拟合，交叉验证技术提供了关于模型如何泛化到独立集的见解。
- en: 'There are two types of cross-validation that can be typed as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的交叉验证，可以如下分类：
- en: '**Exhaustive cross-validation**: This includes leave-p-out cross-validation
    and leave-one-out cross-validation'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**穷尽交叉验证**：包括留p法交叉验证和留一法交叉验证'
- en: '**Non-exhaustive cross-validation**: This includes the K-fold cross-validation
    and repeated random sub-sampling validation cross-validation'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非穷尽交叉验证**：包括K折交叉验证和重复随机子采样验证交叉验证'
- en: Detailed discussion of these will not be conducted in this book due to page
    limitation. Moreover, using Spark ML and Spark MLlib, readers will be able to
    perform the cross-validation following our examples in the next section.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于页面限制，本书不会对这些进行详细讨论。此外，使用Spark ML和Spark MLlib，读者将能够在下一节中按照我们的示例执行交叉验证。
- en: Except for the time series data, in most of the cases, the researcher/data scientist/data
    engineer uses 10-fold cross-validation instead of testing on a validation set
    (where K = 10). This is the most widely used cross-validation technique across
    the use cases and problem type. Moreover, to reduce the variability, multiple
    iterations of cross-validation are performed using different partitions; finally,
    the validation results are averaged over the rounds across.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除了时间序列数据外，在大多数情况下，研究人员/数据科学家/数据工程师使用10折交叉验证，而不是在验证集上进行测试（其中K = 10）。这是最广泛应用的交叉验证技术，适用于各种用例和问题类型。此外，为了减少变异性，使用不同分区进行多次交叉验证迭代；最后，对多轮验证结果进行平均。
- en: 'Using cross-validation instead of conventional validation has two main advantages
    outlined as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉验证而不是传统验证有两个主要优点，如下所述：
- en: Firstly, if there is not enough data available to partition across the separate
    training and test sets, there's the chance of losing significant modelling or
    testing capability.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，如果没有足够的数据可用于在单独的训练和测试集之间进行分区，就有可能失去重要的建模或测试能力。
- en: Secondly, the K-fold cross-validation Estimator has a lower variance than a
    single hold-out set Estimator. This low variance limits the variability and is
    again very important if the amount of available data is limited.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，K折交叉验证估计器的方差低于单个留出集估计器。这种低方差限制了变异性，如果可用数据量有限，则这一点非常重要。
- en: In these circumstances, a fair way to properly estimate the model prediction
    and related performance is to use cross-validation as a powerful general technique
    for model selection and validation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，正确估计模型预测和相关性能的公平方法是使用交叉验证作为一种强大的通用技术进行模型选择和验证。
- en: 'A more technical example will be shown in the *Machine learning model selection*
    section. Let''s draw a concrete example to illustrate this. Suppose, we need to
    perform manual features and a parameter selection for the model tuning and, after
    that, perform a model evaluation with a 10-fold cross-validation on the entire
    dataset. What would be the best strategy? We would suggest you go for the strategy
    that provides an optimistic score as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器学习模型选择*部分将展示一个更加技术性的例子。让我们举一个具体的例子来说明这一点。假设我们需要对模型调整进行手动特征和参数选择，之后，在整个数据集上进行10折交叉验证的模型评估。什么才是最佳策略？我们建议您选择提供乐观分数的策略如下：'
- en: Divide the dataset into training, say 80%, and testing 20% or whatever you chose
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分为训练集（80%）和测试集（20%）或其他比例
- en: Use the K-fold cross-validation on the training set to tune your model
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K折交叉验证来调整你的模型
- en: Repeat the CV until you find your model optimized and therefore tuned
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复交叉验证，直到找到优化的模型，因此调整。
- en: Now use your model to predict on the testing set to get an estimate of out-of-model
    errors
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在使用你的模型在测试集上进行预测，以获得模型外误差的估计
- en: Hypothesis testing
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假设检验
- en: Hypothesis testing is a statistical tool used to determine whether a result
    is statistically significant. Additionally, it can also be used to justify whether
    the result you received occurred by chance or whether it is an actual result.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 假设检验是用于确定结果是否具有统计学意义的统计工具。此外，它还可以用来证明你所得到的结果是偶然发生的还是实际结果。
- en: 'In this regard, moreover, according to Oracle developers at [https://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/method.htm](https://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/method.htm),
    a certain workflow would provide better performance tuning. The typical steps
    they have suggested are as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，此外，根据Oracle开发人员在[https://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/method.htm](https://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/method.htm)上的说法，某种工作流程将提供更好的性能调整。他们建议的典型步骤如下：
- en: Set clear goals for tuning
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为调整设定明确的目标
- en: Create minimum repeatable tests
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建最小可重复测试
- en: Test the hypothesis
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试假设
- en: Keep all records
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留所有记录
- en: Avoid common errors
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免常见错误
- en: Stop tuning when the objectives are achieved
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当达到目标时停止调整
- en: Usually, the observed value tobs of the test statistic T is first computed.
    After that, the probability, also called p-value, is calculated under the null
    hypothesis. Finally, if and only if the p-value is less than the significance
    level (the selected probability) threshold, the null hypothesis is rejected in
    favor of the alternative hypothesis.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，首先计算测试统计量T的观察值tobs。之后，在零假设下计算概率，也称为p值。最后，只有当p值小于显著性水平（所选概率）阈值时，才会拒绝零假设，支持备择假设。
- en: 'To find out more, refer to the following publication: *R.A. Fisher et al.,
    Statistical Tables for Biological Agricultural and Medical Research, 6th ed.,
    Table IV, Oliver & Boyd, Ltd., Edinburgh*.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息，请参考以下出版物：*R.A. Fisher等人，《生物农业和医学研究的统计表》，第6版，表IV，Oliver & Boyd，Ltd.，爱丁堡*。
- en: 'Here are two rules of thumb (although these may vary for your case depending
    on data quality and types):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是两个经验法则（尽管这些可能因数据质量和类型而有所不同）：
- en: If the p-value is p > 0.05, accept your hypothesis. Note that if a deviation
    is small enough that chance could be the acceptance level. A p-value of 0.6, for
    example, means that there is a 60% probability of any deviation from the expected
    result. However, this is within the range of an acceptable deviation.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果p值为p > 0.05，则接受你的假设。请注意，如果偏差足够小，可能是由于机会而接受水平。例如，0.6的p值意味着有60%的概率出现任何偏离预期结果的情况。然而，这在可接受的偏差范围内。
- en: If the p-value is p < 0.05, reject your hypothesis by concluding that some factors
    other than chance are operating for the deviation to be perfect. Similarly, a
    p-value of 0.01 means that there is only a 1% chance that this deviation is due
    to chance alone, which means that other factors must be involved, and these need
    to be addressed.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果p值为p < 0.05，则拒绝你的假设，得出结论是除了机会之外还有其他因素在起作用，使得偏差完美。同样，0.01的p值意味着只有1%的机会是由于机会而产生的偏差，这意味着其他因素必须参与，并且这些因素需要解决。
- en: However, these two rules might not be applicable in every hypothesis testing.
    In the next subsection, we will show an example of hypothesis testing using Spark
    MLlib.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这两条规则可能并不适用于每个假设检验。在下一小节中，我们将展示使用Spark MLlib进行假设检验的示例。
- en: Hypothesis testing using ChiSqTestResult of Spark MLlib
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark MLlib的ChiSqTestResult进行假设检验
- en: 'According to the API documentation provided by Apache at [http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing](http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing),
    the current implementation of the Spark MLlib supports Pearson''s chi-squared
    (*χ2*) tests for goodness of fit and independence:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Apache提供的API文档[http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing](http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing)，Spark
    MLlib的当前实现支持Pearson卡方（*χ2*）拟合度和独立性测试：
- en: The goodness of the fit, or if the independence test is being conducted, is
    determined by the input data types
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拟合度的好坏，或者是否正在进行独立性检验，由输入数据类型决定
- en: The goodness of the fit test requires an input of type vector (mostly dense
    vectors although it works for sparse vectors). On the other hand, the independence
    test requires a Matrix as an input format
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拟合度检验需要一个向量类型的输入（主要是密集向量，尽管对于稀疏向量也适用）。另一方面，独立性检验需要一个矩阵作为输入格式。
- en: In addition to these, Spark MLlib also supports input type RDD [LabeledPoint]
    to enable feature selection via chi-square independence tests, especially for
    the SVM or regression based test where the statistical class provides the necessary
    methods to run Pearson's chi-squared tests.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，Spark MLlib还支持输入类型RDD [LabeledPoint]，以通过卡方独立性检验实现特征选择，特别是对于SVM或基于回归的测试，其中统计类提供了运行Pearson卡方检验所需的方法。
- en: Additionally, Spark MLlib provides a 1-sample, 2-sided implementation of the
    **Kolmogorov-Smirnov** (**KS**) test for equality of probability distributions.
    Spark MLlib provides online implementations of some tests to support use cases
    like A/B testing. These tests may be performed on a Spark Streaming DStream [(Boolean,
    Double)] where the first element of each tuple indicates the control group (false)
    or treatment group (true) and the second element is the value of an observation.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark MLlib提供了**Kolmogorov-Smirnov**（**KS**）概率分布相等的1样本、2边实现的检验。Spark MLlib提供了一些在线实现的测试，以支持A/B测试等用例。这些测试可以在Spark
    Streaming DStream上执行，其中每个元组的第一个元素表示对照组（false）或处理组（true），第二个元素是观察值的值。
- en: 'However, due to brevity and page limitation, this two testing technique will
    not be discussed. The following example demonstrates how to run and interpret
    hypothesis tests through the `ChiSqTestResult`. In the example, we will show three
    tests: goodness of fit of the result on the dense vector created from the breast
    cancer diagnosis dataset, an independence test for a randomly created matrix and
    finally, an independence test on a contingency table from the cancer dataset itself.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于简洁和页面限制，这两种测试技术将不会被讨论。以下示例演示了如何通过`ChiSqTestResult`运行和解释假设检验。在示例中，我们将展示三个测试：对从乳腺癌诊断数据集创建的密集向量的拟合度检验，对随机创建的矩阵的独立性检验，以及对癌症数据集本身的列联表进行独立性检验。
- en: '**Step 1: Load required packages**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：加载所需的包**'
- en: 'Here is the code to load the required packages:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载所需包的代码：
- en: '[PRE0]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2: Create a Spark session**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：创建一个Spark会话**'
- en: 'The following code helps us to create a Spark Session:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码帮助我们创建一个Spark会话：
- en: '[PRE1]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The implementation of the `UtilityForSparkSession` class is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`UtilityForSparkSession`类的实现如下：'
- en: '[PRE2]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Step 3: Perform the goodness of fit test**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：执行拟合度检验**'
- en: 'First we need to prepare a dense vector from a categorical dataset such as
    the Wisconsin Breast Cancer Diagnosis dataset. As we have already provided many
    examples on this dataset, we will not discuss the data exploration anymore in
    this section. The following line of code collects the vectors that we created
    using the `myVector()` method:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从类别数据集（如威斯康星州乳腺癌诊断数据集）准备一个密集向量。由于我们已经在这个数据集上提供了许多示例，所以在本节中我们将不再讨论数据探索。以下代码行收集了我们使用`myVector()`方法创建的向量：
- en: '[PRE3]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Where, the implementation of the `myVector()` method goes as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`myVector()`方法的实现如下：'
- en: '[PRE4]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now let''s compute the goodness of the fit. Note, if a second vector to test
    is not supplied as a parameter, the test runs occur against a uniform distribution
    automatically:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算拟合度。请注意，如果没有提供第二个要测试的向量作为参数，测试将自动针对均匀分布进行：
- en: '[PRE5]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now let''s print the result of the goodness using the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打印拟合度的结果：
- en: '[PRE6]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note the summary of the test includes the p-value, degrees of freedom, test
    statistic, the method used, and the null hypothesis. We got the following output:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，测试的摘要包括p值、自由度、检验统计量、使用的方法和零假设。我们得到了以下输出：
- en: '[PRE7]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There is a very strong presumption against a null hypothesis: the observed
    follows the same distribution as expected.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于零假设：观察结果遵循与预期相同的分布，存在非常强烈的假设。
- en: Since the p-value is low enough to be insignificant, consequently we cannot
    accept the hypothesis based on the data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于p值低到足够不显著，因此我们不能根据数据接受假设。
- en: '**Step 4: An independence test on the contingency matrix**'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：对列联表进行独立性检验**'
- en: 'First let''s create a contingency 4x3 matrix randomly. Here, the matrix appears
    as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们随机创建一个4x3的列联表。在这里，矩阵如下所示：
- en: '[PRE8]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now let''s conduct Pearson''s independence test on the input contingency matrix:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对输入的列联表进行Pearson独立性检验：
- en: '[PRE9]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now let''s evaluate the test result and give the summary of the test including
    the p-value and the degrees of freedom:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们评估测试结果，并给出包括p值和自由度在内的测试摘要：
- en: '[PRE10]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We got the following statistic summarized as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下统计摘要：
- en: '[PRE11]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Step 5: An independence test on the contingency table**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：对列联表进行独立性检验**'
- en: 'First, let''s create a contingency table by means of RDDs from the cancer dataset
    as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过RDDs从癌症数据集创建一个列联表如下：
- en: '[PRE12]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We have constructed a contingency table from the raw (feature, label) pairs
    and used it to conduct the independence test. Now let''s conduct the test as `ChiSquaredTestResult`
    for every feature against the label:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从原始（特征，标签）对构建了一个列联表，并用它进行了独立性测试。现在让我们对每个特征针对标签进行`ChiSquaredTestResult`测试：
- en: '[PRE13]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let''s observe the test result against each column (that is, for each 30
    feature point) using the following code segment:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用以下代码段观察每列（即每30个特征点）的测试结果：
- en: '[PRE14]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: From this result, we can see that for some feature points (that is, column)
    we have a large p-value compared to the others. Readers are, therefore, advised
    to select the proper dataset and do the hypothesis test prior to applying the
    hyperparameter tuning. There is no concrete example in this regard since the result
    may vary against the datasets you have.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个结果，我们可以看到对于一些特征点（即列），我们有一个与其他特征点相比较大的p值。因此，建议读者在应用超参数调整之前选择适当的数据集并进行假设检验。在这方面没有具体的例子，因为结果可能会根据您拥有的数据集而有所不同。
- en: Hypothesis testing using the Kolmogorov–Smirnov test from Spark MLlib
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark MLlib的Kolmogorov-Smirnov测试进行假设检验
- en: Since Spark release 1.1.0, Spark also provides the facility of doing the hypothesis
    testing using for the real-time streaming data through the Kolmogorov-Smirnov
    test. Where, the probability of obtaining a test statistic result (at least as
    extreme as the one) that was actually observed. It actually assumes that the null
    hypothesis is always true.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 自Spark 1.1.0发布以来，Spark还提供了使用Kolmogorov-Smirnov测试进行实时流数据的假设检验的功能。在这里，获得测试统计结果（至少与实际观察到的结果一样极端）的概率。它实际上假设零假设始终为真。
- en: Tip
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'For more details, interested readers should refer to the Java class (`JavaHypothesisTestingKolmogorovSmirnovTestExample.java`)
    in the Spark distribution under the following directory: `spark-2.0.0-bin-hadoop2.7\examples\src\main\java\org\apache\spark\examples\mllib`.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，感兴趣的读者应参考Spark分发中的Java类（`JavaHypothesisTestingKolmogorovSmirnovTestExample.java`）在以下目录下：`spark-2.0.0-bin-hadoop2.7\examples\src\main\java\org\apache\spark\examples\mllib`。
- en: Streaming significance testing of Spark MLlib
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark MLlib的流显著性测试
- en: Other than the Kolmogorov-Smirnov test, Spark also supports Streaming significance
    testing, which is an online implementation of the hypothesis testing like the
    A/B testing? These tests can be performed on a Spark streaming using DStream (more
    technical discussion on this topic will be carried out in [Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data"), *Advanced
    Machine Learning with Streaming and Graph Data*).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Kolmogorov-Smirnov测试之外，Spark还支持流显著性测试，这是假设测试的在线实现，类似于A/B测试？这些测试可以在Spark流中使用DStream进行（关于这个主题的更多技术讨论将在[第9章](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "第9章。使用流和图数据进行高级机器学习")中进行，*使用流和图数据进行高级机器学习*）。
- en: 'The MLlib based streaming significance testing supports the following two parameters:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 基于MLlib的流显著性测试支持以下两个参数：
- en: '**peacePeriod**: This is the number of initial data points from the stream
    to ignore. This is actually used to mitigate novelty effects and the quality of
    the streaming you will be receiving.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**peacePeriod**：这是要忽略的来自流的初始数据点的数量。这实际上用于减轻新颖性效应和您将收到的流的质量。'
- en: '**windowSize**: This is the number of past batches over which to perform hypothesis
    testing. If you set its value to 0, it will perform cumulative processing using
    all the prior batches received and processed.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**windowSize**：这是进行假设检验的过去批次数量。如果将其值设置为0，它将使用所有先前接收和处理的批次进行累积处理。'
- en: Interested readers should refer to the Spark API documentation at [http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing](http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的读者应参考Spark API文档[http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing](http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing)。
- en: Machine learning model selection
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型选择
- en: Most machine learning algorithms are dependent on various parameters. When we
    train a model, we need to provide values for those parameters. The efficacy of
    the trained model is dependent on the model parameters that we choose. The process
    of finding out the optimal set of parameters is known as model selection.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法依赖于各种参数。当我们训练模型时，我们需要为这些参数提供值。训练模型的有效性取决于我们选择的模型参数。找到最佳参数集的过程称为模型选择。
- en: Model selection via the cross-validation technique
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过交叉验证技术进行模型选择
- en: When performing machine learning using Python's scikit-learn library or R, you
    can often get a reasonably predictive performance by using out-of-the-box settings
    for your models. However, the payoff can be huge if you invest some time in tuning
    models to your specific problem and data set.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Python的scikit-learn库或R进行机器学习时，通常可以通过使用模型的开箱即用设置获得相当好的预测性能。然而，如果您花一些时间调整模型以适应您的特定问题和数据集，回报可能会很大。
- en: However, we also need to consider other issues like overfitting, cross-validation,
    and bias-variance trade-off. These ideas are central to doing a good job at optimizing
    the hyperparameters of algorithms.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还需要考虑其他问题，如过度拟合、交叉验证和偏差-方差权衡。这些想法对于优化算法的超参数至关重要。
- en: 'In this section, we will explore the concepts behind Hyperparameter optimization
    and demonstrate the process of tuning and training a logistic regression classifier
    for the famous Spam Filtering dataset. The goal is to tune and apply a logistic
    regression to these features in order to predict whether a given email/SMS is
    spam or not-spam:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨超参数优化的概念，并演示调整和训练逻辑回归分类器以用于著名的垃圾邮件过滤数据集。目标是调整并应用逻辑回归到这些特征，以预测给定的电子邮件/短信是否为垃圾邮件或非垃圾邮件：
- en: '![Model selection via the cross-validation technique](img/00057.jpeg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![通过交叉验证技术进行模型选择](img/00057.jpeg)'
- en: 'Figure 4: Model selection via cross-validation'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：通过交叉验证进行模型选择
- en: Cross-validation and Spark
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉验证和Spark
- en: Pipelines enable model selection by tuning an entire Pipeline at once, rather
    than tuning each element in the Pipeline unconnectedly. See the API documentation
    at [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 管道通过一次调整整个管道来实现模型选择，而不是分别调整管道中的每个元素。请参阅[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)中的API文档。
- en: The current implementation of Spark ML supports model selection using the `CrossValidator`
    class. It takes an Estimator, a set of `ParamMaps`, and an Evaluator. The model
    selection task begins with splitting the dataset (that is, splitting it into a
    set of folds) the folds of which are then used as separate training and test datasets.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的Spark ML实现支持使用`CrossValidator`类进行模型选择。它接受一个Estimator、一组`ParamMaps`和一个Evaluator。模型选择任务始于拆分数据集（即将其拆分为一组折叠），这些折叠随后被用作单独的训练和测试数据集。
- en: 'For example, with K=10 folds, the `CrossValidator` will generate 10 (training,
    test) dataset pairs. Each of these uses two thirds (2/3) of the data for the training,
    and the other third (1/3) for the testing. After that, the `CrossValidator` iterates
    through the set of `ParamMaps`. For each `ParamMap`, it trains the given Estimator
    and evaluates it using the available Evaluator. The Evaluator can be a related
    ML task for example:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于K=10个折叠，`CrossValidator`将生成10个（训练，测试）数据集对。其中每个使用数据的三分之二（2/3）进行训练，另外三分之一（1/3）进行测试。之后，`CrossValidator`遍历`ParamMaps`集合。对于每个`ParamMap`，它训练给定的Estimator并使用可用的Evaluator进行评估。Evaluator可以是相关的ML任务，例如：
- en: '`RegressionEvaluator` for regression related problems'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RegressionEvaluator` 用于回归相关问题'
- en: '`BinaryClassificationEvaluator` for binary data and its related problems'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BinaryClassificationEvaluator` 用于二进制数据及其相关问题'
- en: '`MultiClassClassificationEvaluator` for a multiclass problem'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MultiClassClassificationEvaluator` 用于多类问题'
- en: 'When it comes to the best `ParamMap` selection, a default metric is used. Note
    that the `ParamMap` can be also be overridden by the `setMetric()` method in each
    of these evaluators. In contrast, when it comes to the best model selection:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择最佳的`ParamMap`时，使用默认指标。请注意，`ParamMap`也可以被这些评估器中的`setMetric()`方法覆盖。相反，当涉及到最佳模型选择时：
- en: The `ParamMap` produces the best evaluation metric
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ParamMap`产生最佳的评估指标'
- en: The evaluations metrics are then averaged over the K folds
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些评估指标然后在K个折叠上进行平均
- en: Finally, the best model is selected
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，选择最佳模型
- en: Once the best `ParamMap` and model are selected, the `CrossValidator` fits the
    Estimator using them for the entire dataset.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了最佳的`ParamMap`和模型，`CrossValidator`将使用它们来拟合整个数据集的Estimator。
- en: To get a clearer insight into the `CrossValidator` and to select from a grid
    of parameters, Spark uses the `ParamGridBuilder` utility to construct the parameter
    grid. For example, suppose the parameter grid has a value of 4 as the `hashingTF.numFeatures`
    and a value of 3 for the LR.`regParam`. Also, lets say that the `CrossValidator`
    uses 10 folds.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地了解`CrossValidator`并从参数网格中进行选择，Spark使用`ParamGridBuilder`实用程序来构建参数网格。例如，假设参数网格中`hashingTF.numFeatures`的值为4，LR.`regParam`的值为3。还假设`CrossValidator`使用10个折叠。
- en: Once, these values are multiplying results to 120 (that is, 4*3 *10 = 120),
    this signifies that a significant amount of different models (that is, 120) are
    being trained. Therefore, using the `CrossValidator` sometimes can be very expensive.
    Nevertheless, it is also a well-established method for choosing associated performance
    and hyperparameters that are sounder statistically compared to the heuristic-based
    hand tuning.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些值相乘的结果为120（即4*3*10=120），这意味着有大量不同的模型（即120个）正在被训练。因此，使用`CrossValidator`有时可能非常昂贵。然而，与基于启发式手工调整相比，它也是一种选择相关性能和超参数的统计上更可靠的方法。
- en: Tip
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Interested readers may refer to the following three books for more insight:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者可以参考以下三本书以获得更多见解：
- en: Evan R. Sparks et al., *Automating Model Search for Large-Scale Machine Learning*,
    ACM, 978-1-4503-3651-2/15/08, [http://dx.doi.org/10.1145/2806777.2806945](http://dx.doi.org/10.1145/2806777.2806945).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Evan R. Sparks等人，《大规模机器学习的模型搜索自动化》，ACM，978-1-4503-3651-2/15/08，[http://dx.doi.org/10.1145/2806777.2806945](http://dx.doi.org/10.1145/2806777.2806945)。
- en: Cawley, G. C. & Talbot, N. L on over-fitting in model selection and subsequent
    selection bias in performance evaluation, *The Journal of Machine Learning Research*,
    JMLR. org, 2010, 11, 2079-2107.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Cawley, G. C. & Talbot, N. L关于模型选择中的过度拟合和随后的选择偏差在性能评估中的影响，《机器学习研究杂志》，JMLR.org，2010年，11，2079-2107。
- en: 'N. Japkowicz and M. Shah, *Evaluating learning algorithms: a classification
    perspective*, Cambridge University Press, 2011.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: N. Japkowicz和M. Shah，《评估学习算法：分类视角》，剑桥大学出版社，2011年。
- en: In the next sub-section, we will show how to perform cross-validation on a dataset
    for model selection using Spark ML API.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个小节中，我们将展示如何使用Spark ML API对数据集进行交叉验证以进行模型选择。
- en: Cross-validation using Spark ML for SPAM filtering a dataset
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark ML进行垃圾邮件过滤数据集的交叉验证
- en: In this sub-section, we will show you how to perform cross-validation on the
    e-mail spam dataset for model selection. We will use logistic regression in the
    first place then we will move forward for other models. Finally, we will recommend
    the most suitable model for e-mail spam classification.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小节中，我们将向您展示如何对电子邮件垃圾邮件数据集进行交叉验证以进行模型选择。我们将首先使用逻辑回归，然后我们将继续使用其他模型。最后，我们将推荐最适合电子邮件垃圾邮件分类的模型。
- en: '**Step 1: Import necessary packages/libraries/APIs**'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步：导入必要的包/库/API
- en: 'Here is the code to import necessary packages/libraries/APIs:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是导入必要的包/库/API的代码：
- en: '[PRE15]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Step 2: Initialize the necessary Spark environment**'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步：初始化必要的Spark环境
- en: 'The following code helps us to initialize the necessary Spark environment:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码帮助我们初始化必要的Spark环境：
- en: '[PRE16]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here we set the application name as cross-validation, the master URL as `local[*]`,
    and the Spark session as the entry point of the program. Please set these parameters
    accordingly. Most importantly, set the warehouse directory as `E:/Exp/` and replace
    it with the appropriate path.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将应用程序名称设置为交叉验证，主URL设置为`local[*]`，Spark会话设置为程序的入口点。请相应地设置这些参数。最重要的是，将仓库目录设置为`E:/Exp/`，并将其替换为适当的路径。
- en: '**Step 3: Prepare a Dataset from the SMS spam dataset**'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：从SMS垃圾短信数据集准备数据集**'
- en: 'Take the e-mail spam data as your input, prepare a dataset out of the data,
    using it as raw text, and check if the data was read properly by calling the `show()`
    method:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以电子邮件垃圾邮件数据作为输入，从数据中准备一个数据集，将其用作原始文本，并通过调用`show()`方法检查数据是否被正确读取：
- en: '[PRE17]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Cross-validation using Spark ML for SPAM filtering a dataset](img/00046.jpeg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML进行交叉验证进行垃圾邮件过滤数据集](img/00046.jpeg)'
- en: 'Figure 5: The top 20 rows'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：前20行
- en: To learn more about the data, please refer to the section *Pipeline - an example
    with Spark ML*, in [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Knowledge
    through Feature Engineering*, and the description for the dataset exploration.
    As you can see in *Figure 5*, The top 20 rows, there are only two labels - spam
    or ham (that is, it is a binary classification problem) associated along with
    the text values (that is, each row).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于数据的信息，请参考*Pipeline - an example with Spark ML*部分，在[第4章](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "第4章。通过特征工程提取知识")中，*通过特征工程提取知识*，以及数据集探索的描述。正如您在*图5*中所看到的，前20行，只有两个标签 - 垃圾邮件或正常邮件（即，这是一个二元分类问题），与文本值（即，每行）相关联。
- en: However, there is no numeric label or ID. Therefore, we need to prepare the
    dataset (training set) such that the data frame also contains the ID, and labels
    along with text (that is, value) so that we can prepare a test set and predict
    the corresponding labels using any classification algorithm (that is, logistic
    regression) and can decide if our model selection is appropriate.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，没有数字标签或ID。因此，我们需要准备数据集（训练集），使数据框还包含ID和标签以及文本（即值），以便我们可以准备测试集，并使用任何分类算法（即逻辑回归）预测相应的标签，并决定我们的模型选择是否合适。
- en: However, for doing so we need to prepare the training dataset first. As you
    can see, the data frame shown above has only one column and, as mentioned previously,
    we do need to have three columns. If we prepare an RDD from the previous Dataset
    (that is, `df`), it would be easier for us to make that transformation.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了做到这一点，我们需要先准备训练数据集。如您所见，上面显示的数据框只有一列，并且如前所述，我们确实需要三列。如果我们从先前的数据集（即`df`）准备一个RDD，那么对我们来说将更容易进行转换。
- en: '**Step 4: Create Java pairs of RDDs to store the rows and indices**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：创建用于存储行和索引的Java RDD对**'
- en: 'Create a Java pair of RDDs by converting (transforming) the DataFrame (that
    is, `df`) to Java RDD and by zipping the index:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将DataFrame（即`df`）转换为Java RDD，并通过压缩索引来创建Java RDD对：
- en: '[PRE18]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Step 5: Create LabeledDocument RDDs**'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：创建LabeledDocument RDDs**'
- en: 'Create `LabeledDocument` Java RDDs by splitting the dataset based on two labels
    and converting the text label to a numeric label (that is, `1.0` if ham otherwise
    0.0). Note that the `LabeledDocument` is a user-defined class that is discussed
    in *step 6*:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 通过基于两个标签拆分数据集并将文本标签转换为数字标签（即，如果是正常邮件则为`1.0`，否则为0.0）来创建`LabeledDocument` Java
    RDDs。请注意，`LabeledDocument`是一个在*步骤6*中讨论的用户定义类：
- en: '[PRE19]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Step 6: Prepare the training dataset**'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：准备训练数据集**'
- en: 'Prepare the training dataset from the `LabeledDocument` RDDs using the `createDataFrame()`
    method and by specifying the class. Finally, see the data frame structure using
    the `show()` method as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`createDataFrame()`方法从`LabeledDocument` RDDs准备训练数据集，并指定类。最后，使用`show()`方法查看数据框结构，如下所示：
- en: '[PRE20]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Cross-validation using Spark ML for SPAM filtering a dataset](img/00147.jpeg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML进行交叉验证进行垃圾邮件过滤数据集](img/00147.jpeg)'
- en: 'Figure 6: The newly created label and ID from the dataset; where ID is the
    number of row.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：从数据集中新创建的标签和ID；其中ID是行数。
- en: 'From *Figure 6*, *The newly created label and ID from the dataset; where ID
    is the number of rows*, we can see that the new training dataset has three columns:
    ID, test, and label. It can actually be done by adding a new ID against each row
    (that is, each line) of the original document. Let''s create a class, called `Document`
    for our purpose, that should set a unique ID against each line of text. The structure
    of the class can be something like the following:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图6*中，*从数据集中新创建的标签和ID；其中ID是行数*，我们可以看到新的训练数据集有三列：ID、测试和标签。实际上，可以通过在原始文档的每一行（即每一行）添加一个新的ID来完成。让我们为此创建一个名为`Document`的类，它应该为每行文本设置一个唯一的ID。类的结构可以是以下内容：
- en: '[PRE21]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now let''s look at the structure of the class constructor:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下类构造函数的结构：
- en: '[PRE22]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The setter and getter for the ID could be something like this:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ID的setter和getter可以是这样的：
- en: '[PRE23]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Similarly the setter and getter methods for the text could be something like
    this:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，文本的setter和getter方法可能是这样的：
- en: '[PRE24]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Therefore, if we summarize, the `Document` class could be something like this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们总结一下，`Document`类可能是这样的：
- en: '[PRE25]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The structure of the `LabeledDocument` class, on the other hand, can be as
    follows and that can be extended from the Document class (to be discussed later):'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`LabeledDocument`类的结构可能如下所示，并且可以从`Document`类扩展（稍后讨论）：
- en: 'Now let''s look at the structure of the class constructor:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下类构造函数的结构：
- en: '[PRE26]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'However, we''re not done yet since we will be extending the `Document` class
    we need to inherit the constructor from the `Document` class using the `super()`
    method as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还没有完成，因为我们将扩展`Document`类，我们需要使用`super()`方法从`Document`类继承构造函数，如下所示：
- en: '[PRE27]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now the setter method can be something like this:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在setter方法可以是这样的：
- en: '[PRE28]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'And of course the getter method for the label could be something like this:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，标签的getter方法可能是这样的：
- en: '[PRE29]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Therefore, in a nutshell, the `LabelDocument` class is as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，简而言之，`LabelDocument`类如下：
- en: '[PRE30]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**Step 7: Configure an ML pipeline**'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 第7步：配置ML管道
- en: 'Configure an ML pipeline, which consists of three stages: `tokenizer`, `hashingTF`,
    and `lr`:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 配置一个ML管道，包括三个阶段：`tokenizer`，`hashingTF`和`lr`：
- en: '[PRE31]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Step 8: Construct a grid of parameters to search over**'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 第8步：构建要搜索的参数网格
- en: 'Currently, Spark uses a `ParamGridBuilder` to construct a grid of parameters
    to search over. In this regard, suppose we have three values for `hashingTF.numFeatures`
    and two values for `lr.regParam`, this grid will have 3 x 2 = 6 parameter settings
    for the `CrossValidator` to choose from:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Spark使用`ParamGridBuilder`来构建要搜索的参数网格。在这方面，假设我们有`hashingTF.numFeatures`的三个值和`lr.regParam`的两个值，那么这个网格将有3
    x 2 = 6个参数设置供`CrossValidator`选择：
- en: '[PRE32]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We now treat the pipeline as an estimator, wrapping it in a `CrossValidator`
    instance. This will allow us to jointly choose parameters for all Pipeline stages.
    A `CrossValidator` requires an Estimator, a set of Estimator `ParamMaps`, and
    an Evaluator. Note that the evaluator here is a `BinaryClassificationEvaluator`
    and its default metric is `areaUnderROC`.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将管道视为一个估计器，并将其包装在`CrossValidator`实例中。这将允许我们共同为所有管道阶段选择参数。`CrossValidator`需要一个估计器，一组估计器`ParamMaps`和一个评估器。请注意，这里的评估器是`BinaryClassificationEvaluator`，其默认指标是`areaUnderROC`。
- en: '**Step 9: Create a CrossValidator instance**'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 第9步：创建一个CrossValidator实例
- en: 'Here is the code to create a `CrossValidator` instance:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建`CrossValidator`实例的代码：
- en: '[PRE33]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**Step 10: Run cross-validation**'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 第10步：运行交叉验证
- en: 'Run the cross-validation and choose the best set of parameters. Just use the
    following code segments:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 运行交叉验证并选择最佳参数集。只需使用以下代码段：
- en: '[PRE34]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now your `CrossValidator` model is ready to perform the prediction. However,
    before that, we need a test set or validation set. Now let''s prepare a sample
    test set. Just create a dataset using the following code segment:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的`CrossValidator`模型已准备好执行预测。但在此之前，我们需要一个测试集或验证集。现在让我们准备一个样本测试集。只需使用以下代码段创建数据集：
- en: '[PRE35]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now let''s see the structure of the test set by calling the `show()` method
    in *Figure 7*:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过调用*图7*中的`show()`方法来查看测试集的结构：
- en: '![Cross-validation using Spark ML for SPAM filtering a dataset](img/00072.jpeg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML进行交叉验证进行SPAM过滤数据集](img/00072.jpeg)'
- en: 'Figure 7: The test set'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：测试集
- en: '**Step 11: Create a dataset to collect the prediction parameters**'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 第11步：创建一个数据集来收集预测参数
- en: 'The following code illustrates how to create a dataset:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码说明了如何创建数据集：
- en: '[PRE36]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '**Step 12: Display the prediction parameters for each text in the test set**'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 第12步：显示测试集中每个文本的预测参数
- en: 'With the help of the following code, we can display the prediction parameters:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 借助以下代码，我们可以显示预测参数：
- en: '[PRE37]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![Cross-validation using Spark ML for SPAM filtering a dataset](img/00118.jpeg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark ML进行交叉验证进行SPAM过滤数据集](img/00118.jpeg)'
- en: 'Figure 8: Prediction against each text and ID'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：针对每个文本和ID的预测
- en: Therefore, if you compare the results shown in *Figure 6*, *The newly created
    label and ID from the dataset; where ID is the number of rows,* you'll find that
    the prediction accuracy increases from more sophisticated methods for measuring
    predictive accuracy that can be used to identify places where the error rate can
    be optimized depending on the costs of each type of error.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您比较*图6*中显示的结果，“从数据集中新创建的标签和ID；其中ID是行数”，您会发现预测准确性从更复杂的方法中增加，用于测量预测准确性的方法可以用于识别可以根据每种错误类型的成本来优化错误率的地方。
- en: Model selection via training validation split
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过训练验证拆分进行模型选择
- en: 'According to the API documentation provided by Spark at [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html),
    Spark also offers a `TrainValidationSplit` for hyperparameter tuning along with
    the `CrossValidator`. The idea of the `TrainValidationSplit` is it only evaluates
    each combination of the parameters compared to cross-validation that iterates
    to k times. It is, therefore, computationally less expensive and produces the
    result more quickly. The results, however, will not be as reliable as the `CrossValidator`.
    There is an exception: if the training dataset is sufficiently large then it can
    also produce reliable results.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Spark在[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)提供的API文档，Spark还为超参数调整提供了`TrainValidationSplit`，以及`CrossValidator`。`TrainValidationSplit`的想法是它只评估参数的每个组合，而不是像交叉验证那样迭代k次。因此，它在计算上更便宜，产生结果更快。然而，结果可能不像`CrossValidator`那样可靠。有一个例外：如果训练数据集足够大，那么它也可以产生可靠的结果。
- en: 'The theory behind the `TrainValidationSplit` is that it takes the following
    three as inputs:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`TrainValidationSplit`背后的理论是它将以下三个作为输入：'
- en: An Estimator
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个估计器
- en: A set of `ParamMap``s` provided in the `estimatorParamMaps` parameter
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`estimatorParamMaps`参数中提供的一组`ParamMap`
- en: An Evaluator
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个评估器
- en: Consequently, it begins the model selection by splitting the dataset into two
    parts using the `trainRatio` parameter. The `trainRatio` parameter, on the other
    hand, is used for separate training and test datasets.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它通过使用`trainRatio`参数将模型选择分为两部分开始。另一方面，`trainRatio`参数用于单独的训练和测试数据集。
- en: For example, with *trainRatio = 0.75* (the default value is also 0.75), the
    `TrainValidationSplit` algorithm generates a training and testing pair. In that
    case, 75% of the total data is used for training the model. Consequently, the
    rest of the 25% is used as the validation set.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于*trainRatio = 0.75*（默认值也是0.75），`TrainValidationSplit`算法生成一个训练和测试对。在这种情况下，总数据的75%用于训练模型。因此，其余的25%用作验证集。
- en: Similar to the `CrossValidator`, the `TrainValidationSplit` also iterates through
    a set of ParamMaps as mentioned earlier. For each combination of the parameters,
    it trains the given Estimator in each iteration.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 与`CrossValidator`类似，`TrainValidationSplit`也会遍历一组ParamMaps，如前所述。对于参数的每种组合，它在每次迭代中训练给定的估计器。
- en: Consequently, the model is evaluated using the given Evaluator. After that,
    the best model is selected as the best option, since the `ParamMap` produces the
    best evaluation metric and thereby eases the model selection. The `TrainValidationSplit`
    finally fits the Estimator using the best available `ParamMap` and for the entire
    dataset.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用给定的评估器评估模型。之后，最佳模型被选为最佳选项，因为`ParamMap`产生了最佳的评估指标，从而简化了模型选择。`TrainValidationSplit`最终使用最佳的`ParamMap`拟合估计器，并用于整个数据集。
- en: Linear regression–based model selection for an OCR dataset
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于线性回归的OCR数据集模型选择
- en: In this sub-section, we will show how to perform train validation split tuning
    for OCR data. The logistic regression will be used in the first place; then we
    will move forward for other models. Finally, we will recommend the most suitable
    parameters for the OCR data classification.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将展示如何为OCR数据执行训练验证分割调整。首先将使用逻辑回归，然后我们将继续使用其他模型。最后，我们将为OCR数据分类推荐最合适的参数。
- en: '**Step 1: Import necessary packages/libraries/APIs:**'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：导入必要的包/库/API：**'
- en: '[PRE38]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '**Step 2: Initialize necessary Spark environment**'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：初始化必要的Spark环境**'
- en: '[PRE39]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here we set the application name as `TrainValidationSplit`, the master URL as
    `local[*]`, and the Spark Context is the entry point of the program. Please set
    these parameters accordingly.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将应用名称设置为`TrainValidationSplit`，主URL设置为`local[*]`，Spark上下文是程序的入口点。请相应地设置这些参数。
- en: '**Step 3: Prepare the OCR data as a libsvm format**'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：将OCR数据准备为libsvm格式**'
- en: 'If you recall *Figure 19* in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines,* you will remember the data as follows in *Figure
    9*, *A snapshot of the original OCR dataset as a Data Frame*:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您回忆一下[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines")中的*图19*，*构建可扩展的机器学习管道*，您将记得数据如下所示*图9*，*原始OCR数据集的数据框快照*：
- en: '![Linear regression–based model selection for an OCR dataset](img/00071.jpeg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![基于线性回归的OCR数据集模型选择](img/00071.jpeg)'
- en: 'Figure 9: A snapshot of the original OCR dataset as a Data Frame'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：原始OCR数据集的数据框快照
- en: However, the current implementation of the `TrainValidationSplitModel` API only
    works on datasets that are already in `libsvm` format.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`TrainValidationSplitModel` API的当前实现仅适用于已经处于`libsvm`格式的数据集。
- en: Tip
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Interested readers should refer to the following research article for more
    in depth knowledge: Chih-Chung Chang and Chih-Jen Lin, *LIBSVM - A Library for
    Support Vector Machines*. ACM Transactions on Intelligent Systems and Technology,
    2:27:1--27:27, 2011\. The software is available at [http://www.csie.ntu.edu.tw/~cjlin/libsvm](http://www.csie.ntu.edu.tw/~cjlin/libsvm).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的读者应参考以下研究文章以获取更深入的知识：Chih-Chung Chang和Chih-Jen Lin，*LIBSVM - 支持向量机库*。ACM智能系统和技术交易，2:27:1--27:27，2011年。该软件可在[http://www.csie.ntu.edu.tw/~cjlin/libsvm](http://www.csie.ntu.edu.tw/~cjlin/libsvm)上获得。
- en: Therefore, we do need to convert the dataset from the current tab separated
    OCR data to a `libsvm` format.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们确实需要将数据集从当前的制表符分隔的OCR数据转换为`libsvm`格式。
- en: Tip
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Readers should use the dataset provided with Packt packages or can convert the
    CSV/CSV file to the corresponding `libsvm` format. Interested readers can refer
    to our public script provided on GitHub at [https://github.com/rezacsedu/CSVtoLibSVMConverterinR](https://github.com/rezacsedu/CSVtoLibSVMConverterinR) that
    directly converts a CSV file to `libsvm` format. Just properly show the input
    and output file path and run the script on your RStudio.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应该使用Packt软件包提供的数据集，或者可以将CSV/CSV文件转换为相应的`libsvm`格式。感兴趣的读者可以参考我们在GitHub上提供的公共脚本[https://github.com/rezacsedu/CSVtoLibSVMConverterinR](https://github.com/rezacsedu/CSVtoLibSVMConverterinR)，该脚本可以直接将CSV文件转换为`libsvm`格式。只需正确显示输入和输出文件路径，并在RStudio上运行脚本。
- en: '**Step 4: Prepare the OCR data set and also prepare the training and test set**'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：准备OCR数据集，并准备训练和测试集**'
- en: 'We are assuming that readers already have downloaded the data or have converted
    the OCR data using our GitHub script or using their own script. Now, take the
    OCR `libsvm` format data as input and prepare the Dataset out of the data as raw
    texts and check if the data was read properly by calling the `show()` method as
    follows:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设读者已经下载了数据或使用我们的GitHub脚本或使用自己的脚本转换了OCR数据。现在，将OCR `libsvm`格式数据作为输入，并准备数据集作为原始文本，并通过调用`show()`方法检查数据是否被正确读取，如下所示：
- en: '[PRE40]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![Linear regression–based model selection for an OCR dataset](img/00150.jpeg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![基于线性回归的OCR数据集模型选择](img/00150.jpeg)'
- en: 'Figure 10: The top 20 rows'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：前20行
- en: '[PRE41]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: To learn more about the data, please refer to the section *Pipeline - An Example
    with Spark ML*, in [Chapter 4](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering"), *Extracting Knowledge
    through Feature Engineering*, and the description of dataset exploration. As you
    can see in *Figure 2*, *Spark ML pipeline model using logistic regression estimator
    (dash lines only happen during pipeline fitting)*, there are only two labels (spam
    or ham) associated along with the text values (that is, each row). However, there
    is no numeric label or ID.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于数据的信息，请参考章节*Pipeline - An Example with Spark ML*，在[第4章](part0038_split_000.html#147LC2-5afe140a04e845e0842b44be7971e11a
    "Chapter 4. Extracting Knowledge through Feature Engineering")中，*通过特征工程提取知识*，以及数据集探索的描述。正如您在*图2*中所看到的，*使用逻辑回归估计器的Spark
    ML管道模型（虚线仅在管道拟合期间发生）*，只有两个标签（垃圾邮件或正常邮件）与文本值（即每行）相关联。然而，没有数字标签或ID。
- en: Therefore, we need to prepare the dataset (training set) so that the Dataset
    also contains the ID, and labels along with text (that is, value) so that we can
    prepare a test set and predict their corresponding labels for any classification
    algorithm (that is, logistic regression) and can decide if our model selection
    is appropriate.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要准备数据集（训练集），使得数据集还包含ID和标签以及文本（即值），以便我们可以准备一个测试集，并对任何分类算法（即逻辑回归）预测其相应的标签，并决定我们的模型选择是否合适。
- en: However, in order to do so we first need to prepare the training dataset. As
    you can see, the data frame shown above has only one column and as previously
    mentioned we do need to have three columns. If we prepare an RDD from the above
    Dataset (that is, `df`) it would be easier for us to make that transformation.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为了做到这一点，我们首先需要准备训练数据集。正如您所看到的，上面显示的数据框只有一列，正如之前提到的，我们确实需要三列。如果我们从上述数据集（即`df`）准备一个RDD，那么我们更容易进行转换。
- en: '**Step 5: Configure an ML pipeline using linear regression**'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：使用线性回归配置ML管道**'
- en: '[PRE42]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '**Step 6: Construct a grid of parameters to search over**'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：构建要搜索的参数网格**'
- en: 'Currently, Spark uses a `ParamGridBuilder` to construct a grid of parameters
    to search over. In this regard, then, with three values for `hashingTF.numFeatures`
    and two values for `lr.regParam`, this grid will have 3 x 2 = 6 parameter settings
    for the `CrossValidator` to choose from:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Spark使用`ParamGridBuilder`来构建要搜索的参数网格。在这方面，对于`hashingTF.numFeatures`有三个值，对于`lr.regParam`有两个值，因此这个网格将有3
    x 2 = 6个参数设置供`CrossValidator`选择：
- en: '[PRE43]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We now treat the pipeline as an Estimator, wrapping it in a `CrossValidator`
    instance. This will allow us to jointly choose parameters for all pipeline stages.
    As already discussed, a `CrossValidator` requires an Estimator, a set of Estimator
    `ParamMaps`, and an Evaluator.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将管道视为一个估计器，并将其包装在`CrossValidator`实例中。这将允许我们共同选择所有管道阶段的参数。正如已经讨论的，`CrossValidator`需要一个估计器，一组估计器`ParamMaps`和一个评估器。
- en: Note
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the evaluator here is a `BinaryClassificationEvaluator` and its default
    metric is `areaUnderROC`.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里的评估器是`BinaryClassificationEvaluator`，其默认指标是`areaUnderROC`。
- en: '**Step 7: Create a TrainValidationSplit instance:**'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：创建TrainValidationSplit实例：**'
- en: '[PRE44]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In this case the estimator is simply the linear regression that we created in
    *Step 4*. A `TrainValidationSplit` requires an Estimator, a set of Estimator `ParamMaps`,
    and an Evaluator. In this case, 70% of the data will be used as training and the
    remaining 30% for the validation.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，估计器只是我们在*步骤4*中创建的线性回归。`TrainValidationSplit`需要一个估计器，一组估计器`ParamMaps`和一个评估器。在这种情况下，70%的数据将用于训练，剩下的30%用于验证。
- en: '**Step 8: Run TrainValidationSplit, and chooses parameters**'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：运行TrainValidationSplit，并选择参数**'
- en: 'Run the `TrainValidationSplit` and choose the best set of parameters for your
    problem using the training set. Just use the following code segment:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`TrainValidationSplit`并使用训练集选择最佳参数集。只需使用以下代码段：
- en: '[PRE45]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Step 9: Making a prediction on the test set**'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9：对测试集进行预测**'
- en: 'Make predictions on the test data where model is the model with the combination
    of parameters that performed best. Finally, to show the predictions, use the following
    code segment:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 对测试数据进行预测，其中模型是表现最佳的参数组合的模型。最后，要显示预测，请使用以下代码段：
- en: '[PRE46]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![Linear regression–based model selection for an OCR dataset](img/00036.jpeg)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![基于OCR数据集的线性回归模型选择](img/00036.jpeg)'
- en: 'Figure 11: Prediction against each feature and label'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：对每个特征和标签的预测
- en: In *Figure 11*, we showed the row prediction against the actual label. The first
    column is the actual label, the second column signifies the feature vector, and
    the third column shows the raw prediction based on the feature vectors the `TrainValidationSplitModel`
    created.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图11*中，我们展示了行预测与实际标签的对比。第一列是实际标签，第二列表示特征向量，第三列显示基于`TrainValidationSplitModel`创建的特征向量的原始预测。
- en: Logistic regression-based model selection for the cancer dataset
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于逻辑回归的癌症数据集模型选择
- en: In this sub-section, we will show how to perform train validation split tuning
    for OCR data. We will use the logistic regression in the first place; then we
    will move forward for other models. Finally, we will recommend the most suitable
    parameters for the OCR data classification.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个子部分中，我们将展示如何对OCR数据执行训练验证分割调整。我们将首先使用逻辑回归；然后我们将继续其他模型。最后，我们将为OCR数据分类推荐最合适的参数。
- en: '**Step 1: Import the necessary packages/libraries/APIs**'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：导入必要的包/库/API**'
- en: '[PRE47]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '**Step 2: Initialize the necessary Spark environment**'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：初始化必要的Spark环境**'
- en: '[PRE48]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Here we set the application name as `CancerDiagnosis`, the master URL as `local[*]`
    and the Spark Context as the entry point of the program. Please set these parameters
    accordingly.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将应用名称设置为`CancerDiagnosis`，主URL设置为`local[*]`，Spark上下文设置为程序的入口点。请相应地设置这些参数。
- en: '**Step 3: Create the Java RDD**'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：创建Java RDD**'
- en: 'Parse the cancer diagnosis data and prepare the Java RDDs for strings:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 解析癌症诊断数据并为字符串准备Java RDD：
- en: '[PRE49]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '**Step 4: Prepare the cancer diagnosis LabeledPoint RDDs**'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：准备癌症诊断LabeledPoint RDD**'
- en: 'As already discussed in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*, the cancer diagnosis dataset contains two labels
    *B* and *M* for Benign and Malignant. However, we need to convert them into a
    numeric label. Just use the following code to convert all of them from label transforming
    to `LabeledPoint` RDDs preparation:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d "第6章。构建可扩展的机器学习管道")中已经讨论的，癌症诊断数据集包含良性和恶性的两个标签*B*和*M*。但是，我们需要将它们转换为数字标签。只需使用以下代码将它们全部从标签转换为`LabeledPoint`
    RDDs准备：
- en: '[PRE50]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![Logistic regression-based model selection for the cancer dataset](img/00082.jpeg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![基于癌症数据集的逻辑回归模型选择](img/00082.jpeg)'
- en: 'Figure 12: The Label Point RDDs snapshot'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：标签点RDDs快照
- en: As you can see in *Figure 9*, *The top 20 rows*, the labels B and M have been
    converted into 1.0 and 0.0\. Now we need to create a data frame out of the label
    point RDDs.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*图9*中所看到的，*前20行*，标签B和M已经转换为1.0和0.0。现在我们需要从标签点RDDs创建一个数据框。
- en: '**Step 5: Create a Dataset and also prepare the training and test set**'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：创建数据集并准备训练和测试集**'
- en: 'Create a Dataset from the previous RDDs (that is, `linesRDD`) by specifying
    the Label Point class:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定标签点类从先前的RDDs（即`linesRDD`）创建一个数据集：
- en: '[PRE51]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![Logistic regression-based model selection for the cancer dataset](img/00136.jpeg)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![基于癌症数据集的逻辑回归模型选择](img/00136.jpeg)'
- en: 'Figure 13: The created Dataset showing the top 20 rows.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：显示前20行的创建的数据集。
- en: '[PRE52]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Note, that you will have to set the ratio of the random split based on your
    data and problem type accordingly.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您将不得不根据您的数据和问题类型设置随机拆分的比率。
- en: '**Step 6: Configure an ML pipeline using logistic regression:**'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：使用逻辑回归配置ML管道：**'
- en: '[PRE53]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**Step 7: Construct a grid of parameters to search over**'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：构建要搜索的参数网格**'
- en: 'Currently, Spark uses a `ParamGridBuilder` to construct a grid of parameters
    to search over. In this regard, suppose we have three values for `hashingTF.numFeatures`
    and two values for `lr.regParam`, this grid will have 3 x 2 = 6 parameter settings
    for `CrossValidator` to choose from:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Spark使用`ParamGridBuilder`来构建要搜索的参数网格。在这方面，假设我们有三个值用于`hashingTF.numFeatures`和两个值用于`lr.regParam`，这个网格将有3
    x 2 = 6个参数设置供`CrossValidator`选择：
- en: '[PRE54]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Note, that you will have to set the values of above parameters based on your
    data and problem type accordingly.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您将不得不根据您的数据和问题类型设置上述参数的值。
- en: '**Step 8: Create a TrainValidationSplit instance:**'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：创建一个TrainValidationSplit实例：**'
- en: '[PRE55]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In this case, the estimator is simply the linear regression that we created
    in *Step 4*. Prepare the cancer diagnosis `LabeledPoint` RDDs. A `TrainValidationSplit`
    requires an Estimator, a set of Estimator `ParamMaps`, and an Evaluator that supports
    binary classification since our dataset has only two classes where 80% is used
    for the purpose of training and the remaining 20% for the validation.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，估计器只是我们在*步骤4*中创建的线性回归。准备癌症诊断`LabeledPoint` RDDs。一个`TrainValidationSplit`需要一个估计器，一组估计器`ParamMaps`，以及一个支持二元分类的评估器，因为我们的数据集只有两个类，其中80%用于训练，剩下的20%用于验证。
- en: '**Step 9: Run the TrainValidationSplit and choose the parameters**'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9：运行TrainValidationSplit并选择参数**'
- en: 'Run the `TrainValidationSplit`, and choose the best set of parameters for your
    problem using the training set. Just use the following code segments:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`TrainValidationSplit`，并使用训练集选择问题的最佳参数集。只需使用以下代码段：
- en: '[PRE56]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '**Step 10: Make predictions on the test set**'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤10：对测试集进行预测**'
- en: 'Make predictions on the test data where the model is the model with a combination
    of parameters that performed the best. Finally, show the predictions. Just use
    the following code segment for doing so:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 对测试数据进行预测，其中模型是表现最佳的参数组合的模型。最后，显示预测结果。只需使用以下代码段：
- en: '[PRE57]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![Logistic regression-based model selection for the cancer dataset](img/00038.jpeg)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![基于癌症数据集的逻辑回归模型选择](img/00038.jpeg)'
- en: 'Figure 14: Prediction against each feature and label'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：针对每个特征和标签的预测
- en: Therefore, if you compare these results with those shown in *Figure 6, The newly
    created label and ID from the dataset; where ID is the number of rows*, you find
    that the prediction accuracy increases for more sophisticated methods of measuring
    predictive accuracy that can be used to identify places where the error rate can
    be optimized depending on the costs of each type of error.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您将这些结果与*图6*中显示的结果进行比较，*从数据集中新创建的标签和ID；其中ID是行数*，您会发现预测准确性增加了，用于识别可以优化错误率的更复杂的预测准确性测量方法，这些方法可以根据每种错误的成本来确定可以使用的地方。
- en: Summary
  id: totrans-405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Tuning an algorithm or machine learning application can be thought of as simply
    a process through which one goes when they optimise the parameters that impact
    the model in order to enable the algorithm to perform the best (in terms of run-time
    and memory usages). In this chapter, we have shown how to perform ML model tuning
    using train-validation split and cross-validation techniques of Spark ML.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 调整算法或机器学习应用可以被认为是一个简单的过程，通过这个过程，人们优化影响模型的参数，以使算法在运行时间和内存使用方面表现最佳。在本章中，我们展示了如何使用Spark
    ML的训练验证拆分和交叉验证技术来进行ML模型调优。
- en: We also want to mention that the tuning related support and algorithms are still
    not well enriched until the date of (14th October 2016) the current release of
    Spark. Interested readers are encouraged to visit the Spark tuning page at [http://spark.apache.org/docs/latest/ml-tuning.html](http://spark.apache.org/docs/latest/ml-tuning.html)
    for more updates since we believe that more features will be added to the Spark
    website and they will certainly provide enough documentation.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还想提到，直到2016年10月14日的当前Spark发布日期，调整相关的支持和算法仍然不够丰富。鼓励感兴趣的读者访问Spark调整页面[http://spark.apache.org/docs/latest/ml-tuning.html](http://spark.apache.org/docs/latest/ml-tuning.html)以获取更多更新，因为我们相信Spark网站将添加更多功能，并且他们肯定会提供足够的文档。
- en: In the next chapter, we will discuss how to make your machine learning algorithm
    or models adaptable for new datasets. This chapter covers advanced machine learning
    techniques to be able to make algorithms adaptable to new data. It will mainly
    focus on batch/streaming architectures and on online learning algorithms by using
    Spark streaming.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何使您的机器学习算法或模型适应新的数据集。本章涵盖了高级机器学习技术，以使算法能够适应新数据。它将主要关注批处理/流式架构和使用Spark流的在线学习算法。
- en: The ultimate target is to bring dynamism to the static machine learning models.
    Readers will also see how machine learning algorithms learn incrementally over
    the data; that is to say the models are updated each time they see a new training
    instance.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 最终目标是为静态机器学习模型带来活力。读者还将看到机器学习算法如何逐渐学习数据；也就是说，模型在看到新的训练实例时会进行更新。
