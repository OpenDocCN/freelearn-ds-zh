- en: Lending Club Loan Prediction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lending Club贷款预测
- en: We are almost at the end of the book, but the last chapter is going to utilize
    all the tricks and knowledge we covered in the previous chapters. We showed you
    how to utilize the power of Spark for data manipulation and transformation, and
    we showed you the different methods for data modeling, including linear models,
    tree models, and model ensembles. Essentially, this chapter will be the *kitchen
    sink* of chapters, whereby we will deal with many problems all at once, ranging
    from data ingestion, manipulation, preprocessing, outlier handling, and modeling,
    all the way to model deployment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎已经到了本书的结尾，但最后一章将利用我们在前几章中涵盖的所有技巧和知识。我们向您展示了如何利用Spark的强大功能进行数据处理和转换，以及我们向您展示了包括线性模型、树模型和模型集成在内的数据建模的不同方法。本质上，本章将是各种问题的“综合章节”，我们将一次性处理许多问题，从数据摄入、处理、预处理、异常值处理和建模，一直到模型部署。
- en: One of our main goals is to provide a realistic picture of a data scientists'
    daily life--start with almost raw data, explore the data, build a few models,
    compare them, find the best model, and deploy into production--if only it were
    this easy all the time! In this final chapter, we will borrow a real-life scenario
    from Lending Club, a company that provides peer-to-peer loans. We will apply all
    the skills you learned to see if we can build a model that determines the riskiness
    of a loan. Furthermore, we will compare the results with actual Lending Club data
    to evaluate our process.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要目标之一是提供数据科学家日常生活的真实画面——从几乎原始数据开始，探索数据，构建几个模型，比较它们，找到最佳模型，并将其部署到生产环境——如果一直都这么简单就好了！在本书的最后一章中，我们将借鉴Lending
    Club的一个真实场景，这是一家提供点对点贷款的公司。我们将应用您学到的所有技能，看看是否能够构建一个确定贷款风险性的模型。此外，我们将与实际的Lending
    Club数据进行比较，以评估我们的过程。
- en: Motivation
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: The Lending Club goal is to minimize the investment risk of providing bad loans,
    the loans with a high probability of defaulting or being delayed, but also to
    avoid rejecting good loans and hence losing profits. Here, the main criterion
    is driven by accepted risk - how much risk Lending Club can accept to be still
    profitable.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Lending Club的目标是最小化提供坏贷款的投资风险，即那些有很高违约或延迟概率的贷款，但也要避免拒绝好贷款，从而损失利润。在这里，主要标准是由接受的风险驱动——Lending
    Club可以接受多少风险仍然能够盈利。
- en: Furthermore, for prospective loans, Lending Club needs to provide an appropriate
    interest rate reflecting risk and generating income or provide loan adjustments.
    Therefore, it follows that if a given loan has a high interest rate, we can possibly
    infer that there is more inherent risk than a loan with a lower interest rate.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于潜在的贷款，Lending Club需要提供一个反映风险并产生收入的适当利率，或者提供贷款调整。因此，如果某项贷款的利率较高，我们可能推断出这种贷款的固有风险比利率较低的贷款更大。
- en: In our book, we can benefit from the Lending Club experience since they provide
    historical tracking of not only good loans but also bad loans. Furthermore, all
    historical data is available, including final loan statuses representing a unique
    opportunity to fit into the role of a Lending Club data scientist and try to match
    or even beat their prediction models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的书中，我们可以从Lending Club的经验中受益，因为他们提供了不仅是良好贷款而且是坏贷款的历史追踪。此外，所有历史数据都可用，包括代表最终贷款状态的数据，这为扮演Lending
    Club数据科学家的角色并尝试匹配甚至超越他们的预测模型提供了独特的机会。
- en: We can even go one step further-we can imagine an "autopilot mode". For each
    submitted loan,we can define the investment strategy (that is, how much risk we
    want to accept). The autopilot will accept/reject the loan and propose a machine-generated
    interest rate and compute expected return. The only condition is if you make some
    money using our models, we expect a cut of the profits!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以再进一步——我们可以想象一个“自动驾驶模式”。对于每笔提交的贷款，我们可以定义投资策略（即，我们愿意接受多少风险）。自动驾驶将接受/拒绝贷款，并提出机器生成的利率，并计算预期收益。唯一的条件是，如果您使用我们的模型赚了一些钱，我们希望分享利润！
- en: Goal
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标
- en: 'The overall goal is to create a machine learning application that will be able
    to train models respecting given investment strategy and deploy these models as
    a callable service, processing incoming loan applications. The service will be
    able to decide about a given loan application and compute an interest rate. We
    can define our intentions with a top-down approach starting from business requirements.
    Remember, a good data scientist has a firm understanding of the question(s) being
    asked, which is dependent on understanding the business requirement(s), which
    are as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 总体目标是创建一个机器学习应用程序，能够根据给定的投资策略训练模型，并将这些模型部署为可调用的服务，处理进入的贷款申请。该服务将能够决定是否批准特定的贷款申请并计算利率。我们可以从业务需求开始，自上而下地定义我们的意图。记住，一个优秀的数据科学家对所提出的问题有着牢固的理解，这取决于对业务需求的理解，具体如下：
- en: We need to define what the investment strategy means and how it optimizes/influences
    our machine learning model creation and evaluation. Then, we will take the model's
    findings and apply them to our portfolio of loans to best optimize our profits
    based on specified investment strategy.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要定义投资策略的含义以及它如何优化/影响我们的机器学习模型的创建和评估。然后，我们将采用模型的发现，并根据指定的投资策略将其应用于我们的贷款组合，以最大程度地优化我们的利润。
- en: We need to define a computation of expected return based on the investment strategy,
    and the application should provide the expected return of a lender. This is an
    important loan attribute for investors since it directly connects the loan application,
    investment strategy (that is, risk), and possible profit. We should keep this
    fact in mind, since in real life, the modeling pipelines are used by users who
    are not experts in data science or statistics and who are more interested in more
    high-level interpretation of modeling outputs.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要定义基于投资策略的预期回报计算，并且应用程序应该提供出借人的预期回报。这对于投资者来说是一个重要的贷款属性，因为它直接连接了贷款申请、投资策略（即风险）和可能的利润。我们应该记住这一点，因为在现实生活中，建模管道是由不是数据科学或统计专家的用户使用的，他们更感兴趣于对建模输出的更高层次解释。
- en: 'Furthermore, we need means to design and realize a loan prediction pipeline,
    which consists of the following:'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们需要设计并实现一个贷款预测管道，其中包括以下内容：
- en: A model that is based on loan application data and investment strategy decides
    about the loan status-if the loan should be accepted or rejected.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于贷款申请数据和投资策略的模型决定贷款状态-贷款是否应该被接受或拒绝。
- en: The model needs to be robust enough to reject all bad loans (that is, loans
    that would lead to an investment loss), but on the other hand, do not miss any
    good loans (that is, do not miss any investment opportunity).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型需要足够健壮，以拒绝所有不良贷款（即导致投资损失的贷款），但另一方面，不要错过任何好贷款（即不要错过任何投资机会）。
- en: The model should be interpretable-it should provide an explanation as to why
    a loan was rejected. Interestingly, there is a lot of research regarding this
    subject; the interpretability of models with key stakeholders who want something
    more tangible than just *the model said so*.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型应该是可解释的-它应该解释为什么会拒绝贷款。有趣的是，关于这个主题有很多研究；关键利益相关者希望得到比“模型说了算”更具体的东西。
- en: For those interested in further reading regarding model interpretability, Zachary
    Lipton (UCSD) has an outstanding paper titled *The Mythos of Model Interpretability**, *[https://arxiv.org/abs/1606.03490](https://arxiv.org/abs/1606.03490)
    which directly addresses this topic. This is an especially useful paper for those
    data scientists who are constantly in the hot seat of explaining all their magic!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对模型可解释性感兴趣的人，UCSD的Zachary Lipton有一篇名为*模型可解释性的神话*的杰出论文，[https://arxiv.org/abs/1606.03490](https://arxiv.org/abs/1606.03490)直接讨论了这个话题。对于那些经常需要解释他们的魔法的数据科学家来说，这是一篇特别有用的论文！
- en: There is another model that recommends the interest rate for accepted loans.
    Based on the specified loan application, the model should decide the best interest
    rate-not too high lose a borrower, but not too low to miss a profit.
  id: totrans-17
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有另一个模型，它推荐接受贷款的利率。根据指定的贷款申请，模型应该决定最佳利率，既不能太高以至于失去借款人，也不能太低以至于错失利润。
- en: 'Finally, we need to decide how to deploy this complex, multi-faceted machine
    learning pipeline. Much like our previous chapter, which combines multiple models
    in a single pipeline, we will take all the inputs we have in our dataset-which
    we will see are very different types-and perform processing, feature extraction,
    model prediction, and recommendations based on our investment strategy: a tall
    order but one that we will accomplish in this chapter!'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们需要决定如何部署这个复杂的、多方面的机器学习管道。就像我们之前的章节一样，将多个模型组合成一个管道，我们将使用数据集中的所有输入-我们将看到它们是非常不同类型的-并进行处理、特征提取、模型预测和基于我们的投资策略的推荐：这是一个艰巨的任务，但我们将在本章中完成！
- en: Data
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: Lending Club provides all available loan applications and their results publicly.
    The data for years 2007-2012 and 2013-2014 can be directly downloaded from [https://www.lendingclub.com/info/download-data.action](https://www.lendingclub.com/info/download-data.action).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Lending Club提供所有可用的贷款申请及其结果。2007-2012年和2013-2014年的数据可以直接从[https://www.lendingclub.com/info/download-data.action](https://www.lendingclub.com/info/download-data.action)下载。
- en: 'Download the DECLINED LOAN DATA, as shown in the following screenshot:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下载拒绝贷款数据，如下截图所示：
- en: '![](img/00173.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00173.jpeg)'
- en: The downloaded files contain `filesLoanStats3a.CSV` and `LoanStats3b.CSV`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下载的文件包括`filesLoanStats3a.CSV`和`LoanStats3b.CSV`。
- en: 'The file we have contains approximately 230 k rows that are split into two
    sections:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拥有的文件包含大约230k行，分为两个部分：
- en: 'Loans that meet the credit policy: 168 k'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符合信用政策的贷款：168k
- en: 'Loans that do not meet the credit policy: 62 k (note the imbalanced dataset)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不符合信用政策的贷款：62k（注意不平衡的数据集）
- en: 'As always, it is advisable to look at the data by viewing a sample row or perhaps
    the first 10 rows; given the size of the dataset we have here, we can use Excel
    see at what a row looks like:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，建议通过查看样本行或前10行来查看数据；鉴于我们这里的数据集的大小，我们可以使用Excel来查看一行是什么样子：
- en: '![](img/00174.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00174.jpeg)'
- en: Be careful since the downloaded file can contain a first line with a Lending
    Club download system comment. The best way is to remove it manually before loading
    into Spark.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要小心，因为下载的文件可能包含一行Lending Club下载系统的注释。最好在加载到Spark之前手动删除它。
- en: Data dictionary
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据字典
- en: 'The Lending Club download page also provides a data dictionary that contains
    explanations of individual columns. Specifically, the dataset contains 115 columns
    with specific meanings, collecting data about borrowers, including their bank
    history, credit history, and their loan application. Furthermore, for accepted
    loans, data includes payment progress or the final state of the loan-if it was
    fully paid or defaulted. One reason why it''s crucial to study the data dictionary
    is to prevent using a column that can possibly pre-hint at the result you are
    trying to predict and thereby result in a model that is inaccurate. The message
    is clear but very important: study and know your data!'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Lending Club下载页面还提供了包含单独列解释的数据字典。具体来说，数据集包含115个具有特定含义的列，收集关于借款人的数据，包括他们的银行历史、信用历史和贷款申请。此外，对于已接受的贷款，数据包括付款进度或贷款的最终状态-如果完全支付或违约。研究数据字典的一个重要原因是防止使用可能会预示你试图预测的结果的列，从而导致模型不准确。这个信息很清楚但非常重要：研究并了解你的数据！
- en: Preparation of the environment
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境准备
- en: 'In this Chapter, instead of using Spark shell, we will build two standalone
    Spark applications using Scala API: one for model preparation and the second for
    model deployment. In the case of Spark, the Spark application is a normal Scala
    application with a main method that serves as an entry point for execution. For
    example, here is a skeleton of application for model training:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Scala API构建两个独立的Spark应用程序，一个用于模型准备，另一个用于模型部署，而不是使用Spark shell。在Spark的情况下，Spark应用程序是一个正常的Scala应用程序，具有作为执行入口的主方法。例如，这是一个用于模型训练的应用程序的框架：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Moreover, we will try to extract parts, which can be shared between both applications,
    into a library. This will allow us to follow the DRY (do-not-repeat-yourself)
    principle:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将尝试提取可以在两个应用程序之间共享的部分到一个库中。这将使我们能够遵循DRY（不要重复自己）原则：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Data load
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据加载
- en: 'As usual, the first step involves the loading of data into memory. At this
    point, we can decide to use Spark or H2O data-loading capabilities. Since data
    is stored in the CSV file format, we will use the H2O parser to give us a quick
    visual insight into the data:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，第一步涉及将数据加载到内存中。在这一点上，我们可以决定使用Spark或H2O的数据加载能力。由于数据存储在CSV文件格式中，我们将使用H2O解析器快速地了解数据：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The loaded dataset can be directly explored in the H2O Flow UI. We can directly
    verify the number of rows, columns, and size of data stored in memory:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 加载的数据集可以直接在H2O Flow UI中进行探索。我们可以直接验证存储在内存中的数据的行数、列数和大小：
- en: '![](img/00175.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00175.jpeg)'
- en: Exploration – data analysis
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索-数据分析
- en: 'Now, it is time to explore the data. There are many questions that we can ask,
    such as the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候探索数据了。我们可以问很多问题，比如：
- en: What target features would we like to model supporting our goals?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要模拟支持我们目标的目标特征是什么？
- en: What are the useful training features for each target feature?
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个目标特征的有用训练特征是什么？
- en: Which features are not good for modeling since they leak information about target
    features (see the previous section)?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些特征不适合建模，因为它们泄漏了关于目标特征的信息（请参阅前一节）？
- en: Which features are not useful (for example, constant features, or features containing
    lot of missing values)?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些特征是无用的（例如，常量特征，或者包含大量缺失值的特征）？
- en: How to clean up data? What to do with missing values? Can we engineer new features?
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何清理数据？对缺失值应该怎么处理？我们能工程化新特征吗？
- en: Basic clean up
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本清理
- en: 'During data exploration, we will execute basic data clean up. In our case,
    we can utilize the power of booth tools together: we use the H2O Flow UI to explore
    the data, find suspicious parts of the data, and transform them directly with
    H2O, or, even better, with Spark.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据探索过程中，我们将执行基本的数据清理。在我们的情况下，我们可以利用两种工具的力量：我们使用H2O Flow UI来探索数据，找到数据中可疑的部分，并直接用H2O或者更好地用Spark进行转换。
- en: Useless columns
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无用的列
- en: 'The first step is to remove columns that contain unique values per line. Typical
    examples of this are user IDs or transaction IDs. In our case, we will identify
    them manually based on data description:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是删除每行包含唯一值的列。这种典型的例子是用户ID或交易ID。在我们的情况下，我们将根据数据描述手动识别它们：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00176.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00176.jpeg)'
- en: 'The next step is to identify useless columns, such as the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是识别无用的列，例如以下列：
- en: Constant columns
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常量列
- en: Bad columns (containing only missing values)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坏列（只包含缺失值）
- en: 'The following code will help us do so:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将帮助我们做到这一点：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00177.jpeg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00177.jpeg)'
- en: String columns
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符串列
- en: 'Now, it is time to explore different type of columns within our dataset. The
    easy step is to look at columns containing strings-these columns are like ID columns
    since they hold unique values:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候探索数据集中不同类型的列了。简单的步骤是查看包含字符串的列-这些列就像ID列一样，因为它们包含唯一值：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示在以下截图中：
- en: '![](img/00178.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00178.jpeg)'
- en: 'The question is whether the `url` feature contains any useful information that
    we can extract. We can explore data directly in H2O Flow and look at some samples
    of data in the feature column in the following screenshot:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是`url`特征是否包含我们可以提取的任何有用信息。我们可以直接在H2O Flow中探索数据，并在以下截图中查看特征列中的一些数据样本：
- en: '![](img/00179.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00179.jpeg)'
- en: We can see directly that the `url` feature contains only pointers to the Lending
    Club site using the application ID that we already dropped. Hence, we can decide
    to drop it.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接看到`url`特征只包含指向Lending Club网站的指针，使用我们已经删除的应用程序ID。因此，我们可以决定删除它。
- en: Loan progress columns
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贷款进度列
- en: 'Our target goal is to make a prediction of inherent risk based on loan application
    data, but some of the columns contain information about loan payment progress
    or they were assigned by Lending Club itself. In this example, for simplicity,
    we will drop them and focus only on columns that are part of the loan-application
    process. It is important to mention that in real-life scenarios, even these columns
    could carry interesting information (for example, payment progress) usable for
    prediction. However, we wanted to build our model based on the initial application
    of the loan and not when a loan has already been a) accepted and b) there is historical
    payment history that would not be known at the time of receiving the application.
    Based on the data dictionary, we detected the following columns:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是基于贷款申请数据做出固有风险的预测，但是一些列包含了关于贷款支付进度的信息，或者它们是由Lending Club自己分配的。在这个例子中，为了简单起见，我们将放弃它们，只关注贷款申请流程中的列。重要的是要提到，在现实场景中，甚至这些列可能包含有用的信息（例如支付进度）可用于预测。然而，我们希望基于贷款的初始申请来构建我们的模型，而不是在贷款已经被a）接受和b）有历史支付记录的情况下。根据数据字典，我们检测到以下列：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we can directly record all the columns that we need to remove since they
    do not bring any value for modelling:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以直接记录所有我们需要删除的列，因为它们对建模没有任何价值：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Categorical columns
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类列
- en: 'In the next step, we will explore categorical columns. The H2O parser marks
    a column as a categorical column only if it contains a limited set of string values.
    This is the main difference from columns that are marked as string columns. They
    contain more than 90 percent of unique values (see, for example, the `url` column
    that we explored in the previous paragraph). Let''s collect a list of all the
    categorical columns in our dataset and also the sparsity of individual features:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将探索分类列。H2O解析器只有在列包含有限的字符串值集时才将列标记为分类列。这是与标记为字符串列的列的主要区别。它们包含超过90%的唯一值（例如，我们在上一段中探索的`url`列）。让我们收集我们数据集中所有分类列的列表，以及各个特征的稀疏性：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00180.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00180.jpeg)'
- en: 'Now, we can explore individual columns. For example, the "purpose" column contains
    13 categories, and the main purpose of it is debt consolidation:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以探索单独的列。例如，“purpose”列包含13个类别，主要目的是债务合并：
- en: '![](img/00181.jpeg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00181.jpeg)'
- en: 'This column seems valid, but now, we should focus on suspicious columns, that
    is, first high-cardinality columns: `emp_title`, `title`, `desc`. There are several
    observations:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列看起来是有效的，但现在，我们应该关注可疑的列，即，首先是高基数列：`emp_title`，`title`，`desc`。有几个观察结果：
- en: The highest value for each column is an empty "value". That can mean a missing
    value. However, for these types of column (that is, columns representing a set
    of values) a dedicated level for a missing value makes very good sense. It just
    represents another possible state, "missing". Hence, we can keep it as it is.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每列的最高值是一个空的“值”。这可能意味着一个缺失的值。然而，对于这种类型的列（即，表示一组值的列），一个专门的级别用于缺失值是非常合理的。它只代表另一个可能的状态，“缺失”。因此，我们可以保持它不变。
- en: The "title" column overlaps with the purpose column and can be dropped.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “title”列与“purpose”列重叠，可以被删除。
- en: The `emp_title` and `desc` columns are purely textual descriptions. In this
    case, we will not treat them as categorical, but apply NLP techniques to extract
    important information later.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emp_title`和`desc`列纯粹是文本描述。在这种情况下，我们不会将它们视为分类，而是应用NLP技术以后提取重要信息。'
- en: 'Now, we will focus on columns starting with "mths_", As the name of the column
    suggests, the column should contain numeric values, but our parser decided that
    the columns are categorical. That could be caused by inconsistencies in collected
    data. For example, when we explore the domain of the "mths_since_last_major_derog"
    column, we can easily spot a reason:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将专注于以“mths_”开头的列，正如列名所示，该列应该包含数字值，但我们的解析器决定这些列是分类的。这可能是由于收集数据时的不一致性造成的。例如，当我们探索“mths_since_last_major_derog”列的域时，我们很容易就能发现一个原因：
- en: '![](img/00182.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00182.jpeg)'
- en: 'The most common value in the column is an empty value (that is, the same deficiency
    that we already explored earlier). In this case, we need to decide how to replace
    this value to transform a column to a numeric column: should it be replaced by
    the missing value?'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 列中最常见的值是一个空值（即，我们之前已经探索过的相同缺陷）。在这种情况下，我们需要决定如何替换这个值以将列转换为数字列：它应该被缺失值替换吗？
- en: 'If we want to experiment with different strategies, we can define a flexible
    transformation for this kind of column. In this situation, we will leave the H2O
    API and switch to Spark and define our own Spark UDF. Hence, as in the previous
    chapters, we will define a function. In this case, a function which for a given
    replacement value and a string, produces a float value representing given string
    or returns the specified value if string is empty. Then, the function is wrapped
    into the Spark UDF:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想尝试不同的策略，我们可以为这种类型的列定义一个灵活的转换。在这种情况下，我们将离开H2O API并切换到Spark，并定义我们自己的Spark
    UDF。因此，与前几章一样，我们将定义一个函数。在这种情况下，一个给定替换值和一个字符串的函数，产生代表给定字符串的浮点值，或者如果字符串为空则返回指定值。然后，将该函数包装成Spark
    UDF：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A good practice is to keep our code flexible enough to allow for experimenting,
    but do not make it over complicated. In this case, we simply keep an open door
    for cases that we expect to be explored in more detail.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的做法是保持我们的代码足够灵活，以允许进行实验，但不要使其过于复杂。在这种情况下，我们只是为我们期望更详细探讨的情况留下了一个开放的大门。
- en: 'There are two more columns that need our attention: `int_rate` and `revol_util`.
    Both should be numeric columns expressing percentages; however, if we explore
    them, we can easily see a problem--instead of a numeric value, the column contains
    the "%" sign. Hence, we have two more candidates for column transformations:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两列需要我们关注：`int_rate`和`revol_util`。两者都应该是表示百分比的数字列；然而，如果我们对它们进行探索，我们很容易看到一个问题--列中包含“％”符号而不是数字值。因此，我们有两个更多的候选列需要转换：
- en: '![](img/00183.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00183.jpeg)'
- en: 'However, we will not process the data directly but define the Spark UDF transformation,
    which will transform the string-based rate into a numeric rate. However, in definition
    of our UDF, we will simply use information provided by H2O, which is confirming
    that the list of categories in both columns contains only data suffixed by the
    percent sign:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不会直接处理数据，而是定义Spark UDF转换，将基于字符串的利率转换为数字利率。但是，在我们的UDF定义中，我们将简单地使用H2O提供的信息，确认两列中的类别列表只包含以百分号结尾的数据：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The defined UDF will be applied later with the rest of the Spark transformations.
    Furthermore, we need to realize that these transformations need to be applied
    during training as well as scoring time. Hence, we will put them into our shared
    library.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 定义的UDF将在稍后与其他Spark转换一起应用。此外，我们需要意识到这些转换需要在训练和评分时应用。因此，我们将它们放入我们的共享库中。
- en: Text columns
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本列
- en: In the previous section, we identified the `emp_title` and `desc` columns as
    targets for text transformation. Our theory is that these columns can carry useful
    information that could help distinguish between good and bad loans.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们确定了`emp_title`和`desc`列作为文本转换的目标。我们的理论是这些列可能包含有用的信息，可以帮助区分好坏贷款。
- en: Missing data
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失数据
- en: 'The last step in our data-exploration journey is to explore missing values.
    We already observed that some columns contain a value that represents a missing
    value; however, in this section, we will focus on pure missing values. First,
    we need to collect them:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据探索旅程的最后一步是探索缺失值。我们已经观察到一些列包含表示缺失值的值；然而，在本节中，我们将专注于纯缺失值。首先，我们需要收集它们：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The list contains 111 columns with the number of missing values varying from
    0.2 percent to 86 percent:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 列表包含111列，缺失值的数量从0.2％到86％不等：
- en: '![](img/00184.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00184.jpeg)'
- en: There are plenty of columns with five missing values, which can be caused by
    wrong data collection, and we can easily filter them out if they are represented
    in a pattern. For more "polluted columns" (for example, where there are many missing
    values), we need to figure out the right strategy per column based on the column
    semantics described in the data dictionary.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多列缺少五个值，这可能是由于错误的数据收集引起的，如果它们呈现出某种模式，我们可以很容易地将它们过滤掉。对于更“污染的列”（例如，有许多缺失值的列），我们需要根据数据字典中描述的列语义找出每列的正确策略。
- en: In all these cases, H2O Flow UI allows us to easily and quickly explore basic
    properties of data or even execute basic data cleanup. However, for more advanced
    data manipulations, Spark is the right tool to utilize because of a provided library
    of pre-cooked transformations and native SQL support.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，H2O Flow UI允许我们轻松快速地探索数据的基本属性，甚至执行基本的数据清理。但是，对于更高级的数据操作，Spark是正确的工具，因为它提供了一个预先准备好的转换库和本地SQL支持。
- en: Whew! As we can see, the data clean up, while being fairly laborious, is an
    extremely important task for the data scientist and one that will-hopefully-yield
    good answers to well thought out questions. This process must be carefully considered
    before each and every new problem that is looking to be solved. As the old ad
    age goes, "G*arbage in, garbage out" - *if the inputs are not right, our model
    will suffer the consequences.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！正如我们所看到的，数据清理虽然相当费力，但对于数据科学家来说是一项非常重要的任务，希望能够得到对深思熟虑的问题的良好答案。在解决每一个新问题之前，这个过程必须经过仔细考虑。正如古老的广告语所说，“垃圾进，垃圾出”-如果输入不正确，我们的模型将遭受后果。
- en: 'At this point, it is possible to compose all the identified transformations
    together into shared library functions:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，可以将所有确定的转换组合成共享库函数：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The method takes a Spark DataFrame as an input and applies all identified cleanup
    transformations. Now, it is time to build some models!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法以Spark DataFrame作为输入，并应用所有确定的清理转换。现在，是时候构建一些模型了！
- en: Prediction targets
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测目标
- en: 'After performing our data cleanup, it''s time to examine our prediction targets.
    Our ideal modeling pipeline includes two models: one that controls acceptance
    of the loan and one that estimates interest rate. Already you should be thinking
    that the first model is a binary classification problem (accept or reject the
    loan) while the second model is a regression problem, where the outcome is a numeric
    value.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 进行数据清理后，是时候检查我们的预测目标了。我们理想的建模流程包括两个模型：一个控制贷款接受的模型，一个估计利率的模型。你应该已经想到，第一个模型是一个二元分类问题（接受或拒绝贷款），而第二个模型是一个回归问题，结果是一个数值。
- en: Loan status model
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贷款状态模型
- en: The first model needs to distinguish between bad and good loans. The dataset
    already provides the `loan_status`column, which is the best feature representation
    of our modeling target. Let's look at the column in more detail.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模型需要区分好坏贷款。数据集已经提供了`loan_status`列，这是我们建模目标的最佳特征表示。让我们更详细地看看这一列。
- en: 'The loan status is represented by a categorical feature that has seven levels:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 贷款状态由一个分类特征表示，有七个级别：
- en: 'Fully paid: borrower paid the loan and all interest'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全额支付：借款人支付了贷款和所有利息
- en: 'Current: the loan is actively paid in accordance with a plan'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前：贷款按计划积极支付
- en: 'In grace period: late payment 1-15 days'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宽限期内：逾期付款1-15天
- en: 'Late (16-30 days): late payment'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逾期（16-30天）：逾期付款
- en: 'Late (31-120 days): late payment'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逾期（31-120天）：逾期付款
- en: 'Charged off: a loan is 150 days past the due date'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已冲销：贷款逾期150天
- en: 'Default: a loan was lost'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 违约：贷款丢失
- en: 'For the first modeling goal, we need to distinguish between good and bad loans.
    Good loans could be the loans that were fully paid. The rest of the loans could
    be considered as bad loans with the exception of current loans that need more
    attention (for example, survival analysis) or we could simply remove all rows
    that contain the "Current" status. For transformation of the `loan_status` feature
    into a binary feature, we will define a Spark UDF:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个建模目标，我们需要区分好贷款和坏贷款。好贷款可能是已全额偿还的贷款。其余的贷款可以被视为坏贷款，除了需要更多关注的当前贷款（例如，存活分析），或者我们可以简单地删除包含“Current”状态的所有行。为了将loan_status特征转换为二进制特征，我们将定义一个Spark
    UDF：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can explore the distribution of individual categories in more detail. In
    the following screenshot,we can also see that the ratio between good and bad loans
    is highly unbalanced. We need to keep this fact during the training and evaluation
    of the model, since we would like to optimize the recall probability of detection
    of the bad loan:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更详细地探索各个类别的分布。在下面的截图中，我们还可以看到好贷款和坏贷款之间的比例非常不平衡。在训练和评估模型时，我们需要牢记这一事实，因为我们希望优化对坏贷款的召回概率：
- en: '![](img/00185.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00185.jpeg)'
- en: Properties of the loan_status column.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: loan_status列的属性。
- en: Base model
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本模型
- en: 'At this point, we have prepared the target prediction column and cleaned up
    the input data, and we can now build a base model. The base model gives us basic
    intuition about data. For this purpose, we will use all columns except columns
    detected as being useless. We will also skip handling of missing values, since
    we will use H2O and the RandomForest algorithm, which can handle missing values.
    However, the first step is to prepare a dataset with the help of defined Spark
    transformations:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经准备好了目标预测列并清理了输入数据，现在可以构建一个基本模型了。基本模型可以让我们对数据有基本的直觉。为此，我们将使用除了被检测为无用的列之外的所有列。我们也将跳过处理缺失值，因为我们将使用H2O和RandomForest算法，它可以处理缺失值。然而，第一步是通过定义的Spark转换来准备数据集：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We will simply drop all known columns that are correlated with our target prediction
    column, all high categorical columns that carry a text description (except `title` and
    `desc`, which we will use later), and apply all basic the cleanup transformation
    we identified in the sections earlier.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简单地删除所有已知与我们的目标预测列相关的列，所有携带文本描述的高分类列（除了`title`和`desc`，我们稍后会使用），并应用我们在前面部分确定的所有基本清理转换。
- en: 'The next step involves splitting data into two parts. As usual, we will keep
    the majority of data for training and rest for model validation and transforming
    into a form that is accepted by H2O model builders:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步涉及将数据分割成两部分。像往常一样，我们将保留大部分数据用于训练，其余部分用于模型验证，并将其转换为H2O模型构建器接受的形式：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With the cleanup data, we can easily build a model. We will blindly use the
    RandomForest algorithm since it gives us direct insight into data and importance
    of individual features. We say "blindly" because as you recall from [Chapter 2](part0038.html#147LC0-d18ba71168a441bd917775fac13ca893),
    *Detecting Dark Matter - The Higgs-Boson Particle,* a RandomForest model can take
    inputs of many different types and build many different trees using different
    features, which gives us confidence to use this algorithm as our out-of-the-box
    model, given how well it performs when including all our features. Thus, the model
    also defines a baseline that we would like to improve by engineering new features.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有了清理后的数据，我们可以轻松地构建一个模型。我们将盲目地使用RandomForest算法，因为它直接为我们提供了数据和个体特征的重要性。我们之所以说“盲目”，是因为正如你在[第2章](part0038.html#147LC0-d18ba71168a441bd917775fac13ca893)中回忆的那样，*探测暗物质
    - 强子玻色子粒子*，RandomForest模型可以接受许多不同类型的输入，并使用不同的特征构建许多不同的树，这让我们有信心使用这个算法作为我们的开箱即用模型，因为它在包括所有特征时表现得非常好。因此，该模型也定义了一个我们希望通过构建新特征来改进的基线。
- en: 'We will use default settings. RandomForest brings out-of-the-box validation
    schema based on out-of-bag samples, so we can skip cross-validation for now. However,
    we will increase the number of constructed trees, but limit the model builder
    execution by a `Logloss`-based stopping criterion. Furthermore, we know that the
    prediction target is imbalanced where the number of good loans is much higher
    than bad loans, so we will ask for upsampling minority class by enabling the `balance_classes` option:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用默认设置。RandomForest提供了基于袋外样本的验证模式，因此我们暂时可以跳过交叉验证。然而，我们将增加构建树的数量，但通过基于Logloss的停止准则限制模型构建的执行。此外，我们知道预测目标是不平衡的，好贷款的数量远远高于坏贷款，因此我们将通过启用balance_classes选项要求对少数类进行上采样：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'When the model is built, we can explore its quality, as we did in the previous
    chapters, but our first look will be at the importance of the features:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建完成后，我们可以像在之前的章节中那样探索其质量，但我们首先要看的是特征的重要性：
- en: '![](img/00186.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00186.jpeg)'
- en: The most surprising fact is that the zip_code and **collection_recovery_fee** features
    have a much higher importance than the rest of the columns. This is suspicious
    and could indicate direct correlation between the column and the target variable.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最令人惊讶的事实是，zip_code和collection_recovery_fee特征的重要性远高于其他列。这是可疑的，可能表明该列与目标变量直接相关。
- en: We can revisit the data dictionary, which describes the **zip_code** column
    as "the first three numbers of the zip code provided by the borrower in the loan
    application" and the second column as "post-charge off collection fee". The latter
    one indicates a direct connection to the response column since "good loans" will
    have a value equal to zero. We can also validate this fact by exploring the data.
    In the case of zip_code, there is no obvious connection to the response column.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重新查看数据字典，其中将**zip_code**列描述为“借款人在贷款申请中提供的邮政编码的前三个数字”，第二列描述为“后收费用”。后者指示与响应列的直接联系，因为“好贷款”将具有等于零的值。我们还可以通过探索数据来验证这一事实。在zip_code的情况下，与响应列没有明显的联系。
- en: 'We will therefore do one more model run, but in this case, we will try to ignore
    both the `zip_code` and `collection_recovery_fee` columns:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将进行一次模型运行，但在这种情况下，我们将尝试忽略`zip_code`和`collection_recovery_fee`列：
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After the model is built, we can explore the variable importance graph again
    and see a more meaningful distribution of the importance between the variables.
    Based on the graph, we can decide to use only top 10 input features to simplify
    the model''s complexity and decrease modeling time. It is important to say that
    we still need to consider the removed columns as relevant input features:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型后，我们可以再次探索变量重要性图，并看到变量之间的重要性分布更有意义。根据图表，我们可以决定仅使用前10个输入特征来简化模型的复杂性并减少建模时间。重要的是要说，我们仍然需要考虑已删除的列作为相关的输入特征：
- en: '![](img/00187.jpeg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00187.jpeg)'
- en: '**Base model performance**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**基础模型性能**'
- en: 'Now, we can look at the model performance of the created model. We need to
    keep in mind that in our case, the following applies:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以查看创建模型的模型性能。我们需要记住，在我们的情况下，以下内容适用：
- en: The performance of the model is reported on out-of-bag samples, not on unseen
    data.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的性能是基于袋外样本报告的，而不是未见数据。
- en: We used fixed parameters as the best guess; however, it would be beneficial
    to perform a random parameter search to see how the input parameters influence
    the model's performance.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用固定参数作为最佳猜测；然而，进行随机参数搜索将有益于了解输入参数如何影响模型的性能。
- en: '![](img/00188.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00188.jpeg)'
- en: 'We can see that the AUC measured on out-of-bag sample of data is quite high.
    Even individual class errors are for a selected threshold, which minimizes individual
    classes accuracy, low. However, let''s explore the performance of the model on
    the unseen data. We will use the prepared part of the data for validation:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在袋外样本数据上测得的AUC相当高。即使对于最小化各个类别准确率的选择阈值，各个类别的错误率也很低。然而，让我们探索模型在未见数据上的性能。我们将使用准备好的部分数据进行验证：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00189.jpeg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00189.jpeg)'
- en: The computed model metrics can be explored visually in the Flow UI as well.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 计算得到的模型指标也可以在Flow UI中进行可视化探索。
- en: We can see that the AUC is lower, and individual class errors are higher, but
    are still reasonably good. However, all the measured statistical properties do
    not give us any notion of "business" value of the model-how much money was lent,
    how much money was lost for defaulted loans, and so on. In the next step, we will
    try to design ad-hoc evaluation metrics for the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到AUC较低，各个类别的错误率较高，但仍然相当不错。然而，所有测量的统计属性都无法给我们任何关于模型的“业务”价值的概念-借出了多少钱，违约贷款损失了多少钱等等。在下一步中，我们将尝试为模型设计特定的评估指标。
- en: What does it mean by the statement that the model made a wrong prediction? It
    can consider a good loan application as bad, which will result in the rejection
    of the application. That also means the loss of profit from the loan interest.
    Alternatively, the model can recommend a bad loan application as good, which will
    cause the loss of the full or partial amount of lent money. Let's look at both
    situations in more detail.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 声明模型做出错误预测是什么意思？它可以将良好的贷款申请视为不良的，这将导致拒绝申请。这也意味着从贷款利息中损失利润。或者，模型可以将不良的贷款申请推荐为良好的，这将导致全部或部分借出的资金损失。让我们更详细地看看这两种情况。
- en: 'The former situation can be described by the following function:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 前一种情况可以用以下函数描述：
- en: '[PRE19]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The function returns the amount of money lost if a model predicted a bad loan,
    but the actual data indicated that the loan was good. The returned amount considers
    the predicted interest rate and term. The important variables are `predGoodLoanProb`,
    which holds the model's predicted probability of considering the actual loan as
    a good loan, and `predThreshold`, which allows us to set up a bar when the probability
    of predicting a good loan is good enough for us.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回如果模型预测了不良贷款，但实际数据表明贷款是良好的时候损失的金额。返回的金额考虑了预测的利率和期限。重要的变量是`predGoodLoanProb`，它保存了模型预测的将实际贷款视为良好贷款的概率，以及`predThreshold`，它允许我们设置一个标准，当预测良好贷款的概率对我们来说足够高时。
- en: 'In a similar way, we will describe the latter situation:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们将描述后一种情况：
- en: '[PRE20]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It is good to realize that we are just following the confusion matrix definition
    for false positives and false negatives and applying our domain knowledge of input
    data to define ad-hoc model evaluation metrics.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要意识到我们只是按照假阳性和假阴性的混淆矩阵定义，并应用我们对输入数据的领域知识来定义特定的模型评估指标。
- en: 'Now, it is time to utilize both functions and define `totalLoss`-how much money
    we can lose on accepting bad loans and missing good loans if we follow our model''s
    recommendations:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候利用这两个函数并定义`totalLoss`了-如果我们遵循模型的建议，接受不良贷款和错过良好贷款时我们可以损失多少钱：
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `totalLoss` function is defined for a Spark DataFrame and a threshold.
    The Spark DataFrame holds actual validation data and prediction composed of three
    columns: actual prediction for default threshold, the probability of a good loan,
    and the probability of a bad loan. The threshold helps us define the right bar
    for the good loan probability; that is, if the good loan probability is higher
    than threshold, we can consider that the model recommends to accept the loan.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`totalLoss`函数是为Spark DataFrame和阈值定义的。Spark DataFrame包含实际验证数据和预测，由三列组成：默认阈值的实际预测、良好贷款的概率和不良贷款的概率。阈值帮助我们定义良好贷款概率的合适标准；也就是说，如果良好贷款概率高于阈值，我们可以认为模型建议接受贷款。'
- en: 'If we run the function for different thresholds, including one that minimizes
    individual class errors, we will get the following table:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对不同的阈值运行该函数，包括最小化各个类别错误的阈值，我们将得到以下表格：
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00190.jpeg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00190.jpeg)'
- en: From the table, we can see that the lowest total loss for our metrics is based
    on threshold `0.85`, which represents quite a conservative strategy, which focusing
    on avoiding bad loans.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从表中可以看出，我们的指标的最低总损失是基于阈值`0.85`，这代表了一种相当保守的策略，侧重于避免坏账。
- en: 'We can even define a function that finds the minimal total loss and corresponding
    threshold:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以定义一个函数，找到最小的总损失和相应的阈值：
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00191.jpeg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00191.jpeg)'
- en: Based on the reported results, we can see that the model minimizes the total
    loss for threshold ~ `0.85`, which is higher than the default threshold identified
    by the model (F1 = 0.66). However, we still need to realize that this is just
    a base naive model; we did not perform any tuning and searching of right training
    parameters. We still have two fields, `title` and `desc`, which we can utilize.
    It's time for model improvements!
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 基于报告的结果，我们可以看到模型将总损失最小化到阈值约为`0.85`，这比模型识别的默认阈值（F1 = 0.66）要高。然而，我们仍然需要意识到这只是一个基本的朴素模型；我们没有进行任何调整和搜索正确的训练参数。我们仍然有两个字段，`title`和`desc`，我们可以利用。是时候改进模型了！
- en: The emp_title column transformation
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: emp_title列转换
- en: The first column, `emp_title`, describes the employment title. However, it is
    not unified-there are multiple versions with the same meaning ("Bank of America"
    versus "bank of america") or a similar meaning ("AT&T" and "AT&T Mobility"). Our
    goal is to unify the labels into a basic form, detect similar labels, and replace
    them by a common title. The theory is the employment title has a direct impact
    on the ability to pay back the loan.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列`emp_title`描述了就业头衔。然而，它并不统一-有多个版本具有相同的含义（“Bank of America”与“bank of america”）或类似的含义（“AT&T”和“AT&T
    Mobility”）。我们的目标是将标签统一成基本形式，检测相似的标签，并用一个共同的标题替换它们。理论上，就业头衔直接影响偿还贷款的能力。
- en: 'The basic unification of labels is a simple task-transform labels into lowercase
    form and drop all non-alphanumeric characters ("&" or "."). For this step, we
    will use the Spark API for user-defined functions:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 标签的基本统一是一个简单的任务-将标签转换为小写形式并丢弃所有非字母数字字符（例如“&”或“.”）。对于这一步，我们将使用Spark API进行用户定义的函数：
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The next step defines a tokenizer, a function that splits a sentence into individual
    tokens and drops useless and stop words (for example, too short words or conjunctions).
    In our case, we will make the minimal token length and list of stop words flexible
    as input parameters:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步定义了一个分词器，一个将句子分割成单独标记并丢弃无用和停用词（例如，太短的词或连词）的函数。在我们的情况下，我们将使最小标记长度和停用词列表作为输入参数灵活：
- en: '[PRE25]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It is important to mention that Spark API provides a list of stop words already
    as part of `StopWordsRemover` transformation. Our definition of `tokenizeUdf` directly
    utilizes the provided list of English stop words.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要提到，Spark API已经提供了停用词列表作为`StopWordsRemover`转换的一部分。我们对`tokenizeUdf`的定义直接利用了提供的英文停用词列表。
- en: 'Now, it is time to look at the column in more detail. We will start by selecting
    the `emp_title` column from the already created DataFrame, `loanStatusBaseModelDf`,
    and apply the two functions defined earlier:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候更详细地查看列了。我们将从已创建的DataFrame `loanStatusBaseModelDf`中选择`emp_title`列，并应用前面定义的两个函数：
- en: '[PRE26]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we have a Spark DataFrame with two important columns: the first contains
    unified `emp_title` and the second one is represented by a list of tokens. With
    the help of Spark SQL API, we can easily compute the number of unique values in
    the `emp_title` column or the number of unique tokens with a frequency of more
    than 100 (that is, it means the word was used in more than 100 `emp_titles`):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个重要的Spark DataFrame，其中包含两个重要的列：第一列包含统一的`emp_title`，第二列由标记列表表示。借助Spark
    SQL API，我们可以轻松地计算`emp_title`列中唯一值的数量，或者具有超过100个频率的唯一标记的数量（即，这意味着该单词在超过100个`emp_titles`中使用）：
- en: '[PRE27]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00192.jpeg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00192.jpeg)'
- en: You can see that there are many unique values in the `emp_title` column. On
    the other hand, there are only `717` tokens that are repeated over and over. Our
    goal to *compress* the number of unique values in the column and group similar
    values together. We can experiment with different methods. For example, encode
    each `emp_title` with a representative token or use a more advanced technique
    based on the Word2Vec algorithm.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到`emp_title`列中有许多唯一值。另一方面，只有`717`个标记一遍又一遍地重复。我们的目标是*压缩*列中唯一值的数量，并将相似的值分组在一起。我们可以尝试不同的方法。例如，用一个代表性标记对每个`emp_title`进行编码，或者使用基于Word2Vec算法的更高级的技术。
- en: In the preceding code, we combined DataFrame query capabilities with the computation
    power of raw RDDs. Many queries can be expressed with powerful SQL-based DataFrame
    APIs; however, if we need to process structured data (such as the sequence of
    string tokens in the preceding example), often the RDD API is a quick way to go.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将DataFrame查询功能与原始RDD的计算能力相结合。许多查询可以用强大的基于SQL的DataFrame API来表达；然而，如果我们需要处理结构化数据（例如前面示例中的字符串标记序列），通常RDD
    API是一个快速的选择。
- en: Let's look at the second option. The Word2Vec algorithm transforms text features
    into a vector space where similar words are closed together with respect to cosine
    distance of corresponding vectors representing the words. That's a nice property;
    however, we still need to detect "groups of similar words". For this task, we
    can simply use the KMeans algorithm.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看第二个选项。Word2Vec算法将文本特征转换为向量空间，其中相似的单词在表示单词的相应向量的余弦距离方面彼此靠近。这是一个很好的特性；然而，我们仍然需要检测“相似单词组”。对于这个任务，我们可以简单地使用KMeans算法。
- en: 'The first step is to create the Word2Vec model. Since we have data in a Spark
    DataFrame, we will simply use the Spark implementation from the `ml` package:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建Word2Vec模型。由于我们的数据在Spark DataFrame中，我们将简单地使用`ml`包中的Spark实现：
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The algorithm input is defined by a sequence of tokens representing sentences
    stored in the "tokens" column. The `outputCol` parameter defines the output of
    the model if it is used to transform the data:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 算法输入由存储在“tokens”列中的句子表示的标记序列定义。`outputCol`参数定义了模型的输出，如果用于转换数据的话：
- en: '[PRE29]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00193.jpeg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00193.jpeg)'
- en: From the output of transformation, you can directly see that the DataFrame output
    contains not only the `emp_title` and `emp_title_tokens` input columns, but also
    the `emp_title_w2vVector` column, which represents the output of the w2vModel
    transformation.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 从转换的输出中，您可以直接看到DataFrame输出不仅包含`emp_title`和`emp_title_tokens`输入列，还包含`emp_title_w2vVector`列，它代表了w2vModel转换的输出。
- en: It is important to mention that the Word2Vec algorithm is defined only for words,
    but the Spark implementation transforms sentences (that is, the sequence of words)
    into a vector as well by averaging all the word vectors that the sentence represents.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 需要提到的是，Word2Vec算法仅针对单词，但Spark实现也将句子（即单词序列）转换为向量，方法是通过对句子表示的所有单词向量进行平均。
- en: 'In the next step, we will build a K-means model to partition a vector space
    representing individual employment titles into a predefined number of clusters.
    Before doing this, it''s important to think about why this would be a good thing
    to do in the first place. Think about the many different variations of saying
    "Software Engineer" that you know of: Programmer Analyst, SE, Senor Software Engineer,
    and so on. Given these variations that all essentially mean the same thing and
    will be represented by similar vectors, clustering provides us with a means to
    group similar titles together. However, we need to specify how many K clusters
    we should detect-this needs more experimentation, but for simplicity, we will
    try `500` clusters:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个K均值模型，将代表个人就业头衔的向量空间划分为预定义数量的聚类。在这之前，重要的是要考虑为什么这样做是有益的。想想你所知道的“软件工程师”的许多不同变体：程序分析员，SE，高级软件工程师等等。鉴于这些本质上意思相同并且将由相似向量表示的变体，聚类为我们提供了一种将相似头衔分组在一起的方法。然而，我们需要指定我们应该检测到多少K个聚类-这需要更多的实验，但为简单起见，我们将尝试`500`个聚类：
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The model allows us to transform the input data and explore the clusters. The
    cluster number is stored in a new column called `emp_title_cluster`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型允许我们转换输入数据并探索聚类。聚类编号存储在一个名为`emp_title_cluster`的新列中。
- en: Specifying the number of clusters is tricky given that we are dealing with the
    unsupervised world of machine learning. Often, practitioners will use a simple
    heuristic known as the elbow method( refer the following link: [https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)),
    which basically runs through many K-means models, increasing the number of K-clusters
    as a function of the heterogeneity (uniqueness) among each of the clusters. Usually,
    there is a diminishing gain as we increase the number of K-clusters and the trick
    is to find where the increase becomes marginal to the point where the benefit
    is no longer "worth" the run time.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 指定聚类数量是棘手的，因为我们正在处理无监督的机器学习世界。通常，从业者会使用一个简单的启发式方法，称为肘部法则（参考以下链接：[https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)），基本上通过许多K均值模型，增加K聚类的数量作为每个聚类之间的异质性（独特性）的函数。通常情况下，随着K聚类数量的增加，收益会递减，关键是找到增加变得边际的点，以至于收益不再值得运行时间。
- en: Alternatively, there are some information criteria statistics known as **AIC**
    (**Akaike Information Criteria**) ([https://en.wikipedia.org/wiki/Akaike_information_criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion))
    and **BIC** (**Bayesian Information Criteria**) ([https://en.wikipedia.org/wiki/Bayesian_information_criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion))
    that those of you who are interested should look into for further insight. Note
    that at of the time of writing this book, Spark has yet to implement these information
    criteria, and hence, we will not cover this in more detail.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，还有一些信息准则统计量，被称为**AIC**（**阿凯克信息准则**）（[https://en.wikipedia.org/wiki/Akaike_information_criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion)）和**BIC**（**贝叶斯信息准则**）（[https://en.wikipedia.org/wiki/Bayesian_information_criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion)），对此感兴趣的人应该进一步了解。需要注意的是，在撰写本书时，Spark尚未实现这些信息准则，因此我们不会详细介绍。
- en: 'Take a look at the following code snippet:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下代码片段：
- en: '[PRE31]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00194.jpeg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00194.jpeg)'
- en: 'Additionally, we can explore words associated with a random cluster:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以探索与随机聚类相关的单词：
- en: '[PRE32]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output is as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00195.jpeg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00195.jpeg)'
- en: Look at the preceding cluster and ask yourself, "Do these titles seem like a
    logical cluster?" Perhaps more training may be required, or perhaps we need to
    consider further feature transformations, such as running an n-grammer, which
    can identify sequences of words that occur with a high degree of frequency. Interested
    parties can check out the n-grammer section in Spark here.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 看看前面的聚类，问自己，“这些标题看起来像是一个逻辑聚类吗？”也许需要更多的训练，或者也许我们需要考虑进一步的特征转换，比如运行n-grammer，它可以识别高频发生的单词序列。感兴趣的人可以在Spark中查看n-grammer部分。
- en: 'Furthermore, the `emp_title_cluster` column defines a new feature that we will
    use to replace the original `emp_title` column. We also need to remember all the
    steps and models we used in the process of the column preparation, since we will
    need to reproduce them to enrich the new data. For this purpose, the Spark pipeline
    is defined:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`emp_title_cluster`列定义了一个新特征，我们将用它来替换原始的`emp_title`列。我们还需要记住在列准备过程中使用的所有步骤和模型，因为我们需要重现它们来丰富新数据。为此，Spark管道被定义为：
- en: '[PRE33]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The first two pipeline steps represent the application of user-defined functions.
    We used the same trick that was used in [Chapter 4](part0080.html#2C9D00-d18ba71168a441bd917775fac13ca893), *Predicting
    Movie Reviews Using NLP and Spark Streaming,* to wrap an UDF into a Spark pipeline
    transformer with help of the defined `UDFTransformer` class. The remaining steps
    represent models that we built.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个管道步骤代表了用户定义函数的应用。我们使用了与[第4章](part0080.html#2C9D00-d18ba71168a441bd917775fac13ca893)中使用的相同技巧，将UDF包装成Spark管道转换器，并借助定义的`UDFTransformer`类。其余步骤代表了我们构建的模型。
- en: The defined `UDFTransformer` class is a nice way to wrap UDF into Spark pipeline
    transformer, but for Spark, it is a black box and it cannot perform all the powerful
    transformations. However, it could be replaced by an existing concept of the Spark
    SQLTransformer, which can be understood by the Spark optimizer; on the other hand,
    its usage is not so straightforward.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 定义的`UDFTransformer`类是将UDF包装成Spark管道转换器的一种好方法，但对于Spark来说，它是一个黑匣子，无法执行所有强大的转换。然而，它可以被Spark
    SQLTransformer的现有概念所取代，后者可以被Spark优化器理解；另一方面，它的使用并不那么直接。
- en: 'The pipeline still needs to be fit; however, in our case, since we used only
    Spark transformers, the fit operation bundles all the defined stages into the
    pipeline model:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 管道仍然需要拟合；然而，在我们的情况下，由于我们只使用了Spark转换器，拟合操作将所有定义的阶段捆绑到管道模型中：
- en: '[PRE34]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, it is time to evaluate the impact of the new feature on the model quality.
    We will repeat the same steps we did earlier during the evaluation of the quality
    of the base model:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候评估新特征对模型质量的影响了。我们将重复我们之前在评估基本模型质量时所做的相同步骤：
- en: Prepare training and validation parts and enrich them with a new feature, `emp_title_cluster`.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备训练和验证部分，并用一个新特征`emp_title_cluster`来丰富它们。
- en: Build a model.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建模型。
- en: Compute total the money loss and find the minimal loss.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算总损失金额并找到最小损失。
- en: 'For the first step, we will reuse the prepared train and validation parts;
    however, we need to transform them with the prepared pipeline and drop the "raw"
    column, `desc`:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一步，我们将重用准备好的训练和验证部分；然而，我们需要用准备好的管道对它们进行转换，并丢弃“原始”列`desc`：
- en: '[PRE35]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'When we have the data ready, we can repeat the model training with the same
    parameters we used for the base model training, except that we use the prepared
    input training part:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据准备好时，我们可以使用与基本模型训练相同的参数重复模型训练，只是我们使用准备好的输入训练部分：
- en: '[PRE36]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, we can evaluate the model on the validation data and compute our evaluation
    metrics based on the total money loss:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以在验证数据上评估模型，并根据总损失金额计算我们的评估指标：
- en: '[PRE37]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is shown here:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00196.jpeg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00196.jpeg)'
- en: We can see that employing an NLP technique to detect a similar job title slightly
    improves the quality of the model, resulting in decreasing the total dollar loss
    computed on the unseen data. However, the question is whether we can improve our
    model even more based on the `desc` column, which could include useful information.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，利用自然语言处理技术来检测相似的职位标题略微提高了模型的质量，导致了在未知数据上计算的总美元损失的减少。然而，问题是我们是否可以根据`desc`列进一步改进我们的模型，其中可能包含有用的信息。
- en: The desc column transformation
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: desc列转换
- en: The next column we will explore is `desc`. Our motivation is still to mine any
    possible information from it and improve model's quality. The `desc` column contains
    purely textual descriptions for why the lender wishes to take out a loan. In this
    case, we are not going to treat them as categorical values since most of them
    are unique. However, we will apply NLP techniques to extract important information.
    In contrast to the `emp_title` column, we will not use the Word2Vec algorithm,
    but we will try to find words that are distinguishing bad loans from good loans.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探索的下一列是`desc`。我们的动机仍然是从中挖掘任何可能的信息，并提高模型的质量。`desc`列包含了借款人希望贷款的纯文本描述。在这种情况下，我们不打算将它们视为分类值，因为大多数都是唯一的。然而，我们将应用自然语言处理技术来提取重要信息。与`emp_title`列相反，我们不会使用Word2Vec算法，而是尝试找到能够区分坏贷款和好贷款的词语。
- en: For this goal, we will simply decompose descriptions into individual words (that
    is, tokenization) and assign weights to each used word with the help of tf-idf
    and explore which words are most likely to represent good loans or bad loans.
    Instead of tf-idf, we could help just word counts, but tf-idf values are a better
    separation between informative words (such as "credit") and common words (such
    as "loan").
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目标，我们将简单地将描述分解为单独的单词（即标记化），并根据tf-idf赋予每个使用的单词权重，并探索哪些单词最有可能代表好贷款或坏贷款。我们可以使用词频而不是tf-idf值，但tf-idf值更好地区分了信息性词语（如“信用”）和常见词语（如“贷款”）。
- en: 'Let''s start with the same procedure we performed in the case of the `emp_title` column-defining
    transformations that transcribe the `desc` column into a list of unified tokens:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们在`emp_title`列的情况下执行的相同过程开始，定义将`desc`列转录为统一标记列表的转换：
- en: '[PRE38]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The transformation prepares a `desc_tokens` column that contains a list of
    words for each input `desc` value. Now, we need to translate string tokens into
    numeric form to build the tf-idf model. In this context, we will use `CountVectorizer`,
    which extracts the vocabulary of used words and generates a numeric vector for
    each row. A position in a numeric vector corresponds to a single word in the vocabulary
    and the value represents the number of occurrences. Un which g tokens into a numeric
    vector, since we would like to keep the relation between a number in the vector
    and token representing it. In contrast to Spark HashingTF, `CountVectorizer` preserves
    bijection between a word and the number of its occurrences in a generated vector.
    We will reuse this capability later:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 转换准备了一个包含每个输入`desc`值的单词列表的`desc_tokens`列。现在，我们需要将字符串标记转换为数字形式以构建tf-idf模型。在这种情况下，我们将使用`CountVectorizer`，它提取所使用的单词的词汇表，并为每一行生成一个数值向量。数值向量中的位置对应于词汇表中的单个单词，值表示出现的次数。我们希望将标记转换为数值向量，因为我们希望保留向量中的数字与表示它的标记之间的关系。与Spark
    HashingTF相反，`CountVectorizer`保留了单词与生成向量中其出现次数之间的双射关系。我们稍后将重用这种能力：
- en: '[PRE39]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define the IDF model:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 定义IDF模型：
- en: '[PRE40]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'When we put all the defined transformations into a single pipeline, we can
    directly train it on input data:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将所有定义的转换放入单个管道中时，我们可以直接在输入数据上训练它：
- en: '[PRE41]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, we have a pipeline model that can transform a numeric vector for each
    input `desc` value. Furthermore, we can inspect the pipeline model''s internals
    and extract vocabulary from the computed `CountVectorizerModel` and individual
    word weights from `IDFModel`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个管道模型，可以为每个输入`desc`值转换一个数值向量。此外，我们可以检查管道模型的内部，并从计算的`CountVectorizerModel`中提取词汇表，从`IDFModel`中提取单词权重：
- en: '[PRE42]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output is as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00197.jpeg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00197.jpeg)'
- en: 'At this point, we know individual word weights; however, we still need to compute
    which words are used by "good loans" and "bad loans". For this purpose, we will
    utilize information about word frequencies computed by the prepared pipeline model
    and stored in the `desc_vector` column (in fact, this is an output of `CountVectorizer`).
    We will sum all these vectors separately for good and then for bad loans:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们知道单词的权重；然而，我们仍然需要计算哪些单词被“好贷款”和“坏贷款”使用。为此，我们将利用由准备好的管道模型计算的单词频率信息，并存储在`desc_vector`列中（实际上，这是`CountVectorizer`的输出）。我们将分别为好贷款和坏贷款单独总结所有这些向量：
- en: '[PRE43]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Having computed values, we can easily find words that are used only by good/bad
    loans and explore their computed IDF weights:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 计算了值之后，我们可以轻松地找到只被好/坏贷款使用的单词，并探索它们计算出的IDF权重：
- en: '[PRE44]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00198.jpeg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00198.jpeg)'
- en: The produced information does not seem helpful, since we got only very rare
    words that allow us detect only a limited number of highly specific loan descriptions.
    However, we would like to be more generic and find more common words that are
    used by both loan types, but will still allow us to distinguish between bad and
    good loans.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 产生的信息似乎并不有用，因为我们只得到了非常罕见的单词，这些单词只允许我们检测到一些高度特定的贷款描述。然而，我们希望更通用，并找到更常见的单词，这些单词被两种贷款类型使用，但仍然允许我们区分好坏贷款。
- en: 'Therefore, we need to design a word score that will target words with high-frequency
    usage in good (or bad) loans but penalize rare words. For example, we can define
    it as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要设计一个单词得分，它将针对在好（或坏）贷款中高频使用的单词，但惩罚罕见的单词。例如，我们可以定义如下：
- en: '[PRE45]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'If we apply the word score method on each word in the vocabulary, we will get
    a sorted list of words based on the descending score:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在词汇表中的每个单词上应用单词得分方法，我们将得到一个基于得分降序排列的单词列表：
- en: '[PRE46]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00199.jpeg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00199.jpeg)'
- en: 'Based on the produced list, we can identify interesting words. We can take
    10 or 100 of them. However, we still need to figure out what to do with them.
    The solution is easy; for each word, we will generate a new binary feature-1 if
    a word is present in the `desc` value; otherwise, 0:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 根据生成的列表，我们可以识别有趣的单词。我们可以选择其中的10个或100个。然而，我们仍然需要弄清楚如何处理它们。解决方案很简单；对于每个单词，我们将生成一个新的二进制特征-如果单词出现在`desc`值中，则为1；否则为0：
- en: '[PRE47]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can test our idea on the prepared training and validation sample and measure
    the quality of the model. Again, the first step is to prepare the augmented data
    with a new feature. In this case, a new feature is a vector that contains binary
    features generated by descWordEncoder:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在准备好的训练和验证样本上测试我们的想法，并衡量模型的质量。再次，第一步是准备带有新特征的增强数据。在这种情况下，新特征是一个包含由descWordEncoder生成的二进制特征的向量：
- en: '[PRE48]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, we just need to compute the model''s quality:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要计算模型的质量：
- en: '[PRE49]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output is as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00200.jpeg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00200.jpeg)'
- en: We can see that the new feature helps and improves the precision of our model.
    On the other hand, it also opens a lot of space for experimentation-we can select
    different words, or even use IDF weights instead of binary values if the word
    is part of the `desc` column.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到新特征有所帮助，并提高了我们模型的精度。另一方面，它也为实验开辟了很多空间-我们可以选择不同的单词，甚至在单词是`desc`列的一部分时使用IDF权重而不是二进制值。
- en: 'To summarize our experiments, we will compare the computed results for the
    three models we produced: (1) the base model, (2) the model trained on the data
    augmented by the `emp_title` feature, and (3) the model trained on the data enriched
    by the `desc` feature:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 总结我们的实验，我们将比较我们产生的三个模型的计算结果：（1）基础模型，（2）在通过`emp_title`特征增强的数据上训练的模型，以及（3）在通过`desc`特征丰富的数据上训练的模型：
- en: '[PRE50]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00201.jpeg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00201.jpeg)'
- en: Our small experiments demonstrated the powerful concept of feature generation.
    Each newly generated feature improved the quality of the base model with respect
    to our model-evaluation criterion.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的小实验展示了特征生成的强大概念。每个新生成的特征都改善了基础模型的质量，符合我们的模型评估标准。
- en: At this point, we can finish with exploration and training of the first model
    to detect good/bad loans. We will use the last model we prepared since it gives
    us the best quality. There are still many ways to explore data and improve our
    model quality; however, now, it is time to build our second model.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以完成对第一个模型的探索和训练，以检测好/坏贷款。我们将使用我们准备的最后一个模型，因为它给出了最好的质量。仍然有许多方法可以探索数据和提高我们的模型质量；然而，现在是构建我们的第二个模型的时候了。
- en: Interest RateModel
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利率模型
- en: The second model predicts the interest rate of accepted loans. In this case,
    we will use only the part of the training data that corresponds to good loans,
    since they have assigned a proper interest rate. However, we need to understand
    that the remaining bad loans could carry useful information related to the interest
    rate prediction.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个模型预测已接受贷款的利率。在这种情况下，我们将仅使用对应于良好贷款的训练数据的部分，因为它们已经分配了适当的利率。然而，我们需要了解，剩下的坏贷款可能携带与利率预测相关的有用信息。
- en: 'As in the rest of the cases, we will start with the preparation of training
    data. We will use initial data, filter out bad loans, and drop string columns:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他情况一样，我们将从准备训练数据开始。我们将使用初始数据，过滤掉坏贷款，并删除字符串列：
- en: '[PRE51]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In the next step, we will use the capabilities of H2O random hyperspace search
    to find the best GBM model in a defined hyperspace of parameters. We will also
    constrain the search by additional stopping criteria based on the requested model
    precision and overall search time.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将利用H2O随机超空间搜索的能力，在定义的参数超空间中找到最佳的GBM模型。我们还将通过额外的停止标准限制搜索，这些标准基于请求的模型精度和整体搜索时间。
- en: 'The first step is to define common GBM model builder parameters, such as training,
    validation datasets, and response column:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义通用的GBM模型构建器参数，例如训练、验证数据集和响应列：
- en: '[PRE52]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The next step involves definition of hyperspace of parameters to explore. We
    can encode any interesting values, but keep in mind that the search could use
    any combination of parameters, even those that are useless:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步涉及定义要探索的参数超空间。我们可以对任何有趣的值进行编码，但请记住，搜索可能使用任何参数组合，甚至是无用的参数：
- en: '[PRE53]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, we will define how to traverse the defined hyperspace of parameters. H2O
    provides two strategies: a simple cartesian search that step-by-step builds the
    model for each parameter''s combination or a random search that randomly picks
    the parameters from the defined hyperspace. Surprisingly, the random search has
    quite a good performance, especially if it is used to explore a huge parameter
    space:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义如何遍历定义的参数超空间。H2O提供两种策略：简单的笛卡尔搜索，逐步构建每个参数组合的模型，或者随机搜索，从定义的超空间中随机选择参数。令人惊讶的是，随机搜索的性能相当不错，特别是当用于探索庞大的参数空间时：
- en: '[PRE54]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'In this case, we will also limit the search by two stopping conditions: the
    model performance based on RMSE and the maximum runtime of the whole grid search.
    At this point, we have defined all the necessary inputs, and it is time to launch
    the hyper search:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们还将通过两个停止条件限制搜索：基于RMSE的模型性能和整个网格搜索的最大运行时间。此时，我们已经定义了所有必要的输入，现在是启动超级搜索的时候了：
- en: '[PRE55]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The result of the search is a set of models called `grid`. Let''s find one
    with the lowest RMSE:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索结果是一组称为“grid”的模型。让我们找一个具有最低RMSE的模型：
- en: '[PRE56]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output is as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00202.jpeg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00202.jpeg)'
- en: Here, we can define our evaluation criteria and select the right model not only
    based on selected model metrics, but also consider the term and difference between
    predicted and actual value, and optimize the profit. However, instead of that,
    we will trust our search strategy that it found the best possible model and directly
    jump into deploying our solution.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以定义我们的评估标准，并选择正确的模型，不仅基于选择的模型指标，还要考虑预测值和实际值之间的差异，并优化利润。然而，我们将相信我们的搜索策略找到了最佳的可能模型，并直接跳入部署我们的解决方案。
- en: Using models for scoring
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用模型进行评分
- en: In the previous sections, we explored different data processing steps, and built
    and evaluated several models to predict the loan status and interest rates for
    the accepted loans. Now, it is time to use all built artifacts and compose them
    together to score new loans.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中，我们探索了不同的数据处理步骤，并构建和评估了几个模型，以预测已接受贷款的贷款状态和利率。现在，是时候使用所有构建的工件并将它们组合在一起，对新贷款进行评分了。
- en: 'There are multiple steps that we need to consider:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个步骤需要考虑：
- en: Data cleanup
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据清理
- en: The `emp_title` column preparation pipeline
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “emp_title”列准备管道
- en: The `desc` column transformation into a vector representing significant words
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将“desc”列转换为表示重要单词的向量
- en: The binomial model to predict loan acceptance status
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于预测贷款接受状态的二项模型
- en: The regression model to predict loan interest rate
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于预测贷款利率的回归模型
- en: To reuse these steps, we need to connect them into a single function that accepts
    input data and produces predictions involving loan acceptance status and interest
    rate.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 要重用这些步骤，我们需要将它们连接成一个单一的函数，该函数接受输入数据并生成涉及贷款接受状态和利率的预测。
- en: 'The scoring functions is easy-it replays all the steps that we did in the previous
    chapters:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 评分函数很简单-它重放了我们在前几章中所做的所有步骤：
- en: '[PRE57]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We use all definitions that we prepared before-`basicDataCleanup` method, `empTitleTransformer`,
    `loanStatusModel`, `intRateModel`-and apply them in the corresponding order.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用之前准备的所有定义-“basicDataCleanup”方法，“empTitleTransformer”，“loanStatusModel”，“intRateModel”-并按相应顺序应用它们。
- en: Note that in the definition of the `scoreLoan` functions, we do not need to
    remove any columns. All the defined Spark pipelines and models use only features
    they were defined on and keep the rest untouched.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在“scoreLoan”函数的定义中，我们不需要删除任何列。所有定义的Spark管道和模型只使用它们定义的特征，并保持其余部分不变。
- en: 'The method uses all the generated artifacts. For example, we can score the
    input data in the following way:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法使用所有生成的工件。例如，我们可以以以下方式对输入数据进行评分：
- en: '[PRE58]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output is as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00203.jpeg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00203.jpeg)'
- en: 'However, to score new loans independently from our training code, we still
    need to export trained models and pipelines in some reusable form. For Spark models
    and pipelines, we can directly use Spark serialization. For example, the defined
    `empTitleTransormer` can be exported in this way:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了独立于我们的训练代码对新贷款进行评分，我们仍然需要以某种可重复使用的形式导出训练好的模型和管道。对于Spark模型和管道，我们可以直接使用Spark序列化。例如，定义的`empTitleTransormer`可以以这种方式导出：
- en: '[PRE59]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We also defined the transformation for the `desc` column as a `udf` function,
    `descWordEncoderUdf`. However, we do not need to export it, since we defined it
    as part of our shared library.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还为`desc`列定义了转换为`udf`函数`descWordEncoderUdf`。然而，我们不需要导出它，因为我们将其定义为共享库的一部分。
- en: 'For H2O models, the situation is more complicated since there are several ways
    of model export: binary, POJO, and MOJO. The binary export is similar to the Spark
    export; however, to reuse the exported binary model, it is necessary to have a
    running instance of the H2O cluster. This limitation is removed by the other methods.
    The POJO exports the model as Java code, which can be compiled and run independently
    from the H2O cluster. Finally, the MOJO export model is in a binary form, which
    can be interpreted and used without running the H2O cluster. In this chapter,
    we will use the MOJO export, since it is straightforward and also the recommended
    method for model reuse:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对于H2O模型，情况更加复杂，因为有几种模型导出的方式：二进制、POJO和MOJO。二进制导出类似于Spark导出；然而，要重用导出的二进制模型，需要运行H2O集群的实例。其他方法消除了这种限制。POJO将模型导出为Java代码，可以独立于H2O集群进行编译和运行。最后，MOJO导出模型以二进制形式存在，可以在不运行H2O集群的情况下进行解释和使用。在本章中，我们将使用MOJO导出，因为它简单直接，也是模型重用的推荐方法。
- en: '[PRE60]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We can also export the Spark schema that defines the input data. This will
    be useful for the definition of a parser of the new data:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以导出定义输入数据的Spark模式。这对于新数据的解析器的定义将很有用：
- en: '[PRE61]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Note that the `saveSchema` method processes a given schema and removes all metadata.
    This is not common practice. However, in this case, we will remove them to save
    space.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`saveSchema`方法处理给定的模式并删除所有元数据。这不是常见的做法。然而，在这种情况下，我们将删除它们以节省空间。
- en: It is also important to mention that the data-creation process from the H2O
    frame implicitly attaches plenty of useful statistical information to the resulting
    Spark DataFrame.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 还要提到的是，从H2O框架中创建数据的过程会隐式地将大量有用的统计信息附加到生成的Spark DataFrame上。
- en: Model deployment
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型部署
- en: The model deployment is the most important part of model life cycle. At this
    stage, the model is fed by real-life data and produce results that can support
    decision making (for example, accepting or rejecting a loan).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署是模型生命周期中最重要的部分。在这个阶段，模型由现实生活数据提供支持决策的结果（例如，接受或拒绝贷款）。
- en: In this chapter, we will build a simple application combining the Spark streaming
    the models we exported earlier and shared code library, which we defined while
    writing the model-training application.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个简单的应用程序，结合Spark流式处理我们之前导出的模型和共享代码库，这是我们在编写模型训练应用程序时定义的。
- en: 'The latest Spark 2.1 introduces structural streaming, which is built upon the
    Spark SQL and allows us to utilize the SQL interface transparently with the streaming
    data. Furthermore, it brings a strong feature in the form of "exactly-once" semantics,
    which means that events are not dropped or delivered multiple times. The streaming
    Spark application has the same structure as a "regular" Spark application:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的Spark 2.1引入了结构化流，它建立在Spark SQL之上，允许我们透明地利用SQL接口处理流数据。此外，它以“仅一次”语义的形式带来了一个强大的特性，这意味着事件不会被丢弃或多次传递。流式Spark应用程序的结构与“常规”Spark应用程序相同：
- en: '[PRE63]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'There are three important parts: (1) The creation of input stream, (2) The
    transformation of the created stream, and (3) The writing resulted stream.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个重要部分：（1）输入流的创建，（2）创建流的转换，（3）写入结果流。
- en: Stream creation
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流创建
- en: There are several ways to create a stream, described in the Spark documentation
    ([https://spark.apache.org/docs/2.1.1/structured-streaming-programming-guide.html)](https://spark.apache.org/docs/2.1.1/structured-streaming-programming-guide.html)),
    including socket-based, Kafka, or file-based streams. In this chapter, we will
    use file-based streams, streams that are pointed to a directory and deliver all
    the new files that appear in the directory.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以创建流，Spark文档中有描述（[https://spark.apache.org/docs/2.1.1/structured-streaming-programming-guide.html)](https://spark.apache.org/docs/2.1.1/structured-streaming-programming-guide.html)），包括基于套接字、Kafka或基于文件的流。在本章中，我们将使用基于文件的流，指向一个目录并传递出现在目录中的所有新文件。
- en: 'Moreover, our application will read CSV files; thus, we will connect the stream
    input with the Spark CSV parser. We also need to configure the parser with the
    input data schema, which we exported from the mode-training application. Let''s
    load the schema first:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的应用程序将读取CSV文件；因此，我们将将流输入与Spark CSV解析器连接。我们还需要使用从模型训练应用程序中导出的输入数据模式配置解析器。让我们先加载模式：
- en: '[PRE64]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The `loadSchema` method modifies the loaded schema by marking all the loaded
    fields as nullable. This is a necessary step to allow input data to contain missing
    values in any column, not only in columns that contained missing values during
    model training.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '`loadSchema`方法通过将所有加载的字段标记为可为空来修改加载的模式。这是一个必要的步骤，以允许输入数据在任何列中包含缺失值，而不仅仅是在模型训练期间包含缺失值的列。'
- en: 'In the next step, we will directly configure a CSV parser and the input stream
    to read CSV files from a given data folder:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将直接配置一个CSV解析器和输入流，以从给定的数据文件夹中读取CSV文件：
- en: '[PRE66]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The CSV parser needs a minor configuration to set up the format for timestamp
    features and representation of missing values. At this point, we can even explore
    the structure of the stream:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: CSV解析器需要进行一些配置，以设置时间戳特征的格式和缺失值的表示。在这一点上，我们甚至可以探索流的结构：
- en: '[PRE67]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output is as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00204.jpeg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00204.jpeg)'
- en: Stream transformation
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流转换
- en: The input stream publishes a similar interface as a Spark DataSet; thus, it
    can be transformed via a regular SQL interface or machine learning transformers.
    In our case, we will reuse all the trained models and transformation that were
    saved in the previous sections.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 输入流发布了与Spark DataSet类似的接口；因此，它可以通过常规SQL接口或机器学习转换器进行转换。在我们的情况下，我们将重用在前几节中保存的所有训练模型和转换操作。
- en: 'First, we will load `empTitleTransformer`-it is a regular Spark pipeline transformer
    that can be loaded with help of the Spark `PipelineModel` class:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载`empTitleTransformer`-它是一个常规的Spark管道转换器，可以借助Spark的`PipelineModel`类加载：
- en: '[PRE68]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The `loanStatus` and `intRate` models were saved in the H2O MOJO format. To
    load them, it is necessary to use the `MojoModel` class:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '`loanStatus`和`intRate`模型以H2O MOJO格式保存。要加载它们，需要使用`MojoModel`类：'
- en: '[PRE69]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'At this point, we have all the necessary artifacts ready; however, we cannot
    use H2O MOJO models directly to transform Spark streams. However, we can wrap
    them into a Spark transformer. We have already defined a transformer called UDFTransfomer
    in [Chapter 4](part0080.html#2C9D00-d18ba71168a441bd917775fac13ca893), *Predicting
    Movie Reviews Using NLP and Spark Streaming* so we will follow a similar pattern:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经准备好所有必要的工件；但是，我们不能直接使用H2O MOJO模型来转换Spark流。但是，我们可以将它们包装成Spark transformer。我们已经在[第4章](part0080.html#2C9D00-d18ba71168a441bd917775fac13ca893)中定义了一个名为UDFTransfomer的转换器，*使用NLP和Spark
    Streaming预测电影评论*，因此我们将遵循类似的模式：
- en: '[PRE70]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The defined `MojoTransformer` supports binomial and regression MOJO models.
    It accepts a Spark dataset and enriches it by new columns: two columns holding
    true/false probabilities for binomial models and a single column representing
    the predicted value of the regression model. This is reflected in `transform` method,
    which is using the MOJO wrapper `modelUdf` to transform the input dataset:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 定义的`MojoTransformer`支持二项式和回归MOJO模型。它接受一个Spark数据集，并通过新列对其进行丰富：对于二项式模型，两列包含真/假概率，对于回归模型，一个列代表预测值。这体现在`transform`方法中，该方法使用MOJO包装器`modelUdf`来转换输入数据集：
- en: 'dataset.select(*col*(**"*"**), *modelUdf*(*struct*(args: _*)).as(*outputCol*))'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 'dataset.select(*col*(**"*"**), *modelUdf*(*struct*(args: _*)).as(*outputCol*))'
- en: The `modelUdf` model implements the transformation from the data represented
    as Spark Row into a format accepted by MOJO, the call of MOJO, and the transformation
    of the MOJO prediction into a Spark Row format.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '`modelUdf`模型实现了将数据表示为Spark Row转换为MOJO接受的格式，调用MOJO以及将MOJO预测转换为Spark Row格式的转换。'
- en: 'The defined `MojoTransformer` allows us to wrap the loaded MOJO models into
    the Spark transformer API:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 定义的`MojoTransformer`允许我们将加载的MOJO模型包装成Spark transformer API：
- en: '[PRE71]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'At this point, we have all the necessary building blocks ready, and we can
    apply them on the input stream:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经准备好所有必要的构建模块，并且可以将它们应用于输入流：
- en: '[PRE72]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The code first calls the shared library function `basicDataCleanup` and then
    transform the `desc` column with another shared library function, `descWordEncoderUdf`:
    both cases are implemented on top of Spark DataSet SQL interfaces. The remaining
    steps will apply defined transformers. Again, we can explore the structure of
    the transformed stream and verify that it contains fields introduced by our transformations:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先调用共享库函数`basicDataCleanup`，然后使用另一个共享库函数`descWordEncoderUdf`转换`desc`列：这两种情况都是基于Spark
    DataSet SQL接口实现的。其余步骤将应用定义的转换器。同样，我们可以探索转换后的流的结构，并验证它是否包含我们转换引入的字段：
- en: '[PRE73]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The output is as follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00205.jpeg)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00205.jpeg)'
- en: 'We can see that there are several new fields in the schema: representation
    of the empTitle cluster, the vector of denominating words, and model predictions.
    Probabilities are from the loab status model and the real value from the interest
    rate model.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模式中有几个新字段：empTitle集群的表示，命名词向量和模型预测。概率来自贷款状态模型，实际值来自利率模型。
- en: Stream output
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流输出
- en: 'Spark provides the so-called "Output Sinks" for streams. The sink defines how
    and where the stream is written; for example, as a parquet file or as a in-memory
    table. However, for our application, we will simply show the stream output in
    the console:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: Spark为流提供了所谓的“输出接收器”。接收器定义了流如何以及在哪里写入；例如，作为parquet文件或作为内存表。但是，对于我们的应用程序，我们将简单地在控制台中显示流输出：
- en: '[PRE74]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The preceding code directly starts the stream processing and waits until the
    termination of the application. The application simply process every new file
    in a given folder (in our case, given by the environment variable, `APPDATADIR`).
    For example, given a file with five loan applications, the stream produces a table
    with five scored events:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码直接启动了流处理，并等待应用程序终止。该应用程序简单地处理给定文件夹中的每个新文件（在我们的情况下，由环境变量`APPDATADIR`给出）。例如，给定一个包含五个贷款申请的文件，流会生成一个包含五个评分事件的表：
- en: '![](img/00206.jpeg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00206.jpeg)'
- en: 'The important part of the event is represented by the last columns, which contain
    predicted values:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 事件的重要部分由最后一列表示，其中包含预测值：
- en: '![](img/00207.jpeg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00207.jpeg)'
- en: 'If we write another file with a single loan application into the folder, the
    application will show another scored batch:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在文件夹中再写入一个包含单个贷款申请的文件，应用程序将显示另一个评分批次：
- en: '![](img/00208.jpeg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00208.jpeg)'
- en: In this way, we can deploy trained models and corresponding data-processing
    operations and let them score actual events. Of course, we just demonstrated a
    simple use case; a real-life scenario would be much more complex involving a proper
    model validation, A/B testing with the currently used models, and the storing
    and versioning of the models.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以部署训练模型和相应的数据处理操作，并让它们评分实际事件。当然，我们只是演示了一个简单的用例；实际情况会复杂得多，涉及适当的模型验证，当前使用模型的A/B测试，以及模型的存储和版本控制。
- en: Summary
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter summarizes everything you learned throughout the book with end-to-end
    examples. We analyzed the data, transformed it, performed several experiments
    to figure out how to set up the model-training pipeline, and built models. The
    chapter also stresses on the need for well-designed code, which can be shared
    across several projects. In our example, we created a shared library that was
    used at the time of training as well as being utilized during the scoring time.
    This was demonstrated on the critical operation called "model deployment" when
    trained models and related artifacts are used to score unseen data.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了整本书中你学到的一切，通过端到端的示例。我们分析了数据，对其进行了转换，进行了几次实验，以找出如何设置模型训练流程，并构建了模型。本章还强调了需要良好设计的代码，可以在多个项目中共享。在我们的示例中，我们创建了一个共享库，用于训练时和评分时使用。这在称为“模型部署”的关键操作上得到了证明，训练好的模型和相关工件被用来评分未知数据。
- en: This chapter also brings us to the end of the book. Our goal was to show that
    solving machine learning challenges with Spark is mainly about experimentation
    with data, parameters, models, debugging data / model-related issues, writing
    code that can be tested and reused, and having fun by getting surprising data
    insights and observations.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还将我们带到了书的结尾。我们的目标是要展示，用Spark解决机器学习挑战主要是关于对数据、参数、模型进行实验，调试数据/模型相关问题，编写可测试和可重用的代码，并通过获得令人惊讶的数据洞察和观察来获得乐趣。
