- en: Text Analytics Using Spark ML
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark ML进行文本分析
- en: '"Programs must be written for people to read, and only incidentally for machines
    to execute."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “程序必须为人们阅读而编写，只是偶然为机器执行。”
- en: '- Harold Abelson'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- Harold Abelson'
- en: In this chapter, we will discuss the wonderful field of text analytics using
    Spark ML. Text analytics is a wide area in machine learning and is useful in many
    use cases, such as sentiment analysis, chat bots, email spam detection, and natural
    language processing. We will learn how to use Spark for text analysis with a focus
    on use cases of text classification using a 10,000 sample set of Twitter data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论使用Spark ML进行文本分析的精彩领域。文本分析是机器学习中的一个广泛领域，在许多用例中非常有用，如情感分析、聊天机器人、电子邮件垃圾邮件检测和自然语言处理。我们将学习如何使用Spark进行文本分析，重点关注使用包含1万个Twitter数据样本的文本分类用例。
- en: 'In a nutshell, the following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章将涵盖以下主题：
- en: Understanding text analytics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解文本分析
- en: Transformers and Estimators
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换器和估计器
- en: Tokenizer
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记器
- en: StopWordsRemover
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StopWordsRemover
- en: NGrams
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N-Grams
- en: TF-IDF
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Word2Vec
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec
- en: CountVectorizer
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CountVectorizer
- en: Topic modeling using LDA
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LDA进行主题建模
- en: Implementing text classification
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施文本分类
- en: Understanding text analytics
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解文本分析
- en: 'We have explored the world of machine learning and Apache Spark''s support
    for machine learning in the last few chapters. As we discussed, machine learning
    has a workflow, which is explained in the following steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们已经探索了机器学习的世界和Apache Spark对机器学习的支持。正如我们讨论的那样，机器学习有一个工作流程，下面解释了以下步骤：
- en: Loading or ingesting data.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载或摄取数据。
- en: Cleansing the data.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清洗数据。
- en: Extracting features from the data.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据中提取特征。
- en: Training a model on the data to generate desired outcomes based on features.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据上训练模型，以生成基于特征的期望结果。
- en: Evaluate or predict some outcome based on the data.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据数据评估或预测某种结果。
- en: 'A simplified view of a typical pipeline is as shown in the following diagram:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 典型管道的简化视图如下图所示：
- en: '![](img/00310.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00310.jpeg)'
- en: Hence, there are several stages of transformation of data possible before the
    model is trained and then subsequently deployed. Moreover, we should expect refinement
    of the features and model attributes. We could even explore a completely different
    algorithm repeating the entire sequence of tasks as part of a new workflow.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在模型训练之前和随后部署之前，可能存在多个数据转换阶段。此外，我们应该期望特征和模型属性的改进。我们甚至可以探索完全不同的算法，重复整个任务序列作为新工作流的一部分。
- en: 'A pipeline of steps can be created using several steps of transformation, and
    for this purpose, we use a **domain specific language** (**DSL**) to define the
    nodes (data transformation steps) to create a **DAG** (**Directed Acyclic Graph**)
    of nodes. Hence, the ML pipeline is a sequence of Transformers and Estimators
    to fit a Pipeline model to an input dataset. Each stage in the pipeline is known
    as *Pipeline stage*, which are listed as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用多个转换步骤创建一个管道，并且为此目的，我们使用特定领域的语言（DSL）来定义节点（数据转换步骤）以创建节点的有向无环图（DAG）。因此，ML管道是一系列转换器和估计器，用于将管道模型拟合到输入数据集。管道中的每个阶段称为*管道阶段*，列举如下：
- en: Estimator
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计器
- en: Model
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型
- en: Pipeline
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道
- en: Transformer
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器
- en: Predictor
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测器
- en: When you look at a line of text, we see sentences, phrases, words, nouns, verbs,
    punctuation, and so on, which when put together, have a meaning and purpose. Humans
    are very good at understanding sentences, words, and slangs and annotations or
    contexts extremely well. This comes from years of practice and learning how to
    read/write, proper grammar, punctuation, exclamations, and so on. So, how can
    we write a computer program to try to replicate this kind of capability?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看一行文本时，我们看到句子、短语、单词、名词、动词、标点等等，这些放在一起时有意义和目的。人类非常擅长理解句子、单词和俚语，以及注释或上下文。这来自多年的练习和学习如何阅读/写作、正确的语法、标点、感叹号等等。那么，我们如何编写计算机程序来尝试复制这种能力呢？
- en: Text analytics
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分析
- en: Text analytics is the way to unlock the meaning from a collection of text. By
    using various techniques and algorithms to process and analyze the text data,
    we can uncover patterns and themes in the data. The goal of all this is to make
    sense of the unstructured text in order to derive contextual meaning and relationships.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析是从一系列文本中解锁含义的方法。通过使用各种技术和算法来处理和分析文本数据，我们可以发现数据中的模式和主题。所有这些的目标是理解非结构化文本，以便推导出上下文的含义和关系。
- en: Text analytics utilizes several broad categories of techniques, which we will
    cover next.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析利用了几种广泛的技术类别，接下来我们将介绍。
- en: Sentiment analysis
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: Analyzing the political opinions of people on Facebook, Twitter, and other social
    media is a good example of sentiment analysis. Similarly, analyzing the reviews
    of restaurants on Yelp is also another great example of sentiment analysis.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 分析人们在Facebook、Twitter和其他社交媒体上的政治观点是情感分析的一个很好的例子。同样，分析Yelp上餐厅的评论也是情感分析的另一个很好的例子。
- en: '**Natural Language Processing** (**NLP**) frameworks and libraries, such as
    OpenNLP and Stanford NLP, are typically used to implement sentiment analysis.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）框架和库，如OpenNLP和Stanford NLP，通常用于实现情感分析。
- en: Topic modeling
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模
- en: Topic modeling is a useful technique for detecting the topics or themes in a
    corpus of documents. This is an unsupervised algorithm, which can find themes
    in a set of documents. An example is to detect topics covered in a news article.
    Another example is to detect the ideas in a patent application.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模是一种用于检测语料库中主题或主题的有用技术。这是一种无监督算法，可以在一组文档中找到主题。一个例子是检测新闻文章中涵盖的主题。另一个例子是检测专利申请中的想法。
- en: The **latent dirichlet allocation** (**LDA**) is a popular clustering model
    using unsupervised algorithm, while **latent semantic analysis** (**LSA**) uses
    a probabilistic model on co-occurrence data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（LDA）是使用无监督算法的流行聚类模型，而潜在语义分析（LSA）使用共现数据的概率模型。
- en: TF-IDF (term frequency - inverse document frequency)
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF（词项频率 - 逆文档频率）
- en: TF-IDF measures how frequently words appear in documents and the relative frequency
    across the set of documents. This information can be used in building classifiers
    and predictive models. The examples are spam classification, chat conversations,
    and so on.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF衡量单词在文档中出现的频率以及在文档集中的相对频率。这些信息可以用于构建分类器和预测模型。例如垃圾邮件分类、聊天对话等。
- en: Named entity recognition (NER)
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）
- en: Named entity recognition detects the usage of words and nouns in sentences to
    extract information about persons, organizations, locations, and so on. This gives
    important contextual information on the actual content of the documents rather
    than just treating words as the primary entities.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别检测句子中单词和名词的使用，以提取有关个人、组织、位置等信息。这提供了有关文档实际内容的重要上下文信息，而不仅仅将单词视为主要实体。
- en: Stanford NLP and OpenNLP have implementation for NER algorithms.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福NLP和OpenNLP都实现了NER算法。
- en: Event extraction
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事件提取
- en: Event extraction expands on the NER establishing relationships around the entities
    detected. This can be used to make inferences on the relationship between two
    entities. Hence, there is an additional layer of semantic understanding to make
    sense of the document content.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 事件提取扩展了NER，建立了围绕检测到的实体的关系。这可以用于推断两个实体之间的关系。因此，还有一个额外的语义理解层来理解文档内容。
- en: Transformers and Estimators
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器和估计器
- en: '**Transformer** is a function object that transforms one dataset to another
    by applying the transformation logic (function) to the input dataset yielding
    an output dataset. There are two types of Transformers the standard Transformer
    and the Estimator Transformer.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**变压器**是一个函数对象，通过将变换逻辑（函数）应用于输入数据集，将一个数据集转换为另一个数据集。有两种类型的变压器，标准变压器和估计器变压器。'
- en: Standard Transformer
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准变压器
- en: A standard Transformer transforms the input dataset into the output dataset,
    explicitly applying transformation function to the input data. There is no dependency
    on the input data other than reading the input column and generating the output
    column.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 标准变压器将输入数据集显式地转换为输出数据集，应用变换函数到输入数据上。除了读取输入列并生成输出列之外，不依赖于输入数据。
- en: 'Such Transformers are invoked as shown next:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变压器如下所示被调用：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Examples of standard Transformers are as follows and will be explained in detail
    in the subsequent sections:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 标准变压器的示例如下，并将在后续部分详细解释：
- en: '`Tokenizer`: This splits sentences into words using space as the delimiter'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tokenizer`：这使用空格作为分隔符将句子分割成单词'
- en: '`RegexTokenizer`: This splits sentences into words using regular expressions
    to split'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RegexTokenizer`：这使用正则表达式将句子分割成单词'
- en: '`StopWordsRemover`: This removes commonly used stop words from the list of
    words'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StopWordsRemover`：这从单词列表中移除常用的停用词'
- en: '`Binarizer`: This converts the strings to binary numbers 0/1'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Binarizer`：这将字符串转换为二进制数字0/1'
- en: '`NGram`: This creates N word phrases from the sentences'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NGram`：这从句子中创建N个词组'
- en: '`HashingTF`: This creates Term frequency counts using hash table to index the
    words'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HashingTF`：这使用哈希表创建词项频率计数以索引单词'
- en: '`SQLTransformer`: This implements the transformations, which are defined by
    SQL statements'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SQLTransformer`：这实现了由SQL语句定义的转换'
- en: '`VectorAssembler`: This combines a given list of columns into a single vector
    column'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VectorAssembler`：这将给定的列列表合并成一个单独的向量列'
- en: 'The diagram of a standard Transformer is as follows, where the input column
    from an input dataset is transformed into an output column generating the output
    dataset:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 标准变压器的图示如下，其中来自输入数据集的输入列被转换为生成输出数据集的输出列：
- en: '![](img/00247.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00247.jpeg)'
- en: Estimator Transformer
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计器变压器
- en: An Estimator Transformer transforms the input dataset into the output dataset
    by first generating a Transformer based on the input dataset. Then the Transformer
    processes the input data, reading the input column and generating the output column
    in the output dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器变压器通过首先基于输入数据集生成一个变压器，然后变压器处理输入数据，读取输入列并在输出数据集中生成输出列来将输入数据集转换为输出数据集。
- en: 'Such Transformers are invoked as shown next:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变压器如下所示被调用：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The examples of Estimator Transformers are as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器变压器的示例如下：
- en: IDF
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IDF
- en: LDA
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA
- en: Word2Vec
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec
- en: 'The diagram of an Estimator Transformer is as follows, where the input column
    from an input dataset is transformed into an output column generating the output
    dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器变压器的图示如下，其中来自输入数据集的输入列被转换为生成输出数据集的输出列：
- en: '![](img/00287.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00287.jpeg)'
- en: 'In the next few sections, we will look deeper into text analytics using a simple
    example dataset, which consists of lines of text (sentences), as shown in the
    following screenshot:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将深入研究使用一个简单示例数据集进行文本分析，该数据集由文本行（句子）组成，如下截图所示：
- en: '![](img/00330.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00330.jpeg)'
- en: The upcoming code is used to load the text data into the input dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 即将出现的代码用于将文本数据加载到输入数据集中。
- en: Initialize a sequence of sentences called lines using a sequence of pairs of
    ID and text as shown next.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用下面显示的ID和文本对序列初始化一个名为lines的句子序列。
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, invoke the `createDataFrame()` function to create a DataFrame from the
    sequence of sentences we saw earlier.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，调用`createDataFrame()`函数从我们之前看到的句子序列创建一个DataFrame。
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now you can see the newly created dataset, which shows the Sentence DataFrame
    containing two column IDs and sentences.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以看到新创建的数据集，其中显示了包含两列ID和句子的句子DataFrame。
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tokenization
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记化
- en: '**Tokenizer** converts the input string into lowercase and then splits the
    string with whitespaces into individual tokens. A given sentence is split into
    words either using the default space delimiter or using a customer regular expression
    based Tokenizer. In either case, the input column is transformed into an output
    column. In particular, the input column is usually a String and the output column
    is a Sequence of Words.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tokenizer**将输入字符串转换为小写，然后使用空格将字符串分割为单独的标记。给定的句子被分割成单词，可以使用默认的空格分隔符，也可以使用基于正则表达式的分词器。在任何情况下，输入列都会被转换为输出列。特别是，输入列通常是一个字符串，输出列是一个单词序列。'
- en: 'Tokenizers are available by importing two packages shown next, the `Tokenizer`
    and the `RegexTokenize`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过导入下面显示的两个包，可以使用分词器：`Tokenizer`和`RegexTokenize`：
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'First, you need to initialize a `Tokenizer` specifying the input column and
    the output column:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要初始化一个`Tokenizer`，指定输入列和输出列：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, invoking the `transform()` function on the input dataset yields an output
    dataset:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在输入数据集上调用`transform()`函数会产生一个输出数据集：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following is the output dataset showing the input column IDs, sentence,
    and the output column words, which contain the sequence of words:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出数据集，显示了输入列ID、句子和包含单词序列的输出列words：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'On the other hand, if you wanted to set up a regular expression based `Tokenizer`,
    you have to use the `RegexTokenizer` instead of `Tokenizer`. For this, you need
    to initialize a `RegexTokenizer` specifying the input column and the output column
    along with the regex pattern to be used:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您想要设置基于正则表达式的`Tokenizer`，您需要使用`RegexTokenizer`而不是`Tokenizer`。为此，您需要初始化一个`RegexTokenizer`，指定输入列和输出列，以及要使用的正则表达式模式：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, invoking the `transform()` function on the input dataset yields an output
    dataset:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在输入数据集上调用`transform()`函数会产生一个输出数据集：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following is the output dataset showing the input column IDs, sentence,
    and the output column `regexWordsDF`, which contain the sequence of words:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出数据集，显示了输入列ID、句子和包含单词序列的输出列`regexWordsDF`：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The diagram of a `Tokenizer` is as follows, wherein the sentence from the input
    text is split into words using the space delimiter:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tokenizer`的图示如下，其中来自输入文本的句子使用空格分隔成单词：'
- en: '![](img/00150.jpeg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00150.jpeg)'
- en: StopWordsRemover
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StopWordsRemover
- en: '`StopWordsRemover` is a Transformer that takes a `String` array of words and
    returns a `String` array after removing all the defined stop words. Some examples
    of stop words are I, you, my, and, or, and so on which are fairly commonly used
    in the English language. You can override or extend the set of stop words to suit
    the purpose of the use case. Without this cleansing process, the subsequent algorithms
    might be biased because of the common words.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`StopWordsRemover`是一个转换器，它接受一个`String`数组的单词，并在删除所有定义的停用词后返回一个`String`数组。一些停用词的例子是I，you，my，and，or等，在英语中非常常用。您可以覆盖或扩展停用词集以适应用例的目的。如果没有进行这种清洗过程，后续的算法可能会因为常用单词而产生偏见。'
- en: 'In order to invoke `StopWordsRemover`, you need to import the following package:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调用`StopWordsRemover`，您需要导入以下包：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'First, you need to initialize a `StopWordsRemover` , specifying the input column
    and the output column. Here, we are choosing the words column created by the `Tokenizer`
    and generate an output column for the filtered words after removal of stop words:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要初始化一个`StopWordsRemover`，指定输入列和输出列。在这里，我们选择了`Tokenizer`创建的单词列，并生成了一个输出列，用于删除停用词后的过滤单词：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, invoking the `transform()` function on the input dataset yields an output
    dataset:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在输入数据集上调用`transform()`函数会产生一个输出数据集：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is the output dataset showing the input column IDs, sentence,
    and the output column `filteredWords`, which contains the sequence of words:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出数据集，显示了输入列ID、句子和包含单词序列的输出列`filteredWords`：
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following is the output dataset showing just the sentence and the `filteredWords`,
    which contains the sequence of filtered words:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出数据集，只显示了句子和`filteredWords`，其中包含过滤后的单词序列：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The diagram of the `StopWordsRemover` is as follows, which shows the words
    filtered to remove stop words such as I, should, some, and before:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`StopWordsRemover`的图示如下，显示了过滤后的单词，删除了诸如I，should，some和before等停用词：'
- en: '![](img/00021.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00021.jpeg)'
- en: 'Stop words are set by default, but can be overridden or amended very easily,
    as shown in the following code snippet, where we will remove hello from the filtered
    words considering hello as a stop word:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词默认设置，但可以非常容易地被覆盖或修改，如下面的代码片段所示，在这里我们将从过滤后的单词中删除hello，将hello视为停用词：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: NGrams
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NGrams
- en: NGrams are word combinations created as sequences of words. N stands for the
    number of words in the sequence. For example, 2-gram is two words together, 3-gram
    is three words together. `setN()` is used to specify the value of `N`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: NGrams是由单词组合而成的单词序列。N代表序列中的单词数。例如，2-gram是两个单词在一起，3-gram是三个单词在一起。`setN()`用于指定`N`的值。
- en: 'In order to generate NGrams, you need to import the package:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成NGrams，您需要导入该包：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'First, you need to initialize an `NGram` generator specifying the input column
    and the output column. Here, we are choosing the filtered words column created
    by the `StopWordsRemover` and generating an output column for the filtered words
    after removal of stop words:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要初始化一个`NGram`生成器，指定输入列和输出列。在这里，我们选择了`StopWordsRemover`创建的过滤后的单词列，并生成了一个输出列，用于删除停用词后的过滤单词：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, invoking the `transform()` function on the input dataset yields an output
    dataset:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在输入数据集上调用`transform()`函数会产生一个输出数据集：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following is the output dataset showing the input column ID, sentence,
    and the output column `ngram`, which contain the sequence of n-grams:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出数据集，显示了输入列ID、句子和包含n-gram序列的输出列`ngram`：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following is the output dataset showing the sentence and 2-grams:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出数据集，显示了句子和2-gram：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The diagram of an NGram is as follows, which shows 2-grams generated from the
    sentence after tokenizing and removing stop words:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: NGram的图如下所示，显示了在分词和去除停用词后生成的2-gram：
- en: '![](img/00163.jpeg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00163.jpeg)'
- en: TF-IDF
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF
- en: TF-IDF stands for term frequency-inverse document frequency, which measures
    how important a word is to a document in a collection of documents. It is used
    extensively in informational retrieval and reflects the weightage of the word
    in the document. The TF-IDF value increases in proportion to the number of occurrences
    of the words otherwise known as frequency of the word/term and consists of two
    key elements, the term frequency and the inverse document frequency.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF代表词项频率-逆文档频率，它衡量了一个词在文档集合中对于一个文档的重要性。它在信息检索中被广泛使用，反映了词在文档中的权重。TF-IDF值与词的出现次数成正比增加，也就是词频，由词频和逆文档频率两个关键元素组成。
- en: TF is the term frequency, which is the frequency of a word/term in the document.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: TF是词项频率，即文档中词/术语的频率。
- en: For a term *t*, *tf* measures the number of times term *t* occurs in document
    *d*. *tf* is implemented in Spark using hashing where a term is mapped into an
    index by applying a hash function.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个术语*t*，*tf*衡量了术语*t*在文档*d*中出现的次数。*tf*在Spark中使用哈希实现，其中一个术语通过应用哈希函数映射到一个索引。
- en: 'IDF is the inverse document frequency, which represents the information a term
    provides about the tendency of the term to appear in documents. IDF is a log-scaled
    inverse function of documents containing the term:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: IDF是逆文档频率，代表了一个术语提供的关于该术语在文档中出现倾向的信息。 IDF是包含该术语的文档的对数缩放的逆函数：
- en: IDF = TotalDocuments/Documents containing Term
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: IDF = 总文档数/包含该词的文档数
- en: 'Once we have *TF* and *IDF*, we can compute the *TF-IDF* value by multiplying
    the *TF* and *IDF*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了*TF*和*IDF*，我们可以通过将它们相乘来计算*TF-IDF*值：
- en: TF-IDF = TF * IDF
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF = TF * IDF
- en: We will now look at how we can generate *TF* using the HashingTF Transformer
    in Spark ML.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看一下如何使用Spark ML中的HashingTF Transformer生成*TF*。
- en: HashingTF
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HashingTF
- en: '**HashingTF** is a Transformer, which takes a set of terms and converts them
    into vectors of fixed length by hashing each term using a hash function to generate
    an index for each term. Then, term frequencies are generated using the indices
    of the hash table.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**HashingTF**是一个Transformer，它接受一组术语，并通过使用哈希函数对每个术语进行哈希，将它们转换为固定长度的向量。然后，使用哈希表的索引生成词项频率。'
- en: In Spark, the HashingTF uses the **MurmurHash3** algorithm to hash terms.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，HashingTF使用**MurmurHash3**算法对术语进行哈希。
- en: 'In order to use `HashingTF`, you need to import the following package:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用`HashingTF`，你需要导入以下包：
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'First, you need to initialize a `HashingTF` specifying the input column and
    the output column. Here, we choose the filtered words column created by the `StopWordsRemover`
    Transformer and generate an output column `rawFeaturesDF`. We also choose the
    number of features as 100:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要初始化一个`HashingTF`，指定输入列和输出列。在这里，我们选择了`StopWordsRemover` Transformer创建的过滤词列，并生成一个输出列`rawFeaturesDF`。我们还选择了100个特征：
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, invoking the `transform()` function on the input dataset yields an output
    dataset:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在输入数据集上调用`transform()`函数会产生一个输出数据集：
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following is the output dataset showing the input column IDs, sentence,
    and the output column `rawFeaturesDF`, which contains the features represented
    by a vector:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出数据集，显示了输入列ID、句子和输出列`rawFeaturesDF`，其中包含由向量表示的特征：
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Let's look at the preceding output to have a better understanding. If you just
    look at columns `filteredWords` and `rawFeatures` alone, you can see that,
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下前面的输出，以便更好地理解。如果你只看`filteredWords`和`rawFeatures`两列，你会发现，
- en: The array of words `[hello, there, like, book, and far]` is transformed to raw
    feature vector `(100,[30,48,70,93],[2.0,1.0,1.0,1.0])`.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单词数组`[hello, there, like, book, and far]`被转换为原始特征向量`(100,[30,48,70,93],[2.0,1.0,1.0,1.0])`。
- en: The array of words `(book, stores, coffee, go, book, and store)` is transformed
    to raw feature vector `(100,[43,48,51,77,93],[1.0,1.0,1.0,1.0,2.0])`.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单词数组`(book, stores, coffee, go, book, and store)`被转换为原始特征向量`(100,[43,48,51,77,93],[1.0,1.0,1.0,1.0,2.0])`。
- en: So, what does the vector represent here? The underlying logic is that each word
    is hashed into an integer and counted for the number of occurrences in the word
    array.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个向量代表什么呢？其基本逻辑是，每个单词被哈希成一个整数，并计算在单词数组中出现的次数。
- en: Spark internally uses a `hashMap` for this `mutable.HashMap.empty[Int, Double]`,
    which stores the hash value of each word as `Integer` key and the number of occurrences
    as double value. Double is used so that we can use it in conjunction with IDF
    (we'll talk about it in the next section). Using this map, the array `[book, stores,
    coffee, go, book, store]` can be seen as `[hashFunc(book), hashFunc(stores), hashFunc(coffee),
    hashFunc(go), hashFunc(book), hashFunc(store)]`*,* which is equal to `[43,48,51,77,93]`*.*
    Then, if you count the number of occurrences too, that is, `book-2, coffee-1,go-1,store-1,stores-1`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在内部使用`mutable.HashMap.empty[Int, Double]`来存储每个单词的哈希值作为`Integer`键和出现次数作为double值。使用Double是为了能够与IDF一起使用（我们将在下一节讨论）。使用这个映射，数组`[book,
    stores, coffee, go, book, store]`可以看作是`[hashFunc(book), hashFunc(stores), hashFunc(coffee),
    hashFunc(go), hashFunc(book), hashFunc(store)]`，即`[43,48,51,77,93]`。然后，如果你也计算出现次数，即`book-2,
    coffee-1,go-1,store-1,stores-1`。
- en: Combining the preceding information, we can generate a vector `(numFeatures,
    hashValues, Frequencies)`**,** which in this case will be `(100,[43,48,51,77,93],[1.0,1.0,1.0,1.0,2.0])`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 结合前面的信息，我们可以生成一个向量`(numFeatures, hashValues, Frequencies)`，在这种情况下将是`(100,[43,48,51,77,93],[1.0,1.0,1.0,1.0,2.0])`。
- en: Inverse Document Frequency (IDF)
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逆文档频率（IDF）
- en: '**Inverse Document Frequency** (**IDF**) is an estimator, which is fit onto
    a dataset and then generates features by scaling the input features. Hence, IDF
    works on output of a HashingTF Transformer.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**逆文档频率**（**IDF**）是一个估计器，它适用于数据集，然后通过缩放输入特征生成特征。因此，IDF作用于HashingTF Transformer的输出。'
- en: 'In order to invoke IDF, you need to import the package:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调用IDF，您需要导入该包：
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'First, you need to initialize an `IDF` specifying the input column and the
    output column. Here, we are choosing the words column `rawFeatures` created by
    the HashingTF and generate an output column feature:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要初始化一个`IDF`，指定输入列和输出列。在这里，我们选择由HashingTF创建的`rawFeatures`单词列，并生成一个输出列特征：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, invoking the `fit()` function on the input dataset yields an output Transformer:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在输入数据集上调用`fit()`函数会产生一个输出Transformer：
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Further, invoking the `transform()` function on the input dataset yields an
    output dataset:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在输入数据集上调用`transform()`函数会产生一个输出数据集：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following is the output dataset showing the input column ID and the output
    column features, which contain the vector of scaled features produced by HashingTF
    in the previous transformation:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是显示输入列ID和输出列特征的输出数据集，其中包含由前一个转换中的HashingTF生成的缩放特征的向量：
- en: '[PRE31]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following is the output dataset showing the input column IDs, sentence,
    `rawFeatures`, and the output column features, which contain the vector of scaled
    features produced by HashingTF in the previous transformation:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是显示输入列ID、句子、`rawFeatures`和输出列特征的输出数据集，其中包含由前一个转换中的HashingTF生成的缩放特征的向量：
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The diagram of the TF-IDF is as follows, which shows the generation of **TF-IDF
    Features**:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF的图如下，显示了**TF-IDF特征**的生成：
- en: '![](img/00089.jpeg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00089.jpeg)'
- en: Word2Vec
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec
- en: 'Word2Vec is a sophisticated neural network style natural language processing
    tool and uses a technique called **skip-grams** to convert a sentence of words
    into an embedded vector representation. Let''s look at an example of how this
    can be used by looking at a collection of sentences about animals:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是一种复杂的神经网络风格的自然语言处理工具，使用一种称为**skip-grams**的技术将单词句子转换为嵌入式向量表示。让我们看一个关于动物的句子集合的示例：
- en: A dog was barking
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一只狗在吠叫
- en: Some cows were grazing the grass
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些奶牛在吃草
- en: Dogs usually bark randomly
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狗通常会随机吠叫
- en: The cow likes grass
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 牛喜欢草
- en: Using neural network with a hidden layer (machine learning algorithm used in
    many unsupervised learning applications), we can learn (with enough examples)
    that *dog* and *barking* are related, *cow* and *grass* are related in the sense
    that they appear close to each other a lot, which is measured by probabilities.
    The output of `Word2vec` is a vector of `Double` features.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 使用具有隐藏层的神经网络（在许多无监督学习应用中使用的机器学习算法），我们可以学习（有足够的例子）*dog*和*barking*相关，*cow*和*grass*相关，它们经常一起出现，这是由概率来衡量的。`Word2vec`的输出是`Double`特征的向量。
- en: 'In order to invoke `Word2vec`, you need to import the package:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调用`Word2vec`，您需要导入该包：
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'First, you need to initialize a `Word2vec` Transformer specifying the input
    column and the output column. Here, we are choosing the words column created by
    the `Tokenizer` and generate an output column for the word vector of size 3:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要初始化一个`Word2vec` Transformer，指定输入列和输出列。在这里，我们选择由`Tokenizer`创建的单词列，并生成大小为3的单词向量输出列：
- en: '[PRE34]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, invoking the `fit()` function on the input dataset yields an output Transformer:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在输入数据集上调用`fit()`函数会产生一个输出Transformer：
- en: '[PRE35]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Further, invoking the `transform()` function on the input dataset yields an
    output dataset:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在输入数据集上调用`transform()`函数会产生一个输出数据集：
- en: '[PRE36]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following is the output dataset showing the input column IDs, sentence,
    and the output column `wordvector`:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是显示输入列ID、句子和输出列`wordvector`的输出数据集：
- en: '[PRE37]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The diagram of the **Word2Vec Features** is as follows, which shows the words
    being converted into a vector:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**Word2Vec特征**的图如下，显示了单词被转换为向量：'
- en: '![](img/00347.jpeg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00347.jpeg)'
- en: CountVectorizer
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CountVectorizer
- en: '`CountVectorizer` is used to convert a collection of text documents to vectors
    of token counts essentially producing sparse representations for the documents
    over the vocabulary. The end result is a vector of features, which can then be
    passed to other algorithms. Later on, we will see how to use the output from the
    `CountVectorizer` in LDA algorithm to perform topic detection.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer`用于将一组文本文档转换为标记计数的向量，从本质上为文档生成稀疏表示。最终结果是一组特征向量，然后可以传递给其他算法。稍后，我们将看到如何在LDA算法中使用`CountVectorizer`的输出执行主题检测。'
- en: 'In order to invoke `CountVectorizer`, you need to import the package:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调用`CountVectorizer`，您需要导入该包：
- en: '[PRE38]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'First, you need to initialize a `CountVectorizer` Transformer specifying the
    input column and the output column. Here, we are choosing the `filteredWords`
    column created by the `StopWordRemover` and generate output column features:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要初始化一个`CountVectorizer` Transformer，指定输入列和输出列。在这里，我们选择由`StopWordRemover`创建的`filteredWords`列，并生成输出列特征：
- en: '[PRE39]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, invoking the `fit()` function on the input dataset yields an output Transformer:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在输入数据集上调用`fit()`函数会产生一个输出Transformer：
- en: '[PRE40]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Further, invoking the `transform()` function on the input dataset yields an
    output dataset.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在输入数据集上调用`transform()`函数会产生一个输出数据集。
- en: '[PRE41]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following is the output dataset showing the input column IDs, sentence,
    and the output column features:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是显示输入列ID、句子和输出列特征的输出数据集：
- en: '[PRE42]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The diagram of a `CountVectorizer` is as follows, which shows the features
    generated from `StopWordsRemover` transformation:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer`的图如下，显示了从`StopWordsRemover`转换生成的特征：'
- en: '![](img/00205.jpeg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00205.jpeg)'
- en: Topic modeling using LDA
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LDA进行主题建模
- en: 'LDA is a topic model, which infers topics from a collection of text documents.
    LDA can be thought of as an unsupervised clustering algorithm as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一种主题模型，它从一组文本文档中推断主题。LDA可以被视为一种无监督的聚类算法，如下所示：
- en: Topics correspond to cluster centers and documents correspond to rows in a dataset
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题对应于聚类中心，文档对应于数据集中的行
- en: Topics and documents both exist in a feature space, where feature vectors are
    vectors of word counts
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题和文档都存在于特征空间中，特征向量是单词计数的向量
- en: Rather than estimating a clustering using a traditional distance, LDA uses a
    function based on a statistical model of how text documents are generated
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA不是使用传统距离估计聚类，而是使用基于文本文档生成的统计模型的函数
- en: 'In order to invoke LDA, you need to import the package:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调用LDA，您需要导入该包：
- en: '[PRE43]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '**Step 1.** First, you need to initialize an LDA model setting 10 topics and
    10 iterations of clustering:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1.** 首先，您需要初始化一个设置10个主题和10次聚类的LDA模型：'
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Step 2.** Next invoking the `fit()` function on the input dataset yields
    an output transformer:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2.** 在输入数据集上调用`fit()`函数会产生一个输出转换器：'
- en: '[PRE45]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Step 3.** Extract `logLikelihood`, which calculates a lower bound on the
    provided documents given the inferred topic:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3.** 提取`logLikelihood`，它计算了给定推断主题的文档的下限：'
- en: '[PRE46]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Step 4.** Extract `logPerplexity`, which calculates an upper bound on the
    perplexity of the provided documents given the inferred topics:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4.** 提取`logPerplexity`，它计算了给定推断主题的文档的困惑度的上限：'
- en: '[PRE47]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '**Step 5.** Now, we can use `describeTopics()` to get the topics generated
    by LDA:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5.** 现在，我们可以使用`describeTopics()`来获取LDA生成的主题：'
- en: '[PRE48]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '**Step 6.** The following is the output dataset showing the `topic`, `termIndices`,
    and `termWeights` computed by LDA model:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6.** 以下是LDA模型计算的`topic`、`termIndices`和`termWeights`的输出数据集：'
- en: '[PRE49]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The diagram of an LDA is as follows, which shows the topics created from the
    features of TF-IDF:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: LDA的图如下所示，显示了从TF-IDF特征创建的主题：
- en: '![](img/00175.jpeg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00175.jpeg)'
- en: Implementing text classification
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施文本分类
- en: Text classification is one of the most widely used paradigms in the field of
    machine learning and is useful in use cases such as spam detection and email classification
    and just like any other machine learning algorithm, the workflow is built of Transformers
    and algorithms. In the field of text processing, preprocessing steps such as stop-word
    removal, stemming, tokenizing, n-gram extraction, TF-IDF feature weighting come
    into play. Once the desired processing is complete, the models are trained to
    classify the documents into two or more classes.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类是机器学习领域中最常用的范例之一，在垃圾邮件检测和电子邮件分类等用例中非常有用，就像任何其他机器学习算法一样，工作流程由转换器和算法构建。在文本处理领域，预处理步骤如去除停用词、词干提取、标记化、n-gram提取、TF-IDF特征加权等起着重要作用。一旦所需的处理完成，模型就会被训练来将文档分类为两个或更多类别。
- en: Binary classification is the classification of inputting two output classes
    such as spam/not spam and a given credit card transaction is fraudulent or not.
    Multiclass classification can generate multiple output classes such as hot, cold,
    freezing, and rainy. There is another technique called Multilabel classification,
    which can generate multiple labels such as speed, safety, and fuel efficiency
    can be produced from descriptions of car features.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类是将输入分为两个输出类别，例如垃圾邮件/非垃圾邮件和给定的信用卡交易是否欺诈。多类分类可以生成多个输出类别，例如热、冷、冰冻和多雨。还有一种称为多标签分类的技术，可以从汽车特征的描述中生成多个标签，例如速度、安全性和燃油效率。
- en: For this purpose, we will using a 10k sample dataset of tweets and we will use
    the preceding techniques on this dataset. Then, we will tokenize the text lines
    into words, remove stop words, and then use `CountVectorizer` to build a vector
    of the words (features).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用一个包含10k条推文的样本数据集，并在该数据集上使用前述技术。然后，我们将将文本行标记为单词，删除停用词，然后使用`CountVectorizer`构建单词（特征）的向量。
- en: Then we will split the data into training (80%)-testing (20%) and train a Logistic
    Regression model. Finally, we will evaluate against the test data and look at
    how it is performed.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将数据分为训练（80%）-测试（20%），并训练一个逻辑回归模型。最后，我们将根据测试数据进行评估，并查看其表现如何。
- en: 'The steps in the workflow are shown in the following diagram:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程中的步骤如下图所示：
- en: '![](img/00177.jpeg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00177.jpeg)'
- en: '**Step 1.** Load the input text data containing 10k tweets along with label
    and ID:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1.** 加载包含10k条推文以及标签和ID的输入文本数据：'
- en: '[PRE50]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '**Step 2.** Convert the input lines to a DataFrame:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2.** 将输入行转换为DataFrame：'
- en: '[PRE51]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '**Step 3.** Transform the data into words using a `Tokenizer` with white space
    delimiter:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3.** 使用带有空格分隔符的`Tokenizer`将数据转换为单词：'
- en: '[PRE52]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '**Step 4.** Remove stop words and create a new DataFrame with the filtered
    words:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4.** 删除停用词并创建一个新的DataFrame，其中包含过滤后的单词：'
- en: '[PRE53]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**Step 5.** Create a feature vector from the filtered words:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5.** 从过滤后的单词创建特征向量：'
- en: '[PRE54]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '**Step 6.** Create the `inputData` DataFrame with just a label and the features:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6.** 创建只有标签和特征的`inputData` DataFrame：'
- en: '[PRE55]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '**Step 7.** Split the data using a random split into 80% training and 20% testing
    datasets:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7.** 使用随机拆分将数据拆分为80%的训练数据集和20%的测试数据集：'
- en: '[PRE56]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '**Step 8.** Create a Logistic Regression model:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8.** 创建一个逻辑回归模型：'
- en: '[PRE57]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '**Step 9.** Create a Logistic Regression model by fitting the `trainingData`:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9.** 通过拟合`trainingData`创建一个逻辑回归模型：'
- en: '[PRE58]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '**Step 10.** Examine the model summary especially `areaUnderROC`, which should
    be *> 0.90* for a good model:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤10.** 检查模型摘要，特别是`areaUnderROC`，对于一个好的模型应该是*> 0.90*：'
- en: '[PRE59]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '**Step 11.** Transform both training and testing datasets using the trained
    model:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤11.** 使用训练和测试数据集使用训练好的模型进行转换：'
- en: '[PRE60]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '**Step 12.** Count the number of records with matching label and prediction
    columns. They should match for correct model evaluation else they will mismatch:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤12.** 计算具有匹配标签和预测列的记录数。它们应该匹配以进行正确的模型评估，否则它们将不匹配：'
- en: '[PRE61]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The results can be put into a table as shown next:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以放入下表中：
- en: '| **Dataset** | **Total** | **label == prediction** | **label != prediction**
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **总数** | **标签==预测** | **标签！=预测** |'
- en: '| **Training** | 8048 | 8029 ( 99.76%) | 19 (0.24%) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| **训练** | 8048 | 8029 ( 99.76%) | 19 (0.24%) |'
- en: '| **Testing** | 1951 | 1334 (68.35%) | 617 (31.65%) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| **测试** | 1951 | 1334 (68.35%) | 617 (31.65%) |'
- en: While training data produced excellent matches, the testing data only had 68.35%
    match. Hence, there is room for improvement which can be done by exploring the
    model parameters.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然训练数据产生了很好的匹配，但测试数据只有68.35%的匹配。因此，还有改进的空间，可以通过探索模型参数来实现。
- en: Logistic regression is an easy-to-understand method for predicting a binary
    outcome using a linear combination of inputs and randomized noise in the form
    of a logistic random variable. Hence, Logistic Regression model can be tuned using
    several parameters. (The full set of parameters and how to tune such a Logistic
    Regression model is out of scope for this chapter.)
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种易于理解的方法，用于使用输入的线性组合和逻辑随机变量的随机噪声来预测二元结果。因此，可以使用多个参数来调整逻辑回归模型。（本章不涵盖调整此类逻辑回归模型的全部参数及方法。）
- en: 'Some parameters that can be used to tune the model are:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用于调整模型的一些参数是：
- en: 'Model hyperparameters include the following parameters:'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型超参数包括以下参数：
- en: '`elasticNetParam`: This parameter specifies how you would like to mix L1 and
    L2 regularization'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`elasticNetParam`：此参数指定您希望如何混合L1和L2正则化'
- en: '`regParam`: This parameter determines how the inputs should be regularized
    before being passed in the model'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`regParam`：此参数确定输入在传递到模型之前应如何正则化'
- en: 'Training parameters include the following parameters:'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练参数包括以下参数：
- en: '`maxIter`: This is total number of interactions before stopping'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxIter`：这是停止之前的总交互次数'
- en: '`weightCol`: This is the name of the weight column to weigh certain rows more
    than others'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weightCol`：这是用于对某些行进行加权的权重列的名称'
- en: 'Prediction parameters include the following parameter:'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测参数包括以下参数：
- en: '`threshold`: This is the probability threshold for binary prediction. This
    determines the minimum probability for a given class to be predicted.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold`：这是用于二元预测的概率阈值。这决定了预测给定类别的最小概率。'
- en: We have now seen how to build a simple classification model, so any new tweet
    can be labeled based on the training set. Logistic Regression is only one of the
    models that can be used.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经了解了如何构建一个简单的分类模型，因此可以根据训练集对任何新的推文进行标记。逻辑回归只是可以使用的模型之一。
- en: 'Other models which can be used in place of Logistic Regression are as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用于替代逻辑回归的其他模型如下：
- en: Decision trees
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Random Forest
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Gradient Boosted Trees
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: Multilayer Perceptron
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层感知器
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have introduced the world of text analytics using Spark
    ML with emphasis on text classification. We have learned about Transformers and
    Estimators. We have seen how Tokenizers can be used to break sentences into words,
    how to remove stop words, and generate n-grams. We also saw how to implement `HashingTF`
    and `IDF` to generate TF-IDF-based features. We also looked at `Word2Vec` to convert
    sequences of words into vectors.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了使用Spark ML进行文本分析的世界，重点是文本分类。我们了解了转换器和估计器。我们看到了如何使用分词器将句子分解为单词，如何去除停用词，并生成n-gram。我们还看到了如何实现`HashingTF`和`IDF`来生成基于TF-IDF的特征。我们还研究了`Word2Vec`如何将单词序列转换为向量。
- en: Then, we also looked at LDA, a popular technique used to generate topics from
    documents without knowing much about the actual text. Finally, we implemented
    text classification on the set of 10k tweets from the Twitter dataset to see how
    it all comes together using Transformers, Estimators, and the Logistic Regression
    model to perform binary classification.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们还研究了LDA，这是一种流行的技术，用于从文档中生成主题，而不需要了解实际文本。最后，我们在Twitter数据集的10k条推文集上实现了文本分类，以查看如何使用转换器、估计器和逻辑回归模型来执行二元分类。
- en: In the next chapter, we will dig even deeper toward tuning Spark applications
    for better performance.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地探讨调整Spark应用程序以获得更好性能。
