- en: More Data Mining and Machine Learning Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多数据挖掘和机器学习技术
- en: In this chapter, we talk about a few more data mining and machine learning techniques.
    We will talk about a really simple technique called **k-nearest neighbors** (**KNN**).
    We'll then use KNN to predict a rating for a movie. After that, we'll go on to
    talk about dimensionality reduction and principal component analysis. We'll also
    look at an example of PCA where we will reduce 4D data to two dimensions while
    still preserving its variance.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将讨论更多的数据挖掘和机器学习技术。我们将讨论一个称为**k最近邻居**（**KNN**）的非常简单的技术。然后，我们将使用KNN来预测电影的评级。之后，我们将继续讨论降维和主成分分析。我们还将看一个PCA的例子，其中我们将4D数据降低到两个维度，同时仍保留其方差。
- en: We'll then walk through the concept of data warehousing and see the advantages
    of the newer ELT process over the ETL process. We'll learn the fun concept of
    reinforcement learning and see the technique used behind the intelligent Pac-Man
    agent of the Pac-Man game. Lastly, we'll see some fancy terminology used for reinforcement
    learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将介绍数据仓库的概念，并了解新的ELT过程相对于ETL过程的优势。我们将学习强化学习的有趣概念，并了解智能吃豆人游戏的背后使用的技术。最后，我们将看到一些用于强化学习的花哨术语。
- en: 'We''ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: The concept of k-nearest neighbors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K最近邻居的概念
- en: Implementation of KNN to predict the rating of a movie
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNN的实施以预测电影的评级
- en: Dimensionality reduction and principal component analysis
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维和主成分分析
- en: Example of PCA with the Iris dataset
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鸢尾花数据集的PCA示例
- en: Data warehousing and ETL versus ELT
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据仓库和ETL与ELT
- en: What is reinforcement learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是强化学习
- en: The working behind the intelligent Pac-Man game
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能吃豆人游戏背后的工作
- en: Some fancy words used for reinforcement learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于强化学习的花哨术语
- en: K-nearest neighbors - concepts
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K最近邻居 - 概念
- en: Let's talk about a few data mining and machine learning techniques that employers
    expect you to know about. We'll start with a really simple one called KNN for
    short. You're going to be surprised at just how simple a good supervised machine
    learning technique can be. Let's take a look!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈雇主希望您了解的一些数据挖掘和机器学习技术。我们将从一个称为KNN的非常简单的技术开始。您会对一个好的监督式机器学习技术有多简单感到惊讶。让我们来看看！
- en: KNN sounds fancy but it's actually one of the simplest techniques out there!
    Let's say you have a scatter plot and you can compute the distance between any
    two points on that scatter plot. Let's assume that you have a bunch of data that
    you've already classified, that you can train the system from. If I have a new
    data point, all I do is look at the KNN based on that distance metric and let
    them all vote on the classification of that new point.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: KNN听起来很花哨，但实际上是最简单的技术之一！假设您有一个散点图，并且可以计算该散点图上任意两点之间的距离。假设您已经对一堆数据进行了分类，可以从中训练系统。如果我有一个新的数据点，我只需根据该距离度量查看KNN，并让它们全部对新点的分类进行投票。
- en: 'Let''s imagine that the following scatter plot is plotting movies. The squares
    represent science fiction movies, and the triangles represent drama movies. We''ll
    say that this is plotting ratings versus popularity, or anything else you can
    dream up:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象以下散点图正在绘制电影。方块代表科幻电影，三角形代表戏剧电影。我们将说这是根据评分与受欢迎程度绘制的，或者您可以想象其他任何东西：
- en: '![](img/cdac4f24-ef67-463c-ade8-14188ba538e5.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cdac4f24-ef67-463c-ade8-14188ba538e5.jpg)'
- en: Here, we have some sort of distance that we can compute based on rating and
    popularity between any two points on the scatter plot. Let's say a new point comes
    in, a new movie that we don't know the genre for. What we could do is set *K*
    to *3* and take the *3* nearest neighbors to this point on the scatter plot; they
    can all then vote on the classification of the new point/movie.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一种基于散点图上任意两点之间的评分和受欢迎程度计算的某种距离。假设有一个新点进来，一个我们不知道流派的新电影。我们可以将*K*设置为*3*，并取散点图上这一点的*3*个最近邻居；然后它们可以就新点/电影的分类进行投票。
- en: You can see if I take the three nearest neighbors (*K=3*), I have 2 drama movies
    and 1 science fiction movie. I would then let them all vote, and we would choose
    the classification of drama for this new point based on those 3 nearest neighbors.
    Now, if I were to expand this circle to include 5 nearest neighbors, that is *K=5*,
    I get a different answer. So, in that case I pick up 3 science fiction and 2 drama
    movies. If I let them all vote I would end up with a classification of science
    fiction for the new movie instead.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，如果我选择三个最近的邻居（*K=3*），我有2部戏剧电影和1部科幻电影。然后我会让它们全部投票，我们将根据这3个最近的邻居选择这个新点的戏剧分类。现在，如果我将这个圈扩大到包括5个最近的邻居，即*K=5*，我会得到一个不同的答案。在这种情况下，我挑选了3部科幻电影和2部戏剧电影。如果我让它们全部投票，我最终会得到一个新电影的科幻分类。
- en: Our choice of K can be very important. You want to make sure it's small enough
    that you don't go too far and start picking up irrelevant neighbors, but it has
    to be big enough to enclose enough data points to get a meaningful sample. So,
    often you'll have to use train/test or a similar technique to actually determine
    what the right value of *K* is for a given dataset. But, at the end of the day,
    you have to just start with your intuition and work from there.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择K可能非常重要。您要确保它足够小，以免走得太远并开始挑选无关的邻居，但它必须足够大，以包含足够的数据点以获得有意义的样本。因此，通常您将不得不使用训练/测试或类似的技术来实际确定给定数据集的*K*的正确值。但是，最终，您必须从直觉开始并从那里开始工作。
- en: That's all there is to it, it's just that simple. So, it is a very simple technique.
    All you're doing is literally taking the k nearest neighbors on a scatter plot,
    and letting them all vote on a classification. It does qualify as supervised learning
    because it is using the training data of a set of known points, that is, known
    classifications, to inform the classification of a new point.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这么简单，就是这么简单。因此，这是一种非常简单的技术。您所做的就是在散点图上找到k个最近邻，让它们全部对分类进行投票。它确实符合监督学习，因为它使用一组已知点的训练数据，即已知的分类，来指导新点的分类。
- en: 'But let''s do something a little bit more complicated with it and actually
    play around with movies, just based on their metadata. Let''s see if we can actually
    figure out the nearest neighbors of a movie based on just the intrinsic values
    of those movies, for example, the ratings for it, the genre information for it:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们对此做一些更复杂的事情，并且实际上根据它们的元数据玩弄电影。让我们看看是否可以实际上根据这些电影的内在值，例如其评分、类型信息，找出电影的最近邻：
- en: '![](img/22a520fc-4ebf-43d3-9427-5974ced51f65.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22a520fc-4ebf-43d3-9427-5974ced51f65.jpg)'
- en: 'In theory, we could recreate something similar to *Customers Who Watched This
    Item Also Watched* (the above image is a screenshot from Amazon) just using k-nearest
    Neighbors. And, I can take it one step further: once I identify the movies that
    are similar to a given movie based on the k-nearest Neighbors algorithm, I can
    let them all vote on a predicted rating for that movie.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们可以使用k最近邻算法重新创建类似于*观看此商品的客户还观看了*（上图是亚马逊的截图）的东西。而且，我可以再进一步：一旦我根据k最近邻算法确定了与给定电影相似的电影，我可以让它们全部对预测的电影评分进行投票。
- en: That's what we're going to do in our next example. So you now have the concepts
    of KNN, k-nearest neighbors. Let's go ahead and apply that to an example of actually
    finding movies that are similar to each other and using those nearest neighbor
    movies to predict the rating for another movie we haven't seen before.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们下一个示例要做的。所以现在您已经了解了KNN，k最近邻的概念。让我们继续并将其应用于实际找到彼此相似的电影，并使用这些最近邻的电影来预测我们以前没有看过的电影的评分。
- en: Using KNN to predict a rating for a movie
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用KNN来预测电影的评分
- en: Alright, we're going to actually take the simple idea of KNN and apply that
    to a more complicated problem, and that's predicting the rating of a movie given
    just its genre and rating information. So, let's dive in and actually try to predict
    movie ratings just based on the KNN algorithm and see where we get. So, if you
    want to follow along, go ahead and open up the `KNN.ipynb` and you can play along
    with me.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，我们将实际上采用KNN的简单思想，并将其应用于一个更复杂的问题，即仅根据其类型和评分信息预测电影的评分。因此，让我们深入研究并尝试仅基于KNN算法来预测电影评分，看看我们能得到什么。因此，如果您想跟着做，请打开`KNN.ipynb`，您可以和我一起玩。
- en: What we're going to do is define a distance metric between movies just based
    on their metadata. By metadata I just mean information that is intrinsic to the
    movie, that is, the information associated with the movie. Specifically, we're
    going to look at the genre classifications of the movie.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的是定义基于电影元数据的距离度量。通过元数据，我指的是仅与电影相关的信息，即与电影相关联的信息。具体来说，我们将查看电影的类型分类。
- en: Every movie in our `MovieLens` dataset has additional information on what genre
    it belongs to. A movie can belong to more than one genre, a genre being something
    like science fiction, or drama, or comedy, or animation. We will also look at
    the overall popularity of the movie, given by the number of people who rated it,
    and we also know the average rating of each movie. I can combine all this information
    together to basically create a metric of distance between two movies just based
    on rating information and genre information. Let's see what we get.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`MovieLens`数据集中的每部电影都有关于它所属类型的附加信息。一部电影可以属于多种类型，比如科幻、戏剧、喜剧或动画。我们还将查看电影的整体受欢迎程度，由评分人数给出，并且我们还知道每部电影的平均评分。我可以将所有这些信息结合在一起，基本上创建一个基于评分信息和类型信息的两部电影之间的距离度量。让我们看看我们得到了什么。
- en: We'll use pandas again to make life simple, and if you are following along,
    again make sure to change the path to the `MovieLens` dataset to wherever you
    installed it, which will almost certainly not be what is in this Python notebook.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用pandas来简化生活，如果您跟着做，请确保将`MovieLens`数据集的路径更改为您安装它的位置，这几乎肯定不是这个Python笔记本中的位置。
- en: 'Please go ahead and change that if you want to follow along. As before, we''re
    just going to import the actual ratings data file itself, which is `u.data` using
    the `read_csv()` function in pandas. We''re going to tell that it actually has
    a tab-delimiter and not a comma. We''re going to import the first 3 columns, which
    represent the `user_id`, `movie_id`, and rating, for every individual movie rating
    in our dataset:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请继续进行更改，如果您想跟着做。与以前一样，我们将只导入实际的评分数据文件`u.data`，使用pandas中的`read_csv()`函数。我们将告诉它实际上是一个制表符分隔符而不是逗号。我们将导入前3列，这些列代表`user_id`，`movie_id`和评分，对于数据集中每个电影的评分：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If we go ahead and run that and look at the top of it, we can see that it''s
    working, here''s how the output should look like:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们继续运行并查看顶部，我们可以看到它正在工作，输出应该如下所示：
- en: '![](img/e4c042b5-3659-4827-aa99-624eaac5abda.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4c042b5-3659-4827-aa99-624eaac5abda.png)'
- en: We end up with a `DataFrame` that has `user_id`, `movie_id`, and `rating`. For
    example, `user_id 0` rated `movie_id 50`, which I believe is Star Wars, 5 stars,
    and so on and so forth.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到一个具有`user_id`，`movie_id`和`rating`的`DataFrame`。例如，`user_id 0`对`movie_id
    50`进行了评分，我相信这是《星球大战》，给了5颗星，依此类推。
- en: 'The next thing we have to figure out is aggregate information about the ratings
    for each movie. We use the `groupby()` function in pandas to actually group everything
    by `movie_id`. We''re going to combine together all the ratings for each individual
    movie, and we''re going to output the number of ratings and the average rating
    score, that is the mean, for each movie:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要做的是，为每部电影聚合评分信息。我们使用pandas中的`groupby()`函数，实际上按`movie_id`对所有内容进行分组。我们将合并每部电影的所有评分，并输出每部电影的评分数量和平均评分分数，即平均值：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s go ahead and do that - comes back pretty quickly, here''s how the output
    looks like:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续做这个 - 很快就会回来，以下是输出的样子：
- en: '![](img/cfe272d1-c206-460d-9ebb-759a700f39ec.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfe272d1-c206-460d-9ebb-759a700f39ec.png)'
- en: This gives us another `DataFrame` that tells us, for example, `movie_id 1` had
    **452** ratings (which is a measure of its popularity, that is, how many people
    actually watched it and rated it), and a mean review score of 3.8\. So, **452**
    people watched `movie_id 1`, and they gave it an average review of 3.87, which
    is pretty good.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们另一个`DataFrame`，告诉我们，例如，`movie_id 1`有**452**个评分（这是它受欢迎程度的衡量，即有多少人实际观看并评分），以及平均评分为3.8。因此，有**452**人观看了`movie_id
    1`，他们给出了平均评分为3.87，这相当不错。
- en: Now, the raw number of ratings isn't that useful to us. I mean I don't know
    if **452** means it's popular or not. So, to normalize that, what we're going
    to do is basically measure that against the maximum and minimum number of ratings
    for each movie. We can do that using the `lambda` function. So, we can apply a
    function to an entire `DataFrame` this way.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，评分的原始数量对我们来说并不那么有用。我的意思是，我不知道**452**是否意味着它受欢迎与否。因此，为了使其标准化，我们将基本上根据每部电影的最大和最小评分数量来衡量。我们可以使用`lambda`函数来做到这一点。因此，我们可以以这种方式将函数应用于整个`DataFrame`。
- en: 'What we''re going to do is use the `np.min()` and `np.max()` functions to find
    the maximum number of ratings and the minimum number of ratings found in the entire
    dataset. So, we''ll take the most popular movie and the least popular movie and
    find the range there, and normalize everything against that range:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的是使用`np.min()`和`np.max()`函数来找到整个数据集中发现的最大评分数量和最小评分数量。因此，我们将找到最受欢迎的电影和最不受欢迎的电影，并将一切标准化到这个范围内：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'What this gives us, when we run it, is the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行它时，它给我们的是以下内容：
- en: '![](img/5efb3fdc-8c1e-4a15-8978-7a3bc3e3a954.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5efb3fdc-8c1e-4a15-8978-7a3bc3e3a954.png)'
- en: This is basically a measure of popularity for each movie, on a scale of 0 to
    1\. So, a score of 0 here would mean that nobody watched it, it's the least popular
    movie, and a score of `1` would mean that everybody watched, it's the most popular
    movie, or more specifically, the movie that the most people watched. So, we have
    a measure of movie popularity now that we can use for our distance metric.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是每部电影的受欢迎程度的衡量，范围是0到1。因此，这里的得分为0意味着没有人观看，这是最不受欢迎的电影，得分为`1`意味着每个人都观看了，这是最受欢迎的电影，或者更具体地说，是最多人观看的电影。因此，我们现在有了一个可以用于我们的距离度量的电影受欢迎程度的衡量。
- en: 'Next, let''s extract some general information. So, it turns out that there
    is a `u.item` file that not only contains the movie names, but also all the genres
    that each movie belongs to:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们提取一些一般信息。原来有一个`u.item`文件，不仅包含电影名称，还包含每部电影所属的所有流派：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The code above will actually go through each line of `u.item`. We're doing this
    the hard way; we're not using any pandas functions; we're just going to use straight-up
    Python this time. Again, make sure you change that path to wherever you installed
    this information.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码实际上会遍历`u.item`的每一行。我们正在以困难的方式做这个；我们没有使用任何pandas函数；这次我们将直接使用Python。再次确保将路径更改为您安装此信息的位置。
- en: 'Next, we open our `u.item` file, and then iterate through every line in the
    file one at a time. We strip out the new line at the end and split it based on
    the pipe-delimiters in this file. Then, we extract the `movieID`, the movie name
    and all of the individual genre fields. So basically, there''s a bunch of 0s and
    1s in 19 different fields in this source data, where each one of those fields
    represents a given genre. We then construct a Python dictionary in the end that
    maps movie IDs to their names, genres, and then we also fold back in our rating
    information. So, we will have name, genre, popularity on a scale of 0 to 1, and
    the average rating. So, that''s what this little snippet of code does. Let''s
    run that! And, just to see what we end up with, we can extract the value for `movie_id
    1`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们打开我们的`u.item`文件，然后逐行遍历文件中的每一行。我们去掉末尾的换行符，并根据该文件中的管道分隔符进行拆分。然后，我们提取`movieID`，电影名称和所有单独的流派字段。因此，基本上在这个源数据中有19个不同字段中的一堆0和1，其中每个字段代表一个给定的流派。最后，我们构建一个Python字典，将电影ID映射到它们的名称、流派，然后我们还将我们的评分信息折叠回去。因此，我们将得到名称、流派、受欢迎程度（在0到1的范围内）、以及平均评分。这段代码就是做这个的。让我们运行一下！并且，为了看看我们最终得到了什么，我们可以提取`movie_id
    1`的值：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Following is the output of the preceding code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是上述代码的输出：
- en: '![](img/d0121c0b-60c7-4e78-a982-88ea83f656be.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0121c0b-60c7-4e78-a982-88ea83f656be.png)'
- en: Entry 1 in our dictionary for `movie_id 1` happens to be Toy Story, an old Pixar
    film from 1995 you've probably heard of. Next is a list of all the genres, where
    a 0 indicates it is not part of that genre, and 1 indicates it is part of that
    genre. There is a data file in the `MovieLens` dataset that will tell you what
    each of these genre fields actually corresponds to.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们字典中`movie_id 1`的第一个条目恰好是《玩具总动员》，这是一部你可能听说过的1995年的皮克斯老电影。接下来是所有流派的列表，其中0表示它不属于该流派，1表示它属于该流派。在`MovieLens`数据集中有一个数据文件，可以告诉你这些流派字段实际对应的是什么。
- en: For our purposes, that's not actually important, right? We're just trying to
    measure distance between movies based on their genres. So, all that matters mathematically
    is how similar this vector of genres is to another movie, okay? The actual genres
    themselves, not important! We just want to see how same or different two movies
    are in their genre classifications. So we have that genre list, we have the popularity
    score that we computed, and we have there the mean or average rating for Toy Story.
    Okay, let's go ahead and figure out how to combine all this information together
    into a distance metric, so we can find the k-nearest neighbors for Toy Story,
    for example.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的来说，这实际上并不重要，对吧？我们只是试图根据它们的流派来衡量电影之间的距离。数学上重要的是这个流派向量与另一部电影有多相似，好吗？实际的流派本身并不重要！我们只想看看两部电影在它们的流派分类上有多相同或不同。所以我们有那个流派列表，我们有我们计算的受欢迎程度分数，还有Toy
    Story的平均评分。好了，让我们继续想办法将所有这些信息结合到一个距离度量中，这样我们就可以找到Toy Story的k个最近邻居了。
- en: I've rather arbitrarily computed this `ComputeDistance()` function, that takes
    two movie IDs and computes a distance score between the two. We're going to base
    this, first of all, on the similarity, using a cosine similarity metric, between
    the two genre vectors. Like I said, we're just going to take the list of genres
    for each movie and see how similar they are to each other. Again, a `0` indicates
    it's not part of that genre, a `1` indicates it is.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经相当随意地计算了这个`ComputeDistance()`函数，它接受两个电影ID并计算两者之间的距离分数。首先，我们将基于两个流派向量之间的相似性，使用余弦相似度度量来计算。就像我说的，我们将只是拿出每部电影的流派列表，看看它们彼此有多相似。再次强调，`0`表示它不属于该流派，`1`表示它属于该流派。
- en: We will then compare the popularity scores and just take the raw difference,
    the absolute value of the difference between those two popularity scores and use
    that toward the distance metric as well. Then, we will use that information alone
    to define the distance between two movies. So, for example, if we compute the
    distance between movie IDs 2 and 4, this function would return some distance function
    based only on the popularity of that movie and on the genres of those movies.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将比较受欢迎程度分数，只取原始差异，这两个受欢迎程度分数之间的绝对值差异，并将其用于距离度量。然后，我们将仅使用这些信息来定义两部电影之间的距离。所以，例如，如果我们计算电影ID
    2和4之间的距离，这个函数将返回一些仅基于该电影的受欢迎程度和这些电影的流派的距离函数。
- en: 'Now, imagine a scatter plot if you will, like we saw back in our example from
    the previous sections, where one axis might be a measure of genre similarity,
    based on cosine metric, the other axis might be popularity, okay? We''re just
    finding the distance between these two things:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下一个散点图，就像我们在前面的章节中看到的那样，其中一个轴可能是基于余弦度量的流派相似性的度量，另一个轴可能是受欢迎程度，好吗？我们只是在这两个事物之间找到距离：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For this example, where we''re trying to compute the distance using our distance
    metric between movies 2 and 4, we end up with a score of 0.8:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们试图使用我们的距离度量来计算电影2和4之间的距离，我们得到了一个0.8的分数：
- en: '![](img/feff2ac4-5127-44d7-b8b2-63e72bfbb485.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/feff2ac4-5127-44d7-b8b2-63e72bfbb485.png)'
- en: 'Remember, a far distance means it''s not similar, right? We want the nearest
    neighbors, with the smallest distance. So, a score of 0.8 is a pretty high number
    on a scale of 0 to 1\. So that''s telling me that these movies really aren''t
    similar. Let''s do a quick sanity check and see what these movies really are:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，远距离意味着不相似，对吧？我们想要最近的邻居，距离最小。所以，0.8的分数在0到1的范围内是一个相当高的数字。这告诉我这些电影实际上并不相似。让我们快速进行一次理智检查，看看这些电影实际上是什么：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It turns out it''s the movies GoldenEye and Get Shorty, which are pretty darn
    different movies:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是电影《黄金眼》和《短小的》这两部电影，它们是非常不同的电影：
- en: '![](img/475c25af-41f0-4812-80dc-56ede2b9baf1.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/475c25af-41f0-4812-80dc-56ede2b9baf1.png)'
- en: You know, you have James Bond action-adventure, and a comedy movie - not very
    similar at all! They're actually comparable in terms of popularity, but the genre
    difference did it in. Okay! So, let's put it all together!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，你有詹姆斯·邦德动作冒险片，还有一部喜剧电影 - 完全不相似！它们在受欢迎程度上实际上是可比较的，但是流派的差异让它们不同。好了！那么，让我们把它全部整合在一起吧！
- en: Next, we're going to write a little bit of code to actually take some given
    movieID and find the KNN. So, all we have to do is compute the distance between
    Toy Story and all the other movies in our movie dictionary, and sort the results
    based on their distance score. That's what the following little snippet of code
    does. If you want to take a moment to wrap your head around it, it's fairly straightforward.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将写一小段代码来实际获取一些给定的电影ID并找到KNN。所以，我们所要做的就是计算Toy Story和我们电影字典中的所有其他电影之间的距离，并根据它们的距离分数对结果进行排序。以下的代码片段就是这样做的。如果你想花点时间来理解一下，它其实非常简单。
- en: We have a little `getNeighbors()` function that will take the movie that we're
    interested in, and the K neighbors that we want to find. It'll iterate through
    every movie that we have; if it's actually a different movie than we're looking
    at, it will compute that distance score from before, append that to the list of
    results that we have, and sort that result. Then we will pluck off the K top results.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个小小的`getNeighbors()`函数，它将获取我们感兴趣的电影和我们想要找到的K个邻居。它将遍历我们拥有的每部电影；如果它实际上是一部不同于我们正在查看的电影，它将计算之前的距离分数，将其附加到我们的结果列表中，并对该结果进行排序。然后我们将挑选出前K个结果。
- en: In this example, we're going to set *K* to 10, find the 10 nearest neighbors.
    We will find the 10 nearest neighbors using `getNeighbors()`, and then we will
    iterate through all these 10 nearest neighbors and compute the average rating
    from each neighbor. That average rating will inform us of our rating prediction
    for the movie in question.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将*K*设置为10，找到10个最近的邻居。我们将使用`getNeighbors()`找到10个最近的邻居，然后遍历所有这10个最近的邻居，并计算每个邻居的平均评分。这个平均评分将告诉我们对于所讨论的电影的评分预测。
- en: As a side effect, we also get the 10 nearest neighbors based on our distance
    function, which we could call similar movies. So, that information itself is useful.
    Going back to that "Customers Who Watched Also Watched" example, if you wanted
    to do a similar feature that was just based on this distance metric and not actual
    behavior data, this might be a reasonable place to start, right?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个副作用，我们还根据我们的距离函数得到了10个最近的邻居，我们可以称之为相似的电影。所以，这个信息本身是有用的。回到那个“观看此影片的顾客还观看了”这个例子，如果你想做一个类似的功能，它只是基于这个距离度量而不是实际的行为数据，这可能是一个合理的起点，对吧？
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'So, let''s go ahead and run this, and see what we end up with. The output of
    the following code is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们继续运行这个，看看我们得到了什么。以下是上述代码的输出：
- en: '![](img/4e0fbf04-f04f-4b07-aff6-6d7f67fb0c17.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0fbf04-f04f-4b07-aff6-6d7f67fb0c17.png)'
- en: The results aren't that unreasonable. So, we are using as an example the movie
    Toy Story, which is movieID 1, and what we came back with, for the top 10 nearest
    neighbors, are a pretty good selection of comedy and children's movies. So, given
    that Toy Story is a popular comedy and children's movie, we got a bunch of other
    popular comedy and children's movies; so, it seems to work! We didn't have to
    use a bunch of fancy collaborative filtering algorithms, these results aren't
    that bad.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 结果并不那么不合理。所以，我们以《玩具总动员》这部电影为例，它是电影ID 1，我们得到的前10个最近邻居，是一些相当不错的喜剧和儿童电影。所以，鉴于《玩具总动员》是一部受欢迎的喜剧和儿童电影，我们得到了一堆其他受欢迎的喜剧和儿童电影；所以，似乎是有效的！我们并没有使用一堆花哨的协同过滤算法，这些结果并不那么糟糕。
- en: 'Next, let''s use KNN to predict the rating, where we''re thinking of the rating
    as the classification in this example:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用KNN来预测评分，这里我们将评分视为这个例子中的分类：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Following is the output of the preceding code:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是上述代码的输出：
- en: '![](img/92dc0277-702f-4801-bda5-ae21498f7a55.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92dc0277-702f-4801-bda5-ae21498f7a55.png)'
- en: We end up with a predicted rating of 3.34, which actually isn't all that different
    from the actual rating for that movie, which was 3.87\. So not great, but it's
    not too bad either! I mean it actually works surprisingly well, given how simple
    this algorithm is!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到了一个预测评分为3.34，实际上这与该电影的实际评分3.87并没有太大的不同。所以不是很好，但也不算太糟糕！我的意思是，实际上它的效果出奇的好，考虑到这个算法是多么简单！
- en: Activity
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动
- en: Most of the complexity in this example was just in determining our distance
    metric, and you know we intentionally got a little bit fancy there just to keep
    it interesting, but you can do anything else you want to. So, if you want fiddle
    around with this, I definitely encourage you to do so. Our choice of 10 for K
    was completely out of thin air, I just made that up. How does this respond to
    different K values? Do you get better results with a higher value of K? Or with
    a lower value of K? Does it matter?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，大部分复杂性都在确定我们的距离度量上，你知道我们故意在那里搞了点花样，只是为了让它变得有趣，但你可以做任何其他你想做的事情。所以，如果你想玩弄一下这个，我绝对鼓励你这样做。我们选择K为10完全是凭空想象的，我就是编造出来的。这对不同的K值有什么影响？使用更高的K值会得到更好的结果吗？还是使用更低的K值？这有关系吗？
- en: If you really want to do a more involved exercise you can actually try to apply
    it to train/test, to actually find the value of K that most optimally can predict
    the rating of the given movie based on KNN. And, you can use different distance
    metrics, I kind of made that up too! So, play around the distance metric, maybe
    you can use different sources of information, or weigh things differently. It
    might be an interesting thing to do. Maybe, popularity isn't really as important
    as the genre information, or maybe it's the other way around. See what impact
    that has on your results too. So, go ahead and mess with these algorithms, mess
    with the code and run with it, and see what you can get! And, if you do find a
    significant way of improving on this, share that with your classmates.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的想做一个更复杂的练习，你可以尝试将其应用到训练/测试中，实际上找到最能预测基于KNN的给定电影评分的K值。而且，你可以使用不同的距离度量，我也是凭空想象的！所以，玩一下距离度量，也许你可以使用不同的信息来源，或者以不同的方式权衡事物。这可能是一件有趣的事情。也许，流行度并不像流派信息那样重要，或者反过来也一样。看看这对你的结果有什么影响。所以，继续玩弄这些算法，玩弄代码并运行它，看看你能得到什么！如果你真的找到了一种显著的改进方法，那就和你的同学分享吧。
- en: 'That is KNN in action! So, a very simple concept but it can be actually pretty
    powerful. So, there you have it: similar movies just based on the genre and popularity
    and nothing else. Works out surprisingly well! And, we used the concept of KNN
    to actually use those nearest neighbors to predict a rating for a new movie, and
    that actually worked out pretty well too. So, that''s KNN in action, very simple
    technique but often it works out pretty darn good!'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是KNN的实际运用！所以，这是一个非常简单的概念，但实际上它可能非常强大。所以，你看：仅仅基于流派和流行度就能找到相似的电影，没有别的。结果出奇的好！而且，我们使用了KNN的概念来实际使用那些最近的邻居来预测新电影的评分，这也实际上效果不错。所以，这就是KNN的实际运用，非常简单的技术，但通常效果相当不错！
- en: Dimensionality reduction and principal component analysis
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维和主成分分析
- en: Alright, time to get all trippy! We're going to talking about higher dimensions,
    and dimensionality reduction. Sounds scary! There is some fancy math involved,
    but conceptually it's not as hard to grasp as you might think. So, let's talk
    about dimensionality reduction and principal component analysis next. Very dramatic
    sounding! Usually when people talk about this, they're talking about a technique
    called principal component analysis or PCA, and a specific technique called singular
    value decomposition or SVD. So PCA and SVD are the topics of this section. Let's
    dive into it!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，是时候进入更高维度的世界了！我们要谈论更高维度和降维。听起来有点可怕！这里涉及到一些花哨的数学，但从概念上来说，它并不像你想象的那么难以理解。所以，让我们接下来谈谈降维和主成分分析。听起来非常戏剧化！通常当人们谈论这个时，他们谈论的是一种叫做主成分分析或PCA的技术，以及一种叫做奇异值分解或SVD的特定技术。所以PCA和SVD是本节的主题。让我们深入研究一下！
- en: Dimensionality reduction
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: So, what is the curse of dimensionality? Well, a lot of problems can be thought
    of having many different dimensions. So, for example, when we were doing movie
    recommendations, we had attributes of various movies, and every individual movie
    could be thought of as its own dimension in that data space.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，维度诅咒是什么？嗯，很多问题可以被认为有许多不同的维度。所以，例如，当我们在做电影推荐时，我们有各种电影的属性，每个单独的电影可以被认为是数据空间中的一个维度。
- en: If you have a lot of movies, that's a lot of dimensions and you can't really
    wrap your head around more than 3, because that's what we grew up to evolve within.
    You might have some sort of data that has many different features that you care
    about. You know, in a moment we'll look at an example of flowers that we want
    to classify, and that classification is based on 4 different measurements of the
    flowers. Those 4 different features, those 4 different measurements can represent
    4 dimensions, which again, is very hard to visualize.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有很多电影，那就有很多维度，你真的无法理解超过3个维度，因为这是我们成长演变的范围。你可能有一些你关心的许多不同特征的数据。你知道，在接下来的一刻，我们将看一个我们想要分类的花的例子，而且这个分类是基于花的4个不同的测量。这4个不同的特征，这4个不同的测量可以代表4个维度，再次，这是非常难以可视化的。
- en: For this reason, dimensionality reduction techniques exist to find a way to
    reduce higher dimensional information into lower dimensional information. Not
    only can that make it easier to look at, and classify things, but it can also
    be useful for things like compressing data. So, by preserving the maximum amount
    of variance, while we reduce the number of dimensions, we're more compactly representing
    a dataset. A very common application of dimensionality reduction is not just for
    visualization, but also for compression, and for feature extraction. We'll talk
    about that a little bit more in a moment.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，降维技术存在是为了找到一种将更高维度信息降低到更低维度信息的方法。这不仅可以使它更容易查看和分类事物，而且还可以用于压缩数据。因此，通过保留最大方差，同时减少维度的数量，我们更紧凑地表示数据集。降维的一个非常常见的应用不仅仅是用于可视化，还用于压缩和特征提取。我们稍后会再谈一些。
- en: 'A very simple example of dimensionality reduction can be thought of as k-means
    clustering:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 降维的一个非常简单的例子可以被认为是k均值聚类：
- en: '![](img/8b8a47d6-5330-471d-b6c9-e36d861d9247.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b8a47d6-5330-471d-b6c9-e36d861d9247.png)'
- en: So you know, for example, we might start off with many points that represent
    many different dimensions in a dataset. But, ultimately, we can boil that down
    to K different centroids, and your distance to those centroids. That's one way
    of boiling data down to a lower dimensional representation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你知道，例如，我们可能从数据集中开始有许多点，代表数据集中许多不同的维度。但是，最终，我们可以将其归纳为K个不同的质心，以及你到这些质心的距离。这是将数据归纳为更低维度表示的一种方法。
- en: Principal component analysis
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Usually, when people talk about dimensionality reduction, they're talking about
    a technique called principal component analysis. This is a much more-fancy technique,
    it gets into some pretty involved mathematics. But, at a high-level, all you need
    to know is that it takes a higher dimensional data space, and it finds planes
    within that data space and higher dimensions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当人们谈论降维时，他们谈论的是一种称为主成分分析的技术。这是一种更加复杂的技术，它涉及到一些相当复杂的数学。但是，从高层次来看，你需要知道的是它将一个更高维度的数据空间，找到该数据空间和更高维度内的平面。
- en: 'These higher dimensional planes are called hyper planes, and they are defined
    by things called eigenvectors. You take as many planes as you want dimensions
    in the end, project that data onto those hyperplanes, and those become the new
    axes in your lower dimensional data space:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更高维度的平面被称为超平面，并且它们由称为特征向量的东西定义。你可以取尽可能多的平面，最终在那些超平面上投影数据，那些就成为你的低维数据空间中的新轴：
- en: '![](img/f45171af-5601-499c-a64b-2e13f263c348.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f45171af-5601-499c-a64b-2e13f263c348.png)'
- en: You know, unless you're familiar with higher dimensional math and you've thought
    about it before, it's going to be hard to wrap your head around! But, at the end
    of the day, it means we're choosing planes in a higher dimensional space that
    still preserve the most variance in our data, and project the data onto those
    higher dimensional planes that we then bring into a lower dimensional space, okay?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，除非你熟悉更高维度的数学并且之前考虑过它，否则很难理解！但是，最终，这意味着我们选择更高维度空间中的平面，仍然保留我们数据中的最大方差，并将数据投影到这些更高维度的平面上，然后将其带入更低维度的空间，好吗？
- en: You don't really have to understand all the math to use it; the important point
    is that it's a very principled way of reducing a dataset down to a lower dimensional
    space while still preserving the variance within it. We talked about image compression
    as one application of this. So you know, if I want to reduce the dimensionality
    in an image, I could use PCA to boil it down to its essence.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你真的不需要理解所有的数学来使用它；重要的是，这是一种非常有原则的方法，可以将数据集降低到更低维度的空间，同时仍然保留其中的方差。我们谈到了图像压缩作为这一应用的一个例子。所以你知道，如果我想要在图像中减少维度，我可以使用主成分分析将其归纳到其本质。
- en: Facial recognition is another example. So, if I have a dataset of faces, maybe
    each face represents a third dimension of 2D images, and I want to boil that down,
    SVD and principal component analysis can be a way to identify the features that
    really count in a face. So, it might end up focusing more on the eyes and the
    mouth, for example, those important features that are necessary for preserving
    the variance within that dataset. So, it can produce some very interesting and
    very useful results that just emerge naturally out of the data, which is kind
    of cool!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 面部识别是另一个例子。所以，如果我有一个面部数据集，也许每张脸代表2D图像的第三个维度，并且我想将其归纳，SVD和主成分分析可以是识别真正重要的特征的一种方法。因此，它可能更多地关注眼睛和嘴巴，例如，那些在保留数据集内方差方面是必要的重要特征。因此，它可以产生一些非常有趣和非常有用的结果，这些结果只是自然地从数据中出现，这有点酷！
- en: 'To make it real, we''re going to use a simpler example, using what''s called
    the Iris dataset. This is a dataset that''s included with scikit-learn. It''s
    used pretty commonly in examples, and here''s the idea behind it: So, an Iris
    actually has 2 different kinds of petals on its flower. One''s called a petal,
    which is the flower petals you''re familiar with, and it also has something called
    a sepal, which is kind of this supportive lower set of petals on the flower.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其更真实，我们将使用一个更简单的例子，使用所谓的鸢尾花数据集。这是一个包含在scikit-learn中的数据集。它在示例中经常被使用，其背后的想法是：鸢尾花实际上有两种不同类型的花瓣。一种叫做花瓣，就是你熟悉的花瓣，还有一种叫做萼片，它是花朵下部的一组支持性较低的花瓣。
- en: We can take a bunch of different species of Iris, and measure the petal length
    and width, and the sepal length and width. So, together the length and width of
    the petal, and the length and width of the sepal are 4 different measurements
    that correspond to 4 different dimensions in our dataset. I want to use that to
    classify what species an Iris might belong to. Now, PCA will let us visualize
    this in 2 dimensions instead of 4, while still preserving the variance in that
    dataset. So, let's see how well that works and actually write some Python code
    to make PCA happen on the Iris dataset.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以拿一堆不同种类的鸢尾花，测量花瓣的长度和宽度，以及萼片的长度和宽度。因此，花瓣的长度和宽度，以及萼片的长度和宽度，共有4个不同的测量值对应于我们数据集中的4个不同维度。我想用这些来分类鸢尾花可能属于哪个物种。现在，PCA将让我们在2个维度上可视化这个数据，而仍然保留数据集中的方差。所以，让我们看看这个方法的效果如何，并实际编写一些Python代码来对鸢尾花数据集进行PCA。
- en: So, those were the concepts of dimensionality reduction, principal component
    analysis, and singular value decomposition. All big fancy words and yeah, it is
    kind of a fancy thing. You know, we're dealing with reducing higher dimensional
    spaces down to smaller dimensional spaces in a way that preserves their variance.
    Fortunately, scikit-learn makes this extremely easy to do, like 3 lines of code
    is all you need to actually apply PCA. So let's make that happen!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是降维、主成分分析和奇异值分解的概念。所有这些都是很高级的词汇，是的，这确实是一件高级的事情。你知道，我们正在以一种保留它们的方差的方式将高维空间缩减到低维空间。幸运的是，scikit-learn使这变得非常容易，只需要3行代码就可以应用PCA。所以让我们开始吧！
- en: A PCA example with the Iris dataset
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鸢尾花数据集的PCA示例
- en: Let's apply principal component analysis to the Iris dataset. This is a 4D dataset
    that we're going to reduce down to 2 dimensions. We're going to see that we can
    actually still preserve most of the information in that dataset, even by throwing
    away half of the dimensions. It's pretty cool stuff, and it's pretty simple too.
    Let's dive in and do some principal component analysis and cure the curse of dimensionality.
    Go ahead and open up the `PCA.ipynb` file.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将主成分分析应用于鸢尾花数据集。这是一个4D数据集，我们将将其降低到2个维度。我们将看到，即使丢弃了一半的维度，我们仍然可以保留数据集中的大部分信息。这是相当酷的东西，而且也相当简单。让我们深入研究一下，进行一些主成分分析，并解决维度的诅咒。继续打开`PCA.ipynb`文件。
- en: It's actually very easy to do using scikit-learn, as usual! Again, PCA is a
    dimensionality reduction technique. It sounds very science-fictiony, all this
    talk of higher dimensions. But, just to make it more concrete and real again,
    a common application is image compression. You can think of an image of a black
    and white picture, as 3 dimensions, where you have width, as your x-axis, and
    your y-axis of height, and each individual cell has some brightness value on a
    scale of 0 to 1, that is black or white, or some value in between. So, that would
    be 3D data; you have 2 spatial dimensions, and then a brightness and intensity
    dimension on top of that.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn实际上非常容易！再次强调，PCA是一种降维技术。所有这些关于高维度的讨论听起来非常科幻，但为了使其更具体和真实，一个常见的应用是图像压缩。你可以将一张黑白图片看作是3个维度，其中宽度是x轴，高度是y轴，每个单元格都有一个0到1的亮度值，即黑色或白色，或者介于两者之间的一些值。因此，这将是3D数据；你有2个空间维度，然后还有一个亮度和强度维度。
- en: If you were to distill that down to say 2 dimensions alone, that would be a
    compressed image and, if you were to do that in a technique that preserved the
    variance in that image as well as possible, you could still reconstruct the image,
    without a whole lot of loss in theory. So, that's dimensionality reduction, distilled
    down to a practical example.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将其精炼为仅有2个维度，那将是一个压缩图像，如果你以一种尽可能保留图像方差的技术来做到这一点，你仍然可以重构图像，理论上损失不会太大。所以，这就是降维，精炼为一个实际的例子。
- en: Now, we're going to use a different example here using the Iris dataset, and
    scikit-learn includes this. All it is is a dataset of various Iris flower measurements,
    and the species classification for each Iris in that dataset. And it has also,
    like I said before, the length and width measurement of both the petal and the
    sepal for each Iris specimen. So, between the length and width of the petal, and
    the length and width of the sepal we have 4 dimensions of feature data in our
    dataset.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用鸢尾花数据集的另一个示例，scikit-learn包含了这个数据集。它只是一个包含各种鸢尾花测量值和该数据集中每株鸢尾花物种分类的数据集。就像我之前说的，它还包括每株鸢尾花标本的花瓣和萼片的长度和宽度测量值。因此，在花瓣的长度和宽度以及萼片的长度和宽度之间，我们的数据集中有4个特征数据维度。
- en: 'We want to distill that down to something we can actually look at and understand,
    because your mind doesn''t deal with 4 dimensions very well, but you can look
    at 2 dimensions on a piece of paper pretty easily. Let''s go ahead and load that
    up:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将其精炼为我们实际可以查看和理解的内容，因为你的大脑无法很好地处理4个维度，但你可以很容易地在纸上查看2个维度。让我们继续加载：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'There''s a handy dandy `load_iris()` function built into scikit-learn that
    will just load that up for you with no additional work; so you can just focus
    on the interesting part. Let''s take a look at what that dataset looks like, the
    output of the preceding code is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中有一个方便的`load_iris()`函数，它可以直接加载数据，无需额外的工作；所以你可以专注于有趣的部分。让我们来看看这个数据集是什么样子的，前面代码的输出如下：
- en: '![](img/3c9a69f5-391b-47b4-9fe1-38deb4e9b270.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c9a69f5-391b-47b4-9fe1-38deb4e9b270.png)'
- en: You can see that we are extracting the shape of that dataset, which means how
    many data points we have in it, that is `150`, and how many features, or how many
    dimensions that dataset has, and that is `4`. So, we have `150` Iris specimens
    in our dataset, with 4 dimensions of information. Again, that is the length and
    width of the sepal, and the length and width of the petal, for a total of `4`
    features, which we can think of as `4` dimensions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们正在提取数据集的形状，也就是我们有多少数据点，即`150`，以及数据集有多少特征，或者说有多少维度，即`4`。所以，我们的数据集中有`150`朵鸢尾花标本，有4个信息维度。再次强调，这是萼片的长度和宽度，以及花瓣的长度和宽度，总共有`4`个特征，我们可以将其视为`4`个维度。
- en: 'We can also print out the list of target names in this dataset, which are the
    classifications, and we can see that each Iris belongs to one of three different
    species: Setosa, Versicolor, or Virginica. That''s the data that we''re working
    with: 150 Iris specimens, classified into one of 3 species, and we have 4 features
    associated with each Iris.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以打印出这个数据集中目标名称的列表，即分类，我们可以看到每朵鸢尾花属于三种不同的物种之一：山鸢尾、变色鸢尾或者维吉尼亚鸢尾。这就是我们要处理的数据：150朵鸢尾花标本，分为3种物种之一，并且每朵鸢尾花都有4个特征。
- en: Let's look at how easy PCA is. Even though it's a very complicated technique
    under the hood, doing it is just a few lines of code. We're going to assign the
    entire Iris dataset and we're going to call it X. We will then create a PCA model,
    and we're going to keep `n_components=2`, because we want 2 dimensions, that is,
    we're going to go from 4 to 2.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看PCA有多容易。尽管在底层它是一个非常复杂的技术，但实际操作只需要几行代码。我们将整个鸢尾花数据集分配给X。然后我们将创建一个PCA模型，并保持`n_components=2`，因为我们想要2个维度，也就是说，我们要从4维降到2维。
- en: We're going to use `whiten=True`, that means that we're going to normalize all
    the data, and make sure that everything is nice and comparable. Normally you will
    want to do that to get good results. Then, we will fit the PCA model to our Iris
    dataset `X`. We can use that model then, to transform that dataset down to 2 dimensions.
    Let's go ahead and run that. It happened pretty quickly!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`whiten=True`，这意味着我们将对所有数据进行归一化，确保一切都很好地可比较。通常情况下，为了获得良好的结果，你会想要这样做。然后，我们将把PCA模型拟合到我们的鸢尾花数据集`X`上。然后我们可以使用该模型将数据集转换为2维。让我们来运行一下。这发生得非常快！
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Please think about what just happened there. We actually created a PCA model
    to reduce 4 dimensions down to `2`, and it did that by choosing 2 4D vectors,
    to create hyperplanes around, to project that 4D data down to 2 dimensions. You
    can actually see what those 4D vectors are, those eigenvectors, by printing out
    the actual components of PCA. So, **PCA** stands for **Principal Component Analysis**,
    those principal components are the eigenvectors that we chose to define our planes
    about:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请思考刚才发生了什么。我们实际上创建了一个PCA模型，将4个维度降低到`2`，它通过选择2个4D向量来实现这一点，以创建超平面，将4D数据投影到2维。你实际上可以通过打印PCA的实际成分来看到这些4D向量，即特征向量。所以，**PCA**代表**主成分分析**，这些主成分就是我们选择来定义平面的特征向量：
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Output to the preceding code is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '![](img/481b0278-59d5-4c69-a59c-8dafa30717de.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/481b0278-59d5-4c69-a59c-8dafa30717de.png)'
- en: 'You can actually look at those values, they''re not going to mean a lot to
    you, because you can''t really picture 4 dimensions anyway, but we did that just
    so you can see that it''s actually doing something with principal components.
    So, let''s evaluate our results:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上可以查看这些值，它们对你来说可能没有太多意义，因为你无法真正想象4个维度，但我们这样做是为了让你看到它实际上正在处理主成分。所以，让我们评估一下我们的结果：
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The PCA model gives us back something called `explained_variance_ratio`. Basically,
    that tells you how much of the variance in the original 4D data was preserved
    as I reduced it down to 2 dimensions. So, let''s go ahead and take a look at that:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: PCA模型给我们返回了一个叫做`explained_variance_ratio`的东西。基本上，这告诉你在将原始的4D数据降低到2维时，有多少方差得以保留。所以，让我们来看看：
- en: '![](img/b5dd7d8a-29b5-4e7e-be53-21821ac9418c.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5dd7d8a-29b5-4e7e-be53-21821ac9418c.png)'
- en: What it gives you back is actually a list of 2 items for the 2 dimensions that
    we preserved. This is telling me that in the first dimension I can actually preserve
    92% of the variance in the data, and the second dimension only gave me an additional
    5% of variance. If I sum it together, these 2 dimensions that I projected my data
    down into, I still preserved over 97% of the variance in the source data. We can
    see that 4 dimensions weren't really necessary to capture all the information
    in this dataset, which is pretty interesting. It's pretty cool stuff!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 它实际上给出了一个包含2个项目的列表，用于我们保留的2个维度。这告诉我，在第一个维度中，我实际上可以保留数据中92%的方差，而第二个维度只给了我额外的5%方差。如果将它们加在一起，我将数据投影到的这2个维度中，仍然保留了源数据中超过97%的方差。我们可以看到，其实并不需要4个维度来捕捉这个数据集中的所有信息，这是非常有趣的。这是相当酷的东西！
- en: If you think about it, why do you think that might be? Well, maybe the overall
    size of the flower has some relationship to the species at its center. Maybe it's
    the ratio of length to width for the petal and the sepal. You know, some of these
    things probably move together in concert with each other for a given species,
    or for a given overall size of a flower. So, perhaps there are relationships between
    these 4 dimensions that PCA is extracting on its own. It's pretty cool, and pretty
    powerful stuff. Let's go ahead and visualize this.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细想想，你觉得可能是为什么呢？也许花的整体大小与其物种中心有一定的关系。也许是花瓣和萼片的长度与宽度之比。你知道，这些东西可能会随着给定物种或给定花的整体大小一起协调地移动。因此，也许这4个维度之间存在PCA自行提取的关系。这很酷，也很强大。让我们继续可视化这一点。
- en: 'The whole point of reducing this down to 2 dimensions was so that we could
    make a nice little 2D scatter plot of it, at least that''s our objective for this
    little example here. So, we''re going to do a little bit of Matplotlib magic here
    to do that. There is some sort of fancy stuff going on here that I should at least
    mention. So, what we''re going to do is create a list of colors: red, green and
    blue. We''re going to create a list of target IDs, so that the values 0, 1, and
    2 map to the different Iris species that we have.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个数据降低到2个维度的整个目的是为了我们能够制作一个漂亮的2D散点图，至少这是我们在这个小例子中的目标。因此，我们将在这里做一些Matplotlib的魔术。这里有一些花里胡哨的东西，我至少应该提一下。所以，我们将创建一个颜色列表：红色、绿色和蓝色。我们将创建一个目标ID列表，使值0、1和2映射到我们拥有的不同的鸢尾花物种。
- en: 'What we''re going to do is zip all this up with the actual names of each species.
    The for loop will iterate through the 3 different Iris species, and as it does
    that, we''re going to have the index for that species, a color associated with
    it, and the actual human-readable name for that species. We''ll take one species
    at a time and plot it on our scatter plot just for that species with a given color
    and the given label. We will then add in our legend and show the results:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把所有这些与每个物种的实际名称一起压缩。for循环将遍历3种不同的鸢尾花物种，当它这样做时，我们将有该物种的索引，与之关联的颜色，以及该物种的实际可读名称。我们将一次处理一种物种，并在我们的散点图上用给定的颜色和标签绘制该物种的散点图。然后我们将添加我们的图例并显示结果：
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following is what we end up with:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们得到的结果：
- en: '![](img/c4cb8972-5c7e-4b7a-b42b-2f03a0c0fa57.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4cb8972-5c7e-4b7a-b42b-2f03a0c0fa57.png)'
- en: That is our 4D Iris data projected down to 2 dimensions. Pretty interesting
    stuff! You can see it still clusters together pretty nicely. You know, you have
    all the Virginicas sitting together, all the Versicolors sitting in the middle,
    and the Setosas way off on the left side. It's really hard to imagine what these
    actual values represent. But, the important point is, we've projected 4D data
    down to 2D, and in such a way that we still preserve the variance. We can still
    see clear delineations between these 3 species. A little bit of intermingling
    going on in there, it's not perfect you know. But by and large, it was pretty
    effective.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将4D鸢尾花数据投影到2个维度。非常有趣！你可以看到它们仍然相当好地聚集在一起。你知道，所有的维吉尼亚人坐在一起，所有的变色鸢尾坐在中间，而山鸢尾则远在左侧。真的很难想象这些实际值代表什么。但是，重要的是，我们将4D数据投影到2D，并且以这样的方式保留了方差。我们仍然可以清楚地看到这3个物种之间的明显区分。在其中有一些交织，不是完美的，你知道。但总的来说，它非常有效。
- en: Activity
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动
- en: As you recall from `explained_variance_ratio`, we actually captured most of
    the variance in a single dimension. Maybe the overall size of the flower is all
    that really matters in classifying it; and you can specify that with one feature.
    So, go ahead and modify the results if you are feeling up to it. See if you can
    get away with 2 dimensions, or 1 dimension instead of 2! So, go change that `n_components`
    to `1`, and see what kind of variance ratio you get.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从`explained_variance_ratio`中回忆起的那样，我们实际上在一个维度中捕获了大部分的方差。也许花的整体大小才是真正重要的分类因素；你可以用一个特征来指定这一点。所以，如果你感觉可以的话，继续修改结果。看看你是否可以用2个维度或者1个维度来完成！所以，把`n_components`改成`1`，看看你得到什么样的方差比率。
- en: What happens? Does it makes sense? Play around with it, get some familiarity
    with it. That is dimensionality reduction, principal component analysis, and singular
    value decomposition all in action. Very, very fancy terms, and you know, to be
    fair it is some pretty fancy math under the hood. But as you can see, it's a very
    powerful technique and with scikit-learn, it's not hard to apply. So, keep that
    in your tool chest.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么？这有意义吗？玩弄一下，熟悉一下。这就是降维、主成分分析和奇异值分解的全部过程。非常、非常花哨的术语，你知道，在公平的情况下，在这些术语的背后是一些相当花哨的数学。但正如你所看到的，这是一种非常强大的技术，并且在scikit-learn中，应用起来并不难。因此，请将其放入你的工具箱中。
- en: And there you have it! A 4D dataset of flower information boiled down to 2 dimensions
    that we can both easily visualize, and also still see clear delineations between
    the classifications that we're interested in. So, PCA works really well in this
    example. Again, it's a useful tool for things like compression, or feature extraction,
    or facial recognition as well. So, keep that in your toolbox.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！一个关于花信息的4D数据集被简化为我们可以轻松可视化的2个维度，并且仍然可以清楚地看到我们感兴趣的分类之间的区分。因此，在这个例子中，PCA的效果非常好。再次强调，这是一个用于压缩、特征提取或面部识别等方面的有用工具。因此，请将其放入你的工具箱中。
- en: Data warehousing overview
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据仓库概述
- en: Next, we're going to talk a little bit about data warehousing. This is a field
    that's really been upended recently by the advent of Hadoop, and some big data
    techniques and cloud computing. So, a lot of big buzz words there, but concepts
    that are important for you to understand.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将稍微谈一下数据仓库。这是一个领域，最近被Hadoop的出现以及一些大数据技术和云计算所颠覆。所以，有很多大的关键词，但这些概念对你来说是重要的。
- en: Let's dive in and explore these concepts! Let's talk about ELT and ETL, and
    data warehousing in general. This is more of a concept, as opposed to a specific
    practical technique, so we're going to talk about it conceptually. But, it is
    something that's likely to come up in the setting of a job interview. So, let's
    make sure you understand these concepts.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨这些概念！让我们谈谈ELT和ETL，以及数据仓库的一般情况。这更多是一个概念，而不是一个具体的实际技术，所以我们将从概念上来谈论它。但是，在工作面试中，这可能会出现。所以，让我们确保你理解这些概念。
- en: We'll start by talking about data warehousing in general. What is a data warehouse?
    Well, it's basically a giant database that contains information from many different
    sources and ties them together for you. For example, maybe you work at a big ecommerce
    company and they might have an ordering system that feeds information about the
    stuff people bought into your data warehouse.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先谈论一般的数据仓库。什么是数据仓库？嗯，它基本上是一个包含来自许多不同来源的信息的巨大数据库，并为你将它们联系在一起。例如，也许你在一家大型电子商务公司工作，他们可能有一个订单系统，将人们购买的商品的信息输入到你的数据仓库中。
- en: You might also have information from web server logs that get ingested into
    the data warehouse. This would allow you to tie together browsing information
    on the website with what people ultimately ordered for example. Maybe you could
    also tie in information from your customer service systems, and measure if there's
    a relationship between browsing behavior and how happy the customers are at the
    end of the day.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以从网络服务器日志中获取信息，将其注入到数据仓库中。这将使你能够将网站上的浏览信息与人们最终购买的商品联系起来。也许你还可以将来自客户服务系统的信息联系起来，并衡量浏览行为与客户最终的满意度之间是否存在关系。
- en: A data warehouse has the challenge of taking data from many different sources,
    transforming them into some sort of schema that allows us to query these different
    data sources simultaneously, and it lets us make insights, through data analysis.
    So, large corporations and organizations have this sort of thing pretty commonly.
    We're going into the concept of big data here. You can have a giant Oracle database,
    for example, that contains all this stuff and maybe it's partitioned in some way,
    and replicated and it has all sorts of complexity. You can just query that through
    SQL, structured query language, or, through graphical tools, like Tableau which
    is a very popular one these days. That's what a data analyst does, they query
    large datasets using stuff like Tableau.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库面临着从许多不同来源获取数据的挑战，将它们转换为某种模式，使我们能够同时查询这些不同的数据来源，并通过数据分析得出见解。因此，大型公司和组织通常会有这种情况。我们正在涉及大数据的概念。你可以有一个巨大的Oracle数据库，例如，其中包含所有这些东西，也许以某种方式进行了分区和复制，并且具有各种复杂性。你可以通过SQL，结构化查询语言，或者通过图形工具，比如Tableau，来查询它，这是目前非常流行的一种工具。这就是数据分析师的工作，他们使用诸如Tableau之类的工具查询大型数据集。
- en: That's kind of the difference between a data analyst and a data scientist. You
    might be actually writing code to perform more advanced techniques on data that
    border on AI, as opposed to just using tools to extract graphs and relationships
    out of a data warehouse. It's a very complicated problem. At Amazon, we had an
    entire department for data warehousing that took care of this stuff full time,
    and they never had enough people, I can tell you that; it's a big job!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是数据分析师和数据科学家之间的区别。你可能实际上正在编写代码，对数据执行更高级的技术，涉及到人工智能，而不仅仅是使用工具从数据仓库中提取图表和关系。这是一个非常复杂的问题。在亚马逊，我们有一个专门负责数据仓库的部门，全职负责这些事情，他们从来没有足够的人手，我可以告诉你；这是一项重大工作！
- en: 'You know, there are a lot of challenges in doing data warehousing. One is data
    normalization: so, you have to figure out how do all the fields in these different
    data sources actually relate to each other? How do I actually make sure that a
    column in one data source is comparable to a column from another data source and
    has the same set of data, at the same scale, using the same terminology? How do
    I deal with missing data? How do I deal with corrupt data or data from outliers,
    or from robots and things like that? These are all very big challenges. Maintaining
    those data feeds is also a very big problem.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，做数据仓库有很多挑战。其中之一是数据规范化：因此，你必须弄清楚这些不同数据来源中的所有字段实际上是如何相互关联的？我如何确保一个数据源中的列可以与另一个数据源中的列进行比较，并具有相同的数据集、相同的规模和相同的术语？我如何处理缺失数据？我如何处理损坏的数据或来自异常值、机器人等的数据？这些都是非常大的挑战。维护这些数据源也是一个非常大的问题。
- en: A lot can go wrong when you're importing all this information into your data
    warehouse, especially when you have a very large transformation that needs to
    happen to take the raw data, saved from web logs, into an actual structured database
    table that can be imported into your data warehouse. Scaling also can get tricky
    when you're dealing with a monolithic data warehouse. Eventually, your data will
    get so large that those transformations themselves start to become a problem.
    This starts to get into the whole topic of ELT versus ETL thing.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将所有这些信息导入数据仓库时，很多问题可能会出现，特别是当你需要进行非常大的转换，将从网络日志中保存的原始数据转换为实际的结构化数据库表，然后导入到你的数据仓库中。当你处理一个庞大的数据仓库时，扩展也可能会变得棘手。最终，你的数据会变得如此庞大，以至于这些转换本身开始成为一个问题。这开始涉及到ELT与ETL的整个话题。
- en: ETL versus ELT
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ETL与ELT
- en: Let's first talk about ETL. What does that stand for? It stands for extract,
    transform, and load - and that's sort of the conventional way of doing data warehousing.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先谈谈ETL。它是什么意思？它代表提取、转换和加载-这是做数据仓库的传统方式。
- en: Basically, first you extract the data that you want from the operational systems
    that you want. So, for example, I might extract all of the web logs from our web
    servers each day. Then I need to transform all that information into an actual
    structured database table that I can import into my data warehouse.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，首先从你想要的操作系统中提取你想要的数据。例如，我可能每天从我们的Web服务器中提取所有的Web日志。然后，我需要将所有这些信息转换为一个实际的结构化数据库表，可以将其导入到我的数据仓库中。
- en: This transformation stage might go through every line of those web server logs,
    transform that into an actual table, where I'm plucking out from each web log
    line things like session ID, what page they looked at, what time it was, what
    the referrer was and things like that, and I can organize that into a tabular
    structure that I can then load into the data warehouse itself, as an actual table
    in a database. So, as data becomes larger and larger, that transformation step
    can become a real problem. Think about how much processing work is required to
    go through all of the web logs on Google, or Amazon, or any large website, and
    transform that into something a database can ingest. That itself becomes a scalability
    challenge and something that can introduce stability problems through the entire
    data warehouse pipeline.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换阶段可能会遍历每一行Web服务器日志，将其转换为一个实际的表，从每个Web日志行中提取会话ID、他们查看的页面、时间、引荐者等信息，并将其组织成一个表格结构，然后将其加载到数据仓库本身，作为数据库中的一个实际表。因此，随着数据变得越来越大，这个转换步骤可能会成为一个真正的问题。想想在Google、Amazon或任何大型网站上处理所有Web日志并将其转换为数据库可以摄取的内容需要多少处理工作。这本身就成为一个可扩展性挑战，并且可能会通过整个数据仓库管道引入稳定性问题。
- en: That's where the concept of ELT comes in, and it kind of flips everything on
    its head. It says, "Well, what if we don't use a huge Oracle instance? What if
    instead we use some of these newer techniques that allow us to have a more distributed
    database over a Hadoop cluster that lets us take the power of these distributed
    databases like Hive, or Spark, or MapReduce, and use that to actually do the transformation
    after it's been loaded"
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是ELT概念的出现，并且它有点颠覆了一切。它说，“如果我们不使用一个庞大的Oracle实例会怎样？如果我们使用一些允许我们在Hadoop集群上拥有更分布式数据库的新技术，让我们利用Hive、Spark或MapReduce这些分布式数据库的能力，在加载后实际进行转换。”
- en: The idea here is we're going to extract the information we want as we did before,
    say from a set of web server logs. But then, we're going to load that straight
    in to our data repository, and we're going to use the power of the repository
    itself to actually do the transformation in place. So, the idea here is, instead
    of doing an offline process to transform my web logs, as an example, into a structured
    format, I'm just going to suck those in as raw text files and go through them
    one line at a time, using the power of something like Hadoop, to actually transform
    those into a more structured format that I can then query across my entire data
    warehouse solution.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是，我们将提取我们想要的信息，就像以前一样，比如从一组Web服务器日志中。但是，我们将直接将其加载到我们的数据存储库中，并且我们将使用存储库本身的功能来实际进行转换。因此，这里的想法是，与其进行离线过程来转换我的Web日志，例如，将其作为原始文本文件导入并逐行处理，使用类似Hadoop的东西的功能，实际上将其转换为更结构化的格式，然后可以跨整个数据仓库解决方案进行查询。
- en: Things like Hive let you host a massive database on a Hadoop cluster. There's
    things like Spark SQL that let you also do queries in a very SQL-like data warehouse-like
    manner, on a data warehouse that is actually distributed on Hadoop cluster. There
    are also distributed NoSQL data stores that can be queried using Spark and MapReduce.
    The idea is that instead of using a monolithic database for a data warehouse,
    you're instead using something built on top of Hadoop, or some sort of a cluster,
    that can actually not only scale up the processing and querying of that data,
    but also scale the transformation of that data as well.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 像Hive这样的东西让你在Hadoop集群上托管一个庞大的数据库。还有像Spark SQL这样的东西，让你也可以以非常类似SQL的数据仓库方式进行查询，实际上是在Hadoop集群上分布的数据仓库上进行查询。还有一些分布式NoSQL数据存储，可以使用Spark和MapReduce进行查询。这个想法是，你不是使用单一的数据库作为数据仓库，而是使用建立在Hadoop或某种集群之上的东西，实际上不仅可以扩展数据的处理和查询，还可以扩展数据的转换。
- en: Once again, you first extract your raw data, but then we're going to load it
    into the data warehouse system itself as is. And, then use the power of the data
    warehouse, which might be built on Hadoop, to do that transformation as the third
    step. Then I can query things together. So, it's a very big project, very big
    topic. You know, data warehousing is an entire discipline in and of itself. We're
    going to talk about Spark some more in this book very soon, which is one way of
    handling this thing - there's something called Spark SQL in particular that's
    relevant.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，首先提取原始数据，然后将其加载到数据仓库系统本身。然后使用数据仓库的能力（可能建立在Hadoop上）作为第三步进行转换。然后我可以一起查询这些东西。这是一个非常庞大的项目，非常庞大的主题。你知道，数据仓库本身就是一个完整的学科。我们将很快在这本书中更多地讨论Spark，这是处理这个问题的一种方式——特别是有一个叫做Spark
    SQL的东西是相关的。
- en: The overall concept here is that if you move from a monolithic database built
    on Oracle or MySQL to one of these more modern distributed databases built on
    top of Hadoop, you can take that transform stage and actually do that after you've
    loaded in the raw data, as opposed to before. That can end up being simpler and
    more scalable, and taking advantage of the power of large computing clusters that
    are available today.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 总体概念是，如果你从建立在Oracle或MySQL上的单一数据库转移到建立在Hadoop之上的这些更现代的分布式数据库之一，你可以在加载原始数据后进行转换阶段。这可能会更简单、更可扩展，并且利用今天可用的大型计算集群的能力。
- en: That's ETL versus ELT, the legacy way of doing it with a lot of clusters all
    over the place in cloud-based computing versus a way that makes sense today, when
    we do have large clouds of computing available to us for transforming large datasets.
    That's the concept.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这是ETL与ELT的对比，云计算中的传统方式与当今有意义的方式，当我们有大规模的计算资源可用于转换大型数据集时。这就是概念。
- en: ETL is kind of the old school way of doing it, you transform a bunch of data
    offline before importing it in and loading it into a giant data warehouse, monolithic
    database. But with today's techniques, with cloud-based databases, and Hadoop,
    and Hive, and Spark, and MapReduce, you can actually do it a little bit more efficiently
    and take the power of a cluster to actually do that transformation step after
    you've loaded the raw data into your data warehouse.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ETL是一种老派的做法，你在导入和加载到一个巨大的数据仓库、单片数据库之前，离线转换一堆数据。但是在今天的技术中，使用基于云的数据库、Hadoop、Hive、Spark和MapReduce，你实际上可以更有效地做到这一点，并利用集群的力量在将原始数据加载到数据仓库后执行转换步骤。
- en: This is really changing the field and it's important that you know about it.
    Again, there's a lot more to learn on the subject, so I encourage you to explore
    more on this topic. But, that's the basic concept, and now you know what people
    are talking about when they talk about ETL versus ELT.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的改变了这个领域，你知道这一点很重要。再次强调，关于这个主题还有很多要学习，所以我鼓励你在这个主题上进行更多的探索。但是，这就是基本概念，现在你知道人们谈论ETL与ELT时在谈论什么了。
- en: Reinforcement learning
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'Our next topic''s a fun one: reinforcement learning. We can actually use this
    idea with an example of Pac-Man. We can actually create a little intelligent Pac-Man
    agent that can play the game Pac-Man really well on its own. You''ll be surprised
    how simple the technique is for building up the smarts behind this intelligent
    Pac-Man. Let''s take a look!'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一个话题是一个有趣的话题：强化学习。我们可以用Pac-Man的例子来理解这个概念。我们实际上可以创建一个能够自己玩得很好的智能Pac-Man代理。你会惊讶于构建这个智能Pac-Man背后的技术是多么简单。让我们来看看！
- en: So, the idea behind reinforcement learning is that you have some sort of agent,
    in this case Pac-Man, that explores some sort of space, and in our example that
    space will be the maze that Pac-Man is in. As it goes, it learns the value of
    different state changes within different conditions.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，强化学习的理念是，你有某种代理，比如Pac-Man，在我们的例子中，这个空间将是Pac-Man所在的迷宫。随着它的前进，它学会了在不同条件下不同状态变化的价值。
- en: '![](img/d79e079f-b596-42a2-ad11-58c39caf615f.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d79e079f-b596-42a2-ad11-58c39caf615f.png)'
- en: For example, in the preceding image, the state of Pac-Man might be defined by
    the fact that it has a ghost to the South, and a wall to the West, and empty spaces
    to the North and East, and that might define the current state of Pac-Man. The
    state changes it can take would be to move in a given direction. I can then learn
    the value of going in a certain direction. So, for example, if I were to move
    North, nothing would really happen, there's no real reward associated with that.
    But, if I were to move South I would be destroyed by the ghost, and that would
    be a negative value.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在前面的图像中，Pac-Man的状态可能是由它南边有一个幽灵，西边有一堵墙，北边和东边是空的空间来定义的，这可能定义了Pac-Man的当前状态。它可以进行的状态变化是朝特定方向移动。然后我可以学习朝某个方向前进的价值。例如，如果我向北移动，实际上不会发生什么，这并没有真正的奖励。但是，如果我向南移动，我会被幽灵摧毁，这将是一个负值。
- en: As I go and explore the entire space, I can build up a set of all the possible
    states that Pac-Man can be in, and the values associated with moving in a given
    direction in each one of those states, and that's reinforcement learning. And
    as it explores the whole space, it refines these reward values for a given state,
    and it can then use those stored reward values to choose the best decision to
    make given a current set of conditions. In addition to Pac-Man, there's also a
    game called Cat & Mouse that is an example that's used commonly that we'll look
    at later.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当我去探索整个空间时，我可以建立起一组所有可能的Pac-Man可能处于的状态，并与在每个状态中朝特定方向移动相关联的值，这就是强化学习。随着它探索整个空间，它会为给定状态细化这些奖励值，然后可以使用存储的奖励值来选择在当前条件下做出最佳决策。除了Pac-Man，还有一个叫做Cat
    & Mouse的游戏，这是一个常用的例子，我们稍后会看一下。
- en: The benefit of this technique is that once you've explored the entire set of
    possible states that your agent can be in, you can very quickly have a very good
    performance when you run different iterations of this. So, you know, you can basically
    make an intelligent Pac-Man by running reinforcement learning and letting it explore
    the values of different decisions it can make in different states and then storing
    that information, to very quickly make the right decision given a future state
    that it sees in an unknown set of conditions.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的好处在于，一旦你探索了你的代理可能处于的所有可能状态，你可以在运行不同迭代时非常快速地获得非常好的性能。所以，你可以通过运行强化学习来制作一个智能Pac-Man，让它探索在不同状态下可以做出不同决策的价值，然后存储这些信息，以便在未知条件下看到未来状态时快速做出正确的决策。
- en: Q-learning
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning
- en: 'So, a very specific implementation of reinforcement learning is called Q-learning,
    and this formalizes what we just talked about a little bit more:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，强化学习的一个非常具体的实现被称为Q-learning，这更加正式地阐述了我们刚刚谈到的内容：
- en: So again, you start with a set of environmental states of the agent (Is there
    a ghost next to me? Is there a power pill in front of me? Things like that.),
    we're going to call that s.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，你从代理的一组环境状态开始（我旁边有幽灵吗？我前面有能量丸吗？诸如此类的事情。），我们将称之为s。
- en: I have a set of possible actions that I can take in those states, we're going
    to call that set of actions a. In the case of Pac-Man, those possible actions
    are move up, down, left, or right.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我有一组可能在这些状态下采取的行动，我们将称之为a。在Pac-Man的情况下，这些可能的行动是向上、向下、向左或向右移动。
- en: Then we have a value for each state/action pair that we'll call Q; that's why
    we call it Q-learning. So, for each state, a given set of conditions surrounding
    Pac-Man, a given action will have a value *Q*. So, moving up might have a given
    value Q, moving down might have a negative *Q* value if it means encountering
    a ghost, for example.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们有每个状态/行动对的一个值，我们将其称为Q；这就是为什么我们称之为Q学习。因此，对于每个状态，Pac-Man周围给定的一组条件，给定的行动将有一个值*Q*。因此，向上移动可能会有一个给定的值Q，向下移动可能会有一个负的*Q*值，如果这意味着遇到幽灵，例如。
- en: So, we start off with a *Q* value of 0 for every possible state that Pac-Man
    could be in. And, as Pac-Man explores a maze, as bad things happen to Pac-Man,
    we reduce the *Q* value for the state that Pac-Man was in at the time. So, if
    Pac-Man ends up getting eaten by a ghost, we penalize whatever he did in that
    current state. As good things happen to Pac-Man, as he eats a power pill, or eats
    a ghost, we'll increase the *Q* value for that action, for the state that he was
    in. Then, what we can do is use those *Q* values to inform Pac-Man's future choices,
    and sort of build a little intelligent agent that can perform optimally, and make
    a perfect little Pac-Man. From the same image of Pac-Man that we saw just above,
    we can further define the current state of Pac-Man by defining that he has a wall
    to the West, empty space to the North and East, a ghost to the South.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们从Pac-Man可能处于的每个可能状态开始，都有一个*Q*值为0。随着Pac-Man探索迷宫，当Pac-Man遇到不好的事情时，我们会减少Pac-Man当时所处状态的*Q*值。因此，如果Pac-Man最终被幽灵吃掉，我们会惩罚他在当前状态所做的任何事情。当Pac-Man遇到好事时，当他吃到一个能量丸或吃掉一个幽灵时，我们将增加他所处状态的那个动作的*Q*值。然后，我们可以使用这些*Q*值来指导Pac-Man的未来选择，并构建一个可以表现最佳的小智能体，制作一个完美的小Pac-Man。从我们刚才看到的Pac-Man的相同图像开始，我们可以通过定义他的西边有一堵墙，北边和东边有空地，南边有一个幽灵来进一步定义Pac-Man的当前状态。
- en: 'We can look at the actions he can take: he can''t actually move left at all,
    but he can move up, down, or right, and we can assign a value to all those actions.
    By going up or right, nothing really happens at all, there''s no power pill or
    dots to consume. But if he goes left, that''s definitely a negative value. We
    can say for the state given by the current conditions that Pac-Man is surrounded
    by, moving down would be a really bad choice; there should be a negative *Q* value
    for that. Moving left just can''t be done at all. Moving up or right or staying
    neutral, the *Q* value would remain 0 for those action choices for that given
    state.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看看他可以采取的行动：他实际上根本不能向左移动，但他可以向上、向下或向右移动，我们可以为所有这些行动分配一个值。向上或向右移动，实际上什么都不会发生，没有能量丸或点可以消耗。但如果他向左走，那肯定是一个负值。我们可以说对于由当前条件给出的状态，Pac-Man所处的状态，向下移动将是一个非常糟糕的选择；对于那个给定状态的那些行动选择，应该有一个负的*Q*值。根本不能向左移动。向上或向右或保持中立，*Q*值对于那个给定状态的那些行动选择将保持为0。
- en: 'Now, you can also look ahead a little bit, to make an even more intelligent
    agent. So, I''m actually two steps away from getting a power pill here. So, as
    Pac-Man were to explore this state, if I were to hit the case of eating that power
    pill on the next state, I could actually factor that into the *Q* value for the
    previous state. If you just have some sort of a discount factor, based on how
    far away you are in time, how many steps away you are, you can factor that all
    in together. So, that''s a way of actually building in a little bit of memory
    into the system. You can "look ahead" more than one step by using a discount factor
    when computing Q (here *s* is previous state, *s''* is current state):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你也可以向前看一点，使智能体变得更加智能。因此，实际上我离得到能量丸还有两步。因此，如果Pac-Man要探索这个状态，如果我在下一个状态吃到那个能量丸，我实际上可以将其纳入到先前状态的*Q*值中。如果你只有某种折扣因子，基于你在时间上有多远，你有多少步远，你可以将所有这些因素结合在一起。这是实际上在系统中建立一点记忆的方法。你可以使用折扣因子来计算Q（这里*s*是先前的状态，*s'*是当前的状态）来向前看超过一步：
- en: '*Q(s,a) += discount * (reward(s,a) + max(Q(s'')) - Q(s,a))*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q(s,a) += 折扣 * (奖励(s,a) + max(Q(s'')) - Q(s,a))*'
- en: So, the *Q* value that I experience when I consume that power pill might actually
    give a boost to the previous *Q* values that I encountered along the way. So,
    that's a way to make Q-learning even better.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我消耗那个能量丸时所体验到的*Q*值实际上可能会提升我沿途遇到的先前*Q*值。这是使Q学习变得更好的一种方法。
- en: The exploration problem
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索问题
- en: One problem that we have in reinforcement learning is the exploration problem.
    How do I make sure that I efficiently cover all the different states and actions
    within those states during the exploration phase?
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中我们面临的一个问题是探索问题。在探索阶段，我如何确保有效地覆盖所有不同的状态和这些状态中的行动？
- en: The simple approach
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的方法
- en: One simple approach is to always choose the action for a given state with the
    highest *Q* value that I've computed so far, and if there's a tie, just choose
    at random. So, initially all of my *Q* values might be 0, and I'll just pick actions
    at random at first.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是始终选择具有迄今为止计算出的最高*Q*值的给定状态的动作，如果有平局，就随机选择。因此，最初我的所有*Q*值可能都是0，我会首先随机选择动作。
- en: As I start to gain information about better *Q* values for given actions and
    given states, I'll start to use those as I go. But, that ends up being pretty
    inefficient, and I can actually miss a lot of paths that way if I just tie myself
    into this rigid algorithm of always choosing the best *Q* value that I've computed
    thus far.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始获得关于给定动作和给定状态的更好*Q*值的信息时，我将开始在前进时使用它们。但是，这最终会变得非常低效，如果我只将自己固定在始终选择到目前为止计算出的最佳*Q*值的这种刚性算法中，我实际上可能会错过很多路径。
- en: The better way
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更好的方法
- en: So, a better way is to introduce a little bit of random variation into my actions
    as I'm exploring. So, we call that an epsilon term. So, suppose we have some value,
    that I roll the dice, I have a random number. If it ends up being less than this
    epsilon value, I don't actually follow the highest *Q* value; I don't do the thing
    that makes sense, I just take a path at random to try it out, and see what happens.
    That actually lets me explore a much wider range of possibilities, a much wider
    range of actions, for a wider range of states more efficiently during that exploration
    stage.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，更好的方法是在探索时引入一些随机变化到我的行动中。因此，我们称之为一个epsilon项。假设我们有某个值，我掷骰子，得到一个随机数。如果它小于这个epsilon值，我实际上不遵循最高的*Q*值；我不做有意义的事情，我只是随机选择一条路径来尝试一下，看看会发生什么。这实际上让我在探索阶段更有效地探索更广泛的可能性、更广泛的行动，更广泛的状态。
- en: So, what we just did can be described in very fancy mathematical terms, but
    you know conceptually it's pretty simple.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们刚刚做的事情可以用非常花哨的数学术语来描述，但你知道概念上它相当简单。
- en: Fancy words
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 花哨的词
- en: I explore some set of actions that I can take for a given set of states, I use
    that to inform the rewards associated with a given action for a given set of states,
    and after that exploration is done I can use that information, those *Q* values,
    to intelligently navigate through an entirely new maze for example.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我探索一些我可以在给定状态下采取的行动，我用它来指导与给定状态相关的给定行动的奖励，探索结束后，我可以使用那些*Q*值的信息，来智能地穿越一个全新的迷宫，例如。
- en: This can also be called a Markov decision process. So again, a lot of data science
    is just assigning fancy, intimidating names to simple concepts, and there's a
    ton of that in reinforcement learning.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以称为马尔可夫决策过程。因此，很多数据科学只是给简单的概念赋予花哨、令人生畏的名字，强化学习中也有很多这样的情况。
- en: Markov decision process
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: So, if you look up the definition of Markov decision processes, it is "a mathematical
    framework for modeling decision making in situations where outcomes are partly
    random and partly under the control of a decision maker".
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你查阅马尔可夫决策过程的定义，它是“一个数学框架，用于建模决策，其中结果部分是随机的，部分受决策者控制”。
- en: '**Decision making**: What action do we take given a set of possibilities for
    a given state?'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策**: 在给定状态的一系列可能性中，我们采取什么行动？'
- en: '**In situations where outcomes are partly random**: Hmm, kind of like our random
    exploration there.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在结果部分是随机的情况下**: 嗯，有点像我们的随机探索。'
- en: '**Partly under the control of a decision maker**: The decision maker is our
    *Q* values that we computed.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部分受决策者控制**: 决策者是我们计算出的*Q*值。'
- en: 'So, MDPs, Markov decision processes, are a fancy way of describing our exploration
    algorithm that we just described for reinforcement learning. The notation is even
    similar, states are still described as s, and s'' is the next state that we encounter.
    We have state transition functions that are defined as *P[a]* for a given state
    of s and s''. We have our *Q* values, which are basically represented as a reward
    function, an *R[a]* value for a given s and s''. So, moving from one state to
    another has a given reward associated with it, and moving from one state to another
    is defined by a state transition function:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，MDPs，马尔可夫决策过程，是一种花哨的方式来描述我们刚刚为强化学习描述的探索算法。符号甚至相似，状态仍然被描述为s，s'是我们遇到的下一个状态。我们有被定义为*P[a]*的状态转移函数，对于给定的s和s'。我们有我们的*Q*值，它们基本上被表示为一个奖励函数，对于给定的s和s'有一个*R[a]*值。因此，从一个状态转移到另一个状态有一个与之相关的奖励，从一个状态转移到另一个状态由一个状态转移函数定义：
- en: States are still described as *s* and *s''*
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态仍然被描述为*s*和*s''*
- en: State transition functions are described as *Pa(s,s')*
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态转移函数被描述为*Pa(s,s')*
- en: Our *Q* values are described as a reward function *Ra(s,s')*
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的*Q*值被描述为奖励函数*Ra(s,s')*
- en: 'So again, describing what we just did, only a mathematical notation, and a
    fancier sounding word, Markov decision processes. And, if you want to sound even
    smarter, you can also call a Markov decision process by another name: a discrete
    time stochastic control process. That sounds intelligent! But the concept itself
    is the same thing that we just described.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，再次描述我们刚刚做的事情，只是用数学符号和一个更花哨的词，马尔可夫决策过程。如果你想要听起来更聪明一点，你也可以用另一个名字来称呼马尔可夫决策过程：离散时间随机控制过程。听起来很聪明！但概念本身就是我们刚刚描述的东西。
- en: Dynamic programming
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态规划
- en: 'So, even more fancy words: dynamic programming can be used to describe what
    we just did as well. Wow! That sounds like artificial intelligence, computers
    programming themselves, Terminator 2, Skynet stuff. But no, it''s just what we
    just did. If you look up the definition of dynamic programming, it is a method
    for solving a complex problem by breaking it down into a collection of simpler
    subproblems, solving each of those subproblems just once, and storing their solutions
    ideally, using a memory-based data structure.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，更花哨的词：动态规划也可以用来描述我们刚刚做的事情。哇！听起来像是人工智能，计算机自我编程，《终结者2》，天网之类的东西。但不，这只是我们刚刚做的事情。如果你查阅动态规划的定义，它是一种通过将复杂问题分解为一系列更简单的子问题来解决复杂问题的方法，每个子问题只解决一次，并理想地存储它们的解决方案，使用基于内存的数据结构。
- en: 'The next time the same subproblem occurs, instead of recomputing its solution,
    one simply looks up the previously computed solution thereby saving computation
    time at the expense of a (hopefully) modest expenditure in storage space:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 下次出现相同的子问题时，不需要重新计算其解决方案，只需查找先前计算的解决方案，从而节省计算时间，但以（希望）在存储空间上进行适度的开销：
- en: '**A method for solving a complex problem**: Same as creating an intelligent
    Pac-Man, that''s a pretty complicated end result.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解决复杂问题的方法**: 就像创造一个智能的吃豆人，这是一个相当复杂的最终结果。'
- en: '**By breaking it down into a collection of simpler subproblems**: So, for example,
    what is the optimal action to take for a given state that Pac-Man might be in.
    There are many different states Pac-Man could find himself in, but each one of
    those states represents a simpler subproblem, where there''s a limited set of
    choices I can make, and there''s one right answer for the best move to make.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将其分解为一系列更简单的子问题：例如，对于可能出现在Pac-Man中的给定状态，采取的最佳行动是什么。Pac-Man可能会发现自己处于许多不同的状态，但每个状态都代表一个更简单的子问题，在这个子问题中，我可以做出有限的选择，并且有一个正确的答案来做出最佳的移动。
- en: '**Storing their solutions**: Those solutions being the *Q* values that I associated
    with each possible action at each state.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储它们的解决方案**：这些解决方案是我与每个可能的动作在每个状态关联的*Q*值。'
- en: '**Ideally, using a memory-based data structure**: Well, of course I need to
    store those *Q* values and associate them with states somehow, right?'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理想情况下，使用基于内存的数据结构**：当然，我需要以某种方式存储这些*Q*值并将它们与状态关联起来，对吧？'
- en: '**The next time the same subproblem occurs**: The next time Pac-Man is in a
    given state that I have a set of *Q* values for.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下次出现相同的子问题**：下次Pac-Man处于我已经有一组*Q*值的给定状态时。'
- en: '**Instead of recomputing its solution, one simply looks up the previously computed
    solution**: The *Q* value I already have from the exploration stage.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**而不是重新计算其解决方案，只需查找先前计算的解决方案**：我已经从探索阶段得到的*Q*值。'
- en: '**Thereby saving computation time at the expense of a modest expenditure in
    storage space**: That''s exactly what we just did with reinforcement learning.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从而节省计算时间，以牺牲存储空间的适度开支**：这正是我们刚刚用强化学习做的。'
- en: We have a complicated exploration phase that finds the optimal rewards associated
    with each action for a given state. Once we have that table of the right action
    to take for a given state, we can very quickly use that to make our Pac-Man move
    in an optimal manner in a whole new maze that he hasn't seen before. So, reinforcement
    learning is also a form of dynamic programming. Wow!
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个复杂的探索阶段，找到与每个动作对应的给定状态的最佳奖励。一旦我们有了这个表格，我们就可以非常快速地使用它来使我们的Pac-Man在一个全新的迷宫中以最佳方式移动。因此，强化学习也是一种动态规划的形式。哇！
- en: To recap, you can make an intelligent Pac-Man agent by just having it semi-randomly
    explore different choices of movement given different conditions, where those
    choices are actions and those conditions are states. We keep track of the reward
    or penalty associated with each action or state as we go, and we can actually
    discount, going back multiple steps if you want to make it even better.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，你可以通过让它半随机地探索不同的移动选择来制作一个智能的Pac-Man代理，给定不同的条件，其中这些选择是动作，这些条件是状态。我们在进行时跟踪与每个动作或状态相关联的奖励或惩罚，我们实际上可以打折，回溯多步，如果你想让它变得更好。
- en: 'Then we store those *Q* values that we end up associating with each state,
    and we can use that to inform its future choices. So we can go into a whole new
    maze, and have a really smart Pac-Man that can avoid the ghosts and eat them up
    pretty effectively, all on its own. It''s a pretty simple concept, very powerful
    though. You can also say that you understand a bunch of fancy terms because it''s
    all called the same thing. Q-learning, reinforcement learning, Markov decision
    processes, dynamic programming: all tied up in the same concept.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们存储这些*Q*值，我们可以使用它来指导其未来的选择。因此，我们可以进入一个全新的迷宫，并且有一个非常聪明的Pac-Man，可以有效地避开幽灵并吃掉它们。这是一个非常简单但非常强大的概念。你也可以说你理解了一堆花哨的术语，因为它都是同一个东西。Q学习，强化学习，马尔可夫决策过程，动态规划：都与同一个概念相关联。
- en: 'I don''t know, I think it''s pretty cool that you can actually make sort of
    an artificially intelligent Pac-Man through such a simple technique, and it really
    does work! If you want to go look at it in more detail, following are a few examples
    you can look at that have one actual source code you can look at, and potentially
    play with, **Python Markov Decision Process Toolbox**: [http://pymdptoolbox.readthedocs.org/en/latest/api/mdp.html](http://pymdptoolbox.readthedocs.org/en/latest/api/mdp.html).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道，我觉得你实际上可以通过这样一个简单的技术制作出一种人工智能的Pac-Man，这真的很酷！如果你想更详细地了解它，以下是一些示例，你可以查看其中一个实际的源代码，并且可能进行调试，**Python马尔可夫决策过程工具包**：[http://pymdptoolbox.readthedocs.org/en/latest/api/mdp.html](http://pymdptoolbox.readthedocs.org/en/latest/api/mdp.html)。
- en: There is a Python Markov decision process toolbox that wraps it up in all that
    terminology we talked about. There's an example you can look at, a working example
    of the cat and mouse game, which is similar. And, there is actually a Pac-Man
    example you can look at online as well, that ties in more directly with what we
    were talking about. Feel free to explore these links, and learn even more about
    it.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个Python马尔可夫决策过程工具包，它用我们谈到的所有术语包装起来。有一个你可以查看的示例，一个关于猫和老鼠游戏的工作示例，类似的。实际上，还有一个你可以在线查看的Pac-Man示例，它更直接地与我们谈论的内容相关。请随意探索这些链接，并了解更多。
- en: And so that's reinforcement learning. More generally, it's a useful technique
    for building an agent that can navigate its way through a possible different set
    of states that have a set of actions that can be associated with each state. So,
    we've talked about it mostly in the context of a maze game. But, you can think
    more broadly, and you know whenever you have a situation where you need to predict
    behavior of something given a set of current conditions and a set of actions it
    can take. Reinforcement learning and Q-learning might be a way of doing it. So,
    keep that in mind!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是强化学习。更一般地说，这是一种构建代理程序的有用技术，该代理程序可以在可能具有一组与每个状态相关联的动作的不同状态中导航。因此，我们大多数时候在迷宫游戏的背景下讨论它。但是，你可以更广泛地思考，你知道每当你需要根据一组当前条件和一组可以采取的行动来预测某物的行为时。强化学习和Q学习可能是一种方法。所以，请记住这一点！
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw one of the simplest techniques of machine learning called
    k-nearest neighbors. We also looked at an example of KNN which predicts the rating
    for a movie. We analysed the concepts of dimensionality reduction and principal
    component analysis and saw an example of PCA, which reduced 4D data to two dimensions
    while still preserving its variance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了一种最简单的机器学习技术，称为k最近邻算法。我们还看了一个KNN的例子，它预测了一部电影的评分。我们分析了降维和主成分分析的概念，并看到了一个PCA的例子，它将4D数据降低到两个维度，同时保留了其方差。
- en: Next, we learned the concept of data warehousing and saw how using the ELT process
    instead of ETL makes more sense today. We walked through the concept of reinforcement
    learning and saw how it is used behind the Pac-Man game. Finally, we saw some
    fancy words used for reinforcement learning (Q-learning, Markov decision process,
    and dynamic learning). In the next chapter, we'll see how to deal with real-world
    data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了数据仓库的概念，并看到了如何在今天使用ELT过程而不是ETL更有意义。我们深入了解了强化学习的概念，并看到了它在Pac-Man游戏背后的应用。最后，我们看到了一些用于强化学习的花哨词汇（Q学习，马尔可夫决策过程和动态学习）。在下一章中，我们将看到如何处理真实世界的数据。
