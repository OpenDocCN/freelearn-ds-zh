- en: Learning Machine Learning - Spark MLlib and Spark ML
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习机器学习- Spark MLlib和Spark ML
- en: '"Each of us, actually every animal, is a data scientist. We collect data from
    our sensors, and then we process the data to get abstract rules to perceive our
    environment and control our actions in that environment to minimize pain and/or
    maximize pleasure. We have memory to store those rules in our brains, and then
    we recall and use them when needed. Learning is lifelong; we forget rules when
    they no longer apply or revise them when the environment changes."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “我们每个人，实际上每个动物，都是数据科学家。我们从传感器中收集数据，然后处理数据，得到抽象规则来感知我们的环境，并控制我们在环境中的行为，以最大程度地减少痛苦和/或最大化快乐。我们有记忆来将这些规则存储在我们的大脑中，然后在需要时回忆和使用它们。学习是终身的；当规则不再适用或环境发生变化时，我们会忘记规则或修订规则。”
- en: '- Ethem Alpaydin, Machine Learning: The New AI'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- Ethem Alpaydin，《机器学习：新人工智能》'
- en: 'The purpose of this chapter is to provide a conceptual introduction to statistical
    machine learning (ML) techniques for those who might not normally be exposed to
    such approaches during their typical required statistical training. This chapter
    also aims to take a newcomer from having minimal knowledge of machine learning
    all the way to being a knowledgeable practitioner in a few steps. We will focus
    on Spark''s machine learning APIs, called Spark MLlib and ML, in theoretical and
    practical ways. Furthermore, we will provide some examples covering feature extraction
    and transformation, dimensionality reduction, regression, and classification analysis.
    In a nutshell, we will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是为那些在典型的统计培训中可能不会接触到这些方法的人提供统计机器学习（ML）技术的概念介绍。本章还旨在通过几个步骤，将新手从对机器学习了解甚少，提升到成为了解的实践者。我们将以理论和实践的方式专注于Spark的机器学习API，称为Spark
    MLlib和ML。此外，我们将提供一些涵盖特征提取和转换、降维、回归和分类分析的示例。简而言之，本章将涵盖以下主题：
- en: Introduction to machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习介绍
- en: Spark machine learning APIs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark机器学习API
- en: Feature extractor and transformation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取和转换
- en: Dimensionality reduction using PCA for regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA进行回归的降维
- en: Binary and multiclass classification
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元和多类分类
- en: Introduction to machine learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习介绍
- en: In this section, we will try to define machine learning from computer science,
    statistics, and data analytical perspectives. **Machine learning (ML)** is the
    branch of computer science that provides the computers the ability to learn without
    being explicitly programmed (Arthur Samuel in 1959). This field of study being
    evolved from the study of pattern recognition and computational learning theory
    in artificial intelligence.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试从计算机科学、统计学和数据分析的角度定义机器学习。**机器学习（ML）**是计算机科学的一个分支，它使计算机能够在没有明确编程的情况下学习（1959年Arthur
    Samuel）。这一研究领域是从人工智能中的模式识别和计算学习理论中发展而来的。
- en: 'More specifically, ML explores the study and construction of algorithms that
    can learn from heuristics and make predictions on data. This kind of algorithms
    overcome the strictly static program instructions by making data-driven predictions
    or decisions, through building a model from sample inputs. Now let''s more explicit
    and versatile definition from Prof. Tom M. Mitchell, who explained what machine
    learning really means from the computer science perspective:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，ML探索了可以从启发式学习并对数据进行预测的算法的研究和构建。这种算法通过从样本输入构建模型，克服了严格静态的程序指令，进行数据驱动的预测或决策。现在让我们从计算机科学的角度更明确和多样化地定义，来自Tom
    M. Mitchell教授的定义，解释了机器学习从计算机科学的角度真正意味着什么：
- en: A computer program is said to learn from experience E with respect to some class
    of tasks T and performance measure P, if its performance at tasks in T, as measured
    by P, improves with experience E.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机程序被认为是在某类任务T和性能度量P方面从经验E中学习，如果它在T中的任务表现，根据P的度量，随着经验E的提高而改善。
- en: 'Based on that definition, we can conclude that a computer program or machine
    can:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个定义，我们可以得出结论，计算机程序或机器可以：
- en: Learn from data and histories
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据和历史中学习
- en: Be improved with experience
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过经验改进
- en: Interactively enhance a model that can be used to predict the outcomes of questions
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交互式地增强可以用来预测问题结果的模型
- en: Typical machine learning tasks are concept learning, predictive modeling, clustering,
    and finding useful patterns. The ultimate goal is to improve learning in such
    a way that it becomes automatic so that no human interactions are needed anymore,
    or to reduce the level of human interaction as much as possible. Although machine
    learning is sometimes conflated with **Knowledge Discovery and Data Mining** (**KDDM**),
    but KDDM, focuses more on exploratory data analysis and is known as unsupervised
    learning. Typical machine learning applications can be classified into scientific
    knowledge discovery and more commercial applications, ranging from Robotics or
    **Human-Computer Interaction** (**HCI**) to anti-spam filtering and recommender
    systems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的机器学习任务包括概念学习、预测建模、聚类和发现有用的模式。最终目标是通过改进学习方式，使其变得自动化，以至于不再需要人类干预，或者尽可能减少人类干预的程度。尽管机器学习有时与**知识发现和数据挖掘**（**KDDM**）混淆，但KDDM更侧重于探索性数据分析，被称为无监督学习。典型的机器学习应用可以分为科学知识发现和更商业化的应用，从机器人技术或**人机交互**（**HCI**）到反垃圾邮件过滤和推荐系统。
- en: Typical machine learning workflow
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 典型的机器学习工作流程
- en: 'A typical machine learning application involves several steps ranging from
    the input, processing, to output, which forms a scientific workflow, as shown
    in *Figure 1*. The following steps are involved in a typical machine learning
    application:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的机器学习应用包括从输入、处理到输出的几个步骤，形成了一个科学工作流程，如*图1*所示。典型的机器学习应用涉及以下步骤：
- en: Load the sample data.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载样本数据。
- en: Parse the data into the input format for the algorithm.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据解析成算法的输入格式。
- en: Preprocess the data and handle the missing values.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据和处理缺失值。
- en: 'Split the data into two sets: one for building the model (training dataset)
    and one for testing the model (validation dataset).'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分成两组：用于构建模型的训练数据集和用于测试模型的验证数据集。
- en: Run the algorithm to build or train your ML model.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行算法来构建或训练您的ML模型。
- en: Make predictions with the training data and observe the results.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练数据进行预测并观察结果。
- en: Test and evaluate the model with the test data or, alternatively, validate the
    model using a cross-validator technique using the third dataset, called the validation
    dataset.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用测试数据测试和评估模型，或者使用交叉验证技术使用第三个数据集（验证数据集）验证模型。
- en: Tune the model for better performance and accuracy.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整模型以获得更好的性能和准确性。
- en: Scale up the model so that it will be able to handle massive datasets in future.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展模型，以便能够处理未来的大规模数据集。
- en: Deploy the ML model in commercialization.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在商业化中部署ML模型。
- en: '![](img/00266.jpeg)**Figure 1:** Machine learning workflow'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1**：机器学习工作流程'
- en: Often, the machine learning algorithms have some ways to handle skewness in
    the datasets. That skewness is sometimes immense though. In step 4, the experimental
    dataset is randomly split, often into a training set and a test set, which is
    called sampling. The training dataset is used to train the model, whereas the
    test dataset is used to evaluate the performance of the best model at the very
    end. The better practice is to use the training dataset as much as you can to
    increase generalization performance. On the other hand, it is recommended to use
    the test dataset only once, to avoid the overfitting problem while computing the
    prediction error and the related metrics.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，机器学习算法有一些方法来处理数据集中的偏斜。这种偏斜有时候是巨大的。在步骤4中，实验数据集通常被随机分成训练集和测试集，这被称为抽样。训练数据集用于训练模型，而测试数据集用于评估最佳模型的性能。更好的做法是尽可能多地使用训练数据集来提高泛化性能。另一方面，建议只使用测试数据集一次，以避免在计算预测误差和相关指标时出现过拟合问题。
- en: Machine learning tasks
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习任务
- en: 'depending on the nature of the learning feedback available to a learning system,
    ML tasks or process are typically classified into three broad categories: supervised
    learning, unsupervised learning, and reinforcements learning shown in figure 2\.
    Furthermore, there are other machine learning tasks as well, for example, dimensionality
    reduction, recommendation system, frequent pattern mining, and so on.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据学习系统可用的学习反馈的性质，ML任务或过程通常分为三大类：监督学习、无监督学习和强化学习，如图2所示。此外，还有其他机器学习任务，例如降维、推荐系统、频繁模式挖掘等等。
- en: '![](img/00272.jpeg)**Figure 2:** Machine learning tasks'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2**：机器学习任务'
- en: Supervised learning
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'A supervised learning application makes predictions based on a set of examples,
    and the goal is to learn general rules that map inputs to outputs aligning with
    the real world. For example, a dataset for spam filtering usually contains spam
    messages as well as non-spam messages. Therefore, we are able to know whether
    messages in the training set are spam or ham. Nevertheless, we might have the
    opportunity to use this information to train our model in order to classify new
    unseen messages. The following figure shows the schematic diagram of supervised
    learning. After the algorithm has found the required patterns, those patterns
    can be used to make predictions for unlabeled test data. This is the most popular
    and useful type of machine learning task, that is not an exception for Spark as
    well, where most of the algorithms are supervised learning techniques:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习应用是基于一组示例进行预测的，其目标是学习将输入映射到与现实世界一致的输出的一般规则。例如，用于垃圾邮件过滤的数据集通常包含垃圾邮件和非垃圾邮件。因此，我们能够知道训练集中的消息是垃圾邮件还是正常邮件。然而，我们可能有机会利用这些信息来训练我们的模型，以便对新的未见过的消息进行分类。下图显示了监督学习的示意图。算法找到所需的模式后，这些模式可以用于对未标记的测试数据进行预测。这是最流行和有用的机器学习任务类型，对Spark也不例外，那里的大多数算法都是监督学习技术：
- en: '![](img/00278.jpeg)**Figure 3**: Supervised learning in action'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3**：监督学习实例'
- en: Examples include classification and regression for solving supervised learning
    problems. We will provide several examples of supervised learning, such as logistic
    regression, random forest, decision trees, Naive Bayes, One-vs-the-Rest, and so
    on in this book. However, to make the discussion concrete, only logistic regression
    and the random forest will be discussed, and other algorithms will be discussed
    in [Chapter 12](part0383.html#BD87E1-21aec46d8593429cacea59dbdcd64e1c), *Advanced
    Machine Learning Best Practices*, with some practical examples. On the other hand,
    linear regression will be discussed for the regression analysis.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，分类和回归用于解决监督学习问题。我们将提供几个监督学习的例子，比如逻辑回归、随机森林、决策树、朴素贝叶斯、一对多等等。然而，为了让讨论更具体，本书只会讨论逻辑回归和随机森林，其他算法将在[第12章](part0383.html#BD87E1-21aec46d8593429cacea59dbdcd64e1c)《高级机器学习最佳实践》中讨论，并附有一些实际例子。另一方面，线性回归将用于回归分析。
- en: Unsupervised learning
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In unsupervised learning, data points have no labels related with them. Therefore,
    we need to put labels on it algorithmically, as shown in the following figure.
    In other words, the correct classes of the training dataset in unsupervised learning
    are unknown. Consequently, classes have to be inferred from the unstructured datasets,
    which imply that the goal of an unsupervised learning algorithm is to preprocess
    the data in some structured ways by describing its structure.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，数据点没有与之相关的标签。因此，我们需要以算法方式对其进行标记，如下图所示。换句话说，在无监督学习中，训练数据集的正确类别是未知的。因此，类别必须从非结构化数据中推断出来，这意味着无监督学习算法的目标是通过描述其结构来对数据进行某种结构化的预处理。
- en: To overcome this obstacle in unsupervised learning, clustering techniques are
    commonly used to group the unlabeled samples based on certain similarity measures.
    Therefore, this task also involves mining hidden patterns toward feature learning.
    Clustering is the process of intelligently categorizing the items in your dataset.
    The overall idea is that two items in the same cluster are “closer” to each other
    than items that belong to separate clusters. That is the general definition, leaving
    the interpretation of “closeness” open.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服无监督学习中的障碍，通常使用聚类技术根据某些相似性度量对未标记的样本进行分组。因此，这项任务还涉及挖掘隐藏模式以进行特征学习。聚类是智能地对数据集中的项目进行分类的过程。总体思想是，同一聚类中的两个项目比属于不同聚类的项目“更接近”。这是一般定义，留下了“接近”的解释。
- en: '![](img/00282.jpeg)**Figure 4**: Unsupervised learning'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00282.jpeg)**图4**：无监督学习'
- en: Examples include clustering, frequent pattern mining, and dimensionality reduction
    for solving unsupervised learning problems (it can be applied to supervised learning
    problems too). We will provide several examples of unsupervised learning, such
    as k-means, bisecting k-means, Gaussian mixture model, **Latent dirichlet allocation**
    (**LDA**), and so on, in this book. We will also show how to use a dimensionality
    reduction algorithm such as **Principal Component Analysis** (**PCA**) or **Singular
    Value Decomposition** (**SVD**) in supervised learning through regression analysis.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 示例包括聚类、频繁模式挖掘和降维以解决无监督学习问题（也可以应用于监督学习问题）。我们将在本书中提供几个无监督学习的例子，如k均值、二分k均值、高斯混合模型、**潜在狄利克雷分配**（**LDA**）等。我们还将展示如何通过回归分析在监督学习中使用降维算法，如**主成分分析**（**PCA**）或**奇异值分解**（**SVD**）。
- en: '**Dimensionality reduction** (**DR**): Dimensionality reduction is a technique
    used to reduce the number of random variables under certain considerations. This
    technique is used for both supervised and unsupervised learning. Typical advantages
    of using DR techniques are as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**降维**（**DR**）：降维是一种在特定条件下减少随机变量数量的技术。这种技术用于监督学习和无监督学习。使用降维技术的典型优势如下：'
- en: It reduces the time and storage space required in machine learning tasks
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它减少了机器学习任务所需的时间和存储空间
- en: It helps remove multicollinearity and improves the performance of the machine
    learning model
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有助于消除多重共线性，并提高机器学习模型的性能
- en: Data visualization becomes easier when reduced to very low dimensions such as
    2D or 3D
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化变得更容易，当降低到非常低的维度，如2D或3D时
- en: Reinforcement learning
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: As a human being, you and we also learn from past experiences. We haven't got
    so charming by accident. Years of positive compliments as well as negative criticism
    have all helped shape us who we are today. You learn what makes people happy by
    interacting with friends, family, or even strangers, and you figure out how to
    ride a bike by trying out different muscle movements until it just clicks. When
    you perform actions, you're sometimes rewarded immediately. For example, finding
    a shopping mall nearby might yield instant gratification. Other times, the reward
    doesn't appear right away, such as traveling a long distance to find an exceptional
    place to eat. These are all about Reinforcement Learning (RL)**.**
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，你和我们也从过去的经验中学习。我们并不是偶然变得迷人的。多年来的积极赞美和负面批评都帮助塑造了我们今天的样子。通过与朋友、家人甚至陌生人的互动，你学会了如何让人们快乐，通过尝试不同的肌肉运动，你学会了如何骑自行车，直到顿悟。当你执行动作时，有时会立即得到奖励。例如，找到附近的购物中心可能会带来即时的满足感。其他时候，奖励不会立即出现，比如长途旅行找到一个特别好的吃饭地方。这些都是关于强化学习（RL）的。
- en: 'Thus RL is a technique, where the model itself learns from a series of actions
    or behaviors. The complexity of the dataset, or sample complexity, is very important
    in the reinforcement learning needed for the algorithms to learn a target function
    successfully. Moreover, in response to each data point for achieving the ultimate
    goal, maximization of the reward function should be ensured while interacting
    with an external environment, as demonstrated in the following figure:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RL是一种技术，模型本身从一系列行为或动作中学习。数据集的复杂性或样本复杂性对于强化学习需要的算法成功学习目标函数非常重要。此外，为了实现最终目标，与外部环境交互时应确保最大化奖励函数，如下图所示：
- en: '![](img/00288.jpeg)**Figure 5**: Reinforcement learning'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00288.jpeg)**图5**：强化学习'
- en: 'Reinforcement learning techniques are being used in many areas. Here''s a very
    short list includes the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习技术正在许多领域中使用。以下是一个非常简短的列表：
- en: Advertising helps in learning rank, using one-shot learning for emerging items,
    and new users will bring more money
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广告有助于学习排名，对新出现的项目使用一次性学习，新用户将带来更多的收入
- en: Teaching robots new tasks, while retaining prior knowledge
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教导机器人新任务，同时保留先前的知识
- en: Deriving complex hierarchical schemes, from chess gambits to trading strategies
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从国际象棋开局到交易策略推导复杂的分层方案
- en: Routing problems, for example, management of a shipping fleet, which trucks/truckers
    to assign to which cargo
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路由问题，例如，管理船队，分配卡车/司机到哪个货物
- en: In robotics, the algorithm must choose the robot's next action based on a set
    of sensor readings
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器人技术中，算法必须根据一组传感器读数选择机器人的下一个动作
- en: It is also a natural fit for **Internet of Things** (**IoT**) applications,
    where a computer program interacts with a dynamic environment in which it must
    perform a certain goal without an explicit mentor
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它也是**物联网**（**IoT**）应用的自然选择，其中计算机程序与动态环境进行交互，必须在没有明确导师的情况下实现某个目标
- en: One of the simplest RL problems is called n-armed bandits. The thing is there
    are n-many slot machines but each has different fixed pay-out probability. The
    goal is to maximize the profit by always choosing the machine with the best payout
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最简单的强化学习问题之一被称为n臂老虎机。问题在于有n台老虎机，但每台的固定支付概率不同。目标是通过始终选择支付最佳的机器来最大化利润。
- en: An emerging area for applying is the stock market trading. Where a trader acts
    like a reinforcement agent since buying and selling (i.e. action) particular stock
    changes the state of the trader by generating profit or loss i.e. reward.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个新兴的应用领域是股票市场交易。在这里，交易员就像一个强化学习代理，因为购买和出售（即行动）特定股票会通过产生利润或损失来改变交易员的状态，即奖励。
- en: Recommender system
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统
- en: A recommender system is a subclass of an information filtering system that looks
    to predict the rating or preference that users usually provide for an item. The
    concept of recommender systems has become very common in recent years subsequently
    being applied in different applications.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是信息过滤系统的一个子类，旨在预测用户通常对物品提供的评分或偏好。推荐系统的概念近年来变得非常普遍，并随后被应用于不同的应用程序。
- en: '![](img/00294.jpeg)**Figure 6**: Different recommender system'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00294.jpeg)**图6**：不同的推荐系统'
- en: 'The most popular ones are probably products (for example, movies, music, books,
    research articles, news, search queries, social tags, and so on). Recommender
    systems can be classified into the following four categories typically:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的可能是产品（例如电影、音乐、书籍、研究文章、新闻、搜索查询、社交标签等）。推荐系统通常可以被分类为以下四类：
- en: Collaborative filtering, also referred to as social filtering that filters information
    by using the recommendations of other people. The thing is people who agreed in
    their evaluation of certain items in the past are likely to agree again in the
    future. Therefore, a person who wants to see a movie for example, might ask for
    recommendations from his/her friends. Now once he received the recommendations
    from some of his/her friends who have similar interests, are trusted more than
    recommendations from others. This information is used in the decision on which
    movie to see.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协同过滤，也称为社交过滤，通过使用其他人的推荐来过滤信息。问题在于过去对某些物品评价意见一致的人未来可能再次意见一致。因此，例如，想要观看电影的人可能会向朋友们寻求推荐。一旦他收到了一些有相似兴趣的朋友的推荐，这些推荐就比其他人的推荐更可信。这些信息被用于决定要观看哪部电影。
- en: 'Content-based filtering (also known as cognitive filtering), which recommends
    items based on a comparison between the content of the items and a user profile.
    The content of each item is represented as a set of descriptors or terms, typically
    the words that occur in a document. The user profile is represented with the same
    terms and built up by analyzing the content of items that have been seen by the
    user. However, while implementing these types of recommendation systems, some
    issues that need to be considered are as follows:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于内容的过滤（也称为认知过滤），根据物品内容和用户个人资料之间的比较来推荐物品。每个物品的内容被表示为一组描述符或术语，通常是文档中出现的词语。用户个人资料也用相同的术语表示，并通过分析用户已经看过的物品的内容来构建。然而，在实施这些类型的推荐系统时，需要考虑以下问题：
- en: First, terms can be assigned automatically or manually. For automatic assignment,
    a method has to be chosen so that these items can be extracted from the item list.
    Second, terms have to be represented in a way so that both the user profile and
    the items can be compared in a meaningful way. The learning algorithm itself has
    to be chosen wisely so that it's going to be able to learn a user profile based
    on already observer (that is, seen) items and makes appropriate recommendations
    based on this user profile. Content-based filtering systems are mostly used with
    text documents, where term parsers are used to select single words from the documents.
    The vector space model and latent semantic indexing are two methods that use these
    terms to represent documents as vectors in a multidimensional space. Furthermore,
    it is also used in relevance feedback, genetic algorithms, neural networks, and
    the Bayesian classifier for learning a user profile.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，术语可以自动或手动分配。对于自动分配，必须选择一种方法，以便可以从物品列表中提取这些物品。其次，术语必须以一种方式表示，以便用户个人资料和物品可以进行有意义的比较。学习算法本身必须明智地选择，以便能够基于已观察到的（即已看到的）物品学习用户个人资料，并根据这个用户个人资料做出适当的推荐。基于内容的过滤系统主要用于文本文档，其中术语解析器用于从文档中选择单词。向量空间模型和潜在语义索引是使用这些术语将文档表示为多维空间中的向量的两种方法。此外，它还用于相关反馈、遗传算法、神经网络和贝叶斯分类器来学习用户个人资料。
- en: A hybrid recommender system is a recent research and hybrid approach (that is,
    combining collaborative filtering and content-based filtering). Netflix is a good
    example of such a recommendation system that uses the **Restricted Boltzmann Machines**
    (**RBM**) and a form of the matrix factorization algorithm for large movie database
    like IMDb (see more at [https://pdfs.semanticscholar.org/789a/d4218d1e2e920b4d192023f840fe8246d746.pdf](https://pdfs.semanticscholar.org/789a/d4218d1e2e920b4d192023f840fe8246d746.pdf)).
    This recommendation which simply recommends movies, dramas, or streaming by comparing
    the watching and searching habits of similar users, is called rating prediction.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合推荐系统是最近的研究和混合方法（即，结合协同过滤和基于内容的过滤）。Netflix就是这样一个推荐系统的很好的例子，它使用了**受限玻尔兹曼机**（**RBM**）和一种矩阵分解算法，用于像IMDb这样的大型电影数据库（详见[https://pdfs.semanticscholar.org/789a/d4218d1e2e920b4d192023f840fe8246d746.pdf](https://pdfs.semanticscholar.org/789a/d4218d1e2e920b4d192023f840fe8246d746.pdf)）。这种推荐系统通过比较相似用户的观看和搜索习惯来简单地推荐电影、剧集或流媒体，称为评分预测。
- en: Knowledge-based systems, where knowledge about users and products is used to
    reason what fulfills a user's requirements, using perception tree, decision support
    systems, and case-based reasoning.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于知识的系统，其中使用有关用户和产品的知识来推断满足用户需求的内容，使用感知树、决策支持系统和基于案例的推理。
- en: In this chapter, we will discuss the collaborative filtering based recommender
    system for the movie recommendations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论基于协同过滤的电影推荐系统。
- en: Semisupervised learning
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督学习
- en: 'Between supervised and unsupervised learning, there is a small place for semi-supervised
    learning. In this case, the ML model usually receives an incomplete training signal.
    More statistically, the ML model receives a training set with some of the target
    outputs missing. Semi-supervised learning is more or less assumption based and
    often uses three kinds of assumption algorithms as the learning algorithm for
    the unlabeled datasets. The following assumptions are used: smoothness, cluster,
    and manifold. In other words, semi-supervised learning can furthermore be denoted
    as weakly supervised or a bootstrapping technique for using the hidden wealth
    of unlabeled examples to enhance the learning from a small amount of labeled data.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习和无监督学习之间，半监督学习有一小部分空间。在这种情况下，ML模型通常接收不完整的训练信号。更具体地说，ML模型接收到一组带有一些目标输出缺失的训练集。半监督学习更多地是基于假设，并且通常使用三种假设算法作为未标记数据的学习算法。使用以下假设：平滑性、聚类和流形。换句话说，半监督学习还可以被称为弱监督或使用未标记示例的自举技术，以增强从少量标记数据中学习的隐藏财富。
- en: As already mentioned that the acquisition of labeled data for a learning problem
    often requires a skilled human agent. Therefore, the cost associated with the
    labeling process thus may render a fully labeled training set infeasible, whereas
    acquisition of unlabeled data is relatively inexpensive.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，学习问题的标记数据的获取通常需要一个熟练的人类代理。因此，与标记过程相关的成本可能使得完全标记的训练集变得不可行，而未标记数据的获取相对廉价。
- en: 'For example: to transcribe an audio segment, in determining the 3D structure
    of a protein or determining whether there is oil at a particular location, expectation
    minimization and human cognition, and transitive. The In such situations, semi-supervised
    learning can be of great practical value.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：转录音频片段，确定蛋白质的3D结构或确定特定位置是否存在石油，期望最小化和人类认知，以及传递性。在这种情况下，半监督学习可以具有很大的实际价值。
- en: Spark machine learning APIs
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 机器学习API
- en: In this section, we will describe two key concepts introduced by the Spark machine
    learning libraries (Spark MLlib and Spark ML) and the most widely used implemented
    algorithms that align with the supervised and unsupervised learning techniques
    we discussed in the previous sections.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述由Spark机器学习库（Spark MLlib和Spark ML）引入的两个关键概念，以及与我们在前几节中讨论的监督和无监督学习技术相一致的最常用的实现算法。
- en: Spark machine learning libraries
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 机器学习库
- en: As already stated, in the pre-Spark era, big data modelers typically used to
    build their ML models using statistical languages such as R, STATA, and SAS. However,
    this kind of workflow (that is, the execution flow of these ML algorithms) lacks
    efficiency, scalability, and throughput, as well as accuracy, with, of course,
    extended execution times.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在Spark之前的时代，大数据建模者通常使用统计语言（如R、STATA和SAS）构建他们的ML模型。然而，这种工作流程（即这些ML算法的执行流程）缺乏效率、可伸缩性和吞吐量，以及准确性，当然，执行时间也更长。
- en: 'Then, data engineers used to reimplement the same model in Java, for example,
    to deploy on Hadoop. Using Spark, the same ML model can be rebuilt, adopted, and
    deployed, making the whole workflow much more efficient, robust, and faster, allowing
    you to provide hands-on insight to increase the performance. Moreover, implementing
    these algorithms in Hadoop means that these algorithms can run in parallel that
    cannot be run on R, STATA and SAS and so on. The Spark machine learning library
    is divided into two packages: Spark MLlib (`spark.mllib`) and Spark ML (`spark.ml`).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，数据工程师过去常常需要在Java中重新实现相同的模型，例如在Hadoop上部署。使用Spark，相同的ML模型可以被重建、采用和部署，使整个工作流程更加高效、稳健和快速，从而使您能够提供实时见解以提高性能。此外，在Hadoop中实现这些算法意味着这些算法可以并行运行，而这是R、STATA和SAS等软件无法实现的。Spark
    机器学习库分为两个包：Spark MLlib（`spark.mllib`）和Spark ML（`spark.ml`）。
- en: Spark MLlib
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark MLlib
- en: MLlib is Spark's scalable machine learning library and is an extension of the
    Spark Core API which provides a library of easy-to-use machine learning algorithms.
    Spark algorithms are implemented in Scala and then expose the API for Java, Scala,
    Python, and R. Spark provides support of local vectors and matrix data types stored
    on a single machine, as well as distributed matrices backed by one or multiple
    RDDs. The beauties of Spark MLlib are numerous. For example, algorithms are highly
    scalable and leverage Spark's ability to work with a massive amounts of data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib是Spark的可扩展机器学习库，是Spark Core API的扩展，提供了一系列易于使用的机器学习算法库。Spark算法是用Scala实现的，然后暴露给Java、Scala、Python和R的API。Spark支持存储在单台机器上的本地向量和矩阵数据类型，以及由一个或多个RDD支持的分布式矩阵。Spark
    MLlib的优点是多种多样的。例如，算法具有高度可扩展性，并利用Spark处理大量数据的能力。
- en: They are fast foward designed for parallel computing with an in-memory based
    operation that is 100 times faster compared to MapReduce data processing (they
    also support disk-based operation, which is 10 times faster compared to what MapReduce
    has as normal data processing).
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是为并行计算而设计的快速前进，具有基于内存的操作，比MapReduce数据处理快100倍（它们还支持基于磁盘的操作，比MapReduce的普通数据处理快10倍）。
- en: They are diverse, since they cover common machine learning algorithms for regression
    analysis, classification, clustering, recommender systems, text analytics, and
    frequent pattern mining, and obviously cover all the steps required to build a
    scalable machine learning application.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是不同的，因为它们涵盖了用于回归分析、分类、聚类、推荐系统、文本分析和频繁模式挖掘的常见机器学习算法，并且显然涵盖了构建可扩展机器学习应用程序所需的所有步骤。
- en: Spark ML
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark ML
- en: Spark ML adds a new set of machine learning APIs to let users quickly assemble
    and configure practical machine learning pipelines on top of datasets. Spark ML
    aims to offer a uniform set of high-level APIs built on top of DataFrames rather
    than RDDs that help users create and tune practical machine learning pipelines.
    Spark ML API standardizes machine learning algorithms to make the learning tasks
    easier to combine multiple algorithms into a single pipeline or data workflow
    for data scientists. The Spark ML uses the concepts of DataFrame and Datasets,
    which are much newer concepts introduced (as experimental) in Spark 1.6 and then
    used in Spark 2.0+.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML添加了一组新的机器学习API，让用户可以快速在数据集上组装和配置实用的机器学习管道。Spark ML旨在提供一组统一的高级API，建立在DataFrame而不是RDD之上，帮助用户创建和调整实用的机器学习管道。Spark
    ML API标准化了机器学习算法，使得将多个算法组合成单个管道或数据工作流程更容易，供数据科学家使用。Spark ML使用DataFrame和Datasets的概念，这些概念是在Spark
    1.6中引入的（作为实验性功能），然后在Spark 2.0+中使用。
- en: In Scala and Java, DataFrame and Dataset have been unified, that is, DataFrame
    is just a type alias for a dataset of row. In Python and R, given the lack of
    type safety, DataFrame is the main programming interface.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala和Java中，DataFrame和Dataset已经统一，也就是说，DataFrame只是行数据集的类型别名。在Python和R中，由于缺乏类型安全性，DataFrame是主要的编程接口。
- en: The datasets hold diverse data types such as columns storing text, feature vectors,
    and true labels for the data. In addition to this, Spark ML also uses the transformer
    to transform one DataFrame into another or vice-versa, where the concept of the
    estimator is used to fit on a DataFrame to produce a new transformer. The pipeline
    API, on the other hand, can restrain multiple transformers and estimators together
    to specify an ML data workflow. The concept of the parameter was introduced to
    specify all the transformers and estimators to share a common API under an umbrella
    during the development of an ML application.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含各种数据类型，例如存储文本、特征向量和数据的真实标签的列。除此之外，Spark ML还使用转换器将一个DataFrame转换为另一个，或者反之亦然，其中估计器的概念用于拟合DataFrame以生成新的转换器。另一方面，管道API可以将多个转换器和估计器组合在一起，以指定一个ML数据工作流程。在开发ML应用程序时，参数的概念被引入，以便指定所有转换器和估计器在一个统一的API下共享。
- en: Spark MLlib or Spark ML?
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark MLlib还是Spark ML？
- en: Spark ML provides a higher-level API built on top of DataFrames for constructing
    ML pipelines. Basically, Spark ML provides you with a toolset to create pipelines
    of different machine learning related transformations on your data. It makes it
    easy to, for example, chain feature extraction, dimensionality reduction, and
    the training of a classifier into one model, which as a whole can be later used
    for classification. MLlib, however, is older and has been in development longer,
    it has more features because of this. Therefore, using Spark ML is recommended
    because, the API is more versatile and flexible with DataFrames.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML提供了一个基于DataFrame构建ML管道的高级API。基本上，Spark ML为你提供了一个工具集，用于在数据上构建不同的机器学习相关的转换管道。例如，它可以轻松地将特征提取、降维和分类器的训练链在一起，作为一个整体，后续可以用于分类。然而，MLlib更老，开发时间更长，因此它具有更多的功能。因此，建议使用Spark
    ML，因为其API更加灵活多样，适用于DataFrame。
- en: Feature extraction and transformation
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取和转换
- en: 'Suppose you are going to build a machine learning model that will predict whether
    a credit card transaction is fraudulent or not. Now, based on the available background
    knowledge and data analysis, you might decide which data fields (aka features)
    are important for training your model. For example, amount, customer name, buying
    company name, and the address of the credit card owners are worth to providing
    for the overall learning process. These are important to consider since, if you
    just provide a randomly generated transaction ID, that will not carry any information
    so would not be useful at all. Thus, once you have decided which features to include
    in your training set, you then need to transform those features to train the model
    for better learning. The feature transformations help you add additional background
    information to the training data. The information enables the machine learning
    model to benefit from this experience eventually. To make the preceding discussion
    more concrete, suppose you have the following address of one of the customers
    represented in the string:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你要构建一个机器学习模型，用于预测信用卡交易是否欺诈。现在，基于可用的背景知识和数据分析，你可能会决定哪些数据字段（也就是特征）对于训练模型是重要的。例如，金额、客户姓名、购买公司名称和信用卡所有者的地址都值得提供给整个学习过程。这些都是重要考虑的因素，因为如果你只提供一个随机生成的交易ID，那将不会携带任何信息，因此毫无用处。因此，一旦你决定在训练集中包括哪些特征，你就需要转换这些特征以便更好地训练模型。特征转换可以帮助你向训练数据添加额外的背景信息。这些信息使得机器学习模型最终能够从这种经验中受益。为了使前面的讨论更具体，假设你有一个客户的地址如下所示的字符串：
- en: '`"123 Main Street, Seattle, WA 98101"`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: “123 Main Street, Seattle, WA 98101”
- en: 'If you see the preceding address, the address lacks proper semantics. In other
    words, the string has limited expressive power. This address will be useful only
    for learning address patterns associated with that exact address in a database,
    for example. However, breaking it up into fundamental parts can provide additional
    features such as the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到上述地址，你会发现地址缺乏适当的语义。换句话说，该字符串的表达能力有限。例如，这个地址只对学习与数据库中的确切地址相关的地址模式有用。然而，将其分解为基本部分可以提供额外的特征，例如以下内容：
- en: '"Address" (123 Main Street)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “地址”（123 Main Street）
- en: '"City" (Seattle)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"City" (Seattle)'
- en: '"State" (WA)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"State" (WA)'
- en: '"Zip" (98101)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"Zip" (98101)'
- en: 'If you see the preceding patterns, your ML algorithm can now group more different
    transactions together and discover broader patterns. This is normal, since some
    customer''s zip codes contribute to more fraudulent activity than others. Spark
    provides several algorithms implemented for the feature extractions and to make
    transformation easier. For example, the current version provides the following
    algorithms for feature extractions:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看到前面的模式，您的ML算法现在可以将更多不同的交易分组在一起，并发现更广泛的模式。这是正常的，因为一些客户的邮政编码比其他客户的邮政编码更容易产生欺诈活动。Spark提供了几种用于特征提取和使转换更容易的算法。例如，当前版本提供了以下算法用于特征提取：
- en: TF-IDF
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Word2vec
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2vec
- en: CountVectorizer
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CountVectorizer
- en: 'On the other hand, a feature transformer is an abstraction that includes feature
    transformers and learned models. Technically, a transformer implements a method
    named `transform()`, which converts one DataFrame into another, generally by appending
    one or more columns. Spark supports the following transformers to RDD or DataFrame:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，特征转换器是一个包括特征转换器和学习模型的抽象。从技术上讲，转换器实现了一个名为`transform()`的方法，它通过附加一个或多个列，将一个DataFrame转换为另一个DataFrame。Spark支持以下转换器到RDD或DataFrame：
- en: Tokenizer
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词器
- en: StopWordsRemover
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StopWordsRemover
- en: n-gram
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n-gram
- en: Binarizer
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Binarizer
- en: PCA
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA
- en: PolynomialExpansion
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PolynomialExpansion
- en: Discrete cosine transform (DCT)
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散余弦变换（DCT）
- en: StringIndexer
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StringIndexer
- en: IndexToString
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IndexToString
- en: OneHotEncoder
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OneHotEncoder
- en: VectorIndexer
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VectorIndexer
- en: Interaction
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Interaction
- en: Normalizer
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Normalizer
- en: StandardScaler
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StandardScaler
- en: MinMaxScaler
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MinMaxScaler
- en: MaxAbsScaler
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MaxAbsScaler
- en: Bucketizer
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bucketizer
- en: ElementwiseProduct
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ElementwiseProduct
- en: SQLTransformer
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQLTransformer
- en: VectorAssembler
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VectorAssembler
- en: QuantileDiscretizer
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QuantileDiscretizer
- en: Due to page limitations, we cannot describe all of them. But we will discuss
    some widely used algorithms such as `CountVectorizer`, `Tokenizer`, `StringIndexer`,
    `StopWordsRemover`, `OneHotEncoder`, and so on. PCA, which is commonly used in
    dimensionality reduction, will be discussed in the next section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于页面限制，我们无法描述所有内容。但我们将讨论一些广泛使用的算法，如`CountVectorizer`、`Tokenizer`、`StringIndexer`、`StopWordsRemover`、`OneHotEncoder`等。PCA，通常用于降维，将在下一节中讨论。
- en: CountVectorizer
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CountVectorizer
- en: '`CountVectorizer` and `CountVectorizerModel` aim to help convert a collection
    of text documents to vectors of token counts. When the prior dictionary is not
    available, `CountVectorizer` can be used as an estimator to extract the vocabulary
    and generates a `CountVectorizerModel`. The model produces sparse representations
    for the documents over the vocabulary, which can then be passed to other algorithms
    such LDA.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer`和`CountVectorizerModel`旨在帮助将一组文本文档转换为标记计数的向量。当先前的字典不可用时，`CountVectorizer`可以用作估计器来提取词汇表并生成`CountVectorizerModel`。该模型为文档在词汇表上生成了稀疏表示，然后可以传递给其他算法，如LDA。'
- en: 'Suppose we have the text corpus as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下文本语料库：
- en: '![](img/00296.gif)**Figure 7**: Text corpus containing name only'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00296.gif)**图7**：仅包含名称的文本语料库'
- en: 'Now, if we want to convert the preceding collection of texts to vectors of
    token counts, Spark provides the `CountVectorizer ()` API for doing so. First,
    let''s create a simple DataFrame for the earlier table, as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想将前面的文本集合转换为标记计数的向量，Spark提供了`CountVectorizer()`API来实现。首先，让我们为前面的表创建一个简单的DataFrame，如下所示：
- en: '[PRE0]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In many cases, you can set the input column with `setInputCol`. Let''s look
    at an example of it and let''s fit a `CountVectorizerModel` object from the corpus,
    as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，您可以使用`setInputCol`设置输入列。让我们看一个例子，并让我们从语料库中拟合一个`CountVectorizerModel`对象，如下所示：
- en: '[PRE1]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now let''s downstream the vectorizer using the extractor, as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用提取器下游化向量化器，如下所示：
- en: '[PRE2]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now let''s check to make sure it works properly:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们检查一下，确保它正常工作：
- en: '[PRE3]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding line of code produces the following output:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 上一行代码产生了以下输出：
- en: '![](img/00306.gif)**Figure 8**: Name text corpus has been featurized'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00306.gif)**图8**：名称文本语料库已被特征化'
- en: Now let's move to the feature transformers. One of the most important transformers
    is the tokenizer, which is frequently used in the machine learning task for handling
    categorical data. We will see how to work with this transformer in the next section.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转到特征转换器。最重要的转换器之一是分词器，它经常用于处理分类数据的机器学习任务。我们将在下一节中看到如何使用这个转换器。
- en: Tokenizer
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词器
- en: 'Tokenization is the process of enchanting important components from raw text,
    such as words, and sentences, and breaking the raw texts into individual terms
    (also called words). If you want to have more advanced tokenization on regular
    expression matching, `RegexTokenizer` is a good option for doing so. By default,
    the parameter *pattern* (regex, default: `s+`) is used as delimiters to split
    the input text. Otherwise, you can also set parameter *gaps* to false, indicating
    the regex *pattern* denotes *tokens* rather than splitting gaps. This way, you
    can find all matching occurrences as the tokenization result.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化是从原始文本中提取重要组件（如单词和句子），并将原始文本分解为单个术语（也称为单词）的过程。如果您想对正则表达式匹配进行更高级的标记化，`RegexTokenizer`是一个很好的选择。默认情况下，参数*pattern*（regex，默认：`s+`）用作分隔符来分割输入文本。否则，您还可以将参数*gaps*设置为false，表示正则表达式*pattern*表示*tokens*而不是分割间隙。这样，您可以找到所有匹配的出现作为标记化结果。
- en: 'Suppose you have the following sentences:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有以下句子：
- en: Tokenization,is the process of enchanting words,from the raw text.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记化，是从原始文本中提取单词的过程。
- en: If you want,to have more advance tokenization, `RegexTokenizer`,is a good option.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想进行更高级的标记化，`RegexTokenizer`是一个不错的选择。
- en: Here,will provide a sample example on how to tokenize sentences.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，我们将提供一个示例，演示如何对句子进行标记化。
- en: This way, you can find all matching occurrences.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这样，您可以找到所有匹配的出现。
- en: 'Now, you want to tokenize each meaningful word from the preceding four sentences.
    Let''s create a DataFrame from the earlier sentences, as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您希望从前面的四个句子中对每个有意义的单词进行标记化。让我们从前面的句子中创建一个DataFrame，如下所示：
- en: '[PRE4]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now let''s create a tokenizer by instantiating the `Tokenizer ()` API, as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过实例化`Tokenizer()`API创建一个标记器，如下所示：
- en: '[PRE5]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, count the number of tokens in each sentence using a UDF, as follows: `import
    org.apache.spark.sql.functions._`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用UDF计算每个句子中的标记数，如下所示：`import org.apache.spark.sql.functions._`
- en: '[PRE6]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now tokenize words form each sentence, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对每个句子中的单词进行标记化，如下所示：
- en: '[PRE7]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, show each token against each raw sentence, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，按如下方式显示每个原始句子的每个标记：
- en: '[PRE8]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding line of code prints a snap from the tokenized DataFrame containing
    the raw sentence, bag of words, and number of tokens:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 上一行代码打印了一个从标记化的DataFrame中获取原始句子、词袋和标记数的快照：
- en: '![](img/00315.gif)**Figure 9**: Tokenized words from the raw texts'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00315.gif)**图9：**从原始文本中标记化的单词'
- en: 'However, if you use `RegexTokenizer` API, you will get better results. This
    goes as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您使用`RegexTokenizer`API，您将获得更好的结果。操作如下：
- en: 'Create a regex tokenizer by instantiating the `RegexTokenizer ()` API:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实例化`RegexTokenizer()`API创建一个正则表达式标记器：
- en: '[PRE9]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now tokenize words from each sentence, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对每个句子中的单词进行标记化，如下所示：
- en: '[PRE10]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding line of code prints a snap from the tokenized DataFrame using
    RegexTokenizer containing the raw sentence, bag of words, and number of tokens:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 上一行代码打印了一个使用RegexTokenizer包含原始句子、词袋和标记数的标记化DataFrame的快照：
- en: '![](img/00318.gif)**Figure 10**: Better tokenization using RegexTokenizer'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00318.gif)**图10：**使用RegexTokenizer更好的标记化'
- en: StopWordsRemover
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StopWordsRemover
- en: Stop words are words that should be excluded from the input, typically because
    the words appear frequently and don't carry as much meaning. Spark's `StopWordsRemover`
    takes as input a sequence of strings, which is tokenized by `Tokenizer` or `RegexTokenizer`.
    Then, it removes all the stop words from the input sequences. The list of stop
    words is specified by the `stopWords` parameter. The current implementation for
    the `StopWordsRemover` API provides the options for the Danish, Dutch, Finnish,
    French, German, Hungarian, Italian, Norwegian, Portuguese, Russian, Spanish, Swedish,
    Turkish, and English languages. To provide an example, we can simply extend the
    preceding `Tokenizer` example in the previous section, since they are already
    tokenized. For this example, however, we will use the `RegexTokenizer` API.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是应该从输入中排除的单词，通常是因为这些单词频繁出现且没有太多含义。Spark的`StopWordsRemover`接受一个字符串序列作为输入，该序列由`Tokenizer`或`RegexTokenizer`标记化。然后，它从输入序列中删除所有停用词。停用词列表由`stopWords`参数指定。`StopWordsRemover`API的当前实现为丹麦语、荷兰语、芬兰语、法语、德语、匈牙利语、意大利语、挪威语、葡萄牙语、俄语、西班牙语、瑞典语、土耳其语和英语提供了选项。举个例子，我们可以简单地扩展上一节中的`Tokenizer`示例，因为它们已经被标记化。但是，对于此示例，我们将使用`RegexTokenizer`API。
- en: 'At first, create a stop word remover instance from the `StopWordsRemover ()`
    API, as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过`StopWordsRemover()`API创建一个停用词移除器实例，如下所示：
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let''s remove all the stop words and print the results as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们删除所有停用词并按如下方式打印结果：
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding line of code prints a snap from the filtered DataFrame excluding
    the stop words:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上一行代码打印了一个从过滤后的DataFrame中排除停用词的快照：
- en: '![](img/00324.gif)**Figure 11**: Filtered (that is, without stop words) tokens'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00324.gif)**图11：**过滤（即去除停用词）的标记'
- en: StringIndexer
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StringIndexer
- en: 'StringIndexer encodes a string column of labels to a column of label indices.
    The indices are in `[0, numLabels)`, ordered by label frequencies, so the most
    frequent label gets index 0\. If the input column is numeric, we cast it to string
    and index the string values. When downstream pipeline components such as estimator
    or transformer make use of this string-indexed label, you must set the input column
    of the component to this string-indexed column name. In many cases, you can set
    the input column with `setInputCol`. Suppose you have some categorical data in
    the following format:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: StringIndexer将标签的字符串列编码为标签索引列。索引在`[0，numLabels)`中，按标签频率排序，因此最常见的标签获得索引0。如果输入列是数字，我们将其转换为字符串并索引字符串值。当下游管道组件（如估计器或转换器）使用此字符串索引标签时，您必须将组件的输入列设置为此字符串索引列名称。在许多情况下，您可以使用`setInputCol`设置输入列。假设您有以下格式的一些分类数据：
- en: '![](img/00158.gif)**Figure 12**: DataFrame for applying String Indexer'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00158.gif)**图12：**应用String Indexer的DataFrame'
- en: 'Now, we want to index the name column so that the most frequent name (that
    is, Jason in our case) gets index 0\. To make this, Spark provides `StringIndexer`
    API for doing so. For our example, this can be done, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要对名称列进行索引，以便最常见的名称（在我们的案例中为Jason）获得索引0。为此，Spark提供了`StringIndexer`API。对于我们的示例，可以按如下方式完成：
- en: 'At first, let''s create a simple DataFrame for the preceding table:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们为上表创建一个简单的DataFrame：
- en: '[PRE13]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let''s index the name column, as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对名称列进行索引，如下所示：
- en: '[PRE14]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now let''s downstream the indexer using the transformer, as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用转换器下游索引器，如下所示：
- en: '[PRE15]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now let''s check to make sure if it works properly:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们检查一下是否它正常工作：
- en: '[PRE16]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/00381.gif)**Figure 13**: Label creation using StringIndexer'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00381.gif)**图13：**使用StringIndexer创建标签'
- en: Another important transformer is the OneHotEncoder, which is frequently used
    in machine learning tasks for handling categorical data. We will see how to work
    with this transformer in the next section.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的转换器是OneHotEncoder，在处理分类数据的机器学习任务中经常使用。我们将在下一节中看到如何使用这个转换器。
- en: OneHotEncoder
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OneHotEncoder
- en: 'A one-hot encoding maps a column of label indices to a column of binary vectors,
    with at most a single value. This encoding allows algorithms that expect continuous
    features, such as Logistic Regression, to use categorical features. Suppose you
    have some categorical data in the following format (the same that we used for
    describing the `StringIndexer` in the previous section):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一种独热编码将标签索引列映射到具有最多一个值的二进制向量列。这种编码允许期望连续特征（例如逻辑回归）的算法使用分类特征。假设您有以下格式的一些分类数据（与我们在上一节中描述`StringIndexer`时使用的相同）：
- en: '![](img/00253.gif)**Figure 14:** DataFrame for applying OneHotEncoder'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00253.gif)**图14：**应用OneHotEncoder的DataFrame'
- en: 'Now, we want to index the name column so that the most frequent name in the
    dataset (that is, **Jason** in our case) gets index **0**. However, what''s the
    use of just indexing them? In other words, you can further vectorize them and
    then you can feed the DataFrame to any ML models easily. Since we have already
    seen how to create a DataFrame in the previous section, here, we will just show
    how to encode them toward Vectors:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要对名称列进行索引，以便数据集中最常见的名称（即我们的情况下的**Jason**）获得索引**0**。然而，仅仅对它们进行索引有什么用呢？换句话说，您可以进一步将它们向量化，然后可以轻松地将DataFrame提供给任何ML模型。由于我们已经在上一节中看到如何创建DataFrame，在这里，我们将展示如何将它们编码为向量：
- en: '[PRE17]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now let''s transform it into a vector using `Transformer` and then see the
    contents, as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用`Transformer`将其转换为向量，然后查看内容，如下所示：
- en: '[PRE18]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The resulting DataFrame containing a snap is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 包含快照的结果DataFrame如下：
- en: '![](img/00228.gif)**Figure 15**: Creating category index and vector using OneHotEncoder'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00228.gif)**图15**：使用OneHotEncoder创建类别索引和向量'
- en: Now you can see that a new column containing feature vectors has been added
    in the resulting DataFrame.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以看到，结果DataFrame中添加了一个包含特征向量的新列。
- en: Spark ML pipelines
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark ML管道
- en: MLlib's goal is to make practical machine learning (ML) scalable and easy. Spark
    introduced the pipeline API for the easy creation and tuning of practical ML pipelines.
    As discussed previously, extracting meaningful knowledge through feature engineering
    in an ML pipeline creation involves a sequence of data collection, preprocessing,
    feature extraction, feature selection, model fitting, validation, and model evaluation
    stages. For example, classifying the text documents might involve text segmentation
    and cleaning, extracting features, and training a classification model with cross-validation
    toward tuning. Most ML libraries are not designed for distributed computation
    or they do not provide native support for pipeline creation and tuning.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib的目标是使实际机器学习（ML）可扩展且易于使用。Spark引入了管道API，用于轻松创建和调整实际的ML管道。如前所述，在ML管道创建中通过特征工程提取有意义的知识涉及一系列数据收集、预处理、特征提取、特征选择、模型拟合、验证和模型评估阶段。例如，对文本文档进行分类可能涉及文本分割和清理、提取特征以及使用交叉验证训练分类模型进行调整。大多数ML库都不是为分布式计算设计的，或者它们不提供管道创建和调整的本地支持。
- en: Dataset abstraction
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集抽象
- en: 'When running SQL queries from another programming language (for example, Java),
    the result is returned as a DataFrame. A DataFrame is a distributed collection
    of data organized into named columns. A dataset, on the other hand, is an interface
    that tries to provide the benefits of RDDs out of the Spark SQL. A dataset can
    be constructed from some JVM objects such as primitive types (for example, `String`,
    `Integer`, and `Long`), Scala case classes, and Java Beans. An ML pipeline involves
    a number of the sequences of dataset transformations and models. Each transformation
    takes an input dataset and outputs the transformed dataset, which becomes the
    input to the next stage. Consequently, the data import and export are the start
    and end points of an ML pipeline. To make these easier, Spark MLlib and Spark
    ML provide import and export utilities of a dataset, DataFrame, RDD, and model
    for several application-specific types, including:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一种编程语言（例如Java）运行SQL查询时，结果将作为DataFrame返回。DataFrame是一种分布式的数据集合，组织成具有命名列的数据。另一方面，数据集是一个接口，试图提供Spark
    SQL中RDD的好处。数据集可以从一些JVM对象构造，例如原始类型（例如`String`、`Integer`和`Long`）、Scala case类和Java
    Beans。ML管道涉及一系列数据集转换和模型。每个转换都接受一个输入数据集，并输出转换后的数据集，这成为下一阶段的输入。因此，数据导入和导出是ML管道的起点和终点。为了使这些更容易，Spark
    MLlib和Spark ML提供了数据集、DataFrame、RDD和模型的导入和导出工具，适用于几种特定应用类型，包括：
- en: LabeledPoint for classification and regression
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分类和回归的LabeledPoint
- en: LabeledDocument for cross-validation and Latent Dirichlet Allocation (LDA)
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于交叉验证和潜在狄利克雷分配（LDA）的LabeledDocument
- en: Rating and ranking for collaborative filtering
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协同过滤的评分和排名
- en: However, real datasets usually contain numerous types, such as user ID, item
    IDs, labels, timestamps, and raw records. Unfortunately, the current utilities
    of Spark implementation cannot easily handle datasets consisting of these types,
    especially time-series datasets. The feature transformation usually forms the
    majority of a practical ML pipeline. A feature transformation can be viewed as
    appending or dropping a new column created from existing columns.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，真实数据集通常包含多种类型，例如用户ID、项目ID、标签、时间戳和原始记录。不幸的是，Spark实现的当前工具不能轻松处理由这些类型组成的数据集，特别是时间序列数据集。特征转换通常占据实际ML管道的大部分。特征转换可以被视为从现有列创建新列的附加或删除。
- en: 'In the following figure, you will see that the text tokenizer breaks a document
    into a bag of words. After that, the TF-IDF algorithm converts a bag of words
    into a feature vector. During the transformations, the labels need to be preserved
    for the model-fitting stage:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，您将看到文本标记器将文档分解为词袋。之后，TF-IDF算法将词袋转换为特征向量。在转换过程中，标签需要保留以用于模型拟合阶段：
- en: '![](img/00137.jpeg)**Figure 16**: Text processing for machine learning model
    (DS indicates data sources)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00137.jpeg)**图16**：用于机器学习模型的文本处理（DS表示数据源）'
- en: Here, the ID, text, and words are conceded during the transformations steps.
    They are useful in making predictions and model inspection. However, they are
    actually unnecessary for model fitting to state. These also don't provide much
    information if the prediction dataset contains only the predicted labels. Consequently,
    if you want to inspect the prediction metrics, such as the accuracy, precision,
    recall, weighted true positives, and weighted false positives, it is quite useful
    to look at the predicted labels along with the raw input text and tokenized words.
    The same recommendation also applies to other machine learning applications using
    Spark ML and Spark MLlib.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，ID、文本和单词在转换步骤中被让步。它们对于进行预测和模型检查是有用的。然而，它们实际上对于模型拟合来说是不必要的。如果预测数据集只包含预测标签，它们也不提供太多信息。因此，如果你想要检查预测指标，比如准确性、精确度、召回率、加权真阳性和加权假阳性，查看预测标签以及原始输入文本和标记化单词是非常有用的。相同的建议也适用于使用Spark
    ML和Spark MLlib的其他机器学习应用。
- en: Therefore, an easy conversion between RDDs, dataset, and DataFrames has been
    made possible for in-memory, disk, or external data sources such as Hive and Avro.
    Although creating new columns from existing columns is easy with user-defined
    functions, the manifestation of dataset is a lazy operation. In contrast, the
    dataset supports only some standard data types. However, to increase the usability
    and to make a better fit for the machine learning model, Spark has also added
    the support for the `Vector` type as a user-defined type that supports both dense
    and sparse feature vectors under `mllib.linalg.DenseVector` and `mllib.linalg.Vector`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RDD、数据集和DataFrame之间的简单转换已经成为可能，用于内存、磁盘或外部数据源，如Hive和Avro。虽然使用用户定义的函数从现有列创建新列很容易，但数据集的显现是一种懒惰的操作。相反，数据集仅支持一些标准数据类型。然而，为了增加可用性并使其更适合机器学习模型，Spark还添加了对`Vector`类型的支持，作为一种支持`mllib.linalg.DenseVector`和`mllib.linalg.Vector`下的稠密和稀疏特征向量的用户定义类型。
- en: Complete DataFrame, dataset, and RDD examples in Java, Scala, and Python can
    be found in the `examples/src/main/` folder under the Spark distribution. Interested
    readers can refer to Spark SQL's user guide at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)
    to learn more about DataFrame, dataset, and the operations they support.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark分发的`examples/src/main/`文件夹中可以找到Java、Scala和Python中的完整DataFrame、数据集和RDD示例。感兴趣的读者可以参考Spark
    SQL的用户指南[http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)来了解更多关于DataFrame、数据集以及它们支持的操作。
- en: Creating a simple pipeline
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个简单的管道
- en: 'Spark provides pipeline APIs under Spark ML. A pipeline comprises a sequence
    of stages consisting of transformers and estimators. There are two basic types
    of pipeline stages, called transformer and estimator:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在Spark ML下提供了管道API。管道包括一系列由转换器和估计器组成的阶段。管道阶段有两种基本类型，称为转换器和估计器：
- en: A transformer takes a dataset as an input and produces an augmented dataset
    as the output so that the output can be fed to the next step. For example, **Tokenizer**
    and **HashingTF** are two transformers. Tokenizer transforms a dataset with text
    into a dataset with tokenized words. A HashingTF, on the other hand, produces
    the term frequencies. The concept of tokenization and HashingTF is commonly used
    in text mining and text analytics.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换器将数据集作为输入，并产生增强的数据集作为输出，以便输出可以被传递到下一步。例如，**Tokenizer**和**HashingTF**是两个转换器。Tokenizer将具有文本的数据集转换为具有标记化单词的数据集。另一方面，HashingTF产生术语频率。标记化和HashingTF的概念通常用于文本挖掘和文本分析。
- en: On the contrary, an estimator must be the first on the input dataset to produce
    a model. In this case, the model itself will be used as the transformer for transforming
    the input dataset into the augmented output dataset. For example, a **Logistic
    Regression** or linear regression can be used as an estimator after fitting the
    training dataset with corresponding labels and features.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，估计器必须是输入数据集中的第一个，以产生模型。在这种情况下，模型本身将被用作转换器，将输入数据集转换为增强的输出数据集。例如，在拟合训练数据集与相应的标签和特征之后，可以将**逻辑回归**或线性回归用作估计器。
- en: 'After that, it produces a logistic or linear regression model, which implies
    that developing a pipeline is easy and simple. Well, all you need to do is to
    declare required stages, then configure the related stage''s parameters; finally,
    chain them in a pipeline object, as shown in the following figure:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它产生一个逻辑或线性回归模型，这意味着开发管道是简单而容易的。你所需要做的就是声明所需的阶段，然后配置相关阶段的参数；最后，将它们链接在一个管道对象中，如下图所示：
- en: '![](img/00374.jpeg)**Figure 17**: Spark ML pipeline model using logistic regression
    estimator (DS indicates data store, and the steps inside the dashed line only
    happen during pipeline fitting)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00374.jpeg)**图17**：使用逻辑回归估计器的Spark ML管道模型（DS表示数据存储，在虚线内的步骤仅在管道拟合期间发生）'
- en: If you look at *Figure 17*, the fitted model consists of a Tokenizer, a HashingTF
    feature extractor, and a fitted logistic regression model. The fitted pipeline
    model acts as a transformer that can be used for prediction, model validation,
    model inspection, and, finally, model deployment. However, to increase the performance
    in terms of prediction accuracy, the model itself needs to be tuned.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看一下*图17*，拟合的模型包括一个Tokenizer，一个HashingTF特征提取器和一个拟合的逻辑回归模型。拟合的管道模型充当了一个转换器，可以用于预测、模型验证、模型检查，最后是模型部署。然而，为了提高预测准确性的性能，模型本身需要进行调整。
- en: Now we know about the available algorithms in Spark MLlib and ML, now it's time
    to get prepared before starting to use them in a formal way for solving supervised
    and unsupervised learning problems. In the next section, we will start on feature
    extraction and transformation.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了Spark MLlib和ML中可用的算法，现在是时候在正式解决监督和无监督学习问题之前做好准备了。在下一节中，我们将开始进行特征提取和转换。
- en: Unsupervised machine learning
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督机器学习
- en: In this section, to make the discussion concrete, only the dimensionality reduction
    using PCA and the LDA for topic modeling will be discussed for text clustering.
    Other algorithms for unsupervised learning will be discussed in [Chapter 13](part0413.html#C9ROA1-21aec46d8593429cacea59dbdcd64e1c),
    *My Name is Bayes, Naive Bayes* with some practical examples.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，为了使讨论更具体，将仅讨论使用PCA进行降维和用于文本聚类的LDA主题建模。其他无监督学习算法将在[第13章](part0413.html#C9ROA1-21aec46d8593429cacea59dbdcd64e1c)中讨论，*我的名字是贝叶斯，朴素贝叶斯*，并附有一些实际示例。
- en: Dimensionality reduction
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: Dimensionality reduction is the process of reducing the number of variables
    under consideration. It can be used to extract latent features from raw and noisy
    features or to compress data while maintaining the structure. Spark MLlib provides
    support for dimensionality reduction on the `RowMatrix` class. The most commonly
    used algorithms for reducing the dimensionality of data are PCA and SVD. However,
    in this section, we will discuss PCA only to make the discussion more concrete.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 降维是减少考虑的变量数量的过程。它可以用于从原始和嘈杂的特征中提取潜在特征，或者在保持结构的同时压缩数据。Spark MLlib支持对`RowMatrix`类进行降维。用于降低数据维度的最常用算法是PCA和SVD。然而，在本节中，我们将仅讨论PCA以使讨论更具体。
- en: PCA
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA
- en: 'PCA is a statistical procedure that uses an orthogonal transformation to convert
    a set of observations of possibly correlated variables into a set of values of
    linearly uncorrelated variables called principal components. A PCA algorithm can
    be used to project vectors to a low-dimensional space using PCA. Then, based on
    the reduced feature vectors, an ML model can be trained. The following example
    shows how to project 6D feature vectors into four-dimensional principal components.
    Suppose, you have a feature vector as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种统计程序，它使用正交变换将可能相关的变量的一组观察转换为一组称为主成分的线性不相关变量的值。PCA算法可以用来使用PCA将向量投影到低维空间。然后，基于降维后的特征向量，可以训练一个ML模型。以下示例显示了如何将6D特征向量投影到四维主成分中。假设你有一个特征向量如下：
- en: '[PRE19]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now let''s create a DataFrame from it, as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从中创建一个DataFrame，如下所示：
- en: '[PRE20]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code produces a feature DataFrame having 6D feature vector for
    the PCA:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了一个具有6D特征向量的特征DataFrame用于PCA：
- en: '![](img/00291.gif)**Figure 18**: Creating a feature DataFrame (6-dimensional
    feature vectors) for PCA'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00291.gif)**图18**：创建一个特征DataFrame（6维特征向量）用于PCA'
- en: 'Now let''s instantiate the PCA model by setting necessary parameters as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过设置必要的参数来实例化PCA模型，如下所示：
- en: '[PRE21]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, to make a difference, we set the output column as `pcaFeatures` using
    the `setOutputCol()` method. Then, we set the dimension of the PCA. Finally, we
    fit the DataFrame to make the transformation. Note that the PCA model includes
    an `explainedVariance` member. A model can be loaded from such older data but
    will have an empty vector for `explainedVariance`. Now let''s show the resulting
    features:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了有所不同，我们使用`setOutputCol()`方法将输出列设置为`pcaFeatures`。然后，我们设置PCA的维度。最后，我们拟合DataFrame以进行转换。请注意，PCA模型包括一个`explainedVariance`成员。可以从这样的旧数据加载模型，但`explainedVariance`将为空向量。现在让我们展示结果特征：
- en: '[PRE22]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code produces a feature DataFrame having 4D feature vectors as
    principal components using the PCA:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了一个使用PCA的主成分作为4D特征向量的特征DataFrame：
- en: '![](img/00384.gif)**Figure 19**: Four-dimensional principal components (PCA
    features)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00384.gif)**图19**：四维主成分（PCA特征）'
- en: Using PCA
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PCA
- en: PCA, which is used widely in dimensionality reduction, is a statistical method
    that helps to find the rotation matrix. For example, if we want to check if the
    first coordinate has the largest variance possible. Also it helps to check if
    there is any succeeding coordinate that will turn the largest variance possible.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: PCA广泛用于降维，是一种帮助找到旋转矩阵的统计方法。例如，如果我们想检查第一个坐标是否具有最大可能的方差。它还有助于检查是否有任何后续坐标会产生最大可能的方差。
- en: Eventually, the PCA model calculates such parameters and returns them as a rotation
    matrix. The columns of the rotation matrix are called principal components. Spark
    MLlib supports PCA for tall and skinny matrices stored in a row-oriented format
    and any vectors.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，PCA模型计算这些参数并将它们作为旋转矩阵返回。旋转矩阵的列被称为主成分。Spark MLlib支持对以行为导向格式存储的高瘦矩阵和任何向量进行PCA。
- en: Regression Analysis - a practical use of PCA
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归分析 - PCA的实际用途
- en: In this section, we will first explore the **MSD** (**Million Song Dataset**)
    that will be used for the regression analysis. Then we will show how to use PCA
    to reduce the dimensions of the dataset. Finally, we will evaluate the linear
    regression model for the regression quality.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先探索将用于回归分析的**MSD**（**百万首歌数据集**）。然后我们将展示如何使用PCA来减少数据集的维度。最后，我们将评估回归质量的线性回归模型。
- en: Dataset collection and exploration
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集收集和探索
- en: In this section, we will describe the very famous MNIST dataset. This dataset
    will be used throughout this chapter. The MNIST database of handwritten digits
    (downloaded from [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html))
    has a training set of 60,000 examples and a test set of 10,000 examples. It is
    a subset of a larger set available from NIST. The digits have been size-normalized
    and centered in a fixed-size image. Consequently, this is a very good example
    dataset for those who are trying to learn techniques and pattern recognition methods
    on real-world data while spending minimal efforts on preprocessing and formatting.
    The original black and white (bi-level) images from NIST were size-normalized
    to fit in a 20 x 20 pixel box while preserving their aspect ratio.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述非常著名的MNIST数据集。这个数据集将在本章中使用。手写数字的MNIST数据库（从[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html)下载）有一个包含60,000个示例的训练集和一个包含10,000个示例的测试集。它是NIST提供的更大数据集的子集。这些数字已经被大小标准化并居中在固定大小的图像中。因此，对于那些试图在实际数据上学习技术和模式识别方法，同时在预处理和格式化上付出最少努力的人来说，这是一个非常好的示例数据集。来自NIST的原始黑白（双级）图像被大小标准化以适应20
    x 20像素的框，同时保持它们的长宽比。
- en: 'The MNIST database was constructed from NIST''s special database 3 and special
    database 1, which contain binary images of handwritten digits. A sample of the
    dataset is given in the following:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据库是从NIST的特殊数据库3和特殊数据库1构建的，其中包含手写数字的二进制图像。数据集的样本如下所示：
- en: '![](img/00121.gif)**Figure 20**: A snap of the MNIST dataset'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00121.gif)**图20**：MNIST数据集的快照'
- en: You can see that there are 780 features altogether. Consequently, sometimes,
    many machine learning algorithms will fail due to the high-dimensional nature
    of your dataset. Therefore, to address this issue, in the next section, we will
    show you how to reduce the dimensions without sacrificing the qualities machine
    learning tasks, such as classification. However, before diving into the problem,
    let's get some background knowledge on regression analysis first.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到总共有780个特征。因此，有时许多机器学习算法会因数据集的高维特性而失败。因此，为了解决这个问题，在下一节中，我们将向您展示如何在不牺牲机器学习任务的质量的情况下减少维度。然而，在深入研究问题之前，让我们先了解一些关于回归分析的背景知识。
- en: What is regression analysis?
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是回归分析？
- en: 'Linear regression belongs to the family of regression algorithms. The goal
    of regression is to find relationships and dependencies between variables. It
    is modeling the relationship between a continuous scalar dependent variable *y*
    (also, label or target in machine learning terminology) and one or more (a D-dimensional
    vector) explanatory variables (also, independent variables, input variables, features,
    observed data, observations, attributes, dimensions, data point, and so on) denoted
    *x* using a linear function. In regression analysis, the goal is to predict a
    continuous target variable, as shown in the following figure:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归属于回归算法家族。回归的目标是找到变量之间的关系和依赖性。它是使用线性函数对连续标量因变量*y*（也称为标签或目标，在机器学习术语中）和一个或多个（D维向量）解释变量（也称为自变量、输入变量、特征、观察数据、观测、属性、维度、数据点等）*x*之间的关系进行建模。在回归分析中，目标是预测连续的目标变量，如下图所示：
- en: '![](img/00171.jpeg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00171.jpeg)'
- en: 'Figure 21: A regression algorithm is meant to produce continuous output. The
    input is allowed to be either'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：回归算法旨在产生连续输出。输入可以是
- en: 'discrete or continuous (source: Nishant Shukla, Machine Learning with TensorFlow,
    Manning Publications co. 2017)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 离散或连续（来源：Nishant Shukla，使用TensorFlow进行机器学习，Manning Publications co. 2017）
- en: 'Now, you might have some confusion in your mind about what the basic difference
    between a classification and a regression problem is. The following information
    box will make it clearer:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可能对分类和回归问题的基本区别有些困惑。以下信息框将使其更清晰：
- en: '**Regression versus classification:** On the other hand, another area, called
    classification, is about predicting a label from a finite set but with discrete
    values. This distinction is important to know because discrete-valued output is
    handled better by classification, which will be discussed in upcoming sections.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归与分类：**另一方面，另一个领域称为分类，是关于从有限集合中预测标签，但具有离散值。这种区别很重要，因为离散值输出更适合分类，这将在接下来的部分中讨论。'
- en: 'The model for a multiple regression that involves a linear combination of input
    variables takes the following form:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及输入变量的多元回归模型采用以下形式：
- en: y = ss[0] + ss[1]x[1] + ss[2]x[2] + ss[3]x[3] +..... + e
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: y = ss[0] + ss[1]x[1] + ss[2]x[2] + ss[3]x[3] +..... + e
- en: Figure 22 shows an example of simple linear regression with one independent
    variable (*x* axis). The model (red line) is calculated using training data (blue
    points), where each point has a known label (*y* axis) to fit the points as accurately
    as possible by minimizing the value of a chosen loss function. We can then use
    the model to predict unknown labels (we only know *x* value and want to predict
    *y* value).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图22显示了一个简单线性回归的例子，其中有一个自变量（*x*轴）。模型（红线）是使用训练数据（蓝点）计算的，其中每个点都有一个已知的标签（*y*轴），以尽可能准确地拟合点，通过最小化所选损失函数的值。然后我们可以使用模型来预测未知的标签（我们只知道*x*值，想要预测*y*值）。
- en: '![](img/00383.jpeg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00383.jpeg)'
- en: 'Figure 22: Regression graph that separates data points (the dots [.] refer
    to data points in the graph and the red line refers to the regression)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：回归图分离数据点（点【.】指图中的数据点，红线指回归）
- en: Spark provides an RDD-based implementation of the linear regression algorithm.
    You can train a linear regression model with no regularization using stochastic
    gradient descent. This solves the least squares regression formulation *f (weights)
    = 1/n ||A weights-y||^2* (which is the mean squared error). Here, the data matrix
    has *n* rows, and the input RDD holds the set of rows of *A*, each with its corresponding
    right-hand side label *y*. For more information, refer to [https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/regression/LinearRegression.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/regression/LinearRegression.scala).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了基于RDD的线性回归算法实现。您可以使用随机梯度下降来训练无正则化的线性回归模型。这解决了最小二乘回归公式 *f (weights) =
    1/n ||A weights-y||^2*（即均方误差）。在这里，数据矩阵有*n*行，输入RDD保存了*A*的一组行，每个行都有其相应的右手边标签*y*。有关更多信息，请参阅[https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/regression/LinearRegression.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/regression/LinearRegression.scala)。
- en: '**Step 1\. Load the dataset and create RDD**'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1. 加载数据集并创建RDD**'
- en: 'For loading the MNIST dataset in LIBSVM format, here we used the built-in API
    called MLUtils from Spark MLlib:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 要以LIBSVM格式加载MNIST数据集，我们在这里使用了Spark MLlib的内置API MLUtils：
- en: '[PRE23]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Step 2\. Compute the number of features to make the dimensionality reduction
    easier:**'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2. 计算特征数以便更容易进行降维：**'
- en: '[PRE24]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will result in the following output:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE25]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So the dataset has 780 columns -i.e. features so this can be considered as high-dimensional
    one (features). Therefore, sometimes it is worth reducing the dimensions of the
    dataset.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据集有780列 - 即特征，因此可以将其视为高维数据（特征）。因此，有时值得减少数据集的维度。
- en: '**Step 3\. Now prepare the training and test set as follows:**'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3. 现在按以下方式准备训练和测试集：**'
- en: 'The thing is that we will train the `LinearRegressionwithSGD` model twice.
    First, we will use the normal dataset with the original dimensions of the features,
    secondly, using half of the features. With the original one, the training and
    test set preparation go as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是我们将两次训练`LinearRegressionwithSGD`模型。首先，我们将使用原始特征的正常数据集，其次，使用一半的特征。对于原始数据集，训练和测试集的准备如下进行：
- en: '[PRE26]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, for the reduced features, the training goes as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于减少的特征，训练如下进行：
- en: '[PRE27]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Step 4\. Training the linear regression model**'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4. 训练线性回归模型**'
- en: 'Now iterate 20 times and train the `LinearRegressionWithSGD` for the normal
    features and reduced features, respectively, as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在迭代20次，并分别对正常特征和减少特征进行`LinearRegressionWithSGD`训练，如下所示：
- en: '[PRE28]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Beware! Sometimes, `LinearRegressionWithSGD()` returns `NaN`. In my opinion,
    there are two reasons for this happening:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 注意！有时，`LinearRegressionWithSGD()`会返回`NaN`。在我看来，这种情况发生有两个原因：
- en: If the `stepSize` is big. In that case, you should use something smaller, such
    as 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0, and so on.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`stepSize`很大。在这种情况下，您应该使用较小的值，例如0.0001、0.001、0.01、0.03、0.1、0.3、1.0等。
- en: Your train data has `NaN`. If so, the result will likely be `NaN`. So, it is
    recommended to remove the null values prior to training the model.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的训练数据有`NaN`。如果是这样，结果可能会是`NaN`。因此，建议在训练模型之前删除空值。
- en: '**Step 5\. Evaluating both models**'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5. 评估两个模型**'
- en: 'Before we evaluate the classification model, first, let''s prepare for computing
    the MSE for the normal to see the effects of dimensionality reduction on the original
    predictions. Obviously, if you want a formal way to quantify the accuracy of the
    model and potentially increase the precision and avoid overfitting. Nevertheless,
    you can do from residual analysis. Also it would be worth to analyse the selection
    of the training and test set to be used for the model building and then the evaluation.
    Finally, selection techniques help you to describe the various attributes of a
    model:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估分类模型之前，首先让我们准备计算正常情况下的MSE，以查看降维对原始预测的影响。显然，如果您想要一种正式的方法来量化模型的准确性，并可能增加精度并避免过拟合。尽管如此，您可以通过残差分析来做。还值得分析用于模型构建和评估的训练和测试集的选择。最后，选择技术可以帮助您描述模型的各种属性：
- en: '[PRE29]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now compute the prediction sets for the PCA one as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在按以下方式计算PCA的预测集：
- en: '[PRE30]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now compute the MSE and print them for each case as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在按以下方式计算MSE并打印每种情况：
- en: '[PRE31]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You will get the following output:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下输出：
- en: '[PRE32]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Note that the MSE is actually calculated using the following formula:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，MSE实际上是使用以下公式计算的：
- en: '![](img/00238.gif)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00238.gif)'
- en: '**Step 6.** **Observing the model coefficient for both models**'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6.** **观察两个模型的模型系数**'
- en: 'Compute the model coefficient as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下方式计算模型系数：
- en: '[PRE33]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now you should observer the following output on your terminal/console:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您应该在终端/控制台上观察以下输出：
- en: '[PRE34]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Binary and multiclass classification
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二元和多类分类
- en: Binary classifiers are used to separate the elements of a given dataset into
    one of two possible groups (for example, fraud or not fraud) and are a special
    case of multiclass classification. Most binary classification metrics can be generalized
    to multiclass classification metrics. A multiclass classification describes a
    classification problem, where there are *M>2* possible labels for each data point
    (the case where *M=2* is the binary classification problem).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类器用于将给定数据集的元素分为两种可能的组（例如，欺诈或非欺诈），是多类分类的特例。大多数二元分类指标可以推广为多类分类指标。多类分类描述了一个分类问题，其中每个数据点有*M>2*个可能的标签（*M=2*的情况是二元分类问题）。
- en: For multiclass metrics, the notion of positives and negatives is slightly different.
    Predictions and labels can still be positive or negative, but they must be considered
    in the context of a particular class. Each label and prediction takes on the value
    of one of the multiple classes and so they are said to be positive for their particular
    class and negative for all other classes. So, a true positive occurs whenever
    the prediction and the label match, while a true negative occurs when neither
    the prediction nor the label takes on the value of a given class. By this convention,
    there can be multiple true negatives for a given data sample. The extension of
    false negatives and false positives from the former definitions of positive and
    negative labels is straightforward.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类指标，正例和负例的概念略有不同。预测和标签仍然可以是正面或负面，但必须考虑特定类别的上下文。每个标签和预测都取多个类别中的一个值，因此它们被认为是其特定类别的正面，对于所有其他类别则是负面。因此，每当预测和标签匹配时，就会出现真正的正例，而当预测和标签都不取给定类别的值时，就会出现真负例。按照这个约定，对于给定的数据样本，可能会有多个真负例。从前面对正负标签的定义扩展出的假负例和假正例是直接的。
- en: Performance metrics
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能指标
- en: 'While there are many different types of classification algorithms, evaluation
    metrics more or less shares similar principles. In a supervised classification
    problem, there exists a true output and a model-generated predicted output for
    each data point. For this reason, the results for each data point can be assigned
    to one of four categories:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有许多不同类型的分类算法，但评估指标或多或少共享相似的原则。在监督分类问题中，每个数据点都存在真实输出和模型生成的预测输出。因此，每个数据点的结果可以分配到四个类别中的一个：
- en: '**True positive** (**TP**): Label is positive and prediction is also positive.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正例**（**TP**）：标签为正，预测也为正。'
- en: '**True negative** (**TN**): Label is negative and prediction is also negative.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真负例**（**TN**）：标签为负，预测也为负。'
- en: '**False positive** (**FP**): Label is negative but prediction is positive.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假正例**（**FP**）：标签为负，但预测为正。'
- en: '**False negative** (**FN**): Label is positive but prediction is negative.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假负例**（**FN**）：标签为正，但预测为负。'
- en: 'Now, to get a clearer idea about these parameters, refer to the following figure:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了更清楚地了解这些参数，请参考以下图：
- en: '![](img/00181.jpeg)**Figure 23**: Prediction classifier (that is, confusion
    matrix)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00181.jpeg)**图23**：预测分类器（即混淆矩阵）'
- en: The TP, FP, TN, FN are the building blocks for most classifier evaluation metrics.
    A fundamental point when considering classifier evaluation is that pure accuracy
    (that is, was the prediction correct or incorrect) is not generally a good metric.
    The reason for this is that a dataset may be highly unbalanced. For example, if
    a model is designed to predict fraud from a dataset where 95% of the data points
    are not fraud and 5% of the data points are fraud. Then suppose a naive classifier
    predicts not fraud (regardless of input) will be 95% accurate. For this reason,
    metrics such as precision and recall are typically used because they take into
    account the type of error. In most applications, there is some desired balance
    between precision and recall, which can be captured by combining the two into
    a single metric, called the **F-measure**.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: TP、FP、TN、FN是大多数分类器评估指标的基本组成部分。在考虑分类器评估时的一个基本观点是，纯粹的准确性（即预测是否正确或不正确）通常不是一个好的度量标准。这是因为数据集可能高度不平衡。例如，如果一个模型被设计为从一个数据集中预测欺诈，其中95%的数据点不是欺诈，5%的数据点是欺诈。然后假设一个天真的分类器预测不是欺诈（不考虑输入）将有95%的准确率。因此，通常使用精确度和召回率等指标，因为它们考虑了错误的类型。在大多数应用中，精确度和召回率之间存在一定的平衡，这可以通过将两者结合成一个单一指标来捕捉，称为**F-度量**。
- en: 'Precision signifies how many of the positively classified were relevant. On
    the other hand, recall signifies how good a test is at detecting the positives?
    In binary classification, recall is called sensitivity. It is important to note
    that the the precision may not decrease with recall. The relationship between
    recall and precision can be observed in the stair step area of the plot:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度表示被正确分类的正例有多少是相关的。另一方面，召回率表示测试在检测阳性方面有多好？在二元分类中，召回率称为敏感性。重要的是要注意，精确度可能不会随着召回率而下降。召回率和精确度之间的关系可以在图中的阶梯区域中观察到：
- en: Receiver operating characteristic (ROC)
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接收器操作特性（ROC）
- en: Area under ROC curve
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC曲线下的面积
- en: Area under precision-recall curve
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度-召回率曲线下的面积
- en: 'These curves are typically used in binary classification to study the output
    of a classifier. However, sometimes it is good to combine precision and recall
    to choose between two models. In contrast, using precision and recall with multiple-number
    evaluation metrics makes it harder to compare algorithms. Suppose you have two
    algorithms that perform as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这些曲线通常用于二元分类来研究分类器的输出。然而，有时结合精确度和召回率来选择两个模型是很好的。相比之下，使用多个数字评估指标的精确度和召回率使得比较算法变得更加困难。假设您有两个算法的表现如下：
- en: '| **Classifier** | **Precision** | **Recall** |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| **分类器** | **精确度** | **召回率** |'
- en: '| X | 96% | 89% |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| X | 96% | 89% |'
- en: '| Y | 99% | 84% |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Y | 99% | 84% |'
- en: 'Here, neither classifier is obviously superior, so it doesn''t immediately
    guide you toward picking the optimal one. But using F1 score, which is a measure
    that combines precision and recall (that is, the harmonic mean of precision and
    recall), balanced the F1 score. Let''s calculate it and place it in the table:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，没有一个分类器显然优于另一个，因此它并不能立即指导您选择最佳的分类器。但是使用F1分数，这是一个结合了精确度和召回率（即精确度和召回率的调和平均值）的度量，平衡了F1分数。让我们计算一下，并将其放在表中：
- en: '| **Classifier** | **Precision** | **Recall** | **F1 score** |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| **分类器** | **精确度** | **召回率** | **F1分数** |'
- en: '| X | 96% | 89% | 92.36% |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| X | 96% | 89% | 92.36% |'
- en: '| Y | 99% | 84% | 90.885% |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| Y | 99% | 84% | 90.885% |'
- en: Therefore, having F1-score helps make a decision for selecting from a large
    number of classifiers. It gives a clear preference ranking among all of them and
    therefore a clear direction for progress, that is, classifier **X**.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，具有F1分数有助于从大量分类器中进行选择。它为所有分类器提供了清晰的偏好排名，因此为进展提供了明确的方向，即分类器**X**。
- en: 'For the binary classification, the preceding performance metrics can be calculated
    as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类，可以计算前述性能指标如下：
- en: '![](img/00057.jpeg)**Figure 24**: Mathematical formula for computing performance
    metrics for binary classifiers (source: [https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html))'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00057.jpeg)**图24**：计算二元分类器性能指标的数学公式（来源：[https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html)）'
- en: 'However, in multiclass classification problems where more than two predicted
    labels are associated, computing the earlier metrics is more complex but can be
    computed using the following mathematical equations:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在多类分类问题中，与两个预测标签相关联的情况下，计算先前的指标更加复杂，但可以使用以下数学方程进行计算：
- en: '![](img/00169.jpeg)**Figure 25**: Mathematical formula for computing performance
    metrics for multiclass classifiers'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00169.jpeg)**图25**：计算多类分类器性能指标的数学公式'
- en: 'Where *δ*^(*x*) is called modified delta function and that can be defined as
    follows (source: [https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html)):'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的δ函数称为修改的δ函数，可以定义如下（来源：[https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html)）：
- en: '![](img/00026.jpeg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00026.jpeg)'
- en: Binary classification using logistic regression
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行二元分类
- en: 'Logistic regression is widely used to predict a binary response. This is a
    linear method that can be written mathematically as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归广泛用于预测二元响应。这是一种可以用数学方式表示的线性方法：
- en: '![](img/00160.jpeg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00160.jpeg)'
- en: In the preceding equation, *L(w; x, y)* is the loss function is called logistic
    loss.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，*L(w; x, y)*是称为逻辑损失的损失函数。
- en: 'For binary classification problems, the algorithm will output a binary logistic
    regression model. Given a new data point, denoted by *x*, the model makes predictions
    by applying the logistic function:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类问题，该算法将输出一个二元逻辑回归模型。给定一个新的数据点，用*x*表示，该模型通过应用逻辑函数进行预测：
- en: '![](img/00233.jpeg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00233.jpeg)'
- en: Where *z = w^Tx,* and by default, if *f(w^Tx)>0.5*, the outcome is positive,
    or negative otherwise, though unlike linear SVMs, the raw output of the logistic
    regression model, *f(z)*, has a probabilistic interpretation (that is, the probability
    that *x* is positive).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*z = w^Tx*，默认情况下，如果*f(w^Tx)>0.5*，结果为正，否则为负，尽管与线性支持向量机不同，逻辑回归模型的原始输出*f(z)*具有概率解释（即*x*为正的概率）。
- en: '**Linear SVM** is the newest extremely fast machine learning (data mining)
    algorithm for solving multiclass classification problems from ultralarge datasets
    that implements an original proprietary version of a cutting plane algorithm for
    designing a linear support vector machine (source: [www.linearsvm.com/](http://www.linearsvm.com/)
    ).'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性支持向量机**是最新的极快的机器学习（数据挖掘）算法，用于解决超大数据集的多类分类问题，实现了一个原始专有版本的线性支持向量机的切割平面算法（来源：[www.linearsvm.com/](http://www.linearsvm.com/)）。'
- en: Breast cancer prediction using logistic regression of Spark ML
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark ML的逻辑回归进行乳腺癌预测
- en: In this section, we will look at how to develop a cancer diagnosis pipeline
    with Spark ML. A real dataset will be used to predict the probability of breast
    cancer. To be more specific, Wisconsin Breast Cancer Dataset will be used.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看看如何使用Spark ML开发癌症诊断管道。将使用真实数据集来预测乳腺癌的概率。更具体地说，将使用威斯康星乳腺癌数据集。
- en: Dataset collection
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集收集
- en: 'Here, we have used simpler datasets that are structured and manually curated
    for machine learning application development, and, of course, many of them show
    good classification accuracy. The Wisconsin Breast Cancer Dataset from the UCI
    machine learning repository ([https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)),
    contains data that was donated by researchers at the University of Wisconsin and
    includes measurements from digitized images of fine-needle aspirations of breast
    masses. The values represent characteristics of the cell nuclei present in the
    digital images described in the following subsection:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了更简单的数据集，这些数据集经过结构化和手动筛选，用于机器学习应用程序开发，当然，其中许多数据集显示出良好的分类准确性。来自UCI机器学习库的威斯康星乳腺癌数据集（[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)）包含了由威斯康星大学的研究人员捐赠的数据，并包括来自乳腺肿块的细针抽吸的数字化图像的测量。这些值代表数字图像中细胞核的特征，如下一小节所述：
- en: '[PRE35]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To read more about the Wisconsin Breast Cancer Dataset, refer to the authors''
    publication: *Nuclear feature extraction for breast tumor diagnosis*, *IS&T/SPIE*
    1993 *International Symposium on Electronic Imaging: Science and Technology*,
    volume 1905, pp 861-870 by *W.N. Street*, *W.H. Wolberg*, and *O.L. Mangasarian*,
    1993.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于威斯康星乳腺癌数据集的信息，请参阅作者的出版物：*用于乳腺肿瘤诊断的核特征提取*，*IS&T/SPIE* 1993 *国际电子成像研讨会：科学与技术*，卷1905，第861-870页，作者为*W.N.
    Street*，*W.H. Wolberg*和*O.L. Mangasarian*，1993年。
- en: Developing the pipeline using Spark ML
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark ML开发管道
- en: 'Now we will show you how to predict the possibility of breast cancer with step-by-step
    example:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将向您展示如何通过逐步示例预测乳腺癌的可能性：
- en: '**Step 1: Load and parse the data**'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：加载和解析数据**'
- en: '[PRE36]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `parseRDD()` method goes as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '`parseRDD()`方法如下：'
- en: '[PRE37]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `parseCancer()` method is as follows:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '`parseCancer()`方法如下：'
- en: '[PRE38]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Note that here we have simplified the dataset. For the value 4.0, we have converted
    them to 1.0, and 0.0 otherwise. The `Cancer` class is a case class that can be
    defined as follows:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里我们简化了数据集。对于值4.0，我们已将其转换为1.0，否则为0.0。`Cancer`类是一个可以定义如下的案例类：
- en: '[PRE39]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Step 2: Convert RDD to DataFrame for the ML pipeline**'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：将RDD转换为ML管道的DataFrame**'
- en: '[PRE40]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The DataFrame looks like the following:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame如下所示：
- en: '![](img/00334.gif)**Figure 26:** A snap of the cancer dataset'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00334.gif)**图26：**癌症数据集的快照'
- en: '**Step 3: Feature extraction and transformation**'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：特征提取和转换**'
- en: 'At first, let''s select the feature column, as follows:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们选择特征列，如下所示：
- en: '[PRE41]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now let''s assemble them into a feature vector, as follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将它们组装成一个特征向量，如下所示：
- en: '[PRE42]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now transform them into a DataFrame, as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将它们转换为DataFrame，如下所示：
- en: '[PRE43]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s see the structure of the transformed DataFrame:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下转换后的DataFrame的结构：
- en: '[PRE44]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now you should observe a DataFrame containing the features calculated based
    on the columns on the left:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该观察到一个包含基于左侧列计算的特征的DataFrame：
- en: '![](img/00361.gif)**Figure 27:** New DataFrame containing features'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00361.gif)**图27：**包含特征的新DataFrame'
- en: 'Finally, let''s use the `StringIndexer` and create the label for the training
    dataset, as follows:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用`StringIndexer`为训练数据集创建标签，如下所示：
- en: '[PRE45]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now you should observe a DataFrame containing the features and labels calculated
    based on the columns in the left:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该观察到一个包含基于左侧列计算的特征和标签的DataFrame：
- en: '![](img/00204.gif)**Figure 28:** New DataFrame containing features and labels
    to training the ML models'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00204.gif)**图28：**包含用于训练ML模型的特征和标签的新DataFrame'
- en: '**Step 4: Create test and training set**'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：创建测试和训练集**'
- en: '[PRE46]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Step 5: Creating an estimator using the training sets**'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：使用训练集创建估计器**'
- en: 'Let''s create an estimator for the pipeline using the logistic regression with
    `elasticNetParam`. We also specify the max iteration and regression parameter,
    as follows:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用带有`elasticNetParam`的逻辑回归创建管道的估计器。我们还指定最大迭代次数和回归参数，如下所示：
- en: '[PRE47]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '**Step 6: Getting raw prediction, probability, and prediction for the test
    set**'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：获取测试集的原始预测、概率和预测**'
- en: 'Transform the model using the test set to get raw prediction, probability,
    and prediction for the test set:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 使用测试集转换模型以获取测试集的原始预测、概率和预测：
- en: '[PRE48]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The resulting DataFrame is as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的DataFrame如下所示：
- en: '![](img/00159.gif)**Figure 29:** New DataFrame with raw prediction and actual
    prediction against each row'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00159.gif)**图29：**包含每行的原始预测和实际预测的新DataFrame'
- en: '**Step 7: Generating objective history of training**'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：生成训练的目标历史**'
- en: 'Let''s generate the objective history of the model in each iteration, as follows:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成模型在每次迭代中的目标历史，如下所示：
- en: '[PRE49]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The preceding code segment produces the following output in terms of training
    loss:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码段产生了以下关于训练损失的输出：
- en: '[PRE50]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: As you can see, the loss gradually reduces in later iterations.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，损失在后续迭代中逐渐减少。
- en: '**Step 8: Evaluating the model**'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：评估模型**'
- en: 'First, we will have to make sure that the classifier that we used comes from
    the binary logistic regression summary:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须确保我们使用的分类器来自二元逻辑回归摘要：
- en: '[PRE51]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now let''s obtain the ROC as a `DataFrame` and `areaUnderROC`. A value approximate
    to 1.0 is better:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们获取ROC作为`DataFrame`和`areaUnderROC`。接近1.0的值更好：
- en: '[PRE52]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The preceding lines prints the value of `areaUnderROC`, as follows:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的行打印了`areaUnderROC`的值，如下所示：
- en: '[PRE53]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This is excellent! Now let''s compute other metrics, such as true positive
    rate, false positive rate, false negative rate, and total count, and a number
    of instances correctly and wrongly predicted, as follows:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这太棒了！现在让我们计算其他指标，如真阳性率、假阳性率、假阴性率、总计数以及正确和错误预测的实例数量，如下所示：
- en: '[PRE54]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now you should observe an output from the preceding code as follows:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该观察到前面代码的输出如下：
- en: '**Total Count: 209** **Correctly Predicted: 202** **Wrongly Identified: 7**
    **True Positive: 140** **False Negative: 4** **False Positive: 3** **ratioWrong:
    0.03349282296650718** **ratioCorrect: 0.9665071770334929**'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '**总计数：209** **正确预测：202** **错误识别：7** **真阳性：140** **假阴性：4** **假阳性：3** **错误比率：0.03349282296650718**
    **正确比率：0.9665071770334929**'
- en: 'Finally, let''s judge the accuracy of the model. However, first, we need to
    set the model threshold to maximize `fMeasure`:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们评估模型的准确性。但是，首先，我们需要将模型阈值设置为最大化`fMeasure`：
- en: '[PRE55]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now let''s compute the accuracy, as follows:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算准确性，如下所示：
- en: '[PRE56]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The preceding code produces the following output, which is almost 99.64%:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码产生了以下输出，几乎为99.64%：
- en: '[PRE57]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Multiclass classification using logistic regression
  id: totrans-401
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行多类分类
- en: A binary logistic regression can be generalized into multinomial logistic regression
    to train and predict multiclass classification problems. For example, for *K*
    possible outcomes, one of the outcomes can be chosen as a pivot, and the other
    *K−1* outcomes can be separately regressed against the pivot outcome. In `spark.mllib`,
    the first class 0 is chosen as the `pivot` class.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 二元逻辑回归可以推广为多项式逻辑回归，用于训练和预测多类分类问题。例如，对于*K*个可能的结果，可以选择其中一个结果作为枢轴，其他*K−1*个结果可以分别对枢轴结果进行回归。在`spark.mllib`中，选择第一个类0作为`pivot`类。
- en: For multiclass classification problems, the algorithm will output a multinomial
    logistic regression model, which contains *k−1binary* logistic regression models
    regressed against the first class. Given a new data point, *k−1models* will be
    run, and the class with the largest probability will be chosen as the predicted
    class. In this section, we will show you an example of a classification using
    the logistic regression with L-BFGS for faster convergence.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类分类问题，算法将输出一个多项式逻辑回归模型，其中包含*k−1个*二元逻辑回归模型，回归到第一个类。给定一个新的数据点，将运行*k−1个模型，并选择具有最大概率的类作为预测类。在本节中，我们将通过使用带有L-BFGS的逻辑回归的分类示例来向您展示。
- en: '**Step 1\. Load and parse the MNIST dataset in LIVSVM format**'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：在LIVSVM格式中加载和解析MNIST数据集**'
- en: '[PRE58]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '**Step 2\. Prepare the training and test sets**'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2. 准备训练和测试集**'
- en: 'Split data into training (75%) and test (25%), as follows:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分割为训练集（75%）和测试集（25%），如下所示：
- en: '[PRE59]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '**Step 3\. Run the training algorithm to build the model**'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3. 运行训练算法来构建模型**'
- en: 'Run the training algorithm to build the model by setting a number of classes
    (10 for this dataset). For better classification accuracy, you can also specify
    intercept and validate the dataset using the Boolean true value, as follows:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 运行训练算法，通过设置一定数量的类别（对于这个数据集为10）来构建模型。为了获得更好的分类准确性，您还可以指定截距，并使用布尔值true来验证数据集，如下所示：
- en: '[PRE60]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Set intercept as true if the algorithm should add an intercept using `setIntercept()`.
    If you want the algorithm to validate the training set before the model building
    itself, you should set the value as true using the `setValidateData()` method.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如果算法应该使用`setIntercept()`添加一个截距，则将截距设置为true。如果您希望算法在模型构建之前验证训练集，您应该使用`setValidateData()`方法将值设置为true。
- en: '**Step 4\. Clear the default threshold**'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4. 清除默认阈值**'
- en: 'Clear the default threshold so that the training does not occur with the default
    setting, as follows:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 清除默认阈值，以便训练不使用默认设置进行，如下所示：
- en: '[PRE61]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '**Step 5\. Compute raw scores on the test set**'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5. 在测试集上计算原始分数**'
- en: 'Compute raw scores on the test set so that we can evaluate the model using
    the aforementioned performance metrics, as follows:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上计算原始分数，以便我们可以使用上述性能指标评估模型，如下所示：
- en: '[PRE62]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '**Step 6\. Instantiate a multiclass metrics for the evaluation**'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6. 实例化一个多类度量以进行评估**'
- en: '[PRE63]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '**Step 7\. Constructing the confusion matrix**'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7. 构建混淆矩阵**'
- en: '[PRE64]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'In a confusion matrix, each column of the matrix represents the instances in
    a predicted class, while each row represents the instances in an actual class
    (or vice versa). The name stems from the fact that it makes it easy to see if
    the system is confusing two classes. For more, refer to matrix ([https://en.wikipedia.org/wiki/Confusion_matrix.Confusion](https://en.wikipedia.org/wiki/Confusion_matrix.Confusion)):'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在混淆矩阵中，矩阵的每一列代表预测类别中的实例，而每一行代表实际类别中的实例（反之亦然）。名称源自于这样一个事实，即它很容易看出系统是否混淆了两个类别。更多信息，请参阅矩阵（[https://en.wikipedia.org/wiki/Confusion_matrix.Confusion](https://en.wikipedia.org/wiki/Confusion_matrix.Confusion)）：
- en: '![](img/00065.gif)**Figure 30:** Confusion matrix generated by the logistic
    regression classifier'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00065.gif)**图30：** 逻辑回归分类器生成的混淆矩阵'
- en: '**Step 8\. Overall statistics**'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8. 整体统计**'
- en: 'Now let''s compute the overall statistics to judge the performance of the model:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算整体统计数据来判断模型的性能：
- en: '[PRE65]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The preceding code segment produces the following output, containing some performance
    metrics, such as accuracy, precision, recall, true positive rate , false positive
    rate, and f1 score:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码段产生了以下输出，包含一些性能指标，如准确度、精确度、召回率、真正率、假正率和f1分数：
- en: '[PRE66]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now let''s compute the overall, that is, summary statistics:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算整体统计数据，即总结统计数据：
- en: '[PRE67]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The preceding code segment prints the following output containing weighted
    precision, recall, f1 score, and false positive rate:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码段打印了包含加权精确度、召回率、f1分数和假正率的以下输出：
- en: '[PRE68]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The overall statistics say that the accuracy of the model is more than 92%.
    However, we can still improve it using a better algorithm such as **random forest**
    (**RF**). In the next section, we will look at the random forest implementation
    to classify the same model.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 总体统计数据表明，模型的准确性超过92%。然而，我们仍然可以通过使用更好的算法（如随机森林RF）来改进。在下一节中，我们将看一下随机森林实现，以对同一模型进行分类。
- en: Improving classification accuracy using random forests
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林提高分类准确性
- en: Random forests (also sometimes called random decision forests) are ensembles
    of decision trees. Random forests are one of the most successful machine learning
    models for classification and regression. They combine many decision trees in
    order to reduce the risk of overfitting. Like decision trees, random forests handle
    categorical features, extend to the multiclass classification setting, do not
    require feature scaling, and are able to capture nonlinearities and feature interactions.
    There are numerous advantageous RFs. They can overcome the overfitting problem
    across their training dataset by combining many decision trees.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林（有时也称为随机决策森林）是决策树的集成。随机森林是分类和回归中最成功的机器学习模型之一。它们结合了许多决策树，以减少过拟合的风险。与决策树一样，随机森林处理分类特征，扩展到多类分类设置，不需要特征缩放，并且能够捕捉非线性和特征交互。RF有许多优点。它们可以通过组合许多决策树来克服其训练数据集上的过拟合问题。
- en: A forest in the RF or RDF usually consists of hundreds of thousands of trees.
    These trees are actually trained on different parts of the same training set.
    More technically, an individual tree that has grown very deep tends to learn from
    highly unpredictable patterns. This kind of nature of the trees creates overfitting
    problems on the training sets. Moreover, low biases make the classifier a low
    performer even if your dataset quality is good in terms of features presented.
    On the other hand, an RF helps to average multiple decision trees together with
    the goal of reducing the variance to ensure consistency by computing proximities
    between pairs of cases.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: RF或RDF中的森林通常由数十万棵树组成。这些树实际上是在同一训练集的不同部分上训练的。更技术上地说，生长得非常深的单个树往往会学习到高度不可预测的模式。树的这种性质会在训练集上产生过拟合问题。此外，低偏差使得分类器即使在特征质量良好的情况下也表现不佳。另一方面，RF有助于通过计算案例之间的接近度将多个决策树平均在一起，以减少方差，以确保一致性。
- en: 'However, this increases a small bias or some loss of the interpretability of
    the results. But, eventually, the performance of the final model is increased
    dramatically. While using the RF as a classifier, here goes the parameter setting:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这会增加一些偏差或结果的可解释性的损失。但是，最终模型的性能会显著提高。在使用RF作为分类器时，以下是参数设置：
- en: If the number of trees is 1, then no bootstrapping is used at all; however,
    if the number of trees is *> 1*, then bootstrapping is accomplished. The supported
    values are `auto`, `all`, `sqrt`, `log2`, and `onethird`.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果树的数量为1，则根本不使用自举；但是，如果树的数量为*> 1*，则会进行自举。支持的值为`auto`、`all`、`sqrt`、`log2`和`onethird`。
- en: The supported numerical values are *(0.0-1.0]* and *[1-n]*. However, if `featureSubsetStrategy`
    is chosen as `auto`, the algorithm chooses the best feature subset strategy automatically.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持的数值范围为*(0.0-1.0]*和*[1-n]*。但是，如果`featureSubsetStrategy`选择为`auto`，算法会自动选择最佳的特征子集策略。
- en: If `numTrees == 1`, the `featureSubsetStrategy` is set to be `all`. However,
    if `numTrees > 1` (that is, forest), `featureSubsetStrategy` is set to be `sqrt`
    for classification.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`numTrees == 1`，则`featureSubsetStrategy`设置为`all`。但是，如果`numTrees > 1`（即森林），则`featureSubsetStrategy`设置为`sqrt`用于分类。
- en: Moreover, if a real value *n* is set in the range of *(0, 1.0]*, `n*number_of_features`
    will be used. However, if an integer value say *n* is in the `range (1, the number
    of features)`, only `n` features are used alternatively.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，如果在范围*(0, 1.0]*内设置了实际值*n*，将使用`n*number_of_features`。但是，如果在范围(1,特征数)内设置了整数值*n*，则只会交替使用`n`个特征。
- en: 'The `categoricalFeaturesInfo` parameter , which is a map, is used for storing
    arbitrary categorical features. An entry *(n -> k)* indicates that feature *n*
    is categorical with *k* categories indexed from *0: {0, 1,...,k-1}.*'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`categoricalFeaturesInfo`参数是一个用于存储任意分类特征的映射。条目*(n -> k)*表示特征*n*是具有*k*个类别的分类特征，索引从*0:
    {0, 1,...,k-1}*。'
- en: The impurity criterion is used only for the information gain calculation. The
    supported values are *gini* and *variance* for classification and regression,
    respectively.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅用于信息增益计算的杂质标准。支持的值分别为*gini*和*variance*，用于分类和回归。
- en: The `maxDepth` is the maximum depth of the tree (for example, depth 0 means
    1 leaf node, depth 1 means 1 internal node *+ 2* leaf nodes, and so on).
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxDepth`是树的最大深度（例如，深度0表示1个叶节点，深度1表示1个内部节点+2个叶节点，依此类推）。'
- en: The `maxBins` signifies the maximum number of bins used for splitting the features,
    where the suggested value is 100 to get better results.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxBins`表示用于分割特征的最大箱数，建议值为100以获得更好的结果。'
- en: Finally, the random seed is used for bootstrapping and choosing feature subsets
    to avoid the random nature of the results.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，随机种子用于自举和选择特征子集，以避免结果的随机性。
- en: As already mentioned, since RF is fast and scalable enough for the large-scale
    dataset, Spark is a suitable technology to implement the RF to take the massive
    scalability. However, if the proximities are calculated, storage requirements
    also grow exponentially.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，由于RF对大规模数据集的快速和可扩展性足够强，Spark是实现RF以实现大规模可扩展性的合适技术。但是，如果计算了接近度，存储需求也会呈指数级增长。
- en: Classifying MNIST dataset using random forest
  id: totrans-449
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林对MNIST数据集进行分类
- en: In this section, we will show an example of a classification using the random
    forest. We will break down the code step-by-step so that you can understand the
    solution easily.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示使用随机森林进行分类的示例。我们将逐步分解代码，以便您可以轻松理解解决方案。
- en: '**Step 1\. Load and parse the MNIST dataset in LIVSVM format**'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1. 加载和解析LIVSVM格式的MNIST数据集**'
- en: '[PRE69]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '**Step 2\. Prepare the training and test sets**'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2. 准备训练和测试集**'
- en: 'Split data into training (75%) and test (25%) and also set the seed for the
    reproducibility, as follows:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分为训练集（75%）和测试集（25%），并设置种子以实现可重现性，如下所示：
- en: '[PRE70]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '**Step 3\. Run the training algorithm to build the model**'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3. 运行训练算法来构建模型**'
- en: 'Train a random forest model with an empty `categoricalFeaturesInfo. This required`
    since all the features are continuous in the dataset:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 使用空的`categoricalFeaturesInfo`训练随机森林模型。这是必需的，因为数据集中的所有特征都是连续的：
- en: '[PRE71]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Note that training a random forest model is very resource extensive. Consequently,
    it will take more memory, so beware of OOM. I would say increase the Java heap
    space prior to running this code.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，训练随机森林模型需要大量资源。因此，它将占用更多内存，所以要注意OOM。我建议在运行此代码之前增加Java堆空间。
- en: '**Step 4\. Compute raw scores on the test set**'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4. 在测试集上计算原始分数**'
- en: 'Compute raw scores on the test set so that we can evaluate the model using
    the aforementioned performance metrics, as follows:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上计算原始分数，以便我们可以使用上述性能指标评估模型，如下所示：
- en: '[PRE72]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '**Step 5\. Instantiate a multiclass metrics for the evaluation**'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5. 实例化一个多类指标进行评估**'
- en: '[PRE73]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '**Step 6\. Constructing the confusion matrix**'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6. 构建混淆矩阵**'
- en: '[PRE74]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The preceding code prints the following confusion matrix for our classification:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码打印了我们分类的以下混淆矩阵：
- en: '![](img/00122.gif)**Figure 31:** Confusion matrix generated by the random forest
    classifier'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00122.gif)**图31：**由随机森林分类器生成的混淆矩阵'
- en: '**Step 7\. Overall statistics**'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7. 总体统计**'
- en: 'Now let''s compute the overall statistics to judge the performance of the model:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算总体统计数据，以评估模型的性能：
- en: '[PRE75]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The preceding code segment produces the following output, containing some performance
    metrics, such as accuracy, precision, recall, true positive rate , false positive
    rate, and F1 score:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码段产生以下输出，包含一些性能指标，如准确度、精度、召回率、真正率、假正率和F1分数：
- en: '[PRE76]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Now let''s compute the overall statistics, as follows:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算总体统计数据，如下所示：
- en: '[PRE77]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The preceding code segment prints the following output, containing weighted
    precision, recall, F1 score, and false positive rate:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码段打印以下输出，包含加权精度、召回率、F1分数和假正率：
- en: '[PRE78]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: The overall statistics say that the accuracy of the model is more than 96%,
    which is better than that of logistic regression. However, we can still improve
    it using better model tuning.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 总体统计数据表明，模型的准确度超过96%，比逻辑回归的准确度更高。但是，我们仍然可以通过更好的模型调整来改进它。
- en: Summary
  id: totrans-479
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we had a brief introduction to the topic and got a grasp of
    simple, yet powerful and common ML techniques. Finally, you saw how to build your
    own predictive model using Spark. You learned how to build a classification model,
    how to use the model to make predictions, and finally, how to use common ML techniques
    such as dimensionality reduction and One-Hot Encoding.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要介绍了这个主题，并掌握了简单但强大和常见的机器学习技术。最后，您学会了如何使用Spark构建自己的预测模型。您学会了如何构建分类模型，如何使用模型进行预测，最后，如何使用常见的机器学习技术，如降维和独热编码。
- en: In the later sections, you saw how to apply the regression technique to high-dimensional
    datasets. Then, you saw how to apply a binary and multiclass classification algorithm
    for predictive analytics. Finally, you saw how to achieve outstanding classification
    accuracy using a random forest algorithm. However, we have other topics in machine
    learning that need to be covered too, for example, recommendation systems and
    model tuning for even more stable performance before you finally deploy the models.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的部分，您看到了如何将回归技术应用于高维数据集。然后，您看到了如何应用二元和多类分类算法进行预测分析。最后，您看到了如何使用随机森林算法实现出色的分类准确性。然而，我们还有其他机器学习的主题需要涵盖，例如推荐系统和模型调优，以获得更稳定的性能，然后再部署模型。
- en: In the next chapter, we will cover some advanced topics of Spark. We will provide
    examples of machine learning model tuning for better performance, and we will
    also cover two examples for movie recommendation and text clustering, respectively.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将涵盖一些Spark的高级主题。我们将提供机器学习模型调优的示例，以获得更好的性能，我们还将分别介绍电影推荐和文本聚类的两个示例。
