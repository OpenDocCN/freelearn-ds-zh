- en: Chapter 5. Apache Spark GraphX
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。Apache Spark GraphX
- en: 'In this chapter, I want to examine the Apache Spark GraphX module, and graph
    processing in general. I also want to briefly examine graph-based storage by looking
    at the graph database called Neo4j. So, this chapter will cover the following
    topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我想要研究Apache Spark GraphX模块和图处理。我还想简要介绍一下图数据库Neo4j。因此，本章将涵盖以下主题：
- en: GraphX coding
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GraphX编码
- en: Mazerunner for Neo4j
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neo4j的Mazerunner
- en: The GraphX coding section, written in Scala, will provide a series of graph
    coding examples. The work carried out on the experimental Mazerunner product by
    Kenny Bastani, which I will also examine, ties the two topics together in one
    practical example. It provides an example prototype-based on Docker to replicate
    data between Apache Spark GraphX, and Neo4j storage.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX编码部分使用Scala编写，将提供一系列图编码示例。Kenny Bastani在实验性产品Mazerunner上的工作，我也将进行审查，将这两个主题结合在一个实际示例中。它提供了一个基于Docker的示例原型，用于在Apache
    Spark GraphX和Neo4j存储之间复制数据。
- en: Before writing code in Scala to use the Spark GraphX module, I think it would
    be useful to provide an overview of what a graph actually is in terms of graph
    processing. The following section provides a brief introduction using a couple
    of simple graphs as examples.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中编写代码使用Spark GraphX模块之前，我认为提供一个关于图处理实际上是什么的概述会很有用。下一节将使用一些简单的图示例进行简要介绍。
- en: Overview
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概览
- en: A graph can be considered to be a data structure, which consists of a group
    of vertices, and edges that connect them. The vertices or nodes in the graph can
    be objects or perhaps, people, and the edges are the relationships between them.
    The edges can be directional, meaning that the relationship operates from one
    node to the next. For instance, node A is the father of node B.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图可以被视为一种数据结构，它由一组顶点和连接它们的边组成。图中的顶点或节点可以是对象，也可以是人，而边缘则是它们之间的关系。边缘可以是有方向的，意味着关系是从一个节点到下一个节点的。例如，节点A是节点B的父亲。
- en: In the following diagram, the circles represent the vertices or nodes (**A**
    to **D**), whereas the thick lines represent the edges, or relationships between
    them (**E1** to **E6**). Each node, or edge may have properties, and these values
    are represented by the associated grey squares (**P1** to **P7**).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图表中，圆圈代表顶点或节点（**A**到**D**），而粗线代表边缘或它们之间的关系（**E1**到**E6**）。每个节点或边缘可能具有属性，这些值由相关的灰色方块（**P1**到**P7**）表示。
- en: So, if a graph represented a physical route map for route finding, then the
    edges might represent minor roads or motorways. The nodes would be motorway junctions,
    or road intersections. The node and edge properties might be the road type, speed
    limit, distance, and the cost and grid locations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果一个图代表了寻路的实际路线图，那么边缘可能代表次要道路或高速公路。节点将是高速公路交叉口或道路交叉口。节点和边缘的属性可能是道路类型、速度限制、距离、成本和网格位置。
- en: There are many types of graph implementation, but some examples are fraud modeling,
    financial currency transaction modeling, social modeling (as in friend-to-friend
    connections on Facebook), map processing, web processing, and page ranking.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多类型的图实现，但一些例子包括欺诈建模、金融货币交易建模、社交建模（如Facebook上的朋友关系）、地图处理、网络处理和页面排名。
- en: '![Overview](img/B01989_05_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![概览](img/B01989_05_01.jpg)'
- en: The previous diagram shows a generic example of a graph with associated properties.
    It also shows that the edge relationships can be directional, that is, the **E2**
    edge acts from node **B** to node **C**. However, the following example uses family
    members, and the relationships between them to create a graph. Note that there
    can be multiple edges between two nodes or vertices. For instance, the husband-and-wife
    relationships between **Mike** and **Sarah**. Also, it is possible that there
    could be multiple properties on a node or edge.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了一个带有相关属性的图的通用示例。它还显示了边缘关系可以是有方向的，也就是说，**E2**边缘是从节点**B**到节点**C**的。然而，下面的例子使用了家庭成员及其之间的关系来创建一个图。请注意，两个节点或顶点之间可以有多条边。例如，**Mike**和**Sarah**之间的夫妻关系。此外，一个节点或边上可能有多个属性。
- en: '![Overview](img/B01989_05_02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![概览](img/B01989_05_02.jpg)'
- en: 'So, in the previous example, the **Sister** property acts from node 6 **Flo**,
    to node 1, **Mike**. These are simple graphs to explain the structure of a graph,
    and the element nature. Real graph applications can reach extreme sizes, and require
    both, distributed processing, and storage to enable them to be manipulated. Facebook
    is able to process graphs, containing over 1 trillion edges using **Apache Giraph**
    (source: Avery Ching-Facebook). Giraph is an Apache Hadoop eco-system tool for
    graph processing, which has historically based its processing on Map Reduce, but
    now uses TinkerPop, which will be introduced in [Chapter 6](ch06.html "Chapter 6. Graph-based
    Storage"), *Graph-based Storage*. Although this book concentrates on Apache Spark,
    the number of edges provides a very impressive indicator of the size that a graph
    can reach.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在前面的例子中，**Sister**属性是从节点6 **Flo**到节点1 **Mike**的。这些都是简单的图，用来解释图的结构和元素性质。真实的图应用可能会达到极大的规模，并且需要分布式处理和存储来使它们能够被操作。Facebook能够处理包含超过1万亿边的图，使用**Apache
    Giraph**（来源：Avery Ching-Facebook）。Giraph是用于图处理的Apache Hadoop生态系统工具，它在历史上基于Map
    Reduce进行处理，但现在使用TinkerPop，这将在[第6章](ch06.html "第6章。基于图的存储")中介绍，*基于图的存储*。尽管本书集中讨论Apache
    Spark，但边的数量提供了一个非常令人印象深刻的指标，显示了图可以达到的规模。
- en: In the next section, I will examine the use of the Apache Spark GraphX module
    using Scala.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我将使用Scala来研究Apache Spark GraphX模块的使用。
- en: GraphX coding
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GraphX编码
- en: This section will examine Apache Spark GraphX programming in Scala, using the
    family relationship graph data sample, which was shown in the last section. This
    data will be stored on HDFS, and will be accessed as a list of vertices and edges.
    Although this data set is small, the graphs that you build in this way could be
    very large. I have used HDFS for storage, because if your graph scales to the
    big data scale, then you will need some type of distributed and redundant storage.
    As this chapter shows by way of example, that could be HDFS. Using the Apache
    Spark SQL module, the storage could also be Apache Hive; see [Chapter 4](ch04.html
    "Chapter 4. Apache Spark SQL"), *Apache Spark SQL*, for details.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将使用上一节中展示的家庭关系图数据样本，使用Scala中的Apache Spark GraphX编程来进行分析。这些数据将存储在HDFS上，并将作为顶点和边的列表进行访问。尽管这个数据集很小，但是用这种方式构建的图可能非常大。我使用HDFS进行存储，因为如果你的图扩展到大数据规模，那么你将需要某种类型的分布式和冗余存储。正如本章所示的例子，这可能是HDFS。使用Apache
    Spark SQL模块，存储也可以是Apache Hive；详情请参见[第4章](ch04.html "第4章。Apache Spark SQL")，“Apache
    Spark SQL”。
- en: Environment
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境
- en: 'I have used the hadoop Linux account on the server `hc2nn` to develop the Scala-based
    GraphX code. The structure for SBT compilation follows the same pattern as the
    previous examples, with the code tree existing in a subdirectory named `graphx`,
    where an `sbt` configuration file called `graph.sbt` resides:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了服务器“hc2nn”上的hadoop Linux账户来开发基于Scala的GraphX代码。SBT编译的结构遵循与之前示例相同的模式，代码树存在于名为“graphx”的子目录中，其中有一个名为“graph.sbt”的SBT配置文件：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The source code lives, as expected, under a subtree of this level called `src/main/scala`,
    and contains five code samples:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码如预期的那样位于此级别的子树下，名为“src/main/scala”，包含五个代码示例：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In each graph-based example, the Scala file uses the same code to load data
    from HDFS, and to create a graph; but then, each file provides a different facet
    of GraphX-based graph processing. As a different Spark module is being used in
    this chapter, the `sbt` configuration file `graph.sbt` has been changed to support
    this work:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个基于图的示例中，Scala文件使用相同的代码从HDFS加载数据，并创建图；但是，每个文件提供了基于GraphX的图处理的不同方面。由于本章使用了不同的Spark模块，“sbt”配置文件“graph.sbt”已经更改以支持这项工作：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The contents of the `graph.sbt` file are shown previously, via the Linux `more`
    command. There are only two changes here to note from previous examples—the value
    of name has changed to represent the content. Also, more importantly, the Spark
    GraphX 1.0.0 library has been added as a library dependency.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: “graph.sbt”文件的内容如前所示，通过Linux的“more”命令。这里只有两个变化需要注意——名称的值已更改以表示内容。更重要的是，Spark
    GraphX 1.0.0库已添加为库依赖项。
- en: 'Two data files have been placed on HDFS, under the `/data/spark/graphx/` directory.
    They contain the data that will be used for this section in terms of the vertices,
    and edges that make up a graph. As the Hadoop file system `ls` command shows next,
    the files are called `graph1_edges.cvs` and `graph1_vertex.csv`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 两个数据文件已放置在HDFS的“/data/spark/graphx/”目录下。它们包含将用于本节的顶点和边的数据。如Hadoop文件系统的“ls”命令所示，文件名分别为“graph1_edges.cvs”和“graph1_vertex.csv”：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `vertex` file, shown next, via a Hadoop file system `cat` command, contains
    just six lines, representing the graph used in the last section. Each vertex represents
    a person, and has a vertex ID number, a name and an age value:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示的“顶点”文件，通过Hadoop文件系统的“cat”命令，只包含六行，表示上一节中使用的图。每个顶点代表一个人，具有顶点ID号、姓名和年龄值：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The edge file contains a set of directed edge values in the form of source
    vertex ID, destination vertex ID, and relationship. So, record one forms a Sister
    relationship between `Flo` and `Mike`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 边文件包含一组有向边值，形式为源顶点ID、目标顶点ID和关系。因此，记录一形成了“Flo”和“Mike”之间的姐妹关系：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Having explained the sbt environment, and the HDFS-based data, we are now ready
    to examine some of the GraphX code samples. As in the previous examples, the code
    can be compiled, and packaged as follows from the `graphx` subdirectory. This
    creates a JAR called `graph-x_2.10-1.0.jar` from which the example applications
    can be run:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释了sbt环境和基于HDFS的数据之后，我们现在准备检查一些GraphX代码示例。与之前的示例一样，代码可以从“graphx”子目录编译和打包。这将创建一个名为“graph-x_2.10-1.0.jar”的JAR文件，从中可以运行示例应用程序：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Creating a graph
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建图
- en: 'This section will explain the generic Scala code, up to the point of creating
    a GraphX graph, from the HDFS-based data. This will save time, as the same code
    is reused in each example. Once this is explained, I will concentrate on the actual
    graph-based manipulation in each code example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将解释通用的Scala代码，直到从基于HDFS的数据创建GraphX图为止。这将节省时间，因为相同的代码在每个示例中都被重用。一旦这一点得到解释，我将集中在每个代码示例中的实际基于图的操作上。
- en: 'The generic code starts by importing the Spark context, graphx, and RDD functionality
    for use in the Scala code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通用代码从导入Spark上下文、graphx和RDD功能开始，以便在Scala代码中使用：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, an application is defined, which extends the `App` class, and the application
    name changes, for each example, from `graph1` to `graph5`. This application name
    will be used when running the application using `spark-submit`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，定义一个应用程序，它扩展了“App”类，并且每个示例的应用程序名称从“graph1”更改为“graph5”。在使用“spark-submit”运行应用程序时将使用此应用程序名称：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The data files are defined in terms of the HDFS server and port, the path that
    they reside under in HDFS and their file names. As already mentioned, there are
    two data files that contain the `vertex` and `edge` information:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 数据文件是根据HDFS服务器和端口、它们在HDFS下的路径和它们的文件名来定义的。如前所述，有两个包含“顶点”和“边”信息的数据文件：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The Spark Master URL is defined, as is the application name, which will appear
    in the Spark user interface when the application runs. A new Spark configuration
    object is created, and the URL and name are assigned to it:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了Spark Master URL，以及应用程序名称，当应用程序运行时将出现在Spark用户界面中。创建了一个新的Spark配置对象，并将URL和名称分配给它：
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'A new Spark context is created using the configuration that was just defined:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用刚刚定义的配置创建了一个新的Spark上下文：
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The vertex information from the HDFS-based file is then loaded into an RDD-based
    structure called `vertices` using the `sparkCxt.textFile` method. The data is
    stored as a long `VertexId`, and strings to represent the person''s name and age.
    The data lines are split by commas as this is CSV based-data:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基于HDFS文件的顶点信息然后使用“sparkCxt.textFile”方法加载到名为“vertices”的基于RDD的结构中。数据存储为长整型“VertexId”和字符串表示人的姓名和年龄。数据行按逗号拆分，因为这是基于CSV的数据：
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Similary, the HDFS-based edge data is loaded into an RDD-based data structure
    called `edges`. The CSV-based data is again split by comma values. The first two
    data values are converted into Long values, as they represent the source and destination
    vertex ID''s. The final value, representing the relationship of the edge, is left
    as a string. Note that each record in the RDD structure edges is actually now
    an `Edge` record:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，基于HDFS的边缘数据加载到名为“edges”的基于RDD的数据结构中。基于CSV的数据再次按逗号值拆分。前两个数据值转换为长整型值，因为它们代表源和目标顶点ID。表示边关系的最终值保留为字符串。请注意，RDD结构edges中的每个记录现在实际上是一个“Edge”记录：
- en: '[PRE13]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'A default value is defined in case a connection, or a vertex is missing, then
    the graph is constructed from the RDD-based structures—`vertices`, `edges`, and
    the `default` record:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接或顶点缺失的情况下定义了一个默认值，然后从基于RDD的结构“vertices”、“edges”和“default”记录构建图：
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This creates a GraphX-based structure called `graph`, which can now be used
    for each of the examples. Remember that although these data samples are small,
    you can create extremely large graphs using this approach. Many of these algorithms
    are iterative applications, for instance, PageRank and Triangle Count, and as
    a result, the programs will generate many iterative Spark jobs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个名为“graph”的基于GraphX的结构，现在可以用于每个示例。请记住，尽管这些数据样本很小，但您可以使用这种方法创建非常大的图形。许多这些算法都是迭代应用，例如PageRank和Triangle
    Count，因此程序将生成许多迭代的Spark作业。
- en: Example 1 – counting
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例1 - 计数
- en: 'The graph has been loaded, and we know the data volumes in the data files,
    but what about the data content in terms of vertices, and edges in the actual
    graph itself? It is very simple to extract this information by using the vertices,
    and the edges count function as shown here:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图已加载，我们知道数据文件中的数据量，但是实际图中的顶点和边的数据内容如何？通过使用顶点和边计数函数，可以很容易地提取这些信息，如下所示：
- en: '[PRE15]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Running the `graph1` example, using the example name and the JAR file created
    previously, will provide the count information. The master URL is supplied to
    connect to the Spark cluster, and some default parameters are supplied for the
    executor memory, and the total executor cores:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用先前创建的示例名称和JAR文件运行“graph1”示例将提供计数信息。提供主URL以连接到Spark集群，并为执行器内存和总执行器核心提供一些默认参数：
- en: '[PRE16]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The Spark cluster job called `graph1` provides the following output, which
    is as expected and also, it matches the data files:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 名为“graph1”的Spark集群作业提供了以下输出，这是预期的，也与数据文件匹配：
- en: '[PRE17]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Example 2 – filtering
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例2 - 过滤
- en: 'What happens if we need to create a subgraph from the main graph, and filter
    by the person''s age or relationships? The example code from the second example
    Scala file, `graph2`, shows how this can be done:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要从主图创建一个子图，并按照人的年龄或关系进行筛选，会发生什么？第二个示例Scala文件“graph2”中的示例代码显示了如何做到这一点：
- en: '[PRE18]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The two example counts have been created from the main graph. The first filters
    the person-based vertices on the age, only taking those people who are greater
    than 40 years old. Notice that the `age` value, which was stored as a string,
    has been converted into a long for comparison. The previous second example filters
    the edges on the relationship property of `Mother` or `Father`. The two count
    values: `c1` and `c2` are created, and printed as the Spark output shows here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 两个示例计数是从主图创建的。第一个筛选基于年龄的顶点，只取那些年龄大于40岁的人。请注意，存储为字符串的“年龄”值已转换为长整型进行比较。前面的第二个示例筛选了“母亲”或“父亲”的关系属性的边。创建了两个计数值：“c1”和“c2”，并按照Spark输出显示在这里打印出来：
- en: '[PRE19]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Example 3 – PageRank
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例3 - PageRank
- en: 'The PageRank algorithm provides a ranking value for each of the vertices in
    a graph. It makes the assumption that the vertices that are connected to the most
    edges are the most important ones. Search engines use PageRank to provide ordering
    for the page display during a web search:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank算法为图中的每个顶点提供了一个排名值。它假设连接到最多边的顶点是最重要的。搜索引擎使用PageRank为网页搜索期间的页面显示提供排序：
- en: '[PRE20]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The previous example code creates a `tolerance` value, and calls the graph `pageRank`
    method using it. The vertices are then ranked into a new value ranking. In order
    to make the ranking more meaningful the ranking values are joined with the original
    vertices RDD. The `rankByPerson` value then contains the rank, vertex ID, and
    person's name.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例代码创建了一个“tolerance”值，并使用它调用了图的“pageRank”方法。然后将顶点排名为一个新值排名。为了使排名更有意义，排名值与原始顶点RDD连接。然后，“rankByPerson”值包含排名、顶点ID和人的姓名。
- en: 'The PageRank result, held in `rankByPerson`, is then printed record by record,
    using a case statement to identify the record contents, and a format statement
    to print the contents. I did this, because I wanted to define the format of the
    rank value which can vary:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank结果存储在“rankByPerson”中，然后使用case语句逐条打印记录内容，并使用格式语句打印内容。我这样做是因为我想定义排名值的格式可能会有所不同：
- en: '[PRE21]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output from the application is then shown here. As expected, `Mike` and
    `Sarah` have the highest rank, as they have the most relationships:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的输出如下所示。预期的是，“Mike”和“Sarah”具有最高的排名，因为他们有最多的关系：
- en: '[PRE22]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Example 4 – triangle counting
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例4 - 三角形计数
- en: The triangle count algorithm provides a vertex-based count of the number of
    triangles, associated with this vertex. For instance, vertex `Mike` (1) is connected
    to `Kate` (5), who is connected to `Sarah` (2); `Sarah` is connected to `Mike`
    (1) and so, a triangle is formed. This can be useful for route finding, where
    minimum, triangle-free, spanning tree graphs need to be generated for route planning.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 三角形计数算法提供了与该顶点相关的三角形数量的基于顶点的计数。例如，顶点`Mike`（1）连接到`Kate`（5），后者连接到`Sarah`（2）；`Sarah`连接到`Mike`（1），因此形成了一个三角形。这对于路由查找可能很有用，需要为路由规划生成最小的无三角形的生成树图。
- en: 'The code to execute a triangle count, and print it, is simple, as shown next.
    The graph `triangleCount` method is executed for the graph vertices. The result
    is saved in the value `tCount`, and then printed:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 执行三角形计数并打印的代码很简单，如下所示。对图顶点执行`triangleCount`方法。结果保存在值`tCount`中，然后打印出来：
- en: '[PRE23]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The results of the application job show that the vertices called, `Flo` (4)
    and `Jim` (6), have no triangles, whereas `Mike` (1) and `Sarah` (2) have the
    most, as expected, as they have the most relationships:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 应用作业的结果显示，称为`Flo`（4）和`Jim`（6）的顶点没有三角形，而`Mike`（1）和`Sarah`（2）有最多的三角形，正如预期的那样，因为它们有最多的关系：
- en: '[PRE24]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Example 5 – connected components
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例5 - 连通组件
- en: When a large graph is created from the data, it might contain unconnected subgraphs,
    that is, subgraphs that are isolated from each other, and contain no bridging
    or connecting edges between them. This algorithm provides a measure of this connectivity.
    It might be important, depending upon your processing, to know that all the vertices
    are connected.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当从数据创建一个大图时，它可能包含未连接的子图，也就是说，彼此隔离并且之间没有桥接或连接边的子图。该算法提供了这种连接性的度量。根据您的处理方式，知道所有顶点是否连接可能很重要。
- en: 'The Scala code, for this example, calls two graph methods: `connectedComponents`,
    and `stronglyConnectedComponents`. The strong method required a maximum iteration
    count, which has been set to `1000`. These counts are acting on the graph vertices:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例的Scala代码调用了两个图方法：`connectedComponents`和`stronglyConnectedComponents`。强方法需要一个最大迭代计数，已设置为`1000`。这些计数作用于图的顶点：
- en: '[PRE25]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The vertex counts are then joined with the original vertex records, so that
    the connection counts can be associated with the vertex information, such as the
    person''s name:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将顶点计数与原始顶点记录连接起来，以便将连接计数与顶点信息（例如人的姓名）关联起来。
- en: '[PRE26]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As expected for the `connectedComponents` algorithm, the results show that
    for each vertex, there is only one component. This means that all the vertices
    are the members of a single graph, as the graph diagram earlier in the chapter
    showed:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如`connectedComponents`算法所预期的那样，结果显示每个顶点只有一个组件。这意味着所有顶点都是单个图的成员，就像本章前面显示的图表一样：
- en: '[PRE27]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `stronglyConnectedComponents` method gives a measure of the connectivity
    in a graph, taking into account the direction of the relationships between them.
    The results for the `stronglyConnectedComponents` algorithm output is as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`stronglyConnectedComponents`方法提供了图中连接性的度量，考虑了它们之间关系的方向。`stronglyConnectedComponents`算法的结果如下：'
- en: '[PRE28]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You might notice from the graph that the relationships, `Sister` and `Friend`,
    act from vertices `Flo` (6) and `Jim` (4), to `Mike` (1) as the edge and vertex
    data shows here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到从图中，关系`Sister`和`Friend`是从顶点`Flo`（6）和`Jim`（4）到`Mike`（1）的边和顶点数据如下所示：
- en: '[PRE29]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'So, the strong method output shows that for most vertices, there is only one
    graph component signified by the `1` in the second column. However, vertices `4`
    and `6` are not reachable due to the direction of their relationship, and so they
    have a vertex ID instead of a component ID:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，强方法的输出显示，对于大多数顶点，第二列中的`1`表示只有一个图组件。然而，由于它们关系的方向，顶点`4`和`6`是不可达的，因此它们有一个顶点ID而不是组件ID：
- en: '[PRE30]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Mazerunner for Neo4j
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Neo4j的Mazerunner
- en: In the previous sections, you have been shown how to write Apache Spark graphx
    code in Scala to process the HDFS-based graph data. You have been able to execute
    the graph-based algorithms, such as PageRank, and triangle counting. However,
    this approach has a limitation. Spark does not have storage, and storing graph-based
    data in the flat files on HDFS does not allow you to manipulate it in its place
    of storage. For instance, if you had data stored in a relational database, you
    could use SQL to interrogate it in place. Databases such as Neo4j are graph databases.
    This means that their storage mechanisms and data access language act on graphs.
    In this section, I want to take a look at the work done on Mazerunner, created
    as a GraphX Neo4j processing prototype by Kenny Bastani.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，您已经学会了如何在Scala中编写Apache Spark graphx代码来处理基于HDFS的图形数据。您已经能够执行基于图形的算法，例如PageRank和三角形计数。然而，这种方法有一个限制。Spark没有存储功能，并且将基于图形的数据存储在HDFS上的平面文件中不允许您在其存储位置对其进行操作。例如，如果您的数据存储在关系数据库中，您可以使用SQL在原地对其进行查询。Neo4j等数据库是图数据库。这意味着它们的存储机制和数据访问语言作用于图形。在本节中，我想看一下Kenny
    Bastani创建的Mazerunner，它是一个GraphX Neo4j处理原型。
- en: 'The following figure describes the Mazerunner architecture. It shows that data
    in Neo4j is exported to HDFS, and processed by GraphX via a notification process.
    The GraphX data updates are then saved back to HDFS as a list of key value updates.
    These changes are then propagated to Neo4j to be stored. The algorithms in this
    prototype architecture are accessed via a Rest based HTTP URL, which will be shown
    later. The point here though, is that algorithms can be run via processing in
    graphx, but the data changes can be checked via Neo4j database cypher language
    queries. Kenny''s work and further details can be found at: [http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html](http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图描述了Mazerunner架构。它显示了Neo4j中的数据被导出到HDFS，并通过通知过程由GraphX处理。然后将GraphX数据更新保存回HDFS作为键值更新列表。然后将这些更改传播到Neo4j进行存储。此原型架构中的算法可以通过基于Rest的HTTP
    URL访问，稍后将显示。这里的重点是，算法可以通过graphx中的处理运行，但数据更改可以通过Neo4j数据库cypher语言查询进行检查。Kenny的工作和更多细节可以在以下网址找到：[http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html](http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html)。
- en: This section will be dedicated to explaining the Mazerunner architecture, and
    will show, with the help of an example, how it can be used. This architecture
    provides a unique example of GraphX-based processing, coupled with graph-based
    storage.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将专门解释Mazerunner架构，并将通过示例展示如何使用。该架构提供了一个基于GraphX处理的独特示例，结合了基于图的存储。
- en: '![Mazerunner for Neo4j](img/B01989_05_03.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![Neo4j的Mazerunner](img/B01989_05_03.jpg)'
- en: Installing Docker
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Docker
- en: The process for installing the Mazerunner example code is described via [https://github.com/kbastani/neo4j-mazerunner](https://github.com/kbastani/neo4j-mazerunner).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Mazerunner示例代码的过程在[https://github.com/kbastani/neo4j-mazerunner](https://github.com/kbastani/neo4j-mazerunner)中有描述。
- en: 'I have used the 64 bit Linux Centos 6.5 machine `hc1r1m1` for the install.
    The Mazerunner example uses the Docker tool, which creates virtual containers
    with a small foot print for running HDFS, Neo4j, and Mazerunner in this example.
    First, I must install Docker. I have done this, as follows, using the Linux root
    user via `yum` commands. The first command installs the `docker-io` module (the
    docker name was already used for CentOS 6.5 by another application):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了64位Linux Centos 6.5机器`hc1r1m1`进行安装。Mazerunner示例使用Docker工具，在此示例中创建了运行HDFS、Neo4j和Mazerunner的虚拟容器，占用空间很小。首先，我必须安装Docker。我已经使用Linux
    root用户通过`yum`命令完成了这一点。第一条命令安装了`docker-io`模块（docker名称已经被另一个应用程序用于CentOS 6.5）：
- en: '[PRE31]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'I needed to enable the `public_ol6_latest` repository, and install the `device-mapper-event-libs`
    package, as I found that my current lib-device-mapper, which I had installed,
    wasn''t exporting the symbol Base that Docker needed. I executed the following
    commands as `root`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要启用`public_ol6_latest`存储库，并安装`device-mapper-event-libs`包，因为我发现我当前安装的lib-device-mapper没有导出Docker需要的Base符号。我以`root`身份执行了以下命令：
- en: '[PRE32]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The actual error that I encountered was as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到的实际错误如下：
- en: '[PRE33]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'I can then check that Docker will run by checking the Docker version number
    with the following call:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我可以通过以下调用检查Docker的版本号，以确保Docker可以运行：
- en: '[PRE34]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'I can start the Linux docker service using the following service command. I
    can also force Docker to start on Linux server startup using the following `chkconfig`
    command:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以使用以下服务命令启动Linux docker服务。我还可以使用以下`chkconfig`命令强制Docker在Linux服务器启动时启动：
- en: '[PRE35]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The three Docker images (HDFS, Mazerunner, and Neo4j) can then be downloaded.
    They are large, so this may take some time:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以下载三个Docker镜像（HDFS，Mazerunner和Neo4j）。它们很大，所以可能需要一些时间：
- en: '[PRE36]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Once downloaded, the Docker containers can be started in the order; HDFS, Mazerunner,
    and then Neo4j. The default Neo4j movie database will be loaded and the Mazerunner
    algorithms run using this data. The HDFS container starts as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后，Docker容器可以按顺序启动；HDFS，Mazerunner，然后Neo4j。将加载默认的Neo4j电影数据库，并使用这些数据运行Mazerunner算法。HDFS容器的启动如下：
- en: '[PRE37]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The Mazerunner service container starts as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Mazerunner服务容器的启动如下：
- en: '[PRE38]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is long, so I will not include it all here, but you will see no
    errors. There also comes a line, which states that the install is waiting for
    messages:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出很长，所以我不会在这里全部包含，但你不会看到任何错误。还有一行，说明安装正在等待消息：
- en: '[PRE39]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In order to start the Neo4j container, I need the install to create a new Neo4j
    database for me, as this is a first time install. Otherwise on restart, I would
    just supply the path of the database directory. Using the `link` command, the
    Neo4j container is linked to the HDFS and Mazerunner containers:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启动Neo4j容器，我需要安装程序为我创建一个新的Neo4j数据库，因为这是第一次安装。否则在重新启动时，我只需提供数据库目录的路径。使用`link`命令，Neo4j容器链接到HDFS和Mazerunner容器：
- en: '[PRE40]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'By checking the `neo4j/data` path, I can now see that a database directory,
    named `graph.db` has been created:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查`neo4j/data`路径，我现在可以看到已经创建了一个名为`graph.db`的数据库目录：
- en: '[PRE41]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'I can then use the following `docker inspect` command, which the container-based
    IP address and the Docker-based Neo4j container is making available. The `inspect`
    command supplies me with the local IP address that I will need to access the Neo4j
    container. The `curl` command, along with the port number, which I know from Kenny''s
    website, will default to `7474`, shows me that the Rest interface is running:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我可以使用以下`docker inspect`命令，该命令提供了基于容器的IP地址和Docker基础的Neo4j容器。`inspect`命令提供了我需要访问Neo4j容器的本地IP地址。`curl`命令连同端口号（我从Kenny的网站上知道）默认为`7474`，显示Rest接口正在运行：
- en: '[PRE42]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The Neo4j browser
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Neo4j浏览器
- en: 'The rest of the work in this section will now be carried out using the Neo4j
    browser URL, which is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的其余工作现在将使用Neo4j浏览器URL进行，如下所示：
- en: '`http://172.17.0.5:7474/browser`.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://172.17.0.5:7474/browser`。'
- en: This is a local, Docker-based IP address that will be accessible from the `hc1r1m1`
    server. It will not be visible on the rest of the local intranet without further
    network configuration.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基于本地Docker的IP地址，将可以从`hc1r1m1`服务器访问。如果没有进一步的网络配置，它将不会在本地局域网的其他地方可见。
- en: This will show the default Neo4j browser page. The Movie graph can be installed
    by following the movie link here, selecting the Cypher query, and executing it.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示默认的Neo4j浏览器页面。可以通过点击这里的电影链接，选择Cypher查询并执行来安装电影图。
- en: '![The Neo4j browser](img/B01989_05_04.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![Neo4j浏览器](img/B01989_05_04.jpg)'
- en: The data can then be interrogated using Cypher queries, which will be examined
    in more depth in the next chapter. The following figures are supplied along with
    their associated Cypher queries, in order to show that the data can be accessed
    as graphs that are displayed visually. The first graph shows a simple Person to
    Movie relationship, with the relationship details displayed on the connecting
    edges.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以使用Cypher查询来查询数据，这将在下一章中更深入地探讨。以下图表与它们相关的Cypher查询一起提供，以显示数据可以作为可视化显示的图形进行访问。第一个图表显示了一个简单的人到电影关系，关系细节显示在连接的边上。
- en: '![The Neo4j browser](img/B01989_05_05.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![Neo4j浏览器](img/B01989_05_05.jpg)'
- en: The second graph, provided as a visual example of the power of Neo4j, shows
    a far more complex cypher query, and resulting graph. This graph states that it
    contains 135 nodes and 180 relationships. These are relatively small numbers in
    processing terms, but it is clear that the graph is becoming complex.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个图表作为Neo4j强大性能的视觉示例，展示了一个更复杂的Cypher查询和生成的图表。该图表表示包含135个节点和180个关系。在处理方面，这些数字相对较小，但很明显图表变得复杂。
- en: '![The Neo4j browser](img/B01989_05_06.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![Neo4j浏览器](img/B01989_05_06.jpg)'
- en: 'The following figures show the Mazerunner example algorithms being called via
    an HTTP Rest URL. The call is defined by the algorithm to be called, and the attribute
    that it is going to act upon within the graph:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了通过HTTP Rest URL调用Mazerunner示例算法。调用由要调用的算法和它将在图中操作的属性定义：
- en: '`http://localhost:7474/service/mazerunner/analysis/{algorithm}/{attribute}`.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://localhost:7474/service/mazerunner/analysis/{algorithm}/{attribute}`。'
- en: So for instance, as the next section will show, this generic URL can be used
    to run the PageRank algorithm by setting `algorithm=pagerank`. The algorithm will
    operate on the `follows` relationship by setting `attribute=FOLLOWS`. The next
    section will show how each Mazerunner algorithm can be run along with an example
    of the Cypher output.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如下一节将展示的，可以使用通用URL来运行PageRank算法，设置`algorithm=pagerank`。该算法将通过设置`attribute=FOLLOWS`来操作`follows`关系。下一节将展示如何运行每个Mazerunner算法以及Cypher输出的示例。
- en: The Mazerunner algorithms
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Mazerunner算法
- en: This section shows how the Mazerunner example algorithms may be run using the
    Rest based HTTP URL, which was shown in the last section. Many of these algorithms
    have already been examined, and coded in this chapter. Remember that the interesting
    thing occurring in this section is that data starts in Neo4j, it is processed
    on Spark with GraphX, and then is updated back into Neo4j. It looks simple, but
    there are underlying processes doing all of the work. In each example, the attribute
    that the algorithm has added to the graph is interrogated via a Cypher query.
    So, each example isn't so much about the query, but that the data update to Neo4j
    has occurred.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了如何使用上一节中显示的基于Rest的HTTP URL运行Mazerunner示例算法。这一章中已经检查并编码了许多这些算法。请记住，本节中发生的有趣事情是数据从Neo4j开始，经过Spark和GraphX处理，然后更新回Neo4j。看起来很简单，但是有底层的过程在进行所有的工作。在每个示例中，通过Cypher查询来询问算法已经添加到图中的属性。因此，每个示例不是关于查询，而是数据更新到Neo4j已经发生。
- en: The PageRank algorithm
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PageRank算法
- en: The first call shows the PageRank algorithm, and the PageRank attribute being
    added to the movie graph. As before, the PageRank algorithm gives a rank to each
    vertex, depending on how many edge connections it has. In this case, it is using
    the `FOLLOWS` relationship for processing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个调用显示了PageRank算法和PageRank属性被添加到电影图中。与以前一样，PageRank算法根据顶点的边连接数量给出一个排名。在这种情况下，它使用`FOLLOWS`关系进行处理。
- en: '![The PageRank algorithm](img/B01989_05_07.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![PageRank算法](img/B01989_05_07.jpg)'
- en: The following image shows a screenshot of the PageRank algorithm result. The
    text at the top of the image (starting with `MATCH`) shows the cypher query, which
    proves that the PageRank property has been added to the graph.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了PageRank算法结果的截图。图像顶部的文本（以`MATCH`开头）显示了Cypher查询，证明了PageRank属性已被添加到图中。
- en: '![The PageRank algorithm](img/B01989_05_08.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![PageRank算法](img/B01989_05_08.jpg)'
- en: The closeness centrality algorithm
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 接近度中心性算法
- en: The closeness algorithm attempts to determine the most important vertices in
    the graph. In this case, the `closeness` attribute has been added to the graph.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接近度算法试图确定图中最重要的顶点。在这种情况下，`closeness`属性已被添加到图中。
- en: '![The closeness centrality algorithm](img/B01989_05_09.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![接近度中心性算法](img/B01989_05_09.jpg)'
- en: The following image shows a screenshot of the closeness algorithm result. The
    text at the top of the image (starting with `MATCH`) shows the Cypher query, which
    proves that the `closeness_centrality` property has been added to the graph. Note
    that an alias called `closeness` has been used in this Cypher query, to represent
    the `closeness_centrality` property, and so the output is more presentable.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了接近度算法结果的截图。图像顶部的文本（以`MATCH`开头）显示了Cypher查询，证明了`closeness_centrality`属性已被添加到图中。请注意，此Cypher查询中使用了一个名为`closeness`的别名，表示`closeness_centrality`属性，因此输出更具可读性。
- en: '![The closeness centrality algorithm](img/B01989_05_10.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![接近度中心性算法](img/B01989_05_10.jpg)'
- en: The triangle count algorithm
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 三角形计数算法
- en: The `triangle_count` algorithm has been used to count triangles associated with
    vertices. The `FOLLOWS` relationship has been used, and the `triangle_count` attribute
    has been added to the graph.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`triangle_count`算法已被用于计算与顶点相关的三角形数量。使用了`FOLLOWS`关系，并且`triangle_count`属性已被添加到图中。'
- en: '![The triangle count algorithm](img/B01989_05_11.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![三角形计数算法](img/B01989_05_11.jpg)'
- en: The following image shows a screenshot of the triangle algorithm result. The
    text at the top of the image (starting with `MATCH`) shows the cypher query, which
    proves that the `triangle_count` property has been added to the graph. Note that
    an alias called **tcount** has been used in this cypher query, to represent the
    `triangle_count` property, and so the output is more presentable.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片显示了三角形算法结果的屏幕截图。图像顶部的文本（以`MATCH`开头）显示了Cypher查询，证明了`triangle_count`属性已被添加到图中。请注意，在此Cypher查询中使用了一个名为**tcount**的别名，表示`triangle_count`属性，因此输出更加可呈现。
- en: '![The triangle count algorithm](img/B01989_05_12.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![三角形计数算法](img/B01989_05_12.jpg)'
- en: The connected components algorithm
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接组件算法
- en: The connected components algorithm is a measure of how many actual components
    exist in the graph data. For instance, the data might contain two subgraphs with
    no routes between them. In this case, the `connected_components` attribute has
    been added to the graph.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 连接组件算法是衡量图形数据中存在多少实际组件的一种方法。例如，数据可能包含两个子图，它们之间没有路径。在这种情况下，`connected_components`属性已被添加到图中。
- en: '![The connected components algorithm](img/B01989_05_13.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![连接组件算法](img/B01989_05_13.jpg)'
- en: The following image shows a screenshot of the connected component algorithm
    result. The text at the top of the image (starting with `MATCH`) shows the cypher
    query, which proves that the `connected_components` property has been added to
    the graph. Note that an alias called **ccomp** has been used in this cypher query,
    to represent the `connected_components` property, and so the output is more presentable.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片显示了连接组件算法结果的屏幕截图。图像顶部的文本（以`MATCH`开头）显示了Cypher查询，证明了`connected_components`属性已被添加到图中。请注意，在此Cypher查询中使用了一个名为**ccomp**的别名，表示`connected_components`属性，因此输出更加可呈现。
- en: '![The connected components algorithm](img/B01989_05_14.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![连接组件算法](img/B01989_05_14.jpg)'
- en: The strongly connected components algorithm
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强连接组件算法
- en: 'The strongly connected components algorithm is very similar to the connected
    components algorithm. Subgraphs are created from the graph data using the directional
    `FOLLOWS` relationship. Multiple subgraphs are created until all the graph components
    are used. These subgraphs form the strongly connected components. As seen here,
    a `strongly_connected_components` attribute has been added to the graph:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 强连接组件算法与连接组件算法非常相似。使用方向性的`FOLLOWS`关系从图形数据创建子图。创建多个子图，直到使用所有图形组件。这些子图形成了强连接组件。如此所见，`strongly_connected_components`属性已被添加到图中：
- en: '![The strongly connected components algorithm](img/B01989_05_15.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![强连接组件算法](img/B01989_05_15.jpg)'
- en: The following image shows a screenshot of the strongly connected component algorithm
    result. The text at the top of the image (starting with `MATCH`) shows the cypher
    query, which proves that the `strongly_connected_components` connected component
    property has been added to the graph. Note that an alias called **sccomp** has
    been used in this cypher query, to represent the `strongly_connected_components`
    property, and so the output is more presentable.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片显示了强连接组件算法结果的屏幕截图。图像顶部的文本（以`MATCH`开头）显示了Cypher查询，证明了`strongly_connected_components`连接组件属性已被添加到图中。请注意，在此Cypher查询中使用了一个名为**sccomp**的别名，表示`strongly_connected_components`属性，因此输出更加可呈现。
- en: '![The strongly connected components algorithm](img/B01989_05_16.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![强连接组件算法](img/B01989_05_16.jpg)'
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter has shown, with the help of examples, how the Scala-based code
    can be used to call GraphX algorithms in Apache Spark. Scala has been used, because
    it requires less code to develop the examples, which saves time. A Scala-based
    shell can be used, and the code can be compiled into Spark applications. Examples
    of the application compilation and configuration have been supplied using the
    SBT tool. The configuration and the code examples from this chapter will also
    be available for download with the book.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 本章已经通过示例展示了如何使用基于Scala的代码调用Apache Spark中的GraphX算法。之所以使用Scala，是因为开发示例所需的代码更少，节省时间。可以使用基于Scala的shell，并且可以将代码编译成Spark应用程序。本章提供了使用SBT工具进行应用程序编译和配置的示例。本章的配置和代码示例也将随书提供下载。
- en: Finally, the Mazerunner example architecture (developed by Kenny Bastani while
    at Neo) for Neo4j and Apache Spark has been introduced. Why is Mazerunner important?
    It provides an example of how a graph-based database can be used for graph storage,
    while Apache Spark is used for graph processing. I am not suggesting that Mazerunner
    be used in a production scenario at this time. Clearly, more work needs to be
    done to make this architecture ready for release. However, graph-based storage,
    when associated with the graph-based processing within a distributed environment,
    offers the option to interrogate the data using a query language such as Cypher
    from Neo4j.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，介绍了Mazerunner示例架构（由Kenny Bastani在Neo期间开发）用于Neo4j和Apache Spark。为什么Mazerunner很重要？它提供了一个示例，说明了图形数据库可以用于图形存储，而Apache
    Spark用于图形处理。我并不是建议目前在生产场景中使用Mazerunner。显然，还需要做更多工作，使这种架构准备好发布。然而，与分布式环境中的基于图形处理相关联的基于图形存储，提供了使用Neo4j的Cypher等查询语言来查询数据的选项。
- en: I hope that you have found this chapter useful. The next chapter will delve
    into graph-based storage in more depth. You can now delve into further GraphX
    coding, try to run the examples provided, and try modifying the code, so that
    you become familiar with the development process.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你觉得这一章很有用。下一章将更深入地探讨基于图的存储。你现在可以深入了解更多GraphX编码，尝试运行提供的示例，并尝试修改代码，以便熟悉开发过程。
