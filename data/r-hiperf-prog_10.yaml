- en: Chapter 10. R and Big Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 R与大数据
- en: We have come to the final chapter of this book where we will go to the very
    limits of large-scale data processing. The term *Big Data* has been used to describe
    the ever growing volume, velocity, and variety of data being generated on the
    Internet in connected devices and many other places. Many organizations now have
    massive datasets that measure in petabytes (one petabyte is 1,048,576 gigabytes),
    more than ever before. Processing and analyzing Big Data is extremely challenging
    for traditional data processing tools and database architectures.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经来到了这本书的最后一章，我们将深入探讨大规模数据处理。术语*大数据*被用来描述在互联网上不断增长的、速度和种类都在增加的数据量，这些数据在连接的设备和许多其他地方被生成。现在，许多组织拥有测量在皮字节（一个皮字节是1,048,576千兆字节）以上的海量数据集，比以往任何时候都要多。对于传统的数据处理工具和数据库架构来说，处理和分析大数据极具挑战性。
- en: In 2005, Doug Cutting and Mike Cafarella at Yahoo! developed Hadoop, based on
    earlier work by Google, to address these challenges. They set out to develop a
    new data platform to process, index, and query billions of web pages efficiently.
    With Hadoop, the work which would have previously required very expensive supercomputers
    can now be done on large clusters of inexpensive standard servers. As the volume
    of data grows, more servers can simply be added to a Hadoop cluster to increase
    the storage capacity and computing power. Since then, Hadoop and its ecosystem
    of tools has become one of the most popular suites of tools to collect, store,
    process and analyze large datasets. In this chapter, we will learn how to tap
    into the power of Hadoop from R.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 2005年，Yahoo!的Doug Cutting和Mike Cafarella开发了Hadoop，基于Google之前的工作，以解决这些挑战。他们着手开发一个新的数据平台，以高效地处理、索引和查询数十亿网页。有了Hadoop，以前需要非常昂贵的超级计算机才能完成的工作现在可以在大量廉价的标准化服务器集群上完成。随着数据量的增长，只需简单地向Hadoop集群添加更多服务器即可增加存储容量和计算能力。从那时起，Hadoop及其工具生态系统已经成为最受欢迎的工具套件之一，用于收集、存储、处理和分析大数据集。在本章中，我们将学习如何从R中利用Hadoop的力量。
- en: 'This chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Understanding Hadoop
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Hadoop
- en: Setting up Hadoop on Amazon Web Services
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Amazon Web Services上设置Hadoop
- en: Processing large datasets in batches using RHadoop
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RHadoop批量处理大数据集
- en: Understanding Hadoop
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Hadoop
- en: 'Before we learn how to use Hadoop (for more information refer to [http://hadoop.apache.org/](http://hadoop.apache.org/))
    and related tools in R, we need to understand the basics of Hadoop. For our purposes,
    it suffices to know that Hadoop comprises two key components: the **Hadoop Distributed
    File System (HDFS)** and the **MapReduce** framework to execute data processing
    tasks. Hadoop includes many other components for task scheduling, job management,
    and others, but we shall not concern ourselves with those in this book.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们学习如何使用Hadoop（更多信息请参阅[http://hadoop.apache.org/](http://hadoop.apache.org/))和R中的相关工具之前，我们需要了解Hadoop的基本知识。就我们的目的而言，了解Hadoop包含两个关键组件就足够了：**Hadoop分布式文件系统（HDFS）**和用于执行数据处理任务的**MapReduce**框架。Hadoop还包括许多其他组件，用于任务调度、作业管理等，但在这本书中我们不会关注这些。
- en: HDFS, as the name suggests, is a virtual filesystem that is distributed across
    a cluster of servers. HDFS stores files in blocks, with a default block size of
    128 MB. For example, a 1 GB file is split into eight blocks of 128 MB, which are
    distributed to different servers in the cluster. Furthermore, to prevent data
    loss due to server failure, the blocks are replicated. By default, they are replicated
    three times—there are three copies of each block of data in the cluster, and each
    copy is stored on a different server. That way, even if a few servers in the cluster
    fail, the data is not lost and can be re-replicated to ensure high availability.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，HDFS是一个分布在整个服务器集群上的虚拟文件系统。HDFS以块的形式存储文件，默认块大小为128 MB。例如，一个1 GB的文件会被分割成八个128
    MB的块，这些块被分布到集群中的不同服务器上。此外，为了防止由于服务器故障导致的数据丢失，这些块会被复制。默认情况下，它们会被复制三次——集群中每个数据块的副本有三个，每个副本存储在不同的服务器上。这样，即使集群中的一些服务器失败，数据也不会丢失，并且可以被重新复制以确保高可用性。
- en: MapReduce is the framework to process the data stored in HDFS in a data parallel
    way. Notice how the distributed nature of data storage makes Hadoop a good fit
    for data parallel algorithms that we learned about in [Chapter 8](ch08.html "Chapter 8. Multiplying
    Performance with Parallel Computing"), *Multiplying Performance with Parallel
    Computing*—the chunks of data stored on each worker node are processed simultaneously
    in parallel, and then the results from each node are combined to produce the final
    results. MapReduce works very similarly to the data parallel algorithms in [Chapter
    8](ch08.html "Chapter 8. Multiplying Performance with Parallel Computing"), *Multiplying
    Performance with Parallel Computing*, except that the data already resides in
    the worker nodes; it does not have to be distributed every time a task is run
    as was the case with a cluster of servers that run R. **Map** refers to the step
    of performing computations on the data in each worker node, or mapping data to
    their corresponding output. **Reduce** refers to the process of combining, or
    reducing the results of the worker nodes into the final results.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 是一个框架，用于以数据并行的方式处理存储在 HDFS 中的数据。注意数据存储的分布式特性如何使 Hadoop 成为适合我们已在 [第
    8 章](ch08.html "第 8 章。通过并行计算提高性能") *通过并行计算提高性能* 中了解到的数据并行算法的理想选择——每个工作节点上存储的数据块会同时并行处理，然后每个节点的结果会被合并以产生最终结果。MapReduce
    的工作方式与第 8 章 *通过并行计算提高性能* 中的数据并行算法非常相似，区别在于数据已经驻留在工作节点上；在运行 R 服务器集群时，每次运行任务都不需要将数据分布，正如
    HDFS 和 MapReduce 所做的那样。**Map** 指的是在每个工作节点上对数据进行计算或映射数据到相应的输出的步骤。**Reduce** 指的是将工作节点的结果合并或减少到最终结果的过程。
- en: Data in MapReduce is represented as key-value pairs. Every MapReduce operation
    is essentially a transformation from one set of key-value pairs to another set
    of key-value pairs. A **mapper** might, for example, read a single customer record
    from a database and produce a key-value pair such as `("Alice` `", 32)` that contains
    the name of a customer (`"Alice"`) as the key and the reward points she or he
    collected in a given week (`32`) as the corresponding value. After the map step,
    all the key-value pairs are sorted by the key, and the pairs with the same key
    are given to individual **reducers**. So, for example, there would be one reducer
    for all pairs with the key `"Alice"`, another reducer for the key `"Bob"`, and
    another for `"Charlie"`. A reducer takes all the key-value pairs it is given,
    performs computations on them, and returns the results as another key-value pair.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 中的数据表示为键值对。每个 MapReduce 操作本质上是从一组键值对到另一组键值对的转换。例如，一个 **mapper** 可能会从数据库中读取单个客户记录，并生成一个键值对，如
    `("Alice", 32)`，其中包含客户的名称（"Alice"）作为键，以及她在给定周收集的奖励点（32）作为相应的值。在 map 步骤之后，所有键值对都会根据键进行排序，具有相同键的键值对会被分配给单个
    **reducer**。例如，对于键 `"Alice"` 的所有键值对会有一个 reducer，对于键 `"Bob"` 另一个 reducer，对于 `"Charlie"`
    另一个 reducer。reducer 会接受分配给它的所有键值对，对它们进行计算，并以另一个键值对的形式返回结果。
- en: The reducers in our simple example could compute the mean of weekly reward points
    collected by all customers. The MapReduce system then collects the results of
    all the reducers as the final output, which could be something like `[("Alice",
    26.5), ("Bob", 42.3), ("Charlie", 35.6), ...]`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的简单示例中，reducers 可以计算所有客户收集的每周奖励点的平均值。然后 MapReduce 系统收集所有 reducers 的结果作为最终输出，可能类似于
    `[("Alice", 26.5), ("Bob", 42.3), ("Charlie", 35.6), ...]`。
- en: While HDFS and MapReduce are the foundation of Hadoop, they are not suited for
    all data processing tasks. One key reason is that data stored in HDFS resides
    on the hard drives of the servers. Each time a MapReduce task is performed, the
    data has to be read from the disk, and the results of the computations are written
    back to the disk. Thus, HDFS and MapReduce perform reasonably well for sizeable
    batch processing tasks where the time to complete the computational tasks exceeds
    the overheads of reading/writing data and other overheads of running a Hadoop
    cluster.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 HDFS 和 MapReduce 是 Hadoop 的基础，但它们并不适合所有数据处理任务。一个关键原因是存储在 HDFS 中的数据位于服务器的硬盘上。每次执行
    MapReduce 任务时，数据都必须从磁盘读取，计算结果必须写回磁盘。因此，HDFS 和 MapReduce 对于需要超过读取/写入数据和其他运行 Hadoop
    集群开销的计算任务时间较大的批量处理任务表现合理。
- en: Setting up Hadoop on Amazon Web Services
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Amazon Web Services 上设置 Hadoop
- en: There are many ways to set up a Hadoop cluster. We can install Hadoop on a single
    server in pseudo-distributed mode to simulate a cluster, or on an actual cluster
    of servers, or virtual machines in fully distributed mode. There are also several
    distributions of Hadoop available from the vanilla open source version provided
    by the Apache Foundation to commercial distributions such as Cloudera, Hortonworks,
    and MapR. Covering all the different ways of setting up Hadoop is beyond the scope
    of this book. We instead provide instructions for one way to set up Hadoop and
    other relevant tools for the purpose of the examples in this chapter. If you are
    using an existing Hadoop cluster or setting up one in a different way, you might
    have to modify some of the steps.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 Hadoop 集群的方法有很多。我们可以在伪分布式模式下将 Hadoop 安装在单个服务器上以模拟集群，或者在真实的服务器集群或完全分布式模式下的虚拟机上安装。还有几个
    Hadoop 发行版可供选择，从 Apache 基金会提供的纯开源版本到商业发行版，如 Cloudera、Hortonworks 和 MapR。涵盖所有不同的
    Hadoop 设置方法超出了本书的范围。我们提供了一种设置 Hadoop 以及本章示例中所需的其他相关工具的说明。如果您正在使用现有的 Hadoop 集群或以不同的方式设置，可能需要修改一些步骤。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Because Hadoop and its associated tools are mostly developed for Linux/Unix
    based operating systems, the code in this chapter will probably not work on Windows.
    If you are a Windows user, follow the instructions in this chapter to set up Hadoop,
    R, and the required packages on Amazon Web Services.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Hadoop 及其相关工具主要是为基于 Linux/Unix 的操作系统开发的，因此本章中的代码可能在 Windows 上无法运行。如果您是 Windows
    用户，请按照本章中的说明在亚马逊网络服务（Amazon Web Services）上设置 Hadoop、R 和所需的软件包。
- en: '**Amazon Web Services** (**AWS**) has a service called **Elastic MapReduce**
    (**EMR**) that allows us to rent and run a Hadoop cluster on an hourly basis.
    Creating a Hadoop cluster is as simple as specifying the number of servers in
    the cluster, the size of each server, and the instructions to set up the required
    tools on each server. To set up an account with AWS, follow the instructions in
    *Preface*. Running the examples in this chapter on AWS will cost some money for
    as long as the EMR cluster is running. Check this link out [http://aws.amazon.com/elasticmapreduce/pricing/](http://aws.amazon.com/elasticmapreduce/pricing/)
    for the latest EMR prices.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**亚马逊网络服务**（**AWS**）提供一项名为 **弹性 MapReduce**（**EMR**）的服务，允许我们按小时租用和运行 Hadoop
    集群。创建 Hadoop 集群就像指定集群中的服务器数量、每台服务器的规模以及在每个服务器上设置所需工具的说明一样简单。要设置 AWS 账户，请遵循 *前言*
    中的说明。在本章中运行 AWS 上的示例将产生一些费用，只要 EMR 集群运行，就会产生费用。查看此链接了解最新的 EMR 价格：[http://aws.amazon.com/elasticmapreduce/pricing/](http://aws.amazon.com/elasticmapreduce/pricing/)。'
- en: We also need a script that sets up the required tools on each server. Save the
    following script as `emr-bootstrap.sh`. This scripts installs the R packages needed
    for this chapter, including `rhdfs`, `rmr2`, and `R.utils` on every server in
    the Hadoop cluster.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个脚本，用于在每个服务器上设置所需的工具。将以下脚本保存为 `emr-bootstrap.sh`。此脚本会在 Hadoop 集群中的每个服务器上安装本章所需的
    R 软件包，包括 `rhdfs`、`rmr2` 和 `R.utils`。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Upload `emr-bootstrap.sh` into the AWS Simple Storage Service (S3) so that
    the EMR servers can pick it up during the first run. To do this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `emr-bootstrap.sh` 上传到 AWS 简单存储服务（S3），以便 EMR 服务器在首次运行时可以获取它。为此：
- en: Go to the AWS console, and click on **S3**.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 AWS 控制台，点击 **S3**。
- en: Create a new bucket to store the script in by clicking on **Create Bucket**.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击 **创建存储桶** 创建一个新的存储桶来存储脚本。
- en: Click on the bucket that was just created and click on **Upload** to upload
    the script.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击刚刚创建的存储桶，然后点击 **上传** 以上传脚本。
- en: 'Next, follow these steps to create the Hadoop cluster:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，按照以下步骤创建 Hadoop 集群：
- en: Go to the AWS Console and click on **EMR**.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 AWS 控制台，点击 **EMR**。
- en: Click on **Create cluster**.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **创建集群**。
- en: Under **Software Configuration**, select the Amazon Hadoop distribution (the
    examples in this chapter were tested with Amazon Machine Image (AMI) version 3.2.1).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **软件配置** 下，选择亚马逊 Hadoop 发行版（本章中的示例使用的是亚马逊机器镜像（AMI）版本 3.2.1）。
- en: Remove Hive and Pig from the applications list, as they are not needed.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从应用程序列表中删除 Hive 和 Pig，因为它们不是必需的。
- en: Under **Hardware Configuration**, select the instance types for the Hadoop servers.
    The instance types for both the master and core nodes should have at least 15
    GB of RAM, such as the `m1.xlarge` or `m3.xlarge` instance types. Enter the number
    of nodes you would like in the cluster. Given the default HDFS replication factor
    of three, there should be at least three core nodes. Task nodes are optional.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**硬件配置**下，选择 Hadoop 服务器的实例类型。主节点和核心节点的实例类型至少应有 15 GB 的 RAM，例如 `m1.xlarge` 或
    `m3.xlarge` 实例类型。输入您希望在集群中使用的节点数量。考虑到默认的 HDFS 复制因子为三，至少应有三个核心节点。任务节点是可选的。
- en: Under **Security and Access**, select the EC2 key pair to log in to the cluster
    with.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**安全和访问**下，选择用于登录集群的 EC2 密钥对。
- en: Under **Bootstrap Actions**, select **Custom action,** then click on **Configure
    and add**. In the dialog box that appears under **S3 location**, enter or browse
    for the S3 location where `emr-bootstrap.sh` was uploaded.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**引导操作**下，选择**自定义操作**，然后点击**配置和添加**。在**S3 位置**下出现的对话框中，输入或浏览上传 `emr-bootstrap.sh`
    的 S3 位置。
- en: (Optional) Enable logging under **Cluster Configuration** to have all Hadoop
    logs automatically stored in the S3 bucket. To use this option, first create an
    S3 bucket to store the logs in, and enter the name of the bucket in the **Log
    folder S3 location** field. While optional, storing Hadoop logs is useful for
    tracing errors and debugging, which can be challenging without the logs, as an
    executed program gets spawned across multiple processes and computer nodes in
    Hadoop.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (可选) 在**集群配置**下启用日志记录，以便将所有 Hadoop 日志自动存储在 S3 存储桶中。要使用此选项，首先创建一个用于存储日志的 S3 存储桶，并在**日志文件夹
    S3 位置**字段中输入存储桶的名称。虽然这是可选的，但存储 Hadoop 日志对于追踪错误和调试很有用，没有日志的话，这些操作可能会很具挑战性，因为执行程序会在
    Hadoop 的多个进程和计算机节点上启动。
- en: Click on **Create cluster** and wait a few minutes while the cluster is being
    set up.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建集群**，等待几分钟，直到集群设置完成。
- en: 'Once the EMR cluster is ready, get the Master Public DNS from the cluster details
    page, and log in to the master server from the command line using your AWS EC2
    security key (replace `hadoop.pem` with the name of your key):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当 EMR 集群准备就绪后，从集群详细信息页面获取主公共 DNS，然后使用您的 AWS EC2 安全密钥从命令行登录到主服务器（将 `hadoop.pem`
    替换为您的密钥名称）：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once you are logged in, run R, which comes preinstalled with the EMR cluster:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 登录后，运行 R，它是 EMR 集群预安装的：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Processing large datasets in batches using Hadoop
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Hadoop 批处理处理大型数据集
- en: Batch processing is the most basic type of task that HDFS and MapReduce can
    perform. Similar to the data parallel algorithms in [Chapter 8](ch08.html "Chapter 8. Multiplying
    Performance with Parallel Computing"), *Multiplying Performance with Parallel
    Computing*, the master node sends a set of instructions to the worker nodes, which
    execute the instructions on the blocks of data stored on them. The results are
    then written to the disk in HDFS.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理是 HDFS 和 MapReduce 可以执行的最基本类型的任务。类似于第 8 章[第 8 章.通过并行计算提高性能](ch08.html "Chapter
    8. Multiplying Performance with Parallel Computing")中的数据并行算法，主节点向工作节点发送一组指令，这些节点在其存储的数据块上执行指令。然后，结果被写入
    HDFS 的磁盘。
- en: When an aggregate result is required, both the map and reduce steps are performed
    on the data. For example, in order to compute the mean of a distributed dataset,
    the mappers on the worker nodes first compute the sum and number of elements in
    each local chunk of data. The reducers then add up all these results to compute
    the global mean.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要聚合结果时，对数据进行映射和减少步骤。例如，为了计算分布式数据集的平均值，工作节点上的映射器首先计算每个本地数据块中的总和和元素数量。然后，减少器将这些结果相加以计算全局平均值。
- en: At other times, only the map step is performed when aggregation is not required.
    This is common in data transformation or cleaning operations where the data is
    simply being transformed form one format to another. One example of this is extracting
    email addresses from a set of documents. In this case, the results of the mappers
    on the worker nodes are stored as new datasets in HDFS, and reducers are not needed.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他时候，如果不需要聚合，则只执行映射步骤。这在数据转换或清理操作中很常见，其中数据只是从一种格式转换到另一种格式。一个例子是从一组文档中提取电子邮件地址。在这种情况下，工作节点上映射器的结果作为新的数据集存储在
    HDFS 中，不需要减少器。
- en: The R community has developed several packages to perform MapReduce tasks from
    R. One of these is the RHadoop family of packages developed by Revolution Analytics
    (for more information refer to [https://github.com/RevolutionAnalytics/RHadoop](https://github.com/RevolutionAnalytics/RHadoop)).
    RHadoop includes the packages `rhdfs`, which provides functions to manipulate
    files and directories in HDFS, and `rmr2`, which exposes the functionality of
    MapReduce as R functions. These functions make it easy to use MapReduce without
    having to program with the Hadoop Java APIs. Instead, `rmr2` runs a copy of R
    on every worker node, and the mappers and reducers are written as R functions
    to be applied on each chunk of data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: R社区已经开发了几个包来从R执行MapReduce任务。其中之一是由Revolution Analytics开发的RHadoop系列包（更多信息请参阅[https://github.com/RevolutionAnalytics/RHadoop](https://github.com/RevolutionAnalytics/RHadoop)）。RHadoop包括`rhdfs`包，它提供了在HDFS中操作文件和目录的功能，以及`rmr2`包，它将MapReduce的功能作为R函数暴露出来。这些函数使得在不使用Hadoop
    Java API编程的情况下使用MapReduce变得容易。相反，`rmr2`在每个工作节点上运行R的一个副本，映射器和归约器作为R函数编写，以应用于每个数据块。
- en: If you did not use the Hadoop setup instructions in the preceding section, follow
    the installation instructions for `rhdfs` and `rmr2` at [https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用前一个部分中的Hadoop设置说明，请按照[https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki)中的`rhdfs`和`rmr2`的安装说明进行操作。
- en: Uploading data to HDFS
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据上传到HDFS
- en: The first thing to do for this is to get data into HDFS. For this chapter, we
    will use the Google Books Ngrams data (for more information refer to [http://storage.googleapis.com/books/ngrams/books/datasetsv2.html](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html)).
    Here, n-grams are consecutive words that appear in the text where *n* represents
    the number of words in a phrase—a 1-gram is simply a word (for example, "Batman"),
    a 2-gram is two consecutive words (for example, "Darth Vader"), and a 6-gram is
    six consecutive words (for example, "Humpty Dumpty sat on a wall"). We will use
    the data of 1-grams for our examples.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是将数据放入HDFS。对于本章，我们将使用Google Books Ngrams数据（更多信息请参阅[http://storage.googleapis.com/books/ngrams/books/datasetsv2.html](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html)）。在这里，n-gram是文本中连续出现的单词序列，其中*n*代表短语中的单词数量——1-gram只是一个单词（例如，“Batman”），2-gram是两个连续的单词（例如，“Darth
    Vader”），而6-gram是六个连续的单词（例如，“Humpty Dumpty sat on a wall”）。我们将使用1-gram的数据作为我们的示例。
- en: Note
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The dataset for this chapter is large enough to test the performance of Hadoop
    on a small cluster, but it is still relatively small compared to many other real-world
    datasets. Tools such as `ffdf` ([Chapter 7](ch07.html "Chapter 7. Processing Large
    Datasets with Limited RAM"), *Processing Large Datasets with Limited RAM*) can
    probably be used to process this dataset on a single machine. But when the data
    size gets much larger, Hadoop or other Big Data tools might be the only way to
    process the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的数据集足够大，可以测试Hadoop在小集群上的性能，但与许多其他现实世界的数据集相比，它仍然相对较小。例如，`ffdf`（[第7章](ch07.html
    "第7章. 使用有限RAM处理大型数据集"), *使用有限RAM处理大型数据集*) 工具可能被用来在单机上处理这个数据集。但当数据量变得很大时，Hadoop或其他大数据工具可能是处理数据的唯一方式。
- en: 'The following code downloads the 1-grams data and uploads them into HDFS. Google
    provides the data in separate files, with one file for each letter of the alphabet
    containing the words that start with that letter. In this code, `hdfs.init()`
    first initializes the connection to HDFS. Then, `hdfs.mkdir()` creates the directory
    `/ngrams/data` in HDFS where the data will be stored. The code in the `for` loop
    downloads each file, decompresses it, and uploads it to HDFS using `hdfs.put()`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码下载1-gram数据并将其上传到HDFS。Google提供了单独的文件，每个文件包含以字母表中的每个字母开头的单词。在这段代码中，`hdfs.init()`首先初始化到HDFS的连接。然后，`hdfs.mkdir()`在HDFS中创建用于存储数据的目录`/ngrams/data`。`for`循环中的代码下载每个文件，解压缩它，并使用`hdfs.put()`将其上传到HDFS：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can check that all the files have been uploaded successfully into HDFS:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查所有文件是否已成功上传到HDFS：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Analyzing HDFS data with RHadoop
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用RHadoop分析HDFS数据
- en: Now that the data is loaded into HDFS, we can use MapReduce to analyze the data.
    Say we want to compare the popularity of Batman versus Superman since the 1950s.
    The Google Ngrams data might provide some insight into that.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已加载到HDFS中，我们可以使用MapReduce来分析数据。比如说，我们想要比较自1950年代以来蝙蝠侠与超人的人气。Google Ngrams数据可能对此提供一些见解。
- en: Each line of the Ngrams data is a tab-seperated list of values, starting with
    the Ngram, followed by the year, number of occurrences of that Ngram, and number
    of books that the Ngram appeared in. For example, the following command line indicates
    that in 1978, the word "mountain" appeared 1,435,642 times in 1,453 books in the
    Google Books library.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Ngrams数据中的每一行都是一个制表符分隔的值列表，以Ngram开头，然后是年份，该Ngram的出现次数以及该Ngram出现在其中的书籍数量。例如，以下命令行表示在1978年，单词"mountain"在Google
    Books图书馆的1,453本书中出现了1,435,642次。
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To compare the popularity of Batman and Superman, we need to find the lines
    of code that represent these two words from 1950 onwards and collate the occurrence
    values.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要比较蝙蝠侠和超人的人气，我们需要找到代表这两个单词从1950年及以后的代码行，并整理出现值。
- en: 'Since the data consists of tab-separated text files, we need to specify the
    input format so that the `rmr2` functions know how to read the files. This can
    be done using the `make.input.format()` function:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据由制表符分隔的文本文件组成，我们需要指定输入格式，以便`rmr2`函数知道如何读取文件。这可以通过使用`make.input.format()`函数来完成：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For delimited text files such as comma-separated values or tab-separated values,
    `make.input.format()` accepts most of the same arguments as `read.table()`. In
    fact, `rmr2` uses `read.table()` to read each chunk of data into a data frame.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于如逗号分隔值或制表符分隔值的分隔文本文件，`make.input.format()`接受与`read.table()`大多数相同的参数。事实上，`rmr2`使用`read.table()`将每个数据块读取到一个数据框中。
- en: Besides delimited text files, `rmr2` can also read/write data as raw text (`format
    = "text"`), JSON (`"json"`), R's internal data serialization format (`"native"`),
    Hadoop SequenceFiles (`"sequence.typedbytes"`), HBase tables (`"hbase"`), and
    Hive or Pig (`"pig.hive"`). See the package documentation for the arguments associated
    with these data types.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分隔文本文件外，`rmr2`还可以以原始文本（`format = "text"`）、JSON（`"json"`）、R的内部数据序列化格式（`"native"`）、Hadoop
    SequenceFiles（`"sequence.typedbytes"`）、HBase表（`"hbase"`）和Hive或Pig（`"pig.hive"`）的形式读取/写入数据。有关这些数据类型的参数，请参阅包文档。
- en: 'The map step of our analysis involves filtering each line of data to find the
    relevant records. We will define a `mapper` function, as shown in the following
    code, that accepts a set of keys and a set of values as arguments. Since the Ngrams
    data does not contain keys, the `keys` argument is `NULL`. The argument `values`
    is a data frame that contains a chunk of data. The mapper function looks for rows
    of the data frame that contain the words we are interested in, for the year 1950
    or later. If any relevant rows are found, the `keyval()` function is called to
    return key-value pairs that will be passed to the reduce function. In this case,
    the keys are the words and the values are the corresponding years and occurrence
    counts:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析中的地图步骤涉及过滤每行数据以找到相关记录。我们将定义一个`mapper`函数，如下面的代码所示，该函数接受一组键和一组值作为参数。由于Ngrams数据不包含键，因此`keys`参数为`NULL`。`values`参数是一个包含数据块的数据框。`mapper`函数查找数据框中包含我们感兴趣的单词的行，对于1950年或之后的年份。如果找到任何相关行，则调用`keyval()`函数以返回将传递给reduce函数的键值对。在这种情况下，键是单词，值是对应的年份和出现次数：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you are familiar with MapReduce, you might have noticed that `rmr2` allows
    the mapper to accept and emit key-value pairs as lists and data frames that represent
    a whole chunk of data instead of one record at a time, as is the case with classical
    MapReduce. This can help with R's performance; vectorized R operations can be
    used to process the whole chunk of data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉MapReduce，你可能已经注意到`rmr2`允许mapper接受和作为列表和表示整个数据块的数据框发出键值对，而不是每次一个记录，就像经典MapReduce那样。这可以帮助提高R的性能；可以使用向量化R操作来处理整个数据块。
- en: The next step occurs behind the scenes, where MapReduce collects all the data
    emitted by the mappers and groups them by key. In this example, it will find two
    groups that correspond to the `"batman"` and `"superman"` keys. MapReduce then
    calls the reducer function to process one group of data at a time.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个步骤发生在幕后，MapReduce收集所有由mapper发出的数据，并按键分组。在这个例子中，它将找到两个组，分别对应于`"batman"`和`"superman"`键。然后MapReduce调用reducer函数一次处理一组数据。
- en: 'The job of the reducer, given the data for a particular superhero, is to sum
    the number of occurrences of this superhero''s name by year, using `tapply()`.
    This is required because the words in the Ngrams dataset are case sensitive. So,
    for example, we need to add up the number of times that "Batman", "batman", and
    "BATMAN" appear in each year. The reducer then returns the superhero''s name as
    the key, and a data frame that contains the total number of occurrences by year
    as the value. The code for the reducer is shown here:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 给定特定超级英雄的数据，reducer的职责是使用`tapply()`函数按年汇总这位超级英雄名字出现的次数。这是必需的，因为Ngrams数据集中的单词是区分大小写的。例如，我们需要将"Batman"、"batman"和"BATMAN"在每个年份出现的次数加起来。然后，reducer返回超级英雄的名字作为键，以及包含按年汇总的总出现次数的数据框作为值。reducer的代码如下所示：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we have defined our mapper and reducer functions, we can execute the
    MapReduce job using `mapreduce()`. We will call this function, specify the input
    directory and data format, the output directory where the results are to be written,
    and the mapper and reducer functions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的mapper和reducer函数，我们可以使用`mapreduce()`函数执行MapReduce作业。我们将调用此函数，指定输入目录和数据格式，结果输出目录，以及mapper和reducer函数。
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When this MapReduce job executes, the resultant key-value pairs are written
    to HDFS in the `/ngrams/batmanVsuperman` folder. We can use `from.dfs()` to retrieve
    the results from HDFS into R objects. This function returns a list with two components:
    `key` and `value`. In this case, `key` is a character vector that specifies the
    superhero''s name for each row of data, and `val` is a data frame that contains
    the corresponding years and occurrence counts.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个MapReduce作业执行时，结果键值对将被写入HDFS的`/ngrams/batmanVsuperman`文件夹。我们可以使用`from.dfs()`函数从HDFS检索结果到R对象。此函数返回一个包含两个组件的列表：`key`和`value`。在这种情况下，`key`是一个字符向量，指定了每行数据的超级英雄名字，而`val`是一个包含相应年份和出现次数的数据框。
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s plot the results in order to compare how popular these two superheroes
    have been over the years:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制结果，以便比较这两位超级英雄在多年来的受欢迎程度：
- en: '![Analyzing HDFS data with RHadoop](img/9263OS_10_01.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![使用RHadoop分析HDFS数据](img/9263OS_10_01.jpg)'
- en: Popularity of Batman versus Superman since the 1950s, according to Google Books
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Google Books，自1950年代以来蝙蝠侠与超人的受欢迎程度对比
- en: While both the superheroes popularity steadily increased over the years, there
    is an interesting spike in the number of times Superman was mentioned in books
    in the 1970s. This could be due to the release of the multi Academy Award-winning
    film, *Superman* starring Christopher Reeve in 1978\. However, this surge in popularity
    was short-lived.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两位超级英雄的受欢迎程度在多年稳步上升，但在20世纪70年代，关于超人被提及的次数有一个有趣的峰值。这可能是因为1978年上映的多届奥斯卡获奖电影《超人》，由克里斯托弗·里夫主演。然而，这种受欢迎程度的激增是短暂的。
- en: 'The time it takes to complete the MapReduce algorithm depends on the size of
    the data, the complexity of the task, and the number of nodes in the cluster.
    We tested this example using the `m1.xlarge` AWS servers, which have 4 CPUs and
    15 GB of RAM each, with cluster sizes ranging from 4 to 32 core nodes (in EMR
    terminology, these are nodes that store data and process them). The following
    figure shows how the execution time decreases as more nodes are added to the cluster:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 完成MapReduce算法所需的时间取决于数据的大小、任务的复杂性和集群中的节点数。我们使用`m1.xlarge` AWS服务器测试了这个示例，每个服务器有4个CPU和15
    GB的RAM，集群大小从4到32个核心节点（在EMR术语中，这些是存储数据并处理数据的节点）。以下图表显示了随着集群中节点数量的增加，执行时间如何减少：
- en: '![Analyzing HDFS data with RHadoop](img/9263OS_10_02.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![使用RHadoop分析HDFS数据](img/9263OS_10_02.jpg)'
- en: Execution time as cluster size increases
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 随着集群大小增加的执行时间
- en: Because `rmr2` starts an instance of R on each Hadoop node to process the data,
    the efficiency of the MapReduce task depends on that of the R code for the mapper
    and reducer functions. Many of the techniques in this book to improve the performance
    of serial R programs can be applied when you design the mapper and reducer functions
    too. Furthermore, every MapReduce job incurs overheads of starting a new job,
    reading data from the disk, and coordinating the execution of the job across the
    cluster. Where possible, combining individual tasks into larger MapReduce tasks
    that can be executed at one go will help to improve the overall performance by
    reducing these overheads.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`rmr2`在每个Hadoop节点上启动一个R实例来处理数据，因此MapReduce任务的效率取决于mapper和reducer函数的R代码效率。本书中许多用于提高串行R程序性能的技术也可以在您设计mapper和reducer函数时应用。此外，每个MapReduce作业都会产生启动新作业、从磁盘读取数据以及在集群中协调作业执行的开销。在可能的情况下，将单个任务组合成可以一次性执行的大MapReduce任务，通过减少这些开销来提高整体性能。
- en: Once you are done using the Hadoop cluster, remember to terminate the cluster
    from the AWS EMR console to prevent unexpected charges.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成对Hadoop集群的使用，请记住从AWS EMR控制台终止集群，以防止意外收费。
- en: Other Hadoop packages for R
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他用于R的Hadoop包
- en: 'While the scope of this book allows us to cover only a few R packages that
    interface with Hadoop, the community has developed many more packages to bring
    the power of Hadoop to R. Here are a few more packages that can be useful:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书的范围仅允许我们介绍一些与Hadoop接口的R包，但社区已经开发了更多包，以将Hadoop的力量带给R。以下是一些可能有用的更多包：
- en: 'Besides `rhdfs` and `rmr2`, RHadoop also provides other packages:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`rhdfs`和`rmr2`之外，RHadoop还提供了其他包：
- en: '`plyrmr`: It provides functionality similar to `plyr` on MapReduce'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plyrmr`：它提供了类似于`plyr`在MapReduce上的功能'
- en: '`rhbase`: It provides functions to work with HBase data'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhbase`：它提供了与HBase数据一起工作的函数'
- en: '`ravro`: It provides reading/writing of data in the Avro format'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ravro`：它提供了在Avro格式中读取/写入数据的功能'
- en: 'Another family of packages called `RHIPE` (for more information refer to [http://www.datadr.org/](http://www.datadr.org/))
    provides similar MapReduce capabilities with a slightly different syntax:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个名为`RHIPE`（更多信息请参阅[http://www.datadr.org/](http://www.datadr.org/))的包族提供了类似MapReduce的功能，但语法略有不同：
- en: '`RHIPE`: This package provides the core HDFS and MapReduce functionality'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RHIPE`：此包提供了核心的HDFS和MapReduce功能'
- en: '`datadr`: It provides data manipulation capabilities similar to `plyr`/`dplyr`'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datadr`：它提供了类似于`plyr`/`dplyr`的数据操作功能'
- en: '`Trelliscope`: It provides visualization of large datasets in HDFS'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Trelliscope`：它提供了在HDFS中可视化大数据集的功能'
- en: At the time of writing, `RHIPE` does not support YARN or MapReduce 2.0\. An
    older version of Hadoop is required to use the `RHIPE` packages until this is
    fixed.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，`RHIPE`不支持YARN或MapReduce 2.0。需要使用较旧版本的Hadoop才能使用`RHIPE`包，直到这个问题得到解决。
- en: Another package, `Segue` (for more information refer to [https://code.google.com/p/segue/](https://code.google.com/p/segue/))
    takes a different approach. It does not provide full MapReduce capabilities. Rather,
    it treats Amazon's EMR as an additional computational resource for computationally
    heavy R tasks. This is similar to cluster computing in [Chapter 8](ch08.html "Chapter 8. Multiplying
    Performance with Parallel Computing"), *Multiplying Performance with Parallel
    Computing*, but using EMR as the computational cluster. The `Segue` package provides
    the `emrlapply()` function that performs a parallel `lapply` operation on an EMR
    cluster; this is analogous to `mclapply()` from the `parallel` package.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个名为`Segue`（更多信息请参阅[https://code.google.com/p/segue/](https://code.google.com/p/segue/))的包采取了不同的方法。它不提供完整的MapReduce功能。相反，它将亚马逊的EMR视为计算密集型R任务的额外计算资源。这与第8章中提到的集群计算类似，即*通过并行计算提高性能*，但使用EMR作为计算集群。`Segue`包提供了`emrlapply()`函数，该函数在EMR集群上执行并行`lapply`操作；这类似于`parallel`包中的`mclapply()`。
- en: Summary
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to set up a Hadoop cluster on Amazon Elastic
    MapReduce, and how to use the RHadoop family of packages in order to analyze data
    in HDFS using MapReduce. We saw how the performance of the MapReduce task improves
    dramatically as more servers are added to the Hadoop cluster, but the performance
    eventually reaches a limit due to Amdahl's law ([Chapter 8](ch08.html "Chapter 8. Multiplying
    Performance with Parallel Computing"), *Multiplying Performance with Parallel
    Computing*).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何在 Amazon Elastic MapReduce 上设置 Hadoop 集群，以及如何使用 RHadoop 系列包来使用 MapReduce
    分析 HDFS 中的数据。我们看到了随着更多服务器被添加到 Hadoop 集群中，MapReduce 任务的性能如何显著提高，但最终由于 Amdahl 定律（[第
    8 章](ch08.html "第 8 章。通过并行计算提高性能")，*通过并行计算提高性能*）的限制，性能最终会达到一个极限。
- en: Hadoop and its ecosystem of tools is rapidly evolving. Other tools are being
    actively developed to make Hadoop perform even better. For example, Apache Spark
    ([http://spark.apache.org/](http://spark.apache.org/)) provides Resilient Distributed
    Datasets (RDDs) that store data in memory across a Hadoop cluster. This allows
    data to be read from HDFS once and to be used many times in order to dramatically
    improve the performance of interactive tasks like data exploration and iterative
    algorithms like gradient descent or k-means clustering. Another example is Apache
    Storm ([http://storm.incubator.apache.org/](http://storm.incubator.apache.org/))
    that allows you to process real-time data streams. Because these tools and their
    associated R interfaces are being actively developed, they will likely change
    by the time you read this book, so we have decided not to include them here. But
    they are worth looking into if you have specific needs like in-memory analytics
    or real-time data processing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 及其工具生态系统正在迅速发展。其他工具正在积极开发中，以使 Hadoop 的性能更加出色。例如，Apache Spark ([http://spark.apache.org/](http://spark.apache.org/))
    提供了弹性分布式数据集 (RDDs)，可以在 Hadoop 集群中存储数据。这使得数据可以从 HDFS 中读取一次，并多次使用，从而显著提高数据探索和梯度下降或
    k-means 聚类等迭代任务等交互式任务的性能。另一个例子是 Apache Storm ([http://storm.incubator.apache.org/](http://storm.incubator.apache.org/))，它允许您处理实时数据流。因为这些工具及其相关的
    R 接口正在积极开发中，它们在你阅读这本书的时候可能会发生变化，所以我们决定在这里不包含它们。但如果你有内存分析或实时数据处理等特定需求，它们是值得研究的。
- en: We have come to the end of the book. It has been an exhilarating journey looking
    at a whole spectrum of techniques to improve the performance of R programs, from
    optimizing memory utilization and computational speed to multiplying computational
    power with parallel programming and cluster computing. What we have covered here
    is only the basics; there is much more to learn about writing more efficient R
    code. There are other resources that dive into specific topics in far greater
    detail than we can here. Package documentation is always useful to read, though
    sometimes cryptic; sometimes, the only way to find out what works is to try. Of
    course, there is the great community of R users in online forums, mailing lists
    and other places, who are always eager to help with answers and tips.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经来到了这本书的结尾。回顾过去，我们探索了整个光谱的技术，旨在提高 R 程序的性能，从优化内存利用率和计算速度，到通过并行编程和集群计算来增加计算能力。在这里我们所涵盖的仅仅是基础；关于编写更高效的
    R 代码，还有许多东西需要学习。有其他资源会深入探讨比这里更具体的话题。虽然有时有些晦涩难懂，但阅读包文档总是有用的；有时，了解什么可行唯一的方法就是尝试。当然，还有庞大的
    R 用户社区，他们在在线论坛、邮件列表和其他地方，总是乐于提供答案和建议。
- en: We hope that you have enjoyed this book and learned from it as much as we have
    writing it. Thank you for joining us in this journey, and we wish you the very
    best in exploring the world of R high-performance computing.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望您喜欢这本书，并且从中学到了我们撰写它时所学到的东西。感谢您加入我们的这次旅程，并祝愿您在探索 R 高性能计算的世界中一切顺利。
