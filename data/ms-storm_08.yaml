- en: Integration of Storm and Kafka
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm和Kafka的集成
- en: Apache Kafka is a high-throughput, distributed, fault-tolerant, and replicated
    messaging system that was first developed at LinkedIn. The use cases of Kafka
    vary from log aggregation, to stream processing, to replacing other messaging
    systems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka是一个高吞吐量、分布式、容错和复制的消息系统，最初在LinkedIn开发。Kafka的用例从日志聚合到流处理再到替代其他消息系统都有。
- en: Kafka has emerged as one of the important components of real-time processing
    pipelines in combination with Storm. Kafka can act as a buffer or feeder for messages
    that need to be processed by Storm. Kafka can also be used as the output sink
    for results emitted from Storm topologies.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka已经成为实时处理流水线中与Storm组合使用的重要组件之一。Kafka可以作为需要由Storm处理的消息的缓冲区或者提供者。Kafka也可以作为Storm拓扑发出的结果的输出接收端。
- en: 'In this chapter, we will be covering the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Kafka architecture--broker, producer, and consumer
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka架构——broker、producer和consumer
- en: Installation of the Kafka cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka集群的安装
- en: Sharing the producer and consumer between Kafka
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kafka之间共享producer和consumer
- en: Development of Storm topology using Kafka consumer as Storm spout
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kafka consumer作为Storm spout开发Storm拓扑
- en: Deployment of a Kafka and Storm integration topology
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka和Storm集成拓扑的部署
- en: Introduction to Kafka
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka简介
- en: In this section we are going to cover the architecture of Kafka--broker, consumer,
    and producer.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将介绍Kafka的架构——broker、consumer和producer。
- en: Kafka architecture
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka架构
- en: 'Kafka has an architecture that differs significantly from other messaging systems.
    Kafka is a peer to peer system (each node in a cluster has the same role) in which
    each node is called a **broker**. The brokers coordinate their actions with the
    help of a ZooKeeper ensemble. The Kafka metadata managed by the ZooKeeper ensemble
    is mentioned in the section *Sharing ZooKeeper between Storm and Kafka*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka具有与其他消息系统显著不同的架构。Kafka是一个点对点系统（集群中的每个节点具有相同的角色），每个节点称为**broker**。broker通过ZooKeeper集合协调它们的操作。ZooKeeper集合管理的Kafka元数据在*在Storm和Kafka之间共享ZooKeeper*部分中提到。
- en: '![](img/00051.gif)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00051.gif)'
- en: 'Figure 8.1: A Kafka cluster'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：Kafka集群
- en: 'The following are the important components of Kafka:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Kafka的重要组件：
- en: Producer
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Producer
- en: A producer is an entity that uses the Kafka client API to publish messages into
    the Kafka cluster. In a Kafka broker, messages are published by the producer entity
    to named entities called **topics**. A topic is a persistent queue (data stored
    into topics is persisted to disk).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者是使用Kafka客户端API将消息发布到Kafka集群的实体。在Kafka broker中，消息由生产者实体发布到名为**topics**的实体。主题是一个持久队列（存储在主题中的数据被持久化到磁盘）。
- en: 'For parallelism, a Kafka topic can have multiple partitions. Each partition
    data is represented in a different file. Also, two partitions of a single topic
    can be allocated on a different broker, thus increasing throughput as all partitions
    are independent of each other. Each message in a partition has a unique sequence
    number associated with it called an **offset**:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了并行处理，Kafka主题可以有多个分区。每个分区的数据都以不同的文件表示。同一个主题的两个分区可以分配到不同的broker上，从而增加吞吐量，因为所有分区都是相互独立的。每个分区中的消息都有一个与之关联的唯一序列号，称为**offset**：
- en: '![](img/00052.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00052.jpeg)'
- en: 'Figure 8.2: Kafka topic distribution'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：Kafka主题分布
- en: Replication
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制
- en: Kafka supports the replication of partitions of topics to support fault tolerance.
    Kafka automatically handles the replication of partitions and makes sure that
    the replica of a partition will be assigned to different brokers. Kafka elects
    one broker as the leader of a partition and all writes and reads must go to the
    partition leader. Replication features are introduced in Kafka 8.0.0 version.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka支持主题分区的复制以支持容错。Kafka自动处理分区的复制，并确保分区的副本将分配给不同的broker。Kafka选举一个broker作为分区的leader，并且所有写入和读取都必须到分区leader。复制功能是在Kafka
    8.0.0版本中引入的。
- en: The Kafka cluster manages the list of **in sync replica** (**ISR**)--the replicate
    which are in sync with the partition leader into ZooKeeper. If the partition leader
    goes down, then the followers/replicas that are present in the ISR list are only
    eligible for the next leader of the failed partition.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka集群通过ZooKeeper管理**in sync replica**（ISR）的列表——与分区leader同步的副本。如果分区leader宕机，那么在ISR列表中存在的跟随者/副本才有资格成为失败分区的下一个leader。
- en: Consumer
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Consumer
- en: Consumers read a range of messages from a broker. Each consumer has an assigned
    group ID. All the consumers with the same group ID act as a single logical consumer.
    Each message of a topic is delivered to one consumer from a consumer group (with
    the same group ID). Different consumer groups for a particular topic can process
    messages at their own pace as messages are not removed from the topics as soon
    as they are consumed. In fact, it is the responsibility of the consumers to keep
    track of how many messages they have consumed.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者从broker中读取一系列消息。每个消费者都有一个分配的group ID。具有相同group ID的所有消费者作为单个逻辑消费者。主题的每条消息都会传递给具有相同group
    ID的消费者组中的一个消费者。特定主题的不同消费者组可以以自己的速度处理消息，因为消息在被消费后并不会立即从主题中移除。事实上，消费者有责任跟踪他们已经消费了多少消息。
- en: As mentioned earlier, each message in a partition has a unique sequence number
    associated with it called an offset. It is through this offset that consumers
    know how much of the stream they have already processed. If a consumer decides
    to replay already processed messages, all it needs to do is just set the value
    of an offset to an earlier value before consuming messages from Kafka.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，每个分区中的每条消息都有一个与之关联的唯一序列号，称为offset。通过这个offset，消费者知道他们已经处理了多少流。如果消费者决定重新播放已经处理过的消息，他只需要将offset的值设置为之前的值，然后再从Kafka中消费消息。
- en: Broker
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Broker
- en: 'The broker receives the messages from the producer (push mechanism) and delivers
    the messages to the consumer (pull mechanism). Brokers also manage the persistence
    of messages in a file. Kafka brokers are very lightweight: they only open file
    pointers on a queue (topic partitions) and manage TCP connections.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 经纪人从生产者（推送机制）接收消息，并将消息传递给消费者（拉取机制）。经纪人还管理文件中消息的持久性。Kafka经纪人非常轻量级：它们只在队列（主题分区）上打开文件指针，并管理TCP连接。
- en: Data retention
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据保留
- en: Each topic in Kafka has an associated retention time. When this time expires,
    Kafka deletes the expired data file for that particular topic. This is a very
    efficient operation as it's a file delete operation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka中的每个主题都有一个关联的保留时间。当此时间到期时，Kafka会删除该特定主题的过期数据文件。这是一个非常高效的操作，因为它是一个文件删除操作。
- en: Installation of Kafka brokers
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Kafka经纪人
- en: At the time of writing, the stable version of Kafka is 0.9.x.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Kafka的稳定版本是0.9.x。
- en: The prerequisites for running Kafka are a ZooKeeper ensemble and Java Version
    1.7 or above. Kafka comes with a convenience script that can start a single node
    ZooKeeper but it is not recommended to use it in a production environment. We
    will be using the ZooKeeper cluster we deployed in [Chapter 2](part0034.html#10DJ40-6149dd15e07b443593cc93f2eb31ee7b),
    *Storm Deployment, Topology Development, and Topology Options*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 运行Kafka的先决条件是ZooKeeper集合和Java版本1.7或更高版本。Kafka附带了一个方便的脚本，可以启动单节点ZooKeeper，但不建议在生产环境中使用。我们将使用我们在[第2章](part0034.html#10DJ40-6149dd15e07b443593cc93f2eb31ee7b)中部署的ZooKeeper集群。
- en: We will see how to set up a single node Kafka cluster first and then how to
    add two more nodes to it to run a full-fledged, three node Kafka cluster with
    replication enabled.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先看如何设置单节点Kafka集群，然后再看如何添加另外两个节点以运行一个完整的、启用了复制的三节点Kafka集群。
- en: Setting up a single node Kafka cluster
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置单节点Kafka集群
- en: 'Following are the steps to set up a single node Kafka cluster:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是设置单节点Kafka集群的步骤：
- en: Download the Kafka 0.9.x binary distribution named `kafka_2.10-0.9.0.1.tar.gz`
    from [http://apache.claz.org/kafka/0.9.0.1/kafka_2.10-0.9.0.1.tgz](http://apache.claz.org/kafka/0.9.0.1/kafka_2.10-0.9.0.1.tgz).
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[http://apache.claz.org/kafka/0.9.0.1/kafka_2.10-0.9.0.1.tgz](http://apache.claz.org/kafka/0.9.0.1/kafka_2.10-0.9.0.1.tgz)下载Kafka
    0.9.x二进制分发版，文件名为`kafka_2.10-0.9.0.1.tar.gz`。
- en: 'Extract the archive to wherever you want to install Kafka with the following
    command:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将存档文件提取到您想要安装Kafka的位置：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will refer to the Kafka installation directory as `$KAFKA_HOME` from now
    on.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们将把Kafka安装目录称为`$KAFKA_HOME`。
- en: 'Change the following properties in the `$KAFKA_HOME/config/server.properties`
    file:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改`$KAFKA_HOME/config/server.properties`文件中的以下属性：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, `zoo1`, `zoo2`, and `zoo3` represent the hostnames of the ZooKeeper nodes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`zoo1`、`zoo2`和`zoo3`代表了ZooKeeper节点的主机名。
- en: 'The following are the definitions of the important properties in the `server.properties`
    file:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`server.properties`文件中重要属性的定义：
- en: '`broker.id`: This is a unique integer ID for each of the brokers in a Kafka
    cluster.'
  id: totrans-45
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`broker.id`：这是Kafka集群中每个经纪人的唯一整数ID。'
- en: '`port`: This is the port number for a Kafka broker. Its default value is `9092`.
    If you want to run multiple brokers on a single machine, give a unique port to
    each broker.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`port`：这是Kafka经纪人的端口号。默认值为`9092`。如果您想在单台机器上运行多个经纪人，请为每个经纪人指定一个唯一的端口。'
- en: '`host.name`: The hostname to which the broker should bind and advertise itself.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host.name`：代表经纪人应该绑定和宣传自己的主机名。'
- en: '`log.dirs`: The name of this property is a bit unfortunate as it represents
    not the log directory for Kafka, but the directory where Kafka stores the actual
    data sent to it. This can take a single directory or a comma-separated list of
    directories to store data. Kafka throughput can be increased by attaching multiple
    physical disks to the broker node and specifying multiple data directories, each
    lying on a different disk. It is not much use specifying multiple directories
    on the same physical disk, as all the I/O will still be happening on the same
    disk.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log.dirs`：这个属性的名称有点不幸，因为它代表的不是Kafka的日志目录，而是Kafka存储实际发送到它的数据的目录。它可以接受单个目录或逗号分隔的目录列表来存储数据。通过将多个物理磁盘连接到经纪人节点并指定多个数据目录，每个目录位于不同的磁盘上，可以增加Kafka的吞吐量。在同一物理磁盘上指定多个目录并没有太大用处，因为所有I/O仍然会在同一磁盘上进行。'
- en: '`num.partitions`: This represents the default number of partitions for newly
    created topics. This property can be overridden when creating new topics. A greater
    number of partitions results in greater parallelism at the cost of a larger number
    of files.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num.partitions`：这代表了新创建主题的默认分区数。在创建新主题时，可以覆盖此属性。分区数越多，可以实现更大的并行性，但会增加文件数量。'
- en: '`log.retention.hours`: Kafka does not delete messages immediately after consumers
    consume them. It retains them for the number of hours defined by this property
    so that in the event of any issues the consumers can replay the messages from
    Kafka. The default value is `168` hours, which is 1 week.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log.retention.hours`：Kafka在消费者消费消息后不会立即删除消息。它会保留消息一定小时数，由此属性定义，以便在出现任何问题时，消费者可以从Kafka重放消息。默认值为`168`小时，即1周。'
- en: '`zookeeper.connect`: This is the comma-separated list of ZooKeeper nodes in
    `hostname:port` form.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zookeeper.connect`：这是以`hostname:port`形式的ZooKeeper节点的逗号分隔列表。'
- en: 'Start the Kafka server by running the following command:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令启动Kafka服务器：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you get something similar to the preceding three lines on your console, then
    your Kafka broker is up-and-running and we can proceed to test it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在控制台上得到类似于前三行的内容，那么您的Kafka经纪人已经启动，我们可以继续测试。
- en: 'Now we will verify that the Kafka broker is set up correctly by sending and
    receiving some test messages. First, let''s create a verification topic for testing
    by executing the following command:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将通过发送和接收一些测试消息来验证Kafka经纪人是否设置正确。首先，让我们通过执行以下命令为测试创建一个验证主题：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now let''s verify if the topic creation was successful by listing all the topics:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们通过列出所有主题来验证主题创建是否成功：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The topic is created; let''s produce some sample messages for the Kafka cluster.
    Kafka comes with a command-line producer that we can use to produce messages:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主题已创建；让我们为Kafka集群生成一些示例消息。Kafka附带了一个命令行生产者，我们可以用来生成消息：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Write the following messages on your console:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在控制台上写入以下消息：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s consume these messages by starting a new console consumer on a new console
    window:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过在新的控制台窗口上启动新的控制台消费者来消费这些消息：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, if we enter any message on the producer console, it will automatically
    be consumed by this consumer and displayed on the command line.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们在生产者控制台上输入任何消息，它将自动被此消费者消费并显示在命令行上。
- en: '**Using Kafka''s single node ZooKeeper** If you don''t want to use an external
    ZooKeeper ensemble, you can use the single node ZooKeeper instance that comes
    with Kafka for quick and dirty development. To start using it, first modify the
    `$KAFKA_HOME/config/zookeeper.properties` file to specify the data directory by
    supplying following property:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用Kafka的单节点ZooKeeper** 如果您不想使用外部ZooKeeper集合，可以使用Kafka附带的单节点ZooKeeper实例进行快速开发。要开始使用它，首先修改`$KAFKA_HOME/config/zookeeper.properties`文件以指定数据目录，提供以下属性：'
- en: '`dataDir=/var/zookeeper`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataDir=/var/zookeeper`'
- en: 'Now, you can start the Zookeeper instance with the following command:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用以下命令启动Zookeeper实例：
- en: '`> ./bin/zookeeper-server-start.sh config/zookeeper.properties`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`> ./bin/zookeeper-server-start.sh config/zookeeper.properties`'
- en: Setting up a three node Kafka cluster
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置三节点Kafka集群
- en: 'So far we have a single node Kafka cluster. Follow the steps to deploy the
    Kafka cluster:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有一个单节点Kafka集群。按照以下步骤部署Kafka集群：
- en: Create a three node VM or three physical machines.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个三节点VM或三台物理机。
- en: Perform steps 1 and 2 mentioned in the section *Setting up a single node Kafka
    cluster*.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行*设置单节点Kafka集群*部分中提到的步骤1和2。
- en: 'Change the following properties in the file `$KAFKA_HOME/config/server.properties`:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改文件`$KAFKA_HOME/config/server.properties`中的以下属性：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Make sure that the value of the `broker.id` property is unique for each Kafka
    broker and the value of `zookeeper.connect` must be the same on all nodes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 确保`broker.id`属性的值对于每个Kafka代理都是唯一的，`zookeeper.connect`的值在所有节点上必须相同。
- en: 'Start the Kafka brokers by executing the following command on all three boxes:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在所有三个框上执行以下命令来启动Kafka代理：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now let''s verify the setup. First we create a topic with the following command:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们验证设置。首先使用以下命令创建一个主题：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we will list the topics to see if the topic was created successfully:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将列出主题以查看主题是否成功创建：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we will verify the setup by using the Kafka console producer and consumer
    as done in the *Setting up a single node Kafka cluster* section:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将通过使用Kafka控制台生产者和消费者来验证设置，就像在*设置单节点Kafka集群*部分中所做的那样：
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Write the following messages on your console:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在控制台上写入以下消息：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s consume these messages by starting a new console consumer on a new console
    window:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过在新的控制台窗口上启动新的控制台消费者来消费这些消息：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'So far, we have three brokers on the Kafka cluster working. In the next section,
    we will see how to write a producer that can produce messages to Kafka:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有三个在工作的Kafka集群代理。在下一节中，我们将看到如何编写一个可以向Kafka发送消息的生产者：
- en: Multiple Kafka brokers on a single node
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单个节点上的多个Kafka代理
- en: 'If you want to run multiple Kafka brokers on a single node, then follow the
    following steps:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在单个节点上运行多个Kafka代理，则请按照以下步骤进行操作：
- en: Copy `config/server.properties` to create `config/server1.properties` and `config/server2.properties`.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制`config/server.properties`以创建`config/server1.properties`和`config/server2.properties`。
- en: 'Populate the following properties in `config/server.properties`:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`config/server.properties`中填写以下属性：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Populate the following properties in `config/server1.properties`:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`config/server1.properties`中填写以下属性：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Populate the following properties in `config/server2.properties`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`config/server2.properties`中填写以下属性：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Run the following commands on three different terminals to start Kafka brokers:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在三个不同的终端上运行以下命令以启动Kafka代理：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Share ZooKeeper between Storm and Kafka
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Storm和Kafka之间共享ZooKeeper
- en: We can share the same ZooKeeper ensemble between Kafka and Storm as both store
    the metadata inside the different znodes (ZooKeeper coordinates between the distributed
    processes using the shared hierarchical namespace, which is organized similarly
    to a standard file system. In ZooKeeper, the namespace consisting of data registers
    is called znodes).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Kafka和Storm之间共享相同的ZooKeeper集合，因为两者都将元数据存储在不同的znodes中（ZooKeeper使用共享的分层命名空间协调分布式进程，其组织方式类似于标准文件系统。在ZooKeeper中，由数据寄存器组成的命名空间称为znodes）。
- en: We need to open the ZooKeeper client console to view the znodes (shared namespace)
    created for Kafka and Storm.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要打开ZooKeeper客户端控制台来查看为Kafka和Storm创建的znodes（共享命名空间）。
- en: 'Go to `ZK_HOME` and execute the following command to open the ZooKeeper console:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 转到`ZK_HOME`并执行以下命令以打开ZooKeeper控制台：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Execute the following command to view the list of znodes:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下命令以查看znodes列表：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, consumers, `isr_change_notification`, and brokers are the znodes and the
    Kafka is managing its metadata information into ZooKeeper at this location.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，消费者、`isr_change_notification`和代理是znodes，Kafka正在将其元数据信息管理到ZooKeeper的此位置。
- en: Storm manages its metadata inside the Storm znodes in ZooKeeper.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Storm在ZooKeeper中的Storm znodes中管理其元数据。
- en: Kafka producers and publishing data into Kafka
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka生产者并将数据发布到Kafka
- en: In this section we are writing a Kafka producer that will publish events into
    the Kafka topic.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们正在编写一个Kafka生产者，它将发布事件到Kafka主题中。
- en: 'Perform the following step to create the producer:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤创建生产者：
- en: Create a Maven project by using `com.stormadvance` as `groupId` and `kafka-producer`
    as `artifactId`.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`com.stormadvance`作为`groupId`和`kafka-producer`作为`artifactId`创建一个Maven项目。
- en: 'Add the following dependencies for Kafka in the `pom.xml` file:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pom.xml`文件中为Kafka添加以下依赖项：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Add the following `build` plugins to the `pom.xml` file. It will let us execute
    the producer using Maven:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pom.xml`文件中添加以下`build`插件。这将允许我们使用Maven执行生产者：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now we will create the `KafkaSampleProducer` class in the `com.stormadvance.kafka_producer`
    package. This class will produce each word from the first paragraph of Franz Kafka''s
    Metamorphosis into the `new_topic` topic in Kafka as single message. The following
    is the code for the `KafkaSampleProducer` class with explanations:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将在`com.stormadvance.kafka_producer`包中创建`KafkaSampleProducer`类。该类将从弗朗茨·卡夫卡的《变形记》第一段中的每个单词产生单词，并将其作为单个消息发布到Kafka的`new_topic`主题中。以下是`KafkaSampleProducer`类的代码及解释：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, before running the producer, we need to create `new_topic` in Kafka. To
    do so, execute the following command:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在运行生产者之前，我们需要在Kafka中创建`new_topic`。为此，请执行以下命令：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we can run the producer by executing the following command:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以通过执行以下命令运行生产者：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now let us verify that the message has been produced by using Kafka''s console
    consumer and executing the following command:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们通过使用Kafka的控制台消费者来验证消息是否已被生产，并执行以下命令：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: So, we are able to produce messages into Kafka. In the next section, we will
    see how we can use `KafkaSpout` to read messages from Kafka and process them inside
    a Storm topology.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们能够向Kafka生产消息。在下一节中，我们将看到如何使用`KafkaSpout`从Kafka中读取消息并在Storm拓扑中处理它们。
- en: Kafka Storm integration
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Storm集成
- en: Now we will create a Storm topology that will consume messages from the Kafka
    topic `new_topic` and aggregate words into sentences.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建一个Storm拓扑，该拓扑将从Kafka主题`new_topic`中消费消息并将单词聚合成句子。
- en: 'The complete message flow is shown as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的消息流如下所示：
- en: '![](img/00053.gif)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00053.gif)'
- en: We have already seen `KafkaSampleProducer`, which produces words into the Kafka
    broker. Now we will create a Storm topology that will read those words from Kafka
    to aggregate them into sentences. For this, we will have one `KafkaSpout` in the
    application that will read the messages from Kafka and two bolts, `WordBolt` that
    receive words from `KafkaSpout` and then aggregate them into sentences, which
    are then passed onto the `SentenceBolt`, which simply prints them on the output
    stream. We will be running this topology in a local mode.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了`KafkaSampleProducer`，它将单词生产到Kafka代理中。现在我们将创建一个Storm拓扑，该拓扑将从Kafka中读取这些单词并将它们聚合成句子。为此，我们的应用程序中将有一个`KafkaSpout`，它将从Kafka中读取消息，并且有两个bolt，`WordBolt`从`KafkaSpout`接收单词，然后将它们聚合成句子，然后传递给`SentenceBolt`，它只是在输出流上打印它们。我们将在本地模式下运行此拓扑。
- en: 'Follow the steps to create the Storm topology:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤创建Storm拓扑：
- en: Create a new Maven project with `groupId` as `com.stormadvance` and `artifactId`
    as `kafka-storm-topology`.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Maven项目，`groupId`为`com.stormadvance`，`artifactId`为`kafka-storm-topology`。
- en: 'Add the following dependencies for Kafka-Storm and Storm in the `pom.xml` file:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pom.xml`文件中添加以下Kafka-Storm和Storm的依赖项：
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Add the following Maven plugins to the `pom.xml` file so that we are able to
    run it from the command-line and also to package the topology to be executed in
    Storm:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pom.xml`文件中添加以下Maven插件，以便我们能够从命令行运行它，并且还能够打包拓扑以在Storm中执行：
- en: '[PRE28]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now we will first create the `WordBolt` that will aggregate the words into
    sentences. For this, create a class called `WordBolt` in the `com.stormadvance.kafka` package.
    The code for `WordBolt` is as follows, complete with explanation:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将首先创建`WordBolt`，它将单词聚合成句子。为此，在`com.stormadvance.kafka`包中创建一个名为`WordBolt`的类。`WordBolt`的代码如下，附有解释：
- en: '[PRE29]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next is `SentenceBolt`, which just prints the sentences that it receives. Create
    `SentenceBolt` in the `com.stormadvance.kafka` package. The code is as follows,
    with explanations:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是`SentenceBolt`，它只是打印接收到的句子。在`com.stormadvance.kafka`包中创建`SentenceBolt`。代码如下，附有解释：
- en: '[PRE30]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we will create the `KafkaTopology` that will define the `KafkaSpout` and
    wire it with `WordBolt` and `SentenceBolt`. Create a new class called `KafkaTopology`
    in the `com.stormadvance.kafka` package. The code is as follows, with explanations:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将创建`KafkaTopology`，它将定义`KafkaSpout`并将其与`WordBolt`和`SentenceBolt`连接起来。在`com.stormadvance.kafka`包中创建一个名为`KafkaTopology`的新类。代码如下，附有解释：
- en: '[PRE31]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now we will the run the topology. Make sure the Kafka cluster is running and
    you have executed the producer in the last section so that there are messages
    in Kafka for consumption.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将运行拓扑。确保Kafka集群正在运行，并且您已经在上一节中执行了生产者，以便Kafka中有消息可以消费。
- en: 'Run the topology by executing the following command:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令运行拓扑：
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This will execute the topology. You should see messages similar to the following
    in your output:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这将执行拓扑。您应该在输出中看到类似以下的消息：
- en: '[PRE33]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: So we are able to consume messages from Kafka and process them in a Storm topology.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们能够从Kafka中消费消息并在Storm拓扑中处理它们。
- en: Deploy the Kafka topology on Storm cluster
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Storm集成拓扑中部署Kafka
- en: The deployment of Kafka and Storm integration topology on the Storm cluster
    is similar to the deployment of other topologies. We need to set the number of
    workers and the maximum spout pending Storm config and we need to use the `submitTopology`
    method of `StormSubmitter` to submit the topology on the Storm cluster.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在Storm集群上部署Kafka和Storm集成拓扑与部署其他拓扑类似。我们需要设置工作程序的数量和最大的spout pending Storm配置，并且我们需要使用`StormSubmitter`的`submitTopology`方法将拓扑提交到Storm集群上。
- en: 'Now, we need to build the topology code as mentioned in the following steps
    to create a JAR of the Kafka Storm integration topology:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要按照以下步骤构建拓扑代码，以创建Kafka Storm集成拓扑的JAR包：
- en: Go to project home.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到项目主页。
- en: 'Execute the command:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行命令：
- en: '[PRE34]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令的输出如下：
- en: '[PRE35]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, copy the Kafka Storm topology on the Nimbus machine and execute the following
    command to submit the topology on the Storm cluster:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将Kafka Storm拓扑复制到Nimbus机器上，并执行以下命令将拓扑提交到Storm集群上：
- en: '[PRE36]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The preceding command runs `TopologyMainClass` with the argument. The main function
    of `TopologyMainClass` is to define the topology and submit it to Nimbus. The
    Storm JAR part takes care of connecting to Nimbus and uploading the JAR part.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令运行`TopologyMainClass`并带有参数。`TopologyMainClass`的主要功能是定义拓扑并将其提交到Nimbus。Storm
    JAR部分负责连接到Nimbus并上传JAR部分。
- en: 'Log in on the Storm Nimbus machine and execute the following commands:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录Storm Nimbus机器并执行以下命令：
- en: '[PRE37]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Here, `~/ storm-kafka-topology-0.0.1-SNAPSHOT-jar-with-dependencies.jar` is
    the path of the `KafkaTopology` JAR we are deploying on the Storm cluster.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`~/ storm-kafka-topology-0.0.1-SNAPSHOT-jar-with-dependencies.jar`是我们在Storm集群上部署的`KafkaTopology`
    JAR的路径。
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about the basics of Apache Kafka and how to use
    it as part of a real-time stream processing pipeline build with Storm. We learned
    about the architecture of Apache Kafka and how it can be integrated into Storm
    processing by using `KafkaSpout`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了Apache Kafka的基础知识以及如何将其作为与Storm一起构建实时流处理管道的一部分。我们了解了Apache Kafka的架构以及如何通过使用`KafkaSpout`将其集成到Storm处理中。
- en: In the next chapter, we are going to cover the integration of Storm with Hadoop
    and YARN. We are also going to cover sample examples for this operation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍Storm与Hadoop和YARN的集成。我们还将介绍此操作的示例示例。
