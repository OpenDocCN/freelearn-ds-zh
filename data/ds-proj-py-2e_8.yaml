- en: Appendix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: 1\. Data Exploration and Cleaning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 数据探索与清理
- en: 'Activity 1.01: Exploring the Remaining Financial Features in the Dataset'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 1.01：探索数据集中剩余的财务特征
- en: '**Solution:**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案：**'
- en: 'Before beginning, set up your environment and load in the cleaned dataset as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，设置好你的环境并按如下方式加载已清理的数据集：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Create lists of feature names for the remaining financial features.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为剩余的财务特征创建特征名称列表。
- en: 'These fall into two groups, so we will make lists of feature names as before,
    to facilitate analyzing them together. You can do this with the following code:'
  id: totrans-7
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些可以分为两组，因此我们将像之前一样列出特征名称，以便一起分析。你可以使用以下代码来实现：
- en: '[PRE1]'
  id: totrans-8
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Use `.describe()` to examine statistical summaries of the bill amount features.
    Reflect on what you see. Does it make sense?
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.describe()`方法查看账单金额特征的统计摘要。反思你所看到的内容。这合理吗？
- en: 'Use the following code to view the summary:'
  id: totrans-10
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用以下代码查看摘要：
- en: '[PRE2]'
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output should appear as follows:'
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '![Figure 1.47: Statistical description of bill amounts for the past 6 months'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.47：过去6个月账单金额的统计描述'
- en: '](img/B16392_01_47.jpg)'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_01_47.jpg)'
- en: 'Figure 1.47: Statistical description of bill amounts for the past 6 months'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.47：过去6个月账单金额的统计描述
- en: We see that the average monthly bill is roughly 40,000 to 50,000 NT dollars.
    You are encouraged to examine the conversion rate to your local currency. For
    example, 1 US dollar ~= 30 NT dollars. Do the conversion and ask yourself, is
    this a reasonable monthly payment? We should also confirm this with the client,
    but it seems reasonable.
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们看到平均每月账单大约是40,000到50,000新台币。建议你检查一下本地货币的汇率。例如，1美元约等于30新台币。做一下换算，问问自己，这个月度支付是否合理？我们也应该向客户确认这一点，但看起来是合理的。
- en: We also notice there are some negative bill amounts. This seems reasonable because
    of the possible overpayment of the previous month's bill, perhaps in anticipation
    of a purchase that would show up on the current month's bill. A scenario like
    this would leave that account with a negative balance, in the sense of a credit
    to the account holder.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还注意到有些账单金额为负。这似乎是合理的，因为可能是前一个月的账单超额支付了，或许是预期当前账单中会有某项购买。类似的情况会导致该账户余额为负，意味着该账户持有人有了一个信用额度。
- en: 'Visualize the bill amount features using a 2 by 3 grid of histogram plots using
    the following code:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码，按2x3的网格方式可视化账单金额特征的直方图：
- en: '[PRE3]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The graph should look like this:'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图表应该是这样的：
- en: '![Figure 1.48: Histograms of bill amounts'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.48：账单金额的直方图'
- en: '](img/B16392_01_48.jpg)'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_01_48.jpg)'
- en: 'Figure 1.48: Histograms of bill amounts'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.48：账单金额的直方图
- en: The histogram plots in *Figure 1.48* make sense in several respects. Most accounts
    have relatively small bills. There is a steady decrease in the number of accounts
    as the amount of the bill increases. It also appears that the distribution of
    billed amounts is roughly similar month to month, so we don't notice any data
    inconsistency issues as we did with the payment status features. This feature
    appears to pass our data quality inspection. Now, we'll move on to the final set
    of features.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*图 1.48*中的直方图从多个方面来看是有意义的。大多数账户的账单金额较小。随着账单金额的增加，账户的数量逐渐减少。看起来账单金额的分布在每个月之间大致相似，因此我们没有像处理支付状态特征时那样发现数据不一致问题。该特征似乎通过了我们的数据质量检查。现在，我们将继续分析最后一组特征。'
- en: 'Use the `.describe()` method to obtain a summary of the payment amount features
    using the following code:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.describe()`方法，通过以下代码获取支付金额特征的摘要：
- en: '[PRE4]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output should appear thus:'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 1.49: Statistical description of bill payment amounts for the past
    6 months'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.49：过去6个月账单支付金额的统计描述'
- en: '](img/B16392_01_49.jpg)'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_01_49.jpg)'
- en: 'Figure 1.49: Statistical description of bill payment amounts for the past 6
    months'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.49：过去6个月账单支付金额的统计描述
- en: The average payment amounts are about an order of magnitude (power of 10) lower
    than the average bill amounts we summarized earlier in the activity. This means
    that the "average case" is an account that is not paying off its entire balance
    from month to month. This makes sense in light of our exploration of the `PAY_1`
    feature, for which the most prevalent value was 0 (the account made at least the
    minimum payment but did not pay off the whole balance). There are no negative
    payments, which also seems right.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平均支付金额大约比我们在前面的活动中总结的平均账单金额低一个数量级（10 的幂）。这意味着“平均情况”是一个每月未还清全部余额的账户。从我们对 `PAY_1`
    特征的探索来看，这很有意义，因为该特征中最常见的值是 0（账户至少支付了最低付款额，但没有支付全部余额）。没有负支付，这也似乎是合理的。
- en: 'Plot a histogram of the bill payment features similar to the bill amount features,
    but also apply some rotation to the *x-axis* labels with the `xrot` keyword argument
    so that they don''t overlap. Use the `xrot=<angle>` keyword argument to rotate
    the *x-axis* labels by a given angle in degrees using the following code:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制与账单金额特征类似的支付金额特征的直方图，但还要使用 `xrot` 关键字参数对 *x 轴* 标签进行旋转，以避免重叠。使用 `xrot=<角度>`
    关键字参数按给定的角度（以度为单位）旋转 *x 轴* 标签，使用以下代码：
- en: '[PRE5]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In our case, we found that 30 degrees of rotation worked well. The plot should
    look like this:'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们发现 30 度的旋转效果很好。绘图应如下所示：
- en: '![Figure 1.50: Histograms of raw payment amount data'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.50：原始支付金额数据的直方图'
- en: '](img/B16392_01_50.jpg)'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_01_50.jpg)'
- en: '[PRE6]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Figure 1.50: Histograms of raw payment amount data'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.50：原始支付金额数据的直方图
- en: A quick glance at this figure indicates that this is not a very informative
    graphic; there is only one bin in most of the histograms that is of a noticeable
    height. This is not an effective way to visualize this data. It appears that the
    monthly payment amounts are mainly in a bin that includes 0\. How many are in
    fact 0?
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这张图的快速浏览表明，这不是一个非常有用的图形；大多数直方图只有一个区间的高度较为显著。这不是可视化这些数据的有效方式。看起来，月度支付金额主要集中在包含
    0 的区间中。那么，实际上有多少项是 0 呢？
- en: 'Use a Boolean mask to see how much of the payment amount data is exactly equal
    to 0 using the following code: Do this with the following code:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用布尔掩码来查看支付金额数据中有多少项恰好等于 0，使用以下代码：使用以下代码执行此操作：
- en: '[PRE7]py'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE7]py'
- en: 'The output should look like this:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 1.51: Counts of bill payments equal to 0'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.51：支付金额等于 0 的账单计数'
- en: '](img/B16392_01_51.jpg)'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_01_51.jpg)'
- en: 'Figure 1.51: Counts of bill payments equal to 0'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.51：支付金额等于 0 的账单计数
- en: '`pay_zero_mask`, which is a DataFrame of `True` and `False` values according
    to whether the payment amount is equal to 0\. The second line takes the column
    sums of this DataFrame, interpreting `True` as 1 and `False` as 0, so the column
    sums indicate how many accounts have a value of 0 for each feature.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`pay_zero_mask` 是一个包含 `True` 和 `False` 值的 DataFrame，表示支付金额是否等于 0。第二行对该 DataFrame
    进行列求和，将 `True` 视为 1，将 `False` 视为 0，因此列的和表示每个特征中支付金额为 0 的账户数。'
- en: We see that a substantial portion, roughly around 20-25% of accounts, have a
    bill payment equal to 0 in any given month. However, most bill payments are above
    0\. So, why can't we see them in the histogram? This is due to the **range** of
    values for bill payments relative to the values of the majority of the bill payments.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，约 20-25% 的账户在任何给定的月份里账单支付额为 0。然而，大多数账单支付额大于 0。那么，为什么我们在直方图中看不到它们呢？这是由于账单支付额的**范围**相对于大多数账单支付额的值。
- en: In the statistical summary, we can see that the maximum bill payment in a month
    is typically 2 orders of magnitude (100 times) larger than the average bill payment.
    It seems likely there are only a small number of these very large bill payments.
    But, because of the way the histogram is created, using equal-sized bins, nearly
    all the data is lumped into the smallest bin, and the larger bins are nearly invisible
    because they have so few accounts. We need a strategy to effectively visualize
    this data.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在统计摘要中，我们可以看到一个月中的最大账单支付额通常比平均账单支付额大两个数量级（100倍）。看起来这些非常大的账单支付可能只有少数几个。然而，由于直方图的创建方式，使用相同大小的区间，几乎所有数据都被聚集在最小的区间中，较大的区间几乎不可见，因为它们的账户数太少。我们需要一种有效的策略来可视化这些数据。
- en: 'Ignoring the payments of 0 using the mask you created in the previous step,
    use pandas'' `.apply()` and NumPy''s `np.log10()` method to plot histograms of
    logarithmic transformations of the non-zero payments. You can use `.apply()` to
    apply any function, including `log10`, to all the elements of a DataFrame. Use
    the following code for this:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 忽略前一步创建的掩码中的0支付值，使用pandas的`.apply()`方法和NumPy的`np.log10()`方法绘制非零支付的对数转换直方图。你可以使用`.apply()`将任何函数（包括`log10`）应用到DataFrame的所有元素。使用以下代码来完成此操作：
- en: '[PRE8]py'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE8]py'
- en: This is a relatively advanced use of pandas, so don't worry if you couldn't
    figure it out by yourself. However, it's good to start to get an impression of
    how you can do a lot in pandas with relatively little code.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是pandas的一个相对高级的使用方法，所以如果你自己没弄明白也不必担心。然而，开始理解如何用相对少量的代码在pandas中做很多事情是很有帮助的。
- en: 'The output should be as follows:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 1.52: Base-10 logs of non-zero bill payment amounts'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图1.52：非零账单支付金额的10为基对数'
- en: '](img/B16392_01_52.jpg)'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_01_52.jpg)'
- en: 'Figure 1.52: Base-10 logs of non-zero bill payment amounts'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.52：非零账单支付金额的10为基对数
- en: While we could have tried to create variable-width bins for better visualization
    of the payment amounts, a more convenient approach that is often used to visualize,
    and sometimes even model, data that has a few values on a much different scale
    than most of the values is a logarithmic transformation, or **log transform**.
    We used a base-10 log transform. Roughly speaking, this transform tells us the
    number of zeros in a value. In other words, a balance of at least 1 million dollars,
    but less than 10 million, would have a log transform of at least 6 but less than
    7, because 106 = 1,000,000 (and conversely log10(1,000,000) = 6) while 107 = 10,000,000.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们本可以尝试创建不同宽度的区间来更好地可视化支付金额，但另一种常用且便捷的方法是对数变换，或称为**对数变换**。我们使用了10为基的对数变换。大致来说，这种变换告诉我们一个数值中有多少个零。换句话说，一个余额至少为100万美元但不到1000万美元的账户，其对数变换结果会是6到7之间，因为106
    = 1,000,000（而`log10(1,000,000)` = 6），而107 = 10,000,000。
- en: To apply this transformation to our data, first, we needed to mask out the zero
    payments, because `log10(0)` is undefined (another common approach in this case
    is to add a very small number to all values, such as 0.01, so there are no zeros).
    We did this with the Python logical `not` operator `~` and the zero mask we created
    already. Then we used the pandas `.apply()` method, which applies any function
    we like to the data we have selected. In this case, we wished to apply a base-10
    logarithm, calculated by `np.log10`. Finally, we made histograms of these values.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将此变换应用到我们的数据，首先需要屏蔽掉零支付值，因为`log10(0)`是未定义的（另一种常见方法是对所有值加上一个非常小的数字，例如0.01，这样就没有零值）。我们使用了Python逻辑运算符`not`（`~`）和我们已经创建的零值掩码。然后，我们使用了pandas的`.apply()`方法，它可以将我们喜欢的任何函数应用到我们选择的数据上。在这种情况下，我们希望应用的是一个基于10的对数，使用`np.log10`来计算。最后，我们对这些值绘制了直方图。
- en: 'The result is a more effective data visualization: the values are spread in
    a more informative way across the histogram bins. We can see that the most commonly
    occurring bill payments are in the range of thousands (`log10(1,000) = 3`), which
    matches what we observed for the mean bill payment in the statistical summary.
    There are some pretty small bill payments, and also a few pretty large ones. Overall,
    the distribution of bill payments appears pretty consistent from month to month,
    so we don''t see any potential issues with this data.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是更加有效的数据可视化：这些值在直方图的区间中分布得更具信息量。我们可以看到，最常出现的账单支付金额位于千元范围内（`log10(1,000) =
    3`），这与我们在统计摘要中观察到的平均账单支付金额一致。也有一些非常小的账单支付金额，以及少数较大的支付金额。总体来看，账单支付金额的分布从每月来看似乎非常一致，因此我们没有发现该数据中存在任何潜在问题。
- en: 2\. Introduction to Scikit-Learn and Model Evaluation
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. Scikit-Learn简介与模型评估
- en: 'Activity 2.01: Performing Logistic Regression with a New Feature and Creating
    a Precision-Recall Curve'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动2.01：使用新特征执行逻辑回归并创建精准率-召回率曲线
- en: '**Solution:**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案：**'
- en: Use scikit-learn's `train_test_split` to make a new set of training and test
    data. This time, instead of `EDUCATION`, use `LIMIT_BAL`, the account's credit
    limit, as the feature.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用scikit-learn的`train_test_split`生成新的训练和测试数据集。这次，使用`LIMIT_BAL`，即账户的信用额度，作为特征，而不是`EDUCATION`。
- en: 'Execute the following code to do this:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行以下代码来实现：
- en: '[PRE9]py'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE9]py'
- en: Notice here we create new training and test splits, with new variable names.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，这里我们创建了新的训练和测试数据集，并且变量名称也发生了变化。
- en: Train a logistic regression model using the training data from your split.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你拆分后的训练数据训练一个逻辑回归模型。
- en: 'The following code does this:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码实现了这个功能：
- en: '[PRE10]py'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE10]py'
- en: You can reuse the same model object you used earlier, `example_lr`, if you're
    running the whole chapter in a single notebook. You can **re-train** this object
    to learn the relationship between this new feature and the response. You could
    even try a different train/test split, if you wanted to, without creating a new
    model object. The existing model object has been updated **in-place** in these scenarios.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你在一个单一的笔记本中运行整个章节，可以重新使用之前使用的模型对象`example_lr`。你可以**重新训练**这个对象，以学习这个新特征与响应之间的关系。如果你愿意，也可以尝试不同的训练/测试拆分，而不必创建新的模型对象。在这些场景中，现有的模型对象已经被**原地更新**。
- en: Create the array of predicted probabilities for the test data.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建测试数据的预测概率数组。
- en: 'Here is the code for this step:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是此步骤的代码：
- en: '[PRE11]py'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE11]py'
- en: Calculate the ROC AUC using the predicted probabilities and the true labels
    of the test data. Compare this to the ROC AUC from using the `EDUCATION` feature.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预测的概率和测试数据的真实标签计算ROC AUC。将其与使用`EDUCATION`特征的ROC AUC进行比较。
- en: 'Run this code for this step:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行以下代码进行这一步操作：
- en: '[PRE12]py'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE12]py'
- en: 'The output is as follows:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE13]py'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE13]py'
- en: Notice that we index the predicted probabilities array in order to get the predicted
    probability of the positive class from the second column. How does this compare
    to the ROC AUC from the `EDUCATION` logistic regression? The AUC is higher. This
    may be because now we are using a feature that has something to do with an account's
    financial status (credit limit), to predict something else related to the account's
    financial status (whether or not it will default), instead of using something
    less directly related to finances.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们对预测的概率数组进行了索引，以便从第二列获取正类的预测概率。与`EDUCATION`的逻辑回归ROC AUC相比，结果如何？AUC更高。这可能是因为现在我们使用的是与账户财务状况（信用额度）相关的特征，来预测与账户财务状况相关的另一项内容（是否违约），而不是使用与财务关系较弱的特征。
- en: Plot the ROC curve.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制ROC曲线。
- en: 'Here is the code to do this; it''s similar to the code we used in the previous exercise:'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是实现此功能的代码；它与我们在前一个练习中使用的代码类似：
- en: '[PRE14]py'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE14]py'
- en: 'The plot should appear as follows:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图形应如下所示：
- en: '![Figure 2.30: ROC curve for the LIMIT_BAL logistic regression'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.30：LIMIT_BAL 逻辑回归的ROC曲线'
- en: '](img/B16392_02_30.jpg)'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_02_30.jpg)'
- en: 'Figure 2.30: ROC curve for the LIMIT_BAL logistic regression'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.30：LIMIT_BAL 逻辑回归的ROC曲线
- en: 'This looks a little closer to an ROC curve that we''d like to see: it''s a
    bit further from the random chance line than the model using only `EDUCATION`.
    Also notice that the variation in pairs of true and false positive rates is a
    little smoother over the range of thresholds, reflective of the larger number
    of distinct values of the `LIMIT_BAL` feature.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这看起来有点像我们希望看到的ROC曲线：它比仅使用`EDUCATION`特征的模型更远离随机机会线。还注意到，真实和假阳性率的变化在阈值范围内更加平滑，反映了`LIMIT_BAL`特征具有更多不同的值。
- en: Calculate the data for the precision-recall curve on the test data using scikit-learn's
    functionality.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用scikit-learn的功能计算测试数据上精确度-召回率曲线的数据。
- en: 'Precision is often considered in tandem with recall. We can use `precision_recall_curve`
    in `sklearn.metrics` to automatically vary the threshold and calculate pairs of
    precision and recall values at each threshold value. Here is the code to retrieve
    these values, which is similar to `roc_curve`:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精确度通常与召回率一起考虑。我们可以使用`sklearn.metrics`中的`precision_recall_curve`来自动调整阈值，并计算每个阈值下的精确度和召回率对。以下是提取这些值的代码，类似于`roc_curve`：
- en: '[PRE15]py'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE15]py'
- en: 'Plot the precision-recall curve using matplotlib: we can do this with the following code.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用matplotlib绘制精确度-召回率曲线：我们可以通过以下代码实现这一点。
- en: 'Note that we put recall on the `x`-axis and precision on the `y`-axis, and
    we set the axes'' limits to the range [0, 1]:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们将召回率放在`x`轴，将精确度放在`y`轴，并将坐标轴的限制设置为[0, 1]范围：
- en: '[PRE16]py'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE16]py'
- en: '![Figure 2.31: Plot of the precision-recall curve'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.31：精确度-召回率曲线图'
- en: '](img/B16392_02_31.jpg)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_02_31.jpg)'
- en: 'Figure 2.31: Plot of the precision-recall curve'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.31：精确度-召回率曲线图
- en: Use scikit-learn to calculate the area under the precision-recall curve.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用scikit-learn计算精确度-召回率曲线下的面积。
- en: 'Here is the code for this:'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是实现此操作的代码：
- en: '[PRE17]py'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE17]py'
- en: 'You will obtain the following output:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '[PRE18]py'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE18]py'
- en: We saw that the precision-recall curve shows that precision is generally fairly
    low for this model; for nearly all of the range of thresholds, the precision,
    or portion of positive classifications that are correct, is less than half. We
    can calculate the area under the precision-recall curve as a way to compare this
    classifier with other models or feature sets we may consider.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们看到，精确率-召回率曲线表明，该模型的精确率通常相对较低；在几乎所有阈值范围内，精确率（即正确的正类分类所占比例）都不到一半。我们可以通过计算精确率-召回率曲线下面积来比较这个分类器与我们可能考虑的其他模型或特征集。
- en: 'Scikit-learn offers functionality for calculating an AUC for any set of `x-y`
    data, using the trapezoid rule, which you may recall from calculus: `metrics.auc`.
    We used this functionality to get the area under the precision-recall curve.'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Scikit-learn提供了一个计算任何`x-y`数据AUC的功能，使用的是梯形规则，你可能还记得这个方法来自微积分：`metrics.auc`。我们使用这个功能来获取精确率-召回率曲线下面积。
- en: Now recalculate the ROC AUC, except this time do it for the training data. How
    is this different, conceptually and quantitatively, from your earlier calculation?
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在重新计算ROC AUC，不过这次要计算训练数据的ROC AUC。这在概念上和定量上与之前的计算有何不同？
- en: 'First, we need to calculate predicted probabilities using the training data,
    as opposed to the test data. Then we can calculate the ROC AUC using the training
    data labels. Here is the code:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们需要使用训练数据而不是测试数据来计算预测概率。然后，我们可以使用训练数据标签来计算ROC AUC。以下是代码：
- en: '[PRE19]py'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE19]py'
- en: 'You should obtain the following output:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE20]py'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE20]py'
- en: Quantitatively, we can see that this AUC is not all that different from the
    test data ROC AUC we calculated earlier. Both are about 0.62\. Conceptually, what
    is the difference? When we calculate this metric on the training data, we are
    measuring the model's skill in predicting the same data that "taught" the model
    how to make predictions. We are seeing *how well the model fits the data*. On
    the other hand, test data metrics indicate performance on out-of-sample data the
    model hasn't "seen" before. If there was much of a difference in these scores,
    which usually would come in the form of a higher training score than the test
    score, it would indicate that while the model fits the data well, the trained
    model does not generalize well to new, unseen data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 定量来看，我们可以看到这个AUC与我们之前计算的测试数据ROC AUC差别不大。两者都大约是0.62。概念上，这有什么不同？当我们在训练数据上计算这个指标时，我们衡量的是模型在预测“教会”模型如何进行预测的相同数据上的能力。我们看到的是*模型如何拟合数据*。另一方面，测试数据的指标表示模型在未见过的外部样本数据上的表现。如果这些得分差异很大，通常表现为训练得分高于测试得分，那就意味着虽然模型很好地拟合了数据，但训练好的模型无法很好地泛化到新的、未见过的数据。
- en: In this case, the training and test scores are similar, meaning the model does
    about as well on out-of-sample data as it does on the same data used in model
    training. We will learn more about the insights we can gain by comparing training
    and test scores in *Chapter 4,* *The Bias-Variance Trade-Off*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，训练得分和测试得分相似，这意味着模型在训练数据和未见过的数据（外部样本数据）上的表现差不多。我们将在*第4章*，*偏差-方差权衡*中学习更多关于通过比较训练和测试得分可以获得的见解。
- en: 3\. Details of Logistic Regression and Feature Exploration
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 逻辑回归和特征探索的详细信息
- en: 'Activity 3.01: Fitting a Logistic Regression Model and Directly Using the Coefficients'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动3.01：拟合逻辑回归模型并直接使用系数
- en: '**Solution:**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**解答：**'
- en: 'The first few steps are similar to things we''ve done in previous activities:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前几个步骤与我们在之前的活动中做的类似：
- en: 'Create a train/test split (80/20) with `PAY_1` and `LIMIT_BAL` as features:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个训练/测试数据集（80/20），以`PAY_1`和`LIMIT_BAL`作为特征：
- en: '[PRE21]py'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE21]py'
- en: 'Import `LogisticRegression`, with the default options, but set the solver to `''liblinear''`:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`LogisticRegression`，使用默认选项，但将求解器设置为`'liblinear'`：
- en: '[PRE22]py'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE22]py'
- en: 'Train on the training data and obtain predicted classes, as well as class probabilities,
    using the test data:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上进行训练，并使用测试数据获取预测类别以及类别概率：
- en: '[PRE23]py'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE23]py'
- en: Pull out the coefficients and intercept from the trained model and manually
    calculate predicted probabilities. You'll need to add a column of ones to your
    features, to multiply by the intercept.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练好的模型中提取系数和截距，并手动计算预测概率。你需要在特征中添加一列值为1的列，以便与截距相乘。
- en: 'First, let''s create the array of features, with a column of ones added, using
    horizontal stacking:'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，让我们创建特征数组，添加一列1值，使用水平堆叠：
- en: '[PRE24]py'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE24]py'
- en: 'Now we need the intercept and coefficients, which we reshape and concatenate
    from scikit-learn output:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们需要截距和系数，我们将从 scikit-learn 输出中重新调整形状并连接它们：
- en: '[PRE25]py'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE25]py'
- en: 'To repeatedly multiply the intercept and coefficients by all the rows of `ones_and_features`,
    and take the sum of each row (that is, find the linear combination), you could
    write this all out using multiplication and addition. However, it''s much faster
    to use the dot product:'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了反复将截距和系数乘以 `ones_and_features` 的所有行，并求每行的和（也就是求线性组合），你可以使用乘法和加法将这些全部写出来。不过，使用点积会更快：
- en: '[PRE26]py'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE26]py'
- en: 'Now `X_lin_comb` has the argument we need to pass to the sigmoid function we
    defined, in order to calculate predicted probabilities:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在 `X_lin_comb` 包含了我们需要传递给我们定义的 sigmoid 函数的参数，以计算预测概率：
- en: '[PRE27]py'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE27]py'
- en: Using a threshold of `0.5`, manually calculate predicted classes. Compare this
    to the class predictions outputted by scikit-learn.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `0.5` 的阈值，手动计算预测的类别。与 scikit-learn 输出的类别预测进行比较。
- en: 'The manually predicted probabilities, `y_pred_proba_manual`, should be the
    same as `y_pred_proba`; we''ll check that momentarily. First, manually predict
    the classes with the threshold:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 手动预测的概率 `y_pred_proba_manual` 应该与 `y_pred_proba` 相同，我们马上检查这一点。首先，使用阈值手动预测类别：
- en: '[PRE28]py'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE28]py'
- en: 'This array will have a different shape than `y_pred`, but it should contain
    the same values. We can check whether all the elements of two arrays are equal
    like this:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个数组的形状将与 `y_pred` 不同，但它应该包含相同的值。我们可以像这样检查两个数组的所有元素是否相等：
- en: '[PRE29]py'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE29]py'
- en: This should return a logical `True` if the arrays are equal.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果数组相等，这应该返回一个逻辑值`True`。
- en: Calculate ROC AUC using both scikit-learn's predicted probabilities and your
    manually predicted probabilities, and compare them.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的预测概率和手动预测的概率计算 ROC AUC，并进行比较。
- en: 'First, import the following:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，导入以下内容：
- en: '[PRE30]py'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE30]py'
- en: 'Then, calculate this metric on both versions, taking care to access the correct
    column, or reshape as necessary:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，在两个版本上计算此指标，确保访问正确的列，或根据需要调整形状：
- en: '![Figure 3.37: Calculating the ROC AUC from predicted probabilities'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.37：从预测概率计算 ROC AUC'
- en: '](img/B16392_03_37.jpg)'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_03_37.jpg)'
- en: 'Figure 3.37: Calculating the ROC AUC from predicted probabilities'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.37：从预测概率计算 ROC AUC
- en: 'The AUCs are, in fact, the same. What have we done here? We''ve confirmed that
    all we really need from this fitted scikit-learn model is three numbers: the intercept
    and the two coefficients. Once we have these, we could create model predictions
    using a few lines of code, with mathematical functions, that are equivalent to
    the predictions directly made from scikit-learn.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，AUC 是相同的。我们在这里做了什么？我们已经确认，从这个拟合的 scikit-learn 模型中，我们实际上只需要三个数字：截距和两个系数。一旦我们得到了这些，就可以使用几行代码，借助数学函数，来创建模型预测，这与直接从
    scikit-learn 生成的预测是等效的。
- en: This is good to confirm your understanding, but otherwise, why would you ever
    want to do this? We'll talk about **model deployment** in the final chapter. However,
    depending on your circumstances, you may be in a situation where you don't have
    access to Python in the environment where new features will need to be input into
    the model for prediction. For example, you may need to make predictions entirely
    in SQL. While this is a limitation in general, with logistic regression you can
    use mathematical functions that are available in SQL to re-create the logistic
    regression prediction, only needing to copy and paste the intercept and coefficients
    somewhere in your SQL code. The dot product may not be available, but you can
    use multiplication and addition to accomplish the same purpose.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于确认你对知识的理解，但除此之外，你为什么要这么做呢？我们将在最后一章讨论**模型部署**。不过，根据你的具体情况，你可能会遇到一种情形，在其中你没有
    Python 环境来为模型输入新的特征进行预测。例如，你可能需要完全在 SQL 中进行预测。虽然这在一般情况下是一个限制，但使用逻辑回归时，你可以利用 SQL
    中可用的数学函数重新创建逻辑回归预测，只需要将截距和系数粘贴到 SQL 代码的某个地方即可。点积可能不可用，但你可以使用乘法和加法来实现相同的目的。
- en: 'Now, what about the results themselves? What we''ve seen here is that we can
    slightly boost model performance above our previous efforts: using just `LIMIT_BAL`
    as a feature in the previous chapter''s activity, the ROC AUC was a bit less at
    0.62, instead of 0.63 here. In the next chapter, we''ll learn advanced techniques
    with logistic regression that we can use to further improve performance.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，结果如何呢？我们看到的是，通过稍微提高模型的表现，我们可以超过之前的尝试：在上一章的活动中，仅使用 `LIMIT_BAL` 作为特征时，ROC AUC
    稍低为 0.62，而此处为 0.63。下一章我们将学习使用逻辑回归的高级技术，进一步提升性能。
- en: 4\. The Bias-Variance Trade-Off
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 偏差-方差权衡
- en: 'Activity 4.01: Cross-Validation and Feature Engineering with the Case Study
    Data'
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 4.01：使用案例研究数据进行交叉验证和特征工程
- en: '**Solution:**'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**解答：**'
- en: Select out the features from the DataFrame of the case study data.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从案例研究数据的 DataFrame 中选择特征。
- en: 'You can use the list of feature names that we''ve already created in this chapter,
    but be sure not to include the response variable, which would be a very good (but
    entirely inappropriate) feature:'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用我们在本章中已经创建的特征名称列表，但请确保不要包括响应变量，它本可以是一个非常好的（但完全不合适的）特征：
- en: '[PRE31]py'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE31]py'
- en: 'Make a training/test split using a random seed of 24:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机种子 24 进行训练/测试集拆分：
- en: '[PRE32]py'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE32]py'
- en: We'll use this going forward and reserve this test data as the unseen test set.
    By specifying the random seed, we can easily create separate notebooks with other
    modeling approaches using the same training data.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将在后续中使用这个数据，并将其作为未见测试集进行保留。通过指定随机种子，我们可以轻松创建包含其他建模方法的独立笔记本，使用相同的训练数据。
- en: 'Instantiate `MinMaxScaler` to scale the data, as shown in the following code:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化 `MinMaxScaler` 来缩放数据，如下所示的代码：
- en: '[PRE33]py'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE33]py'
- en: 'Instantiate a logistic regression model with the `saga` solver, L1 penalty,
    and set `max_iter` to `1000`, as we''d like to allow the solver enough iterations
    to find a good solution:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个逻辑回归模型，使用 `saga` 求解器，L1 惩罚，并将 `max_iter` 设置为 `1000`，因为我们希望允许求解器有足够的迭代次数来找到一个好的解：
- en: '[PRE34]py'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE34]py'
- en: 'Import the `Pipeline` class and create a pipeline with the scaler and the logistic
    regression model, using the names `''scaler''` and `''model''` for the steps, respectively:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `Pipeline` 类并创建一个包含缩放器和逻辑回归模型的管道，分别使用 `'scaler'` 和 `'model'` 作为步骤的名称：
- en: '[PRE35]py'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE35]py'
- en: 'Use the `get_params` and `set_params` methods to see how to view the parameters
    from each stage of the pipeline and change them (execute each of the following
    lines in a separate cell in your notebook and observe the output):'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `get_params` 和 `set_params` 方法查看每个阶段的参数，并更改它们（在你的笔记本中分别执行以下每行代码并观察输出）：
- en: '[PRE36]py'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE36]py'
- en: 'Create a smaller range of *C* values to test with cross-validation, as these
    models will take longer to train and test with more data than our previous exercise;
    we recommend *C = [10*2*, 10, 1, 10*-1*, 10*-2*, 10*-3*]*:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个较小范围的 *C* 值进行交叉验证测试，因为这些模型相比我们之前的练习，在训练和测试时需要更多的数据，因此训练时间会更长；我们推荐的 *C =
    [10*2*, 10, 1, 10*-1*, 10*-2*, 10*-3*]*：
- en: '[PRE37]py'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE37]py'
- en: Make a new version of the `cross_val_C_search` function, called `cross_val_C_search_pipe`.
    Instead of the `model` argument, this function will take a `pipeline` argument.
    The changes inside the function will be to set the *C* value using `set_params(model__C
    = <value you want to test>)` on the pipeline, replacing the model with the pipeline
    for the `fit` and `predict_proba` methods, and accessing the *C* value using `pipeline.get_params()['model__C']`
    for the printed status update.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `cross_val_C_search` 函数的新版本，命名为 `cross_val_C_search_pipe`。这个函数将接受一个 `pipeline`
    参数，而不是 `model` 参数。函数内的更改是使用 `set_params(model__C = <你想测试的值>)` 设置 *C* 值，替换模型为管道，并在
    `fit` 和 `predict_proba` 方法中使用管道，且通过 `pipeline.get_params()['model__C']` 获取 *C*
    值以打印状态更新。
- en: 'The changes are as follows:'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更改如下：
- en: '[PRE38]py'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE38]py'
- en: Note
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: For the complete code, refer to [https://packt.link/AsQmK](https://packt.link/AsQmK).
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关完整的代码，请参阅 [https://packt.link/AsQmK](https://packt.link/AsQmK)。
- en: 'Run this function as in the previous exercise, but using the new range of *C*
    values, the pipeline you created, and the features and response variable from
    the training split of the case study data. You may see warnings here, or in later
    steps, regarding the non-convergence of the solver; you could experiment with
    the `tol` or `max_iter` options to try and achieve convergence, although the results
    you obtain with `max_iter = 1000` are likely to be sufficient. Here is the code
    to do this:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照上一个练习的方式运行这个函数，但使用新的*C*值范围、你创建的管道以及从案例研究数据训练集分割得到的特征和响应变量。你可能会在此或后续步骤中看到关于求解器未收敛的警告；你可以尝试调整`tol`或`max_iter`选项以尝试实现收敛，尽管使用`max_iter
    = 1000`时得到的结果应该是足够的。以下是执行此操作的代码：
- en: '[PRE39]py'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE39]py'
- en: 'You will obtain the following output:'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '[PRE40]py'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE40]py'
- en: 'Plot the average training and test ROC AUC across folds, for each *C* value,
    using the following code:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码绘制每个*C*值下，跨折叠的平均训练和测试ROC AUC：
- en: '[PRE41]py'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE41]py'
- en: 'You will obtain the following output:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '![Figure 4.25: Cross-validation test performance'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.25：交叉验证测试性能'
- en: '](img/B16392_04_25.jpg)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_04_25.jpg)'
- en: 'Figure 4.25: Cross-validation test performance'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.25：交叉验证测试性能
- en: 'You should notice that regularization does not impart much benefit here, as
    may be expected: for lower *C* values, which correspond to stronger regularization,
    model testing (as well as training) performance decreases. While we are able to
    increase model performance over our previous efforts by using all the features
    available, it appears there is no overfitting going on. Instead, the training
    and test scores are about the same. Instead of overfitting, it''s possible that
    we may be underfitting. Let''s try engineering some interaction features to see
    if they can improve performance.'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该注意到，正则化在这里并没有带来太多好处，正如预期的那样：对于较低的*C*值，即较强的正则化，模型的测试（以及训练）性能下降。虽然通过使用所有可用特征，我们能够提高模型的性能，但似乎并没有出现过拟合。相反，训练和测试分数差不多。与其说是过拟合，不如说我们可能存在欠拟合的情况。让我们尝试构建一些交互特征，看看它们是否能提升性能。
- en: 'Create interaction features for the case study data and confirm that the number
    of new features makes sense using the following code:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码为案例研究数据创建交互特征，并确认新特征的数量是合理的：
- en: '[PRE42]py'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE42]py'
- en: 'You will obtain the following output:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '[PRE43]py'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE43]py'
- en: From this you should see the new number of features is 153, which is *17 + "17
    choose 2" = 17 + 136 = 153*. The *"17 choose 2"* part comes from choosing all
    possible combinations of 2 features to interact from the 17 original features.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从中你应该看到新特征的数量是153，计算方式是*17 + "17选2" = 17 + 136 = 153*。*“17选2”*部分来自于从17个原始特征中选择所有可能的两个特征的组合进行交互。
- en: Repeat the cross-validation procedure and observe the model performance when
    using interaction features; that is, repeat *steps 9* and *10*. Note that this
    will take substantially more time, due to the larger number of features, but it
    will probably take less than 10 minutes.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复交叉验证过程，并观察使用交互特征时模型的表现；也就是说，重复*步骤9*和*步骤10*。请注意，由于特征数量增加，这将需要更多时间，但应该不会超过10分钟。
- en: 'You will obtain the following output:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '![Figure 4.26: Improved cross-validation test performance from adding interaction
    features'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.26：通过添加交互特征改善的交叉验证测试性能'
- en: '](img/B16392_04_26.jpg)'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_04_26.jpg)'
- en: 'Figure 4.26: Improved cross-validation test performance from adding interaction
    features'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.26：通过添加交互特征改善的交叉验证测试性能
- en: So, does the average cross-validation test performance improve with the interaction
    features? Is regularization useful?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，交互特征是否改善了平均交叉验证测试性能？正则化是否有用？
- en: Engineering the interaction features increases the best model test score to
    about *ROC AUC = 0.74* on average across the folds, from about 0.72 without including
    interactions. These scores happen at *C = 100*, that is, with negligible regularization.
    On the plot of training versus test scores for the model with interactions, you
    can see that the training score is a bit higher than the test score, so it could
    be said that some amount of overfitting is going on. However, we cannot increase
    the test score through regularization here, so this may not be a problematic instance
    of overfitting. In most cases, whatever strategy yields the highest test score
    is the best strategy.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 构造交互特征使得最佳模型测试分数在各折叠之间平均约为*ROC AUC = 0.74*，而没有交互特征时大约是0.72。这些分数出现在*C = 100*时，即几乎没有正则化。在包含交互特征的模型的训练与测试分数对比图上，你可以看到训练分数稍高于测试分数，因此可以说存在一定程度的过拟合。然而，在这里我们无法通过正则化来提高测试分数，因此这可能不是过拟合的一个问题实例。在大多数情况下，任何能够产生最高测试分数的策略就是最佳策略。
- en: In summary, adding interaction features improved cross-validation performance,
    and regularization appears not to be useful for the case study at this point,
    using a logistic regression model. We will reserve the step of fitting on all
    the training data for later when we've tried other models in cross-validation
    to find the best model.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，添加交互特征提高了交叉验证的表现，而正则化在目前使用逻辑回归模型的案例中似乎并不有用。我们将在稍后进行全量训练数据拟合的步骤，当我们尝试其他模型并在交叉验证中找到最佳模型时再进行。
- en: 5\. Decision Trees and Random Forests
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 决策树与随机森林
- en: 'Activity 5.01: Cross-Validation Grid Search with Random Forest'
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 5.01：使用随机森林进行交叉验证网格搜索
- en: '**Solution:**'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案：**'
- en: 'Create a dictionary representing the grid for the `max_depth` and `n_estimators`
    hyperparameters that will be searched. Include depths of 3, 6, 9, and 12, and
    10, 50, 100, and 200 trees. Leave the other hyperparameters at their defaults.
    Create the dictionary using this code:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个字典，表示将要搜索的`max_depth`和`n_estimators`超参数的网格。包括深度为3、6、9和12，以及树的数量为10、50、100和200。保持其他超参数为默认值。使用以下代码创建字典：
- en: '[PRE44]py'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE44]py'
- en: Note
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: There are many other possible hyperparameters to search over. In particular,
    the scikit-learn documentation for random forest indicates that "The main parameters
    to adjust when using these methods are `n_estimators` and `max_features`" and
    that "Empirical good default values are … `max_features=sqrt(n_features)` for
    classification tasks."
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 还有许多其他可能的超参数需要搜索。特别是，scikit-learn文档中关于随机森林指出：“使用这些方法时，主要调整的参数是`n_estimators`和`max_features`”，并且“经验上良好的默认值是……分类任务时，`max_features=sqrt(n_features)`。”
- en: 'Source: https://scikit-learn.org/stable/modules/ensemble.html#parameters'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 来源：https://scikit-learn.org/stable/modules/ensemble.html#parameters
- en: For the purposes of this book, we will use `max_features='auto'` (which is equal
    to `sqrt(n_features)`) and limit our exploration to `max_depth` and `n_estimators`
    for the sake of a shorter runtime. In a real-world situation, you should explore
    other hyperparameters according to how much computational time you can afford.
    Remember that in order to search in especially large parameter spaces, you can
    use `RandomizedSearchCV` to avoid exhaustively calculating metrics for every combination
    of hyperparameters in the grid.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 出于本书的目的，我们将使用`max_features='auto'`（等同于`sqrt(n_features)`）并将探索限制在`max_depth`和`n_estimators`上，以缩短运行时间。在实际情况中，你应根据可承受的计算时间探索其他超参数。记住，为了在特别大的参数空间中搜索，你可以使用`RandomizedSearchCV`，以避免为网格中每个超参数组合计算所有指标。
- en: 'Instantiate a `GridSearchCV` object using the same options that we have previously
    used in this chapter, but with the dictionary of hyperparameters created in step
    1 here. Set `verbose=2` to see the output for each fit performed. You can reuse
    the same random forest model object, `rf`, that we have been using or create a
    new one. Create a new random forest object and instantiate the `GridSearchCV`
    class using this code:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们在本章之前使用的相同选项实例化一个`GridSearchCV`对象，但使用步骤1中创建的超参数字典。设置`verbose=2`以查看每次拟合的输出。你可以重用我们一直在使用的相同随机森林模型对象`rf`，或者创建一个新的。创建一个新的随机森林对象，并使用以下代码实例化`GridSearchCV`类：
- en: '[PRE45]py'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE45]py'
- en: 'Fit the `GridSearchCV` object on the training data. Perform the grid search
    using this code:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上拟合`GridSearchCV`对象。使用以下代码执行网格搜索：
- en: '[PRE46]py'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE46]py'
- en: 'Because we chose the `verbose=2` option, you will see a relatively large amount
    of output in the notebook. There will be output for each combination of hyperparameters
    and, for each fold, as it is fitted and tested. Here are the first few lines of
    output:'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们选择了`verbose=2`选项，你将看到笔记本中相对较多的输出。每个超参数组合都会有输出，并且对于每个折叠，都有拟合和测试的输出。以下是输出的前几行：
- en: '![Figure 5.22: The verbose output from cross-validation'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.22：交叉验证的冗长输出'
- en: '](img/B16392_05_22.jpg)'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_05_22.jpg)'
- en: 'Figure 5.22: The verbose output from cross-validation'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.22：交叉验证的冗长输出
- en: While it's not necessary to see all this output for shorter cross-validation
    procedures, for longer ones, it can be reassuring to see that the cross-validation
    is working and to give you an idea of how long the fits are taking for various
    combinations of hyperparameters. If things are taking too long, you may want to
    interrupt the kernel by pushing the stop button (square) at the top of the notebook
    and choosing hyperparameters that will take less time to run, or use a more limited
    set of hyperparameters.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然在较短的交叉验证过程中不必查看所有这些输出，但对于较长的交叉验证，看到这些输出可以让你确认交叉验证正在进行，并且能给你一些关于不同超参数组合下拟合所需时间的概念。如果某些操作花费的时间过长，你可能需要通过点击笔记本顶部的停止按钮（方形）中断内核，并选择一些运行时间更短的超参数，或者使用一个更有限的超参数集。
- en: 'When this is all done, you should see the following output:'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当这一切完成时，你应该会看到以下输出：
- en: '![Figure 5.22: The cross-validation output upon completion'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.22：交叉验证完成后的输出'
- en: '](img/B16392_05_23.jpg)'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_05_23.jpg)'
- en: 'Figure 5.23: The cross-validation output upon completion'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.23：交叉验证完成后的输出
- en: This cross-validation job took about 2 minutes to run. As your jobs grow, you
    may wish to explore parallel processing with the `n_jobs` parameter to see whether
    it's possible to speed up the search. Using `n_jobs=-1` for parallel processing,
    you should be able to achieve shorter runtimes than with serial processing. However,
    with parallel processing, you won't be able to see the output of each individual
    model fitting operation, as shown in *Figure 5.23*.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个交叉验证任务大约运行了 2 分钟。随着任务规模的增大，你可能希望通过`n_jobs`参数探索并行处理，以查看是否能够加速搜索。使用`n_jobs=-1`进行并行处理时，运行时间应该比串行处理更短。然而，在并行处理中，你将无法看到每个单独模型拟合操作的输出，如*图
    5.23*所示。
- en: 'Put the results of the grid search in a pandas DataFrame. Use this code to
    put the results in a DataFrame:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将网格搜索结果放入 pandas DataFrame 中。使用以下代码将结果放入 DataFrame：
- en: '[PRE47]py'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE47]py'
- en: 'Create a `pcolormesh` visualization of the mean testing score for each combination
    of hyperparameters. Here is the code to create a mesh graph of cross-validation
    results. It''s similar to the example graph that we created previously, but with
    annotation that is specific to the cross-validation we performed here:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`pcolormesh`可视化图，显示每个超参数组合的平均测试分数。以下是创建交叉验证结果网格图的代码。它与我们之前创建的示例图类似，但带有特定于此处执行的交叉验证的注释：
- en: '[PRE48]py'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE48]py'
- en: 'The main change from our previous example is that instead of plotting the integers
    from 1 to 16, we''re plotting the mean testing scores that we''ve retrieved and
    reshaped with `cv_rf_results_df[''mean_test_score''].values.reshape((4,4))`. The
    other new things here are that we are using list comprehensions to create lists
    of strings for tick labels, based on the numerical values of hyperparameters in
    the grid. We access them from the dictionary that we defined, and then convert
    them individually to the `str` (string) data type within the list comprehension,
    for example, `ax_rf.set_xticklabels([str(tick_label) for tick_label in rf_params[''n_estimators'']])`.
    We have already set the tick locations to the places where we want the ticks using
    `set_xticks`. Also, we make a square-shaped graph using `ax_rf.set_aspect(''equal'')`.
    The graph should appear as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们与之前示例的主要区别在于，我们不是绘制从 1 到 16 的整数，而是绘制我们通过`cv_rf_results_df['mean_test_score'].values.reshape((4,4))`检索并重塑的平均测试分数。这里的另一个新内容是，我们使用列表推导式基于网格中超参数的数值，创建字符串的列表作为刻度标签。我们从定义的字典中访问这些数值，然后在列表推导式中将它们逐一转换为`str`（字符串）数据类型，例如，`ax_rf.set_xticklabels([str(tick_label)
    for tick_label in rf_params['n_estimators']])`。我们已经使用`set_xticks`将刻度位置设置为我们希望显示刻度的位置。此外，我们使用`ax_rf.set_aspect('equal')`创建了一个正方形图形。图形应该如下所示：
- en: '![Figure 5.24: Results of cross-validation of a random forest'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.24：对随机森林进行交叉验证的结果'
- en: over a grid with two hyperparameters
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在包含两个超参数的网格上
- en: '](img/B16392_05_24.jpg)'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_05_24.jpg)'
- en: 'Figure 5.24: Results of cross-validation of a random forest over a grid with
    two hyperparameters'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.24：在两个超参数的网格上进行随机森林交叉验证的结果
- en: Conclude which set of hyperparameters to use.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定使用哪一组超参数。
- en: What can we conclude from our grid search? There certainly seems to be an advantage
    to using trees with a depth of more than 3\. Of the parameter combinations that
    we tried, `max_depth=9` with 200 trees yields the best average testing score,
    which you can look up in the DataFrame and confirm is ROC AUC = 0.776\.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从我们的网格搜索中可以得出什么结论？显然，使用深度大于3的树有一定的优势。在我们尝试的参数组合中，`max_depth=9`且树木数量为200的组合提供了最佳的平均测试得分，你可以在数据框中查找并确认其ROC
    AUC = 0.776。
- en: This is the best model we've found from all of our efforts so far.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是我们迄今为止所有努力中找到的最佳模型。
- en: In a real-world scenario, we'd likely do a more thorough search. Some good next
    steps would be to try a larger number of trees and not spend any more time with
    `n_estimators` < 200, since we know that we need at least 200 trees to get the
    best performance. You may search a more granular space of `max_depth` instead
    of jumping by 3s, as we've done here, and try a couple of other hyperparameters,
    such as `max_features`. For our purposes, however, we will assume that we've found
    the optimal hyperparameters here and move forward..
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在实际场景中，我们可能会进行更彻底的搜索。接下来的好步骤是尝试更多的树木数量，并且不再花费时间在`n_estimators` < 200上，因为我们知道至少需要200棵树才能获得最佳性能。你还可以更精细地搜索`max_depth`的空间，而不是像我们这里一样按3递增，并尝试其他超参数，比如`max_features`。不过为了我们的目的，我们将假设已经找到了最佳超参数并继续前进。
- en: 6\. Gradient Boosting, XGBoost, and SHAP Values
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6. 梯度提升、XGBoost和SHAP值
- en: 'Activity 6.01: Modeling the Case Study Data with XGBoost and Explaining the Model
    with SHAP'
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动6.01：使用XGBoost建模案例研究数据并通过SHAP解释模型
- en: '**Solution:**'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案：**'
- en: In this activity, we'll take what we've learned in this chapter with a synthetic
    dataset and apply it to the case study data. We'll see how an XGBoost model performs
    on a validation set and explain the model predictions using SHAP values. We have
    prepared the dataset for this activity by replacing the samples that had missing
    values for the `PAY_1` feature, that we had previously ignored, while maintaining
    the same train/test split for the samples with no missing values. You can see
    how the data was prepared in the Appendix to the notebook for this activity.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，我们将结合本章学习的内容，使用合成数据集并将其应用到案例研究数据中。我们将观察XGBoost模型在验证集上的表现，并使用SHAP值解释模型预测。我们已经通过替换之前忽略的`PAY_1`特征缺失值的样本来准备该数据集，同时保留了没有缺失值的样本的训练/测试分割。你可以在本活动的笔记本附录中查看数据是如何准备的。
- en: 'Load the case study data that has been prepared for this exercise. The file
    path is `../../Data/Activity_6_01_data.pkl` and the variables are: `features_response,
    X_train_all, y_train_all, X_test_all, y_test_all`:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载为本次练习准备的案例研究数据。文件路径为`../../Data/Activity_6_01_data.pkl`，变量包括：`features_response,
    X_train_all, y_train_all, X_test_all, y_test_all`：
- en: '[PRE49]py'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE49]py'
- en: 'Define a validation set to train XGBoost with early stopping:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个验证集，用于训练XGBoost并进行提前停止：
- en: '[PRE50]py'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE50]py'
- en: 'Instantiate an XGBoost model. We''ll use the `lossguide` grow policy and examine
    validation set performance for several values of `max_leaves`:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个XGBoost模型。我们将使用`lossguide`增长策略，并检查不同`max_leaves`值下的验证集表现：
- en: '[PRE51]py'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE51]py'
- en: 'Search values of `max_leaves` from 5 to 200, counting by 5''s:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索`max_leaves`的值，从5到200，步长为5：
- en: '[PRE52]py'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE52]py'
- en: 'Create the evaluation set for early stopping:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建用于提前停止的评估集：
- en: '[PRE53]py'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE53]py'
- en: 'Loop through hyperparameter values and create a list of validation ROC AUCs,
    using the same technique as in *Exercise 6.01: Randomized Grid Search for Tuning
    XGBoost Hyperparameters*:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历超参数值，并创建一个验证集的ROC AUC列表，采用与*练习6.01：随机化网格搜索调优XGBoost超参数*相同的方法：
- en: '[PRE54]py'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE54]py'
- en: 'Create a data frame of the hyperparameter search results and plot the validation
    AUC against `max_leaves`:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数据框，记录超参数搜索结果，并绘制验证AUC与`max_leaves`的关系：
- en: '[PRE55]py'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE55]py'
- en: 'The plot should look something like this:'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图表应该看起来像这样：
- en: '![Figure 6.15: Validation AUC versus max_leaves for the case study data'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.15：案例研究数据中验证AUC与max_leaves的关系'
- en: '](img/B16392_06_15.jpg)'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_06_15.jpg)'
- en: 'Figure 6.15: Validation AUC versus `max_leaves` for the case study data'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.15：案例研究数据中验证AUC与`max_leaves`的关系
- en: Although the relationship is somewhat noisy, we see that in general, lower values
    of `max_leaves` result in a higher validation set ROC AUC. This is because limiting
    the complexity of trees by allowing fewer leaves results in less overfitting,
    and increases the validation set score.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管关系有些噪声，我们可以看到，通常较低的`max_leaves`值会导致较高的验证集ROC AUC。这是因为通过允许较少的叶子来限制树的复杂度，从而减少了过拟合，增加了验证集得分。
- en: 'Observe the number of `max_leaves` corresponding to the highest ROC AUC on
    the validation set:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察`max_leaves`对应于验证集上最高ROC AUC的数量：
- en: '[PRE56]py'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE56]py'
- en: 'The result should be as follows:'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应如下所示：
- en: '![Figure 6.16: Optimal max_leaves and validation set AUC for the case study
    data'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.16：案例研究数据的最佳max_leaves和验证集AUC'
- en: '](img/B16392_06_16.jpg)'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_06_16.jpg)'
- en: 'Figure 6.16: Optimal `max_leaves` and validation set AUC for the case study
    data'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.16：案例研究数据的最佳`max_leaves`和验证集AUC
- en: We would like to interpret these results in light of our previous efforts in
    modeling the case study data. This is not a perfect comparison, because here we
    have missing values in the training and validation data, while previously we ignored
    them, and here we only have one validation set, as opposed to the k-folds cross-validation
    used earlier (although the interested reader could try using k-folds cross-validation
    for multiple training/validation splits in XGBoost with early stopping).
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们希望结合之前在建模案例数据方面的努力来解释这些结果。这不是一个完美的对比，因为这里我们在训练和验证数据中有缺失值，而之前我们忽略了这些缺失值，并且这里只有一个验证集，而不像之前使用的k折交叉验证（尽管有兴趣的读者可以尝试在XGBoost中使用k折交叉验证进行多个训练/验证分割，并加上早停）。
- en: However, even given these limitations, the validation results here should provide
    a measure of out-of-sample performance similar to the k-folds cross-validation
    we performed earlier. We note that the validation ROC AUC here of 0.779 here is
    a bit higher than the 0.776 obtained previously with random forest in *Activity
    5.01*, *Cross-Validation Grid Search with Random Forest*, from *Chapter 5, Decision
    Trees and Random Forests*. These validation scores are fairly similar and it would
    probably be fine to use either model in practice. We'll now move forward with
    the XGBoost model.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，即使考虑到这些限制，下面的验证结果应提供一个类似于我们之前进行的k折交叉验证的样本外表现度量。我们注意到，这里的验证ROC AUC值为0.779，比之前在*活动5.01*中使用随机森林获得的0.776略高，*随机森林的交叉验证网格搜索*，出自*第5章，决策树与随机森林*。这些验证分数相当接近，实际上使用这两种模型都应该是可以的。接下来我们将继续使用XGBoost模型。
- en: 'Refit the XGBoost model with the optimal hyperparameter:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳超参数重新拟合XGBoost模型：
- en: '[PRE57]py'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE57]py'
- en: 'So that we can examine SHAP values for the validation set, make a data frame
    of this data:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了能够检查验证集的SHAP值，我们需要将此数据做成一个数据框：
- en: '[PRE58]py'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE58]py'
- en: 'Create an SHAP explainer for our new model using the validation data as the
    background dataset, obtain the SHAP values, and make a summary plot:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个SHAP解释器，使用验证数据作为背景数据集，获取SHAP值，并制作一个摘要图：
- en: '[PRE59]py'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE59]py'
- en: 'The plot should look like this:'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图应如下所示：
- en: '![Figure 6.17: SHAP values for the XGBoost model of the case study data on
    the validation set'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.17：案例研究数据在验证集上的XGBoost模型SHAP值'
- en: '](img/B16392_06_17.jpg)'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_06_17.jpg)'
- en: 'Figure 6.17: SHAP values for the XGBoost model of the case study data on the
    validation set'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.17：案例研究数据在验证集上的XGBoost模型SHAP值
- en: From *Figure 6.17*, we can see that the most important features in the XGBoost
    model are somewhat different from those in the random forest model we explored
    in *Chapter 5*, *Decision Trees and Random Forests* (*Figure 5.15*). No longer
    is `PAY_1` the most important feature, although it is still quite important at
    number 3\. `LIMIT_BAL`, the borrower's credit limit, is now the most important
    feature. This makes sense as an important feature as the lender has likely based
    the credit limit on how risky a borrower is, so it should be a good predictor
    of the risk of default.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从*图6.17*中，我们可以看到，XGBoost模型中最重要的特征与我们在*第5章，决策树与随机森林*中探索的随机森林模型中的特征略有不同（参见*图5.15*）。`PAY_1`不再是最重要的特征，尽管它仍然在第三位，仍然很重要。现在，`LIMIT_BAL`（借款人的信用额度）是最重要的特征。这个结果是合理的，因为贷方可能根据借款人的风险来设定信用额度，因此它应该是一个很好的违约风险预测因子。
- en: Let's explore whether `LIMIT_BAL` has any interesting SHAP interactions with
    other features. Instead of specifying which feature to color the scatter plot
    by, we can let the `shap` package pick the feature that has the most interaction
    by not indexing the explainer object for the color argument.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们探索`LIMIT_BAL`是否与其他特征有任何有趣的SHAP交互。我们可以通过不为颜色参数指定特征，允许`shap`包自动选择与其他特征交互最强的特征，从而绘制散点图。
- en: 'Make a scatter plot of `LIMIT_BAL` SHAP values, colored by the feature with
    the strongest interaction:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制`LIMIT_BAL`的SHAP值散点图，按最强交互特征进行着色：
- en: '[PRE60]py'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE60]py'
- en: 'The plot should look like this:'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该图应如下所示：
- en: '![Figure 6.18: Scatter plot of SHAP values of LIMIT_BAL and the'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.18：LIMIT_BAL和其他特征的SHAP值散点图'
- en: feature with the strongest interaction
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与特征最强交互的特征
- en: '](img/B16392_06_18.jpg)'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_06_18.jpg)'
- en: 'Figure 6.18: Scatter plot of SHAP values of `LIMIT_BAL` and the feature with
    the strongest interaction'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.18：`LIMIT_BAL`和与之有最强交互特征的SHAP值散点图
- en: '`BILL_AMT2`, the amount of the bill from two months previous, has the strongest
    interaction with `LIMIT_BAL`. We can see that for most values of `LIMIT_BAL`,
    if the bill was particularly high, this leads to more positive SHAP values, meaning
    an increased risk of default. This can be observed by noting that most of the
    reddest colored dots appear along the top of the band of dots in *Figure 6.18*.
    This makes intuitive sense: even if a borrower was given a large credit limit,
    if their bill becomes very large, this may signal an increased risk of default.'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`BILL_AMT2`，即两个月前的账单金额，和`LIMIT_BAL`的交互最强。我们可以看到，对于大多数`LIMIT_BAL`的值，如果账单特别高，这会导致更高的SHAP值，意味着违约风险增加。通过观察*图
    6.18*中最红的点分布，可以发现这些点集中在点带的顶部。这符合直觉：即使借款人获得了较大的信用额度，如果账单金额过大，也可能意味着违约风险增加。'
- en: Finally, we will save the model along with the training and test data for analysis
    and delivery to our business partner. We accomplish this using Python's `pickle` functionality.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们将保存模型以及训练和测试数据以供分析，并交付给我们的商业伙伴。我们通过使用Python的`pickle`功能来完成此操作。
- en: 'Save the trained model along with the training and test data to a file:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练好的模型以及训练和测试数据保存到文件中：
- en: '[PRE61]py'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE61]py'
- en: 7\. Test Set Analysis, Financial Insights, and Delivery to the Client
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7. 测试集分析、财务洞察和交付给客户
- en: 'Activity 7.01: Deriving Financial Insights'
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 7.01：推导财务洞察
- en: '**Solution:**'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案：**'
- en: Using the testing set, calculate the cost of all defaults if there were no counseling program.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用测试集，计算如果没有辅导程序的所有违约成本。
- en: 'Use this code for the calculation:'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用此代码进行计算：
- en: '[PRE62]py'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE62]py'
- en: 'The output should be this:'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应为：
- en: '[PRE63]py'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE63]py'
- en: Calculate by what percent the cost of defaults can be decreased by the counseling
    program.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算辅导项目可以降低违约成本的百分比。
- en: 'The potential decrease in the cost of default is the greatest possible net
    savings of the counseling program, divided by the cost of all defaults in the
    absence of a program:'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 违约成本的潜在降低是辅导项目可能带来的最大净节省量，除以在没有辅导项目情况下所有违约的成本：
- en: '[PRE64]py'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE64]py'
- en: 'The output should be this:'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应为：
- en: '[PRE65]py'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE65]py'
- en: Results indicate that we can decrease the cost of defaults by 22% using a counseling
    program, guided by predictive modeling.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果表明，使用辅导程序可以将违约成本降低22%，这一结果是通过预测建模得出的。
- en: Calculate the net savings per account (considering all accounts it might be
    possible to counsel, in other words relative to the whole test set) at the optimal threshold.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算在最优阈值下每个账户的净节省（考虑所有可能进行辅导的账户，也就是相对于整个测试集）。
- en: 'Use this code for the calculation:'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用此代码进行计算：
- en: '[PRE66]py'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE66]py'
- en: 'The output should be as follows:'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE67]py'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE67]py'
- en: Results like these help the client scale the potential amount of savings they
    could create with the counseling program, to as many accounts as they serve.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这样的结果帮助客户根据辅导项目的潜在节省量，推算其可以为服务的所有账户节省的金额。
- en: Plot the net savings per account against the cost of counseling per account
    for each threshold.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制每个阈值下，每个账户的净节省与每个账户辅导成本的关系图。
- en: 'Create the plot with this code:'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用此代码创建图表：
- en: '[PRE68]py'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE68]py'
- en: 'The resulting plot should appear like this:'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成的图应如下所示：
- en: '![Figure 7.14: The initial cost of the counseling program needed'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.14：需要的辅导项目初始成本'
- en: to achieve a given amount of savings
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 达到给定节省量
- en: '](img/B16392_07_14.jpg)'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_07_14.jpg)'
- en: 'Figure 7.14: The initial cost of the counseling program needed to achieve a
    given amount of savings'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.14：实现一定节省金额所需的咨询项目初始成本
- en: This indicates how much money the client needs to budget to the counseling program
    in a given month, to achieve a given amount of savings. It looks like the greatest
    benefit can be created by budgeting up to about NT$1300 per account (you could
    find the exact budgeted amount corresponding to maximum net savings using `np.argmax`).
    However, net savings are relatively flat for upfront investments between NT$1000
    and 2000, being lower outside that range. The client may not actually be able
    to budget this much for the program. However, this graphic gives them evidence
    to argue for a larger budget if they need to.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这表明客户需要在某个月份为咨询项目预算多少资金，以实现一定的节省金额。看起来最大利益可以通过为每个账户预算大约 NT$1300 来实现（你可以使用 `np.argmax`
    找到对应于最大净节省的精确预算金额）。然而，在 NT$1000 到 2000 之间，前期投资的净节省相对平稳，超出此范围则较低。客户实际上可能无法为该项目预算这么多资金，但这个图表为他们提供了证据，如果需要的话可以为更大预算进行辩护。
- en: This result corresponds to our graphic from the previous exercise. Although
    we've shown the optimal threshold is 0.36, it may be fine for the client to use
    a higher threshold up to about 0.5, thus making fewer positive predictions, offering
    counseling to fewer account holders, and having a smaller upfront program cost.
    *Figure 7.14* shows how this plays out in terms of cost and net savings per account.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个结果与我们在上一个练习中的图形相对应。虽然我们显示了最优阈值是 0.36，但对客户而言，使用更高的阈值（高至 0.5）可能是可以接受的，这样就会做出更少的正预测，向更少的账户持有者提供咨询，并且前期项目成本较小。*图
    7.14* 显示了这一点在成本和每个账户的净节省方面的体现。
- en: Plot the fraction of accounts predicted as positive (this is called the "flag
    rate") at each threshold.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个阈值下，绘制预测为正的账户比例（这称为“标记率”）。
- en: 'Use this code to plot the flag rate against the threshold:'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用此代码绘制标记率与阈值的关系：
- en: '[PRE69]py'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE69]py'
- en: 'The plot should appear as follows:'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图表应如下所示：
- en: '![Figure 7.15: Flag rate against threshold for the credit counseling program'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.15：信用咨询项目的标记率与阈值关系'
- en: '](img/B16392_07_15.jpg)'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_07_15.jpg)'
- en: 'Figure 7.15: Flag rate against threshold for the credit counseling program'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.15：信用咨询项目的标记率与阈值关系
- en: This plot shows the fraction of people who will be predicted to default and
    therefore will be recommended outreach at each threshold. It appears that at the
    optimal threshold of 0.36, only about 20% of accounts will be flagged for counseling.
    This shows how using a model to prioritize accounts for counseling can help focus
    on the right accounts and reduce wasted resources. Higher thresholds, which may
    result in nearly optimal savings up to a threshold of about 0.5 as shown in *Figure
    7.12* (*Chapter 7*, *Test Set Analysis, Financial Insights, and Delivery to the
    Client*) result in lower flag rates.
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个图表显示了在每个阈值下，将被预测为违约并因此会被推荐接触的账户比例。看起来在最优阈值 0.36 时，只有大约 20% 的账户会被标记为需要咨询。这表明，使用模型优先考虑需要咨询的账户可以帮助集中资源，减少资源浪费。更高的阈值，可能会导致几乎最优的节省，直到约
    0.5 的阈值，如 *图 7.12* 所示（*第 7 章*，*测试集分析、财务洞察及交付给客户*）会导致更低的标记率。
- en: 'Plot a precision-recall curve for the testing data using the following code:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码为测试数据绘制精度-召回率曲线：
- en: '[PRE70]py'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PRE70]py'
- en: 'The plot should look like this:'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该图应如下所示：
- en: '![Figure 7.16: Precision-recall curve'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.16：精度-召回率曲线'
- en: '](img/B16392_07_16.jpg)'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_07_16.jpg)'
- en: 'Figure 7.16: Precision-recall curve'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.16：精度-召回率曲线
- en: '*Figure 7.16* shows that in order to start getting a true positive rate (that
    is, recall) much above 0, we need to accept a precision of about 0.8 or lower.'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*图 7.16* 显示，为了开始获得高于 0 的真实正率（即召回率），我们需要接受约 0.8 或更低的精准度。'
- en: 'Precision and recall have a direct link to the cost and savings of the program:
    the more precise our predictions are, the less money we are wasting on counseling
    due to incorrect model predictions. And, the higher the recall, the more savings
    we can create by successfully identifying accounts that would default. Compare
    the code in this step to the code used to calculate costs and savings in the previous
    exercise to see this.'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精度和召回率与项目的成本和节省直接相关：我们预测越精准，由于模型预测错误而浪费在咨询上的钱就越少。而且，召回率越高，我们就能通过成功识别即将违约的账户创造更多节省。将此步骤中的代码与上一个练习中用于计算成本和节省的代码进行比较，即可看到这一点。
- en: To see the connection of precision and recall with the threshold used to define
    positive and negative predictions, it can be instructive to plot them separately.
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了查看精度和召回率与定义正负预测的阈值之间的关系，分别绘制它们可能会很有启发性。
- en: Plot precision and recall separately on the *y*-axis against threshold on the
    *x*-axis.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将精度和召回率分别绘制在*y*轴上，阈值绘制在*x*轴上。
- en: 'Use this code to produce the plot:'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用这个代码来生成图表：
- en: '[PRE71]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The plot should appear as follows:'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图表应如下所示：
- en: '![Figure 7.17: Precision and recall plotted separately against the threshold'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.17：精度和召回率分别绘制与阈值的关系'
- en: '](img/B16392_07_17.jpg)'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16392_07_17.jpg)'
- en: 'Figure 7.17: Precision and recall plotted separately against the threshold'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17：精度和召回率分别绘制与阈值的关系
- en: This plot sheds some light on why the optimal threshold turned out to be 0.36\.
    While the optimal threshold also depends on the financial analysis of costs and
    savings, we can see here that the steepest part of the initial increase in precision,
    which represents the correctness of positive predictions and is therefore a measure
    of how cost-effective the model-guided counseling can be, happens up to a threshold
    of about 0.36.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表揭示了为何最佳阈值为0.36的原因。虽然最佳阈值还取决于成本和节省的财务分析，但我们可以看到，精度最初增加的陡峭部分，代表了正向预测的准确性，因此也反映了模型引导的咨询有多具成本效益，发生在大约0.36的阈值之前。
- en: '![Author](img/Author_Page.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![作者](img/Author_Page.png)'
- en: Hey!
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嘿！
- en: I am Stephen Klosterman, the author of this book. I really hope you enjoyed
    reading my book and found it useful.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我是Stephen Klosterman，本书的作者。我非常希望你喜欢阅读我的书，并且觉得它对你有所帮助。
- en: It would really help me (and other potential readers!) if you could leave a
    review on Amazon sharing your thoughts on *Data Science Projects with Python*,
    *Second Edition*.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能在亚马逊上留下关于《*使用Python进行数据科学项目*》第二版的评论，分享你的想法，将对我（和其他潜在读者！）非常有帮助。
- en: Go to the link [https://packt.link/r/1800564481](https://packt.link/r/1800564481).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 请访问链接[https://packt.link/r/1800564481](https://packt.link/r/1800564481)。
- en: OR
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: Scan the QR code to leave your review.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描二维码留下你的评论。
- en: '![Barcode'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '![条形码'
- en: '](img/Barcode.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Barcode.jpg)'
- en: Your review will help me to understand what's worked well in this book and what
    could be improved upon for future editions, so it really is appreciated.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 你的评论将帮助我了解这本书中哪些地方做得好，哪些地方可以改进，以便为未来的版本做出改进，因此我非常感激。
- en: Best wishes,
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的祝愿，
- en: Stephen Klosterman
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Stephen Klosterman
