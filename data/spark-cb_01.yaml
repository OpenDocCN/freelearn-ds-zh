- en: Chapter 1. Getting Started with Apache Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章. 开始使用Apache Spark
- en: 'In this chapter, we will set up Spark and configure it. This chapter is divided
    into the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将设置和配置Spark。本章分为以下教程：
- en: Installing Spark from binaries
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从二进制文件安装Spark
- en: Building the Spark source code with Maven
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Maven构建Spark源代码
- en: Launching Spark on Amazon EC2
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Amazon EC2上启动Spark
- en: Deploying Spark on a cluster in standalone mode
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在独立模式下在集群上部署Spark
- en: Deploying Spark on a cluster with Mesos
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mesos集群上部署Spark
- en: Deploying Spark on a cluster with YARN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在YARN集群上部署Spark
- en: Using Tachyon as an off-heap storage layer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Tachyon作为离堆存储层
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Apache Spark is a general-purpose cluster computing system to process big data
    workloads. What sets Spark apart from its predecessors, such as MapReduce, is
    its speed, ease-of-use, and sophisticated analytics.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个通用的集群计算系统，用于处理大数据工作负载。Spark与其前身MapReduce的区别在于其速度、易用性和复杂的分析。
- en: Apache Spark was originally developed at AMPLab, UC Berkeley, in 2009\. It was
    made open source in 2010 under the BSD license and switched to the Apache 2.0
    license in 2013\. Toward the later part of 2013, the creators of Spark founded
    Databricks to focus on Spark's development and future releases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark最初是在2009年由加州大学伯克利分校的AMPLab开发的。它于2010年以BSD许可证开源，并于2013年切换到Apache
    2.0许可证。在2013年后期，Spark的创造者成立了Databricks，专注于Spark的开发和未来发布。
- en: Talking about speed, Spark can achieve sub-second latency on big data workloads.
    To achieve such low latency, Spark makes use of the memory for storage. In MapReduce,
    memory is primarily used for actual computation. Spark uses memory both to compute
    and store objects.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到速度，Spark可以在大数据工作负载上实现亚秒延迟。为了实现如此低的延迟，Spark利用内存进行存储。在MapReduce中，内存主要用于实际计算。Spark使用内存来计算和存储对象。
- en: Spark also provides a unified runtime connecting to various big data storage
    sources, such as HDFS, Cassandra, HBase, and S3\. It also provides a rich set
    of higher-level libraries for different big data compute tasks, such as machine
    learning, SQL processing, graph processing, and real-time streaming. These libraries
    make development faster and can be combined in an arbitrary fashion.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Spark还提供了一个统一的运行时，连接到各种大数据存储源，如HDFS、Cassandra、HBase和S3。它还提供了丰富的高级库，用于不同的大数据计算任务，如机器学习、SQL处理、图处理和实时流处理。这些库使开发更快，并且可以以任意方式组合。
- en: Though Spark is written in Scala, and this book only focuses on recipes in Scala,
    Spark also supports Java and Python.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Spark是用Scala编写的，而本书只关注Scala中的教程，但Spark也支持Java和Python。
- en: Spark is an open source community project, and everyone uses the pure open source
    Apache distributions for deployments, unlike Hadoop, which has multiple distributions
    available with vendor enhancements.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个开源社区项目，每个人都使用纯开源的Apache发行版进行部署，不像Hadoop有多个带有供应商增强的发行版可用。
- en: 'The following figure shows the Spark ecosystem:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了Spark生态系统：
- en: '![Introduction](img/3056_01_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Introduction](img/3056_01_01.jpg)'
- en: The Spark runtime runs on top of a variety of cluster managers, including YARN
    (Hadoop's compute framework), Mesos, and Spark's own cluster manager called **standalone
    mode**. Tachyon is a memory-centric distributed file system that enables reliable
    file sharing at memory speed across cluster frameworks. In short, it is an off-heap
    storage layer in memory, which helps share data across jobs and users. Mesos is
    a cluster manager, which is evolving into a data center operating system. YARN
    is Hadoop's compute framework that has a robust resource management feature that
    Spark can seamlessly use.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Spark运行时在各种集群管理器上运行，包括YARN（Hadoop的计算框架）、Mesos和Spark自己的集群管理器**独立模式**。Tachyon是一个以内存为中心的分布式文件系统，可以在集群框架之间以内存速度可靠地共享文件。简而言之，它是内存中的离堆存储层，有助于在作业和用户之间共享数据。Mesos是一个集群管理器，正在演变成数据中心操作系统。YARN是Hadoop的计算框架，具有强大的资源管理功能，Spark可以无缝使用。
- en: Installing Spark from binaries
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从二进制文件安装Spark
- en: Spark can be either built from the source code or precompiled binaries can be
    downloaded from [http://spark.apache.org](http://spark.apache.org). For a standard
    use case, binaries are good enough, and this recipe will focus on installing Spark
    using binaries.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以从源代码构建，也可以从[http://spark.apache.org](http://spark.apache.org)下载预编译的二进制文件。对于标准用例，二进制文件已经足够好，本教程将重点介绍使用二进制文件安装Spark。
- en: Getting ready
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: All the recipes in this book are developed using Ubuntu Linux but should work
    fine on any POSIX environment. Spark expects Java to be installed and the `JAVA_HOME`
    environment variable to be set.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有教程都是在Ubuntu Linux上开发的，但在任何POSIX环境中都应该可以正常工作。Spark需要安装Java，并设置`JAVA_HOME`环境变量。
- en: 'In Linux/Unix systems, there are certain standards for the location of files
    and directories, which we are going to follow in this book. The following is a
    quick cheat sheet:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux/Unix系统中，有关文件和目录位置的一些标准，我们将在本书中遵循。以下是一个快速的备忘单：
- en: '| Directory | Description |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 目录 | 描述 |'
- en: '| --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `/bin` | Essential command binaries |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `/bin` | 基本命令二进制文件 |'
- en: '| `/etc` | Host-specific system configuration |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| `/etc` | 特定主机系统配置 |'
- en: '| `/opt` | Add-on application software packages |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| `/opt` | 附加应用软件包 |'
- en: '| `/var` | Variable data |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| `/var` | 可变数据 |'
- en: '| `/tmp` | Temporary files |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| `/tmp` | 临时文件 |'
- en: '| `/home` | User home directories |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| `/home` | 用户主目录 |'
- en: How to do it...
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: At the time of writing this, Spark's current version is 1.4\. Please check the
    latest version from Spark's download page at [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
    Binaries are developed with a most recent and stable version of Hadoop. To use
    a specific version of Hadoop, the recommended approach is to build from sources,
    which will be covered in the next recipe.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Spark的当前版本是1.4。请从Spark的下载页面[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)检查最新版本。二进制文件是使用最新和稳定版本的Hadoop开发的。要使用特定版本的Hadoop，推荐的方法是从源代码构建，这将在下一个教程中介绍。
- en: 'The following are the installation steps:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是安装步骤：
- en: 'Open the terminal and download binaries using the following command:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端并使用以下命令下载二进制文件：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Unpack binaries:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解压二进制文件：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Rename the folder containing binaries by stripping the version information:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过剥离版本信息重命名包含二进制文件的文件夹：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Move the configuration folder to the `/etc` folder so that it can be made a
    symbolic link later:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将配置文件夹移动到`/etc`文件夹，以便稍后可以将其创建为符号链接：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create your company-specific installation directory under `/opt`. As the recipes
    in this book are tested on `infoobjects` sandbox, we are going to use `infoobjects`
    as directory name. Create the `/opt/infoobjects` directory:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`/opt`目录下创建您公司特定的安装目录。由于本书中的示例在`infoobjects`沙箱上进行了测试，我们将使用`infoobjects`作为目录名称。创建`/opt/infoobjects`目录：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Move the `spark` directory to `/opt/infoobjects` as it''s an add-on software
    package:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`spark`目录移动到`/opt/infoobjects`，因为它是一个附加软件包：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Change the ownership of the `spark` home directory to `root`:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改`spark`主目录的所有权为`root`：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Change permissions of the `spark` home directory, `0755 = user:read-write-execute
    group:read-execute world:read-execute`:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改`spark`主目录的权限，`0755 = 用户：读-写-执行组：读-执行世界：读-执行`：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Move to the `spark` home directory:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到`spark`主目录：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create the symbolic link:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建符号链接：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Append to `PATH` in `.bashrc`:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`.bashrc`中追加`PATH`：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Open a new terminal.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的终端。
- en: 'Create the `log` directory in `/var`:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`/var`中创建`log`目录：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Make `hduser` the owner of the Spark `log` directory.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`hduser`设置为Spark `log`目录的所有者。
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create the Spark `tmp` directory:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark `tmp`目录：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Configure Spark with the help of the following command lines:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令行配置Spark：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Building the Spark source code with Maven
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Maven构建Spark源代码
- en: 'Installing Spark using binaries works fine in most cases. For advanced cases,
    such as the following (but not limited to), compiling from the source code is
    a better option:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，使用二进制文件安装Spark效果很好。对于高级情况，例如以下情况（但不限于此），从源代码编译是更好的选择：
- en: Compiling for a specific Hadoop version
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为特定的Hadoop版本编译
- en: Adding the Hive integration
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加Hive集成
- en: Adding the YARN integration
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加YARN集成
- en: Getting ready
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'The following are the prerequisites for this recipe to work:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的先决条件是：
- en: Java 1.6 or a later version
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java 1.6或更高版本
- en: Maven 3.x
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maven 3.x
- en: How to do it...
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The following are the steps to build the Spark source code with Maven:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用Maven构建Spark源代码的步骤：
- en: 'Increase `MaxPermSize` for heap:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加`MaxPermSize`以扩展堆：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Open a new terminal window and download the Spark source code from GitHub:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的终端窗口并从GitHub下载Spark源代码：
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Unpack the archive:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解压缩存档：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Move to the `spark` directory:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到`spark`目录：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Compile the sources with these flags: Yarn enabled, Hadoop version 2.4, Hive
    enabled, and skipping tests for faster compilation:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下标志编译源代码：启用Yarn，Hadoop版本2.4，启用Hive，并跳过测试以加快编译速度：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Move the `conf` folder to the `etc` folder so that it can be made a symbolic
    link:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`conf`文件夹移动到`etc`文件夹，以便稍后可以将其创建为符号链接：
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Move the `spark` directory to `/opt` as it''s an add-on software package:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`spark`目录移动到`/opt`，因为它是一个附加软件包：
- en: '[PRE21]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Change the ownership of the `spark` home directory to `root`:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改`spark`主目录的所有权为`root`：
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Change the permissions of the `spark` home directory `0755 = user:rwx group:r-x
    world:r-x`:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改`spark`主目录的权限`0755 = 用户：rwx组：r-x世界：r-x`：
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Move to the `spark` home directory:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到`spark`主目录：
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Create a symbolic link:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个符号链接：
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Put the Spark executable in the path by editing `.bashrc`:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过编辑`.bashrc`将Spark可执行文件放入路径中：
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create the `log` directory in `/var`:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`/var`中创建`log`目录：
- en: '[PRE27]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Make `hduser` the owner of the Spark `log` directory:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`hduser`设置为Spark `log`目录的所有者：
- en: '[PRE28]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create the Spark `tmp` directory:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark `tmp`目录：
- en: '[PRE29]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Configure Spark with the help of the following command lines:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令行配置Spark：
- en: '[PRE30]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Launching Spark on Amazon EC2
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Amazon EC2上启动Spark
- en: '**Amazon Elastic Compute Cloud** (**Amazon EC2**) is a web service that provides
    resizable compute instances in the cloud. Amazon EC2 provides the following features:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon Elastic Compute Cloud**（**Amazon EC2**）是一种提供可调整大小的云中计算实例的网络服务。Amazon
    EC2提供以下功能：'
- en: On-demand delivery of IT resources via the Internet
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过互联网按需交付IT资源
- en: The provision of as many instances as you like
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供您喜欢的实例数量
- en: Payment for the hours you use instances like your utility bill
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按小时支付您使用实例的费用，就像您的水电费一样
- en: No setup cost, no installation, and no overhead at all
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有设置费用，没有安装费用，也没有任何额外费用
- en: When you no longer need instances, you either shut down or terminate and walk
    away
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您不再需要实例时，您可以关闭或终止并离开
- en: The availability of these instances on all familiar operating systems
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些实例在所有熟悉的操作系统上都是可用的
- en: EC2 provides different types of instances to meet all compute needs, such as
    general-purpose instances, micro instances, memory-optimized instances, storage-optimized
    instances, and others. They have a free tier of micro-instances to try.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: EC2提供不同类型的实例，以满足所有计算需求，例如通用实例、微型实例、内存优化实例、存储优化实例等。它们有一个免费的微型实例套餐可供尝试。
- en: Getting ready
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: The `spark-ec2` script comes bundled with Spark and makes it easy to launch,
    manage, and shut down clusters on Amazon EC2.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark-ec2`脚本与Spark捆绑在一起，可以轻松在Amazon EC2上启动、管理和关闭集群。'
- en: 'Before you start, you need to do the following things:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，您需要做以下事情：
- en: Log in to the Amazon AWS account ([http://aws.amazon.com](http://aws.amazon.com)).
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到Amazon AWS帐户（[http://aws.amazon.com](http://aws.amazon.com)）。
- en: Click on **Security Credentials** under your account name in the top-right corner.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在右上角的帐户名称下单击**安全凭据**。
- en: Click on **Access Keys** and **Create New Access Key**:![Getting ready](img/3056_01_02.jpg)
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击**访问密钥**和**创建新的访问密钥**：![准备就绪](img/3056_01_02.jpg)
- en: Note down the access key ID and secret access key.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记下访问密钥ID和秘密访问密钥。
- en: Now go to **Services** | **EC2**.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在转到**服务** | **EC2**。
- en: Click on **Key Pairs** in left-hand menu under NETWORK & SECURITY.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧菜单中单击**密钥对**，然后单击**网络和安全**下的**密钥对**。
- en: Click on **Create Key Pair** and enter `kp-spark` as key-pair name:![Getting
    ready](img/3056_01_15.jpg)
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击**创建密钥对**，并输入 `kp-spark` 作为密钥对名称：![准备中](img/3056_01_15.jpg)
- en: Download the private key file and copy it in the `/home/hduser/keypairs folder`.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载私钥文件并将其复制到 `/home/hduser/keypairs` 文件夹中。
- en: Set permissions on key file to `600`.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将密钥文件权限设置为 `600`。
- en: 'Set environment variables to reflect access key ID and secret access key (please
    replace sample values with your own values):'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置环境变量以反映访问密钥 ID 和秘密访问密钥（请用您自己的值替换示例值）：
- en: '[PRE31]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How to do it...
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Spark comes bundled with scripts to launch the Spark cluster on Amazon EC2\.
    Let''s launch the cluster using the following command:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 预先捆绑了用于在 Amazon EC2 上启动 Spark 集群的脚本。让我们使用以下命令启动集群：
- en: '[PRE32]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Launch the cluster with the example value:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用示例值启动集群：
- en: '[PRE33]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '`<key-pair>`: This is the name of EC2 key-pair created in AWS'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<key-pair>`: 这是在 AWS 中创建的 EC2 密钥对的名称'
- en: '`<key-file>`: This is the private key file you downloaded'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<key-file>`: 这是您下载的私钥文件'
- en: '`<num-slaves>`: This is the number of slave nodes to launch'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<num-slaves>`: 这是要启动的从节点数量'
- en: '`<cluster-name>`: This is the name of the cluster'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<cluster-name>`: 这是集群的名称'
- en: 'Sometimes, the default availability zones are not available; in that case,
    retry sending the request by specifying the specific availability zone you are
    requesting:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时，默认的可用区不可用；在这种情况下，通过指定您正在请求的特定可用区来重试发送请求：
- en: '[PRE34]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If your application needs to retain data after the instance shuts down, attach
    EBS volume to it (for example, a 10 GB space):'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的应用程序需要在实例关闭后保留数据，请将 EBS 卷附加到它（例如，10 GB 空间）：
- en: '[PRE35]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'If you use Amazon spot instances, here''s the way to do it:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您使用 Amazon spot 实例，以下是操作方法：
- en: '[PRE36]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Spot instances allow you to name your own price for Amazon EC2 computing capacity.
    You simply bid on spare Amazon EC2 instances and run them whenever your bid exceeds
    the current spot price, which varies in real-time based on supply and demand (source:
    [amazon.com](http://amazon.com)).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Spot 实例允许您为 Amazon EC2 计算能力命名自己的价格。您只需对多余的 Amazon EC2 实例进行竞标，并在您的出价超过当前 spot
    价格时运行它们，该价格根据供求实时变化（来源：[amazon.com](http://amazon.com)）。
- en: After everything is launched, check the status of the cluster by going to the
    web UI URL that will be printed at the end.![How to do it...](img/3056_01_03.jpg)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一切都启动后，通过转到最后打印的 web UI URL 来检查集群的状态。![如何做...](img/3056_01_03.jpg)
- en: Check the status of the cluster:![How to do it...](img/3056_01_04.jpg)
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查集群的状态：![如何做...](img/3056_01_04.jpg)
- en: 'Now, to access the Spark cluster on EC2, let''s connect to the master node
    using **secure shell protocol** (**SSH**):'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，要访问 EC2 上的 Spark 集群，让我们使用**安全外壳协议**（**SSH**）连接到主节点：
- en: '[PRE37]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You should get something like the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该得到类似以下的内容：
- en: '![How to do it...](img/3056_01_05.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/3056_01_05.jpg)'
- en: 'Check directories in the master node and see what they do:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查主节点中的目录并查看它们的作用：
- en: '| Directory | Description |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 目录 | 描述 |'
- en: '| --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `ephemeral-hdfs` | This is the Hadoop instance for which data is ephemeral
    and gets deleted when you stop or restart the machine. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `ephemeral-hdfs` | 这是 Hadoop 实例，其中的数据是暂时的，当您停止或重新启动机器时会被删除。 |'
- en: '| `persistent-hdfs` | Each node has a very small amount of persistent storage
    (approximately 3 GB). If you use this instance, data will be retained in that
    space. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `persistent-hdfs` | 每个节点都有非常少量的持久存储（大约 3 GB）。如果使用此实例，数据将保留在该空间中。 |'
- en: '| `hadoop-native` | These are native libraries to support Hadoop, such as snappy
    compression libraries. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `hadoop-native` | 这些是支持 Hadoop 的本地库，如 snappy 压缩库。 |'
- en: '| `Scala` | This is Scala installation. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `Scala` | 这是 Scala 安装。 |'
- en: '| `shark` | This is Shark installation (Shark is no longer supported and is
    replaced by Spark SQL). |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| `shark` | 这是 Shark 安装（Shark 不再受支持，已被 Spark SQL 取代）。 |'
- en: '| `spark` | This is Spark installation |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| `spark` | 这是 Spark 安装 |'
- en: '| `spark-ec2` | These are files to support this cluster deployment. |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| `spark-ec2` | 这些是支持此集群部署的文件。 |'
- en: '| `tachyon` | This is Tachyon installation |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| `tachyon` | 这是 Tachyon 安装 |'
- en: 'Check the HDFS version in an ephemeral instance:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令检查暂时实例中的 HDFS 版本：
- en: '[PRE38]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Check the HDFS version in persistent instance with the following command:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令检查持久实例中的 HDFS 版本：
- en: '[PRE39]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Change the configuration level in logs:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改日志中的配置级别：
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The default log level information is too verbose, so let's change it to Error:![How
    to do it...](img/3056_01_06.jpg)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认的日志级别信息太冗长了，所以让我们将其更改为错误：![如何做...](img/3056_01_06.jpg)
- en: 'Create the `log4.properties` file by renaming the template:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过重命名模板创建 `log4.properties` 文件：
- en: '[PRE41]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Open `log4j.properties` in vi or your favorite editor:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 vi 或您喜欢的编辑器中打开 `log4j.properties`：
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Change second line from `| log4j.rootCategory=INFO, console` to `| log4j.rootCategory=ERROR,
    console`.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第二行从 `| log4j.rootCategory=INFO, console` 更改为 `| log4j.rootCategory=ERROR,
    console`。
- en: 'Copy the configuration to all slave nodes after the change:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改后将配置复制到所有从节点：
- en: '[PRE43]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You should get something like this:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该得到类似以下的内容：
- en: '![How to do it...](img/3056_01_07.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/3056_01_07.jpg)'
- en: 'Destroy the Spark cluster:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 销毁 Spark 集群：
- en: '[PRE44]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: See also
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[http://aws.amazon.com/ec2](http://aws.amazon.com/ec2)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://aws.amazon.com/ec2](http://aws.amazon.com/ec2)'
- en: Deploying on a cluster in standalone mode
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在独立模式下的集群部署
- en: Compute resources in a distributed environment need to be managed so that resource
    utilization is efficient and every job gets a fair chance to run. Spark comes
    along with its own cluster manager conveniently called **standalone mode**. Spark
    also supports working with YARN and Mesos cluster managers.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式环境中管理计算资源，以便资源利用率高效，并且每个作业都有公平的运行机会。Spark 预先捆绑了其自己的集群管理器，方便地称为**独立模式**。Spark
    还支持与 YARN 和 Mesos 集群管理器一起工作。
- en: The cluster manager that should be chosen is mostly driven by both legacy concerns
    and whether other frameworks, such as MapReduce, are sharing the same compute
    resource pool. If your cluster has legacy MapReduce jobs running, and all of them
    cannot be converted to Spark jobs, it is a good idea to use YARN as the cluster
    manager. Mesos is emerging as a data center operating system to conveniently manage
    jobs across frameworks, and is very compatible with Spark.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 应该选择的集群管理器主要受到传统问题的驱动，以及其他框架（如MapReduce）是否共享相同的计算资源池。如果您的集群有传统的MapReduce作业运行，并且所有这些作业都无法转换为Spark作业，那么使用YARN作为集群管理器是一个好主意。Mesos正在成为一个数据中心操作系统，方便地跨框架管理作业，并且与Spark非常兼容。
- en: If the Spark framework is the only framework in your cluster, then standalone
    mode is good enough. As Spark evolves as technology, you will see more and more
    use cases of Spark being used as the standalone framework serving all big data
    compute needs. For example, some jobs may be using Apache Mahout at present because
    MLlib does not have a specific machine-learning library, which the job needs.
    As soon as MLlib gets this library, this particular job can be moved to Spark.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Spark框架是集群中唯一的框架，那么独立模式就足够了。随着Spark作为技术的发展，您将看到越来越多的Spark被用作独立框架来满足所有大数据计算需求的用例。例如，目前可能有一些作业正在使用Apache
    Mahout，因为MLlib没有特定的机器学习库，而作业需要。一旦MLlib获得了这个库，这个特定的作业就可以迁移到Spark中。
- en: Getting ready
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Let''s consider a cluster of six nodes as an example setup: one master and
    five slaves (replace them with actual node names in your cluster):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个六个节点的集群为例：一个主节点和五个从节点（用集群中实际的节点名称替换它们）：
- en: '[PRE45]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: How to do it...
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Since Spark''s standalone mode is the default, all you need to do is to have
    Spark binaries installed on both master and slave machines. Put `/opt/infoobjects/spark/sbin`
    in path on every node:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于Spark的独立模式是默认模式，所以您只需要在主节点和从节点上安装Spark二进制文件。在每个节点上将`/opt/infoobjects/spark/sbin`添加到路径中：
- en: '[PRE46]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Start the standalone master server (SSH to master first):'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动独立的主服务器（首先SSH到主节点）：
- en: '[PRE47]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Master, by default, starts on port 7077, which slaves use to connect to it.
    It also has a web UI at port 8088.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Master在端口7077上启动，从节点使用该端口连接到Master。它还在端口8088上有一个Web UI。
- en: 'Please SSH to master node and start slaves:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请SSH到主节点并启动从节点：
- en: '[PRE48]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '| Argument (for fine-grained configuration, the following parameters work with
    both master and slaves) | Meaning |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '- 参数（用于细粒度配置，以下参数适用于主节点和从节点） | 意义'
- en: '| --- | --- |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '- --- | ---'
- en: '| `-i <ipaddress>,-ip <ipaddress>` | IP address/DNS service listens on |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '- `-i <ipaddress>,-ip <ipaddress>` | IP地址/DNS服务监听的地址'
- en: '| `-p <port>, --port <port>` | Port service listens on |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '- `-p <port>, --port <port>` | 服务监听的端口'
- en: '| `--webui-port <port>` | Port for web UI (by default, 8080 for master and
    8081 for worker) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '- `--webui-port <port>` | Web UI的端口（默认情况下，主节点为8080，从节点为8081）'
- en: '| `-c <cores>,--cores <cores>` | Total CPU cores Spark applications that can
    be used on a machine (worker only) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '- `-c <cores>,--cores <cores>` | 机器上可以用于Spark应用程序的总CPU核心数（仅限worker）'
- en: '| `-m <memory>,--memory <memory>` | Total RAM Spark applications that can be
    used on a machine (worker only) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '- `-m <memory>,--memory <memory>` | 机器上可以用于Spark应用程序的总RAM（仅限worker）'
- en: '| `-d <dir>,--work-dir <dir>` | The directory to use for scratch space and
    job output logs |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '- `-d <dir>,--work-dir <dir>` | 用于临时空间和作业输出日志的目录'
- en: Rather than manually starting master and slave daemons on each node, it can
    also be accomplished using cluster launch scripts.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与手动在每个节点上启动主和从守护程序相比，也可以使用集群启动脚本来完成。
- en: 'First, create the `conf/slaves` file on a master node and add one line per
    slave hostname (using an example of five slaves nodes, replace with the DNS of
    slave nodes in your cluster):'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在主节点上创建`conf/slaves`文件，并添加每个从节点主机名的一行（使用五个从节点的示例，用集群中从节点的DNS替换）：
- en: '[PRE49]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Once the slave machine is set up, you can call the following scripts to start/stop
    cluster:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从节点设置好，就可以调用以下脚本来启动/停止集群：
- en: '| Script name | Purpose |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '- 脚本名称 | 目的'
- en: '| --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '- --- | ---'
- en: '| `start-master.sh` | Starts a master instance on the host machine |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '- `start-master.sh` | 在主机上启动主实例'
- en: '| `start-slaves.sh` | Starts a slave instance on each node in the slaves file
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '- `start-slaves.sh` | 在slaves文件中的每个节点上启动一个从节点实例'
- en: '| `start-all.sh` | Starts both master and slaves |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '- `start-all.sh` | 启动主节点和从节点'
- en: '| `stop-master.sh` | Stops the master instance on the host machine |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '- `stop-master.sh` | 停止主机上的主实例 |'
- en: '| `stop-slaves.sh` | Stops the slave instance on all nodes in the slaves file
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '- `stop-slaves.sh` | 停止slaves文件中所有节点上的从节点实例'
- en: '| `stop-all.sh` | Stops both master and slaves |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '- `stop-all.sh` | 停止主节点和从节点'
- en: 'Connect an application to the cluster through the Scala code:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过Scala代码将应用程序连接到集群：
- en: '[PRE50]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Connect to the cluster through Spark shell:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过Spark shell连接到集群：
- en: '[PRE51]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How it works...
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In standalone mode, Spark follows the master slave architecture, very much like
    Hadoop, MapReduce, and YARN. The compute master daemon is called **Spark master**
    and runs on one master node. Spark master can be made highly available using ZooKeeper.
    You can also add more standby masters on the fly, if needed.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立模式下，Spark遵循主从架构，非常类似于Hadoop、MapReduce和YARN。计算主守护程序称为**Spark master**，在一个主节点上运行。Spark
    master可以使用ZooKeeper实现高可用性。如果需要，还可以在运行时添加更多的备用主节点。
- en: 'The compute slave daemon is called **worker** and is on each slave node. The
    worker daemon does the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 计算从节点守护程序称为**worker**，位于每个从节点上。worker守护程序执行以下操作：
- en: Reports the availability of compute resources on a slave node, such as the number
    of cores, memory, and others, to Spark master
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告从节点上计算资源的可用性，例如核心数、内存等，到Spark master
- en: Spawns the executor when asked to do so by Spark master
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当Spark master要求时，生成执行程序
- en: Restarts the executor if it dies
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果执行程序死掉，则重新启动执行程序
- en: There is, at most, one executor per application per slave machine.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 每个应用程序每个从节点最多只有一个执行程序。
- en: 'Both Spark master and worker are very lightweight. Typically, memory allocation
    between 500 MB to 1 GB is sufficient. This value can be set in `conf/spark-env.sh`
    by setting the `SPARK_DAEMON_MEMORY` parameter. For example, the following configuration
    will set the memory to 1 gigabits for both master and worker daemon. Make sure
    you have `sudo` as the super user before running it:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的master和worker都非常轻量级。通常，500 MB到1 GB之间的内存分配就足够了。可以通过在`conf/spark-env.sh`中设置`SPARK_DAEMON_MEMORY`参数来设置这个值。例如，以下配置将为master和worker
    daemon设置内存为1 GB。在运行之前确保你有`sudo`超级用户权限：
- en: '[PRE52]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'By default, each slave node has one worker instance running on it. Sometimes,
    you may have a few machines that are more powerful than others. In that case,
    you can spawn more than one worker on that machine by the following configuration
    (only on those machines):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，每个从属节点上都有一个工作程序实例在运行。有时，您可能有一些比其他机器更强大的机器。在这种情况下，可以通过以下配置在该机器上生成多个工作程序（仅在这些机器上）：
- en: '[PRE53]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Spark worker, by default, uses all cores on the slave machine for its executors.
    If you would like to limit the number of cores the worker can use, you can set
    it to that number (for example, 12) by the following configuration:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Spark worker默认使用从属机器上的所有核心作为其执行器。如果要限制工作程序可以使用的核心数，可以通过以下配置将其设置为该数字（例如12）：
- en: '[PRE54]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Spark worker, by default, uses all the available RAM (1 GB for executors).
    Note that you cannot allocate how much memory each specific executor will use
    (you can control this from the driver configuration). To assign another value
    for the total memory (for example, 24 GB) to be used by all executors combined,
    execute the following setting:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Spark worker默认使用所有可用的RAM（执行器为1 GB）。请注意，您无法分配每个特定执行器将使用多少内存（您可以从驱动程序配置中控制此操作）。要为所有执行器组合使用的总内存（例如，24
    GB）分配另一个值，请执行以下设置：
- en: '[PRE55]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'There are some settings you can do at the driver level:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在驱动程序级别可以进行一些设置：
- en: 'To specify the maximum number of CPU cores to be used by a given application
    across the cluster, you can set the `spark.cores.max` configuration in Spark submit
    or Spark shell as follows:'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要指定集群中给定应用程序可以使用的最大CPU核心数，可以在Spark submit或Spark shell中设置`spark.cores.max`配置如下：
- en: '[PRE56]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'To specify the amount of memory each executor should be allocated (the minimum
    recommendation is 8 GB), you can set the `spark.executor.memory` configuration
    in Spark submit or Spark shell as follows:'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要指定每个执行器应分配的内存量（最低建议为8 GB），可以在Spark submit或Spark shell中设置`spark.executor.memory`配置如下：
- en: '[PRE57]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The following diagram depicts the high-level architecture of a Spark cluster:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了Spark集群的高级架构：
- en: '![How it works...](img/3056_01_08.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/3056_01_08.jpg)'
- en: See also
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[http://spark.apache.org/docs/latest/spark-standalone.html](http://spark.apache.org/docs/latest/spark-standalone.html)
    to find more configuration options'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/spark-standalone.html](http://spark.apache.org/docs/latest/spark-standalone.html)查找更多配置选项'
- en: Deploying on a cluster with Mesos
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在具有Mesos的集群上部署
- en: Mesos is slowly emerging as a data center operating system to manage all compute
    resources across a data center. Mesos runs on any computer running the Linux operating
    system. Mesos is built using the same principles as Linux kernel. Let's see how
    we can install Mesos.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos正在逐渐成为数据中心操作系统，用于管理数据中心中的所有计算资源。Mesos可以在运行Linux操作系统的任何计算机上运行。Mesos是使用与Linux内核相同的原则构建的。让我们看看如何安装Mesos。
- en: How to do it...
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Mesosphere provides a binary distribution of Mesos. The most recent package
    for the Mesos distribution can be installed from the Mesosphere repositories by
    performing the following steps:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Mesosphere提供了Mesos的二进制发行版。可以通过执行以下步骤从Mesosphere存储库安装Mesos的最新软件包：
- en: 'Execute Mesos on Ubuntu OS with the trusty version:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Ubuntu OS的trusty版本执行Mesos：
- en: '[PRE58]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Update the repositories:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新存储库：
- en: '[PRE59]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Install Mesos:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装Mesos：
- en: '[PRE60]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: To connect Spark to Mesos to integrate Spark with Mesos, make Spark binaries
    available to Mesos and configure the Spark driver to connect to Mesos.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Spark连接到Mesos以将Spark与Mesos集成，使Spark二进制文件可用于Mesos，并配置Spark驱动程序以连接到Mesos。
- en: 'Use Spark binaries from the first recipe and upload to HDFS:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第一个配方中的Spark二进制文件并上传到HDFS：
- en: '[PRE61]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The master URL for single master Mesos is `mesos://host:5050`, and for the ZooKeeper
    managed Mesos cluster, it is `mesos://zk://host:2181`.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单主Mesos的主URL是`mesos://host:5050`，而ZooKeeper管理的Mesos集群的主URL是`mesos://zk://host:2181`。
- en: 'Set the following variables in `spark-env.sh`:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`spark-env.sh`中设置以下变量：
- en: '[PRE62]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Run from the Scala program:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Scala程序运行：
- en: '[PRE63]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Run from the Spark shell:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Spark shell运行：
- en: '[PRE64]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Note
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Mesos has two run modes:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos有两种运行模式：
- en: '**Fine-grained**: In fine-grained (default) mode, every Spark task runs as
    a separate Mesos task'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**细粒度**：在细粒度（默认）模式下，每个Spark任务都作为单独的Mesos任务运行'
- en: '**Coarse-grained**: This mode will launch only one long-running Spark task
    on each Mesos machine'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗粒度**：此模式将在每个Mesos机器上启动一个长时间运行的Spark任务'
- en: 'To run in the coarse-grained mode, set the `spark.mesos.coarse` property:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在粗粒度模式下运行，设置`spark.mesos.coarse`属性：
- en: '[PRE65]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Deploying on a cluster with YARN
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在具有YARN的集群上部署
- en: '**Yet another resource negotiator** (**YARN**) is Hadoop''s compute framework
    that runs on top of HDFS, which is Hadoop''s storage layer.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**另一个资源协商者**（**YARN**）是Hadoop的计算框架，运行在HDFS之上，HDFS是Hadoop的存储层。'
- en: YARN follows the master slave architecture. The master daemon is called `ResourceManager`
    and the slave daemon is called `NodeManager`. Besides this application, life cycle
    management is done by `ApplicationMaster`, which can be spawned on any slave node
    and is alive for the lifetime of an application.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: YARN遵循主从架构。主守护程序称为`ResourceManager`，从守护程序称为`NodeManager`。除此应用程序外，生命周期管理由`ApplicationMaster`完成，它可以在任何从节点上生成，并在应用程序的生命周期内保持活动状态。
- en: When Spark is run on YARN, `ResourceManager` performs the role of Spark master
    and `NodeManagers` work as executor nodes.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 当Spark在YARN上运行时，`ResourceManager`扮演Spark master的角色，而`NodeManagers`作为执行器节点工作。
- en: While running Spark with YARN, each Spark executor is run as YARN container.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用YARN运行Spark时，每个Spark执行器都作为YARN容器运行。
- en: Getting ready
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Running Spark on YARN requires a binary distribution of Spark that has YARN
    support. In both Spark installation recipes, we have taken care of it.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在YARN上运行Spark需要具有YARN支持的Spark二进制发行版。在两个Spark安装配方中，我们已经注意到了这一点。
- en: How to do it...
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'To run Spark on YARN, the first step is to set the configuration:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在YARN上运行Spark，第一步是设置配置：
- en: '[PRE66]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'You can see this in the following screenshot:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下截图中看到这一点：
- en: '![How to do it...](img/3056_01_09.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/3056_01_09.jpg)'
- en: 'The following command launches YARN Spark in the `yarn-client` mode:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下命令在`yarn-client`模式下启动YARN Spark：
- en: '[PRE67]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Here''s an example:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子：
- en: '[PRE68]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The following command launches Spark shell in the `yarn-client` mode:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下命令在`yarn-client`模式下启动Spark shell：
- en: '[PRE69]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The command to launch in the `yarn-cluster` mode is as follows:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以`yarn-cluster`模式启动的命令如下：
- en: '[PRE70]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Here''s an example:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子：
- en: '[PRE71]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: How it works…
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'Spark applications on YARN run in two modes:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: YARN上的Spark应用程序以两种模式运行：
- en: '`yarn-client`: Spark Driver runs in the client process outside of YARN cluster,
    and `ApplicationMaster` is only used to negotiate resources from ResourceManager'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`yarn-client`：Spark Driver在YARN集群之外的客户端进程中运行，`ApplicationMaster`仅用于从ResourceManager协商资源'
- en: '`yarn-cluster`: Spark Driver runs in `ApplicationMaster` spawned by `NodeManager`
    on a slave node'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`yarn-cluster`：Spark Driver在由从节点上的`NodeManager`生成的`ApplicationMaster`中运行'
- en: The `yarn-cluster` mode is recommended for production deployments, while the
    y`arn-client` mode is good for development and debugging when you would like to
    see immediate output. There is no need to specify Spark master in either mode
    as it's picked from the Hadoop configuration, and the master parameter is either
    `yarn-client` or `yarn-cluster`.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '`yarn-cluster`模式适用于生产部署，而`yarn-client`模式适用于开发和调试，当您希望立即看到输出时。在任何模式下都不需要指定Spark主节点，因为它是从Hadoop配置中选择的，主参数是`yarn-client`或`yarn-cluster`。'
- en: 'The following figure shows how Spark is run with YARN in the client mode:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了在客户端模式下如何使用YARN运行Spark：
- en: '![How it works…](img/3056_01_10.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/3056_01_10.jpg)'
- en: 'The following figure shows how Spark is run with YARN in the cluster mode:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了在集群模式下如何使用YARN运行Spark：
- en: '![How it works…](img/3056_01_11.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/3056_01_11.jpg)'
- en: 'In the YARN mode, the following configuration parameters can be set:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在YARN模式下，可以设置以下配置参数：
- en: '`--num-executors`: Configure how many executors will be allocated'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--num-executors`：配置将分配多少个executor'
- en: '`--executor-memory`: RAM per executor'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--executor-memory`：每个executor的RAM'
- en: '`--executor-cores`: CPU cores per executor'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--executor-cores`：每个executor的CPU核心'
- en: Using Tachyon as an off-heap storage layer
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Tachyon作为离堆存储层
- en: 'Spark RDDs are a great way to store datasets in memory while ending up with
    multiple copies of the same data in different applications. Tachyon solves some
    of the challenges with Spark RDD management. A few of them are:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: Spark RDD是一种在内存中存储数据集的好方法，同时在不同应用程序中产生相同数据的多个副本。Tachyon解决了Spark RDD管理中的一些挑战。其中一些是：
- en: RDD only exists for the duration of the Spark application
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD仅存在于Spark应用程序的持续时间内
- en: The same process performs the compute and RDD in-memory storage; so, if a process
    crashes, in-memory storage also goes away
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同一进程执行计算和RDD内存存储；因此，如果一个进程崩溃，内存存储也会消失
- en: 'Different jobs cannot share an RDD even if they are for the same underlying
    data, for example, an HDFS block that leads to:'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使是针对相同底层数据的不同作业也不能共享RDD，例如导致HDFS块的情况：
- en: Slow writes to disk
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向磁盘写入速度慢
- en: Duplication of data in memory, higher memory footprint
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存中数据的重复，内存占用更高
- en: If the output of one application needs to be shared with the other application,
    it's slow due to the replication in the disk
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个应用程序的输出需要与另一个应用程序共享，由于磁盘中的复制，速度会很慢
- en: 'Tachyon provides an off-heap memory layer to solve these problems. This layer,
    being off-heap, is immune to process crashes and is also not subject to garbage
    collection. This also lets RDDs be shared across applications and outlive a specific
    job or session; in essence, one single copy of data resides in memory, as shown
    in the following figure:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: Tachyon提供了一个离堆内存层来解决这些问题。这一层是离堆的，不受进程崩溃的影响，也不受垃圾回收的影响。这也允许RDD在应用程序之间共享，并且在特定作业或会话之外存在；实质上，数据的内存中只有一个副本，如下图所示：
- en: '![Using Tachyon as an off-heap storage layer](img/3056_01_12.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![使用Tachyon作为离堆存储层](img/3056_01_12.jpg)'
- en: How to do it...
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s download and compile Tachyon (Tachyon, by default, comes configured
    for Hadoop 1.0.4, so it needs to be compiled from sources for the right Hadoop
    version). Replace the version with the current version. The current version at
    the time of writing this book is 0.6.4:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们下载并编译Tachyon（默认情况下，Tachyon配置为Hadoop 1.0.4，因此需要根据正确的Hadoop版本从源代码编译）。将版本替换为当前版本。撰写本书时的当前版本为0.6.4：
- en: '[PRE72]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Unarchive the source code:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解压源代码：
- en: '[PRE73]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Remove the version from the `tachyon` source folder name for convenience:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为方便起见，从`tachyon`源文件夹名称中删除版本：
- en: '[PRE74]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Change the directory to the `tachyon` folder:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到`tachyon`文件夹：
- en: '[PRE75]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Comment the following line:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注释以下行：
- en: '[PRE76]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Uncomment the following line:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取消注释以下行：
- en: '[PRE77]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Change the following properties:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改以下属性：
- en: '[PRE78]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Replace `${tachyon.home}` with `/var/log/tachyon`.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`${tachyon.home}`替换为`/var/log/tachyon`。
- en: 'Create a new `core-site.xml` file in the `conf` directory:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`conf`目录中创建一个新的`core-site.xml`文件：
- en: '[PRE79]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Add `<tachyon home>/bin` to the path:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`<tachyon home>/bin`添加到路径中：
- en: '[PRE80]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Restart the shell and format Tachyon:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新启动shell并格式化Tachyon：
- en: '[PRE81]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Tachyon''s web interface is `http://hostname:19999`:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: Tachyon的web界面是`http://hostname:19999`：
- en: '![How to do it...](img/3056_01_13.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/3056_01_13.jpg)'
- en: 'Run the sample program to see whether Tachyon is running fine:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行示例程序，查看Tachyon是否正常运行：
- en: '[PRE82]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '![How to do it...](img/3056_01_14.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/3056_01_14.jpg)'
- en: 'You can stop Tachyon any time by running the following command:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以随时通过运行以下命令停止Tachyon：
- en: '[PRE83]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Run Spark on Tachyon:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Tachyon上运行Spark：
- en: '[PRE84]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: See also
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf](http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf)
    to learn about the origins of Tachyon'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击链接[http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf](http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf)了解Tachyon的起源
- en: '[http://www.tachyonnexus.com](http://www.tachyonnexus.com)'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击链接[http://www.tachyonnexus.com](http://www.tachyonnexus.com)
