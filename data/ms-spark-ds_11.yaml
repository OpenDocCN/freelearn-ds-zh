- en: Chapter 11. Anomaly Detection on Sentiment Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。情感分析中的异常检测
- en: 'When we look back at the year 2016, we will surely remember it as a time of
    many significant geo-political events ranging from Brexit, Great Britain''s vote
    to leave the European Union, to the untimely passing of many beloved celebrities,
    including the sudden death of the singer David Bowie (covered in [Chapter 6](ch06.xhtml
    "Chapter 6. Scraping Link-Based External Data"), *Scraping Link-Based External
    Data* and [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"), *Building
    Communities*). However, perhaps the most notable occurrence of the year was the
    tense US presidential election and its eventual outcome, the election of President
    Donald Trump. A campaign that will long be remembered, not least for its unprecedented
    use of social media, and the stirring up of passion among its users, most of whom
    made their feelings known through the use of hashtags: either positive ones, such
    as *#MakeAmericaGreatAgain* or *#StrongerTogether*, or conversely negative ones,
    such as *#DumpTrump* or *#LockHerUp*. Since this chapter is about sentiment analysis,
    the election presents the ideal use case. However, instead of trying to predict
    the outcome itself, we will aim to detect abnormal tweets during the US election
    using a real-time Twitter feed. We will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们回顾2016年时，我们肯定会记得这是一个许多重大地缘政治事件的时期，从英国脱欧，即英国决定退出欧盟的投票，到许多深受喜爱的名人的不幸去世，包括歌手大卫·鲍伊的突然去世（在[第6章](ch06.xhtml
    "第6章。抓取基于链接的外部数据")，*抓取基于链接的外部数据*和[第7章](ch07.xhtml "第7章。构建社区")，*构建社区*中有介绍）。然而，也许今年最显著的事件是紧张的美国总统选举及其最终结果，即唐纳德·特朗普当选总统。这将是一个长久被记住的竞选活动，尤其是因为它对社交媒体的前所未有的使用，以及在其用户中激起的激情，其中大多数人通过使用标签表达了他们的感受：要么是积极的，比如*#让美国再次伟大*或*#更强大*，要么是负面的，比如*#扔掉特朗普*或*#关起来*。由于本章是关于情感分析的，选举提供了理想的用例。但是，我们不打算试图预测结果本身，而是打算使用实时Twitter信息流来检测美国选举期间的异常推文。我们将涵盖以下主题：
- en: Acquiring Twitter data in real-time and batch
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时和批量获取Twitter数据
- en: Extracting sentiment using Stanford NLP
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用斯坦福NLP提取情感
- en: Storing sentiment time series in *Timely*
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*Timely*中存储情感时间序列
- en: Deriving features from only 140 characters using *Word2Vec*
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*Word2Vec*从仅140个字符中提取特征
- en: Introducing the concepts of graph *ergodicity* and *shortest paths*
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍图*遍历性*和*最短路径*的概念
- en: Training a KMeans model to detect potential anomalies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练KMeans模型以检测潜在的异常
- en: Visualizing models with *Embedding Projector* from *TensorFlow*
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*TensorFlow*的*嵌入式投影仪*可视化模型
- en: Following the US elections on Twitter
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Twitter上关注美国选举
- en: On November 8, 2016, American citizens went in millions to polling stations
    to cast their votes for the next President of the United States. Counting began
    almost immediately and, although not officially confirmed until sometime later,
    the forecasted result was well known by the next morning. Let's start our investigation
    a couple of days before the major event itself, on November 6, 2016, so that we
    can preserve some context in the run-up. Although we do not exactly know what
    we will find in advance, we know that *Twitter* will play an oversized role in
    the political commentary given its influence in the build-up, and it makes sense
    to start collecting data as soon as possible. In fact, data scientists may sometimes
    experience this as a *gut feeling* - a strange and often exciting notion that
    compels us to commence working on something without a clear plan or absolute justification,
    just a sense that it will pay off. And actually, this approach can be vital since,
    given the normal time required to formulate and realize such a plan and the transient
    nature of events, a major news event may occur (refer to [Chapter 10](ch10.xhtml
    "Chapter 10. Story De-duplication and Mutation"), *Story De-duplication and Mutation*),
    a new product may have been released, or the stock market may be trending differently
    (see [Chapter 12](ch12.xhtml "Chapter 12. TrendCalculus"), *TrendCalculus*); by
    this time, the original dataset may no longer be available
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年11月8日，美国公民成千上万地前往投票站，为下一任美国总统投票。计票几乎立即开始，尽管直到稍后才正式确认，但预测的结果在第二天早上就已经众所周知。让我们从主要事件发生的几天前开始调查，即2016年11月6日，这样我们就可以在选举前保留一些背景信息。尽管我们事先不知道会发现什么，但我们知道*Twitter*将在政治评论中发挥超大作用，因为它在选举前的影响力很大，所以尽快开始收集数据是有意义的。事实上，数据科学家有时可能会有这种*直觉*
    - 一种奇怪而令人兴奋的想法，促使我们开始做某事，没有明确的计划或绝对的理由，只是觉得会有回报。实际上，这种方法可能至关重要，因为在制定和实现这样的计划所需的正常时间和事件的瞬息万变之间，可能会发生重大新闻事件（参见[第10章](ch10.xhtml
    "第10章。故事去重和变异")，*故事去重和变异*），可能会发布新产品，或者股票市场可能会有不同的趋势（参见[第12章](ch12.xhtml "第12章。趋势演算")，*趋势演算*）；到那时，原始数据集可能已不再可用。
- en: Acquiring data in stream
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在流中获取数据
- en: The first action is to start acquiring Twitter data. As we plan to download
    more than 48 hours worth of tweets, the code should be robust enough to not fail
    somewhere in the middle of the process; there is nothing more frustrating than
    a fatal `NullPointerException` occurring after many hours of intense processing.
    We know we will be working on sentiment analysis at some point down the line,
    but for now we do not wish to over-complicate our code with large dependencies
    as this can decrease stability and lead to more unchecked exceptions. Instead,
    we will start by collecting and storing the data and subsequent processing will
    be done offline on the collected data, rather than applying this logic to the
    live stream.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是开始获取Twitter数据。由于我们计划下载超过48小时的推文，因此代码应该足够健壮，不会在过程中的某个地方失败；没有什么比在经过多小时的密集处理后发生致命的`NullPointerException`更令人沮丧的了。我们知道在未来某个时候我们将进行情感分析，但现在我们不希望用大型依赖项过度复杂化我们的代码，因为这可能会降低稳定性并导致更多未经检查的异常。相反，我们将开始收集和存储数据，随后的处理将在收集的数据上离线进行，而不是将此逻辑应用于实时流。
- en: We create a new Streaming context reading from Twitter 1% firehose using the
    utility methods created in [Chapter 9](ch09.xhtml "Chapter 9.  News Dictionary
    and Real-Time Tagging System") *, News Dictionary and Real-Time Tagging System*.
    We also use the excellent GSON library to serialize Java class `Status` (Java
    class embedding Twitter4J records) to JSON objects.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个新的流上下文，使用[第9章](ch09.xhtml "第9章。新闻词典和实时标记系统")中创建的实用方法从Twitter 1%的数据流中读取，新闻词典和实时标记系统。我们还使用优秀的GSON库将Java类`Status`（嵌入Twitter4J记录的Java类）序列化为JSON对象。
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We read Twitter data every 5 minutes and have a choice to optionally supply
    Twitter filters as command line arguments. Filters can be keywords such as ***Trump***
    , ***Clinton* **or ***#MAGA*** , ***#StrongerTogether*** . However, we must bear
    in mind that by doing this we may not capture all relevant tweets as we can never
    be fully up to date with the latest hashtag trends (such as ***#DumpTrump*** ,
    ***#DrainTheSwamp*** , ***#LockHerUp*** , or *#LoveTrumpsHate*) and many tweets
    will be overlooked with an inadequate filter, so we will use an empty filter list
    to ensure that we catch everything.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们每5分钟读取一次Twitter数据，并可以选择作为命令行参数提供Twitter过滤器。过滤器可以是关键词，如***Trump***，***Clinton***或***#MAGA***，***#StrongerTogether***。然而，我们必须记住，通过这样做，我们可能无法捕获所有相关的推文，因为我们可能永远无法完全跟上最新的标签趋势（如***#DumpTrump***，***#DrainTheSwamp***，***#LockHerUp***或***#LoveTrumpsHate***），并且许多推文将被忽视，因为过滤器不足，因此我们将使用一个空的过滤器列表来确保我们捕捉到一切。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We serialize our `Status` class using the GSON library and persist our JSON
    objects in HDFS. Note that the serialization occurs within a `Try` clause to ensure
    that unwanted exceptions are not thrown. Instead, we return JSON as an optional
    `String`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用GSON库对我们的`Status`类进行序列化，并将我们的JSON对象持久化在HDFS中。请注意，序列化发生在`Try`子句中，以确保不会抛出不需要的异常。相反，我们将JSON作为可选的`String`返回：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, we run our Spark Streaming context and keep it alive until a new president
    has been elected, no matter what happens!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们运行我们的Spark流上下文，并保持其活动状态，直到新总统当选，无论发生什么！
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Acquiring data in batch
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量获取数据
- en: Only 1% of tweets are retrieved through the Spark Streaming API, meaning that
    99% of records will be discarded. Although able to download around 10 million
    tweets, we can potentially download more data, but this time only for a selected
    hashtag and within a small period of time. For example, we can download all tweets
    related to the ***#LockHerUp*** or ***#BuildTheWall*** hashtags.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 只有1%的推文通过Spark流API检索，意味着99%的记录将被丢弃。虽然能够下载大约1000万条推文，但这次我们可以潜在地下载更多的数据，但这次只针对选定的标签和在短时间内。例如，我们可以下载所有与***#LockHerUp***或***#BuildTheWall***标签相关的推文。
- en: The search API
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 搜索API
- en: 'For that purpose, we consume Twitter historical data through the `twitter4j`
    Java API. This library comes as a transitive dependency of `spark-streaming-twitter_2.11`.
    To use it outside of a Spark project, the following maven dependency should be
    used:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们通过`twitter4j` Java API消耗Twitter历史数据。这个库作为`spark-streaming-twitter_2.11`的传递依赖项。要在Spark项目之外使用它，应该使用以下maven依赖项：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We create a Twitter4J client as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个Twitter4J客户端，如下所示：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we consume the `/search/tweets` service through the `Query` object:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过`Query`对象消耗`/search/tweets`服务：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, we get a list of `Status` objects that can easily be serialized using
    the GSON library introduced earlier.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到了一个`Status`对象的列表，可以很容易地使用之前介绍的GSON库进行序列化。
- en: Rate limit
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 速率限制
- en: 'Twitter is a fantastic resource for data science, but it is far from a non-profit
    organization, and as such, they know how to value and price data. Without any
    special agreement, the search API is limited to a few days retrospective, a maximum
    of 180 queries per 15 minute window and 450 records per query. This limit can
    be confirmed on both the Twitter DEV website ([https://dev.twitter.com/rest/public/rate-limits](https://dev.twitter.com/rest/public/rate-limits))
    and from the API itself using the `RateLimitStatus` class:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter是数据科学的一个很棒的资源，但它远非一个非营利组织，因此他们知道如何评估和定价数据。在没有任何特殊协议的情况下，搜索API限制为几天的回顾，每15分钟窗口最多180次查询和每次查询最多450条记录。可以在Twitter
    DEV网站（[https://dev.twitter.com/rest/public/rate-limits](https://dev.twitter.com/rest/public/rate-limits)）和API本身使用`RateLimitStatus`类来确认这一限制：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Unsurprisingly, any queries on popular terms, such as ***#MAGA*** on November
    9, 2016, hit this threshold. To avoid a rate limit exception, we have to page
    and throttle our download requests by keeping track of the maximum number of tweet
    IDs processed and monitor our status limit after each search request.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，任何关于热门词汇的查询，比如2016年11月9日的***#MAGA***，都会达到这个阈值。为了避免速率限制异常，我们必须通过跟踪处理的推文ID的最大数量，并在每次搜索请求后监视我们的状态限制来分页和限制我们的下载请求。
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With around half a billion tweets a day, it will be optimistic, if not Naive,
    to gather all US-related data. Instead, the simple ingest process detailed earlier
    should be used to intercept tweets matching specific queries only. Packaged as
    main class in an assembly jar, it can be executed as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每天大约有50亿条推文，如果收集所有与美国相关的数据，这将是乐观的，如果不是天真的。相反，应该使用前面详细介绍的简单摄取过程来拦截与特定查询匹配的推文。作为装配jar中的主类，可以按照以下方式执行：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, the `twitter.properties` file contains your Twitter API keys:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`twitter.properties`文件包含您的Twitter API密钥：
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Analysing sentiment
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析情感
- en: After 4 days of intense processing, we extracted around 10 million tweets; representing
    approximately 30 GB worth of JSON data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 经过4天的密集处理，我们提取了大约1000万条推文；大约30GB的JSON数据。
- en: Massaging Twitter data
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整理Twitter数据
- en: One of the key reasons Twitter became so popular is that any message has to
    fit into a maximum of 140 characters. The drawback is also that every message
    has to fit into a maximum of 140 characters! Hence, the result is massive increase
    in the use of abbreviations, acronyms, slang words, emoticons, and hashtags. In
    this case, the main emotion may no longer come from the text itself, but rather
    from the emoticons used ([http://dl.acm.org/citation.cfm?id=1628969](http://dl.acm.org/citation.cfm?id=1628969)),
    though some studies showed that the emoticons may sometimes lead to inadequate
    predictions in sentiment ([https://arxiv.org/pdf/1511.02556.pdf](https://arxiv.org/pdf/1511.02556.pdf)).
    Emojis are even broader than emoticons as they include pictures of animals, transportation,
    business icons, and so on. Also, while emoticons can easily be retrieved through
    simple regular expressions, emojis are usually encoded in Unicode and are more
    difficult to extract without a dedicated library.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter变得如此受欢迎的一个关键原因是任何消息都必须适应最多140个字符。缺点也是每条消息都必须适应最多140个字符！因此，结果是缩写词、首字母缩略词、俚语、表情符号和标签的使用大幅增加。在这种情况下，主要情感可能不再来自文本本身，而是来自使用的表情符号（[http://dl.acm.org/citation.cfm?id=1628969](http://dl.acm.org/citation.cfm?id=1628969)），尽管一些研究表明表情符号有时可能导致情感预测不准确（[https://arxiv.org/pdf/1511.02556.pdf](https://arxiv.org/pdf/1511.02556.pdf)）。表情符号甚至比表情符号更广泛，因为它们包括动物、交通工具、商业图标等图片。此外，虽然表情符号可以通过简单的正则表达式轻松检索，但表情符号通常以Unicode编码，并且没有专用库更难提取。
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `Emoji4J` library is easy to use (although computationally expensive) and
    given some text with emojis/emoticons, we can either `codify` - replace Unicode
    values with actual code names - or `clean` - simply remove any emojis.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`Emoji4J`库易于使用（尽管计算成本高昂），并且给定一些带有表情符号/表情符号的文本，我们可以`编码`- 用实际代码名称替换Unicode值 -
    或`清理`- 简单地删除任何表情符号。'
- en: '![Massaging Twitter data](img/B05261_11_01.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![整理Twitter数据](img/B05261_11_01.jpg)'
- en: 'Figure 1: Emoji parsing'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：表情符号解析
- en: 'So firstly, let''s clean our text from any junk (special characters, emojis,
    accents, URLs, and so on) to access plain English content:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先让我们清理文本中的任何垃圾（特殊字符、表情符号、重音符号、URL等），以便访问纯英文内容：
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s also codify and extract all emojis and emoticons and keep them aside
    as a list:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也对所有表情符号和表情进行编码和提取，并将它们作为列表放在一边：
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Writing these methods inside an *implicit class* means that they can be applied
    directly a String through a simple import statement.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些方法写在*implicit class*中意味着它们可以通过简单的导入语句直接应用于字符串。
- en: '![Massaging Twitter data](img/image_11_002.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![整理Twitter数据](img/image_11_002.jpg)'
- en: 'Figure 2: Twitter parsing'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Twitter解析
- en: Using the Stanford NLP
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用斯坦福NLP
- en: 'Our next step is to pass our cleaned text through a *Sentiment Annotator*.
    We use the Stanford NLP library for that purpose:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是通过*情感注释器*传递我们清理过的文本。我们使用斯坦福NLP库来实现这一目的：
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We create a Stanford `annotator` that tokenizes content into sentences (`tokenize`),
    splits sentences (`ssplit`), tags elements (`pos`), and lemmatizes each word (`lemma`)
    before analyzing the overall sentiment:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个斯坦福`注释器`，将内容标记为句子(`tokenize`)，分割句子(`ssplit`)，标记元素(`pos`)，并在分析整体情感之前对每个词进行词形还原(`lemma`)：
- en: '[PRE15]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Any word is replaced by its most basic form, that is, *you're* is replaced with
    *you be* and *aren't you doing* replaced with *be not you do*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 任何单词都被其最基本形式替换，即*you're*被替换为*you be*，*aren't you doing*被替换为*be not you do*。
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A sentiment spans from *Very Negative* (0.0) to *Very Positive* (4.0) and is
    averaged per sentence. As we do not get more than 1 or 2 sentences per tweet,
    we expect a very small variance; most of the tweets should be *Neutral* (around
    2.0), with only extremes to be scored (below ~1.5 or above ~2.5).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 情感范围从*非常消极*（0.0）到*非常积极*（4.0），并且每个句子的情感平均值。由于我们每条推文不会超过1或2个句子，我们预计方差非常小；大多数推文应该是*中性*（大约2.0），只有极端情感会得分（低于~1.5或高于~2.5）。
- en: Building the Pipeline
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建管道
- en: 'For each of our Twitter records (stored as JSON objects), we do the following
    things:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的每条Twitter记录（存储为JSON对象），我们要做以下事情：
- en: Parse the JSON object using `json4s` library
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`json4s`库解析JSON对象
- en: Extract the date
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取日期
- en: Extract the text
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取文本
- en: Extract the location and map it to a US state
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取位置并将其映射到美国州
- en: Clean the text
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理文本
- en: Extract emojis
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取表情符号
- en: Lemmatize text
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对文本进行词形还原
- en: Analyze sentiment
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析情感
- en: 'We then wrap all these values into the following `Tweet` case class:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将所有这些值封装到以下`Tweet`案例类中：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As mentioned in previous chapters, creating a new NLP instance wouldn''t scale
    for each record out of our dataset of 10 million records. Instead, we create only
    one `annotator` per `Iterator` (which means one per partition):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，为我们数据集中的每条记录创建一个新的NLP实例并不可行。相反，我们每个`迭代器`（即每个分区）只创建一个`注释器`：
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Using Timely as a time series database
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Timely作为时间序列数据库
- en: Now that we are able to transform raw information into a clean series of Twitter
    sentiment with parameters such as hashtags, emojis, or US states, such a time
    series should be stored reliably and made available for fast query lookups.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够将原始信息转换为一系列干净的Twitter情感，其中包括标签、表情符号或美国州等参数，这样的时间序列应该能够可靠地存储，并且可以快速查询。
- en: In the Hadoop ecosystem, *OpenTSDB* ([http://opentsdb.net/](http://opentsdb.net/))
    is the default database for storing millions of chronological data points. However,
    instead of using the obvious candidate, we will introduce one you may not have
    come across before, called *Timely* ([https://nationalsecurityagency.github.io/timely/](https://nationalsecurityagency.github.io/timely/)).
    Timely is a recently open sourced project started by the **National Security Agency**
    (**NSA**), as a clone of OpenTSDB, which uses Accumulo instead of HBase for its
    underlying storage. As you may recall, Accumulo supports cell-level security,
    and we will see this later on.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop生态系统中，*OpenTSDB*（[http://opentsdb.net/](http://opentsdb.net/)）是存储数百万时间点数据的默认数据库。然而，我们将介绍一个您可能以前没有接触过的数据库，名为*Timely*（[https://nationalsecurityagency.github.io/timely/](https://nationalsecurityagency.github.io/timely/)）。Timely是最近由**国家安全局**（**NSA**）开源的项目，作为OpenTSDB的克隆，它使用Accumulo而不是HBase作为其底层存储。正如您可能记得的那样，Accumulo支持单元级安全，我们稍后将看到这一点。
- en: Storing data
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储数据
- en: 'Each record is composed of a metric name (for example, hashtag), timestamp,
    metric value (for example, sentiment), an associated set of tags (for example,
    state), and a cell visibility:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每条记录由一个指标名称（例如，标签），时间戳，指标值（例如，情感），一组相关标签（例如，州），以及一个单元可见性组成：
- en: '[PRE19]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For this exercise, we will filter out data for tweets only mentioning Trump
    or Clinton:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将筛选出只提到特朗普或克林顿的推文数据：
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we build a `Metric` object with names `io.gzet.state.clinton` and `io.gzet.state.trump`
    and an associated visibility. For the purpose of this exercise, we will assume
    that a junior analyst without the `SECRET` permission will not be granted access
    to highly negative tweets. This allows us to demonstrate Accumulo''s excellent
    cell-level security:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个名为`io.gzet.state.clinton`和`io.gzet.state.trump`的`Metric`对象，并附带一个可见性。在这个练习中，我们假设没有`SECRET`权限的初级分析师将不被授予访问高度负面的推文。这使我们能够展示Accumulo出色的单元级安全性：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In addition, we will also need to handle *duplicate records*. In the event
    where multiple tweets are received at the exact same time (with potentially different
    sentiments), they will override an existing cell on Accumulo:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要处理*重复记录*。如果在完全相同的时间收到多条推文（可能情感不同），它们将覆盖Accumulo上的现有单元：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We insert data either from a `POST` request or simply by piping data through
    an opened socket back to the Timely server:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`POST`请求插入数据，也可以通过打开的套接字将数据传送回Timely服务器：
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Our data is now securely stored in Accumulo and available to anyone with the
    correct access permissions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在安全地存储在Accumulo中，并且任何具有正确访问权限的人都可以使用。
- en: 'We have created a series of input formats to retrieve Timely data back into
    a Spark job. This will not be covered here but can be found in our GitHub repository:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了一系列的输入格式，以便将Timely数据检索回Spark作业中。这里不会涉及，但可以在我们的GitHub存储库中找到：
- en: '[PRE24]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Tip
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: At the time of writing, Timely is still under active development and, as such,
    does not yet have a clean input/output format that can be used from Spark/MapReduce.
    The only ways to send data are via HTTP or Telnet.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Timely仍在积极开发中，因此尚无法从Spark/MapReduce中使用干净的输入/输出格式。发送数据的唯一方式是通过HTTP或Telnet。
- en: Using Grafana to visualize sentiment
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Grafana来可视化情感
- en: Timely does not come with a visualization tool as such. However, it does integrate
    well, and securely, with *Grafana* ([https://grafana.net/](https://grafana.net/))
    using the timely-grafana plugin. More information can be found on the Timely website.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Timely本身并不具备可视化工具。但是，它与*Grafana*（[https://grafana.net/](https://grafana.net/)）集成良好且安全，使用timely-grafana插件。更多信息可以在Timely网站上找到。
- en: Number of processed tweets
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理的推文数量
- en: 'As a first simple visualization, we display the number of tweets for both the
    candidates on November 8 and 9, 2016 (UTC):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一个简单的可视化，我们显示了2016年11月8日和9日（协调世界时）两位候选人的推文数量：
- en: '![Number of processed tweets](img/B05261_11_03.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![处理的推文数量](img/B05261_11_03.jpg)'
- en: 'Figure 3: Timely-processed tweets'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Timely处理的推文
- en: We observe more and more tweets related to Trump as the results of the election
    are published. On average, we observe around 6 times more Trump-related tweets
    than Clinton-related tweets.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 随着选举结果的公布，我们观察到与特朗普有关的推文越来越多。平均而言，我们观察到与克林顿相关的推文约为特朗普相关推文的6倍。
- en: Give me my Twitter account back
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还我推特账号
- en: A quick study of the sentiment shows that it's relatively negative (1.3 on an
    average) and there's no significant difference between the tweets of both the
    candidates that would have helped predict the outcome of the US election.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 情感的快速研究显示，情感相对较消极（平均为1.3），两位候选人的推文没有显著差异，这不会帮助预测美国大选的结果。
- en: '![Give me my Twitter account back](img/B05261_11_04.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![还我推特账号](img/B05261_11_04.jpg)'
- en: 'Figure 4: Timely-timeseries'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Timely时间序列
- en: However, on closer inspection, we find a truly interesting phenomenon. On November
    8, 2016, around 1pm GMT (8am EST, that is, when the first polling stations opened
    in New York), we observe a massive drop-off in the *sentiment variance*. An oddity,
    seen in the preceding figure, which can't be completely explained. We can speculate
    that either the first vote cast officially marked the end of the turbulent presidential
    campaign and was the starting point of a retrospective period after the election
    - perhaps, a more *fact-based* dialog than before - or maybe Trump's advisors
    taking away his Twitter account really was their greatest idea.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仔细观察后，我们发现了一个真正有趣的现象。2016年11月8日，格林尼治标准时间下午1点左右（东部标准时间上午8点，也就是纽约第一个投票站开放的时间），我们观察到*情感方差*出现了大幅下降。在前面的图中可以看到这种奇怪的现象，这不能完全解释。我们可以推测，要么第一张正式投票标志着动荡的总统竞选活动的结束，并且是选举后回顾期的开始
    - 也许是一个比以前更加基于事实的对话 - 或者特朗普的顾问们真的把他的Twitter账号收走是他们最伟大的主意。
- en: 'Now we give an example of the versatility of Accumulo security by logging into
    Grafana as another user, this time with no `SECRET` authorization granted. As
    expected, in the proceeding image , the sentiment looks much more positive (as
    extremely negative sentiment is hidden), hence confirming the visibility settings
    on Timely; the elegance of Accumulo speaks for itself:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们举一个Accumulo安全性的多功能性的例子，通过以另一个用户登录Grafana，这次没有授予`SECRET`授权。正如预期的那样，在接下来的图像中，情感看起来积极得多（因为极端负面情感被隐藏了），从而确认了Timely上的可见性设置；Accumulo的优雅自然显而易见：
- en: '![Give me my Twitter account back](img/image_11_005.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![还我推特账号](img/image_11_005.jpg)'
- en: 'Figure 5: Timely-timeseries for non-SECRET'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：非秘密的及时时间序列
- en: An example of how to create an Accumulo user can be found in [Chapter 7](ch07.xhtml
    "Chapter 7. Building Communities"), *Building Communities*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如何创建Accumulo用户的示例可以在[第7章](ch07.xhtml "第7章。建立社区")*建立社区*中找到。
- en: Identifying the swing states
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 识别摇摆州
- en: 'The last interesting feature we will leverage from Timely and Grafana is tree
    map aggregations. As all the US states'' names are stored as part of the metric
    attributes, we will create a simple tree map for both the candidates. The size
    of each box corresponds to the number of observations, while the color is relative
    to the observed sentiment:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Timely和Grafana中利用的最后一个有趣特性是树状图聚合。由于所有美国州的名称都存储为度量属性的一部分，我们将为两位候选人创建一个简单的树状图。每个框的大小对应观察次数，颜色与观察到的情感相关：
- en: '![Identifying the swing states](img/image_11_006.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![识别摇摆州](img/image_11_006.jpg)'
- en: 'Figure 6: Timely - Tree map of the US states for Hillary Clinton'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：及时-希拉里·克林顿的美国州树状图
- en: When we used the 2-day sentiment average previously, we couldn't differentiate
    between the republican and democrat states as the sentiment was statistically
    flat and relatively bad (1.3 on average). However, if we consider only the day
    prior to the election, then it seems much more interesting because we observed
    much more variance in our sentiment data. In the preceding image, we see Florida,
    North Carolina, and Pennsylvania - 3 of the 12 swing states-showing unexpectedly
    bad sentiment for Hillary Clinton. Could this pattern be an early indicator of
    the election outcome?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们之前使用2天情感平均值时，我们无法区分共和党和民主党州，因为情感在统计上是平坦的，而且相对糟糕（平均为1.3）。然而，如果我们只考虑选举前一天，那么它似乎更有趣，因为我们观察到情感数据中有更多的变化。在前面的图像中，我们看到佛罗里达州、北卡罗来纳州和宾夕法尼亚州-12个摇摆州中的3个-对希拉里·克林顿的情感表现出意外的糟糕。这种模式是否可能是选举结果的早期指标？
- en: Twitter and the Godwin point
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter和戈德温点
- en: With our text content properly cleaned up, we can feed a *Word2Vec* algorithm
    and attempt to understand the words in their actual *context*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通过适当清理我们的文本内容，我们可以使用*Word2Vec*算法并尝试理解单词在其实际*上下文*中的含义。
- en: Learning context
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习上下文
- en: As it says on the tin, the *Word2Ve*c algorithm transforms a word into a vector.
    The idea is that similar words will be embedded into similar vector spaces and,
    as such, will look close to one another contextually.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如它所说的，*Word2Vec*算法将一个单词转换为一个向量。其想法是相似的单词将嵌入到相似的向量空间中，并且因此在上下文中看起来彼此接近。
- en: Note
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: More information about `Word2Vec` algorithm can be found at [https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`Word2Vec`算法的更多信息可以在[https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)找到。
- en: 'Well integrated into Spark, a `Word2Vec` model can be trained as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 很好地集成到Spark中，可以通过以下方式训练*Word2Vec*模型：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here we extract each tweet as a sequence of words, only keeping records with
    at least `4` distinct words. Note that the list of all words needs to fit in memory
    as it is collected back to the driver as a map of word and vector (as an array
    of float). The vector size and learning rate can be tuned through the `setVectorSize`
    and `setLearningRate` methods respectively.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将每条推文提取为一个单词序列，只保留至少有`4`个不同单词的记录。请注意，所有单词的列表需要适应内存，因为它被收集回驱动程序作为单词和向量的映射（作为浮点数组）。向量大小和学习率可以通过`setVectorSize`和`setLearningRate`方法进行调整。
- en: 'Next, we use a Zeppelin notebook to interact with our model, sending different
    words and asking the model to obtain the closest synonyms. The results are quite
    impressive:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用Zeppelin笔记本与我们的模型进行交互，发送不同的单词并要求模型获取最接近的同义词。结果相当令人印象深刻：
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'While hashtags generally pass through standard NLP unnoticed, they do have
    a major contribution to make to tone and emotion. A tweet marked as neutral can
    be, in fact, much worse than it sounds using hashtags like *#HillaryForPrison*
    or ***#LockHerUp*** . So, let''s attempt to take this into account using an interesting
    feature called *word-vector association*. A common example of this association
    given by the original *Word2Vec* algorithm is shown here:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然标签通常在标准NLP中被忽略，但它们对语气和情感有很大的贡献。标记为中性的推文实际上可能比听起来更糟，因为使用*#HillaryForPrison*或***#LockHerUp***等标签。因此，让我们尝试使用一个有趣的特征，称为*word-vector
    association*来考虑这一点。原始*Word2Vec*算法给出的这种关联的一个常见例子如下所示：
- en: '[PRE27]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This can be translated as the following vector:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以翻译为以下向量：
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The nearest point should therefore be `[WOMEN]`. Technically speaking, this
    can be translated as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最近的点应该是`[WOMEN]`。从技术上讲，这可以翻译如下：
- en: '[PRE29]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Saving/retrieving this model can be done as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 保存/检索这个模型可以通过以下方式完成：
- en: '[PRE30]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Visualizing our model
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化我们的模型
- en: As our vectors are 100 dimensions wide, they are difficult to represent in a
    graph using traditional methods. However, you may have come across the *Tensor
    Flow* project and its recently open sourced *Embedding Projector* ([http://projector.tensorflow.org/](http://projector.tensorflow.org/)).
    This project offers a nice way to visualize our models due to its ability to quickly
    render high-dimensional data. It's easy to use as well - we simply export our
    vectors as tab-separated data points, load them into a web browser, and voila!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的向量有100个维度，使用传统方法在图中表示它们很困难。但是，您可能已经了解到*Tensor Flow*项目及其最近开源的*Embedding
    Projector*（[http://projector.tensorflow.org/](http://projector.tensorflow.org/)）。由于其快速渲染高维数据的能力，该项目提供了一种很好的可视化我们模型的方式。它也很容易使用-我们只需将我们的向量导出为制表符分隔的数据点，加载到Web浏览器中，就可以了！
- en: '![Visualizing our model](img/image_11_007.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![可视化我们的模型](img/image_11_007.jpg)'
- en: 'Figure 7: Embedding project, neighbours of Computer'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：嵌入项目，计算机的邻居
- en: '*Embedding Projector* projects high-dimensional vector onto 3D space, where
    each dimension represents one of the first three **principal components** (**PCA**).
    We can also build our own projection where we basically stretch our vectors toward
    four specific directions. In the following representation, we stretch our vectors
    left, right, up, and down to [`Trump`], [`Clinton`], [`Love`], and [`Hate`]:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌入投影仪*将高维向量投影到3D空间，其中每个维度代表前三个**主要成分**（**PCA**）之一。我们还可以构建自己的投影，基本上将我们的向量朝着四个特定方向拉伸。在下面的表示中，我们将我们的向量向左、向右、向上和向下拉伸到[`特朗普`]、[`克林顿`]、[`爱`]和[`恨`]：'
- en: '![Visualizing our model](img/B05261_11_08.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![可视化我们的模型](img/B05261_11_08.jpg)'
- en: 'Figure 8: Embedding project, custom projection'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：嵌入项目，自定义投影
- en: Now that we have a greatly simplified vector space, we can more easily understand
    each word and how it relates to its neighbors (`democrat` versus `republican`
    and `love` versus `hate`). For example, with the French election coming up next
    year, we see that France is closer to Trump than it is to Clinton. Could this
    be seen as an early indicator of the upcoming election?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个大大简化的向量空间，我们可以更容易地理解每个单词以及它与邻居的关系（`民主党`与`共和党`，`爱`与`恨`）。例如，明年法国大选即将到来，我们看到法国与特朗普的距离比与克林顿的距离更近。这可能被视为即将到来的选举的早期指标吗？
- en: Word2Graph and Godwin point
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Graph和Godwin点
- en: 'You don''t have to play around with the Twitter *Word2Vec* model for very long
    before you come across sensitive terms and references to World War II. In fact,
    this is an occurrence that was originally asserted by Mike Godwin in 1990 as Godwin''s
    Law ([https://www.wired.com/1994/10/godwin-if-2/](https://www.wired.com/1994/10/godwin-if-2/)),
    which states as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在您使用Twitter的*Word2Vec*模型很长时间之前，您可能已经遇到了敏感术语和对第二次世界大战的引用。事实上，这是迈克·戈德温在1990年最初提出的戈德温定律([https://www.wired.com/1994/10/godwin-if-2/](https://www.wired.com/1994/10/godwin-if-2/))，其规定如下：
- en: '*As an online discussion grows longer, the probability of a comparison involving
    Nazis or Hitler approaches 1*'
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*随着在线讨论的延长，涉及纳粹或希特勒的比较的概率接近1*'
- en: As of 2012, it is even part of the Oxford English Dictionary.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2012年，它甚至是牛津英语词典的一部分。
- en: '![Word2Graph and Godwin point](img/image_11_009.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![Word2Graph和Godwin点](img/image_11_009.jpg)'
- en: 'Figure 9: The Godwin law'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：戈德温定律
- en: Building a Word2Graph
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建Word2Graph
- en: 'Although more of a rhetorical device than an actual mathematical law, Godwin''s
    Law remains a fascinating anomaly and seems to be relevant to the US election.
    Naturally, we will decide to explore the idea further using the graph theory.
    The first step is to broadcast our model back to the executors and parallelize
    our list of words. For each word, we output the top five synonyms and build an
    `Edge` object with word similarity as edge weight. Let''s take a look:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管戈德温定律更多是修辞手法而不是实际的数学定律，但它仍然是一个引人入胜的异常现象，并且似乎与美国大选相关。自然地，我们决定使用图论进一步探索这个想法。第一步是将我们的模型广播回执行器并将我们的单词列表并行化。对于每个单词，我们输出前五个同义词，并构建一个带有单词相似度作为边权重的“Edge”对象。让我们来看一下：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To prove Godwin''s law, we will have to prove that no matter the input node,
    we can always find a path from that node to the *Godwin point*. In mathematical
    terms, this assumes the graph to be *ergodic*. With more than one connected component,
    our graph cannot be ergodic as some nodes will never lead to the Godwin point.
    Therefore:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明戈德温定律，我们必须证明无论输入节点如何，我们都可以从该节点找到一条通往*Godwin点*的路径。在数学术语中，这假设图是*遍历*的。由于我们有多个连接的组件，我们的图不能是遍历的，因为一些节点永远不会通向Godwin点。因此：
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As we only have one connected component, the next step is to compute the shortest
    path for each node to that Godwin point:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只有一个连接的组件，下一步是计算每个节点到Godwin点的最短路径：
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The shortest path algorithm is quite simple and can be easily implemented with
    *Pregel* using the same techniques described in [Chapter 7](ch07.xhtml "Chapter 7. Building
    Communities"), *Building Communities*. The basic approach is to start Pregel on
    the target node (our Godwin point) and send a message back to its incoming edges,
    incrementing a counter at each hop. Each node will always keep the smallest possible
    counter and propagate this value downstream to its incoming edges. The algorithm
    stops when no further edge is found.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最短路径算法非常简单，可以很容易地使用*Pregel*实现，使用[第7章](ch07.xhtml "第7章。构建社区")中描述的相同技术，*构建社区*。基本方法是在目标节点（我们的Godwin点）上启动Pregel，并向其传入的边发送消息，每个跳跃增加一个计数器。每个节点将始终保持最小可能的计数器，并将此值向下游传播到其传入的边。当找不到更多的边时，算法停止。
- en: 'We normalize this distance using a Godwin depth of 16, calculated as the maximum
    of each of the shortest paths:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Godwin深度为16来标准化这个距离，该深度是每个最短路径的最大值：
- en: '[PRE34]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following figure shows a depth of 4 - we normalize the scores of 0, 1,
    2, 3, and 4 to **0.0**, **0.25**, **0.5**, **0.75**, and **1.0** respectively:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了深度为4-我们将0、1、2、3和4的分数标准化为**0.0**、**0.25**、**0.5**、**0.75**和**1.0**：
- en: '![Building a Word2Graph](img/image_11_010.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![构建Word2Graph](img/image_11_010.jpg)'
- en: 'Figure 10: The normalized Godwin distance'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：标准化的Godwin距离
- en: Finally, we collect each vertex with its associated distance as a map. We can
    easily sort this collection from the most to the least-sensitive word, but we
    will not report our findings here (for obvious reasons!).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们收集每个顶点及其关联距离作为一个映射。我们可以很容易地将这个集合从最敏感的词到最不敏感的词进行排序，但我们不会在这里报告我们的发现（出于明显的原因！）。
- en: On November 7 and 8, 2016, this map contained all the words from our Twitter
    dictionary, implying a full ergodicity. According to Godwin's Law, any word, given
    enough time, can lead to the Godwin point. We will use this map later in the chapter
    when we build features from Twitter text content.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年11月7日和8日，这张地图包含了我们Twitter字典中的所有单词，意味着完全的遍历性。根据Godwin定律，任何单词，只要时间足够长，都可以导致Godwin点。在本章中，当我们从Twitter文本内容构建特征时，我们将稍后使用这张地图。
- en: Random walks
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机游走
- en: 'One way to simulate random walks through the *Word2Vec* algorithm is to treat
    the graph as a series of **Markov chains**. Assuming *N* random walks and a transition
    matrix *T*, we compute the transition matrix *T^N*. Given a state, *S[1]* (meaning
    a word *w[1]*), we extract the probability distribution to jump from *S[1]* to
    an *S[N]* state in *N* given transitions. In practice, given a dictionary of ~100k
    words, a dense representation of such a transition matrix will require around
    50 GB to fit in memory. We can easily build a sparse representation of *T* using
    the `IndexedRowMatrix` class from MLlib:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*Word2Vec*算法模拟随机游走的一种方法是将图形视为一系列**马尔可夫链**。假设*N*个随机游走和转移矩阵*T*，我们计算转移矩阵*T^N*。给定一个状态*S[1]*（表示一个单词*w[1]*），我们提取从*S[1]*到*N*给定转移中的*S[N]*状态跳转的概率分布。实际上，给定一个约100k个单词的字典，这样一个转移矩阵的密集表示将需要大约50GB的内存。我们可以使用MLlib中的`IndexedRowMatrix`类轻松构建*T*的稀疏表示：
- en: '[PRE35]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Unfortunately, there is no built-in method in Spark to perform matrix multiplication
    with sparse support. Therefore, the m2 matrix needs to be dense and must fit in
    memory. A solution can be to decompose this matrix (using SVD) and play with the
    symmetric property of the word2vec matrix (if word *w[1]* is a synonym to *w[2]*,
    then *w[2]* is a synonym to *w[1]*) in order to simplify this process. Using simple
    matrix algebra, one can prove that given a matrix *M*:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Spark中没有内置的方法来执行支持稀疏矩阵的矩阵乘法。因此，矩阵m2需要是密集的，并且必须适合内存。一种解决方法是分解这个矩阵（使用SVD）并利用word2vec矩阵的对称性质（如果单词*w[1]*是单词*w[2]*的同义词，那么*w[2]*是*w[1]*的同义词）来简化这个过程。使用简单的矩阵代数，可以证明给定一个矩阵*M*：
- en: '![Random walks](img/B05261_11_11.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![随机游走](img/B05261_11_11.jpg)'
- en: and *M* symmetric, then
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 和*M*对称，那么
- en: '![Random walks](img/B05261_11_12.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![随机游走](img/B05261_11_12.jpg)'
- en: for even and odd value of *n* respectively. In theory, we only need to compute
    the multiplication of *S* that is a diagonal matrix. In practice, this requires
    lot of effort and is computationally expensive for no real value (all we want
    is to generate random word association). Instead, we generate random walks using
    our Word2Vec graph, the Pregel API, and a Monte Carlo simulation. This will generate
    word associations starting from a seed `love`. The algorithm stops after 100 iterations
    or when a path reaches our Godwin point. The detail of this algorithm can be found
    in our code repository.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*n*的偶数和奇数值分别。理论上，我们只需要计算对角矩阵*S*的乘积。实际上，这需要大量的工作量，计算成本高，而且没有真正的价值（我们只是想生成随机词语关联）。相反，我们使用我们的Word2Vec图、Pregel
    API和蒙特卡洛模拟生成随机游走。这将从种子`love`开始生成词语关联。算法在100次迭代后停止，或者当路径达到我们的Godwin点时停止。该算法的详细信息可以在我们的代码库中找到。
- en: '[PRE36]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Tip
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: It is also worth mentioning that a Matrix, *M*, is said to be ergodic (hence
    also proving the Godwin Law) if there exists an integer, *n*, such that M^n> 0.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得一提的是，如果存在一个整数*n*，使得M^n> 0，则矩阵*M*被称为遍历的（因此也证明了Godwin定律）。
- en: A Small Step into sarcasm detection
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对讽刺检测的一小步
- en: Detecting sarcasm is an active area of research ([http://homes.cs.washington.edu/~nasmith/papers/bamman+smith.icwsm15.pdf](http://homes.cs.washington.edu/~nasmith/papers/bamman+smith.icwsm15.pdf)).
    In fact, detecting sarcasm is often not easy for humans, so how can it be easy
    for computers? If I say "*We will make America great again*"; without knowing
    me, observing me, or hearing the tone I'm using, how could you know if I really
    meant what I said? Now, if you were to read a tweet from me that says "*We will
    make America great again :(:(:(*", does it help in a sense?
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 检测讽刺是一个活跃的研究领域（[http://homes.cs.washington.edu/~nasmith/papers/bamman+smith.icwsm15.pdf](http://homes.cs.washington.edu/~nasmith/papers/bamman+smith.icwsm15.pdf)）。事实上，对于人类来说，检测讽刺通常并不容易，那么对于计算机来说又怎么可能容易呢？如果我说“我们将让美国再次伟大”；在不了解我、观察我或听到我使用的语气的情况下，你怎么知道我是否真的是认真的？现在，如果你读到我发的一条推文，上面写着“我们将让美国再次伟大
    :(:(:(”，这有帮助吗？
- en: Building features
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建特征
- en: We believe that sarcasm cannot be detected using plain English text only, especially
    not when the plain text fits into less than 140 characters. However, we showed
    in this chapter that emojis can play a major role in the definition of emotion.
    A naive assumption is that a tweet with both positive sentiment and negative emojis
    can potentially lead to sarcasm. In addition to the tone, we also found that some
    words were closer to some ideas/ideologies that can be classified as fairly negative.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信仅凭英文文本是无法检测出讽刺的，尤其是当纯文本不超过140个字符时。然而，我们在本章中展示了表情符号在情感定义中可以起到重要作用。一个天真的假设是，一条既有积极情绪又有负面表情符号的推文可能会导致讽刺。除了语气，我们还发现一些词语与一些可以被分类为相当负面的想法/意识形态更接近。
- en: '#LoveTrumpsHates'
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '#爱战胜仇恨'
- en: 'We have demonstrated that any word can be represented in a highly dimensional
    space between words such as [`clinton`], [`trump`], [`love`], and [`hate`]. Therefore,
    for our first extractor, we build features using the average cosine similarity
    between these words:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经证明了任何单词都可以在诸如[`clinton`]、[`trump`]、[`love`]和[`hate`]之类的单词之间的高维空间中表示。因此，对于我们的第一个提取器，我们使用这些单词之间的平均余弦相似度来构建特征：
- en: '[PRE37]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We expose this method as a user-defined function so that each tweet can be
    scored against each of these four dimensions:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这种方法公开为用户定义的函数，以便对每条推文可以根据这四个维度进行评分：
- en: '[PRE38]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Scoring Emojis
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评分表情符号
- en: 'We can extract all emojis and run a basic word count to retrieve only the most
    used emojis. We can then categorize them into five different groups: `love`, `joy`,
    `joke`, `sad`, and `cry`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提取所有表情符号并运行基本的词频统计，以检索只使用最多的表情符号。然后我们可以将它们分类为五个不同的组：`爱`，`喜悦`，`笑话`，`悲伤`和`哭泣`：
- en: '[PRE39]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Again, we expose this method as a UDF that can be applied to a DataFrame. An
    emoji score of 1.0 will be extremely positive, and 0.0 will be highly negative.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将此方法公开为可以应用于DataFrame的UDF。表情符号得分为1.0将非常积极，而0.0将非常消极。
- en: Training a KMeans model
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练KMeans模型
- en: 'With the UDFs set, we get our initial Twitter DataFrame and build the feature
    vectors:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 设置了UDF后，我们获得了我们的初始Twitter DataFrame并构建了特征向量：
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We normalize our vectors using the `Normalizer` class and feed a KMeans algorithm
    with only five clusters. Compared to [Chapter 10](ch10.xhtml "Chapter 10. Story
    De-duplication and Mutation"), *Story De-duplication and Mutation*, the KMeans
    optimization (in terms of *k*) does not really matter here as we are not interested
    in grouping tweets into categories, but rather detecting outliers (tweets that
    are far away from any cluster center):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`Normalizer`类对向量进行归一化，并将KMeans算法的输入限制为只有五个簇。与[第10章](ch10.xhtml "第10章。故事去重和变异")相比，*故事去重和变异*，这里KMeans优化（以*k*表示）并不重要，因为我们不感兴趣将推文分组到类别中，而是检测异常值（远离任何簇中心的推文）：
- en: '[PRE41]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We recommend the use of the ML package instead of MLlib. There have been huge
    improvements to this package over the past few versions of Spark in terms of dataset
    adoption and catalyst optimization. Unfortunately, there is a major limitation:
    all ML classes are defined as private and cannot be extended. As we want to extract
    the distance alongside the predicted cluster, we will have to build our own Euclidean
    measure as a UDF function:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用ML包而不是MLlib。在过去几个Spark版本中，这个包在数据集采用和催化剂优化方面有了巨大的改进。不幸的是，存在一个主要限制：所有ML类都被定义为私有的，不能被扩展。因为我们想要提取预测的簇旁边的距离，我们将不得不构建我们自己的欧几里得测量作为UDF函数：
- en: '[PRE42]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, we predict our clusters and Euclidean distances from our *featured
    tweets* DataFrame and register this DataFrame as a persistent Hive table:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们从我们的*特色推文* DataFrame中预测我们的簇和欧几里得距离，并将此DataFrame注册为持久的Hive表：
- en: '[PRE43]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Detecting anomalies
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测异常
- en: We consider a tweet as abnormal if its feature vector is too far from any known
    cluster center (in terms of Euclidean distance). Since we stored our predictions
    as a Hive table, we can sort all points through a simple SQL statement and only
    take the first few records.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征向量与任何已知簇中心的距离太远（以欧几里得距离表示），我们将认为推文是异常的。由于我们将预测存储为Hive表，我们可以通过简单的SQL语句对所有点进行排序，并只取前几条记录。
- en: 'An example is reported, as follows, when querying Hive from our Zeppelin notebook:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的Zeppelin笔记本查询Hive时，报告了一个示例，如下所示：
- en: '![Detecting anomalies](img/B05261_11_13.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![检测异常](img/B05261_11_13.jpg)'
- en: 'Figure 11: Zeppelin notebook for detecting anomalies'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：用于检测异常的Zeppelin笔记本
- en: 'Without getting into too much detail (abnormal tweets can be sensitive), a
    few examples extracted from Hive queries are listed here:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 不详细介绍（异常推文可能会敏感），以下是从Hive查询中提取的一些示例：
- en: 'good luck today america #vote #imwithher [grimacing]'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '今天祝你好运，美国 #投票 #我和她在一起 [鬼脸]'
- en: this is so great we be america great again [cry, scream]
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这太棒了，我们让美国再次变得伟大 [哭泣，尖叫]
- en: we love you sir thank you for you constant love [cry]
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们爱你先生，谢谢你的不断爱 [哭泣]
- en: 'i can not describe how incredibly happy i am right now #maga [cry, rage]'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '我无法描述我现在有多么开心 #maga [哭泣，愤怒]'
- en: Note, however, that the outliers we found were not all sarcastic tweets. We
    have only just begun our study of sarcasm, and lots of refining (including manual
    work) and probably more advanced models (such as *neural networks*) will be needed
    in order to write a comprehensive detector.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，我们发现的异常值并不都是讽刺性的推文。我们刚刚开始研究讽刺，需要进行大量的细化（包括手动工作），可能还需要更先进的模型（如*神经网络*）才能编写全面的检测器。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'The purpose of this chapter was to cover different topics around time series,
    word embedding, sentiment analysis, graph theory, and anomaly detection. It''s
    worth noting that the tweets used to illustrate the examples in no way reflect
    the authors'' own opinions: "Whether or not America will be great again is out
    of scope here":(:( - sarcasm or not?'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是涵盖关于时间序列、词嵌入、情感分析、图论和异常检测的不同主题。值得注意的是，用来说明示例的推文绝不反映作者自己的观点：“美国是否会再次变得伟大超出了本书的范围”：（：（-讽刺与否？
- en: In the next chapter, we will cover an innovative approach to detect trends out
    of Time Series data using the *TrendCalculus* method. This will be used against
    market data, but can easily be applied in different use cases, including the *Sentiment
    Time Series* we built here.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一种创新的方法，使用*TrendCalculus*方法从时间序列数据中检测趋势。这将用于市场数据，但可以轻松应用于不同的用例，包括我们在这里构建的*情感时间序列*。
