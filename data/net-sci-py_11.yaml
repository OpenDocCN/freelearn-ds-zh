- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Unsupervised Machine Learning on Network Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督机器学习在网络数据上的应用
- en: Welcome to another exciting chapter exploring network science and data science
    together. In the last chapter, we used supervised ML to train a model that was
    able to detect the revolutionaries from the book *Les Miserables*, using graph
    features alone. In this chapter, we are going to explore unsupervised ML and how
    it can also be useful in graph analysis as well as node classification with supervised
    ML.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到另一个激动人心的章节，我们将一起探索网络科学和数据科学。在上一章中，我们使用监督学习训练了一个模型，通过图特征来识别《悲惨世界》中的革命者。在本章中，我们将探讨无监督机器学习以及它如何在图分析和节点分类中与监督学习结合使用。
- en: 'The order these two chapters have been written in was intentional. I wanted
    you to learn how to create your own training data using graphs rather than being
    reliant on embeddings from unsupervised ML. The reason for this is important:
    when you rely on embeddings, you lose the ability to interpret why ML models have
    been classified the way that they have. You lose interpretability and explainability.
    The classifier essentially works as a black box, no matter which model you use.
    I wanted to show you the interpretable and explainable approach first.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这两章的编写顺序是有意安排的。我希望你能够学习如何通过图创建自己的训练数据，而不是依赖于无监督机器学习的嵌入。这一点很重要：当你依赖嵌入时，你失去了理解机器学习模型分类原因的能力。你失去了可解释性和可说明性。无论使用哪种模型，分类器基本上都像一个黑箱。我想先向你展示可解释和可说明的方法。
- en: In this chapter, we will be using a Python library called **Karate Club**. The
    library is excellent for use in both community detection and the creation of graph
    embeddings using graph ML. However, there is no way to gain insights into what
    exactly the model found useful when using this approach. So, I saved it for last.
    It can still be very effective if you don’t mind the loss of interpretability.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个名为 **Karate Club** 的 Python 库。这个库在社区检测和使用图机器学习创建图嵌入方面非常出色。然而，使用这种方法时，无法得知模型究竟发现了哪些有用的信息。因此，我将其放在最后介绍。如果你不介意失去可解释性，它仍然非常有效。
- en: This is going to be a fun chapter, as we will be pulling so many things from
    this book together. We will create a graph, create training data, do community
    detection, create graph embeddings, do some network visualization, and even do
    some node classification using supervised ML. If you started the book by reading
    this chapter, this will all probably look like magic. If you have been following
    along since [*Chapter 1*](B17105_01.xhtml#_idTextAnchor014), this should all make
    sense and be easy to understand.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的章节，因为我们将把书中的许多内容汇聚在一起。我们将创建图、生成训练数据、进行社区检测、创建图嵌入、做一些网络可视化，甚至使用监督机器学习进行节点分类。如果你从本章开始阅读这本书，可能一切看起来都像魔法。如果你从[*第
    1 章*](B17105_01.xhtml#_idTextAnchor014)就跟着阅读，那么这一切应该都能理解，而且很容易掌握。
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using the Python libraries NetworkX, pandas, scikit-learn,
    and Karate Club. Other than Karate Club, these libraries should be installed by
    now, so they should be ready for your use. The steps for installing Karate Club
    are included in this chapter. If other libraries are not installed, you can install
    Python libraries with the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 Python 库 NetworkX、pandas、scikit-learn 和 Karate Club。除了 Karate Club
    之外，这些库应该已经安装好，可以直接使用。安装 Karate Club 的步骤会在本章中介绍。如果其他库没有安装，你可以通过以下方式安装 Python 库：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For instance, to install NetworkX, you would do this:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要安装 NetworkX，你可以这样操作：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In [*Chapter 4*](B17105_04.xhtml#_idTextAnchor158), we also introduced the `draw_graph()`
    function, which uses both NetworkX and `scikit-network`. You will need that code
    anytime that we do network visualization. Keep it handy!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 4 章*](B17105_04.xhtml#_idTextAnchor158)中，我们还介绍了 `draw_graph()` 函数，它同时使用了
    NetworkX 和 `scikit-network`。每当我们进行网络可视化时，你将需要这段代码。随时准备好使用它！
- en: 'All the code is available from the GitHub repo: [https://github.com/PacktPublishing/Network-Science-with-Python](https://github.com/PacktPublishing/Network-Science-with-Python).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码都可以从 GitHub 仓库获取：[https://github.com/PacktPublishing/Network-Science-with-Python](https://github.com/PacktPublishing/Network-Science-with-Python)。
- en: What is unsupervised ML?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是无监督机器学习？
- en: 'In books and courses about ML, it is often explained that there are three different
    kinds: supervised learning, unsupervised learning, and reinforcement learning.
    Sometimes, combinations will be explained, such as semi-supervised learning. With
    supervised learning, we provide data (X) and an answer (y), and the model learns
    to make predictions. With unsupervised learning, we provide data (X), but no answer
    (y) is given. The goal is for the model to learn to identify patterns and characteristics
    of the data by itself, and then we use those patterns and characteristics for
    something else. For instance, we can use unsupervised ML to automatically learn
    the characteristics of a graph and convert those characteristics into embeddings
    that we can use in supervised ML prediction tasks. In this situation, an unsupervised
    ML algorithm is given a graph (G), and it generates embeddings that will serve
    as the training data (X) that will be used to be able to predict answers.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于机器学习的书籍和课程中，通常会解释有三种不同的类型：监督学习、无监督学习和强化学习。有时会解释组合方法，比如半监督学习。在监督学习中，我们提供数据（X）和答案（y），模型学习进行预测。而在无监督学习中，我们只提供数据（X），没有答案（y）。目标是让模型自主学习识别数据的模式和特征，然后我们可以利用这些模式和特征做其他事情。例如，我们可以使用无监督机器学习自动学习图形的特征，并将这些特征转换为可以在监督学习预测任务中使用的嵌入。在这种情况下，无监督机器学习算法接受一个图（G），并生成作为训练数据（X）的嵌入，这些数据将用于预测答案。
- en: In short, the goal of unsupervised ML is to identify patterns in data. Often,
    we call these patterns clusters, but this is not limited to clustering. Creating
    embeddings is not clustering. However, with embeddings, a complex network has
    been reduced to a few numeric features that ML will be better able to use.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，无监督机器学习的目标是识别数据中的模式。我们通常称这些模式为簇（clusters），但这不仅仅限于聚类。创建嵌入（embeddings）并不是聚类。然而，通过嵌入，一个复杂的网络被简化为几个数字特征，机器学习将更容易使用这些特征。
- en: In this chapter, you’ll see firsthand what that actually looks like, as well
    as the pros and cons of this approach. This is not all positive. There are some
    less-than-desirable side effects to using embeddings as training data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将亲眼看到这种方法实际的样子，以及它的优缺点。这并非全是积极的。使用嵌入作为训练数据会有一些不太理想的副作用。
- en: Introducing Karate Club
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Karate Club
- en: 'I’m going to showcase a Python library that we have touched on previously in
    this book: Karate Club. I mentioned it briefly in previous chapters, but now we
    are going to actually use it. I purposefully held off on going into detail before
    now, because I wanted to teach core approaches to working with networks before
    showing seemingly easy approaches to extracting communities and embeddings from
    networks using ML. This is because there are some undesirable side effects to
    using network embeddings rather than metrics extracted from a network. I will
    get into that in a bit. For now, I want to introduce this awesome, performant,
    and reliable Python library.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我将展示一本书中我们之前提到过的Python库：Karate Club。我在前几章简要提到过它，但现在我们将实际使用它。我故意推迟详细讲解，因为我想先教授一些关于如何处理网络的核心方法，再展示使用机器学习从网络中提取社区和嵌入的看似简单的方法。这是因为使用网络嵌入而不是从网络中提取的度量数据，可能会带来一些不良副作用。我稍后会详细说明。现在，我想介绍这个强大、高效且可靠的Python库。
- en: 'Karate Club’s documentation ([https://karateclub.readthedocs.io/en/latest/](https://karateclub.readthedocs.io/en/latest/))
    gives a clear and concise explanation of what the library does:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Karate Club的文档（[https://karateclub.readthedocs.io/en/latest/](https://karateclub.readthedocs.io/en/latest/)）清晰简洁地解释了该库的功能：
- en: Karate Club is an unsupervised machine learning extension library for NetworkX.
    It builds on other open source linear algebra, machine learning, and graph signal
    processing libraries such as NumPy, SciPy, Gensim, PyGSP, and Scikit-learn. Karate
    Club consists of state-of-the-art methods to do unsupervised learning on graph-structured
    data. To put it simply, it is a Swiss Army knife for small-scale graph mining
    research.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Karate Club是一个为NetworkX提供的无监督机器学习扩展库。它基于其他开源线性代数、机器学习和图信号处理库，如NumPy、SciPy、Gensim、PyGSP和Scikit-learn。Karate
    Club包含了用于对图结构数据进行无监督学习的最先进的方法。简单来说，它是小规模图挖掘研究的瑞士军刀。
- en: 'Two things should stand out from this paragraph: *unsupervised machine learning*
    and *graph*. You can think of Karate Club simply as unsupervised learning for
    graphs. The outputs of Karate Club can then be used with other libraries for actual
    prediction.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这一段中有两点应该特别引起注意：*无监督机器学习*和*图形*。你可以将Karate Club简单地看作是图形的无监督学习。然后，Karate Club的输出可以与其他库一起用于实际的预测。
- en: There are so many cool approaches to unsupervised learning stacked into Karate
    Club that it is a real thrill to learn about them. You can learn about them at
    [https://karateclub.readthedocs.io/en/latest/modules/root.html](https://karateclub.readthedocs.io/en/latest/modules/root.html).
    The thing that I love the most is that the documentation links to the original
    research papers that were written about the algorithms. This allows you to really
    get to know the processes behind unsupervised ML models. To pick out models to
    use for this chapter, I read seven research papers, and I loved every moment of
    it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Karate Club中有许多很酷的无监督学习方法，这让了解它们成为一种真正的乐趣。你可以在[https://karateclub.readthedocs.io/en/latest/modules/root.html](https://karateclub.readthedocs.io/en/latest/modules/root.html)上了解它们。我最喜欢的一点是，文档链接到关于这些算法的原始研究论文。这让你能够真正了解无监督机器学习模型背后的过程。为了选择本章使用的模型，我阅读了七篇研究论文，每一刻我都很喜欢。
- en: Another nice thing about this library is that the outputs are standardized across
    models. The embeddings generated by one model will be like the embeddings generated
    by another model. This means that you can easily experiment with different approaches
    for embeddings, and see how they affect models used for classification. We will
    do exactly that in this chapter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库的另一个优点是，输出在各个模型间是标准化的。一个模型生成的嵌入与另一个模型生成的嵌入是相似的。这意味着你可以轻松地尝试不同的嵌入方法，看看它们如何影响用于分类的模型。我们将在本章中准确地做到这一点。
- en: Finally, I’ve never seen community detection as simple as I have with Karate
    Club. Using NetworkX or other libraries for Louvain community detection can take
    a little work to set up. Using **Scalable Community Detection** (**SCD**) from
    Karate Club, you can go from a graph to identified communities in very few lines
    of code. It’s so clean.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我从未见过像Karate Club那样简单的社区检测。使用NetworkX或其他库进行Louvain社区检测需要一些工作来进行设置。而使用Karate
    Club中的**可扩展社区检测**（**SCD**），你可以通过非常少的代码行从图形转换为已识别的社区。它非常简洁。
- en: If you want to learn more about Karate Club and graph machine learning, I recommend
    the book *Graph Machine Learning*. You can pick up a copy at [https://www.amazon.com/Graph-Machine-Learning-techniques-algorithms/dp/1800204493/](https://www.amazon.com/Graph-Machine-Learning-techniques-algorithms/dp/1800204493/).
    The book goes into much greater detail on Karate Club’s capabilities than this
    chapter will be able to. It is also a good follow-up book to read after this book,
    as this book teaches the fundamentals of interacting with networks using Python,
    and *Graph Machine Learning* takes this knowledge a step further.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于Karate Club和图机器学习的内容，我推荐《*图机器学习*》这本书。你可以在[https://www.amazon.com/Graph-Machine-Learning-techniques-algorithms/dp/1800204493/](https://www.amazon.com/Graph-Machine-Learning-techniques-algorithms/dp/1800204493/)上购买。这本书比本章将要讨论的内容更详细地讲解了Karate
    Club的能力。它也是本书之后的好跟读书籍，因为本书讲解了如何使用Python与网络进行交互的基础，而《*图机器学习*》则在此基础上更进一步。
- en: Network science options
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络科学选项
- en: It is important to know that you do not *need* to use ML to work with graphs.
    ML can just be useful. There is also a blurry line between what is and isn’t ML.
    For instance, I would consider any form of community detection to be unsupervised
    ML, as these algorithms are capable of automatically identifying communities that
    exist in a network. By that definition, we could consider some of the approaches
    offered by NetworkX unsupervised ML, but they are not given the same level of
    attention in the data science community, because they are not explicitly called
    graph ML. There is a level of hype to be aware of.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，你并不*需要*使用机器学习来处理图形。机器学习只是很有用。实际上，什么是机器学习、什么不是机器学习之间有一个模糊的界限。例如，我认为任何形式的社区检测都可以视为无监督机器学习，因为这些算法能够自动识别网络中存在的社区。按照这个定义，我们可以认为NetworkX提供的一些方法是无监督机器学习，但由于它们没有明确地被称为图机器学习，它们并没有在数据科学界受到同等的关注。对此需要保持警惕。
- en: I am saying this because I want you to keep in mind that there are approaches
    that you have already learned that can eliminate the need to use what is advertised
    as graph ML. For instance, you can use Louvain to identify communities, or even
    just connected components. You can use PageRank to identify hubs – you don’t need
    embeddings for that. You can use `k_corona(0)` to identify isolates – you don’t
    need ML at all for that. You can chain together several graph features into training
    data, like we did in the last chapter. You don’t *need* to use Karate Club to
    create embeddings, and you *shouldn’t* use Karate Club embeddings if you are interested
    in any kind of model interpretability.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我之所以这么说，是希望你记住，已经学过的一些方法可以避免使用所谓的图机器学习（graph ML）。例如，你可以使用Louvain方法来识别社区，甚至只是识别连接组件。你可以使用PageRank来识别枢纽——你不需要嵌入方法来做这些。你可以使用`k_corona(0)`来识别孤立点——这完全不需要机器学习。你可以将几个图特征链在一起作为训练数据，就像我们在上一章所做的那样。如果你对模型可解释性感兴趣，你*不需要*使用卡拉泰社交网络来创建嵌入，甚至*不应该*使用卡拉泰社交网络的嵌入。
- en: Remember what you have learned in this book for interrogating and dissecting
    networks. Use what is in this chapter as a shortcut or if the science behind what
    you are doing is already figured out. Embeddings can be a nice shortcut, but any
    model using these embeddings will become a non-interpretable black box.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 记住你在本书中学到的关于分析和剖析网络的内容。将本章中的内容作为捷径使用，或者如果你所做的事情的背后原理已经弄清楚了，嵌入方法可以作为一个不错的捷径，但任何使用这些嵌入的模型都会变成一个不可解释的黑箱。
- en: 'My recommendation: use network science approaches (in NetworkX) rather than
    Karate Club when possible, but be aware of Karate Club and that it can be useful.
    This suggestion isn’t due to any disdain for Karate Club. It’s because I find
    the insights I can extract from models to be illuminating, and almost nothing
    is worth losing those insights to me. For instance, what characteristics allow
    a model to predict bots and artificial amplification?'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是：尽可能使用网络科学方法（在NetworkX中），而不是卡拉泰社交网络（Karate Club），但要注意卡拉泰社交网络，并且它可能有其用处。这个建议并不是因为我对卡拉泰社交网络有任何蔑视，而是因为我发现从模型中提取的洞察非常有启发性，几乎没有什么能让我放弃这些洞察。例如，什么特征使得一个模型能够预测机器人和人工放大效应？
- en: Loss of interpretability means that you won’t be able to understand your model’s
    behavior. This is never a good thing. This is not a dig at approaches that decompose
    graphs into embeddings, or the research around those approaches; it is just worth
    knowing that certain approaches can lead to a total loss of interpretability of
    model behavior.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性的丧失意味着你将无法理解模型的行为。这绝不是一件好事。这并不是对将图分解为嵌入方法或这些方法背后的研究的贬低；只不过值得知道，某些方法可能导致模型行为完全无法解释。
- en: Uses of unsupervised ML on network data
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在网络数据上使用无监督机器学习
- en: 'If you take a look at the Karate Club website, you will probably notice that
    the two approaches to unsupervised ML fall into two categories: identifying communities
    or creating embeddings. Unsupervised ML can be useful for creating embeddings
    not just for nodes, but also for edges or for whole graphs.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看卡拉泰社交网络的网站，你可能会注意到，针对无监督机器学习的两种方法可以分为两类：识别社区和创建嵌入。无监督机器学习不仅可以为节点创建嵌入，还可以为边或整个图创建嵌入。
- en: Community detection
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社区检测
- en: Community detection is the easiest to understand. The goal of using a community
    detection algorithm is to identify the communities of nodes that exist in a network.
    You can think of communities as clusters or clumps of nodes that interact with
    each other in some way. In social network analysis, this is called community detection,
    because it is literally about identifying communities in a social network. However,
    community detection can be useful outside of social network analysis involving
    people. Maybe it helps to think of a graph as just a social network of things
    that somehow interact. Websites interact. Countries and cities interact. People
    interact. There are communities of countries and cities that interact (allies
    and enemies). There are communities of websites that interact. There are communities
    of people that interact. It’s just about identifying groups of things that interact.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 社区检测最容易理解。使用社区检测算法的目标是识别网络中存在的节点社区。你可以把社区看作是相互以某种方式互动的节点群体。在社交网络分析中，这被称为社区检测，因为它本质上是识别社交网络中的社区。然而，社区检测不仅仅局限于涉及人类的社交网络分析。也许可以将图形看作是一个由相互作用的事物组成的社交网络。网站之间有互动。国家和城市之间有互动。人们之间有互动。国家和城市之间有互动的社区（盟友和敌人）。网站之间有互动的社区。人们之间有互动的社区。这只是识别那些相互作用的事物群体。
- en: We discuss community detection in [*Chapter 9*](B17105_09.xhtml#_idTextAnchor364)
    of this book. If you haven’t yet read that, I encourage you to go back to that
    chapter to learn more about it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书的[*第9章*](B17105_09.xhtml#_idTextAnchor364)中讨论了社区检测。如果你还没读过这一章，我建议你回去阅读，深入了解它。
- en: 'Here is an example community to refresh your memory:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个示例社区，帮助你刷新记忆：
- en: '![Figure 11.1 – Community from Les Miserables](img/B17105_11_001.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1 – 来自《悲惨世界》的社区](img/B17105_11_001.jpg)'
- en: Figure 11.1 – Community from Les Miserables
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 来自《悲惨世界》的社区
- en: Looking at this community, we can see that it is tightly knit. Each member is
    connected with every other member of the community. Other communities are more
    sparsely connected.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 看这个社区，我们可以看到它是紧密相连的。每个成员与社区中的其他所有成员都相互连接。其他社区的连接较为稀疏。
- en: Graph embeddings
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图嵌入
- en: 'I like to think about graph embeddings as a translation of a complex network
    into a data format that mathematical models will be better able to use. For instance,
    if you use a graph edge list or a NetworkX graph (G) with Random Forest, nothing
    is going to happen. The model will have no way of using the input data for anything.
    In order to make use of these models, we need to deconstruct graphs into a more
    usable format. In the previous chapter on supervised machine learning, we converted
    a graph into training data in this format:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢把图嵌入看作是将复杂的网络转化为数学模型能够更好使用的数据格式。例如，如果你使用图的边列表或NetworkX图（G）与随机森林模型，什么都不会发生。模型根本无法使用输入数据。因此，为了让这些模型发挥作用，我们需要将图形分解成更易用的格式。在前一章关于监督式机器学习的内容中，我们将图转换成了这种格式的训练数据：
- en: '![Figure 11.2 – Hand-crafted graph training data](img/B17105_11_002.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图11.2 – 手工制作的图形训练数据](img/B17105_11_002.jpg)'
- en: Figure 11.2 – Hand-crafted graph training data
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 – 手工制作的图形训练数据
- en: We also included a **label**, which is the answer that an ML model will learn
    from. After this, we tacked on the adjacency matrix for each node, so that the
    classification model would also learn from network connections.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还包括了一个**标签**，这是机器学习模型将从中学习的答案。之后，我们为每个节点附加了邻接矩阵，以便分类模型也能从网络连接中学习。
- en: As you can see, it’s easy for us to know what the features are in this training
    data. First, we have a node’s degrees, then its clustering, the number of triangles,
    its betweenness and closeness centrality, and finally, its PageRank score.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们很容易知道训练数据中的特征。首先，我们有一个节点的度数，然后是它的聚类、三角形的数量、它的中介中心性和接近中心性，最后是它的PageRank得分。
- en: 'With embeddings, all of the information in a graph is deconstructed into a
    series of embeddings. If you read the article behind the model, you can get an
    understanding of what is happening in the process, but by the time the embeddings
    are created, it’s really not super interpretable. This is what embeddings look
    like:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌入技术，图中的所有信息都被解构成一系列的嵌入。如果你阅读该模型背后的文章，你可以理解过程中的发生情况，但在嵌入创建之后，它实际上是不可直接解释的。这就是嵌入的样子：
- en: '![Figure 11.3 – Unsupervised ML graph embeddings](img/B17105_11_003.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3 – 无监督机器学习图嵌入](img/B17105_11_003.jpg)'
- en: Figure 11.3 – Unsupervised ML graph embeddings
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 无监督机器学习图嵌入
- en: Sweet, we’ve converted a graph into 1,751 columns of… what?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，我们把一个图转换成了 1,751 列……什么？
- en: Still, these embeddings are useful and can be fed directly to supervised ML
    models for prediction, and the predictions can be quite useful, even if the model
    and data are not very interpretable.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这些嵌入仍然很有用，可以直接输入到监督学习模型中进行预测，尽管模型和数据的可解释性可能不高，但这些预测仍然非常有用。
- en: But what can these embeddings be used for? They’re just a whole bunch of columns
    of numeric data with no description. How can that be useful? Well, there are two
    downstream uses, one involving more unsupervised ML for clustering, and another
    using supervised ML for classification.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这些嵌入能做什么呢？它们仅仅是一大堆没有描述的数字列。它怎么可能有用呢？好吧，有两个下游应用，一个是使用更多的无监督机器学习进行聚类，另一个是使用监督学习进行分类。
- en: Clustering
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类
- en: In **clustering**, your goal is to identify clusters, clumps, or groups of things
    that look or behave similarly. With both the hand-crafted training data and the
    Karate Club-generated embeddings, clustering is possible. Both of these datasets
    can be fed to a clustering algorithm (such as K-means) to identify similar nodes,
    for example. There are implications to using any model, though, so spend time
    learning about the models you are interested in using. For instance, to use K-means,
    you have to specify the number of clusters that you expect to exist in the data,
    and that is practically never known.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在**聚类**中，你的目标是识别出看起来或行为相似的聚类、群组或事物。通过手工制作的训练数据和 Karate Club 生成的嵌入，可以进行聚类。将这两个数据集输入聚类算法（如
    K-means）中，可以识别出相似的节点。例如，使用任何模型都有其含义，因此要花时间了解你打算使用的模型。例如，使用 K-means 时，你需要指定期望在数据中存在的聚类数量，而这个数量通常是无法事先知道的。
- en: Getting back to the `k_corona`, looked at the connected components, or sorted
    nodes by PageRank. If you are using ML, you should first ask yourself whether
    there is a network-based approach to this that eliminates the need for ML.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 回到`k_corona`，查看了连接组件，或者按 PageRank 对节点进行排序。如果你在使用机器学习，你应该首先问自己，是否有一种基于网络的方法可以消除使用机器学习的需求。
- en: Classification
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类
- en: With classification, your goal is to predict something. In a social network,
    you might want to predict who will eventually become friends, who might like to
    become friends, who might click on an advertisement, or who might want to buy
    a product. If you can make these predictions, you can automate recommendations
    and ad placement. Or, you might want to identify fraud, artificial amplification,
    or abuse. If you can make these predictions, you can automatically quarantine
    what looks like bad behavior, and automate the fielding of responding to these
    kinds of cases.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类任务中，你的目标是预测某些内容。在社交网络中，你可能想预测谁最终会成为朋友，谁可能想成为朋友，谁可能会点击广告，或者谁可能想购买某个产品。如果你能做出这些预测，就能自动化推荐和广告投放。或者，你可能想识别欺诈、人工放大或滥用行为。如果你能做出这些预测，你就可以自动隔离那些看起来像是不良行为的内容，并自动化响应这些类型的案例。
- en: Classification usually gets the most attention in ML. It deserves the glory.
    With classification, we can prevent spam from ruining our inboxes and productivity,
    we can automatically translate text from one language into another, and we can
    prevent malware from ruining our infrastructure and allowing criminals to take
    advantage of us. Classification can literally make the world a better and safer
    place when used well and responsibly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 分类通常是机器学习中最受关注的部分，它当之无愧。在分类任务中，我们可以防止垃圾邮件破坏我们的邮箱和生产力，我们可以自动将文本从一种语言翻译成另一种语言，我们还可以防止恶意软件破坏我们的基础设施并让犯罪分子利用我们。分类可以在正确且负责任的使用下，确实让世界变得更美好、更安全。
- en: In the previous chapter, we invented a fun game called “Spot the Revolutionary.”
    The same game could be played in real life with different purposes. You could
    automatically spot the influencer, spot the fraudulent behavior, spot the malware,
    or spot the cyber attack. Not all classifiers are deadly serious. Some classifiers
    help us learn more about the world around us. For instance, if you are using hand-crafted
    training data rather than embeddings, you could train a model to predict bot-like
    behavior, and then you could learn what features the model found most useful in
    identifying bots. For instance, maybe the fact that a bot account was created
    two days ago, has done zero tweets, has done 2,000 retweets, and already has 15,000
    followers could have something to do with it. A model trained on embeddings might
    tell you that embedding number 72 was useful, which means nothing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们发明了一个有趣的游戏，叫做“发现革命者”。这个游戏在现实生活中可以有不同的目的。你可以自动识别影响力人物、识别欺诈行为、识别恶意软件，或者识别网络攻击。并非所有分类器都是严肃认真的。有些分类器帮助我们更好地了解周围的世界。例如，如果你使用的是手工制作的训练数据而非嵌入数据，你可以训练一个模型来预测机器人的行为，然后你可以了解模型在识别机器人时认为最有用的特征。例如，可能是一个机器人账号创建于两天前，做了零条推文，做了2000条转发，并且已经有了15000个粉丝，这些可能与此有关。一个训练在嵌入数据上的模型可能告诉你，嵌入编号72很有用，但这没有任何意义。
- en: Alright, enough talk. Let’s get to coding and see all of this in action. For
    the rest of this chapter, we will be using Karate Club approaches.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，够多的说法了。让我们开始编码，看看这些如何实际运行。在本章剩下的部分，我们将使用Karate Club的方法。
- en: Constructing a graph
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建图
- en: Before we can do anything, we need a graph to play with. As with the last chapter,
    we will make use of the NetworkX *Les Miserables* graph, for familiarity.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行任何操作之前，我们需要一个图来进行实验。和上一章一样，我们将使用NetworkX的*《悲惨世界》*图，确保熟悉。
- en: 'First, we’ll create the graph and remove the additional fields that we don’t
    need:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建图，并去除不需要的附加字段：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you look closely, I’ve included two lines of code that create a `G_named`
    graph as a copy of G, and have converted node labels on graph G to numbers for
    use in Karate Club a bit later in this chapter. This is a required step for working
    with Karate Club.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细看，我已经包括了两行代码，创建了一个`G_named`图，作为G的副本，并且将图G中的节点标签转换为数字，以便稍后在本章中使用Karate Club。这是使用Karate
    Club时必需的步骤。
- en: 'Let’s visualize graph G for a sanity check:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先可视化一下图G，进行简单的检查：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This produces the following graph. We are not including node labels, so it will
    just be dots and lines (nodes and edges).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下图形。我们没有包含节点标签，因此它只会显示点和线（节点和边）。
- en: '![Figure 11.4 – Les Miserables network](img/B17105_11_004.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4 – 《悲惨世界》网络](img/B17105_11_004.jpg)'
- en: Figure 11.4 – Les Miserables network
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 – 《悲惨世界》网络
- en: This looks as expected. Each node has a label, but we are not showing them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来符合预期。每个节点都有标签，但我们没有显示它们。
- en: 'I have also created some training data with labels. The data is included in
    the `/data` section of the GitHub repo accompanying this book:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我还创建了一些带标签的训练数据。这些数据包含在本书附带的GitHub仓库的`/data`部分：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The process of creating the training data is a bit involved and was explained
    in the previous chapter, so please use those steps to learn how to do this manually.
    For this chapter, you can just use the CSV file to save time. Let’s check that
    the data looks correct:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 创建训练数据的过程稍微复杂一些，并且已经在上一章中解释过，所以请按照那些步骤手动学习如何操作。对于本章，你可以直接使用CSV文件来节省时间。让我们检查一下数据是否正确：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Figure 11.5 – Hand-crafted training data](img/B17105_11_005.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5 – 手工制作的训练数据](img/B17105_11_005.jpg)'
- en: Figure 11.5 – Hand-crafted training data
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 – 手工制作的训练数据
- en: In this chapter, only one of the models will use the hand-crafted training data
    as input, but we will use the labels with our embeddings. I’ll show how a bit
    later.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，只有一个模型会使用手工制作的训练数据作为输入，但我们会将标签与我们的嵌入数据一起使用。我稍后会展示如何做。
- en: With the graph and the training data, we are set to continue.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有了图和训练数据，我们就可以继续进行下去了。
- en: Community detection in action
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 社区检测实践
- en: 'With community detection, our obvious goal is to identify the communities that
    exist in a network. I explained various approaches in [*Chapter 9*](B17105_09.xhtml#_idTextAnchor364),
    *Community Detection*. In this chapter, we will make use of two Karate Club algorithms:
    SCD and EgoNetSplitter.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在社区检测中，我们的明显目标是识别网络中存在的社区。我在[*第9章*](B17105_09.xhtml#_idTextAnchor364)中解释了各种方法，*社区检测*。在本章中，我们将使用两种Karate
    Club算法：SCD和EgoNetSplitter。
- en: For this chapter, and in general, I tend to gravitate toward models that can
    scale well. If a model or algorithm is only useful on a tiny network, I’ll avoid
    it. Real networks are large, sparse, and complicated. I don’t think I’ve ever
    seen something that doesn’t scale well that is actually better than algorithms
    that do. This is especially true in community detection. The best algorithms do
    scale well. My least favorite do not scale well at all.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一章，通常来说，我倾向于选择那些能够良好扩展的模型。如果一个模型或算法仅在小型网络中有用，我会避免使用它。现实世界的网络庞大、稀疏且复杂。我不认为我见过某个扩展性差的模型比那些扩展性好的算法更优秀。在社区检测中尤为如此。最好的算法确实具有良好的扩展性。我最不喜欢的则完全不具备扩展性。
- en: SCD
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SCD
- en: The first community detection algorithm I want to showcase is SCD. You can find
    the documentation and journal article about the model at [https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我想展示的第一个社区检测算法是SCD。你可以在[https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)找到关于该模型的文档和期刊文章。
- en: This model claims to be much faster than the most accurate state-of-the-art
    community detection solutions while retaining or even exceeding their quality.
    It also claims to be able to handle graphs with billions of edges, which means
    that it can be useful with real-world networks. It claims to perform better than
    Louvain, the fastest community detection algorithm.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型声称比最准确的最先进社区检测解决方案要快得多，同时保持或甚至超过它们的质量。它还声称能够处理具有数十亿条边的图，这意味着它可以在现实世界网络中使用。它声称比Louvain算法表现得更好，后者是最快的社区检测算法。
- en: 'Those are some bold claims. Louvain is extremely useful for community detection
    for a few reasons. First, it is very fast and useful on large networks. Second,
    the Python implementation is simple to work with. So, we already know that Louvain
    is fast and easy to work with. How much better is this? Let’s try it out:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是大胆的声明。Louvain在社区检测中非常有用，原因有几点。首先，它非常快速，适用于大规模网络。其次，Python实现简单易用。因此，我们已经知道Louvain是快速且易于使用的。这个模型究竟有多好呢？让我们试试：
- en: First, make sure you have Karate Club installed on your computer. You can do
    so with a simple `pip` `install karateclub`.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，确保你已经在计算机上安装了Karate Club。你可以通过简单的`pip install karateclub`来安装。
- en: 'Now, let’s use the model. First, start with the imports. You need these two:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用该模型。首先，从导入开始。你需要这两个：
- en: '[PRE6]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now that we have those, getting the communities for our graph is as simple
    as 1, 2, 3:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 既然我们有了这些，将图的社区分配到节点上就像1、2、3一样简单：
- en: '[PRE8]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We first instantiate SCD, then we fit the graph to SCD, and then we get the
    cluster memberships for each node. Karate Club models are this simple to work
    with. You need to read the articles to know what is happening under the hood.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先实例化SCD，然后将图拟合到SCD，接着获取每个节点的聚类成员关系。Karate Club模型就是这么简单。你需要阅读文章来了解其背后的运作。
- en: 'What do the clusters look like? If we print the `clusters` variable, we should
    see this:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类是什么样的？如果我们打印`clusters`变量，应该会看到如下内容：
- en: '[PRE11]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Node zero is in cluster 34, nodes 1-3 are in cluster 14, node 4 is in cluster
    33, and so forth.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 节点零在聚类34中，节点1-3在聚类14中，节点4在聚类33中，以此类推。
- en: 'Next, we shove the clusters into a `numpy` array so that we can use the data
    with our named nodes to more easily determine what nodes belong to what clusters:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将这些聚类数据塞进一个`numpy`数组中，以便可以用我们的命名节点更容易地确定哪些节点属于哪些聚类：
- en: '[PRE24]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `clusters` variable will now look like this:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`clusters`变量看起来是这样的：
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we create a `cluster` DataFrame:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个`cluster`数据框：
- en: '[PRE26]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This gives us the following output:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们如下输出：
- en: '![Figure 11.6 – SCD cluster DataFrame](img/B17105_11_006.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 – SCD聚类数据框](img/B17105_11_006.jpg)'
- en: Figure 11.6 – SCD cluster DataFrame
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – SCD聚类数据框
- en: Great. This is much easier to understand in this format. We now have actual
    people nodes as well as the community that they belong to.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了。以这种格式呈现更容易理解。我们现在有了实际的节点和它们所属的社区。
- en: 'Let’s find the largest communities by node membership:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过节点成员关系找出最大的社区：
- en: '[PRE28]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This gives us the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们如下结果：
- en: '![Figure 11.7 – SCD communities by node count](img/B17105_11_007.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.7 – 按节点数划分的SCD社区](img/B17105_11_007.jpg)'
- en: Figure 11.7 – SCD communities by node count
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 – 按节点数划分的SCD社区
- en: 'Community 1 is the largest, with 13 members, followed by community 15, which
    has 10 members. Let’s examine both of these:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 社区1是最大的，有13个成员，其次是社区15，有10个成员。让我们一起检查这两个社区：
- en: '[PRE30]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This gives us the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了以下内容：
- en: '![Figure 11.8 – SCD community 1](img/B17105_11_008.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – SCD 社区 1](img/B17105_11_008.jpg)'
- en: Figure 11.8 – SCD community 1
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – SCD 社区 1
- en: This is excellent. This is a clear community of highly connected nodes. This
    is a densely connected community. Not all nodes are equally well connected. Some
    nodes are more central than others.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常棒。这是一个清晰的高度连接的社区。这是一个密集连接的社区，并不是所有节点都连接得一样好，一些节点比其他节点更为中心。
- en: 'Let’s look at community 15:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看社区15：
- en: '[PRE34]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The results follow:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 11.9 – SCD community 15](img/B17105_11_009.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.9 – SCD 社区 15](img/B17105_11_009.jpg)'
- en: Figure 11.9 – SCD community 15
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 – SCD 社区 15
- en: This is another high-quality community extraction. All nodes are connected to
    other nodes in the community. Some nodes are more central than others.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个高质量的社区提取。所有节点都与社区中的其他节点相连。一些节点比其他节点更为中心。
- en: 'Let’s look at one more community:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再看看一个社区：
- en: '[PRE38]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We get *Figure 11**.10*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了*图 11.10*。
- en: '![Figure 11.10 – SCD community 7](img/B17105_11_010.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.10 – SCD 社区 7](img/B17105_11_010.jpg)'
- en: Figure 11.10 – SCD community 7
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 – SCD 社区 7
- en: This is another high-quality community extraction. All nodes in the community
    are connected. In this case, this is quite a pleasing visualization to look at,
    as all nodes are equally connected. It is quite symmetric and beautiful.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个高质量的社区提取。社区中的所有节点都相互连接。在这种情况下，这是一种相当令人愉悦的可视化效果，因为所有节点的连接性相同。它非常对称且美丽。
- en: The *Les Miserables* network is tiny, so naturally, the SCD model was able to
    train on it essentially instantly.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*悲惨世界*网络非常小，因此，自然地，SCD模型几乎能立即进行训练。'
- en: One thing that I do like about this approach is that the setup is simpler than
    the approaches I explained in [*Chapter 9*](B17105_09.xhtml#_idTextAnchor364).
    I can go from a graph to communities in no time and with very little code. The
    fact that this can supposedly scale to networks with billions of edges is incredible,
    if true. It is fast, clean, and useful.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢这种方法的一个原因是，它的设置比我在[*第9章*](B17105_09.xhtml#_idTextAnchor364)中解释的其他方法简单。我可以在几乎不需要任何代码的情况下，从图形直接生成社区。事实上，如果它真的能扩展到具有数十亿条边的网络，那将是不可思议的。它快速、简洁且实用。
- en: EgoNetSplitter
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EgoNetSplitter
- en: 'The next model we will test for community detection is named EgoNetSplitter.
    You can learn about it here: [https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要测试的下一个社区检测模型叫做EgoNetSplitter。你可以在这里了解它：[https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)。
- en: 'In Jupyter, if you *Shift* + *Tab* into the model instantiation code, you can
    read about it:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter中，如果你按*Shift* + *Tab*进入模型实例化代码，你可以查看相关信息：
- en: The tool first creates the ego-nets of nodes. A persona-graph is created which
    is clustered by the Louvain method. The resulting overlapping cluster memberships
    are stored as a dictionary.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 工具首先创建节点的自我网络。然后，使用Louvain方法创建一个人物图，并对其进行聚类。生成的重叠聚类成员关系以字典形式存储。
- en: 'So, this model creates ego networks, then uses Louvain for clustering, and
    then overlapping memberships are stored as a dictionary. It’s an interesting approach
    and different from other approaches, so I thought it’d be neat to test it out
    and see how it performs. The steps are slightly different from those with SCD:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这个模型创建自我网络，然后使用Louvain进行聚类，最后将重叠的成员关系存储为字典。这是一个有趣的方式，与其他方法不同，所以我觉得测试一下它的表现会很有意思。步骤与SCD的略有不同：
- en: 'To begin, let’s get the model in place:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们先把模型搭建好：
- en: '[PRE42]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This gets us our clusters. Then, creating our `cluster` DataFrame and doing
    visualizations follows the same code as with SCD:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将得到我们的聚类。接下来，创建我们的`cluster`数据框并进行可视化的代码与SCD相同：
- en: '[PRE48]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let’s check community membership by count:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过计数来检查社区成员：
- en: '[PRE49]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We get the following output:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![Figure 11.11 – EgoNetSplitter communities by node count](img/B17105_11_011.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.11 – 按节点数量划分的EgoNetSplitter社区](img/B17105_11_011.jpg)'
- en: Figure 11.11 – EgoNetSplitter communities by node count
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11 – 按节点数量划分的EgoNetSplitter社区
- en: Already, the results look different from SCD. This should be interesting.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与 SCD 已经看起来不同了。这应该很有趣。
- en: 'Let’s take a look to see what is different. Clusters 7 and 1 are the largest,
    so let’s take a look at those two:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看有什么不同。聚类 7 和 1 是最大的，我们来看看这两个：
- en: '[PRE51]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This will draw our ego network.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这将绘制我们的自我网络。
- en: '![Figure 11.12 – EgoNetSplitter community 7](img/B17105_11_012.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.12 – EgoNetSplitter 社区 7](img/B17105_11_012.jpg)'
- en: Figure 11.12 – EgoNetSplitter community 7
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12 – EgoNetSplitter 社区 7
- en: I don’t like that. I don’t believe that the nodes on the left should be a part
    of the same community as the nodes that are connected to the densely connected
    nodes on the right. Personally, I don’t find this to be as useful as SCD’s results.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我不喜欢这样。我认为左侧的节点不应该和右侧与密集连接节点相连的节点属于同一个社区。就我个人而言，我觉得这不像 SCD 的结果那样有用。
- en: 'Let’s take a look at the next most populated cluster:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看下一个人口最多的聚类：
- en: '[PRE55]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '*Figure 11**.13* shows the resulting output.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11.13* 显示了结果输出。'
- en: '![Figure 11.13 – EgoNetSplitter community 1](img/B17105_11_013.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.13 – EgoNetSplitter 社区 1](img/B17105_11_013.jpg)'
- en: Figure 11.13 – EgoNetSplitter community 1
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13 – EgoNetSplitter 社区 1
- en: Once again, we are seeing similar behavior, where one node has been included
    in the network that really should not be. **MotherPlutarch** might be connected
    to **Mabeuf**, but she really has nothing to do with the other people in the community.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 再次出现类似的情况，其中一个节点被包含在网络中，但实际上它不应该在其中。**MotherPlutarch** 可能与 **Mabeuf** 连接，但她与社区中的其他人没有任何关系。
- en: 'Let’s take a final look at the next community:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们最后来看一下下一个社区：
- en: '[PRE59]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The code produces the following output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 代码产生了以下输出：
- en: '![Figure 11.14 – EgoNetSplitter community 5](img/B17105_11_014.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.14 – EgoNetSplitter 社区 5](img/B17105_11_014.jpg)'
- en: Figure 11.14 – EgoNetSplitter community 5
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.14 – EgoNetSplitter 社区 5
- en: Again, we see one node connected to one other node, but not connected to the
    rest of the nodes in the network.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 再次看到一个节点与另一个节点连接，但没有与网络中的其他节点连接。
- en: I don’t want to say that EgoNetSplitter is inferior to SCD or any other model.
    I’d say that I personally prefer the outputs of SCD over EgoNetSplitter for community
    detection. However, it could be argued that it is better to include the few extra
    nodes as part of the community due to their one connection than it would be to
    leave them out. It’s important to know the difference between the two approaches,
    as well as the differences in their results.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我不想说 EgoNetSplitter 比 SCD 或其他任何模型差。我个人更喜欢 SCD 的社区检测输出，而不是 EgoNetSplitter。然而，也可以说，考虑到它们之间仅有的一条连接，将这些额外的节点作为社区的一部分可能比将它们排除在外更好。了解这两种方法的区别以及它们结果的差异非常重要。
- en: However, due to the scalability claims of SCD and due to its clean separation
    of communities, I lean toward SCD for community detection.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，鉴于 SCD 的可扩展性声明以及它对社区的清晰划分，我倾向于选择 SCD 进行社区检测。
- en: Now that we have explored using unsupervised ML for community detection, let’s
    move on to using unsupervised ML for creating graph embeddings.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经探讨了使用无监督机器学习进行社区检测，那么让我们继续使用无监督机器学习来创建图嵌入。
- en: Graph embeddings in action
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图嵌入实战
- en: Now that we are past the comfort of community detection, we are getting into
    some weird territory with graph embeddings. The simplest way I think of graph
    embeddings is just the deconstruction of a complex network into a format more
    suitable for ML tasks. It’s the translation of a complex data structure into a
    less complex data structure. That’s a simple way of thinking about it.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经走出了社区检测的舒适区，现在我们进入了图嵌入的奇异领域。我理解图嵌入的最简单方式就是将一个复杂的网络解构成一个更适合机器学习任务的格式。这是将复杂的数据结构转换为不那么复杂的数据结构。这是一个简单的理解方式。
- en: Some unsupervised ML models will create more dimensions (more columns/features)
    of embeddings than others, as you will see in this section. In this section, we
    are going to create embeddings, inspect nodes that have similar embeddings, and
    then use the embeddings with supervised ML to predict “revolutionary or not,”
    like our “Spot the Revolutionary” game from the last chapter.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一些无监督机器学习模型会创建比其他模型更多的嵌入维度（更多的列/特征），正如你将在本节中看到的那样。在本节中，我们将创建嵌入，检查具有相似嵌入的节点，然后使用这些嵌入与监督机器学习结合来预测“是否革命性”，就像我们上一章的“找出革命者”游戏。
- en: We’re going to quickly run through the use of several different models – this
    chapter would be hundreds of pages long if I went into great detail about each
    model. So, to save time, I’ll provide the link to the documentation and a simple
    summary, and we’ll just do some simple comparisons. Please know that you should
    never use ML this blindly. Please read the documentation, read the articles, and
    know how the models work. I did the legwork, and you should too. Do feel free
    to just play around with different models to see how they behave. If you are just
    experimenting and not putting them into production, you aren’t going to accidentally
    cause a rip in the fabric of spacetime by playing with a `scikit-learn` model.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将快速浏览几种不同模型的使用方法——如果我详细讲解每个模型，这一章节可能会有几百页长。所以，为了节省时间，我会提供文档链接和一个简单的总结，我们将做一些简单的比较。请理解，使用机器学习时绝不可盲目操作。请阅读文档、阅读相关文章，了解模型如何工作。我已经做了很多准备工作，你也应该这样做。当然，可以随意尝试不同的模型，看看它们的表现。如果你只是进行实验，并且不将它们投入生产环境，你不会因为使用
    `scikit-learn` 模型而意外造成时空裂缝。
- en: 'We’re going to need this helper function for visualizations of the upcoming
    embeddings:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要这个辅助函数来可视化接下来的嵌入：
- en: '[PRE63]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: I need to explain a few things. First, this `draw_clustering` function uses
    `plotly` to create an interactive scatter plot. You can zoom in and out and inspect
    nodes interactively. You will need to have `plotly` installed, which can be done
    with `pip` `install plotly`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要解释一些事情。首先，这个 `draw_clustering` 函数使用 `plotly` 来创建一个交互式散点图。你可以进行缩放并交互式地检查节点。你需要安装
    `plotly`，可以通过 `pip` `install plotly` 来完成安装。
- en: Second, I’m using **Principal Component Analysis** (**PCA**) to reduce embeddings
    into two dimensions, just for the sake of visualization. PCA is also unsupervised
    learning and useful for dimension reduction. I needed to do this so that I could
    show you that these embedding models behave differently. Reducing embeddings to
    two dimensions allows me to visualize them on a scatter plot. I do not recommend
    doing PCA after creating embeddings. I am only using this process for visualization.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我使用 **主成分分析**（**PCA**）将嵌入降至二维，主要是为了可视化。PCA 也是一种无监督学习方法，适用于降维。我需要这样做，以便向你展示这些嵌入模型表现不同。将嵌入降至二维后，我可以在散点图上可视化它们。我不建议在创建嵌入后进行
    PCA。此过程仅用于可视化。
- en: FEATHER
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FEATHER
- en: The first algorithm we will use is called **FEATHER**, and you can learn about
    it at [https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的第一个算法叫做 **FEATHER**，你可以在 [https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)
    了解它。
- en: 'In Jupyter, if you *Shift* + *Tab* into the model instantiation code, you can
    read about it:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter 中，如果你 *Shift* + *Tab* 进入模型实例化代码，你可以阅读相关内容：
- en: 'An implementation of “FEATHER-N” <[https://arxiv.org/abs/2005.07959](https://arxiv.org/abs/2005.07959)>
    from the CIKM ‘20 paper “Characteristic Functions on Graphs: Birds of a Feather,
    from Statistical Descriptors to Parametric Models”. The procedure uses characteristic
    functions of node features with random walk weights to describe node neighborhoods.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: “FEATHER-N” 的实现，< [https://arxiv.org/abs/2005.07959](https://arxiv.org/abs/2005.07959)
    >，来自 CIKM ‘20 论文《图上的特征函数：同类相聚，从统计描述符到参数模型》。该过程使用节点特征的特征函数与随机游走权重来描述节点邻域。
- en: FEATHER claims to create high-quality graph representations, perform transfer
    learning effectively, and scale well to large networks. It creates node embeddings.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: FEATHER 声称能够创建高质量的图形表示，有效地执行迁移学习，并且能很好地扩展到大规模网络。它创建节点嵌入。
- en: 'This is actually a very interesting model, as it is able to take both a graph
    and additional training data for use in creating embeddings. I would love to explore
    that idea more, to see how well it does with different kinds of training data
    such as `tf-idf` or topics:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型实际上非常有趣，因为它可以同时使用图形和附加的训练数据来创建嵌入。我很想进一步探索这个想法，看看它在不同类型的训练数据（如 `tf-idf` 或主题）下表现如何：
- en: 'For now, let’s give it the hand-crafted training data that we used before:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用之前用过的手工制作的训练数据：
- en: '[PRE64]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: First, we import the model, then we instantiate it. On the `model.fit` line,
    notice that we are passing in both `G` and `clf_df`. The latter is the training
    data that we created by hand. With every other model, we only pass in G. To me,
    this is fascinating, as it seems like it’d give the model the ability to learn
    more about the network based on other contextual data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入模型，然后实例化它。在`model.fit`这一行，注意到我们同时传入了`G`和`clf_df`。后者是我们手动创建的训练数据。与其他模型不同，我们只传入`G`。对我来说，这是非常有趣的，因为它似乎让模型能够基于其他上下文数据学习更多关于网络的内容。
- en: 'Let’s visualize these embeddings to see how the model is working:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们可视化这些嵌入，以便看看模型是如何工作的：
- en: '[PRE68]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We get the following output:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![Figure 11.15 – FEATHER embeddings](img/B17105_03_007.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.15 – FEATHER 嵌入](img/B17105_03_007.jpg)'
- en: Figure 11.15 – FEATHER embeddings
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.15 – FEATHER 嵌入
- en: 'This is interesting to look at. We can see that there are several nodes that
    appear together. As this is an interactive visualization, we can inspect any of
    them. If we zoom in on the bottom-left cluster, we can see this:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣。我们可以看到有几个节点出现在一起。由于这是一个交互式可视化，我们可以检查其中任何一个。如果我们放大左下角的聚类，我们可以看到以下内容：
- en: '![Figure 11.16 – FEATHER embeddings zoomed](img/B17105_11_016.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.16 – FEATHER 嵌入放大图](img/B17105_11_016.jpg)'
- en: Figure 11.16 – FEATHER embeddings zoomed
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.16 – FEATHER 嵌入放大图
- en: It’s difficult to read, due to the overlap, but **Feuilly** is shown on the
    bottom left, close to **Prouvaire**.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 由于重叠，很难阅读，但**Feuilly**出现在左下角，靠近**Prouvaire**。
- en: 'Let’s check both of their ego networks to see what is similar:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一下他们的自我网络，看看有哪些相似之处：
- en: '[PRE70]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'That produces *Figure 11**.17*:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了*图 11**.17*：
- en: '![Figure 11.17 – Feuilly ego network](img/B17105_11_017.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.17 – Feuilly 自我网络](img/B17105_11_017.jpg)'
- en: Figure 11.17 – Feuilly ego network
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.17 – Feuilly 自我网络
- en: 'Now, let’s inspect Prouvaire’s ego network:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们检查一下Prouvaire的自我网络：
- en: '[PRE73]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: This outputs *Figure 11**.18*.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这输出了*图 11**.18*。
- en: '![Figure 11.18 – Prouvaire ego network](img/B17105_11_018.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.18 – Prouvaire 自我网络](img/B17105_11_018.jpg)'
- en: Figure 11.18 – Prouvaire ego network
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.18 – Prouvaire 自我网络
- en: Nice. The first observation is that they are both part of each other’s ego network
    and also part of each other’s community. Second, their nodes are quite connected.
    On both ego networks, both nodes show as being quite well connected and also part
    of a densely connected community.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。第一个观察结果是，它们都是彼此自我网络的一部分，并且也是彼此社区的一部分。第二，它们的节点连接性相当强。在两个自我网络中，两个节点都显示出相当强的连接性，并且都是密集连接社区的一部分。
- en: 'Let’s take a look at a few other nodes:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看一下其他一些节点：
- en: '![Figure 11.19 – FEATHER embeddings zoomed](img/B17105_11_019.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.19 – FEATHER 嵌入放大图](img/B17105_11_019.jpg)'
- en: Figure 11.19 – FEATHER embeddings zoomed
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.19 – FEATHER 嵌入放大图
- en: 'Let’s inspect the ego networks for **MotherInnocent** and **MmeMagloire**.
    **MotherInnocent** first:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一下**MotherInnocent**和**MmeMagloire**的自我网络。首先是**MotherInnocent**：
- en: '[PRE76]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '*Figure 11**.20* shows the output.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11**.20*显示了输出。'
- en: '![Figure 11.20 – MotherInnocent ego network](img/B17105_11_020.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.20 – MotherInnocent 自我网络](img/B17105_11_020.jpg)'
- en: Figure 11.20 – MotherInnocent ego network
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.20 – MotherInnocent 自我网络
- en: 'And now **MmeMagloire**:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是**MmeMagloire**：
- en: '[PRE79]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '*Figure 11**.21* shows the results.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11**.21*显示了结果。'
- en: '![Figure 11.21 – MmeMagloire ego network](img/B17105_11_021.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.21 – MmeMagloire 自我网络](img/B17105_11_021.jpg)'
- en: Figure 11.21 – MmeMagloire ego network
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.21 – MmeMagloire 自我网络
- en: '**MotherInnocent** has two edges, and **MmeMagloire** has three. Their ego
    networks are quite small. These similarities are being picked up by FEATHER and
    translated into embeddings.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**MotherInnocent**有两条边，**MmeMagloire**有三条。它们的自我网络相当小。这些相似性被FEATHER捕捉并转化为嵌入。'
- en: But what do the actual embeddings look like?
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但实际的嵌入是什么样的呢？
- en: '[PRE80]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: This produces the following DataFrame.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下数据框。
- en: '![Figure 11.22 – FEATHER embeddings DataFrame](img/B17105_11_022.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.22 – FEATHER 嵌入数据框](img/B17105_11_022.jpg)'
- en: Figure 11.22 – FEATHER embeddings DataFrame
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.22 – FEATHER 嵌入数据框
- en: The graph was translated into 1,750 embedding dimensions. In this format, you
    can think of them as columns or features. A simple network was converted into
    1,750 columns, which is quite a lot of data for such a small network. Pay attention
    to the number of dimensions created by these models as we go through the others
    after FEATHER.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图形被转化为1,750维的嵌入。在这种格式下，你可以将它们看作列或特征。一个简单的网络被转化为1,750列，这对于这么小的网络来说数据量相当大。我们在处理其他模型（如FEATHER）时，注意这些模型所创建的维度数量。
- en: These embeddings are useful for classification, so let’s do just that. I’m going
    to just throw data at a classification model and hope for the best. This is never
    a good idea other than for simple experimentation, but that is exactly what we
    are doing. I encourage you to dig deeper into any of these models that you find
    interesting.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入对分类很有用，所以我们就做这个。我将直接把数据丢给分类模型，并希望能够得到好结果。这种做法除了用于简单实验之外从来不是一个好主意，但这正是我们要做的。我鼓励你深入探索你感兴趣的任何模型。
- en: 'The preceding code already added the `label` field, but we need to create our
    `X` and `y` data for classification:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的代码已经添加了`label`字段，但我们还需要创建我们的`X`和`y`数据来进行分类：
- en: '[PRE83]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '`X` is our data, `y` is the correct answer. This is our “Spot the Revolutionary”
    training data. Nodes that are labeled as revolutionaries have a `y` of `1`. The
    rest have a `y` of `0`.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`X`是我们的数据，`y`是正确答案。这是我们的“识别革命者”训练数据。标记为革命者的节点其`y`为`1`，其余节点的`y`为`0`。'
- en: 'Let’s train a Random Forest model, as I want to show you something about interpretability:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们训练一个随机森林模型，因为我想向你展示一些关于可解释性的东西：
- en: '[PRE88]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'If we run this code, we get these results:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行这段代码，得到的结果是：
- en: '[PRE94]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Using the FEATHER embeddings as training data, the model was able to correctly
    spot the revolutionary 100% of the time on unseen data. This is a tiny network,
    though, and never, ever, *ever* trust a model that gives 100% accuracy on anything.
    A model that appears to be hitting 100% accuracy is often hiding a deeper problem,
    such as data leakage, so it’s a good idea to be skeptical of very high scores
    or to be skeptical of model results in general until the model has been thoroughly
    validated. This is a toy model. However, what this shows is that these embeddings
    can be created using a graph and that a supervised ML model can use these embeddings
    in prediction.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 使用FEATHER嵌入作为训练数据，模型能够在未见过的数据上100%准确地识别出革命者。这是一个小型网络，不过，绝对不要，*永远不要*信任任何能给出100%准确度的模型。一个看似达到100%准确度的模型，通常会隐藏一个更深层次的问题，比如数据泄漏，因此很有必要对非常高的分数持怀疑态度，或者在模型经过彻底验证之前，通常要对模型结果保持怀疑。这是一个玩具模型。然而，这表明这些嵌入可以通过图形创建，并且监督式机器学习模型可以在预测中使用这些嵌入。
- en: 'There’s a nasty downside to using these embeddings with models, though. You
    lose all interpretability. With our hand-crafted training data, as shown earlier
    in this chapter, we could see which features the model found to be most useful
    in making predictions. Let’s inspect the importances with these embeddings:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用这些嵌入与模型结合时有一个严重的缺点。你失去了所有的可解释性。在前面这一章中展示的手工制作的训练数据中，我们可以看到模型在做出预测时最有用的特征是什么。让我们检查一下这些嵌入的特征重要性：
- en: '[PRE95]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'We get this output:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到这个输出：
- en: '![Figure 11.23 – FEATHER embedding feature importance](img/B17105_11_023.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图11.23 – FEATHER嵌入特征重要性](img/B17105_11_023.jpg)'
- en: Figure 11.23 – FEATHER embedding feature importance
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.23 – FEATHER嵌入特征重要性
- en: Wonderful! Embedding 1531 was found to be slightly more useful than 1134, but
    both of these were found to be quite a bit more useful than the other embeddings!
    Excellent! This is a total loss of interpretability, but the embeddings do work.
    If you just want to go from graph to ML, this approach will work, but you end
    up with a black-box model, no matter which model you use for prediction.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！发现1531嵌入比1134稍微有用，但这两个嵌入都比其他嵌入要有用得多！太棒了！这是完全丧失了解释性，但这些嵌入确实有效。如果你只是想从图到机器学习，这种方法可以使用，但无论使用哪个模型进行预测，最终都会得到一个黑箱模型。
- en: OK, for the rest of the models, I’m going to go a lot faster. We’re going to
    reuse a lot of this code, I’m just going to do less visualization and give less
    code so that this chapter doesn’t end up being 100 pages long.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，对于接下来的模型，我将加快速度。我们将重复使用很多代码，我只是会减少可视化部分，并减少代码量，以便本章不会变成100页长。
- en: NodeSketch
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NodeSketch
- en: The next algorithm we will look at is **NodeSketch**, and you can learn about
    it at [https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要看的算法是**NodeSketch**，你可以在[https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)了解更多信息。
- en: 'In Jupyter, if you *Shift* + *Tab* into the model instantiation code, you can
    read about it:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter中，如果你按*Shift* + *Tab*进入模型实例化代码，你可以查看相关信息：
- en: An implementation of “NodeSketch” <https://exascale.info/assets/pdf/yang2019nodesketch.pdf>
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: “NodeSketch”的实现 <https://exascale.info/assets/pdf/yang2019nodesketch.pdf>
- en: 'from the KDD ‘19 paper “NodeSketch: Highly-Efficient Graph Embeddings'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '来自 KDD ‘19 论文 “NodeSketch: 高效图嵌入'
- en: via Recursive Sketching”. The procedure starts by sketching the self-loop-augmented
    adjacency matrix of the graph to output low-order node embeddings, and then recursively
    generates k-order node embeddings based on the self-loop-augmented adjacency matrix
    and (k-1)-order node embeddings.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 通过递归草图化”。该过程从绘制图的自环增强邻接矩阵开始，输出低阶节点嵌入，然后基于自环增强邻接矩阵和（k-1）阶节点嵌入递归生成 k 阶节点嵌入。
- en: 'Like FEATHER, NodeSketch also creates node embeddings:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 与 FEATHER 类似，NodeSketch 也会创建节点嵌入：
- en: 'Let’s use the model and do the visualization in one shot:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用模型，一次性进行可视化：
- en: '[PRE96]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'The following graph is the result:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是结果：
- en: '![Figure 11.24 – NodeSketch embeddings](img/B17105_11_024.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.24 – NodeSketch 嵌入](img/B17105_11_024.jpg)'
- en: Figure 11.24 – NodeSketch embeddings
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.24 – NodeSketch 嵌入
- en: As before, this visualization is interactive and you can zoom in on clusters
    of nodes for closer inspection. Let’s look at a few nodes that were found to be
    similar.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所示，这个可视化是互动式的，你可以放大节点集群以进行更细致的检查。让我们来看几个被发现相似的节点。
- en: 'First, **Eponine**:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，**Eponine**：
- en: '[PRE102]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: You can see the result in *Figure 11**.25*.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 *图 11**.25* 中看到结果。
- en: '![Figure 11.25 – Eponine ego network](img/B17105_11_025.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.25 – Eponine 自我网络](img/B17105_11_025.jpg)'
- en: Figure 11.25 – Eponine ego network
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.25 – Eponine 自我网络
- en: 'Next, **Brujon**:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，**Brujon**：
- en: '[PRE103]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Shown in *Figure 11**.26*.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 11**.26* 所示。
- en: '![Figure 11.26 – Brujon ego network](img/B17105_11_026.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.26 – Brujon 自我网络](img/B17105_11_026.jpg)'
- en: Figure 11.26 – Brujon ego network
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.26 – Brujon 自我网络
- en: Upon inspection, the ego networks look quite different, but the two nodes seem
    to have about the same number of connections, and they are part of a pretty well-connected
    community. I’m satisfied that these two nodes are pretty similar in structure
    and placement. Both nodes are also part of the same community.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 经检查，自我网络看起来差异很大，但这两个节点似乎具有相同数量的连接，并且它们都是一个连接良好的社区的一部分。我很满意这两个节点在结构和位置上非常相似。两个节点也是同一个社区的一部分。
- en: What do the embeddings look like?
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入的样子是什么？
- en: '[PRE106]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: This will show our DataFrame.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这将展示我们的数据框。
- en: '![Figure 11.27 – NodeSketch embeddings DataFrame](img/B17105_11_027.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.27 – NodeSketch 嵌入数据框](img/B17105_11_027.jpg)'
- en: Figure 11.27 – NodeSketch embeddings DataFrame
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.27 – NodeSketch 嵌入数据框
- en: Wow, this is a much simpler dataset than what FEATHER produced. **32** features
    rather than 1,750\. Also, note that the values in the embeddings are integers
    rather than floats. How well is Random Forest able to make predictions with this
    training data?
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这是一个比 FEATHER 生成的数据集简单得多。**32** 个特征，而不是 1,750 个。另外，请注意，嵌入中的值是整数而不是浮动值。随机森林能在这个训练数据上做出多好的预测？
- en: '[PRE109]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'If we run the code, we get these results:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行代码，会得到以下结果：
- en: '[PRE110]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: The model was able to predict with 98% accuracy on the training data and with
    100% accuracy on the test data. Again, never ever test a model that gives 100%
    accuracy. But this still shows that the model is able to use the embeddings.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型能够在训练数据上以 98% 的准确率进行预测，在测试数据上则达到了 100% 的准确率。同样，永远不要测试一个给出 100% 准确率的模型。但这仍然显示出该模型能够使用这些嵌入。
- en: What features did it find important?
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它发现了哪些重要特征？
- en: '[PRE111]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: This results in *Figure 11**.28*.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生 *图 11**.28*。
- en: '![Figure 11.28 – NodeSketch embedding feature importance](img/B17105_11_028.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.28 – NodeSketch 嵌入特征重要性](img/B17105_11_028.jpg)'
- en: Figure 11.28 – NodeSketch embedding feature importance
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.28 – NodeSketch 嵌入特征重要性
- en: Great. As I showed before, the use of these embeddings has turned Random Forest
    into a black-box model that we cannot get any model interpretability for. We know
    that the model found features **24** and **23** to be most useful, but we have
    no idea why. I won’t be showing feature importances after this. You get the point.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。如前所示，使用这些嵌入将随机森林变成了一个黑盒模型，我们无法从中获取任何模型可解释性。我们知道模型发现特征 **24** 和 **23** 最为有用，但我们不知道为什么。之后我不会再展示特征重要性了，你明白了。
- en: This is a cool model, and it creates simpler embeddings by default than FEATHER.
    Random Forest did well with both models’ embeddings, and we can’t say which is
    better without a lot more experimentation, which is outside of the scope of this
    chapter. Have fun experimenting!
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很酷的模型，它默认创建比 FEATHER 更简单的嵌入。随机森林在这两种模型的嵌入上表现得都很好，我们无法在没有更多实验的情况下说哪个更好，而这超出了本章的范围。祝你在实验中玩得开心！
- en: RandNE
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RandNE
- en: Next up is **RandNE**, which claims to be useful for “billion-scale network
    embeddings.” This means that this is useful for networks with either billions
    of nodes or billions of edges. It’s a claim that would make this model useful
    for large real-world networks. You can read the documentation at [https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是**RandNE**，它声称对于“百亿级网络嵌入”非常有用。这意味着它适用于具有数十亿节点或数十亿边的网络。这一声明使得该模型对于大规模的现实世界网络非常有用。你可以在[https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)阅读相关文档。
- en: 'In Jupyter, if you *Shift* + *Tab* into the model instantiation code, you can
    read about it:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter中，如果你*Shift* + *Tab*进入模型实例化代码，你可以查看相关信息：
- en: An implementation of “RandNE” <https://zw-zhang.github.io/files/2018_ICDM_RandNE.pdf>
    from the ICDM ‘18 paper “Billion-scale Network Embedding with Iterative Random
    Projection”. The procedure uses normalized adjacency matrix based smoothing on
    an orthogonalized random normally generate base node embedding matrix.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: “RandNE”实现来自ICDM '18论文《百亿级网络嵌入与迭代随机投影》 <https://zw-zhang.github.io/files/2018_ICDM_RandNE.pdf>。该过程使用基于正则化邻接矩阵的平滑方法，并在正交化的随机正态生成基节点嵌入矩阵上进行操作。
- en: 'Once again, let’s generate the embeddings and do the visualization in one shot:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，我们来一次性生成嵌入并进行可视化：
- en: '[PRE115]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'The output is the following graph:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的图表如下：
- en: '![Figure 11.29 – RandNE embeddings](img/B17105_11_029.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.29 – RandNE 嵌入](img/B17105_11_029.jpg)'
- en: Figure 11.29 – RandNE embeddings
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.29 – RandNE 嵌入
- en: 'Right away, you can see that this scatterplot looks very different from both
    FEATHER and NodeSketch. Let’s take a look at the ego networks for `Marius` and
    `MotherPlutarch`, two nodes that have been found to be similar:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以立即看到，这个散点图与FEATHER和NodeSketch的都很不同。让我们来看看`Marius`和`MotherPlutarch`这两个已被认为是相似的节点的自我网络：
- en: '[PRE121]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'We get a network output:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个网络输出：
- en: '![Figure 11.30 – Marius ego network](img/B17105_11_030.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.30 – Marius 自我网络](img/B17105_11_030.jpg)'
- en: Figure 11.30 – Marius ego network
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.30 – Marius 自我网络
- en: 'Next, **MotherPlutarch**:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是**MotherPlutarch**：
- en: '[PRE124]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'And the network is as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 网络如下所示：
- en: '![Figure 11.31 – MotherPlutarch ego network](img/B17105_11_031.jpg)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.31 – MotherPlutarch 自我网络](img/B17105_11_031.jpg)'
- en: Figure 11.31 – MotherPlutarch ego network
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.31 – MotherPlutarch 自我网络
- en: Wow, these ego networks are so different, and so are the nodes. **Marius** is
    a well-connected node, and **MotherPlutarch** has a single edge with another node.
    These are two very different nodes, and the embeddings found them to be similar.
    However, it could be due to the PCA step for the scatter plot visualization, so
    please don’t be too quick to judge RandNE from this one example. Check out some
    of the other similar nodes. I will leave this up to you, for your own practice
    and learning.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这些自我网络差异如此之大，节点也一样。**Marius**是一个连接良好的节点，而**MotherPlutarch**与另一个节点只有一条边。这是两个非常不同的节点，而嵌入结果却显示它们是相似的。不过，这可能是由于散点图可视化中的PCA步骤导致的，因此请不要仅凭这一例子就对RandNE做出过快判断。查看其他相似节点。我将把这个留给你，作为你自己的练习和学习。
- en: What do the embeddings look like?
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入结果是什么样子的？
- en: '[PRE127]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: This will show our embeddings.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示我们的嵌入。
- en: '![Figure 11.32 – RandNE embeddings DataFrame](img/B17105_11_032.jpg)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.32 – RandNE 嵌入数据框](img/B17105_11_032.jpg)'
- en: Figure 11.32 – RandNE embeddings DataFrame
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.32 – RandNE 嵌入数据框
- en: The embeddings ended up being 77 features, so this creates simpler embeddings
    by default than FEATHER. NodeSketch created 32 features, in comparison.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的嵌入是77个特征，因此它默认创建的嵌入比FEATHER更简单。相比之下，NodeSketch创建了32个特征。
- en: How well is Random Forest able to use the embeddings?
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机森林能够多好地利用这些嵌入？
- en: '[PRE128]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'If we run this code, we get these results:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行这段代码，结果将如下所示：
- en: '[PRE137]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: The model was able to predict on the test set with 98.1% accuracy, and 91.7%
    accuracy on the test set. This is worse than with FEATHER and NodeSketch embeddings,
    but it could be a fluke. I wouldn’t trust these results with so little training
    data. The model was able to successfully use the embeddings as training data.
    However, as before, if you inspect the feature importances of the embeddings,
    you will not be able to interpret the results.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在测试集上的准确率为98.1%，在测试集上的准确率为91.7%。这比使用FEATHER和NodeSketch嵌入的结果要差，但这可能是个偶然。我不会在如此少的训练数据下相信这些结果。该模型能够成功地将嵌入作为训练数据使用。然而，如前所述，如果你检查嵌入的特征重要性，你将无法解释这些结果。
- en: Other models
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他模型
- en: These are not the only three models that Karate Club has available for creating
    node embeddings. Here are two more. You can experiment with them the same way
    that we did with FEATHER, NodeSketch, and RandNE. The results with Random Forest
    for all of the embedding models were about the same. They can all be useful. I
    recommend that you get curious about Karate Club and start investigating what
    it has available.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 这些并不是Karate Club中唯一可用于创建节点嵌入的三个模型。还有两个模型。你可以像我们使用FEATHER、NodeSketch和RandNE一样进行实验。所有嵌入模型在随机森林上的结果大致相同。它们都可以是有用的。我建议你对Karate
    Club保持好奇，开始研究它提供的内容。
- en: These models do the same thing, but the implementation is different. Their implementations
    are very interesting. I recommend that you read the papers that were written about
    the approaches. You can see these as an evolution for creating node embeddings.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型做的是相同的事情，但实现方式不同。它们的实现非常有趣。我建议你阅读关于这些方法的论文。你可以把这些看作是创建节点嵌入的演变过程。
- en: GraRep
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GraRep
- en: '**GraRep** is another model we can use. You can find the documentation here:
    [https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep):'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '**GraRep** 是我们可以使用的另一个模型。你可以在这里找到文档：[https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)：'
- en: '[PRE138]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: DeepWalk
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepWalk
- en: '**DeepWalk** is another possible model we can use: [https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk):'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeepWalk** 是我们可以使用的另一个可能模型：[https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)：'
- en: '[PRE139]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: Now that we have several options for creating graph embeddings, let’s use them
    in supervised ML for classification.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们有了几种创建图嵌入的选项，接下来让我们在监督式机器学习中使用它们进行分类。
- en: Using embeddings in supervised ML
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在监督式机器学习中使用嵌入
- en: Alright! We’ve made it through some really fun hands-on work involving network
    construction, community detection, and both unsupervised and supervised ML; done
    some egocentric network visualization; and inspected the results of the use of
    different embeddings. This chapter really brought everything together. I hope
    you enjoyed the hands-on work as much as I did, and I hope you found it useful
    and informative. Before concluding this chapter, I want to go over the pros and
    cons of using embeddings the way that we have.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！我们已经完成了涉及网络构建、社区检测、无监督和监督机器学习的有趣实践工作；进行了自我中心网络可视化；并检查了使用不同嵌入的结果。本章将所有内容整合在一起。我希望你和我一样享受这次实践工作，也希望你觉得它有用且富有启发性。在结束本章之前，我想回顾一下我们使用嵌入的优缺点。
- en: Please also keep in mind that there are many other classification models we
    could have tested with, not just Random Forest. You can use these embeddings in
    a neural network if you want, or you could test them with logistic regression.
    Use what you learned here and go have as much fun as possible while learning.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们本来可以测试很多其他分类模型，而不仅仅是随机森林。你也可以将这些嵌入用于神经网络，或者用逻辑回归进行测试。利用你在这里学到的知识，去尽可能多地学习并享受乐趣。
- en: Pros and cons
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优缺点
- en: 'Let’s discuss the pros and cons of using these embeddings. First, let’s start
    with the cons. I’ve already mentioned this a few times in this chapter, so I’ll
    just repeat it one last time: if you use these embeddings, no matter how interpretable
    a classification model is, you lose all interpretability. No matter what, you
    now have a black-box model, for better or for worse. If someone asks you why your
    model is predicting a certain way, you’ll just have to shrug and say it’s magic.
    You lost the ability to inspect importances when you went with embeddings. It’s
    gone. The way back is to use hand-crafted network training data like we created
    at the beginning of this chapter and in the previous chapter, but that requires
    knowledge of network science, which is probably why some people are happy to just
    use these embeddings. This leads to the benefit of these embeddings, the pros.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来讨论使用这些嵌入式方法的优缺点。首先，我们从缺点开始。我在本章中已经提到过几次这一点，所以我再重复一次：如果你使用这些嵌入式方法，无论分类模型多么可解释，你都会失去所有的可解释性。无论如何，你现在得到的是一个黑箱模型，好的坏的都不重要。如果有人问你为什么你的模型会有某种预测，你只能耸耸肩说那是魔法。当你选择使用嵌入式方法时，你就失去了检查特征重要性的能力。它没了。回到原点的方法是使用像我们在本章开头和上一章中创建的手工制作的网络训练数据，但那需要网络科学的知识，这也许是一些人宁愿只使用这些嵌入式方法的原因。这就引出了这些嵌入式方法的优点。
- en: 'The benefit is that creating and using these embeddings is much easier and
    much faster than creating your own training data. You have to know about network
    science to know what centralities, clustering coefficients, and connected components
    are. You don’t have to know anything about network science to run this code:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 好处是，创建和使用这些嵌入式方法比创建你自己的训练数据要容易得多，速度也更快。你必须了解网络科学，才能知道什么是中心性、聚类系数和连通组件。而你不需要了解任何关于网络科学的知识，就可以运行以下代码：
- en: '[PRE140]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: It’s a problem when people blindly use stuff in data science, but it happens
    all the time. I am not excusing it. I am stating that this happens all over the
    place, and Karate Club’s embeddings give you a shortcut to not really needing
    to know anything about networks to use graph data in classification. I think that’s
    a problem, but it doesn’t just happen with graphs. It happens in NLP and ML in
    general, all the time.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们盲目使用数据科学中的工具时，这就是一个问题，但这总是发生。我并不是为此辩解。我只是陈述事实，这种情况在各个领域都存在，而空手道俱乐部的嵌入式方法让你在使用图数据进行分类时，不需要真正了解任何关于网络的知识。我认为这是一个问题，但它不仅仅发生在图数据中。在自然语言处理（NLP）和机器学习（ML）中，这种情况普遍存在。
- en: Loss of explainability and insights
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 失去可解释性和洞察力
- en: The worst part about using these embeddings is you lose all model explainability
    and insights. Personally, building models doesn’t excite me. I get excited about
    the insights I can pull from predictions and from learned importances. I get excited
    about understanding the behaviors that the models picked up on. With embeddings,
    that’s gone. I’ve thrown interpretability in the trash in the hope of quickly
    creating an effective model.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些嵌入式方法的最糟糕部分是你失去了所有模型的可解释性和洞察力。就我个人而言，构建模型并不会让我感到兴奋。我更兴奋的是通过预测和学习到的重要性获得的洞察力。我兴奋的是理解模型捕捉到的行为。使用嵌入式方法后，这一切都没有了。我把可解释性丢进了垃圾桶，希望能快速创建一个有效的模型。
- en: This is the same problem I have with **PCA**. If you use it for dimension reduction,
    you lose all interpretability. I hope you have done the science first before deciding
    to use either PCA or graph embeddings. Otherwise, it’s data alchemy, not data
    science.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我对**主成分分析（PCA）**的看法是一样的。如果你使用它来进行降维，你就会失去所有的可解释性。我希望你在决定使用PCA或图嵌入之前已经做过科学探索。否则，这就是数据炼金术，而不是数据科学。
- en: An easier workflow for classification and clustering
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更简便的分类和聚类工作流
- en: It’s not all bad, though. If you find that one of these types of embeddings
    is reliable and very high-quality, then you do have a shortcut to classification
    and clustering, so long as you don’t need the interpretability. You can go from
    a graph to classification or clustering in minutes rather than hours. That’s a
    huge speedup compared to hand-crafting training data from graph features. So,
    if you just want to see whether a graph can be useful for predicting something,
    this is one definite shortcut to building a prototype.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，并非全是坏事。如果你发现某种类型的嵌入式方法是可靠且高质量的，那么你确实可以通过它快速进行分类和聚类，只要你不需要可解释性。你可以在几分钟内从图数据到分类或聚类，而不是几个小时。这与从图特征手工制作训练数据相比，是一个巨大的加速。因此，如果你只是想看看图数据是否能用于预测某些东西，这确实是构建原型的一个捷径。
- en: It’s all pros and cons and use cases. If you need interpretability, you won’t
    get it here. If you need to move fast, this can help. And it’s very likely that
    there are insights that can be harvested from embeddings after learning more about
    them. I have seen that come true as well. Sometimes there are insights that you
    can find – it just takes an indirect approach to get to them, and hopefully, this
    book has given you some ideas.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 这全是优缺点和使用案例。如果你需要可解释性，在这里是得不到的。如果你需要快速推进，这里可以提供帮助。而且，很有可能在了解更多关于嵌入的内容之后，你会从中获得一些见解。我也确实见证过这一点。有时候，你能够找到一些见解——只是需要间接的方法才能得到它们，希望本书能给你一些启发。
- en: Summary
  id: totrans-427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: I can’t believe we’ve made it to this point. At the beginning of this book,
    this felt like an impossible task, and yet here we are. In order to do the hands-on
    exercises for this chapter, we’ve used what we learned in the previous chapters.
    I hope I have shown you how networks can be useful, and how to work with them.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我简直不敢相信我们已经走到这一步。在本书开始时，这看起来像是一个不可能完成的任务，但现在我们已经做到了。为了完成本章的实践练习，我们使用了前几章学到的内容。我希望我已经向你展示了网络如何有用，以及如何与它们一起工作。
- en: At the beginning of this book, I set out to write a practical hands-on book
    that would be code-heavy, not math-heavy. There are tons of network analysis books
    out there that have an emphasis on math but do not show actual implementation
    very well, or at all. I hope this book has effectively bridged the gap, giving
    a new skill to coders, and showing social scientists programmatic ways to take
    their network analysis to new heights. Thank you so much for reading this book!
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的开头，我决定写一本实践性强的书，内容侧重于代码，而非数学。有很多网络分析书籍，侧重于数学，但很少展示实际的实现，甚至没有。我希望本书能够有效填补这一空白，为程序员提供一项新技能，并向社会科学家展示通过编程方式提升他们的网络分析水平。非常感谢你阅读这本书！
