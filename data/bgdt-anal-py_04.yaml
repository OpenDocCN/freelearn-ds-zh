- en: '*Chapter 4*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*'
- en: Diving Deeper with Spark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解Spark
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Implement the basic Spark DataFrame API
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现基本的Spark DataFrame API。
- en: Read data and create Spark DataFrames from different data sources
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从不同数据源读取数据并创建Spark DataFrame。
- en: Manipulate and process data using different Spark DataFrame options
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的Spark DataFrame选项来操作和处理数据。
- en: Visualize data in Spark DataFrames using different plots
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的图表可视化Spark DataFrame中的数据。
- en: In this chapter, we will use the Spark as an analysis tool for big datasets.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Spark作为大数据集的分析工具。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: The last chapter introduced us to one of the most popular distributed data processing
    platforms used to process big data—Spark.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章介绍了Spark，这是一个最受欢迎的分布式数据处理平台，用于处理大数据。
- en: In this chapter, we will learn more about how to work with Spark and Spark DataFrames
    using its Python API—**PySpark**. It gives us the capability to process petabyte-scale
    data, but also implements **machine learning** (**ML**) algorithms at petabyte
    scale in real time. This chapter will focus on the data processing part using
    Spark DataFrames in PySpark.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用Python API——**PySpark**来操作Spark和Spark DataFrame。它使我们能够处理PB级数据，同时也在实时环境中实现**机器学习**（**ML**）算法。本章将重点介绍使用Spark
    DataFrame在PySpark中的数据处理部分。
- en: Note
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: We will be using the term DataFrame quite frequently during this chapter. This
    will explicitly refer to the Spark DataFrame, unless mentioned otherwise. Please
    do not confuse this with the pandas DataFrame.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们会频繁使用“DataFrame”这个术语。这里指的明确是Spark的DataFrame，除非特别说明。请不要将其与pandas的DataFrame混淆。
- en: Spark DataFrames are a distributed collection of data organized as named columns.
    They are inspired from R and Python DataFrames and have complex optimizations
    at the backend that make them fast, optimized, and scalable.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrame是分布式的数据集合，以命名列的形式组织。它们的灵感来自R和Python的DataFrame，并在后台有复杂的优化，使得它们快速、优化且可扩展。
- en: The DataFrame API was developed as part of **Project Tungsten** and is designed
    to improve the performance and scalability of Spark. It was first introduced with
    Spark 1.3.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API作为**Project Tungsten**的一部分开发，旨在提高Spark的性能和可扩展性。它首次在Spark 1.3中引入。
- en: Spark DataFrames are much easier to use and manipulate than their predecessor
    RDDs. They are *immutable,* like RDDs, and support lazy loading, which means no
    transformation is performed on the DataFrames unless an action is called. The
    execution plan for the DataFrames is prepared by Spark itself and hence is more
    optimized, making operations on DataFrames faster than those on RDDs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrame比其前身RDD更容易使用和操作。它们像RDD一样是*不可变的*，并且支持延迟加载，这意味着除非调用动作，否则对DataFrame不会执行任何变换。DataFrame的执行计划由Spark本身准备，因此更加优化，使得在DataFrame上的操作比在RDD上的操作更快。
- en: Getting Started with Spark DataFrames
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入门Spark DataFrame
- en: To get started with Spark DataFrames, we will have to create something called
    a SparkContext first. SparkContext configures the internal services under the
    hood and facilitates command execution from the Spark execution environment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用Spark DataFrame，我们首先需要创建一个名为SparkContext的对象。SparkContext配置了内部服务并促进了来自Spark执行环境的命令执行。
- en: Note
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: We will be using Spark version 2.1.1, running on Python 3.7.1\. Spark and Python
    are installed on a MacBook Pro, running macOS Mojave version 10.14.3, with a 2.7
    GHz Intel Core i5 processor and 8 GB 1867 MHz DDR3 RAM.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Spark版本2.1.1，运行在Python 3.7.1环境中。Spark和Python安装在一台MacBook Pro上，操作系统是macOS
    Mojave 10.14.3，配备2.7 GHz Intel Core i5处理器和8 GB 1867 MHz DDR3 RAM。
- en: 'The following code snippet is used to create `SparkContext`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段用于创建`SparkContext`：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In case you are working in the PySpark shell, you should skip this step, as
    the shell automatically creates the `sc (SparkContext)` variable when it is started.
    However, be sure to create the `sc` variable while creating a PySpark script or
    working with Jupyter Notebook, or your code will throw an error.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在PySpark shell中工作，应该跳过这一步，因为该shell在启动时会自动创建`sc（SparkContext）`变量。不过，在创建PySpark脚本或使用Jupyter
    Notebook时，务必创建`sc`变量，否则代码会抛出错误。
- en: 'We also need to create an `SQLContext` before we can start working with DataFrames.
    `SQLContext` in Spark is a class that provides SQL-like functionality within Spark.
    We can create `SQLContext` using `SparkContext`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用DataFrame之前，我们还需要创建一个`SQLContext`。Spark中的`SQLContext`是一个类，提供了类似SQL的功能。我们可以使用`SparkContext`来创建`SQLContext`：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There are three different ways of creating a DataFrame in Spark:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中创建 DataFrame 有三种不同的方法：
- en: We can programmatically specify the schema of the DataFrame and manually enter
    the data in it. However, since Spark is generally used to handle big data, this
    method is of little use, apart from creating data for small test/sample cases.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以以编程方式指定 DataFrame 的模式并手动输入数据。然而，由于 Spark 通常用于处理大数据，除了为小型测试/示例案例创建数据外，这种方法几乎没有其他用途。
- en: Another method to create a DataFrame is from an existing RDD object in Spark.
    This is useful, because working on a DataFrame is way easier than working directly
    with RDDs.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 DataFrame 的另一种方法是从现有的 Spark RDD 对象中创建。这是有用的，因为在 DataFrame 上工作比直接在 RDD 上工作要容易得多。
- en: We can also read the data directly from a data source to create a Spark DataFrame.
    Spark supports a variety of external data sources, including CSV, JSON, parquet,
    RDBMS tables, and Hive tables.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以直接从数据源读取数据以创建 Spark DataFrame。Spark 支持多种外部数据源，包括 CSV、JSON、Parquet、关系型数据库表和
    Hive 表。
- en: 'Exercise 24: Specifying the Schema of a DataFrame'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 24：指定 DataFrame 的模式
- en: 'In this exercise, we will create a small sample DataFrame by manually specifying
    the schema and entering data in Spark. Even though this method has little application
    in a practical scenario, it will be a good starting point in getting started with
    Spark DataFrames:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将通过手动指定模式并输入数据来创建一个小的示例 DataFrame。尽管这种方法在实际场景中的应用较少，但它是开始学习 Spark DataFrames
    的一个不错的起点：
- en: 'Importing the necessary files:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的文件：
- en: '[PRE2]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Import SQL utilities from the PySpark module and specify the schema of the
    sample DataFrame:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 PySpark 模块导入 SQL 工具并指定示例 DataFrame 的模式：
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create rows for the DataFrame as per the specified schema:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据指定模式创建 DataFrame 的行：
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Combine the rows together to create the DataFrame:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将行合并在一起创建 DataFrame：
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, show the DataFrame using the following command:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令显示 DataFrame：
- en: '[PRE6]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.1: Sample PySpark DataFrame](img/C12913_04_01.jpg)'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.1：示例 PySpark DataFrame](img/C12913_04_01.jpg)'
- en: 'Figure 4.1: Sample PySpark DataFrame'
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.1：示例 PySpark DataFrame
- en: 'Exercise 25: Creating a DataFrame from an Existing RDD'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 25：从现有 RDD 创建 DataFrame
- en: 'In this exercise, we will create a small sample DataFrame from an existing
    RDD object in Spark:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将从现有的 Spark RDD 对象创建一个小的示例 DataFrame：
- en: 'Create an RDD object that we will convert into DataFrame:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 RDD 对象，我们将把它转换成 DataFrame：
- en: '[PRE7]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Convert the RDD object into a DataFrame:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 RDD 对象转换为 DataFrame：
- en: '[PRE8]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, show the DataFrame using the following command:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令显示 DataFrame：
- en: '[PRE9]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Figure 4.2: DataFrame converted from the RDD object](img/C12913_04_02.jpg)'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.2：从 RDD 对象转换的 DataFrame](img/C12913_04_02.jpg)'
- en: 'Figure 4.2: DataFrame converted from the RDD object'
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.2：从 RDD 对象转换的 DataFrame
- en: 'Exercise 25: Creating a DataFrame Using a CSV File'
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 25：使用 CSV 文件创建 DataFrame
- en: A variety of different data sources can be used to create a DataFrame. In this
    exercise, we will use the open source Iris dataset, which can be found under datasets
    in the scikit-learn library. The Iris dataset is a multivariate dataset containing
    150 records, with 50 records for each of the 3 species of Iris flower (Iris Setosa,
    Iris Virginica, and Iris Versicolor).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用多种不同的数据源来创建 DataFrame。在本练习中，我们将使用开源的 Iris 数据集，该数据集可以在 scikit-learn 库的 datasets
    中找到。Iris 数据集是一个多变量数据集，包含 150 条记录，每个品种的 Iris 花（Iris Setosa、Iris Virginica 和 Iris
    Versicolor）有 50 条记录。
- en: 'The dataset contains five attributes for each of the Iris species, namely,
    `petal length`, `petal width`, `sepal length`, `sepal width`, and `species`. We
    have stored this dataset in an external CSV file that we will read into Spark:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含每个 Iris 品种的五个属性，即 `花瓣长度`、`花瓣宽度`、`萼片长度`、`萼片宽度` 和 `品种`。我们已将此数据集存储在一个外部 CSV
    文件中，我们将其读入 Spark：
- en: 'Download and install the PySpark CSV reader package from the Databricks website:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Databricks 网站下载并安装 PySpark CSV 阅读器包：
- en: '[PRE10]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Read the data from the CSV file into the Spark DataFrame:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据从 CSV 文件读取到 Spark DataFrame 中：
- en: '[PRE11]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, show the DataFrame using the following command:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令显示 DataFrame：
- en: '[PRE12]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Figure 4.3: Iris DataFrame, first four rows](img/C12913_04_03.jpg)'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.3：Iris DataFrame，前四行](img/C12913_04_03.jpg)'
- en: 'Figure 4.3: Iris DataFrame, first four rows'
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.3：Iris DataFrame，前四行
- en: Note for the Instructor
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 给讲师的备注
- en: Motivate the students to explore other data sources, such as tab-separated files,
    parquet files, and relational databases, as well.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 激励学生探索其他数据源，如制表符分隔文件、Parquet 文件和关系型数据库等。
- en: Writing Output from Spark DataFrames
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 Spark DataFrame 写入输出
- en: Spark gives us the ability to write the data stored in Spark DataFrames into
    a local pandas DataFrame, or write them into external structured file formats
    such as CSV. However, before converting a Spark DataFrame into a local pandas
    DataFrame, make sure that the data would fit in the local driver memory.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 使我们能够将存储在 Spark DataFrame 中的数据写入本地 pandas DataFrame，或写入 CSV 等外部结构化文件格式。然而，在将
    Spark DataFrame 转换为本地 pandas DataFrame 之前，请确保数据能够适应本地驱动程序内存。
- en: In the following exercise, we will explore how to convert the Spark DataFrame
    to a pandas DataFrame.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将探索如何将 Spark DataFrame 转换为 pandas DataFrame。
- en: 'Exercise 27: Converting a Spark DataFrame to a Pandas DataFrame'
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '练习 27: 将 Spark DataFrame 转换为 Pandas DataFrame'
- en: 'In this exercise, we will use the pre-created Spark DataFrame of the Iris dataset
    in the previous exercise, and convert it into a local pandas DataFrame. We will
    then store this DataFrame into a CSV file. Perform the following steps:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用前一个练习中预创建的 Iris 数据集的 Spark DataFrame，并将其转换为本地 pandas DataFrame。然后我们将把这个
    DataFrame 存储到 CSV 文件中。执行以下步骤：
- en: 'Convert the Spark DataFrame into a pandas DataFrame using the following command:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将 Spark DataFrame 转换为 pandas DataFrame：
- en: '[PRE13]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now use the following command to write the pandas DataFrame to a CSV file:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用以下命令将 pandas DataFrame 写入 CSV 文件：
- en: '[PRE14]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Writing the contents of a Spark DataFrame to a CSV file requires a one-liner
    using the `spark-csv` package:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将 Spark DataFrame 的内容写入 CSV 文件需要使用 `spark-csv` 包的一行代码：
- en: '`df.write.csv(''iris.csv'')`'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`df.write.csv(''iris.csv'')`'
- en: Exploring Spark DataFrames
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 Spark DataFrame
- en: One of the major advantages that the Spark DataFrames offer over the traditional
    RDDs is the ease of data use and exploration. The data is stored in a more structured
    tabular format in the DataFrames and hence is easier to make sense of. We can
    compute basic statistics such as the number of rows and columns, look at the schema,
    and compute summary statistics such as mean and standard deviation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrame 相比传统的 RDDs 的一个主要优势是数据的易用性和探索性。数据以更结构化的表格格式存储在 DataFrame 中，因此更容易理解。我们可以计算基本统计信息，如行数和列数，查看模式，并计算摘要统计信息，如均值和标准差。
- en: 'Exercise 28: Displaying Basic DataFrame Statistics'
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '练习 28: 显示基本的 DataFrame 统计信息'
- en: 'In this exercise, we will show basic DataFrame statistics of the first few
    rows of the data, and summary statistics for all the numerical DataFrame columns
    and an individual DataFrame column:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将展示数据前几行的基本 DataFrame 统计信息，以及所有数值 DataFrame 列和单个 DataFrame 列的摘要统计信息：
- en: 'Look at the DataFrame schema. The schema is displayed in a tree format on the
    console:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 DataFrame 的模式。模式会以树形结构显示在控制台上：
- en: '[PRE15]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Figure 4.4: Iris DataFrame schema](img/Image39860.jpg)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.4: Iris DataFrame 模式](img/Image39860.jpg)'
- en: 'Figure 4.4: Iris DataFrame schema'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.4: Iris DataFrame 模式'
- en: 'Now, use the following command to print the column names of the Spark DataFrame:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令打印 Spark DataFrame 的列名：
- en: '[PRE16]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Figure 4.5: Iris column names](img/C12913_04_05.jpg)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.5: Iris 列名](img/C12913_04_05.jpg)'
- en: 'Figure 4.5: Iris column names'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.5: Iris 列名'
- en: 'To retrieve the number of rows and columns present in the Spark DataFrame,
    use the following command:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取 Spark DataFrame 中行数和列数，请使用以下命令：
- en: '[PRE17]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s fetch the first *n* rows of the data. We can do this by using the `head()`
    method. However, we use the `show()` method as it displays the data in a better
    format:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们获取数据的前 *n* 行。我们可以使用 `head()` 方法来实现。然而，我们使用 `show()` 方法，因为它可以以更好的格式显示数据：
- en: '[PRE18]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.6: Iris DataFrame, first four rows](img/C12913_04_06.jpg)'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.6: Iris DataFrame，前四行](img/C12913_04_06.jpg)'
- en: 'Figure 4.6: Iris DataFrame, first four rows'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.6: Iris DataFrame，前四行'
- en: 'Now, compute the summary statistics, such as mean and standard deviation, for
    all the numerical columns in the DataFrame:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，计算所有数值列的摘要统计信息，如均值和标准差：
- en: '[PRE19]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.7: Iris DataFrame, summary statistics](img/C12913_04_07.jpg)'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.7: Iris DataFrame，摘要统计](img/C12913_04_07.jpg)'
- en: 'Figure 4.7: Iris DataFrame, summary statistics'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.7: Iris DataFrame，摘要统计'
- en: 'To compute the summary statistics for an individual numerical column of a Spark
    DataFrame, use the following command:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要计算 Spark DataFrame 中某一数值列的摘要统计信息，请使用以下命令：
- en: '[PRE20]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.8: Iris DataFrame, summary statistics of Sepalwidth column](img/C12913_04_08.jpg)'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.8: Iris DataFrame，Sepalwidth 列的摘要统计](img/C12913_04_08.jpg)'
- en: 'Figure 4.8: Iris DataFrame, summary statistics of Sepalwidth column'
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.8: Iris DataFrame，Sepalwidth 列的摘要统计'
- en: 'Activity 9: Getting Started with Spark DataFrames'
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 9：Spark DataFrames 入门
- en: 'In this activity, we will use the concepts learned in the previous sections
    and create a Spark DataFrame using all three methods. We will also compute DataFrame
    statistics, and finally, write the same data into a CSV file. Feel free to use
    any open source dataset for this activity:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将利用前几部分学习的概念，使用三种方法创建一个 Spark DataFrame。我们还将计算 DataFrame 的统计信息，最后将相同的数据写入
    CSV 文件。你可以随意使用任何开源数据集来完成这个活动：
- en: Create a sample DataFrame by manually specifying the schema.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过手动指定模式创建一个示例 DataFrame。
- en: Create a sample DataFrame from an existing RDD.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从现有的 RDD 创建一个示例 DataFrame。
- en: Create a sample DataFrame by reading the data from a CSV file.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从 CSV 文件中读取数据创建一个示例 DataFrame。
- en: Print the first seven rows of the sample DataFrame read in step 3.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印第3步中读取的示例 DataFrame 的前七行。
- en: Print the schema of the sample DataFrame read in step 3.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印第3步中读取的示例 DataFrame 的模式。
- en: Print the number of rows and columns in the sample DataFrame.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印示例 DataFrame 中的行数和列数。
- en: Print the summary statistics of the DataFrame and any 2 individual numerical
    columns.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 DataFrame 的摘要统计信息以及任何两个单独的数值列。
- en: Write the first 7 rows of the sample DataFrame to a CSV file using both methods
    mentioned in the exercises.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用练习中提到的两种方法将示例 DataFrame 的前7行写入 CSV 文件。
- en: Note
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 215.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 215 页找到。
- en: Data Manipulation with Spark DataFrames
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Spark DataFrame 进行数据操作
- en: Data manipulation is a prerequisite for any data analysis. To draw meaningful
    insights from the data, we first need to understand, process, and massage the
    data. But this step becomes particularly hard with the increase in the size of
    data. Due to the scale of data, even simple operations such as filtering and sorting
    become complex coding problems. Spark DataFrames make data manipulation on big
    data a piece of cake.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 数据操作是任何数据分析的前提。为了从数据中提取有意义的洞察，我们首先需要理解、处理和调整数据。但随着数据量的增加，这一步变得尤为困难。由于数据的规模，即使是简单的操作，如过滤和排序，也会变成复杂的编码问题。Spark
    DataFrame 使得在大数据上进行数据操作变得轻而易举。
- en: Manipulating the data in Spark DataFrames is quite like working on regular pandas
    DataFrames. Most of the data manipulation operations on Spark DataFrames can be
    done using simple and intuitive one-liners. We will use the Spark DataFrame containing
    the Iris dataset that we created in previous exercises for these data manipulation
    exercises.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark DataFrame 中进行数据操作与在常规 pandas DataFrame 中的操作非常相似。大多数 Spark DataFrame
    的数据操作都可以通过简单直观的单行代码完成。我们将使用在之前练习中创建的包含鸢尾花数据集的 Spark DataFrame 来进行这些数据操作练习。
- en: 'Exercise 29: Selecting and Renaming Columns from the DataFrame'
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 29：选择并重命名 DataFrame 中的列
- en: In this exercise, we will first rename the column using the `withColumnRenamed`
    method and then select and print the schema using the `select` method.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将首先使用 `withColumnRenamed` 方法重命名列，然后使用 `select` 方法选择并打印模式。
- en: 'Perform the following steps:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'Rename the columns of a Spark DataFrame using the `withColumnRenamed()` method:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `withColumnRenamed()` 方法重命名 Spark DataFrame 的列：
- en: '[PRE21]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Spark does not recognize column names containing a period(`.`). Make sure to
    rename them using this method.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Spark 无法识别包含点号（`.`）的列名。确保使用此方法重命名它们。
- en: 'Select a single column or multiple columns from a Spark DataFrame using the
    `select` method:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `select` 方法从 Spark DataFrame 中选择单个列或多个列：
- en: '[PRE22]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Figure 4.9: Iris DataFrame, Sepalwidth and Sepallength column](img/C12913_04_09.jpg)'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.9：鸢尾花 DataFrame，Sepalwidth 和 Sepallength 列](img/C12913_04_09.jpg)'
- en: 'Figure 4.9: Iris DataFrame, Sepalwidth and Sepallength column'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.9：鸢尾花 DataFrame，Sepalwidth 和 Sepallength 列
- en: 'Exercise 30: Adding and Removing a Column from the DataFrame'
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 30：向 DataFrame 中添加和移除列
- en: 'In this exercise, we will add a new column in the dataset using the `withColumn`
    method, and later, using the `drop` function, will remove it. Now, let''s perform
    the following steps:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 `withColumn` 方法在数据集中添加新列，之后使用 `drop` 函数将其移除。现在，让我们执行以下步骤：
- en: 'Add a new column in a Spark DataFrame using the `withColumn` method:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `withColumn` 方法在 Spark DataFrame 中添加一个新列：
- en: '[PRE23]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Use the following command to show the dataset with the newly added column:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令显示包含新添加列的数据集：
- en: '[PRE24]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Figure 4.10: Introducing new column, Half_sepal_width](img/C12913_04_10.jpg)'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.10：引入新列 Half_sepal_width](img/C12913_04_10.jpg)'
- en: 'Figure 4.10: Introducing new column, Half_sepal_width'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.10：引入新列 Half_sepal_width
- en: 'Now, to remove a column in a Spark DataFrame, use the `drop` method illustrated
    here:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，要在 Spark 数据框中移除一列，请使用这里说明的 `drop` 方法：
- en: '[PRE25]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s show the dataset to verify that the column has been removed:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们展示数据集以验证列是否已被移除：
- en: '[PRE26]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Figure 4.11: Iris DataFrame after dropping the Half_sepal_width column](img/C12913_04_11.jpg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.11：移除 Half_sepal_width 列后的鸢尾花数据框](img/C12913_04_11.jpg)'
- en: 'Figure 4.11: Iris DataFrame after dropping the Half_sepal_width column'
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.11：移除 Half_sepal_width 列后的鸢尾花数据框
- en: 'Exercise 31: Displaying and Counting Distinct Values in a DataFrame'
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 31：在数据框中显示和计数不同的值
- en: 'To display the distinct values in a DataFrame, we use the `distinct().show()`
    method. Similarly, to count the distinct values, we will be using the `distinct().count()`
    method. Perform the following procedures to print the distinct values with the
    total count:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要显示数据框中的不同值，我们使用 `distinct().show()` 方法。同样，要计数不同的值，我们将使用 `distinct().count()`
    方法。执行以下步骤以打印不同的值及其总数：
- en: 'Select the distinct values in any column of a Spark DataFrame using the `distinct`
    method, in conjunction with the `select` method:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `distinct` 方法，结合 `select` 方法，从 Spark 数据框中选择任何列的不同值：
- en: '[PRE27]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Figure 4.12: Iris DataFrame, Species column](img/C12913_04_12.jpg)'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.12：鸢尾花数据框，物种列](img/C12913_04_12.jpg)'
- en: 'Figure 4.12: Iris DataFrame, Species column'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.12：鸢尾花数据框，物种列
- en: 'To count the distinct values in any column of a Spark DataFrame, use the `count`
    method, in conjunction with the `distinct` method:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要计算 Spark 数据框中任何列的不同值，请使用 `count` 方法，并结合 `distinct` 方法：
- en: '[PRE28]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Exercise 32: Removing Duplicate Rows and Filtering Rows of a DataFrame'
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 32：移除重复行和过滤数据框中的行
- en: In this exercise, we will learn how to remove the duplicate rows from the dataset,
    and later, perform filtering operations on the same column.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将学习如何从数据集中移除重复的行，并随后在同一列上执行过滤操作。
- en: 'Perform these steps:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这些步骤：
- en: 'Remove the duplicate values from a DataFrame using the `dropDuplicates()` method:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `dropDuplicates()` 方法从数据框中移除重复值：
- en: '[PRE29]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![Figure 4.13: Iris DataFrame, Species column after removing duplicate column](img/C12913_04_13.jpg)'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.13：移除重复列后的鸢尾花数据框，物种列](img/C12913_04_13.jpg)'
- en: 'Figure 4.13: Iris DataFrame, Species column after removing duplicate column'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.13：移除重复列后的鸢尾花数据框，物种列
- en: 'Filter the rows from a DataFrame using one or multiple conditions. These multiple
    conditions can be passed together to the DataFrame using Boolean operators such
    as and (`&`), or `|`, similar to how we do it for pandas DataFrames:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一个或多个条件从数据框中过滤行。这些多个条件可以通过布尔运算符如 `&`（与）或 `|`（或）传递给数据框，类似于我们对 pandas 数据框的操作：
- en: '[PRE30]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](img/Image39953.jpg)'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/Image39953.jpg)'
- en: 'Figure 4.14: Iris DataFrame after filtering with single conditions'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.14：使用单一条件过滤后的鸢尾花数据框
- en: 'Now, to filter the column using multiple conditions, use the following command:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，要使用多个条件过滤列，请使用以下命令：
- en: '[PRE31]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![Figure 4.15: Iris DataFrame after filtering with multiple conditions](img/C12913_04_16.jpg)'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.15：使用多个条件过滤后的鸢尾花数据框](img/C12913_04_16.jpg)'
- en: 'Figure 4.15: Iris DataFrame after filtering with multiple conditions'
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.15：使用多个条件过滤后的鸢尾花数据框
- en: 'Exercise 33: Ordering Rows in a DataFrame'
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 33：对数据框中的行进行排序
- en: 'In this exercise, we will explore how to sort the rows in a DataFrame in ascending
    and descending order. Let''s perform these steps:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将探索如何按升序和降序对数据框中的行进行排序。让我们执行以下步骤：
- en: 'Sort the rows in a DataFrame, using one or multiple conditions, in ascending
    or descending order:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一个或多个条件按升序或降序排序数据框中的行：
- en: '[PRE32]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![Figure 4.16: Filtered Iris DataFrame](img/C12913_04_161.jpg)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.16：过滤后的鸢尾花数据框](img/C12913_04_161.jpg)'
- en: 'Figure 4.16: Filtered Iris DataFrame'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.16：过滤后的鸢尾花数据框
- en: 'To sort the rows in descending order, use the following command:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要按降序排序行，请使用以下命令：
- en: '[PRE33]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![Figure 4.17: Iris DataFrame after sorting it in the descending order](img/C12913_04_17.jpg)'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.17：按降序排序后的鸢尾花数据框](img/C12913_04_17.jpg)'
- en: 'Figure 4.17: Iris DataFrame after sorting it in the descending order'
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.17：按降序排序后的鸢尾花数据框
- en: 'Exercise 34: Aggregating Values in a DataFrame'
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 34：在数据框中聚合值
- en: 'We can group the values in a DataFrame by one or more variable, and calculate
    aggregated metrics such as `mean`, `sum`, `count`, and many more. In this exercise,
    we will calculate the mean sepal width for each of the flower species in the Iris
    dataset. We will also calculate the count of the rows for each species:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个或多个变量对 DataFrame 中的值进行分组，并计算汇总指标，如 `mean`、`sum`、`count` 等等。在这个练习中，我们将计算鸢尾花数据集中每个花卉物种的平均花萼宽度。我们还将计算每个物种的行数：
- en: 'To calculate the mean sepal width for each species, use the following command:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令计算每个物种的平均花萼宽度：
- en: '[PRE34]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](img/Image39999.jpg)'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/Image39999.jpg)'
- en: 'Figure 4.18: Iris DataFrame, calculating mean sepal width'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.18：鸢尾花 DataFrame，计算平均花萼宽度
- en: 'Now, let''s calculate the number of rows for each species by using the following
    command:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令计算每个物种的行数：
- en: '[PRE35]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Figure 4.19: Iris DataFrame, calculating number of rows for each species](img/C12913_04_19.jpg)'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.19：鸢尾花 DataFrame，计算每个物种的行数](img/C12913_04_19.jpg)'
- en: 'Figure 4.19: Iris DataFrame, calculating number of rows for each species'
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.19：鸢尾花 DataFrame，计算每个物种的行数
- en: Note
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE36]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Activity 10: Data Manipulation with Spark DataFrames'
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 10：使用 Spark DataFrame 进行数据操作
- en: 'In this activity, we will use the concepts learned in the previous sections
    to manipulate the data in the Spark DataFrame created using the Iris dataset.
    We will perform basic data manipulation steps to test our ability to work with
    data in a Spark DataFrame. Feel free to use any open source dataset for this activity.
    Make sure the dataset you use has both numerical and categorical variables:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将使用之前部分中学到的概念，操作使用鸢尾花数据集创建的 Spark DataFrame。我们将执行基本的数据操作步骤，测试我们在 Spark
    DataFrame 中处理数据的能力。你可以自由使用任何开源数据集进行此活动。确保所用数据集包含数值变量和类别变量：
- en: Rename any five columns of the DataFrame. If the DataFrame has more than columns,
    rename all the columns.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重命名 DataFrame 中的五列。如果 DataFrame 中有更多列，则重命名所有列。
- en: Select two numeric and one categorical column from the DataFrame.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 DataFrame 中选择两列数值型列和一列类别型列。
- en: Count the number of distinct categories in the categorical variable.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算类别变量中不同类别的数量。
- en: Create two new columns in the DataFrame by summing up and multiplying together
    the two numerical columns.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 DataFrame 中创建两个新列，分别通过将两列数值型列相加和相乘得到。
- en: Drop both the original numerical columns.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除两个原始数值列。
- en: Sort the data by the categorical column.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照类别列对数据进行排序。
- en: Calculate the mean of the summation column for each distinct category in the
    categorical variable.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个类别变量中每个不同类别的求和列的平均值。
- en: Filter the rows with values greater than the mean of all the mean values calculated
    in step 7.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤出花萼宽度大于步骤 7 中计算出的所有平均值的行。
- en: De-duplicate the resultant DataFrame to make sure it has only unique records.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对结果 DataFrame 进行去重，确保它只包含唯一记录。
- en: Note
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 219.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 219 页找到。
- en: Graphs in Spark
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 中的图表
- en: The ability to effectively visualize data is of paramount importance. Visual
    representations of data help the user develop a better understanding of data and
    uncover trends that might go unnoticed in text form. There are numerous types
    of plots available in Python, each with its own context.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 有效可视化数据的能力至关重要。数据的可视化帮助用户更好地理解数据，并发现文本形式中可能忽略的趋势。在 Python 中，有许多类型的图表，每种图表都有其特定的使用场景。
- en: We will be exploring some of these plots, including bar charts, density plots,
    boxplots, and linear plots for Spark DataFrames, using the widely used Python
    plotting packages of Matplotlib and Seaborn. The point to note here is that Spark
    deals with big data. So, make sure that your data size is reasonable enough (that
    is, it fits in your computer's RAM) before plotting it. This can be achieved by
    filtering, aggregating, or sampling the data before plotting it.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索一些图表，包括条形图、密度图、箱线图和线性图，用于 Spark DataFrame，使用流行的 Python 绘图包 Matplotlib 和
    Seaborn。需要注意的是，Spark 处理的是大数据。因此，在绘制图表之前，请确保数据大小足够合理（即能容纳在计算机的内存中）。这可以通过过滤、汇总或抽样数据来实现。
- en: We are using the Iris dataset, which is small, hence we do not need to do any
    such pre-processing steps to reduce the data size.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的是鸢尾花数据集，它较小，因此我们不需要进行任何预处理步骤来减少数据大小。
- en: Note for the Instructor
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 教师说明
- en: The user should install and load the Matplotlib and Seaborn packages beforehand,
    in the development environment, before getting started with the exercises in this
    section. If you are unfamiliar with installing and loading these packages, visit
    the official websites of Matplotlib and Seaborn.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 用户应在开发环境中预先安装并加载Matplotlib和Seaborn包，然后再开始本节的练习。如果您不熟悉如何安装和加载这些包，请访问Matplotlib和Seaborn的官方网站。
- en: 'Exercise 35: Creating a Bar Chart'
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 35：创建条形图
- en: 'In this exercise, we will try to plot the number of records available for each
    species using a bar chart. We will have to first aggregate the data and count
    the number of records for each species. We can then convert this aggregated data
    into a regular pandas DataFrame and use Matplotlib and Seaborn packages to create
    any kind of plots of it that we wish:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将尝试通过条形图绘制每种物种的记录数量。我们需要首先聚合数据并计算每种物种的记录数。然后，我们可以将聚合后的数据转换为常规的pandas
    DataFrame，并使用Matplotlib和Seaborn包创建我们需要的任何类型的图表：
- en: 'First, calculate the number of rows for each flower species and convert the
    result to a pandas DataFrame:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，计算每种花卉物种的行数，并将结果转换为pandas DataFrame：
- en: '[PRE37]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, create a bar plot from the resulting pandas DataFrame:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，从结果的pandas DataFrame创建一个条形图：
- en: '[PRE38]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The plot is as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘制的图形如下：
- en: '![Figure 4.20: Bar plot for Iris DataFrame after calculating the number of
    rows for each flower species](img/C12913_04_20.jpg)'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.20：计算每个花卉物种的行数后，Iris DataFrame的条形图](img/C12913_04_20.jpg)'
- en: 'Figure 4.20: Bar plot for Iris DataFrame after calculating the number of rows
    for each flower species'
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.20：计算每个花卉物种的行数后，Iris DataFrame的条形图
- en: 'Exercise 36: Creating a Linear Model Plot'
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 36：创建线性模型图
- en: 'In this exercise, we will plot the data points of two different variables and
    fit a straight line on them. This is similar to fitting a linear model on two
    variables and can help identify correlations between the two variables:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将绘制两个不同变量的数据点，并在其上拟合一条直线。这类似于在两个变量上拟合一个线性模型，并有助于识别这两个变量之间的相关性：
- en: 'Create a `data` object from the pandas DataFrame:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从pandas DataFrame创建一个`data`对象：
- en: '[PRE39]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Plot the DataFrame using the following command:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令绘制DataFrame：
- en: '[PRE40]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![Figure 4.21: Linear model plot for Iris DataFrame](img/C12913_04_21.jpg)'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.21：Iris DataFrame的线性模型图](img/C12913_04_21.jpg)'
- en: 'Figure 4.21: Linear model plot for Iris DataFrame'
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.21：Iris DataFrame的线性模型图
- en: 'Exercise 37: Creating a KDE Plot and a Boxplot'
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 37：创建KDE图和箱线图
- en: 'In this exercise, we will create a **kernel density estimation** (**KDE**)
    plot, followed by a **boxplot**. Follow these instructions:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将创建一个**核密度估计**（**KDE**）图，并接着绘制一个**箱线图**。请按照以下步骤操作：
- en: 'First, plot a KDE plot that shows us the distribution of a variable. Make sure
    it gives us an idea of the skewness and the kurtosis of a variable:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，绘制一个KDE图，展示变量的分布情况。确保它能帮助我们了解变量的偏斜度和峰度：
- en: '[PRE41]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![Figure 4.22: KDE plot for the Iris DataFrame](img/C12913_04_22.jpg)'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.22：Iris DataFrame的KDE图](img/C12913_04_22.jpg)'
- en: 'Figure 4.22: KDE plot for the Iris DataFrame'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.22：Iris DataFrame的KDE图
- en: 'Now, plot the boxplots for the Iris dataset using the following command:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令绘制Iris数据集的箱线图：
- en: '[PRE42]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![Figure 4.23: Boxplot for the Iris DataFrame](img/C12913_04_23.jpg)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.23：Iris DataFrame的箱线图](img/C12913_04_23.jpg)'
- en: 'Figure 4.23: Boxplot for the Iris DataFrame'
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.23：Iris DataFrame的箱线图
- en: Boxplots are a good way to look at the data distribution and locate outliers.
    They represent the distribution using the 1st quartile, the median, the 3rd quartile,
    and the interquartile range (25th to 75th percentile).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图是查看数据分布并定位异常值的好方法。它们通过1st四分位数、中位数、3rd四分位数和四分位间距（25%到75%的百分位数）来表示数据分布。
- en: 'Activity 11: Graphs in Spark'
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 11：Spark中的图表
- en: 'In this activity, we will use the plotting libraries of Python to visually
    explore our data using different kind of plots. For this activity, we are using
    the `mtcars` dataset from Kaggle ([https://www.kaggle.com/ruiromanini/mtcars](https://www.kaggle.com/ruiromanini/mtcars)):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，我们将使用Python的绘图库，通过不同类型的图表来可视化探索数据。我们使用的是Kaggle上的`mtcars`数据集（[https://www.kaggle.com/ruiromanini/mtcars](https://www.kaggle.com/ruiromanini/mtcars)）：
- en: Import all the required packages and libraries in the Jupyter Notebook.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中导入所有必需的包和库。
- en: Read the data into Spark object from the `mtcars` dataset.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`mtcars`数据集将数据读取到Spark对象中。
- en: 'Visualize the discrete frequency distribution of any continuous numeric variable
    from your dataset using a histogram:![Figure 4.24: Histogram for the Iris DataFrame](img/C12913_04_24.jpg)'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用直方图可视化数据集中任意连续数值变量的离散频率分布：![图 4.24：Iris 数据框的直方图](img/C12913_04_24.jpg)
- en: 'Figure 4.24: Histogram for the Iris DataFrame'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.24：Iris 数据框的直方图
- en: 'Visualize the percentage share of the categories in the dataset using a pie
    chart:![Figure 4.25: Pie chart for the Iris DataFrame](img/C12913_04_25.jpg)'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用饼图可视化数据集中各类别的百分比份额：![图 4.25：Iris 数据框的饼图](img/C12913_04_25.jpg)
- en: 'Figure 4.25: Pie chart for the Iris DataFrame'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.25：Iris 数据框的饼图
- en: 'Plot the distribution of a continuous variable across the categories of a categorical
    variable using a boxplot:![Figure 4.26: Boxplot for the Iris DataFrame](img/Image40067.jpg)'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用箱型图绘制连续变量在类别变量各类别下的分布：![图 4.26：Iris 数据框的箱型图](img/Image40067.jpg)
- en: 'Figure 4.26: Boxplot for the Iris DataFrame'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.26：Iris 数据框的箱型图
- en: 'Visualize the values of a continuous numeric variable using a line chart:![Figure
    4.27: Line chart for the Iris DataFrame](img/C12913_04_27.jpg)'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用线性图表可视化连续数值变量的值：![图 4.27：Iris 数据框的线性图表](img/C12913_04_27.jpg)
- en: 'Figure 4.27: Line chart for the Iris DataFrame'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.27：Iris 数据框的线性图表
- en: 'Plot the values of multiple continuous numeric variables on the same line chart:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一条线性图表中绘制多个连续数值变量的值：
- en: '![Figure 4.28: Line chart for the Iris DataFrame plotting multiple continuous
    numeric variables](img/C12913_04_28.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.28：Iris 数据框中绘制多个连续数值变量的线性图表](img/C12913_04_28.jpg)'
- en: 'Figure 4.28: Line chart for the Iris DataFrame plotting multiple continuous
    numeric variables'
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.28：Iris 数据框中绘制多个连续数值变量的线性图表
- en: Note
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 224.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 224 页找到。
- en: Summary
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小结
- en: In this chapter, we saw a basic introduction of Spark DataFrames and how they
    are better than RDDs. We explored different ways of creating Spark DataFrames
    and writing the contents of Spark DataFrames to regular pandas DataFrames and
    output files.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 Spark 数据框的基本概念，并探讨了它们为何优于 RDD。我们探索了创建 Spark 数据框的不同方法，并将 Spark 数据框的内容写入常规的
    pandas 数据框和输出文件。
- en: We tried out hands-on data exploration in PySpark by computing basic statistics
    and metrics for Spark DataFrames. We played around with the data in Spark DataFrames
    and performed data manipulation operations such as filtering, selection, and aggregation.
    We tried our hands at plotting the data to generate insightful visualizations.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 PySpark 中尝试了实际的数据探索，通过计算 Spark 数据框的基本统计和指标。我们在 Spark 数据框中操作数据，执行数据处理操作，如过滤、选择和聚合。我们还尝试通过绘制数据生成有意义的可视化图表。
- en: Furthermore, we consolidated our understanding of various concepts by practicing
    hands-on exercises and activities.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们通过实践操作和活动巩固了对各种概念的理解。
- en: In the next chapter, we will explore how to handle missing values and compute
    correlation between variables in PySpark.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何处理缺失值以及如何计算 PySpark 中变量之间的相关性。
