- en: Chapter 4. Spark SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。Spark SQL
- en: 'Spark SQL is a Spark module for processing a structured data. This chapter
    is divided into the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是用于处理结构化数据的Spark模块。本章分为以下几个部分：
- en: Understanding the Catalyst optimizer
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解Catalyst优化器
- en: Creating HiveContext
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建HiveContext
- en: Inferring schema using case classes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用案例类推断模式
- en: Programmatically specifying the schema
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以编程方式指定模式
- en: Loading and saving data using the Parquet format
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Parquet格式加载和保存数据
- en: Loading and saving data using the JSON format
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用JSON格式加载和保存数据
- en: Loading and saving data from relational databases
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从关系数据库加载和保存数据
- en: Loading and saving data from an arbitrary source
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从任意源加载和保存数据
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Spark can process data from various data sources such as HDFS, Cassandra, HBase,
    and relational databases, including HDFS. Big data frameworks (unlike relational
    database systems) do not enforce schema while writing. HDFS is a perfect example
    where any arbitrary file is welcome during the write phase. Reading data is a
    different story, however. You need to give some structure to even completely unstructured
    data to make sense out of it. With this structured data, SQL comes very handy
    when it comes to analysis.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以处理来自各种数据源的数据，如HDFS、Cassandra、HBase和关系数据库，包括HDFS。大数据框架（不像关系数据库系统）在写入时不强制执行模式。HDFS是一个完美的例子，在写入阶段任何任意文件都是可以的。然而，读取数据是另一回事。即使是完全非结构化的数据，你也需要给它一些结构来理解。有了这些结构化数据，SQL在分析时非常方便。
- en: Spark SQL is a relatively new component in Spark ecosystem, introduced in Spark
    1.0 for the first time. It incorporates a project named Shark, which was an attempt
    to make Hive run on Spark.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是Spark生态系统中相对较新的组件，首次在Spark 1.0中引入。它包含了一个名为Shark的项目，这是一个让Hive在Spark上运行的尝试。
- en: Hive is essentially a relational abstraction, which converts SQL queries to
    MapReduce jobs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Hive本质上是一个关系抽象，它将SQL查询转换为MapReduce作业。
- en: '![Introduction](img/3056_04_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![介绍](img/3056_04_01.jpg)'
- en: Shark replaced the MapReduce part with Spark while retaining most of the code
    base.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Shark用Spark替换了MapReduce部分，同时保留了大部分代码库。
- en: '![Introduction](img/3056_04_02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![介绍](img/3056_04_02.jpg)'
- en: Initially, it worked fine, but very soon, Spark developers hit roadblocks and
    could not optimize it any further. Finally, they decided to write the SQL Engine
    from scratch and that gave birth to Spark SQL.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，它运行良好，但很快，Spark开发人员遇到了障碍，无法进一步优化。最终，他们决定从头开始编写SQL引擎，这就诞生了Spark SQL。
- en: '![Introduction](img/3056_04_03.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![介绍](img/3056_04_03.jpg)'
- en: Spark SQL took care of all the performance challenges, but it had to provide
    compatibility with Hive and for that reason, a new wrapper context, `HiveContext`,
    was created on top of `SQLContext`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL解决了所有性能挑战，但它必须与Hive兼容，因此，`HiveContext`上面创建了一个新的包装器上下文`SQLContext`。
- en: Spark SQL supports accessing data using standard SQL queries and HiveQL, a SQL-like
    query language that Hive uses. In this chapter, we will explore different features
    of Spark SQL. It supports a subset of HiveQL as well as a subset of SQL 92\. It
    runs SQL/HiveQL queries alongside, or replacing the existing Hive deployments.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL支持使用标准SQL查询和HiveQL访问数据，HiveQL是Hive使用的类似SQL的查询语言。在本章中，我们将探索Spark SQL的不同特性。它支持HiveQL的一个子集以及SQL
    92的一个子集。它可以在现有的Hive部署中运行SQL/HiveQL查询或替代它们。
- en: Running SQL is only a part of the reason for the creation of Spark SQL. One
    big reason is that it helps to create and run Spark programs faster. It lets developers
    write less code, program read less data, and let the catalyst optimizer do all
    the heavy lifting.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 运行SQL只是创建Spark SQL的原因之一。一个很大的原因是它有助于更快地创建和运行Spark程序。它让开发人员编写更少的代码，程序读取更少的数据，并让优化器来处理所有繁重的工作。
- en: Spark SQL uses a programming abstraction called **DataFrame**. It is a distributed
    collection of data organized in named columns. DataFrame is equivalent to a database
    table, but provides much finer level of optimization. The DataFrame API also ensures
    that Spark's performance is consistent across different language bindings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL使用了一个名为DataFrame的编程抽象。它是一个以命名列组织的分布式数据集合。DataFrame相当于数据库表，但提供了更精细的优化级别。DataFrame
    API还确保了Spark在不同语言绑定中的性能是一致的。
- en: Let's contrast DataFrames with RDDs. An RDD is an opaque collection of objects
    with no idea about the format of the underlying data. In contrast, DataFrames
    have schema associated with them. You can also look at DataFrames as RDDs with
    schema added to them. In fact, until Spark 1.2, there was an artifact called **SchemaRDD**,
    which has now evolved into DataFrame. They provide much richer functionality than
    SchemaRDDs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对比一下DataFrame和RDD。RDD是一个不透明的对象集合，对底层数据格式一无所知。相反，DataFrame与它们关联了模式。实际上，直到Spark
    1.2，有一个名为**SchemaRDD**的组件，现在已经演变成了DataFrame。它们提供比SchemaRDD更丰富的功能。
- en: This extra information about schema makes possible to do a lot of optimizations,
    which were not otherwise possible.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模式的额外信息使得许多优化成为可能，这是其他情况下不可能的。
- en: DataFrames also transparently load from various data sources, such as Hive tables,
    Parquet files, JSON files, and external databases using JDBC. DataFrames can be
    viewed as RDDs of row objects, allowing users to call the procedural Spark APIs
    such as map.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame还可以透明地从各种数据源加载，如Hive表、Parquet文件、JSON文件和使用JDBC的外部数据库。DataFrame可以被视为一组行对象的RDD，允许用户调用过程式的Spark
    API，如map。
- en: The DataFrame API is available in Scala, Java, Python, and also R starting Spark
    1.4.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API在Spark 1.4开始提供Scala、Java、Python和R。
- en: Users can perform relational operations on DataFrames using a **domain-specific
    language** (**DSL**). DataFrames support all the common relational operators and
    they all take expression objects in a limited DSL that lets Spark capture the
    structure of the expression.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以使用**领域特定语言**（**DSL**）在DataFrame上执行关系操作。DataFrame支持所有常见的关系操作符，它们都使用有限的DSL中的表达式对象，让Spark捕获表达式的结构。
- en: We will start with the entry point into Spark SQL, that is, SQLContext. We will
    also cover HiveContext that is a wrapper around SQLContext to support Hive functionality.
    Please note that HiveContext is more battle-tested and provides a richer functionality,
    so it is strongly recommended to use it even if you do not plan to connect to
    Hive. Slowly, SQLContext will come to the same level of functionality as HiveContext
    is.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Spark SQL的入口点SQLContext开始。我们还将介绍HiveContext，它是SQLContext的包装器，用于支持Hive功能。请注意，HiveContext经过了更多的实战检验，并提供了更丰富的功能，因此强烈建议即使您不打算连接到Hive，也要使用它。慢慢地，SQLContext将达到与HiveContext相同的功能水平。
- en: There are two ways to associate schema with RDDs to create DataFrames. The easy
    way is to leverage Scala case classes, which we are going to cover first. Spark
    uses Java reflection to deduce schema from case classes. There is also a way to
    programmatically specify schema for advanced needs, which we will cover next.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以将模式与RDD关联起来创建DataFrame。简单的方法是利用Scala case类，我们将首先介绍这种方法。Spark使用Java反射从case类中推断模式。还有一种方法可以为高级需求编程指定模式，我们将在下一节中介绍。
- en: Spark SQL provides an easy way to both load and save the Parquet files, which
    will also be covered. Lastly, we will cover loading from and saving data to JSON.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL提供了一种简单的方法来加载和保存Parquet文件，我们也将介绍。最后，我们将介绍从JSON加载和保存数据。
- en: Understanding the Catalyst optimizer
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Catalyst优化器
- en: Most of the power of Spark SQL comes due to Catalyst optimizer, so it makes
    sense to spend some time understanding it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL的大部分功能都来自于Catalyst优化器，因此花一些时间来了解它是有意义的。
- en: '![Understanding the Catalyst optimizer](img/3056_04_04.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![理解Catalyst优化器](img/3056_04_04.jpg)'
- en: How it works…
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: Catalyst optimizer primarily leverages functional programming constructs of
    Scala such as pattern matching. It offers a general framework for transforming
    trees, which we use to perform analysis, optimization, planning, and runtime code
    generation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器主要利用了Scala的函数式编程构造，如模式匹配。它提供了一个通用的框架来转换树，我们用它来执行分析、优化、规划和运行时代码生成。
- en: 'Catalyst optimizer has two primary goals:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器有两个主要目标：
- en: Make adding new optimization techniques easy
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使添加新的优化技术变得容易
- en: Enable external developers to extend the optimizer
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使外部开发人员能够扩展优化器
- en: 'Spark SQL uses Catalyst''s transformation framework in four phases:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL在四个阶段使用Catalyst的转换框架：
- en: Analyzing a logical plan to resolve references
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析逻辑计划以解析引用
- en: Logical plan optimization
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑计划优化
- en: Physical planning
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理规划
- en: Code generation to compile the parts of the query to Java bytecode
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码生成以将查询的部分编译为Java字节码
- en: Analysis
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析
- en: 'The analysis phase involved looking at a SQL query or a DataFrame, creating
    a logical plan out of it, which is still unresolved (the columns referred may
    not exist or may be of wrong datatype) and then resolving this plan using the
    Catalog object (which connects to the physical data source), and creating a logical
    plan, as shown in the following diagram:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 分析阶段涉及查看SQL查询或DataFrame，创建一个逻辑计划（仍未解析）（引用的列可能不存在或类型错误），然后使用Catalog对象解析此计划（连接到物理数据源），并创建一个逻辑计划，如下图所示：
- en: '![Analysis](img/3056_04_05.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![分析](img/3056_04_05.jpg)'
- en: Logical plan optimization
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑计划优化
- en: The logical plan optimization phase applies standard rule-based optimization
    to the logical plan. These include constant folding, predicate pushdown, projection
    pruning, null propagation, Boolean expression simplification, and other rules.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑计划优化阶段对逻辑计划应用标准的基于规则的优化。这些包括常量折叠、谓词下推、投影修剪、空值传播、布尔表达式简化和其他规则。
- en: I would like to draw special attention to predicate the pushdown rule here.
    The concept is simple; if you issue a query in one place to run against the massive
    data, which is another place, it can lead to a lot of unnecessary data moving
    across the network.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我想特别注意这里的谓词下推规则。这个概念很简单；如果您在一个地方发出查询来运行对大量数据的查询，这个数据存储在另一个地方，它可能导致大量不必要的数据在网络上移动。
- en: If we can push down the part of the query to where the data is stored, and thus
    filter out unnecessary data, it reduces network traffic significantly.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以将查询的一部分下推到数据存储的地方，从而过滤掉不必要的数据，就可以显著减少网络流量。
- en: '![Logical plan optimization](img/3056_04_06.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑计划优化](img/3056_04_06.jpg)'
- en: Physical planning
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 物理规划
- en: In the physical planning phase, Spark SQL takes a logical plan and generates
    one or more physical plans. It then measures the cost of each physical plan and
    generates one physical plan based on that.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理规划阶段，Spark SQL接受逻辑计划并生成一个或多个物理计划。然后它测量每个物理计划的成本，并基于此生成一个物理计划。
- en: '![Physical planning](img/3056_04_07.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![物理规划](img/3056_04_07.jpg)'
- en: Code generation
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码生成
- en: The final phase of query optimization involves generating Java bytecode to run
    on each machine. It uses a special Scala feature called **Quasi quotes** to accomplish
    that.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 查询优化的最后阶段涉及生成Java字节码以在每台机器上运行。它使用了一种特殊的Scala功能，称为**准引用**来实现这一点。
- en: Creating HiveContext
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建HiveContext
- en: '`SQLContext` and its descendant `HiveContext` are the two entry points into
    the world of Spark SQL. `HiveContext` provides a superset of functionality provided
    by SQLContext. The additional features are:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`SQLContext`及其后代`HiveContext`是进入Spark SQL世界的两个入口点。`HiveContext`提供了SQLContext提供的功能的超集。附加功能包括：'
- en: More complete and battle-tested HiveQL parser
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更完整和经过实战检验的HiveQL解析器
- en: Access to Hive UDFs
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问Hive UDFs
- en: Ability to read data from Hive tables
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Hive表中读取数据的能力
- en: 'From Spark 1.3 onwards, the Spark shell comes loaded with sqlContext (which
    is an instance of `HiveContext` not `SQLContext`). If you are creating `SQLContext`
    in Scala code, it can be created using `SparkContext`, as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark 1.3开始，Spark shell加载了sqlContext（它是`HiveContext`的实例，而不是`SQLContext`）。如果您在Scala代码中创建`SQLContext`，可以使用`SparkContext`来创建，如下所示：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this recipe, we will cover how to create instance of `HiveContext`, and then
    access Hive functionality through Spark SQL.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将介绍如何创建`HiveContext`的实例，然后通过Spark SQL访问Hive功能。
- en: Getting ready
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To enable Hive functionality, make sure that you have Hive enabled (-Phive)
    assembly JAR is available on all worker nodes; also, copy `hive-site.xml` into
    the `conf` directory of the Spark installation. It is important that Spark has
    access to `hive-site.xml`; otherwise, it will create its own Hive metastore and
    will not connect to your existing Hive warehouse.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用Hive功能，请确保已启用Hive（-Phive）装配JAR可在所有工作节点上使用；另外，将`hive-site.xml`复制到Spark安装的`conf`目录中。Spark必须能够访问`hive-site.xml`，否则它将创建自己的Hive元数据存储，并且不会连接到现有的Hive仓库。
- en: By default, all the tables created by Spark SQL are Hive-managed tables, that
    is, Hive has complete control on life cycle of a table, including deleting it
    if table metadata is dropped using the `drop table` command. This holds true only
    for persistent tables. Spark SQL also has mechanism to create temporary tables
    out of DataFrames for ease of writing queries, and they are not managed by Hive.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Spark SQL创建的所有表都是由Hive管理的表，也就是说，Hive完全控制表的生命周期，包括使用`drop table`命令删除表元数据。这仅适用于持久表。Spark
    SQL还有机制可以将DataFrame创建为临时表，以便编写查询，它们不受Hive管理。
- en: Please note that Spark 1.4 supports Hive versions 0.13.1\. You can specify a
    version of Hive you would like to build against using the `-Phive-<version> build`
    option while building with Maven. For example, to build with 0.12.0, you can use
    `-Phive-0.12.0`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Spark 1.4支持Hive版本0.13.1。您可以在使用Maven构建时使用`-Phive-<version> build`选项指定要构建的Hive版本。例如，要使用0.12.0构建，您可以使用`-Phive-0.12.0`。
- en: How to do it...
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell并为其提供一些额外的内存：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create an instance of `HiveContext`:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`HiveContext`的实例：
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a Hive table `Person` with `first_name`, `last_name`, and `age` as columns:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个Hive表`Person`，其中`first_name`、`last_name`和`age`作为列：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Open another shell and create the `person` data in a local file:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个shell中创建`person`数据并放入本地文件：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Load the data in the `person` table:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`person`表中加载数据：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Alternatively, load that data in the `person` table from HDFS:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，从HDFS中加载`person`表中的数据：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that using `load data inpath` moves the data from another HDFS location
    to the Hive's `warehouse` directory, which is, by default, `/user/hive/warehouse`.
    You can also specify fully qualified path such as `hdfs://localhost:9000/user/hduser/person`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用`load data inpath`将数据从另一个HDFS位置移动到Hive的`warehouse`目录，默认为`/user/hive/warehouse`。您还可以指定完全限定的路径，例如`hdfs://localhost:9000/user/hduser/person`。
- en: 'Select the person data using HiveQL:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用HiveQL选择人员数据：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create a new table from the output of a `select` query:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`select`查询的输出创建新表：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can also copy directly from one table to another:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以直接从一个表复制到另一个表：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create two tables `people_by_last_name` and `people_by_age` to keep counts:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个表`people_by_last_name`和`people_by_age`来保持计数：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can also insert records into multiple tables using a HiveQL query:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以使用HiveQL查询将记录插入多个表中：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Inferring schema using case classes
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用案例类推断模式
- en: Case classes are special classes in Scala that provide you with the boiler plate
    implementation of the constructor, getters (accessors), equals and hashCode, and
    implement `Serializable`. Case classes work really well to encapsulate data as
    objects. Readers, familiar with Java, can relate it to **plain old Java objects**
    (**POJOs**) or Java bean.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 案例类是Scala中的特殊类，为您提供了构造函数、getter（访问器）、equals和hashCode的样板实现，并实现了`Serializable`。案例类非常适合封装数据作为对象。熟悉Java的读者可以将其与**普通旧的Java对象**（**POJOs**）或Java
    bean相关联。
- en: The beauty of case classes is that all that grunt work, which is required in
    Java, can be done with case classes in a single line of code. Spark uses reflection
    on case classes to infer schema.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 案例类的美妙之处在于，所有在Java中需要的繁重工作都可以在案例类中用一行代码完成。Spark使用案例类的反射来推断模式。
- en: How to do it...
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell并为其提供一些额外的内存：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Import for the implicit conversions:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入隐式转换：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create a `Person` case class:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`Person`案例类：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In another shell, create some sample data to be put in HDFS:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个shell中，创建一些样本数据放入HDFS中：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Load the `person` directory as an RDD:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`person`目录加载为RDD：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Split each line into an array of string, based on a comma, as a delimiter:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据逗号将每行拆分为字符串数组，作为分隔符：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Convert the RDD of Array[String] into the RDD of `Person` case objects:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Array[String]的RDD转换为`Person`案例对象的RDD：
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Convert the `personRDD` into the `personDF` DataFrame:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`personRDD`转换为`personDF` DataFrame：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Register the `personDF` as a table:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`personDF`注册为表：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Run a SQL query against it:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对其运行SQL查询：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Get the output values from `persons`:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`persons`获取输出值：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Programmatically specifying the schema
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以编程方式指定模式
- en: There are few cases where case classes might not work; one of these cases is
    that the case classes cannot take more than 22 fields. Another case can be that
    you do not know about schema beforehand. In this approach, the data is loaded
    as an RDD of the `Row` objects. Schema is created separately using the `StructType`
    and `StructField` objects, which represent a table and a field respectively. Schema
    is applied to the `Row` RDD to create a DataFrame.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 有些情况下案例类可能不起作用；其中之一是案例类不能拥有超过22个字段。另一种情况可能是您事先不知道模式。在这种方法中，数据被加载为`Row`对象的RDD。模式是使用`StructType`和`StructField`对象分别创建的，它们分别表示表和字段。模式应用于`Row`
    RDD以创建DataFrame。
- en: How to do it...
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell并为其提供一些额外的内存：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Import for the implicit conversion:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入隐式转换：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Import the Spark SQL datatypes and `Row` objects:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark SQL数据类型和`Row`对象：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In another shell, create some sample data to be put in HDFS:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个shell中，创建一些样本数据放入HDFS中：
- en: '[PRE26]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Load the `person` data in an RDD:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在RDD中加载`person`数据：
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Split each line into an array of string, based on a comma, as a delimiter:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据逗号将每行拆分为字符串数组，作为分隔符：
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Convert the RDD of array[string] to the RDD of the `Row` objects:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将array[string]的RDD转换为`Row`对象的RDD：
- en: '[PRE29]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create schema using the `StructType` and `StructField` objects. The `StructField`
    object takes parameters in the form of param name, param type, and nullability:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`StructType`和`StructField`对象创建模式。`StructField`对象以参数名、参数类型和可空性的形式接受参数：
- en: '[PRE30]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Apply schema to create the `personDF` DataFrame:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用模式以创建`personDF` DataFrame：
- en: '[PRE31]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Register the `personDF` as a table:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`personDF`注册为表：
- en: '[PRE32]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Run a SQL query against it:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对其运行SQL查询：
- en: '[PRE33]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Get the output values from `persons`:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`persons`获取输出值：
- en: '[PRE34]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In this recipe, we learned how to create a DataFrame by programmatically specifying
    schema.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们学习了如何通过以编程方式指定模式来创建DataFrame。
- en: How it works…
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'A `StructType` object defines the schema. You can consider it equivalent to
    a table or a row in the relational world. `StructType` takes in an array of the
    `StructField` objects, as in the following signature:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`StructType`对象定义了模式。您可以将其视为关系世界中的表或行的等价物。`StructType`接受`StructField`对象的数组，如以下签名所示：'
- en: '[PRE35]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'A `StructField` object has the following signature:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`StructField`对象具有以下签名：'
- en: '[PRE36]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here is some more information on the parameters used:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是有关使用的参数的更多信息：
- en: '`name`: This represents the name of the field.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：这代表字段的名称。'
- en: '`dataType`: This shows the datatype of this field.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataType`：这显示了该字段的数据类型。'
- en: 'The following datatypes are allowed:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 允许以下数据类型：
- en: '| `IntegerType` | `FloatType` |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| `IntegerType` | `FloatType` |'
- en: '| `BooleanType` | `ShortType` |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| `BooleanType` | `ShortType` |'
- en: '| `LongType` | `ByteType` |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `LongType` | `ByteType` |'
- en: '| `DoubleType` | `StringType` |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `DoubleType` | `StringType` |'
- en: '`nullable`: This shows whether this field can be null.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nullable`：这显示了该字段是否可以为null。'
- en: '`metadata`: This shows the metadata of this field. Metadata is a wrapper over
    `Map[String,Any]` so that it can contain any arbitrary metadata.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata`：这显示了该字段的元数据。元数据是`Map[String,Any]`的包装器，因此它可以包含任意元数据。'
- en: Loading and saving data using the Parquet format
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Parquet格式加载和保存数据
- en: Apache Parquet is a columnar data storage format, specifically designed for
    big data storage and processing. Parquet is based on record shredding and assembly
    algorithm in the Google Dremel paper. In Parquet, data in a single column is stored
    contiguously.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet是一种列式数据存储格式，专为大数据存储和处理而设计。Parquet基于Google Dremel论文中的记录分解和组装算法。在Parquet中，单个列中的数据是连续存储的。
- en: The columnar format gives Parquet some unique benefits. For example, if you
    have a table with 100 columns and you mostly access 10 columns, in a row-based
    format you will have to load all 100 columns, as granularity level is at row level.
    But, in Parquet, you will only load 10 columns. Another benefit is that since
    all the data in a given column is of the same datatype (by definition), compression
    is much more efficient.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列格式为Parquet带来了一些独特的好处。例如，如果您有一个具有100列的表，并且您主要访问10列，在基于行的格式中，您将不得不加载所有100列，因为粒度级别在行级别。但是，在Parquet中，您只会加载10列。另一个好处是，由于给定列中的所有数据都是相同的数据类型（根据定义），因此压缩效率要高得多。
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Open the terminal and create the `person` data in a local file:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端并在本地文件中创建`person`数据：
- en: '[PRE37]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Upload the `person` directory to HDFS:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`person`目录上传到HDFS：
- en: '[PRE38]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell并为其提供一些额外的内存：
- en: '[PRE39]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Import for the implicit conversion:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入隐式转换：
- en: '[PRE40]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create a case class for `Person`:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为`Person`创建一个case类：
- en: '[PRE41]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Load the `person` directory from HDFS and map it to the `Person` case class:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从HDFS加载`person`目录并将其映射到`Person` case类：
- en: '[PRE42]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Convert the `personRDD` into the `person` DataFrame:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`personRDD`转换为`person` DataFrame：
- en: '[PRE43]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Register the `person` DataFrame as a temp table so that SQL queries can be run
    against it. Please note that the DataFrame name does not have to be the same as
    the table name.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`person` DataFrame注册为临时表，以便可以对其运行SQL查询。请注意，DataFrame名称不必与表名相同。
- en: '[PRE44]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Select all the person with age over 60 years:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择所有年龄超过60岁的人：
- en: '[PRE45]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Print values:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印值：
- en: '[PRE46]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s save this `sixtyPlus` RDD in the Parquet format:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们以Parquet格式保存这个`sixtyPlus` RDD：
- en: '[PRE47]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The previous step created a directory called `sp.parquet` in the HDFS root.
    You can run the `hdfs dfs -ls` command in another shell to make sure that it''s
    created:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上一步在HDFS根目录中创建了一个名为`sp.parquet`的目录。您可以在另一个shell中运行`hdfs dfs -ls`命令来确保它已创建：
- en: '[PRE48]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Load contents of the Parquet files in the Spark shell:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Spark shell中加载Parquet文件的内容：
- en: '[PRE49]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Register the loaded `parquet` DF as a `temp` table:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将加载的`parquet` DF注册为`temp`表：
- en: '[PRE50]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Run a query against the preceding `temp` table:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对上述`temp`表运行查询：
- en: '[PRE51]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How it works…
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Let''s spend some time understanding the Parquet format deeper. The following
    is sample data represented in the table format:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一些时间更深入地了解Parquet格式。以下是以表格格式表示的示例数据：
- en: '| First_Name | Last_Name | Age |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 名 | 姓 | 年龄 |'
- en: '| --- | --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Barack | Obama | 53 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Barack | Obama | 53 |'
- en: '| George | Bush | 68 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| George | Bush | 68 |'
- en: '| Bill | Clinton | 68 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Bill | Clinton | 68 |'
- en: 'In the row format, the data will be stored like this:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在行格式中，数据将存储如下：
- en: '| Barack | Obama | 53 | George | Bush | 68 | Bill | Clinton | 68 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Barack | Obama | 53 | George | Bush | 68 | Bill | Clinton | 68 |'
- en: 'In the columnar layout, the data will be stored like this:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在列式布局中，数据将存储如下：
- en: '| Row group => | Barack | George | Bill | Obama | Bush | Clinton | 53 | 68
    | 68 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 行组 => | Barack | George | Bill | Obama | Bush | Clinton | 53 | 68 | 68 |'
- en: '|   | Column chunk | Column chunk | Column chunk |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|   | 列块 | 列块 | 列块 |'
- en: 'Here''s a brief description about the different parts:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是有关不同部分的简要描述：
- en: '**Row group**: This shows the horizontal partitioning of data into rows. A
    row group consists of column chunks.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行组**：这显示了数据在行中的水平分区。行组由列块组成。'
- en: '**Column chunk**: A column chunk has data for a given column in a row group.
    A column chunk is always physically contiguous. A row group has only one column
    chunk per column.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列块**：列块包含行组中给定列的数据。列块始终是物理上连续的。每个行组每列只有一个列块。'
- en: '**Page**: A column chunk is divided into pages. A page is a unit of storage
    and cannot be further divided. Pages are written back to back in column chunk.
    The data for a page can be compressed.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**页面**：列块被分成页面。页面是存储单位，不能进一步分割。页面在列块中依次写入。页面的数据可以被压缩。'
- en: 'If there is already data in a Hive table, say, the `person` table, you can
    directly save it in the Parquet format by performing the following steps:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Hive表中已经有数据，比如`person`表，您可以通过以下步骤直接将其保存为Parquet格式：
- en: 'Create a table named `person_parquet` with schema, the same as `person`, but
    in the Parquet storage format (for Hive 0.13 onwards):'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建名为`person_parquet`的表，模式与`person`相同，但存储格式为Parquet（从Hive 0.13开始）：
- en: '[PRE52]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Insert data in the `person_parquet` table by importing it from the `person`
    table:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从`person`表导入数据，在`person_parquet`表中插入数据：
- en: '[PRE53]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Tip
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Sometimes, data imported from other sources, such as Impala, saves string as
    binary. To convert it to string while reading, set the following property in `SparkConf`:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，从其他来源（如Impala）导入的数据会将字符串保存为二进制。在读取时将其转换为字符串，设置以下属性在`SparkConf`中：
- en: '[PRE54]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: There's more…
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'If you are using Spark 1.4 or later, there is a new interface both to write
    to and read from Parquet. To write the data to Parquet (step 11 rewritten), let''s
    save this `sixtyPlus` RDD to the Parquet format (RDD implicitly gets converted
    to DataFrame):'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是Spark 1.4或更高版本，有一个新的接口可以写入和从Parquet中读取。要将数据写入Parquet（第11步重写），让我们将这个`sixtyPlus`
    RDD保存为Parquet格式（RDD隐式转换为DataFrame）：
- en: '[PRE55]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To read from Parquet (step 13 rewritten; the result is DataFrame), load the
    contents of the Parquet files in the Spark shell:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Parquet中读取（第13步重写；结果是DataFrame），在Spark shell中加载Parquet文件的内容：
- en: '[PRE56]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Loading and saving data using the JSON format
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用JSON格式加载和保存数据
- en: JSON is a lightweight data-interchange format. It is based on a subset of the
    JavaScript programming language. JSON's popularity is directly related to XML
    getting unpopular. XML was a great solution to provide a structure to the data
    in a plain text format. With time, XML documents became more and more heavy and
    the overhead was not worth it.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是一种轻量级的数据交换格式。它基于JavaScript编程语言的一个子集。JSON的流行与XML的不受欢迎直接相关。XML是提供数据结构的一种很好的解决方案，以纯文本格式呈现。随着时间的推移，XML文档变得越来越沉重，开销不值得。
- en: JSON solved this problem by providing structure with minimal overhead. Some
    people call JSON **fat-free XML**.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: JSON通过提供结构并最小化开销解决了这个问题。有些人称JSON为**无脂肪XML**。
- en: 'The JSON syntax follows these rules:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: JSON语法遵循以下规则：
- en: 'Data is in the form of key-value pairs:'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据以键值对的形式呈现：
- en: '[PRE57]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'There are four datatypes in JSON:'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON中有四种数据类型：
- en: 'String ("firstName" : "Barack")'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串（"firstName"："Barack"）
- en: 'Number ("age" : 53)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字（"age"：53）
- en: 'Boolean ("alive": true)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布尔值（"alive"：true）
- en: 'null ("manager" : null)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: null（"manager"：null）
- en: Data is delimited by commas
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据由逗号分隔
- en: 'Curly braces {} represents an object:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花括号{}表示对象：
- en: '[PRE58]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Square brackets [] represent an array:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方括号[]表示数组：
- en: '[PRE59]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: In this recipe, we will explore how to save and load it in the JSON format.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将探讨如何以JSON格式保存和加载数据。
- en: How to do it...
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Open the terminal and create the `person` data in the JSON format:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端并以JSON格式创建`person`数据：
- en: '[PRE60]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Upload the `jsondata` directory to HDFS:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`jsondata`目录上传到HDFS：
- en: '[PRE61]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell并为其提供一些额外的内存：
- en: '[PRE62]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Create an instance of `SQLContext`:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`SQLContext`的实例：
- en: '[PRE63]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Import for the implicit conversion:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入隐式转换：
- en: '[PRE64]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Load the `jsondata` directory from HDFS:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从HDFS加载`jsondata`目录：
- en: '[PRE65]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Register the `person` DF as a `temp` table so that the SQL queries can be run
    against it:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`person` DF注册为`temp`表，以便对其运行SQL查询：
- en: '[PRE66]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Select all the persons with age over 60 years:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择所有年龄超过60岁的人：
- en: '[PRE67]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Print values:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印值：
- en: '[PRE68]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Let's save this `sixtyPlus` DF in the JSON format
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们以JSON格式保存这个`sixtyPlus`数据框
- en: '[PRE69]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Last step created a directory called `sp` in the HDFS root. You can run the
    `hdfs dfs -ls` command in another shell to make sure it''s created:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上一步在HDFS根目录创建了一个名为`sp`的目录。您可以在另一个shell中运行`hdfs dfs -ls`命令来确保它已创建：
- en: '[PRE70]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: How it works…
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: The `sc.jsonFile` internally uses `TextInputFormat`, which processes one line
    at a time. Therefore, one JSON record cannot be on multiple lines. It would be
    a valid JSON format if you use multiple lines, but it will not work with Spark
    and will throw an exception.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`sc.jsonFile`内部使用`TextInputFormat`，它一次处理一行。因此，一个JSON记录不能跨多行。如果使用多行，它将是有效的JSON格式，但不会在Spark中工作，并会抛出异常。'
- en: 'It is allowed to have more than one object in a line. For example, you can
    have the information of two persons in one line as an array, as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 允许一行中有多个对象。例如，您可以将两个人的信息作为数组放在一行中，如下所示：
- en: '[PRE71]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This recipe concludes saving and loading data in the JSON format using Spark.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程介绍了使用Spark以JSON格式保存和加载数据的方法。
- en: There's more…
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'If you are using Spark Version 1.4 or later, `SqlContext` provides an easier
    interface to load the `jsondata` directory from HDFS:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是Spark 1.4或更高版本，`SqlContext`提供了一个更容易的接口来从HDFS加载`jsondata`目录：
- en: '[PRE72]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The `sqlContext.jsonFile` is deprecated in version 1.4, and `sqlContext.read.json`
    is the recommend approach.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`sqlContext.jsonFile`在1.4版本中已被弃用，推荐使用`sqlContext.read.json`。'
- en: Loading and saving data from relational databases
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从关系数据库加载和保存数据
- en: In the previous chapter, we learned how to load data from a relational data
    into an RDD using JdbcRDD. Spark 1.4 has support to load data directly into Dataframe
    from a JDBC resource. This recipe will explore how to do it.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何使用JdbcRDD将关系数据加载到RDD中。Spark 1.4支持直接从JDBC资源加载数据到Dataframe。本教程将探讨如何实现这一点。
- en: Getting ready
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Please make sure that JDBC driver JAR is visible on the client node and all
    the slaves nodes on which executor will run.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保JDBC驱动程序JAR在客户端节点和所有执行器将运行的从节点上可见。
- en: How to do it...
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Create a table named `person` in MySQL using the following DDL:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在MySQL中创建名为`person`的表，使用以下DDL：
- en: '[PRE73]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Insert some data:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入一些数据：
- en: '[PRE74]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Download `mysql-connector-java-x.x.xx-bin.jar` from [http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/).
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/)下载`mysql-connector-java-x.x.xx-bin.jar`。
- en: 'Make MySQL driver available to the Spark shell and launch it:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使MySQL驱动程序可用于Spark shell并启动它：
- en: '[PRE75]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Note
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that `path-to-mysql-jar` is not the actual path name. You need to
    use your path name.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`path-to-mysql-jar`不是实际的路径名。您需要使用您的路径名。
- en: 'Construct a JDBC URL:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建JDBC URL：
- en: '[PRE76]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Create a connection properties object with username and password:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含用户名和密码的连接属性对象：
- en: '[PRE77]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Load DataFrame with JDBC data source (url, table name, properties):'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用JDBC数据源加载DataFrame（url、表名、属性）：
- en: '[PRE78]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Show the results in a nice tabular format by executing the following command:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令以漂亮的表格格式显示结果：
- en: '[PRE79]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'This has loaded the whole table. What if I only would like to load males (url,
    table name, predicates, properties)? To do this, run the following command:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这已经加载了整个表。如果我只想加载男性（url、表名、谓词、属性）怎么办？要做到这一点，请运行以下命令：
- en: '[PRE80]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Show only first names by executing the following command:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令只显示名字：
- en: '[PRE81]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Show only people below age 60 by executing the following command:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令只显示年龄低于60岁的人：
- en: '[PRE82]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Group people by gender as follows:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按性别对人进行分组：
- en: '[PRE83]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Find the number of males and females by executing the following command:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令找到男性和女性的数量：
- en: '[PRE84]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Find the average age of males and females by executing the following command:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令找到男性和女性的平均年龄：
- en: '[PRE85]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Now if you''d like to save this `avg_age` data to a new table, run the following
    command:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，如果您想将这个`avg_age`数据保存到一个新表中，请运行以下命令：
- en: '[PRE86]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Save the people DataFrame in the Parquet format:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将people DataFrame以Parquet格式保存：
- en: '[PRE87]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Save the people DataFrame in the JSON format:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将people DataFrame保存为JSON格式：
- en: '[PRE88]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Loading and saving data from an arbitrary source
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从任意数据源加载和保存数据
- en: So far, we have covered three data sources that are inbuilt with DataFrames—`parquet`
    (default), `json`, and `jdbc`. Dataframes are not limited to these three and can
    load and save to any arbitrary data source by specifying the format manually.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了三种内置于DataFrame中的数据源——`parquet`（默认），`json`和`jdbc`。DataFrame不仅限于这三种，可以通过手动指定格式加载和保存到任意数据源。
- en: In this recipe, we will cover loading and saving data from arbitrary sources.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将介绍从任意数据源加载和保存数据。
- en: How to do it...
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Start the Spark shell and give it some extra memory:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell并为其提供一些额外的内存：
- en: '[PRE89]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Load the data from Parquet; since `parquet` is the default data source, you
    do not have to specify it:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Parquet加载数据；由于`parquet`是默认数据源，您不必指定它：
- en: '[PRE90]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Load the data from Parquet by manually specifying the format:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过手动指定格式从Parquet加载数据：
- en: '[PRE91]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'For inbuilt datatypes (`parquet`,`json`, and `jdbc`), you do not have to specify
    the full format name, only specifying `"parquet"`, `"json"`, or `"jdbc"` works:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于内置数据类型（`parquet`，`json`和`jdbc`），您不必指定完整的格式名称，只需指定`"parquet"`，`"json"`或`"jdbc"`即可：
- en: '[PRE92]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Note
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'When writing data, there are four save modes: `append`, `overwrite`, `errorIfExists`,
    and `ignore`. The `append` mode adds data to data source, `overwrite` overwrites
    it, `errorIfExists` throws an exception that data already exists, and `ignore`
    does nothing when data already exists.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在写入数据时，有四种保存模式：`append`，`overwrite`，`errorIfExists`和`ignore`。`append`模式将数据添加到数据源，`overwrite`将其覆盖，`errorIfExists`在数据已经存在时抛出异常，`ignore`在数据已经存在时不执行任何操作。
- en: 'Save people as JSON in the `append` mode:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将people保存为JSON格式，使用`append`模式：
- en: '[PRE93]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: There's more…
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The Spark SQL's data source API saves to a variety of data sources. To find
    more information, visit [http://spark-packages.org/](http://spark-packages.org/).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL的数据源API可以保存到各种数据源。要获取更多信息，请访问[http://spark-packages.org/](http://spark-packages.org/)。
