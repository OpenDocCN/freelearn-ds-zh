- en: Obtaining, Processing, and Preparing Data with Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark获取、处理和准备数据
- en: Machine learning is an extremely broad field, and these days, applications can
    be found across areas that include web and mobile applications, the Internet of
    Things and sensor networks, financial services, healthcare, and various scientific
    fields, to name just a few.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个非常广泛的领域，如今，应用可以在包括网络和移动应用、物联网和传感器网络、金融服务、医疗保健以及各种科学领域等领域找到。
- en: Therefore, the range of data available for potential use in machine learning
    is enormous. In this book, we will focus mostly on business applications. In this
    context, the data available often consists of data internal to an organization
    (such as transactional data for a financial services company) as well as external
    data sources (such as financial asset price data for the same financial services
    company).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，机器学习可用的数据范围是巨大的。在本书中，我们将主要关注业务应用。在这种情况下，可用的数据通常包括组织内部的数据（例如金融服务公司的交易数据）以及外部数据源（例如同一金融服务公司的金融资产价格数据）。
- en: For example, you'll recall from [Chapter 3](fbb4c025-a861-4b26-8284-a8ae5f0f0d88.xhtml),
    *Designing a Machine Learning System*, that the main internal source of data for
    our hypothetical internet business, Movie Stream, consists of data on the movies
    available on the site, the users of the service, and their behavior. This includes
    data about movies and other content (for example, title, categories, description,
    images, actors, and directors), user information (for example, demographics, location,
    and so on), and user activity data (for example, web page views, title previews
    and views, ratings, reviews, and social data such as *likes*, *shares*, and social
    network profiles on services, including Facebook and Twitter).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您会从[第3章](fbb4c025-a861-4b26-8284-a8ae5f0f0d88.xhtml)中回忆起，*设计一个机器学习系统*，我们假想的互联网业务Movie
    Stream的主要内部数据来源包括网站上可用电影的数据，服务的用户以及他们的行为。这包括有关电影和其他内容的数据（例如标题，类别，描述，图片，演员和导演），用户信息（例如人口统计学，位置等），以及用户活动数据（例如网页浏览，标题预览和浏览，评分，评论，以及*喜欢*，*分享*等社交数据，包括Facebook和Twitter等社交网络资料）。
- en: External data sources in this example may include weather and geolocation services,
    third-party movie ratings, and review sites such as *IMDB* and *Rotten Tomatoes*,
    and so on.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，外部数据源可能包括天气和地理位置服务，第三方电影评分和评论网站，比如*IMDB*和*Rotten Tomatoes*等。
- en: Generally speaking, it is quite difficult to obtain data of an internal nature
    for real-world services and businesses, as it is commercially sensitive (in particular,
    data on purchasing activity, user or customer behavior, and revenue) and of great
    potential value to the organization concerned. This is why it is also often the
    most useful and interesting data on which to apply machine learning--a good machine
    learning model that can make accurate predictions can be highly valuable (witness
    the success of machine learning competitions, such as *Netflix Prize* and *Kaggle*).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，要获取真实世界服务和企业的内部数据是非常困难的，因为这些数据具有商业敏感性（特别是购买活动数据，用户或客户行为以及收入数据），对相关组织具有巨大的潜在价值。这也是为什么这些数据通常是应用机器学习的最有用和有趣的数据--一个能够做出准确预测的好的机器学习模型可能具有很高的价值（比如机器学习竞赛的成功，比如*Netflix
    Prize*和*Kaggle*）。
- en: In this book, we will make use of datasets that are publicly available to illustrate
    concepts around data processing and training of machine learning models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将利用公开可用的数据集来说明数据处理和机器学习模型训练的概念。
- en: 'In this chapter, we will:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将：
- en: Briefly cover the types of data typically used in machine learning.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简要介绍机器学习中通常使用的数据类型。
- en: Provide examples of where to obtain interesting datasets, often publicly available
    on the internet. We will use some of these datasets throughout the book to illustrate
    the use of the models we introduce.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供获取有趣数据集的例子，这些数据集通常可以在互联网上公开获取。我们将在整本书中使用其中一些数据集来说明我们介绍的模型的使用。
- en: Discover how to process, clean, explore, and visualize our data.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何处理、清理、探索和可视化我们的数据。
- en: Introduce various techniques to transform our raw data into features that can
    be used as input to machine learning algorithms.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍各种技术，将我们的原始数据转换为可以用作机器学习算法输入的特征。
- en: Learn how to normalize input features using external libraries as well as Spark's
    built-in functionality.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用外部库以及Spark内置功能来规范输入特征。
- en: Accessing publicly available datasets
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问公开可用的数据集
- en: 'Fortunately, while commercially sensitive data can be hard to come by, there
    are still a number of useful datasets available publicly. Many of these are often
    used as benchmark datasets for specific types of machine learning problems. Examples
    of common data sources include:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，虽然商业敏感数据可能很难获得，但仍然有许多有用的公开数据集可用。其中许多经常被用作特定类型的机器学习问题的基准数据集。常见数据来源的例子包括：
- en: '**UCI Machine Learning Repository**: This is a collection of almost 300 datasets
    of various types and sizes for tasks, including classification, regression, clustering,
    and recommender systems. The list is available at [http://archive.ics.uci.edu/ml/](http://archive.ics.uci.edu/ml/).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UCI机器学习库**：这是一个包含近300个各种类型和大小的数据集的集合，用于分类、回归、聚类和推荐系统等任务。列表可在[http://archive.ics.uci.edu/ml/](http://archive.ics.uci.edu/ml/)找到。'
- en: '**Amazon AWS public datasets**: This is a set of often very large datasets
    that can be accessed via Amazon S3\. These datasets include the Human Genome Project,
    the Common Crawl web corpus, Wikipedia data, and Google Books Ngrams. Information
    on these datasets can be found at [http://aws.amazon.com/publicdatasets/](http://aws.amazon.com/publicdatasets/).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon AWS公共数据集**：这是一组通常非常庞大的数据集，可以通过Amazon S3访问。这些数据集包括人类基因组计划，Common Crawl网络语料库，维基百科数据和Google图书Ngrams。这些数据集的信息可以在[http://aws.amazon.com/publicdatasets/](http://aws.amazon.com/publicdatasets/)找到。'
- en: '**Kaggle**: This is a collection of datasets used in machine learning competitions
    run by Kaggle. Areas include classification, regression, ranking, recommender
    systems, and image analysis. These datasets can be found under the Competitions
    section at [http://www.kaggle.com/competitions](http://www.kaggle.com/competitions).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kaggle**：这是Kaggle举办的机器学习竞赛中使用的数据集的集合。领域包括分类、回归、排名、推荐系统和图像分析。这些数据集可以在[http://www.kaggle.com/competitions](http://www.kaggle.com/competitions)的竞赛部分找到。'
- en: '**KDnuggets**: This has a detailed list of public datasets, including some
    of those mentioned earlier. The list is available at [http://www.kdnuggets.com/datasets/index.html](http://www.kdnuggets.com/datasets/index.html).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KDnuggets**：这里有一个详细的公共数据集列表，包括之前提到的一些。列表可在[http://www.kdnuggets.com/datasets/index.html](http://www.kdnuggets.com/datasets/index.html)找到。'
- en: There are many other resources to find public datasets depending on the specific
    domain and machine-learning task. Hopefully, you might also have exposure to some
    interesting academic or commercial data of your own!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 根据具体领域和机器学习任务的不同，还有许多其他资源可以找到公共数据集。希望你也可能接触到一些有趣的学术或商业数据！
- en: To illustrate a few key concepts related to data processing, transformation,
    and feature extraction in Spark, we will download a commonly used dataset for
    movie recommendations; this dataset is known as the **MovieLens** dataset. As
    it is applicable to recommender systems as well as potentially other machine learning
    tasks, it serves as a useful example dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明Spark中与数据处理、转换和特征提取相关的一些关键概念，我们将下载一个常用的用于电影推荐的数据集；这个数据集被称为**MovieLens**数据集。由于它适用于推荐系统以及潜在的其他机器学习任务，它作为一个有用的示例数据集。
- en: The MovieLens 100k dataset
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MovieLens 100k数据集
- en: The MovieLens 100k dataset is a set of 100,000 data points related to ratings
    given by a set of users to a set of movies. It also contains movie metadata and
    user profiles. While it is a small dataset, you can quickly download it and run
    Spark code on it. This makes it ideal for illustrative purposes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: MovieLens 100k数据集是与一组用户对一组电影的评分相关的10万个数据点。它还包含电影元数据和用户配置文件。虽然它是一个小数据集，但你可以快速下载并在其上运行Spark代码。这使得它非常适合作为示例。
- en: You can download the dataset from [http://files.grouplens.org/datasets/movielens/ml-100k.zip](http://files.grouplens.org/datasets/movielens/ml-100k.zip).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从[http://files.grouplens.org/datasets/movielens/ml-100k.zip](http://files.grouplens.org/datasets/movielens/ml-100k.zip)下载数据集。
- en: 'Once you have downloaded the data, unzip it using your terminal:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下载数据后，使用终端解压缩它：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will create a directory called `ml-100k`. Change into this directory and
    examine the contents. The important files are `u.user` (user profiles), `u.item`
    (movie metadata), and `u.data` (the ratings given by users to movies):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为`ml-100k`的目录。进入此目录并检查内容。重要的文件是`u.user`（用户配置文件）、`u.item`（电影元数据）和`u.data`（用户对电影的评分）：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `README` file contains more information on the dataset, including the variables
    present in each data file. We can use the head command to examine the contents
    of the various files.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`README`文件包含有关数据集的更多信息，包括每个数据文件中存在的变量。我们可以使用head命令来检查各个文件的内容。'
- en: 'For example, we can see that the `u.user` file contains the user ID, age, gender,
    occupation, and ZIP code fields, separated by a pipe (`|` character):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到`u.user`文件包含用户ID、年龄、性别、职业和邮政编码字段，用管道(`|`)字符分隔：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `u.item` file contains the movie ID, title, release data, and IMDB link
    fields and a set of fields related to movie category data. It is also separated
    by a `|` character:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`u.item`文件包含电影ID、标题、发布日期和IMDB链接字段以及一组与电影类别数据相关的字段。它也是用`|`字符分隔的：'
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The previous data listed has the following format:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 前面列出的数据格式如下：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The last 19 fields are the genres, a 1 indicates the movie is of that genre,
    a 0 indicates it is not; movies can be in several genres at once.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后19个字段是电影的流派，1表示电影属于该流派，0表示不属于；电影可以同时属于几种流派。
- en: 'The movie IDs are the ones used in the `u.data` dataset. It contains100000
    ratings by 943 users on 1682 items. Each user has rated at least 20 movies. Users
    and items are numbered consecutively from 1\. The data is randomly ordered. This
    is a tab separated list of following fields:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 电影ID是`u.data`数据集中使用的ID。它包含943个用户对1682个项目的100000个评分。每个用户至少对20部电影进行了评分。用户和项目从1开始编号。数据是随机排序的。这是一个以制表符分隔的字段列表：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The time stamps are Unix seconds since 1/1/1970 UTC.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳是自1970年1月1日UTC以来的Unix秒。
- en: 'Let''s look at some data from the u.data file:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下`u.data`文件中的一些数据：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Exploring and visualizing your data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索和可视化你的数据
- en: 'Source code for the chapter can be found at `PATH/spark-ml/Chapter04`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的源代码可以在`PATH/spark-ml/Chapter04`找到：
- en: Python code is available at `/MYPATH/spark-ml/Chapter_04/python`
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python代码位于`/MYPATH/spark-ml/Chapter_04/python`
- en: Scala code is available at`/MYPATH/spark-ml/Chapter_04/scala`
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala代码位于`/MYPATH/spark-ml/Chapter_04/scala`
- en: 'Pythons Samples are available for both version 1.6.2 and 2.0.0; we will focus
    on 2.0.0 in this book:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Python示例可用于1.6.2和2.0.0版本；我们将在本书中专注于2.0.0版本：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Scala samples are structured as shown in the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Scala示例的结构如下所示：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Scala 2.0.0 Samples:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 2.0.0示例：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Go to the following directory and run the following commands to run the samples:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 转到以下目录并运行以下命令来运行示例：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Exploring the user dataset
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索用户数据集
- en: First, we will analyze the characteristics of MovieLens users.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将分析MovieLens用户的特征。
- en: 'We use a `custom_schema` to load the `|` delimited data into a DataFrame. This
    Python code is in `com/sparksamples/Util.py`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`custom_schema`将`|`分隔的数据加载到DataFrame中。这个Python代码在`com/sparksamples/Util.py`中：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This function is called from `user_data.py` as show following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数是从`user_data.py`中调用的，如下所示：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You should see output similar to this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似于这样的输出：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Code-listing:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单：
- en: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/user_data.py](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/user_data.py)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/user_data.py](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/user_data.py)'
- en: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/util.py](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/util.py)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/util.py](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/util.py)'
- en: 'Similar code in Scala for loading data into a DataFrame is listed as follows.
    This code is in `Util.scala`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据加载到DataFrame中的Scala中的类似代码如下。此代码在`Util.scala`中：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You should see output similar to this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似于这样的输出：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The code listing is at : [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserData.scala)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单在：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserData.scala)
- en: As we can see, this is the first line of our user data file, separated by the
    `"|"` character.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，这是我们用户数据文件的第一行，由`"|"`字符分隔。
- en: The `first` function is similar to `collect`, but it only returns the first
    element of the RDD to the driver. We can also use `take(k)` to collect only the
    first *k* elements of the RDD to the driver.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`first`函数类似于`collect`，但它只将RDD的第一个元素返回给驱动程序。我们还可以使用`take(k)`来仅将RDD的前*k*个元素收集到驱动程序。'
- en: 'We will use the DataFrame created earlier and use the `groupBy` function followed
    by `count()` and `collect()` to calculate number of users, genders, ZIPcodes,
    and occupations. We will then count the number of users, genders, occupations,
    and ZIP codes. We can achieve this by running the following code. Note that we
    do not cache the data, as it is unnecessary for this small size:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前创建的DataFrame，并使用`groupBy`函数，然后是`count()`和`collect()`来计算用户数、性别、邮政编码和职业。然后计算用户数、性别、职业和邮政编码的数量。我们可以通过运行以下代码来实现这一点。请注意，我们不需要对数据进行缓存，因为这个大小是不必要的：
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You will see the following output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到以下输出：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Similarly, we can implement the logic of getting number of users, genders, occupations,
    and zip codes using Scala.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以使用Scala实现获取用户数、性别、职业和邮政编码的逻辑。
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You will see the following output:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到以下输出：
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserData.scala)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserData.scala)
- en: Next, we will create a histogram to analyze the distribution of user ages.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个直方图来分析用户年龄的分布。
- en: In Python, first we get the `DatFrame` into variable `user_data`. Next, we'll
    call `select('age')` and collect the result into List of Row object. Then, we
    iterate and extract age parameter and populate `user_ages_list`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，首先我们将`DataFrame`获取到变量`user_data`中。接下来，我们将调用`select('age')`并将结果收集到Row对象的列表中。然后，我们迭代并提取年龄参数并填充`user_ages_list`。
- en: We will be using Python matplotlib library's `hist` function.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Python matplotlib库的`hist`函数。
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Find the code listing at:[ https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/plot_user_ages.py](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/plot_user_ages.py)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/plot_user_ages.py](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/plot_user_ages.py)
- en: We passed in the `user_ages_list`, together with the number of bins for our
    histogram (20 in this case), to the `hist` function. Using the `normed=True` argument,
    we also specified that we want the histogram to be normalized so that each bucket
    represents the percentage of the overall data that falls into that bucket.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`user_ages_list`和我们直方图的箱数（在这种情况下为20）一起传递给`hist`函数。使用`normed=True`参数，我们还指定希望直方图被归一化，以便每个桶代表落入该桶的整体数据的百分比。
- en: You will see an image containing the histogram chart, which looks something
    like the one shown here. As we can see, the ages of MovieLens users are somewhat
    skewed toward younger viewers. A large number of users are between the ages of
    about 15 and 35.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到包含直方图图表的图像，看起来类似于这里显示的图像。正如我们所看到的，MovieLens用户的年龄有些偏向年轻的观众。大量用户的年龄在15到35岁左右。
- en: '![](img/image_04_001.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_001.png)'
- en: Distribution of user ages
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 用户年龄的分布
- en: For Scala Histogram Chart, we are using a JFreeChart-based library. We divided
    the data into 16 bins to the show the distribution.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Scala直方图图表，我们使用基于JFreeChart的库。我们将数据分成16个箱子来显示分布。
- en: We are using the [https://github.com/wookietreiber/scala-chart](https://github.com/wookietreiber/scala-chart)
    library to create a barchart from the Scala map `m_sorted`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用[https://github.com/wookietreiber/scala-chart](https://github.com/wookietreiber/scala-chart)库从Scala映射`m_sorted`创建条形图。
- en: First we extract the `ages_array` from `userDataFrame` using the `select("age")`
    function.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`select("age")`函数从`userDataFrame`中提取`ages_array`。
- en: 'Then, we populate the `mx` Map, which is the bins for displaying. We sort the
    mx Map to create a `ListMap`, which is then used to populate `DefaultCategorySet
    ds`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们填充`mx` Map，这是用于显示的箱子。我们对mx Map进行排序以创建`ListMap`，然后用它来填充`DefaultCategorySet
    ds`：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The complete code can be found in `UserAgesChart.scala`file and is listed here:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在`UserAgesChart.scala`文件中找到，并在此处列出：
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserAgesChart.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserAgesChart.scala)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserAgesChart.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserAgesChart.scala)
- en: '![](img/image_04_002.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_002.png)'
- en: Count by occupation
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按职业计数
- en: We count a number of the various occupations of our users.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们统计用户各种职业的数量。
- en: The following steps were implemented to get the occupation DataFrame and populate
    the list, which was displayed using Matplotlib.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 实施以下步骤以获取职业DataFrame并填充列表，然后使用Matplotlib显示。
- en: Get `user_data`.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取`user_data`。
- en: Extract occupation count using `groupby("occupation")` and calling `count()`
    on it.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`groupby("occupation")`提取职业计数并对其调用`count()`。
- en: Extract list of `tuple("occupation","count")` from the list of rows.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从行列表中提取`tuple("occupation","count")`的列表。
- en: Create a `numpy` array of values in `x_axis` and `y_axis`.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`x_axis`和`y_axis`中的值的`numpy`数组。
- en: Create a plot of type bar.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建类型为bar的图表。
- en: Display the chart.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示图表。
- en: 'The complete code listing can be found following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码清单如下：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The image you have generated should look like the one here. It appears that
    the most prevalent occupations are **student**, **other**, **educator**, **administrator**,
    **engineer**, and **programmer**.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您生成的图像应该看起来像这里的图像。看起来最普遍的职业是**学生**，**其他**，**教育工作者**，**管理员**，**工程师**和**程序员**。
- en: '![](img/image_04_003.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_003.png)'
- en: Distribution of user occupations
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 用户职业分布
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/plot_user_occupations.py](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/plot_user_occupations.py)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/plot_user_occupations.py](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/plot_user_occupations.py)
- en: 'In Scala, we follow the following steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中，我们按以下步骤进行操作：
- en: First get the `userDataFrame`
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先获取`userDataFrame`
- en: 'We extract occupation column:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们提取职业列：
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Group the rows by occupation:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按职业对行进行分组：
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Sort the rows by count:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按计数对行进行排序：
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Populate Default category set ds from: `occupation_groups_collection`'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`occupation_groups_collection`中填充默认类别集ds
- en: Display the Jfree Bar Chart
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示Jfree Bar Chart
- en: 'The complete code listing is given following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码清单如下：
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output of this code is shown here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的输出如下所示：
- en: '![](img/image_04_004.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_004.png)'
- en: 'The following figure shows the JFreeChart generated from the previous source
    code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了从先前源代码生成的JFreeChart：
- en: '![](img/image_04_005.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_005.png)'
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branched2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserOccupationChart.scala](https://github.com/ml-resources/spark-ml/blob/branched2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserOccupationChart.scala)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branched2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserOccupationChart.scala](https://github.com/ml-resources/spark-ml/blob/branched2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserOccupationChart.scala)
- en: Movie dataset
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 电影数据集
- en: 'Next, we will investigate a few properties of the movie catalog. We can inspect
    a row of the movie data file, as we did for the user data earlier, and then count
    the number of movies:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将调查电影目录的一些属性。我们可以检查电影数据文件的一行，就像我们之前对用户数据所做的那样，然后计算电影的数量：
- en: 'We will create a DataFrame of movie data by parsing using the format `com.databrick.spark.csv`
    and giving a `|` delimiter. Then, we use a `CustomSchema` to populate the DataFrame
    and return it:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用格式`com.databrick.spark.csv`进行解析并给出`|`分隔符来创建电影数据的DataFrame。然后，我们使用`CustomSchema`来填充DataFrame并返回它：
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This method is then called from the `MovieData` Scala Object.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后从`MovieData` Scala对象调用此方法。
- en: 'The following steps are implemented to filter the date and format it into a
    `Year`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 实施以下步骤以过滤日期并将其格式化为`Year`：
- en: Create a Temp View.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个临时视图。
- en: Register the function `Util.convertYear` as a UDF with `SparkSession`.`Util.spark`
    (this is our custom class).
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`SparkSession`.`Util.spark`将函数`Util.convertYear`注册为UDF（这是我们的自定义类）。
- en: Execute SQLon this `SparkSession` as shown following.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此`SparkSession`上执行SQL，如下所示。
- en: Group the resulting DataFrame by `Year` and call `count()` function.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的DataFrame按`Year`分组并调用`count()`函数。
- en: 'The complete code listing for the logic is given here:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑的完整代码清单如下：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output will be similar as shown here:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将与此处显示的类似：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieData.scala)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieData.scala)
- en: Next, we draw the graph of ages of movies collection created earlier. We use
    JFreeChart in Scala and populate `org.jfree.data.category.DefaultCategoryDataset`
    from the collection created by `MovieData.getMovieYearsCountSorted()`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们绘制先前创建的电影收藏年龄的图表。我们在Scala中使用JFreeChart，并从`MovieData.getMovieYearsCountSorted()`创建的收藏中填充`org.jfree.data.category.DefaultCategoryDataset`。
- en: '[PRE30]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Note that most of the movies are from 1996\. The graph created is shown here:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，大多数电影来自1996年。创建的图表如下所示：
- en: '![](img/image_04_006.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_006.png)'
- en: Distribution of movie ages
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 电影年龄分布
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieAgesChart.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieAgesChart.scala)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieAgesChart.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieAgesChart.scala)
- en: Exploring the rating dataset
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索评分数据集
- en: 'Let''s now take a look at the rating data:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下评分数据：
- en: 'The code is located under `RatingData`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 代码位于`RatingData`下：
- en: '[PRE31]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output of the preceding code is listed here:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下所示：
- en: '[PRE32]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: There are 100,000 ratings, and unlike the user and movie datasets, these records
    are split with a tab character (`"t"`). As you might have guessed, we'd probably
    want to compute some basic summary statistics and frequency histograms for the
    rating values. Let's do this now.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 有100,000个评分，与用户和电影数据集不同，这些记录是用制表符（`"t"`）分隔的。正如你可能已经猜到的，我们可能想要计算一些基本的摘要统计和评分值的频率直方图。让我们现在来做这个。
- en: 'Data is separated. As you might have guessed, we''d probably want to compute
    some basic summary statistics and frequency histograms for the rating values.
    Let''s do this now:). As you might have guessed, we probably want to compute some
    basic summary statistics and frequency histograms for the rating values. Let''s
    do this now:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 数据被分开了。正如你可能已经猜到的，我们可能想要计算一些基本的摘要统计和评分值的频率直方图。让我们现在来做这个：）。正如你可能已经猜到的，我们可能想要计算一些基本的摘要统计和评分值的频率直方图。让我们现在来做这个：
- en: We will calculate the max, min, and average ratings. We will also calculate
    ratings per user and ratings per movie. We are using Spark SQL to extract the
    max, min, and average value of movie ratings.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将计算最大、最小和平均评分。我们还将计算每个用户和每部电影的评分。我们正在使用Spark SQL来提取电影评分的最大、最小和平均值。
- en: '[PRE33]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output of the preceding code is listed here:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下所示：
- en: '[PRE34]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala)
- en: Rating count bar chart
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评分计数条形图
- en: Looking at the results, the average rating given by a user to a movie is around
    3.5, so we might expect that the distribution of ratings will be skewed towards
    slightly higher ratings. Let's see whether this is true, by creating a bar chart
    of rating values using a similar procedure as we did for occupations.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果来看，用户对电影的平均评分大约为3.5，因此我们可能期望评分的分布会偏向稍高的评分。让我们通过使用与职业相似的过程创建一个评分值的条形图来看看这是否成立。
- en: 'The code for plotting ratings versus count is shown here. This is available
    in the file `CountByRatingChart.scala`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制评分与计数的代码如下所示。这在文件`CountByRatingChart.scala`中可用：
- en: '[PRE35]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'On executing the previous code, you will get the bar chart as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行上一个代码后，您将得到以下条形图：
- en: '![](img/image_04_007.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_007.png)'
- en: Distribution of number ratings
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评分数量的分布
- en: We can also look at the distribution of the number of ratings made by each user.
    Recall that we previously computed the `rating_data` RDD used in the preceding
    code by splitting the ratings with the tab character. We will now use the `rating_data`
    variable again in the following code.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看每个用户所做评分的分布。回想一下，我们之前通过使用制表符分割评分来计算了上述代码中使用的`rating_data` RDD。我们现在将在下面的代码中再次使用`rating_data`变量。
- en: Code resides in the class `UserRatingChart`. We will create a DataFrame from
    `u.data` file which is tab separated and then `groupbyuser_id` and sort by the
    count of ratings given by each user in ascending order.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 代码位于`UserRatingChart`类中。我们将从`u.data`文件创建一个DataFrame，该文件是以制表符分隔的，然后按每个用户给出的评分数量进行分组并按升序排序。
- en: '[PRE36]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Let us first try to show the ratings.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先尝试显示评分。
- en: '[PRE37]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output of the preceding code is listed here:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下所示：
- en: '[PRE38]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: After showing the data textually, let's show it using JFreeChart by loading
    `DefaultCategorySet` with data from the `rating_nos_by_user DataFrame`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在以文本方式显示数据后，让我们通过从`rating_nos_by_user DataFrame`中加载数据到`DefaultCategorySet`来使用JFreeChart显示数据。
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/image_04_008.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_008.png)'
- en: In the previous graph, x-axis is the user ID and y-axis is the number of ratings,
    which varies from minimum rating of 20 to a maximum of 737.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，x轴是用户ID，y轴是评分数量，从最低的20到最高的737不等。
- en: Processing and transforming your data
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理和转换您的数据
- en: In order to make the raw data usable in a machine learning algorithm, we first
    need to clean it up and possibly transform it in various ways before extracting
    useful features from the transformed data. The transformation and feature extraction
    steps are closely linked, and in some cases, certain transformations are themselves
    a case of feature extraction.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使原始数据可用于机器学习算法，我们首先需要清理数据，并可能以各种方式对其进行转换，然后从转换后的数据中提取有用的特征。转换和特征提取步骤是密切相关的，在某些情况下，某些转换本身就是特征提取的一种情况。
- en: 'We have already seen an example of the need to clean data in the movie dataset.
    Generally, real-world datasets contain bad data, missing data points, and outliers.
    Ideally, we would correct bad data; however, this is often not possible, as many
    datasets derive from some form of collection process that cannot be repeated (this
    is the case, for example, in web activity data and sensor data). Missing values
    and outliers are also common and can be dealt with in a manner similar to bad
    data. Overall, the broad options are as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了在电影数据集中清理数据的需要的一个例子。一般来说，现实世界的数据集包含不良数据、缺失数据点和异常值。理想情况下，我们会纠正不良数据；然而，这通常是不可能的，因为许多数据集来自某种不能重复的收集过程（例如在Web活动数据和传感器数据中的情况）。缺失值和异常值也很常见，可以以类似于不良数据的方式处理。总的来说，广泛的选择如下：
- en: '**Filter out or remove records with bad or missing values**: This is sometimes
    unavoidable; however, this means losing the good part of a bad or missing record.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤或删除具有不良或缺失值的记录**：有时是不可避免的；然而，这意味着丢失不良或缺失记录的好部分。'
- en: '**Fill in bad or missing data**: We can try to assign a value to bad or missing
    data based on the rest of the data we have available. Approaches can include assigning
    a zero value, assigning the global mean or median, interpolating nearby or similar
    data points (usually, in a time-series dataset), and so on. Deciding on the correct
    approach is often a tricky task and depends on the data, situation, and one''s
    own experience.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填补坏数据或缺失数据：我们可以尝试根据我们可用的其余数据为坏数据或缺失数据分配一个值。方法可以包括分配零值，分配全局均值或中位数，插值附近或类似的数据点（通常在时间序列数据集中），等等。决定正确的方法通常是一个棘手的任务，取决于数据、情况和个人经验。
- en: '**Apply robust techniques to outliers**: The main issue with outliers is that
    they might be correct values, even though they are extreme. They might also be
    errors. It is often very difficult to know which case you are dealing with. Outliers
    can also be removed or filled in, although fortunately, there are statistical
    techniques (such as robust regression) to handle outliers and extreme values.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对异常值应用健壮的技术：异常值的主要问题在于它们可能是正确的值，即使它们是极端的。它们也可能是错误的。很难知道你正在处理哪种情况。异常值也可以被移除或填充，尽管幸运的是，有统计技术（如健壮回归）来处理异常值和极端值。
- en: '**Apply transformations to potential outliers**: Another approach for outliers
    or extreme values is to apply transformations, such as a logarithmic or Gaussian
    kernel transformation, to features that have potential outliers, or display large
    ranges of potential values. These types of transformations have the effect of
    dampening the impact of large changes in the scale of a variable and turning a
    nonlinear relationship into one that is linear.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对潜在异常值应用转换：另一种处理异常值或极端值的方法是应用转换，比如对具有潜在异常值或显示潜在值范围较大的特征应用对数或高斯核转换。这些类型的转换可以减弱变量规模的大幅变化对结果的影响，并将非线性关系转换为线性关系。
- en: Filling in bad or missing data
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填补坏数据或缺失数据
- en: Let's take a looks at the year of movie review and clean it up.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下电影评论的年份并清理它。
- en: 'We have already seen an example of filtering out bad data. Following on from
    the preceding code, the following code snippet applies the fill-in approach to
    the bad release date record by assigning a value which is Empty String as 1900
    (this will be later replaced by the Median):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一个过滤坏数据的例子。在前面的代码之后，以下代码片段将填充方法应用于坏的发布日期记录，将空字符串分配为1900（稍后将被中位数替换）：
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In the previous code, we used the `replaceEmtryStr` function described here:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了此处描述的`replaceEmtryStr`函数：
- en: '[PRE41]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we extract filtered years which are not 1900, replace `Array[Row]` with
    `Array[int]` and calculate various metrics:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提取不是1900年的经过筛选的年份，将`Array[Row]`替换为`Array[int]`并计算各种指标：
- en: Total sum of Entries
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条目的总和
- en: Total No. of Entries
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条目的总数
- en: Mean value of Year
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年份的平均值
- en: Median value of Year
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年份的中位数
- en: Total Years after conversion
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换后的总年数
- en: Count of 1900
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1900的计数
- en: '[PRE42]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The output of the previous code is listed in the following; values with `1900`
    after replacement with median shows that our processing has been successful
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下；替换为中位数后带有`1900`的值表明我们的处理是成功的
- en: '[PRE43]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieDataFillingBadValues.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieDataFillingBadValues.scala)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处查找代码列表：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieDataFillingBadValues.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieDataFillingBadValues.scala)
- en: We computed both the mean and the median year of release here. As can be seen
    from the output, the median release year is considerably higher, because of the
    skewed distribution of the years. While it is not always straightforward to decide
    on precisely which fill-in value to use for a given situation, in this case, it
    is certainly feasible to use the median due to this skew.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里计算了发布年份的均值和中位数。从输出中可以看出，中位数发布年份要高得多，因为年份的分布是倾斜的。虽然要准确决定在给定情况下使用哪个填充值并不总是直截了当的，但在这种情况下，由于这种偏斜，使用中位数是可行的。
- en: Note that the preceding code example is, strictly speaking, not very scalable,
    as it requires collecting all the data to the driver. We can use Spark's `mean`
    function for numeric RDDs to compute the mean, but there is no median function
    available currently. We can solve this by creating our own or by computing the
    median on a sample of the dataset created using the `sample` function (we will
    see more of this in the upcoming chapters).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前面的代码示例严格来说不太可扩展，因为它需要将所有数据收集到驱动程序中。我们可以使用Spark的`mean`函数来计算数值RDD的均值，但目前没有中位数函数可用。我们可以通过创建自己的函数或使用`sample`函数创建的数据集样本来计算中位数来解决这个问题（我们将在接下来的章节中看到更多）。
- en: Extracting useful features from your data
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据中提取有用的特征
- en: Once we are done with the cleaning of our data, we are ready to get down to
    the business of extracting actual features from the data, with which our machine
    learning model can be trained.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理完成后，我们准备从数据中提取实际特征，用于训练机器学习模型。
- en: '**Features** refer to the variables that we use to train our model. Each row
    of data contains information that we would like to extract into a training example.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: “特征”是我们用来训练模型的变量。每一行数据包含我们想要提取为训练示例的信息。
- en: Almost all machine learning models ultimately work on numerical representations
    in the form of a vector; hence, we need to convert raw data into numbers.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的机器学习模型最终都是基于数字表示形式的向量进行工作；因此，我们需要将原始数据转换为数字。
- en: 'Features broadly fall into a few categories, which are as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 特征大致分为几类，如下所示：
- en: '**Numerical features**: These features are typically real or integer numbers,
    for example, the user age that we used in an example earlier.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值特征**：这些特征通常是实数或整数，例如我们之前使用的用户年龄。'
- en: '**Categorical features**: These features refer to variables that can take one
    of a set of possible states at any given time. Examples from our dataset might
    include a user''s gender or occupation or movie categories.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类特征**：这些特征指的是变量在任何给定时间可以取一组可能状态中的一个。我们数据集中的示例可能包括用户的性别或职业，或电影类别。'
- en: '**Text features**: These are features derived from the text content in the
    data, for example, movie titles, descriptions, or reviews.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本特征**：这些是从数据中的文本内容派生出来的特征，例如电影标题、描述或评论。'
- en: '**Other features**: Most other types of features are ultimately represented
    numerically. For example, images, video, and audio can be represented as sets
    of numerical data. Geographical locations can be represented as latitude and longitude
    or geohash data.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**其他特征**：大多数其他类型的特征最终都以数值形式表示。例如，图像、视频和音频可以表示为一组数值数据。地理位置可以表示为纬度和经度或地理哈希数据。'
- en: Here we will cover numerical, categorical, and text features.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将涵盖数值、分类和文本特征。
- en: Numerical features
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数值特征
- en: What is the difference between any old number and a numerical feature? Well,
    in reality, any numerical data can be used as an input variable. However, in a
    machine learning model, you learn about a vector of weights for each feature.
    The weights play a role in mapping feature values to an outcome or target variable
    (in the case of supervised learning models).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 任何普通数字和数值特征之间有什么区别？实际上，任何数值数据都可以用作输入变量。然而，在机器学习模型中，您会了解每个特征的权重向量。这些权重在将特征值映射到结果或目标变量（在监督学习模型的情况下）中起着作用。
- en: Thus, we want to use features that make sense, that is, where the model can
    learn the relationship between feature values and the target variable. For example,
    age might be a reasonable feature. Perhaps there is a direct relationship between
    increasing age and a certain outcome. Similarly, height is a good example of a
    numerical feature that can be used directly.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望使用有意义的特征，即模型可以学习特征值和目标变量之间的关系的特征。例如，年龄可能是一个合理的特征。也许增长年龄和某个结果之间存在直接关系。同样，身高是一个可以直接使用的数值特征的很好的例子。
- en: We will often see that numerical features are less useful in their raw form
    but can be turned into representations that are more useful. The location is an
    example of such a case.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常会看到，数值特征在其原始形式下不太有用，但可以转化为更有用的表示。位置就是这样一个例子。
- en: Using raw locations (say, latitude and longitude) might not be that useful unless
    our data is very dense indeed since our model might not be able to learn about
    a useful relationship between the raw location and an outcome. However, a relationship
    might exist between some aggregated or binned representation of the location (for
    example, a city or country) and the outcome.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用原始位置（比如纬度和经度）可能并不那么有用，除非我们的数据确实非常密集，因为我们的模型可能无法学习原始位置和结果之间的有用关系。然而，某种聚合或分箱表示的位置（例如城市或国家）与结果之间可能存在关系。
- en: Categorical features
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类特征
- en: Categorical features cannot be used as input in their raw form, as they are
    not numbers; instead, they are members of a set of possible values that the variable
    can take. In the example mentioned earlier, user occupation is a categorical variable
    that can take the value of student, programmer, and so on.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 分类特征不能以其原始形式用作输入，因为它们不是数字；相反，它们是变量可以取的一组可能值的成员。在前面提到的示例中，用户职业是一个可以取学生、程序员等值的分类变量。
- en: To transform categorical variables into a numerical representation, we can use
    a common approach known as **1-of-k** encoding. An approach such as 1-of-k encoding
    is required to represent.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将分类变量转换为数值表示，我们可以使用一种常见的方法，称为**1-of-k**编码。需要使用1-of-k编码这样的方法来表示。
- en: An approach such as 1-of-k encoding is required to represent nominal variables
    in a way that makes sense for machine learning tasks. Ordinal variables might
    be used in their raw form but are often encoded in the same way as nominal variables.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 需要使用1-of-k编码这样的方法来表示名义变量，使其对机器学习任务有意义。有序变量可能以其原始形式使用，但通常以与名义变量相同的方式进行编码。
- en: Assume that there are k possible values that the variable can take. If we assign
    each possible value an index from the set of 1 to k, then we can represent a given
    state of the variable using a binary vector of length k; here, all entries are
    zero, except the entry at the index that corresponds to the given state of the
    variable. This entry is set to one.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 假设变量可以取k个可能的值。如果我们为每个可能的值分配一个从1到k的索引，那么我们可以使用长度为k的二进制向量来表示变量的给定状态；在这里，除了对应于给定变量状态的索引处的条目设置为1之外，所有条目都为零。
- en: e.g. student is [0], programmer is [1]
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，学生是[0]，程序员是[1]
- en: 'so values of:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，值为：
- en: student become [ 1, 0]
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 学生变成[1,0]
- en: programmer become [0,1]
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员变成[0,1]
- en: 'Extract the binary encoding for two occupations, followed by binary feature
    vector creation with a length of 21:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 提取两个职业的二进制编码，然后创建长度为21的二进制特征向量：
- en: '[PRE44]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output of the previous `println` statements is listed here:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 前面`println`语句的输出如下：
- en: '[PRE45]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output of the previous commands, which show the binary feature vector and
    length of binary vector is listed here:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 前面命令的输出，显示了二进制特征向量和二进制向量的长度：
- en: '[PRE46]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处查找代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala)
- en: Derived features
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 派生特征
- en: As we mentioned earlier, it is often useful to compute a derived feature from
    one or more available variables. We hope that the derived feature can add more
    information than only using the variable in its raw form available variables.
    We hope that the derived feature can add more information than only using the
    variable in its raw form.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，通常有必要从一个或多个可用变量计算派生特征。我们希望派生特征可以提供比仅使用原始形式的变量更多的信息。
- en: For instance, we can compute the average rating given by each user to all the
    movies they rated. This would be a feature that could provide a *user-specific*
    intercept in our model (in fact, this is a commonly used approach in recommendation
    models). We have taken the raw rating data and created a new feature that can
    allow us to learn a better model.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以计算每个用户对其评分的所有电影的平均评分。这将是一个特征，可以为我们的模型提供*用户特定*的截距（事实上，这是推荐模型中常用的方法）。我们已经从原始评分数据中创建了一个新特征，可以帮助我们学习更好的模型。
- en: Examples of features derived from raw data include computing average values,
    median values, variances, sums, differences, maximums or minimums, and counts.
    We have already seen a case of this when we created a new `movie age` feature
    from the year of release of the movie and the current year. Often, the idea behind
    using these transformations is to summarize the numerical data in some way that
    might make it easier for a model to learn features, for example, by binning features.
    Common examples of these include variables such as age, geolocation, and time.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始数据派生特征的示例包括计算平均值、中位数、方差、总和、差异、最大值或最小值和计数。我们已经在创建`电影年龄`特征时看到了这种情况，该特征是从电影的发行年份和当前年份派生出来的。通常，使用这些转换的背后思想是以某种方式总结数值数据，这可能会使模型更容易学习特征，例如通过分箱特征。这些常见的示例包括年龄、地理位置和时间等变量。
- en: Transforming timestamps into categorical features
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将时间戳转换为分类特征
- en: Extract time of Day
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提取一天中的时间
- en: To illustrate how to derive categorical features from numerical data, we will
    use the times of the ratings given by users to movies. Extract the date and time
    from the timestamp and, in turn, extract the `hour` of the day.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何从数值数据中派生分类特征，我们将使用用户对电影的评分时间。从时间戳中提取日期和时间，然后提取一天中的`小时`。
- en: 'We will need a function to extract a `datetime` representation of the rating
    timestamp (in seconds); we will create this function now: extract the date and
    time from the timestamp and, in turn, extract the `hour` of the day. This will
    result in an RDD of the hour of the day for each rating.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个函数来提取评分时间戳（以秒为单位）的`datetime`表示；我们现在将创建这个函数：从时间戳中提取日期和时间，然后提取一天中的`小时`。这将导致每个评分的一天中的小时的RDD。
- en: Scala
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Scala
- en: 'First, we define a function which extracts `currentHour` from a date string:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个函数，该函数从日期字符串中提取`currentHour`：
- en: '[PRE47]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output of the preceding code is listed here:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '[PRE48]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Relevant code listing:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 相关代码清单：
- en: '[PRE49]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下链接找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala)
- en: Extract time of day
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取一天中的时间
- en: We have transformed the raw time data into a categorical feature that represents
    the hour of the day in which the rating was given.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将原始时间数据转换为表示给出评分的一天中的小时的分类特征。
- en: Now, say that we decide this is too coarse a representation. Perhaps we want
    to further refine the transformation. We can assign each hour-of-the-day value
    into a defined bucket that represents a time of day.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们决定这是一个太粗糙的表示。也许我们想进一步完善转换。我们可以将每个一天中的小时值分配到表示一天中的时间的定义桶中。
- en: For example, we can say that morning is from 7 a.m. to 11 a.m., while lunch
    is from 11 a.m. to 1 a.m., and so on. Using these buckets, we can create a function
    to assign a time of day, given the hour of the day as input.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以说早晨是从上午7点到上午11点，午餐是从上午11点到下午1点，依此类推。使用这些时间段，我们可以创建一个函数，根据输入的小时来分配一天中的时间。
- en: Scala
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Scala
- en: 'In Scala, we define a function which takes inputer as absolute HR in 24 HR
    format and returns time of day: `morning`, `lunch`, `afternoon`, `evening`, or
    `night`:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中，我们定义一个函数，该函数以24小时制的绝对时间作为输入，并返回一天中的时间：`早晨`、`午餐`、`下午`、`晚上`或`夜晚`。
- en: '[PRE50]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We register this function as a UDF and call it on temp view timestamps within
    a select call.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此函数注册为UDF，并在select调用中对temp视图时间戳进行调用。
- en: '[PRE51]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下链接找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala)
- en: We have now transformed the timestamp variable (which can take on thousands
    of values and is probably not useful to a model in its raw form) into hours (taking
    on 24 values) and then into a time of day (taking on five possible values). Now
    that we have a categorical feature, we can use the same 1-of-k encoding method
    outlined earlier to generate a binary feature vector.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经将时间戳变量（可以取数千个值，可能以原始形式对模型没有用）转换为小时（取24个值），然后转换为一天中的时间（取五个可能的值）。现在我们有了一个分类特征，我们可以使用之前概述的相同的1-of-k编码方法来生成一个二进制特征向量。
- en: Text features
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本特征
- en: In some ways, text features are a form of categorical and derived features.
    Let's take the example of the description for a movie (which we do not have in
    our dataset). Here, the raw text could not be used directly, even as a categorical
    feature, since there are virtually unlimited possible combinations of words that
    could occur if each piece of text was a possible value `true`, since there are
    virtually unlimited possible combinations of words that could occur if each piece
    of text was a possible value. Our model would almost never see two occurrences
    of the same feature and would not be able to learn effectively. Therefore, we
    would like to turn raw text into a form that is more amenable to machine learning,
    since there are virtually unlimited possible combinations of words that could
    occur if each piece of text was a possible value.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，文本特征是一种分类和派生特征的形式。让我们以电影描述为例（我们的数据集中没有）。在这里，原始文本不能直接使用，即使作为分类特征，因为每个文本可能的值几乎是无限的。我们的模型几乎不会看到两次相同特征的出现，也无法有效学习。因此，我们希望将原始文本转换为更适合机器学习的形式，因为每个文本可能的值几乎是无限的。
- en: There are numerous ways of dealing with text, and the field of natural language
    processing is dedicated to processing, representing, and modeling textual content.
    A full treatment is beyond the scope of this book, but we will introduce a simple
    and standard approach for text-feature extraction; this approach is known as the
    bag-of-words representation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 处理文本的方法有很多种，自然语言处理领域致力于处理、表示和建模文本内容。全面的处理超出了本书的范围，但我们将介绍一种简单和标准的文本特征提取方法；这种方法被称为词袋模型表示。
- en: 'The bag-of-words approach treats a piece of text content as a set of the words,
    and possibly numbers, in the text (these are often referred to as terms). The
    process of the bag-of-words approach is as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型方法将文本内容视为文本中的单词和可能的数字的集合（这些通常被称为术语）。词袋模型的过程如下：
- en: '**Tokenization**: First, some form of tokenization is applied to the text to
    split it into a set of tokens (generally words, numbers, and so on). An example
    of this is simple whitespace tokenization, which splits the text on each space
    and might remove punctuation and other characters that are not alphabetical or
    numerical.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**：首先，对文本应用某种形式的分词，将其分割成一组标记（通常是单词、数字等）。一个例子是简单的空格分词，它在每个空格上分割文本，并可能删除不是字母或数字的标点符号和其他字符。'
- en: '**Stop word removal**: Next, it is usual to remove very common words such as
    "the", "and", and "but" (these are known as stop words).'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停用词去除**：接下来，通常会去除非常常见的词，比如"the"、"and"和"but"（这些被称为停用词）。'
- en: '**Stemming**: The next step can include stemming, which refers to taking a
    term and reducing it to its base form or stem. A common example is plural terms
    becoming singular (for example, dogs becomes dog, and so on). There are many approaches
    to stemming, and text-processing libraries often contain various stemming algorithms,
    for example, OpenNLP, NLTK and so on. Covering stemming in detail is outside the
    scope of this book, but feel free to explore these libraries on your own.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取**：下一步可以包括词干提取，指的是将一个词语减少到其基本形式或词干。一个常见的例子是复数变成单数（例如，dogs变成dog，等等）。有许多词干提取的方法，文本处理库通常包含各种词干提取算法，例如OpenNLP、NLTK等。详细介绍词干提取超出了本书的范围，但欢迎自行探索这些库。'
- en: '**Vectorization**: The final step is turning the processed terms into a vector
    representation. The simplest form is, perhaps, a binary vector representation,
    where we assign a value of one if a term exists in the text and zero if it does
    not. This is essentially identical to the categorical 1-of-k encoding we encountered
    earlier. Like 1-of-k encoding, this requires a dictionary of terms mapping a given
    term to an index number. As you might gather, there are potentially millions of
    individual possible terms (even after stop word removal and stemming). Hence,
    it becomes critical to use a sparse vector representation compute `time.computetime.computetime.compute`
    time.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量化**：最后一步是将处理后的术语转换为向量表示。最简单的形式可能是二进制向量表示，如果一个术语存在于文本中则赋值为1，如果不存在则赋值为0。这与我们之前遇到的分类1-of-k编码基本相同。与1-of-k编码一样，这需要一个术语字典，将给定的术语映射到索引号。你可能会发现，即使在停用词去除和词干提取之后，可能仍然有数百万个可能的术语。因此，使用稀疏向量表示计算`time.computetime.computetime.compute`时间变得至关重要。'
- en: In [Chapter 10](789e8b8c-28e8-444d-92a6-aace3a4dfdd6.xhtml), *Advanced Text
    Processing with Spark*, we will cover more complex text processing and feature
    extraction, including methods to weight terms; these methods go beyond the basic
    binary encoding we saw earlier.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](789e8b8c-28e8-444d-92a6-aace3a4dfdd6.xhtml)中，*使用Spark进行高级文本处理*，我们将涵盖更复杂的文本处理和特征提取，包括加权术语的方法；这些方法超出了我们之前看到的基本二进制编码。
- en: Simple text feature extraction
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的文本特征提取
- en: To show an example of extracting textual features in the binary vector representation,
    we can use the movie titles that we have available.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示用二进制向量表示提取文本特征的示例，我们可以使用现有的电影标题。
- en: First, we will create a function to strip away the year of release for each
    movie, if the year is present, leaving only the title of the movie.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个函数，用于剥离每部电影的发行年份，如果有年份存在的话，只留下电影的标题。
- en: We will use a regular expression, to search for the year between parentheses
    in the movie titles. If we find a match with this regular expression, we will
    extract only the title up to the index of the first match (that is, the index
    in the title string of the opening parenthesis).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用正则表达式，在电影标题中搜索括号之间的年份。如果我们找到与这个正则表达式匹配的内容，我们将仅提取标题直到第一个匹配的索引（即标题字符串中开括号的索引）。
- en: Scala
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Scala
- en: First, we create a function which takes the input string and filters the output
    with a regular expression.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个函数，该函数接受输入字符串并使用正则表达式过滤输出。
- en: '[PRE52]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Extract DataFrame with only the raw title and create a temp view `titles`. Register
    the function created above with Spark and then run it on the DataFrame within
    the `select` statement.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 提取只有原始标题的DataFrame并创建一个名为`titles`的临时视图。使用Spark注册上面创建的函数，然后在`select`语句中对DataFrame运行它。
- en: '[PRE53]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output of the preceding code is listed here:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '[PRE54]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then, to apply our function to the raw titles and apply a tokenization scheme
    to the extracted titles to convert them to terms, we will use the simple whitespace
    tokenization we covered earlier:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将我们的函数应用于原始标题，并对提取的标题应用标记化方案，将它们转换为术语，我们将使用我们之前介绍的简单的空格标记化：
- en: Next, we split the `titles` into single words
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将`titles`拆分成单词
- en: '[PRE55]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Applying this simple tokenization gives the following result:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这种简单的标记化得到以下结果：
- en: '[PRE56]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Then, we convert the rdd of words and find the total number of words--we get
    the collection of total words and index of `"Dead"` and `"Rooms"`.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们转换单词的rdd并找到单词的总数-我们得到总单词的集合以及`"Dead"`和`"Rooms"`的索引。
- en: '[PRE57]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This will result in the following output:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE58]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We can also achieve the same result more efficiently using Spark''s `zipWithIndex`
    function. This function takes an RDD of values and merges them together with an
    index to create a new RDD of key-value pairs, where the key will be the term and
    the value will be the index in the term dictionary. We will use `collectAsMap`
    to collect the key-value RDD to the driver as a Python `dict` method:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用Spark的`zipWithIndex`函数更有效地实现相同的结果。这个函数接受一个值的RDD，并将它们与索引合并在一起，创建一个新的键值对RDD，其中键将是术语，值将是术语字典中的索引。我们将使用`collectAsMap`将键值对RDD收集到驱动程序作为Python
    `dict`方法：
- en: Scala
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Scala
- en: '[PRE59]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output is as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE60]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Sparse Vectors from Titles
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标题的稀疏向量
- en: 'The final step is to create a function that converts a set of terms into a
    sparse vector representation. To do this, we will create an empty sparse matrix
    with one row and a number of columns equal to the total number of terms in our
    dictionary. We will then step through each term in the input list of terms and
    check whether this term is in our term dictionary. If it is, we assign a value
    of `1` to the vector at the index that corresponds to the term in our dictionary
    mapping:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是创建一个将一组术语转换为稀疏向量表示的函数。为此，我们将创建一个空的稀疏矩阵，其中有一行，列数等于字典中术语的总数。然后，我们将遍历输入术语列表中的每个术语，并检查该术语是否在我们的术语字典中。如果是，我们将在对应于字典映射中的术语的索引处为向量分配一个值`1`：
- en: 'extracted terms:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的术语：
- en: Scala
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Scala
- en: '[PRE61]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We can then inspect the first few records of our new RDD of sparse vectors:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以检查我们新的稀疏向量RDD的前几条记录：
- en: '[PRE62]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/exploredataset/explore_movies.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/exploredataset/explore_movies.scala)
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下网址找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/exploredataset/explore_movies.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/exploredataset/explore_movies.scala)
- en: We can see that each movie title has now been transformed into a sparse vector.
    We can see that the titles where we extracted two terms have two non-zero entries
    in the vector, titles where we extracted only one term have one non-zero entry,
    and so on.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，每个电影标题现在都被转换为稀疏向量。我们可以看到，我们提取了两个术语的标题在向量中有两个非零条目，我们只提取了一个术语的标题有一个非零条目，依此类推。
- en: Note the use of Spark's `broadcast` method in the preceding example code to
    create a broadcast variable that contains the term dictionary. In real-world applications,
    such term dictionaries can be extremely large, so using a broadcast variable is
    not advisable.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意在前面示例代码中使用了Spark的`broadcast`方法来创建一个包含术语字典的广播变量。在实际应用中，这样的术语字典可能非常庞大，因此不建议使用广播变量。
- en: Normalizing features
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化特征
- en: 'Once the features have been extracted into the form of a vector, a common preprocessing
    step is to normalize the numerical data. The idea behind this is to transform
    each numerical feature in a way that scales it to a standard size. We can perform
    different kinds of normalization, which are as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦特征被提取为向量的形式，常见的预处理步骤是对数值数据进行归一化。其背后的想法是以一种方式转换每个数值特征，使其缩放到标准大小。我们可以执行不同类型的归一化，如下所示：
- en: 'Normalize a feature: This is usually a transformation applied to an individual
    feature across the dataset, for example, subtracting the mean (centering the feature)
    or applying the standard normal transformation (such that the feature has a mean
    of zero and a standard deviation of 1).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化特征：这通常是应用于数据集中的单个特征的转换，例如，减去均值（使特征居中）或应用标准正态转换（使特征的平均值为零，标准差为1）。
- en: 'Normalize a feature vector: This is usually a transformation applied to all
    features in a given row of the dataset such that the resulting feature vector
    has a normalized length. That is, we will ensure that each feature in the vector
    is scaled such that the vector has a norm of 1 (typically, on an L1 or L2 norm).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化特征向量：这通常是应用于数据集中给定行的所有特征的转换，使得结果特征向量具有归一化长度。也就是说，我们将确保向量中的每个特征都被缩放，使得向量的范数为1（通常是在L1或L2范数上）。
- en: 'We will use the second case as an example. We can use the `norm` function of
    `numpy` to achieve the vector normalization by first computing the L2 norm of
    a random vector and then dividing each element in the vector by this norm to create
    our normalized vector:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以第二种情况为例。我们可以使用`numpy`的`norm`函数通过首先计算随机向量的L2范数，然后将向量中的每个元素除以这个范数来实现向量归一化：
- en: '[PRE63]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output of the preceding code is listed here:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '[PRE64]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Using ML for feature normalization
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ML进行特征归一化
- en: Spark provides some built-in functions for feature scaling and standardization
    in its machine learning library. These include `StandardScaler`, which applies
    the standard normal transformation, and `Normalizer,` which applies the same feature
    vector normalization we showed you in our preceding example code.lization we showed
    you in our preceding example code.lization, we showed you in our preceding example
    code.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在其机器学习库中提供了一些内置函数用于特征缩放和标准化。这些包括`StandardScaler`，它应用标准正态转换，以及`Normalizer`，它应用我们在前面示例代码中展示的相同特征向量归一化。
- en: 'We will explore the use of these methods in the upcoming chapters, but for
    now, let''s simply compare the results of using MLlib''s `Normalizer` to our own
    results:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中探讨这些方法的使用，但现在，让我们简单比较一下使用MLlib的`Normalizer`和我们自己的结果：
- en: '[PRE65]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: After importing the required class, we will instantiate `Normalizer` (by default,
    it will use the L2 norm as we did earlier). Note that, as in most situations in
    Spark, we need to provide `Normalizer` with an RDD as input (it contains `numpy`
    arrays or MLlib vectors); hence, we will create a single-element RDD from our
    vector `x` for illustrative purposes.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入所需的类之后，我们将实例化`Normalizer`（默认情况下，它将使用L2范数，就像我们之前做的那样）。请注意，在Spark的大多数情况下，我们需要为`Normalizer`提供RDD作为输入（它包含`numpy`数组或MLlib向量）；因此，我们将从我们的向量`x`创建一个单元素RDD，以便说明目的。
- en: 'We will then use the `transform` function of `Normalizer` on our RDD. Since
    the RDD only has one vector in it, we will return our vector to the driver by
    calling `first` and finally by calling the `toArray` function to convert the vector
    back into a `numpy` array:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将在RDD上使用`Normalizer`的`transform`函数。由于RDD中只有一个向量，我们将通过调用`first`将我们的向量返回给驱动程序，最后通过调用`toArray`函数将向量转换回`numpy`数组：
- en: '[PRE66]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Finally, we can print out the same details as we did previously, comparing
    the results:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以打印出与之前相同的细节，比较结果：
- en: '[PRE67]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'You will end up with exactly the same normalized vector as we did with our
    own code. However, using MLlib''s built-in methods is certainly more convenient
    and efficient than writing our own functions! Equivalent Scala implementation
    is as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到与我们自己的代码完全相同的归一化向量。但是，使用MLlib的内置方法肯定比编写我们自己的函数更方便和高效！等效的Scala实现如下：
- en: '[PRE68]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output of the preceding code is listed following:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '[PRE69]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Using packages for feature extraction
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特征提取包
- en: While from these common tasks each and every time. Certainly, we can create
    our own reusable code libraries for this purpose; however, fortunately, we can
    rely on the existing tools and packages. Since Spark supports Scala, Java, and
    Python bindings, we can use packages available in these languages that provide
    sophisticated tools to process and extract features and represent them as vectors.
    A few examples of packages for feature extraction include `scikit-learn`, `gensim`,
    `scikit-image`, `matplotlib`, and `NLTK` in Python, `OpenNLP` in Java, and `Breeze`
    and `Chalk` in Scala. In fact, `Breeze` has been part of Spark MLlib since version
    1.0, and we will see how to use some Breeze functionality for linear algebra in
    the later chapters.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然每次都从这些常见任务中获得。当然，我们可以为此目的创建自己的可重用代码库；但是，幸运的是，我们可以依赖现有的工具和包。由于Spark支持Scala、Java和Python绑定，我们可以使用这些语言中提供的包，这些包提供了处理和提取特征并将其表示为向量的复杂工具。一些用于特征提取的包的示例包括Python中的`scikit-learn`、`gensim`、`scikit-image`、`matplotlib`和`NLTK`，Java中的`OpenNLP`，以及Scala中的`Breeze`和`Chalk`。实际上，自从1.0版本以来，`Breeze`一直是Spark
    MLlib的一部分，我们将在后面的章节中看到如何使用一些Breeze功能进行线性代数。
- en: TFID
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TFID
- en: '**tf-idf** is short term for **term frequency-inverse document frequency**.
    It is a numerical statistic that is intended to reflect how important a word is
    to a document in a collection or corpus. It is used as a weighting factor in information
    retrieval and text mining. The tf-idf value increases in proportion to the number
    of times a word appears in a document. It is offset by the frequency of the word
    in the corpus, that helps to adjust for some words which appear more frequently
    in general.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '**tf-idf**是**术语频率-逆文档频率**的简称。它是一个数值统计量，旨在反映一个词对于集合或语料库中的文档的重要性。它在信息检索和文本挖掘中用作加权因子。tf-idf值与单词在文档中出现的次数成比例增加。它受到语料库中单词频率的影响，有助于调整一些在一般情况下更频繁出现的单词。'
- en: tf-idf is used by search engines or text processing engines as a tool in scoring
    and ranking a document's relevance for a user query.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: tf-idf被搜索引擎或文本处理引擎用作评分和排名用户查询的文档相关性的工具。
- en: The simplest ranking functions are computed by summing the tf-idf for each query
    term; more sophisticated ranking functions are variants of this simple model.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的排名函数是通过对每个查询术语的tf-idf求和来计算的；更复杂的排名函数是这个简单模型的变体。
- en: 'In the term frequency `tf(t,d)` calculation, one choice is to use the raw frequency
    of a term in a document: the number of times term t occurs in document `d`. If
    raw frequency of `t` is `f(t,d)`, then the simple `tf` scheme is `tf(t,d) = ft,d`.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在术语频率`tf(t,d)`计算中，一种选择是使用文档中术语的原始频率：术语t在文档`d`中出现的次数。如果`t`的原始频率是`f(t,d)`，那么简单的`tf`方案是`tf(t,d)
    = ft,d`。
- en: Spark's implementation of `tf(t.d)` uses the hashing. A raw word is mapped into
    an index (term) by applying a hash function.The term frequencies are calculated
    using the mapped indices.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`tf(t.d)`实现使用了哈希。通过应用哈希函数，将原始单词映射到索引（术语）。使用映射的索引计算术语频率。
- en: 'References:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 参考：
- en: '[https://spark.apache.org/docs/1.6.0/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF](https://spark.apache.org/docs/1.6.0/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF)'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/1.6.0/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF](https://spark.apache.org/docs/1.6.0/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF)'
- en: '[https://en.wikipedia.org/wiki/Tf%E2%80%93idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Tf%E2%80%93idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)'
- en: '[https://spark.apache.org/docs/1.6.0/mllib-feature-extraction.html](https://spark.apache.org/docs/1.6.0/mllib-feature-extraction.html)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/1.6.0/mllib-feature-extraction.html](https://spark.apache.org/docs/1.6.0/mllib-feature-extraction.html)'
- en: IDF
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IDF
- en: 'The **inverse document frequency**(**IDF**)represents how much information
    the word provides: is the term common or rare across the corpus. It is the log
    scaled inverse fraction of the documents containing the word, calculated by division
    of the total number of documents by the number of documents containing the term**TF-IDF**'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '**逆文档频率**（**IDF**）表示单词提供的信息量：术语在语料库中是常见的还是罕见的。它是包含该单词的文档的总数与包含该术语的文档数量的倒数的对数比例**TF-IDF**'
- en: TF-IDF is calculated by multiplying TF and IDF.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF是通过将TF和IDF相乘来计算的。
- en: '![](img/image_04_009.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_009.png)'
- en: 'The following example calculates TFIDF for each term in the Apache Spark `README.md`
    file:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例计算Apache Spark `README.md`文件中每个术语的TFIDF：
- en: '[PRE70]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala2.0.0/src/main/scala/org/sparksamples/featureext/TfIdfSample.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala2.0.0/src/main/scala/org/sparksamples/featureext/TfIdfSample.scala)
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下位置找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala2.0.0/src/main/scala/org/sparksamples/featureext/TfIdfSample.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala2.0.0/src/main/scala/org/sparksamples/featureext/TfIdfSample.scala)
- en: Word2Vector
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vector
- en: The Word2Vec tools take text data as input and produce the word vectors as output.
    This tool constructs a vocabulary from the training text data and learns vector
    representation of words. The resulting word vector file can be used as features
    for many natural language processing and machine learning applications.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec工具以文本数据作为输入，并将单词向量作为输出。该工具从训练文本数据中构建词汇表并学习单词的向量表示。生成的单词向量文件可以用作许多自然语言处理和机器学习应用的特征。
- en: The easiest way to investigate the learned representations is to find the closest
    words for a user-specified word.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 调查学习到的表示的最简单方法是找到用户指定单词的最接近单词。
- en: Word2Vec implementation in Apache Spark computes distributed vector representation
    of words. Apache Spark's implementation is a more scalable approach as compared
    to single machine Word2Vec implementations provided by Google).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark中的Word2Vec实现计算单词的分布式向量表示。与Google提供的单机Word2Vec实现相比，Apache Spark的实现是一种更可扩展的方法。
- en: ([https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/))
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ([https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/))
- en: 'Word2Vec can be implemented using two learning algorithms: continuous bag-of-words
    and continuous skip-gram.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec可以使用两种学习算法实现：连续词袋和连续跳字。
- en: Skip-gram model
  id: totrans-355
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跳字模型
- en: 'The training objective of the skip-gram model is to find word representations
    useful for predicting the surrounding words in a document or a sentence. Given
    a sequence of words *w1,* *w2, w3, . . , wT*, skip-gram model maximizes the average
    log probability shown as following:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 跳字模型的训练目标是找到对预测文档或句子中周围单词有用的单词表示。给定一系列单词*w1,* *w2, w3, . . , wT*，跳字模型最大化以下平均对数概率：
- en: '![](img/image_04_010.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_010.png)'
- en: '*c* is the size of the training context (which can be a function of the center
    word *wt*). Larger *c* results in more training examples leading to a higher accuracy,
    at the expense of the training time. The basic skip-gram formulation defines *p(wt+j
    |wt)* using the `softmax` function:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '*c*是训练上下文的大小（可以是中心词*wt*的函数）。较大的*c*会导致更多的训练示例，从而提高准确性，但训练时间会增加。基本的跳字式公式使用`softmax`函数定义了*p(wt+j
    |wt)*：'
- en: '![](img/image_04_011.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_04_011.png)'
- en: '*v[w]*, *v'' *and, *w* are the *input* and *output* vector representations
    of *w*, and *W* is the number of words in the vocabulary'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '*v[w]*，*v'' *和，*w*是*输入*和*输出*单词的向量表示，*W*是词汇表中的单词数'
- en: In Spark Hierarchical soft-max approach is used to predicting word *wi* given
    word *wj*.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，使用分层软最大值方法来预测单词*wi*给定单词*wj*。
- en: The following example shows how to create word vectors using Apache Spark.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了如何使用Apache Spark创建单词向量。
- en: '[PRE71]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/featureext/ConvertWordsToVectors.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/featureext/ConvertWordsToVectors.scala)
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下位置找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/featureext/ConvertWordsToVectors.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/featureext/ConvertWordsToVectors.scala)
- en: 'The output of the preceding code:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出：
- en: '[PRE72]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Standard scalar
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准缩放器
- en: Standard scalar standardizes features of the data set by scaling to unit variance
    and removing the mean (optionally) using column summary statistics on the samples
    in the training set.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 标准缩放器通过对训练集中的样本使用列摘要统计数据，将数据集的特征标准化为单位方差并去除均值（可选）。
- en: This process is a very common pre-processing step.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程是一个非常常见的预处理步骤。
- en: Standardization improves the convergence rate during the optimization process.
    It also prevents features with large variances from exerting an overly large influence
    during model training.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化可以提高优化过程中的收敛速度。它还可以防止具有较大方差的特征在模型训练过程中产生过大的影响。
- en: '`StandardScaler` class has the following parameters in the constructor:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '`StandardScaler`类在构造函数中具有以下参数：'
- en: 'new StandardScaler(withMean: Boolean, withStd: Boolean)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '新的StandardScaler（withMean: Boolean, withStd: Boolean）'
- en: '`withMean`: `False` by default. Centers the data with mean before scaling.
    It will build a dense output, does not work on sparse input and will raise an
    exception.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`withMean`：默认为`False`。在缩放之前使用均值对数据进行中心化。它将构建一个密集输出，在稀疏输入上不起作用，并将引发异常。'
- en: '`withStd`: `True` by default. It Scales the data to unit standard deviation.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`withStd`：默认为`True`。将数据缩放到单位标准差。'
- en: Annotations
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: Available @Since("1.1.0" )
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 可用@Since("1.1.0" )
- en: '[PRE73]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/featureext/StandardScalarSample.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/featureext/StandardScalarSample.scala)
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下链接找到代码清单：[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/featureext/StandardScalarSample.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/featureext/StandardScalarSample.scala)
- en: Summary
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw how to find common, publicly available datasets that
    can be used to test various machine learning models. You learned how to load,
    process, and clean data, as well as how to apply common techniques to transform
    raw data into feature vectors that can be used as training examples for our models.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何找到常见的、公开可用的数据集，这些数据集可以用来测试各种机器学习模型。您学会了如何加载、处理和清理数据，以及如何应用常见技术将原始数据转换为特征向量，这些特征向量可以作为我们模型的训练样本。
- en: In the next chapter, you will learn the basics of recommender systems and explore
    how to create a recommendation model, use the model to make predictions, and evaluate
    the model.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习推荐系统的基础知识，探索如何创建推荐模型，使用模型进行预测，并评估模型。
