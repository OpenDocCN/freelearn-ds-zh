- en: Tackle Big Data – Spark Comes to the Party
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决大数据问题- Spark加入派对
- en: An approximate answer to the right problem is worth a good deal more than an
    exact answer to an approximate problem.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 对正确问题的近似答案比对近似问题的精确答案更有价值。
- en: '- John Tukey'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 约翰·图基'
- en: In this chapter, you learn about data analysis and big data; we see the challenges
    that big data provides and how they are dealt with. You will learn about distributed
    computing and the approach suggested by functional programming; we introduce Google's
    MapReduce, Apache Hadoop, and finally Apache Spark and see how they embrace this
    approach and these techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解数据分析和大数据；我们将看到大数据提供的挑战以及如何应对。您将了解分布式计算和函数式编程建议的方法；我们介绍Google的MapReduce，Apache
    Hadoop，最后是Apache Spark，并看到它们如何采用这种方法和这些技术。
- en: 'In a nutshell, the following topics will be covered throughout this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章将涵盖以下主题：
- en: Introduction to data analytics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析简介
- en: Introduction to big data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据简介
- en: Distributed computing using Apache Hadoop
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Hadoop进行分布式计算
- en: Here comes Apache Spark
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark来了
- en: Introduction to data analytics
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析简介
- en: '**Data analytics** is the process of applying qualitative and quantitative
    techniques when examining data with the goal of providing valuable insights. Using
    various techniques and concepts, data analytics can provide the means to explore
    the data **Exploratory Data Analysis** (**EDA**) as well as draw conclusions about
    the data **Confirmatory Data Analysis** (**CDA**). EDA and CDA are fundamental
    concepts of data analytics, and it is important to understand the difference between
    the two.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据分析**是在检查数据时应用定性和定量技术的过程，目的是提供有价值的见解。使用各种技术和概念，数据分析可以提供探索数据**探索性数据分析**（**EDA**）以及对数据**验证性数据分析**（**CDA**）的结论的手段。EDA和CDA是数据分析的基本概念，重要的是要理解两者之间的区别。'
- en: EDA involves methodologies, tools, and techniques used to explore data with
    the intention of finding patterns in the data and relationships between various
    elements of the data. CDA involves methodologies, tools, and techniques used to
    provide an insight or conclusion on a specific question based on a hypothesis
    and statistical techniques or simple observation of the data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: EDA涉及用于探索数据的方法、工具和技术，目的是在数据中找到模式和数据各个元素之间的关系。CDA涉及用于根据假设和统计技术或对数据的简单观察提供关于特定问题的见解或结论的方法、工具和技术。
- en: A quick example to understand these ideas is that of a grocery store, which
    has asked you to give them ways to improve sales and customer satisfaction as
    well as keep the cost of operations low.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速的例子来理解这些想法是杂货店，他们要求您提供改善销售和顾客满意度以及保持运营成本低的方法。
- en: 'The following is a grocery store with aisles of various products:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个有各种产品过道的杂货店：
- en: '![](img/00107.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00107.jpeg)'
- en: Assume that all sales at the grocery store are stored in some database and that
    you have access to the data for the last 3 months. Typically, businesses store
    data for years as you need sufficient data over a period of time to establish
    any hypothesis or observe any patterns. In this example, our goal is to perform
    better placement of products in various aisles based on how customers are buying
    the products. One hypothesis is that customers often buy products, that are both
    at eye level and also close together. For instance, if Milk is on one corner of
    the store and Yogurt is in other corner of the store, some customers might just
    choose either Milk or Yogurt and just leave the store, causing a loss of business.
    More adverse affects might result in customers choosing another store where products
    are better placed because if the feeling that *things are hard to find at this
    store*. Once that feeling sets in, it also percolates to friends and family eventually
    causing a bad social presence. This phenomenon is not uncommon in the real world
    causing some businesses to succeed while others fail while both seem to be very
    similar in products and prices.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设杂货店的所有销售数据都存储在某个数据库中，并且您可以访问过去3个月的数据。通常，企业会将数据存储多年，因为您需要足够长时间的数据来建立任何假设或观察任何模式。在这个例子中，我们的目标是根据顾客购买产品的方式更好地放置各种过道中的产品。一个假设是，顾客经常购买产品，这些产品既在视线范围内，又彼此靠近。例如，如果牛奶在商店的一个角落，酸奶在商店的另一个角落，一些顾客可能会选择牛奶或酸奶中的任何一种，然后离开商店，导致业务损失。更严重的影响可能导致顾客选择另一家产品摆放更好的商店，因为他们觉得*在这家商店很难找到东西*。一旦这种感觉产生，它也会传播给朋友和家人，最终导致不良的社交影响。这种现象在现实世界中并不罕见，导致一些企业成功，而其他企业失败，尽管它们在产品和价格上似乎非常相似。
- en: There are many ways to approach this problem starting from customer surveys
    to professional statisticians to machine learning scientists. Our approach will
    be to understand what we can from just the sales transactions alone.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以解决这个问题，从客户调查到专业统计学家再到机器学习科学家。我们的方法是仅从销售交易中了解我们可以得到什么。
- en: 'The following is an example of what the transactions might look like:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是交易可能看起来像的一个例子：
- en: '![](img/00111.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00111.jpeg)'
- en: 'The following are the steps you could follow as part of EDA:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是您可以作为EDA的一部分遵循的步骤：
- en: Calculate *Average number of products bought per day = Total of all products
    sold in a day / Total number of receipts for the* *day*.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算*每天购买的产品平均数量=一天内所有售出的产品总数/当天的收据总数*。
- en: Repeat the preceding step for last 1 week, month, and quarter.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复上一步骤，为过去1周、1个月和1个季度。
- en: Try to understand if there is a difference between weekends and weekdays and
    also time of the day (morning, noon, and evening)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试了解周末和工作日之间以及一天中的时间（早上、中午和晚上）是否有差异
- en: For each product, create a list of all other products to see which products
    are usually bought together (same receipt)
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每种产品，创建一个所有其他产品的列表，以查看通常一起购买哪些产品（同一张收据）
- en: Repeat the preceding step for 1 day, 1 week, month, and quarter.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复上一步骤，为1天、1周、1个月和1个季度。
- en: Try to determine which products should be placed closer together by the number
    of transactions (sorted in descending order).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试通过交易数量（按降序排列）确定哪些产品应该靠近放置。
- en: Once we have completed the preceding 6 steps, we can try to reach some conclusions
    for CDA.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了前面的6个步骤后，我们可以尝试得出一些CDA的结论。
- en: 'Let''s assume this is the output we get:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这是我们得到的输出：
- en: '| **Item** | **Day Of Week** | **Quantity** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **商品** | **星期几** | **数量** |'
- en: '| Milk | Sunday | 1244 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 牛奶 | 星期日 | 1244 |'
- en: '| Bread | Monday | 245 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 面包 | 星期一 | 245 |'
- en: '| Milk | Monday | 190 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 牛奶 | 星期一 | 190 |'
- en: 'In this case, we could state that **Milk** is bought more on *weekends* so
    its better to increase the quantity and variety of Milk products over weekends.
    Take a look at the following table:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以说**牛奶**在*周末*购买更多，因此最好在周末增加牛奶产品的数量和种类。看一下下表：
- en: '| **Item1** | **Item2** | **Quantity** |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **商品1** | **商品2** | **数量** |'
- en: '| Milk | Eggs | 360 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 牛奶 | 鸡蛋 | 360 |'
- en: '| Bread | Cheese | 335 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 面包 | 奶酪 | 335 |'
- en: '| Onions | Tomatoes | 310 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 洋葱 | 西红柿 | 310 |'
- en: In this case, we could state that **Milk** and **Eggs** are bought by *more*
    customers in one purchase followed by **Bread** and **Cheese.** So, we could recommend
    that the store realigns the aisles and shelves to move **Milk** and **Eggs** *closer*
    to each other.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以说**牛奶**和**鸡蛋**在一次购买中被*更多*顾客购买，接着是**面包**和**奶酪**。因此，我们建议商店重新调整通道和货架，将**牛奶**和**鸡蛋***靠近*彼此。
- en: 'The two conclusions we have are:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得出的两个结论是：
- en: '**Milk** is bought more on *weekends,* so it''s better to increase the quantity
    and variety of Milk products over weekends.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**牛奶**在*周末*购买更多，因此最好在周末增加牛奶产品的数量和种类。'
- en: '**Milk** and **Eggs** are bought by *more* customers in one purchase followed
    by **Bread** and **Cheese.** So, we could recommend that the store realigns the
    aisles and shelves to move **Milk** and **Eggs** *closer* to each other.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**牛奶**和**鸡蛋**在一次购买中被*更多*顾客购买，接着是**面包**和**奶酪**。因此，我们建议商店重新调整通道和货架，将**牛奶**和**鸡蛋***靠近*彼此。'
- en: Conclusions are usually tracked over a period of time to evaluate the gains.
    If there is no significant impact on sales even after adopting the preceding two
    recommendations for 6 months, we simply invested in the recommendations which
    are not able to give you a good Return On Investment (ROI).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 结论通常会在一段时间内进行跟踪以评估收益。如果在采纳前述两项建议6个月后销售额没有显着影响，那么我们只是投资于无法给您良好投资回报率（ROI）的建议。
- en: Similarly, you can also perform some analysis with respect to the Profit margin
    and pricing optimizations. This is why you will typically see a single item costing
    more than the average of multiple numbers of the same item bought. Buy one Shampoo
    for $7 or two bottles of Shampoo for $12.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您也可以进行一些关于利润率和定价优化的分析。这就是为什么您通常会看到单个商品的成本高于购买多个相同商品的平均成本。购买一瓶洗发水7美元，或者两瓶洗发水12美元。
- en: Think about other aspects you can explore and recommend for the grocery store.
    For example, can you guess which products to position near checkout registers
    just based on fact that these have no affinity toward any particular product--chewing
    gum, magazines, and so on.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下您可以探索和为杂货店推荐的其他方面。例如，您能否根据这些产品对任何特定产品都没有亲和力这一事实，猜测哪些产品应该靠近结账柜台--口香糖、杂志等。
- en: Data analytics initiatives support a wide variety of business uses. For example,
    banks and credit card companies analyze withdrawal and spending patterns to prevent
    fraud and identity theft. Advertising companies analyze website traffic to identify
    prospects with a high likelihood of conversion to a customer. Department stores
    analyze customer data to figure out if better discounts will help boost sales.
    Cell Phone operators can figure out pricing strategies. Cable companies are constantly
    looking for customers who are likely to churn unless given some offer or promotional
    rate to retain their customer. Hospitals and pharmaceutical companies analyze
    data to come up with better products and detect problems with prescription drugs
    or measure the performance of prescription drugs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析举措支持各种各样的业务用途。例如，银行和信用卡公司分析取款和消费模式以防止欺诈和身份盗用。广告公司分析网站流量以确定有高转化可能性的潜在客户。百货商店分析客户数据，以确定更好的折扣是否有助于提高销售额。手机运营商可以制定定价策略。有线电视公司不断寻找可能会流失客户的客户，除非给予一些优惠或促销价格来留住他们的客户。医院和制药公司分析数据，以提出更好的产品，并检测处方药的问题或衡量处方药的表现。
- en: Inside the data analytics process
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在数据分析过程中
- en: Data analytics applications involve more than just analyzing data. Before any
    analytics can be planned, there is also a need to invest time and effort in collecting,
    integrating, and preparing data, checking the quality of the data and then developing,
    testing, and revising analytical methodologies. Once data is deemed ready, data
    analysts and scientists can explore and analyze the data using statistical methods
    such as SAS or machine learning models using Spark ML. The data itself is prepared
    by data engineering teams and the data quality team checks the data collected.
    Data governance becomes a factor too to ensure the proper collection and protection
    of the data. Another not commonly known role is that of a Data Steward who specializes
    in understanding data to the byte, exactly where it is coming from, all transformations
    that occur, and what the business really needs from the column or field of data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析应用不仅涉及数据分析。在计划任何分析之前，还需要投入时间和精力来收集、整合和准备数据，检查数据的质量，然后开发、测试和修订分析方法。一旦数据被认为准备就绪，数据分析师和科学家可以使用统计方法（如SAS）或使用Spark
    ML的机器学习模型来探索和分析数据。数据本身由数据工程团队准备，数据质量团队检查收集的数据。数据治理也成为一个因素，以确保数据的正确收集和保护。另一个不常为人所知的角色是数据监护人，他专门研究数据到字节的理解，确切地了解数据的来源，所有发生的转换，以及业务真正需要的数据列或字段。
- en: Various entities in the business might be dealing with addresses differently,
    **123 N Main St** as opposed to **123 North Main Street.** But, our analytics
    depends on getting the correct address field; otherwise both the addresses mentioned
    above will be considered different and our analytics will not have the same accuracy.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 企业中的各种实体可能以不同的方式处理地址，例如**123 N Main St**与**123 North Main Street**。但是，我们的分析取决于获取正确的地址字段；否则上述两个地址将被视为不同，我们的分析将无法达到相同的准确性。
- en: The analytics process starts with data collection based on what the analysts
    might need from the data warehouse, collecting all sorts of data in the organization
    (Sales, Marketing, Employee, Payroll, HR, and so on). Data stewards and the Governance
    team are important here to make sure the right data is collected and that any
    information deemed confidential or private is not accidentally exported out even
    if the end users are all employees.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 分析过程始于根据分析师可能需要的数据仓库中收集数据，收集组织中各种类型的数据（销售、营销、员工、工资单、人力资源等）。数据监护人和治理团队在这里非常重要，以确保收集正确的数据，并且任何被视为机密或私人的信息都不会被意外地导出，即使最终用户都是员工。
- en: Social Security Numbers or full addresses might not be a good idea to include
    in analytics as this can cause a lot of problems to the organization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 社会安全号码或完整地址可能不适合包含在分析中，因为这可能会给组织带来很多问题。
- en: Data quality processes must be established to make sure the data being collected
    and engineered is correct and will match the needs of the data scientists. At
    this stage, the main goal is to find and fix data quality problems that could
    affect the accuracy of analytical needs. Common techniques are profiling the data
    and cleansing the data to make sure that the information in a dataset is consistent,
    and also that any errors and duplicate records are removed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 必须建立数据质量流程，以确保收集和工程化的数据是正确的，并且能够满足数据科学家的需求。在这个阶段，主要目标是发现和修复可能影响分析需求准确性的数据质量问题。常见的技术包括对数据进行概要分析和清洗，以确保数据集中的信息是一致的，并且移除任何错误和重复记录。
- en: Data from disparate source systems may need to be combined, transformed, and
    normalized using various data engineering techniques, such as distributed computing
    or MapReduce programming, Stream processing, or SQL queries, and then stored on
    Amazon S3, Hadoop cluster, NAS, or SAN storage devices or a traditional data warehouse
    such as Teradata. Data preparation or engineering work involves techniques to
    manipulate and organize the data for the planned analytics use.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 来自不同来源系统的数据可能需要使用各种数据工程技术进行合并、转换和规范化，例如分布式计算或MapReduce编程、流处理或SQL查询，然后存储在Amazon
    S3、Hadoop集群、NAS或SAN存储设备上，或者传统的数据仓库，如Teradata。数据准备或工程工作涉及操纵和组织数据的技术，以满足计划中的分析需求。
- en: Once we have the data prepared and checked for quality, and it is available
    for the Data scientists or analysts to use, the actual analytical work starts.
    A Data scientist can now build an analytical model using predictive modeling tools
    and languages such as SAS, Python, R, Scala, Spark, H2O, and so on. The model
    is initially run against a partial dataset to test its accuracy in the *training
    phase*. Several iterations of the training phase are common and expected in any
    analytical project. After adjustments at the model level, or sometimes going all
    the way to the Data Steward to get or fix some data being collected or prepared,
    the model output tends to get better and better. Finally, a stable state is reached
    when further tuning does not change the outcome noticeably; at this time, we can
    think of the model as being ready for production usage.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据准备并经过质量检查，并且可供数据科学家或分析师使用，实际的分析工作就开始了。数据科学家现在可以使用预测建模工具和语言，如SAS、Python、R、Scala、Spark、H2O等来构建分析模型。模型最初针对部分数据集进行运行，以测试其在*训练阶段*的准确性。在任何分析项目中，训练阶段的多次迭代是常见且预期的。在模型层面进行调整后，或者有时需要到数据监护人那里获取或修复一些被收集或准备的数据，模型的输出往往会变得越来越好。最终，当进一步调整不会明显改变结果时，我们可以认为模型已经准备好投入生产使用。
- en: Now, the model can be run in production mode against the full dataset and generate
    outcomes or results based on how we trained the model. The choices made in building
    the analysis, either statistical or machine learning, directly affect the quality
    and the purpose of the model. You cannot look at the sales from groceries and
    figure out if Asians buy more milk than Mexicans as that needs additional elements
    from demographical data. Similarly, if our analysis was focused on customer experience
    (returns or exchanges of products) then it is based on different techniques and
    models than if we are trying to focus on revenue or up-sell customers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型可以针对完整数据集运行，并根据我们训练模型的方式生成结果或成果。在构建分析时所做的选择，无论是统计还是机器学习，都直接影响模型的质量和目的。你不能仅仅通过杂货销售来判断亚洲人是否比墨西哥人购买更多的牛奶，因为这需要来自人口统计数据的额外元素。同样，如果我们的分析侧重于客户体验（产品退货或换货），那么它所基于的技术和模型与我们试图专注于收入或向客户推销产品时是不同的。
- en: You will see various machine learning techniques in later chapters.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在后面的章节中看到各种机器学习技术。
- en: Analytical applications can thus be realized using several disciplines, teams,
    and skillsets. Analytical applications can be used to generate reports all the
    way to automatically triggering business actions. For example, you can simply
    create daily sales reports to be emailed out to all managers every day at 8 a.m.
    in the morning. But, you can also integrate with Business process management applications
    or some custom stock trading application to take action, such as buying, selling,
    or alerting on activities in the stock market. You can also think of taking in
    news articles or social media information to further influence the decisions to
    be made.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分析应用可以利用多种学科、团队和技能集来实现。分析应用可以用于生成报告，甚至自动触发业务行动。例如，你可以简单地创建每天早上8点给所有经理发送的每日销售报告。但是，你也可以与业务流程管理应用程序或一些定制的股票交易应用程序集成，以采取行动，如在股票市场上进行买卖或警报活动。你还可以考虑接收新闻文章或社交媒体信息，以进一步影响要做出的决策。
- en: Data visualization is an important piece of data analytics and it's hard to
    understand numbers when you are looking at a lot of metrics and calculation. Rather,
    there is an increasing dependence on **Business Intelligence** (**BI**) tools,
    such as Tableau, QlikView, and so on, to explore and analyze data. Of course,
    large-scale visualization such as showing all Uber cars in the country or heat
    maps showing the water supply in New York City requires more custom applications
    or specialized tools to be built.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化是数据分析的重要组成部分，当你看着大量的指标和计算时，很难理解数字。相反，人们越来越依赖商业智能工具，如Tableau、QlikView等，来探索和分析数据。当然，像显示全国所有优步车辆或显示纽约市供水的热力图这样的大规模可视化需要构建更多定制应用程序或专门的工具。
- en: Managing and analyzing data has always been a challenge across many organizations
    of different sizes across all industries. Businesses have always struggled to
    find a pragmatic approach to capturing information about their customers, products,
    and services. When the company only had a handful of customers who bought a few
    of their items, it was not that difficult. It was not as big a challenge. But
    over time, companies in the markets started growing. Things have become more complicated.
    Now, we have branding Information and social media. We have things that are sold
    and bought over the Internet. We need to come up with different solutions. Web
    development, organizations, pricing, social networks, and segmentations; there's
    a lot of different data that we're dealing with that brings a lot more complexity
    when it comes to dealing, managing, organizing, and trying to gain some insight
    from the data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在各行各业的许多不同规模的组织中，管理和分析数据一直是一个挑战。企业一直在努力寻找一个实用的方法来获取有关他们的客户、产品和服务的信息。当公司只有少数客户购买少量商品时，这并不困难。随着时间的推移，市场上的公司开始增长。事情变得更加复杂。现在，我们有品牌信息和社交媒体。我们有在互联网上销售和购买的商品。我们需要提出不同的解决方案。网站开发、组织、定价、社交网络和细分；我们处理的数据有很多不同的类型，这使得处理、管理、组织和尝试从数据中获得一些见解变得更加复杂。
- en: Introduction to big data
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据介绍
- en: As seen in the preceding section, data analytics incorporates techniques, tools,
    and methodologies to explore and analyze data to produce quantifiable outcomes
    for the business. The outcome could be a simple choice of a color to paint the
    storefront or more complicated predictions of customer behavior. As businesses
    grow, more and more varieties of analytics are coming into the picture. In 1980s
    or 1990s , all we could get was what was available in a SQL Data Warehouse; nowadays
    a lot of external factors are all playing an important role in influencing the
    way businesses run.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中可以看到，数据分析包括探索和分析数据的技术、工具和方法，以产生业务的可量化结果。结果可能是简单的选择商店外观的颜色，也可能是更复杂的客户行为预测。随着企业的发展，越来越多种类的分析出现在画面中。在20世纪80年代或90年代，我们所能得到的只是SQL数据仓库中可用的数据；如今，许多外部因素都在影响企业运营的方式。
- en: Twitter, Facebook, Amazon, Verizon, Macy's, and Whole Foods are all companies
    that run their business using data analytics and base many of the decisions on
    it. Think about what kind of data they are collecting, how much data they might
    be collecting, and then how they might be using the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter、Facebook、亚马逊、Verizon、Macy's和Whole Foods都是利用数据分析来经营业务并基于数据做出许多决策的公司。想想他们可能收集的数据类型、可能收集的数据量，以及他们可能如何使用这些数据。
- en: Let's look at our grocery store example seen earlier. What if the store starts
    expanding its business to set up 100s of stores. Naturally, the sales transactions
    will have to be collected and stored on a scale that is 100s of times more than
    the single store. But then, no business works independently any more. There is
    a lot of information out there starting from local news, tweets, yelp reviews,
    customer complaints, survey activities, competition from other stores, changing
    demographics, or the economy of the local area, and so on. All such additional
    data can help in better understanding customer behavior and revenue models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下之前提到的杂货店的例子。如果商店开始扩大业务，建立数百家店铺，那么销售交易将不可避免地需要以比单一店铺多数百倍的规模进行收集和存储。但是，现在没有任何企业是独立运作的。从当地新闻、推特、yelp评论、客户投诉、调查活动、其他商店的竞争、人口构成的变化，以及当地经济等方面都有大量信息。所有这些额外的数据都可以帮助更好地理解客户行为和收入模型。
- en: For example, if we see increasing negative sentiment regarding the store parking
    facility, then we could analyze this and take corrective action such as validated
    parking or negotiating with the city public transportation department to provide
    more frequent trains or buses for better reach.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们发现关于商店停车设施的负面情绪在增加，那么我们可以分析这一点，并采取纠正措施，比如提供验证停车或与城市公共交通部门协商，提供更频繁的火车或公交车，以便更好地到达。
- en: Such increasing quantity and a variety of data while provides better analytics
    also poses challenges to the business IT organization trying to store, process,
    and analyze all the data. It is, in fact, not uncommon to see TBs of data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不断增加的数量和多样性的数据，虽然提供了更好的分析，但也给企业IT组织存储、处理和分析所有数据带来了挑战。事实上，看到TB级别的数据并不罕见。
- en: Every day, we create more than 2 quintillion bytes of data (2 Exa Bytes), and
    it is estimated that more than 90% of the data has been generated in the last
    few years alone.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每天，我们创造超过2百万亿字节的数据（2艾字节），据估计，超过90%的数据仅在过去几年内生成。
- en: '**1 KB = 1024 Bytes**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 KB = 1024字节**'
- en: '**1 MB = 1024 KB**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 MB = 1024 KB**'
- en: '**1 GB = 1024 MB**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 GB = 1024 MB**'
- en: '**1 TB = 1024 GB ~ 1,000,000 MB**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 TB = 1024 GB ~ 1,000,000 MB**'
- en: '**1 PB = 1024 TB ~ 1,000,000 GB ~ 1,000,000,000 MB**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 PB = 1024 TB ~ 1,000,000 GB ~ 1,000,000,000 MB**'
- en: '**1 EB = 1024 PB ~ 1,000,000 TB ~ 1,000,000,000 GB ~ 1,000,000,000,000 MB**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 EB = 1024 PB ~ 1,000,000 TB ~ 1,000,000,000 GB ~ 1,000,000,000,000 MB**'
- en: Such large amounts of data since the 1990s, and the need to understand and make
    sense of the data, gave rise to the term *big data*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自20世纪90年代以来的大量数据以及理解和理解数据的需求，催生了“大数据”这个术语。
- en: The term big data, which spans computer science and statistics/econometrics,
    probably originated in the lunch-table conversations at Silicon Graphics in the
    mid-1990s, in which John Mashey figured prominently.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据这个跨越计算机科学和统计/计量经济学的术语，可能起源于20世纪90年代中期Silicon Graphics的午餐桌谈话，John Mashey在其中扮演了重要角色。
- en: In 2001, Doug Laney, then an analyst at consultancy Meta Group Inc (which got
    acquired by Gartner) introduced the idea of 3Vs (variety, velocity, and volume).
    Now, we refer to 4 Vs instead of 3Vs with the addition of Veracity of data to
    the 3Vs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 2001年，当时是咨询公司Meta Group Inc（后来被Gartner收购）的分析师的Doug Laney提出了3V（多样性、速度和数量）的概念。现在，我们提到4个V，而不是3个V，增加了数据的真实性到3个V。
- en: 4 Vs of big data
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据的4个V
- en: The following are the 4 Vs of big data used to describe the properties of big
    data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于描述大数据属性的4个V。
- en: Variety of Data
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的多样性
- en: Data can be from weather sensors, car sensors, census data, Facebook updates,
    tweets, transactions, sales, and marketing. The data format is both structured
    and unstructured as well. Data types can also be different; binary, text, JSON,
    and XML.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以来自气象传感器、汽车传感器、人口普查数据、Facebook更新、推文、交易、销售和营销。数据格式既结构化又非结构化。数据类型也可以不同；二进制、文本、JSON和XML。
- en: Velocity of Data
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的速度
- en: Data can be obtained from a data warehouse, batch mode file archives, near real-time
    updates, or instantaneous real-time updates from the Uber ride you just booked.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以来自数据仓库、批处理文件存档、近实时更新，或者刚刚预订的Uber车程的即时实时更新。
- en: Volume of Data
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据量
- en: Data can be collected and stored for an hour, a day, a month, a year, or 10
    years. The size of data is growing to 100s of TBs for many companies.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以收集和存储一小时、一天、一个月、一年或10年。对于许多公司来说，数据的大小正在增长到数百TB。
- en: Veracity of Data
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的真实性
- en: Data can be analyzed for actionable insights, but with so much data of all types
    being analyzed from across data sources, it is very difficult to ensure correctness
    and proof of accuracy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以分析出可操作的见解，但由于来自各种数据源的大量数据被分析，确保正确性和准确性证明是非常困难的。
- en: 'The following are the 4 Vs of big data:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是大数据的4个V：
- en: '![](img/00115.jpeg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00115.jpeg)'
- en: To make sense of all the data and apply data analytics to big data, we need
    to expand the concept of data analytics to operate at a much larger scale dealing
    with the 4 Vs of big data. This changes not only the tools, technologies, and
    methodologies used in analyzing data, but also the way we even approach the problem.
    If a SQL database was used for data in a business in 1999, now to handle the data
    for the same business we will need a distributed SQL database scalable and adaptable
    to the nuances of the big data space.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解所有数据并将数据分析应用于大数据，我们需要扩展数据分析的概念，以在更大的规模上处理大数据的4个V。这不仅改变了分析数据所使用的工具、技术和方法，还改变了我们处理问题的方式。如果在1999年业务中使用SQL数据库来处理数据，现在为了处理同一业务的数据，我们将需要一个可扩展和适应大数据空间细微差别的分布式SQL数据库。
- en: Big data analytics applications often include data from both internal systems
    and external sources, such as weather data or demographic data on consumers compiled
    by third-party information services providers. In addition, streaming analytics
    applications are becoming common in big data environments, as users look to do
    real-time analytics on data fed into Hadoop systems through Spark's Spark streaming
    module or other open source stream processing engines, such as Flink and Storm.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据分析应用通常包括来自内部系统和外部来源的数据，例如天气数据或第三方信息服务提供商编制的有关消费者的人口统计数据。此外，流式分析应用在大数据环境中变得常见，因为用户希望对通过Spark的Spark流模块或其他开源流处理引擎（如Flink和Storm）输入Hadoop系统的数据进行实时分析。
- en: Early big data systems were mostly deployed on-premises particularly in large
    organizations that were collecting, organizing, and analyzing massive amounts
    of data. But cloud platform vendors, such as **Amazon Web Services** (**AWS**)
    and Microsoft, have made it easier to set up and manage Hadoop clusters in the
    cloud, as have Hadoop suppliers such as Cloudera and Hortonworks, which support
    their distributions of the big data framework on the AWS and Microsoft Azure clouds.
    Users can now spin up clusters in the cloud, run them for as long as needed, and
    then take them offline, with usage-based pricing that doesn't require ongoing
    software licenses.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的大数据系统大多部署在大型组织的内部，这些组织正在收集、组织和分析大量数据。但云平台供应商，如亚马逊网络服务（AWS）和微软，已经让在云中设置和管理Hadoop集群变得更加容易，Hadoop供应商，如Cloudera和Hortonworks，也支持它们在AWS和微软Azure云上的大数据框架分发。用户现在可以在云中启动集群，运行所需的时间，然后将其下线，使用基于使用量的定价，无需持续的软件许可证。
- en: Potential pitfalls that can trip up organizations on big data analytics initiatives
    include a lack of internal analytics skills and the high cost of hiring experienced
    data scientists and data engineers to fill the gaps.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据分析项目中可能会遇到的潜在问题包括缺乏内部分析技能以及雇佣经验丰富的数据科学家和数据工程师的高成本来填补这些空缺。
- en: The amount of data that's typically involved, and its variety, can cause data
    management issues in areas including data quality, consistency, and governance;
    also, data silos can result from the use of different platforms and data stores
    in a big data architecture. In addition, integrating Hadoop, Spark, and other
    big data tools into a cohesive architecture that meets an organization's big data
    analytics needs is a challenging proposition for many IT and analytics teams,
    which have to identify the right mix of technologies and then put the pieces together.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通常涉及的数据量及其多样性可能会导致数据管理问题，包括数据质量、一致性和治理；此外，在大数据架构中使用不同平台和数据存储可能会导致数据孤立。此外，将Hadoop、Spark和其他大数据工具集成到满足组织大数据分析需求的统一架构中对许多IT和分析团队来说是一个具有挑战性的任务，他们必须确定合适的技术组合，然后将各个部分组合在一起。
- en: Distributed computing using Apache Hadoop
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Hadoop进行分布式计算
- en: Our world is filled with devices starting from the smart refrigerator, smart
    watch, phone, tablet, laptops, kiosks at the airport, ATM dispensing cash to you,
    and many many more. We are able to do things we could not even imagine just a
    few years ago. Instagram, Snapchat, Gmail, Facebook, Twitter, and Pinterest are
    a few of the applications we are now so used to; it is difficult to imagine a
    day without access to such applications.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的世界充满了各种设备，从智能冰箱、智能手表、手机、平板电脑、笔记本电脑、机场的信息亭、向您提供现金的ATM等等。我们能够做一些我们几年前无法想象的事情。Instagram、Snapchat、Gmail、Facebook、Twitter和Pinterest是我们现在如此习惯的一些应用程序；很难想象一天没有访问这些应用程序。
- en: With the advent of Cloud computing, using a few clicks we are able to launch
    100s if not, 1000s of machines in AWS, Azure (Microsoft), or Google Cloud among
    others and use immense resources to realize our business goals of all sorts.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 随着云计算的出现，我们能够通过几次点击在AWS、Azure（微软）或Google Cloud等平台上启动数百甚至数千台机器，并利用巨大的资源实现各种业务目标。
- en: Cloud computing has introduced us to the concepts of IaaS, PaaS, and SaaS, which
    gives us the ability to build and operate scalable infrastructures serving all
    types of use cases and business needs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算为我们引入了IaaS、PaaS和SaaS的概念，使我们能够构建和运营满足各种用例和业务需求的可扩展基础设施。
- en: '**IaaS** (**Infrastructure as a Service**) - Reliable-managed hardware is provided
    without the need for a Data center, power cords, Airconditioning, and so on.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**IaaS**（基础设施即服务）-提供可靠的托管硬件，无需数据中心、电源线、空调等。'
- en: '**PaaS** (**Platform as a Service**) - On top of IaaS, managed platforms such
    as Windows, Linux , Databases and so on are provided.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**PaaS**（平台即服务）-在IaaS之上，提供Windows、Linux、数据库等托管平台。'
- en: '**SaaS** (**Software as a Service**) - On top of SaaS, managed services such
    as SalesForce, [Kayak.com](https://www.kayak.co.in/?ispredir=true) and so on are
    provided to everyone.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**SaaS**（软件即服务）-在SaaS之上，为每个人提供SalesForce、[Kayak.com](https://www.kayak.co.in/?ispredir=true)等托管服务。'
- en: Behind the scenes is the world of highly scalable distributed computing, which
    makes it possible to store and process PB (PetaBytes) of data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 幕后是高度可扩展的分布式计算世界，这使得存储和处理PB（百万亿字节）数据成为可能。
- en: 1 ExaByte = 1024 PetaBytes (50 Million Blue Ray Movies)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 1艾克萨字节=1024百万亿字节（5000万部蓝光电影）
- en: 1 PetaByte = 1024 Tera Bytes (50,000 Blue Ray Movies)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 1 PB=1024 TB（50,000部蓝光电影）
- en: 1 TeraByte = 1024 Giga Bytes (50 Blue Ray Movies)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 1 TB=1024 GB（50部蓝光电影）
- en: Average size of 1 Blue Ray Disc for a Movie is ~ 20 GB
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 电影蓝光光盘的平均大小约为20 GB
- en: Now, the paradigm of Distributed Computing is not really a genuinely new topic
    and has been pursued in some shape or form over decades primarily at research
    facilities as well as by a few commercial product companies. **Massively Parallel
    Processing** (**MPP**) is a paradigm that was in use decades ago in several areas
    such as Oceanography, Earthquake monitoring, and Space exploration. Several companies
    such as Teradata also implemented MPP platforms and provided commercial products
    and applications. Eventually, tech companies such as Google and Amazon among others
    pushed the niche area of scalable distributed computing to a new stage of evolution,
    which eventually led to the creation of Apache Spark by Berkeley University.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，分布式计算范式并不是一个真正全新的话题，几十年来一直在研究机构以及一些商业产品公司主要进行研究和追求。**大规模并行处理**（MPP）是几十年前在海洋学、地震监测和太空探索等领域使用的一种范式。很多公司如Teradata也实施了MPP平台并提供商业产品和应用。最终，谷歌、亚马逊等科技公司推动了可扩展分布式计算这一小众领域的新阶段，最终导致了伯克利大学创建了Apache
    Spark。
- en: Google published a paper on **Map Reduce** (**MR**) as well as **Google File
    System** (**GFS**), which brought the principles of distributed computing to everyone.
    Of course, due credit needs to be given to Doug Cutting, who made it possible
    by implementing the concepts given in the Google white papers and introducing
    the world to Hadoop.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌发表了关于**Map Reduce**（MR）以及**Google File System**（GFS）的论文，将分布式计算原理带给了每个人。当然，应该给予Doug
    Cutting应有的赞誉，他通过实施谷歌白皮书中的概念并向世界介绍Hadoop，使这一切成为可能。
- en: The Apache Hadoop Framework is an open source software framework written in
    Java. The two main areas provided by the framework are storage and processing.
    For Storage, the Apache Hadoop Framework uses **Hadoop Distributed File System**
    (**HDFS**), which is based on the Google File System paper released on October
    2003\. For processing or computing, the framework depends on MapReduce, which
    is based on a Google paper on MR released in December 2004.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop框架是用Java编写的开源软件框架。框架提供的两个主要领域是存储和处理。对于存储，Apache Hadoop框架使用基于2003年10月发布的Google文件系统论文的Hadoop分布式文件系统（HDFS）。对于处理或计算，该框架依赖于基于2004年12月发布的Google关于MR的论文的MapReduce。
- en: The MapReduce framework evolved from V1 (based on Job Tracker and Task Tracker)
    to V2 (based on YARN).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce框架从V1（基于作业跟踪器和任务跟踪器）发展到V2（基于YARN）。
- en: Hadoop Distributed File System (HDFS)
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop分布式文件系统（HDFS）
- en: HDFS is a software-based filesystem implemented in Java and sits on top of the
    native file system. The main concept behind HDFS is that it divides a file into
    blocks (typically 128 MB) instead of dealing with a file as a whole. This allowed
    many features such as distribution, replication, failure recovery, and more importantly
    distributed processing of the blocks using multiple machines.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS是用Java实现的软件文件系统，位于本地文件系统之上。HDFS背后的主要概念是将文件分成块（通常为128 MB），而不是将整个文件处理。这允许许多功能，例如分布、复制、故障恢复，更重要的是使用多台机器对块进行分布式处理。
- en: Block sizes can be 64 MB, 128 MB, 256 MB, or 512 MB, whatever suits the purpose.
    For a 1 GB file with 128 MB blocks, there will be 1024 MB / 128 MB = 8 blocks.
    If you consider replication factor of 3, this makes it 24 blocks.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小可以是64 MB、128 MB、256 MB或512 MB，适合任何目的。对于具有128 MB块的1 GB文件，将有1024 MB / 128 MB
    = 8个块。如果考虑复制因子为3，这将使其成为24个块。
- en: 'HDFS provides a distributed storage system with fault tolerance and failure
    recovery. HDFS has two main components: name node and data node(s). Name node
    contains all the metadata of all content of the file system. Data nodes connect
    to the Name Node and rely on the name node for all metadata information regarding
    the content in the file system. If the name node does not know any information,
    data node will not be able to serve it to any client who wants to read/write to
    the HDFS.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS提供了具有容错和故障恢复功能的分布式存储系统。HDFS有两个主要组件：NameNode和DataNode。NameNode包含文件系统所有内容的所有元数据。DataNode连接到NameNode，并依赖于NameNode提供有关文件系统内容的所有元数据信息。如果NameNode不知道任何信息，DataNode将无法将其提供给任何想要读取/写入HDFS的客户端。
- en: 'The following is the HDFS architecture:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是HDFS架构：
- en: '![](img/00120.jpeg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00120.jpeg)'
- en: NameNode and DataNode are JVM processes so any machine that supports Java can
    run the NameNode or the DataNode process. There is only one NameNode (the second
    NameNode will be there too if you count the HA deployment) but 100s if not 1000s
    of DataNodes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: NameNode和DataNode都是JVM进程，因此任何支持Java的机器都可以运行NameNode或DataNode进程。只有一个NameNode（如果计算HA部署，则还会有第二个NameNode），但有100个或1000个DataNode。
- en: It is not advisable to have 1000s of DataNodes because all operations from all
    the DataNodes will tend to overwhelm the NameNode in a real production environment
    with a lot of data-intensive applications.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 不建议拥有1000个DataNode，因为来自所有DataNode的所有操作都会倾向于在具有大量数据密集型应用程序的真实生产环境中压倒NameNode。
- en: The existence of a single NameNode in a cluster greatly simplifies the architecture
    of the system. The NameNode is the arbitrator and repository for all HDFS metadata
    and any client, that wants to read/write data first contacts the NameNode for
    the metadata information. The data never flows directly through the NameNode,
    which allows 100s of DataNodes (PBs of data) to be managed by 1 NameNode.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中存在一个NameNode极大地简化了系统的架构。NameNode是HDFS元数据的仲裁者和存储库，任何想要读取/写入数据的客户端都首先与NameNode联系以获取元数据信息。数据永远不会直接流经NameNode，这允许1个NameNode管理100个DataNode（PB级数据）。
- en: HDFS supports a traditional hierarchical file organization with directories
    and files similar to most other filesystems. You can create, move, and delete
    files, and directories. The NameNode maintains the filesystem namespace and records
    all changes and the state of the filesystem. An application can specify the number
    of replicas of a file that should be maintained by HDFS and this information is
    also stored by the NameNode.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS支持传统的分层文件组织，具有类似于大多数其他文件系统的目录和文件。您可以创建、移动和删除文件和目录。NameNode维护文件系统命名空间，并记录文件系统的所有更改和状态。应用程序可以指定HDFS应该维护的文件副本数量，这些信息也由NameNode存储。
- en: HDFS is designed to reliably store very large files in a distributed manner
    across machines in a large cluster of data nodes. To deal with replication, fault
    tolerance, as well as distributed computing, HDFS stores each file as a sequence
    of blocks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS旨在以分布式方式可靠存储非常大的文件，跨大型数据节点集群中的机器进行存储。为了处理复制、容错以及分布式计算，HDFS将每个文件存储为一系列块。
- en: The NameNode makes all decisions regarding the replication of blocks. This is
    mainly dependent on a Block report from each of the DataNodes in the cluster received
    periodically at a heart beat interval. A block report contains a list of all blocks
    on a DataNode, which the NameNode then stores in its metadata repository.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: NameNode对块的复制做出所有决定。这主要取决于集群中每个DataNode定期在心跳间隔处接收的块报告。块报告包含DataNode上所有块的列表，然后NameNode将其存储在其元数据存储库中。
- en: The NameNode stores all metadata in memory and serves all requests from clients
    reading from/writing to HDFS. However, since this is the master node maintaining
    all the metadata about the HDFS, it is critical to maintain consistent and reliable
    metadata information. If this information is lost, the content on the HDFS cannot
    be accessed.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: NameNode将所有元数据存储在内存中，并为从/写入HDFS的客户端提供所有请求。但是，由于这是维护有关HDFS的所有元数据的主节点，因此维护一致且可靠的元数据信息至关重要。如果丢失此信息，则无法访问HDFS上的内容。
- en: For this purpose, HDFS NameNode uses a transaction log called the EditLog, which
    persistently records every change that occurs to the metadata of the filesystem.
    Creating a new file updates EditLog, so does moving a file or renaming a file,
    or deleting a file. The entire filesystem namespace, including the mapping of
    blocks to files and filesystem properties, is stored in a file called the `FsImage`.
    The **NameNode** keeps everything in memory as well. When a NameNode starts up,
    it loads the EditLog and the `FsImage` initializes itself to set up the HDFS.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，HDFS NameNode使用称为EditLog的事务日志，该日志持久记录文件系统元数据发生的每个更改。创建新文件会更新EditLog，移动文件或重命名文件，或删除文件也会如此。整个文件系统命名空间，包括块到文件的映射和文件系统属性，都存储在一个名为`FsImage`的文件中。**NameNode**也将所有内容保存在内存中。当NameNode启动时，它加载EditLog和`FsImage`，并初始化自身以设置HDFS。
- en: The DataNodes, however, have no idea about the HDFS, purely relying on the blocks
    of data stored. DataNodes rely entirely on the NameNode to perform any operations.
    Even when a client wants to connect to read a file or write to a file, it's the
    NameNode that tells the client where to connect to.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，DataNodes对于HDFS一无所知，完全依赖于存储的数据块。DataNodes完全依赖于NameNode执行任何操作。即使客户端想要连接以读取文件或写入文件，也是NameNode告诉客户端要连接到哪里。
- en: HDFS High Availability
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS高可用性
- en: HDFS is a Master-Slave cluster with the NameNode as the master and the 100s,
    if not 1000s of DataNodes as slaves, managed by the master node. This introduces
    a **Single Point of Failure** (**SPOF**) in the cluster as if the Master NameNode
    goes down for some reason, the entire cluster is going to be unusable. HDFS 1.0
    supports an additional Master Node known as the **Secondary NameNode** to help
    with recovery of the cluster. This is done by maintaining a copy of all the metadata
    of the filesystem and is by no means a Highly Available System requiring manual
    interventions and maintenance work. HDFS 2.0 takes this to the next level by adding
    support for full **High Availability** (**HA**).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS是一个主从集群，其中NameNode是主节点，而DataNodes是从节点，如果不是数百，就是数千个，由主节点管理。这在集群中引入了**单点故障**（**SPOF**），因为如果主NameNode因某种原因而崩溃，整个集群将无法使用。HDFS
    1.0支持另一个称为**Secondary NameNode**的附加主节点，以帮助恢复集群。这是通过维护文件系统的所有元数据的副本来完成的，绝不是一个需要手动干预和维护工作的高可用系统。HDFS
    2.0通过添加对完整**高可用性**（**HA**）的支持将其提升到下一个级别。
- en: HA works by having two Name Nodes in an active-passive mode such that one Name
    Node is active and other is passive. When the primary NameNode has a failure,
    the passive Name Node will take over the role of the Master Node.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: HA通过将两个NameNode设置为主备模式来工作，其中一个NameNode是活动的，另一个是被动的。当主NameNode发生故障时，被动NameNode将接管主节点的角色。
- en: 'The following diagram shows how the active-passive pair of NameNodes will be
    deployed:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了主备NameNode对的部署方式：
- en: '![](img/00123.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00123.jpeg)'
- en: HDFS Federation
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS联邦
- en: 'HDFS Federation is a way of using multiple name nodes to spread the filesystem
    namespace over. Unlike the first HDFS versions, which simply managed entire clusters
    using a single NameNode, which does not scale that well as the size of the cluster
    grows, HDFS Federation can support significantly larger clusters and horizontally
    scales the NameNode or name service using multiple federated name nodes. Take
    a look at the following diagram:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS联邦是使用多个名称节点来分布文件系统命名空间的一种方式。与最初的HDFS版本不同，最初的HDFS版本仅使用单个NameNode管理整个集群，随着集群规模的增长，这种方式并不那么可扩展，HDFS联邦可以支持规模显著更大的集群，并且可以使用多个联邦名称节点水平扩展NameNode或名称服务。请看下面的图表：
- en: '![](img/00127.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00127.jpeg)'
- en: HDFS Snapshot
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS快照
- en: 'Hadoop 2.0 also added a new capability: taking a snapshot (read-only copy and
    copy-on-write) of the filesystem (data blocks) stored on the data nodes. Using
    Snapshots, you can take a copy of directories seamlessly using the NameNode''s
    metadata of the data blocks. Snapshot creation is instantaneous and doesn''t require
    interference with other regular HDFS operations.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2.0还增加了一个新功能：对存储在数据节点上的文件系统（数据块）进行快照（只读副本和写时复制）。使用快照，可以使用NameNode的数据块元数据无缝地对目录进行快照。快照创建是瞬时的，不需要干预其他常规HDFS操作。
- en: 'The following is an illustration of how snapshot works on specific directories:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是快照在特定目录上的工作原理的示例：
- en: '![](img/00131.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00131.jpeg)'
- en: HDFS Read
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS读取
- en: Client connects to the NameNode and ask about a file using the name of the file.
    NameNode looks up the block locations for the file and returns the same to the
    client. The client can then connect to the DataNodes and read the blocks needed.
    NameNode does not participate in the data transfer.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端连接到NameNode，并使用文件名询问文件。NameNode查找文件的块位置并将其返回给客户端。然后客户端可以连接到DataNodes并读取所需的块。NameNode不参与数据传输。
- en: The following is the flow of a read request from a client. First, the client
    gets the locations and then pulls the blocks from the DataNodes. If a DataNode
    fails in the middle, then the client gets the replica of the block from another
    DataNode.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是客户端的读取请求流程。首先，客户端获取位置，然后从DataNodes拉取块。如果DataNode在中途失败，客户端将从另一个DataNode获取块的副本。
- en: '![](img/00264.jpeg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00264.jpeg)'
- en: HDFS Write
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS写入
- en: The client connects to the NameNode and asks the NameNode to let it write to
    the HDFS. The NameNode looks up information and plans the blocks, the Data Nodes
    to be used to store the blocks, and the replication strategy to be used. The NameNode
    does not handle any data and only tells the client where to write. Once the first
    DataNode receives the block, based on the replication strategy, the NameNode tells
    the first DataNode where else to replicate. So, the DataNode that is received
    from client sends the block over to the second DataNode (where the copy of the
    block is supposed to be written to) and then the second DataNode sends it to a
    third DataNode (if replication-factor is 3).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端连接到NameNode，并要求NameNode让其写入HDFS。NameNode查找信息并计划块、用于存储块的Data Nodes以及要使用的复制策略。NameNode不处理任何数据，只告诉客户端在哪里写入。一旦第一个DataNode接收到块，根据复制策略，NameNode告诉第一个DataNode在哪里复制。因此，从客户端接收的DataNode将块发送到第二个DataNode（应该写入块的副本所在的地方），然后第二个DataNode将其发送到第三个DataNode（如果复制因子为3）。
- en: The following is the flow of a write request from a client. First, the client
    gets the locations and then writes to the first DataNode. The DataNode that receives
    the block replicates the block to the DataNodes that should hold the replica copy
    of the block. This happens for all the blocks being written to from the client.
    If a DataNode fails in the middle, then the block gets replicated to another DataNode
    as determined by the NameNode.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自客户端的写入请求的流程。首先，客户端获取位置，然后写入第一个DataNode。接收块的DataNode将块复制到应该保存块副本的DataNodes。这对从客户端写入的所有块都是如此。如果一个DataNode在中间失败，那么块将根据NameNode确定的另一个DataNode进行复制。
- en: '![](img/00267.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00267.jpeg)'
- en: So far, we have seen how HDFS provides a distributed filesystem using blocks,
    the NameNode, and DataNodes. Once data is stored at a PB scale, it is also important
    to actually process the data to serve the various use cases of the business.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到HDFS使用块、NameNode和DataNodes提供了分布式文件系统。一旦数据存储在PB规模，实际处理数据以满足业务的各种用例也变得非常重要。
- en: MapReduce framework was created in the Hadoop framework to perform distributed
    computation. We will look at this further in the next section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce框架是在Hadoop框架中创建的，用于执行分布式计算。我们将在下一节中进一步讨论这个问题。
- en: MapReduce framework
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce框架
- en: '**MapReduce** (**MR**) framework enables you to write distributed applications
    to process large amounts of data from a filesystem such as HDFS in a reliable
    and fault-tolerant manner. When you want to use the MapReduce Framework to process
    data, it works through the creation of a job, which then runs on the framework
    to perform the tasks needed.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**MapReduce** (**MR**)框架使您能够编写分布式应用程序，以可靠和容错的方式处理来自文件系统（如HDFS）的大量数据。当您想要使用MapReduce框架处理数据时，它通过创建一个作业来运行框架以执行所需的任务。'
- en: A MapReduce job usually works by splitting the input data across worker nodes
    running **Mapper** tasks in a parallel manner. At this time, any failures that
    happen either at the HDFS level or the failure of a Mapper task are handled automatically
    to be fault-tolerant. Once the Mappers are completed, the results are copied over
    the network to other machines running **Reducer** tasks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce作业通常通过在多个工作节点上运行**Mapper**任务并行地分割输入数据来工作。此时，无论是在HDFS级别发生的任何故障，还是Mapper任务的故障，都会自动处理以实现容错。一旦Mapper完成，结果就会通过网络复制到运行**Reducer**任务的其他机器上。
- en: An easy way to understand this concept is to imagine that you and your friends
    want to sort out piles of fruit into boxes. For that, you want to assign each
    person the task of going through one raw basket of fruit (all mixed up) and separate
    out the fruit into various boxes. Each person then does the same with this basket
    of fruit.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这个概念的一个简单方法是想象你和你的朋友想要把一堆水果分成盒子。为此，你想要指派每个人的任务是去处理一个原始的水果篮子（全部混在一起），并将水果分成不同的盒子。然后每个人都用同样的方法处理这个水果篮子。
- en: In the end, you end up with a lot of boxes of fruit from all your friends. Then,
    you can assign a group to put the same kind of fruit together in a box, weight
    the box, and seal the box for shipping.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你最终会得到很多盒子水果，都是来自你的朋友。然后，你可以指派一个小组将相同种类的水果放在一起放进一个盒子里，称重，封箱以便运输。
- en: 'The following depicts the idea of taking fruit baskets and sorting the fruit
    by the type of fruit:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下描述了将水果篮子拿来按水果类型分类的想法：
- en: '![](img/00270.jpeg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00270.jpeg)'
- en: MapReduce framework consists of a single resource manager and multiple node
    managers (usually Node Managers coexist with the data nodes of HDFS). When an
    application wants to run, the client launches the application master, which then
    negotiates with the resource manager to get resources in the cluster in form of
    containers.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce框架由一个资源管理器和多个节点管理器组成（通常节点管理器与HDFS的数据节点共存）。当应用程序想要运行时，客户端启动应用程序主管，然后与资源管理器协商以获取容器形式的集群资源。
- en: A container represents CPUs (cores) and memory allocated on a single node to
    be used to run tasks and processes. Containers are supervised by the node manager
    and scheduled by the resource manager.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 容器表示分配给单个节点用于运行任务和进程的CPU（核心）和内存。容器由节点管理器监督，并由资源管理器调度。
- en: 'Examples of containers:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 容器的示例：
- en: 1 core + 4 GB RAM
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 1核+4GB RAM
- en: 2 cores + 6 GB RAM
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 2核+6GB RAM
- en: 4 cores + 20 GB RAM
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 4核+20GB RAM
- en: Some Containers are assigned to be Mappers and other to be Reducers; all this
    is coordinated by the application master in conjunction with the resource manager.
    This framework is called **Yet Another Resource Negotiator** (**YARN**)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一些容器被分配为Mappers，其他容器被分配为Reducers；所有这些都由应用程序主管与资源管理器协调。这个框架被称为**Yet Another Resource
    Negotiator** (**YARN**)
- en: 'The following is a depiction of YARN:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是YARN的描述：
- en: '![](img/00276.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00276.jpeg)'
- en: 'A classic example showing the MapReduce framework at work is the word count
    example. The following are the various stages of processing the input data, first
    splitting the input across multiple worker nodes and then finally generating the
    output counts of words:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 展示MapReduce框架工作的一个经典例子是单词计数示例。以下是处理输入数据的各个阶段，首先是将输入分割到多个工作节点，最后生成单词计数的输出：
- en: '![](img/00279.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00279.jpeg)'
- en: Though MapReduce framework is very successful all across the world and has been
    adopted by most companies, it does run into issues mainly because of the way it
    processes data. Several technologies have come into existence to try and make
    MapReduce easier to use such as Hive and Pig but the complexity remains.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MapReduce框架在全球范围内非常成功，并且已被大多数公司采用，但它确实遇到了问题，主要是因为它处理数据的方式。已经出现了几种技术来尝试使MapReduce更易于使用，例如Hive和Pig，但复杂性仍然存在。
- en: 'Hadoop MapReduce has several limitations such as:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop MapReduce有一些限制，例如：
- en: Performance bottlenecks due to disk-based processing
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于基于磁盘的处理而导致性能瓶颈
- en: Batch processing doesn't serve all needs
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理无法满足所有需求
- en: Programming can be verbose and complex
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程可能冗长复杂
- en: Scheduling of the tasks is slow as there is not much reuse of resources
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务调度速度慢，因为资源的重复利用不多
- en: No good way to do real-time event processing
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有很好的实时事件处理方式
- en: Machine learning takes too long as usually ML involves iterative processing
    and MR is too slow for this
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习太慢，因为通常ML涉及迭代处理，而MR对此太慢
- en: Hive was created by Facebook as a SQL-like interface to MR. Pig was created
    by Yahoo with a scripting interface to MR. Moreover, several enhancements such
    as Tez (Hortonworks) and LLAP (Hive2.x) are in use, which makes use of in-memory
    optimizations to circumvent the limitations of MapReduce.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Hive是Facebook创建的MR的类似SQL接口。Pig是Yahoo创建的MR的脚本接口。此外，还有一些增强功能，如Tez（Hortonworks）和LLAP（Hive2.x），它们利用内存优化来规避MapReduce的限制。
- en: In the next section, we will look at Apache Spark, which has already solved
    some of the limitations of Hadoop technologies.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看一下Apache Spark，它已经解决了Hadoop技术的一些限制。
- en: Here comes Apache Spark
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark来了
- en: Apache Spark is a unified distributed computing engine across different workloads
    and platforms. Spark can connect to different platforms and process different
    data workloads using a variety of paradigms such as Spark streaming, Spark ML,
    Spark SQL, and Spark GraphX.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个统一的分布式计算引擎，可跨不同的工作负载和平台进行连接。Spark可以连接到不同的平台，并使用各种范例处理不同的数据工作负载，如Spark流式处理、Spark
    ML、Spark SQL和Spark GraphX。
- en: Apache Spark is a fast in-memory data processing engine with elegant and expressive
    development APIs to allow data workers to efficiently execute streaming machine
    learning or SQL workloads that require fast interactive access to data sets. Apache
    Spark consists of Spark core and a set of libraries. The core is the distributed
    execution engine and the Java, Scala, and Python APIs offer a platform for distributed
    application development. Additional libraries built on top of the core allow workloads
    for streaming, SQL, Graph processing, and machine learning. Spark ML, for instance,
    is designed for data science and its abstraction makes data science easier.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个快速的内存数据处理引擎，具有优雅和富有表现力的开发API，允许数据工作者高效执行流式机器学习或SQL工作负载，需要快速交互式访问数据集。Apache
    Spark由Spark核心和一组库组成。核心是分布式执行引擎，Java、Scala和Python API提供了分布式应用程序开发的平台。在核心之上构建的其他库允许流式、SQL、图处理和机器学习工作负载。例如，Spark
    ML专为数据科学而设计，其抽象使数据科学更容易。
- en: Spark provides real-time streaming, queries, machine learning, and graph processing.
    Before Apache Spark, we had to use different technologies for different types
    of workloads, one for batch analytics, one for interactive queries, one for real-time
    streaming processing and another for machine learning algorithms. However, Apache
    Spark can do all of these just using Apache Spark instead of using multiple technologies
    that are not always integrated.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供实时流式处理、查询、机器学习和图处理。在Apache Spark之前，我们必须使用不同的技术来处理不同类型的工作负载，一个用于批量分析，一个用于交互式查询，一个用于实时流处理，另一个用于机器学习算法。然而，Apache
    Spark可以只使用Apache Spark来完成所有这些工作，而不是使用不一定总是集成的多种技术。
- en: Using Apache Spark, all types of workload can be processed and Spark also supports
    Scala, Java, R, and Python as a means of writing client programs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Apache Spark，可以处理各种类型的工作负载，Spark还支持Scala、Java、R和Python作为编写客户端程序的手段。
- en: 'Apache Spark is an open-source distributed computing engine which has key advantages
    over the MapReduce paradigm:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个开源的分布式计算引擎，相对于MapReduce范式具有关键优势：
- en: Uses in-memory processing as much as possible
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能使用内存处理
- en: General purpose engine to be used for batch, real-time workloads
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用引擎用于批处理、实时工作负载
- en: Compatible with YARN and also Mesos
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与YARN和Mesos兼容
- en: Integrates well with HBase, Cassandra, MongoDB, HDFS, Amazon S3, and other file
    systems and data sources
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与HBase、Cassandra、MongoDB、HDFS、Amazon S3和其他文件系统和数据源良好集成
- en: 'Spark was created in Berkeley back in 2009 and was a result of the project
    to build Mesos, a cluster management framework to support different kinds of cluster
    computing systems. Take a look at the following table:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是2009年在伯克利创建的，是构建Mesos的项目的结果，Mesos是一个支持不同类型的集群计算系统的集群管理框架。看一下下表：
- en: '| Version | Release date | Milestones |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 版本 | 发布日期 | 里程碑 |'
- en: '| 0.5 | 2012-10-07 | First available version for non-production usage |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 2012-10-07 | 非生产使用的第一个可用版本 |'
- en: '| 0.6 | 2013-02-07 | Point release with various changes |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 0.6 | 2013-02-07 | 各种更改的点版本发布 |'
- en: '| 0.7 | 2013-07-16 | Point release with various changes |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 0.7 | 2013-07-16 | 各种更改的点版本发布 |'
- en: '| 0.8 | 2013-12-19 | Point release with various changes |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 0.8 | 2013-12-19 | 各种更改的点版本发布 |'
- en: '| 0.9 | 2014-07-23 | Point release with various changes |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 0.9 | 2014-07-23 | 各种更改的点版本发布 |'
- en: '| 1.0 | 2014-08-05 | First production ready, backward-compatible release. Spark
    Batch, Streaming, Shark, MLLib, GraphX |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 | 2014-08-05 | 第一个生产就绪，向后兼容的发布。Spark Batch，Streaming，Shark，MLLib，GraphX|'
- en: '| 1.1 | 2014-11-26 | Point release with various changes |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 1.1 | 2014-11-26 | 各种变更的点发布|'
- en: '| 1.2 | 2015-04-17 | Structured Data, SchemaRDD (subsequently evolved into
    DataFrames) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 1.2 | 2015-04-17 | 结构化数据，SchemaRDD（后来演变为DataFrames）|'
- en: '| 1.3 | 2015-04-17 | API to provide a unified API to read from structured and
    semi-structured sources |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 1.3 | 2015-04-17 | 提供统一的API来从结构化和半结构化源读取的API|'
- en: '| 1.4 | 2015-07-15 | SparkR, DataFrame API, Tungsten improvements |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 1.4 | 2015-07-15 | SparkR，DataFrame API，Tungsten改进|'
- en: '| 1.5 | 2015-11-09 | Point release with various changes |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 1.5 | 2015-11-09 | 各种变更的点发布|'
- en: '| 1.6 | 2016-11-07 | Dataset DSL introduced |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 1.6 | 2016-11-07 | 引入数据集DSL|'
- en: '| 2.0 | 2016-11-14 | DataFrames and Datasets API as fundamental layer for ML,
    Structured Streaming,SparkR improvements. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 2.0 | 2016-11-14 | DataFrames和Datasets API作为机器学习、结构化流处理、SparkR改进的基本层。|'
- en: '| 2.1 | 2017-05-02 | Event time watermarks, ML, GraphX improvements |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 2.1 | 2017-05-02 | 事件时间水印，机器学习，GraphX改进|'
- en: 2.2 has been released 2017-07-11 which has several improvements especially Structured
    Streaming which is now GA.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2已于2017-07-11发布，其中有几项改进，特别是结构化流处理现在是GA。
- en: 'Spark is a platform for distributed computing that has several features:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个分布式计算平台，具有几个特点：
- en: Transparently processes data on multiple nodes via a simple API
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过简单的API在多个节点上透明地处理数据
- en: Resiliently handles failures
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有弹性处理故障
- en: Spills data to disk as necessary though predominantly uses memory
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据需要将数据溢出到磁盘，尽管主要使用内存
- en: Java, Scala, Python, R, and SQL APIs are supported
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持Java，Scala，Python，R和SQL API
- en: The same Spark code can run standalone, in Hadoop YARN, Mesos, and the cloud
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同的Spark代码可以独立运行，在Hadoop YARN，Mesos和云中
- en: Scala features such as implicits, higher-order functions, structured types,
    and so on allow us to easily build DSL's and integrate them with the language.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Scala的特性，如隐式，高阶函数，结构化类型等，使我们能够轻松构建DSL，并将其与语言集成。
- en: Apache Spark does not provide a Storage layer and relies on HDFS or Amazon S3
    and so on. Hence, even if Apache Hadoop technologies are replaced with Apache
    Spark, HDFS is still needed to provide a reliable storage layer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark不提供存储层，并依赖于HDFS或Amazon S3等。因此，即使将Apache Hadoop技术替换为Apache Spark，仍然需要HDFS来提供可靠的存储层。
- en: Apache Kudu provides an alternative to HDFS and there is already integration
    between Apache Spark and Kudu Storage layer, further decoupling Apache Spark and
    the Hadoop ecosystem.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kudu提供了HDFS的替代方案，Apache Spark和Kudu存储层之间已经有集成，进一步解耦了Apache Spark和Hadoop生态系统。
- en: Hadoop and Apache Spark are both popular big data frameworks, but they don't
    really serve the same purposes. While Hadoop provides distributed storage and
    a MapReduce distributed computing framework, Spark on the other hand is a data
    processing framework that operates on the distributed data storage provided by
    other technologies.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop和Apache Spark都是流行的大数据框架，但它们实际上并不提供相同的功能。虽然Hadoop提供了分布式存储和MapReduce分布式计算框架，但Spark则是一个在其他技术提供的分布式数据存储上运行的数据处理框架。
- en: Spark is generally a lot faster than MapReduce because of the way it processes
    data. MapReduce operates on splits using Disk operations, Spark operates on the
    dataset much more efficiently than MapReduce, with the main reason behind the
    performance improvement in Apache Spark being the efficient off-heap in-memory
    processing rather than solely relying on disk-based computations.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Spark通常比MapReduce快得多，因为它处理数据的方式不同。MapReduce使用磁盘操作来操作拆分，而Spark比MapReduce更有效地处理数据集，Apache
    Spark性能改进的主要原因是高效的堆外内存处理，而不仅仅依赖于基于磁盘的计算。
- en: MapReduce's processing style can be sufficient if you were data operations and
    reporting requirements are mostly static and it is okay to use batch processing
    for your purposes, but if you need to do analytics on streaming data or your processing
    requirements need multistage processing logic, you will probably want to want
    to go with Spark.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据操作和报告需求大部分是静态的，并且可以使用批处理来满足您的需求，那么MapReduce的处理方式可能足够了，但是如果您需要对流数据进行分析，或者您的处理需求需要多阶段处理逻辑，那么您可能会选择Spark。
- en: There are three layers in the Spark stack. The bottom layer is the cluster manager,
    which can be standalone, YARN, or Mesos.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Spark堆栈中有三层。底层是集群管理器，可以是独立的，YARN或Mesos。
- en: Using local mode, you don't need a cluster manager to process.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本地模式，您不需要集群管理器来处理。
- en: In the middle, above the cluster manager, is the layer of Spark core, which
    provides all the underlying APIs to perform task scheduling and interacting with
    storage.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群管理器之上的中间层是Spark核心层，它提供了执行任务调度和与存储交互的所有基础API。
- en: At the top are modules that run on top of Spark core such as Spark SQL to provide
    interactive queries, Spark streaming for real-time analytics, Spark ML for machine
    learning, and Spark GraphX for graph processing.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部是在Spark核心之上运行的模块，如Spark SQL提供交互式查询，Spark streaming用于实时分析，Spark ML用于机器学习，Spark
    GraphX用于图处理。
- en: 'The three layers are as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这三层分别是：
- en: '![](img/00283.jpeg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00283.jpeg)'
- en: As seen in the preceding diagram, the various libraries such as Spark SQL, Spark
    streaming, Spark ML, and GraphX all sit on top of Spark core, which is the middle
    layer. The bottom layer shows the various cluster manager options.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，各种库（如Spark SQL，Spark streaming，Spark ML和GraphX）都位于Spark核心之上，而Spark核心位于中间层。底层显示了各种集群管理器选项。
- en: 'Let''s now look at each of the component briefly:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们简要地看一下每个组件：
- en: Spark core
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark核心
- en: Spark core is the underlying general execution engine for the Spark platform
    that all other functionality is built upon. Spark core contains basic Spark functionalities
    required for running jobs and needed by other components. It provides in-memory
    computing and referencing datasets in external storage systems, the most important
    being the **Resilient Distributed Dataset** (**RDD**).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Spark核心是构建在其上的所有其他功能的基础通用执行引擎。Spark核心包含运行作业所需的基本Spark功能，并且其他组件需要这些功能。它提供了内存计算和引用外部存储系统中的数据集，最重要的是**弹性分布式数据集**（**RDD**）。
- en: In addition, Spark core contains logic for accessing various filesystems, such
    as HDFS, Amazon S3, HBase, Cassandra, relational databases, and so on. Spark core
    also provides fundamental functions to support networking, security, scheduling,
    and data shuffling to build a high scalable, fault-tolerant platform for distributed
    computing.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark核心包含访问各种文件系统（如HDFS、Amazon S3、HBase、Cassandra、关系数据库等）的逻辑。Spark核心还提供了支持网络、安全、调度和数据洗牌的基本功能，以构建一个高可伸缩、容错的分布式计算平台。
- en: We cover Spark core in detail in [Chapter 6](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c),
    *Start Working with Spark - REPL* and RDDs and [Chapter 7](part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c),
    *Special RDD Operations*.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第6章](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c) *开始使用Spark -
    REPL*和RDDs以及[第7章](part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c) *特殊RDD操作*中详细介绍了Spark核心。
- en: DataFrames and datasets built on top of RDDs and introduced with Spark SQL are
    becoming the norm now over RDDs in many use cases. RDDs are still more flexible
    in terms of handling totally unstructured data, but in future datasets, API might
    eventually become the core API.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多用例中，构建在RDD之上并由Spark SQL引入的DataFrame和数据集现在正在成为RDD的标准。就处理完全非结构化数据而言，RDD仍然更灵活，但在未来，数据集API可能最终成为核心API。
- en: Spark SQL
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Spark SQL is a component on top of Spark core that introduces a new data abstraction
    called **SchemaRDD**, which provides support for structured and semi-structured
    data. Spark SQL provides functions for manipulating large sets of distributed,
    structured data using an SQL subset supported by Spark and Hive QL. Spark SQL
    simplifies the handling of structured data through DataFrames and datasets at
    a much more performant level as part of the Tungsten initative. Spark SQL also
    supports reading and writing data to and from various structured formats and data
    sources, files, parquet, orc, relational databases, Hive, HDFS, S3, and so on.
    Spark SQL provides a query optimization framework called **Catalyst** to optimize
    all operations to boost the speed (compared to RDDs Spark SQL is several times
    faster). Spark SQL also includes a Thrift server, which can be used by external
    systems to query data through Spark SQL using classic JDBC and ODBC protocols.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是Spark核心之上的一个组件，引入了一个名为**SchemaRDD**的新数据抽象，它提供对结构化和半结构化数据的支持。Spark
    SQL提供了用于操作大型分布式结构化数据集的函数，使用Spark和Hive QL支持的SQL子集。Spark SQL通过DataFrame和数据集简化了对结构化数据的处理，作为Tungsten计划的一部分，它在更高的性能水平上运行。Spark
    SQL还支持从各种结构化格式和数据源（文件、parquet、orc、关系数据库、Hive、HDFS、S3等）读取和写入数据。Spark SQL提供了一个名为**Catalyst**的查询优化框架，以优化所有操作以提高速度（与RDD相比，Spark
    SQL快几倍）。Spark SQL还包括一个Thrift服务器，可以被外部系统使用，通过经典的JDBC和ODBC协议通过Spark SQL查询数据。
- en: We cover Spark SQL in detail in [Chapter 8](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c),
    *Introduce a Little Structure - Spark SQL*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第8章](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c) *引入一点结构 - Spark
    SQL*中详细介绍了Spark SQL。
- en: Spark streaming
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark流处理
- en: Spark streaming leverages Spark core's fast scheduling capability to perform
    streaming analytics by ingesting real-time streaming data from various sources
    such as HDFS, Kafka, Flume, Twitter, ZeroMQ, Kinesis, and so on. Spark streaming
    uses micro-batches of data to process the data in chunks and, uses a concept known
    as DStreams, Spark streaming can operate on the RDDs, applying transformations
    and actions as regular RDDs in the Spark core API. Spark streaming operations
    can recover from failure automatically using various techniques. Spark streaming
    can be combined with other Spark components in a single program, unifying real-time
    processing with machine learning, SQL, and graph operations.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Spark流处理利用Spark核心的快速调度能力，通过从各种来源（如HDFS、Kafka、Flume、Twitter、ZeroMQ、Kinesis等）摄取实时流数据来执行流式分析。Spark流处理使用数据的微批处理来处理数据，并且使用称为DStreams的概念，Spark流处理可以在RDD上操作，将转换和操作应用于Spark核心API中的常规RDD。Spark流处理操作可以使用各种技术自动恢复失败。Spark流处理可以与其他Spark组件结合在一个程序中，将实时处理与机器学习、SQL和图操作统一起来。
- en: We cover Spark streaming in detail in the [Chapter 9](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c),
    *Stream Me Up, Scotty - Spark Streaming*.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第9章](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c) *Stream Me Up,
    Scotty - Spark Streaming*中详细介绍了Spark流处理。
- en: In addition, the new Structured Streaming API makes Spark streaming programs
    more similar to Spark batch programs and also allows real-time querying on top
    of streaming data, which is complicated with the Spark streaming library before
    Spark 2.0+.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，新的Structured Streaming API使得Spark流处理程序更类似于Spark批处理程序，并且还允许在流数据之上进行实时查询，这在Spark
    2.0+之前的Spark流处理库中是复杂的。
- en: Spark GraphX
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark GraphX
- en: GraphX is a distributed graph processing framework on top of Spark. Graphs are
    data structures comprising vertices and the edges connecting them. GraphX provides
    functions for building graphs, represented as Graph RDDs. It provides an API for
    expressing graph computation that can model user-defined graphs by using the Pregel
    abstraction API. It also provides an optimized runtime for this abstraction. GraphX
    also contains implementations of the most important algorithms of graph theory,
    such as page rank, connected components, shortest paths, SVD++, and others.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX是在Spark之上的分布式图形处理框架。图形是由顶点和连接它们的边组成的数据结构。GraphX提供了用于构建图形的函数，表示为图形RDD。它提供了一个API，用于表达可以使用Pregel抽象API模拟用户定义的图形的图形计算。它还为此抽象提供了优化的运行时。GraphX还包含图论中最重要的算法的实现，例如PageRank、连通组件、最短路径、SVD++等。
- en: We cover Spark Graphx in detail in [Chapter 10](part0326.html#9MSNC1-21aec46d8593429cacea59dbdcd64e1c),
    *Everything is Connected - GraphX*.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第10章](part0326.html#9MSNC1-21aec46d8593429cacea59dbdcd64e1c)中详细介绍了Spark
    Graphx，*一切都连接在一起-GraphX*。
- en: A newer module known as GraphFrames is in development, which makes it easier
    to do Graph processing using DataFrame-based Graphs. GraphX is to RDDs what GraphFrames
    are to DataFrames/datasets. Also, this is currently separate from GraphX and is
    expected to support all the functionality of GraphX in the future, when there
    might be a switch over to GraphFrames.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名为GraphFrames的新模块正在开发中，它使使用基于DataFrame的图形处理变得更加容易。GraphX对RDDs的作用类似于GraphFrames对DataFrame/数据集的作用。此外，目前这与GraphX是分开的，并且预计在未来将支持GraphX的所有功能，届时可能会切换到GraphFrames。
- en: Spark ML
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark ML
- en: MLlib is a distributed machine learning framework above Spark core and handles
    machine-learning models used for transforming datasets in the form of RDDs. Spark
    MLlib is a library of machine-learning algorithms providing various algorithms
    such as logistic regression, Naive Bayes classification, **Support Vector Machines**
    (**SVMs**), decision trees, random forests, linear regression, **Alternating Least
    Squares** (**ALS**), and k-means clustering. Spark ML integrates very well with
    Spark core, Spark streaming, Spark SQL, and GraphX to provide a truly integrated
    platform where data can be real-time or batch.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib是在Spark核心之上的分布式机器学习框架，处理用于转换RDD形式的数据集的机器学习模型。Spark MLlib是一个机器学习算法库，提供各种算法，如逻辑回归、朴素贝叶斯分类、支持向量机（SVMs）、决策树、随机森林、线性回归、交替最小二乘法（ALS）和k均值聚类。Spark
    ML与Spark核心、Spark流、Spark SQL和GraphX集成非常好，提供了一个真正集成的平台，其中数据可以是实时的或批处理的。
- en: We cover Spark ML in detail in [Chapter 11](part0170.html#523VK1-21aec46d8593429cacea59dbdcd64e1c),
    *Learning Machine Learning - Spark MLlib and ML*.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第11章](part0170.html#523VK1-21aec46d8593429cacea59dbdcd64e1c)中详细介绍了Spark
    ML，*学习机器学习-Spark MLlib和ML*。
- en: In addition, PySpark and SparkR are also available as means to interact with
    Spark clusters and use the Python and R APIs. Python and R integrations truly
    open up Spark to a population of Data scientists and Machine learning modelers
    as the most common languages used by Data scientists in general are Python and
    R. This is the reason why Spark supports Python integration and also R integration,
    so as to avoid the costly process of learning a new language of Scala. Another
    reason is that there might be a lot of existing code written in Python and R,
    and if we can leverage some of the code, that will improve the productivity of
    the teams rather than building everything again from scratch.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，PySpark和SparkR也可用作与Spark集群交互并使用Python和R API的手段。Python和R的集成真正为数据科学家和机器学习建模者打开了Spark，因为一般数据科学家使用的最常见的语言是Python和R。这也是Spark支持Python集成和R集成的原因，以避免学习Scala这种新语言的成本。另一个原因是可能存在大量用Python和R编写的现有代码，如果我们可以利用其中的一些代码，那将提高团队的生产力，而不是从头开始构建所有内容。
- en: There is increasing popularity for, and usage of, notebook technologies such
    as Jupyter and Zeppelin, which make it significantly easier to interact with Spark
    in general, but particularly very useful in Spark ML where a lot of hypotheses
    and analysis are expected.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的人开始使用Jupyter和Zeppelin等笔记本技术，这使得与Spark进行交互变得更加容易，特别是在Spark ML中，预计会有很多假设和分析。
- en: PySpark
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark
- en: PySpark uses Python-based `SparkContext` and Python scripts as tasks and then
    uses sockets and pipes to executed processes to communicate between Java-based
    Spark clusters and Python scripts. PySpark also uses `Py4J`, which is a popular
    library integrated within PySpark that lets Python interface dynamically with
    Java-based RDDs.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark使用基于Python的`SparkContext`和Python脚本作为任务，然后使用套接字和管道来执行进程，以在基于Java的Spark集群和Python脚本之间进行通信。PySpark还使用`Py4J`，这是一个在PySpark中集成的流行库，它让Python动态地与基于Java的RDD进行交互。
- en: Python must be installed on all worker nodes running the Spark executors.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行Spark执行程序的所有工作节点上必须安装Python。
- en: 'The following is how PySpark works by communicating between Java processed
    and Python scripts:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是PySpark通过在Java进程和Python脚本之间进行通信的方式：
- en: '![](img/00286.jpeg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00286.jpeg)'
- en: SparkR
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkR
- en: '`SparkR` is an R package that provides a light-weight frontend to use Apache
    Spark from R. SparkR provides a distributed data frame implementation that supports
    operations such as selection, filtering, aggregation, and so on. SparkR also supports
    distributed machine learning using MLlib. SparkR uses R-based `SparkContext` and
    R scripts as tasks and then uses JNI and pipes to executed processes to communicate
    between Java-based Spark clusters and R scripts.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkR`是一个R包，提供了一个轻量级的前端，用于从R中使用Apache Spark。SparkR提供了一个分布式数据框架实现，支持诸如选择、过滤、聚合等操作。SparkR还支持使用MLlib进行分布式机器学习。SparkR使用基于R的`SparkContext`和R脚本作为任务，然后使用JNI和管道来执行进程，以在基于Java的Spark集群和R脚本之间进行通信。'
- en: R must be installed on all worker nodes running the Spark executors.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行Spark执行程序的所有工作节点上必须安装R。
- en: 'The following is how SparkR works by communicating between Java processed and
    R scripts:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是SparkR通过在Java进程和R脚本之间进行通信的方式：
- en: '![](img/00289.jpeg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00289.jpeg)'
- en: Summary
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We explored the evolution of the Hadoop and MapReduce frameworks and discussed
    YARN, HDFS concepts, HDFS Reads and Writes, and key features as well as challenges.
    Then, we discussed the evolution of Apache Spark, why Apache Spark was created
    in the first place, and the value it can bring to the challenges of big data analytics
    and processing.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了Hadoop和MapReduce框架的演变，并讨论了YARN、HDFS概念、HDFS读写、关键特性以及挑战。然后，我们讨论了Apache Spark的演变，为什么首次创建了Apache
    Spark，以及它可以为大数据分析和处理的挑战带来的价值。
- en: Finally, we also took a peek at the various components in Apache Spark, namely,
    Spark core, Spark SQL, Spark streaming, Spark GraphX, and Spark ML as well as
    PySpark and SparkR as a means of integrating Python and R language code with Apache
    Spark.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还瞥见了Apache Spark中的各种组件，即Spark核心、Spark SQL、Spark流处理、Spark GraphX和Spark ML，以及PySpark和SparkR作为将Python和R语言代码与Apache
    Spark集成的手段。
- en: Now that we have seen big data analytics, the space and the evolution of the
    Hadoop Distributed computing platform, and the eventual development of Apache
    Spark along with a high-level overview of how Apache Spark might solve some of
    the challenges, we are ready to start learning Spark and how to use it in our
    use cases.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了大数据分析、Hadoop分布式计算平台的空间和演变，以及Apache Spark的最终发展，以及Apache Spark如何解决一些挑战的高层概述，我们准备开始学习Spark以及如何在我们的用例中使用它。
- en: "In the next chapter, we will delve more deeply into Apache Spark and start\
    \ to look under the hood of how it all works in [Chapter 6\uFEFF](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c),\
    \ *Start Working with Spark - REPL and RDDs*."
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地了解Apache Spark，并开始深入了解它的工作原理，《第6章》*开始使用Spark - REPL和RDDs*。
