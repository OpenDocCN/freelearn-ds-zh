- en: An Introduction to Machine Learning Concepts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习概念简介
- en: Machine learning has become a commonplace topic in our day-to-day lives. The
    advancement in the field has been so dramatic that today, even cell phones incorporate
    advanced machine learning and artificial intelligence-related facilities, capable
    of responding and taking actions based on human instructions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习已经成为我们日常生活中的常见话题。该领域的进展非常迅速，今天，甚至连手机都集成了先进的机器学习和人工智能相关功能，能够根据人类指令作出响应并采取行动。
- en: A subject that was once limited to university classrooms has transformed into
    a full-fledged industry, pervading our daily lives in ways we could not have envisioned
    even just a few years ago.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经仅限于大学课堂的一个学科，已经转变为一个成熟的行业，渗透到我们日常生活中的方方面面，甚至是我们几年前无法想象的方式。
- en: The aim of this chapter is to introduce the reader to the underpinnings of machine
    learning and explain the concepts in simple, lucid terms that will help readers
    become familiar with the core ideas in the subject. We'll start off with a high-level
    overview of machine learning, and explain the different categories and how to
    distinguish them. We'll explain some of the salient concepts in machine learning,
    such as data pre-processing, feature engineering, and variable importance. The
    next chapter will go into more detail regarding individual algorithms and theoretical
    machine learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是介绍机器学习的基本原理，并用简单、清晰的术语解释这些概念，帮助读者熟悉该领域的核心思想。我们将从机器学习的高层概述开始，解释不同的分类及其区分方法。我们还将解释机器学习中的一些重要概念，如数据预处理、特征工程和变量重要性。下一章将深入探讨具体的算法和理论机器学习。
- en: We'll conclude with exercises that leverage real-world datasets to perform machine
    learning operations using R.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过利用真实世界的数据集来进行机器学习操作，使用R语言进行练习，最后作结。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What is machine learning?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: The popular emergence
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的兴起
- en: Machine learning, statistics, and artificial intelligence (AI)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习、统计学和人工智能（AI）
- en: Categories of machine learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的分类
- en: Core concepts in machine learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的核心概念
- en: Machine learning tutorial
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习教程
- en: What is machine learning?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: '**Machine learning** is not a new subject; it has existed in academia for well
    over 70 years as a formal discipline, but known by different names: statistics,
    and more generally mathematics, then **artificial intelligence** (**AI**), and
    today as machine learning. While the other related subject areas of statistics
    and AI are just as prevalent, machine learning has carved out a separate niche
    and become an independent discipline in and of itself.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习** 不是一门新学科；它作为一门正式学科在学术界已存在超过70年，但以不同的名字出现过：统计学，更广泛地说是数学，然后是**人工智能**（**AI**），今天则称为机器学习。虽然其他相关领域，如统计学和人工智能，也同样普及，机器学习却开辟了一个独立的领域，成为了一门独立的学科。'
- en: In simple terms, machine learning involves predicting future events based on
    historical data. We see it manifested in our day-to-day lives and indeed we employ,
    knowingly or otherwise, principles of machine learning on a daily basis.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，机器学习涉及基于历史数据预测未来事件。我们在日常生活中都能看到它的体现，实际上，我们每天都在无意识地或有意识地运用机器学习的原理。
- en: When we casually comment on whether a movie will succeed at the box office using
    our understanding of the popularity of the individuals in the lead roles, we are
    applying machine learning, albeit subconsciously. Our understanding of the characters
    in the lead roles has been shaped over years of watching movies where they appeared.
    And, when we make a determination of the success of a future movie featuring the
    same person, we are using historical information to make an assessment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们随便评论一部电影是否会在票房上成功，利用我们对主演人气的了解时，我们其实已经在应用机器学习，尽管这是潜意识的。我们对主演人物的了解，是通过多年来观看他们出演的电影逐渐形成的。而当我们判断一部未来上映的电影是否成功时，实际上是运用了历史信息来进行评估。
- en: As another example, if we had data on temperature, humidity, and precipitation
    (rain) over a period of say, 12 months, can we use that information to predict
    whether it will rain today, given information on temperature and humidity?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，如果我们有关于温度、湿度和降水量（雨量）的数据，假设这些数据涵盖了12个月的时间，我们是否可以利用这些信息预测今天是否会下雨，仅凭温度和湿度的数据？
- en: This is akin to common regression problems found in statistics. But, machine
    learning involves applying a much higher level of rigor to the exercise to reach
    a conclusive decision based not only on theoretical calculations, but verification
    of the calculations hundreds or thousands of times using iterative methods before
    reaching a conclusion.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于统计学中的常见回归问题。然而，机器学习涉及在这一过程中应用更高层次的严谨性，通过不仅仅依赖理论计算，而且通过反复验证这些计算数百或数千次来得出结论，最终作出决定。
- en: It should be noted and clarified here that the term *machine learning* relates
    to algorithms or programs that are executed typically on a computing device whose
    objective it is to predict outcomes. The algorithms build mathematical models
    that can then be used to make predictions. It is a common misconception that machine
    learning quite literally refers to a *machine* that is *learning*. The actual
    implication, as just explained, is much less dramatic.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 需要在此指出并澄清的是，*机器学习*这一术语与通常在计算设备上执行的算法或程序有关，其目的是预测结果。这些算法构建数学模型，随后可以用来做出预测。一个常见的误解是，机器学习字面上指的是*机器*在*学习*。正如刚才所解释的，实际含义远没有那么戏剧化。
- en: The evolution of machine learning
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的演变
- en: The timeline of machine learning, as available on Wikipedia ([https://en.wikipedia.org/wiki/Timeline_of_machine_learning](https://en.wikipedia.org/wiki/Timeline_of_machine_learning)),
    provides a succinct and insightful overview of the evolution of the field. The
    roots can be traced back to as early as the mid-1700s, when Thomas Bayes presented
    his paper on *inverse probability* at the Royal Society of London. Inverse probability,
    more commonly known today as probability distribution, deals with the problem
    of determining the state of a system given a prior set of events. For example,
    if a box contained milk chocolate and white chocolate, you took out a few at random,
    and received two milk and three white chocolates, can we infer how many of each
    chocolate there are in the box?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的时间线，在维基百科上有提供（[https://en.wikipedia.org/wiki/Timeline_of_machine_learning](https://en.wikipedia.org/wiki/Timeline_of_machine_learning)），提供了该领域演变的简明而深刻的概述。其根源可以追溯到18世纪中期，当时托马斯·贝叶斯在伦敦皇家学会发表了关于*逆概率*的论文。逆概率，今天更常被称为概率分布，涉及到在给定一组先前事件的情况下，确定系统状态的问题。例如，如果一个盒子里有牛奶巧克力和白巧克力，你随机取出一些，结果得到了两块牛奶巧克力和三块白巧克力，那么我们能推测出盒子里各有多少块巧克力吗？
- en: In other words, what can we infer about the unknown given a few points of data
    with which we can postulate a formal theory? Bayes' work was developed further
    into Bayes' Theorem by Pierre-Simon Laplace in his text, *Théorie Analytique des
    Probabilités*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，给定一些数据点，我们可以推测出什么关于未知的内容，从而提出一个正式的理论？贝叶斯的工作进一步发展为贝叶斯定理，由皮埃尔-西蒙·拉普拉斯在他的著作《*概率论分析理论*》中完成。
- en: In the early 1900s, Andrey Markov's analysis of Pushkin's Poem, Eugeny Onegin,
    to determine the alliteration of consonants and vowels in Russian literature,
    led to the development of a technique known as Markov Chains, used today to model
    complex situations involving random events. Google's PageRank algorithm implements
    a form of Markov Chains.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪初，安德烈·马尔可夫对普希金的诗歌《叶甫盖尼·奥涅金》进行的分析，旨在确定俄语文学中辅音和元音的头韵现象，这导致了马尔可夫链技术的发展，今天被用来模拟涉及随机事件的复杂情况。谷歌的PageRank算法就是实现了一种形式的马尔可夫链。
- en: 'The first formal application of machine learning, or more generally, AI, and
    its eventual emergence as a discipline, should be attributed to Alan Turing. He
    developed the Turing Test - a way to determine whether a machine is intelligent
    enough to mimic human behavior. Turing presented this in his paper, *Computing
    Machinery and Intelligence*, which starts out with the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的首次正式应用，或者更广泛地说，人工智能（AI）的出现及其作为一门学科的形成，应归功于艾伦·图灵。他开发了图灵测试——一种用来确定机器是否足够智能以模仿人类行为的方法。图灵在他的论文《*计算机器与智能*》中提出了这一点，文章开头写道：
- en: I propose to consider the question, "Can machines think?" This should begin
    with definitions of the meaning of the terms "machine" and "think." The definitions
    might be framed so as to reflect so far as possible the normal use of the words,
    but this attitude is dangerous, If the meaning of the words "machine" and "think"
    are to be found by examining how they are commonly used it is difficult to escape
    the conclusion that the meaning and the answer to the question, "Can machines
    think?" is to be sought in a statistical survey such as a Gallup poll. But this
    is absurd. Instead of attempting such a definition I shall replace the question
    by another, which is closely related to it and is expressed in relatively unambiguous
    words.[...]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我提议考虑这个问题：“机器能思考吗？”这应该从“机器”和“思考”这两个术语的定义开始。定义可以尽量反映这些词语的常规用法，但这种态度是危险的。如果通过考察这些词语的常用意义来理解“机器”和“思考”的含义，几乎难以避免得出结论，认为“机器能思考吗？”这个问题的答案应通过诸如盖洛普民意调查之类的统计调查来寻找。然而，这显然是荒谬的。在无法定义的情况下，我将用另一个与之密切相关且表达相对明确的问题来替代该问题。[...]
- en: 'Later in the paper, Turing writes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在文章的后面，图灵写道：
- en: '*The original question, "Can machines think?" I believe to be too meaningless
    to deserve discussion. Nevertheless I believe that at the end of the century the
    use of words and general educated opinion will have altered so much that one will
    be able to speak of machines thinking without expecting to be contradicted. I
    believe further that no useful purpose is served by concealing these beliefs.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*“机器能思考吗？”这个原始问题，我认为过于没有意义，甚至不值得讨论。然而，我相信，在本世纪末，随着语言的使用和普遍的教育意见的变化，人们将能够谈论机器思考，而不必担心遭到反驳。我还相信，隐瞒这些信念并不会起到任何有用的作用。*'
- en: Turing's work on AI was followed by a series of seminal events in machine learning
    and AI. The first neural network was developed by Marvin Misky in 1951, Arthur
    Samuel began his work on the first machine learning programs that played checkers
    in 1952, and Rosenblatt invented the perceptron, a fundamental unit of neural
    networks, in 1957\. Pioneers such as Leo Breiman, Jerome Friedman, Vladimir Vapnik
    and Alexey Chervonenkis, Geoff Hinton, and YannLeCun made significant contributions
    through the late 1990s to bring machine learning into the limelight. We are greatly
    indebted to their work and contributions, which have made machine learning stand
    out as a distinct area of research today.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图灵在人工智能领域的工作之后，机器学习和人工智能领域发生了一系列具有开创性意义的事件。第一个神经网络由马文·明斯基于1951年开发，亚瑟·塞缪尔于1952年开始研究第一个能下跳棋的机器学习程序，而罗森布拉特在1957年发明了感知器——神经网络的基本单元。像李欧·布雷曼、杰罗姆·弗里德曼、弗拉基米尔·瓦普尼克、阿列克谢·切尔沃涅基斯、杰夫·辛顿和扬·勒昆等先驱者在1990年代末做出了重大贡献，使机器学习成为备受关注的研究领域。我们深感荣幸能借助他们的工作和贡献，使得机器学习如今成为一个独立且显著的研究领域。
- en: In 1997, IBM's Deep Blue beat Kasparov and it immediately became a worldwide
    sensation. The ability of a machine to beat the world's top chess champion was
    no ordinary achievement. The event gave some much-needed credibility to machine
    learning as a formidable contender for the intelligent machines that Turing envisaged.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 1997年，IBM的深蓝战胜了卡斯帕罗夫，并立即成为全球轰动的新闻。机器能够击败世界顶级象棋冠军，这可不是一项普通的成就。这个事件为机器学习赢得了必要的信誉，使其成为图灵设想中的智能机器的强有力竞争者。
- en: Factors that led to the success of machine learning
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习成功的因素
- en: 'Given machine learning, as a subject, has existed for many decades, it begs
    the question: why hadn''t it become as popular as it is today much sooner? Indeed,
    the theories of complex machine learning algorithms such as neural networks were
    well known by the late 1990s, and the foundation had been established well before
    that in the theoretical realm.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于机器学习作为一个学科已经存在了数十年，这就引出了一个问题：为什么它没有像今天这样早早地变得流行起来呢？事实上，到1990年代末，神经网络等复杂机器学习算法的理论已经广为人知，而且其基础在此之前的理论领域已奠定。
- en: 'There are a few factors that can be attributed to the success of machine learning:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些因素可以归因于机器学习的成功：
- en: '**The Internet**: The web played a critical role in democratizing information
    and connecting people in an unprecedented way. It made the exchange of information
    simple in a way that could not have been achieved through the pre-existing methods
    of print media communication. Not only did the web transform and revolutionize
    the dissemination of information, it also opened up new opportunities. Google''s
    PageRank, as mentioned earlier, was one of the first large-scale and highly visible
    successes in the application of statistical models to develop a highly successful
    web enterprise.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互联网**：网络在信息民主化和人们连接方面发挥了至关重要的作用。它使信息交换变得简单，而这种简单是通过以前的印刷媒体交流方式无法实现的。网络不仅改变并彻底革新了信息传播方式，还开辟了新的机会。正如之前提到的，谷歌的PageRank是应用统计模型开发成功的第一个大规模且高度可见的例子，它为创建一个高度成功的网络企业提供了支持。'
- en: '**Social media**: While the web provided a platform for communication, it lacked
    a level of flexibility akin to how people interacted with one another in the real
    world. There was a noticeable, but understated, and arguably unexplored gap. Tools
    such as IRC and Usenet were the precursors to social network websites such as
    Myspace, which was one of the first web-based platforms intended to create personal
    networks. By early-mid 2000, Facebook had emerged as the leader in social networking.
    These platforms provided a unique opportunity to leverage the Internet to collect
    data at an individual level. Each user left a trail of messages, ripe for collection
    and analysis using Natural Language Processing and other techniques.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交媒体**：虽然网络提供了一个交流的平台，但它缺乏一种类似人们在现实世界中互动的灵活性。存在一个明显但被低估的、可以说是未被充分探索的空白。IRC和Usenet等工具是社交网络网站的前身，例如Myspace，它是最早旨在创建个人网络的基于网络的平台之一。到2000年代中期，Facebook已经成为社交网络的领导者。这些平台为利用互联网收集个人层面的数据提供了独特的机会。每个用户都留下了一连串消息，适合通过自然语言处理等技术进行收集和分析。'
- en: '**Computing hardware**: Hardware used for computers developed at an exponential
    rate. Machine learning algorithms are inherently compute and resource intensive,
    that is, they require powerful CPUs, fast disks, and high memory depending on
    the size of data. The invention of new ways to store data on **solid state drives**
    (**SSDs**) was a leap from the erstwhile process of storing on spinning hard drives.
    Faster access meant that data could be delivered to the CPU at a much faster rate
    and reduce the I/O bottleneck that has traditionally been a weak area in computing.
    Faster CPUs meant it was possible to perform hundreds and thousands of iterations
    demanded by machine learning algorithms in a timely manner. Finally, the demand
    led to the reduction in prices for computing resources, allowing more people to
    be able to afford buying computing hardware that was prohibitively expensive.
    Algorithms existed, but the resources were finally able to execute them in a reasonable
    time and cost.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算硬件**：用于计算机的硬件以指数级的速度发展。机器学习算法本质上需要大量的计算和资源，也就是说，它们根据数据的大小需要强大的CPU、快速的硬盘和大量的内存。**固态硬盘**（**SSDs**）的发明是从传统的旋转硬盘存储过程的一次飞跃。更快的访问速度意味着数据可以以更快的速度传递给CPU，从而减少了传统计算中一直是瓶颈的I/O问题。更快的CPU使得能够按时执行机器学习算法要求的成百上千次迭代。最后，需求推动了计算资源价格的降低，使得更多人能够负担得起以前价格过高的计算硬件。虽然算法早已存在，但资源终于能够在合理的时间和成本内执行这些算法。'
- en: '**Programming languages and packages**: Communities such as R and Python developers
    seized the opportunity, and individuals started releasing packages that exposed
    their work to a broader community of programmers. In particular, packages that
    provided machine learning algorithms became immediately popular and inspired other
    practitioners to release their individual code repositories, making platforms
    such as R a truly global collaborative effort. Today there are over 10,000 packages
    in R, up from 2000 in 2010.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编程语言与包**：像R和Python这样的开发者社区抓住了机会，个人开始发布包，将他们的工作展现给更广泛的编程社区。特别是提供机器学习算法的包立即受到欢迎，并激励其他从业者发布他们的个人代码库，使得像R这样的平台成为一个真正的全球协作努力。今天，R中已经有超过10,000个包，而在2010年只有2000个。'
- en: Machine learning, statistics, and AI
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习、统计学与人工智能
- en: Machine learning is a term that has various synonyms - names that are the result
    of either marketing activities by corporates or just terms that have been used
    interchangeably. Although some may argue that they have different implications,
    they all ultimately refer to machine learning as a subject that facilitates the
    prediction of future events using historical information.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个具有多种同义词的术语——这些名字通常来源于企业的营销活动或只是被交替使用的术语。虽然有人可能认为它们有不同的含义，但它们最终都指向机器学习这一学科，它通过利用历史信息来预测未来事件。
- en: The commonly heard terms for machine learning include predictive analysis, predictive
    analytics, predictive modeling, and many others. As such, unless the entity that
    publishes material explaining their interpretation of the term and more specifically,
    how it is different, it is generally safe to assume that they are referring to
    machine learning. This is often a source of confusion among those new to the subject,
    largely due to the misuse and overuse of technical verbiage.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 常听到的与机器学习相关的术语包括预测分析、预测分析学、预测建模等。因此，除非发布材料的实体解释了他们对该术语的定义，并且更具体地说明其不同之处，否则通常可以假定它们指的是机器学习。这通常是那些初学者感到困惑的原因，主要是由于技术术语的误用和滥用。
- en: Statistics, on the other hand, is a distinct subject area that has been well
    known for over 200 years. The word is derived from the new Latin, *statisticum
    collegium* (council of state, in English) and the Italian word *statista*, meaning
    statesman or politician. You can visit [https://en.wikipedia.org/wiki/History_of_statistics#Etymology](https://en.wikipedia.org/wiki/History_of_statistics#Etymology)
    for more details on this topic. Machine learning implements various statistical
    models, which due to the rigor of computation involved, is distinct from the branch
    of classical statistics.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，统计学是一个独立的学科领域，已有200多年历史。该词源自新拉丁语，*statisticum collegium*（在英语中为国家议会）和意大利语单词*statista*，意为政治家或国会议员。你可以访问[https://en.wikipedia.org/wiki/History_of_statistics#Etymology](https://en.wikipedia.org/wiki/History_of_statistics#Etymology)查看更多关于这一主题的详细信息。机器学习实现了各种统计模型，由于涉及的计算复杂性，它不同于经典统计学分支。
- en: AI is also closely related to machine learning, but is a much broader subject.
    It can be loosely defined as systems (software/hardware) that, in the presence
    of uncertainties, can arrive at a concrete decision in (usually) a responsible
    and socially aware manner to attain a target end objective. In other words, AI
    aims to produce actions by systematically processing a situation that involves
    both known and unknown (latent) factors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能与机器学习密切相关，但它是一个更广泛的主题。它可以被宽泛地定义为在不确定性的情况下，能够以（通常）负责任且具有社会意识的方式做出具体决策的系统（软件/硬件），以实现预定的目标。换句话说，人工智能的目标是通过系统地处理既有已知因素又有未知（潜在）因素的情况来产生行动。
- en: AI conjures up images of smart and sometimes rebellious robots in sci-fi movies,
    just as much as it reminds us of intelligent systems, such as IBM Watson, that
    can parse complex questions and process ambiguous statements to find concrete
    answers.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能让人联想到科幻电影中那些聪明且有时叛逆的机器人，也让人想起像IBM Watson这样的智能系统，它们能够解析复杂问题并处理模糊陈述，找到具体答案。
- en: Machine learning shares some of the same traits - the step-wise development
    of a model using training data, and measuring accuracy using test data. However,
    AI has existed for many decades and has been a familiar household term. Institutions
    in the US, such as Carnegie Mellon University, have led the way in establishing
    key principles and guidelines of AI.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习具有一些相同的特征——通过训练数据逐步开发模型，并使用测试数据来衡量准确性。然而，人工智能已经存在了数十年，并且是家喻户晓的词汇。美国的机构，如卡内基梅隆大学，在建立人工智能的关键原则和指南方面处于领先地位。
- en: The online resources/articles on AI versus machine learning do not seem to provide
    any conclusive answers on how they differ. However, the syllabus of AI courses
    at universities makes the differences very obvious. You can learn more about AI
    at [https://cs.brown.edu/courses/csci1410/lectures.html](https://cs.brown.edu/courses/csci1410/lectures.html).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 关于人工智能与机器学习的在线资源/文章似乎无法提供如何区分它们的确切答案。然而，大学的人工智能课程大纲使这些差异非常明显。你可以在[https://cs.brown.edu/courses/csci1410/lectures.html](https://cs.brown.edu/courses/csci1410/lectures.html)学习更多关于人工智能的内容。
- en: 'AI refers to a vast array of study areas that involve:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能涉及广泛的研究领域，包括：
- en: '**Constrained optimization**: Reach best possible results given a set of constraints
    or limitations in a given situation'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**约束优化**：在给定约束或限制的情况下，达到最佳结果'
- en: '**Game theory**: For instance, zero-sum games, equilibrium, and others - taking
    a measured decision based on how the decision can affect future decisions and
    impact desired end goals'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**博弈论**：例如零和博弈、均衡等——根据决策如何影响未来决策并影响期望的最终目标来做出衡量决策'
- en: '**Uncertainty/Bayes'' rule**: Given prior information, what is the likelihood
    of this happening given something else has already happened'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不确定性/贝叶斯规则**：在给定先验信息的情况下，已发生某些事件后，发生其他事件的可能性'
- en: '**Planning**: Formulating a plan of action = a set of paths (graph) to tackle
    a situation/reach an end goal'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划**：制定行动计划 = 一组路径（图形）来应对某种情况/实现最终目标'
- en: '**Machine learning**: The implementation (realization) of the preceding goals
    by using algorithms that are designed to handle uncertainties and imitate human
    reasoning. The machine learning algorithms generally used for AI include:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习**：通过使用设计来处理不确定性并模仿人类推理的算法实现前述目标。通常用于人工智能的机器学习算法包括：'
- en: Neural networks/deep learning (find hidden factors)
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络/深度学习（发现隐藏因素）
- en: Natural language processing (NLP) (understand context using tenor, linguistics,
    and such)
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）（使用语气、语言学等理解上下文）
- en: Visual object recognition
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉物体识别
- en: Probabilistic models (for example, Bayes' classifiers)
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率模型（例如，贝叶斯分类器）
- en: Markov decision processes (decisions for random events, for example, gambling)
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（针对随机事件的决策，例如赌博）
- en: Various other machine learning Algorithms (clustering, SVM)
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种其他机器学习算法（聚类、支持向量机）
- en: '**Sociology**: A study of how machine learning decisions affect society and
    take remedial steps to correct issues'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社会学**：研究机器学习决策如何影响社会，并采取纠正措施来解决问题'
- en: Categories of machine learning
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的分类
- en: Arthur Samuel coined the term **machine learning** in 1959 while at IBM. A popular
    definition of machine learning is due to Arthur, who, it is believed, called machine
    learning *a field of computer science that gives computers the ability to learn
    without being explicitly programmed*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 亚瑟·塞缪尔（Arthur Samuel）在1959年于IBM时创造了**机器学习**这一术语。机器学习的一个流行定义来自亚瑟，他被认为将机器学习称为*一门让计算机无需明确编程便能学习的计算机科学领域*。
- en: Tom Mitchell, in 1998, added a more specific definition to machine learning
    and called it a, study of algorithms that improve their performance P at some
    task T with experience E.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 汤姆·米切尔（Tom Mitchell）在1998年为机器学习添加了一个更具体的定义，称其为：研究那些通过经验E改进在某个任务T上表现P的算法。
- en: A simple explanation would help to illustrate this concept. By now, most of
    us are familiar with the concept of spam in emails. Most email accounts also contain
    a separate folder known as **Junk**, **Spam**, or a related term. A cursory check
    of the folders will usually indicate the presence of several emails, many of which
    were presumably unsolicited and contain meaningless information.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的解释有助于阐明这一概念。到目前为止，我们大多数人都熟悉电子邮件中的垃圾邮件概念。大多数电子邮件帐户中也包含一个名为**垃圾邮件**、**Spam**或相关术语的单独文件夹。简单检查这些文件夹通常会发现几封电子邮件，其中许多显然是未经请求的，并且包含无意义的信息。
- en: The mere task of categorizing emails as spam and moving them to a folder involves
    the application of machine learning. Andrew Ng highlighted this elegantly in his
    popular MOOC course on machine learning.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 单纯将电子邮件分类为垃圾邮件并将其移到文件夹的任务，涉及机器学习的应用。安德鲁·吴在他流行的机器学习MOOC课程中优雅地强调了这一点。
- en: 'In Mitchell''s terms, the spam classification process involves:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 用米切尔的术语来说，垃圾邮件分类过程包括：
- en: '**Task T**: Classifying emails as spam/not spam'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务 T**：将电子邮件分类为垃圾邮件/非垃圾邮件'
- en: '**Performance P**: Number of emails accurately identified as spam'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表现P**：准确识别为垃圾邮件的电子邮件数量'
- en: '**Experience E**: The model is provided emails that have been marked as spam/not
    spam and uses that information to determine whether a new email is spam or not'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经验E**：模型通过提供标记为垃圾邮件/非垃圾邮件的电子邮件，并利用这些信息来判断新电子邮件是否为垃圾邮件'
- en: 'Broadly speaking, there are two distinct types of machine learning:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上讲，机器学习有两种截然不同的类型：
- en: Supervised machine learning
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有监督的机器学习
- en: Unsupervised machine learning
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督机器学习
- en: We shall discuss them in turn here.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此依次讨论它们。
- en: Supervised and unsupervised machine learning
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有监督与无监督机器学习
- en: Let us start with supervised machine learning first.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先从有监督机器学习开始。
- en: Supervised machine learning
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有监督的机器学习
- en: '**Supervised machine learning** refers to machine learning exercises that involve
    predicting outcomes with labelled data. Labelled data simply refers to the fact
    that the dataset we are using to make the predictions (as well as the outcome
    we will predict) has a definite value (irrespective of what it is). For instance,
    classifying emails as spam or not spam, predicting temperature, and identifying
    faces from images are all examples of supervised machine learning.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**是指涉及通过标签数据来预测结果的机器学习练习。标签数据简单来说就是指我们用于进行预测的数据集（以及我们将要预测的结果）有一个明确的值（无论是什么）。例如，将电子邮件分类为垃圾邮件或非垃圾邮件、预测温度、识别图像中的人脸等，都是监督学习的例子。'
- en: Vehicle Mileage, Number Recognition and other examples
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 车辆里程、数字识别和其他例子
- en: Given a dataset containing information on miles per gallon, number of cylinders,
    and such of various cars, can we predict what the value for miles per gallon would
    be if we only had the other values available?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含每加仑英里数、气缸数等各种汽车信息的数据集，我们能否预测如果只知道其他值的情况下，每加仑英里数的值会是多少？
- en: 'In this case, our outcome is `mpg` and we are using the other variables of
    `cyl` (Cylinders), `hp` (Horsepower), `gear` (number of gears), and others to
    build a model that can then be applied against a dataset where the values for
    mpg are marked as `MISSING`. The model reads the information in these columns
    in the first five rows of the data and, based on that information, predicts what
    the value for `mpg` would be in the other rows, as shown in the following image:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的结果是`mpg`，而我们使用其他变量如`cyl`（气缸数）、`hp`（马力）、`gear`（档位数）等来构建一个模型，该模型可以应用于一个数据集，其中`mpg`的值被标记为`MISSING`。该模型读取数据中前五行的这些列的信息，并根据这些信息预测其他行中`mpg`的值，如下图所示：
- en: '![](img/5c1b160c-2b40-4223-9d7c-b9fb43c86b0d.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c1b160c-2b40-4223-9d7c-b9fb43c86b0d.png)'
- en: 'The reason this is considered supervised is that in the course of building
    our machine learning model, we provided the model with information on what the
    outcome was. Other examples include:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以将其视为监督学习，是因为在构建机器学习模型的过程中，我们为模型提供了关于结果的信息。其他的例子包括：
- en: '**Recognizing letters and numbers**: In such cases, the input to the model
    are the images, say of letters and numbers, and the outcome is the alpha-numeric
    value shown on the image. Once the model is built, it can then be used against
    pictures to recognize and predict what numbers are shown in the picture. A simple
    example, but very powerful. Imagine if you were given 100,000 images of houses
    with house numbers. The manual way of identifying the house numbers would be to
    go through each image individually and write down the numbers. A machine learning
    model allows us to completely automate the entire operation. Instead of having
    to manually go through individual images, you could simply run the model against
    the images and get the results in a very short amount of time.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别字母和数字**：在这种情况下，模型的输入是字母和数字的图像，输出是图像上显示的字母数字值。一旦模型构建完成，就可以将其应用于图片，识别并预测图片中显示的数字。这是一个简单的例子，但非常强大。想象一下，如果你得到了100,000张带有房屋编号的房子图像，人工识别房号的方法就是逐个查看每张图像并写下房号。机器学习模型使我们能够完全自动化整个操作。你不需要逐个手动查看图像，而只需将模型应用于这些图像，就能在很短的时间内得到结果。'
- en: '**Self-driving autonomous cars**: The input to the algorithms are images where
    the objects in the image have been identified, for example, person, street sign,
    car, trees, shops, and other elements. The algorithm *learns* to recognize and
    differentiate among different elements once a sufficient number of images have
    been shown and thereafter given an unlabeled image, that is, an image where the
    objects have not been identified is able to recognize them individually. To be
    fair, this is a highly simplified explanation of a very complex topic, but the
    overall principle is the same.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动驾驶汽车**：算法的输入是已经识别出图像中物体的图像，例如人、路标、汽车、树木、商店和其他元素。一旦展示了足够数量的图像，算法*学习*识别并区分不同的元素，之后即便给出一张未标记的图像（即物体没有被识别的图像），也能单独识别这些物体。公平地说，这是对一个非常复杂话题的高度简化的解释，但总体原理是一样的。'
- en: 'MNIST Dataset used for number recognition:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 用于数字识别的MNIST数据集：
- en: '![](img/18749693-441f-4359-8c23-45af6249b0c1.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18749693-441f-4359-8c23-45af6249b0c1.png)'
- en: Unsupervised machine learning
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: '**Unsupervised machine learning** involves datasets that do not have labeled
    outcomes. Taking the example of predicting mpg values for cars, in an unsupervised
    exercise, our dataset would have looked as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督机器学习**涉及没有标签结果的数据集。以预测汽车的mpg值为例，在无监督的练习中，我们的数据集可能是这样的：'
- en: '![](img/22338df9-3af6-4217-a4b9-be7b4556b7db.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22338df9-3af6-4217-a4b9-be7b4556b7db.png)'
- en: If all the outcomes are *missing*, it would be impossible to know what the values
    might have been. Recall that the primary premise of machine learning is to use
    historical information to make predictions on datasets whose outcome is not known.
    But, if the historical information itself does not have any identified outcomes,
    then it would not be possible to build a model. Without knowing any other information,
    the values of mpg in the table could be all 0 or all 100; it is not possible to
    tell, as we do not have any data point that will help lead us to the value.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有结果都是*缺失*的，那么就无法知道这些值可能是什么。回想一下，机器学习的基本前提是利用历史信息来对结果未知的数据集进行预测。但如果历史信息本身没有任何已标识的结果，那么就无法构建模型。没有其他信息可用的情况下，表格中的mpg值可能全是0，也可能全是100；我们无法判断，因为没有任何数据点能帮助我们得出这些值。
- en: This is where *unsupervised* machine learning gets applied. In this type of
    machine learning, we are not trying to predict outcomes. Rather, we are trying
    to determine which items are most similar to one another.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*无监督*机器学习应用的地方。在这种类型的机器学习中，我们并不是试图预测结果，而是试图确定哪些项目彼此最为相似。
- en: A common name for such an exercise is *clustering*, that is, we are attempting
    to find *clusters* or groups of records that are most similar to one another.
    Where can we use this information and what are some examples of unsupervised learning?
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这种练习的常见名称是*聚类*，也就是说，我们试图找到*聚类*或最为相似的记录组。我们可以在什么地方使用这些信息，以及无监督学习的例子有哪些？
- en: 'There are various news aggregators on the web - sites that do not themselves
    publish information, but collect information from other news sources. One such
    aggregator is Google News. If, say, we had to search for information on the last
    images taken by the satellite Cassini of Saturn, we could do a simple search for
    the phrase on Google News [https://news.google.com/news/?gl=US&amp;ned=us&amp;hl=en](https://news.google.com/news/?gl=US&ned=us&hl=en).
    An example is shown here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 网上有各种新闻聚合器——这些网站本身不发布信息，而是收集来自其他新闻源的信息。一个这样的聚合器是谷歌新闻。如果我们需要搜索关于卫星卡西尼拍摄的最后一批土星图像的信息，我们可以在谷歌新闻上进行简单搜索，使用如下短语：[https://news.google.com/news/?gl=US&amp;ned=us&amp;hl=en](https://news.google.com/news/?gl=US&ned=us&hl=en)。这里展示了一个示例：
- en: '![](img/4ad124ec-c378-451b-a50d-c4da5aab8847.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ad124ec-c378-451b-a50d-c4da5aab8847.png)'
- en: Notice that there is a link for View all at the bottom of the news articles.
    Clicking the link will take you to a page with all the other related news articles.
    Surely, Google didn't manually classify the articles as belonging to the specific
    search term. In fact, Google doesn't know in advance what the user will search
    for. The search term could have well been *images of Saturn rings from space*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在新闻文章的底部有一个“查看全部”链接。点击该链接将带你到一个页面，列出所有相关的新闻文章。当然，谷歌并没有手动将这些文章分类为属于特定的搜索词。实际上，谷歌并不知道用户会搜索什么。搜索词可能是*来自太空的土星环图像*。
- en: So, how does Google know which articles belong to a specific search term? The
    answer lies in the application of clustering or principles of unsupervised learning.
    Unsupervised learning examines the attributes of a specific dataset in order to
    determine which articles are most similar to one another. To do this, the algorithm
    doesn't even need to know the contextual background.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，谷歌是如何知道哪些文章属于特定搜索词的呢？答案在于聚类或无监督学习原理的应用。无监督学习通过检查特定数据集的属性，来确定哪些文章彼此最为相似。为了做到这一点，算法甚至不需要了解上下文背景。
- en: Suppose you were given two sets of books with no covers, a set of books on gardening
    and a set of books on computer programming. Although you may not know the title
    of the book, it would be fairly easy to distinguish books on computers from books
    on gardening. One set of books would have an overwhelming number of terms related
    to computing, while the other would have an overwhelming number of terms related
    to plants. To make the distinction that there were two distinct categories of
    books would not be difficult just by virtue of the images in the books, even for
    a reader who, let's assume, is not aware of either computers or gardening.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你被给出了两套没有封面的书，一套是关于园艺的书，另一套是关于计算机编程的书。虽然你可能不知道书的标题，但要区分计算机书籍和园艺书籍却相当容易。一套书会有大量与计算机相关的术语，而另一套书则会有大量与植物相关的术语。通过书中的图片，就能很容易地区分这两类书籍，即使是一个假设不懂计算机或园艺的读者。
- en: Other examples of unsupervised machine learning include detection of malignant
    and non-malignant tumors, and gene sequencing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其他非监督学习的例子包括恶性肿瘤和良性肿瘤的检测，基因测序等。
- en: Subdividing supervised machine learning
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 细分监督学习
- en: 'Supervised machine learning can be further subdivided into exercises that involve
    either of the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习可以进一步细分为涉及以下任何一个的练习：
- en: '**Classification**'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**'
- en: '**Regression**'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**'
- en: The concepts are quite straightforward.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念相当简单。
- en: Classification involves a machine learning task that has a discrete outcome
    - a **categorical** outcome. All **nouns** are categorical variables, such as
    fruits, trees, color, and true/false.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 分类涉及一个具有离散结果的机器学习任务——一个**类别**结果。所有的**名词**都是类别变量，如水果、树木、颜色和真/假。
- en: The outcome variables in classification exercises are also known as **discrete
    or categorical variables**.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 分类练习中的结果变量也被称为**离散变量或类别变量**。
- en: 'Some examples include:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一些例子包括：
- en: Identifying the fruit given size, weight, and shape
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据大小、重量和形状识别水果
- en: Identifying numbers given a set of images of numbers (as shown in the earlier
    chapter)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据一组数字图像识别数字（如前章所示）
- en: Identifying objects on the streets
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别街道上的物体
- en: Identifying playing cards as diamonds, spades, hearts and clubs
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别扑克牌的花色，如方块、黑桃、红桃和梅花
- en: Identifying the class rank of a student based on the student's grade
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据学生的成绩识别学生的班级排名
- en: The last one might not seem obvious, but a rank, that is, 1^(st), 2^(nd), 3^(rd)
    denotes a fixed category. A student could rank, say, 1^(st) or 2^(nd), but not
    have a rank of 1.5!
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个可能看起来不太明显，但排名，即1^(st)、2^(nd)、3^(rd)，表示一个固定的类别。一个学生可能排名第1或第2，但不能有第1.5名！
- en: 'Images of some atypical classification examples are shown below:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一些典型分类例子的图片如下所示：
- en: '| ![](img/06771d1b-3810-4a9b-a9d2-5f5f6c48be27.png)Classification of different
    types of fruits | ![](img/02f51c3d-2e4f-46dc-b332-3cbac84c3bfe.png)Classification
    of playing cards: diamonds, spades, hearts, and clubs |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/06771d1b-3810-4a9b-a9d2-5f5f6c48be27.png)不同类型水果的分类 | ![](img/02f51c3d-2e4f-46dc-b332-3cbac84c3bfe.png)扑克牌分类：方块、黑桃、红桃和梅花
    |'
- en: '**Regression**, on the other hand, involves calculating numeric outcomes. Any
    outcome on which you can perform numeric operations, such as addition, subtraction,
    multiplication, and division, would constitute a regression problem.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归**，另一方面，涉及计算数值结果。任何可以进行数值运算（如加法、减法、乘法和除法）的结果都构成回归问题。'
- en: 'Examples of regression include:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 回归的例子包括：
- en: Predicting daily temperature
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测每日温度
- en: Calculating stock prices
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算股票价格
- en: Predicting the sales price of residential properties and others
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测住宅物业销售价格等
- en: Images of some atypical regression examples are shown below. In both the cases,
    we are dealing with quantitative numeric data that is continuous. Hence, the outcome
    variables of regression are also known as **quantitative or continuous variables**.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一些典型回归例子的图片如下所示。在这两种情况下，我们处理的是连续的定量数值数据。因此，回归的结果变量也被称为**定量变量或连续变量**。
- en: '| ![](img/10902bef-e6e1-49ef-879d-212ac46fbea9.png)Calculating house prices
    | ![](img/40718ef7-1629-42bc-868a-c3e136387bf9.png)Calculating stock prices using
    other market data |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/10902bef-e6e1-49ef-879d-212ac46fbea9.png)计算房价 | ![](img/40718ef7-1629-42bc-868a-c3e136387bf9.png)使用其他市场数据计算股票价格
    |'
- en: Note that the concepts of classification or regression do not as such apply
    to unsupervised learning. Since there are no labels in unsupervised learning,
    there is no discrete classification or regression in the strict sense. That said,
    since unsupervised learning categories data into clusters, objects in a cluster
    are often said to belong to the same class (as other objects in the same cluster).
    This is akin to classification, except that it is created after-the-fact and no
    classes existed prior to the objects being classified into individual clusters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，分类或回归的概念不适用于无监督学习。由于无监督学习没有标签，因此严格意义上没有离散的分类或回归。然而，由于无监督学习将数据分成多个簇，簇中的对象通常被认为属于同一类（与同一簇中的其他对象一样）。这类似于分类，只不过是事后创建的，而在对象被分类到各个簇之前并没有存在类。
- en: Common terminologies in machine learning
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的常见术语
- en: In machine learning, you'll often hear the terms features, predictors, and dependent
    variables. They are all one and the same. They all refer to the variables that
    are used to predict an outcome. In our previous example of cars, the variables
    **cyl** (Cylinder), **hp** (Horsepower), **wt** (Weight), and **gear** (Gear)
    are the predictors and **mpg** (Miles Per Gallon) is the outcome.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，你会经常听到特征、预测变量和因变量这些术语。它们其实是一样的，都指代用于预测结果的变量。在我们之前的汽车示例中，**cyl**（气缸数）、**hp**（马力）、**wt**（重量）和**gear**（齿轮数）是预测变量，而**mpg**（每加仑英里数）是结果变量。
- en: 'In simpler terms, taking the example of a spreadsheet, the names of the columns
    are, in essence, known as features, predictors, and dependent variables. As an
    example, if we were given a dataset of toll booth charges and were tasked with
    predicting the amount charged based on the time of day and other factors, a hypothetical
    example could be as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单地说，拿电子表格的例子来说，列名本质上就是特征、预测变量和因变量。举个例子，如果我们得到一个收费亭费用的数据集，任务是根据时间和其他因素预测收费金额，一个假设的示例如下：
- en: '![](img/3fbda53b-25e6-41f7-996f-4c992c3bbee5.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fbda53b-25e6-41f7-996f-4c992c3bbee5.png)'
- en: In this spreadsheet, the columns **date**, **time**, **agency**, **type**, **prepaid**,
    and **rate** are the features or predictors, whereas, the column **amount** is
    our outcome or dependent variable (what we are predicting).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个电子表格中，列 **date**（日期）、**time**（时间）、**agency**（机构）、**type**（类型）、**prepaid**（预付）、**rate**（费率）是特征或预测变量，而列
    **amount**（金额）是我们的结果或因变量（我们要预测的内容）。
- en: The value of amount *depends* on the value of the other variables (which are
    thus known as *independent variables*).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 数值的大小*依赖*于其他变量的值（因此这些变量被称为*独立变量*）。
- en: Simple equations also reflect the obvious distinction, for example, in an equation,
    *y = a + b + c*, the **left hand side** (**LHS**) is the dependent/outcome variable
    and *a*, *b* and *c* are the features/predictors.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的方程式也反映了显著的区别，例如在方程式 *y = a + b + c* 中，**左侧**（**LHS**）是因变量/结果变量，而 *a*、*b*
    和 *c* 是特征/预测变量。
- en: 'In summary:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：
- en: '![](img/09eb5136-5bcd-42a3-960d-e0cefa6602b0.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09eb5136-5bcd-42a3-960d-e0cefa6602b0.png)'
- en: The core concepts in machine learning
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的核心概念
- en: There are many important concepts in machine learning; we'll go over some of
    the more common topics. Machine learning involves a multi-step process that starts
    with data acquisition, data mining, and eventually leads to building the predictive
    models.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中有许多重要的概念，我们将讨论一些常见的主题。机器学习涉及一个多步骤的过程，首先是数据采集、数据挖掘，最终建立预测模型。
- en: 'The key aspects of the model-building process involve:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型的关键方面包括：
- en: '**Data pre-processing**: Pre-processing and feature selection (for example,
    centering and scaling, class imbalances, and variable importance)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据预处理**：预处理和特征选择（例如，居中和缩放、类别不平衡以及变量重要性）'
- en: '**Train, test splits and cross-validation**:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集、测试集划分和交叉验证**：'
- en: Creating the training set (say, 80 percent of the data)
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建训练集（例如，80%的数据）
- en: Creating the test set (~ 20 percent of the data)
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建测试集（大约占数据的20%）
- en: Performing cross-validation
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行交叉验证
- en: '**Create model, get predictions**:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建模型，获取预测结果**：'
- en: Which algorithms should you try?
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该尝试哪些算法？
- en: What accuracy measures are you trying to optimize?
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望优化哪些准确性指标？
- en: What tuning parameters should you use?
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该使用哪些调参参数？
- en: Data management steps in machine learning
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的数据管理步骤
- en: Pre-processing, or more generally processing the data, is an integral part of
    most machine learning exercises. A dataset that you start out with is seldom going
    to be in the exact format against which you'll be building your machine learning
    models; it will invariably require a fair amount of cleansing in the majority
    of cases. In fact, data cleansing is often the most time-consuming part of the
    entire process. In this section, we will briefly highlight a few of the top data
    processing steps that you may encounter in practice.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理，或更广泛地说，数据处理，是大多数机器学习任务中不可或缺的一部分。你开始时的数据集很少会以你将要构建机器学习模型的精确格式存在；在大多数情况下，它不可避免地需要经过相当数量的清洗。事实上，数据清洗通常是整个过程最耗时的部分。在本节中，我们将简要介绍一些在实践中你可能遇到的主要数据处理步骤。
- en: Pre-processing and feature selection techniques
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理和特征选择技术
- en: '**Data pre-processing**, as the name implies, involves curating the data to
    make it suitable for machine learning exercises. There are various methods for
    pre-processing and a few of the more common ones have been illustrated here.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据预处理**，顾名思义，涉及整理数据，使其适合机器学习任务。有多种预处理方法，以下展示了几种较常见的方法。'
- en: Note that data pre-processing should be performed as part of the cross-validation
    step, that is, pre-processing should not be done *before the fact*, but rather
    during the model-building process. This will be explained in more detail afterward.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据预处理应作为交叉验证步骤的一部分执行，也就是说，预处理不应在*事前*进行，而应在模型构建过程中进行。这将在后续详细解释。
- en: Centering and scaling
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 中心化和缩放
- en: Applying center and scale function on numeric columns is often done in order
    to standardize data and remove the effect of large variations in the magnitude
    or differences of numbers. You may have encountered this in college or university
    courses where students would be graded on a standardized basis, or a curve.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对数值列应用中心化和缩放函数通常是为了标准化数据，并消除数值幅度或差异较大时的影响。你可能在大学或课程中遇到过这种情况，在这种情况下，学生的成绩会基于标准化或曲线来评分。
- en: For instance, say an exam paper was unusually difficult and half of all the
    students in a class of 10 students received scores below 60 - the passing rate
    set for the course. The professor can either a) make a determination that 50%
    of the students should re-take the course, or b) standardize the scores to find
    how students performed relative to one another.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，假设一张试卷异常难，10名学生中有一半的成绩低于60——课程设定的及格线。教授可以选择a）决定50%的学生需要重新修习这门课程，或者b）将成绩标准化，查看学生之间的相对表现。
- en: 'Say the class scores were:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 假设班级成绩为：
- en: 45,66,66,55,55,52,61,64,65,49
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 45,66,66,55,55,52,61,64,65,49
- en: With the passing score set at 60, this implies that the students who scored
    45, 55, 55, 52 and 49 will not successfully complete the course.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 设定及格分数为60，这意味着得分为45、55、55、52和49的学生将无法顺利完成课程。
- en: 'However, this might not be a truly accurate representation of their relative
    merits. The professor may alternatively choose to instead use a center-and-scale
    method, commonly known as standardization, which involves:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能并不能准确反映他们的相对优劣。教授可以选择使用一种中心化和缩放的方法，通常称为标准化，其过程如下：
- en: Finding the mean of all the scores
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算所有成绩的均值
- en: Subtracting the mean from the scores
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从成绩中减去均值
- en: Dividing the result by the standard deviation of all the scores
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将结果除以所有成绩的标准差
- en: The operation is illustrated below for reference.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是该操作的示意图，供参考。
- en: 'The mean of the scores is 57.8\. Hence, subtracting 57.8 from each of the numbers
    produce the numbers shown in the second row. But, we are not done yet. We need
    to divide the numbers by the *standard deviation* of the scores to get the final
    standardized values:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 成绩的均值为57.8。因此，将57.8从每个数字中减去，得到第二行中的数字。但这还不是最终结果。我们需要将这些数字除以成绩的*标准差*，以获得最终的标准化值：
- en: '![](img/4b4b2a16-5582-41f7-84df-f219eec1c0e6.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b4b2a16-5582-41f7-84df-f219eec1c0e6.png)'
- en: Dividing by the **SD** (**standard deviation**) shows that there were only two
    students whose scores were below one standard deviation across the range of all
    the test scores. Hence, instead of five students who do not complete the course
    successfully based on the raw numbers, we can narrow it down to only two students.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 除以**标准差**（**SD**）后，显示只有两名学生的得分低于所有测试成绩范围内的一个标准差。因此，基于原始分数，原本应该有五名学生无法成功完成课程，但我们可以将范围缩小到只有两名学生。
- en: Although this is a truly simple operation, it is not hard to see that it is
    very effective in smoothing out large variations in data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这只是一个非常简单的操作，但不难看出，它在平滑数据中大幅波动方面非常有效。
- en: 'Centering and scaling can be performed very easily in R using the scale command
    as shown here:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下`scale`命令在R中轻松地执行居中和缩放操作：
- en: '[PRE0]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The near-zero variance function
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 近零方差函数
- en: 'The near-zero variance, available in the `nearZeroVar` function in the `R package,
    caret`, is used to identify variables that have little or no variance. Consider
    a set of 10,000 numbers with only three distinct values. Such a variable may add
    very little value to an algorithm. In order to use the `nearZeroVar` function,
    first install the R package, caret, in RStudio (which we had set up [Chapter 3](5ca02405-8ab4-4274-8611-af003aab7c9f.xhtml), *The
    Analytics Toolkit*. The exact code to replicate the effect of using `nearZeroVar`
    is shown here:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`nearZeroVar`函数（位于`R`包`caret`中）用于识别方差几乎为零或完全没有方差的变量。考虑一个包含10,000个数字的集合，其中只有三个不同的值。这样的变量可能对算法的贡献非常小。为了使用`nearZeroVar`函数，首先在RStudio中安装`caret`包（我们在[第3章](5ca02405-8ab4-4274-8611-af003aab7c9f.xhtml)中已进行设置，*分析工具包*）。使用`nearZeroVar`的效果的确切代码如下所示：'
- en: '[PRE1]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As the example shows, the function was able to correctly detect the variable
    that met the criteria.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如示例所示，该函数能够正确检测满足标准的变量。
- en: Removing correlated variables
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除相关变量
- en: Correlated variables can produce results that over-emphasize the contribution
    of the variables. In regression exercises, this has the effect of increasing the
    value of R^2, and does not accurately represent the actual performance of the
    model. Although many classes of machine learning algorithms are resistant to the
    effects of correlated variables, it deserves some mention as it is a common topic
    in the discipline.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 相关变量可能会产生过度强调变量贡献的结果。在回归分析中，这会导致R^2值的增加，但并不能准确表示模型的实际表现。尽管许多类型的机器学习算法能够抵抗相关变量的影响，但它仍然值得提及，因为它是该学科中的一个常见话题。
- en: The premise of removing such variables is related to the fact that redundant
    variables do not add incremental value to a model. For instance, if a dataset
    contained height in inches and height in meters, these variables would have a
    near exact correlation of 1, and using one of them is just as good as using the
    other. Practical exercises that involve variables that we cannot judge intuitively,
    using methods of removing correlated variables, can greatly help in simplifying
    the model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 移除这些变量的前提与冗余变量不会为模型增添增量价值的事实相关。例如，如果数据集中同时包含英寸和米为单位的身高，这些变量之间的相关性几乎为1，使用其中一个变量和使用另一个变量一样好。涉及我们无法直观判断的变量的实践练习，通过使用移除相关变量的方法，可以大大帮助简化模型。
- en: The following example illustrates the process of removing correlated variables.
    The dataset, **Pima Indians Diabetes**, contains vital statistics about the diet
    of Pima Indians and an outcome variable called `diabetes`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例说明了移除相关变量的过程。数据集**皮马印第安糖尿病**包含关于皮马印第安人饮食的基本统计信息和一个名为`diabetes`的结果变量。
- en: 'During the course of the examples in successive chapters, we will refer to
    this dataset often. A high level overview of the meaning of the different columns
    in the dataset is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节示例中，我们将频繁引用此数据集。数据集不同列的高层次概述如下：
- en: '[PRE2]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We are interested in finding out if any of the variables, apart from diabetes
    (which is our outcome variable) are correlated. If so, it may be useful to remove
    the redundant variables.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望了解除了糖尿病（作为我们的结果变量）之外，是否有其他变量是相关的。如果是这样，移除冗余变量可能会很有帮助。
- en: 'Install the packages `mlbench` and `corrplot` in RStudio and execute the commands
    as shown here:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在RStudio中安装`mlbench`和`corrplot`包，并执行如下命令：
- en: '[PRE3]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The command will produce a plot as shown here using the `corrplot` package
    from [http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram](http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将使用来自[http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram](http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram)的`corrplot`包生成如下图：
- en: '| ![](img/edccd408-6cf1-49de-b18d-a77282d496a8.png) | >![](img/81bbdc2b-2af2-4adb-93c0-b35330a18bb5.png)
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/edccd408-6cf1-49de-b18d-a77282d496a8.png) | >![](img/81bbdc2b-2af2-4adb-93c0-b35330a18bb5.png)
    |'
- en: The darker the shade, the higher the correlation. In this case, it shows that
    age and pregnancy have a relatively high correlation. We can find the exact values
    by using `method="number"` as shown. You can also view the plot at [http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram](http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 阴影越深，相关性越高。在这种情况下，它显示年龄和怀孕有较高的相关性。我们可以通过使用`method="number"`来查看具体数值，如所示。你还可以在[http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram](http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram)查看图形。
- en: 'We can also use functions such as the following to directly find the correlated
    variables without plotting the correlograms:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用以下函数直接找到相关的变量，而无需绘制相关图：
- en: '[PRE4]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Other common data transformations
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他常见的数据转换
- en: Several other data transformations are available and applicable to different
    situations. A summary of these transformations can be found at the documentation
    site for the `caret` package under **Pre-Processing** at [https://topepo.github.io/caret/pre-processing.html](https://topepo.github.io/caret/pre-processing.html).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他几种数据转换方法适用于不同的情况。这些转换的总结可以在`caret`包的文档网站上找到，位置在**预处理**部分：[https://topepo.github.io/caret/pre-processing.html](https://topepo.github.io/caret/pre-processing.html)。
- en: 'The options available in the pre-process function of caret can be found from
    its help section, by running the command `?preProcess` in RStudio. The code for
    it is given here:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在RStudio中运行命令`?preProcess`来查看`caret`的预处理函数中提供的选项。它的代码如下：
- en: '[PRE5]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Data sampling
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据采样
- en: You may encounter datasets that have a high level of imbalanced outcome classes.
    For instance, if you were working with a dataset on a rare disease, with your
    outcome variable being true or false, due to the rarity of the occurrence, you
    may find that the number of observations marked as false (that is, the person
    did not have the rare disease) is much higher than the number of observations
    marked as true (that is, the person did have the rare disease).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到那些具有高度不平衡结果类别的数据集。例如，如果你在处理关于罕见疾病的数据集，且结果变量为“真”或“假”，由于疾病的罕见性，你可能会发现标记为“假”（即该人没有罕见疾病）的观察值远高于标记为“真”（即该人患有罕见疾病）的观察值。
- en: Machine learning algorithms attempt to maximize performance, which in many cases
    could be the accuracy of the predictions. Say, in a sample of 1000 records, only
    10 are marked as true and the rest of the `990` observations are false.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法试图最大化性能，在许多情况下，这可能是预测准确性。假设在1000条记录中，只有10条标记为真，剩余的`990`条观察值为假。
- en: 'If someone were to randomly assign *all* observations as false, the accuracy
    rate would be:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人随机将*所有*观察值都标记为假，那么准确率将是：
- en: '[PRE6]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: But, the objective of the exercise was to find the *individuals who had the
    rare disease*. We are already well aware that due to the nature of the disease,
    most individuals will not belong to the category.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这项工作的目标是找到*患有罕见疾病的个体*。我们已经很清楚，由于该疾病的性质，大多数个体将不属于这一类别。
- en: Data sampling, in essence, is the process of *maximizing machine learning metrics
    such as specificity, sensitivity, precision, recall, and kappa*. These will be
    discussed at a later stage, but for the purposes of this section, we'll show some
    ways by which you can *sample* the data so as to produce a more evenly balanced
    dataset.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 数据采样本质上是*最大化机器学习度量指标，如特异性、敏感性、精确度、召回率和kappa值*的过程。稍后将讨论这些指标，但为了本节的目的，我们将展示一些方法，通过这些方法你可以*采样*数据，从而生成一个更加平衡的数据集。
- en: The R package, `caret`, includes several helpful functions to create a balanced
    distribution of the classes from an imbalanced dataset.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: R包`caret`包括几个有用的函数，可以从不平衡数据集中创建类别的平衡分布。
- en: In these cases, we need to re-sample the data to get a better distribution of
    the classes in order to build a more effective model.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，我们需要重新采样数据，以便获得更好的类别分布，从而构建一个更有效的模型。
- en: 'Some of the general methods include:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的方法包括：
- en: '**Up-sample**: Increase instances of the class with lesser examples'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上采样**：增加具有较少样本的类别的实例'
- en: '**Down-sample**: Reduce the instances of the class with higher examples'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下采样**：减少具有较多样本的类别的实例'
- en: '**Create synthetic examples** (for example, **SMOTE** (**Synthetic Minority
    Oversampling TechniquE**))'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建合成样本**（例如，**SMOTE**（**合成少数类过采样技术**））'
- en: Random oversampling (for example, (**ROSE**) **Randomly OverSampling Examples**)
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机过采样（例如，(**ROSE**) **随机过采样示例**）
- en: 'We will create a simulated dataset using the same data from the prior example
    where 95% of the rows will be marked as negative:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前示例中的相同数据来创建一个模拟数据集，其中95%的行将被标记为负面：
- en: '[PRE7]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The **SMOTE** (**Synthetic Minority Over-sampling TechniquE**) is a third method
    that, instead of plain vanilla up-/down-sampling, creates synthetic records from
    the nearest neighbors of the minority class. In our simulated dataset, it is obvious
    that `neg` is the minority class, that is, the class with the lowest number of
    occurrences.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SMOTE**（**合成少数类过采样技术**）是第三种方法，它不同于简单的上采样/下采样，而是通过少数类的最近邻创建合成记录。在我们的模拟数据集中，显然`neg`是少数类，也就是出现次数最少的类别。'
- en: 'The help file on the SMOTE function explains the concept succinctly:'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SMOTE函数的帮助文件简要地解释了这个概念：
- en: Unbalanced classification problems cause problems to many learning algorithms.
    These problems are characterized by the uneven proportion of cases that are available
    for each class of the problem.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡的分类问题对许多学习算法造成了困扰。这些问题的特点是每个类别可用的样本比例不均。
- en: 'SMOTE (Chawla et al., 2002) is a well-known algorithm to fight this problem.
    The general idea of this method is to artificially generate new examples of the
    minority class using the nearest neighbors of these cases. Furthermore, the majority
    class examples are also under-sampled, leading to a more balanced dataset:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE（Chawla 等人，2002）是一个著名的算法，用来解决这个问题。该方法的一般思想是通过使用这些案例的最近邻，人工生成少数类的新样本。此外，多数类样本也会被下采样，从而得到一个更加平衡的数据集：
- en: '[PRE8]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**ROSE (Randomly OverSampling Examples)**, the final method in this section,
    is available via the ROSE package in R. Like SMOTE, it is a method for generating
    synthetic samples. The help file for ROSE states the high-level use of the function
    as follows:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROSE（随机过采样示例）**，本节中的最后一种方法，可以通过R中的ROSE包使用。像SMOTE一样，它是一种生成合成样本的方法。ROSE的帮助文件中简要说明了该函数的高级使用方法：'
- en: Generation of synthetic data by Randomly Over Sampling Examples creates a sample
    of synthetic data by enlarging the features space of minority and majority class
    examples. Operationally, the new examples are drawn from a conditional kernel
    density estimate of the two classes, as described in Menardi and Torelli (2013).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 通过随机过采样示例生成的合成数据通过扩大少数类和多数类样本的特征空间来创建合成数据样本。在操作上，这些新样本是从这两类的条件核密度估计中抽取的，正如Menardi和Torelli（2013）中所描述的那样。
- en: '[PRE9]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Data imputation
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据插补
- en: Sometimes, your data may have missing values. This could be due to errors in
    the data collection process, genuinely missing data, or any other reason, with
    the net result being that the information is not available. Real world examples
    of missing data can be found in surveys where the respondent did not answer a
    specific question on the survey.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你的数据可能会有缺失值。这可能是由于数据收集过程中的错误、真正缺失的数据或其他任何原因，结果就是信息不可用。在现实中，缺失数据的例子可以在调查中找到，例如受访者没有回答调查中的某个特定问题。
- en: You may have a dataset of, say, 1,000 records and 20 columns of which a certain
    column has 100 missing values. You may choose to discard this column altogether,
    but that also means discarding 90 percent of the information. You still have 19
    other columns that have complete data. Another option is to simply exclude the
    column, but that means you cannot leverage the benefit afforded by the data that
    is available in the respective column.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能有一个包含1,000条记录和20个列的数据集，其中某一列有100个缺失值。你可以选择完全丢弃这一列，但这也意味着丢弃了90%的信息。你仍然可以使用其他19列中有完整数据的列。另一种选择是直接排除该列，但这意味着你无法利用该列中可用数据所带来的好处。
- en: Several methods exist for data imputation, that is, the process of filling in
    missing data. We do not know what the exact values are, but by looking at the
    other entries in the table, we may be able to make an educated and systematic
    assessment of what the values might be.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 存在几种数据插补方法，即填补缺失数据的过程。我们不知道确切的值是什么，但通过查看表中的其他条目，我们可能能够对这些值做出有根据且系统的评估。
- en: 'Some of the common methods in data imputation involve:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 数据插补中的一些常见方法包括：
- en: '**Mean, median, mode imputation**: Substituting the missing values using the
    mean, median, or mode value for the column. This, however, has the disadvantage
    of increasing the correlation among the variables that are imputed, which might
    not be desirable for multivariate analysis.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均值、中位数、众数插补**：使用列的均值、中位数或众数值来替代缺失值。然而，这种方法的缺点是会增加被插补变量之间的相关性，这在多元分析中可能并不理想。'
- en: '**K-nearest neighbors imputation**: kNN imputation is a process of using a
    machine learning approach (nearest-neighbors) in order to impute missing values.
    It works by finding k records that are most similar to the one that has missing
    values and calculates the weighted average using Euclidean distance relative to
    k records.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K近邻插补**：kNN 插补是一种使用机器学习方法（最近邻）来插补缺失值的过程。其通过找到与缺失值记录最相似的 k 条记录，并使用与 k 条记录的欧氏距离加权平均来进行计算。'
- en: '**Imputation using regression models**: Regression methods use standard regression
    methods in R to predict the value of the missing variables. However, as noted
    in the respective section on Regression-based imputation on Wikipedia [https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression](https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression),
    the problem (with regression imputation) is that the imputed data do not have
    an error term included in their estimation. Thus, the estimates fit perfectly
    along the regression line without any residual variance. This causes relationships
    to be over identified and suggests greater precision in the imputed values than
    is warranted.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归模型插补**：回归方法使用 R 中的标准回归方法来预测缺失变量的值。然而，正如维基百科中关于回归插补的相关章节所指出的[https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression](https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression)，回归插补的问题在于插补数据的估计中没有包括误差项。因此，估算值完美地符合回归线，没有任何残差方差。这导致关系被过度识别，并暗示插补值的精度高于实际所需。'
- en: '**Hot-deck imputation**: Another technique for filling missing values with
    observations from the dataset itself. This method, although very prevalent, does
    have a limitation in that, by assigning say, a single value, to a large range
    of missing values, it could add a significant bias in the observations and can
    produce misleading results.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**热 deck 插补**：另一种使用数据集本身的观测值来填补缺失值的技术。尽管这种方法非常流行，但它的局限性在于，如果将一个值赋给大量的缺失值，可能会在观测值中引入显著的偏差，并产生误导性的结果。'
- en: A short example has been provided here to demonstrate how imputation can be
    done using kNN Imputation. We simulate missing data by changing a large number
    of values to NA in the `PimaIndiansDiabetes` dataset.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供了一个简短的示例，展示如何使用 kNN 插补进行插补。我们通过将 `PimaIndiansDiabetes` 数据集中的大量值更改为 NA 来模拟缺失数据。
- en: 'We make use of the following factors for the process:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个过程中使用以下因素：
- en: We use mean to fill in the NA values.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用均值来填补 NA 值。
- en: 'We use kNN imputation to fill in the missing values. We then compare how the
    two methods performed:'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 kNN 插补填补缺失值。然后，我们比较这两种方法的表现：
- en: '[PRE10]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We get the output as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的输出如下：
- en: '![](img/0dec61a7-716d-4f85-9804-2290ca5419ea.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0dec61a7-716d-4f85-9804-2290ca5419ea.png)'
- en: '[PRE11]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/6ad62a66-066c-4ce9-b69b-258304df178c.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ad62a66-066c-4ce9-b69b-258304df178c.png)'
- en: '[PRE12]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: While it may not represent a dramatic change, it's still better than using a
    naïve approach such as using simply a mean or constant value.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它可能不会代表一个显著的变化，但总比使用简单的均值或常数值等天真的方法要好。
- en: 'There are several packages in R for data imputation. A few prominent ones are
    as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: R 中有多个用于数据插补的包，以下是几个著名的包：
- en: '**Amelia II**: Missing information in time-series data'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amelia II**：时间序列数据中的缺失信息'
- en: '[https://gking.harvard.edu/amelia](https://gking.harvard.edu/amelia)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://gking.harvard.edu/amelia](https://gking.harvard.edu/amelia)'
- en: '**Hot-deck imputation with R package**: HotDeckImputation and hot.deck'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 R 包进行热 deck 插补**：HotDeckImputation 和 hot.deck'
- en: '[https://cran.r-project.org/web/packages/HotDeckImputation/](https://cran.r-project.org/web/packages/HotDeckImputation/)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cran.r-project.org/web/packages/HotDeckImputation/](https://cran.r-project.org/web/packages/HotDeckImputation/)'
- en: '[https://cran.r-project.org/web/packages/hot.deck/](https://cran.r-project.org/web/packages/hot.deck/)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cran.r-project.org/web/packages/hot.deck/](https://cran.r-project.org/web/packages/hot.deck/)'
- en: '**Multivariate imputation (by Chained Equations)**'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多元插补（链式方程法）**'
- en: '[https://cran.r-project.org/web/packages/mice/index.html](https://cran.r-project.org/web/packages/mice/index.html)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cran.r-project.org/web/packages/mice/index.html](https://cran.r-project.org/web/packages/mice/index.html)'
- en: '**Imputing values in a Bayesian framework with R package**: mi'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在贝叶斯框架中使用 R 包进行缺失值填充**：mi'
- en: '[https://cran.r-project.org/web/packages/mi/index.html](https://cran.r-project.org/web/packages/mi/index.html)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cran.r-project.org/web/packages/mi/index.html](https://cran.r-project.org/web/packages/mi/index.html)'
- en: The importance of variables
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变量的重要性
- en: During model-building exercises, datasets may have tens of variables. Not all
    of them may add value to the predictive model. It is not uncommon to reduce the
    dataset to include a subset of the variables and allow the machine learning programmer
    to devote more time toward fine-tuning the chosen variables and the model-building
    process. There is also a technical justification for reducing the number of variables
    in the dataset. Performing machine learning modeling on very large, that is, high
    dimensional datasets can be very compute-intensive, that is, it may require a
    significant amount of time, CPU, and RAM to perform the numerical operations.
    This not only makes the application of certain algorithms impractical, it also
    has the effect of causing unwarranted delays. Hence, the methodical selection
    of variables helps both in terms of analysis time as well as computational requirements
    for algorithmic analysis.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模过程中，数据集可能包含数十个变量。并不是所有的变量都会为预测模型增加价值。将数据集缩小到一个子集，允许机器学习程序员将更多的时间用于优化选定的变量和建模过程并不罕见。减少数据集中变量的数量也有技术上的理由。对非常大的数据集（即高维数据集）进行机器学习建模可能会非常耗费计算资源，即可能需要大量的时间、CPU和内存来执行数值运算。这不仅使某些算法的应用变得不切实际，还会导致不必要的延迟。因此，方法性的变量选择有助于分析时间和算法分析的计算需求。
- en: '**Variable selection** is also known as feature selection/attribute selection.
    Algorithms such as random forests and lasso regression implement variable selection
    as part of their algorithmic operations. But, variable selection can be done as
    a separate exercise.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**变量选择**也叫做特征选择/属性选择。像随机森林和套索回归这样的算法将变量选择作为其算法操作的一部分。但是，变量选择也可以作为一个独立的练习进行。'
- en: The R package, `caret`, provides a very simple-to-use and intuitive interface
    for variable selection. As we haven't yet discussed the modeling process, we will
    learn how to find the important variables, and in the next chapter delve deeper
    into the subject.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: R 包`caret`提供了一个非常简单易用且直观的界面用于变量选择。由于我们还没有讨论建模过程，我们将在这里学习如何找到重要变量，并在下一章深入探讨这个主题。
- en: We'll use a common, well-known algorithm called `RandomForest` that is used
    for building decision trees. The algorithm will be described in more detail in
    the next chapter, but the purpose of using it here is merely to show how variable
    selection can be performed. The example is illustrative of what the general process
    is.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个常见的、广为人知的算法，叫做`RandomForest`，它用于构建决策树。这个算法将在下一章中详细描述，但在这里使用它的目的是仅仅为了展示如何进行变量选择。这个例子说明了整个过程的一般步骤。
- en: 'We''ll re-use the dataset we have been working with, that is, the `PimaIndiansDiabetes`
    data from the `mlbench` package. We haven''t discussed the model training process
    yet, but it has been used here in order to derive the values for variable importance.
    The outcome variable in this case is diabetes and the other variables are used
    as the independent variables. In other words, can we predict if the person has
    diabetes using the data available:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重新使用之前使用过的数据集，即来自`mlbench`包的`PimaIndiansDiabetes`数据。我们还没有讨论模型训练过程，但在这里已经使用该数据集来推导变量重要性值。此案例中的结果变量是糖尿病，其他变量则作为自变量使用。换句话说，我们是否可以使用现有的数据预测某人是否患有糖尿病：
- en: '[PRE13]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output of the preceding code is as shown below. It indicates that glucose,
    mass and age were the variables that contributed the most towards creating the
    model (to predict diabetes)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下所示。它表明葡萄糖、体重和年龄是对创建模型（预测糖尿病）贡献最大的变量。
- en: '![](img/d6f247da-1244-4e39-b6e8-fd56c9607d9f.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6f247da-1244-4e39-b6e8-fd56c9607d9f.png)'
- en: The train, test splits, and cross-validation concepts
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练集、测试集拆分和交叉验证概念
- en: The train, test splits, and cross-validation sets are a fundamental concept
    in machine learning. This is one of the areas where a pure statistical approach
    differs materially from the machine learning approach. Whereas in a statistical
    modeling task, one may perform regressions, parametric/non-parametric tests, and
    apply other methods, in machine learning, the algorithmic approach is supplemented
    with an element of iterative assessment of the results being produced and subsequent
    improvisation of the model with each iteration.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集、测试集划分和交叉验证集是机器学习中的基本概念。这是纯统计方法与机器学习方法在本质上不同的一个领域。虽然在统计建模任务中，可能会进行回归、参数化/非参数化检验并应用其他方法，但在机器学习中，算法方法是通过对结果的反复评估与每次迭代中模型的即兴改进来补充的。
- en: Splitting the data into train and test sets
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集
- en: Every machine learning modeling exercise begins with the process of data cleansing,
    as discussed earlier. The next step is to split the data into a train and test
    set. This is usually done by randomly selecting rows from the data that will be
    used to create the model. The rows that weren't selected would then be used to
    test the final model.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习建模任务都从数据清洗过程开始，如前所述。下一步是将数据分为训练集和测试集。这通常通过从数据中随机选择行来创建模型，未被选择的行将用于测试最终模型。
- en: The usual split varies between 70-80 percent (training data versus test data).
    In an 80-20 split, 80% of the data would be used in order to create the model.
    The remaining 20% would be used to test the final model produced.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的数据集划分比例在70%到80%之间（训练数据与测试数据）。在80-20的划分中，80%的数据将用于创建模型，剩余的20%将用于测试最终生成的模型。
- en: 'We applied this in the earlier section, but we can revisit the code once again.
    The `createDataPartition` function was used with the parameter `p = 0.80` in order
    to split the data. The `training_index` variable holds the training indices (of
    the `dataset`, `diab`) that we will use:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的部分应用过这个方法，但我们可以再次回顾代码。`createDataPartition`函数与参数`p = 0.80`一起使用，以便对数据进行划分。`training_index`变量保存了我们将使用的训练集索引（来自`dataset`，`diab`）：
- en: '[PRE14]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We do not have to necessarily use the `createDataPartition` function and instead,
    a random sample created using simple R commands as shown here will suffice:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不一定非要使用`createDataPartition`函数，相反，使用简单的R命令创建的随机样本，如这里所示，也可以满足需求：
- en: '[PRE15]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The cross-validation parameter
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证参数
- en: Cross-validation takes the train-test split concept to the next stage. The aim
    of the machine learning exercise is, in essence, to find what set of model parameters
    will provide the best performance. A model parameter indicates the arguments that
    the function (the model) takes. For example, for a decision tree model, parameters
    may include the number of levels deep the model should be built, number of splits,
    and so on. If, say, there are *n* different parameters, each having *k* different
    values, the total number of parameters would be *k*^*n*. We generally select a
    fixed set of combinations for each of the parameters and could easily end with
    100-1000+ combinations. We will test the performance of the model (for example,
    accuracy in predicting the outcome correctly) for each of the parameters.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证将训练-测试集划分概念提升到了一个新的阶段。机器学习任务的目标，本质上是找到一组模型参数，从而提供最佳的性能。模型参数指的是函数（即模型）所接受的参数。例如，对于决策树模型，参数可能包括模型应构建的深度层数、划分数等。如果有*n*个不同的参数，每个参数有*k*个不同的值，那么总的参数组合数将是*k*^*n*。我们通常为每个参数选择固定的一组组合，可能最终有100到1000+种组合。我们将测试每个参数下模型的表现（例如，正确预测结果的准确性）。
- en: With a simple train-test split, say, if there were 500 combinations of parameters
    we had selected, we just need to run them against the training dataset and determine
    which one shows the optimal performance.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单的训练-测试集划分，比如，如果我们选择了500个参数组合，我们只需要将它们应用于训练数据集，并确定哪个组合表现最佳。
- en: With cross-validation, we further split the training set into smaller subsets,
    for example, three- or five-fold is commonly used. If there are three folds, that
    is, we split the training set into three subsets, we keep aside one fold, say,
    Fold 2, and create a model using a set of parameters using Folds 1 and 3\. We
    then test its accuracy against Fold 2\. This step is repeated several times, with
    each iteration representing a unique set of folds on which the training-test process
    is being executed and accuracy measures are collected. Eventually, we would arrive
    at an optimal combination by selecting the parameters that showed the best overall
    performance.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉验证时，我们将训练集进一步拆分成更小的子集，例如，常用三折或五折。如果使用三折，即将训练集拆分为三个子集，我们将其中一个折叠（如 Fold 2）留出，使用
    Fold 1 和 Fold 3 的数据集构建模型，并测试其在 Fold 2 上的准确性。这一步会重复多次，每次迭代代表在不同的折叠集合上执行训练-测试过程并收集准确度度量。最终，我们会通过选择表现最佳的模型参数组合来得到最优解。
- en: 'The standard approach can be summarized as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 标准方法可以总结如下：
- en: Create an 80-20 train-test split
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 80-20 的训练-测试拆分
- en: Execute your model(s) using different combinations of model parameters
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用不同的模型参数组合执行你的模型
- en: Select the model parameters that show the best overall performance and create
    the final model
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择显示最佳整体性能的模型参数并创建最终模型
- en: Apply the final model on the test set to see the results
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上应用最终模型以查看结果
- en: 'The cross-validation approach mandates that we should further split the training
    dataset into smaller subsets. These subsets are generally known as **folds** and
    collectively they are known as the **k-folds**, where *k* represents the number
    of splits:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证方法要求我们将训练数据集进一步拆分成更小的子集。这些子集通常称为**折叠（folds）**，统称为**k-折叠（k-folds）**，其中 *k*
    代表拆分的数量：
- en: Create an 80-20 train-test split
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 80-20 的训练-测试拆分
- en: Split the training set into k-folds, say, three folds
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集拆分成 k 折，例如，三折
- en: Set aside Fold 1 and build a model using Fold 2 and Fold 3
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Fold 1 留出，使用 Fold 2 和 Fold 3 构建模型
- en: Test your model performance on Fold 1 (for example, the percentage of accurate
    results)
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Fold 1 上测试你的模型性能（例如，准确结果的百分比）
- en: Set aside Fold 2 and build a model using Fold 1 and Fold 3
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Fold 2 留出，使用 Fold 1 和 Fold 3 构建模型
- en: Test your model performance on Fold 2
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Fold 2 上测试你的模型性能
- en: Set aside Fold 3 and build a model using Fold 1 and Fold 2
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Fold 3 留出，使用 Fold 1 和 Fold 2 构建模型
- en: Test your model performance on Fold 3
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Fold 3 上测试你的模型性能
- en: Take the average performance of the model across all three folds
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在所有三个折叠上的平均性能
- en: Repeat Step 1 for *each set of model parameters*
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 *每组模型参数* 重复步骤 1
- en: Select the model parameters that show the best overall performance and create
    the final model
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择显示最佳整体性能的模型参数并创建最终模型
- en: Apply the final model on the test set to see the results
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上应用最终模型以查看结果
- en: '![](img/c300d7bd-53ec-426a-9b9d-71455234a676.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c300d7bd-53ec-426a-9b9d-71455234a676.png)'
- en: This image illustrates the difference between using an approach without cross-validation
    and one with cross-validation. The cross-validation method is arguably more robust
    and involves a rigorous evaluation of the model. That said, it is often useful
    to attempt creating a model initially without cross-validation to get a sense
    of the kind of performance that may be expected. For example, if a model built
    with say 2-3 training-test splits shows a performance of say, 30% accuracy, it
    is unlikely that any other approach, including cross-validation would somehow
    make that 90%. In other words the standard approach helps to get a sense of the
    kind of performance that may be expected. As cross-validations can be quite compute-intensive
    and time consuming getting an initial feedback on performance helps in a preliminary
    analysis of the overall process.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图说明了使用不带交叉验证的方式与使用交叉验证的方式之间的区别。交叉验证方法可以说更为稳健，涉及对模型的严格评估。话虽如此，通常还是有必要先尝试不使用交叉验证来构建一个初步的模型，以便大致了解可能期望的性能。例如，如果一个使用
    2-3 次训练-测试拆分构建的模型显示的准确率为 30%，那么不太可能通过任何其他方法，包括交叉验证，使其准确率提升到 90%。换句话说，标准方法有助于大致了解可能期望的性能。由于交叉验证可能计算密集且耗时，获取初步的性能反馈有助于对整体过程进行初步分析。
- en: The caret package in R provides a very user-friendly approach to building models
    using cross-validation. Recall that data pre-processing must be passed or made
    an integral part of the cross-validation process. So, say, we had to center and
    scale the dataset and perform a five-fold cross-validation, all we would have
    to do is define the type of sampling we'd like to use in caret's `trainControl`
    function.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: R 中的 caret 包提供了一种非常用户友好的方法，通过交叉验证来构建模型。回想一下，数据预处理必须传递或作为交叉验证过程的一个组成部分。所以，如果我们需要对数据集进行中心化和标准化，并执行五折交叉验证，我们所需要做的就是在
    caret 的 `trainControl` 函数中定义我们希望使用的采样类型。
- en: Caret's webpage on `trainControl` provides a detailed overview of the functions
    along with worked-out examples at [https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning](https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Caret 的 `trainControl` 网页提供了关于功能的详细概述，并附有示例，[https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning](https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning)。
- en: 'We have used this approach in our earlier exercise where we built a model using
    `RandomForest` on the `PimaIndiansDiabetes` dataset. It is shown again here to
    indicate where the technique was used:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的练习中使用了这种方法，在该练习中我们使用 `RandomForest` 构建了一个模型，数据集是 `PimaIndiansDiabetes`。这里再次展示它是为了说明该技巧的应用：
- en: '[PRE16]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can get a more detailed explanation of `summaryFunction` from [https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf](https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 [https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf](https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf)
    获取关于 `summaryFunction` 的更详细解释。
- en: 'The `summaryFunction` argument is used to pass in a function that takes the
    observed and predicted values and estimates some measure of performance. Two such
    functions are already included in the package: `defaultSummary` and `twoClassSummary`.
    The latter will compute measures specific to two-class problems, such as the area
    under the ROC curve, the sensitivity and specificity. Since the ROC curve is based
    on the predicted class probabilities (which are not computed automatically), another
    option is required. The `classProbs = TRUE` option is used to include these calculations.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`summaryFunction` 参数用于传入一个函数，该函数接收观察值和预测值并估计某些性能度量。该包中已经包含了两个这样的函数：`defaultSummary`
    和 `twoClassSummary`。后者将计算特定于二分类问题的度量，如 ROC 曲线下面积、敏感性和特异性。由于 ROC 曲线是基于预测的类别概率（这些概率不会自动计算），因此需要另一个选项。`classProbs
    = TRUE` 选项用于包含这些计算。'
- en: Here is an explanation of `tuneLength` from the help file for the train function
    of `caret`.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自 `caret` 的 train 函数帮助文件中关于 `tuneLength` 的解释。
- en: '`tuneLength` is an integer denoting the amount of granularity in the tuning
    parameter grid. By default, this argument is the number of levels for each tuning
    parameter that should be generated by train. If `trainControl` has the option
    `search = random`, this is the maximum number of tuning parameter combinations
    that will be generated by the random search.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`tuneLength` 是一个整数，表示调优参数网格的粒度。默认情况下，此参数是每个调优参数应由 train 生成的级别数量。如果 `trainControl`
    具有选项 `search = random`，则这是随机搜索将生成的调优参数组合的最大数量。'
- en: Note that if this argument is given it must be named.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果提供了该参数，它必须具有名称。
- en: Creating the model
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建模型
- en: The final step after creating the model is to use the model against the test
    dataset to get the predictions. This is generally done using the `predict` function
    in R, with the first argument being the model that was created and the second
    argument being the dataset against which you'd like to get the predictions for.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 创建模型后的最终步骤是使用该模型对测试数据集进行预测。通常，这使用 R 中的 `predict` 函数来完成，第一个参数是已创建的模型，第二个参数是你想要预测的目标数据集。
- en: 'Taking our example of the `PimaIndiansDiabetes` dataset, after the model has
    been built, we can get the predictions on the test dataset as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们的 `PimaIndiansDiabetes` 数据集为例，在模型构建完成后，我们可以对测试数据集进行预测，如下所示：
- en: '[PRE17]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s check what the confusion matrix tells us:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查混淆矩阵告诉我们什么：
- en: '[PRE18]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The plot is as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 情节如下：
- en: '![](img/7d0a354a-6134-40f5-adc9-2a7a7932b0f5.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d0a354a-6134-40f5-adc9-2a7a7932b0f5.png)'
- en: '[PRE19]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We get the plot as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的图表如下：
- en: '![](img/08ce8ffe-fa37-435e-a852-bd95dd78a78d.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08ce8ffe-fa37-435e-a852-bd95dd78a78d.png)'
- en: 'Per the documentation of *`fourfoldplot`* [Source: [https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/fourfoldplot.html](https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/fourfoldplot.html)],
    an association (odds ratio different from 1) between the binary row and column
    variables is indicated by the tendency of diagonally opposite cells in one direction
    to differ in size from those in the other direction; color is used to show this
    direction. Confidence rings for the odds ratio allow a visual test of the null
    of no association; the rings for adjacent quadrants overlap if and only if the
    observed counts are consistent with the null hypothesis.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '根据*`fourfoldplot`*的文档 [来源: [https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/fourfoldplot.html](https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/fourfoldplot.html)]，二元行列变量之间的关联（比1的比值比）通过在一个方向上，斜对角的单元格与另一个方向的单元格在大小上的差异来表示；颜色用来显示这种方向。比值比的置信环允许对无关联的零假设进行视觉测试；相邻象限的环重叠当且仅当观察到的计数与零假设一致时。'
- en: Leveraging multicore processing in the model
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在模型中利用多核处理
- en: The exercise in the previous section is repeated here using the PimaIndianDiabetes2
    dataset instead. This dataset contains several missing values. As a result, we
    will first impute the missing values and then run the machine learning example.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中的练习在这里被重复，改用PimaIndianDiabetes2数据集。这个数据集包含一些缺失值。因此，我们将首先对缺失值进行插补，然后运行机器学习示例。
- en: The exercise has been repeated with some additional nuances, such as using multicore/parallel
    processing in order to make the cross-validations run faster.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 该练习已经进行了重复，并加入了一些额外的细节，比如使用多核/并行处理来加快交叉验证的运行速度。
- en: 'To leverage multicore processing, install the package `doMC` using the following
    code:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 要利用多核处理，使用以下代码安装`doMC`包：
- en: '[PRE20]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we will run the program as shown in the code here:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将按照这里的代码运行程序：
- en: '[PRE21]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Even with 650+ missing values, our model was able to achieve an accuracy of
    80%+.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有650多个缺失值，我们的模型也能达到80%以上的准确率。
- en: It can certainly be improved, but as a baseline, it shows the kind of performance
    that can be expected of machine learning models.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 它当然可以改进，但作为基准，它展示了机器学习模型可以达到的表现。
- en: 'In a case of a dichotomous outcome variable, a random guess would have had
    a 50% chance of being accurate. An accuracy of 80% is significantly higher than
    the accuracy we could have achieved using just guess-work:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个二分结果变量的情况下，随机猜测有50%的机会是准确的。80%的准确率显著高于仅凭猜测可能达到的准确率：
- en: '[PRE22]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The resulting plot is as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如下所示：
- en: '![](img/e3f98e5b-e43e-4a7e-bad9-3242aed7141b.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3f98e5b-e43e-4a7e-bad9-3242aed7141b.png)'
- en: '[PRE23]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The result is depicted in the following plot:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下图所示：
- en: '![](img/b2a4c384-7f1d-4f36-ac67-f15fd3df0f0d.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2a4c384-7f1d-4f36-ac67-f15fd3df0f0d.png)'
- en: Summary
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learnt about the basic fundamentals of Machine Learning,
    the different types such as Supervised and Unsupervised and major concepts such
    as data pre-processing, data imputation, managing imbalanced classes and other
    topics.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了机器学习的基本原理，包括监督学习和无监督学习等不同类型，以及数据预处理、数据插补、管理不平衡类和其他主题的主要概念。
- en: We also learnt about the key distinctions between terms that are being used
    interchangeably today, in particular the terms AI and Machine Learning. We learned
    that artificial intelligence deals with a vast array of topics, such as game theory,
    sociology, constrained optimizations, and machine learning; AI is much broader
    in scope relative to machine learning.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了今天常被交替使用的术语之间的关键区别，特别是“人工智能”和“机器学习”这两个术语。我们了解到，人工智能涉及广泛的主题，如博弈论、社会学、约束优化和机器学习；与机器学习相比，人工智能的范围要广泛得多。
- en: Machine learning facilitates AI; namely, machine learning algorithms are used
    to create systems that are *artificially intelligent*, but they differ in scope.
    A regression problem (finding the line of best fit given a set of points) can
    be considered a machine learning *algorithm*, but it is much less likely to be
    seen as an AI algorithm (conceptually, although it technically could be).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习促进了人工智能的实现；即，机器学习算法用于创建*人工智能*系统，但它们在范围上有所不同。回归问题（给定一组点，找出最佳拟合线）可以被视为一种机器学习*算法*，但它更不可能被看作是一种AI算法（概念上，尽管从技术上讲它也可以是）。
- en: In the next chapter, we will look at some of the other concepts in Machine Learning
    such as Bias, Variance and Regularization. We will also read about a few important
    algorithms and learn how to apply them using machine learning packages in R.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨机器学习中的其他一些概念，如偏差、方差和正则化。我们还将学习一些重要的算法，并了解如何使用 R 中的机器学习包应用它们。
