- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: NLP and Network Synergy
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP 和网络的协同作用
- en: In the previous chapters, we discussed **natural language processing** (**NLP**),
    network analysis, and the tools used in the Python programming language for both.
    We also discussed non-programmatic tools for doing network analysis.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了**自然语言处理**（**NLP**）、网络分析以及在 Python 编程语言中用于这两者的工具。我们还讨论了用于进行网络分析的非编程工具。
- en: In this chapter, we are going to put all of that knowledge to work. I hope to
    explain the power and insights that can be unveiled by combining NLP and network
    analysis, which is the theme of this book. In later chapters, we will continue
    with this theme, but we’ll also discuss other tools for working with NLP and networks,
    such as unsupervised and supervised machine learning. This chapter will demonstrate
    techniques for determining who or what a piece of text is talking about.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将把所有这些知识付诸实践。我希望能解释通过结合 NLP 和网络分析所揭示的强大功能和洞察力，这也是本书的主题。在后续章节中，我们将继续围绕这一主题展开讨论，同时还会涉及其他用于处理
    NLP 和网络的工具，如无监督和监督的机器学习。本章将展示确定文本片段所讲述的对象或内容的技术。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Why are we learning about NLP in a network book?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们在一本网络书中学习 NLP？
- en: Asking questions to tell a story
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出问题讲述故事
- en: Introducing web scraping
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍网页抓取
- en: Choosing between libraries, APIs, and source data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在库、API 和源数据之间进行选择
- en: Using the Natural Language Toolkit (NLTK) library for part-of-speech (PoS) tagging
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自然语言工具包（NLTK）库进行词性标注（PoS）
- en: Using spaCy for PoS tagging and named-entity recognition (NER)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 spaCy 进行词性标注和命名实体识别（NER）
- en: Converting entity lists into network data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将实体列表转换为网络数据
- en: Converting network data into networks
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将网络数据转换为网络
- en: Doing a network visualization spot check
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 做网络可视化的抽查
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will be using several different Python libraries. The `pip
    install` command is listed in each section for installing each library, so just
    follow along and do the installations as needed. If you run into installation
    problems, there is usually an answer on Stack Overflow. Google the error!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用几种不同的 Python 库。在每一节中都会列出`pip install`命令来安装每个库，因此只需要跟着做，按需进行安装。如果遇到安装问题，通常可以在
    Stack Overflow 上找到答案。用 Google 搜索错误信息！
- en: Before we start, I would like to explain one thing so that the number of libraries
    we are using doesn’t seem so overwhelming. It is the reason why we use each library
    that matters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我想解释一件事，这样我们使用的库的数量就不会显得那么令人不知所措。重要的是我们使用每个库的理由。
- en: 'For most of this book, we will be doing one of three things: network analysis,
    network visualization, or using network data for machine learning (also known
    as GraphML).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的大部分内容将会做三件事之一：网络分析、网络可视化，或使用网络数据进行机器学习（也称为 GraphML）。
- en: Anytime we have network data, we will be using `NetworkX` to use it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们处理网络数据时，我们都会使用`NetworkX`来操作它。
- en: Anytime we are doing analysis, we will probably be using `pandas`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们进行分析时，可能会使用`pandas`。
- en: 'The relationship looks like this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 关系看起来是这样的：
- en: '`NetworkX`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NetworkX`'
- en: '`pandas`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`scikit-network`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-network`'
- en: '`scikit-learn` and `Karate Club`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn` 和 `Karate Club`'
- en: Look at the first words. If I want to do network analysis, then I’ll be using
    `NetworkX` and `pandas`. If I want to do network visualization, I will be using
    `NetworkX` and `scikit-network`. If I want to do machine learning using network
    data, I will be using `NetworkX`, `scikit-learn`, and possibly `Karate Club`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 看看最开始的几个词。如果我要进行网络分析，我会使用`NetworkX`和`pandas`。如果我要做网络可视化，我会使用`NetworkX`和`scikit-network`。如果我要用网络数据做机器学习，我会使用`NetworkX`、`scikit-learn`，可能还会使用`Karate
    Club`。
- en: These are the core libraries that will be used in this chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是本章中将使用的核心库。
- en: Also, *you must keep the code for the* `draw_graph()` *function handy*, as you
    will be using it throughout this book. That code is a bit gnarly because, at the
    time of writing, it needs to be. Unfortunately, `NetworkX` is not great for network
    visualization, and `scikit-network` is not great for network construction or analysis,
    so I use both libraries together for visualization, and that works well. I am
    hoping this will not be the case for later editions of this book and that visualization
    will be improved and simplified over time.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，*你必须随时准备好`draw_graph()`* *函数的代码*，因为你将在本书中多次使用它。那段代码有点复杂，因为在写作时它确实需要这样做。不幸的是，`NetworkX`在网络可视化方面并不优秀，而`scikit-network`在网络构建或分析方面也不够强大，所以我将两者结合起来进行可视化，这效果很好。我希望这本书的后续版本中，网络可视化能够得到改进和简化。
- en: All of the necessary code can be found at [https://github.com/PacktPublishing/Network-Science-with-Python](https://github.com/PacktPublishing/Network-Science-with-Python).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所有必要的代码可以在[https://github.com/PacktPublishing/Network-Science-with-Python](https://github.com/PacktPublishing/Network-Science-with-Python)找到。
- en: Why are we learning about NLP in a network book?
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们在一本网络书中学习NLP？
- en: 'I briefly answered this question in the introduction of the first chapter,
    but it is worth repeating in more detail. Many people who work on text analysis
    are aware of sentiment analysis and text classification. **Text classification**
    is the ability to predict whether a piece of text can be classified as something.
    For example, let’s take this string:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我在第一章的介绍中简要回答了这个问题，但值得更详细地重复一遍。许多从事文本分析工作的人都知道情感分析和文本分类。**文本分类**是预测一段文本是否可以被归类为某种类型的能力。例如，我们来看这个字符串：
- en: “*How are you* *doing today?*”
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: “*你今天* *怎么样？*”
- en: What can we tell about this string? Is it a question or a statement? It’s a
    question. Who is being asked this question? You are. Is there a positive, negative,
    or neutral emotion tied to the question? It looks neutral to me. Let’s try another
    string.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能从这段字符串中得出什么结论？这是一个问题还是陈述？这是一个问题。这个问题是问谁的？是问你。这个问题中有积极、消极还是中立的情绪？我觉得它看起来是中立的。我们再试试另一段字符串。
- en: “*Jack and Jill went up the hill, but Jack fell down because he is* *an idiot.*”
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: “*Jack和Jill爬上了山坡，但Jack跌倒了，因为他是* *个傻瓜。*”
- en: This is a statement about Jack and Jill, and Jack is being called an idiot,
    which is not a very nice thing to say. However, it seems like the insult was written
    jokingly, so it is unclear whether the author was angry or laughing when it was
    written. I can confirm that I was laughing when I wrote it, so there is a positive
    emotion behind it, but text classification would struggle to pick up on that.
    Let’s try another.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于Jack和Jill的陈述，其中Jack被称为傻瓜，这并不是个很友善的说法。然而，这个侮辱似乎是开玩笑写的，因此不清楚作者写的时候是生气还是在笑。我可以确认，在写这句话时我是在笑的，所以其中包含了积极的情绪，但文本分类可能难以识别这一点。我们再试试另一个。
- en: “*I have never been so angry in my entire life as I am* *right now!*”
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: “*我这一生从未像现在这样愤怒过!* *真是愤怒到了极点!*”
- en: The author is expressing a very strong negative sentiment. Sentiment analysis
    and text classification would easily pick up on that.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作者表达了非常强烈的负面情绪。情感分析和文本分类很容易识别这一点。
- en: '**Sentiment analysis** is a set of techniques that use algorithms to automatically
    detect emotions that are embedded into a piece of text or transcribed recording.
    Text classification uses the same algorithms, but the goal is not to identify
    an emotion but rather a theme. For instance, text classification can detect whether
    there is abusive language in text.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**情感分析**是一种使用算法自动检测文本或转录记录中嵌入的情绪的技术集合。文本分类使用相同的算法，但其目标不是识别情绪，而是识别主题。例如，文本分类可以检测文本中是否存在侮辱性语言。'
- en: This is not a chapter about sentiment analysis or text classification. The purpose
    of this chapter is to explain how to automatically extract the entities (people,
    places, things, and more) that exist in text so that you can identify and study
    the social network that is being described in the text. However, text classification
    and sentiment analysis can be coupled with these techniques to provide additional
    context about social networks, or for building specialized social networks, such
    as friendship networks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是关于情感分析或文本分类的章节。本章节的目的是解释如何自动提取文本中存在的实体（人物、地点、事物等），以便识别和研究文本中描述的社交网络。然而，情感分析和文本分类可以与这些技术结合，为社交网络提供更多上下文，或者用于构建专门的社交网络，例如友谊网络。
- en: Asking questions to tell a story
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提出问题来讲述故事
- en: 'I approach my work from a storytelling perspective; I let my story dictate
    the work, not the other way around. For instance, if I am starting a project,
    I’ll ponder or even write down a series of who, what, where, when, why, and how
    questions:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我从讲故事的角度进行工作；我让我的故事决定工作，而不是反过来。例如，如果我开始一个项目，我会考虑，甚至写下关于谁、什么、哪里、何时、为什么和如何的一系列问题：
- en: What data do we have? Is it enough?
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有哪些数据？足够吗？
- en: Where do we get more?
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从哪里获得更多数据？
- en: How do we get more?
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何获得更多数据？
- en: What blockers prevent us from getting more?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么阻碍了我们获取更多数据？
- en: How often will we need more?
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们多久需要更多数据？
- en: 'But this is a different kind of project. We want to gain deeper insights into
    a piece of text than we might gather through reading. Even after reading a whole
    book, most people can’t memorize the relationships that are described in the text,
    and it would likely be a faulty recollection anyway. But we should have questions
    such as these:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是一个不同类型的项目。我们想要深入了解一篇文本的内容，而不仅仅是通过阅读来获得信息。即使读完一本书后，大多数人也无法记住文本中描述的关系，而且即使记得，也可能是错误的回忆。但我们应该有这样的疑问：
- en: Who is mentioned in the text?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文中提到了谁？
- en: Who do they know?
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们认识谁？
- en: Who are their adversaries?
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们的对手是谁？
- en: What are the themes of this text?
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这篇文章的主题是什么？
- en: What emotions are present?
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在哪些情感？
- en: What places are mentioned?
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文中提到了哪些地方？
- en: When did this take place?
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这件事发生在什么时候？
- en: It is important to come up with a solid set of questions before starting any
    kind of analysis. More questions will come to you the deeper you go.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始任何形式的分析之前，制定一套可靠的问题非常重要。深入分析时，你会遇到更多的问题。
- en: This chapter will give you the tools to automatically investigate all of these
    questions other than the ones about themes and adversaries. The knowledge from
    this chapter, coupled with an understanding of text classification and sentiment
    analysis, will give you the ability to answer *all* these questions. That is the
    “why” of this chapter. We want to automatically extract people, places, and maybe
    even some things that are mentioned. Most of the time, I only want people and
    places, as I want to study the social network.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将为你提供工具，自动调查所有这些问题，除了关于主题和对手的问题。本章的知识，再加上对文本分类和情感分析的理解，将使你能够回答*所有*这些问题。这就是本章的“为什么”。我们希望自动提取文中提到的人、地点，甚至可能还有一些事物。大多数时候，我只想要人物和地点，因为我想研究社交网络。
- en: However, it is important to explain that this is useful for accompanying text
    classification and sentiment analysis. For example, a positive Amazon or Yelp
    review is of little use if you do not know what is being reviewed.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重要的是要解释清楚，这对于辅助文本分类和情感分析非常有用。例如，如果你不知道正在评审的内容，正面的亚马逊或 Yelp 评论几乎没有什么意义。
- en: 'Before we can do any work to uncover the relationships that exist in text,
    we need to get some text. For practice, we have a few options. We could load it
    using a Python library such as NLTK, we could harvest it using the Twitter (or
    another social network) library, or we could scrape it ourselves. Even scraping
    has options, but I am only going to explain one way: using Python’s `BeautifulSoup`
    library. Just know that there are options, but I love the flexibility of `BeautifulSoup`.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行任何揭示文本中存在的关系的工作之前，我们需要获取一些文本。作为练习，我们有几个选择。我们可以使用像 NLTK 这样的 Python 库加载它，我们可以使用
    Twitter（或其他社交网络）库收集它，或者我们可以自己抓取它。即使是抓取也有不同的方式，但我只会解释一种方法：使用 Python 的`BeautifulSoup`库。只要知道有很多选择，但我喜欢`BeautifulSoup`的灵活性。
- en: In this chapter, the demos will use text scraped off of the internet, and you
    can make slight changes to the code for your own web scraping needs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，演示将使用从互联网上抓取的文本，你可以对代码做些小修改，以适应你自己的网页抓取需求。
- en: Introducing web scraping
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍网页抓取
- en: 'First, what even is **web scraping**, and who can do it? Anyone with any programming
    skill can do scraping using several different programming languages, but we will
    do this with Python. Web scraping is the action of harvesting content from web
    resources so that you may use the data in your products and software. You can
    use scraping to pull information that a website hasn’t exposed as a data feed
    or through an API. But one warning: do not scrape too aggressively; otherwise,
    you could knock down a web server through an accidental **denial-of-service**
    (**DoS**) attack. Just get what you need as often as you need it. Go slow. Don’t
    be greedy or selfish.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，什么是 **网页抓取**，谁能做呢？任何具有编程技能的人都可以使用几种不同的编程语言进行抓取，但我们将使用 Python。网页抓取是从网络资源中获取内容的行为，您可以将数据用于您的产品和软件。您可以使用抓取来获取网站没有通过数据馈送或
    API 暴露的信息。但有一个警告：不要抓取得过于激进，否则您可能会通过意外的 **拒绝服务攻击**（**DoS**）击垮一个网站服务器。只获取您需要的内容，按需获取。慢慢来，不要贪婪或自私。
- en: Introducing BeautifulSoup
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 BeautifulSoup
- en: '**BeautifulSoup** is a powerful Python library for scraping anything that you
    have access to online. I frequently use this to harvest story URLs from news websites,
    and then I scrape each of these URLs for their text content. I typically do not
    want the actual HTML, CSS, or JavaScript, so I render the web page and then scrape
    the content.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**BeautifulSoup** 是一个强大的 Python 库，用于抓取您可以访问的任何在线内容。我经常用它从新闻网站收集故事的 URL，然后我抓取这些
    URL 的文本内容。我通常不需要实际的 HTML、CSS 或 JavaScript，所以我会渲染网页，然后抓取内容。'
- en: '`BeautifulSoup` is an important Python library to know about if you are going
    to be doing web scraping. There are other options for web scraping with Python,
    but `BeautifulSoup` is commonly used.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`BeautifulSoup` 是一个重要的 Python 库，如果您打算进行网页抓取，了解它非常重要。虽然 Python 还有其他网页抓取选项，但
    `BeautifulSoup` 是最常用的。'
- en: There is no better way to explain `BeautifulSoup` than to see it in action,
    so let’s get to work!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么比看到 `BeautifulSoup` 在实际操作中的效果更能解释它的了，所以让我们开始吧！
- en: Loading and scraping data with BeautifulSoup
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BeautifulSoup 加载和抓取数据
- en: In this hands-on demonstration, we will see three different ways of loading
    and scraping data. They are all useful independently, but they can also be used
    together in ways.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实践演示中，我们将看到三种不同的加载和抓取数据的方式。它们各自独立都有用，但也可以以多种方式结合使用。
- en: First, the easiest approach is to use a library that can get exactly what you
    want in one shot, with minimal cleanup. Several libraries such as `pandas`, `Wikipedia`,
    and `NLTK` have ways to load data, so let’s start with them. As this book is primarily
    about extracting relationships from text and then analyzing them, we want text
    data. I will demonstrate a few approaches, and then describe the pros and cons
    of each.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，最简单的方法是使用一个库，一次性获取您想要的内容，且需要最少的清理。多个库，如 `pandas`、`Wikipedia` 和 `NLTK` 都有加载数据的方法，让我们从这些开始。由于本书主要讲解如何从文本中提取关系并进行分析，因此我们需要文本数据。我将演示几种方法，然后描述每种方法的优缺点。
- en: Python library – Wikipedia
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python 库 – Wikipedia
- en: 'There is a powerful library for pulling data from Wikipedia, also called `Wikipedia`.
    It can be installed by running the following command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个强大的库可以从维基百科中提取数据，叫做 `Wikipedia`。您可以通过运行以下命令安装它：
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once it has been installed, you may import it into your code like this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，您可以像这样将其导入到代码中：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once it has been imported, you have access to Wikipedia programmatically, allowing
    you to search for and use any content you are curious about. As we will be doing
    a lot of social network analysis while working through this book, let’s see what
    `Wikipedia` has on the subject:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦导入，您就可以通过编程方式访问维基百科，允许您搜索并使用任何您感兴趣的内容。由于在本书中我们将进行大量的社交网络分析，让我们看看 `Wikipedia`
    对该主题有什么内容：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The last line, `content[0:680]`, just shows what is inside the `content` string,
    up to the 680th character, which is the end of the sentence shown in the following
    code. There is a lot more past 680\. I chose to cut it off for this demonstration:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行，`content[0:680]`，仅显示 `content` 字符串中的内容，直到第 680 个字符，这是以下代码中展示的句子的结束部分。680
    之后还有很多内容。我选择在此演示中截断它：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With a few lines of code, we were able to pull text data directly from Wikipedia.
    Can we see what links exist on that Wikipedia page? Yes, we can!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过几行代码，我们能够直接从维基百科中提取文本数据。我们能看到那个维基百科页面上有什么链接吗？是的，我们可以！
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'I’m using a square bracket, `[`, to select only the first 10 entries from `links`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用方括号`[`，只选择 `links` 中的前 10 项：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If we are only in the A-links after choosing only the first 10 links, I think
    it’s safe to say that there are probably many more. There’s quite a lot to learn
    about social network analysis on Wikipedia! That was so simple! Let’s move on
    to another option for easy scraping: the `pandas` library.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仅选择前 10 个链接并只在 A 链接中，那么我认为可以安全地说，可能还有更多。维基百科上关于社交网络分析的内容非常丰富！这真的很简单！让我们继续尝试另一种便捷的抓取方法：`pandas`库。
- en: Python library – pandas
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python 库 – pandas
- en: If you are interested in using Python for data science, you will inevitably
    learn about and use `pandas`. This library is powerful and versatile for working
    with data. For this demonstration, I will use it for pulling tabular data from
    web pages, but it can do much more. If you are interested in data science, learn
    as much as you can about `pandas`, and get comfortable using it.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣使用 Python 进行数据科学，你将不可避免地学习和使用`pandas`。这个库在处理数据时非常强大和多功能。在这个演示中，我将用它从网页中提取表格数据，但它可以做更多的事情。如果你对数据科学感兴趣，尽可能多地学习关于`pandas`的知识，并熟悉它的使用。
- en: '`pandas` can be useful for pulling tabular data from web pages, but it is much
    less useful for raw text. If you want to load text from Wikipedia, you should
    use the `Wikipedia` library shown previously. If you want to pull text off other
    web pages, you should use the `Requests` and `BeautifulSoup` libraries together.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas`可以从网页中提取表格数据，但它对原始文本的处理能力较弱。如果你想从维基百科加载文本，应该使用之前提到的`Wikipedia`库。如果你想从其他网页提取文本，则应该同时使用`Requests`和`BeautifulSoup`库。'
- en: 'Let’s use `pandas` to pull some tabular data from Wikipedia. First, let’s try
    to scrape the same Wikipedia page using `pandas` and see what we get. If you have
    installed Jupyter on your computer, you likely already have `pandas` installed,
    so, let’s jump straight to the code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`pandas`从维基百科提取一些表格数据。首先，尝试使用`pandas`抓取同一个维基百科页面，看看我们能得到什么。如果你已经在电脑上安装了
    Jupyter，那么很可能你已经安装了`pandas`，所以让我们直接进入代码：
- en: 'Start by importing `pandas`:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先导入`pandas`库：
- en: '[PRE6]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we imported the `pandas` library and gave it a shorter name, and then
    used `Pandas` to read the same Wikipedia page on social network analysis. What
    did we get back? What does `type(data)` show? I would expect a `pandas` DataFrame.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们导入了`pandas`库并给它起了一个简短的名字，然后使用`pandas`读取了关于社交网络分析的同一维基百科页面。我们得到了什么？`type(data)`显示了什么？我预计会得到一个`pandas`
    DataFrame。
- en: 'Enter the following code. Just type `data` and run the code:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入以下代码。只需输入`data`并运行代码：
- en: '[PRE10]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You should see that we get a list back. In `pandas`, if you do a `read` operation,
    you will usually get a DataFrame back, so why did we get a list? This happened
    because there are multiple data tables on this page, so `pandas` has returned
    a Python list of all of the tables.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到我们得到了一个列表。在`pandas`中，如果你执行`read`操作，通常会得到一个 DataFrame，那么为什么我们得到了一个列表呢？这是因为该页面上有多个数据表，因此`pandas`返回了所有表格的
    Python 列表。
- en: 'Let’s check out the elements of the list:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一下列表的元素：
- en: '[PRE11]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This should give us a `pandas` DataFrame of the first table from a Wikipedia
    page:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会给我们一个来自维基百科页面的第一个表格的`pandas` DataFrame：
- en: '![Figure 4.1 – pandas DataFrame of the first element of Wikipedia data](img/B17105_04_001.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – pandas DataFrame 显示维基百科数据的第一个元素](img/B17105_04_001.jpg)'
- en: Figure 4.1 – pandas DataFrame of the first element of Wikipedia data
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – pandas DataFrame 显示维基百科数据的第一个元素
- en: This data is already looking a bit problematic. Why are we getting a bunch of
    text inside of a table? Why does the last row look like a bunch of code? What
    does the next table look like?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据看起来有点问题。为什么我们在表格中看到一堆文本？为什么最后一行看起来像是一堆代码？下一个表格是什么样子的呢？
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will give us a `pandas` DataFrame of the second table from the same Wikipedia
    page. Please be aware that Wikipedia pages are occasionally edited, so your results
    might be different:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个来自同一维基百科页面的第二个表格的`pandas` DataFrame。请注意，维基百科页面偶尔会被编辑，因此您的结果可能会有所不同：
- en: '![Figure 4.2 – pandas DataFrame of the second element of Wikipedia data](img/B17105_04_002.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – pandas DataFrame 显示维基百科数据的第二个元素](img/B17105_04_002.jpg)'
- en: Figure 4.2 – pandas DataFrame of the second element of Wikipedia data
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – pandas DataFrame 显示维基百科数据的第二个元素
- en: Gross. This looks even worse. I say that because it appears that we have some
    very short strings that look like web page sections. This doesn’t look very useful.
    Remember, `pandas` is great for loading tabular data, but not great on tables
    of text data, which Wikipedia uses. We can already see that this is less useful
    than the results we very easily captured using the Wikipedia library.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 糟糕。这看起来更糟。我之所以这么说，是因为似乎有一些非常短的字符串，看起来像是网页部分。这看起来不太有用。记住，`pandas`非常适合加载表格数据，但对于维基百科使用的文本数据表格并不适用。我们已经看到，这比我们通过维基百科库轻松捕获的数据更没用。
- en: 'Let’s try a page that has useful data in a table. This page contains tabular
    data about crime in Oregon:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试一个包含有用数据的页面。这个页面包含关于俄勒冈州犯罪的表格数据：
- en: '[PRE13]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will show us the last five rows of a `Pandas` DataFrame containing Oregon
    crime data:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这将展示一个包含俄勒冈州犯罪数据的`Pandas`数据框架中的最后五行：
- en: '![Figure 4.3 – Pandas DataFrame of tabular numeric Wikipedia data](img/B17105_04_003.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – Pandas数据框架中的维基百科表格数值数据](img/B17105_04_003.jpg)'
- en: Figure 4.3 – Pandas DataFrame of tabular numeric Wikipedia data
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – Pandas数据框架中的维基百科表格数值数据
- en: Wow, this looks like useful data. However, the data does not show anything more
    recent than 2009, which is quite a while ago. Maybe there is a better dataset
    out there that we should use instead. Regardless, this shows that `Pandas` can
    easily pull tabular data off the web. However, there are a few things to keep
    in mind.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这看起来是有用的数据。然而，数据中没有显示2009年以后的信息，这已经是相当久远的时间了。也许有一个更好的数据集，我们应该使用那个。无论如何，这显示了`Pandas`能够轻松地从网上抓取表格数据。然而，有几点需要注意。
- en: First, if you use scraped data in your projects, know that you are putting yourself
    at the mercy of the website administrators. If they decided to throw away the
    data table or rename or reorder the columns, that may break your application if
    it reads directly from Wikipedia. You can protect yourself against this by keeping
    a local copy of the data when you do a scrape, as well as including error handling
    in your code to watch for exceptions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果你在项目中使用抓取的数据，知道自己已经把自己交给了网站管理员的支配。如果他们决定丢弃数据表，或者重命名或重新排序列，那么直接从维基百科读取数据的应用程序可能会崩溃。你可以通过在抓取数据时保留本地数据副本以及在代码中加入错误处理来防范这种情况。
- en: Be prepared for the worst. When you are scraping, build in any error-checking
    that you need as well as ways of knowing when your scrapers are no longer able
    to pull data. Let’s move on to the next approach – using `NLTK`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为最坏的情况做好准备。当你进行数据抓取时，要建立必要的错误检查机制，并且知道何时抓取程序无法再拉取数据。接下来我们将转向下一种方法——使用`NLTK`。
- en: Python library – NLTK
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python库 – NLTK
- en: 'Let’s get straight to it:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直奔主题：
- en: 'First, NLTK does not come installed with Jupyter, so you will need to install
    it. You can do so with the following command:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，NLTK并不随Jupyter安装，因此你需要自行安装。你可以使用以下命令安装：
- en: '[PRE17]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Python’s **NLTK** library can easily pull data from Project Gutenberg, which
    is a library of over 60,000 freely available books. Let’s see what we can get:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Python的**NLTK**库可以轻松地从古腾堡计划中提取数据，古腾堡计划是一个包含超过60,000本免费书籍的库。让我们看看我们能获取到什么：
- en: '[PRE18]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: As you can easily see, this is far fewer than 60,000 results, so we are limited
    in what we can get using this approach, but it is still useful data for practicing
    NLP.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，结果远远少于60,000个，因此我们通过这种方式能获取的内容有限，但这仍然是练习自然语言处理（NLP）有用的数据。
- en: 'Let’s see what is inside the `blake-poems.txt` file:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看`blake-poems.txt`文件中有什么内容：
- en: '[PRE39]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We can load the entire file. It is messy in that it contains line breaks and
    other formatting, but we will clean that out. What if we want one of the other
    books from the full library of 60,000 books that are not on this list? Are we
    out of luck? Browsing the website, I can see that I can read the plain text of
    one of my favorite books, Franz Kafka’s *The Metamorphosis*, at [https://www.gutenberg.org/files/5200/5200-0.txt](https://www.gutenberg.org/files/5200/5200-0.txt).
    Let’s try to get that data, but this time, we will use Python’s Requests library.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以加载整个文件。文件内容比较凌乱，包含了换行符和其他格式，但我们会清理这些内容。如果我们想要获取不在此列表中的其他书籍，而这些书籍位于包含60,000本书的完整库中，该怎么办呢？我们就没机会了吗？浏览网站时，我看到我可以阅读我最喜欢的书之一——弗朗茨·卡夫卡的*《变形记》*，网址是[https://www.gutenberg.org/files/5200/5200-0.txt](https://www.gutenberg.org/files/5200/5200-0.txt)。让我们尝试获取这个数据，不过这次我们将使用Python的Requests库。
- en: Python library – Requests
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python库 – Requests
- en: The **Requests** library comes pre-installed with Python, so there should be
    nothing to do but import it. Requests is used to scrape raw text off the web,
    but it can do more. Please research the library to learn about its capabilities.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**Requests**库随Python预安装，所以你只需要导入它。Requests用于从网页抓取原始文本，但它的功能不止如此。请研究这个库，了解它的更多功能。'
- en: Important note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you use this approach, please note that you should not do this aggressively.
    It is okay to load one book at a time like this, but if you attempt to download
    too many books at once, or aggressively crawl all books available on Project Gutenberg,
    you will very likely end up getting your IP address temporarily blocked.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用这种方法，请注意不要过于激进。像这样一次加载一本书是可以的，但如果你尝试一次性下载太多书籍，或者在古腾堡项目上过于激烈地抓取所有书籍，你很可能会暂时被封禁IP地址。
- en: 'For our demonstration, let’s import the library and then scrape the raw text
    from Gutenberg’s offering of *The Metamorphosis*:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的演示中，首先导入库，然后从古腾堡提供的*变形记*中抓取原始文本：
- en: '[PRE44]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: I chopped off all text after “`vermin`” just to briefly show what is in the
    data. Just like that, we’ve got the full text of the book. As was the case with
    NLTK, the data is full of formatting and other characters, so this data needs
    to be cleaned before it will be useful. Cleaning is a very important part of everything
    that I will explain in this book. Let’s keep moving; I will show some ways to
    clean text during these demonstrations.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我将所有在“`vermin`”之后的文本剪切掉，目的是简要展示数据中的内容。就这样，我们得到了整本书的完整文本。正如在NLTK中一样，数据充满了格式和其他字符，因此这些数据需要清理才能变得有用。清理是本书中我将解释的每一部分的一个非常重要的环节。我们继续吧；在这些演示中，我会展示一些清理文本的方法。
- en: I’ve shown how easy it is to pull text from Gutenberg or Wikipedia. But these
    two do not even begin to scratch the surface of what is available for scraping
    on the internet. `pandas` can read tabular data from any web page, but that is
    limited. Most content on the web is not perfectly formatted or clean. What if
    we want to set up scrapers to harvest text and content from various news websites
    that we are interested in? NLTK won’t be able to help us get that data, and Pandas
    will be limited in what it can return. What are our options? We saw that the Requests
    library was able to pull another Gutenberg book that wasn’t available through
    NLTK. Can we similarly use requests to pull HTML from websites? Let’s try getting
    some news from Okinawa, Japan!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经展示了从古腾堡或维基百科提取文本是多么简单。但这两者只是互联网中可供抓取的内容的一小部分。`pandas`可以从任何网页读取表格数据，但它是有限的。大部分网页内容并非完美格式化或干净的。如果我们想要设置爬虫从我们感兴趣的各种新闻网站上收集文本和内容怎么办？NLTK无法帮助我们获取这些数据，而Pandas在返回的数据上也有限制。我们有哪些选择呢？我们看到`Requests`库能够拉取另一本通过NLTK无法获取的古腾堡书籍。我们能否像这样使用requests从网站抓取HTML呢？让我们尝试抓取一些来自日本冲绳的新闻吧！
- en: '[PRE45]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: There we go! We’ve just loaded the raw HTML from the given URL, and we could
    do that for any publicly accessible web page. However, if you thought the Gutenberg
    data was messy, look at this! Do we have any hope in the world of using this,
    let alone building automation to parse HTML and mine useful data? Amazingly, the
    answer is yes, and we can thank the `BeautifulSoup` Python library and other scraping
    libraries for that. They have truly opened up a world of data for us. Let’s see
    what we can get out of this using `BeautifulSoup`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 好了！我们刚刚从给定的URL加载了原始HTML，我们可以对任何公开访问的网页进行相同的操作。但是，如果你认为古腾堡的数据已经很杂乱，那看看这个！我们在这个HTML中还能指望什么呢？更别提建立自动化工具来解析HTML并提取有用数据了。令人惊讶的是，答案是肯定的，我们可以感谢`BeautifulSoup`
    Python库以及其他抓取库。它们为我们打开了一个数据的新世界。让我们看看如何利用`BeautifulSoup`从中提取内容。
- en: Python library – BeautifulSoup
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python库 - BeautifulSoup
- en: 'First, `BeautifulSoup` is used along with the `Requests` library. Requests
    comes pre-installed with Python, but `BeautifulSoup` does not, so you will need
    to install it. You can do so with the following command:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`BeautifulSoup`与`Requests`库一起使用。Requests随Python预安装，但`BeautifulSoup`并没有，因此你需要安装它。你可以使用以下命令来安装：
- en: '[PRE46]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'There’s a lot you can do with `BeautifulSoup`, so please go explore the library.
    But what would it take to extract all of the links from the Okinawa News URL?
    This is how you can do exactly that:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`BeautifulSoup`有很多功能，所以请去探索这个库。但提取冲绳新闻网站上所有链接需要做什么呢？下面是你可以做的：'
- en: '[PRE47]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: That was easy! I am only showing the first several extracted links. The first
    line imported the library, the second line set `BeautifulSoup` up for parsing
    HTML, the third line looked for all links containing an `href` attribute, and
    then finally the last line displayed the links. How many links did we harvest?
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单！我这里只展示了前几个提取的链接。第一行导入了库，第二行设置了 `BeautifulSoup` 以便解析 HTML，第三行查找了所有包含 `href`
    属性的链接，最后一行则显示了这些链接。我们抓取了多少个链接？
- en: '[PRE48]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Your result might be different, as the page may have been edited after this
    book was written.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你的结果可能会有所不同，因为该页面可能在这本书写完后进行了编辑。
- en: '277 links were harvested in less than a second! Let’s see whether we can clean
    these up and just extract the URLs. Let’s not worry about the link text. We should
    also convert this into a list of URLs rather than a list of `<a>` HTML tags:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在不到一秒钟的时间内，已经抓取了 277 个链接！让我们看看是否能清理它们并只提取 URL。我们不需要担心链接的文本内容。我们还应该将其转换为一个 URL
    列表，而不是 `<a>` HTML 标签列表：
- en: '[PRE49]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Here, we are using list comprehension and `BeautifulSoup` to extract the value
    stored in `href` for each of our harvested links. I can see that there is some
    duplication, so we should remove that before eventually storing the results. Let’s
    see whether we lost any of the former 277 links:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了列表推导式和 `BeautifulSoup` 来提取每个已抓取链接中的 `href` 值。我发现有些重复的链接，因此我们需要在最终存储结果之前去除它们。让我们看看是否丢失了原先的
    277 个链接：
- en: '[PRE50]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Perfect! Let’s take one of them and see whether we can extract the raw text
    from the page, with all HTML removed. Let’s try this URL that I have hand-selected:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！我们选择其中一个，看看是否能够从页面中提取出原始文本，去掉所有 HTML。我们来试试这个我手动选择的 URL：
- en: '[PRE51]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Done! We have captured pretty clean and usable text from a web page! This can
    be automated to constantly harvest links and text from any website of interest.
    Now, we have what we need to get to the fun stuff in this chapter: extracting
    entities from text, and then using entities to build social networks. It should
    be obvious by now that some cleanup will be needed for any of the options that
    we have explored, so let’s just make short work of that now. To get this perfect,
    you need to do more than I am about to do, but let’s at least make this somewhat
    usable:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 完成！我们已经从网页中捕获到了相当干净且可用的文本！这可以自动化，以便不断从任何感兴趣的网站抓取链接和文本。现在，我们已经拥有了进入本章有趣部分的基础：从文本中提取实体，然后利用实体来构建社交网络。到现在为止，应该显而易见，我们探索的所有选项都需要进行一些清理，所以我们现在就开始处理这一部分。要做到完美，你需要做的比我接下来要做的更多，但至少我们可以让它变得有些可用：
- en: '[PRE52]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The first thing that stands out to me is the amount of text formatting and
    special characters that exist in this text. We have a few options. First, we could
    convert all of the line spaces into empty spaces. Let’s see what that looks like:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 第一件引起我注意的事是文本中存在大量的文本格式和特殊字符。我们有几个选择。首先，我们可以把所有的换行符转换为空格。我们来看看效果如何：
- en: '[PRE53]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'If you scroll further down the text, you may see that the story ends at “`Go
    to Japanese`,” so let’s remove that as well as everything after:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你继续向下滚动文本，你可能会看到故事在“`Go to Japanese`”处结束，那么我们也将删除该部分及其之后的内容：
- en: '[PRE54]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This shows that the cutoff string starts at the 1,984th character. Let’s keep
    everything up to the cutoff:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了截断字符串从第 1,984 个字符开始。让我们保留所有直到截断位置的内容：
- en: '[PRE55]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This has successfully removed the footer junk, but there is still some header
    junk to deal with, so let’s see whether we can remove that. This part is always
    tricky, and every website is unique in some way, but let’s remove everything before
    the story as an exercise. Looking closer, I can see that the story starts at the
    second occurrence of “`Hirokazu Uevonabaru`.” Let’s capture everything from that
    point and beyond. We will be using `.rindex()` instead of `.index()`to capture
    the last occurrence. This code is too specific for real-world use, but hopefully,
    you can see that you have options for cleaning dirty data:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这成功地去除了页脚的杂项，但仍然有一些页头的杂项需要处理，看看我们能否去除它。这个部分总是比较棘手的，每个网站都有其独特之处，但我们可以尝试去除故事之前的所有内容作为练习。仔细看，我发现故事从第二次出现“`Hirokazu
    Uevonabaru`”开始。我们从那个点开始捕获所有内容。我们将使用 `.rindex()` 代替 `.index()` 来捕获最后一次出现。这段代码对于实际应用来说过于具体，但希望你能看到你有一些清理脏数据的选项：
- en: '[PRE56]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: If you are not familiar with Python, that might look a bit strange to you. We
    are keeping everything after the beginning of the last occurrence of “`Hirokazu
    Ueyonabaru`,” which is where the story starts. How does it look now?
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉 Python，这可能会让你感到有些奇怪。我们保留了从最后一次出现“`Hirokazu Ueyonabaru`”开始的所有内容，这正是故事的起点。现在看起来怎么样？
- en: '[PRE57]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: That looks just about perfect! There is always more cleaning that can be done,
    but this is good enough! When you first get started with any new scraping, you
    will need to clean, inspect, clean, inspect, clean, and inspect – gradually, you
    will stop finding obvious things to remove. You don’t want to cut too much. Just
    get the text to be usable – we will do additional cleaning at later steps.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来几乎完美！总是可以做更多的清理，但现在这样已经足够好了！当你刚开始进行网页抓取时，你需要清理、检查、清理、检查、清理和检查——逐渐地，你会停止发现明显需要删除的东西。你不想削减太多。只需要让文本可以使用——我们将在后续步骤中进行额外的清理。
- en: Choosing between libraries, APIs, and source data
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择库、API和源数据
- en: As part of this demonstration, I showed several ways to pull useful data off
    of the internet. I showed that several libraries have ways to load data directly
    but that there are limitations to what they have available. NLTK only offered
    a small portion of the complete Gutenberg book archive, so we had to use the `Requests`
    library to load *The Metamorphosis*. I also demonstrated that `Requests` accompanied
    by `BeautifulSoup` can easily harvest links and raw text.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这次演示的一部分，我展示了几种从互联网获取有用数据的方法。我展示了几个库如何直接加载数据，但它们提供的数据是有限的。NLTK只提供了完整古腾堡书籍档案的一小部分，因此我们必须使用`Requests`库来加载*变形记*。我还演示了如何通过`Requests`和`BeautifulSoup`轻松提取链接和原始文本。
- en: Python libraries can also make loading data very easy when those libraries have
    data loading functionality as part of their library, but you are limited by what
    those libraries make available. If you just want some data to play with, with
    minimal cleanup, this may be ideal, but there will still be cleanup. You will
    not get away from that when working with text.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当Python库将数据加载功能集成到库中时，它们也可以使加载数据变得非常简单，但你只能使用那些库提供的数据。如果你只是想要一些数据来玩，并且不需要太多清理，这可能是理想选择，但仍然需要清理。在处理文本时，你无法避免这一点。
- en: Other web resources expose their own APIs, which makes it pretty simple to load
    data after sending a request to them. Twitter does this. You authenticate using
    your API key, and then you can pull whatever data you want. This is a happy middle-ground
    between Python libraries and web scraping.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 其他网络资源提供了自己的API，这使得在向它们发送请求后加载数据变得非常简单。Twitter就是这样做的。你通过API密钥进行身份验证，然后你就可以提取你想要的任何数据。这是在Python库和网页抓取之间的一个理想平衡。
- en: Finally, web scraping opens up the entire web to you. If you can access a web
    page, you can scrape it and use any text and data that it has made available.
    You have flexibility with web scraping, but it is more difficult, and the results
    require more cleanup.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，网页抓取让你能够访问整个互联网。如果你能访问一个网页，你就可以抓取它并使用它提供的任何文本和数据。网页抓取具有灵活性，但它更为复杂，且结果需要更多的清理。
- en: 'I tend to approach my own scraping and data enrichment projects by making considerations
    in the following order:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常会按照以下顺序考虑我的抓取和数据增强项目：
- en: Is there a Python library that will make it easy for me to load the data I want?
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有没有Python库可以让我轻松加载我想要的数据？
- en: No? OK, is there an API that I can use to pull the data that I want?
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有吗？好吧，有没有我可以用来提取我想要的数据的API？
- en: No? OK, can I just scrape it using `BeautifuSoup`? Yes? Game on. Let’s dance.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有吗？好吧，我能用`BeautifulSoup`直接抓取吗？能吗？那就开始吧。我们开始跳舞吧。
- en: Start simple and scale out in terms of complexity only as needed. Starting simple
    means starting with the simplest approach – in this case, Python libraries. If
    libraries won’t help, add a little complexity by seeing whether an API is available
    to help and whether it is affordable. If one is not available, then web scraping
    is the solution that you need, and there is no way around it, but you will get
    the data that you need.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从简单开始，只有在需要时才逐步增加复杂度。开始简单意味着从最简单的方法开始——在这种情况下，就是使用Python库。如果库不能满足需求，可以通过查看是否有API可用来帮助，且是否负担得起，来增加一点复杂度。如果没有API可用，那么网页抓取就是你需要的解决方案，无法避免，但你将能够获得你需要的数据。
- en: Now that we have text, we are going to move into NLP. Specifically, we will
    be using **PoS tagging and NER** as two distinct ways to extract entities (people
    and things) from raw text.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了文本，我们将进入自然语言处理（NLP）。具体来说，我们将使用**词性标注（PoS tagging）和命名实体识别（NER）**，这两种不同的方法从原始文本中提取实体（人物和事物）。
- en: Using NLTK for PoS tagging
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NLTK进行词性标注
- en: In this section, I will explain how to do what is called PoS tagging using the
    NLTK Python library. NLTK is an older Python NLP library, but it is still very
    useful. There are also pros and cons when comparing NLTK with other Python NLP
    libraries, such as `spaCy`, so it doesn’t hurt to understand the pros and cons
    of each. However, during my coding for this demonstration, I realized just how
    much easier `spaCy` has made both PoS tagging as well as NER, so, if you want
    the easiest approach, feel free to just skip ahead to `spaCy`. I am still fond
    of `NLTK`, and in some ways, the library still feels more natural to me than `spaCy`,
    but that may simply be due to years of use. Anyway, I’d like to demonstrate `PoS
    tagging` with `NLTK`, and then I will demonstrate both `PoS tagging` and NER with
    `spaCy` in the next section.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我将解释如何使用NLTK Python库进行所谓的PoS标注。NLTK是一个较老的Python自然语言处理库，但它仍然非常有用。在将NLTK与其他Python自然语言处理库（例如`spaCy`）进行比较时，也有其优缺点，因此了解每个库的优缺点是有益的。然而，在我进行这次演示的编码过程中，我意识到`spaCy`在PoS标注和命名实体识别（NER）方面确实让一切变得更加简便。因此，如果你想要最简单的方法，随时可以跳到`spaCy`部分。我仍然喜欢`NLTK`，在某些方面，这个库对我来说比`spaCy`更自然，但这可能只是因为我已经用了很多年。无论如何，我想先用`NLTK`演示PoS标注，然后在下一部分演示如何使用`spaCy`进行PoS标注和NER。
- en: PoS tagging is a process that takes text tokens and identifies the PoS that
    the token belongs to. Just as a review, a token is a single word. A token might
    be *apples*.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: PoS标注是一个过程，它将文本中的单词标记为相应的词性。回顾一下，token是单个单词。一个token可能是*apples*。
- en: With NLP, tokens are useful, but bigrams are often even more useful, and they
    can improve the results for text classification, sentiment analysis, and even
    unsupervised learning. A bigram is essentially two tokens – for example, t*wo
    tokens*.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中，token很有用，但二元组（bigram）通常更加有用，它们可以提高文本分类、情感分析，甚至是无监督学习的结果。二元组本质上是两个token——例如，t*wo
    tokens*。
- en: Let’s not overthink this. It’s just two tokens. What do you think a trigram
    is? That’s right – three tokens. For instance, if filler words were removed from
    some text before the trigrams were captured, you could have one for *green* *eggs
    ham*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不要想太多。这只是两个token。你认为三元组（trigram）是什么？没错，就是三个token。例如，如果在提取三元组之前移除掉一些填充词，你可能会得到一个像*green*
    *eggs ham*这样的三元组。
- en: 'There are many different `pos_tags`, and you can see the list here: [https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的`pos_tags`，你可以在这里查看完整列表：[https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)。
- en: For the work that we are doing, we will only use the NLP features that we need,
    and `PoS tagging` and NER are two different approaches that are useful for identifying
    entities (people and things) that are being described in text. In the mentioned
    list, the ones that we want are NNP and NNPS, and in most cases, we’ll find NNP,
    not NNPS.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们正在做的工作，我们将只使用我们需要的NLP特性，PoS标注和NER是两种有用的不同方法，可以帮助我们识别文本中描述的实体（人和物）。在上述列表中，我们需要的是NNP和NNPS，在大多数情况下，我们会找到NNP，而不是NNPS。
- en: 'To explain what we are trying to do, we are going to follow these steps:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释我们要做的事情，我们将遵循以下步骤：
- en: Get some text to work with.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一些文本进行处理。
- en: Split the text into sentences.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本拆分成句子。
- en: Split each sentence into tokens.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个句子拆分成token。
- en: Identify the PoS tag for each token.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别每个token的PoS标签。
- en: Extract each token that is a proper noun.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取每个专有名词的token。
- en: 'A proper noun is the name of a person, place, or thing. I have been saying
    that we want to extract entities and define entities as people, places, or things,
    so the NNP tag will identify exactly what we want:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 专有名词是指人、地方或事物的名称。我一直在说我们要提取实体，并将实体定义为人、地方或事物，因此NNP标签将准确标识我们想要的内容：
- en: Let’s get to work and get some text data!
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始工作，获取一些文本数据！
- en: '[PRE58]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We used this code previously to load the entire text from Kafka’s book *The
    Metamorphosis*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前使用这段代码加载了卡夫卡的书《变形记》中的整个文本，*The Metamorphosis*。
- en: 'There is a lot of junk in the header of this file, but the story starts at
    “`One morning`,” so let’s remove everything from before that. Feel free to explore
    the `text` variable as we go. I am leaving out repeatedly showing the data to
    save space:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个文件的头部有很多杂乱的内容，但故事从“`One morning`”开始，所以我们从那之前的部分删除掉。你可以在我们操作时随意查看`text`变量。我省略了反复展示数据的步骤以节省空间：
- en: '[PRE60]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Here, we have identified the starting point of the phrase, `One morning`, and
    then removed everything up to that point. It’s all just header junk that we don’t
    need.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们已经识别出了短语的起始点`One morning`，并移除了所有到这个点为止的内容。那只是我们不需要的头部垃圾。
- en: 'Next, if you look at the bottom of the text, you can see that the story ends
    at, `*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS`, so let’s cut from
    that point onward as well:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，如果你查看文本底部，你会看到故事在`*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS`处结束，那么让我们从这个点开始裁剪：
- en: '[PRE62]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Look closely at the cutoff and you will see that the cutoff is in a different
    position from that used for removing the header. I am essentially saying, “*Give
    me everything up to the cutoff*.” How does the ending text look now?
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细看截断，你会发现截断的位置与删除头部时使用的位置不同。我本质上是在说，“*给我所有内容直到截断位置*。”现在结束的文本看起来怎么样？
- en: '[PRE64]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Not bad. That’s a noticeable improvement, and we are getting closer to clean
    text. Apostrophes are also being mangled, being shown as `â\x80\x99`, so let’s
    replace those:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 不错。这是一个显著的进步，我们离干净的文本又近了一步。撇号也被破坏，显示为`â\x80\x99`，所以我们需要将其替换：
- en: '[PRE65]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'That is about perfect, so let’s convert these steps into a reusable function:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个几乎完美了，接下来我们将这些步骤转化为一个可重用的函数：
- en: '[PRE66]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'We should have fairly clean text after running this function:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行这个函数后，我们应该得到相当干净的文本：
- en: '[PRE80]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Outstanding! We are now ready for the next steps.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 出色！我们现在准备好进入下一步了。
- en: 'Before we move on, I want to explain one thing: if you do `PoS tagging` on
    the complete text of any book or article, then the text will be treated as one
    massive piece of text, and you lose your opportunity to understand how entities
    relate and interact. All you will end up with is a giant entity list, which isn’t
    very helpful for our needs, but it can be useful if you just want to extract entities
    from a given piece of text.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我想解释一件事：如果你对任何书籍或文章的完整文本进行`PoS标注`，那么文本会被当作一个巨大的整体处理，你就失去了理解实体如何关联和互动的机会。你最终得到的只会是一个巨大的实体列表，这对于我们的需求并不太有帮助，但如果你只是想从某个文本中提取实体，它倒是很有用。
- en: 'For our purposes, the first thing you need to do is split the text into sentences,
    chapters, or some other desirable bucket. For simplicity, let’s do this by sentences.
    This can easily be done using NLTK’s sentence tokenizer:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，首先你需要做的事情是将文本拆分成句子、章节或其他想要的类别。为了简单起见，我们就按句子来拆分。这可以通过NLTK的句子标记化工具轻松完成：
- en: '[PRE84]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '*Beautiful*! We have a list of sentences to work with. Next, we want to take
    each of these sentences and extract any mentioned entities. We want the NNP-tagged
    tokens. This part takes a little work, so I will walk you through it. If we just
    feed the sentences to NLTK’s `pos_tag tool`, it will misclassify everything:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*漂亮*! 我们现在有了一个句子列表可以使用。接下来，我们要做的是从这些句子中提取出任何提到的实体。我们需要的是NNP标记的词语。这部分稍微复杂一些，我会一步步带你完成。如果我们直接将句子传给NLTK的`pos_tag工具`，它会错误分类所有内容：'
- en: '[PRE85]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Good effort, but that is not what we need. What we need to do is go through
    each sentence and identify the PoS tags, so let’s do this manually for a single
    sentence:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 很好的尝试，但这不是我们需要的。我们需要做的是遍历每个句子并识别PoS标签，所以我们先手动处理一个句子：
- en: '[PRE86]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'First, we need to tokenize the sentence. There are many different tokenizers
    in NLTK, with strengths and weaknesses. I have gotten comfortable with the casual
    tokenizer, so I’ll just use that. The casual tokenizer does well with casual text,
    but there are several other tokenizers available to choose from:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要对句子进行标记化。NLTK中有许多不同的标记化工具，每个工具有自己的优缺点。我习惯了使用随意的标记化工具，所以我将使用它。随意的标记化工具适用于随意的文本，但也有其他几个标记化工具可以选择：
- en: '[PRE87]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Great. Now, for each token, we can find its corresponding `pos_tag`:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。现在，对于每个标记，我们可以找到它对应的`pos_tag`：
- en: '[PRE88]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'That’s also perfect! We are interested in extracting the NNPs. Can you see
    the two tokens that we want to extract? That’s right, it’s Gregor Samsa. Let’s
    loop through these PoS tags and extract the NNP tokens:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这也完美！我们要提取的是NNP。你能看到我们想要提取的两个标记吗？没错，就是Gregor Samsa。让我们遍历这些PoS标签并提取NNP标记：
- en: '[PRE89]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'This is what we need. NER would hopefully identify these two results as one
    person, but once this is thrown into a graph, it’s very easy to correct. Let’s
    convert this into a function that will take a sentence and return the NNP tokens
    – the entities:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要的。希望NER能识别这两项结果为同一个人，但一旦把它放进图中，很容易就能纠正。让我们将其转化为一个函数，它将接受一个句子并返回NNP标记——即实体：
- en: '[PRE90]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: That looks good. Let’s give it a shot!
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错。我们试试看！
- en: '[PRE91]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Now, let’s be bold and try this out against every sentence in the entire book:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们大胆尝试一下，将其应用到整本书的每一个句子上：
- en: '[PRE92]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Just to make analysis a bit easier, let’s do two things: first, let’s replace
    those empty lists with `None`. Second, let’s throw all of this into a `Pandas`
    DataFrame:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让分析稍微简单一些，我们做两件事：首先，将那些空的列表替换成`None`。其次，把所有这些数据放入一个`Pandas` DataFrame中：
- en: '[PRE93]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'This will give us a DataFrame of sentences and entities extracted from sentences:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个包含句子和从句子中提取的实体的DataFrame：
- en: '![Figure 4.4 – Pandas DataFrame of sentence entities](img/B17105_04_004.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – Pandas DataFrame中的句子实体](img/B17105_04_004.jpg)'
- en: Figure 4.4 – Pandas DataFrame of sentence entities
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – Pandas DataFrame中的句子实体
- en: 'That’s a good start. We can see that “**What’s**” has been incorrectly flagged
    by NLTK, but it’s normal for junk to get through when dealing with text. That’ll
    get cleaned up soon. For now, we want to be able to build a social network using
    this book, so let’s grab every entity list that contains two or more entities.
    We need at least two to identify a relationship:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个好的开始。我们可以看到“**What’s**”被NLTK错误地标记了，但在处理文本时，垃圾信息被误识别是正常的。这些问题很快会得到清理。现在，我们想利用这本书构建社交网络，所以让我们获取所有包含两个或更多实体的实体列表。我们至少需要两个实体来识别关系：
- en: '[PRE94]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'This looks pretty good other than some punctuation sneaking in. Let’s revisit
    the previous code and disregard any non-alphabetical characters. That way, `Gregor''s`
    will become `Gregor`, `I''d` will become `I`, and so forth. That’ll make cleanup
    a lot easier:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 除了一些标点符号悄悄混入外，这看起来还不错。让我们回顾一下之前的代码，并忽略所有非字母字符。这样，`Gregor's`会变成`Gregor`，`I'd`会变成`I`，以此类推。这样清理起来会更加容易：
- en: '[PRE95]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Let’s put this back into a DataFrame and repeat our steps to see whether the
    entity list is looking better:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个数据重新放入DataFrame中，并重复我们的步骤，看看实体列表是否有所改善：
- en: '[PRE96]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'This is getting better, but some double quotes are still in the data. Let’s
    just remove any punctuation and anything after:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经好很多了，但数据中仍然有一些双引号。让我们去除所有标点符号以及之后的内容：
- en: '[PRE97]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'This is good enough! We could add a little more logic to prevent the same token
    from appearing twice in a row, but we can very easily remove those from a network,
    so let’s refactor our code and move on:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果已经足够好了！我们可以增加一些逻辑，防止同一标记连续出现两次，但我们可以很容易地从网络中去除这些，所以让我们重构代码继续进行：
- en: '[PRE98]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: This is excellent. At this point, no matter whether we do PoS tagging or NER,
    we want an entity list, and this is close enough. Next, we will do the same using
    spaCy, and you should be able to see how spaCy is much easier in some regards.
    However, there is more setup involved, as you need to install a language model
    to work with spaCy. There are pros and cons to everything.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常棒。此时，不管我们是做词性标注还是NER，我们都需要一个实体列表，这已经足够接近了。接下来，我们将使用spaCy做同样的操作，你应该能够看到spaCy在某些方面更为简便。然而，它的设置更为复杂，因为你需要安装一个语言模型来与spaCy一起使用。每种方法都有其优缺点。
- en: Using spaCy for PoS tagging and NER
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用spaCy进行词性标注和NER
- en: In this section, I am going to explain how to do what we have just done with
    NLTK, but this time using spaCy. I will also show you how to use NER as an often-superior
    alternative to PoS tagging for identifying and extracting entities. Before I started
    working on this chapter, I primarily used NLTK’s `PoS tagging` as the heart of
    my entity extraction, but since writing the code for this section and exploring
    a bit more, I have come to realize that spaCy has improved quite a bit, so I do
    think that what I am about to show you in this section is superior to what I previously
    did with NLTK. I do think that it is helpful to explain the usefulness of NLTK.
    Learn both and use what works best for you. But for entity extraction, I believe
    spaCy is superior in terms of ease of use and speed of delivery.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我将解释如何使用spaCy做我们刚刚用NLTK完成的工作。我还将展示如何使用NER作为一种通常优于词性标注（PoS）的方式来识别和提取实体。在开始编写本章内容之前，我主要使用NLTK的`PoS
    tagging`作为我的实体提取核心，但在为这一节编写代码并稍微深入探索后，我意识到spaCy有了显著的改进，因此我认为我在这一节展示的内容优于之前用NLTK做的工作。我认为解释NLTK的有用性还是有帮助的。学习两者并使用最适合你的方法。但对于实体提取，我相信spaCy在易用性和处理速度方面优于NLTK。
- en: Previously, I wrote a function to load Franz Kafka’s book *The Metamorphosis*,
    so we will use that loader here as well, as it has no dependence on either NLTK
    or spaCy, and it can be easily modified to load any book from the Project Gutenberg
    archive.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我编写了一个函数来加载弗朗茨·卡夫卡的书《变形记》，所以我们也将使用这个加载器，因为它不依赖于NLTK或spaCy，并且可以很容易地修改，以便从古腾堡计划的档案中加载任何书籍。
- en: 'To do anything with spaCy, the first thing you need to do is load the spaCy
    language model of choice. Before we can load it, we must first install it. You
    can do so by running the following command:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 spaCy 做任何事情，首先需要做的是加载所选的 spaCy 语言模型。在我们加载之前，必须先安装它。你可以通过运行以下命令来完成安装：
- en: '[PRE99]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'There are several different models available, but the three that I use for
    English text are small, medium, and large. `md` stands for medium. You could swap
    that out for `sm` or `lg` to get the small or large models, respectively. You
    can find out more about spaCy’s models here: [https://spacy.io/usage/models](https://spacy.io/usage/models).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同的模型可供选择，但我使用的三个英文文本模型分别是小型、中型和大型。`md`代表中型。你可以将其替换为`sm`或`lg`，分别获取小型或大型模型。你可以在这里了解更多关于
    spaCy 模型的信息：[https://spacy.io/usage/models](https://spacy.io/usage/models)。
- en: 'Once the model has been installed, we can load it into our Python scripts:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型安装完成，我们就可以将其加载到我们的 Python 脚本中：
- en: '[PRE100]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: As mentioned previously, you could swap that `md` for `sm` or `lg`, depending
    on what model you want to use. The larger model requires more storage and memory
    for use. The smaller model requires less. Pick the one that works well enough
    for what you are doing. You may not need the large model. The medium and small
    ones work well, and the difference between models is often unnoticeable.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，你可以根据需要将 `md` 替换为 `sm` 或 `lg`，取决于你想使用的模型。较大的模型需要更多的存储和内存。较小的模型需要较少。选择一个足够适合你工作的模型。你可能不需要大型模型。中型和小型模型表现很好，不同模型之间的差异通常不易察觉。
- en: 'Next, we need some text, so let’s reuse the function that we previously wrote:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一些文本，因此让我们重用之前编写的函数：
- en: '[PRE101]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'There are a few extra spaces in the middle of sentences, but that won’t present
    any problem for us at all. Tokenization will clean that up without any additional
    effort on our part. We will get to that shortly. First, as done with NLTK, we
    need to split the text into sentences so that we can build a network based on
    the entities that are uncovered in each sentence. That is much easier to do using
    spaCy rather than NLTK, and here is how to do it:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中间有一些额外的空格，但这对我们来说不会构成任何问题。分词会自动清理这些空格，而我们无需做额外的工作。我们很快就会涉及到这个问题。首先，像在 NLTK
    中那样，我们需要将文本拆分成句子，以便可以根据每个句子中揭示的实体构建网络。使用 spaCy 而非 NLTK 做这件事要容易得多，下面是操作方法：
- en: '[PRE102]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'The first line feeds the full text of *The Metamorphosis* into spaCy and uses
    our language model of choice, while the second line extracts the sentences that
    are in the text. Now, we should have a Python list of sentences. Let’s inspect
    the first six sentences in our list:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行将 *变形记* 的完整文本传递给 spaCy，并使用我们选择的语言模型，而第二行则提取文本中的句子。现在，我们应该得到一个 Python 列表，包含所有的句子。让我们检查列表中的前六个句子：
- en: '[PRE103]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Six probably seems like a strange number of sentences to inspect, but I wanted
    to show you something. Take a look at the last two sentences. SpaCy has successfully
    extracted the main character’s inner dialog as a standalone sentence, as well
    as created a separate sentence to complete the surrounding sentence. For our entity
    extraction, it wouldn’t present any problem at all if those sentences were combined,
    but I like this. It’s not a bug, it’s a feature, as we software engineers like
    to say.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 六个句子可能看起来是一个奇怪的数量，但我想给你展示一些东西。看看最后两个句子。SpaCy 已成功地将主要角色的内心独白提取为一个独立的句子，并且创建了一个单独的句子来补充周围的句子。对于我们的实体提取来说，如果这些句子合并在一起也不会有任何问题，但我喜欢这样。这不是一个bug，这是一个功能，正如我们软件工程师常说的那样。
- en: SpaCy PoS tagging
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SpaCy 词性标注
- en: 'Now that we have our sentences, let’s use spaCy’s PoS tagging as pre-processing
    for entity extraction:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了句子，让我们使用 spaCy 的词性标注作为实体提取的预处理：
- en: '[PRE104]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Nice. What we want are the NNPs as these are proper nouns. You can see this
    if you use `pos_` instead of `tag_`:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。我们需要的是 NNP，因为这些是专有名词。如果你使用 `pos_` 而不是 `tag_`，你可以看到这一点：
- en: '[PRE105]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Let’s add a little logic for extracting them. We need to do two things – we
    need a list to store the results, and we need some logic to extract the NNPs:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加一些提取的逻辑。我们需要做两件事——我们需要一个列表来存储结果，并且需要一些逻辑来提取 NNP：
- en: '[PRE106]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'For NLTK, we created a function that would extract entities for a given sentence,
    but this way, I just did everything in one go, and it was quite simple. Let’s
    convert this into a function so that we can easily use this for other future work.
    Let’s also prevent empty lists from being returned inside the entity list, as
    we have no use for those:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 NLTK，我们创建了一个函数来提取给定句子的实体，但是通过这种方式，我一次性做完了所有事情，而且非常简单。让我们把这个转化为一个函数，这样我们就能方便地用于以后的工作。同时，让我们防止实体列表中返回空列表，因为我们不需要这些：
- en: '[PRE107]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'We should now have a clean entity list, with no empty inner lists included:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该有一个干净的实体列表，其中不包含任何空的内部列表：
- en: '[PRE108]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: This looks a lot better than NLTK’s results, and with fewer steps. This is simple
    and elegant. I didn’t need to use `Pandas` for anything, drop empty rows, or clean
    out any punctuation that somehow slipped in. We can use this function on any text
    we have, after cleaning. You can use it before cleaning, but you’ll end up getting
    a bunch of junk entities, especially if you use it against scraped web data.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这比 NLTK 的结果好多了，而且步骤更少。这简单而优雅。我不需要用到`Pandas`，也不需要删除空行，或者清理那些莫名其妙出现的标点符号。我们可以在任何清理过的文本上使用这个函数。你也可以在清理之前使用它，但那时你会得到一些无用的实体，尤其是在针对抓取的网页数据时。
- en: SpaCy NER
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SpaCy 命名实体识别（NER）
- en: SpaCy’s NER is equally easy and simple. The difference between `PoS tagging`
    and NER is that NER goes a step further and identifies people, places, things,
    and more. For a detailed description of spaCy’s linguistic features, I heartily
    recommend Duygu Altinok’s book *Mastering spaCy*. To be concise, spaCy labels
    tokens as one of *18* different kinds of entities. In my opinion, that is a bit
    excessive, as `MONEY` is not an entity, but I just take what I want. Please check
    spaCy for the full list of entity types. What we want are entities that are labeled
    as `PERSON`, `ORG`, or `GPE`. `ORG` stands for organization, and `GPE` contains
    countries, cities, and states.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy 的 NER 同样简单易用。`词性标注（PoS tagging）` 和 NER 的区别在于，NER 进一步识别了人物、地点、事物等。关于 spaCy
    语言特性的详细描述，我强烈推荐 Duygu Altinok 的书籍 *Mastering spaCy*。简而言之，spaCy 将标记归类为 *18* 种不同的实体类型。在我看来，这有些过多，因为
    `MONEY` 并不是一个实体，但我只取我需要的。请查看 spaCy 获取完整的实体类型列表。我们需要的是被标记为 `PERSON`、`ORG` 或 `GPE`
    的实体。`ORG` 代表组织，`GPE` 包含国家、城市和州。
- en: 'Let’s loop through all the tokens in the first sentence and see how this looks
    in practice:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们遍历第一句话中的所有标记，看看这在实践中是如何运作的：
- en: '[PRE109]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'This works, but there is a slight problem: we want Gregor Samsa to appear as
    one entity, not as two. What we need to do is create a new spaCy doc and then
    loop through the doc’s `ents` rather than the individual tokens. In that regard,
    the NER approach is slightly different from `PoS tagging`:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法有效，但有一个小问题：我们希望“Gregor Samsa”作为一个实体出现，而不是两个。我们需要做的是创建一个新的 spaCy 文档，然后遍历文档的
    `ents`，而不是单独的标记。在这方面，NER 的方法与 `词性标注（PoS tagging）`略有不同：
- en: '[PRE110]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Perfect! Let’s do a few things: we will redo our previous entity extraction
    function, but this time using NER rather than `PoS tagging`, and then limit our
    entities to `PERSON`, `ORG`, and `GPE`. Please note that I am only adding entities
    if there is more than one in a sentence. We are looking to identify relationships
    between people, and you need at least two people to have a relationship:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！接下来做几件事：我们将重做之前的实体提取函数，不过这次使用 NER 而不是`词性标注（PoS tagging）`，然后将实体限制为`PERSON`、`ORG`
    和 `GPE`。请注意，只有在一句话中有多个实体时，我才会添加实体。我们需要识别人物之间的关系，而你至少需要两个人才算得上有关系：
- en: '[PRE111]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'I added a bit of code to remove any whitespace that has snuck in and also to
    remove duplicates from each `sentence_entity` list. I also removed any `''s` characters
    that appeared after a name – for example, `Gregor''s` – so that it’d show up as
    `Gregor`. I could have cleaned this up in network cleanup, but this is a nice
    optimization. Let’s see how our results look. I named the entity list `morph_entities`
    for `metaMORPHosis` entities. I wanted a descriptive name, and this is the best
    I could come up with:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我添加了一些代码，去除任何多余的空白，并且清除了每个 `sentence_entity` 列表中的重复项。我还去除了出现在名字后面的 `'s` 字符——比如
    `Gregor's`，它会显示为 `Gregor`。我本可以在网络清理时处理这些，但这是一个不错的优化。让我们看看结果如何。我把实体列表命名为 `morph_entities`，代表
    `metaMORPHosis` 实体。我想要一个描述性的名字，这个是我能想到的最合适的：
- en: '[PRE112]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: That looks great! Wow, I haven’t read *The Metamorphosis* in many years and
    forgot how few characters were in the story!
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很棒！哇，我已经很多年没读过 *变形记* 了，竟然忘记了故事中的人物有多少！
- en: SpaCy’s NER is done using a pre-trained deep learning model, and machine learning
    is never perfect. Please keep that in mind. There is always some cleanup. SpaCy
    allows you to customize their language models for your documents, which is useful
    if you work within a specialized domain, but I prefer to use spaCy’s models as
    general-purpose tools, as I deal with a huge variety of different text. I don’t
    want to customize spaCy for tweets, literature, disinformation, and news. I would
    prefer to just use it as is and clean up as needed. That has worked very well
    for me.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy的NER是使用预训练的深度学习模型完成的，而机器学习从来不是完美的。请记住这一点。总是需要一些清理工作。SpaCy允许你根据自己的文档自定义语言模型，如果你在某个专业领域工作，这非常有用，但我更倾向于将spaCy的模型作为通用工具使用，因为我处理的文本类型种类繁多。我不想为推文、文学作品、虚假信息和新闻定制spaCy。我宁愿按原样使用它，并根据需要进行清理。对我来说，这一直是非常有效的。
- en: You should be able to see that there is a lot of duplication. Apparently, in
    *The Metamorphosis*, a lot of time is spent talking about Gregor. We will be able
    to remove those duplicates with a single line of `NetworkX` code later, so I’ll
    just leave them in rather than tweaking the function. Good enough is good enough.
    If you are working with massive amounts of data and paying for cloud storage,
    you should probably fix inefficiencies.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看到有很多重复项。显然，在*变形记*中，很多时间都在谈论格雷戈尔。稍后我们可以通过一行`NetworkX`代码删除这些重复项，所以我决定保留它们，而不是调整函数。能达到“足够好”就可以。如果你正在处理大量数据并且支付云存储费用，你可能需要修复这些低效之处。
- en: For the rest of this chapter, I’m going to use the NER results as our network
    data. We could just as easily use the `pos_tag` entities, but this is better,
    as NER can combine first name and last name. In our current entities, none of
    those came through, but they will with other text. This is just how *The Metamorphosis*
    was written. We’ll just clean that up as part of the network creation.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我将使用NER结果作为我们的网络数据。我们同样可以使用`pos_tag`实体，但这样做不如NER好，因为NER能够结合名字和姓氏。在我们当前的实体中，这些信息并没有被提取出来，但在其他文本中会有。这就是*变形记*的写作方式。我们将在创建网络时清理这些数据。
- en: Just for a sanity check, let’s check the entities from *Alice’s Adventures*
    *in Wonderland*!
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做个理智检查，让我们看看《爱丽丝梦游仙境》中的实体！
- en: '[PRE113]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: I agree with you, Alice.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我同意你的看法，爱丽丝。
- en: I have tweaked the loading function to load the book *Alice’s Adventures in
    Wonderland* and chop off any header or footer text. That’s actually kind of funny.
    OFF WITH THEIR HEAD(er)S! Let’s try extracting entities. I expect this to be a
    bit messy, as we are working with fantasy characters, but let’s see what happens.
    Something by Jane Austen might give better results. We’ll see!
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经调整了加载函数，以加载《爱丽丝梦游仙境》这本书，并去除了任何页眉或页脚文本。其实这挺有趣的。砍掉他们的头（或页头）！让我们尝试提取实体。我预期这会有点杂乱，因为我们在处理幻想类角色，但让我们看看会发生什么。也许简·奥斯汀的作品会给出更好的结果。我们拭目以待！
- en: '[PRE114]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: That is much better than I expected, but some junk has snuck in. We’ll use both
    of these entity lists for network creation and visualization. This is a good foundation
    for our next steps. Now that we have some pretty useful-looking entities, let’s
    work toward creating a Pandas DataFrame that we can load into a NetworkX graph!
    That’s what’s needed to convert an entity list into an actual social network.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 结果比我预期的要好，但还是有一些垃圾数据混进来了。我们将使用这两个实体列表来创建和可视化网络。这为我们的下一步工作打下了良好的基础。现在我们有了一些看起来非常有用的实体，接下来让我们着手创建一个可以加载到NetworkX图形中的Pandas
    DataFrame！这正是将实体列表转换为实际社交网络所需要的。
- en: 'That concludes our demonstration on using spaCy for both PoS tagging as well
    as NER. I hope you can see that although there was one additional dependency (the
    language model), the process of entity extraction was much simpler. Now, it is
    time to move on to what I think is the most exciting part: converting entity lists
    into network data, which is then used to create a social network, which we can
    visualize and investigate.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们关于使用spaCy进行词性标注（PoS tagging）和命名实体识别（NER）的演示。我希望你能看到，虽然增加了一个额外的依赖项（语言模型），但实体提取过程变得简单得多。现在，是时候进入我认为最激动人心的部分：将实体列表转换为网络数据，然后用这些数据创建社交网络，之后我们可以可视化并进行调查。
- en: Converting entity lists into network data
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将实体列表转换为网络数据
- en: 'Now that we have pretty clean entity data, it is time to convert it into a
    Pandas DataFrame that we can easily load into NetworkX for creating an actual
    social network graph. There’s a bit to unpack in that sentence, but this is our
    workflow:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经得到了相当干净的实体数据，接下来是将它转换成一个可以轻松加载到 NetworkX 中的 Pandas DataFrame，以便创建一个实际的社交网络图。这句话需要解释的内容有点多，但我们的工作流程是这样的：
- en: Load text.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载文本。
- en: Extract entities.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取实体。
- en: Create network data.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建网络数据。
- en: Create a graph using network data.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用网络数据创建图。
- en: Analyze the graph.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析图。
- en: Again, I use the terms graph and network interchangeably. That does cause confusion,
    but I did not come up with the names. I prefer to say “network,” but then people
    think I am talking about computer networks, so then I have to remind them that
    I am talking about graphs, and they then think I am talking about bar charts.
    You just can’t win when it comes to explaining graphs and networks to those who
    are not familiar, and even I get confused when people start talking about networks
    and graphs. Do you mean nodes and edges, or do you mean TCP/IP and bar charts?
    Oh well.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我将“图”和“网络”这两个术语交替使用。虽然这样会引起一些混淆，但这些名字不是我起的。我更喜欢说“网络”，但是人们会认为我在讲计算机网络，然后我不得不提醒他们我是在讲图，接着他们又会认为我在讲柱状图。对于不熟悉这些概念的人来说，解释图和网络真的很难，即使是我自己，在别人开始谈论网络和图时也会感到困惑。你是指节点和边，还是指
    TCP/IP 和柱状图？唉，真是没法赢。
- en: 'For this next part, we do have choices in how we implement this, but I will
    explain my typical method. Look at the entities from *Alice’s Adventures* *in
    Wonderland*:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们确实有多种方法来实现这一点，但我会解释我通常使用的方法。来看一下来自*爱丽丝梦游仙境*的实体：
- en: '[PRE115]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'In most of these, there are only two entities in each inner list, but sometimes,
    there are three or more. What I usually do is consider the first entity as the
    source and any additional entities as targets. What does that look like in a sentence?
    Let’s take this sentence: “*Jack and Jill went up the hill to say hi to their
    friend Mark.*” If we converted that into entities, we would have this list:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，每个内部列表中只有两个实体，但有时会有三个或更多。我通常做的是将第一个实体视为源，任何其他实体视为目标。那在句子中是怎样的呢？我们来看这个句子：“*Jack
    和 Jill 上山去向他们的朋友 Mark 打招呼*。”如果我们将其转换为实体，列表会是这样：
- en: '[PRE116]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'To implement my approach, I will take the first element of my list and add
    it to my sources list, and then take everything after the first element and add
    that to my target list. Here is what that looks like in code, but using entities
    from *Alice*:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现我的方法，我会将列表中的第一个元素添加到我的源列表中，然后将第一个元素后的所有内容添加到我的目标列表中。下面是用代码展示的样子，但我使用的是*Alice*中的实体：
- en: '[PRE117]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: Take a close look at the two lines that capture both `source` and `targets`.
    `source` is the first element of each entity list, and `targets` is everything
    after the first element of each entity list. Then, for each target, I add the
    source and target to `final_sources` and `final_targets`. I loop through `targets`
    because there can be one or more of them. There will never be more than one `source`,
    as it is the first element. This is important to understand because this procedure
    is crucial for how relationships are shown in the resulting social network. We
    could have used an alternative approach of linking each entity to the other, but
    I prefer my shown approach. Later lists may bridge any gaps if there is evidence
    of those relationships. How do our final sources look?
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细看一下捕获`source`和`targets`的两行代码。`source`是每个实体列表的第一个元素，`targets`是每个实体列表中除第一个元素外的所有内容。然后，对于每个目标，我将源和目标添加到`final_sources`和`final_targets`中。我循环遍历`targets`，因为它们可以是一个或多个。`source`永远不会多于一个，因为它是第一个元素。理解这一点很重要，因为这个过程对于如何在最终的社交网络中展示关系至关重要。我们本可以采用另一种方法，将每个实体相互连接，但我更喜欢我展示的方法。稍后的列表如果有关系证据，可能会填补任何空白。那么我们的最终源是什么样的呢？
- en: '[PRE118]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: Both look great. Remember, we used NER to capture people, places, and things,
    so this looks fine. Later, I will very easily drop a few of these sources and
    targets directly from the social network. This is good enough for now.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 两者看起来都不错。记住，我们使用了命名实体识别（NER）来捕获人物、地点和事物，所以这看起来没问题。稍后，我会直接从社交网络中删除一些源和目标。现在这样就足够了。
- en: The approach of taking the first element and linking it to targets is something
    that I still regularly consider. Another approach would be to take every entity
    that appears in the same sentence and link them together. I prefer my approach,
    but you should consider both options.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 将第一个元素与目标连接的这种方法，我仍然经常考虑。另一种方法是将同一句中出现的每个实体都连接起来。我更倾向于我的方法，但你应该考虑这两种选择。
- en: 'The first entity interacts with other entities from the same sentence, but
    it is not always the case that all entities interact with each other. For instance,
    look at this sentence:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实体与同一句中的其他实体互动，但并非所有实体都会彼此互动。例如，看看这句话：
- en: “*John went to see his good friend Aaron, and then he went to the park* *with
    Jake.*”
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: “*John 去看他的好朋友 Aaron，然后他和 Jake 一起去了公园*。”
- en: 'This is what the entity list would look like:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是实体列表的样子：
- en: '[PRE119]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'In this example, Aaron may know Jake, but we can’t tell for certain based on
    this sentence. The hope is that if there is a relationship, that will be picked
    up eventually. Maybe in another sentence, such as this one:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，Aaron 可能认识 Jake，但根据这句话我们无法确定。希望的是，如果确实存在某种关系，最终会被识别出来。也许在另一句话中，例如这句：
- en: “*Aaron and Jake went ice skating and then ate* *some pizza.*”
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: “*Aaron 和 Jake 去滑冰，然后吃了些比萨饼*。”
- en: After that sentence, there will be a definite connection. My preferred approach
    requires further evidence before connecting entities.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在那句话之后，将会有明确的联系。我的首选方法需要更多证据才能连接实体。
- en: 'We now have code to take an entity list and create two lists: `final_sources`
    and `final_targets`, but this isn’t practical for feeding to NetworkX to create
    a graph. Let’s do two more things: use these two lists to create a Pandas DataFrame,
    and then create a reusable function that takes any entity list and returns this
    DataFrame:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了一个代码，用来处理实体列表并创建两个列表：`final_sources` 和 `final_targets`，但这对于传递给 NetworkX
    来创建图形并不实用。我们做两个额外的事情：使用这两个列表创建一个 Pandas DataFrame，然后创建一个可重用的函数，接受任何实体列表并返回这个 DataFrame：
- en: '[PRE120]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: That looks great. Let’s see it in action!
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错。让我们看看它的实际效果！
- en: '[PRE121]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'This will display a DataFrame of network data consisting of source and target
    nodes. This is called an edge list:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示一个包含源节点和目标节点的网络数据的 DataFrame。这被称为边列表：
- en: '![Figure 4.5 – Pandas DataFrame of Alice in Wonderland entity relationships](img/B17105_04_005.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5 – 《爱丽丝梦游仙境》实体关系的 Pandas DataFrame](img/B17105_04_005.jpg)'
- en: Figure 4.5 – Pandas DataFrame of Alice in Wonderland entity relationships
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – 《爱丽丝梦游仙境》实体关系的 Pandas DataFrame
- en: Great. How does it do with our entities from *The Metamorphosis*?
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。那么它如何处理来自*变形记*的实体？
- en: '[PRE122]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'This will display the network edge list for *The Metamorphosis*:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示*变形记*的网络边列表：
- en: '![Figure 4.6 – Pandas DataFrame of The Metamorphosis entity relationships](img/B17105_04_006.jpg)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – 《变形记》实体关系的 Pandas DataFrame](img/B17105_04_006.jpg)'
- en: Figure 4.6 – Pandas DataFrame of The Metamorphosis entity relationships
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 《变形记》实体关系的 Pandas DataFrame
- en: Perfect, and the function is reusable!
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 完美，而且这个函数是可重用的！
- en: We are now ready to convert both of these into actual NetworkX graphs. This
    is where things get interesting, in my opinion. Everything we did previously was
    just pre-processing. Now, we get to play with networks and specifically social
    network analysis! After this chapter, we will primarily be learning about social
    network analysis and network science. There are some areas of NLP that I blew
    past, such as lemmatization and stemming, but I purposefully did so because they
    are less relevant to extracting entities than PoS tagging and NER. I recommend
    that you check out Duygu Altinok’s book *Mastering spaCy* if you want to go deeper
    into NLP. This is as far as we will go with NLP in this book because it is all
    we need.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备将这两个都转化为实际的 NetworkX 图形。我认为这是有趣的地方。我们之前做的所有事情只是预处理工作。现在，我们可以玩转网络，尤其是社交网络分析！在这一章之后，我们将主要学习社交网络分析和网络科学。至于自然语言处理（NLP）中的一些领域，我略过了，例如词形还原和词干提取，但我故意这样做，因为它们在实体提取上没有
    PoS 标注和命名实体识别（NER）那么重要。如果你想更深入了解 NLP，推荐你阅读 Duygu Altinok 的《Mastering spaCy》。在本书中，我们的
    NLP 内容就到这里，因为这已经是我们所需的全部内容。
- en: Converting network data into networks
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将网络数据转换为网络
- en: It is time to take our created network data and create two graphs, one for *Alice’s
    Adventures in Wonderland*, and another for *The Metamorphosis*. We aren’t going
    to dive deep into network analysis yet, as that is for later chapters. But let’s
    see how they look and see what insights emerge.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候拿出我们创建的网络数据，创建两个图，一个是 *Alice’s Adventures in Wonderland*，另一个是 *The Metamorphosis*。我们暂时不深入进行网络分析，那是在后面的章节里讲的内容。但让我们看看它们的样子，看看能从中得出什么见解。
- en: First, we need to import the NetworkX library, and then we need to create our
    graphs. This is extremely easy to do because we have created Pandas DataFrames,
    which NetworkX will use. This is the easiest way I have found of creating graphs.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入 NetworkX 库，然后创建我们的图。这非常简单，因为我们已经创建了 Pandas DataFrame，NetworkX 将使用这些
    DataFrame。这是我发现的创建图的最简单方法。
- en: 'First, if you haven’t done so yet, you need to install NetworkX. You can do
    so with the following command:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果你还没有安装 NetworkX，你需要通过以下命令安装：
- en: '[PRE123]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'Now that we have installed NetworkX, let’s create our two networks:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了 NetworkX，让我们来创建两个网络：
- en: '[PRE124]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'It’s that easy. We have already done the difficult work in our text pre-processing.
    Did it work? Let’s peek into each graph:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单。我们已经在文本预处理上完成了困难的工作。它成功了吗？让我们看一下每个图：
- en: '[PRE125]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: Nice! Already, we are gaining insights into the differences between both social
    networks. In a graph, a node is just a thing that has relationships with other
    nodes, typically. Nodes without any relationships are called isolates, but due
    to the way our graphs have been constructed, there will be no isolates. It’s not
    possible, as we looked for sentences with two or more entities. Try to take a
    mental picture of those two entities as dots with a line between them. That’s
    literally what a graph/network visualization looks like, except that there are
    typically many dots and many lines. The relationship that exists between two nodes
    is called an edge. You will need to understand the difference between nodes and
    edges to work on graphs.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！我们已经在了解两个社交网络之间的差异了。在图中，节点通常是与其他节点有关系的事物。没有任何关系的节点叫做孤立节点，但由于我们图的构建方式，不会有孤立节点。因为我们查找了包含两个或更多实体的句子。试着想象这两个实体就像两个点，中间有一条线。这实际上就是图/网络可视化的样子，只不过通常有许多点和许多线。两个节点之间的关系被称为边。你需要理解节点和边的区别，才能更好地处理图。
- en: Looking at the summary information about the *Alice* graph, we can see that
    there are 68 nodes (characters) and 71 edges (relationships between those characters).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下 *Alice* 图的汇总信息，我们可以看到有 68 个节点（角色）和 71 条边（这些角色之间的关系）。
- en: Looking at the summary information about the network from *The Metamorphosis*,
    we can see that there are only three nodes (characters) and three edges (relationships
    between those characters. When visualized, this is going to be a really basic
    network to look at, so I am glad that we did *Alice* as well.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下 *The Metamorphosis* 网络的汇总信息，我们可以看到只有三个节点（角色）和三条边（这些角色之间的关系）。当进行可视化时，这将是一个非常简单的网络，因此我很高兴我们也做了
    *Alice*。
- en: There are many other useful metrics and summaries tucked away inside NetworkX,
    and we will discuss those when we go over centralities, shortest paths, and other
    social network analysis and network science topics.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: NetworkX 中还隐藏着许多其他有用的度量和总结，我们将在讨论中心性、最短路径以及其他社交网络分析和网络科学主题时讨论这些内容。
- en: Doing a network visualization spot check
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做一个网络可视化抽查
- en: Let’s visualize these networks, take a brief look, and then complete this chapter.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化这些网络，快速看一眼，然后完成本章内容。
- en: Here are two visualization functions that I frequently use. In my opinion, `sknetwork`,
    I have not looked back to NetworkX for visualization.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个我经常使用的可视化函数。在我看来，使用 `sknetwork` 后，我再也没有回过头去使用 NetworkX 进行可视化。
- en: The first function converts a NetworkX graph into an adjacency matrix, which
    sknetwork uses to calculate `PageRank` (an importance score) and then to render
    the network as an SVG image. The second function uses the first function, but
    the goal is to visualize an `ego_graph`, which will be described later. In an
    ego graph, you explore the relationships that exist around a single node. The
    first function is more general-purpose.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数将 NetworkX 图转换为邻接矩阵，sknetwork 使用该矩阵来计算 `PageRank`（重要性分数），然后将网络渲染为 SVG 图像。第二个函数使用第一个函数，但目标是可视化
    `ego_graph`，稍后会介绍。在 ego 图中，你探索围绕单个节点存在的关系。第一个函数是更通用的。
- en: 'Enough talk. This will be more understandable when you see the results:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 说够了。你会在看到结果时更能理解：
- en: '[PRE126]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: Next, let’s create a function for displaying ego graphs.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个显示自我图的函数。
- en: 'To be clear, having `import` statements inside a function is not ideal. It
    is best to keep import statements external to functions. However, in this case,
    it makes it easier to copy and paste into your various Jupyter or Colab notebooks,
    so I am making an exception:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确，最好不要在函数内部包含`import`语句。最好将`import`语句保留在函数外部。然而，在这个情况下，这样做可以更容易地将代码复制并粘贴到你的Jupyter或Colab笔记本中，所以我做个例外：
- en: '[PRE127]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'Look closely at these two functions. Pick them apart and try to figure out
    what they are doing. To quickly complete this chapter, I’m going to show the results
    of using these functions. I have abstracted away the difficulty of visualizing
    these networks so that you can do this:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细看看这两个函数。拆解它们，试着弄清楚它们在做什么。为了快速完成本章内容，我将展示使用这些函数的结果。我已经抽象化了可视化这些网络的难度，让你可以做这件事：
- en: '[PRE128]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'As we are not passing any parameters to the function, this should display a
    very simple network visualization. It will look like a bunch of dots (nodes),
    with some dots connected to other dots by a line (edge):'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有向函数传递任何参数，这应该会显示一个非常简单的网络可视化。它将看起来像一堆点（节点），其中一些点通过线（边）连接到其他点：
- en: Important
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Please keep the `draw_graph` function handy. We will use it throughout this
    book.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 请将`draw_graph`函数保持在手边。我们将在本书中多次使用它。
- en: '![Figure 4.7 – Rough social network of Alice’s Adventures in Wonderland](img/B17105_04_007.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – 爱丽丝梦游仙境的粗略社交网络](img/B17105_04_007.jpg)'
- en: Figure 4.7 – Rough social network of Alice’s Adventures in Wonderland
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 爱丽丝梦游仙境的粗略社交网络
- en: 'Well, that’s a bit unhelpful to look at, but this is intentional. I typically
    work with large networks, so I prefer to keep node names left out at first so
    that I can visually inspect the network. However, you can override the default
    values I am using. Let’s do that as well as decrease the line width a bit, increase
    the node size, and add node names:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这看起来有点没帮助，但这是故意的。我通常处理的是大规模网络，因此我更倾向于一开始省略节点名称，以便可以直观地检查网络。不过，你可以覆盖我正在使用的默认值。让我们做这些，并稍微减少线条宽度，增加节点大小，并添加节点名称：
- en: '[PRE129]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: This will draw our social network, with labels!
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这将绘制我们的社交网络，并带有标签！
- en: '![Figure 4.8 – Labeled social network of Alice’s Adventures in Wonderland](img/B17105_04_008.jpg)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – 爱丽丝梦游仙境的标记社交网络](img/B17105_04_008.jpg)'
- en: Figure 4.8 – Labeled social network of Alice’s Adventures in Wonderland
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 爱丽丝梦游仙境的标记社交网络
- en: 'The font size is a bit small and difficult to read, so let’s increase that
    and reduce `node_size` by one:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 字体大小有点小，看起来有些难以阅读，所以我们将增加字体大小，并将`node_size`减小一个：
- en: '[PRE130]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'This creates the following network:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了以下网络：
- en: '![Figure 4.9 – Finalized social network of Alice in Wonderland](img/B17105_04_009.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – 爱丽丝梦游仙境的最终社交网络](img/B17105_04_009.jpg)'
- en: Figure 4.9 – Finalized social network of Alice in Wonderland
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 爱丽丝梦游仙境的最终社交网络
- en: That is excellent. Consider what we have done. We have taken raw text from *Alice*,
    extracted all entities, and built the social network that is described in this
    book. This is so powerful, and it also opens the door for you to learn more about
    social network analysis and network science. For instance, would you rather analyze
    somebody else’s toy dataset, or would you rather investigate something you are
    interested in, such as your favorite book? I prefer to chase my own curiosity.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了。想一想我们做了什么。我们从*爱丽丝*中提取了原始文本，提取了所有实体，并构建了书中描述的社交网络。这是如此强大，它还为你打开了学习更多关于社交网络分析和网络科学的大门。例如，你是想分析别人的玩具数据集，还是更愿意研究你感兴趣的东西，比如你最喜欢的书？我更喜欢追随自己的好奇心。
- en: Let’s see what the ego graph looks like around Alice!
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看爱丽丝周围的自我图是什么样子的！
- en: '[PRE131]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'This gives us the following network:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了以下网络：
- en: '![Figure 4.10 – Alice ego graph](img/B17105_04_010.jpg)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 爱丽丝自我图](img/B17105_04_010.jpg)'
- en: Figure 4.10 – Alice ego graph
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 爱丽丝自我图
- en: What?! That’s incredible. We can see that some trash came through in the entity
    list, but we’ll learn how to clean that up in the next chapter. We can take this
    one step further. What if we want to take Alice out of her ego graph and just
    explore the relationships that exist around her? Is that possible?
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 什么？！这太不可思议了。我们可以看到实体列表中有些杂乱的内容，但我们将在下一章学习如何清理这些内容。我们可以再进一步。如果我们想从她的自我图中取出爱丽丝，只探究她周围的关系，这可能吗？
- en: '[PRE132]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'This gives us the following visualization:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了以下的可视化：
- en: '![Figure 4.11 – Alice ego graph with dropped center](img/B17105_04_011.jpg)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11 – 爱丽丝自我图（去除中心）](img/B17105_04_011.jpg)'
- en: Figure 4.11 – Alice ego graph with dropped center
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 爱丽丝自我图（去除中心）
- en: Too easy. But it’s difficult to analyze the clusters of groups that exist. After
    dropping the center, many nodes became isolates. If only there were a way to remove
    the isolates so that we could more easily see the groups. OH, WAIT!
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 太简单了。但是分析现有的群体聚类是很困难的。在去除中心后，许多节点变成了孤立节点。如果有一种方法能去除这些孤立节点，我们就能更容易地看到群体。哦，等等！
- en: '[PRE133]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'We get the following output:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下的输出：
- en: '![Figure 4.12 – Alice ego graph with dropped center and dropped isolates](img/B17105_04_012.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12 – 爱丽丝自我图（去除中心和孤立节点）](img/B17105_04_012.jpg)'
- en: Figure 4.12 – Alice ego graph with dropped center and dropped isolates
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – 爱丽丝自我图（去除中心和孤立节点）
- en: Overall, the *Alice* social network looks pretty good. There’s some cleanup
    to do, but we can investigate relationships. What does the social network of *The
    Metamorphosis* look like? Remember, there are only three nodes and three edges.
    Even Alice’s ego graph is more complicated than the social network from *The Metamorphosis*.
    Let’s visualize it!
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，*爱丽丝*社交网络看起来相当不错。虽然还需要一些清理工作，但我们可以研究关系。那么*变形记*的社交网络是什么样的呢？记住，这里只有三个节点和三条边。即使是爱丽丝的自我图，也比*变形记*的社交网络要复杂。让我们来可视化它吧！
- en: '[PRE134]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'This code produces the following network:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码生成了以下网络：
- en: '![Figure 4.13 – Labeled social network of The Metamorphosis](img/B17105_04_013.jpg)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.13 – *变形记*的标记社交网络](img/B17105_04_013.jpg)'
- en: Figure 4.13 – Labeled social network of The Metamorphosis
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 – *变形记*的标记社交网络
- en: 'Wait, but why are there six edges? I only see three. The reason is that `sknetwork`
    will draw multiple edges as a single edge. We do have options, such as increasing
    the line width according to the number of edges but let’s just look at the Pandas
    DataFrame to make sure my thinking is correct:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，但为什么有六条边？我只看到了三条。原因是`sknetwork`会将多条边绘制为一条边。我们确实有一些选项，比如根据边的数量增加线条宽度，但我们还是来看看Pandas
    DataFrame，确保我的想法是正确的：
- en: '[PRE135]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'This gets us the following DataFrame:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下的DataFrame：
- en: '![Figure 4.14 – Pandas DataFrame of network data for The Metamorphosis](img/B17105_04_014.jpg)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.14 – *变形记*的网络数据 Pandas DataFrame](img/B17105_04_014.jpg)'
- en: Figure 4.14 – Pandas DataFrame of network data for The Metamorphosis
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – *变形记*的网络数据 Pandas DataFrame
- en: What happens if we drop duplicates?
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们去掉重复项，会发生什么？
- en: '[PRE136]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'We get the following DataFrame:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下的DataFrame：
- en: '![Figure 4.15 – Pandas DataFrame of network data for The Metamorphosis (dropped
    duplicates)](img/B17105_04_015.jpg)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.15 – *变形记*的网络数据 Pandas DataFrame（去除重复项）](img/B17105_04_015.jpg)'
- en: Figure 4.15 – Pandas DataFrame of network data for The Metamorphosis (dropped
    duplicates)
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 – *变形记*的网络数据 Pandas DataFrame（去除重复项）
- en: Aha! There is a relationship between Gregor and Grete, but the reverse is also
    true. One thing that I can see is that Samsa links to Gregor and Grete, but Grete
    does not link back to Samsa. Another way of saying this, which we will discuss
    in this book, is that directionality also matters. You can have a directed graph.
    In this case, I am just using an undirected graph, because relationships are often
    (but not always) reciprocal.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 啊哈！Gregor和Grete之间有关系，但反过来也是如此。我看到的一点是，Samsa与Gregor和Grete都有连接，但Grete没有回连到Samsa。换句话说，正如我们在本书中将要讨论的，方向性也是很重要的。你可以有一个有向图。在这种情况下，我只是使用了一个无向图，因为关系往往（但并非总是）是互惠的。
- en: This marks the end of this demonstration. We originally set out to take raw
    text and use it to create a social network, and we easily accomplished our goals.
    Now, we have network data to play with. Now, this book is going to get more interesting.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着本次演示的结束。我们最初的目标是使用原始文本创建一个社交网络，而我们轻松地达成了目标。现在，我们有了网络数据可以进行操作。接下来，本书将变得更加有趣。
- en: Additional NLP and network considerations
  id: totrans-444
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外的NLP和网络考虑
- en: This has been a marathon of a chapter. Please bear with me a little longer.
    I have a few final thoughts that I’d like to express, and then we can conclude
    this chapter.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章真是一次马拉松式的挑战。请再耐心等我一会儿。我有一些最后的想法想表达，然后我们就可以结束这一章了。
- en: Data cleanup
  id: totrans-446
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清理
- en: First, if you work with language data, there will always be cleanup. Language
    is messy and difficult. If you are only comfortable working with pre-cleaned tabular
    data, this is going to feel very messy. I love that, as every project allows me
    to improve my techniques and tactics.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果你处理的是语言数据，总是需要清理。语言是混乱且复杂的。如果你只习惯处理预先清理过的表格数据，那么这会显得很凌乱。我喜欢这种挑战，因为每个项目都能让我提高技巧和战术。
- en: 'I showed two different approaches for extracting entities: PoS tagging and
    NER. Both approaches work very well, but consider which approach gets us closer
    to a clean and useful entity list the quickest and easiest. With `PoS tagging`,
    we get one token at a time. With NER, we very quickly get to entities, but the
    models occasionally misbehave or don’t catch everything, so there is always cleanup
    with this as well.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我展示了两种提取实体的方法：PoS标注和NER。两种方法都非常有效，但考虑一下哪种方法能够更快、更轻松地让我们得到一个干净且有用的实体列表。使用`PoS标注`时，我们一次得到一个词元。而使用NER时，我们能很快得到实体，但模型有时会出错或漏掉一些内容，因此仍然需要进行清理。
- en: There is no silver bullet. I want to use whatever approach gets me as close
    to the goal as quickly as possible because cleanup is inevitable. The less correction
    I have to do, the quicker I am playing with and pulling insights out of networks.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 没有银弹。我希望使用任何能够让我尽快接近目标的方法，因为清理是不可避免的。越少的修正意味着我可以更快地开始从网络中提取洞察。
- en: Comparing PoS tagging and NER
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较PoS标注和NER
- en: PoS tagging can involve extra steps, but cleanup is often easier. On the other
    hand, NER can involve fewer steps, but you can get mangled results if you use
    it against scraped web text. There may be fewer steps for some things, but the
    cleanup may be daunting. I have seen spaCy’s NER false a lot on scraped web content.
    If you are dealing with web text, spend extra time on cleanup before feeding the
    data to NER.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: PoS标注可能需要额外的步骤，但清理通常更为简单。另一方面，NER可能步骤较少，但如果你将它应用于抓取的网页文本，可能会得到错误的结果。某些情况下步骤虽然少，但清理工作可能让人头疼。我曾看到spaCy的NER在抓取的网页内容上出现很多误标。如果你处理的是网页文本，在将数据送入NER之前，务必花些时间进行清理。
- en: Finally, slightly messy results are infinitely better than no results. This
    stuff is so useful for enriching datasets and extracting the “who, what, and where”
    parts of any piece of text.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，稍微有些混乱的结果远比没有结果要好得多。这些技术对于丰富数据集以及提取文本中的“谁、什么、在哪里”部分是非常有用的。
- en: Scraping considerations
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抓取注意事项
- en: There are also a few things to keep in mind when planning any scraping project.
    First, privacy. If you are scraping social media text, and you are extracting
    entities from someone else’s text, you are running surveillance on them. Ponder
    how you would feel if someone did the same to you. Further, if you store this
    data, you are storing personal data, and there may be legal considerations as
    well. To save yourself headaches, unless you work in government or law enforcement,
    it might be wise to just use these techniques against literature and news, until
    you have a plan for other types of content.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在规划任何抓取项目时，也有一些需要注意的事项。首先是隐私问题。如果你抓取社交媒体的文本，并从他人的文本中提取实体信息，那么你就相当于在对他们进行监控。想一想，如果有人对你做同样的事情，你会有什么感受。此外，如果你存储这些数据，你就存储了个人数据，这也可能涉及到法律问题。为了避免麻烦，除非你在政府或执法机关工作，否则最好仅针对文学和新闻使用这些技术，直到你有了处理其他类型内容的计划。
- en: There are also ethical considerations. If you decide to use these techniques
    to build a surveillance engine, you should consider whether building this is an
    ethical thing to do. Consider whether it is ethical to run surveillance on random
    strangers.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 还有伦理问题。如果你决定使用这些技术来构建一个监控引擎，你应该考虑构建这样一个系统是否符合伦理。考虑一下对随机陌生人进行监控是否是道德的行为。
- en: Finally, scraping is like browsing a website, automatically, but scrapers can
    do damage. If you hit a website with a scraper a thousand times in a second, you
    could accidentally hit it with a **DoS** attack. Get what you need at the pace
    that you need it. If you are looping through all of the links on a website and
    then scraping them, add a 1-second delay before each scrape rather than hitting
    it a thousand times every second. You will be liable if you take down a web server,
    even by accident.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，抓取就像是自动浏览一个网站，但抓取工具可能会造成损害。如果你在一秒钟内用抓取工具访问一个网站一千次，你可能不小心触发了**DoS**攻击。按你需要的速度获取你需要的数据。如果你在循环遍历网站上的所有链接并进行抓取，最好在每次抓取前加一个1秒的延迟，而不是每秒钟抓取一千次。即便是无意中，你也要为导致网站服务器瘫痪的后果负责。
- en: That was a lot of words just to say that unless you are using this for news
    or literature, be mindful of what you are doing. For news and literature, this
    can be revealing and may allow new technologies to be created. For other types
    of content, think about what you are doing before you jump into the work.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多话只是为了告诉你，除非你是为了新闻或文学用途，否则要注意你在做什么。对于新闻和文学内容来说，这可能会揭示一些信息，并可能促使新技术的诞生。对于其他类型的内容，在动手之前请先考虑清楚你在做什么。
- en: Summary
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we learned how to find and scrape raw text, convert it into
    an entity list, and then convert that entity list into an actual social network
    so that we can investigate revealed entities and relationships. Did we capture
    the *who*, *what*, and *where* of a piece of text? Absolutely. I hope you can
    now understand the usefulness of NLP and social network analysis when used together.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了如何找到并抓取原始文本，将其转化为实体列表，然后将实体列表转化为实际的社交网络，以便我们可以调查揭示出的实体和关系。我们是否捕获了文本中的*谁*、*什么*和*哪里*？绝对是的。我希望你现在能理解当自然语言处理（NLP）和社交网络分析结合使用时的有用性。
- en: In this chapter, I showed several ways to get data. If you are not familiar
    with web scraping, then this might seem a bit overwhelming, but it’s not so bad
    once you get started. However, in the next chapter, I will show several easier
    ways to get data.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我展示了几种获取数据的方法。如果你不熟悉网页抓取，这可能会显得有些压倒性，但一旦开始，你会发现其实并不难。不过，在下一章中，我会展示几种更简单的数据获取方法。
