- en: Testing and Debugging Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试和调试Spark
- en: In an ideal world, we write perfect Spark codes and everything runs perfectly
    all the time, right? Just kidding; in practice, we know that working with large-scale
    datasets is hardly ever that easy, and there are inevitably some data points that
    will expose any corner cases with your code.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想世界中，我们编写的Spark代码完美无缺，一切总是运行得完美无瑕，对吧？开个玩笑；实际上，我们知道处理大规模数据集几乎从未那么简单，总会有一些数据点暴露出你代码中的边缘情况。
- en: 'Considering the aforementioned challenges, therefore, in this chapter, we will
    see how difficult it can be to test an application if it is distributed; then,
    we will see some ways to tackle this. In a nutshell, the following topics will
    be cover throughout this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到上述挑战，因此，在本章中，我们将探讨如果应用程序是分布式的，测试它会有多困难；然后，我们将探讨一些应对方法。简而言之，本章将涵盖以下主题：
- en: Testing in a distributed environment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式环境下的测试
- en: Testing Spark application
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试Spark应用程序
- en: Debugging Spark application
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调试Spark应用程序
- en: Testing in a distributed environment
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式环境下的测试
- en: 'Leslie Lamport defined the term distributed system as follows:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Leslie Lamport将分布式系统定义如下：
- en: '"A distributed system is one in which I cannot get any work done because some
    machine I have never heard of has crashed."'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '"分布式系统是指由于某些我从未听说过的机器崩溃，导致我无法完成任何工作的系统。"'
- en: Resource sharing through **World Wide Web** (aka **WWW**), a network of connected
    computers (aka a cluster), is a good example of distributed systems. These distributed
    environments are often complex and lots of heterogeneity occurs frequently. Testing
    in these kinds of heterogeneous environments is also challenging. In this section,
    at first, we will observe some commons issues that are often raised while working
    with such a system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**万维网**（又称**WWW**），一个连接的计算机网络（又称集群）共享资源，是分布式系统的一个好例子。这些分布式环境通常很复杂，经常出现大量异质性。在这些异质环境中进行测试也是具有挑战性的。在本节中，首先，我们将观察在处理此类系统时经常出现的一些常见问题。
- en: Distributed environment
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式环境
- en: 'There are numerous definitions of distributed systems. Let''s see some definition
    and then we will try to correlate the aforementioned categories afterward. Coulouris
    defines a distributed system as *a system in which hardware or software components
    located at networked computers communicate and coordinate their actions only by
    message passing*. On the other hand, Tanenbaum defines the term in several ways:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式系统有众多定义。让我们看一些定义，然后我们将尝试将上述类别与之关联。Coulouris将分布式系统定义为*一个系统，其中位于网络计算机上的硬件或软件组件仅通过消息传递进行通信和协调其动作*。另一方面，Tanenbaum以几种方式定义了这个术语：
- en: '*A collection of independent computers that appear to the users of the system
    as a single computer.*'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一组独立的计算机，对系统用户而言，它们表现为一台单一的计算机。*'
- en: '*A system that consists of a collection of two or more independent Computers
    which coordinate their processing through the exchange of synchronous or asynchronous
    message passing.*'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*由两个或多个独立计算机组成的系统，它们通过同步或异步消息传递协调其处理。*'
- en: '*A distributed system is a collection of autonomous computers linked by a network
    with software designed to produce an integrated computing facility.*'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分布式系统是一组通过网络连接的自主计算机，其软件设计旨在提供一个集成的计算设施。*'
- en: 'Now, based on the preceding definition, distributed systems can be categorized
    as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基于前面的定义，分布式系统可以分类如下：
- en: 'Only hardware and software are distributed: The local distributed system is
    connected through LAN.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有硬件和软件是分布式的：通过LAN连接的本地分布式系统。
- en: Users are distributed, but there are computing and hardware resources that are
    running backend, for example, WWW.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户是分布式的，但存在运行后端的计算和硬件资源，例如WWW。
- en: 'Both users and hardware/software are distributed: Distributed computing cluster
    that is connected through WAN. For example, you can get these types of computing
    facilities while using Amazon AWS, Microsoft Azure, Google Cloud, or Digital Ocean''s
    droplets.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户和硬件/软件都是分布式的：通过WAN连接的分布式计算集群。例如，在使用Amazon AWS、Microsoft Azure、Google Cloud或Digital
    Ocean的droplets时，你可以获得这类计算设施。
- en: Issues in a distributed system
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式系统中的问题
- en: Here we will discuss some major issues that need to be taken care of during
    the software and hardware testing so that Spark jobs run smoothly in cluster computing,
    which is essentially a distributed computing environment.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此讨论软件和硬件测试期间需要注意的一些主要问题，以确保Spark作业在集群计算中顺畅运行，集群计算本质上是一种分布式计算环境。
- en: 'Note that all the issues are unavoidable, but we can at least tune them for
    betterment. You should follow the instructions and recommendations given in the
    previous chapter. According to *Kamal Sheel Mishra* and *Anil Kumar Tripathi*,
    *Some Issues, Challenges and Problems of Distributed Software System*, in *International
    Journal of Computer Science and Information Technologies*, Vol. 5 (4), 2014, 4922-4925\.
    URL: [https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf](https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf),
    there ...'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有这些问题都是不可避免的，但我们可以至少对其进行优化。您应遵循上一章节中给出的指导和建议。根据*卡马尔·希尔·米什拉*和*阿尼尔·库马尔·特里帕蒂*在《国际计算机科学与信息技术杂志》第5卷（4），2014年，4922-4925页中的《分布式软件系统的某些问题、挑战和问题》，网址为[https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf](https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf)，其中...
- en: Challenges of software testing in a distributed environment
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式环境中的软件测试挑战
- en: There are some common challenges associated with the tasks in an agile software
    development, and those challenges become more complex while testing the software
    in a distributed environment before deploying them eventually. Often team members
    need to merge the software components in parallel after the bugs proliferating.
    However, based on urgency, often the merging occurs before the testing phase.
    Sometimes, many stakeholders are distributed across teams. Therefore, there's
    a huge potential for misunderstanding and teams often lose in between.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在敏捷软件开发中，与任务相关的一些常见挑战，在最终部署前在分布式环境中测试软件时变得更加复杂。团队成员经常需要在错误激增后并行合并软件组件。然而，根据紧急程度，合并往往发生在测试阶段之前。有时，许多利益相关者分布在不同的团队中。因此，存在巨大的误解潜力，团队往往在其中迷失。
- en: For example, Cloud Foundry ([https://www.cloudfoundry.org/](https://www.cloudfoundry.org/))
    is an open source heavily distributed PaaS software system for managing deployment
    and scalability of applications in the Cloud. It promises different features such
    as scalability, reliability, and elasticity that come inherently to deployments
    on Cloud Foundry require the underlying distributed system to implement measures
    to ensure robustness, resiliency, and failover.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Cloud Foundry（[https://www.cloudfoundry.org/](https://www.cloudfoundry.org/)）是一个开源的、高度分布式的PaaS软件系统，用于管理云中应用程序的部署和可扩展性。它承诺提供诸如可扩展性、可靠性和弹性等特性，这些特性在Cloud
    Foundry上的部署中是固有的，需要底层分布式系统实施措施以确保鲁棒性、弹性和故障转移。
- en: 'The process of software testing is long known to comprise *unit testing*, *integration
    testing*, *smoke testing*, *acceptance testing*, *scalability testing*, *performance
    testing*, and *quality of service testing*. In Cloud Foundry, the process of testing
    a distributed system is shown in the following figure:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 软件测试的过程早已被熟知包括*单元测试*、*集成测试*、*冒烟测试*、*验收测试*、*可扩展性测试*、*性能测试*和*服务质量测试*。在Cloud Foundry中，分布式系统的测试过程如下图所示：
- en: '![](img/3c974ded-af48-4a29-a107-4dce0e42a32a.png)**Figure 1:** An example of
    software testing in a distributed environment like Cloud'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/3c974ded-af48-4a29-a107-4dce0e42a32a.png)**图1：** 类似Cloud的分布式环境中软件测试的一个示例'
- en: As shown in the preceding figure (first column), the process of testing in a
    distributed environment like Cloud starts with running unit tests against the
    smallest points of contract in the system. Following successful execution of all
    the unit tests, integration tests are run to validate the behavior of interacting
    components as part of a single coherent software system (second column) running
    on a single box (for example, a **Virtual Machine** (**VM**) or bare metal). However,
    while these tests validate the overall behavior of the system as a monolith, they
    do not guarantee system validity in a distributed deployment. Once integration
    tests pass, the next step (third column) is to validate distributed deployment
    of the system and run the smoke tests.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图（第一列）所示，在云这样的分布式环境中进行测试的过程始于对系统中最小的接触点运行单元测试。在所有单元测试成功执行后，运行集成测试以验证作为单个连贯软件系统（第二列）一部分的交互组件的行为，该系统在单个盒子（例如，**虚拟机**（**VM**）或裸机）上运行。然而，虽然这些测试验证了系统作为单体的整体行为，但它们并不能保证系统在分布式部署中的有效性。一旦集成测试通过，下一步（第三列）就是验证系统的分布式部署并运行冒烟测试。
- en: As you know, that the successful configuration of the software and execution
    of unit tests prepares us to validate acceptability of system behavior. This verification
    is done by running acceptance tests (fourth column). Now, to overcome the aforementioned
    issues and challenges in distributed environments, there are also other hidden
    challenges that need to be solved by researchers and big data engineers, but those
    are actually out of the scope of this book.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，软件的成功配置和单元测试的执行使我们能够验证系统行为的可接受性。这种验证是通过运行验收测试（第四列）来完成的。现在，为了克服分布式环境中上述问题和挑战，还有其他隐藏的挑战需要由研究人员和大数据工程师解决，但这些实际上超出了本书的范围。
- en: Now that we know what real challenges are for the software testing in a distributed
    environment, now let's start testing our Spark code a bit. The next section is
    dedicated to testing Spark applications.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道在分布式环境中软件测试面临的真正挑战是什么，现在让我们开始测试我们的Spark代码。下一节专门介绍测试Spark应用程序。
- en: Testing Spark applications
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试Spark应用程序
- en: 'There are many ways to try to test your Spark code, depending on whether it''s
    Java (you can do basic JUnit tests to test non-Spark pieces) or ScalaTest for
    your Scala code. You can also do full integration tests by running Spark locally
    or on a small test cluster. Another awesome choice from Holden Karau is using
    Spark-testing base. You probably know that there is no native library for unit
    testing in Spark as of yet. Nevertheless, we can have the following two alternatives
    to use two libraries:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试测试Spark代码的方法有很多，取决于它是Java（你可以进行基本的JUnit测试来测试非Spark部分）还是ScalaTest用于你的Scala代码。你还可以通过在本地或小型测试集群上运行Spark来进行完整的集成测试。Holden
    Karau提供的另一个很棒的选择是使用Spark-testing base。你可能知道，到目前为止，Spark还没有原生的单元测试库。尽管如此，我们可以使用以下两个库作为替代方案：
- en: ScalaTest
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ScalaTest
- en: Spark-testing base
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark-testing base
- en: However, before starting to test your Spark applications written in Scala, some
    background knowledge about unit testing and testing Scala methods is a mandate.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在开始测试用Scala编写的Spark应用程序之前，了解单元测试和测试Scala方法的一些背景知识是必要的。
- en: Testing Scala methods
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试Scala方法
- en: 'Here, we will see some simple techniques for testing Scala methods. For Scala
    users, this is the most familiar unit testing framework (you can also use it for
    testing Java code and soon for JavaScript). ScalaTest supports a number of different
    testing styles, each designed to support a specific type of testing need. For
    details, see ScalaTest User Guide at [http://www.scalatest.org/user_guide/selecting_a_style](http://www.scalatest.org/user_guide/selecting_a_style).
    Although ScalaTest supports many styles, one of the quickest ways to get started
    is to use the following ScalaTest traits and write the tests in the **TDD** (**test-driven
    development**) style:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将看到一些测试Scala方法的简单技巧。对于Scala用户来说，这是最熟悉的单元测试框架（你也可以用它来测试Java代码，很快也可以用于JavaScript）。ScalaTest支持多种不同的测试风格，每种风格都是为了支持特定类型的测试需求而设计的。详情请参阅ScalaTest用户指南，网址为[http://www.scalatest.org/user_guide/selecting_a_style](http://www.scalatest.org/user_guide/selecting_a_style)。尽管ScalaTest支持多种风格，但最快上手的方法之一是使用以下ScalaTest特性，并以**TDD**（**测试驱动开发**）风格编写测试：
- en: '`FunSuite`'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`FunSuite`'
- en: '`Assertions`'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Assertions`'
- en: '`BeforeAndAfter`'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`BeforeAndAfter`'
- en: Feel free to browse the preceding URLs to learn more about these traits; that
    will make the rest of this tutorial go smoothly.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎浏览上述URL以了解更多关于这些特性的信息，这将使本教程的其余部分顺利进行。
- en: It is to be noted that the TDD is a programming technique to develop software,
    and it states that you should start development from tests. Hence, it doesn't
    affect how tests are written, but when tests are written. There is no trait or
    testing style to enforce or encourage TDD in `ScalaTest.FunSuite`, `Assertions`,
    and `BeforeAndAfter` are only more similar to the xUnit testing frameworks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，TDD是一种开发软件的编程技术，它指出您应该从测试开始开发。因此，它不影响测试的编写方式，而是影响测试的编写时机。在`ScalaTest.FunSuite`、`Assertions`和`BeforeAndAfter`中没有特质或测试风格来强制或鼓励TDD，它们仅与xUnit测试框架更为相似。
- en: 'There are three assertions available in the ScalaTest in any style trait:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在ScalaTest的任何风格特质中，有三种断言可用：
- en: '`assert`: This is used for general assertions in your Scala program.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assert`：这在您的Scala程序中用于通用断言。'
- en: '`assertResult`: This helps differentiate expected value from the actual values.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assertResult`：这有助于区分预期值与实际值。'
- en: '`assertThrows`: This is used to ensure a bit of code throws an expected exception.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assertThrows`：这用于确保一段代码抛出预期的异常。'
- en: 'The ScalaTest''s assertions are defined in the trait `Assertions`, which is
    further extended by `Suite`. In brief, the `Suite` trait is the super trait for
    all the style traits. According to the ScalaTest documentation at [http://www.scalatest.org/user_guide/using_assertions](http://www.scalatest.org/user_guide/using_assertions),
    the `Assertions` trait also provides the following features:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ScalaTest的断言定义在特质`Assertions`中，该特质进一步被`Suite`扩展。简而言之，`Suite`特质是所有风格特质的超特质。根据ScalaTest文档（[http://www.scalatest.org/user_guide/using_assertions](http://www.scalatest.org/user_guide/using_assertions)），`Assertions`特质还提供了以下功能：
- en: '`assume` to conditionally cancel a test'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assume` 用于条件性地取消测试'
- en: '`fail` to fail a test unconditionally'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fail` 无条件地使测试失败'
- en: '`cancel` to cancel a test unconditionally'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cancel` 无条件取消测试'
- en: '`succeed` to make a test succeed unconditionally'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`succeed` 使测试无条件成功'
- en: '`intercept` to ensure a bit of code throws an expected exception and then make
    assertions about the exception'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intercept` 确保一段代码抛出预期的异常，然后对异常进行断言'
- en: '`assertDoesNotCompile` to ensure a bit of code does not compile'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assertDoesNotCompile` 确保一段代码无法编译'
- en: '`assertCompiles` to ensure a bit of code does compile'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assertCompiles` 确保一段代码能够编译'
- en: '`assertTypeError` to ensure a bit of code does not compile because of a type
    (not parse) error'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assertTypeError` 确保一段代码因类型（非解析）错误而无法编译'
- en: '`withClue` to add more information about a failure'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`withClue` 用于添加有关失败的更多信息'
- en: 'From the preceding list, we will show a few of them. In your Scala program,
    you can write assertions by calling `assert` and passing a `Boolean` expression
    in. You can simply start writing your simple unit test case using `Assertions`.
    The `Predef` is an object, where this behavior of assert is defined. Note that
    all the members of the `Predef` get imported into your every Scala source file.
    The following source code will print `Assertion success` for the following case:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述列表中，我们将展示其中几个。在您的Scala程序中，您可以通过调用`assert`并传递一个`Boolean`表达式来编写断言。您可以简单地开始编写您的简单单元测试用例，使用`Assertions`。`Predef`是一个对象，其中定义了assert的这种行为。请注意，`Predef`的所有成员都会被导入到您的每个Scala源文件中。以下源代码将针对以下情况打印`Assertion
    success`：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'However, if you make `a = 2` and `b = 1`, for example, the assertion will fail
    and you will experience the following output:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您设置`a = 2`和`b = 1`，例如，断言将失败，您将看到以下输出：
- en: '![](img/7f37e532-3351-4ae6-80a6-4679d0579aec.png)**Figure 2:** An example of
    assertion fail'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/7f37e532-3351-4ae6-80a6-4679d0579aec.png)**图2：**断言失败的示例'
- en: If you pass a true expression, assert will return normally. However, assert
    will terminate abruptly with an Assertion Error if the supplied expression is
    false. Unlike the `AssertionError` and `TestFailedException` forms, the ScalaTest's
    assert provides more information that will tell you exactly in which line the
    test case failed or for which expression. Therefore, ScalaTest's assert provides
    better error messages than Scala's assert.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您传递一个真表达式，assert将正常返回。然而，如果提供的表达式为假，assert将以AssertionError异常突然终止。与`AssertionError`和`TestFailedException`形式不同，ScalaTest的assert提供了更多信息，它会告诉您确切在哪一行测试用例失败或对于哪个表达式。因此，ScalaTest的assert提供的错误信息比Scala的assert更优。
- en: 'For example, for the following source code, you should experience `TestFailedException`
    that will tell that 5 did not equal 4:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于以下源代码，您应该会遇到`TestFailedException`，它会告诉您5不等于4：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following figure shows the output of the preceding Scala test:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了前述Scala测试的输出：
- en: '![](img/2ac606f4-4805-4633-a8c5-5a99799c8f3e.png)**Figure 3:** An example of
    TestFailedException'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/2ac606f4-4805-4633-a8c5-5a99799c8f3e.png)**图3：**TestFailedException的一个示例'
- en: 'The following source code explains the use of the `assertResult` unit test
    to test the result of your method:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下源代码说明了使用`assertResult`单元测试来测试您方法结果的用法：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding assertion will be failed and Scala will throw an exception `TestFailedException`
    and prints `Expected 3 but got 4` (*Figure 4*):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 上述断言将会失败，Scala将抛出异常`TestFailedException`并打印出`Expected 3 but got 4`（*图4*）：
- en: '![](img/1b1bcd86-c72c-494a-b7b0-6ce8c22ddc47.png)**Figure 4:** Another example
    of TestFailedException'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/1b1bcd86-c72c-494a-b7b0-6ce8c22ddc47.png)**图4：**TestFailedException的另一个示例'
- en: 'Now, let''s see a unit testing to show expected exception:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一个单元测试，展示预期的异常：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you try to access an array element outside the index, the preceding code
    will tell you if you''re allowed to access the first character of the preceding
    string `Hello world!`. If your Scala program can access the value in an index,
    the assertion will fail. This also means that the test case has failed. Thus,
    the preceding test case will fail naturally since the first index contains the
    character `H`, and you should experience the following error message (*Figure
    5*):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尝试访问超出索引范围的数组元素，上述代码将告诉您是否允许访问前述字符串`Hello world!`的第一个字符。如果您的Scala程序能够访问索引中的值，断言将会失败。这也意味着测试案例失败了。因此，由于第一个索引包含字符`H`，上述测试案例自然会失败，您应该会遇到以下错误信息（*图5*）：
- en: '![](img/53b86f5b-c893-4daa-8061-2a9f025b803b.png)**Figure 5:** Third example
    of TestFailedException'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/53b86f5b-c893-4daa-8061-2a9f025b803b.png)**图5：**TestFailedException的第三个示例'
- en: 'However, now let''s try to access the index at position `-1` as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在让我们尝试访问位于`-1`位置的索引，如下所示：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now the assertion should be true, and consequently, the test case will be passed.
    Finally, the code will terminate normally. Now, let''s check our code snippets
    if it will compile or not. Very often, you may wish to ensure that a certain ordering
    of the code that represents emerging "user error" does not compile at all. The
    objective is to check the strength of the library against the error to disallow
    unwanted result and behavior. ScalaTest''s `Assertions` trait includes the following
    syntax for that purpose:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在断言应为真，因此测试案例将会通过。最后，代码将正常终止。现在，让我们检查我们的代码片段是否能编译。很多时候，您可能希望确保代表潜在“用户错误”的特定代码顺序根本不编译。目的是检查库对错误的抵抗力，以防止不希望的结果和行为。ScalaTest的`Assertions`特质为此目的包括了以下语法：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you want to ensure that a snippet of code does not compile because of a
    type error (as opposed to a syntax error), use the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想确保由于类型错误（而非语法错误）某段代码不编译，请使用以下方法：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A syntax error will still result on a thrown `TestFailedException`. Finally,
    if you want to state that a snippet of code does compile, you can make that more
    obvious with the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 语法错误仍会导致抛出`TestFailedException`。最后，如果您想声明某段代码确实编译通过，您可以通过以下方式使其更加明显：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'A complete example is shown as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 完整示例如下所示：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output of the preceding code is shown in the following figure:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出显示在以下图中：
- en: '![](img/2fbf984e-a29a-4dda-a9ff-389d870142c4.png)**Figure 6:** Multiple tests
    together'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/2fbf984e-a29a-4dda-a9ff-389d870142c4.png)**图6：**多个测试合并进行'
- en: Now we would like to finish the Scala-based unit testing due to page limitation.
    However, for other unit test cases, you can refer the Scala test guideline at
    [http://www.scalatest.org/user_guide](http://www.scalatest.org/user_guide).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅限制，我们希望结束基于Scala的单元测试。但对于其他单元测试案例，您可以参考[Scala测试指南](http://www.scalatest.org/user_guide)。
- en: Unit testing
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单元测试
- en: In software engineering, often, individual units of source code are tested to
    determine whether they are fit for use or not. This way of software testing method
    is also called the unit testing. This testing ensures that the source code developed
    by a software engineer or developer meets the design specifications and works
    as intended.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程中，通常会对源代码的各个单元进行测试，以确定它们是否适合使用。这种软件测试方法也称为单元测试。这种测试确保软件工程师或开发者编写的源代码符合设计规范并按预期工作。
- en: 'On the other hand, the goal of unit testing is to separate each part of the
    program (that is, in a modular way). Then try to observe if all the individual
    parts are working normally. There are several benefits of unit testing in any
    software system:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，单元测试的目标是将程序的每个部分（即以模块化的方式）分开。然后尝试观察所有单独的部分是否正常工作。单元测试在任何软件系统中都有几个好处：
- en: '**Find problems early:** It finds bugs or missing parts of the specification
    early in the development cycle.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**早期发现问题：** 它在开发周期的早期发现错误或规范中缺失的部分。'
- en: '**Facilitates change:** It helps in refactoring ...'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**便于变更：** 它有助于重构...'
- en: Testing Spark applications
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试Spark应用程序
- en: 'We have already seen how to test your Scala code using built-in `ScalaTest`
    package of Scala. However, in this subsection, we will see how we could test our
    Spark application written in Scala. The following three methods will be discussed:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何使用Scala内置的`ScalaTest`包测试Scala代码。然而，在本小节中，我们将看到如何测试我们用Scala编写的Spark应用程序。以下三种方法将被讨论：
- en: '**Method 1:** Testing Spark applications using JUnit'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方法1：** 使用JUnit测试Spark应用程序'
- en: '**Method 2:** Testing Spark applications using `ScalaTest` package'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方法2：** 使用`ScalaTest`包测试Spark应用程序'
- en: '**Method 3:** Testing Spark applications using Spark testing base'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方法3：** 使用Spark测试基进行Spark应用程序测试'
- en: Methods 1 and 2 will be discussed here with some practical codes. However, a
    detailed discussion on method 3 will be provided in the next subsection. To keep
    the understanding easy and simple, we will use the famous word counting applications
    to demonstrate methods 1 and 2.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 方法1和2将在这里讨论，并附带一些实际代码。然而，方法3的详细讨论将在下一小节中提供。为了保持理解简单明了，我们将使用著名的单词计数应用程序来演示方法1和2。
- en: 'Method 1: Using Scala JUnit test'
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法1：使用Scala JUnit测试
- en: 'Suppose you have written an application in Scala that can tell you how many
    words are there in a document or text file as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经编写了一个Scala应用程序，它可以告诉你文档或文本文件中有多少单词，如下所示：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The preceding code simply parses a text file and performs a `flatMap` operation
    by simply splitting the words. Then, it performs ...
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码简单地解析一个文本文件，并通过简单地分割单词执行`flatMap`操作。然后，它执行...
- en: 'Method 2: Testing Scala code using FunSuite'
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法2：使用FunSuite测试Scala代码
- en: 'Now, let''s redesign the preceding test case by returning only the RDD of the
    texts in the document, as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过仅返回文档中文本的RDD来重新设计前面的测试案例，如下所示：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So, the `prepareWordCountRDD()` method in the preceding class returns an RDD
    of string and integer values. Now, if we want to test the `prepareWordCountRDD()`
    method''s functionality, we can do it more explicit by extending the test class
    with `FunSuite` and `BeforeAndAfterAll` from the `ScalaTest` package of Scala.
    The testing works in the following ways:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，前面类中的`prepareWordCountRDD()`方法返回一个字符串和整数值的RDD。现在，如果我们想要测试`prepareWordCountRDD()`方法的功能，我们可以通过扩展测试类与`FunSuite`和`BeforeAndAfterAll`从Scala的`ScalaTest`包来更明确地进行。测试工作的方式如下：
- en: Extend the test class with `FunSuite` and `BeforeAndAfterAll` from the `ScalaTest`
    package of Scala
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过扩展`FunSuite`和`BeforeAndAfterAll`从Scala的`ScalaTest`包来扩展测试类
- en: Override the `beforeAll()` that creates Spark context
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖`beforeAll()`方法以创建Spark上下文
- en: Perform the test using the `test()` method and use the `assert()` method inside
    the `test()` method
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`test()`方法执行测试，并在`test()`方法内部使用`assert()`方法
- en: Override the `afterAll()` method that stops the Spark context
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖`afterAll()`方法以停止Spark上下文
- en: 'Based on the preceding steps, let''s see a class for testing the preceding
    `prepareWordCountRDD()` method:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的步骤，让我们看一个用于测试前面`prepareWordCountRDD()`方法的类：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The first test says that if two RDDs materialize in two different ways, the
    contents should be the same. Thus, the first test should get passed. We will see
    this in the following example. Now, for the second test, as we have seen previously,
    the word count of RDD is 214, but let's assume it unknown for a while. If it's
    214 coincidentally, the test case should pass, which is its expected behavior.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个测试表明，如果两个RDD以两种不同的方式实现，内容应该相同。因此，第一个测试应该通过。我们将在下面的例子中看到这一点。现在，对于第二个测试，正如我们之前所见，RDD的单词计数为214，但让我们暂时假设它是未知的。如果它恰好是214，测试案例应该通过，这是其预期行为。
- en: 'Thus, we are expecting both tests to be passed. Now, on Eclipse, run the test
    suite as `ScalaTest-File`, as shown in the following figure:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们期望两个测试都通过。现在，在Eclipse中，运行测试套件为`ScalaTest-File`，如下所示：
- en: '![](img/87342913-3a2b-4a9e-8a70-45cffdb44044.png) **Figure 10:** running the
    test suite as ScalaTest-File'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/87342913-3a2b-4a9e-8a70-45cffdb44044.png) **图10:** 以ScalaTest-File形式运行测试套件'
- en: Now you should observe the following output (*Figure 11*). The output shows
    how many test cases we performed and how many of them passed, failed, canceled,
    ignored, or was in pending. It also shows the time to execute the overall test.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您应该观察到以下输出（*图11*）。输出显示了我们执行了多少测试案例，以及其中有多少通过了、失败了、被取消了、被忽略了或处于待定状态。它还显示了执行整个测试所需的时间。
- en: '![](img/0db3c30f-5d87-43d8-a784-4cbbf13bb513.png)**Figure 11:** Test result
    when running the two test suites as ScalaTest-file'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/0db3c30f-5d87-43d8-a784-4cbbf13bb513.png)**图11:** 运行两个测试套件作为ScalaTest-file时的测试结果'
- en: 'Fantastic! The test case passed. Now, let''s try changing the compare value
    in the assertion in the two separate tests using the `test()` method as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！测试案例通过了。现在，让我们尝试在两个单独的测试中通过使用`test()`方法改变断言中的比较值：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, you should expect that the test case will be failed. Now run the earlier
    class as `ScalaTest-File` (*Figure 12*):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该预料到测试案例会失败。现在运行之前的类作为`ScalaTest-File`（*图12*）：
- en: '![](img/7ca0967a-280e-46f0-a6e8-934dbcc0e93f.png)**Figure 12:** Test result
    when running the preceding two test suites as ScalaTest-File'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/7ca0967a-280e-46f0-a6e8-934dbcc0e93f.png)**图12:** 运行前两个测试套件作为ScalaTest-File时的测试结果'
- en: Well done! We have learned how to perform the unit testing using Scala's FunSuite.
    However, if you evaluate the preceding method carefully, you should agree that
    there are several disadvantages. For example, you need to ensure an explicit management
    of `SparkContext` creation and destruction. As a developer or programmer, you
    have to write more lines of code for testing a sample method. Sometimes, code
    duplication occurs as the *Before* and the *After* step has to be repeated in
    all test suites. However, this is debatable since the common code could be put
    in a common trait.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！我们已经学会了如何使用Scala的FunSuite进行单元测试。然而，如果您仔细评估前面的方法，您应该同意存在一些缺点。例如，您需要确保`SparkContext`的创建和销毁有明确的管理。作为开发者或程序员，您需要为测试一个示例方法编写更多行代码。有时，代码重复发生，因为*Before*和*After*步骤必须在所有测试套件中重复。然而，这一点有争议，因为公共代码可以放在一个公共特质中。
- en: Now the question is how could we improve our experience? My recommendation is
    using the Spark testing base to make life easier and more straightforward. We
    will discuss how we could perform the unit testing the Spark testing base.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是我们如何能改善我们的体验？我的建议是使用Spark测试基底来使生活更轻松、更直接。我们将讨论如何使用Spark测试基底进行单元测试。
- en: 'Method 3: Making life easier with Spark testing base'
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法3：利用Spark测试基底简化生活
- en: Spark testing base helps you to test your most of the Spark codes with ease.
    So, what are the pros of this method then? There are many in fact. For example,
    using this the code is not verbose but we can get a very succinct code. The API
    is itself richer than that of ScalaTest or JUnit. Multiple languages support,
    for example, Scala, Java, and Python. It has the support of built-in RDD comparators.
    You can also use it for testing streaming applications. And finally and most importantly,
    it supports both local and cluster mode testings. This is most important for testing
    in a distributed environment.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Spark测试基底助您轻松测试大部分Spark代码。那么，这种方法的优势何在？实际上，优势颇多。例如，使用此方法，代码不会冗长，却能得到非常简洁的代码。其API本身比ScalaTest或JUnit更为丰富。支持多种语言，如Scala、Java和Python。内置RDD比较器。还可用于测试流应用程序。最后且最重要的是，它支持本地和集群模式测试。这对于分布式环境中的测试至关重要。
- en: The GitHub repo is located at [https://github.com/holdenk/spark-testing-base](https://github.com/holdenk/spark-testing-base).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub仓库位于[https://github.com/holdenk/spark-testing-base](https://github.com/holdenk/spark-testing-base)。
- en: Before starting ...
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 开始之前...
- en: Configuring Hadoop runtime on Windows
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Windows上配置Hadoop运行时
- en: We have already seen how to test your Spark applications written in Scala on
    Eclipse or IntelliJ, but there is another potential issue that should not be overlooked.
    Although Spark works on Windows, Spark is designed to be run on the UNIX-like
    operating system. Therefore, if you are working in a Windows environment, then
    extra care needs to be taken.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何在Eclipse或IntelliJ上测试用Scala编写的Spark应用程序，但还有一个潜在问题不容忽视。虽然Spark可以在Windows上运行，但Spark设计为在类UNIX操作系统上运行。因此，如果您在Windows环境中工作，则需要格外小心。
- en: 'While using Eclipse or IntelliJ to develop your Spark applications for solving
    data analytics, machine learning, data science, or deep learning applications
    on Windows, you might face an I/O exception error and your application might not
    compile successfully or may be interrupted. Actually, the thing is that Spark
    expects that there is a runtime environment for Hadoop on Windows too. For example,
    if you run a Spark application, say `KMeansDemo.scala`, on Eclipse for the first
    time, you will experience an I/O exception saying the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Eclipse或IntelliJ为Windows上的数据分析、机器学习、数据科学或深度学习应用程序开发Spark应用程序时，您可能会遇到I/O异常错误，您的应用程序可能无法成功编译或可能被中断。实际上，Spark期望Windows上也有Hadoop的运行时环境。例如，如果您第一次在Eclipse上运行Spark应用程序，比如`KMeansDemo.scala`，您将遇到一个I/O异常，如下所示：
- en: '[PRE13]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The reason is that by default, Hadoop is developed for the Linux environment,
    and if you are developing your Spark applications on Windows platform, a bridge
    is required that will provide an environment for the Hadoop runtime for Spark
    to be properly executed. The details of the I/O exception can be seen in the following
    figure:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是默认情况下，Hadoop是为Linux环境开发的，如果您在Windows平台上开发Spark应用程序，则需要一个桥梁，为Spark提供一个Hadoop运行时环境，以便正确执行。I/O异常的详细信息可以在下图看到：
- en: '![](img/32afa337-6d87-4b24-859d-c30a53ab627f.png)**Figure 14:** I/O exception
    occurred due to the failure of not to locate the winutils binary in the Hadoop
    binary path'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/32afa337-6d87-4b24-859d-c30a53ab627f.png)**图14：**由于未能在Hadoop二进制路径中定位winutils二进制文件，导致发生了I/O异常'
- en: Now, how to get rid of this problem then? The solution is straightforward. As
    the error message says, we need to have an executable, namely `winutils.exe`.
    Now download the `winutils.exe` file from [https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin),
    paste it in the Spark distribution directory, and configure Eclipse. More specifically,
    suppose your Spark distribution containing Hadoop is located at `C:/Users/spark-2.1.0-bin-hadoop2.7`.
    Inside the Spark distribution, there is a directory named bin. Now, paste the
    executable there (that is, `path = C:/Users/spark-2.1.0-binhadoop2.7/bin/`).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何解决这个问题呢？解决方案很简单。正如错误信息所说，我们需要一个可执行文件，即`winutils.exe`。现在从[https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin)下载`winutils.exe`文件，将其粘贴到Spark分发目录中，并配置Eclipse。更具体地说，假设包含Hadoop的Spark分发位于`C:/Users/spark-2.1.0-bin-hadoop2.7`。在Spark分发中，有一个名为bin的目录。现在，将可执行文件粘贴到那里（即`path
    = C:/Users/spark-2.1.0-binhadoop2.7/bin/`）。
- en: 'The second phase of the solution is going to Eclipse and then selecting the
    main class (that is, `KMeansDemo.scala` in this case), and then going to the Run
    menu. From the Run menu, go to the Run Configurations option and from there select
    the Environment tab, as shown in the following figure:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案的第二阶段是前往Eclipse，然后选择主类（即本例中的`KMeansDemo.scala`），接着进入运行菜单。从运行菜单中，选择运行配置选项，并从那里选择环境标签，如图所示：
- en: '![](img/b5e41aab-8cce-4deb-8f05-446767d78561.png)**Figure 15:** Solving the
    I/O exception occurred due to the absence of winutils binary in the Hadoop binary
    path'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/b5e41aab-8cce-4deb-8f05-446767d78561.png)**图15：**解决因Hadoop二进制路径中缺少winutils二进制文件而发生的I/O异常'
- en: If you select the tab, you a will have the option to create a new environmental
    variable for Eclipse using the JVM. Now create a new environmental variable named
    `HADOOP_HOME` and put the value as `C:/Users/spark-2.1.0-bin-hadoop2.7/`. Now
    press on Apply button and rerun your application, and your problem should be resolved.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择了该标签，您将有机会使用JVM为Eclipse创建一个新的环境变量。现在创建一个名为`HADOOP_HOME`的新环境变量，并将其值设置为`C:/Users/spark-2.1.0-bin-hadoop2.7/`。现在点击应用按钮并重新运行您的应用程序，您的问题应该得到解决。
- en: It is to be noted that while working with Spark on Windows in a PySpark, the
    `winutils.exe` file is required too.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在使用PySpark在Windows上运行Spark时，也需要`winutils.exe`文件。
- en: Please make a note that the preceding solution is also applicable in debugging
    your applications. Sometimes, even if the preceding error occurs, your Spark application
    will run properly. However, if the size of the dataset is large, it is most likely
    that the preceding error will occur.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述解决方案也适用于调试您的应用程序。有时，即使出现上述错误，您的Spark应用程序仍能正常运行。然而，如果数据集规模较大，很可能会出现上述错误。
- en: Debugging Spark applications
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试Spark应用程序
- en: In this section, we will see how to debug Spark applications that are running
    locally (on Eclipse or IntelliJ), standalone or cluster mode in YARN or Mesos.
    However, before diving deeper, it is necessary to know about logging in the Spark
    application.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解如何调试在本地（在Eclipse或IntelliJ上）、独立模式或YARN或Mesos集群模式下运行的Spark应用程序。然而，在深入之前，有必要了解Spark应用程序中的日志记录。
- en: Logging with log4j with Spark recap
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark使用log4j进行日志记录的回顾
- en: 'As stated earlier, Spark uses log4j for its own logging. If you configured
    Spark properly, Spark gets logged all the operation to the shell console. A sample
    snapshot of the file can be seen from the following figure:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Spark使用log4j进行自己的日志记录。如果正确配置了Spark，所有操作都会记录到shell控制台。可以从以下图表中看到文件的示例快照：
- en: '![](img/e5aa3075-1e4f-4fd6-8594-01e12076c1ce.png)**Figure 16:** A snap of the
    log4j.properties file'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/e5aa3075-1e4f-4fd6-8594-01e12076c1ce.png)**图16：**log4j.properties文件的快照'
- en: 'Set the default spark-shell log level to WARN. When running the spark-shell,
    the log level for this class is used to overwrite the root logger''s log level
    so that the user can have different defaults for the shell and regular Spark apps.
    We also need to append JVM arguments when launching a job executed by an executor
    and managed by the driver. For this, you should edit the `conf/spark-defaults.conf`.
    In short, the following options can be added:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 将默认的spark-shell日志级别设置为WARN。运行spark-shell时，此类的日志级别用于覆盖根日志记录器的日志级别，以便用户可以为shell和常规Spark应用设置不同的默认值。我们还需要在启动由执行器执行并由驱动程序管理的作业时附加JVM参数。为此，您应该编辑`conf/spark-defaults.conf`。简而言之，可以添加以下选项：
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To make the discussion clearer, we need to hide all the logs generated by Spark.
    We then can redirect them to be logged in the file system. On the other hand,
    we want our own logs to be logged in the shell and a separate file so that they
    don''t get mixed up with the ones from Spark. From here, we will point Spark to
    the files where our own logs are, which in this particular case is `/var/log/sparkU.log`.
    This `log4j.properties` file is then picked up by Spark when the application starts,
    so we don''t have to do anything aside of placing it in the mentioned location:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使讨论更清晰，我们需要隐藏所有由Spark生成的日志。然后我们可以将它们重定向到文件系统中进行记录。另一方面，我们希望自己的日志记录在shell和单独的文件中，以免与Spark的日志混淆。从这里开始，我们将指示Spark指向存放我们自己日志的文件，在本例中为`/var/log/sparkU.log`。当应用程序启动时，Spark会拾取这个`log4j.properties`文件，因此我们除了将其放置在提及的位置外，无需做其他事情：
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code, everything is printed as INFO once the log level is
    set to `INFO` until you set the level to a new level for example `WARN`. However,
    after that no info or trace and so on, that will not be printed. In addition to
    that, there are several valid logging levels supported by log4j with Spark. The
    successful execution of the preceding code should generate the following output:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，一旦将日志级别设置为`INFO`，所有内容都会作为INFO打印，直到您将级别设置为新的级别，例如`WARN`。然而，在那之后，不会有任何信息、跟踪等被打印出来。此外，log4j支持Spark的几个有效日志级别。成功执行前面的代码应该会产生以下输出：
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can also set up the default logging for Spark shell in `conf/log4j.properties`.
    Spark provides a template of the log4j as a property file, and we can extend and
    modify that file for logging in Spark. Move to the `SPARK_HOME/conf` directory
    and you should see the `log4j.properties.template` file. You should use the following
    `conf/log4j.properties.template` after renaming it to `log4j.properties`. While
    developing your Spark application, you can put the `log4j.properties` file under
    your project directory while working on an IDE-based environment such as Eclipse.
    However, to disable logging completely, just set the `log4j.logger.org` flags
    as `OFF` as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在`conf/log4j.properties`中设置Spark shell的默认日志记录。Spark提供了一个log4j的属性文件模板，我们可以扩展和修改该文件以在Spark中进行日志记录。转到`SPARK_HOME/conf`目录，您应该会看到`log4j.properties.template`文件。在重命名后，您应该使用以下`conf/log4j.properties.template`作为`log4j.properties`。在基于IDE的环境（如Eclipse）中开发Spark应用程序时，您可以将`log4j.properties`文件放在项目目录下。但是，要完全禁用日志记录，只需将`log4j.logger.org`标志设置为`OFF`，如下所示：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'So far, everything is very easy. However, there is a problem we haven''t noticed
    yet in the preceding code segment. One drawback of the `org.apache.log4j.Logger`
    class is that it is not serializable, which implies that we cannot use it inside
    a closure while doing operations on some parts of the Spark API. For example,
    suppose we do the following in our Spark code:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切都很容易。然而，我们还没有注意到前述代码段中的一个问题。`org.apache.log4j.Logger`类的一个缺点是它不是可序列化的，这意味着我们在使用Spark
    API的某些部分进行操作时，不能在闭包内部使用它。例如，假设我们在Spark代码中执行以下操作：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You should experience an exception that says `Task` not serializable as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会遇到一个异常，它会说`Task`不可序列化，如下所示：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'At first, we can try to solve this problem in a naive way. What you can do
    is just make the Scala class (that does the actual operation) `Serializable` using
    `extends Serializable` . For example, the code looks as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以尝试用一种简单的方法来解决这个问题。你可以做的就是让执行实际操作的Scala类`Serializable`，使用`extends Serializable`。例如，代码如下所示：
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This section is intended for carrying out a discussion on logging. However,
    we take the opportunity to make it more versatile for general purpose Spark programming
    and issues. In order to overcome the `task not serializable` error in a more efficient
    way, the compiler will try to send the whole object (not only the lambda) by making
    it serializable and forces SPark to accept that. However, it increases shuffling
    significantly, especially for big objects! The other ways are making the whole
    class `Serializable` or by declaring the instance only within the lambda function
    passed in the map operation. Sometimes, keeping the not `Serializable` objects
    across the nodes can work. Lastly, use the `forEachPartition()` or `mapPartitions()`
    instead of just `map()` and create the not `Serializable` objects. In summary,
    these are the ways to solve the problem around:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在进行关于日志记录的讨论。然而，我们借此机会使其更适用于通用Spark编程和问题。为了更有效地克服`task not serializable`错误，编译器将尝试发送整个对象（不仅仅是lambda），使其可序列化，并强制Spark接受它。然而，这会显著增加数据混洗，尤其是对于大型对象！其他方法是将整个类设为`Serializable`，或者仅在传递给map操作的lambda函数中声明实例。有时，在节点之间保留不可`Serializable`的对象可能有效。最后，使用`forEachPartition()`或`mapPartitions()`而不是仅使用`map()`，并创建不可`Serializable`的对象。总之，这些是解决问题的方法：
- en: Serializable the class
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列化该类
- en: Declare the instance only within the lambda function passed in the map
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅在传递给map的lambda函数中声明实例
- en: Make the NotSerializable object as a static and create it once per machine
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将不可序列化对象设为静态，并在每台机器上创建一次
- en: Call the `forEachPartition ()` or `mapPartitions()` instead of `map()` and create
    the NotSerializable object
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用`forEachPartition ()`或`mapPartitions()`而不是`map()`，并创建不可序列化对象
- en: 'In the preceding code, we have used the annotation `@transient lazy`, which
    marks the `Logger` class to be nonpersistent. On the other hand, an object containing
    the method apply (i.e. `MyMapperObject`) that instantiate the object of the `MyMapper`
    class is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们使用了注解`@transient lazy`，它标记`Logger`类为非持久性的。另一方面，包含应用方法（即`MyMapperObject`）的对象，用于实例化`MyMapper`类的对象，如下所示：
- en: '[PRE21]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, the object containing the `main()` method is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，包含`main()`方法的对象如下：
- en: '[PRE22]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, let''s see another example that provides better insight to keep fighting
    the issue we are talking about. Suppose we have the following class that computes
    the multiplication of two integers:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看另一个例子，它提供了更好的洞察力，以继续解决我们正在讨论的问题。假设我们有以下类，用于计算两个整数的乘法：
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, essentially, if you try to use this class for computing the multiplication
    in the lambda closure using `map()`, you will get the `Task Not Serializable`
    error that we described earlier. Now we simply can use `foreachPartition()` and
    the lambda inside as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，本质上，如果你尝试使用这个类来计算lambda闭包中的乘法，使用`map()`，你将会遇到我们之前描述的`Task Not Serializable`错误。现在我们只需简单地使用`foreachPartition()`和内部的lambda，如下所示：
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, if you compile it, it should return the desired result. For your ease,
    the complete code with the `main()` method is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你编译它，它应该返回期望的结果。为了方便，包含`main()`方法的完整代码如下：
- en: '[PRE25]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Debugging the Spark application
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试Spark应用程序
- en: In this section, we will discuss how to debug Spark applications running on
    locally on Eclipse or IntelliJ, as standalone or cluster mode in YARN or Mesos.
    Before getting started, you can also read the debugging documentation at [https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/](https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何在本地Eclipse或IntelliJ上调试运行在独立模式或集群模式（在YARN或Mesos上）的Spark应用程序。在开始之前，您还可以阅读调试文档：[https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/](https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/)。
- en: Debugging Spark application on Eclipse as Scala debug
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Eclipse上以Scala调试方式调试Spark应用程序
- en: 'To make this happen, just configure your Eclipse to debug your Spark applications
    as a regular Scala code debug. To configure select Run | Debug Configuration |
    Scala Application as shown in the following figure:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这一目标，只需将您的Eclipse配置为将Spark应用程序作为常规Scala代码进行调试。配置方法为选择运行 | 调试配置 | Scala应用程序，如图所示：
- en: '![](img/eb20cffd-d0b0-4286-96bb-2ebb36b82048.png)**Figure 17:** Configuring
    Eclipse to debug Spark applications as a regular Scala code debug'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/eb20cffd-d0b0-4286-96bb-2ebb36b82048.png)**图17：**配置Eclipse以将Spark应用程序作为常规Scala代码进行调试'
- en: 'Suppose we want to debug our `KMeansDemo.scala` and ask Eclipse (you can have
    similar options on InteliJ IDE) to start the execution at line 56 and set the
    breakpoint in line 95\. To do so, run your Scala code as debugging and you should
    observe the following scenario on Eclipse:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要调试我们的`KMeansDemo.scala`，并要求Eclipse（您可以在InteliJ IDE中拥有类似选项）从第56行开始执行并在第95行设置断点。为此，请以调试模式运行您的Scala代码，您应该在Eclipse上观察到以下场景：
- en: '![](img/bb345bf6-ae2b-4a92-9811-37038227fc01.png)**Figure 18:** Debugging Spark
    applications on Eclipse'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/bb345bf6-ae2b-4a92-9811-37038227fc01.png)**图18：**在Eclipse上调试Spark应用程序'
- en: 'Then, Eclipse will pause on the line you ask it to stop the execution in line
    95, as shown in the following screenshot:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Eclipse将在您要求它停止执行的第95行暂停，如下面的截图所示：
- en: '![](img/1d4e11ff-3edf-456e-be9f-fa62cacac199.png)**Figure 19:** Debugging Spark
    applications on Eclipse (breakpoint)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d4e11ff-3edf-456e-be9f-fa62cacac199.png)**图19：**在Eclipse上调试Spark应用程序（断点）'
- en: In summary, to simplify the preceding example, if there is any error between
    line 56 and line 95, Eclipse will show where the error actually occurs. Otherwise,
    it will follow the normal workflow if not interrupted.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，为了简化上述示例，如果在第56行和第95行之间出现任何错误，Eclipse将显示错误实际发生的位置。否则，如果没有中断，它将遵循正常的工作流程。
- en: Debugging Spark jobs running as local and standalone mode
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试作为本地和独立模式运行的Spark作业
- en: 'While debugging your Spark application locally or as standalone mode, you should
    know that debugging the driver program and debugging one of the executors is different
    since using these two types of nodes requires different submission parameters
    passed to `spark-submit`. Throughout this section, I''ll use port 4000 as the
    address. For example, if you want to debug the driver program, you can add the
    following to your `spark-submit` command:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地或独立模式下调试您的Spark应用程序时，您应该知道调试驱动程序程序和调试其中一个执行程序是不同的，因为使用这两种节点需要向`spark-submit`传递不同的提交参数。在本节中，我将使用端口4000作为地址。例如，如果您想调试驱动程序程序，您可以在您的`spark-submit`命令中添加以下内容：
- en: '[PRE28]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: After that, you should set your remote debugger to connect to the node where
    you have submitted the driver program. For the preceding case, port number 4000
    was ...
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，您应将远程调试器设置为连接到您提交驱动程序程序的节点。对于上述情况，端口号4000是...
- en: Debugging Spark applications on YARN or Mesos cluster
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在YARN或Mesos集群上调试Spark应用程序
- en: 'When you run a Spark application on YARN, there is an option that you can enable
    by modifying `yarn-env.sh`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在YARN上运行Spark应用程序时，有一个选项可以通过修改`yarn-env.sh`来启用：
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, the remote debugging will be available through port 4000 on your Eclipse
    or IntelliJ IDE. The second option is by setting the `SPARK_SUBMIT_OPTS`. You
    can use either Eclipse or IntelliJ to develop your Spark applications that can
    be submitted to be executed on remote multinode YARN clusters. What I do is that
    I create a Maven project on Eclipse or IntelliJ and package my Java or Scala application
    as a jar file and then submit it as a Spark job. However, in order to attach your
    IDE such as Eclipse or IntelliJ debugger to your Spark application, you can define
    all the submission parameters using the `SPARK_SUBMIT_OPTS` environment variable
    as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，远程调试将通过Eclipse或IntelliJ IDE上的4000端口可用。第二种方法是设置`SPARK_SUBMIT_OPTS`。您可以使用Eclipse或IntelliJ开发可以提交到远程多节点YARN集群执行的Spark应用程序。我所做的是在Eclipse或IntelliJ上创建一个Maven项目，将我的Java或Scala应用程序打包成jar文件，然后作为Spark作业提交。然而，为了将IDE（如Eclipse或IntelliJ）调试器附加到您的Spark应用程序，您可以使用`SPARK_SUBMIT_OPTS`环境变量定义所有提交参数，如下所示：
- en: '[PRE30]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then submit your Spark job as follows (please change the values accordingly
    based on your requirements and setup):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后如下提交您的Spark作业（请根据您的需求和设置相应地更改值）：
- en: '[PRE31]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'After running the preceding command, it will wait until you connect your debugger,
    as shown in the following: `Listening for transport dt_socket at address: 4000`.
    Now you can configure your Java remote application (Scala application will work
    too) on the IntelliJ debugger, as shown in the following screenshot:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '执行上述命令后，它将等待您连接调试器，如下所示：`Listening for transport dt_socket at address: 4000`。现在，您可以在IntelliJ调试器中配置Java远程应用程序（Scala应用程序也可以），如下面的截图所示：'
- en: '![](img/15c30522-5742-4de4-885f-fc568d44748e.png)**Figure 20:** Configuring
    remote debugger on IntelliJ'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/15c30522-5742-4de4-885f-fc568d44748e.png)**图20：**在IntelliJ上配置远程调试器'
- en: For the preceding case, 10.200.1.101 is the IP address of the remote computing
    node where your Spark job is basically running. Finally, you will have to start
    the debugger by clicking on Debug under IntelliJ's Run menu. Then, if the debugger
    connects to your remote Spark app, you will see the logging info in the application
    console on IntelliJ. Now if you can set the breakpoints and the rests of them
    are normal debugging.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述情况，10.200.1.101是远程计算节点上运行Spark作业的基本IP地址。最后，您需要通过点击IntelliJ的运行菜单下的调试来启动调试器。然后，如果调试器连接到您的远程Spark应用程序，您将在IntelliJ的应用程序控制台中看到日志信息。现在，如果您可以设置断点，其余的调试就是正常的了。
- en: 'The following figure shows an example how will you see on the IntelliJ when
    pausing a Spark job with a breakpoint:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了在IntelliJ中暂停带有断点的Spark作业时的示例视图：
- en: '![](img/2d8b430a-9583-4009-b8ba-bebcc6dfbd13.png)**Figure 21:** An example
    how will you see on the IntelliJ when pausing a Spark job with a breakpoint'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/2d8b430a-9583-4009-b8ba-bebcc6dfbd13.png)**图21：**在IntelliJ中暂停带有断点的Spark作业时的示例视图'
- en: 'Although it works well, sometimes I experienced that using `SPARK_JAVA_OPTS`
    won''t help you much in the debug process on Eclipse or even IntelliJ. Instead,
    use and export `SPARK_WORKER_OPTS` and `SPARK_MASTER_OPTS` while running your
    Spark jobs on a real cluster (YARN, Mesos, or AWS) as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管效果良好，但有时我发现使用`SPARK_JAVA_OPTS`在Eclipse甚至IntelliJ的调试过程中帮助不大。相反，在运行Spark作业的真实集群（YARN、Mesos或AWS）上，使用并导出`SPARK_WORKER_OPTS`和`SPARK_MASTER_OPTS`，如下所示：
- en: '[PRE32]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then start your Master node as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然后如下启动Master节点：
- en: '[PRE33]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now open an SSH connection to your remote machine where the Spark job is actually
    running and map your localhost at 4000 (aka `localhost:4000`) to `host_name_to_your_computer.org:5000`,
    assuming the cluster is at `host_name_to_your_computer.org:5000` and listening
    on port 5000\. Now that your Eclipse will consider that you''re just debugging
    your Spark application as a local Spark application or process. However, to make
    this happen, you will have to configure the remote debugger on Eclipse, as shown
    in the following figure:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打开一个SSH连接到运行Spark作业的远程机器，并将本地主机映射到4000（即`localhost:4000`）到`host_name_to_your_computer.org:5000`，假设集群位于`host_name_to_your_computer.org:5000`并监听端口5000。现在，您的Eclipse将认为您只是在调试本地Spark应用程序或进程。然而，要实现这一点，您需要在Eclipse上配置远程调试器，如下所示：
- en: '![](img/591d8d5c-0a0a-4e5d-9fd0-d38bf0b71f89.png)**Figure 22:** Connecting
    remote host on Eclipse for debugging Spark application'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/591d8d5c-0a0a-4e5d-9fd0-d38bf0b71f89.png)**图22：**在Eclipse上连接远程主机以调试Spark应用程序'
- en: That's it! Now you can debug on your live cluster as if it were your desktop.
    The preceding examples are for running with the Spark Master set as YARN-client.
    However, it should also work when running on a Mesos cluster. If you're running
    using YARN-cluster mode, you may have to set the driver to attach to your debugger
    rather than attaching your debugger to the driver since you won't necessarily
    know in advance what mode the driver will be executing on.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在你可以在实时集群上调试，就像在桌面一样。前面的示例是在Spark Master设置为YARN-client的情况下运行的。然而，在Mesos集群上运行时也应该有效。如果你使用的是YARN-cluster模式，你可能需要将驱动程序设置为附加到调试器，而不是将调试器附加到驱动程序，因为你事先不一定知道驱动程序将执行的模式。
- en: Debugging Spark application using SBT
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SBT调试Spark应用程序
- en: 'The preceding setting works mostly on Eclipse or IntelliJ using the Maven project.
    Suppose that you already have your application done and are working on your preferred
    IDEs such as IntelliJ or Eclipse as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 上述设置主要适用于使用Maven项目的Eclipse或IntelliJ。假设你已经完成了应用程序，并正在你喜欢的IDE（如IntelliJ或Eclipse）中工作，如下所示：
- en: '[PRE34]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now, if you want to get this job to the local cluster (standalone), the very
    first step is packaging ...
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你想将这项工作部署到本地集群（独立模式），第一步是打包...
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you saw how difficult the testing and debugging your Spark
    applications are. These can even be more critical in a distributed environment.
    We also discussed some advanced ways to tackle them altogether. In summary, you
    learned the way of testing in a distributed environment. Then you learned a better
    way of testing your Spark application. Finally, we discussed some advanced ways
    of debugging Spark applications.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你看到了测试和调试Spark应用程序的难度。在分布式环境中，这些甚至可能更为关键。我们还讨论了一些高级方法来全面应对这些问题。总之，你学习了在分布式环境中的测试方法。然后你学习了测试Spark应用程序的更好方法。最后，我们讨论了一些调试Spark应用程序的高级方法。
- en: This is more or less the end of our little journey with advanced topics on Spark.
    Now, a general suggestion from our side to you as readers or if you are relatively
    newer to the data science, data analytics, machine learning, Scala, or Spark is
    that you should at first try to understand what types of analytics you want to
    perform. To be more specific, for example, if your problem is a machine learning
    problem, try to guess what type of learning algorithms should be the best fit,
    that is, classification, clustering, regression, recommendation, or frequent pattern
    mining. Then define and formulate the problem, and after that, you should generate
    or download the appropriate data based on the feature engineering concept of Spark
    that we have discussed earlier. On the other hand, if you think that you can solve
    your problem by using deep learning algorithms or APIs, you should use other third-party
    algorithms and integrate with Spark and work straight away.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是我们关于Spark高级主题的小旅程的结束。现在，我们给读者的一般建议是，如果你是数据科学、数据分析、机器学习、Scala或Spark的相对新手，你应该首先尝试了解你想执行哪种类型的分析。更具体地说，例如，如果你的问题是机器学习问题，尝试猜测哪种学习算法最适合，即分类、聚类、回归、推荐或频繁模式挖掘。然后定义和制定问题，之后你应该根据我们之前讨论的Spark特征工程概念生成或下载适当的数据。另一方面，如果你认为你可以使用深度学习算法或API解决问题，你应该使用其他第三方算法并与Spark集成，直接工作。
- en: Our final recommendation to the readers is to browse the Spark website (at [http://spark.apache.org/](http://spark.apache.org/))
    regularly to get the updates and also try to incorporate the regular Spark-provided
    APIs with other third-party applications or tools to get the best result of the
    collaboration.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给读者的最终建议是定期浏览Spark官网（位于[http://spark.apache.org/](http://spark.apache.org/)）以获取更新，并尝试将常规的Spark提供的API与其他第三方应用程序或工具结合使用，以实现最佳的协同效果。
