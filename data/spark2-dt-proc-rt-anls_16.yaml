- en: Spark Streaming and Machine Learning Library
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark流处理与机器学习库
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下内容：
- en: Structured streaming for near real-time machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流处理，用于近实时机器学习
- en: Streaming DataFrames for real-time machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流式DataFrames进行实时机器学习
- en: Streaming Datasets for real-time machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流式数据集进行实时机器学习
- en: Streaming data and debugging with queueStream
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用queueStream进行流数据和调试
- en: Downloading and understanding the famous Iris data for unsupervised classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载并理解著名的鸢尾花数据集，用于无监督分类
- en: Streaming KMeans for a real-time online classifier
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为实时在线分类器实现流式KMeans
- en: Downloading wine quality data for streaming regression
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载葡萄酒质量数据，用于流式回归
- en: Streaming linear regression for a real-time regression
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为实时回归实现流式线性回归
- en: Downloading Pima Diabetes data for supervised classification
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载Pima糖尿病数据集，用于监督分类
- en: Streaming logistic regression for an on-line classifier
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为在线分类器实现流式逻辑回归
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Spark streaming is an evolving journey toward unification and structuring of
    the APIs in order to address the concerns of batch versus stream. Spark streaming
    has been available since Spark 1.3 with **Discretized Stream** (**DStream**).
    The new direction is to abstract the underlying framework using an unbounded table
    model in which the users can query the table using SQL or functional programming
    and write the output to another output table in multiple modes (complete, delta,
    and append output). The Spark SQL Catalyst optimizer and Tungsten (off-heap memory
    manager) are now an intrinsic part of the Spark streaming, which leads to a much
    efficient execution.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Spark流处理是一个不断发展的过程，旨在统一和结构化API，以解决批处理与流处理的关切。自Spark 1.3起，Spark流处理就已提供**离散流**（**DStream**）。新的方向是使用无界表模型抽象底层框架，用户可以使用SQL或函数式编程查询表，并将输出写入到另一个输出表中，支持多种模式（完整、增量和追加输出）。Spark
    SQL Catalyst优化器和Tungsten（堆外内存管理器）现已成为Spark流处理的核心部分，从而实现更高效的执行。
- en: In this chapter, we not only cover the streaming facilities available in Spark's
    ...
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不仅涵盖了Spark中可用的流处理功能...
- en: Structured streaming for near real-time machine learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流处理，用于近实时机器学习
- en: In this recipe, we explore the new structured streaming paradigm introduced
    in Spark 2.0\. We explore real-time streaming using sockets and structured streaming
    API to vote and tabulate the votes accordingly.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨Spark 2.0引入的新结构化流处理范式。我们通过使用套接字和结构化流API进行实时流处理，并相应地投票和统计票数。
- en: We also explore the newly introduced subsystem by simulating a stream of randomly
    generated votes to pick the most unpopular comic book villain.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过模拟一系列随机生成的投票来探索新引入的子系统，以选出最不受欢迎的漫画书反派角色。
- en: There are two distinct programs (`VoteCountStream.scala` and `CountStreamproducer.scala`)
    that make up this recipe.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含两个不同的程序（`VoteCountStream.scala`和`CountStreamproducer.scala`）。
- en: How to do it...
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages for the Spark context to get access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包，以便Spark上下文可以访问集群，并使用`log4j.Logger`减少Spark产生的输出量：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define a Scala class to generate voting data onto a client socket:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala类，用于将投票数据生成到客户端套接字：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How it works...
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'In this recipe, we created a simple data generation server to simulate a stream
    of voting data and then counted the vote. The following figure provides a high-level
    depiction of this concept:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们创建了一个简单的数据生成服务器，用于模拟投票数据流，并对投票进行计数。下图提供了这一概念的高层次描述：
- en: '![](img/cf9e0f2a-e5e9-4b19-af52-bd11232731a3.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf9e0f2a-e5e9-4b19-af52-bd11232731a3.png)'
- en: First, we began by executing the data generation server. Second, we defined
    a socket data source, which allows us to connect to the data generation server.
    Third, we constructed a simple Spark expression to group by villain (that is,
    bad superheroes) and count all currently received votes. Finally, we configured
    a threshold trigger of 10 seconds to execute our streaming query, which dumps
    the accumulated results onto the console.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们启动了数据生成服务器。其次，我们定义了一个套接字数据源，以便连接到数据生成服务器。接着，我们构建了一个简单的Spark表达式，按反派（即坏超级英雄）分组并统计当前收到的所有投票。最后，我们设置了一个10秒的阈值触发器来执行我们的流查询，该查询将累积结果输出到控制台。
- en: 'There are two short programs involved in this recipe:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本方案涉及两个简短的程序：
- en: '`CountStreamproducer.scala`:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CountStreamproducer.scala`：'
- en: The producer - data generation server
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产者 - 数据生成服务器
- en: Simulates the voting for itself and broadcasts it
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟投票过程并进行广播
- en: '`VoteCountStream.scala`:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VoteCountStream.scala`：'
- en: The consumer - consumes and aggregates/tabulates the data
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者 - 消费并聚合/制表数据
- en: Receives and count votes for our villain superhero
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接收并统计我们反派超级英雄的投票
- en: There's more...
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The topic of how to program using Spark streaming and structured streaming in
    Spark is out of scope for this book, but we felt it is necessary to share some
    programs to introduce the concepts before drilling down into ML streaming offering
    for Spark.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不涉及使用Spark流处理和结构化流处理的编程主题，但我们认为有必要分享一些程序以引入概念，然后再深入探讨Spark的ML流处理功能。
- en: 'For a solid introduction to streaming, please consult the following documentation
    on Spark:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如需对流处理进行全面介绍，请参阅Spark相关文档：
- en: Information of Spark 2.0+ structured streaming is available at [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关Spark 2.0+结构化流的信息，请访问[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes)
- en: Information of Spark 1.6 streaming is available at [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 1.6流处理信息可在[https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)获取
- en: See also
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for structured streaming is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.package](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.package)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.package](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.package)获取
- en: Documentation for DStream (pre-Spark 2.0) is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DStream（Spark 2.0之前）文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream)获取
- en: Documentation for `DataStreamReader` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataStreamReader`文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)获取'
- en: Documentation for `DataStreamWriter` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataStreamWriter`文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)获取'
- en: Documentation for `StreamingQuery` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamingQuery`文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery)获取'
- en: Streaming DataFrames for real-time machine learning
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时机器学习的流数据帧
- en: In this recipe, we explore the concept of a streaming DataFrame. We create a
    DataFrame consisting of the name and age of individuals, which we will be streaming
    across a wire. A streaming DataFrame is a popular technique to use with Spark
    ML since we do not have a full integration between Spark structured ML at the
    time of writing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们探讨了流式DataFrame的概念。我们创建了一个包含个人姓名和年龄的DataFrame，该数据将通过网络流式传输。流式DataFrame是与Spark
    ML配合使用的一种流行技术，因为在撰写本文时，Spark结构化ML尚未完全集成。
- en: We limit this recipe to only the extent of demonstrating a streaming DataFrame
    and leave it up to the reader to adapt this to their own custom ML pipelines.
    While streaming DataFrame is not available out of the box in Spark 2.1.0, it will
    be a natural evolution to see it in later versions of Spark.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本例仅限于演示流式DataFrame，并留给读者将其适配到自己的自定义ML管道中。虽然流式DataFrame在Spark 2.1.0中并非开箱即用，但它将是Spark后续版本的自然演进。
- en: How to do it...
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Import the necessary packages:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create a `SparkSession` as an entry point to the Spark cluster:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`SparkSession`作为访问Spark集群的入口点：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    the logging level to warning:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志消息的交错导致输出难以阅读，因此将日志级别设置为警告：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, load the person data file to infer a data schema without hand coding
    the structure types:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，加载人员数据文件以推断数据模式，无需手动编码结构类型：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'From the console, you will see the following output:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台，您将看到以下输出：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now configure a streaming DataFrame for ingestion of the data:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在配置一个流式DataFrame以接收数据：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let us execute a simple data transform, by filtering on age greater than `60`:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们执行一个简单的数据转换，通过筛选年龄大于`60`的记录：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We now output the transformed streaming data to the console, which will trigger
    every second:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将转换后的流数据输出到控制台，该操作将每秒触发一次：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We start our defined streaming query and wait for data to appear in the stream:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们启动定义的流查询，并等待数据出现在流中：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, the result of our streaming query will appear in the console:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，我们的流查询结果将显示在控制台上：
- en: '![](img/d7e4aeb9-e4f9-4cbf-b311-136b78be704a.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7e4aeb9-e4f9-4cbf-b311-136b78be704a.png)'
- en: How it works...
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we first discover the underlying schema for a person object
    using a quick method (using a JSON object) as described in step 6\. The resulting
    DataFrame will know the schema that we subsequently impose on the streaming input
    (simulated via streaming a file) and treated as a streaming DataFrame as seen
    in step 7.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们首先使用快速方法（使用JSON对象）发现人员对象的底层模式，如步骤6所述。生成的DataFrame将了解我们随后对流输入（通过流式传输文件模拟）施加的模式，并作为流式DataFrame处理，如步骤7所示。
- en: The ability to treat the stream as a DataFrame and act on it using a functional
    or SQL paradigm is a powerful concept that can be seen in step 8\. We then proceed
    to output the result using `writestream()` with `append` mode and a 1-second batch
    interval trigger.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将流视为DataFrame并使用函数式或SQL范式对其进行操作的能力是一个强大的概念，如步骤8所示。然后，我们继续使用`writestream()`以`append`模式和1秒批处理间隔触发器输出结果。
- en: There's more...
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The combination of DataFrames and structured programming is a powerful concept
    that helps us to separate the data layer from the stream, which makes the programming
    significantly easier. One of the biggest drawbacks with DStream (pre-Spark 2.0)
    was its inability to isolate the user from details of the underlying details of
    stream/RDD implementation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames与结构化编程的结合是一个强大的概念，有助于我们将数据层与流分离，从而使编程变得更加简单。DStream（Spark 2.0之前）最大的缺点之一是其无法将用户与流/RDD实现的底层细节隔离。
- en: 'Documentation for DataFrames:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 关于DataFrames的文档：
- en: '`DataFrameReader`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataFrameReader`：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader)'
- en: '`DataFrameWriter`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DataFrameWriter: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter)'
- en: See also
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Documentation for Spark data stream reader and writer:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Spark数据流读写器文档：
- en: DataStreamReader: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DataStreamReader: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)'
- en: DataStreamWriter: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DataStreamWriter: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)'
- en: Streaming Datasets for real-time machine learning
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时机器学习的流式数据集
- en: In this recipe, we create a streaming Dataset to demonstrate the use of Datasets
    with a Spark 2.0 structured programming paradigm. We stream stock prices from
    a file using a Dataset and apply a filter to select the day's stock that closed
    above $100.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们创建了一个流式数据集，以展示在Spark 2.0结构化编程范式中使用数据集的方法。我们通过数据集从文件中流式传输股票价格，并应用过滤器选择当日收盘价高于$100的股票。
- en: The recipe demonstrates how streams can be used to filter and to act on the
    incoming data using a simple structured streaming programming model. While it
    is similar to a DataFrame, there are some differences in the syntax. The recipe
    is written in a generalized manner so the user can customize it for their own
    Spark ML programming projects.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例展示了如何使用简单的结构化流编程模型来过滤和处理传入数据。虽然它类似于DataFrame，但在语法上存在一些差异。本示例以通用方式编写，以便用户可以根据自己的Spark
    ML编程项目进行定制。
- en: How to do it...
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Import the necessary packages:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define a Scala `case class` to model streaming data:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala `case class`来模拟流数据：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create `SparkSession` to use as an entry point to the Spark cluster:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`SparkSession`作为访问Spark集群的入口点：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we will be utilizing the market data of closing prices for **General
    Electric** (**GE**) dating back to 1972\. To simplify the data, we have preprocessed
    for the purposes of this recipe. We use the same method from the previous recipe, *Streaming
    DataFrames for real-time machine learning*, by peeking into the JSON object to
    discover the schema (step 7), which we impose on the stream in step 8.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将利用**通用电气**（**GE**）自1972年以来的收盘价市场数据。为了简化数据，我们已为本次示例预处理了数据。我们采用了上一示例《实时机器学习的流式数据帧》中的方法，通过查看JSON对象来发现模式（步骤7），并在步骤8中将其应用于流。
- en: 'The following code shows how to use the schema to make the stream look like
    a simple table that you can read from on the fly. This is a powerful concept that
    makes stream programming accessible to more programmers. The `schema(s.schema)` and
    `as[StockPrice]`from the following code snippet are required to create the streaming
    Dataset, which has a schema associated with it:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用模式使流看起来像一个简单的表格，以便实时从中读取数据。这是一个强大的概念，使得流编程对更多程序员来说变得易于访问。以下代码片段中的`schema(s.schema)`和`as[StockPrice]`是创建具有关联模式的流式数据集所必需的：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There's more...
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Documentation for all the APIs available under Dataset at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
    website[.](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据集下可用的API文档位于：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
- en: See also
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'The following documentation is helpful while exploring the streaming Dataset
    concept:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文档在探索流式数据集概念时很有帮助：
- en: '`StreamReader`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamReader`：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)'
- en: '`StreamWriter`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamWriter`：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)'
- en: '`StreamQuery`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamQuery`：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery)'
- en: Streaming data and debugging with queueStream
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用queueStream进行流数据和调试
- en: In this recipe, we explore the concept of `queueStream()`*,* which is a valuable
    tool while trying to get a streaming program to work during the development cycle.
    We found the `queueStream()` API very useful and felt that other developers can
    benefit from a recipe that fully demonstrates its usage.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们探讨了`queueStream()`的概念，这是在开发周期中尝试使流式程序工作时的宝贵工具。我们发现`queueStream()` API非常有用，并认为其他开发人员可以从完全展示其用法的食谱中受益。
- en: 'We start by simulating a user browsing various URLs associated with different
    web pages using the program `ClickGenerator.scala` and then proceed to consume
    and tabulate the data (user behavior/visits) using the `ClickStream.scala` program:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用`ClickGenerator.scala`程序模拟用户浏览与不同网页关联的各种URL，然后使用`ClickStream.scala`程序消费和汇总数据（用户行为/访问）：
- en: '![](img/84401a18-69d7-490d-8946-511151c882b3.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84401a18-69d7-490d-8946-511151c882b3.png)'
- en: We use Spark's streaming API with `Dstream()`, which will require the use ...
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Spark的流式API，`Dstream()`，这将需要使用...
- en: How to do it...
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Import the necessary packages:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define a Scala `case class` to model click events by users that contain user
    identifier, IP address, time of the event, URL, and HTTP status code:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个Scala `case class`来模拟用户点击事件，包含用户标识符、IP地址、事件时间、URL和HTTP状态码：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define status codes for generation:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成的状态码：
- en: '[PRE21]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Define URLs for generation:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成的URL：
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define IP address range for generation:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成的IP地址范围：
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define timestamp range for generation:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成的时间戳范围：
- en: '[PRE24]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Define user identifier range for generation:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成的用户标识符范围：
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Define a function to generate one or more pseudo-random events:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来生成一个或多个伪随机事件：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define a function to parse a pseudo-random `ClickEvent` from a string:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，从字符串解析伪随机`ClickEvent`：
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Create Spark''s configuration and Spark streaming context with 1-second duration:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark配置和Spark流式上下文，持续时间为1秒：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志消息的交错导致输出难以阅读，因此将日志级别设置为警告：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create a mutable queue to append our generated data onto:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个可变队列，将我们生成的数据附加到其上：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Create a Spark queue stream from the streaming context passing in a reference
    of our data queue:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从流式上下文中创建一个Spark队列流，传递我们的数据队列的引用：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Process any data received by the queue stream and count the total number of
    each particular link users have clicked upon:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理队列流接收到的任何数据，并计算用户点击每个特定链接的总次数：
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Print out the `12` URLs and their totals:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出`12`个URL及其总数：
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Start our streaming context to receive micro-batches:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动我们的流式上下文以接收微批量：
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Loop 10 times generating 100 pseudo-random events on each iteration and append
    them our mutable queue so they materialize in the streaming queue abstraction:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环10次，每次迭代生成100个伪随机事件，并将它们附加到我们的可变队列，以便它们在流式队列抽象中实现：
- en: '[PRE35]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We close the program by stopping the Spark streaming context:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过停止Spark流式上下文来关闭程序：
- en: '[PRE36]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works...
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: With this recipe, we introduced Spark Streaming using a technique many overlook,
    which allows us to craft a streaming application utilizing Spark's `QueueInputDStream`
    class. The `QueueInputDStream` class is not only a beneficial tool for understanding
    Spark streaming, but also for debugging during the development cycle. In the beginning
    steps, we set up a few data structures, in order to generate pseudo-random `clickstream`
    event data for stream processing at a later stage.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本教程，我们介绍了使用许多人忽视的技术来引入Spark Streaming，即利用Spark的`QueueInputDStream`类构建流式应用程序。`QueueInputDStream`类不仅有助于理解Spark流处理，而且在开发周期中调试也非常有用。在初始步骤中，我们设置了一些数据结构，以便稍后生成用于流处理的伪随机`clickstream`事件数据。
- en: It should be noted that in step 12, we are creating a streaming context instead
    of a SparkContext. The streaming context is what we use for Spark streaming applications.
    Next, the creation of a queue and queue stream is done to receive streaming data.
    Now steps ...
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在第12步中，我们创建的是流式上下文而非SparkContext。流式上下文用于Spark流处理应用。接下来，创建队列和队列流以接收流数据。现在步骤...
- en: See also
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'At its core, `queueStream()` is just a queue of RDDs that we have after the
    Spark streaming (pre-2.0) turns into RDD:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，`queueStream()`只是一个RDD队列，在Spark流处理（2.0版本之前）转换为RDD后形成：
- en: Documentation for structured streaming (Spark 2.0+): [https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html](https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html)
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流处理文档（Spark 2.0+）：[https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html](https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html)
- en: Documentation for streaming (pre-Spark 2.0): [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流处理文档（Spark 2.0之前）：[https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
- en: Downloading and understanding the famous Iris data for unsupervised classification
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载并理解著名的鸢尾花数据，用于无监督分类
- en: In this recipe, we download and inspect the well-known Iris dataset in preparation
    for the upcoming streaming KMeans recipe, which lets you see classification/clustering
    in real-time.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们下载并检查了著名的鸢尾花数据集，为即将到来的流式KMeans教程做准备，该教程让您实时看到分类/聚类过程。
- en: The data is housed on the UCI machine learning repository, which is a great
    source of data to prototype algorithms on. You will notice that R bloggers tend
    to love this dataset.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储在UCI机器学习库中，这是一个用于算法原型设计的数据宝库。你会发现R语言博客作者们往往钟爱这个数据集。
- en: How to do it...
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'You can start by downloading the dataset using either two of the following
    commands:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过以下任一命令开始下载数据集：
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You can also use the following command:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用以下命令：
- en: '[PRE38]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can also use the following command:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用以下命令：
- en: '[PRE39]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now we begin our first step of data exploration by examining how the data in
    `iris.data` is formatted:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们通过检查`iris.data`中的数据格式开始数据探索的第一步：
- en: '[PRE40]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we take a look at the iris data to know how it is formatted:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来看看鸢尾花数据的格式：
- en: '[PRE41]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: How it works...
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The data is made of 150 observations. Each observation is made of four numerical
    features (measured in centimeters) and a label that signifies which class each
    Iris belongs to:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包含150个观测值。每个观测值由四个数值特征（以厘米为单位）和一个标签组成，该标签指示每朵鸢尾花所属的类别：
- en: '**Features/attributes**:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征/属性**：'
- en: Sepal length in cm
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片长度（厘米）
- en: Sepal width in cm
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片宽度（厘米）
- en: Petal length in cm
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣长度（厘米）
- en: Petal width in cm
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度（厘米）
- en: '**Label/class**:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签/类别**：'
- en: Iris Setosa
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 山鸢尾
- en: Iris Versicolour
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变色鸢尾
- en: Iris Virginic
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维吉尼亚鸢尾
- en: There's more...
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'The following image depicts an Iris flower with Petal and Sepal marked for
    clarity:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 下图清晰地标示了一朵鸢尾花的花瓣和萼片：
- en: '![](img/fec7666e-bdb6-48c5-9df0-da2283f1877b.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fec7666e-bdb6-48c5-9df0-da2283f1877b.png)'
- en: See also
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'The following link explores the Iris dataset in more detail:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接更详细地探讨了鸢尾花数据集：
- en: '[https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)'
- en: Streaming KMeans for a real-time on-line classifier
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时在线分类器的流式KMeans
- en: In this recipe, we explore the streaming version of KMeans in Spark used in
    unsupervised learning schemes. The purpose of streaming KMeans algorithm is to
    classify or group a set of data points into a number of clusters based on their
    similarity factor.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们探索了Spark中用于无监督学习方案的流式KMeans。流式KMeans算法的目的在于根据数据点的相似性因子将其分类或分组到多个簇中。
- en: There are two implementations of the KMeans classification method, one for static/offline
    data and another version for continuously arriving real-time updating data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans分类方法有两种实现，一种用于静态/离线数据，另一种版本用于持续到达的实时更新数据。
- en: We will be streaming iris dataset clustering as new data streams into our streaming
    context.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对鸢尾花数据集进行流式聚类，因为新数据流入了我们的流式上下文。
- en: How to do it...
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE42]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Import the necessary packages:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE43]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We begin by defining a function to load iris data into memory, filtering out
    blank lines, attaching an identifier to each element, and finally returning a
    tuple of type string and long:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义一个函数，将鸢尾花数据加载到内存中，过滤掉空白行，为每个元素附加一个标识符，最后返回一个字符串和长整型的元组：
- en: '[PRE44]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: How it works...
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we begin by loading the iris dataset and using the `zip()` API
    to pair data with a unique identifier to the data for generating *labeled points* data
    structure for use with the KMeans algorithm.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们首先加载鸢尾花数据集，并使用`zip()` API将数据与唯一标识符配对，以生成用于KMeans算法的*标记点*数据结构。
- en: Next, the mutable queues and `QueueInputDStream` are created for appending data
    to simulate streaming. Once the `QueueInputDStream` starts receiving data then
    the streaming k-mean clustering begins to dynamically cluster data and printing
    out results. The interesting thing you will notice here is we are streaming the
    training dataset on one queue stream and the test data on another queue stream.
    As we append data to our queues, the KMeans clustering algorithm is processing
    our incoming data and dynamically generating clusters.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建了可变队列和`QueueInputDStream`，用于追加数据以模拟流式处理。一旦`QueueInputDStream`开始接收数据，流式k均值聚类就会开始动态地聚类数据并输出结果。您会注意到的一个有趣之处是，我们正在一个队列流上对训练数据集进行流式处理，而在另一个队列流上对测试数据进行流式处理。当我们向队列追加数据时，KMeans聚类算法正在处理我们的传入数据，并动态生成簇。
- en: There's more...
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Documentation for *StreamingKMeans()*:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 关于*StreamingKMeans()*的文档：
- en: '`StreamingKMeans`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamingKMeans`：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)'
- en: '`StreamingKMeansModel`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamingKMeansModel`：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)'
- en: See also
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'The hyper parameters defined via a builder pattern or `streamingKMeans` are:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构建器模式或`streamingKMeans`定义的超参数为：
- en: '[PRE45]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Downloading wine quality data for streaming regression
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载葡萄酒质量数据以进行流式回归
- en: In this recipe, we download and inspect the wine quality dataset from the UCI
    machine learning repository to prepare data for Spark's streaming linear regression
    algorithm from MLlib.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们从UCI机器学习存储库下载并检查葡萄酒质量数据集，为Spark的流式线性回归算法准备数据。
- en: How to do it...
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'You will need one of the following command-line tools `curl` or `wget` to retrieve
    specified data:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要以下命令行工具之一`curl`或`wget`来检索指定数据：
- en: 'You can start by downloading the dataset using either of the following three
    commands. The first one is as follows:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以从以下三个命令中任选其一下载数据集。第一个命令如下：
- en: '[PRE46]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You can also use the following command:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用以下命令：
- en: '[PRE47]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This command is the third way to do the same:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令是执行相同操作的第三种方式：
- en: '[PRE48]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now we begin our first steps of data exploration by seeing how the data in
    `winequality-white.csv` is formatted:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们开始通过查看`winequality-white.csv`中的数据格式来进行数据探索的第一步：
- en: '[PRE49]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now we take a look at the wine quality data to know how it is formatted:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来看看葡萄酒质量数据，了解其格式：
- en: '[PRE50]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: How it works...
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其工作原理...
- en: The data is comprised of 1,599 red wines and 4,898 white wines with 11 features
    and an output label that can be used during training.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 数据由1,599种红葡萄酒和4,898种白葡萄酒组成，具有11个特征和一个可用于训练的输出标签。
- en: 'The following is a list of features/attributes:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是特征/属性的列表：
- en: Fixed acidity
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 固定酸度
- en: Volatile acidity
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挥发性酸度
- en: Citric acid
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 柠檬酸
- en: Residual sugar
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残余糖分
- en: Chlorides
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 氯化物
- en: Free sulfur dioxide
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游离二氧化硫
- en: Total sulfur dioxide
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总二氧化硫
- en: Density
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密度
- en: pH
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pH
- en: Sulphates
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硫酸盐
- en: Alcohol
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 酒精
- en: 'The following is the output label:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出标签：
- en: quality (a numeric value between 0 to 10)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质量（介于0到10之间的数值）
- en: There's more...
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The following link lists datasets for popular machine learning algorithms. A
    new dataset can be chosen to experiment with as needed.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接列出了流行的机器学习算法的数据集。根据需要可以选择新数据集进行实验。
- en: Alternative datasets are available at [https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据集可在[https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research)获取。
- en: We selected the Iris dataset so we can use continuous numerical features for
    a linear regression model.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了鸢尾花数据集，以便我们可以使用连续数值特征进行线性回归模型。
- en: Streaming linear regression for a real-time regression
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时回归的流式线性回归
- en: In this recipe, we will use the wine quality dataset from UCI and Spark's streaming
    linear regression algorithm from MLlib to predict the quality of a wine based
    on a group of wine features.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将使用UCI的葡萄酒质量数据集和MLlib的Spark流式线性回归算法来基于一组葡萄酒特征预测葡萄酒质量。
- en: The difference between this recipe and the traditional regression recipes we
    saw before is the use of Spark ML streaming to score the quality of the wine in
    real time using a linear regression model.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方与之前看到的传统回归配方之间的区别在于使用Spark ML流式传输来实时使用线性回归模型评估葡萄酒质量。
- en: How to do it...
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE51]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Import the necessary packages:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE52]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Create Spark''s configuration and streaming context:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和流式上下文：
- en: '[PRE53]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志消息的交错导致难以阅读的输出，因此将日志级别设置为警告：
- en: '[PRE54]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Load the wine quality CSV using the Databricks CSV API into a DataFrame:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Databricks CSV API将葡萄酒质量CSV加载到DataFrame中：
- en: '[PRE55]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Convert the DataFrame into an `rdd` and `zip` a unique identifier onto it:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将DataFrame转换为`rdd`，并将唯一标识符`zip`到其上：
- en: '[PRE56]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Build a lookup map to compare predicted quality against actual quality value
    later:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建查找映射以稍后比较预测质量与实际质量值：
- en: '[PRE57]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Convert wine quality into label points for use with the machine learning library:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将葡萄酒质量转换为标签点，以便与机器学习库一起使用：
- en: '[PRE58]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Create a mutable queue for appending data to:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个可变队列以追加数据：
- en: '[PRE59]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Create Spark streaming queues to receive streaming data:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark流式队列以接收流式数据：
- en: '[PRE60]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Configure streaming linear regression model:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置流式线性回归模型：
- en: '[PRE61]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Train regression model and predict final values:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练回归模型并预测最终值：
- en: '[PRE62]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Start Spark streaming context:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark流式上下文：
- en: '[PRE63]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Split label point data into the training set and test set:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标签点数据拆分为训练集和测试集：
- en: '[PRE64]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Append data to training data queue for processing:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据追加到训练数据队列以进行处理：
- en: '[PRE65]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now split test data in half and append to queue for processing:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将测试数据分成两半并追加到队列以进行处理：
- en: '[PRE66]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Once data is received by the queue stream, you will see the following output:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据被队列流接收，您将看到以下输出：
- en: '![](img/e814ddc2-eebd-43f6-a81a-b84a2b60742c.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e814ddc2-eebd-43f6-a81a-b84a2b60742c.png)'
- en: 'Close the program by stopping the Spark streaming context:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止Spark流式上下文来关闭程序：
- en: '[PRE67]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: How it works...
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其工作原理...
- en: We started by loading the wine quality dataset into a DataFrame via Databrick's
    `spark-csv` library. The next step was to attach a unique identifier to each row
    in our dataset to later match the predicted quality to the actual quality. The
    raw data was converted to labeled points so it can be used as input for the streaming
    linear regression algorithm. In steps 9 and 10, we created instances of mutable
    queues and Spark's `QueueInputDStream` class to be used as a conduit into the
    regression algorithm.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过Databrick的`spark-csv`库将葡萄酒质量数据集加载到DataFrame中。下一步是为我们数据集中的每一行附加一个唯一标识符，以便稍后将预测质量与实际质量匹配。原始数据被转换为带标签的点，以便它可以作为流式线性回归算法的输入。在步骤9和10中，我们创建了可变队列和Spark的`QueueInputDStream`类的实例，用作进入回归算法的通道。
- en: We then created the streaming linear regression model, which will predict wine
    quality for our final results. We customarily created training and test datasets
    from the original data and appended them to the appropriate queue to start ...
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建了流式线性回归模型，该模型将预测我们的最终结果——葡萄酒质量。我们通常从原始数据中创建训练和测试数据集，并将它们附加到适当的队列以开始...
- en: There's more...
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Documentation for `StreamingLinearRegressionWithSGD()`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamingLinearRegressionWithSGD()`的文档：[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD)。'
- en: See also
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Hyper parameters for `StreamingLinearRegressionWithSGD()`*:*
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamingLinearRegressionWithSGD()`的超参数*：*'
- en: '`setInitialWeights(Vectors.*zeros*())`'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setInitialWeights(Vectors.*zeros*())`'
- en: '`setNumIterations()`'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setNumIterations()`'
- en: '`setStepSize()`'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setStepSize()`'
- en: '`setMiniBatchFraction()`'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setMiniBatchFraction()`'
- en: 'There is also a `StreamingLinearRegression()` API that does not use the **stochastic
    gradient descent** (**SGD**) version:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个不使用**随机梯度下降**（**SGD**）版本的`StreamingLinearRegression()` API：
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearAlgorithm](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearAlgorithm)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearAlgorithm](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearAlgorithm)'
- en: 'The following link provides a quick reference for linear regression:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了线性回归的快速参考：
- en: '[https://en.wikipedia.org/wiki/Linear_regression](https://en.wikipedia.org/wiki/Linear_regression)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Linear_regression](https://en.wikipedia.org/wiki/Linear_regression)'
- en: Downloading Pima Diabetes data for supervised classification
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载Pima糖尿病数据用于监督分类
- en: In this recipe, we download and inspect the Pima Diabetes dataset from the UCI
    machine learning repository. We will use the dataset later with Spark's streaming
    logistic regression algorithm.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们从UCI机器学习仓库下载并检查了Pima糖尿病数据集。稍后我们将使用该数据集与Spark的流式逻辑回归算法。
- en: How to do it...
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'You will need one of the following command-line tools `curl` or `wget` to retrieve
    the specified data:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要以下命令行工具之一`curl`或`wget`来检索指定数据：
- en: 'You can start by downloading the dataset using either two of the following
    commands. The first command is as follows:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过以下两个命令之一开始下载数据集。第一个命令如下：
- en: '[PRE68]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This is an alternative that you can use:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个你可以使用的替代方案：
- en: '[PRE69]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now we begin our first steps of data exploration by seeing how the data in
    `pima-indians-diabetes.data` is formatted (from Mac or Linux Terminal):'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们开始通过查看`pima-indians-diabetes.data`中的数据格式来进行数据探索的第一步（从Mac或Linux终端）：
- en: '[PRE70]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: How it works...
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We have 768 observations for the dataset. Each line/record is comprised of 10
    features and a label value that can be used for a supervised learning model (that
    is, logistic regression). The label/class is either a `1`, meaning tested positive
    for diabetes and `0` if the test came back negative.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集有768个观测值。每行/记录包含10个特征和一个标签值，可用于监督学习模型（即逻辑回归）。标签/类别为`1`表示检测出糖尿病阳性，`0`表示检测结果为阴性。
- en: '**Features/Attributes:**'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征/属性：**'
- en: Number of times pregnant
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 怀孕次数
- en: Plasma glucose concentration a 2 hours in an oral glucose tolerance test
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 口服葡萄糖耐量试验2小时血浆葡萄糖浓度
- en: Diastolic blood pressure (mm Hg)
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 舒张压（mm Hg）
- en: Triceps skin fold thickness (mm)
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三头肌皮肤褶皱厚度（mm）
- en: 2-hour serum insulin (mu U/ml)
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2小时血清胰岛素（mu U/ml）
- en: Body mass index (weight in kg/(height in m)^2)
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身体质量指数（体重（kg）/（身高（m）^2））
- en: Diabetes pedigree function
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 糖尿病遗传函数
- en: Age (years)
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄（岁）
- en: Class variable (0 or 1)
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别变量（0或1）
- en: '[PRE71]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: There's more...
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We found the following alternative datasets from Princeton University very
    helpful:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现普林斯顿大学提供的以下替代数据集非常有帮助：
- en: '[http://data.princeton.edu/wws509/datasets](http://data.princeton.edu/wws509/datasets)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://data.princeton.edu/wws509/datasets](http://data.princeton.edu/wws509/datasets)'
- en: See also
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: The dataset that you can use to explore this recipe has to be structured in
    a way that the label (prediction class) has to be binary (tested positive/negative
    for diabetes).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以用来探索此配方的数据集必须以这样的方式结构化：标签（预测类别）必须是二元的（检测为糖尿病阳性/阴性）。
- en: Streaming logistic regression for an on-line classifier
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式逻辑回归用于在线分类器
- en: In this recipe, we will be using the Pima Diabetes dataset we downloaded in
    the previous recipe and Spark's streaming logistic regression algorithm with SGD
    to predict whether a Pima with various features will test positive as a diabetic.
    It is an on-line classifier that learns and predicts based on the streamed data.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将使用之前配方中下载的Pima糖尿病数据集和Spark的流式逻辑回归算法与SGD来预测具有各种特征的Pima是否会检测为糖尿病阳性。它是一个在线分类器，根据流数据进行学习和预测。
- en: How to do it...
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序将驻留的包位置：
- en: '[PRE72]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Import the necessary packages:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE73]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Create a `SparkSession` object as an entry point to the cluster and a `StreamingContext`:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`SparkSession`对象作为集群的入口点和一个`StreamingContext`：
- en: '[PRE74]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    the logging level to warning:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志消息的交错导致难以阅读的输出，因此将日志级别设置为警告：
- en: '[PRE75]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Load the Pima data file into a Dataset of type string:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Pima数据文件加载到字符串类型的数据集中：
- en: '[PRE76]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Build an RDD from our raw Dataset by generating a tuple consisting of the last
    item into a record as the label and everything else as a sequence:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过生成一个元组，将最后一个项目作为标签，其余所有内容作为序列，从我们的原始数据集构建RDD：
- en: '[PRE77]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Convert the preprocessed data into label points for use with the machine learning
    library:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预处理的数据转换为标签点，以便与机器学习库一起使用：
- en: '[PRE78]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Create mutable queues for appending data to:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为追加数据创建可变队列：
- en: '[PRE79]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Create Spark streaming queues to receive streaming data:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark流队列以接收流数据：
- en: '[PRE80]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Configure the streaming logistic regression model:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置流式逻辑回归模型：
- en: '[PRE81]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Train the regression model and predict final values:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练回归模型并预测最终值：
- en: '[PRE82]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Start Spark streaming context:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark流上下文：
- en: '[PRE83]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Split label point data into the training set and test set:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标签点数据拆分为训练集和测试集：
- en: '[PRE84]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Append data to training data queue for processing:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据追加到训练数据队列以进行处理：
- en: '[PRE85]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Now split test data in half and append to the queue for processing:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将测试数据对半拆分并追加到队列以进行处理：
- en: '[PRE86]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Once data is received by the queue stream, you will see the following output:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据被队列流接收，您将看到以下输出：
- en: '[PRE87]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Close the program by stopping the Spark streaming context:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止Spark流上下文来关闭程序：
- en: '[PRE88]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: How it works...
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we loaded the Pima Diabetes Dataset into a Dataset and parsed it into
    a tuple by taking every element as a feature except the last one, which we used
    as a label. Second, we morphed the RDD of tuples into labeled points so it can
    be used as input to the streaming logistic regression algorithm. Third, we created
    instances of mutable queues and Spark's `QueueInputDStream` class to be used as
    a pathway into the logistic algorithm.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将Pima糖尿病数据集加载到数据集中，并将其解析为元组，除了最后一个元素外，我们将每个元素作为特征，最后一个元素作为标签。其次，我们将元组RDD转换为标记点，以便它可以作为流式逻辑回归算法的输入。第三，我们创建了可变队列和Spark的`QueueInputDStream`类的实例，用作逻辑算法的通道。
- en: Fourth, we created the streaming logistic regression model, which will predict
    wine quality for our final results. Finally, we customarily created training and
    test datasets from original data and appended it to the appropriate queue to trigger
    the model's processing of streaming data. The final ...
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，我们创建了流式逻辑回归模型，该模型将预测我们的最终结果的葡萄酒质量。最后，我们通常从原始数据创建训练和测试数据集，并将其追加到适当的队列以触发模型处理流数据。最终...
- en: There's more...
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Documentation for `StreamingLogisticRegressionWithSGD()` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamingLogisticRegressionWithSGD()`的文档可在[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD)查阅'
- en: See also
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'The hyper parameters for the model:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的超参数：
- en: '`setInitialWeights()`'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setInitialWeights()`'
- en: '`setNumIterations()`'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setNumIterations()`'
- en: '`setStepSize()`'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setStepSize()`'
- en: '`setMiniBatchFraction()`'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setMiniBatchFraction()`'
