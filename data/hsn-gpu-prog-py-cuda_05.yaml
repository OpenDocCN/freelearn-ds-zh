- en: Streams, Events, Contexts, and Concurrency
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流、事件、上下文和并发
- en: 'In the prior chapters, we saw that there are two primary operations we perform
    from the host when interacting with the GPU:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了在与GPU交互时，从主机执行的两个主要操作：
- en: Copying memory data to and from the GPU
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将内存数据复制到和从GPU
- en: Launching kernel functions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动内核函数
- en: We know that *within* a single kernel, there is one level of concurrency among
    its many threads; however, there is another level of concurrency *over* multiple
    kernels *and* GPU memory operations that are also available to us. This means
    that we can launch multiple memory and kernel operations at once, without waiting
    for each operation to finish. However, on the other hand, we will have to be somewhat
    organized to ensure that all inter-dependent operations are synchronized; this
    means that we shouldn't launch a particular kernel until its input data is fully
    copied to the device memory, or we shouldn't copy the output data of a launched
    kernel to the host until the kernel has finished execution.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在单个内核中，其众多线程之间存在一个并发级别；然而，还有另一个并发级别，是针对多个内核以及我们可用的GPU内存操作。这意味着我们可以同时启动多个内存和内核操作，而无需等待每个操作完成。然而，另一方面，我们必须要有一定的组织性，以确保所有相互依赖的操作都得到同步；这意味着我们不应该在输入数据完全复制到设备内存之前启动特定的内核，或者不应该在内核执行完成之前将启动内核的输出数据复制到主机。
- en: To this end, we have what are known as **CUDA** **streams**—a **stream** is
    a sequence of operations that are run in order on the GPU. By itself, a single
    stream isn't of any use—the point is to gain concurrency over GPU operations issued
    by the host by using multiple streams. This means that we should interleave launches
    of GPU operations that correspond to different streams, in order to exploit this
    notion.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，我们有了所谓的**CUDA流**——一个**流**是一系列按顺序在GPU上运行的操作。单独来看，一个流本身并没有什么用处——关键是通过使用多个流来获得对主机发出的GPU操作的并发性。这意味着我们应该交错启动对应不同流的GPU操作，以利用这个概念。
- en: We will be covering this notion of streams extensively in this chapter. Additionally,
    we will look at **events**, which are a feature of streams that are used to precisely
    time kernels and indicate to the host as to what operations have been completed
    within a given stream.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将广泛介绍流的概念。此外，我们还将探讨**事件**，这是流的一个特性，用于精确地计时内核，并向主机指示在给定流中完成了哪些操作。
- en: Finally, we will briefly look at CUDA **contexts**. A **context** can be thought
    of as analogous to a process in your operating system, in that the GPU keeps each
    context's data and kernel code *walled off* and encapsulated away from the other
    contexts currently existing on the GPU. We will see the basics of this near the
    end of the chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将简要介绍CUDA **上下文**。一个**上下文**可以被视为与操作系统中进程的类似，因为GPU将每个上下文的数据和内核代码*隔离*并封装起来，使其与其他当前存在于GPU上的上下文隔离开。我们将在本章末尾看到这个概念的基本内容。
- en: 'The following are the learning outcomes for this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为本章的学习成果：
- en: Understanding the concepts of device and stream synchronization
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解设备和流同步的概念
- en: Learning how to effectively use streams to organize concurrent GPU operations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何有效地使用流来组织并发GPU操作
- en: Learning how to effectively use CUDA events
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何有效地使用CUDA事件
- en: Understanding CUDA contexts
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解CUDA上下文
- en: Learning how to explicitly synchronize within a given context
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何在给定上下文中显式同步
- en: Learning how to explicitly create and destroy a CUDA context
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何显式创建和销毁CUDA上下文
- en: Learning how to use contexts to allow for GPU usage among multiple processes
    and threads on the host
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用上下文以允许在主机上的多个进程和线程中使用GPU
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016—onward) is required
    for this chapter, with all necessary GPU drivers and the CUDA Toolkit (9.0–onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本章，需要一个配备现代NVIDIA GPU（2016年及以后）的Linux或Windows 10 PC，并安装所有必要的GPU驱动程序和CUDA Toolkit（9.0及以后）。还需要一个合适的Python
    2.7安装（例如Anaconda Python 2.7），并包含PyCUDA模块。
- en: 'This chapter''s code is also available on GitHub:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码也可在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)'
- en: For more information about the prerequisites, check the *Preface* of this book,
    and for the software and hardware requirements, check the README in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 关于先决条件的更多信息，请查看本书的**前言**，而对于软件和硬件要求，请查看[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)中的README。
- en: CUDA device synchronization
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA设备同步
- en: Before we can use CUDA streams, we need to understand the notion of **device
    synchronization**. This is an operation where the host blocks any further execution
    until all operations issued to the GPU (memory transfers and kernel executions)
    have completed. This is required to ensure that operations dependent on prior
    operations are not executed out-of-order—for example, to ensure that a CUDA kernel
    launch is completed before the host tries to read its output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够使用CUDA流之前，我们需要理解**设备同步**的概念。这是一个操作，其中主机阻止任何进一步的执行，直到所有发送到GPU的操作（内存传输和内核执行）都已完成。这是为了确保依赖于先前操作的操作不会以错误的顺序执行——例如，确保在主机尝试读取输出之前，CUDA内核启动已完成。
- en: 'In CUDA C, device synchronization is performed with the `cudaDeviceSynchronize`
    function. This function effectively blocks further execution on the host until
    all GPU operations have completed. `cudaDeviceSynchronize` is so fundamental that
    it is usually one of the very first topics covered in most books on CUDA C—we
    haven''t seen this yet, because PyCUDA has been invisibly calling this for us
    automatically as needed. Let''s take a look at an example of CUDA C code to see
    how this is done manually:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA C中，设备同步是通过`cudaDeviceSynchronize`函数实现的。这个函数有效地阻止了主机上的进一步执行，直到所有GPU操作都已完成。`cudaDeviceSynchronize`如此基本，以至于它通常是大多数CUDA
    C书籍中最早涉及的主题之一——我们还没有看到这一点，因为PyCUDA已经在我们需要时自动为我们调用。让我们看看CUDA C代码的一个例子，看看这是如何手动完成的：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this block of code, we see that we have to synchronize with the device directly
    after every single GPU operation. If we only have a need to call a single CUDA
    kernel at a time, as seen here, this is fine. But if we want to concurrently launch
    multiple independent kernels and memory operations operating on different arrays
    of data, it would be inefficient to synchronize across the entire device. In this
    case, we should synchronize across multiple streams. We'll see how to do this
    right now.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码块中，我们可以看到，在每次GPU操作之后，我们必须直接与设备同步。如果我们只需要一次调用一个CUDA内核，就像这里看到的那样，这是可以的。但如果我们想要并发启动多个独立的核心和操作不同数据数组的内存操作，在整个设备上进行同步将是不高效的。在这种情况下，我们应该在多个流中进行同步。我们现在将看到如何做到这一点。
- en: Using the PyCUDA stream class
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyCUDA流类
- en: We will start with a simple PyCUDA program; all this will do is generate a series
    of random GPU arrays, process each array with a simple kernel, and copy the arrays
    back to the host. We will then modify this to use streams. Keep in mind this program
    will have no point at all, beyond illustrating how to use streams and some basic
    performance gains you can get. (This program can be seen in the `multi-kernel.py`
    file, under the `5` directory in the GitHub repository.)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的PyCUDA程序开始；所有这些都将做的是生成一系列随机的GPU数组，使用简单的内核处理每个数组，并将数组复制回主机。然后我们将修改这个程序以使用流。请记住，这个程序将没有任何意义，除了说明如何使用流以及你可以获得的一些基本性能提升。（这个程序可以在GitHub仓库中的`multi-kernel.py`文件下找到，位于`5`目录中。）
- en: 'Of course, we''ll start by importing the appropriate Python modules, as well
    as the `time` function:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们首先需要导入适当的Python模块，以及`time`函数：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We now will specify how many arrays we wish to process—here, each array will
    be processed by a different kernel launch. We also specify the length of the random
    arrays we will generate, as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将指定我们希望处理多少数组——在这里，每个数组将由不同的内核启动处理。我们还指定了我们将生成的随机数组的长度，如下所示：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now have a kernel that operates on each array; all this will do is iterate
    over each point in the array, and multiply and divide it by 2 for 50 times, ultimately
    leaving the array intact. We want to restrict the number of threads that each
    kernel launch will use, which will help us gain concurrency among many kernel
    launches on the GPU so that we will have each thread iterate over different parts
    of the array with a `for` loop. (Again, remember that this kernel function will
    be completely useless for anything other than for learning about streams and synchronization!)
    If each kernel launch uses too many threads, it will be harder to gain concurrency
    later:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个对每个数组进行操作的内核；所有这些将做的只是遍历数组中的每个点，并将其乘以 2 并除以 2 共 50 次，最终保持数组不变。我们希望限制每个内核启动使用的线程数，这将帮助我们获得
    GPU 上多个内核启动的并发性，以便每个线程可以通过 `for` 循环遍历数组的不同部分。(再次提醒，这个内核函数除了了解流和同步之外，对其他任何用途都是完全无用的!)
    如果每个内核启动使用太多的线程，那么在以后获得并发性将会更难：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we will generate some random data array, copy these arrays to the GPU,
    iteratively launch our kernel over each array across 64 threads, and then copy
    the output data back to the host and assert that the same with NumPy''s `allclose`
    function. We will time the duration of all operations from start to finish by
    using Python''s `time` function, as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将生成一些随机数据数组，将这些数组复制到 GPU 上，迭代地在 64 个线程上启动我们的内核，然后将输出数据复制回主机，并使用 NumPy 的
    `allclose` 函数断言它们相同。我们将使用 Python 的 `time` 函数记录从开始到结束的所有操作的持续时间，如下所示：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are now prepared to run this program. I will run it right now:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好运行这个程序。我现在就会运行它：
- en: '![](img/e590fec8-0e98-4fce-bd4e-091871758825.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e590fec8-0e98-4fce-bd4e-091871758825.png)'
- en: So, it took almost three seconds for this program to complete. We will make
    a few simple modifications so that our program can use streams, and then see if
    we can get any performance gains (this can be seen in the `multi-kernel_streams.py`
    file in the repository).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个程序完成几乎花了三秒钟。我们将进行一些简单的修改，以便我们的程序可以使用流，然后看看我们是否能获得任何性能提升（这可以在存储库中的 `multi-kernel_streams.py`
    文件中看到）。
- en: 'First, we note that for each kernel launch we have a separate array of data
    that it processes, and these are stored in Python lists. We will have to create
    a separate stream object for each individual array/kernel launch pair, so let''s
    first add an empty list, entitled `streams`, that will hold our stream objects:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们注意到对于每个内核启动，我们都有一个处理的数据数组，这些数据存储在 Python 列表中。我们必须为每个单独的数组/内核启动对创建一个单独的流对象，所以让我们首先添加一个名为
    `streams` 的空列表，它将保存我们的流对象：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now generate a series of streams that we will use to organize the kernel
    launches. We can get a stream object from the `pycuda.driver` submodule with the
    `Stream` class. Since we''ve imported this submodule and aliased it as `drv`,
    we can fill up our list with new stream objects, as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以生成一系列我们将用于组织内核启动的流。我们可以从 `pycuda.driver` 子模块使用 `Stream` 类获取流对象。由于我们已经导入了这个子模块并将其别名为
    `drv`，我们可以用新的流对象填充我们的列表，如下所示：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we will have to first modify our memory operations that transfer data
    to the GPU. Consider the following steps for it:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们首先需要修改将数据传输到 GPU 的内存操作。考虑以下步骤：
- en: 'Look for the first loop that copies the arrays to the GPU with the `gpuarray.to_gpu`
    function. We will want to switch to the asynchronous and stream-friendly version
    of this function, `gpu_array.to_gpu_async`, instead. (We must now also specify
    which stream each memory operation should use with the `stream` parameter):'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '查找第一个使用 `gpuarray.to_gpu` 函数将数组复制到 GPU 的循环。我们希望切换到异步和流友好的版本，即 `gpu_array.to_gpu_async`
    函数。 (我们现在还必须使用 `stream` 参数指定每个内存操作应使用的流):'
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can now launch our kernels. This is exactly as before, only we must specify
    what stream to use by using the `stream` parameter:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以启动我们的内核。这完全和以前一样，只是我们必须通过使用 `stream` 参数指定要使用哪个流：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we need to pull our data off the GPU. We can do this by switching
    the `gpuarray get` function to `get_async`, and again using the `stream` parameter,
    as follows:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要将我们的数据从 GPU 上拉下来。我们可以通过将 `gpuarray get` 函数切换到 `get_async`，并再次使用 `stream`
    参数来实现，如下所示：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We are now ready to run our stream-friendly modified program:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好运行我们的流友好修改后的程序：
- en: '![](img/1e11ea75-4947-459b-a915-363bdad7b241.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1e11ea75-4947-459b-a915-363bdad7b241.png)'
- en: In this case, we have a triple-fold performance gain, which is not too bad considering
    the very few numbers of modifications we had to make. But before we move on, let's
    try to get a deeper understanding as to why this works.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有三重性能提升，考虑到我们不得不做的修改非常少，这并不算太坏。但在我们继续之前，让我们尝试更深入地理解为什么这会起作用。
- en: 'Let''s consider the case of two CUDA kernel launches. We will also perform
    GPU memory operations corresponding to each kernel before and after we launch
    our kernels, for a total of six operations. We can visualize the operations happening
    on the GPU with respect to time with a graph as such—moving to the right on the
    *x*-axis corresponds to time duration, while the *y*-axis corresponds to operations
    being executed on the GPU at a particular time. This is depicted with the following
    diagram:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑两个CUDA内核调用的案例。我们将在启动内核前后执行与每个内核对应的GPU内存操作，总共六个操作。我们可以用以下图表可视化GPU上随时间发生的操作——沿*x*轴向右移动对应时间长度，而*y*轴对应特定时间执行的GPU操作。这可以用以下图表表示：
- en: '![](img/9c272bdc-c6e1-4438-96ad-392af521175d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9c272bdc-c6e1-4438-96ad-392af521175d.png)'
- en: 'It''s not too hard to visualize why streams work so well in performance increase—since
    operations in a single stream are blocked until only all *necessary* prior operations
    are competed, we will gain concurrency among distinct GPU operations and make
    full use of our device. This can be seen by the large overlap of concurrent operations.
    We can visualize stream-based concurrency over time as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化为什么流在性能提升方面工作得如此之好并不太难——由于单个流中的操作只有在所有*必要*的先前操作都完成后才会被阻塞，我们将获得不同GPU操作之间的并发性，并充分利用我们的设备。这可以通过并发操作的巨大重叠来看到。我们可以如下可视化基于流的并发性随时间的变化：
- en: '![](img/d01793e3-d5a6-4ff8-b3fc-f22c198c7962.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d01793e3-d5a6-4ff8-b3fc-f22c198c7962.png)'
- en: Concurrent Conway's game of life using CUDA streams
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA流并发执行康威生命游戏
- en: We will now see a more interesting application—we will modify the LIFE (Conway's
    *Game of Life*) simulation from the last chapter, so that we will have four independent
    windows of animation displayed concurrently. (It is suggested you look at this
    example from the last chapter, if you haven't yet.)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到一个更有趣的应用——我们将修改上一章的LIFE（康威生命游戏）模拟，这样我们就可以同时显示四个独立的动画窗口。（如果你还没有看过上一章的例子，建议你看看这个例子。）
- en: Let's get a copy of the old LIFE simulation from the last chapter in the repository,
    which should be under `conway_gpu.py` in the `4` directory. We will now modify
    this into our new CUDA-stream based concurrent LIFE simulation. (This new streams-based
    simulation that we will see in a moment is also available in the `conway_gpu_streams.py`
    file in this chapter's directory, `5`.)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从存储库中获取上一章的旧LIFE模拟副本，它应该在`4`目录下的`conway_gpu.py`文件中。我们现在将修改它以创建基于CUDA流的新并发LIFE模拟。（我们将在下一章的`5`目录下的`conway_gpu_streams.py`文件中看到这个基于流的模拟。）
- en: 'Go to the main function at the end of the file. We will set a new variable
    that indicates how many concurrent animations we will display at once with `num_concurrent`
    (where `N` indicates the height/width of the simulation lattice, as before). We
    will set it to `4` here, but you can feel free to try other values:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 前往文件末尾的主函数。我们将设置一个新变量`num_concurrent`，它指示我们一次将显示多少个并发动画（其中`N`表示模拟格子的高度/宽度，如前所述）。我们在这里将其设置为`4`，但你可以自由尝试其他值：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will now need a collection of `num_concurrent` stream objects, and will
    also need to allocate a collection of input and output lattices on the GPU. We''ll
    of course just store these in lists and initialize the lattices as before. We
    will set up some empty lists and fill each with the appropriate objects over a
    loop, as such (notice how we set up a new initial state lattice on each iteration,
    send it to the GPU, and concatenate it to `lattices_gpu`):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要一组`num_concurrent`流对象，并且还需要在GPU上分配一组输入和输出格子。当然，我们将将这些存储在列表中，并像以前一样初始化格子。我们将设置一些空列表，并在循环中填充适当的对象，如下所示（注意我们如何在每次迭代中设置一个新的初始状态格子，将其发送到GPU，并将其连接到`lattices_gpu`）：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Since we're only doing this loop once during the startup of our program and
    the virtually all of the computational work will be in the animation loop, we
    really don't have to worry about actually using the streams we just immediately
    generated.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只是在程序启动时执行这个循环一次，并且几乎所有计算工作都会在动画循环中完成，所以我们实际上并不需要担心使用我们刚刚立即生成的流。
- en: 'We will now set up the environment with Matplotlib using the subplots function;
    notice how we can set up multiple animation plots by setting the `ncols` parameter.
    We will have another list structure that will correspond to the images that are
    required for the animation updates in `imgs`. Notice how we can now set this up
    with `get_async` and the appropriate corresponding stream:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用 Matplotlib 的子图功能设置环境；注意我们如何通过设置 `ncols` 参数来设置多个动画图。我们将有一个另一个列表结构，它将对应于
    `imgs` 中所需的动画更新图像。注意我们现在如何使用 `get_async` 和适当的对应流来设置它：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The last thing to change in the main function is the penultimate line starting
    with `ani = animation.FuncAnimation`. Let''s modify the arguments to the `update_gpu`
    function to reflect the new lists we are using and add two more arguments, one
    to pass our `streams` list, plus a parameter to indicate how many concurrent animations
    there should be:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在主函数中最后要更改的是以 `ani = animation.FuncAnimation` 开头的倒数第二行。让我们修改 `update_gpu` 函数的参数，以反映我们正在使用的新列表，并添加两个额外的参数，一个用于传递我们的
    `streams` 列表，以及一个参数来指示应该有多少个并发动画：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We now duly make the required modifications to the `update_gpu` function to
    take these extra parameters. Scroll up a bit in the file and modify the parameters
    as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对 `update_gpu` 函数进行必要的修改，以接受这些额外的参数。在文件中向上滚动一点，并按以下方式修改参数：
- en: '`def update_gpu(frameNum, imgs, newLattices_gpu, lattices_gpu, N, streams,
    num_concurrent)`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`def update_gpu(frameNum, imgs, newLattices_gpu, lattices_gpu, N, streams,
    num_concurrent)`:'
- en: 'We now need to modify this function to iterate `num_concurrent` times and set
    each element of `imgs` as before, before finally returning the whole `imgs` list:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要修改这个函数，使其迭代 `num_concurrent` 次，并在返回整个 `imgs` 列表之前，像之前一样设置 `imgs` 的每个元素：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice the changes we made—each kernel is launched in the appropriate stream,
    while `get` has been switched to a `get_async` synchronized with the same stream.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们做出的更改——每个内核都在适当的流中启动，而 `get` 已切换到与同一流同步的 `get_async`。
- en: Finally, the last line in the loop copies GPU data from one device array to
    another without any re-allocation. Before, we could use the shorthand slicing
    operator `[:]` to directly copy the elements between the arrays without re-allocating
    any memory on the GPU; in this case, the slicing operator notation acts as an
    alias for the PyCUDA `set` function for GPU arrays. (`set`, of course, is the
    function that copies one GPU array to another of the same size, without any re-allocation.)
    Luckily, there is indeed a stream-synchronized also version of this function,
    `set_async`, but we need to use this specifically to call this function, explicitly
    specifying the array to copy and the stream to use.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，循环中的最后一行将 GPU 数据从一个设备数组复制到另一个数组，而不进行任何重新分配。在此之前，我们可以使用简写切片操作符 `[:]` 直接在数组之间复制元素，而不在
    GPU 上重新分配任何内存；在这种情况下，切片操作符的表示法充当了 PyCUDA `set` 函数的 GPU 数组的别名。（当然，“set” 是将一个 GPU
    数组复制到另一个相同大小的数组而不进行任何重新分配的函数。）幸运的是，确实存在这个函数的流同步版本，即 `set_async`，但我们需要明确使用这个函数，并指定要复制的数组和要使用的流。
- en: 'We''re now finished and ready to run this. Go to a Terminal and enter `python
    conway_gpu_streams.py` at the command line to enjoy the show:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成并准备好运行这个程序。转到终端，在命令行中输入 `python conway_gpu_streams.py` 来享受这个展示：
- en: '![](img/93d56393-5968-409d-bcab-d56330f6bc91.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/93d56393-5968-409d-bcab-d56330f6bc91.png)'
- en: Events
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事件
- en: '**Events** are objects that exist *on the GPU*, whose purpose is to act as
    milestones or progress markers for a stream of operations. Events are generally
    used to provide measure time duration *on the device side* to precisely time operations;
    the measurements we have been doing so far have been with host-based Python profilers
    and standard Python library functions such as `time`. Additionally, events they
    can also be used to provide a status update for the host as to the state of a
    stream and what operations it has already completed, as well as for explicit stream-based
    synchronization.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**事件**是存在于 GPU 上的对象，其目的是作为操作流的里程碑或进度标记。事件通常用于在设备端提供测量时间持续性的功能，以精确地计时操作；我们迄今为止所做的时间测量都是使用基于主机的
    Python 性能分析器和标准 Python 库函数，如 `time`。此外，事件还可以用于向主机提供有关流的状态以及它已经完成哪些操作的状态更新，以及用于基于流的显式同步。'
- en: Let's start with an example that uses no explicit streams and uses events to
    measure only one single kernel launch. (If we don't explicitly use streams in
    our code, CUDA actually invisibly defines a default stream that all operations
    will be placed into).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从使用无显式流的示例开始，并使用事件仅测量单个内核启动。（如果我们不在我们的代码中显式使用流，CUDA实际上会无意识地定义一个默认流，所有操作都将放入该流中）。
- en: 'Here, we will use the same useless multiply/divide loop kernel and header as
    we did at the beginning of the chapter, and modify most of the following contents.
    We want a single kernel instance to run a long time for this example, so we will
    generate a huge array of random numbers for the kernel to process, as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用与本章开头相同的无用的乘除循环内核和头文件，并修改以下大部分内容。我们希望这个示例中单个内核实例运行很长时间，因此我们将生成一个巨大的随机数数组供内核处理，如下所示：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We now construct our events using the `pycuda.driver.Event` constructor (where,
    of course, `pycuda.driver` has been aliased as `drv` by our prior import statement).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用`pycuda.driver.Event`构造函数构建我们的事件（当然，`pycuda.driver`已经被我们之前的导入语句别名为`drv`）。
- en: 'We will create two event objects here, one for the start of the kernel launch,
    and the other for the end of the kernel launch, (We will always need *two* event
    objects to measure any single GPU operation, as we will see soon):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里创建两个事件对象，一个用于内核启动的开始，另一个用于内核启动的结束（我们将始终需要两个事件对象来测量任何单个GPU操作，就像我们很快就会看到的那样）：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we are about ready to launch our kernel, but first, we have to mark the
    `start_event` instance''s place in the stream of execution with the event record
    function. We launch the kernel and then mark the place of `end_event` in the stream
    of execution, and also with `record`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们即将启动内核，但首先，我们必须使用事件记录函数标记`start_event`实例在执行流中的位置。我们启动内核，然后标记`end_event`在执行流中的位置，并使用`record`：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Events have a binary value that indicates whether they were reached or not
    yet, which is given by the function query. Let''s print a status update for both
    events, immediately after the kernel launch:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 事件具有二进制值，表示它们是否已到达或尚未到达，这由函数query给出。让我们在内核启动后立即打印两个事件的更新状态：
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s run this right now and see what happens:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在运行这个程序并看看会发生什么：
- en: '![](img/b7cd2aa1-cb0c-485a-939b-cf2d7fe35d1e.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7cd2aa1-cb0c-485a-939b-cf2d7fe35d1e.png)'
- en: Our goal here is to ultimately measure the time duration of our kernel execution,
    but the kernel hasn't even apparently launched yet. Kernels in PyCUDA have launched
    asynchronously (whether they exist in a specific stream or not), so we have to
    have to ensure that our host code is properly synchronized with the GPU.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是最终测量内核执行的持续时间，但内核显然还没有启动。PyCUDA中的内核是异步启动的（无论它们是否存在于特定的流中），因此我们必须确保我们的主机代码与GPU正确同步。
- en: 'Since `end_event` comes last, we can block further host code execution until
    the kernel completes by this event object''s synchronize function; this will ensure
    that the kernel has completed before any further lines of host code are executed.
    Let''s add a line a line of code to do this in the appropriate place:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`end_event`是最后一个到达的，我们可以通过这个事件对象的synchronize函数阻塞进一步的宿主代码执行，直到内核完成；这将确保在执行任何进一步的宿主代码行之前内核已经完成。让我们在适当的位置添加一行代码来完成这个操作：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we are ready to measure the execution time of the kernel; we do this
    with the event object''s `time_till` or `time_since` operations to compare to
    another event object to get the time between these two events in milliseconds.
    Let''s use the `time_till` operation of `start_event` on `end_event`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备测量内核的执行时间；我们使用事件对象的`time_till`或`time_since`操作来与另一个事件对象进行比较，以获取这两个事件之间的时间（以毫秒为单位）。让我们使用`start_event`的`time_till`操作来测量`end_event`：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Time duration can be measured between two events that have already occurred
    on the GPU with the `time_till` and `time_since` functions. Note that these functions
    always return a value in terms of milliseconds!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`time_till`和`time_since`函数测量GPU上已经发生的两个事件之间的持续时间。请注意，这些函数始终以毫秒为单位返回值！
- en: 'Let''s try running our program again now:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在再次尝试运行我们的程序：
- en: '![](img/29aadcbf-d395-487a-ac70-f3c422ae6f12.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29aadcbf-d395-487a-ac70-f3c422ae6f12.png)'
- en: (This example is also available in the `simple_event_example.py` file in the
    repository.)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: （此示例也可在存储库中的`simple_event_example.py`文件中找到。）
- en: Events and streams
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事件和流
- en: We will now see how to use event objects with respect to streams; this will
    give us a highly intricate level of control over the flow of our various GPU operations,
    allowing us to know exactly how far each individual stream has progressed via
    the `query` function, and even allowing us to synchronize particular streams with
    the host while ignoring the other streams.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将了解如何使用事件对象与流相关联；这将使我们能够对各种GPU操作的流程有高度细致的控制，使我们能够确切地知道每个单独的流通过`query`函数已经进展到什么程度，甚至允许我们在忽略其他流的同时，将特定流与主机同步。
- en: 'First, though, we have to realize this—each stream has to have its own dedicated
    collection of event objects; multiple streams cannot share an event object. Let''s
    see what this means exactly by modifying the prior example, `multi_kernel_streams.py`.
    After the kernel definition, let''s add two additional empty lists—`start_events`
    and `end_events`. We will fill these lists up with event objects, which will correspond
    to each stream that we have. This will allow us to time one GPU operation in each
    stream, since every GPU operation requires two events:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须认识到这一点——每个流都必须有自己的专用事件对象集合；多个流不能共享一个事件对象。让我们通过修改先前的示例`multi_kernel_streams.py`来具体看看这意味着什么。在内核定义之后，让我们添加两个额外的空列表——`start_events`和`end_events`。我们将用事件对象填充这些列表，这些对象将对应于我们拥有的每个流。这将使我们能够为每个流中的每个GPU操作计时，因为每个GPU操作都需要两个事件：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we can time each kernel launch individually by modifying the second loop
    to use the record of the event at the beginning and end of the launch. Notice
    that here, since there are multiple streams, we have to input the appropriate
    stream as a parameter to each event object''s `record` function. Also, notice
    that we can capture the end events in a second loop; this will still allow us
    to capture kernel execution duration perfectly, without any delay in launching
    the subsequent kernels. Now consider the following code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过修改第二个循环来单独计时每个内核启动，使用启动开始和结束的事件记录。请注意，由于存在多个流，我们必须将适当的流作为参数输入到每个事件对象的`record`函数中。另外，请注意，我们可以在第二个循环中捕获结束事件；这仍然允许我们完美地捕获内核执行持续时间，而不会在启动后续内核时产生任何延迟。现在考虑以下代码：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now we''re going to extract the duration of each individual kernel launch.
    Let''s add a new empty list after the iterative assert check, and fill it with
    the duration by way of the `time_till` function:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将提取每个单独内核启动的持续时间。在迭代断言检查之后添加一个新空列表，并通过`time_till`函数填充持续时间：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s now add two `print` statements at the very end, to tell us the mean
    and standard deviation of the kernel execution times:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在最后添加两个`print`语句，以告诉我们内核执行时间的平均值和标准差：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can now run this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行这个：
- en: '![](img/40d38973-beee-4d08-8e0b-3847ae757c8e.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40d38973-beee-4d08-8e0b-3847ae757c8e.png)'
- en: (This example is also available as `multi-kernel_events.py` in the repository.)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: （此示例也可在存储库中作为`multi-kernel_events.py`找到。）
- en: We see that there is a relatively low degree of standard deviation in kernel
    duration, which is good, considering each kernel processes the same amount of
    data over the same block and grid size—if there were a high degree of deviation,
    then that would mean that we were making highly uneven usage of the GPU in our
    kernel executions, and we would have to re-tune parameters to gain a greater level
    of concurrency.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，内核持续时间的标准偏差相对较低，这是好的，因为每个内核在相同的块和网格大小上处理相同数量的数据——如果存在高度偏差，那么这意味着我们在内核执行中对GPU的使用非常不均匀，我们就必须重新调整参数以获得更高的并发级别。
- en: Contexts
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文
- en: A CUDA **context** is usually described as being analogous to a process in an
    operating system. Let's review what this means—a process is an instance of a single
    program running on a computer; all programs outside of the operating system kernel
    run in a process. Each process has its own set of instructions, variables, and
    allocated memory, and is, generally speaking, blind to the actions and memory
    of other processes. When a process ends, the operating system kernel performs
    a cleanup, ensuring that all memory that the process allocated has been de-allocated,
    and closing any files, network connections, or other resources the process has
    made use of. (Curious Linux users can view the processes running on their computer
    with the command-line `top` command, while Windows users can view them with the
    Windows Task Manager).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA **上下文**通常被描述为类似于操作系统中的一个进程。让我们回顾一下这意味着什么——进程是单个程序在计算机上运行的实例；所有在操作系统内核之外运行的程序都在进程中运行。每个进程都有自己的指令集、变量和分配的内存，并且一般来说，对其他进程的动作和内存是盲目的。当一个进程结束时，操作系统内核执行清理操作，确保进程分配的所有内存都已释放，并关闭进程所使用的任何文件、网络连接或其他资源。（好奇的Linux用户可以使用命令行`top`命令查看他们计算机上运行的进程，而Windows用户可以使用Windows任务管理器查看它们）。
- en: Similar to a process, a context is associated with a single host program that
    is using the GPU. A context holds in memory all CUDA kernels and allocated memory
    that is making use of and is blind to the kernels and memory of other currently
    existing contexts. When a context is destroyed (at the end of a GPU based program,
    for example), the GPU performs a cleanup of all code and allocated memory within
    the context, freeing resources up for other current and future contexts. The programs
    that we have been writing so far have all existed within a single context, so
    these operations and concepts have been invisible to us.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 与进程类似，上下文与单个使用GPU的主程序相关联。上下文在内存中保存所有正在使用和分配的CUDA内核和内存，对其他当前存在的上下文中的内核和内存是盲目的。当一个上下文被销毁（例如在基于GPU的程序结束时），GPU将执行上下文中所有代码和分配的内存的清理，为其他当前和未来的上下文释放资源。我们迄今为止编写的所有程序都存在于单个上下文中，因此这些操作和概念对我们来说是不可见的。
- en: Let's also remember that a single program starts as a single process, but it
    can fork itself to run across multiple processes or threads. Analogously, a single
    CUDA host program can generate and use multiple CUDA contexts on the GPU. Usually,
    we will create a new context when we want to gain host-side concurrency when we
    fork new processes or threads of a host process. (It should be emphasized, however,
    that there is no exact one-to-one relation between host processes and CUDA contexts).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们记住，单个程序最初是一个单个进程，但它可以自身复制以在多个进程或线程中运行。类似地，单个CUDA主机程序可以在GPU上生成和使用多个CUDA上下文。通常，当我们想要在新的进程或线程中获取主机并发性时，我们会创建一个新的上下文。（然而，应该强调的是，主机进程和CUDA上下文之间没有确切的1对1关系）。
- en: As in many other areas of life, we will start with a simple example. We will
    first see how to access a program's default context and synchronize across it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 就像生活中的许多其他领域一样，我们将从一个简单的例子开始。我们首先将了解如何访问程序默认的上下文并在其上进行同步。
- en: Synchronizing the current context
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同步当前上下文
- en: We're going to see how to explicitly synchronize our device within a context
    from within Python as in CUDA C; this is actually one of the most fundamental
    skills to know in CUDA C, and is covered in the first or second chapters in most
    other books on the topic. So far, we have been able to avoid this topic, since
    PyCUDA has performed most synchronizations for us automatically with `pycuda.gpuarray`
    functions such as `to_gpu` or `get`; otherwise, synchronization was handled by
    streams in the case of the `to_gpu_async` or `get_async` functions, as we saw
    at the beginning of this chapter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将了解如何在Python中显式同步我们的设备上下文，就像在CUDA C中一样；这实际上是CUDA C中需要了解的最基本技能之一，并且在大多数其他关于此主题的书籍的第一章或第二章中都有涉及。到目前为止，我们能够避免这个话题，因为PyCUDA已经通过`pycuda.gpuarray`函数（如`to_gpu`或`get`）为我们自动执行了大多数同步操作；否则，在`to_gpu_async`或`get_async`函数的情况下，同步由流处理，正如我们在本章开头所看到的。
- en: We will be humble and start by modifying the program we wrote in [Chapter 3](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml),
    *Getting Started with PyCUDA,* which generates an image of the Mandelbrot set
    using explicit context synchronization. (This is available here as the file `gpu_mandelbrot0.py`
    under the `3` directory in the repository.)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将谦逊地开始修改我们在[第3章](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml)，“PyCUDA入门”中编写的程序，该程序使用显式上下文同步生成Mandelbrot集的图像。（这可以在存储库中的`3`目录下的文件`gpu_mandelbrot0.py`中找到。）
- en: We won't get any performance gains over our original Mandelbrot program here;
    the only point of this exercise is just to help us understand CUDA contexts and
    GPU synchronization.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会比原始的Mandelbrot程序获得任何性能提升；这个练习的唯一目的只是帮助我们理解CUDA上下文和GPU同步。
- en: Looking at the header, we, of course, see the `import pycuda.autoinit` line.
    We can access the current context object with `pycuda.autoinit.context`, and we
    can synchronize in our current context by calling the `pycuda.autoinit.context.synchronize()`
    function.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 查看头文件，我们当然会看到`import pycuda.autoinit`这一行。我们可以使用`pycuda.autoinit.context`访问当前上下文对象，并且我们可以通过调用`pycuda.autoinit.context.synchronize()`函数在我们的当前上下文中进行同步。
- en: 'Now let''s modify the `gpu_mandelbrot` function to handle explicit synchronization.
    The first GPU-related line we see is this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们修改`gpu_mandelbrot`函数以处理显式同步。我们看到的第一个与GPU相关的行是这一行：
- en: '`mandelbrot_lattice_gpu = gpuarray.to_gpu(mandelbrot_lattice)`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`mandelbrot_lattice_gpu = gpuarray.to_gpu(mandelbrot_lattice)`'
- en: 'We can now change this to be explicitly synchronized. We can copy to the GPU
    asynchronously with `to_gpu_async`, and then synchronize as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将这个改为显式同步。我们可以使用`to_gpu_async`异步地将数据复制到GPU，然后按照以下方式同步：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We then see the next line allocates memory on the GPU with the `gpuarray.empty`
    function. Memory allocation in CUDA is, by the nature of the GPU architecture,
    automatically synchronized; there is no *asynchronous* memory allocation equivalent
    here. Hence, we keep this line as it was before.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到下一行使用`gpuarray.empty`函数在GPU上分配内存。由于GPU架构的本质，CUDA中的内存分配总是自动同步的；这里没有*异步*内存分配的等效项。因此，我们保留这一行不变。
- en: Memory allocation in CUDA is always synchronized!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中进行内存分配始终是同步的！
- en: 'We now see the next two lines—our Mandelbrot kernel is launched with an invocation
    to `mandel_ker`, and we copy the contents of our Mandelbrot `gpuarray` object
    with an invocation to `get`. We synchronize after the kernel launch, switch `get`
    to `get_async`, and finally synchronize one last line:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在看到接下来的两行——我们的Mandelbrot内核通过调用`mandel_ker`来启动，我们通过调用`get`来复制我们的Mandelbrot
    `gpuarray`对象的内容。在内核启动后进行同步，将`get`切换到`get_async`，最后同步最后一行：
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We can now run this, and it will produce a Mandelbrot image to disk, exactly
    as in [Chapter 3](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml), *Getting Started
    with PyCUDA.*
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行这个程序，它将生成Mandelbrot图像到磁盘，就像在[第3章](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml)，“PyCUDA入门”中一样。
- en: (This example is also available as `gpu_mandelbrot_context_sync.py` in the repository.)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: （此示例也可在存储库中作为`gpu_mandelbrot_context_sync.py`文件找到。）
- en: Manual context creation
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动创建上下文
- en: So far, we have been importing `pycuda.autoinit` at the beginning of all of
    our PyCUDA programs; this effectively creates a context at the beginning of our
    program and has it destroyed at the end.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在所有PyCUDA程序的开始处导入`pycuda.autoinit`；这实际上在程序开始时创建了一个上下文，并在程序结束时销毁它。
- en: Let's try doing this manually. We will make a small program that just copies
    a small array to the GPU, copies it back to the host, prints the array, and exits.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试手动进行。我们将编写一个小程序，该程序仅将一个小数组复制到GPU，将其复制回主机，打印数组，然后退出。
- en: 'We start with the imports:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从导入开始：
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'First, we initialize CUDA with the `pycuda.driver.init` function, which is
    here aliased as `drv`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`pycuda.driver.init`函数初始化CUDA，这里将其别名为`drv`：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now we choose which GPU we wish to work with; this is necessary for the cases
    where one has more than one GPU. We can select a specific GPU with `pycuda.driver.Device`;
    if you only have one GPU, as I do, you can access it with `pycuda.driver.Device(0)`,
    as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们选择我们希望与之工作的GPU；在一个人拥有多个GPU的情况下，这是必要的。我们可以使用`pycuda.driver.Device`来选择一个特定的GPU；如果你只有一个GPU，就像我一样，你可以使用`pycuda.driver.Device(0)`来访问它，如下所示：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can now create a new context on this device with `make_context`, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`make_context`在这个设备上创建一个新的上下文，如下所示：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that we have a new context, this will automatically become the default
    context. Let''s copy an array into the GPU, copy it back to the host, and print
    it:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个新的上下文，这将成为默认上下文。让我们将一个数组复制到GPU上，再复制回主机，并打印它：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now we are done. We can destroy the context by calling the `pop` function:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了。我们可以通过调用 `pop` 函数来销毁上下文：
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: That's it! We should always remember to destroy contexts that we explicitly
    created with `pop` before our program exists.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们应该始终记住，在程序退出之前，使用 `pop` 显式创建的上下文应该被销毁。
- en: (This example can be seen in the `simple_context_create.py` file under this
    chapter's directory in the repository.)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: （此示例可在存储库中该章节目录下的 `simple_context_create.py` 文件中查看。）
- en: Host-side multiprocessing and multithreading
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主机端多进程和多线程
- en: Of course, we may seek to gain concurrency on the host side by using multiple
    processes or threads on the host's CPU. Let's make the distinction right now between
    a host-side operating system process and thread with a quick overview.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可能希望通过在主机CPU上使用多个进程或线程来获得主机端的并发性。让我们现在通过快速概述来区分主机端操作系统进程和线程。
- en: Every host-side program that exists outside the operating system kernel is executed
    as a process, and can also exist in multiple processes. A process has its own
    address space, as it runs concurrently with, and independently of, all other processes.
    A process is, generally speaking, blind to the actions of other processes, although
    multiple processes can communicate through sockets or pipes. In Linux and Unix,
    new processes are spawned with the fork system call.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 每个存在于操作系统内核之外的主机端程序都作为进程执行，也可以存在于多个进程中。进程有自己的地址空间，因为它与其他所有进程并发运行，独立于其他进程。一般来说，进程对其他进程的行为是盲目的，尽管多个进程可以通过套接字或管道进行通信。在Linux和Unix中，使用fork系统调用创建新进程。
- en: In contrast, a host-side thread exists within a single process, and multiple
    threads can also exist within a single process. Multiple threads in a single process
    run concurrently. All threads in the same process share the same address space
    within the process and have access to the same shared variables and data. Generally,
    resource locks are used for accessing data among multiple threads, so as to avoid
    race conditions. In compiled languages such as C, C++, or Fortran, multiple process
    threads are usually managed with the Pthreads or OpenMP APIs.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，主机端线程存在于单个进程内，单个进程内也可以存在多个线程。单个进程中的多个线程是并发运行的。同一进程中的所有线程共享进程内的同一地址空间，并可以访问相同的共享变量和数据。通常，使用资源锁来访问多个线程之间的数据，以避免竞态条件。在C、C++或Fortran等编译型语言中，通常使用Pthreads或OpenMP
    API来管理多个进程线程。
- en: Threads are much more lightweight than processes, and it is far faster for an
    operating system kernel to switch tasks between multiple threads in a single process,
    than to switch tasks between multiple processes. Normally, an operating system
    kernel will automatically execute different threads and processes on different
    CPU cores to establish true concurrency.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 线程比进程轻量得多，操作系统内核在单个进程中的多个线程之间切换任务比在多个进程之间切换任务要快得多。通常，操作系统内核会自动在不同的CPU核心上执行不同的线程和进程，以建立真正的并发。
- en: A peculiarity of Python is that while it supports multi-threading through the
    `threading` module, all threads will execute on the same CPU core. This is due
    to technicalities of Python being an interpreted scripting language, and is related
    to Python's Global Identifier Lock (GIL). To achieve true multi-core concurrency
    on the host through Python, we, unfortunately, must spawn multiple processes with
    the `multiprocessing` module. (Unfortunately, the multiprocessing module is currently
    not fully functional under Windows, due to how Windows handles processes. Windows
    users will sadly have to stick to single-core multithreading here if they want
    to have any form of host-side concurrency.)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Python的一个特性是，虽然它通过 `threading` 模块支持多线程，但所有线程都会在同一个CPU核心上执行。这是由于Python作为解释型脚本语言的技术细节，与Python的全局解释器锁（GIL）有关。要通过Python在主机上实现真正的多核并发，我们不幸地必须使用
    `multiprocessing` 模块来生成多个进程。（不幸的是，由于Windows处理进程的方式，multiprocessing模块目前在Windows上尚不完全可用。Windows用户如果想要在主机端有并发性，将不得不坚持使用单核多线程。）
- en: We will now see how to use both threads in Python to use GPU based operations;
    Linux users should note that this can be easily extended to processes by switching
    references of `threading` to `multiprocessing`, and references to `Thread` to
    `Process`, as both modules look and act similarly. By the nature of PyCUDA, however,
    we will have to create a new CUDA context for every thread or process that we
    will use that will make use of the GPU. Let's see how to do this right now.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到如何使用 Python 中的两个线程来使用基于 GPU 的操作；Linux 用户请注意，这可以通过将 `threading` 的引用切换到
    `multiprocessing`，并将 `Thread` 的引用切换到 `Process` 来轻松扩展到进程，因为这两个模块看起来和表现得很相似。然而，由于
    PyCUDA 的性质，我们必须为每个将使用 GPU 的线程或进程创建一个新的 CUDA 上下文。让我们看看如何立即做到这一点。
- en: Multiple contexts for host-side concurrency
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主机端并发的多个上下文
- en: 'Let''s first briefly review how to create a single host thread in Python that
    can return a value to the host with a simple example. (This example can also be
    seen in the `single_thread_example.py` file under `5` in the repository.) We will
    do this by using the `Thread` class in the `threading` module to create a subclass
    of `Thread`, as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们简要回顾一下如何在 Python 中创建一个可以返回值的单个主机线程的示例。 (此示例也可以在存储库中的 `5` 目录下的 `single_thread_example.py`
    文件中找到。) 我们将通过使用 `threading` 模块中的 `Thread` 类来创建 `Thread` 的子类来实现这一点，如下所示：
- en: '[PRE33]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We now set up our constructor. We call the parent class''s constructor and
    set up an empty variable within the object that will be the return value from
    the thread:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在设置构造函数。我们调用父类的构造函数，并在对象内部设置一个空变量，该变量将是线程的返回值：
- en: '[PRE34]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We now set up the run function within our thread class, which is what will
    be executed when the thread is launched. We''ll just have it print a line and
    set the return value:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在线程类中设置 `run` 函数，这是线程启动时将执行的内容。我们只需让它打印一行并设置返回值：
- en: '[PRE35]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We finally have to set up the join function. This will allow us to receive
    a return value from the thread:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终必须设置 `join` 函数。这将允许我们从线程接收返回值：
- en: '[PRE36]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now we are done setting up our thread class. Let''s start an instance of this
    class as the `NewThread` object, spawn the new thread by calling the `start` method,
    and then block execution and get the output from the host thread by calling `join`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了我们的线程类。让我们创建一个 `NewThread` 对象的实例，通过调用 `start` 方法来启动新线程，然后通过调用 `join`
    方法阻塞执行并从主机线程获取输出：
- en: '[PRE37]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now let''s run this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行这个：
- en: '![](img/9dc8f524-03ac-4f2c-a21c-8736c1feb1cf.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9dc8f524-03ac-4f2c-a21c-8736c1feb1cf.png)'
- en: Now, we can expand this idea among multiple concurrent threads on the host to
    launch concurrent CUDA operations by way of multiple contexts and threading. We
    will now look at one last example. Let's re-use the pointless multiply/divide
    kernel from the beginning of this chapter and launch it within each thread that
    we spawn.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在主机上的多个并发线程中扩展这个想法，通过多个上下文和线程来启动并发 CUDA 操作。我们现在将查看最后一个示例。让我们重用本章开头提到的无意义的乘除内核，并在我们创建的每个线程中启动它。
- en: 'First, let''s look at the imports. Since we are making explicit contexts, remember
    to remove `pycuda.autoinit` and add an import `threading` at the end:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看导入部分。由于我们正在创建显式上下文，请记住删除 `pycuda.autoinit` 并在最后添加一个 `threading` 的导入：
- en: '[PRE38]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We will use the same array size as before, but this time we will have a direct
    correspondence between the number of the threads and the number of the arrays.
    Generally, we don''t want to spawn more than 20 or so threads on the host, so
    we will only go for `10` arrays. So, consider now the following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与之前相同的数组大小，但这次我们将线程的数量与数组数量直接对应。通常，我们不想在主机上创建超过 20 个线程，所以我们只创建 `10` 个数组。现在，考虑以下代码：
- en: '[PRE39]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we will store our old kernel as a string object; since this can only be
    compiled within a context, we will have to compile this in each thread individually:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将我们的旧内核存储为一个字符串对象；由于这只能在上下文中编译，因此我们必须在每个线程中单独编译它：
- en: '[PRE40]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we can begin setting up our class. We will make another subclass of `threading.Thread`
    as before, and set up the constructor to take one parameter as the input array.
    We will initialize an output variable with `None`, as we did before:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始设置我们的类。我们将创建 `threading.Thread` 的另一个子类，就像之前一样，并将构造函数设置为接受一个输入数组作为参数。我们将使用
    `None` 初始化一个输出变量，就像之前做的那样：
- en: '[PRE41]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can now write the `run` function. We choose our device, create a context
    on that device, compile our kernel, and extract the kernel function reference.
    Notice the use of the `self` object:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以编写 `run` 函数。我们选择我们的设备，在该设备上创建一个上下文，编译我们的内核，并提取内核函数引用。注意 `self` 对象的使用：
- en: '[PRE42]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We now copy the array to the GPU, launch the kernel, and copy the output back
    to the host. We then destroy the context:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将数组复制到 GPU 上，启动内核，并将输出复制回主机。然后我们销毁上下文：
- en: '[PRE43]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, we set up the join function. This will return `output_array` to the
    host:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们设置了 `join` 函数。这将返回 `output_array` 到主机：
- en: '[PRE44]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We are now done with our subclass. We will set up some empty lists to hold
    our random test data, thread objects, and thread output values, similar to before.
    We will then generate some random arrays to process and set up a list of kernel
    launcher threads that will operate on each corresponding array:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在完成了我们的子类。我们将设置一些空列表来保存我们的随机测试数据、线程对象和线程输出值，类似于之前。然后我们将生成一些随机数组进行处理，并设置一个内核启动线程列表，这些线程将操作每个相应的数组：
- en: '[PRE45]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We will now launch each thread object, and extract its output into the `gpu_out`
    list by using `join`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将启动每个线程对象，并使用 `join` 将其输出提取到 `gpu_out` 列表中：
- en: '[PRE46]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, we just do a simple assert on the output arrays to ensure they are
    the same as the input:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们只是对输出数组进行简单的断言，以确保它们与输入相同：
- en: '[PRE47]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This example can be seen in the `multi-kernel_multi-thread.py` file in the repository.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子可以在存储库中的 `multi-kernel_multi-thread.py` 文件中看到。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We started this chapter by learning about device synchronization and the importance
    of synchronization of operations on the GPU from the host; this allows dependent
    operations to allow antecedent operations to finish before proceeding. This concept
    has been hidden from us, as PyCUDA has been handling synchronization for us automatically
    up to this point. We then learned about CUDA streams, which allow for independent
    sequences of operations to execute on the GPU simultaneously without synchronizing
    across the entire GPU, which can give us a big performance boost; we then learned
    about CUDA events, which allow us to time individual CUDA kernels within a given
    stream, and to determine if a particular operation in a stream has occurred. Next,
    we learned about contexts, which are analogous to processes in a host operating
    system. We learned how to synchronize across an entire CUDA context explicitly
    and then saw how to create and destroy contexts. Finally, we saw how we can generate
    multiple contexts on the GPU, to allow for GPU usage among multiple threads or
    processes on the host.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从学习设备同步以及从主机对 GPU 上操作同步的重要性开始本章；这允许依赖操作在继续之前完成先前的操作。这个概念一直被隐藏起来，因为 PyCUDA
    到目前为止一直在为我们自动处理同步。然后我们学习了 CUDA 流，它允许在 GPU 上独立执行操作序列，而无需在整个 GPU 上同步，这可以给我们带来很大的性能提升；然后我们学习了
    CUDA 事件，它允许我们在给定的流中计时单个 CUDA 内核，并确定流中的特定操作是否发生。接下来，我们学习了上下文，它在主机操作系统中的过程类似。我们学习了如何在整个
    CUDA 上下文中显式同步，然后看到了如何创建和销毁上下文。最后，我们看到了如何在 GPU 上生成多个上下文，以便在主机上的多个线程或进程之间共享 GPU
    使用。
- en: Questions
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: In the launch parameters for the kernel in the first example, our kernels were
    each launched over 64 threads. If we increase the number of threads to and beyond
    the number of cores in our GPU, how does this affect the performance of both the
    original to the stream version?
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个示例的内核启动参数中，我们的内核是针对 64 个线程启动的。如果我们增加线程数量到 GPU 核心数及以上，这会如何影响原始版本和流版本的性能？
- en: Consider the CUDA C example that was given at the very beginning of this chapter,
    which illustrated the use of `cudaDeviceSynchronize`. Do you think it is possible
    to get some level of concurrency among multiple kernels without using streams
    and only using `cudaDeviceSynchronize`?
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑一下本章开头给出的 CUDA C 示例，它说明了 `cudaDeviceSynchronize` 的使用。你认为在不使用流而仅使用 `cudaDeviceSynchronize`
    的情况下，是否可以在多个内核之间获得某种程度的并发性？
- en: If you are a Linux user, modify the last example that was given to operate over
    processes rather than threads.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你是一名 Linux 用户，请修改之前给出的最后一个示例，使其操作过程而不是线程：
- en: Consider the `multi-kernel_events.py` program; we said it is good that there
    was a low standard deviation of kernel execution durations. Why would it be bad
    if there were a high standard deviation?
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑一下 `multi-kernel_events.py` 程序；我们说内核执行持续时间的低标准偏差是好事。如果存在高标准偏差，那会是什么问题？
- en: We only used 10 host-side threads in the last example. Name two reasons why
    we have to use a relatively small number of threads or processes for launching
    concurrent GPU operations on the host.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一个例子中，我们只使用了10个主机端线程。列举两个原因说明为什么我们必须使用相对较少的线程或进程来在主机上启动并发GPU操作。
