- en: 'Chapter 11: Data Visualization with PySpark'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章：使用 PySpark 进行数据可视化
- en: So far, from [*Chapter 1*](B16736_01_Final_JM_ePub.xhtml#_idTextAnchor014)*,*
    *Distributed Computing Primer**,* through [*Chapter 9*](B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164),
    *Machine Learning Life Cycle Management*, you have learned how to ingest, integrate,
    and cleanse data, as well as how to make data conducive for analytics. You have
    also learned how to make use of clean data for practical business applications
    using data science and machine learning. This chapter will introduce you to the
    basics of deriving meaning out of data using data visualizations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，从 [*第一章*](B16736_01_Final_JM_ePub.xhtml#_idTextAnchor014)*,* **分布式计算基础**，到
    [*第9章*](B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164)，*机器学习生命周期管理*，你已经学习了如何获取、整合和清洗数据，以及如何使数据适合用于分析。你还学会了如何使用清洗后的数据，通过数据科学和机器学习来解决实际的业务应用。本章将向你介绍如何利用数据可视化从数据中提取意义的基本知识。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Importance of data visualization
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化的重要性
- en: Techniques for visualizing data using PySpark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PySpark 进行数据可视化的技巧
- en: Considerations for PySpark to pandas conversion
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 转换为 pandas 时的注意事项
- en: Data visualization is the process of graphically representing data using visual
    elements such as charts, graphs, and maps. Data visualization helps you understand
    patterns within data in a visual manner. In the big data world, with massive amounts
    of data, it is even more important to make use of data visualizations to derive
    meaning out of such data and present it in a simple and easy-to-understand way
    to business users; this helps them make data-driven decisions.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化是通过使用图表、图形和地图等视觉元素，将数据图形化呈现的过程。数据可视化帮助你以视觉化的方式理解数据中的模式。在大数据的世界中，面对海量的数据，使用数据可视化来从中提取意义并以简单易懂的方式呈现给业务用户变得尤为重要；这有助于他们做出基于数据的决策。
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will be using Databricks Community Edition to run our code.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 Databricks Community Edition 来运行代码。
- en: Databricks Community Edition can be accessed at [https://community.cloud.databricks.com](https://community.cloud.databricks.com).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks Community Edition 可以通过 [https://community.cloud.databricks.com](https://community.cloud.databricks.com)
    访问。
- en: Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册说明可以在 [https://databricks.com/try-databricks](https://databricks.com/try-databricks)
    找到。
- en: The code and data for this chapter can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter11](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter11).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的代码和数据可以从 [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter11](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter11)
    下载。
- en: Importance of data visualization
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可视化的重要性
- en: Data visualization is the process of translating data into a pictorial representation
    in the form of graphs, charts, or maps. This makes it easier for the human mind
    to comprehend complex information. Typically, data visualization is the final
    stage of business analytics and the first step of any data science process. Though
    there are professionals who deal solely with data visualizations, any data professional
    needs to be able to understand and produce data visualizations. They help convey
    complex patterns that are hidden within data in an easy-to-understand way to business
    users. Every business needs information for optimal performance, and data visualization
    helps businesses make easier data-driven decisions by representing relationships
    between datasets in a visual way and surfacing actionable insights. With the advent
    of big data, there has been an explosion of both structured and unstructured data,
    and it is difficult to make sense of it without the help of visual aids. Data
    visualization helps in accelerating the decision-making process by surfacing key
    business information and helps business users act on those insights quickly. Data
    visualization also aids the storytelling process by helping convey the right message
    to the right audience.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化是将数据转化为图形、图表或地图等形式的过程。这使得人类大脑更容易理解复杂的信息。通常，数据可视化是商业分析的最后阶段，也是任何数据科学过程的第一步。虽然有些专业人员专门从事数据可视化，但任何数据专业人员都需要能够理解和制作数据可视化。数据可视化帮助以易于理解的方式向业务用户传达隐藏在数据中的复杂模式。每个企业都需要信息以实现最佳性能，而数据可视化通过以视觉方式展示数据集之间的关系，并揭示可操作的见解，帮助企业做出更简单的数据驱动决策。随着大数据的到来，结构化和非结构化数据爆炸性增长，若没有可视化工具的帮助，很难理解这些数据。数据可视化通过揭示关键信息，加速决策过程，帮助业务用户迅速采取行动。数据可视化还帮助讲故事的过程，通过向正确的受众传递正确信息来传达信息。
- en: A data visualization can be a simple graph representing a single aspect of the
    current state of the business, a complex sales report, or a dashboard that gives
    a holistic view of an organization's performance. Data visualization tools are
    key to unlocking the power of data visualizations. We will explore the different
    types of data visualization tools that are available in the following section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化可以是一个简单的图表，代表当前业务状态的某一方面，也可以是一个复杂的销售报告，或是一个展示组织整体表现的仪表盘。数据可视化工具是解锁数据可视化潜力的关键。我们将在接下来的部分探讨不同类型的数据可视化工具。
- en: Types of data visualization tools
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据可视化工具类型
- en: Data visualization tools provide us with an easier way to create data visualizations.
    They allow data analysts and data scientists to create data visualizations conveniently
    by providing a graphical user interface, database connections, and sometimes data
    manipulation tools in a single, unified interface. There are different types of
    data visualizations tools, and each serves a slightly different purpose. We will
    explore them in this section.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化工具为我们提供了一个更简单的方式来创建数据可视化。它们通过提供图形用户界面、数据库连接和有时的数据处理工具，在单一统一的界面中，方便数据分析师和数据科学家创建数据可视化。有不同类型的数据可视化工具，每种工具都有略微不同的用途。在这一部分，我们将深入探讨这些工具。
- en: Business intelligence tools
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 商业智能工具
- en: '**Business intelligence** **(BI)** tools are typically enterprise-grade tools
    that help organizations track and visually represent their **Key Performance Indicators**
    (**KPIs**). BI tools typically include provisions for creating complex logical
    data models and contain data cleansing and integration mechanisms. BI tools also
    include connectors to a myriad of data sources and built-in data visualizations
    with drag-and-drop features to help business users quickly create data visualizations,
    operational and performance dashboards, and scorecards to track the performance
    of an individual department or the entire organization. The primary users of BI
    tools are business analysts and business executives involved in making tactical
    and strategic decisions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**商业智能** (**BI**) 工具通常是企业级工具，帮助组织追踪和直观呈现其**关键绩效指标** (**KPIs**)。BI工具通常包括用于创建复杂逻辑数据模型的功能，并包含数据清洗和集成功能。BI工具还包括连接到各种数据源的连接器，并内置了拖放功能的数据可视化，帮助业务用户快速创建数据可视化、运营和绩效仪表盘以及计分卡，以追踪某一部门或整个组织的表现。BI工具的主要用户是参与制定战术和战略决策的业务分析师和高管。'
- en: BI tools traditionally use data warehouses as their data sources, but modern
    BI tools support RDMS, NoSQL databases, and data lakes as data sources. Some examples
    of prominent BI tools include **Tableau**, **Looker**, **Microsoft Power BI**,
    **SAP Business Objects**, **MicroStrategy**, **IBM Cognos**, and **Qlikview**,
    to name a few. BI tools can connect to Apache Spark and consume data stored in
    Spark SQL tables using an ODBC connection. These concepts will be explored in
    detail in [*Chapter 13*](B16736_13_Final_JM_ePub.xhtml#_idTextAnchor214), *Integrating
    External Tools with Spark SQL*. A class of data visualization tools with all the
    necessary data visualization and data connectivity components, minus any data
    processing capabilities such as Redash, also exist and they can also connect to
    Apache Spark via an ODBC connection.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: BI 工具传统上使用数据仓库作为数据源，但现代 BI 工具支持 RDMS、NoSQL 数据库和数据湖作为数据源。一些著名的 BI 工具包括 **Tableau**、**Looker**、**Microsoft
    Power BI**、**SAP Business Objects**、**MicroStrategy**、**IBM Cognos** 和 **Qlikview**
    等。BI 工具可以连接到 Apache Spark，并通过 ODBC 连接消费存储在 Spark SQL 表中的数据。这些概念将在 [*第13章*](B16736_13_Final_JM_ePub.xhtml#_idTextAnchor214)，*与
    Spark SQL 集成外部工具* 中详细探讨。一类没有任何数据处理能力，但具备所有必要的数据可视化和数据连接组件的数据可视化工具，如 Redash，也可以通过
    ODBC 连接到 Apache Spark。
- en: Observability tools
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可观察性工具
- en: Observability is the process of constantly monitoring and understanding what's
    happening in highly distributed systems. Observability helps us understand what
    is slow or broken, as well as what needs to be fixed to improve performance. However,
    since modern cloud environments are dynamic and constantly increasing in scale
    and complexity, most problems are neither known nor monitored. Observability addresses
    common issues with modern cloud environments that are dynamic and ever-increasing
    in scale by enabling you to continuously monitor and surface any issues that might
    arise. Observability tools help businesses continuously monitor systems and applications
    and enable a business to receive actionable insights into system behavior, as
    well as predict outages or problems before they occur. Data visualization is an
    important component of observability tools; a few popular examples include Grafana
    and Kibana.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性是一个持续监控和理解高度分布式系统中发生的事情的过程。可观察性帮助我们理解哪些地方变慢或出现故障，以及哪些方面需要修复以提高性能。然而，由于现代云环境是动态的，并且不断扩展和复杂化，大多数问题既不被知晓，也没有被监控。可观察性通过使你能够持续监控和暴露可能出现的问题，解决了现代云环境中常见的问题，这些问题具有动态性并且规模不断扩大。可观察性工具帮助企业持续监控系统和应用程序，并使企业能够获得关于系统行为的可操作见解，提前预测停机或问题的发生。数据可视化是可观察性工具的重要组成部分；一些流行的示例包括
    Grafana 和 Kibana。
- en: Data teams are typically not responsible for monitoring and maintaining the
    health of data processing systems – this is usually handled by specialists such
    as **DevOps** engineers. Apache Spark doesn't have any direct integrations with
    any observability tools out of the box, but it can be integrated with popular
    observability platforms such as **Prometheus** and **Grafana**. Apache Spark's
    integration with observability tools is outside the scope of this book, so we
    won't discuss this here.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据团队通常不负责监控和维护数据处理系统的健康状况——这通常由 **DevOps** 工程师等专业人员来处理。Apache Spark 默认没有与任何可观察性工具的直接集成，但它可以与流行的可观察性平台如
    **Prometheus** 和 **Grafana** 集成。Apache Spark 与可观察性工具的集成超出了本书的范围，因此我们在此不做讨论。
- en: Notebooks
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Notebooks
- en: Notebooks are interactive computing tools that are used to execute code, visualize
    results, and share insights. Notebooks are indispensable tools in the data science
    process and are becoming prominent in the entire data analytics development life
    cycle, as you have witnessed throughout this book. Notebooks are also excellent
    data visualization tools as they help you convert your Python or SQL code into
    easy-to-understand interactive data visualizations. Some notebooks, such as Databricks,
    Jupyter, and Zeppelin notebooks can also be used as standalone dashboards. The
    remainder of this chapter will focus on how notebooks can be used as data visualization
    tools when using PySpark.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Notebooks 是交互式计算工具，用于执行代码、可视化结果并分享见解。Notebooks 是数据科学过程中不可或缺的工具，并且在整个数据分析开发生命周期中变得越来越重要，正如你在本书中所见。Notebooks
    也是优秀的数据可视化工具，它们帮助你将 Python 或 SQL 代码转化为易于理解的交互式数据可视化。一些 notebooks，如 Databricks、Jupyter
    和 Zeppelin notebooks，甚至可以作为独立的仪表盘使用。本章剩余部分将重点介绍如何在使用 PySpark 时将 notebooks 用作数据可视化工具。
- en: Techniques for visualizing data using PySpark
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PySpark 可视化数据的技术
- en: Apache Spark is a unified data processing engine and doesn't come out of the
    box with a graphical user interface, per se. As discussed in the previous sections,
    data that's been processed by Apache Spark can be stored in data warehouses and
    visualized using BI tools or natively visualized using notebooks. In this section,
    we will focus on how to leverage notebooks to interactively process and visualize
    data using PySpark. As we have done throughout this book, we will be making use
    of notebooks that come with **Databricks Community Edition**, though **Jupyter**
    and **Zeppelin** notebooks can also be used.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个统一的数据处理引擎，默认并不带有图形用户界面。正如前面部分所讨论的，经过 Apache Spark 处理的数据可以存储在数据仓库中，并使用
    BI 工具进行可视化，或者使用笔记本进行本地可视化。在本节中，我们将重点介绍如何利用笔记本以交互方式使用 PySpark 处理和可视化数据。正如我们在本书中所做的那样，我们将使用
    **Databricks Community Edition** 提供的笔记本，尽管 **Jupyter** 和 **Zeppelin** 笔记本也可以使用。
- en: PySpark native data visualizations
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySpark 本地数据可视化
- en: There aren't any data visualization libraries that can work with PySpark DataFrames
    natively. However, the notebook implementations of cloud-based Spark distributions
    such as Databricks and Qubole support natively visualizing Spark DataFrames using
    the built-in `display()` function. Let's see how we can use the `display()` function
    to visualize PySpark DataFrames in Databricks Community Edition.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何数据可视化库可以原生地与 PySpark DataFrame 一起使用。然而，基于云的 Spark 分发版的笔记本实现，如 Databricks
    和 Qubole，支持使用内置的 `display()` 函数原生可视化 Spark DataFrame。让我们看看如何在 Databricks Community
    Edition 中使用 `display()` 函数可视化 PySpark DataFrame。
- en: 'We will use the cleansed, integrated, and wrangled dataset that we produced
    toward the end of [*Chapter 6*](B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107),
    *Feature Engineering – Extraction, Transformation, and Selection*, here, as shown
    in the following code snippet:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们在 [*第 6 章*](B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107) 结束时制作的已清洗、集成和整理的数据集，*特征工程
    – 提取、转换和选择*，如下所示的代码片段：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the previous code snippet, we read a table into a Spark DataFrame and selected
    the columns that we intend to visualize. Then, we called the `display()` method
    on the Spark DataFrame. The result is a grid display in the notebook, as shown
    in the following screenshot:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们将一个表格读入了 Spark DataFrame，并选择了我们打算可视化的列。接着，我们在 Spark DataFrame 上调用了
    `display()` 方法。结果是在笔记本中显示的一个网格，如下图所示：
- en: '![Figure 11.1 – The grid widget'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.1 – 网格小部件'
- en: '](img/B16736_11_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_01.jpg)'
- en: Figure 11.1 – The grid widget
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 网格小部件
- en: The previous screenshot shows the result of calling the `display()` function
    on a Spark DataFrame within a Databricks notebook. This way, any Spark DataFrame
    can be visualized in a tabular format within Databricks notebooks. The tabular
    grid supports sorting arbitrary columns. Databricks notebooks also support charts
    and graphs that can be used from within the notebooks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 上一截图显示了在 Databricks 笔记本中调用 `display()` 函数在 Spark DataFrame 上的结果。通过这种方式，任何 Spark
    DataFrame 都可以在 Databricks 笔记本中以表格格式进行可视化。该表格网格支持对任意列进行排序。Databricks 笔记本还支持图表和图形，这些可以在笔记本内使用。
- en: Tip
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Databricks's `display()` method supports all of Spark's programming APIs, including
    Python, Scala, R, and SQL. In addition, the `display()` method can also render
    Python pandas DataFrames.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 的 `display()` 方法支持 Spark 所有编程 API，包括 Python、Scala、R 和 SQL。此外，`display()`
    方法还可以渲染 Python pandas DataFrame。
- en: 'We can use the same grid display and convert it into a graph by clicking on
    the graph icon and choosing the desired graph from the list, as shown in the following
    screenshot:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的网格显示，并通过点击图表图标并从列表中选择所需的图表，将其转换为图表，如下图所示：
- en: '![Figure 11.2 – Graph options'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.2 – 图表选项'
- en: '](img/B16736_11_02.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_02.jpg)'
- en: Figure 11.2 – Graph options
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 图表选项
- en: 'As we can see, the graph menu has multiple chart options, with the bar chart
    being the first on the list. If you choose the bar chart, plot options can be
    used to configure the chart''s key, value, and series grouping options. Similarly,
    we can use a line graph or a pie chart, as shown here:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，图表菜单有多个图表选项，柱状图排在列表的首位。如果选择柱状图，图表选项可以用来配置图表的关键字段、值字段和系列分组选项。类似地，我们也可以使用折线图或饼图，如下所示：
- en: '![Figure 11.3 – Pie chart'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.3 – 饼图'
- en: '](img/B16736_11_03.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_03.jpg)'
- en: Figure 11.3 – Pie chart
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 饼图
- en: 'Here, the `display()` function can be used to display various kinds of charts
    within the notebook and help configure various graph options. Databricks notebooks
    also support a rudimentary map widget that can visualize metrics on a world map,
    as illustrated in the following screenshot:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`display()` 函数可以用于在笔记本中显示各种图表，并帮助配置各种图形选项。Databricks 笔记本还支持一个基础的地图小部件，可以在世界地图上可视化指标，如以下截图所示：
- en: '![Figure 11.4 – World map'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.4 – 世界地图'
- en: '](img/B16736_11_04.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_04.jpg)'
- en: Figure 11.4 – World map
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – 世界地图
- en: The previous screenshot shows metrics on a world map. Since our dataset only
    contains a few European countries, France and the UK have been shaded in on the
    map widget.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图展示了世界地图上的指标。由于我们的数据集只包含少数几个欧洲国家，法国和英国在地图小部件中已经被高亮显示。
- en: Note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For this widget, the values should be either country codes in ISO 3166-1 alpha-3
    format ("GBR") or US state abbreviations ("TX").
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个小部件，值应为 ISO 3166-1 alpha-3 格式的国家代码（例如 "GBR"）或美国州的缩写（例如 "TX"）。
- en: 'In addition to basic bars and charts, Databricks notebooks also support scientific
    visualizations such as **scatter plots**, **histograms**, **quantile plots**,
    and **Q-Q** plots, as illustrated in the following figure:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基本的条形图和图表外，Databricks 笔记本还支持科学可视化，如**散点图**、**直方图**、**分位数图**和**Q-Q 图**，如下图所示：
- en: '![Figure 11.5 – Quantile plot'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.5 – 分位数图'
- en: '](img/B16736_11_05.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_05.jpg)'
- en: Figure 11.5 – Quantile plot
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 分位数图
- en: A quantile plot, as illustrated in the previous figure, helps determine whether
    two datasets have a common distribution. Quantile plots are available in Databricks
    notebooks via the graph menu, and plot properties such as keys, values, and series
    groupings are available via the plot options menu.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面图中所示，分位数图帮助判断两个数据集是否具有共同的分布。Databricks 笔记本可以通过图表菜单访问分位数图，图表属性如键、值和系列分组可以通过图表选项菜单进行设置。
- en: 'We can use the following code to make Databricks notebooks display images:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码使 Databricks 笔记本显示图像：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The previous code snippet uses Apache Spark''s built-in image data source to
    load images from a directory on persistent storage such as a data lake:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段使用 Apache Spark 内置的图像数据源从持久存储（如数据湖）中的目录加载图像：
- en: '![Figure 11.6 – Image data'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.6 – 图像数据'
- en: '](img/B16736_11_06.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_06.jpg)'
- en: Figure 11.6 – Image data
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – 图像数据
- en: This image is rendered in a notebook using Databricks's `display()` function
    as it is capable of displaying image previews.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图像通过 Databricks 的 `display()` 函数在笔记本中渲染显示，因为它能够显示图像预览。
- en: 'Databricks notebooks are also capable of rendering machine learning-specific
    visualizations such as `display()` function to visualize a **decision tree** model
    that we had trained, as illustrated in the following screenshot:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 笔记本还能够渲染机器学习特定的可视化内容，例如使用 `display()` 函数可视化我们训练的**决策树**模型，如下图所示：
- en: '![Figure 11.7 – Decision tree model'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.7 – 决策树模型'
- en: '](img/B16736_11_07.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_07.jpg)'
- en: Figure 11.7 – Decision tree model
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 – 决策树模型
- en: The previous screenshot shows the decision tree model that we built using Spark
    ML, rendered in a Databricks notebook.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图展示了我们使用 Spark ML 构建的决策树模型，并在 Databricks 笔记本中渲染显示。
- en: Tip
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'More information on rendering machine learning-specific visualizations using
    Databricks notebooks can be found in Databricks''s public documentation here:
    [https://docs.databricks.com/notebooks/visualizations/index.html#machine-learning-visualizations](https://docs.databricks.com/notebooks/visualizations/index.html#machine-learning-visualizations).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于使用 Databricks 笔记本渲染机器学习特定可视化的信息，请参阅 Databricks 的公开文档：[https://docs.databricks.com/notebooks/visualizations/index.html#machine-learning-visualizations](https://docs.databricks.com/notebooks/visualizations/index.html#machine-learning-visualizations)。
- en: Interactive visuals using JavaScript and HTML
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 JavaScript 和 HTML 的交互式可视化
- en: 'Databricks notebooks also support `displayHTML()` function. You can pass any
    arbitrary HTML code to `displayHTML()` and have it rendered in a notebook, as
    shown in the following code snippet:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 笔记本还支持 `displayHTML()` 函数。你可以将任意 HTML 代码传递给 `displayHTML()`，并将其渲染在笔记本中，如以下代码片段所示：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding code snippet displays an arbitrary HTML hyperlink in a notebook.
    Other HTML elements such as paragraphs, headings, images, and more can also be
    used with the `displayHTML()` function.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段在笔记本中显示了一个任意的 HTML 超链接。其他 HTML 元素，如段落、标题、图片等，也可以与 `displayHTML()` 函数一起使用。
- en: Tip
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: HTML blocks such as hyperlinks, images, and tables can be used to make your
    notebooks more descriptive and interactive for end users and can aid in the storytelling
    process.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用超链接、图像和表格等HTML块来使你的笔记本更具描述性和交互性，这些可以帮助讲述过程中的交互性。
- en: 'Similarly, SVG graphics can also be rendered using the `displayHTML()` function,
    as shown in the following code block:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，可以使用`displayHTML()`函数来渲染SVG图形，如下代码块所示：
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code renders an orange-colored, animated ellipse that fades in
    and out. Far more complex SVG graphics can also be rendered and data from a Spark
    DataFrame can be passed along. Similarly, the popular HTML and JavaScript-based
    visualization library can also be leveraged with Databricks notebooks, as shown
    here:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码渲染了一个橙色的动画椭圆，可以淡入淡出。还可以渲染更复杂的SVG图形，并且可以传递来自Spark DataFrame的数据。同样，流行的基于HTML和JavaScript的可视化库也可以与Databricks笔记本一起使用，如下所示：
- en: '![Figure 11.8 – Word cloud using D3.js'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.8 – 使用D3.js的词云'
- en: '](img/B16736_11_08.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_08.jpg)'
- en: Figure 11.8 – Word cloud using D3.js
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8 – 使用D3.js的词云
- en: Here, we have taken the `description` column from the `retail_sales` Delta table
    that we created during our data processing steps in the previous chapters, and
    then we extracted individual words from the item description column. Then, we
    rendered the words using a word cloud visualization by using HTML, CSS, and JavaScript.
    After, we used the popular D3.js JavaScript library to manipulate the HTML documents
    based on data. The code for this visualization can be found at [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/Chapter11/databricks-charts-graphs.py](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/Chapter11/databricks-charts-graphs.py).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从我们在前几章节中处理数据时创建的`retail_sales` Delta表中取出了`description`列，然后从商品描述列中提取了各个单词。接着，我们利用HTML、CSS和JavaScript使用词云可视化渲染了这些单词。之后，我们使用流行的D3.js
    JavaScript库基于数据操作HTML文档。这个可视化的代码可以在[https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/Chapter11/databricks-charts-graphs.py](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/Chapter11/databricks-charts-graphs.py)找到。
- en: So far, you have seen some of the basic and statistical graphs that are available
    via the Databricks notebook interface, which can work natively with Spark DataFrames.
    However, sometimes, you may need some additional graphs and charts that aren't
    available within the notebook, or you may need more control over your graph. In
    these instances, popular visualization libraries for Python such as `matplotlib`,
    `plotly`, `seaborn`, `altair`, `bokeh`, and so on can be used with PySpark. We
    will explore some of these visualization libraries in the next section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到了一些通过Databricks笔记本界面可以直接与Spark DataFrame一起使用的基本和统计图表。然而，有时您可能需要一些在笔记本中不可用的额外图表和图形，或者您可能需要更多对图表的控制。在这些情况下，可以使用诸如`matplotlib`、`plotly`、`seaborn`、`altair`、`bokeh`等流行的Python可视化库与PySpark一起使用。我们将在下一节中探讨一些这些可视化库。
- en: Using Python data visualizations with PySpark
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PySpark进行Python数据可视化
- en: As you learned in the previous section, PySpark doesn't inherently have any
    visualization capabilities, but you can choose to use Databricks notebook capabilities
    to visualize data in Spark DataFrames. In situations where using Databricks notebooks
    is not possible, you can use popular Python-based visualizations libraries to
    visualize your data using any notebook interface that you are comfortable with.
    In this section, we will explore some prominent Python visualization libraries
    and how to use them for data visualization in Databricks notebooks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前一节中学到的那样，PySpark本身并不具备任何可视化能力，但您可以选择使用Databricks笔记本功能来在Spark DataFrame中可视化数据。在无法使用Databricks笔记本的情况下，您可以选择使用流行的基于Python的可视化库，在任何您熟悉的笔记本界面中进行数据可视化。在本节中，我们将探讨一些著名的Python可视化库以及如何在Databricks笔记本中使用它们进行数据可视化。
- en: Creating two-dimensional plots using Matplotlib
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Matplotlib创建二维图
- en: '`PyPI` repository using a package manager such as `pip`. The following code
    example shows how Matplotlib can be used with PySpark:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像`pip`这样的包管理器在`PyPI`仓库中获取。以下代码示例展示了如何在PySpark中使用Matplotlib：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the previous code snippet, we did the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们执行了以下操作：
- en: First, we imported the `pandas` and `matplotlib` libraries, assuming they are
    already installed in the notebook.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入了`pandas`和`matplotlib`库，假设它们已经在笔记本中安装好了。
- en: Then, we generated a Spark DataFrame with the required columns using the online
    retail dataset that we have created during the data processing steps in the previous
    chapters.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用在前几章的数据处理步骤中创建的在线零售数据集生成了一个包含所需列的 Spark DataFrame。
- en: Since Python-based visualization libraries cannot directly use Spark DataFrames,
    we converted the Spark DataFrame into a pandas DataFrame.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于基于 Python 的可视化库不能直接使用 Spark DataFrame，我们将 Spark DataFrame 转换为 pandas DataFrame。
- en: Then, we converted the quantity column into a numeric data type so that we could
    plot it.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们将数量列转换为数值数据类型，以便进行绘制。
- en: After that, we defined a plot on the pandas DataFrame using the `plot()` method
    of the Matplotlib library, specified the type of plot to be generated as a bar
    graph, and passed the x-axis and y-axis column names.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们使用 Matplotlib 库的`plot()`方法在 pandas DataFrame 上定义了一个图表，指定生成的图表类型为柱状图，并传入了
    x 轴和 y 轴的列名。
- en: Some notebook environments may require you to explicitly call the `display()`
    function for the plot to be displayed.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 某些笔记本环境可能需要显式调用 `display()` 函数才能显示图表。
- en: 'This way, Matplotlib can be used with any Spark DataFrame if we convert it
    into a pandas DataFrame. The plot that was generated looks as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，如果我们将 Spark DataFrame 转换为 pandas DataFrame，就可以使用 Matplotlib。生成的图表如下所示：
- en: '![Figure 11.9 – Matplotlib visualization'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.9 – Matplotlib 可视化'
- en: '](img/B16736_11_09.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_09.jpg)'
- en: Figure 11.9 – Matplotlib visualization
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 – Matplotlib 可视化
- en: The previous graph depicts the number of items that have been sold over a certain
    period.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个图表显示了在某一特定时间段内已售出的商品数量。
- en: Scientific visualizations using Seaborn
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Seaborn 进行科学可视化
- en: '`pip`. The following code sample shows how Seaborn can be used with PySpark
    DataFrames:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip`。以下代码示例展示了如何在 PySpark DataFrame 中使用 Seaborn：'
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the previous code snippet, we did the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个代码片段中，我们执行了以下操作：
- en: First, we imported the `matplotlib` and `seaborn` libraries.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入了 `matplotlib` 和 `seaborn` 库。
- en: Next, we converted the Spark DataFrame, which contains a single column called
    `unit_price`, into a pandas DataFrame using the `toPandas()` PySpark function.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将包含名为 `unit_price` 的单列的 Spark DataFrame 使用 `toPandas()` PySpark 函数转换为
    pandas DataFrame。
- en: Then, we defined our plot dimensions using the `plot.figure()` Matplotlib method.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们使用 `plot.figure()` Matplotlib 方法定义了图表的尺寸。
- en: 'Finally, we plotted a boxplot by invoking the `seaborn.boxplot()` method and
    passing the pandas DataFrame with a single column. The resultant plot is shown
    in the following screenshot:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过调用 `seaborn.boxplot()` 方法并传入包含单列的 pandas DataFrame 绘制了箱线图。生成的图表如下所示：
- en: '![Figure 11.10 – Seaborn boxplot visualization'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.10 – Seaborn 箱线图可视化'
- en: '](img/B16736_11_10.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_10.jpg)'
- en: Figure 11.10 – Seaborn boxplot visualization
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 – Seaborn 箱线图可视化
- en: The previous screenshot depicts how the **unit_price** column can be distributed
    as a box plot using its minimum, first quartile, median, third quartile, and maximum
    values.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个截图显示了如何使用 **unit_price** 列的最小值、第一个四分位数、中位数、第三个四分位数和最大值将其分布为箱线图。
- en: Interactive visualizations using Plotly
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Plotly 进行交互式可视化
- en: '**Plotly** is a JavaScript-based visualization library that enables Python
    users to create interactive web-based visualizations that can be displayed in
    notebooks or saved to standalone HTML files. Plotly comes pre-installed with Databricks
    and can be used like so:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**Plotly** 是一个基于 JavaScript 的可视化库，使得 Python 用户能够创建交互式的网页可视化，并可以在笔记本中展示或保存为独立的
    HTML 文件。Plotly 已预装在 Databricks 中，可以按以下方式使用：'
- en: '[PRE6]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the previous code snippet, we did the following actions:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个代码片段中，我们执行了以下操作：
- en: First, we imported the `matplotlib` and `seaborn` libraries.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入了 `matplotlib` 和 `seaborn` 库。
- en: Next, we converted the Spark DataFrame, along with the required columns, into
    a pandas DataFrame.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将包含所需列的 Spark DataFrame 转换为 pandas DataFrame。
- en: Then, we defined the Plotly plot parameters using the `plot.scatter()` method.
    This method configures a scatter plot with three dimensions.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用 `plot.scatter()` 方法定义了 Plotly 图表的参数。该方法配置了一个具有三维坐标的散点图。
- en: 'Finally, we rendered the plot using the `fig.show()` method. The resultant
    plot is shown in the following screenshot:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用 `fig.show()` 方法渲染了图表。生成的图表如下所示：
- en: '![Figure 11.11 – Plotly bubble chart visualization'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.11 – Plotly 气泡图可视化'
- en: '](img/B16736_11_11.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_11.jpg)'
- en: Figure 11.11 – Plotly bubble chart visualization
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11 – Plotly 气泡图可视化
- en: The preceding screenshot shows a bubble graph that depicts three metrics along
    three dimensions. The plot is interactive, and information is provided when you
    hover your mouse over various parts of the graph.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图展示了一个气泡图，显示了三个指标在三个维度上的分布。该图是互动式的，当您将鼠标悬停在图表的不同部分时，会显示相关信息。
- en: Declarative visualizations using Altair
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Altair的声明性可视化
- en: '**Altair** is a declarative statistical visualization library for Python. Altair
    is based on an open source, declarative grammar engine called **Vega**. Altair
    also offers a concise visualization grammar that enables users to build a wide
    range of visualizations quickly. It can be installed using the following command:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**Altair** 是一个用于Python的声明性统计可视化库。Altair基于一个名为**Vega**的开源声明性语法引擎。Altair还提供了一种简洁的可视化语法，使用户能够快速构建各种各样的可视化图表。可以使用以下命令安装它：'
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The previous command installs Altair in the notebook''s local Python kernel
    and restarts it. Once Altair has been successfully installed, it can be invoked
    using the usual Python `import` statements, as shown in the following code sample:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的命令将Altair安装到笔记本的本地Python内核中，并重新启动它。一旦Altair成功安装后，可以像通常那样使用Python的`import`语句来调用它，代码示例如下：
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the previous code snippet, we imported the Altair and pandas libraries.
    Then, we selected the required columns from the Spark table and convert them into
    a pandas DataFrame. Once we have data in Python in a pandas DataFrame, Altair
    can be used to create a plot, as shown here:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们导入了Altair和pandas库。然后，我们从Spark表中选择所需的列，并将其转换为pandas DataFrame。一旦数据在Python中以pandas
    DataFrame的形式存在，就可以使用Altair来创建图表，如下所示：
- en: '![Figure 11.12 – Altair isotype visualization'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.12 – Altair等型图可视化'
- en: '](img/B16736_11_12.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_12.jpg)'
- en: Figure 11.12 – Altair isotype visualization
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12 – Altair等型图可视化
- en: The preceding figure depicts an isotype visualization that shows the distribution
    of occupations by gender, across countries. Other open source libraries such as
    `bokeh`, `pygal`, and `leather` can also be used to visualize PySpark DataFrames.
    Bokeh is another popular data visualization library in Python that provides high-performance
    interactive charts and plots. Bokeh is based on JavaScript and HTML and unlike
    Matplotlib, it lets users create custom visualizations. Information on using Bokeh
    in Databricks notebooks can be found in Databricks's public documentation at [https://docs.databricks.com/notebooks/visualizations/bokeh.html#bokeh](https://docs.databricks.com/notebooks/visualizations/bokeh.html#bokeh).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图展示了一个等型图可视化，显示了不同国家按性别分布的职业情况。其他开源库，如`bokeh`、`pygal`和`leather`，也可以用于可视化PySpark
    DataFrame。Bokeh是另一个流行的Python数据可视化库，提供高性能的互动图表和图形。Bokeh基于JavaScript和HTML，与Matplotlib不同，它允许用户创建自定义可视化图表。关于在Databricks笔记本中使用Bokeh的信息，可以在Databricks的公共文档中找到，网址为[https://docs.databricks.com/notebooks/visualizations/bokeh.html#bokeh](https://docs.databricks.com/notebooks/visualizations/bokeh.html#bokeh)。
- en: So far, you have learned how to use some of the popular visualizations that
    are available for Python with Spark DataFrames by converting PySpark DataFrames
    into pandas DataFrames. However, there are some performance considerations and
    limitations you must consider when converting PySpark DataFrames into pandas DataFrames.
    We will look at these in the next section.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学习了如何通过将PySpark DataFrame转换为pandas DataFrame，使用一些在Python中与Spark DataFrame兼容的流行可视化方法。然而，在将PySpark
    DataFrame转换为pandas DataFrame时，您需要考虑一些性能问题和限制。我们将在下一部分讨论这些问题。
- en: Considerations for PySpark to pandas conversion
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark到pandas转换的注意事项
- en: This section will introduce **pandas**, demonstrate the differences between
    pandas and PySpark, and the considerations that need to be kept in mind while
    converting datasets between PySpark and pandas.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍**pandas**，演示pandas与PySpark的差异，并介绍在PySpark与pandas之间转换数据集时需要注意的事项。
- en: Introduction to pandas
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: pandas简介
- en: '**pandas** is one of the most widely used open source data analysis libraries
    for Python. It contains a diverse set of utilities for processing, manipulating,
    cleaning, munging, and wrangling data. pandas is much easier to work with than
    Pythons lists, dictionaries, and loops. In some ways, pandas is like other statistical
    data analysis tools such as R or SPSS, which makes it very popular with data science
    and machine learning enthusiasts.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**pandas** 是 Python 中最广泛使用的开源数据分析库之一。它包含了一系列用于处理、操作、清洗、整理和转换数据的实用工具。与 Python
    的列表、字典和循环相比，pandas 更加易于使用。从某种意义上讲，pandas 类似于其他统计数据分析工具，如 R 或 SPSS，这使得它在数据科学和机器学习爱好者中非常受欢迎。'
- en: The primary abstractions of pandas are **Series** and **DataFrames**, with the
    former essentially being a one-dimensional array and the latter a two-dimensional
    array. One of the fundamental differences between pandas and PySpark is that pandas
    represents its datasets as one- and two-dimensional **NumPy** arrays, while PySpark
    DataFrames are collections of **Row** and **Column** objects, based on Spark SQL.
    While pandas DataFrames can only be manipulated using pandas DSL, PySpark DataFrames
    can be manipulated using Spark's DataFrame DSL, as well as SQL. Owing to this
    difference, developers familiar with manipulating pandas might find PySpark to
    be different and may face a learning curve when working with the platform. The
    Apache Spark community realized this difficulty and launched a new open source
    project called Koalas. Koalas implements a pandas-like API on top of Spark DataFrames
    to try and overcome the previously mentioned difference between pandas and PySpark.
    More information on using Koalas will be presented in [*Chapter 10*](B16736_10_Final_JM_ePub.xhtml#_idTextAnchor176),
    *Scaling Out Single-Node Machine Learning Using PySpark*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 的主要抽象是**Series**和**DataFrames**，前者本质上是一个一维数组，后者是一个二维数组。pandas 和 PySpark
    之间的基本区别之一在于，pandas 将其数据集表示为一维和二维的**NumPy**数组，而 PySpark 的 DataFrames 是基于 Spark
    SQL 的**Row**和**Column**对象的集合。虽然 pandas DataFrames 只能通过 pandas DSL 进行操作，但 PySpark
    DataFrames 可以使用 Spark 的 DataFrame DSL 和 SQL 进行操作。由于这个差异，熟悉使用 pandas 的开发者可能会发现
    PySpark 不同，并且在使用该平台时可能会遇到学习曲线。Apache Spark 社区意识到这一难题，启动了一个新的开源项目——Koalas。Koalas
    在 Spark DataFrames 上实现了类似 pandas 的 API，以尝试克服 pandas 和 PySpark 之间的差异。关于如何使用 Koalas
    的更多信息将在[*第 10 章*](B16736_10_Final_JM_ePub.xhtml#_idTextAnchor176)，*使用 PySpark
    扩展单节点机器学习*中介绍。
- en: Note
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'NumPy is a Python package for scientific computing that provides a multi-dimensional
    array and a set of routines for fast operations on arrays. More information about
    NumPy can be found here: [https://numpy.org/doc/stable/user/whatisnumpy.html](https://numpy.org/doc/stable/user/whatisnumpy.html).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 是一个用于科学计算的 Python 包，它提供了多维数组和一组用于快速操作数组的例程。有关 NumPy 的更多信息，请访问：[https://numpy.org/doc/stable/user/whatisnumpy.html](https://numpy.org/doc/stable/user/whatisnumpy.html)。
- en: The other fundamental difference, in the context of big data and processing
    massive amounts of data at big data scale, is that pandas was designed to process
    data on a single machine and PySpark, by design, is distributed and can process
    data on multiple machines in a massively parallel manner. This brings up an important
    limitation of pandas compared to PySpark, as well as some important considerations
    that the developer needs to keep in mind while converting from pandas into PySpark.
    We will look at these in the following section.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个基本区别是在大数据和处理大规模数据的上下文中，pandas 是为了处理单台机器上的数据而设计的，而 PySpark 从设计上就是分布式的，可以以大规模并行的方式在多台机器上处理数据。这突显了
    pandas 与 PySpark 相比的一个重要限制，以及开发者在从 pandas 转换到 PySpark 时需要考虑的一些关键因素。我们将在接下来的章节中探讨这些内容。
- en: Converting from PySpark into pandas
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 PySpark 转换到 pandas
- en: 'The PySpark API comes with a handy utility function called `DataFrame.toPandas()`
    that converts PySpark DataFrames into pandas DataFrames. This function has been
    demonstrated throughout this chapter. If you recall our discussions from [*Chapter
    1*](B16736_01_Final_JM_ePub.xhtml#_idTextAnchor014),*Distributed Computing Primer*,
    especially the *Spark''s cluster architecture* section, a Spark cluster consists
    of a **Driver** process and a set of executor processes on worker machines, with
    the Driver being responsible for compiling user code, passing it on to the workers,
    managing and communicating with the workers, and if required, aggregating and
    collecting data from the workers. The Spark workers are responsible for all the
    data processing tasks. However, pandas is not based on the distributed computing
    paradigm and works solely on a single computing machine. Thus, when you execute
    pandas code on a Spark cluster, it executes on the Driver or the Master node,
    as depicted in the following diagram:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark API 提供了一个方便的实用函数 `DataFrame.toPandas()`，可以将 PySpark DataFrame 转换为 pandas
    DataFrame。该函数在本章中有多次演示。如果你回顾一下我们在[*第一章*](B16736_01_Final_JM_ePub.xhtml#_idTextAnchor014)，“*分布式计算入门*”中的讨论，尤其是关于
    *Spark 集群架构* 部分，Spark 集群由 **Driver** 进程和一组在工作节点上运行的执行进程组成，Driver 负责编译用户代码，将其传递给工作节点，管理并与工作节点通信，并在需要时从工作节点聚合和收集数据。而
    Spark 工作节点则负责所有数据处理任务。然而，pandas 并非基于分布式计算范式，它仅在单一计算机上运行。因此，当你在 Spark 集群上执行 pandas
    代码时，它会在 Driver 或 Master 节点上执行，如下图所示：
- en: '![Figure 11.13 – PySpark architecture'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.13 – PySpark 架构'
- en: '](img/B16736_11_13.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_11_13.jpg)'
- en: Figure 11.13 – PySpark architecture
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13 – PySpark 架构
- en: As we can see, Python and `toPandas()` function is called on a Spark DataFrame,
    it collects rows from all the Spark workers and then creates a pandas DataFrame
    locally on the Driver inside the Python kernel.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，当在 Spark DataFrame 上调用 Python 和 `toPandas()` 函数时，它会从所有 Spark 工作节点收集行数据，然后在
    Driver 内部的 Python 内核上创建一个 pandas DataFrame。
- en: The first issue with this is that `toPandas()` practically collects all the
    data from the workers and brings it back to the Driver. This may cause the Driver
    to run out of memory if the dataset being collected is too large. Another issue
    with this process is that by default, the **Row** objects of the Spark DataFrame
    are collected on the Driver as a **list** of **tuples**, and then converted to
    a pandas DataFrame. This ends up using a large amount of memory and sometimes
    even data that's twice the size of the Spark DataFrame being collected.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程面临的第一个问题是 `toPandas()` 函数实际上会将所有数据从工作节点收集并带回 Driver。如果收集的数据集过大，这可能会导致 Driver
    内存不足。另一个问题是，默认情况下，Spark DataFrame 的 **Row** 对象会作为 **list** 的 **tuples** 收集到 Driver
    上，然后再转换为 pandas DataFrame。这通常会消耗大量内存，有时甚至会导致收集到的数据占用的内存是 Spark DataFrame 本身的两倍。
- en: To mitigate some of the memory issues during PySpark to pandas conversion, Apache
    Arrow can be used. Apache Arrow is an in-memory, columnar data format that is
    similar to Spark's internal representation of datasets and is efficient at transferring
    data between the JVM and Python processes. Apache Arrow is not enabled by default
    in Spark and needs to be enabled by setting the `spark.sql.execution.arrow.enabled`
    Spark configuration to `true`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻 PySpark 转换为 pandas 时的内存问题，可以使用 Apache Arrow。Apache Arrow 是一种内存中的列式数据格式，类似于
    Spark 内部数据集的表示方式，且在 JVM 和 Python 进程之间传输数据时非常高效。默认情况下，Spark 没有启用 Apache Arrow，需要通过将
    `spark.sql.execution.arrow.enabled` Spark 配置设置为 `true` 来启用它。
- en: Note
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: PyArrow, the Python binding of Apache Arrow, is pre-installed on Databricks
    Runtime. However, you might need to install a version of PyArrow that's appropriate
    for the Spark and Python versions of your Spark cluster.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: PyArrow，Apache Arrow 的 Python 绑定，已在 Databricks Runtime 中预装。然而，你可能需要安装适用于你的 Spark
    集群和 Python 版本的 PyArrow 版本。
- en: Apache Arrow helps mitigate some of the memory issues that might arise from
    using `toPandas()`. Despite this optimization, the conversion operation still
    results in all records in the Spark DataFrame being collected by the Driver, so
    you should only perform the conversion on a small subset of the original data.
    Thus, by making use of the PyArrow format and taking care to sample down datasets,
    you can still use all the open source visualizations that are compatible with
    standard Python to visualize your PySpark DataFrames in a notebook environment.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Arrow 有助于缓解使用 `toPandas()` 时可能出现的一些内存问题。尽管进行了优化，但转换操作仍然会导致 Spark DataFrame
    中的所有记录被收集到 Driver 中，因此你应该仅在原始数据的一个小子集上执行转换。因此，通过利用 PyArrow 格式并小心地对数据集进行抽样，你仍然可以在笔记本环境中使用所有与标准
    Python 兼容的开源可视化库来可视化你的 PySpark DataFrame。
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about the importance of using data visualization
    to convey meaning from complex datasets in a simple way, as well as to easily
    surface patterns among data to business users. Various strategies for visualizing
    data with Spark were introduced. You also learned how to use data visualizations
    with PySpark natively using Databricks notebooks. We also looked at techniques
    for using plain Python visualization libraries to visualize data with Spark DataFrames.
    A few of the prominent open source visualization libraries, such as Matplotlib,
    Seaborn, Plotly, and Altair, were introduced, along with practical examples of
    their usage and code samples. Finally, you learned about the pitfalls of using
    plain Python visualizations with PySpark, the need for PySpark conversion, and
    some strategies to overcome these issues.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了使用数据可视化以简单的方式传达复杂数据集中的含义的重要性，以及如何轻松地将数据模式呈现给业务用户。介绍了多种使用 Spark 进行数据可视化的策略。你还学习了如何在
    Databricks 笔记本中原生使用 PySpark 进行数据可视化。我们还探讨了使用普通 Python 可视化库来可视化 Spark DataFrame
    数据的技巧。介绍了一些知名的开源可视化库，如 Matplotlib、Seaborn、Plotly 和 Altair，并提供了它们的实际使用示例和代码示例。最后，你了解了在
    PySpark 中使用普通 Python 可视化时的陷阱，PySpark 转换的需求，以及克服这些问题的一些策略。
- en: The next chapter will cover the topic of connecting various BI and SQL analysis
    tools to Spark, which will help you perform ad hoc data analysis and build complex
    operational and performance dashboards.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论如何将各种 BI 和 SQL 分析工具连接到 Spark，这将帮助你执行临时数据分析并构建复杂的操作和性能仪表板。
