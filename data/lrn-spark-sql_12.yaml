- en: Spark SQL in Large-Scale Application Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL在大规模应用程序架构中的应用
- en: In this book, we started with the basics of Spark SQL and its components, and
    its role in Spark applications. Later, we presented a series of chapters focusing
    on its usage in various types of applications. With DataFrame/Dataset API and
    the Catalyst optimizer at the heart of Spark SQL, it is no surprise that it plays
    a key role in all applications based on the Spark technology stack. These applications
    include large-scale machine learning, large-scale graphs, and deep learning applications.
    Additionally, we presented Spark SQL-based Structured Streaming applications that
    operate in complex environments as continuous applications. In this chapter, we
    will explore application architectures that leverage Spark modules and Spark SQL
    in real-world applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们从Spark SQL及其组件的基础知识开始，以及它在Spark应用程序中的作用。随后，我们提出了一系列关于其在各种类型应用程序中的使用的章节。作为Spark
    SQL的核心，DataFrame/Dataset API和Catalyst优化器在所有基于Spark技术栈的应用程序中发挥关键作用，这并不奇怪。这些应用程序包括大规模机器学习、大规模图形和深度学习应用程序。此外，我们提出了基于Spark
    SQL的结构化流应用程序，这些应用程序作为连续应用程序在复杂环境中运行。在本章中，我们将探讨在现实世界应用程序中利用Spark模块和Spark SQL的应用程序架构。
- en: More specifically, we will cover key architectural components and patterns in
    large-scale applications that architects and designers will find useful as a starting
    point for their specific use-cases. We will describe the deployment of some of
    the main processing models being used for batch processing, streaming applications,
    and machine learning pipelines. The underlying architecture for these processing
    models is required to support ingesting very large volumes of various types of
    data arriving at high velocities at one end, while making the output data available
    for use by analytical tools, and reporting and modeling software at the other
    end. Additionally, we will present supporting code using Spark SQL for monitoring,
    troubleshooting, and gathering/reporting metrics.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，我们将涵盖大规模应用程序中的关键架构组件和模式，这些对架构师和设计师来说将作为特定用例的起点。我们将描述一些用于批处理、流处理应用程序和机器学习管道的主要处理模型的部署。这些处理模型的基础架构需要支持在一端到达高速的各种类型数据的大量数据，同时在另一端使输出数据可供分析工具、报告和建模软件使用。此外，我们将使用Spark
    SQL提供支持代码，用于监控、故障排除和收集/报告指标。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中涵盖以下主题：
- en: Understanding Spark-based batch and stream processing architectures
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基于Spark的批处理和流处理架构
- en: Understanding Lambda and Kappa Architectures
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Lambda和Kappa架构
- en: Implementing scalable stream processing with structured streaming
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用结构化流实现可扩展的流处理
- en: Building robust **Extract-Transform-Load** (**ETL**) pipelines using Spark SQL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark SQL构建强大的ETL管道
- en: Implementing a scalable monitoring solution using Spark SQL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark SQL实现可扩展的监控解决方案
- en: Deploying Spark machine learning pipelines
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署Spark机器学习管道
- en: 'Using cluster managers: Mesos and Kubernetes'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集群管理器：Mesos和Kubernetes
- en: Understanding Spark-based application architectures
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解基于Spark的应用程序架构
- en: 'Apache Spark is an emerging platform that leverages distributed storage and
    processing frameworks to support querying, reporting, analytics, and intelligent
    applications at scale. Spark SQL has the necessary features, and supports the
    key mechanisms required, to access data across a diverse set of data sources and
    formats, and prepare it for downstream applications either with low-latency streaming
    data or high-throughput historical data stores. The following figure shows a high-level
    architecture that incorporates these requirements in typical Spark-based batch
    and streaming applications:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个新兴的平台，利用分布式存储和处理框架来支持规模化的查询、报告、分析和智能应用。Spark SQL具有必要的功能，并支持所需的关键机制，以访问各种数据源和格式的数据，并为下游应用程序做准备，无论是低延迟的流数据还是高吞吐量的历史数据存储。下图显示了典型的基于Spark的批处理和流处理应用程序中包含这些要求的高级架构：
- en: '![](img/00307.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00307.jpeg)'
- en: 'Additionally, as organizations start employing big data and NoSQL-based solutions
    across a number of projects, a data layer comprising RDBMSes alone is no longer
    considered the best fit for all the use-cases in a modern enterprise application.
    RDBMS-only based architectures illustrated in the following figure are rapidly
    disappearing across the industry, in order to meet the requirements of typical
    big-data applications:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着组织开始在许多项目中采用大数据和NoSQL解决方案，仅由RDBMS组成的数据层不再被认为是现代企业应用程序所有用例的最佳选择。仅基于RDBMS的架构在下图所示的行业中迅速消失，以满足典型大数据应用程序的要求：
- en: '![](img/00308.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00308.jpeg)'
- en: 'A more typical scenario comprising of multiple types of data store is shown
    in the next figure. Applications today use several types of data store that represent
    the best fit for a given set of use cases. Using multiple data storage technologies,
    chosen based on the way, data is being used by applications, is called **polyglot
    persistence**. Spark SQL is an excellent enabler of this and other similar persistence
    strategies in the cloud or on-premise deployments:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个更典型的场景，其中包含多种类型的数据存储。如今的应用程序使用多种数据存储类型，这些类型最适合特定的用例。根据应用程序使用数据的方式选择多种数据存储技术，称为多语言持久性。Spark
    SQL在云端或本地部署中是这种和其他类似持久性策略的极好的实现者：
- en: '![](img/00309.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00309.jpeg)'
- en: 'Additionally, we observe that only a small fraction of real-world ML systems
    are composed of ML code (the smallest box in the following figure). However, the
    infrastructure surrounding this ML code is vast and complex. Later in this chapter,
    we will use Spark SQL to create some of the key parts in such applications, including
    scalable ETL pipelines and monitoring solutions. Subsequently, we will also discuss
    the production deployment of machine learning pipelines, and the use of cluster
    managers such as Mesos and Kubernetes:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们观察到，现实世界中只有一小部分ML系统由ML代码组成（下图中最小的方框）。然而，围绕这些ML代码的基础设施是庞大且复杂的。在本章的后面，我们将使用Spark
    SQL来创建这些应用程序中的一些关键部分，包括可扩展的ETL管道和监控解决方案。随后，我们还将讨论机器学习管道的生产部署，以及使用Mesos和Kubernetes等集群管理器：
- en: '![](img/00310.gif)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00310.gif)'
- en: 'Reference: "Hidden Technical Debt in Machine Learning Systems," Google NIPS
    2015'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参考：“机器学习系统中的隐藏技术债务”，Google NIPS 2015
- en: In the next section, we will discuss the key concepts and challenges in Spark-based
    batch and stream processing architectures.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论基于Spark的批处理和流处理架构中的关键概念和挑战。
- en: Using Apache Spark for batch processing
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行批处理
- en: Typically, batch processing is done on huge volumes of data to create batch
    views in order to support ad hoc querying and MIS reporting functionality, and/or
    to apply scalable machine learning algorithms for classification, clustering,
    collaborative filtering, and analytics applications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，批处理是针对大量数据进行的，以创建批量视图，以支持特定查询和MIS报告功能，和/或应用可扩展的机器学习算法，如分类、聚类、协同过滤和分析应用。
- en: Due to the data volume involved in batch processing, these applications are
    typically long-running jobs and can easily extend over hours, days, or weeks,
    for example, aggregation queries such as count of daily visitors to a page, unique
    visitors to a website, and total sales per week.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于批处理涉及的数据量较大，这些应用通常是长时间运行的作业，并且很容易延长到几个小时、几天或几周，例如，聚合查询，如每日访问者数量、网站的独立访问者和每周总销售额。
- en: Increasingly, Apache Spark is becoming popular as the engine for large-scale
    data processing. It can run programs up to 100x faster than Hadoop MapReduce in
    memory, or 10x faster on disk. An important reason for the rapid adoption of Spark
    is the common/similar coding required to address both batch and stream processing
    requirements.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的人开始将Apache Spark作为大规模数据处理的引擎。它可以在内存中运行程序，比Hadoop MapReduce快100倍，或者在磁盘上快10倍。Spark被迅速采用的一个重要原因是，它需要相似的编码来满足批处理和流处理的需求。
- en: In the next section, we will introduce the key characteristics and concepts
    of stream processing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍流处理的关键特征和概念。
- en: Using Apache Spark for stream processing
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行流处理
- en: Most modern businesses are trying to deal with high data volumes (and associated
    rapid and unbounded growth of such data), combined with low-latency processing
    requirements. Additionally, higher value is being associated with near real-time
    business insights derived from real-time streaming data than traditional batch
    processed MIS reports. In contrast to streaming systems, the traditional batch
    processing systems were designed to process large amounts of a set of bounded
    data. Such systems are provided with all the data they need at the beginning of
    the execution. As the input data grows continuously, the results provided by such
    batch systems become dated, quickly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代企业都在努力处理大量数据（以及相关数据的快速和无限增长），同时还需要低延迟的处理需求。此外，与传统的批处理MIS报告相比，从实时流数据中获得的近实时业务洞察力被赋予了更高的价值。与流处理系统相反，传统的批处理系统旨在处理一组有界数据的大量数据。这些系统在执行开始时就提供了它们所需的所有数据。随着输入数据的不断增长，这些批处理系统提供的结果很快就会过时。
- en: Typically, in stream processing, data is not collected over significant time
    periods before triggering the required processing. Commonly, the incoming data
    is moved to a queuing system, such as Apache Kafka or Amazon Kinesis. This data
    is then accessed by the stream processor, which executes certain computations
    on it to generate the resulting output. A typical stream processing pipeline creates
    incremental views, which are often updated based on the incremental data flowing
    into the system.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在流处理中，数据在触发所需处理之前不会在显著的时间段内收集。通常，传入的数据被移动到排队系统，例如Apache Kafka或Amazon Kinesis。然后，流处理器访问这些数据，并对其执行某些计算以生成结果输出。典型的流处理管道创建增量视图，这些视图通常根据流入系统的增量数据进行更新。
- en: 'The incremental views are made available through a **Serving Layer** to support
    querying and real-time analytics requirements, as shown in the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 增量视图通过**Serving Layer**提供，以支持查询和实时分析需求，如下图所示：
- en: '![](img/00311.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00311.jpeg)'
- en: 'There are two types of time that are important in stream processing systems:
    event time and processing time. Event time is the time at which the events actually
    occurred (at source), while processing time is the time when the events are observed
    in the processing system. Event time is typically embedded in the data itself,
    and for many use cases, it is the time you want to operate on. However, extracting
    event time from data, and handling late or out-of-order data can present significant
    challenges in streaming applications. Additionally, there is a skew between the
    event times and the processing times due to resource limitations, the distributed
    processing model, and so on. There are many use cases requiring aggregations by
    event time; for example, the number of system errors in one-hour windows.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在流处理系统中有两种重要的时间类型：事件时间和处理时间。事件时间是事件实际发生的时间（在源头），而处理时间是事件在处理系统中被观察到的时间。事件时间通常嵌入在数据本身中，对于许多用例来说，这是您想要操作的时间。然而，从数据中提取事件时间，并处理延迟或乱序数据在流处理应用程序中可能会带来重大挑战。此外，由于资源限制、分布式处理模型等原因，事件时间和处理时间之间存在偏差。有许多用例需要按事件时间进行聚合；例如，在一个小时的窗口中系统错误的数量。
- en: There can be other issues as well; for example, in windowing functionality,
    we need to determine whether all the data for a given event time has been observed
    yet. These systems need to be designed in a manner that allow them to function
    well in uncertain environments. For example, in Spark Structured Streaming, event-time,
    window-based aggregation queries can be defined consistently for a data stream
    because it can handle late arriving data, and update older aggregates appropriately.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 还可能存在其他问题；例如，在窗口功能中，我们需要确定是否已观察到给定事件时间的所有数据。这些系统需要设计成能够在不确定的环境中良好运行。例如，在Spark结构化流处理中，可以为数据流一致地定义基于事件时间的窗口聚合查询，因为它可以处理延迟到达的数据，并适当更新旧的聚合。
- en: Fault tolerance is crucial when dealing with large data streaming applications,
    for example, a stream processing job that keeps a count of all the tuples it has
    seen so far. Here, each tuple may represent a stream of user activity, and the
    application may want to report the total activity seen so far. A node failure
    in such a system can result in an inaccurate count because of the unprocessed
    tuples (on the failed node).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大数据流应用程序时，容错性至关重要，例如，一个流处理作业可以统计到目前为止看到的所有元组的数量。在这里，每个元组可能代表用户活动的流，应用程序可能希望报告到目前为止看到的总活动。在这样的系统中，节点故障可能导致计数不准确，因为有未处理的元组（在失败的节点上）。
- en: A naive way to recover from this situation would be to replay the entire Dataset.
    This is a costly operation given the size of data involved. Checkpointing is a
    common technique used to avoid reprocessing the entire Dataset. In the case of
    failures, the application data state is reverted to the last checkpoint, and the
    tuples from that point on, are replayed. To prevent data loss in Spark Streaming
    applications, a **write-ahead log** (**WAL**) is used, from which data can be
    replayed after failures.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从这种情况中恢复的一个天真的方法是重新播放整个数据集。考虑到涉及的数据规模，这是一个昂贵的操作。检查点是一种常用的技术，用于避免重新处理整个数据集。在发生故障的情况下，应用程序数据状态将恢复到最后一个检查点，并且从那一点开始重新播放元组。为了防止Spark
    Streaming应用程序中的数据丢失，使用了**预写式日志**（**WAL**），在故障后可以从中重新播放数据。
- en: In the next section, we will introduce the Lambda architecture, which is a popular
    pattern implemented in Spark-centric applications, as it can address requirements
    of both, batch and stream processing, using very similar code.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍Lambda架构，这是在Spark中心应用程序中实施的一种流行模式，因为它可以使用非常相似的代码满足批处理和流处理的要求。
- en: Understanding the Lambda architecture
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Lambda架构
- en: 'The Lambda architectural pattern attempts to combine the best of both worlds--batch
    processing and stream processing. This pattern consists of several layers: **Batch
    Layer** (ingests and processes data on persistent storage such as HDFS and S3),
    **Speed Layer** (ingests and processes streaming data that has not been processed
    by the **Batch Layer** yet), and the **Serving Layer** (combines outputs from
    the **Batch** and **Speed Layers** to present merged results). This is a very
    popular architecture in Spark environments because it can support both the **Batch**
    and **Speed Layer** implementations with minimal code differences between the
    two.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda架构模式试图结合批处理和流处理的优点。该模式由几个层组成：**批处理层**（在持久存储上摄取和处理数据，如HDFS和S3），**速度层**（摄取和处理尚未被**批处理层**处理的流数据），以及**服务层**（将**批处理**和**速度层**的输出合并以呈现合并结果）。这是Spark环境中非常流行的架构，因为它可以支持**批处理**和**速度层**的实现，两者之间的代码差异很小。
- en: 'The given figure depicts the Lambda architecture as a combination of batch
    processing and stream processing:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的图表描述了Lambda架构作为批处理和流处理的组合：
- en: '![](img/00312.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00312.jpeg)'
- en: 'The next figure shows an implementation of the Lambda architecture using AWS
    Cloud services (**Amazon Kinesis**, **Amazon S3** Storage, **Amazon EMR**, **Amazon
    DynamoDB**, and so on) and Spark:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了使用AWS云服务（**Amazon Kinesis**，**Amazon S3**存储，**Amazon EMR**，**Amazon DynamoDB**等）和Spark实现Lambda架构：
- en: '![](img/00313.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00313.jpeg)'
- en: For more details on the AWS implementation of Lambda architecture, refer to [https://d0.awsstatic.com/whitepapers/lambda-architecure-on-for-batch-aws.pdf](https://d0.awsstatic.com/whitepapers/lambda-architecure-on-for-batch-aws.pdf).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有关AWS实施Lambda架构的更多详细信息，请参阅[https://d0.awsstatic.com/whitepapers/lambda-architecure-on-for-batch-aws.pdf](https://d0.awsstatic.com/whitepapers/lambda-architecure-on-for-batch-aws.pdf)。
- en: In the next section, we will discuss a simpler architecture called Kappa Architecture,
    which dispenses with the **Batch Layer** entirely and works with stream processing
    in the **Speed Layer** only.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论一个更简单的架构，称为Kappa架构，它完全放弃了**批处理层**，只在**速度层**中进行流处理。
- en: Understanding the Kappa Architecture
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Kappa架构
- en: The **Kappa Architecture** is simpler than the Lambda pattern as it comprises
    the Speed and Serving Layers only. All the computations occur as stream processing
    and there are no batch re-computations done on the full Dataset. Recomputations
    are only done to support changes and new requirements.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kappa架构**比Lambda模式更简单，因为它只包括速度层和服务层。所有计算都作为流处理进行，不会对完整数据集进行批量重新计算。重新计算仅用于支持更改和新需求。'
- en: 'Typically, the incoming real-time data stream is processed in memory and is
    persisted in a database or HDFS to support queries, as illustrated in the following
    figure:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，传入的实时数据流在内存中进行处理，并持久化在数据库或HDFS中以支持查询，如下图所示：
- en: '![](img/00314.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00314.jpeg)'
- en: The Kappa Architecture can be realized by using Apache Spark combined with a
    queuing solution, such as Apache Kafka. If the data retention times are bound
    to several days to weeks, then Kafka could also be used to retain the data for
    the limited period of time.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Kappa架构可以通过使用Apache Spark结合排队解决方案（如Apache Kafka）来实现。如果数据保留时间限制在几天到几周，那么Kafka也可以用来保留数据一段有限的时间。
- en: In the next few sections, we will introduce a few hands-on exercises using Apache
    Spark, Scala, and Apache Kafka that are very useful in the real-world applications
    development context. We will start by using Spark SQL and Structured Streaming
    to implement a few streaming use cases.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将介绍一些使用Apache Spark、Scala和Apache Kafka的实际应用开发环境中非常有用的实践练习。我们将首先使用Spark
    SQL和结构化流来实现一些流式使用案例。
- en: Design considerations for building scalable stream processing applications
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建可扩展流处理应用的设计考虑
- en: 'Building robust stream processing applications is challenging. The typical
    complexities associated with stream processing include the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 构建健壮的流处理应用是具有挑战性的。与流处理相关的典型复杂性包括以下内容：
- en: '**Complex Data**: Diverse data formats and the quality of data create significant
    challenges in streaming applications. Typically, the data is available in various
    formats, such as JSON, CSV, AVRO, and binary. Additionally, dirty data, or late
    arriving, and out-of-order data, can make the design of such applications extremely
    complex.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂数据**：多样化的数据格式和数据质量在流应用中带来了重大挑战。通常，数据以各种格式可用，如JSON、CSV、AVRO和二进制。此外，脏数据、延迟到达和乱序数据会使这类应用的设计变得极其复杂。'
- en: '**Complex workloads**: Streaming applications need to support a diverse set
    of application requirements, including interactive queries, machine learning pipelines,
    and so on.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂工作负载**：流应用需要支持多样化的应用需求，包括交互式查询、机器学习流水线等。'
- en: '**Complex systems**: With diverse storage systems, including Kafka, S3, Kinesis,
    and so on, system failures can lead to significant reprocessing or bad results.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂系统**：具有包括Kafka、S3、Kinesis等多样化存储系统，系统故障可能导致重大的重新处理或错误结果。'
- en: Steam processing using Spark SQL can be fast, scalable, and fault-tolerant.
    It provides an extensive set of high-level APIs to deal with complex data and
    workloads. For example, the data sources API can integrate with many storage systems
    and data formats.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark SQL进行流处理可以快速、可扩展和容错。它提供了一套高级API来处理复杂数据和工作负载。例如，数据源API可以与许多存储系统和数据格式集成。
- en: For a detailed coverage of building scalable and fault-tolerant structured streaming applications,
    refer to [https://spark-summit.org/2017/events/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark/](https://spark-summit.org/2017/events/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark/).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有关构建可扩展和容错的结构化流处理应用的详细覆盖范围，请参阅[https://spark-summit.org/2017/events/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark/](https://spark-summit.org/2017/events/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark/)。
- en: A streaming query allows us to specify one or more data sources, transform the
    data using DataFrame/Dataset APIs or SQL, and specify various sinks to output
    the results. There is built-in support for several data sources, such as files,
    Kafka, and sockets, and we can also combine multiple data sources, if required.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 流查询允许我们指定一个或多个数据源，使用DataFrame/Dataset API或SQL转换数据，并指定各种接收器来输出结果。内置支持多种数据源，如文件、Kafka和套接字，如果需要，还可以组合多个数据源。
- en: The Spark SQL Catalyst optimizer figures out the mechanics of incrementally
    executing the transformations. The query is converted to a series of incremental
    execution plans that operate on the new batches of data. The sink accepts the
    output of each batch and the updates are completed within a transaction context.
    You can also specify various output modes (**Complete**, **Update**, or **Append**)
    and triggers to govern when to output the results. If no trigger is specified,
    the results are continuously updated. The progress of a given query, and restarts
    after failures are managed by persisting checkpoints.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL Catalyst优化器可以找出增量执行转换的机制。查询被转换为一系列对新数据批次进行操作的增量执行计划。接收器接受每个批次的输出，并在事务上下文中完成更新。您还可以指定各种输出模式（**完整**、**更新**或**追加**）和触发器来控制何时输出结果。如果未指定触发器，则结果将持续更新。通过持久化检查点来管理给定查询的进度和故障后的重启。
- en: Spark structured streaming enables streaming analytics without having to worry
    about the complex underlying mechanisms that make streaming work. In this model,
    the input can be thought of as data from an append-only table (that grows continuously).
    A trigger specifies the time interval for checking the input for the arrival of
    new data and the query represents operations such as map, filter, and reduce on
    the input. The result represents the final table that is updated in each trigger
    interval (as per the specified query operation).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的数据格式
- en: For detailed illustrations on structured streaming internals, check out [http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有关结构化流内部的详细说明，请查看[http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)。
- en: We can also execute sliding window operations on streaming data. Here, we define
    aggregations over a sliding window, in which we group the data and compute appropriate
    aggregations (for each group).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Spark结构化流使得流式分析变得简单，无需担心使流式工作的复杂底层机制。在这个模型中，输入可以被视为来自一个不断增长的追加表的数据。触发器指定了检查输入是否到达新数据的时间间隔，查询表示对输入进行的操作，如映射、过滤和减少。结果表示在每个触发间隔中更新的最终表（根据指定的查询操作）。
- en: In the next section, we will discuss Spark SQL features that can help in building
    robust ETL pipelines.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论Spark SQL功能，这些功能可以帮助构建强大的ETL管道。
- en: Building robust ETL pipelines using Spark SQL
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL构建强大的ETL管道
- en: ETL pipelines execute a series of transformations on source data to produce
    cleansed, structured, and ready-for-use output by subsequent processing components. The
    transformations required to be applied on the source will depend on nature of
    the data. The input or source data can be structured (RDBMS, Parquet, and so on),
    semi-structured (CSV, JSON, and so on) or unstructured data (text, audio, video,
    and so on).  After being processed through such pipelines, the data is ready for
    downstream data processing, modeling, analytics, reporting, and so on.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ETL管道在源数据上执行一系列转换，以生成经过清洗、结构化并准备好供后续处理组件使用的输出。需要应用在源数据上的转换将取决于数据的性质。输入或源数据可以是结构化的（关系型数据库，Parquet等），半结构化的（CSV，JSON等）或非结构化数据（文本，音频，视频等）。通过这样的管道处理后，数据就可以用于下游数据处理、建模、分析、报告等。
- en: 'The following figure illustrates an application architecture in which the input
    data from Kafka, and other sources such as application and server logs, are cleansed
    and transformed (using an ETL pipeline) before being stored in an enterprise data
    store. This data store can eventually feed other applications (via Kafka), support
    interactive queries, store subsets or views of the data in serving databases,
    train ML models, support reporting applications, and so on:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了一个应用架构，其中来自Kafka和其他来源（如应用程序和服务器日志）的输入数据在存储到企业数据存储之前经过清洗和转换（使用ETL管道）。这个数据存储最终可以供其他应用程序使用（通过Kafka），支持交互式查询，将数据的子集或视图存储在服务数据库中，训练ML模型，支持报告应用程序等。
- en: '![](img/00315.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: 在下一节中，我们将介绍一些标准，可以帮助您选择适当的数据格式，以满足特定用例的要求。
- en: As the abbreviation (ETL) suggests, we need to retrieve the data from various
    sources (Extract), transform the data for downstream consumption (Transform),
    and transmit it to different destinations (Load).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如缩写（ETL）所示，我们需要从各种来源检索数据（提取），转换数据以供下游使用（转换），并将其传输到不同的目的地（加载）。
- en: Over the next few sections, we will use Spark SQL features to access and process
    various data sources and data formats for ETL purposes. Spark SQL's flexible APIs,
    combined with the Catalyst optimizer and tungsten execution engine, make it highly
    suitable for building end-to-end ETL pipelines.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将使用Spark SQL功能来访问和处理各种数据源和数据格式，以实现ETL的目的。Spark SQL灵活的API，结合Catalyst优化器和tungsten执行引擎，使其非常适合构建端到端的ETL管道。
- en: 'In the following code block, we present a simple skeleton of a single ETL query
    that combines all the three (Extract, Transform, and Load) functions. These queries
    can also be extended to execute complex joins between tables containing data from
    multiple sources and source formats:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们提供了一个简单的单个ETL查询的框架，结合了所有三个（提取、转换和加载）功能。这些查询也可以扩展到执行包含来自多个来源和来源格式的数据的表之间的复杂连接：
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the next section, we will introduce a few criteria that can help you make
    appropriate choices regarding data formats to satisfy the requirements of your
    specific use cases.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对流数据执行滑动窗口操作。在这里，我们定义了对滑动窗口的聚合，其中我们对数据进行分组并计算适当的聚合（对于每个组）。
- en: Choosing appropriate data formats
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '![](img/00315.jpeg)'
- en: In enterprise settings, the data is available in many different data sources
    and formats. Spark SQL supports a set of built-in and third-party connectors.
    In addition, we can also define custom data source connectors. Data formats include
    structured, semi-structured, and unstructured formats, such as plain text, JSON,
    XML, CSV, RDBMS records, images, and video. More recently, big data formats such
    as Parquet, ORC, and Avro are becoming increasingly popular. In general, unstructured
    formats such as plain text files are more flexible, while structured formats such
    as Parquet and AVRO are more efficient from a storage and performance perspective.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业设置中，数据以许多不同的数据源和格式可用。Spark SQL支持一组内置和第三方连接器。此外，我们还可以定义自定义数据源连接器。数据格式包括结构化、半结构化和非结构化格式，如纯文本、JSON、XML、CSV、关系型数据库记录、图像和视频。最近，Parquet、ORC和Avro等大数据格式变得越来越受欢迎。一般来说，纯文本文件等非结构化格式更灵活，而Parquet和AVRO等结构化格式在存储和性能方面更有效率。
- en: In the case of structured data formats, the data has a rigid, well-defined schema
    or structure associated with it. For example, columnar data formats make it more
    efficient to extract values from columns. However, this rigidity can make changes
    to the schema, or the structure, challenging. By contrast, unstructured data sources,
    such as free-form text, contain no markup or separators as in CSV or TSV files.
    Such data sources generally require some context around the data; for example,
    you need to know that the contents of files contain text from blogs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化数据格式的情况下，数据具有严格的、明确定义的模式或结构。例如，列式数据格式使得从列中提取值更加高效。然而，这种严格性可能会使对模式或结构的更改变得具有挑战性。相比之下，非结构化数据源，如自由格式文本，不包含CSV或TSV文件中的标记或分隔符。这样的数据源通常需要一些关于数据的上下文；例如，你需要知道文件的内容包含来自博客的文本。
- en: Typically, we need many transformations and feature extraction techniques to
    interpret diverse Datasets. Semi-structured data is structured at a record level,
    but not necessarily across all the records. As a result, each data record contains
    the associated schema information as well.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们需要许多转换和特征提取技术来解释不同的数据集。半结构化数据在记录级别上是结构化的，但不一定在所有记录上都是结构化的。因此，每个数据记录都包含相关的模式信息。
- en: The JSON format is probably the most common example of semi-structured data.
    JSON records are in a human-readable form, making it more convenient for development
    and debugging purposes. However, these formats suffer from parsing-related overheads,
    and are typically not the best choice for supporting the ad hoc querying functionality.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: JSON格式可能是半结构化数据最常见的例子。JSON记录以人类可读的形式呈现，这对于开发和调试来说更加方便。然而，这些格式受到解析相关的开销的影响，通常不是支持特定查询功能的最佳选择。
- en: Often, applications will have to be designed to span and traverse across varied
    data sources and formats to efficiently store and process the data. For example,
    Avro is a good choice when access is required to complete rows of data, as in
    the case of access to features in an ML pipeline. In cases where flexibility in
    the schema is required, using JSON may be the most appropriate choice for the
    data format. Furthermore, in cases where the data does not have a fixed schema,
    it is probably best to use the plain text file format.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，应用程序需要设计成能够跨越各种数据源和格式高效存储和处理数据。例如，当需要访问完整的数据行时，Avro是一个很好的选择，就像在ML管道中访问特征的情况一样。在需要模式的灵活性的情况下，使用JSON可能是数据格式的最合适选择。此外，在数据没有固定模式的情况下，最好使用纯文本文件格式。
- en: Transforming data in ETL pipelines
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ETL管道中的数据转换
- en: Typically, semi-structured formats such as JSON contain struct, map, and array
    data types; for example, request and/or response payloads for REST web services
    contain JSON data with nested fields and arrays.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，诸如JSON之类的半结构化格式包含struct、map和array数据类型；例如，REST Web服务的请求和/或响应负载包含具有嵌套字段和数组的JSON数据。
- en: In this section, we will present examples of Spark SQL-based transformations
    on Twitter data. The input Dataset is a file (`cache-0.json.gz`) containing `10
    M` tweets from a set of Datasets containing over `170 M` tweets collected during
    the three months leading up to the 2012 US presidential elections. This file can
    be downloaded from [https://datahub.io/dataset/twitter-2012-presidential-election](https://datahub.io/dataset/twitter-2012-presidential-election).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将展示基于Spark SQL的Twitter数据转换的示例。输入数据集是一个文件（`cache-0.json.gz`），其中包含了在2012年美国总统选举前三个月内收集的超过`1.7亿`条推文中的`1千万`条推文。这个文件可以从[https://datahub.io/dataset/twitter-2012-presidential-election](https://datahub.io/dataset/twitter-2012-presidential-election)下载。
- en: 'Before starting with the following examples, start Zookeeper and the Kafka
    broker as described in [Chapter 5](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL in Streaming Applications*. Also, create a new Kafka topic, called
    tweetsa. We generate the schema from the input JSON Dataset, as shown. This schema
    definition will be used later in this section:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始以下示例之前，按照[第5章](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c)中描述的方式启动Zookeeper和Kafka代理。另外，创建一个名为tweetsa的新Kafka主题。我们从输入JSON数据集生成模式，如下所示。这个模式定义将在本节后面使用：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Set up to read the streaming tweets from the Kafka topic (*tweetsa*), and then
    parse the JSON data using the schema from the previous step.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 设置从Kafka主题（*tweetsa*）中读取流式推文，并使用上一步的模式解析JSON数据。
- en: 'We select all the fields in the tweet by `specifying data.*` in this statement:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个声明中，我们通过`指定数据.*`来选择推文中的所有字段：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You will need to use the following command repeatedly (as you work through
    the examples) to pipe the tweets contained in the input file to the Kafka topic,
    as illustrated:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在你通过示例工作时，你需要反复使用以下命令将输入文件中包含的推文传输到Kafka主题中，如下所示：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Given the size of the input file, this can potentially result in space-related
    issues on your machine. If this happens, use appropriate Kafka commands to delete
    and recreate the topic (refer to [https://kafka.apache.org/0102/documentation.html](https://kafka.apache.org/0102/documentation.html)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到输入文件的大小，这可能会导致您的计算机出现空间相关的问题。如果发生这种情况，请使用适当的Kafka命令来删除并重新创建主题（参考[https://kafka.apache.org/0102/documentation.html](https://kafka.apache.org/0102/documentation.html)）。
- en: 'Here, we reproduce a section of the schema to help understand the structure
    we are working with over the next few examples:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们重现了一个模式的部分，以帮助理解我们在接下来的几个示例中要处理的结构：
- en: '![](img/00316.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00316.jpeg)'
- en: 'We can select specific fields from nested columns in the JSON string. We use
    the . (dot) operator to choose the nested field, as shown:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从JSON字符串中的嵌套列中选择特定字段。我们使用`.`（点）运算符来选择嵌套字段，如下所示：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we write the output stream to the screen to view the results. You will
    need to execute the following statement after each of the transformations in order
    to view and evaluate the results. Also, in the interests of saving time, you should
    execute `s5.stop()`, after you have seen sufficient output on the screen. Alternatively,
    you can always choose to work with a smaller set of data extracted from the original
    input file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将输出流写入屏幕以查看结果。您需要在每个转换之后执行以下语句，以查看和评估结果。此外，为了节省时间，您应该在看到足够的屏幕输出后执行`s5.stop()`。或者，您可以选择使用从原始输入文件中提取的较小数据集进行工作：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/00317.gif)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00317.gif)'
- en: 'In the next example, we will flatten a struct using star (*) to select all
    the subfields in the struct:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们将使用星号（*）展平一个struct以选择struct中的所有子字段：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The results can be viewed by writing the output stream, as shown in the preceding
    example:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过编写输出流来查看结果，如前面的示例所示：
- en: '![](img/00318.gif)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00318.gif)'
- en: We can use the struct function to create a new struct (for nesting the columns)
    as illustrated in the following code snippet. We can select a specific field or
    fields for creating the new struct. We can also nest all the columns using star
    (*), if required.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用struct函数创建一个新的struct（用于嵌套列），如下面的代码片段所示。我们可以选择特定字段或字段来创建新的struct。如果需要，我们还可以使用星号（*）嵌套所有列。
- en: 'Here, we reproduce the section of the schema used in this example:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们重现了此示例中使用的模式部分：
- en: '![](img/00319.jpeg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00319.jpeg)'
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/00320.gif)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00320.gif)'
- en: 'In the next example, we select a single array (or map) element using `getItem()`.
    Here, we are operating on the following part of the schema:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们使用`getItem()`选择单个数组（或映射）元素。在这里，我们正在操作模式的以下部分：
- en: '![](img/00321.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00321.jpeg)'
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/00322.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00322.jpeg)'
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/00323.gif)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00323.gif)'
- en: 'We can use the `explode()` function to create a new row for each element in
    an array, as shown. To illustrate the results of `explode()`, we first show the
    rows containing the arrays, and then show the results of applying the explode
    function:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`explode()`函数为数组中的每个元素创建新行，如所示。为了说明`explode()`的结果，我们首先展示包含数组的行，然后展示应用explode函数的结果：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following output is obtained:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 获得以下输出：
- en: '![](img/00324.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00324.jpeg)'
- en: 'Note the separate rows created for the array elements after applying the explode
    function:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在应用explode函数后，为数组元素创建了单独的行：
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following is the output obtained:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的输出如下：
- en: '![](img/00325.jpeg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00325.jpeg)'
- en: Spark SQL also has functions such as `to_json()`, to transform a `struct` to
    a JSON string, and `from_json()`, to convert a JSON string to a `struct`. These
    functions are very useful to read from or write to Kafka topics. For example,
    if the "value" field contains data in a JSON string, then we can use the `from_json()`
    function to extract the data, transform it, and then push it out to a different
    Kafka topic, and/or write it out to a Parquet file or a serving database.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL还具有诸如`to_json()`之类的函数，用于将`struct`转换为JSON字符串，以及`from_json()`，用于将JSON字符串转换为`struct`。这些函数对于从Kafka主题读取或写入非常有用。例如，如果“value”字段包含JSON字符串中的数据，则我们可以使用`from_json()`函数提取数据，转换数据，然后将其推送到不同的Kafka主题，并/或将其写入Parquet文件或服务数据库。
- en: 'In the following example, we use the `to_json()` function to convert a struct
    to a JSON string:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用`to_json()`函数将struct转换为JSON字符串：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/00326.gif)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00326.gif)'
- en: We can use the `from_json()` function to convert a column containing JSON data
    into a `struct` data type. Further, we can flatten the preceding struct into separate
    columns. We show an example of using this function in a later section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`from_json()`函数将包含JSON数据的列转换为`struct`数据类型。此外，我们可以将前述结构展平为单独的列。我们在后面的部分中展示了使用此函数的示例。
- en: For more detailed coverage of transformation functions, refer to [https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html](https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有关转换函数的更详细覆盖范围，请参阅[https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html](https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html)。
- en: Addressing errors in ETL pipelines
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决ETL管道中的错误
- en: ETL tasks are usually considered to be complex, expensive, slow, and error-prone.
    Here, we will examine typical challenges in ETL processes, and how Spark SQL features
    assist in addressing them.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ETL任务通常被认为是复杂、昂贵、缓慢和容易出错的。在这里，我们将研究ETL过程中的典型挑战，以及Spark SQL功能如何帮助解决这些挑战。
- en: 'Spark can automatically infer the schema from a JSON file. For example, for
    the following JSON data, the inferred schema includes all the labels and the data
    types based on the content. Here, the data types for all the elements in the input
    data are longs by default:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以自动从JSON文件中推断模式。例如，对于以下JSON数据，推断的模式包括基于内容的所有标签和数据类型。在这里，输入数据中所有元素的数据类型默认为长整型：
- en: '**test1.json**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**test1.json**'
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can print the schema to verify the data types, as shown:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以打印模式以验证数据类型，如下所示：
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'However, in the following JSON data, if the value of `e` in the third row and
    the value of `b` in the last row are changed to include fractions, and the value
    of `f` in the second-from-last row is enclosed in quotes, the inferred schema
    changes the data types of `b` and `e` to double, and `f` to string type:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在以下JSON数据中，如果第三行中的`e`的值和最后一行中的`b`的值被更改以包含分数，并且倒数第二行中的`f`的值被包含在引号中，那么推断的模式将更改`b`和`e`的数据类型为double，`f`的数据类型为字符串：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we want to associate specific structure or data types to elements, we need
    to use a user-specified schema. In the next example, we use a CSV file with a
    header containing the field names. The field names in the schema are derived from
    the header, and the data types specified in the user-defined schema are used against
    them, as shown:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要将特定结构或数据类型与元素关联起来，我们需要使用用户指定的模式。在下一个示例中，我们使用包含字段名称的标题的CSV文件。模式中的字段名称来自标题，并且用户定义的模式中指定的数据类型将用于它们，如下所示：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following output is obtained:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 获取以下输出：
- en: '![](img/00327.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00327.jpeg)'
- en: Issues can also occur in ETL pipelines due to file and data corruption. If the
    data is not mission-critical, and the corrupt files can be safely ignored, we
    can set `config property spark.sql.files.ignoreCorruptFiles = true`. This setting
    lets Spark jobs continue running even when corrupt files are encountered. Note
    that contents that are successfully read will continue to be returned.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文件和数据损坏，ETL管道中也可能出现问题。如果数据不是关键任务，并且损坏的文件可以安全地忽略，我们可以设置`config property spark.sql.files.ignoreCorruptFiles
    = true`。此设置允许Spark作业继续运行，即使遇到损坏的文件。请注意，成功读取的内容将继续返回。
- en: 'In the following example, there is bad data for `b` in row `4`. We can still
    read the data using the `PERMISSIVE` mode. In this case, a new column, called
    `_corrupt_record`, is added to the DataFrame, and the contents of the corrupted
    rows appear in that column with the rest of the fields initialized to nulls. We
    can focus on the data issues by reviewing the data in this column and initiate
    suitable actions to fix them. By setting the `spark.sql.columnNameOfCorruptRecord`
    property, we can configure the default name of the corrupted contents column:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，第4行的`b`存在错误数据。我们仍然可以使用`PERMISSIVE`模式读取数据。在这种情况下，DataFrame中会添加一个名为`_corrupt_record`的新列，并且损坏行的内容将出现在该列中，其余字段初始化为null。我们可以通过查看该列中的数据来关注数据问题，并采取适当的措施来修复它们。通过设置`spark.sql.columnNameOfCorruptRecord`属性，我们可以配置损坏内容列的默认名称：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/00328.gif)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00328.gif)'
- en: 'Now, we use the `DROPMALFORMED` option to drop all malformed records. Here,
    the fourth row is dropped due to the bad value for `b`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用`DROPMALFORMED`选项来删除所有格式不正确的记录。在这里，由于`b`的坏值，第四行被删除：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/00329.gif)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00329.gif)'
- en: 'For critical data, we can use the `FAILFAST` option to fail immediately upon
    encountering a bad record. For example, in the following example, due to the value
    of `b` in the fourth row, the operation throws an exception and exits immediately:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关键数据，我们可以使用`FAILFAST`选项，在遇到坏记录时立即失败。例如，在以下示例中，由于第四行中`b`的值，操作会抛出异常并立即退出：
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the following example, we have a record that spans two rows; we can read
    this record by setting the `wholeFile` option to true:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们有一条跨越两行的记录；我们可以通过将`wholeFile`选项设置为true来读取此记录：
- en: '[PRE20]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: For more details on Spark SQL-based ETL pipelines and roadmaps, visit [https://spark-summit.org/2017/events/building-robust-etl-pipelines-with-apache-spark/](https://spark-summit.org/2017/events/building-robust-etl-pipelines-with-apache-spark/).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 有关基于Spark SQL的ETL管道和路线图的更多详细信息，请访问[https://spark-summit.org/2017/events/building-robust-etl-pipelines-with-apache-spark/](https://spark-summit.org/2017/events/building-robust-etl-pipelines-with-apache-spark/)。
- en: The preceding reference presents several higher-order SQL transformation functions,
    new formats for the DataframeWriter API, and a unified `Create Table` (as `Select`)
    constructs in Spark 2.2 and 2.3-Snapshot.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 上述参考介绍了几个高阶SQL转换函数，DataframeWriter API的新格式以及Spark 2.2和2.3-Snapshot中的统一`Create
    Table`（作为`Select`）构造。
- en: Other requirements addressed by Spark SQL include scalability and continuous
    ETL using structured streaming. We can use structured streaming to enable raw
    data to be available as structured data ready for analysis, reporting, and decision-making
    as soon as possible, instead of incurring the hours of delay typically associated
    with running periodic batch jobs. This type of processing is especially important
    in applications such as anomaly detection, fraud detection, and so on, where time
    is of the essence.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL解决的其他要求包括可扩展性和使用结构化流进行持续ETL。我们可以使用结构化流来使原始数据尽快可用作结构化数据，以进行分析、报告和决策，而不是产生通常与运行周期性批处理作业相关的几小时延迟。这种处理在应用程序中尤为重要，例如异常检测、欺诈检测等，时间至关重要。
- en: In the next section, we will shift our focus to building a scalable monitoring
    solution using Spark SQL.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把重点转移到使用Spark SQL构建可扩展的监控解决方案。
- en: Implementing a scalable monitoring solution
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施可扩展的监控解决方案
- en: Building a scalable monitoring function for large-scale deployments can be challenging
    as there could be billions of data points captured each day. Additionally, the
    volume of logs and the number of metrics can be difficult to manage without a
    suitable big data platform with streaming and visualization support.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为大规模部署构建可扩展的监控功能可能具有挑战性，因为每天可能捕获数十亿个数据点。此外，日志的数量和指标的数量可能难以管理，如果没有适当的具有流式处理和可视化支持的大数据平台。
- en: Voluminous logs collected from applications, servers, network devices, and so
    on are processed to provide real-time monitoring that help detect errors, warnings,
    failures, and other issues. Typically, various daemons, services, and tools are
    used to collect/send log records to the monitoring system. For example, log entries
    in the JSON format can be sent to Kafka queues or Amazon Kinesis. These JSON records
    can then be stored on S3 as files and/or streamed to be analyzed in real time
    (in a Lambda architecture implementation). Typically, an ETL pipeline is run to
    cleanse the log data, transform it into a more structured form, and then load
    it into files such as Parquet files or databases, for querying, alerting, and
    reporting purposes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 从应用程序、服务器、网络设备等收集的大量日志被处理，以提供实时监控，帮助检测错误、警告、故障和其他问题。通常，各种守护程序、服务和工具用于收集/发送日志记录到监控系统。例如，以JSON格式的日志条目可以发送到Kafka队列或Amazon
    Kinesis。然后，这些JSON记录可以存储在S3上作为文件和/或流式传输以实时分析（在Lambda架构实现中）。通常，会运行ETL管道来清理日志数据，将其转换为更结构化的形式，然后加载到Parquet文件或数据库中，以进行查询、警报和报告。
- en: 'The following figure illustrates one such platform using **Spark Streaming
    Jobs**, a **Scalable Time Series Database** such as OpenTSDB or Graphite, and
    **Visualization Tools** such as Grafana:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了一个使用**Spark Streaming Jobs**、**可扩展的时间序列数据库**（如OpenTSDB或Graphite）和**可视化工具**（如Grafana）的平台：
- en: '![](img/00330.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00330.jpeg)'
- en: For more details on this solution, refer to [https://spark-summit.org/2017/events/scalable-monitoring-using-apache-spark-and-friends/](https://spark-summit.org/2017/events/scalable-monitoring-using-apache-spark-and-friends/).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此解决方案的更多详细信息，请参阅[https://spark-summit.org/2017/events/scalable-monitoring-using-apache-spark-and-friends/](https://spark-summit.org/2017/events/scalable-monitoring-using-apache-spark-and-friends/)。
- en: Monitoring and troubleshooting problems are challenging tasks in large distributed
    environments comprising of several Spark clusters with varying configurations
    and versions, running different types of workloads. In these environments hundreds
    of thousands metrics may be received. Additionally, hundreds of MBs of logs are
    generated per second. These metrics need to be tracked and the logs analyzed for
    anomalies, failures, bugs, environmental issues, and so on to support alerting
    and troubleshooting functions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在由多个具有不同配置和版本、运行不同类型工作负载的Spark集群组成的大型分布式环境中，监控和故障排除问题是具有挑战性的任务。在这些环境中，可能会收到数十万条指标。此外，每秒生成数百MB的日志。这些指标需要被跟踪，日志需要被分析以发现异常、故障、错误、环境问题等，以支持警报和故障排除功能。
- en: The following figure illustrates an AWS-based data pipeline that pushes all
    the metrics and logs (both structured and unstructured) to Kinesis. A structured
    streaming job can read the raw logs from Kinesis and save the data as Parquet
    files on S3.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了一个基于AWS的数据管道，将所有指标和日志（结构化和非结构化）推送到Kinesis。结构化流作业可以从Kinesis读取原始日志，并将数据保存为S3上的Parquet文件。
- en: 'The structured streaming query can strip known error patterns and raise suitable
    alerts, if a new error type is observed. Other Spark batch and streaming applications
    can use these Parquet files to perform additional processing and output their
    results as new Parquet files on S3:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流查询可以剥离已知的错误模式，并在观察到新的错误类型时提出适当的警报。其他Spark批处理和流处理应用程序可以使用这些Parquet文件进行额外处理，并将其结果输出为S3上的新Parquet文件：
- en: '![](img/00331.jpeg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00331.jpeg)'
- en: In this architecture, discovering issues from unstructured logs may be required
    to determine their scope, duration, and impact. **Raw Logs** typically contain
    many near-duplicate error messages. For efficient processing of these logs, we
    need to normalize, deduplicate, and filter out well-known error conditions to
    discover and reveal new ones.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，可能需要从非结构化日志中发现问题，以确定其范围、持续时间和影响。**原始日志**通常包含许多近似重复的错误消息。为了有效处理这些日志，我们需要对其进行规范化、去重和过滤已知的错误条件，以发现和揭示新的错误。
- en: For details on a pipeline to process raw logs, refer to [https://spark-summit.org/2017/events/lessons-learned-from-managing-thousands-of-production-apache-spark-clusters-daily/](https://spark-summit.org/2017/events/lessons-learned-from-managing-thousands-of-production-apache-spark-clusters-daily/).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 有关处理原始日志的管道的详细信息，请参阅[https://spark-summit.org/2017/events/lessons-learned-from-managing-thousands-of-production-apache-spark-clusters-daily/](https://spark-summit.org/2017/events/lessons-learned-from-managing-thousands-of-production-apache-spark-clusters-daily/)。
- en: In this section, we will explore some of the features Spark SQL and Structured
    Streaming provide to create a scalable monitoring solution.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨Spark SQL和结构化流提供的一些功能，以创建可扩展的监控解决方案。
- en: 'First, start the Spark shell with the Kafka packages:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用Kafka包启动Spark shell：
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Download the traces for the month of July, 1995, containing HTTP requests to
    the NASA Kennedy Space Center WWW server in Florida from [http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 下载1995年7月的痕迹，其中包含了对佛罗里达州NASA肯尼迪航天中心WWW服务器的HTTP请求[http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html)。
- en: 'Import the following packages for the hands-on exercises in this chapter:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的实践练习中，导入以下包：
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, define the schema for the records in the file:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为文件中的记录定义模式：
- en: '[PRE23]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For simplicity, we read the input file as a CSV file with a space separator,
    as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，我们将输入文件读取为以空格分隔的CSV文件，如下所示：
- en: '[PRE24]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we create a DataFrame containing the log events. As the timestamp changes
    to the local time zone (by default) in the preceding step, we also retain the
    original timestamp with time zone information in the `original_dateTime` column,
    as illustrated:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含日志事件的DataFrame。由于时间戳在前面的步骤中更改为本地时区（默认情况下），我们还在`original_dateTime`列中保留了带有时区信息的原始时间戳，如下所示：
- en: '[PRE25]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can check the results of the streaming read, as shown:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查流式读取的结果，如下所示：
- en: '[PRE26]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](img/00332.gif)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00332.gif)'
- en: 'We can save the streaming input to Parquet files, partitioned by date to support
    queries more efficiently, as demonstrated:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将流输入保存为Parquet文件，按日期分区以更有效地支持查询，如下所示：
- en: '[PRE27]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can read input so that the latest records are available first by specifying
    the `latestFirst` option:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过指定`latestFirst`选项来读取输入，以便最新的记录首先可用：
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can also write out the output in the JSON format, partitioned by date, easily,
    as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以按日期将输出以JSON格式输出，如下所示：
- en: '[PRE29]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we show the use of Kafka for input and output in streaming Spark applications.
    Here, we have to specify the format parameter as `kafka`, and the kafka broker
    and topic:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们展示了在流式Spark应用程序中使用Kafka进行输入和输出的示例。在这里，我们必须将格式参数指定为`kafka`，并指定kafka代理和主题：
- en: '[PRE30]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we are reading a stream of JSON data from Kafka. The starting offset is
    set to the earliest to specify the starting point for our query. This applies
    only when a new streaming query is started:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们正在从Kafka中读取JSON数据流。将起始偏移设置为最早以指定查询的起始点。这仅适用于启动新的流式查询时：
- en: '[PRE31]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can print out the schema for records read from Kafka, as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下方式打印从Kafka读取的记录的模式：
- en: '[PRE32]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we define the schema for input records, as shown:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义输入记录的模式，如下所示：
- en: '[PRE33]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we can specify the schema, as illustrated. The star `*` operator is used
    to select all the `subfields` in a `struct`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以指定模式，如所示。星号`*`运算符用于选择`struct`中的所有`subfields`：
- en: '[PRE34]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we show an example of selecting specific fields. Here, we set the `outputMode`
    to append so that only the new rows appended to the result table are written out
    to external storage. This is applicable only on queries where the existing rows
    in the result table are not expected to change:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示选择特定字段的示例。在这里，我们将`outputMode`设置为append，以便只有追加到结果表的新行被写入外部存储。这仅适用于查询结果表中现有行不会发生变化的情况：
- en: '[PRE35]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/00333.gif)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00333.gif)'
- en: 'We can also specify `read` (not `readStream`) to read the records into a regular
    DataFrame:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以指定`read`（而不是`readStream`）将记录读入常规DataFrame中：
- en: '[PRE36]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can now do all the standard DataFrame operations against this DataFrame;
    for example, we create a table and query it, as shown:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以对这个DataFrame执行所有标准的DataFrame操作；例如，我们创建一个表并查询它，如下所示：
- en: '[PRE37]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/00334.jpeg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00334.jpeg)'
- en: 'Then, we read the records from Kafka and apply the schema:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从Kafka中读取记录并应用模式：
- en: '[PRE38]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can execute the following query to check the contents of the records:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以执行以下查询来检查记录的内容：
- en: '[PRE39]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/00335.gif)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00335.gif)'
- en: 'We can select all the fields from the records, as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从记录中选择所有字段，如下所示：
- en: '[PRE40]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can also select specific fields of interest from the DataFrame:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从DataFrame中选择感兴趣的特定字段：
- en: '[PRE41]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we can use window operations, as demonstrated, and maintain counts for
    various HTTP codes. Here, we use `outputMode` set to `complete` since we want
    the entire updated result table to be written to the external storage:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用窗口操作，并为各种HTTP代码维护计数，如所示。在这里，我们将`outputMode`设置为`complete`，因为我们希望将整个更新后的结果表写入外部存储：
- en: '[PRE42]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/00336.gif)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00336.gif)'
- en: 'Next, we show another example of using `groupBy` and computed counts for various
    page requests in these windows. This can be used to compute and report the top
    pages visited type metrics:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示了另一个使用`groupBy`和计算各窗口中各种页面请求计数的示例。这可用于计算和报告访问类型指标中的热门页面：
- en: '[PRE43]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![](img/00337.gif)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00337.gif)'
- en: Note that the examples presented earlier are instances of stateful processing.
    The counts have to be saved as a distributed state between triggers. Each trigger
    reads the previous state and writes the updated state. This state is stored in
    memory, and is backed by the persistent WAL, typically located on HDFS or S3 storage.
    This allows the streaming application to automatically handle data arriving late.
    Keeping this state allows the late data to update the counts of old windows.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面提到的示例是有状态处理的实例。计数必须保存为触发器之间的分布式状态。每个触发器读取先前的状态并写入更新后的状态。此状态存储在内存中，并由持久的WAL支持，通常位于HDFS或S3存储上。这使得流式应用程序可以自动处理延迟到达的数据。保留此状态允许延迟数据更新旧窗口的计数。
- en: However, the size of the state can increase indefinitely, if the old windows
    are not dropped. A watermarking approach is used to address this issue. A watermark
    is a moving threshold of how late data is expected to be and when to drop the
    old state. It trails behind the max seen event time. Data newer than watermark
    may be late, but is allowed into the aggregate, while data older than the watermark
    is considered "too late" and dropped. Additionally, windows older than the watermark
    are automatically deleted to limit the amount of intermediate state that is required
    to be maintained by the system.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果不丢弃旧窗口，状态的大小可能会无限增加。水印方法用于解决此问题。水印是预期数据延迟的移动阈值，以及何时丢弃旧状态。它落后于最大观察到的事件时间。水印之后的数据可能会延迟，但允许进入聚合，而水印之前的数据被认为是“太晚”，并被丢弃。此外，水印之前的窗口会自动删除，以限制系统需要维护的中间状态的数量。
- en: 'A watermark specified for the previous query is given here:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个查询中指定的水印在这里给出：
- en: '[PRE44]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: For more details on watermarking, refer to [https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html](https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 有关水印的更多详细信息，请参阅[https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html](https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html)。
- en: In the next section, we will shift our focus to deploying Spark-based machine
    learning pipelines in production.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把重点转移到在生产环境中部署基于Spark的机器学习管道。
- en: Deploying Spark machine learning pipelines
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署Spark机器学习管道
- en: 'The following figure illustrates a machine learning pipeline at a conceptual
    level. However, real-life ML pipelines are a lot more complicated, with several
    models being trained, tuned, combined, and so on:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 下图以概念级别说明了机器学习管道。然而，现实生活中的ML管道要复杂得多，有多个模型被训练、调整、组合等：
- en: '![](img/00338.jpeg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00338.jpeg)'
- en: 'The next figure shows the core elements of a typical machine learning application
    split into two parts: the modeling, including model training, and the deployed
    model (used on streaming data to output the results):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了典型机器学习应用程序的核心元素分为两部分：建模，包括模型训练，以及部署的模型（用于流数据以输出结果）：
- en: '![](img/00339.jpeg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00339.jpeg)'
- en: Typically, data scientists experiment or do their modeling work in Python and/or
    R. Their work is then reimplemented in Java/Scala before deployment in a production
    environment. Enterprise production environments often consist of web servers,
    application servers, databases, middleware, and so on. The conversion of prototypical
    models to production-ready models results in additional design and development
    effort that lead to delays in rolling out updated models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据科学家在Python和/或R中进行实验或建模工作。然后在部署到生产环境之前，他们的工作会在Java/Scala中重新实现。企业生产环境通常包括Web服务器、应用服务器、数据库、中间件等。将原型模型转换为生产就绪模型会导致额外的设计和开发工作，从而导致更新模型的推出延迟。
- en: We can use Spark MLlib 2.x model serialization to directly use the models and
    pipelines saved by data scientists (to disk) in production environments by loading
    them from the persisted model files.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Spark MLlib 2.x模型序列化直接在生产环境中加载数据科学家保存的模型和管道（到磁盘）的模型文件。
- en: 'In the following example (source: [https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html)),
    we will illustrate creating and saving a ML pipeline in Python (using `pyspark`
    shell) and then retrieving it in a Scala environment (using Spark shell).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中（来源：[https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html)），我们将演示在Python中创建和保存ML管道（使用`pyspark`
    shell），然后在Scala环境中检索它。
- en: 'Start the `pyspark` shell and execute the following sequence of Python statements:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 启动`pyspark` shell并执行以下Python语句序列：
- en: '[PRE45]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Start the Spark shell and execute the following sequence of Scala statements:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 启动Spark shell并执行以下Scala语句序列：
- en: '[PRE46]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we create a `test` Dataset and run it through the ML pipeline:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个`test`数据集，并通过ML管道运行它：
- en: '[PRE47]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The results of running the model on the `test` Dataset are shown here:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在`test`数据集上运行模型的结果如下：
- en: '[PRE48]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The key parameters of the saved logistic regression model are read into a DataFrame,
    as illustrated in the following code block. Earlier, when the model was saved
    in the `pyspark` shell, these parameters were saved to a Parquet file located
    in a subdirectory associated with the final stage of our pipeline:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 保存的逻辑回归模型的关键参数被读入DataFrame，如下面的代码块所示。在之前，当模型在`pyspark` shell中保存时，这些参数被保存到与我们管道的最终阶段相关的子目录中的Parquet文件中：
- en: '[PRE49]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The following output is obtained:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 获得以下输出：
- en: '![](img/00340.jpeg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00340.jpeg)'
- en: '[PRE50]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is, as shown:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00341.jpeg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00341.jpeg)'
- en: For more details on how to productionize ML models, refer to [https://spark-summit.org/2017/events/how-to-productionize-your-machine-learning-models-using-apache-spark-mllib-2x/](https://spark-summit.org/2017/events/how-to-productionize-your-machine-learning-models-using-apache-spark-mllib-2x/).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何将ML模型投入生产的更多详细信息，请参阅[https://spark-summit.org/2017/events/how-to-productionize-your-machine-learning-models-using-apache-spark-mllib-2x/](https://spark-summit.org/2017/events/how-to-productionize-your-machine-learning-models-using-apache-spark-mllib-2x/)。
- en: Understanding the challenges in typical ML deployment environments
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解典型ML部署环境中的挑战
- en: Production deployment environments for ML models can be very diverse and complex.
    For example, models may need to be deployed in web applications, portals, real-time
    and batch processing systems, and as an API or a REST service, embedded in devices
    or in large legacy environments.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ML模型的生产部署环境可能非常多样化和复杂。例如，模型可能需要部署在Web应用程序、门户、实时和批处理系统中，以及作为API或REST服务，嵌入设备或大型遗留环境中。
- en: Additionally, enterprise technology stacks can comprise of Java Enterprise,
    C/C++, legacy mainframe environments, relational databases, and so on. The non-functional
    requirements and customer SLAs with respect to response times, throughput, availability,
    and uptime can also vary widely. However, in almost all cases, our deployment
    process needs to support A/B testing, experimentation, model performance evaluation,
    and be agile and responsive to business needs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，企业技术堆栈可以包括Java企业、C/C++、遗留主机环境、关系数据库等。与响应时间、吞吐量、可用性和正常运行时间相关的非功能性要求和客户SLA也可能差异很大。然而，在几乎所有情况下，我们的部署过程需要支持A/B测试、实验、模型性能评估，并且需要灵活和响应业务需求。
- en: Typically, practitioners use various methods to benchmark and phase-in new or
    updated models to avoid high-risk, big bang production deployments.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，从业者使用各种方法来对新模型或更新模型进行基准测试和逐步推出，以避免高风险、大规模的生产部署。
- en: In the next section, we will explore a few model deployment architectures.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨一些模型部署架构。
- en: Understanding types of model scoring architectures
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解模型评分架构的类型
- en: 'The simplest model is to precompute model results using Spark (batch processing),
    save the results to a database, and then serve the results to web and mobile applications
    from the database. Many large-scale recommendation engines and search engines
    use this architecture:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的模型是使用Spark（批处理）预计算模型结果，将结果保存到数据库，然后从数据库为Web和移动应用程序提供结果。许多大规模的推荐引擎和搜索引擎使用这种架构：
- en: '![](img/00342.jpeg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00342.jpeg)'
- en: 'A second model scoring architecture computes the features and runs prediction
    algorithms using Spark Streaming. The prediction results can be cached using caching
    solutions, such as Redis, and can be made available via an API. Other applications
    can then use these APIs to obtain the prediction results from the deployed model.
    This option is illustrated in this figure:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种模型评分架构使用Spark Streaming计算特征并运行预测算法。预测结果可以使用缓存解决方案（如Redis）进行缓存，并可以通过API提供。其他应用程序可以使用这些API从部署的模型中获取预测结果。此选项在此图中有所说明：
- en: '![](img/00343.jpeg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00343.jpeg)'
- en: In a third architectural model, we can use Spark for model training purposes
    only. The model is then copied into the production environment. For example, we
    can load the coefficients and intercept of a logistic regression model from a
    JSON file. This approach is resource-efficient and results in a high-performing
    system. It is also a lot easier to deploy in existing or complex environments.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三种架构模型中，我们可以仅使用Spark进行模型训练。然后将模型复制到生产环境中。例如，我们可以从JSON文件中加载逻辑回归模型的系数和截距。这种方法资源高效，并且会产生高性能的系统。在现有或复杂环境中部署也更加容易。
- en: 'It is illustrated here:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示：
- en: '![](img/00344.jpeg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00344.jpeg)'
- en: 'Continuing with our earlier example, we can read in the saved model parameters
    from a Parquet file and convert it to a JSON format that can, in turn, be conveniently
    imported into any application (inside or outside the Spark environment) and applied
    to new data:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们之前的例子，我们可以从Parquet文件中读取保存的模型参数，并将其转换为JSON格式，然后可以方便地导入到任何应用程序（在Spark环境内部或外部）并应用于新数据：
- en: '[PRE51]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can display the intercept, coefficients, and other key parameters using
    standard OS commands, as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用标准操作系统命令显示截距、系数和其他关键参数，如下所示：
- en: '[PRE52]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](img/00345.jpeg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00345.jpeg)'
- en: As models are becoming bigger and more complex, it can be challenging to deploy
    and serve them. Models may not scale well, and their resource requirements can
    become very expensive. Databricks and Redis-ML provide solutions to deploy the
    trained model.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型变得越来越大和复杂，部署和提供服务可能会变得具有挑战性。模型可能无法很好地扩展，其资源需求可能变得非常昂贵。Databricks和Redis-ML提供了部署训练模型的解决方案。
- en: In the Redis-ML solution, the model is applied to the new data directly in the
    Redis environment.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在Redis-ML解决方案中，模型直接应用于Redis环境中的新数据。
- en: This can provide the required overall performance, scalability, and availability
    at a much lower price point than running the model in a Spark environment.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以以比在Spark环境中运行模型的价格更低的价格提供所需的整体性能、可伸缩性和可用性。
- en: 'The following figure shows Redis-ML being used as a serving engine (implementing
    the third model scoring architectural pattern, as described earlier):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了Redis-ML作为服务引擎的使用情况（实现了先前描述的第三种模型评分架构模式）：
- en: '![](img/00346.jpeg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00346.jpeg)'
- en: In the next section, we will briefly discuss using Mesos and Kubernetes as cluster
    managers in production environments.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将简要讨论在生产环境中使用Mesos和Kubernetes作为集群管理器。
- en: Using cluster managers
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集群管理器
- en: 'In this section, we will briefly discuss Mesos and Kubernetes at a conceptual
    level. The Spark framework can be deployed through Apache **Mesos**, **YARN**,
    Spark Standalone, or the **Kubernetes** cluster manager, as depicted:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在概念层面简要讨论Mesos和Kubernetes。Spark框架可以通过Apache Mesos、YARN、Spark Standalone或Kubernetes集群管理器进行部署，如下所示：
- en: '![](img/00347.jpeg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00347.jpeg)'
- en: Mesos can enable easy scalability and replication of data, and is a good unified
    cluster management solution for heterogeneous workloads.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos可以实现数据的轻松扩展和复制，并且是异构工作负载的良好统一集群管理解决方案。
- en: To use Mesos from Spark, the Spark binaries should be accessible by Mesos and
    the Spark driver configured to connect to Mesos. Alternatively, you can also install
    Spark binaries on all the Mesos slaves. The driver creates a job and then issues
    the tasks for scheduling, while Mesos determines the machines to handle them.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Spark使用Mesos，Spark二进制文件应该可以被Mesos访问，并且Spark驱动程序配置为连接到Mesos。或者，您也可以在所有Mesos从属节点上安装Spark二进制文件。驱动程序创建作业，然后发出任务进行调度，而Mesos确定处理它们的机器。
- en: 'Spark can run over Mesos in two modes: coarse-grained (the default) and fine-grained
    (deprecated in Spark 2.0.0). In the coarse-grained mode, each Spark executor runs
    as a single Mesos task. This mode has significantly lower start up overheads,
    but reserves Mesos resources for the duration of the application. Mesos also supports
    dynamic allocation where the number of executors is adjusted based on the statistics
    of the application.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以在Mesos上以两种模式运行：粗粒度（默认）和细粒度（在Spark 2.0.0中已弃用）。在粗粒度模式下，每个Spark执行器都作为单个Mesos任务运行。这种模式具有显着较低的启动开销，但会为应用程序的持续时间保留Mesos资源。Mesos还支持根据应用程序的统计数据调整执行器数量的动态分配。
- en: 'The following figure illustrates a deployment that collocates the **Mesos Master**
    and **Zookeeper** nodes. The **Mesos Slave** and **Cassandra Node** are also collocated
    for better data locality. In addition, the Spark binaries are deployed on all
    worker nodes:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了将Mesos Master和Zookeeper节点放置在一起的部署。Mesos Slave和Cassandra节点也放置在一起，以获得更好的数据局部性。此外，Spark二进制文件部署在所有工作节点上：
- en: '![](img/00348.jpeg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00348.jpeg)'
- en: Another emerging Spark cluster management solution is Kubernetes, which is being
    developed as a native cluster manager for Spark. It is an open source system that
    can be used for automating the deployment, scaling, and management of containerized
    Spark applications.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个新兴的Spark集群管理解决方案是Kubernetes，它正在作为Spark的本机集群管理器进行开发。它是一个开源系统，可用于自动化容器化Spark应用程序的部署、扩展和管理。
- en: 'The next figure depicts a high-level view of Kubernetes. Each node contains
    a daemon, called a **Kublet**, which talks to the **Master** node. The users also
    talks to the Master to declaratively specify what they want to run. For example,
    a user can request running a specific number of web server instances. The Master
    will take the user''s request and schedule the workload on the nodes:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了Kubernetes的高层视图。每个节点都包含一个名为Kublet的守护程序，它与Master节点通信。用户还可以与Master节点通信，以声明性地指定他们想要运行的内容。例如，用户可以请求运行特定数量的Web服务器实例。Master将接受用户的请求并在节点上安排工作负载：
- en: '![](img/00349.jpeg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00349.jpeg)'
- en: 'The nodes run one or more pods. A pod is a higher-level abstraction on containers,
    and each pod can contain a set of colocated containers. Each pod has its own IP
    address and can communicate with the pods in other nodes. The storage volumes
    can be local or network-attached. This can be seen in the following figure:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 节点运行一个或多个pod。Pod是容器的更高级抽象，每个pod可以包含一组共同放置的容器。每个pod都有自己的IP地址，并且可以与其他节点中的pod进行通信。存储卷可以是本地的或网络附加的。这可以在下图中看到：
- en: '![](img/00350.jpeg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00350.jpeg)'
- en: Kubernetes promotes resource sharing between different types of Spark workload
    to reduce operational costs and improve infrastructure utilization. In addition,
    several add-on services can be used out-of-the-box with Spark applications, including
    logging, monitoring, security, container-to-container communications, and so on.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes促进不同类型的Spark工作负载之间的资源共享，以减少运营成本并提高基础设施利用率。此外，可以使用几个附加服务与Spark应用程序一起使用，包括日志记录、监视、安全性、容器间通信等。
- en: For more details on Spark on Kubernetes, visit [https://github.com/apache-spark-on-k8s/spark](https://github.com/apache-spark-on-k8s/spark).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在Kubernetes上使用Spark的更多详细信息，请访问[https://github.com/apache-spark-on-k8s/spark](https://github.com/apache-spark-on-k8s/spark)。
- en: In the following figure, the dotted line separates Kubernetes from Spark. Spark
    Core is responsible for getting new executors, pushing new configurations, removing
    executors, and so on. The **Kubernetes Scheduler Backend** takes the Spark Core
    requests and converts them into primitives that Kubernetes can understand. Additionally,
    it handles all resource requests and all communications with Kubernetes.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，虚线将Kubernetes与Spark分隔开。Spark Core负责获取新的执行器、推送新的配置、移除执行器等。**Kubernetes调度器后端**接受Spark
    Core的请求，并将其转换为Kubernetes可以理解的原语。此外，它处理所有资源请求和与Kubernetes的所有通信。
- en: 'Other services, such as a File Staging Server, can make your local files and
    JARs available to the Spark cluster, and the Spark shuffle service can store shuffle
    data for the dynamic allocation of resources; for example, it can enable elastically
    changing the number of executors in a particular stage. You can also extend the
    Kubernetes API to include custom or application-specific resources; for example,
    you can create dashboards to display the progress of jobs:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 其他服务，如文件暂存服务器，可以使您的本地文件和JAR文件可用于Spark集群，Spark洗牌服务可以存储动态分配资源的洗牌数据；例如，它可以实现弹性地改变特定阶段的执行器数量。您还可以扩展Kubernetes
    API以包括自定义或特定于应用程序的资源；例如，您可以创建仪表板来显示作业的进度。
- en: '![](img/00351.jpeg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00351.jpeg)'
- en: Kubernetes also provides useful administrative features to help manage clusters,
    for example, RBAC and namespace-level resource quotas, audit logging, monitoring
    node, pod, cluster-level metrics, and so on.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还提供了一些有用的管理功能，以帮助管理集群，例如RBAC和命名空间级别的资源配额、审计日志记录、监视节点、pod、集群级别的指标等。
- en: Summary
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we presented several Spark SQL-based application architectures
    for building highly-scalable applications. We explored the main concepts and challenges
    in batch processing and stream processing. We discussed the features of Spark
    SQL that can help in building robust ETL pipelines. We also presented some code
    towards building a scalable monitoring application. Additionally, we explored
    an efficient deployment technique for machine learning pipelines, and some basic
    concepts involved in using cluster managers such as Mesos and Kubernetes.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了几种基于Spark SQL的应用程序架构，用于构建高度可扩展的应用程序。我们探讨了批处理和流处理中的主要概念和挑战。我们讨论了Spark
    SQL的特性，可以帮助构建强大的ETL流水线。我们还介绍了一些构建可扩展监控应用程序的代码。此外，我们探讨了一种用于机器学习流水线的高效部署技术，以及使用Mesos和Kubernetes等集群管理器的一些基本概念。
- en: In conclusion, this book attempts to help you build a strong foundation in Spark
    SQL and Scala. However, there are still many areas that you can explore in greater
    depth to build deeper expertise. Depending on your specific domain, the nature
    of data and problems could vary widely and your approach to solving them would
    typically encompass one or more areas described in this book. However, in all
    cases EDA and data munging skills will be required, and the more you practice,
    the more proficient you will become at it. Try downloading and working with different
    types of data covering structured, semi-structured and unstructured data. Additionally,
    read the references mentioned in all the chapters to get deeper insights into
    how other data science practitioners approach problems. Reference Apache Spark
    site for latest releases of the software, and explore other machine learning algorithms
    you could use in your ML pipelines. Finally, topics such as deep learning and
    cost-based optimizations are still evolving in Spark, try to keep up with the
    developments in these areas as they will be key to solving many interesting problems
    in the near future.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本书试图帮助您在Spark SQL和Scala方面建立坚实的基础。然而，仍然有许多领域可以深入探索，以建立更深入的专业知识。根据您的特定领域，数据的性质和问题可能差异很大，您解决问题的方法通常会涵盖本书中描述的一个或多个领域。然而，在所有情况下，都需要EDA和数据整理技能，而您练习得越多，就会变得越熟练。尝试下载并处理不同类型的数据，包括结构化、半结构化和非结构化数据。此外，阅读各章节中提到的参考资料，以深入了解其他数据科学从业者如何解决问题。参考Apache
    Spark网站获取软件的最新版本，并探索您可以在ML流水线中使用的其他机器学习算法。最后，诸如深度学习和基于成本的优化等主题在Spark中仍在不断发展，尝试跟上这些领域的发展，因为它们将是解决未来许多有趣问题的关键。
