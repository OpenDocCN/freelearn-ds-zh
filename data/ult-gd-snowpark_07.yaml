- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Developing Data Science Projects with Snowpark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Snowpark开发数据科学项目
- en: The emergence of cloud technologies has ushered in a new era of possibilities.
    With the advent of Data Cloud, a robust platform that unifies data storage, processing,
    and analysis, data scientists have many opportunities to explore, analyze, and
    extract meaningful insights from vast datasets. In this intricate digital realm,
    the role of Snowpark, a cutting-edge data processing framework, becomes paramount.
    This chapter serves as an illuminating guide, delving deep into developing data
    science projects with Snowpark, unraveling its intricacies, and harnessing its
    potential to the fullest extent.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 云技术的出现引领了一个新的可能性时代。随着数据云的出现，一个统一数据存储、处理和分析的强大平台，数据科学家有许多机会探索、分析和从大量数据集中提取有意义的见解。在这个错综复杂的数字领域，Snowpark这一前沿数据处理框架的作用变得至关重要。本章作为一本指导性的指南，深入探讨了使用Snowpark开发数据科学项目，揭示了其复杂性，并最大限度地利用了其潜力。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Data science in Data Cloud
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学在数据云中
- en: Exploring and preparing data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索和准备数据
- en: Training **machine learning** (**ML**) models in Snowpark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Snowpark中训练**机器学习**（**ML**）模型
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, you’ll require an active Snowflake account and Python installed
    with Anaconda configured locally. You can sign up for a Snowflake Trial account
    at [https://signup.snowflake.com/](https://signup.snowflake.com/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本章，你需要一个活跃的Snowflake账户，并在本地安装了Anaconda的Python。你可以在[https://signup.snowflake.com/](https://signup.snowflake.com/)注册Snowflake试用账户。
- en: To configure Anaconda, follow [https://conda.io/projects/conda/en/latest/user-guide/getting-started.html](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置Anaconda，请遵循[https://conda.io/projects/conda/en/latest/user-guide/getting-started.html](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html)。
- en: In addition, to install and set up Python for VS Code, follow [https://code.visualstudio.com/docs/python/python-tutorial](https://code.visualstudio.com/docs/datascience/jupyter-notebooks).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，要安装和设置VS Code中的Python，请遵循[https://code.visualstudio.com/docs/python/python-tutorial](https://code.visualstudio.com/docs/python/python-tutorial)。
- en: To learn how to operate Jupyter Notebook in VS Code, go to [https://code.visualstudio.com/docs/datascience/jupyter-notebooks](https://code.visualstudio.com/docs/datascience/jupyter-notebooks).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何在VS Code中操作Jupyter Notebook，请访问[https://code.visualstudio.com/docs/datascience/jupyter-notebooks](https://code.visualstudio.com/docs/datascience/jupyter-notebooks)。
- en: The supporting materials for this chapter are available in this book’s GitHub
    repository at [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的支持材料可在本书的GitHub存储库中找到，网址为[https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark)。
- en: Data science in Data Cloud
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学在数据云中
- en: Data science transcends traditional boundaries in the Data Cloud ecosystem,
    offering a dynamic environment where data scientists can harness the power of
    distributed computing and advanced analytics. With the ability to seamlessly integrate
    various data sources, including structured and unstructured data, Data Cloud provides
    a data exploration and experimentation environment. We will start this section
    with a brief data science and ML refresher.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学在数据云生态系统中超越了传统边界，提供了一个数据科学家可以利用分布式计算和高级分析能力的动态环境。通过能够无缝集成各种数据源，包括结构化和非结构化数据，数据云提供了一个数据探索和实验环境。我们将从这个部分开始，简要回顾数据科学和机器学习。
- en: Data science and ML concepts
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据科学和机器学习概念
- en: Data science and ML have surged to the forefront of technological and business
    innovation, becoming integral components of decision-making, strategic planning,
    and product development across virtually all industries. The journey to their
    current popularity and influence is a testament to several factors, including
    advancements in technology, the explosion of data, and the increasing computational
    power available. This section will briefly discuss data science and ML concepts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学和机器学习已经迅速成为技术创新和商业创新的先锋，成为几乎所有行业中决策、战略规划和产品开发的核心组成部分。它们目前受欢迎和影响力的旅程是对多个因素的证明，包括技术的进步、数据的爆炸性增长以及可用计算能力的增加。本节将简要讨论数据科学和机器学习概念。
- en: Data science
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据科学
- en: Data science is a multidisciplinary field that relies on various software tools,
    algorithms, and ML principles to extract valuable insights from extensive datasets.
    Data scientists are pivotal in collecting, transforming, and converting data into
    predictive and prescriptive insights. By employing sophisticated techniques, data
    science uncovers hidden patterns and meaningful correlations within data, enabling
    businesses to act on informed decisions based on empirical evidence.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学是一个多学科领域，它依赖于各种软件工具、算法和机器学习（ML）原则，从大量数据集中提取有价值的见解。数据科学家在收集、转换和将数据转化为预测性和规范性见解方面发挥着关键作用。通过采用复杂技术，数据科学揭示了数据中的隐藏模式和有意义的关联，使企业能够基于经验证据做出明智的决策。
- en: Artificial intelligence
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工智能
- en: '**Artificial intelligence** (**AI**) encompasses the science and engineering
    of creating intelligent machines and brilliant computer programs capable of autonomously
    processing information and generating outcomes. AI systems are designed to solve
    intricate problems using logic and reasoning, similar to human cognitive processes.
    These systems operate autonomously, aiming to emulate human-like intelligence
    in decision-making and problem-solving tasks.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）包括创建能够自主处理信息和生成结果的智能机器和卓越计算机程序的科学和工程。人工智能系统旨在使用逻辑和推理解决复杂问题，类似于人类的认知过程。这些系统自主运行，旨在在决策和问题解决任务中模仿人类智能。'
- en: ML
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习
- en: ML, a subset of AI, involves specialized algorithms integrated into the data
    science workflow. These algorithms are meticulously crafted software programs
    that are designed to detect patterns, identify correlations, and pinpoint anomalies
    within data. ML algorithms excel at predicting outcomes based on existing data
    and continue to learn and improve their accuracy as they encounter new data and
    situations. Unlike humans, ML algorithms can process thousands of attributes and
    features, enabling the discovery of unique combinations and correlations in vast
    datasets. This capability makes ML indispensable for extracting valuable insights
    and predictions from extensive data collections.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML），作为人工智能（AI）的一个子集，涉及将专用算法集成到数据科学工作流程中。这些算法是精心设计的软件程序，旨在检测数据中的模式、识别相关性以及定位异常。机器学习算法擅长根据现有数据进行预测，并在遇到新数据和情境时不断学习和提高其准确性。与人类不同，机器学习算法可以处理数千个属性和特征，从而在庞大的数据集中发现独特的组合和相关性。这种能力使得机器学习在从大量数据集中提取有价值的见解和预测方面变得不可或缺。
- en: Now that we have the terminologies straightened out, we will discuss how the
    Data Cloud paradigm has helped the growth of data science and ML for organizations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经明确了术语，我们将讨论数据云范式如何帮助组织的数据科学和机器学习（ML）的发展。
- en: The Data Cloud paradigm
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据云范式
- en: Data science in the cloud represents a paradigm shift in how data-driven insights
    are derived and applied. In this innovative approach, data science processes,
    tools, and techniques are seamlessly integrated into the cloud, allowing organizations
    to leverage the power of scalable infrastructure and advanced analytics.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 云端数据科学代表了数据驱动见解的获取和应用方式的范式转变。在这种创新方法中，数据科学流程、工具和技术无缝集成到云端，使组织能够利用可扩展基础设施和高级分析的力量。
- en: At the heart of this paradigm lies Data Cloud, a dynamic ecosystem that transcends
    traditional data storage and processing constraints. The Data Cloud paradigm represents
    a seismic shift from conventional data silos, offering a unified platform where
    structured and unstructured data coalesce seamlessly. Through distributed computing,
    parallel processing, and robust data management, Data Cloud sets the stage for
    a data science revolution. The capabilities and tools that empower data scientists
    are seamlessly integrated and are designed to handle diverse data types and analytical
    workloads within Data Cloud. As such, Data Cloud offers various advantages for
    running data science and ML workloads.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这一范式的核心是数据云，这是一个超越传统数据存储和处理限制的动态生态系统。数据云范式代表了从传统数据孤岛到统一平台的地震式转变，其中结构化和非结构化数据无缝融合。通过分布式计算、并行处理和强大的数据管理，数据云为数据科学革命奠定了基础。赋予数据科学家能力的功能和工具无缝集成，并设计用于处理数据云内各种数据类型和分析工作负载。因此，数据云为运行数据科学和机器学习工作负载提供了各种优势。
- en: Advantages of Data Cloud for data science
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据云对数据科学的优势
- en: One of the key advantages of Snowflake’s Data Cloud is the ability to store
    and process vast amounts of data without the constraints of hardware limitations.
    It offers a scalable solution to handling vast volumes of data, enabling data
    scientists to work with extensive datasets without having to worry about computing
    or storage constraints. The cloud-based interface provides a collaborative and
    flexible environment for data scientists and analysts and comes with built-in
    collaboration features, version control, and support for popular data science
    libraries and frameworks through Snowpark.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake的Data Cloud的一个关键优势是能够在没有硬件限制的约束下存储和处理大量数据。它提供了一个可扩展的解决方案来处理大量数据，使数据科学家能够在不担心计算或存储限制的情况下处理大量数据集。基于云的界面为数据科学家和分析师提供了一个协作和灵活的环境，并内置了协作功能、版本控制和通过Snowpark支持流行的数据科学库和框架。
- en: Furthermore, Data Cloud offers a diverse ecosystem of services and resources
    tailored for data science tasks through managed services that simplify these processes,
    from data ingestion and preparation to ML model training and deployment. For instance,
    data pipelines can be automated using serverless computing, and ML models can
    be trained on powerful GPU instances, leading to faster experimentation and iteration.
    Data security and compliance are paramount in data science, especially when dealing
    with sensitive information, and Data Cloud provides different security measures,
    including encryption, access control, and row-level policies, ensuring that data
    scientists can work with sensitive data in a secure and compliant manner, adhering
    to industry regulations and organizational policies. The Snowpark framework is
    at the center of Snowflake’s Data Cloud to support these capabilities. The following
    section will discuss why Snowpark is used for data science and ML.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Data Cloud通过管理服务提供了一系列针对数据科学任务定制的服务和资源，简化了这些流程，从数据摄取和准备到机器学习模型训练和部署。例如，可以使用无服务器计算自动化数据管道，机器学习模型可以在强大的GPU实例上训练，从而实现更快的实验和迭代。数据安全和合规性在数据科学中至关重要，尤其是在处理敏感信息时，Data
    Cloud提供了不同的安全措施，包括加密、访问控制和行级策略，确保数据科学家可以以安全和合规的方式处理敏感数据，遵守行业法规和组织政策。Snowpark框架是Snowflake
    Data Cloud的核心，以支持这些功能。下一节将讨论为什么使用Snowpark进行数据科学和机器学习。
- en: Why Snowpark for data science and ML?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择Snowpark进行数据科学和机器学习？
- en: Snowpark offers unparalleled integration capabilities for data engineers, enabling
    seamless interaction with data stored in large volumes and diverse formats. Its
    versatile API facilitates effortless data exploration, transformation, and manipulation,
    laying a robust foundation for data science models and ML development and empowering
    data scientists to harness the full potential of their analytical workflows. Data
    science teams can now focus on their core tasks without the hassle of infrastructure
    or environment maintenance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark为数据工程师提供了无与伦比的集成能力，使他们能够无缝地与存储在大量和不同格式中的数据交互。其多功能的API简化了数据探索、转换和处理，为数据科学模型和机器学习开发奠定了坚实的基础，并赋予数据科学家充分利用其分析工作流程的潜力。数据科学团队现在可以专注于他们的核心任务，而无需烦恼于基础设施或环境维护。
- en: Snowpark excels in scalability and performance, which is crucial for enterprise
    data science and ML workloads; leveraging Snowflake’s distributed architecture
    to handle massive datasets and complex computations with remarkable efficiency
    and the ability to parallelize processing tasks and distribute workloads across
    multiple nodes ensures lightning-fast execution, even when dealing with petabytes
    of data. These features, combined with Snowflake’s automatic optimization features,
    allow data scientists to focus on their analyses without being burdened by infrastructure
    limitations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark在可扩展性和性能方面表现出色，这对于企业数据科学和机器学习工作负载至关重要；利用Snowflake的分布式架构以非凡的效率处理大量数据集和复杂计算，以及并行化处理任务和将工作负载分配到多个节点的能力，确保了闪电般的执行速度，即使在处理PB级数据时也是如此。这些功能与Snowflake的自动优化功能相结合，使数据科学家能够专注于他们的分析，而不受基础设施限制的负担。
- en: Snowpark offers a rich array of advanced analytics capabilities that are indispensable
    for data science and ML tasks. From statistical analysis to predictive modeling,
    geospatial analytics, or even data mining, it provides a comprehensive toolkit
    for data scientists to explore complex patterns and extract valuable insights.
    Its support for ML libraries and algorithms further amplifies its utility, enabling
    the development of sophisticated models for classification, regression, and clustering.
    With the rich features and functionalities mentioned previously, Snowpark provides
    many benefits for data science and ML workloads. In the next section, we will
    explore the world of the Snowpark ML library and its different functionalities.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark提供了一系列高级分析能力，对于数据科学和ML任务是不可或缺的。从统计分析到预测建模、地理空间分析，甚至数据挖掘，它为数据科学家提供了一套全面的工具包，以探索复杂模式并提取宝贵见解。其对ML库和算法的支持进一步增强了其效用，使得能够开发用于分类、回归和聚类的复杂模型。凭借之前提到的丰富特性和功能，Snowpark为数据科学和ML工作负载提供了许多好处。在下一节中，我们将探索Snowpark
    ML库及其不同的功能。
- en: Introduction to Snowpark ML
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Snowpark ML简介
- en: Snowpark constitutes a compendium of libraries and runtimes within Snowflake,
    facilitating the secure deployment and processing of non-SQL code by encompassing
    languages such as Python with the code execution that occurs server-side within
    the Snowflake infrastructure, all while leveraging a virtual warehouse. The newest
    addition to the Snowpark libraries is Snowpark ML. Snowpark ML represents a groundbreaking
    fusion of Snowflake’s powerful data processing capabilities and the transformative
    potential of ML. As the frontier of data science expands, Snowpark ML emerges
    as a cutting-edge framework that’s designed to empower data professionals to harness
    the full potential of their data within Snowflake’s cloud-based environment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark是Snowflake内部各种库和运行时的汇编，通过包含Python等语言，并在Snowflake基础设施中服务器端执行的代码执行，同时利用虚拟仓库，简化了非SQL代码的安全部署和处理。Snowpark库的最新增补是Snowpark
    ML。Snowpark ML代表了Snowflake强大的数据处理能力与ML变革潜力的突破性融合。随着数据科学的前沿不断扩展，Snowpark ML作为一个前沿框架出现，旨在赋予数据专业人士在Snowflake云环境中充分利用其数据的权力。
- en: 'At its core, Snowpark ML is engineered to facilitate seamless integration between
    Snowflake’s data processing capabilities and advanced ML techniques. With Snowpark
    ML, data scientists, analysts, and engineers can leverage familiar programming
    languages and libraries to develop sophisticated ML models directly within Snowflake.
    This integration eliminates the barriers between data storage, processing, and
    modeling, streamlining the end-to-end data science workflow. Snowpark ML catalyzes
    innovation, enabling data professionals to efficiently explore, transform, and
    model data. By bridging the gap between data processing and ML, Snowpark ML empowers
    organizations to make data-driven decisions, uncover valuable insights, and drive
    business growth in the digital age. The following figure shows the Snowpark ML
    framework:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，Snowpark ML被设计用来促进Snowflake的数据处理能力与高级ML技术之间的无缝集成。借助Snowpark ML，数据科学家、分析师和工程师可以直接在Snowflake中利用熟悉的编程语言和库来开发复杂的ML模型。这种集成消除了数据存储、处理和建模之间的障碍，简化了端到端的数据科学工作流程。Snowpark
    ML催化创新，使数据专业人士能够高效地探索、转换和建模数据。通过弥合数据处理与ML之间的差距，Snowpark ML赋予组织在数字时代基于数据做出决策、发现宝贵见解并推动业务增长的权力。以下图显示了Snowpark
    ML框架：
- en: '![Figure 5.1 – Snowpark ML](img/B19923_05_1.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – Snowpark ML](img/B19923_05_1.jpg)'
- en: Figure 5.1 – Snowpark ML
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – Snowpark ML
- en: The preceding architecture consists of various components that work cohesively
    together. We will look at each of these components in more detail in the next
    section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 上述架构由各种协同工作的组件组成。我们将在下一节中更详细地查看这些组件。
- en: Snowpark ML API
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Snowpark ML API
- en: 'Similar to Snowpark DataFrame, which helps operate with the data, Snowpark
    ML provides APIs as a Python library called `snowflake-ml` to support every stage
    of the ML development and deployment process, allowing support for pre-processing
    data and training, managing, and deploying ML models all within Snowflake:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与帮助操作数据的Snowpark DataFrame类似，Snowpark ML提供API作为名为`snowflake-ml`的Python库，以支持ML开发与部署的每个阶段，允许在Snowflake中支持预处理数据、训练、管理和部署ML模型：
- en: '![Figure 5.2 – Snowpark ML API](img/B19923_05_2.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – Snowpark ML API](img/B19923_05_2.jpg)'
- en: Figure 5.2 – Snowpark ML API
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – Snowpark ML API
- en: The Snowpark ML API consists of Snowpark ML modeling for developing and training
    the models and Snowpark ML Ops for monitoring and operating the model. The `snowflake.ml.modeling`
    module provides APIs for pre-processing, feature engineering, and model training
    based on familiar libraries, such as scikit-learn and XGBoost. The complete end-to-end
    ML experience can be done using Snowpark. We’ll cover this in the next section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark机器学习API包括用于开发和训练模型的Snowpark机器学习建模以及用于监控和操作模型的Snowpark机器学习操作。`snowflake.ml.modeling`模块提供了基于熟悉库（如scikit-learn和XGBoost）的API，用于预处理、特征工程和模型训练。完整的端到端机器学习体验可以使用Snowpark完成。我们将在下一节中介绍这一点。
- en: End-to-end ML with Snowpark
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Snowpark的端到端机器学习
- en: The quest for seamless, end-to-end ML solutions has become paramount, and Snowpark
    offers a comprehensive ecosystem for end-to-end ML. This section delves into the
    intricate world of leveraging Snowpark to craft end-to-end ML pipelines, from
    data ingestion and preprocessing to model development, training, and deployment,
    unveiling the seamless process of developing ML within Snowflake’s robust framework.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 寻求无缝、端到端的机器学习解决方案已成为当务之急，Snowpark为端到端机器学习提供了一个全面的生态系统。本节将深入探讨利用Snowpark构建端到端机器学习管道的复杂世界，从数据摄取和预处理到模型开发、训练和部署，揭示在Snowflake强大框架内开发机器学习的无缝流程。
- en: 'ML processes involve a systematic approach to solving complex problems through
    data processing. This typically includes stages such as defining the problem,
    collecting and preparing data, **exploratory data analysis** (**EDA**), feature
    engineering, model selection, training, evaluation, and deployment, with each
    operation being crucial and iterative. It allows data scientists to refine their
    approaches based on insights gained along the way. The process is often cyclical,
    with continuous iterations to improve models and predictions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习过程涉及通过数据处理解决复杂问题的系统方法。这通常包括定义问题、收集和准备数据、**探索性数据分析（EDA**）、特征工程、模型选择、训练、评估和部署等阶段，每个操作都至关重要且是迭代的。它允许数据科学家根据沿途获得的见解来完善他们的方法。这个过程通常是循环的，不断迭代以改进模型和预测：
- en: '![Figure 5.3 – End-to-end ML flow](img/B19923_05_3.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – 端到端机器学习流程](img/B19923_05_3.jpg)'
- en: Figure 5.3 – End-to-end ML flow
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 端到端机器学习流程
- en: We can broadly classify the ML stages as preparing and transforming, training
    and building the model, and interfering with the model to obtain prediction results.
    We will discuss each of these steps next.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将机器学习阶段大致分为准备和转换、训练和构建模型以及干扰模型以获得预测结果。我们将在下面讨论这些步骤。
- en: Preparing and transforming the data
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备和转换数据
- en: Raw data is often messy, containing missing values, outliers, and inconsistencies.
    Getting the correct data from multiple systems usually consumes most of the data
    scientist’s time. Snowflake solves this problem by providing a governed Data Cloud
    paradigm that supports all types of data and provides a unified place to instantly
    store and consume relevant data to unlock ML models’ power. The data preparation
    and transformation process involves EDA, cleaning, and processing, and ends with
    feature engineering. This step also consists of data engineering pipelines, which
    help apply data transformations to prepare the data for the next step. For data
    pre-processing, `snowflake.ml.modeling`, preprocessing, and Snowpark functions
    can be used to transform the data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据通常很杂乱，包含缺失值、异常值和不一致性。从多个系统中获取正确数据通常消耗了数据科学家的大部分时间。Snowflake通过提供支持所有类型数据的受管数据云范式来解决此问题，该范式提供了一个统一的地方来即时存储和消费相关数据，以释放机器学习模型的力量。数据准备和转换过程包括数据探索分析（EDA）、清理和处理，并以特征工程结束。这一步骤还包括数据工程管道，它有助于应用数据转换，为下一步准备数据。对于数据预处理，可以使用`snowflake.ml.modeling`、预处理和Snowpark函数来转换数据。
- en: EDA
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据探索分析（EDA）
- en: EDA is a critical step that involves preliminary investigations to understand
    the data’s structure, patterns, and trends as it helps uncover hidden patterns
    and guide feature selection. Data scientists and analysts collaborate closely
    with business stakeholders to define the specific questions that need to be answered
    or the problems that need to be solved, which guides them in selecting the relevant
    data. Through charts, graphs, and statistical summaries, data scientists can identify
    patterns, trends, correlations, and outliers within the dataset, all of which
    provide valuable insights into the data’s distribution and help them understand
    the data better to build feature selection.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: EDA（探索性数据分析）是一个关键步骤，涉及初步调查以了解数据的结构、模式和趋势，因为它有助于揭示隐藏的模式并指导特征选择。数据科学家和分析师与业务利益相关者紧密合作，定义需要回答的具体问题或需要解决的问题，这指导他们选择相关数据。通过图表、图形和统计摘要，数据科学家可以在数据集中识别模式、趋势、相关性和异常值，所有这些都为数据的分布提供了宝贵的见解，并帮助他们更好地理解数据以进行特征选择。
- en: Data cleaning and preprocessing
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据清洗和预处理
- en: Data cleaning involves handling missing data, correcting errors, and ensuring
    consistency. The data is suitable for training the model through preprocessing
    techniques such as normalization, scaling, and transformations, along with various
    sampling techniques that are applied to evaluate a subset of the data, providing
    insights into its richness and variability.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗涉及处理缺失数据、纠正错误和确保一致性。通过预处理技术，如归一化、缩放和变换，以及各种采样技术，数据适用于训练模型，这些采样技术应用于评估数据子集，提供对其丰富性和变异性的见解。
- en: Feature engineering
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征工程
- en: Feature engineering involves creating new features or modifying existing ones
    to enhance the performance of ML models. It requires domain expertise to identify
    relevant features that can improve predictive accuracy. Performing feature engineering
    on the centralized data in Snowflake accelerates model development, reduces costs,
    and enables the reuse of new features. Some techniques, such as creating interaction
    terms and transforming variables, extract meaningful information from raw data,
    making it more informative for modeling.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程涉及创建新特征或修改现有特征以增强机器学习模型的性能。这需要领域专业知识来识别可以提高预测准确性的相关特征。在Snowflake中对集中数据进行特征工程可以加速模型开发，降低成本，并使新特征得以重复使用。一些技术，如创建交互项和变换变量，可以从原始数据中提取有意义的信息，使其对建模更具信息量。
- en: Training and building the model
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练和构建模型
- en: Once the data is ready and the features have been built, the next step is to
    train and develop the model. In this stage, the data scientist trains various
    models, such as regression, classification, clustering, or deep learning, depending
    on the nature of the problem, by passing a subset of the data, or training set,
    through the modeling function to derive a predictive function. The model is developed
    using statistical methods for hypothesis testing and inferential statistics. Advanced
    techniques, such as ensemble methods, neural networks, and natural language processing,
    are also used, depending on the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据准备就绪且特征已构建，下一步就是训练和开发模型。在这个阶段，数据科学家根据问题的性质，通过将数据子集或训练集传递给建模函数来推导预测函数，训练各种模型，如回归、分类、聚类或深度学习。模型的开发使用假设检验和推断统计的统计方法。根据数据，还使用了高级技术，如集成方法、神经网络和自然语言处理。
- en: Once the model has been developed, it’s tested on data that wasn’t part of the
    training set to determine its effectiveness, which is usually measured in terms
    of its predictive strength and robustness, and the model is optimized with hyperparameter
    tuning. Cross-validation techniques optimize the model’s performance, ensuring
    accurate predictions and valuable insights.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型已经开发，它将在训练集之外的数据上进行测试，以确定其有效性，这通常以预测强度和鲁棒性来衡量，并通过超参数调整对模型进行优化。交叉验证技术优化模型性能，确保准确预测和有价值的见解。
- en: This combination of steps enables data scientists to conduct in-depth feature
    engineering, tune hyperparameters, and iteratively create and assess ML models.
    Intuitions become accurate predictions as data scientists experiment with various
    algorithms, evaluating the performance of each model and adjusting parameters
    on their chosen model to optimize the code for their specific datasets. `snowflake.ml.modeling`
    can be used for training by utilizing the `fit()` method for an algorithm such
    as XGBoost.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤的组合使数据科学家能够进行深入的特征工程，调整超参数，并迭代创建和评估机器学习模型。随着数据科学家对各种算法进行实验，评估每个模型的性能，并在所选模型上调整参数以优化针对其特定数据集的代码，直觉变成了准确的预测。`snowflake.ml.modeling`
    可以通过使用 XGBoost 等算法的 `fit()` 方法进行训练。
- en: Inference
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理
- en: Once the models have been trained, Snowpark ML supports their seamless deployment
    and inference. Models can be deployed for inference, enabling organizations to
    make data-driven decisions based on predictive insights. Snowpark ML has a model
    registry to manage and organize Snowpark models throughout their life cycle. The
    model registry supports versioning of the models and stores metadata information
    about the models, hyperparameters, and evaluation metrics, facilitating experimentation
    and model comparison. It also supports model monitoring and auditing and aids
    in collaboration between data scientists working on the model. The model registry
    is part of Snowpark MLOps and can be accessed through `snowpark.ml.registry`.
    The pipelines can be orchestrated using Snowflake Tasks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被训练，Snowpark ML 支持它们的无缝部署和推理。模型可以用于推理，使组织能够根据预测洞察力做出数据驱动的决策。Snowpark ML
    有一个模型注册表来管理和组织 Snowpark 模型在其生命周期中的所有阶段。模型注册表支持模型的版本控制，并存储有关模型、超参数和评估指标元数据信息，从而促进实验和模型比较。它还支持模型监控和审计，并有助于在从事模型工作的数据科学家之间进行协作。模型注册表是
    Snowpark MLOps 的一部分，可以通过 `snowpark.ml.registry` 访问。可以使用 Snowflake Tasks 来编排管道。
- en: Now that we have established the foundations of Snowpark ML, its place in ML,
    and how Snowpark supports data science workloads, we will dive deep into the complete
    data science scenario with Snowpark. The following section will focus on exploring
    and preparing the data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了 Snowpark ML 的基础、其在机器学习中的位置以及 Snowpark 如何支持数据科学工作负载，我们将深入探讨 Snowpark
    的完整数据科学场景。下一节将专注于探索和准备数据。
- en: A note on data engineering
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据工程的说明
- en: In the next section, we’ll conduct exploration, transformation, and feature
    engineering using Snowpark Python and pandas. As we proceed to build models with
    SnowparkML, we will incorporate some of the steps discussed earlier in this section.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用 Snowpark Python 和 pandas 进行探索、转换和特征工程。随着我们使用 SnowparkML 构建模型，我们将结合本节前面讨论的一些步骤。
- en: Exploring and preparing data
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索和准备数据
- en: In the first step of the ML process, we must explore and prepare the data in
    Snowflake using Snowpark to make it available for training the ML models. We will
    work with the Bike Sharing dataset from Kaggle, which offers an hourly record
    of rental data for 2 years. The primary objective is to forecast the number of
    bikes rented each hour for a specific timeframe based solely on the information
    available before the rental period. In essence, the model will harness the power
    of historical data to predict future bike rental patterns using Snowpark. More
    information about the particular dataset has been provided in the respective GitHub
    repository ([https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习过程的第一个步骤中，我们必须使用 Snowpark 在 Snowflake 中探索和准备数据，使其可用于训练机器学习模型。我们将使用 Kaggle
    的共享单车数据集，该数据集提供了两年内租赁数据的每小时记录。主要目标是仅基于租赁期之前的信息，预测特定时间段内每小时租赁的自行车数量。本质上，该模型将利用历史数据，通过
    Snowpark 预测未来的自行车租赁模式。有关特定数据集的更多信息已在相应的 GitHub 仓库中提供（[https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark)）。
- en: 'Data exploration allows us to dissect the data to uncover intricate details
    that might otherwise stay hidden, acting as the foundation for our entire analysis.
    We will start the process by loading the dataset into a Snowpark DataFrame:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索使我们能够剖析数据，揭示可能否则隐藏的复杂细节，这为我们整个分析奠定了基础。我们将通过将数据集加载到 Snowpark DataFrame 中开始这个过程：
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the data has been successfully loaded, the subsequent imperative is to
    gain a comprehensive understanding of the dataset’s scale:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据成功加载，接下来的关键任务是全面了解数据集的规模：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Fortunately, Snowpark provides functions specifically designed to facilitate
    this critical task:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Snowpark提供了专门设计来简化这一关键任务的函数：
- en: '![Figure 5.4 – Total number of columns](img/B19923_05_4.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 – 列总数](img/B19923_05_4.jpg)'
- en: Figure 5.4 – Total number of columns
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 列总数
- en: 'Now that we know the scale of the data, let’s get a sense of it by looking
    at a few rows of the dataset:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了数据的大小，让我们通过查看数据集的几行来对其有一个直观的了解：
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This returns the two rows from the data for analysis:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回用于分析的数据的两行：
- en: '![Figure 5.5 – Two rows of data](img/B19923_05_5.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – 两行数据](img/B19923_05_5.jpg)'
- en: Figure 5.5 – Two rows of data
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 两行数据
- en: 'As depicted in the preceding figure, the `COUNT` column is a straightforward
    aggregation of `CASUAL` and `REGISTERED`. In data science, these types of variables
    are commonly referred to as “leakage variables.” When we construct our models,
    we’ll delve deeper into strategies for managing these variables. Date columns
    consistently present an intriguing and complex category to grapple with. Within
    this dataset, there is potential to create valuable new features derived from
    the `DATETIME` column, which could significantly influence our response variables.
    Before we start with data cleansing and the feature engineering process, let’s
    see the column type to understand and make more informed decisions:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，`COUNT`列是`CASUAL`和`REGISTERED`的直接聚合。在数据科学中，这类变量通常被称为“泄漏变量”。当我们构建模型时，我们将深入探讨管理这些变量的策略。日期列始终是一个引人入胜且复杂的类别，需要我们应对。在此数据集中，我们可以从`DATETIME`列中创建有价值的新的特征，这可能会显著影响我们的响应变量。在我们开始数据清洗和特征工程过程之前，让我们查看列类型，以便更好地理解和做出更明智的决策：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will give us the schema and the data types for each field so that we can
    understand the data better:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出每个字段的架构和数据类型，以便我们更好地理解数据：
- en: '![Figure 5.6 – Schema information](img/B19923_05_6.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – 架构信息](img/B19923_05_6.jpg)'
- en: Figure 5.6 – Schema information
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 架构信息
- en: Now that we are equipped with basic information about the data, let’s start
    finding the missing values in the data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经具备了关于数据的基本信息，让我们开始寻找数据中的缺失值。
- en: Missing value analysis
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺失值分析
- en: 'Addressing missing values is a fundamental preprocessing step in ML. Incomplete
    data can disrupt model training and hinder predictive accuracy, potentially leading
    to erroneous conclusions or suboptimal performance. By systematically imputing
    or filling these gaps, we can bolster the integrity of our dataset, providing
    ML algorithms with a more comprehensive and coherent dataset for more robust and
    reliable analyses and predictions. This practice is akin to affording our models
    the necessary information to make sound, data-driven decisions. Let’s check for
    any missing values in our dataset:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，处理缺失值是一个基本的预处理步骤。不完整的数据可能会干扰模型训练并阻碍预测准确性，可能导致错误的结论或次优性能。通过系统地填充或填补这些空白，我们可以增强数据集的完整性，为机器学习算法提供更全面和一致的数据集，以便进行更稳健和可靠的分析和预测。这种做法类似于为我们提供模型所需的信息，以便做出明智的数据驱动决策。让我们检查数据集中是否存在任何缺失值：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code helps us find out whether any values are empty or missing:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码帮助我们找出是否有任何值为空或缺失：
- en: '![Figure 5.7  – Missing value analysis](img/B19923_05_7.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7 – 缺失值分析](img/B19923_05_7.jpg)'
- en: Figure 5.7 – Missing value analysis
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 缺失值分析
- en: 'Our initial examination of missing values in the column shows no missing values
    in our dataset. However, a closer examination reveals the presence of numerous
    0s within the `WINDSPEED` column, which is indicative of potentially missing values.
    Logically, windspeed cannot equate to zero, implying that each `0` within the
    column signifies a missing value:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对列中缺失值的初步检查显示，我们的数据集中没有缺失值。然而，更仔细的检查揭示了`WINDSPEED`列中存在许多0，这表明可能存在缺失值。从逻辑上讲，风速不可能等于零，这意味着列中的每个`0`都表示一个缺失值：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will print out the following output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '![Figure 5.8 – Output value](img/B19923_05_8.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8 – 输出值](img/B19923_05_8.jpg)'
- en: Figure 5.8 – Output value
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – 输出值
- en: 'We can see that there are `1313` values in the `WINDSPEED` column. With this
    column harboring missing data, the subsequent challenge is determining an effective
    strategy for imputing these missing values. As is widely acknowledged, various
    methods exist for addressing missing data within a column. In our case, we’ll
    employ a straightforward imputation, substituting the 0s with the mean value of
    the column:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `WINDSPEED` 列中有 `1313` 个值。由于该列包含缺失数据，接下来的挑战是确定一个有效的策略来填充这些缺失值。正如广泛认可的，存在各种方法来解决列中的缺失数据。在我们的案例中，我们将采用一种简单的填充方法，用列的平均值替换
    0：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding code replaces the 0s with the mean value of the column:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码用列的平均值替换了 0：
- en: '![Figure 5.9 – Pre-processed data](img/B19923_05_9.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 预处理数据](img/B19923_05_9.jpg)'
- en: Figure 5.9 – Pre-processed data
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 预处理数据
- en: This concludes our preprocessing journey. Next, we’ll perform outlier analysis.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们的预处理之旅结束。接下来，我们将执行异常值分析。
- en: Outlier analysis
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常值分析
- en: The process of detecting and removing outliers is pivotal in enhancing model
    accuracy and robustness. Outliers are data points that significantly deviate from
    most datasets, often stemming from errors, anomalies, or rare events. These aberrations
    can unduly influence model training, leading to skewed predictions or reduced
    generalization capabilities. By identifying and eliminating outliers, we can improve
    the quality and reliability of our models and ensure that they are better equipped
    to discern meaningful patterns within the data. This practice fosters more accurate
    predictions and a higher level of resilience, ultimately contributing to the overall
    success of ML endeavors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 检测和移除异常值是提高模型准确性和鲁棒性的关键步骤。异常值是与大多数数据集显著偏离的数据点，通常源于错误、异常或罕见事件。这些偏差可能会不当地影响模型训练，导致预测偏差或降低泛化能力。通过识别和消除异常值，我们可以提高模型的质量和可靠性，并确保它们能够更好地从数据中辨别出有意义的模式。这种做法促进了更准确的预测和更高的弹性，最终有助于机器学习项目的整体成功。
- en: 'We will be transforming the DataFrame into a pandas DataFrame so that we can
    conduct insightful analyses, including constructing visualizations to extract
    meaningful patterns. Our initial focus is on the `COUNT` column as the response
    variable. Before model development, it is imperative to ascertain whether the
    `COUNT` column contains any outlier values:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把 DataFrame 转换为 pandas DataFrame，以便我们可以进行有洞察力的分析，包括构建可视化来提取有意义的模式。我们的初步重点是
    `COUNT` 列作为响应变量。在模型开发之前，确定 `COUNT` 列是否包含任何异常值至关重要：
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code generates a plot using the `seaborn` and the `matplotlib`
    library to help us find the outliers:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码使用 `seaborn` 和 `matplotlib` 库生成一个图表，帮助我们找到异常值：
- en: '![Figure 5.10 – Outlier plot](img/B19923_05_10.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – 异常值图](img/B19923_05_10.jpg)'
- en: Figure 5.10 – Outlier plot
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 异常值图
- en: 'As we can see, the `COUNT` column exhibits outlier data points that can potentially
    negatively impact model performance if they’re not adequately addressed. Mitigating
    outliers is a critical preprocessing step. One widely adopted approach involves
    removing data points that lie beyond a predefined threshold or permissible range,
    as outlined here:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`COUNT` 列显示出可能对模型性能产生负面影响且未得到充分处理的异常数据点。缓解异常值是一个关键的前处理步骤。一种广泛采用的方法是移除位于预定义阈值或允许范围之外的数据点，如下所述：
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code uses the Snowpark library to analyze a dataset stored in
    `df_table`. It calculates the mean (average) and standard deviation (a measure
    of data spread) of the `''count''` column in the dataset. Then, it identifies
    and removes outliers from the dataset. Outliers are data points that significantly
    differ from the average. In this case, it defines outliers as data points more
    than three times the standard deviation away from the mean. After identifying
    these outliers, it displays the dataset without the outlier values, using `df_without_outlier.show()`
    to help with further analysis:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码使用 Snowpark 库分析存储在 `df_table` 中的数据集。它计算数据集中 `'count'` 列的平均值（平均）和标准差（数据分散程度的度量）。然后，它识别并从数据集中移除异常值。异常值是与平均值显著不同的数据点。在这种情况下，它将异常值定义为与平均值相差超过三个标准差的数据点。在识别这些异常值之后，它使用
    `df_without_outlier.show()` 显示没有异常值的数据集，以帮助进行进一步分析：
- en: '![Figure 5.11 – Outliers removed](img/B19923_05_11.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – 移除异常值](img/B19923_05_11.jpg)'
- en: Figure 5.11 – Outliers removed
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 移除异常值
- en: Now that we have taken care of the outliers, we can perform correlation analysis.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经处理了异常值，我们可以进行相关性分析。
- en: Correlation analysis
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关性分析
- en: Identifying correlations among variables is of paramount importance for several
    vital reasons. Correlations provide valuable insights into how different features
    in the dataset relate to each other. By understanding these relationships, ML
    models can make more informed predictions as they leverage the strength and direction
    of correlations to uncover patterns and dependencies. Moreover, identifying and
    quantifying correlations aids feature selection, where irrelevant or highly correlated
    features can be excluded to enhance model efficiency and interpretability. It
    also helps identify potential multicollinearity issues, where two or more features
    are highly correlated, leading to unstable model coefficients. Recognizing and
    harnessing correlations empowers ML models to make better predictions and yield
    more robust results, making it a fundamental aspect of modeling.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 识别变量之间的相关性对于几个至关重要的原因至关重要。相关性提供了关于数据集中不同特征如何相互关联的宝贵见解。通过理解这些关系，机器学习模型可以做出更明智的预测，因为它们利用相关性的强度和方向来揭示模式和依赖关系。此外，识别和量化相关性有助于特征选择，其中不相关或高度相关的特征可以被排除，以提高模型效率和可解释性。它还有助于识别潜在的共线性问题，其中两个或更多特征高度相关，导致模型系数不稳定。认识和利用相关性使机器学习模型能够做出更好的预测并产生更稳健的结果，使其成为建模的基本方面。
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding code generates the correlation matrix as a heatmap visualization:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码生成了相关矩阵的热图可视化：
- en: '![Figure 5.12 – Correlation matrix heatmap](img/B19923_05_12_V.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12 – 相关矩阵热图](img/B19923_05_12_V.jpg)'
- en: Figure 5.12 – Correlation matrix heatmap
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – 相关矩阵热图
- en: This heatmap visualization reveals a substantial correlation between the `TEMP`
    and `ATEMP` variables, signifying a condition known as multicollinearity. Multicollinearity
    occurs when two or more predictors in a model are highly correlated, distorting
    the model’s interpretability and stability. To mitigate this issue and ensure
    the reliability of our analysis, we have opted to retain the `TEMP` variable while
    removing `ATEMP` from consideration in our subsequent modeling endeavors. This
    strategic decision is made to maintain model robustness and effectively capture
    the essence of the data without the confounding effects of multicollinearity.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此热图可视化揭示了`TEMP`和`ATEMP`变量之间存在显著的关联性，这表明了一种称为共线性的条件。当模型中的两个或多个预测变量高度相关时，就会发生共线性，这会扭曲模型的解释性和稳定性。为了减轻这个问题并确保我们分析的可信度，我们选择保留`TEMP`变量，同时在后续的建模工作中排除`ATEMP`的考虑。这个战略决策是为了保持模型稳健性并有效地捕捉数据的本质，而不受共线性干扰的影响。
- en: Leakage variables
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 泄露变量
- en: '**Leakage variables** in data science inadvertently include information that
    would not be available during prediction or decision-making in a real-world scenario.
    Eliminating them is crucial because using leakage variables can lead to overly
    optimistic model performance and unreliable results. It’s essential to detect
    and exclude these variables during data preprocessing to ensure that our models
    make predictions based on the same information that would be accessible. By doing
    so, we prevent the risk of building models that work well on historical data but
    fail to perform in real-world situations, which is a crucial goal in data science.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中的**泄露变量**无意中包含了在现实世界的预测或决策过程中不可用的信息。消除它们至关重要，因为使用泄露变量可能导致模型性能过于乐观和结果不可靠。在数据预处理期间检测和排除这些变量是至关重要的，以确保我们的模型基于可访问的相同信息进行预测。通过这样做，我们防止了构建在历史数据上表现良好但在现实世界情况下无法执行的模型的风险，这是数据科学的一个关键目标。
- en: As mentioned previously, the `CASUAL`, `REGISTERED`, and `COUNT` columns exhibit
    high collinearity, with `COUNT` being an explicit summation of `CASUAL` and `REGISTERED`.
    This redundancy renders the inclusion of all three variables undesirable, resulting
    in a leakage variable situation. To preserve the integrity of our model-building
    process, we shall eliminate `CASUAL` and `REGISTERED` from our feature set, thereby
    mitigating any potential confounding effects and ensuring the model’s ability
    to make predictions based on the most relevant and non-redundant information.
    The next step is to perform feature engineering with the prepared data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`CASUAL`、`REGISTERED`和`COUNT`列表现出高度共线性，其中`COUNT`是`CASUAL`和`REGISTERED`的显式求和。这种冗余使得包含所有三个变量不可取，导致泄漏变量情况。为了保持我们的模型构建过程的完整性，我们将从特征集中消除`CASUAL`和`REGISTERED`，从而减轻任何潜在的混淆效应，并确保模型能够基于最相关和非冗余的信息进行预测。下一步是对准备好的数据进行特征工程。
- en: Feature engineering
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: '**Feature engineering** in ML is like crafting the perfect tool for a specific
    job. It involves selecting, transforming, or creating new features (variables)
    from the available data to make it more suitable for ML algorithms. This process
    is crucial because it helps the models better understand the patterns and relationships
    in the data, leading to improved predictions and insights. By carefully engineering
    features, we can uncover hidden information, reduce noise, and enhance the model’s
    performance, making it a vital step in building effective and accurate ML systems.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，**特征工程**就像为特定工作制作完美的工具。它涉及从可用数据中选择、转换或创建新的特征（变量），使其更适合机器学习算法。这个过程至关重要，因为它有助于模型更好地理解数据中的模式和关系，从而提高预测和洞察力。通过精心设计特征，我们可以揭示隐藏信息，减少噪声，并提高模型性能，使其在构建有效和准确的机器学习系统中成为关键步骤。
- en: 'Analyzing the data shows that the `DATETIME` column is a promising candidate
    for feature engineering within this dataset. Given the dependency of the predictive
    outcome on temporal factors such as the time of day and day of the week, deriving
    time-related features assumes paramount significance. Extracting these temporal
    features is pivotal as it enhances the model’s performance and elevates the overall
    predictive accuracy by capturing essential nuances about the dataset’s material
    characteristics:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 分析数据表明，`DATETIME`列是此数据集中特征工程的有希望的候选者。鉴于预测结果依赖于时间因素，如一天中的时间和一周中的某一天，推导出时间相关特征具有至关重要的意义。提取这些时间特征至关重要，因为它可以增强模型性能，并通过捕捉数据集材料特性的关键细微差别来提高整体预测准确性：
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code enriches a DataFrame by creating new columns that capture
    specific time and date details from the `DATETIME` column:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码通过从`DATETIME`列中提取特定的时间和日期细节来丰富DataFrame，创建新的列：
- en: '![Figure 5.13 – DATETIME data](img/B19923_05_13.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图5.13 – DATETIME数据](img/B19923_05_13.jpg)'
- en: Figure 5.13 – DATETIME data
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 – DATETIME数据
- en: The `hour` column tells us the hour of the day, the `month` column identifies
    the month, the `date` column extracts the date itself, and the `weekday` column
    signifies the day of the week. These additional columns provide a more comprehensive
    view of the time-related information within the dataset, enhancing its potential
    for in-depth analysis and ML applications.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`hour`列告诉我们一天中的小时，`month`列识别月份，`date`列提取日期本身，而`weekday`列表示一周中的某一天。这些额外的列提供了对数据集中时间相关信息的更全面视角，增强了其深入分析和机器学习应用潜力。'
- en: This step concludes our data preparation and exploration journey. The following
    section will use this prepared data to build and train our model using Snowpark.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步结束了我们的数据准备和探索之旅。下一节将使用这些准备好的数据，使用Snowpark构建和训练我们的模型。
- en: A note on the model-building process
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模型构建过程的说明
- en: In our model-building process, we won’t be incorporating all the steps we’ve
    discussed thus far. Instead, we’ll focus on two significant transformations to
    showcase Snowpark ML pipelines. Additionally, the accompanying notebook (**chapter_5.ipynb**)
    illustrates model building using Python’s scikit-learn library and how to call
    them as stored procedures. This will allow you to compare and contrast how the
    model-building process is simplified through Snowpark ML. To follow through the
    chapter, you can skip the model building process using the scikit-learn section
    and directly go to the Snowpark ML section in the notebook.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型构建过程中，我们不会包含迄今为止讨论的所有步骤。相反，我们将专注于两个重要的转换，以展示Snowpark ML管道。此外，附带的笔记本（**chapter_5.ipynb**）展示了使用Python的scikit-learn库进行模型构建以及如何将其作为存储过程调用。这将使您能够比较和对比通过Snowpark
    ML简化的模型构建过程。为了继续本章内容，您可以跳过使用scikit-learn部分进行模型构建的过程，直接进入笔记本中的Snowpark ML部分。
- en: Training ML models in Snowpark
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Snowpark中训练ML模型
- en: 'Now that we have prepared our dataset, the pinnacle of our journey involves
    the model-building process, for which we will be leveraging the power of Snowpark
    ML. Snowpark ML emerges as a recent addition to the Snowpark arsenal, strategically
    deployed to streamline the intricacies of the model-building process. Its elegance
    becomes apparent when we engage in a comparative exploration of the model-building
    procedure through the novel ML library. We will start by developing the pipeline
    that we’ll use to train the model using the data we prepared previously:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了我们的数据集，我们旅程的巅峰就是模型构建过程，我们将利用Snowpark ML的力量。Snowpark ML作为Snowpark工具箱中的最新成员，战略性地部署以简化模型构建过程的复杂性。当我们通过新颖的ML库进行模型构建过程的比较探索时，其优雅性变得显而易见。我们将首先开发我们将使用之前准备的数据来训练模型的管道：
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code creates a preprocessing pipeline for the dataset by using
    various Snowpark ML functions. The `preprocessing` and `pipeline` modules are
    imported as these are essential for developing and training the model:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码通过使用各种Snowpark ML函数为数据集创建预处理管道。导入`preprocessing`和`pipeline`模块，因为这些是开发和训练模型所必需的：
- en: '![Figure 5.14 – Transformed data](img/B19923_05_14.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图5.14 – 转换后的数据](img/B19923_05_14.jpg)'
- en: Figure 5.14 – Transformed data
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 – 转换后的数据
- en: 'The pipeline includes ordinal encoding for categorical columns (`SEASON` and
    `WEATHER`) and min-max scaling for numerical columns (`TEMP`). The pipeline is
    saved into the stage using the `joblib` library, which can be utilized for consistent
    preprocessing in future analyses. Now that we have the pipeline code ready, we
    will build the features that are required for the model:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 管道包括对分类列（`SEASON`和`WEATHER`）进行顺序编码，对数值列（`TEMP`）进行min-max缩放。该管道使用`joblib`库保存到阶段，可用于未来分析的持续预处理。现在我们已经准备好了管道代码，我们将构建模型所需的特征：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code defines lists representing categorical columns, one-hot
    encoded categorical columns, and columns for min-max scaling. It also specifies
    a feature list, label columns, and output columns for an ML model. The `preprocessing_pipeline.joblib`
    file is loaded and assumed to contain a previously saved preprocessing pipeline.
    These elements collectively prepare the necessary data and configurations for
    subsequent ML tasks, ensuring consistent handling of categorical variables, feature
    scaling, and model predictions based on the pre-established pipeline. We will
    now split the data into training and testing sets:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码定义了表示分类列的列表、单热编码的分类列和用于min-max缩放的列。它还指定了ML模型的特征列表、标签列和输出列。加载了`preprocessing_pipeline.joblib`文件，并假设它包含之前保存的预处理管道。这些元素共同准备后续ML任务所需的数据和配置，确保对分类变量、特征缩放和基于预先建立的管道进行模型预测的一致处理。我们现在将数据分为训练集和测试集：
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code divides the dataset into training (70%) and testing (30%)
    sets using a random split. It applies the previously defined preprocessing pipeline
    to transform both sets, displaying the transformed training and testing datasets
    and ensuring consistent preprocessing for model training and evaluation. The output
    shows the different training and testing data:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用随机分割将数据集分为训练集（70%）和测试集（30%）。它应用之前定义的预处理管道来转换这两个集合，显示转换后的训练集和测试集，并确保模型训练和评估的预处理一致性。输出显示了不同的训练和测试数据：
- en: '![Figure 5.15 – Training and testing dataset](img/B19923_05_15.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图5.15 – 训练和测试数据集](img/B19923_05_15.jpg)'
- en: Figure 5.15 – Training and testing dataset
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 训练和测试数据集
- en: 'Next, we’ll train the model with the training data:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用训练数据训练模型：
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `LinearRegression` class defines the model, specifying the input columns
    (categorical columns after one-hot encoding and additional features), label columns
    (the target variable – that is, `COUNT`), and output columns for predictions.
    The model is trained on the transformed training dataset using `fit`, and then
    predictions are generated for the transformed testing dataset using `predict`.
    The resulting predictions are displayed, assessing the model’s performance on
    the test data:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`LinearRegression` 类定义了模型，指定了输入列（一热编码后的分类列和附加特征）、标签列（目标变量，即 `COUNT`）和预测输出列。模型使用
    `fit` 在转换后的训练数据集上训练，然后使用 `predict` 对转换后的测试数据集生成预测。显示的结果预测了模型在测试数据上的性能：'
- en: '![Figure 5.16 – Predicted output](img/B19923_05_16.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.16 – 预测输出](img/B19923_05_16.jpg)'
- en: Figure 5.16 – Predicted output
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – 预测输出
- en: 'The next step is to calculate various performance metrics to evaluate the accuracy
    of the linear regression model’s predictions:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是计算各种性能指标以评估线性回归模型预测的准确性：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code calculates various performance metrics to assess the accuracy
    of the linear regression model’s predictions. Metrics such as mean squared error,
    explained variance score, mean absolute error, mean fundamental percentage error,
    d2 definitive error score, and d2 pinball score are computed based on the actual
    (`COUNT`) and predicted (`PREDICTED_COUNT`) values stored in the `result` DataFrame:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码计算了各种性能指标以评估线性回归模型预测的准确性。基于存储在 `result` DataFrame 中的实际（`COUNT`）和预测（`PREDICTED_COUNT`）值，计算了均方误差、解释方差得分、平均绝对误差、平均基本百分比误差、d2
    确定性误差得分和 d2 针球得分等指标：
- en: '![Figure 5.17 – Performance metrics](img/B19923_05_17.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.17 – 性能指标](img/B19923_05_17.jpg)'
- en: Figure 5.17 – Performance metrics
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – 性能指标
- en: These performance metrics provide a comprehensive evaluation of the model’s
    performance across different aspects of prediction accuracy.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这些性能指标全面评估了模型在预测准确性不同方面的性能。
- en: Model results and efficiency
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 模型结果和效率
- en: The presented model metrics might need to showcase more exceptional results.
    It’s crucial to emphasize that the primary objective of this case study is to
    elucidate the model-building process and highlight the facilitative role of Snowpark
    ML. The focus of this chapter has been on illustrating the construction of a linear
    regression model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 呈现的模型指标可能需要展示更出色的结果。强调本案例研究的主要目标是阐明模型构建过程并突出 Snowpark ML 的促进作用。本章的重点在于说明线性回归模型的构建。
- en: The efficiency of Snowpark ML
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Snowpark ML 的效率
- en: In delving into the intricacies of the model-building process facilitated by
    Snowpark ML, the initial standout feature is its well-thought-out design. A notable
    departure from the conventional approach is evident as Snowpark ML closely mirrors
    the streamlined methodology found in scikit-learn. A significant advantage is
    eliminating the need to create separate **user-defined functions** (**UDFs**)
    and stored procedures, streamlining the entire model-building workflow.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探究由 Snowpark ML 便利的模型构建过程的复杂性时，其精心设计的初始特点尤为突出。与传统方法的一个显著不同之处在于 Snowpark ML
    与 scikit-learn 中发现的简化方法非常相似。一个显著的优势是消除了创建单独的**用户定义函数**（**UDFs**）和存储过程的必要性，从而简化了整个模型构建工作流程。
- en: It’s crucial to recognize that Snowpark ML seamlessly integrates with scikit-learn
    while adhering to similar conventions in the model construction process. A noteworthy
    distinction is a prerequisite in scikit-learn for data to be passed as a pandas
    DataFrame. Consequently, the Snowflake table must be converted into a pandas DataFrame
    before you can initiate the model-building phase. However, it’s imperative to
    be mindful of potential memory constraints, especially when dealing with substantial
    datasets. Converting a large table into a pandas DataFrame demands a significant
    amount of memory since the entire dataset is loaded into memory.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到 Snowpark ML 与 scikit-learn 无缝集成，同时在模型构建过程中遵循类似的传统规范至关重要。一个值得注意的区别是 scikit-learn
    中数据必须以 pandas DataFrame 的形式传递。因此，在启动模型构建阶段之前，必须将 Snowflake 表转换为 pandas DataFrame。然而，务必注意潜在的内存限制，尤其是在处理大量数据集时。将大型表转换为
    pandas DataFrame 需要大量的内存，因为整个数据集都加载到内存中。
- en: In contrast, Snowpark ML provides a more native and memory-efficient approach
    to the model-building process. This native integration with Snowflake’s environment
    not only enhances the efficiency of the workflow but also mitigates memory-related
    challenges associated with large datasets. The utilization of Snowpark ML emerges
    as a strategic and seamless choice for executing complex model-building tasks
    within the Snowflake ecosystem.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，Snowpark ML为模型构建过程提供了一个更原生且内存效率更高的方法。这种与Snowflake环境的原生集成不仅提高了工作流程的效率，还减轻了与大数据集相关的内存挑战。Snowpark
    ML的应用成为在Snowflake生态系统中执行复杂模型构建任务的战略性和无缝选择。
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Snowpark ML emerges as a versatile and powerful tool for data scientists, enabling
    them to tackle complex data science tasks within Snowflake’s unified data platform.
    Its integration with popular programming languages, scalability, and real-time
    processing capabilities make it invaluable for various applications, from predictive
    modeling to real-time analytics and advanced AI tasks. With Snowpark ML, organizations
    can harness the full potential of their data, drive innovation, and gain a competitive
    edge in today’s data-driven landscape.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark ML成为数据科学家的一个多才多艺且强大的工具，使他们能够在Snowflake统一的数据库平台上处理复杂的数据科学任务。它与流行编程语言的集成、可扩展性和实时处理能力使其在从预测建模到实时分析和高级人工智能任务的各种应用中变得极其宝贵。借助Snowpark
    ML，组织可以利用其数据的全部潜力，推动创新，并在当今以数据驱动为导向的竞争环境中获得优势。
- en: In the next chapter, we will continue by deploying the model in Snowflake and
    operationalizing it.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续通过在Snowflake中部署模型并实现其运营来推进。
