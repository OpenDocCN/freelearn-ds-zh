- en: Machine Learning with MLlib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MLlib进行机器学习
- en: 'In this chapter, we will cover how to build machine learning models with PySpark''s
    MLlib module. Even though it is now being deprecated and most of the models are
    now being moved to the ML module, if you store your data in RDDs, you can use
    MLlib to do machine learning. You will learn the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何使用PySpark的MLlib模块构建机器学习模型。尽管它现在已经被弃用，大多数模型现在都被移动到ML模块，但如果您将数据存储在RDD中，您可以使用MLlib进行机器学习。您将学习以下示例：
- en: Loading the data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据
- en: Exploring the data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据
- en: Testing the data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据
- en: Transforming the data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换数据
- en: Standardizing the data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化数据
- en: Creating an RDD for training
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建用于训练的RDD
- en: Predicting hours of work for census respondents
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测人口普查受访者的工作小时数
- en: Forecasting the income level of census respondents
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测人口普查受访者的收入水平
- en: Building a clustering model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建聚类模型
- en: Computing performance statistics
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算性能统计
- en: Loading the data
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: In order to build a machine learning model, we need data. Thus, before we start,
    we need to read some data. In this recipe, and throughout this chapter, we will
    be using the 1994 census income data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个机器学习模型，我们需要数据。因此，在开始之前，我们需要读取一些数据。在这个示例中，以及在本章的整个过程中，我们将使用1994年的人口普查收入数据。
- en: Getting ready
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. If you
    do not have one, you might want to go back to [Chapter 1](part0026.html#OPEK0-dc04965c02e747b9b9a057725c821827), *Installing
    and Configuring Spark* and follow the recipes you will find there.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个示例，您需要一个可用的Spark环境。如果没有，您可能需要回到[第1章](part0026.html#OPEK0-dc04965c02e747b9b9a057725c821827)，*安装和配置Spark*，并按照那里找到的示例进行操作。
- en: The dataset was sourced from [http://archive.ics.uci.edu/ml/datasets/Census+Income](http://archive.ics.uci.edu/ml/datasets/Census+Income).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集来自[http://archive.ics.uci.edu/ml/datasets/Census+Income](http://archive.ics.uci.edu/ml/datasets/Census+Income)。
- en: The dataset is located in the `data` folder in the GitHub repository for the
    book.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集位于本书的GitHub存储库的`data`文件夹中。
- en: All the code that you will need in this chapter can be found in the GitHub repository
    we set up for the book: [http://bit.ly/2ArlBck](http://bit.ly/2ArlBck); go to
    `Chapter05` and open the `5\. Machine Learning with MLlib.ipynb` notebook.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中您需要的所有代码都可以在我们为本书设置的GitHub存储库中找到：[http://bit.ly/2ArlBck](http://bit.ly/2ArlBck)；转到`Chapter05`，打开`5\.
    Machine Learning with MLlib.ipynb`笔记本。
- en: No other prerequisites are required.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We will read the data into a DataFrame so it is easier for us to work with.
    Later on, we will convert it into an RDD of labeled points. To read the data,
    execute the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据读入DataFrame，这样我们就可以更容易地处理。稍后，我们将把它转换成带标签的RDD。要读取数据，请执行以下操作：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How it works...
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we specify the path to our dataset. In our case, as with all the other
    datasets we use in this book, `census_income.csv` is located in the `data` folder,
    accessible from the parent folder.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们指定了我们数据集的路径。在我们的情况下，与本书中使用的所有其他数据集一样，`census_income.csv`位于`data`文件夹中，可以从父文件夹中访问。
- en: Next, we use the `.read` property of `SparkSession`, which returns the `DataFrameReader`
    object. The first parameter to the `.csv(...)` method specifies the path to the
    data. Our dataset has the column names in the first row, so we use the `header`
    option to instruct the reader to use the first row for column names. The `inferSchema`
    parameter instructs the `DataFrameReader` to automatically detect the datatype
    of each column.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`SparkSession`的`.read`属性，它返回`DataFrameReader`对象。`.csv(...)`方法的第一个参数指定了数据的路径。我们的数据集在第一行中有列名，因此我们使用`header`选项指示读取器使用第一行作为列名。`inferSchema`参数指示`DataFrameReader`自动检测每列的数据类型。
- en: 'Let''s check whether the datatype inference is correct:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查数据类型推断是否正确：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code produces the following output:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '![](img/00107.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00107.jpeg)'
- en: As you can see, the datatype of certain columns was detected properly; without
    the `inferSchema` parameter, all the columns would default to strings.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，某些列的数据类型被正确地检测到了；如果没有`inferSchema`参数，所有列将默认为字符串。
- en: There's more...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'However, there''s a small problem with our dataset: most of the string columns
    have either leading or trailing white spaces. Here''s how you can correct this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的数据集存在一个小问题：大多数字符串列都有前导或尾随空格。以下是您可以纠正此问题的方法：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We loop through all the columns in the `census` DataFrame.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们循环遍历`census` DataFrame中的所有列。
- en: The `.dtypes` property of a DataFrame is a list of tuples where the first element
    is the column name and the second element is the datatype.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame的`.dtypes`属性是一个元组列表，其中第一个元素是列名，第二个元素是数据类型。
- en: 'If the type of the column is equal to string, we apply two functions: `.ltrim(...)`,
    which removes any leading whitespaces in a string, and `.rtrim(...)`, which removes
    any trailing whitespaces. The `.withColumn(...)` method does not append any new
    columns as we reuse the same name for the column: `col`.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果列的类型等于字符串，我们应用两个函数：`.ltrim(...)`，它删除字符串中的任何前导空格，以及`.rtrim(...)`，它删除字符串中的任何尾随空格。`.withColumn(...)`方法不会附加任何新列，因为我们重用相同的列名：`col`。
- en: Exploring the data
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: Jumping straight into modeling the data is a misstep almost every new data scientist
    makes; we get too eager to get to the reward stage, so we forget about the fact
    that most of the time is actually spent doing the boring stuff of cleaning up
    our data and getting familiar with it. In this recipe, we will explore the census
    dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 直接进入对数据建模是几乎每个新数据科学家都会犯的错误；我们太急于获得回报阶段，所以忘记了大部分时间实际上都花在清理数据和熟悉数据上。在这个示例中，我们将探索人口普查数据集。
- en: Getting ready
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. You should
    have already gone through the previousrecipe where we loaded the census data into
    a DataFrame.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个示例，您需要一个可用的Spark环境。您应该已经完成了之前的示例，其中我们将人口普查数据加载到了DataFrame中。
- en: No other prerequisites are required.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'First, we list all the columns we want to keep:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们列出我们想要保留的所有列：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we select the numerical and categorical features as we will be exploring
    these separately:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择数值和分类特征，因为我们将分别探索这些特征：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works...
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: First, we extract all the columns with their corresponding datatypes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们提取所有带有相应数据类型的列。
- en: We have already discussed the `.dtypes` property of DataFrame stores in the
    previous recipe.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在上一节中讨论了DataFrame存储的`.dtypes`属性。
- en: We will only keep `label`, which is the column that holds an identifier regarding
    whether a person makes more than $50,000 or not, and a handful of other numeric
    columns. In addition, we carry over all the string features.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只保留`label`，这是一个包含有关一个人是否赚超过5万美元的标识符的列，以及其他一些数字列。此外，我们保留所有的字符串特征。
- en: Next, we create a DataFrame with only the selected columns and extract all the
    numeric and categorical columns; we store these in the `cols_num` and `cols_cat`
    lists, respectively.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个仅包含所选列的DataFrame，并提取所有的数值和分类列；我们分别将它们存储在`cols_num`和`cols_cat`列表中。
- en: Numerical features
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数值特征
- en: 'Let''s explore the numerical features. Just like in [Chapter 4](part0186.html#5HC8K0-dc04965c02e747b9b9a057725c821827), *Preparing
    Data for Modeling*, for the numerical variables, we will calculate some basic
    descriptive statistics:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索数值特征。就像在[第4章](part0186.html#5HC8K0-dc04965c02e747b9b9a057725c821827)中的*为建模准备数据*一样，对于数值变量，我们将计算一些基本的描述性统计：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: First, we further subset our `census_subset` to include only the numerical columns.
    Next, we extract the underlying RDD. Since every element of this RDD is a row,
    we first need to create a list so we can work with it; we achieve that using the
    `.map(...)` method.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们进一步将我们的`census_subset`子集化为仅包含数值列。接下来，我们提取底层RDD。由于此RDD的每个元素都是一行，因此我们首先需要创建一个列表，以便我们可以使用它；我们使用`.map(...)`方法实现这一点。
- en: For documentation on the `Row` class, check out [http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`Row`类的文档，请查看[http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row)。
- en: Now that we have our RDD ready, we simply call the `.colStats(...)` method from
    the statistics module of MLlib. `.colStats(...)` accepts an RDD of numeric values;
    these can be either lists or vectors (either dense or sparse, see the documentation
    on `pyspark.mllib.linalg.Vectors` at [http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors)).
    A `MultivariateStatisticalSummary` trait is returned, which contains data such
    as count, max, mean, min, norms L1 and L2, number of nonzero observations, and
    the variance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的RDD准备好了，我们只需从MLlib的统计模块中调用`.colStats(...)`方法。`.colStats(...)`接受一个数值值的RDD；这些可以是列表或向量（密集或稀疏，参见`pyspark.mllib.linalg.Vectors`的文档[http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors)）。返回一个`MultivariateStatisticalSummary`特征，其中包含计数、最大值、平均值、最小值、L1和L2范数、非零观测数和方差等数据。
- en: If you are familiar with C++ or Java, traits can be viewed as virtual classes
    (C++) or interfaces (Java). You can read more about traits at [https://docs.scala-lang.org/tour/traits.html](https://docs.scala-lang.org/tour/traits.html).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉C++或Java，traits可以被视为虚拟类（C++）或接口（Java）。您可以在[https://docs.scala-lang.org/tour/traits.html](https://docs.scala-lang.org/tour/traits.html)上阅读更多关于traits的信息。
- en: 'In our example, we only select the min, mean, max, and variance. Here''s what
    we get back:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们只选择了最小值、平均值、最大值和方差。这是我们得到的结果：
- en: '![](img/00108.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00108.jpeg)'
- en: So, the average age is about 39 years old. However, we definitely have an outlier
    in our dataset of 90 years old. In terms of capital gain or loss, the census respondents
    seem to be making more money than losing. On average, the respondents worked 40
    hours per week but we had someone working close to 100-hour weeks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，平均年龄约为39岁。但是，我们的数据集中有一个90岁的异常值。就资本收益或损失而言，人口普查调查对象似乎赚的比亏的多。平均而言，受访者每周工作40小时，但我们有人工作接近100小时。
- en: Categorical features
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类特征
- en: 'For the categorical data, we cannot calculate simple descriptive statistics.
    Thus, we are going to calculate frequencies for each distinct value in each categorical
    column. Here''s a code snippet that will achieve this:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类数据，我们无法计算简单的描述性统计。因此，我们将计算每个分类列中每个不同值的频率。以下是一个可以实现这一目标的代码片段：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'First, we repeat what we have just done for the numerical columns but for the
    categorical ones: we subset `census_subset` to only the categorical columns and
    the label, access the underlying RDD, and transform each row into a list. We''re
    going to store the results in the `results_cat` dictionary. We loop through all
    the categorical columns and aggregate the data using the `.groupBy(...)` transformation.
    Finally, we create a list of tuples where the first element is the value (`el[0]`)
    and the second element is the frequency (`len(el[1])`).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们重复了我们刚刚为数值列所做的工作，但是对于分类列：我们将`census_subset`子集化为仅包含分类列和标签，访问底层RDD，并将每行转换为列表。我们将结果存储在`results_cat`字典中。我们遍历所有分类列，并使用`.groupBy(...)`转换来聚合数据。最后，我们创建一个元组列表，其中第一个元素是值（`el[0]`），第二个元素是频率（`len(el[1])`）。
- en: The `.groupBy(...)` transformation outputs a list where the first element is
    the value and the second is a `pyspark.resultIterable.ResultIterable` object that
    is effectively a list of all elements from the RDD that contains the value.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: “`.groupBy(...)`”转换输出一个列表，其中第一个元素是值，第二个元素是一个`pyspark.resultIterable.ResultIterable`对象，实际上是包含该值的RDD中的所有元素的列表。
- en: 'Now that we have our data aggregated, let''s see what we deal with:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经聚合了我们的数据，让我们看看我们要处理的内容：
- en: '![](img/00109.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.jpeg)'
- en: The preceding list is abbreviated for brevity. Check (or run the code in) the `5\.
    Machine Learning with MLlib.ipynb` notebook present in our GitHub repository.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述列表为简洁起见进行了缩写。检查（或运行代码）我们的GitHub存储库中的`5\. Machine Learning with MLlib.ipynb`笔记本。
- en: 'As you can see, we are dealing with an imbalanced sample: it is heavily skewed
    toward males and mostly white people. Also, in 1994 there were not many people
    earning more than $50,000, only about a quarter.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们处理的是一个不平衡的样本：它严重偏向男性，大部分是白人。此外，在1994年，收入超过50000美元的人并不多，只有大约四分之一。
- en: There's more...
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Another important metric you might want to check is the correlations between
    numerical variables. Calculating correlations with MLlib is very easy:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想要检查的另一个重要指标是数值变量之间的相关性。使用MLlib计算相关性非常容易：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `.corr(...)` action returns a NumPy array or arrays, or, in other words,
    a matrix where each element is a Pearson (by default) or Spearman correlation
    coefficient.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`.corr(...)`操作返回一个NumPy数组或数组，换句话说，一个矩阵，其中每个元素都是皮尔逊（默认）或斯皮尔曼相关系数。'
- en: 'To print it out, we just loop through all the elements:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要打印出来，我们只需循环遍历所有元素：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We only print the upper triangular portion of the matrix without the diagonal.
    Using the enumerate allows us to print out the column names since the correlations
    NumPy matrix does not list them. Here''s what we get:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只打印矩阵的上三角部分，不包括对角线。使用enumerate允许我们打印出列名，因为相关性NumPy矩阵没有列出它们。这是我们得到的内容：
- en: '![](img/00110.jpeg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00110.jpeg)'
- en: As you can see, there is not much correlation between our numerical variables.
    This is actually a good thing, as we can use all of them in our model since we
    will not suffer from much multicollinearity.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们的数值变量之间并没有太多的相关性。这实际上是件好事，因为我们可以在我们的模型中使用它们，因为我们不会遭受太多的多重共线性。
- en: If you do not know what multicollinearity, is check out this lecture: [https://onlinecourses.science.psu.edu/stat501/node/343](https://onlinecourses.science.psu.edu/stat501/node/343).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道什么是多重共线性，请查看这个讲座：[https://onlinecourses.science.psu.edu/stat501/node/343](https://onlinecourses.science.psu.edu/stat501/node/343)。
- en: See also
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: You might also want to check out this tutorial from Berkeley University: [http://ampcamp.berkeley.edu/big-data-mini-course/data-exploration-using-spark.html](http://ampcamp.berkeley.edu/big-data-mini-course/data-exploration-using-spark.html)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能还想查看伯克利大学的这个教程：[http://ampcamp.berkeley.edu/big-data-mini-course/data-exploration-using-spark.html](http://ampcamp.berkeley.edu/big-data-mini-course/data-exploration-using-spark.html)
- en: Testing the data
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试数据
- en: 'In order to build a successful statistical or machine learning model, we need
    to follow a simple (but hard!) rule: make it as simple as possible (so it generalizes
    the phenomenon being modeled well) but not too simple (so it loses its main ability
    to predict). A visual example of how this manifests is as follows (from [http://bit.ly/2GpRybB](http://bit.ly/2GpRybB)):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个成功的统计或机器学习模型，我们需要遵循一个简单（但困难！）的规则：尽可能简单（这样它才能很好地概括被建模的现象），但不要太简单（这样它就失去了预测的主要能力）。这种情况的视觉示例如下（来自[http://bit.ly/2GpRybB](http://bit.ly/2GpRybB)）：
- en: '![](img/00111.jpeg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00111.jpeg)'
- en: 'The middle chart shows a good fit: the model line follows the true function
    well. The model line on the left chart oversimplifies the phenomenon and has literally
    no predictive power (apart from a handful of points)—a perfect example of underfitting.
    The model line on the right follows the training data almost perfectly but if
    new data was presented, it would most likely misrepresent it—a concept known as
    overfitting, that is, it does not generalize well. As you can see from these three
    charts, the complexity of the model needs to be just right so it models the phenomenon
    well.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 中间的图表显示了一个很好的拟合：模型线很好地跟随了真实函数。左侧图表上的模型线过分简化了现象，几乎没有预测能力（除了少数几点）——这是欠拟合的完美例子。右侧的模型线几乎完美地跟随了训练数据，但如果出现新数据，它很可能会错误地表示——这是一种称为过拟合的概念，即它不能很好地概括。从这三个图表中可以看出，模型的复杂性需要恰到好处，这样它才能很好地模拟现象。
- en: Some machine learning models have a tendency to overtrain. For example, any
    models that try to find a mapping (a function) between the input data and the
    independent variable (or a label) have a tendency to overfit; these include parametric
    regression models, such as linear or generalized regression models, as well as
    recently (again!) popular neural networks (or deep learning models). On the other
    hand, some decision tree-based models (such as random forests) are less prone
    to overfitting even with more complex models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习模型有过度训练的倾向。例如，任何试图在输入数据和独立变量（或标签）之间找到映射（函数）的模型都有过拟合的倾向；这些模型包括参数回归模型，如线性或广义回归模型，以及最近（再次！）流行的神经网络（或深度学习模型）。另一方面，一些基于决策树的模型（如随机森林）即使是更复杂的模型也不太容易过拟合。
- en: 'So, how do we get the model just right? There are four rules of thumb:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何才能得到恰到好处的模型呢？有四个经验法则：
- en: Select your features wisely
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明智地选择你的特征
- en: Do not overtrain, or select a model that is less prone to overfitting
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要过度训练，或选择不太容易过拟合的模型
- en: Run multiple model estimations with randomly selected data from your dataset
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用从数据集中随机选择的数据运行多个模型估计
- en: Tune hyperparameters
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整超参数
- en: In this recipe, we will focus on the first point, the remaining points will
    be covered in some of the recipes found in this and the two next chapters.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将专注于第一个要点，其余要点将在本章和下两章的一些示例中涵盖。
- en: Getting ready
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. You would
    have already gone through the *Loading the data* recipe where we loaded the census
    data into a DataFrame.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要一个可用的Spark环境。您可能已经完成了*加载数据*示例，其中我们将人口普查数据加载到了一个DataFrame中。
- en: No other prerequisites are required.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In order to find the best features for the problem at hand, we first need to
    understand what problem we are dealing with, as different methods will be used
    for selecting features in regression problems or for classifiers:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到问题的最佳特征，我们首先需要了解我们正在处理的问题，因为不同的方法将用于选择回归问题或分类器中的特征：
- en: '**Regression**: In regression, your target (or ground truth) is a *continuous*
    variable (such as number of work hours per week). You have two methods to select
    your best features:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：在回归中，您的目标（或地面真相）是*连续*变量（例如每周工作小时数）。您有两种方法来选择最佳特征：'
- en: '**Pearson''s correlation**: We covered this one in the previous recipe. As
    noted there, the correlation can only be calculated between two numerical (continuous)
    features.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**皮尔逊相关系数**：我们在上一个示例中已经涵盖了这个。如前所述，相关性只能在两个数值（连续）特征之间计算。'
- en: '**Analysis of variance (ANOVA)**: It is a tool to explain (or test) the distribution
    of observations conditional on some categories. Thus, it can be used to select
    the most discriminatory (categorical) features of the continuous dependent variable.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差分析（ANOVA）**：这是一个解释（或测试）观察结果分布的工具，条件是某些类别。因此，它可以用来选择连续因变量的最具歧视性（分类）特征。'
- en: '**Classification**: In classification, your target (or label) is a discrete
    variable of two (binomial) or many (multinomial) levels. There are also two methods
    that help to select the best features:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：在分类中，您的目标（或标签）是两个（二项式）或多个（多项式）级别的离散变量。还有两种方法可以帮助选择最佳特征：'
- en: '**Linear discriminant analysis (LDA)**: This helps to find a linear combination
    of continuous features that best explains the variance of the categorical label'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性判别分析（LDA）**：这有助于找到最能解释分类标签方差的连续特征的线性组合'
- en: '***χ²* test**: A test that tests the independence between two categorical variables'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***χ²* 检验**：测试两个分类变量之间的独立性'
- en: Spark, for now, allows us to test (or select) the best features between comparable
    variables; it only implements the correlations (the `pyspark.mllib.stat.Statistics.corr(...)`
    we covered earlier) and the χ² test (the `pyspark.mllib.stat.Statistics.chiSqTest(...)`
    or the `pyspark.mllib.feature.ChiSqSelector(...)` methods).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Spark允许我们在可比较的变量之间测试（或选择）最佳特征；它只实现了相关性（我们之前涵盖的`pyspark.mllib.stat.Statistics.corr(...)`）和χ²检验（`pyspark.mllib.stat.Statistics.chiSqTest(...)`或`pyspark.mllib.feature.ChiSqSelector(...)`方法）。
- en: 'In this recipe, we will use `.chiSqTest(...)` to test the independence between
    our label (that is, an indicator that someone is earning more than $50,000) and
    the occupation of the census responder. Here''s a snippet that does this for us:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用`.chiSqTest(...)`来测试我们的标签（即指示某人是否赚取超过5万美元的指标）和人口普查回答者的职业之间的独立性。以下是一个为我们执行此操作的片段：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How it works...
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we import the linear algebra portion of MLlib; we will be using some
    matrix representations later.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入MLlib的线性代数部分；稍后我们将使用一些矩阵表示。
- en: 'Next, we build a pivot table where we group by the `occupation` feature and
    pivot by the `label` column (either `<=50K` or `>50K`). Each occurrence is counted
    and this results in the following table:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们建立一个数据透视表，其中我们按`occupation`特征进行分组，并按`label`列（`<=50K`或`>50K`）进行数据透视。每次出现都会被计算，结果如下表所示：
- en: '![](img/00112.jpeg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00112.jpeg)'
- en: 'Next, we flatten the output by accessing the underlying RDD and selecting only
    the counts with the map transformation: `.map(lambda row: (row[1:]))`. The `.flatMap(...)`
    transformation creates a long list of all the values we need. We collect all the
    data on the driver so we can later create `DenseMatrix`.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们通过访问底层RDD并仅选择具有映射转换的计数来展平输出：`.map(lambda row: (row[1:]))`。`.flatMap(...)`转换创建了我们需要的所有值的长列表。我们在驱动程序上收集所有数据，以便稍后创建`DenseMatrix`。'
- en: You should be cautious about using the `.collect(...)` action since it brings
    all the data to the driver. As you can see, we are only bringing the heavily aggregated
    representation of our dataset.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该谨慎使用`.collect(...)`操作，因为它会将所有数据带到驱动程序。正如您所看到的，我们只带来了数据集的高度聚合表示。
- en: 'Once we have all our numbers on the driver, we can create their matrix representation;
    we will have a matrix of 15 rows and 2 columns. First, we check how many distinct
    occupation values there are by checking the count of the `census_occupation` elements.
    Next, we call the `DenseMatrix(...)` constructor to create our matrix. The first
    parameter specifies the number of rows, the second one the number of columns.
    The third parameter specifies the data, and the final one indicates whether the
    data is transposed or not. The dense representation looks as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在驱动程序上拥有所有数字，我们就可以创建它们的矩阵表示；我们将有一个15行2列的矩阵。首先，我们通过检查`census_occupation`元素的计数来检查有多少个不同的职业值。接下来，我们调用`DenseMatrix(...)`构造函数来创建我们的矩阵。第一个参数指定行数，第二个参数指定列数。第三个参数指定数据，最后一个指示数据是否被转置。密集表示如下：
- en: '![](img/00113.jpeg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00113.jpeg)'
- en: 'And in a more readable format (as a NumPy matrix), it looks like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 以更易读的格式（作为NumPy矩阵）呈现如下：
- en: '![](img/00114.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00114.jpeg)'
- en: 'Now, we simply call the `.chiSqTest(...)` and pass our matrix as its only parameter.
    What is left is to check `pValue` and whether `nullHypothesis` was rejected or
    not:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需调用`.chiSqTest(...)`并将我们的矩阵作为其唯一参数传递。剩下的就是检查`pValue`以及是否拒绝了`nullHypothesis`：
- en: '![](img/00115.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00115.jpeg)'
- en: So, as you can see, `pValue` is `0.0`, so we can reject the null hypothesis
    that states the distribution of occupation between those that earn more than $50,000
    versus those that earn less than $50,000 is the same. Thus, we can conclude, as
    Spark tells us, that the occurrence of the outcomes is statistically independent,
    that is, occupation should be a strong indicator for someone who earns more than
    $50,000.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如您所看到的，`pValue`是`0.0`，因此我们可以拒绝空假设，即宣称赚取5万美元以上和赚取5万美元以下的人之间的职业分布相同。因此，我们可以得出结论，正如Spark告诉我们的那样，结果的发生是统计独立的，也就是说，职业应该是某人赚取5万美元以上的强有力指标。
- en: See also...
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅...
- en: There are many statistical tests that help to establish whether two populations
    (or samples) are similar or not, or whether they follow certain distributions.
    For a good overview, we suggest the following document: [http://www.statstutor.ac.uk/resources/uploaded/tutorsquickguidetostatistics.pdf](http://www.statstutor.ac.uk/resources/uploaded/tutorsquickguidetostatistics.pdf).
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有许多统计测试可以帮助确定两个总体（或样本）是否相似，或者它们是否遵循某些分布。为了获得良好的概述，我们建议阅读以下文档：[http://www.statstutor.ac.uk/resources/uploaded/tutorsquickguidetostatistics.pdf](http://www.statstutor.ac.uk/resources/uploaded/tutorsquickguidetostatistics.pdf)。
- en: Transforming the data
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换数据
- en: '**Machine learning** (**ML**) is a field of study that aims at using machines
    (computers) to understand world phenomena and predict their behavior. In order
    to build an ML model, all our data needs to be numeric. Since almost all of our
    features are categorical, we need to transform our features. In this recipe, we
    will learn how to use a hashing trick and dummy encoding.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）是一个旨在使用机器（计算机）来理解世界现象并预测其行为的研究领域。为了构建一个ML模型，我们所有的数据都需要是数字。由于我们几乎所有的特征都是分类的，我们需要转换我们的特征。在这个示例中，我们将学习如何使用哈希技巧和虚拟编码。'
- en: Getting ready
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做好准备
- en: To execute this recipe, you need to have a working Spark environment. You would
    have already gone through the *Loading the data *recipe where we loaded the census
    data into a DataFrame.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要有一个可用的Spark环境。您可能已经完成了*加载数据*示例，其中我们将人口普查数据加载到了DataFrame中。
- en: No other prerequisites are required.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We will be reducing the dimensionality of our dataset roughly by half, so first
    we need to extract the total number of distinct values in each column:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将将数据集的维度大致减少一半，因此首先我们需要提取每列中不同值的总数：
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, for each feature, we will use the `.HashingTF(...)` method to encode
    our data:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，对于每个特征，我们将使用`.HashingTF（...）`方法来对我们的数据进行编码：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: How it works...
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'First, we loop through all the categoricals and append a tuple of the column
    name (the `col`) and the count of distinct values found in that column. The latter
    is achieved by selecting the column of interest, running the `.distinct()` transformation,
    and counting the resulting number of values. `len_ftrs` is now a list of tuples.
    By calling the `dict(...)` method, Python will create a dictionary that will take
    the first element of the tuple as a key and the second element as the corresponding
    value. The resulting dictionary looks as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们循环遍历所有的分类变量，并附加一个元组，其中包括列名（`col`）和在该列中找到的不同值的计数。后者是通过选择感兴趣的列，运行`.distinct（）`转换，并计算结果值的数量来实现的。`len_ftrs`现在是一个元组列表。通过调用`dict（...）`方法，Python将创建一个字典，该字典将第一个元组元素作为键，第二个元素作为相应的值。生成的字典如下所示：
- en: '![](img/00116.jpeg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00116.jpeg)'
- en: 'Now that we know the total number of distinct values in each feature, we can
    use the hashing trick. First, we import the feature component of the MLlib as
    that is where the `.HashingTF(...)` is located. Next, we subset the census DataFrame
    to only the columns we want to keep. We then use the `.map(...)` transformation
    on the underlying RDD: for each element, we enumerate all the columns and if the
    index of the column is greater than or equal to five, we create a new instance
    of `.HashingTF(...)`, which we then use to transform the value and convert it
    into an NumPy array. The only thing you need to specify for the `.HashingTF(...)`
    method is the output number of elements; in our case, we roughly halve the number
    of the number of distinct values so we will have some hashing collisions, but
    that is fine.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了每个特征中不同值的总数，我们可以使用哈希技巧。首先，我们导入MLlib的特征组件，因为那里有`.HashingTF（...）`。接下来，我们将census
    DataFrame子集化为我们想要保留的列。然后，我们在基础RDD上使用`.map（...）`转换：对于每个元素，我们枚举所有列，如果列的索引大于或等于五，我们创建一个新的`.HashingTF（...）`实例，然后用它来转换值并将其转换为NumPy数组。对于`.HashingTF（...）`方法，您唯一需要指定的是输出元素的数量；在我们的情况下，我们大致将不同值的数量减半，因此我们将有一些哈希碰撞，但这没关系。
- en: 'For your reference, our `cols_to_keep` looks as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 供您参考，我们的`cols_to_keep`如下：
- en: '![](img/00117.jpeg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00117.jpeg)'
- en: 'After doing the preceding to our current dataset, `final_data`, it looks as
    follows; note the format might look a bit odd but we will soon be getting it ready
    for creating the training RDD:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在对我们当前的数据集`final_data`进行上述操作之后，它看起来如下；请注意，格式可能看起来有点奇怪，但我们很快将准备好创建训练RDD：
- en: '![](img/00118.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00118.jpeg)'
- en: There's more...
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The only thing that''s left is to handle our label; as you can see, it is still
    a categorical variable. However, since it only takes two values, we can encode
    it as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一剩下的就是处理我们的标签；如您所见，它仍然是一个分类变量。但是，由于它只有两个值，我们可以将其编码如下：
- en: '[PRE14]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `labelEncode(...)` method takes the label and checks whether it is `'>50k'`
    or not; if yes, we get a Boolean true, otherwise we get false. We can represent
    the Boolean data as integers by simply wrapping it inside Python's `int(...)` method.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`labelEncode（...）`方法获取标签并检查它是否为`''>50k''`；如果是，我们得到一个布尔值true，否则我们得到false。我们可以通过简单地将布尔数据包装在Python的`int（...）`方法中来表示布尔数据为整数。'
- en: 'Finally, we again use `.map(...)`, where we pass the first element of our `row`—the
    label—to the `labelEncode(...)` method. We then loop through all the remaining
    lists and combine them together. That portion of the code might look a bit peculiar
    at first, but it is actually fairly easy to understand. We loop through all the
    remaining elements (the `row[1:]`) and since each element is a list (hence we
    name it `sublist`), we create another loop (the `for item in sublist` portion)
    to extract the individual items. The resulting RDD looks as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们再次使用`.map（...）`，在那里我们将`row`的第一个元素（标签）传递给`labelEncode（...）`方法。然后，我们循环遍历所有剩余的列表并将它们组合在一起。代码的这部分一开始可能看起来有点奇怪，但实际上很容易理解。我们循环遍历所有剩余的元素（`row[1:]`），并且由于每个元素都是一个列表（因此我们将其命名为`sublist`），我们创建另一个循环（`for
    item in sublist`部分）来提取单个项目。生成的RDD如下所示：
- en: '![](img/00119.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00119.jpeg)'
- en: See also...
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅...
- en: Check out this link for a nice overview of how to deal with categorical features
    in Python: [http://pbpython.com/categorical-encoding.html](http://pbpython.com/categorical-encoding.html)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看此链接，了解如何在Python中处理分类特征的概述：[http://pbpython.com/categorical-encoding.html](http://pbpython.com/categorical-encoding.html)
- en: Standardizing the data
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据标准化
- en: 'Data standardization (or normalization) is important for a number of reasons:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标准化（或归一化）对许多原因都很重要：
- en: Some algorithms converge faster on standardized (or normalized) data
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某些算法在标准化（或归一化）数据上收敛得更快
- en: If your input variables are on vastly different scales, the interpretability
    of coefficients might be hard or conclusions drawn might be wrong
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的输入变量在不同的尺度上，系数的可解释性可能很难或得出的结论可能是错误的
- en: For some models, the optimal solution might be wrong if you do not standardize
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于某些模型，如果不进行标准化，最优解可能是错误的
- en: In this recipe, we will show you how to standardize the data so if your modeling
    project requires standardized data, you will know how to do it.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个操作中，我们将向您展示如何标准化数据，因此如果您的建模项目需要标准化数据，您将知道如何操作。
- en: Getting ready
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. You would
    have already gone through the previous recipe where we encoded the census data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此操作，您需要拥有一个可用的Spark环境。您可能已经完成了之前的操作，其中我们对人口普查数据进行了编码。
- en: No other prerequisites are required.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'MLlib offers a method to do most of this work for us. Even though the following
    code might be confusing at first, we will walk through it step by step:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib提供了一个方法来为我们完成大部分工作。尽管以下代码一开始可能会令人困惑，但我们将逐步介绍它：
- en: '[PRE15]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: How it works...
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'First, we create the `StandardScaler(...)` object. The two parameters set to
    `True`—the former stands for mean, the latter stands for standard deviation—indicate
    that we want the model to standardize our features using Z-score: ![](img/00120.jpeg),
    where ![](img/00121.jpeg) is the *i*^(th) observation of the *f* feature, μ^(*f*)
    is the mean of all the observations in the *f* feature, and σ^(*f*) is the standard
    deviation of all the observations in the *f* feature.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建`StandardScaler(...)`对象。设置为`True`的两个参数——前者代表均值，后者代表标准差——表示我们希望模型使用Z分数对特征进行标准化：![](img/00120.jpeg)，其中![](img/00121.jpeg)是*f*特征的第*i*^(th)观察值，μ^(*f*)是*f*特征中所有观察值的均值，σ^(*f*)是*f*特征中所有观察值的标准差。
- en: Next, we `.fit(...)` the data using `StandardScaler(...)`. Note that we do not
    standardize the first feature as it is actually our label. Finally, we `.transform(...)`
    our dataset so we get the scaled features.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`StandardScaler(...)`对数据进行`.fit(...)`。请注意，我们不会对第一个特征进行标准化，因为它实际上是我们的标签。最后，我们对数据集进行`.transform(...)`，以获得经过缩放的特征。
- en: 'However, since we do not scale our label, we need to somehow bring it back
    to our scaled dataset. So first, from `final_data`, we extract the label (using
    the `.map(lamba row: row[0])` transformation). However, we would not be able to
    join it with the `final_data_scaled` as it is since there is no key to join on. Note,
    we essentially want to join in a row-by-row fashion. So, we use the `.zipWithIndex()`
    method, which gives us a tuple in return, with the first element being the data
    and the second element being the row number. Since we want to join on the row
    number, we need to bring it to the first position in the tuple since that is how
    the `.join(...)` works for RDDs; we achieve this with the second `.map(...)` operation.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，由于我们不对标签进行缩放，我们需要以某种方式将其带回我们的缩放数据集。因此，首先从`final_data`中提取标签（使用`.map(lamba
    row: row[0])`转换）。然而，我们将无法将其与`final_data_scaled`直接连接，因为没有键可以连接。请注意，我们实际上希望以逐行方式进行连接。因此，我们使用`.zipWithIndex()`方法，它会返回一个元组，第一个元素是数据，第二个元素是行号。由于我们希望根据行号进行连接，我们需要将其带到元组的第一个位置，因为这是RDD的`.join(...)`的工作方式；我们通过第二个`.map(...)`操作实现这一点。'
- en: In RDDs, the `.join(...)` operation cannot specify the key explicitly; both
    RDDs need to be two-element tuples, where the first element is the key and the
    second element is the data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在RDD中，`.join(...)`操作不能明确指定键；两个RDD都需要是两个元素的元组，其中第一个元素是键，第二个元素是数据。
- en: 'Once the join is complete, we simply extract the joined data by using the `.map(lambda
    row: row[1])` transformation.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦连接完成，我们只需使用`.map(lambda row: row[1])`转换来提取连接的数据。'
- en: 'Here''s how our data looks now:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据看起来是这样的：
- en: '![](img/00122.jpeg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00122.jpeg)'
- en: 'We can also peek into `sModel` to see what means and standard deviations were
    used to transform our data:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看`sModel`，以了解用于转换我们的数据的均值和标准差：
- en: '![](img/00123.jpeg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00123.jpeg)'
- en: Creating an RDD for training
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建用于训练的RDD
- en: Before we can train an ML model, we need to create an RDD where each element
    is a labeled point. In this recipe, we will use the `final_data` RDD we created
    in the previous recipe to prepare our RDD for training.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以训练ML模型之前，我们需要创建一个RDD，其中每个元素都是一个标记点。在这个操作中，我们将使用之前操作中创建的`final_data` RDD来准备我们的训练RDD。
- en: Getting ready
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. You would
    have already gone through the previous recipe when we standardized the encoded
    census data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此操作，您需要拥有一个可用的Spark环境。您可能已经完成了之前的操作，当时我们对编码的人口普查数据进行了标准化。
- en: No other prerequisites are required.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: Many of the MLlib models require an RDD of labeled points to train. The next
    code snippets will create such an RDD for us to build classification and regression
    model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 许多MLlib模型需要一个标记点的RDD进行训练。下一个代码片段将为我们创建这样的RDD，以构建分类和回归模型。
- en: Classification
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: 'Here''s the snippet to create the classification RDD of labeled points that
    we will be using to predict whether someone is making more than $50,000:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建分类标记点RDD的片段，我们将使用它来预测某人是否赚取超过$50,000：
- en: '[PRE16]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Regression
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归
- en: 'Here''s the snippet to create the regression RDD of labeled points that we
    will be using to predict the number of hours people work:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建用于预测人们工作小时数的回归标记点RDD的片段：
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How it works...
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'Before we create the RDDs, we have to import the `pyspark.mllib.regression`
    submodule, as that is where we can access the `LabeledPoint` class:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建RDD之前，我们必须导入`pyspark.mllib.regression`子模块，因为那里可以访问`LabeledPoint`类：
- en: '[PRE18]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, we simply loop through all the elements of the `final_data` RDD and create
    a labeled point for each element using the `.map(...)` transformation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们只需循环遍历`final_data` RDD的所有元素，并使用`.map(...)`转换为每个元素创建一个带标签的点。
- en: The first parameter of `LabeledPoint(...)` is the label. If you look at the
    the two code snippets, the only difference between them is what we consider labels
    and features.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`LabeledPoint(...)`的第一个参数是标签。如果您查看这两个代码片段，它们之间唯一的区别是我们认为标签和特征是什么。'
- en: As a reminder, a classification problem aims to find the probability of an observation
    belonging to a specific class; thus, the label is normally a categorical or, in
    other words, discrete. On the other hand, the regression problem aims to predict
    a value given an observation; thus, the label is normally numerical, or continuous
    if you will.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，分类问题旨在找到观察结果属于特定类别的概率；因此，标签通常是分类的，换句话说，是离散的。另一方面，回归问题旨在预测给定观察结果的值；因此，标签通常是数值的，或者连续的。
- en: So, in the `final_data_income` case, we are using the binary indicator for whether
    the census respondent earns more (a value of 1) or less (the label equal to 0)
    than $50,000, whereas in the `final_data_hours`, we use the `hours-per-week` feature
    (see the *Loading the data* recipe), which, in our case, is the fifth piece of
    each of the elements of the `final_data` RDD. Note for this label we need to scale
    it back, so we need to multiply by the standard deviation and add the mean.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在`final_data_income`的情况下，我们使用二进制指示符，表示人口普查受访者是否赚得更多（值为1）还是更少（标签等于0）50,000美元，而在`final_data_hours`中，我们使用`hours-per-week`特征（请参阅*加载数据*示例），在我们的情况下，它是`final_data`
    RDD的每个元素的第五部分。请注意，对于此标签，我们需要将其缩放回来，因此我们需要乘以标准差并加上均值。
- en: We assume here that you are working through the `5\. Machine Learning with MLlib.ipynb`
    notebook and have the `sModel` object already created. If you do not, please go
    back to the previous recipe and follow the steps outlined there.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里假设您正在通过`5\. Machine Learning with MLlib.ipynb`笔记本进行工作，并且已经创建了`sModel`对象。如果没有，请返回到上一个示例，并按照那里概述的步骤进行操作。
- en: The second parameter of the `LabeledPoint(...)` is a vector of all the features.
    You can pass either a NumPy array, list, `scipy.sparse` column matrix, or `pyspark.mllib.linalg.SparseVector`
    or `pyspark.mllib.linalg.DenseVector`; in our case, we encoded our features into `DenseVector`
    as we have already encoded all our features using the hashing trick.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`LabeledPoint(...)`的第二个参数是所有特征的向量。您可以传递NumPy数组、列表、`scipy.sparse`列矩阵或`pyspark.mllib.linalg.SparseVector`或`pyspark.mllib.linalg.DenseVector`；在我们的情况下，我们使用哈希技巧对所有特征进行了编码，因此我们将特征编码为`DenseVector`。'
- en: There's more...
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We could use the full dataset to train our models, but we would then run into
    another problem: how do we evaluate how good our model is? Therefore, any data
    scientist normally performs a split of the data into two subsets: training and
    testing.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用完整数据集来训练我们的模型，但是我们会遇到另一个问题：我们如何评估我们的模型有多好？因此，任何数据科学家通常都会将数据拆分为两个子集：训练和测试。
- en: See the *See also* section of this recipe for why this often isn't good enough,
    and you should actually be splitting the data into training, testing, and validation
    datasets.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅此示例的*另请参阅*部分，了解为什么这通常还不够好，您实际上应该将数据拆分为训练、测试和验证数据集。
- en: 'Here are two code snippets that show how easily this can be done in PySpark:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是两个代码片段，显示了在PySpark中如何轻松完成此操作：
- en: '[PRE19]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is the second:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第二个：
- en: '[PRE20]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By simply calling the `.randomSplit(...)` method of an RDD, we can quickly divide
    our RDDs into training and testing subsets. The only required parameter for the
    `.randomSplit(...)` method is a list where each element specifies the proportion
    of the dataset to randomly select. Note, these proportions need to sum up to 1.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单调用RDD的`.randomSplit(...)`方法，我们可以快速将RDD分成训练和测试子集。`.randomSplit(...)`方法的唯一必需参数是一个列表，其中每个元素指定要随机选择的数据集的比例。请注意，这些比例需要加起来等于1。
- en: We could have passed a list of three elements if we wanted to get the training,
    testing, and validation subsets.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要获取训练、测试和验证子集，我们可以传递一个包含三个元素的列表。
- en: See also
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Why you should be splitting into three datasets, and not two, is nicely explained
    here: [http://bit.ly/2GFyvtY](http://bit.ly/2GFyvtY)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么应该将数据拆分为三个数据集，而不是两个，可以在这里很好地解释：[http://bit.ly/2GFyvtY](http://bit.ly/2GFyvtY)
- en: Predicting hours of work for census respondents
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测人口普查受访者的工作小时数
- en: In this recipe, we will build a simple linear regression model that will aim
    to predict the number of hours each of the census respondents works per week.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将构建一个简单的线性回归模型，旨在预测人口普查受访者每周工作的小时数。
- en: Getting ready
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. You would
    have already gone through the previous recipe where we created training and testing
    datasets for estimating regression models.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要一个可用的Spark环境。您可能已经通过之前的示例创建了用于估计回归模型的训练和测试数据集。
- en: No other prerequisites are required.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Training models with MLlib is pretty straightforward. See the following code
    snippet:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MLlib训练模型非常简单。请参阅以下代码片段：
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How it works...
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As you can see, we first create the `LinearRegressionWithSGD` object and call
    its `.train(...)` method.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们首先创建`LinearRegressionWithSGD`对象，并调用其`.train(...)`方法。
- en: For a very good overview of different derivatives of stochastic gradient descent,
    check this out: [http://ruder.io/optimizing-gradient-descent/](http://ruder.io/optimizing-gradient-descent/).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机梯度下降的不同派生的很好的概述，请查看这个链接：[http://ruder.io/optimizing-gradient-descent/](http://ruder.io/optimizing-gradient-descent/)。
- en: 'The first, and the only, required parameter we pass to the method is an RDD
    of labeled points that we created earlier. There is a host of parameters, though,
    that you can specify:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递给方法的第一个，也是唯一需要的参数是我们之前创建的带有标记点的RDD。不过，您可以指定一系列参数：
- en: Number of iterations; the default is `100`
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数；默认值为`100`
- en: Step is the parameter used in SGD; the default is `1.0`
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步长是SGD中使用的参数；默认值为`1.0`
- en: '`miniBatchFraction` specifies the proportion of data to be used in each SGD
    iteration; the default is `1.0`'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`miniBatchFraction`指定在每个SGD迭代中使用的数据比例；默认值为`1.0`'
- en: The `initialWeights` parameter allows us to initialize the coefficients to some
    specific values; it has no defaults and the algorithm will start with the weights
    equal to `0.0`
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initialWeights`参数允许我们将系数初始化为特定值；它没有默认值，算法将从权重等于`0.0`开始'
- en: 'The regularizer type parameter, `regType`, allows us to specify the type of
    the regularizer used: `''l1''` for L1 regularization and `''l2''` for L2 regularization;
    the default is `None`, no regularization'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化类型参数`regType`允许我们指定所使用的正则化类型：`'l1'`表示L1正则化，`'l2'`表示L2正则化；默认值为`None`，无正则化
- en: The `regParam` parameter specifies the regularizer parameter; the default is
    `0.0`
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`regParam`参数指定正则化参数；默认值为`0.0`'
- en: The model can also fit the intercept but it is not set by default; the default
    is false
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型也可以拟合截距，但默认情况下未设置；默认值为false
- en: Before training, the model by default can validate data
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练之前，默认情况下，模型可以验证数据
- en: You can also specify `convergenceTol`; the default is `0.001`
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还可以指定`convergenceTol`；默认值为`0.001`
- en: 'Let''s now see how well our model predicts working hours:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看我们的模型预测工作小时的效果如何：
- en: '[PRE22]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'First, from our full testing dataset, we select 10 observations (so we can
    print them on the screen). Next, we extract the true value from the testing dataset,
    whereas for the prediction we simply call the `.predict(...)` method of the `workhours_model_lm`
    model and pass the `.features` vector. Here is what we get:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从我们的完整测试数据集中，我们选择10个观察值（这样我们可以在屏幕上打印出来）。接下来，我们从测试数据集中提取真实值，而对于预测，我们只需调用`workhours_model_lm`模型的`.predict(...)`方法，并传递`.features`向量。这是我们得到的结果：
- en: '![](img/00124.jpeg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00124.jpeg)'
- en: As you can see, our model does not do very well, so further refining would be
    necessary. This, however, goes beyond the scope of this chapter and the book itself.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们的模型效果不佳，因此需要进一步改进。然而，这超出了本章和本书的范围。
- en: Forecasting the income levels of census respondents
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测人口普查受访者的收入水平
- en: 'In this recipe, we will show you how to solve a classification problem with
    MLlib by building two models: the ubiquitous logistic regression and a slightly
    more sophisticated model, the **SVM** ( **Support Vector Machine**).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将向您展示如何使用MLlib解决分类问题，方法是构建两个模型：无处不在的逻辑回归和稍微复杂一些的模型，即**SVM**（**支持向量机**）。
- en: Getting ready
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. You would
    have already gone through the *Creating an RDD for training* recipe where we created
    training and testing datasets for estimating classification models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此示例，您需要一个可用的Spark环境。您可能已经完成了*为训练创建RDD*示例，在那里我们为估计分类模型创建了训练和测试数据集。
- en: No other prerequisites are required.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Just like with the linear regression, building a logistic regression starts
    with creating a `LogisticRegressionWithSGD` object:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 就像线性回归一样，构建逻辑回归始于创建`LogisticRegressionWithSGD`对象：
- en: '[PRE23]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: How it works...
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'As with the `LinearRegressionWithSGD` model, the only required parameter is
    the RDD with labeled points. Also, you can specify the same set of parameters:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 与`LinearRegressionWithSGD`模型一样，唯一需要的参数是带有标记点的RDD。此外，您可以指定相同的一组参数：
- en: The number of iterations; the default is `100`
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数；默认值为`100`
- en: The step is the parameter used in SGD; the default is ``1.0``
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步长是SGD中使用的参数；默认值为`1.0`
- en: '`miniBatchFraction` specifies the proportion of data to be used in each SGD
    iteration; the default is `1.0`'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`miniBatchFraction`指定在每个SGD迭代中使用的数据比例；默认值为`1.0`'
- en: The `initialWeights` parameter allows us to initialize the coefficients to some
    specific values; it has no defaults and the algorithm will start with the weights
    equal to `0.0`
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initialWeights`参数允许我们将系数初始化为特定值；它没有默认值，算法将从权重等于`0.0`开始'
- en: 'The regularizer type parameter, `regType`, allows us to specify the type of
    the regularizer used: `l1` for L1 regularization and `l2` for L2 regularization;
    the default is `None`, no regularization'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化类型参数`regType`允许我们指定所使用的正则化类型：`l1`表示L1正则化，`l2`表示L2正则化；默认值为`None`，无正则化
- en: The `regParam` parameter specifies the regularizer parameter; the default is
    `0.0`
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`regParam`参数指定正则化参数；默认值为`0.0`'
- en: The model can also fit the intercept but it is not set by default; the default
    is false
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型也可以拟合截距，但默认情况下未设置；默认值为false
- en: Before training, the model by default can validate data
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练之前，默认情况下，模型可以验证数据
- en: You can also specify `convergenceTol`; the default is `0.001`
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还可以指定`convergenceTol`；默认值为`0.001`
- en: The `LogisticRegressionModel(...)` object that is returned upon finalizing the
    training allows us to utilize the model. By passing a vector of features to the
    `.predict(...)` method, we can predict the class the observations will most likely
    be associated with.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成训练后返回的`LogisticRegressionModel(...)`对象允许我们利用该模型。通过将特征向量传递给`.predict(...)`方法，我们可以预测观察值最可能关联的类别。
- en: Any classification model produces a set of probabilities and logistic regression
    is not an exception. In the binary case, we can specify a threshold that, once
    breached, would indicate that the observation would be assigned with the class
    equal to 1 rather than 0; this threshold is normally set to `0.5`. `LogisticRegressionModel(...)`
    assumes `0.5` by default, but you can change it by calling the `.setThreshold(...)`
    method and passing a desired threshold value that is between 0 and 1 (not inclusive).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 任何分类模型都会产生一组概率，逻辑回归也不例外。在二元情况下，我们可以指定一个阈值，一旦突破该阈值，就会表明观察结果将被分配为等于1的类，而不是0；此阈值通常设置为`0.5`。`LogisticRegressionModel(...)`默认情况下假定为`0.5`，但您可以通过调用`.setThreshold(...)`方法并传递介于0和1之间（不包括）的所需阈值值来更改它。
- en: 'Let''s see how our model performs:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的模型表现如何：
- en: '[PRE24]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As with the linear regression example, we first extract 10 records from our
    test dataset so we can fit them on the screen. Next, we extract the desired label
    and call the `income_model_lr` model of `.predict(...)` the class. Here''s what
    we get back:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归示例一样，我们首先从测试数据集中提取10条记录，以便我们可以在屏幕上适应它们。接下来，我们提取所需的标签，并调用`.predict(...)`类的`income_model_lr`模型。这是我们得到的结果：
- en: '![](img/00125.jpeg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00125.jpeg)'
- en: So, out of 10 records, we got 9 right. Not bad.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在10条记录中，我们得到了9条正确的。还不错。
- en: In the *Computing performance statistics* recipe, we will learn how to use the
    full testing dataset to more formally evaluate our models.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在*计算性能统计*配方中，我们将学习如何使用完整的测试数据集更正式地评估我们的模型。
- en: There's more...
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Logistic regression is normally the benchmark used to asses the relative performance
    of other classification models, that is, whether they are performing better or
    worse. The drawback of logistic regression, however, is that it cannot handle
    cases where two classes cannot be separated by a line. SVMs do not have these
    kinds of problem, as their kernel can be expressed in quite flexible ways:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归通常是用于评估其他分类模型相对性能的基准，即它们是表现更好还是更差。然而，逻辑回归的缺点是它无法处理两个类无法通过一条线分开的情况。SVM没有这种问题，因为它们的核可以以非常灵活的方式表达：
- en: '[PRE25]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this example, just like with the `LogisticRegressionWithSGD` model, we can
    specify a host of parameters (we will not be repeating them here). However, the
    `miniBatchFraction` parameter instructs the SVM model to only use half of the
    data in each iteration; this helps preventing overfitting.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，就像`LogisticRegressionWithSGD`模型一样，我们可以指定一系列参数（我们不会在这里重复它们）。但是，`miniBatchFraction`参数指示SVM模型在每次迭代中仅使用一半的数据；这有助于防止过拟合。
- en: 'The results for the 10 observations from the `small_sample_income` RDD are
    calculated the same way as with the logistic regression model:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 从`small_sample_income` RDD中计算的10个观察结果与逻辑回归模型的计算方式相同：
- en: '[PRE26]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The model produces the same results as the logistic regression model, so we
    will not be repeating them here. However, in the *Computing performance statistics*
    recipe, we will see how these differ.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型产生与逻辑回归模型相同的结果，因此我们不会在这里重复它们。但是，在*计算性能统计*配方中，我们将看到它们的不同。
- en: Building a clustering models
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建聚类模型
- en: Often, it is hard to get our hands on data that is labeled. Also, sometimes
    you might want to find underlying patterns in your dataset. In this recipe, we
    will learn how to build the popular k-means clustering model in Spark.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，很难获得有标签的数据。而且，有时您可能希望在数据集中找到潜在的模式。在这个配方中，我们将学习如何在Spark中构建流行的k-means聚类模型。
- en: Getting ready
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To execute this recipe, you need to have a working Spark environment. You should
    have already gone through the *Standardizing the data* recipe where we standardized
    the encoded census data.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此配方，您需要拥有一个可用的Spark环境。您应该已经完成了*标准化数据*配方，其中我们对编码的人口普查数据进行了标准化。
- en: No other prerequisites are required.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Just like with classification or regression models, building clustering models
    is pretty straightforward in Spark. Here''s the code that aims to find patterns
    in the census data:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 就像分类或回归模型一样，在Spark中构建聚类模型非常简单。以下是旨在在人口普查数据中查找模式的代码：
- en: '[PRE27]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: How it works...
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'First, we need to import the clustering submodule of MLlib. Just like before,
    we first create the clustering estimator object, `KMeans`. The `.train(...)` method
    requires two parameters: the RDD we want to use to find the clusters in, and the
    number of clusters we expect. We also chose to randomly initialize the centroids
    of the clusters by specifying `initializationMode`; the default for this one is
    `k-means||`. Other parameters include:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入MLlib的聚类子模块。就像以前一样，我们首先创建聚类估计器对象`KMeans`。`.train(...)`方法需要两个参数：我们要在其中找到集群的RDD，以及我们期望的集群数。我们还选择通过指定`initializationMode`来随机初始化集群的质心；这个的默认值是`k-means||`。其他参数包括：
- en: '`maxIterations` specifies after how many iterations the estimation should stop;
    the default is `100`'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxIterations`指定估计应在多少次迭代后停止；默认值为`100`'
- en: '`initializationSteps` is only useful if the default initialization mode is
    used; the default for this parameter is `2`'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializationSteps`仅在使用默认初始化模式时有用；此参数的默认值为`2`'
- en: '`epsilon` is a stopping criteria—if all the centers of the centroids move (in
    terms of the Euclidean distance) less than this, the iterations stop; the default
    is `0.0001`'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon`是一个停止标准-如果所有质心的中心移动（以欧几里德距离表示）小于此值，则迭代停止；默认值为`0.0001`'
- en: '`initialModel` allows you to specify the centers previously estimated in the
    form of `KMeansModel`; the default is `None`'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initialModel`允许您指定以`KMeansModel`形式先前估计的中心；默认值为`None`'
- en: There's more...
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Once the model is estimated, we can use it to predict the clusters and see
    how good our model actually is. However, at the moment, Spark does not provide
    the means to evaluate clustering models. Thus, we will use the metrics provided
    by scikit-learn:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦估计出模型，我们就可以使用它来预测聚类，并查看我们的模型实际上有多好。但是，目前，Spark并没有提供评估聚类模型的手段。因此，我们将使用scikit-learn提供的度量标准：
- en: '[PRE29]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The clustering metrics are located in the `.metrics` submodule of scikit-learn.
    We are using two of the metrics available: homogeneity and completeness. Homogeneity
    measures whether all the points in a cluster come from the same class whereas
    the completeness score estimates whether, for a given class, all the points end
    up in the same cluster; a value of 1 for either of the scores means a perfect
    model.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类指标位于scikit-learn的`.metrics`子模块中。我们使用了两个可用的指标：同质性和完整性。同质性衡量了一个簇中的所有点是否来自同一类，而完整性得分估计了对于给定的类，所有点是否最终在同一个簇中；任一得分为1表示一个完美的模型。
- en: 'Let''s see what we get:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们得到了什么：
- en: '![](img/00126.jpeg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00126.jpeg)'
- en: 'Well, our clustering model did not do so well: the homogeneity score of 15%
    means that the remaining 85% of observations were misclustered, and we only clustered
    ∼12% properly of all those that belong to the same class.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，我们的聚类模型表现不佳：15%的同质性得分意味着剩下的85%观察值被错误地聚类，我们只正确地聚类了∼12%属于同一类的所有观察值。
- en: See also
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: For more on the evaluation of clustering models, you might want to check out [https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关聚类模型评估的更多信息，您可能想查看：[https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)
- en: Computing performance statistics
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算性能统计
- en: In the previous recipes, we have already seen some values predicted by our classification
    and regression models and how far or how close they were from/to the original
    values. In this recipe, we will learn how to fully calculate the performance statistics
    for these models.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们已经看到了我们的分类和回归模型预测的一些值，以及它们与原始值的差距。在这个示例中，我们将学习如何完全计算这些模型的性能统计数据。
- en: Getting ready
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In order to execute this recipe, you need to have a working Spark environment
    and you should have gone through the *Predicting hours of work for census respondents*
    and *Forecasting income levels of census respondents* recipes presented earlier
    in this chapter.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行这个示例，您需要有一个可用的Spark环境，并且您应该已经完成了本章前面介绍的*预测人口普查受访者的工作小时数*和*预测人口普查受访者的收入水平*的示例。
- en: No other prerequisites are required.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要其他先决条件。
- en: How to do it...
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Getting the performance metrics for regression and classification in Spark
    is extremely simple:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中获取回归和分类的性能指标非常简单：
- en: '[PRE31]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How it works...
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: First, we load the evaluation module; doing this exposes the `.RegressionMetrics(...)`
    and the `.BinaryClassificationMetrics(...)` methods, which we can use.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载评估模块；这样做会暴露`.RegressionMetrics(...)`和`.BinaryClassificationMetrics(...)`方法，我们可以使用它们。
- en: Regression metrics
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归指标
- en: '`true_pred_reg` is an RDD of tuples where the first element is the prediction
    from our linear regression model and the second element is the expected value
    (the number of hours worked per week). Here''s how we create it:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '`true_pred_reg`是一个元组的RDD，其中第一个元素是我们线性回归模型的预测值，第二个元素是期望值（每周工作小时数）。以下是我们创建它的方法：'
- en: '[PRE32]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The `metrics_lm` object contains a variety of metrics: `explainedVariance`,
    `meanAbsouteError`, `meanSquaredError`, `r2`, and `rootMeanSquaredError`. Here,
    we will only print out a couple of them:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`metrics_lm`对象包含各种指标：`解释方差`、`平均绝对误差`、`均方误差`、`r2`和`均方根误差`。在这里，我们只打印其中的一些：'
- en: '[PRE33]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s see what we got for the linear regression model:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看线性回归模型的结果：
- en: '![](img/00127.jpeg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00127.jpeg)'
- en: Not unexpectedly, the model performs really poorly, given what we have already
    seen. Do not be too surprised by the negative R-squared; it can turn negative,
    that is, a nonsensical value for R-squared, if the predictions of the model are
    nonsensical.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不意外，考虑到我们已经看到的内容，模型表现非常糟糕。不要对负的R平方感到太惊讶；如果模型的预测是荒谬的，R平方可以变成负值，也就是说，R平方的值是不合理的。
- en: Classification metrics
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类指标
- en: 'We will evaluate the two models we built earlier; here is the logistic regression:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将评估我们之前构建的两个模型；这是逻辑回归模型：
- en: '[PRE34]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'And here is the SVM:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这是SVM模型：
- en: '[PRE35]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The two metrics—the area under the **Precision-Recall** (**PR**) and the area
    under the **Receiver Operating Characteristics** (**ROC**) curve—allow us to compare
    the two models.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 两个指标——**精确率-召回率**（**PR**）曲线下的面积和**接收者操作特征**（**ROC**）曲线下的面积——允许我们比较这两个模型。
- en: Check out this interesting discussion about the two metrics on stack exchange: [https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 查看关于这两个指标的有趣讨论：[https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves)。
- en: 'Let''s see what we got. For the logistic regression, we have:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们得到了什么。对于逻辑回归，我们有：
- en: '![](img/00128.jpeg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00128.jpeg)'
- en: 'And for the SVM, we have:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SVM，我们有：
- en: '![](img/00129.jpeg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00129.jpeg)'
- en: 'It comes a bit as a surprise that the SVM performed a bit worse than the logistic
    regression. Let''s see the confusion matrix to see where these two models differ.
    For the logistic regression, we achieve this with the following code:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 有点令人惊讶的是，SVM的表现比逻辑回归稍差。让我们看看混淆矩阵，看看这两个模型的区别在哪里。对于逻辑回归，我们可以用以下代码实现：
- en: '[PRE36]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'And we get:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到：
- en: '![](img/00130.jpeg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00130.jpeg)'
- en: 'For the SVM, the code looks pretty much the same, with the exception of the
    input RDD:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SVM，代码看起来基本相同，唯一的区别是输入RDD：
- en: '[PRE37]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'With the preceding, we get:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述步骤，我们得到：
- en: '![](img/00131.jpeg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00131.jpeg)'
- en: As you can see, the logistic regression is more accurate in predicting both
    the positive and negative cases, thus achieving fewer of the misclassified (false
    positives and false negatives) observations. However, the differences are not
    that dramatic.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，逻辑回归在预测正例和负例时更准确，因此实现了更少的误分类（假阳性和假阴性）观察。然而，差异并不是那么明显。
- en: 'To calculate the overall error rate, we can use the following code:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算总体错误率，我们可以使用以下代码：
- en: '[PRE38]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'For the SVM, the preceding code looks the same, with an exception of using `true_pred_class_svm`
    instead of `true_pred_class_lr`. The preceding produces the following. For the
    logistic regression, we get:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SVM，前面的代码看起来一样，唯一的区别是使用`true_pred_class_svm`而不是`true_pred_class_lr`。前面的产生了以下结果。对于逻辑回归，我们得到：
- en: '![](img/00132.jpeg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00132.jpeg)'
- en: 'For the SVM, the results look as follows:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SVM，结果如下：
- en: '![](img/00133.jpeg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00133.jpeg)'
- en: The error is slightly higher for the SVM, but still a fairly reasonable model.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的误差略高，但仍然是一个相当合理的模型。
- en: See also
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'If you want to learn more about various performance metrics, we suggest you
    visit the following URL: [https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/](https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想了解更多有关各种性能指标的信息，我们建议您访问以下网址：[https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/](https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/)
