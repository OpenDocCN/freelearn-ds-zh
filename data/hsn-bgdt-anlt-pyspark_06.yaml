- en: Putting Structure on Your Big Data with SparkSQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SparkSQL为您的大数据添加结构
- en: In this chapter, we'll learn how to manipulate DataFrames with Spark SQL schemas,
    and use the Spark DSL to build queries for structured data operations. By now
    we have already learned to get big data into the Spark Environment using RDDs
    and carried out multiple operations on that big data. Let us now look that how
    to manipulate our DataFrames and build queries for structured data operations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用Spark SQL模式操作数据框，并使用Spark DSL构建结构化数据操作的查询。到目前为止，我们已经学会了将大数据导入Spark环境使用RDD，并对这些大数据进行多个操作。现在让我们看看如何操作我们的数据框并构建结构化数据操作的查询。
- en: 'In particular, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将涵盖以下主题：
- en: Manipulating DataFrames with Spark SQL schemas
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark SQL模式操作数据框
- en: Using Spark DSL to build queries
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark DSL构建查询
- en: Manipulating DataFrames with Spark SQL schemas
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL模式操作数据框
- en: In this section, we will learn more about DataFrames and learn how to use Spark
    SQL.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习更多关于数据框，并学习如何使用Spark SQL。
- en: The Spark SQL interface is very simple. For this reason, taking away labels
    means that we are in unsupervised learning territory. Also, Spark has great support
    for clustering and dimensionality reduction algorithms. We can tackle learning
    problems effectively by using Spark SQL to give big data a structure.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL接口非常简单。因此，去除标签意味着我们处于无监督学习领域。此外，Spark对聚类和降维算法有很好的支持。通过使用Spark SQL为大数据赋予结构，我们可以有效地解决学习问题。
- en: 'Let''s take a look at the code that we will be using in our Jupyter Notebook. To
    maintain consistency, we will be using the same KDD cup data:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们将在Jupyter Notebook中使用的代码。为了保持一致，我们将使用相同的KDD杯数据：
- en: 'We will first type `textFile` into a `raw_data` variable as follows:'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先将`textFile`输入到`raw_data`变量中，如下所示：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'What''s new here is that we are importing two new packages from `pyspark.sql`:'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新的是我们从`pyspark.sql`中导入了两个新包：
- en: '`Row`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Row`'
- en: '`SQLContext`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SQLContext`'
- en: 'The following code shows us how to import these packages:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码向我们展示了如何导入这些包：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using `SQLContext`, we create a new `sql_context` variable that holds the object
    of the `SQLContext` variable created by PySpark. As we're using `SparkContext`
    to start this `SQLContext` variable, we need to pipe in `sc` as the first parameter
    of the `SQLContext` creator. After this, we need to take our `raw_data` variable
    and map it with the `l.split` lambda function to create an object that holds our
    **comma-separated values** (**CSV**).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`SQLContext`，我们创建一个新的`sql_context`变量，其中包含由PySpark创建的`SQLContext`变量的对象。由于我们使用`SparkContext`来启动这个`SQLContext`变量，我们需要将`sc`作为`SQLContext`创建者的第一个参数。之后，我们需要取出我们的`raw_data`变量，并使用`l.split`lambda函数将其映射为一个包含我们的逗号分隔值（CSV）的对象。
- en: 'We''ll leverage our new important `Row` objects to create a new object that
    has defined labels. This is to label our datasets by what feature we are looking
    at, as follows:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将利用我们的新重要`Row`对象来创建一个新对象，其中定义了标签。这是为了通过我们正在查看的特征对我们的数据集进行标记，如下所示：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we've taken our comma-separated values (`csv`), and we've
    created a `Row` object that takes the first feature, called `duration`; the second
    feature, called `protocol`; and the third feature, called `service`. This directly
    corresponds to our labels in the actual datasets.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们取出了我们的逗号分隔值（csv），并创建了一个`Row`对象，其中包含第一个特征称为`duration`，第二个特征称为`protocol`，第三个特征称为`service`。这直接对应于实际数据集中的标签。
- en: 'Now, we can create a new DataFrame by calling the `createDataFrame` function
    in our `sql_context` variable. To create this DataFrame, we need to feed in our
    row data objects, and the resulting object would be a DataFrame in `df`. After
    this, we need to register a temporary table. Here, we are just calling it `rdd`.
    By doing this, we can now use ordinary SQL syntax to query the content in this
    temporary table that is constructed by our rows:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以通过在`sql_context`变量中调用`createDataFrame`函数来创建一个新的数据框。要创建这个数据框，我们需要提供我们的行数据对象，结果对象将是`df`中的数据框。之后，我们需要注册一个临时表。在这里，我们只是称之为`rdd`。通过这样做，我们现在可以使用普通的SQL语法来查询由我们的行构造的临时表中的内容：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In our example, we need to select the duration from `rdd`, which is a temporary
    table. The protocol we have selected here is equal to `''tcp''`, and the duration,
    which is our first feature in a row, is larger than `2000`, as demonstrated in
    the following code snippet:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们需要从`rdd`中选择`duration`，这是一个临时表。我们在这里选择的协议等于`'tcp'`，而我们在一行中的第一个特征是大于`2000`的`duration`，如下面的代码片段所示：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, when we call the `show` function, it gives us every single data point
    that matches these criteria:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，当我们调用`show`函数时，它会给我们每个符合这些条件的数据点：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will then get the following output:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将得到以下输出：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using the preceding example, we can infer that we can use the `SQLContext` variable
    from the PySpark package to package our data in a SQL friendly format.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的示例，我们可以推断出我们可以使用PySpark包中的`SQLContext`变量将数据打包成SQL友好格式。
- en: Therefore, not only does PySpark support using SQL syntax to query the data,
    but it can also use the Spark **domain-specific language** (**DSL**) to build
    queries for structured data operations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，PySpark不仅支持使用SQL语法查询数据，还可以使用Spark领域特定语言（DSL）构建结构化数据操作的查询。
- en: Using Spark DSL to build queries
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark DSL构建查询
- en: 'In this section, we will use Spark DSL to build queries for structured data
    operations:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Spark DSL构建结构化数据操作的查询：
- en: 'In the following command, we have used the same query as used earlier; this
    time expressed in the Spark DSL to illustrate and compare how using the Spark
    DSL is different, but achieves the same goal as our SQL is shown in the previous
    section:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下命令中，我们使用了与之前相同的查询；这次使用了Spark DSL来说明和比较使用Spark DSL与SQL的不同之处，但实现了与我们在前一节中展示的SQL相同的目标：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this command, we first take the `df` object that we created in the previous
    section. We then select the duration by calling the `select` function and feeding
    in the `duration` parameter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个命令中，我们首先取出了在上一节中创建的`df`对象。然后我们通过调用`select`函数并传入`duration`参数来选择持续时间。
- en: 'Next, in the preceding code snippet, we call the `filter` function twice, first
    by using `df.duration`, and the second time by using `df.protocol`. In the first
    instance, we are trying to see whether the duration is larger than `2000`, and
    in the second instance, we are trying to see whether the protocol is equal to
    `"tcp"`. We also need to append the `show` function at the very end of the command
    to get the same results, as shown in the following code block:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在前面的代码片段中，我们两次调用了`filter`函数，首先使用`df.duration`，第二次使用`df.protocol`。在第一种情况下，我们试图查看持续时间是否大于`2000`，在第二种情况下，我们试图查看协议是否等于`"tcp"`。我们还需要在命令的最后附加`show`函数，以获得与以下代码块中显示的相同结果。
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we have the top 20 rows of the data points again that fit the description
    of the code used to get this result.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们再次有了符合代码描述的前20行数据点的结果。
- en: Summary
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered Spark DSL and learned how to build queries. We also
    learned how to manipulate DataFrames with Spark SQL schemas, and then we used
    Spark DSL to build queries for structured data operations. Now that we have a
    good knowledge of Spark, let's look at a few tips, tricks, and techniques in Apache
    Spark in the following chapters.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了Spark DSL，并学习了如何构建查询。我们还学习了如何使用Spark SQL模式操纵DataFrames，然后我们使用Spark
    DSL构建了结构化数据操作的查询。现在我们对Spark有了很好的了解，让我们在接下来的章节中看一些Apache Spark中的技巧和技术。
- en: In the next chapter, we will look at transformations and actions in an Apache
    Spark program.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看一下Apache Spark程序中的转换和操作。
