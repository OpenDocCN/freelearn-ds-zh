- en: Chapter 1. Why Do You Need Clean Data?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章 为什么需要清洁数据？
- en: Big data, data mining, machine learning, and visualization—it seems like data
    is at the center of everything great happening in computing lately. From statisticians
    to software developers to graphic designers, everyone is suddenly interested in
    data science. The confluence of cheap hardware, better processing and visualization
    tools, and massive amounts of freely available data means that we can now discover
    trends and make predictions more accurately and more easily than ever before.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据、数据挖掘、机器学习和可视化——看起来数据是最近计算领域中一切伟大事件的中心。从统计学家到软件开发人员再到图形设计师，每个人都突然对数据科学感兴趣。便宜的硬件、更好的处理和可视化工具以及大量免费可用的数据的结合意味着我们现在可以比以往更准确更轻松地发现趋势和进行预测。
- en: What you might *not* have heard, though, is that all of these data science hopes
    and dreams are predicated on the fact that data is messy. Usually, data has to
    be moved, compressed, cleaned, chopped, sliced, diced, and subjected to any number
    of other transformations before it is ready to be used in the algorithms or visualizations
    that we think of as the heart of **data science**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可能*没有*听说过的是，所有这些数据科学的希望和梦想都基于数据是混乱的事实。通常情况下，数据需要被移动、压缩、清洗、切割、切片、切块，并经历任何其他变换，然后才能准备好用于我们认为是数据科学核心的算法或可视化。
- en: 'In this chapter, we will cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: A simple six-step process you can follow for data science, including cleaning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的包括数据清洁在内的数据科学六步骤流程
- en: Helpful guidelines to communicate how you cleaned your data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有助于传达如何清洁数据的有用指南
- en: Some tools that you might find helpful for data cleaning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些你可能会发现对数据清洁有帮助的工具
- en: An introductory example that shows how data cleaning fits into the overall data
    science process
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的例子，展示数据清洁如何融入整体数据科学过程中
- en: A fresh perspective
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一种新鲜的视角
- en: 'We recently read that The New York Times called data cleaning **janitor work**
    and said that 80 percent of a data scientist''s time will be spent doing this
    kind of cleaning. As we can see in the following figure, despite its importance,
    data cleaning has not really captured the public imagination in the same way as
    big data, data mining, or machine learning:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最近读到《纽约时报》称数据清洁为**清洁工作**，并称数据科学家80%的时间将花在这种清洁工作上。正如我们在下图中看到的那样，尽管其重要性，数据清洁并没有像大数据、数据挖掘或机器学习那样真正吸引公众的想象力：
- en: '![A fresh perspective](img/image00228.jpeg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![一种新鲜的视角](img/image00228.jpeg)'
- en: Who can blame us for not wanting to gather in droves to talk about how fun and
    super-cool janitor work is? Well, unfortunately—and this is true for actual housekeeping
    chores as well—we would all be a lot better off if we just got the job done rather
    than ignoring it, complaining about it, and giving it various demeaning names.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 谁能责怪我们不想成群结队地讨论清洁工作有多有趣和超级酷呢？不幸的是——这对于实际的家务活也是如此——如果我们只是完成工作而不是忽视它、抱怨它并给它各种贬低的名字，我们所有人都会更好。
- en: Not convinced yet? Consider a different metaphor instead, you are not a data
    janitor; you are a data chef. Imagine you have been handed a market basket overflowing
    with the most gorgeous heirloom vegetables you have ever seen, each one handpicked
    at the peak of freshness and sustainably produced on an organic farm. The tomatoes
    are perfectly succulent, the lettuce is crisp, and the peppers are bright and
    firm. You are excited to begin cooking, but you look around and the kitchen is
    filthy, the pots and pans have baked-on, caked-on who-knows-what, and, as for
    tools, you have nothing but a rusty knife and a soggy towel. The sink is broken
    and you just saw a beetle crawl out from under that formerly beautiful lettuce.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 还不信？考虑一个不同的隐喻，你不是数据清洁工；你是数据大厨。想象一下，你被交给了一篮子盛满了你曾经见过的最美味的传统蔬菜，每一种都在新鲜和有机农场上经过精心挑选并且可持续生产。番茄是完美多汁的，生菜是脆的，辣椒是明亮而坚固的。你兴奋地开始烹饪，但四周都是脏乱的厨房，锅碗瓢盆上堆积着不知道是什么的脏污，而且，对于工具，你只有一把生锈的刀和一块潮湿的毛巾。水槽坏了，你刚刚看到一只甲虫从那些曾经美丽的生菜下爬出来。
- en: Even a beginner chef knows you should not cook in a place like this. At the
    very least, you will destroy that perfectly good delicious basket of goodies you
    have been given. And at worst, you will make people sick. Plus, cooking like this
    is not even fun, and it would take all day to chop the veggies with an old rusty
    knife.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 即便是初学者厨师也知道，你不应该在这样的地方做饭。至少，你会毁掉那一篮子原本美味的食材。而最糟糕的是，你会让人们生病。而且像这样做饭根本不有趣，用一把生锈的刀切菜整天都切不完。
- en: Just as you would in a kitchen, it's definitely worth spending time cleaning
    and preparing your data science workspace, your tools, and your raw materials
    upfront. The old computer programming adage from the 1960s—garbage in, garbage
    out—is also true with data science.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在厨房里一样，提前花时间清理和准备你的数据科学工作空间、工具和原材料是绝对值得的。1960年代的老计算机编程格言“垃圾进，垃圾出”在数据科学中同样适用。
- en: The data science process
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学流程
- en: How does cleaning fit into the rest of the job of data science? Well, the short
    answer is that it's a critical piece, and it directly affects the processes that
    come before and after it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 清理数据如何融入数据科学的其余工作中呢？简而言之，它是一个关键环节，并且直接影响到前后的各个过程。
- en: 'The longer answer relies on describing the data science process in six steps,
    as shown in the following lists. Data cleaning is right in the middle, at step
    3\. But rather than thinking of these steps as a linear, start-to-finish framework,
    we will revisit the steps as needed several times over the course of a project
    in more of an iterative manner. It is also worth pointing out that not every project
    will have all the steps; for example, sometimes, we do not have a collection step
    or a visualization step. It really depends on the particular needs of the project:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 更长的答案依赖于通过六个步骤来描述数据科学过程，如下所示。数据清理正处于中间阶段，位于步骤3。与其将这些步骤视为一个线性的、从开始到结束的框架，我们将根据需要在项目过程中多次回顾这些步骤，采用更迭代的方式进行处理。还值得指出的是，并非每个项目都会包含所有步骤；例如，有时我们没有收集步骤或可视化步骤。这完全取决于项目的具体需求：
- en: The first step is to come up with the problem statement. Identify what the problem
    you are trying to solve is.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是提出问题陈述。明确你要解决的问题是什么。
- en: The next step is data collection and storage. Where did the data come from that
    is helping you answer this question? Where did you store it and in what format?
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是数据收集与存储。帮助你回答这个问题的数据来自哪里？你将其存储在哪里，是什么格式？
- en: Then comes data cleaning. Did you change the data at all? Did you delete anything?
    How did you prepare it for the analysis and mining step next?
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是数据清理。你是否对数据进行了任何更改？删除了什么吗？你是如何为接下来的分析和挖掘步骤做准备的？
- en: The next step is data analysis and machine learning. What kind of processing
    did you do to the data? What transformations? What algorithms did you use? What
    formulas did you apply? What machine learning algorithms did you use? In what
    order?
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是数据分析和机器学习。你对数据做了什么样的处理？进行了哪些转换？使用了什么算法？应用了什么公式？用了哪些机器学习算法？顺序是什么？
- en: Representation and visualization is the next step. How do you show the results
    of your work? This can be one or more tables, drawings, graphs, charts, network
    diagrams, word clouds, maps, and so on. Is this the best visualization to represent
    the data? What alternatives did you consider?
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 表示与可视化是下一步。你如何展示你的工作结果？这可以是一个或多个表格、图纸、图表、图形、网络图、词云、地图等等。这是最适合表示数据的可视化方式吗？你考虑过哪些替代方案？
- en: The last step is problem resolution. What is the answer to the question or problem
    you posed in step 1? What limitations do you have on your results? Were there
    parts of the question that you could not answer with this method? What could you
    have done differently? What are the next steps?
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是问题解决。你在第一步提出的问题或问题的答案是什么？你的结果有什么局限性？是否有些问题你无法通过此方法回答？你本可以做些什么不同的事情？下一步该做什么？
- en: It makes sense that data cleaning needs to happen before you attempt the analysis
    / mining / machine learning or visualization steps. Although, remember that, as
    this is an iterative process, we may revisit cleaning several times during the
    course of a project. Also, the type of mining or analysis we are going to do will
    often drive the way we clean the data. We consider cleaning to include a variety
    of tasks that may be dictated by the analysis method chosen, for example, swapping
    file formats, changing character encodings, or parsing out pieces of data to operate
    on.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理在进行分析/挖掘/机器学习或可视化步骤之前是有道理的。不过，记住，由于这是一个迭代过程，在项目进行过程中，我们可能需要多次回顾清理工作。此外，我们将要进行的挖掘或分析类型通常会影响我们清理数据的方式。我们认为清理包括一系列任务，这些任务可能会根据所选的分析方法而有所不同，例如，转换文件格式、改变字符编码，或者解析数据中的某些部分进行处理。
- en: Data cleaning is also going to be very closely tied to the collection and storage
    step (step 2). This means that you may have to collect raw data, store it, clean
    it, store the cleaned data again, collect some more, clean that, combine it with
    the previous data, clean it again, store it, and so on. As such, it is going to
    be very important to remember what you did and be able to repeat that process
    again if needed or tell someone else what you did.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理还将与数据收集和存储步骤（步骤2）紧密相关。这意味着你可能需要先收集原始数据，存储它，清理它，然后再存储已清理的数据，接着收集一些更多的数据，清理这些数据，将其与之前的数据合并，再次清理，存储，依此类推。因此，记住你做了什么，并能够在需要时重复该过程或告诉别人你做了什么，变得非常重要。
- en: Communicating about data cleaning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据清理的沟通
- en: As the six-step process is organized around a story arc that starts with a question
    and ends with the resolution of the question, it works nicely as a reporting framework.
    If you decide to use the six-step framework as a way to report on your data science
    process, you will find that you get to write about your cleaning in the third
    step.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于六步过程是围绕一个故事情节组织的，从一个问题开始，最后解决该问题，因此它非常适合作为报告框架。如果你决定将六步框架作为报告数据科学过程的一种方式，你会发现，在第三步，你将能够写出关于数据清理的内容。
- en: But even if you do not document your data science process in a formal report,
    you will find that it is extremely helpful to keep careful notes of what you did
    to the data and in what order.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使你没有以正式报告的形式记录你的数据科学过程，你会发现，仔细记录你对数据所做的事情及其顺序是非常有帮助的。
- en: 'Remember that even for the smallest, lowest-stakes project, you are always
    working for an audience of at least two people: you now and you 6 months from
    now. Believe me when I tell you that the you-in-six-months isn''t going to remember
    what the you-of-today did to clean your data, much less why you did it or how
    to do it again!'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，即使是最小、风险最低的项目，你也总是为至少两个人服务：现在的你和六个月后的你。相信我，当我告诉你六个月后的你不会记得今天的你是如何清理数据的，更不用说为什么这么做或如何再次进行清理了！
- en: 'The simplest solution for this is to just keep a **log** of what you did. The
    log should include links, screenshots, or copy and pastes of the specific commands
    you ran as well as a short explanation for why you did this. The following example
    shows a log for a very small text mining project with embedded links to the output
    files at each stage as well as links to the cleaning scripts. Don''t worry if
    you are unfamiliar with some of the technologies mentioned in this log. This example
    shows you what a log might look like:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的解决方案就是记录一个**日志**，记录你所做的事情。日志应包括链接、截图或你运行的具体命令的复制粘贴，并附上简短的解释说明为什么这么做。以下示例展示了一个非常小的文本挖掘项目的日志，其中嵌入了每个阶段输出文件的链接，以及清理脚本的链接。如果你对日志中提到的一些技术不熟悉，请不要担心。这个例子会告诉你日志可能是什么样子的：
- en: We wrote a SQL query to retrieve a list of every item and its description.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们写了一个SQL查询来检索每个项目及其描述的列表。
- en: In order to conduct a term frequency analysis in Python, we needed data in a
    specific JSON format. We constructed a PHP script that looped through the results
    of our query, placing its results in a JSON file (version 1).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在Python中进行词频分析，我们需要特定格式的JSON数据。我们构建了一个PHP脚本，遍历查询的结果，将其结果存放在一个JSON文件中（版本1）。
- en: This file had some formatting errors, such as unescaped quotation marks and
    embedded HTML tags. These errors were corrected with a second PHP script, which
    when run, printed this cleaned JSON file (version 2).
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个文件存在一些格式错误，比如未转义的引号和嵌入的HTML标签。这些错误通过第二个PHP脚本得到了修正，运行该脚本后，会打印出这个已清理的JSON文件（版本2）。
- en: Note that our log tries to explain what we did and why we did it. It is short
    and can include links when possible.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的日志尝试解释我们做了什么以及为什么这么做。它简短且可以在可能的情况下包含链接。
- en: There are many more sophisticated solutions to communicate about data cleaning
    should you choose to use them, for example, if you are familiar with **version
    control systems** such as **Git** or Subversion, which are usually used to manage
    a software project, you can probably conceive how to extend them to keep track
    of your data cleaning. Whatever system you choose, even if it is a simple log,
    the most important thing is to actually use it. So, pick something that will encourage
    its use and not impede your progress.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意使用更复杂的解决方案来进行数据清理的管理，有很多选择，例如，如果你熟悉**版本控制系统**，如**Git**或Subversion，这些通常用于管理软件项目，那么你可能能想到如何扩展它们来跟踪数据清理的过程。无论你选择什么系统，哪怕是一个简单的日志，最重要的是实际上去使用它。因此，选择一个能鼓励你使用它而不是妨碍你进展的工具。
- en: Our data cleaning environment
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的数据清理环境
- en: The approach to data cleaning we are using in this book is a general-purpose,
    widely applicable one. It does not require or assume that you have any high-end
    specialty single-vendor database or data analysis products (in fact, these vendors
    and products may have their own cleaning routines or methods). I have designed
    the cleaning tutorials in this book around common, everyday issues that you might
    encounter when using real-world datasets. I have designed the book around real-world
    data that anyone can access. I'll show you how to clean data using open source,
    general-purpose software and technologies that are easy to get and are commonly
    used in the workplace.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用的数据清理方法是一种通用且广泛适用的方法。它不要求或假设你拥有任何高端的单一供应商数据库或数据分析产品（事实上，这些供应商和产品可能有自己的清理程序或方法）。我设计了本书中的清理教程，围绕你在使用现实世界数据集时可能遇到的常见问题。我将本书设计为围绕任何人都可以访问的真实数据。我将向你展示如何使用开源的通用软件和技术来清理数据，这些工具易于获得且在职场中被广泛使用。
- en: 'Here are some of the tools and technologies that you should be ready to use:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你需要准备使用的一些工具和技术：
- en: For nearly every chapter, we will use a terminal window and its command-line
    interface, such as the Terminal program on Mac OSX or bash on a Linux system.
    In Windows, some commands will be able to be run using Windows Command Prompt,
    but other commands may require the use of a more full-featured Windows command-line
    program, such as CygWin.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于几乎每一章，我们将使用一个终端窗口及其命令行界面，例如Mac OSX上的Terminal程序或Linux系统中的bash。在Windows上，某些命令可以通过Windows命令提示符运行，但其他命令可能需要使用更全功能的Windows命令行程序，如CygWin。
- en: For nearly every chapter, we will use a text editor or programmer's editor,
    such as Text Wrangler on a Mac, vi or emacs in Linux, or Notepad++ or Sublime
    Editor on Windows.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于几乎每一章，我们将使用文本编辑器或程序员编辑器，例如Mac上的Text Wrangler，Linux中的vi或emacs，或Windows中的Notepad++或Sublime
    Editor。
- en: For most chapters, we will need a Python 2.7 client, such as Enthought Canopy,
    and we will need enough permissions to install packages. Many of the examples
    will work with Python 3, but some will not, so if you already have that, you may
    wish to create an alternate 2.7 installation.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大多数章节，我们将需要一个Python 2.7客户端，例如Enthought Canopy，并且我们需要足够的权限来安装软件包。许多示例将适用于Python
    3，但有些则不适用，因此，如果你已经安装了Python 3，可能需要创建一个备用的2.7安装环境。
- en: For [Chapter 3](part0024.xhtml#aid-MSDG2 "Chapter 3. Workhorses of Clean Data
    – Spreadsheets and Text Editors"), *Workhorses of Clean Data – Spreadsheets and
    Text Editors*, we will need a spreadsheet program (we will focus on Microsoft
    Excel and Google Spreadsheets).
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于[第3章](part0024.xhtml#aid-MSDG2 "第3章. 清洁数据的主力工具——电子表格和文本编辑器")，*清洁数据的主力工具——电子表格和文本编辑器*，我们将需要一个电子表格程序（我们将专注于Microsoft
    Excel和Google Spreadsheets）。
- en: For [Chapter 7](part0045.xhtml#aid-1AT9A1 "Chapter 7. RDBMS Cleaning Techniques"),
    *RDBMS Cleaning Techniques*, we will need a working MySQL installation and the
    client software to access it.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于[第7章](part0045.xhtml#aid-1AT9A1 "第7章. RDBMS清理技术")，*RDBMS清理技术*，我们将需要一个可用的MySQL安装和客户端软件来访问它。
- en: An introductory example
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个入门示例
- en: To get started, let's sharpen our chef's knife with a small example that integrates
    the six-step framework and illustrates how to tackle a few simple cleaning issues.
    This example uses the publicly available Enron e-mail dataset. This is a very
    famous dataset consisting of e-mail messages sent to, from, and between employees
    working at the now-defunct Enron Corporation. As part of the U.S. Government investigation
    into accounting fraud at Enron, the e-mails became part of the public record and
    are now downloadable by anyone. Researchers in a variety of domains have found
    the e-mails helpful for studying workplace communication, social networks, and
    more.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，让我们用一个小例子来锐化我们的厨刀，该例子结合了六步框架，并说明如何处理一些简单的数据清理问题。这个例子使用了公开的Enron电子邮件数据集。这个数据集非常著名，包含了已故的Enron公司员工之间发送和接收的电子邮件。作为美国政府对Enron会计欺诈的调查的一部分，这些电子邮件成为了公共记录，现在任何人都可以下载。来自多个领域的研究人员发现这些电子邮件对于研究职场沟通、社交网络等非常有帮助。
- en: Note
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You can read more about Enron and the financial scandal that led to its demise
    on its Wikipedia page at [http://en.wikipedia.org/wiki/Enron](http://en.wikipedia.org/wiki/Enron),
    and you can read about the Enron e-mail corpus itself on its separate page at
    [http://en.wikipedia.org/wiki/Enron_Corpus](http://en.wikipedia.org/wiki/Enron_Corpus).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[http://en.wikipedia.org/wiki/Enron](http://en.wikipedia.org/wiki/Enron)上阅读更多关于Enron及其导致公司倒闭的财务丑闻的信息，此外，你还可以在[http://en.wikipedia.org/wiki/Enron_Corpus](http://en.wikipedia.org/wiki/Enron_Corpus)上了解更多关于Enron电子邮件数据集的信息。
- en: In this example, we will implement the six-step framework on a simple data science
    question. Suppose we want to reveal trends and patterns in e-mail usage over time
    within Enron Corporation. Let's start by counting messages that were sent to/from
    Enron employees by date. We will then show the counts visually on a graph over
    time.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将在一个简单的数据科学问题上实现六步框架。假设我们想揭示Enron公司内电子邮件使用的趋势和模式。我们将从按日期统计发送/接收的电子邮件开始。然后，我们将在图表上按时间展示这些计数。
- en: First, we need to download the MySQL Enron corpus using the instructions at
    [http://www.ahschulz.de/enron-email-data/](http://www.ahschulz.de/enron-email-data/).
    Another (backup) source for this file is [https://www.cs.purdue.edu/homes/jpfeiff/enron.html](https://www.cs.purdue.edu/homes/jpfeiff/enron.html).
    Following these instructions, we will need to import the data into a new database
    scheme called **Enron** on a MySQL server. The data is now ready to be queried
    using either the MySQL command-line interface or using a web-based tool such as
    PHPMyAdmin.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要按照[http://www.ahschulz.de/enron-email-data/](http://www.ahschulz.de/enron-email-data/)上的说明下载MySQL
    Enron数据集。另一个（备用）下载源是[https://www.cs.purdue.edu/homes/jpfeiff/enron.html](https://www.cs.purdue.edu/homes/jpfeiff/enron.html)。按照这些说明，我们需要将数据导入到MySQL服务器上的一个新的数据库架构中，名为**Enron**。数据现在已经可以通过MySQL命令行界面或通过PHPMyAdmin等基于Web的工具进行查询。
- en: 'Our first count query is shown as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个计数查询如下所示：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Right away, we notice that numerous e-mails have incorrect dates, for example,
    there are a number of dates that seem to predate or postdate the existence of
    the corporation (for example, 1979) or that were from years that were illogical
    (for example, 0001 or 2044). E-mail is old but not *that* old!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 立刻，我们注意到有许多电子邮件的日期不正确，例如，有一些日期似乎早于或晚于公司存在的时间（例如，1979年），或者是一些年份显得不合逻辑（例如，0001年或2044年）。电子邮件很旧，但可不是*那么*旧！
- en: 'The following table shows an excerpt of a few of the weird lines (the complete
    result set is about 1300 rows long) All of these dates are formatted correctly;
    however, some of the dates are definitely wrong:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了部分异常行的摘录（完整的结果集大约有1300行）。所有这些日期格式都正确；然而，一些日期显然是错误的：
- en: '| dateSent | numMsg |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| dateSent | numMsg |'
- en: '| --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `0002-03-05` | `1` |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `0002-03-05` | `1` |'
- en: '| `0002-03-07` | `3` |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `0002-03-07` | `3` |'
- en: '| `0002-03-08` | `2` |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `0002-03-08` | `2` |'
- en: '| `0002-03-12` | `1` |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `0002-03-12` | `1` |'
- en: '| `1979-12-31` | `6` |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `1979-12-31` | `6` |'
- en: '| `1997-01-01` | `1` |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `1997-01-01` | `1` |'
- en: '| `1998-01-04` | `1` |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `1998-01-04` | `1` |'
- en: '| `1998-01-05` | `1` |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `1998-01-05` | `1` |'
- en: '| `1998-10-30` | `3` |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `1998-10-30` | `3` |'
- en: 'These bad dates are most likely due to misconfigured e-mail clients. At this
    point, we have three choices for what to do:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些错误的日期很可能是由于邮件客户端配置不当造成的。此时，我们有三种选择来处理：
- en: '**Do nothing**: Maybe we can just ignore the bad data and get away with building
    the line graph anyway. But, as the lowest bad date was from the year 0001 and
    the highest was from the year 2044, we can imagine our line graph with the 1300
    tick marks on the time axis, each showing a count of 1 or 2\. This graph does
    not sound very appealing or informative, so doing nothing will not work.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**什么都不做**：也许我们可以忽略错误数据，照样制作折线图。但是，最低的错误日期来自公元0001年，最高的来自2044年，我们可以想象我们的折线图上有1300个时间轴刻度，每个刻度显示1或2。这个图看起来既不吸引人，也不具备信息价值，因此什么都不做是不可行的。'
- en: '**Fix the data**: We could try to figure out what the correct date for each
    bad message was and produce a corrected dataset that we can then use to build
    our graph.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修复数据**：我们可以尝试找出每条错误消息的正确日期，并生成一个修正后的数据集，然后用来构建我们的图表。'
- en: '**Throw out the affected e-mails**: We can just make an informed decision to
    discard any e-mail that has a date that falls outside a predetermined window.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**丢弃受影响的电子邮件**：我们可以做出一个明智的决定，丢弃任何日期超出预定范围的电子邮件。'
- en: 'In order to decide between options 2 and 3, we will need to count how many
    messages will be affected using only a 1999-2002 window. We can use the following
    SQL:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在选项2和选项3之间做出决定，我们需要使用1999-2002年的窗口来统计受影响的消息数量。我们可以使用以下SQL：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '325 messages with bad dates may initially seem like a lot, but then again,
    they are only about 1 percent of the entire dataset. Depending on our goals, we
    might decide to fix these dates manually, but let''s assume here that we do not
    mind losing 1 percent of the messages. We can proceed cautiously toward option
    3, throwing out the affected e-mails. Here is the amended query:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 325条带有错误日期的消息最初看起来很多，但实际上它们仅占整个数据集的1%左右。根据我们的目标，我们可能决定手动修正这些日期，但这里假设我们不介意丢失1%的消息。我们可以谨慎地选择第3个选项，丢弃受影响的电子邮件。以下是修改后的查询：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The cleaned data now consists of 1,211 rows, each with a count. Here is an
    excerpt of the new dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 清理后的数据现在包含1211行，每行都有一个计数。以下是新数据集的一个片段：
- en: '| dateSent | numMsg |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| dateSent | numMsg |'
- en: '| --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `1998-01-04` | `1` |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `1998-01-04` | `1` |'
- en: '| `1998-01-05` | `1` |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `1998-01-05` | `1` |'
- en: '| `1998-10-30` | `3` |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `1998-10-30` | `3` |'
- en: '| `1998-11-02` | `1` |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `1998-11-02` | `1` |'
- en: '| `1998-11-03` | `1` |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `1998-11-03` | `1` |'
- en: '| `1998-11-04` | `4` |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `1998-11-04` | `4` |'
- en: '| `1998-11-05` | `1` |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `1998-11-05` | `1` |'
- en: '| `1998-11-13` | `2` |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `1998-11-13` | `2` |'
- en: In this example, it looks like there are two questionable dates in January 1998
    and no other messages until October, at which point, the messages start coming
    in more regularly. This seems weird, and it also points to another issue, is it
    important that we have every date on the *x* axis, even if there were no e-mails
    sent that day?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，看起来1998年1月有两个可疑的日期，直到10月才开始有规律地收到消息。这有些奇怪，同时也引出了另一个问题，我们是否需要在 *x* 轴上显示每一个日期，即使那一天没有发送电子邮件？
- en: If we answer yes, it is important to show every date, even those with 0 counts;
    this may mean going through another round of cleaning in order to produce rows
    that show the date with a zero.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回答“是”，即使某些日期的计数为零，也需要显示每个日期；这可能意味着需要再进行一轮数据清理，以便生成显示零值日期的行。
- en: But then again, maybe we can be more strategic about this. Whether we need to
    have zero values in our raw data actually depends on what tool we are using to
    create the graph and what type of graph it is, for example, Google Spreadsheets
    will build a line or bar graph that can automatically detect that there are missing
    dates on the *x* axis and will fill in zero values even if they are not given
    in the initial dataset. In our data, these zero values would be the mysterious
    missing dates from most of 1998.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 但也许我们可以更有策略性地考虑这个问题。是否需要在原始数据中保留零值，实际上取决于我们使用的工具和图表的类型。例如，Google 表格会构建一个折线图或条形图，自动检测
    *x* 轴上缺失的日期，并且即使初始数据集中没有给出，仍会填充零值。在我们的数据中，这些零值将代表1998年大部分时间里缺失的日期。
- en: 'The next three figures show each of these tools and how they handle zero values
    on a date axis. Note the long zero tails at the beginning and end of the Google
    Spreadsheets representation of the data shown here:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的三张图展示了这些工具及它们如何处理日期轴上的零值。请注意，Google 表格中数据的表示方式在开始和结束处有较长的零尾：
- en: '![An introductory example](img/image00229.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![示例图](img/image00229.jpeg)'
- en: Google Spreadsheets automatically fills in any missing days with a zero.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Google 表格会自动填充任何缺失的日期为零。
- en: The D3 JavaScript visualization library will do the same, filling in zero values
    for missing dates in a range by default, as shown in the next graph.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: D3 JavaScript 可视化库也会执行相同的操作，默认情况下填充缺失日期的零值，如下图所示。
- en: Tip
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'For a simple D3 line graph example, take a look at this tutorial: [http://bl.ocks.org/mbostock/3883245](http://bl.ocks.org/mbostock/3883245).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个简单的 D3 折线图示例，看看这个教程：[http://bl.ocks.org/mbostock/3883245](http://bl.ocks.org/mbostock/3883245)。
- en: '![An introductory example](img/image00230.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![入门示例](img/image00230.jpeg)'
- en: D3 automatically fills in any missing days with a zero.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: D3 会自动用零填充任何缺失的日期。
- en: 'Excel also has identical date-filling behavior in its default line graph, as
    shown here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Excel 在其默认折线图中也有相同的日期填充行为，如下所示：
- en: '![An introductory example](img/image00231.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![入门示例](img/image00231.jpeg)'
- en: Excel automatically fills in any missing days with a zero.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Excel 会自动用零填充任何缺失的日期。
- en: Next, we need to consider whether, by allowing zero values for dates, we are
    also making our *x* axis substantially longer (my count query yielded 1211 rows,
    but there are a total of 1822 days in the range specified, which is 1998-2002).
    Maybe showing zero count days might not work; if the graph is so crowded, we cannot
    see the gaps anyway.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要考虑的是，允许日期的零值是否会使我们的 *x* 轴变得更长（我的计数查询返回了 1211 行，但指定的日期范围总共有 1822 天，范围是
    1998-2002）。也许显示零计数的日期并不适用；如果图表太拥挤，我们反正也看不见间隙。
- en: 'To compare, we can quickly run the same data into Google Spreadsheets (you
    can do this in Excel or D3 too), but this time, we will only select our count
    column to build the graph, thereby forcing Google Spreadsheets to *not* show dates
    on the *x* axis. The result is the true shape of only the data that came from
    the database count query with no zero count days filled in. The long tails are
    gone, but the overall shape of the important part of the graph (the middle) remains
    the same:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，我们可以快速将相同的数据导入到 Google 表格（你也可以在 Excel 或 D3 中做这件事），但这次我们只选择计数列来构建图表，从而强制
    Google 表格 *不* 显示 *x* 轴上的日期。结果是只显示来自数据库计数查询的真正数据形状，未填充零计数日期。长尾消失了，但图表的关键部分（中间部分）的整体形状保持不变：
- en: '![An introductory example](img/image00232.jpeg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![入门示例](img/image00232.jpeg)'
- en: The graph now shows only dates with one or more message.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，图表仅显示有一条或多条消息的日期。
- en: Lucky for us, the shape of the data is similar, save for a shorter head and
    tail on the graph. Based on this comparison, and based on what we plan to do with
    the data (remember that all we wanted to do was create a simple line graph), we
    can feel good about our decision to move forward without specifically creating
    a dataset showing zero count days.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，数据的形状相似，除了图表的头部和尾部较短。基于这种比较，并且基于我们计划对数据进行的操作（记住，我们的目标只是创建一个简单的折线图），我们可以放心继续推进，而不需要专门创建一个包含零计数日期的数据集。
- en: When all is said and done, the line graphs reveal that Enron had several significant
    peaks in e-mail traffic. The largest peaks and heaviest traffic occurred in the
    October and November of 2001, when the scandal broke. The two smaller peaks occurred
    around June 26-27 of 2001 and December 12-13 of 2000, when similar newsworthy
    events involving Enron transpired (one involving the California energy crisis
    and another involving a leadership change at the company).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有操作后，折线图显示了安然公司在电子邮件流量方面的几次显著峰值。最大的一些峰值和最重的流量出现在 2001 年 10 月和 11 月，当时丑闻爆发。两个较小的峰值发生在
    2001 年 6 月 26-27 日和 2000 年 12 月 12-13 日，那时安然发生了类似的引人注目的事件（一次是加利福尼亚能源危机，另一次是公司领导层变动）。
- en: If you get excited by data analysis, you probably have all sorts of cool ideas
    for what to do next with this data. And now that you have cleaned data, it will
    make your analysis tasks easier, hopefully!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对数据分析感到兴奋，你可能会对如何进一步处理这些数据有各种酷点子。现在，既然你已经清理了数据，分析工作应该会更轻松，希望如此！
- en: Summary
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: After all that work, it looks like The New York Times was right. As you can
    see from this simple exercise, data cleaning indeed comprises about 80 percent
    of the effort of answering even a tiny data-oriented question (in this case, talking
    through the rationale and choices for data cleaning took 700 words out of the
    900-word case study). Data cleaning really is a pivotal part of the data science
    process, and it involves understanding technical issues and also requires us to
    make some value judgments. As part of data cleaning, we even had to take into
    account the desired outcomes of both the analysis and visualization steps even
    though we had not really completed them yet.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这么多的工作，看起来《纽约时报》说得对。从这个简单的例子中可以看出，数据清理确实占据了解答一个即使是微小数据相关问题的80%的工作量（在这个案例中，关于数据清理的
    rationale 和选择的讨论就占用了900字案例研究中的700字）。数据清理真的是数据科学过程中的关键部分，它不仅涉及理解技术问题，还需要我们做出一些价值判断。作为数据清理的一部分，我们甚至必须考虑到分析和可视化步骤的预期结果，尽管我们还没有真正完成这些步骤。
- en: After considering the role of data cleaning as presented in this chapter, it
    becomes even more obvious how improvements in our cleaning effectiveness could
    quickly add up to substantial time savings.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑本章所呈现的数据清理角色后，我们会更加明显地意识到，提升数据清理效率能够迅速转化为大量的时间节省。
- en: The next chapter will describe a few of the fundamentals that will be required
    for any "data chef" who wants to move into a bigger, better "kitchen", including
    file formats, data types, and character encodings.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍一些基本知识，任何想要进入更大、更好的“厨房”的“数据大厨”都需要掌握这些，包括文件格式、数据类型和字符编码等内容。
