- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Group By
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分组操作
- en: One of the most fundamental tasks during data analysis involves splitting data
    into independent groups before performing a calculation on each group. This methodology
    has been around for quite some time, but has more recently been referred to as
    *split-apply-combine*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析中最基本的任务之一是将数据分成独立的组，然后对每个组执行计算。这种方法已经存在了很长时间，但最近被称为*split-apply-combine*。
- en: Within the *apply* step of the *split-apply-combine* paradigm, it is additionally
    helpful to know whether we are trying to perform a *reduction* (also referred
    to as an aggregation) or a *transformation*. The former reduces the values in
    a group down to *one value* whereas the latter attempts to maintain the shape
    of the group.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在*split-apply-combine*范式的*apply*步骤中，了解我们是在进行*归约*（也称为聚合）还是*转换*是非常有帮助的。前者会将组中的值归约为*一个值*，而后者则试图保持组的形状不变。
- en: 'To illustrate, here is what *split-apply-combine* looks like for a reduction:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，以下是归约操作的 *split-apply-combine* 示例：
- en: '![](img/B31091_08_01.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_08_01.png)'
- en: 'Figure 8.1: Split-apply-combine paradigm for a reduction'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：归约的 split-apply-combine 范式
- en: 'Here is the same paradigm for a *transformation*:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是*转换*的相同范式：
- en: '![](img/B31091_08_02.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_08_02.png)'
- en: 'Figure 8.2: Split-apply-combine paradigm for a transformation'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：转换的 split-apply-combine 范式
- en: In pandas, the `pd.DataFrame.groupby` method is responsible for splitting, applying
    a function of your choice, and combining the results back together for you as
    an end user.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pandas 中，`pd.DataFrame.groupby`方法负责将数据进行分组，应用你选择的函数，并将结果合并回最终结果。
- en: 'We are going to cover the following recipes in this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下内容：
- en: Group by basics
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分组基础
- en: Grouping and calculating multiple columns
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分组并计算多个列
- en: Group by apply
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分组应用
- en: Window operations
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口操作
- en: Selecting the highest rated movies by year
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按年份选择评分最高的电影
- en: Comparing the best hitter in baseball across years
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较不同年份的棒球最佳击球手
- en: Group by basics
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分组基础
- en: True mastery of the pandas group by mechanisms is a powerful skill for any data
    analyst. With pandas, you can easily summarize data, find patterns within different
    groups, and compare groups to one another. The number of algorithms you can apply
    alongside a group by are endless in theory, giving you as an analyst tons of flexibility
    to explore your data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 熟练掌握 pandas 的分组机制是每个数据分析师的重要技能。使用 pandas，你可以轻松地总结数据，发现不同组之间的模式，并进行组与组之间的比较。从理论上讲，能够在分组后应用的算法数目是无穷无尽的，这为分析师提供了极大的灵活性来探索数据。
- en: In this first recipe, we are going to start with a very simple summation against
    different groups in an intentionally small dataset. While this example is overly
    simplistic, a solid theoretical understanding of how group by works is important
    as you look toward real-world applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个初步示例中，我们将从一个非常简单的求和操作开始，针对不同的组进行计算，数据集故意很小。虽然这个示例过于简化，但理解分组操作如何工作是非常重要的，这对于将来的实际应用非常有帮助。
- en: How to do it
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'To get familiarized with how group by works in code, let’s create some sample
    data that matches our starting point in *Figures 8.1* and *8.2*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了熟悉分组操作的代码实现，接下来我们将创建一些示例数据，匹配我们在*图 8.1*和*图 8.2*中的起始点：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Our `pd.DataFrame` has two distinct groups: `group_a` and `group_b`. As you
    can see, the `group_a` rows are associated with `value` data of `0` and `2`, whereas
    the `group_b` rows are associated with `value` data of `1`, `3`, and `5`. Summing
    the values within each `group` should therefore yield a result of `2` and `9`,
    respectively.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`pd.DataFrame`有两个不同的组：`group_a`和`group_b`。如你所见，`group_a`的行与`value`数据的`0`和`2`关联，而`group_b`的行与`value`数据的`1`、`3`和`5`关联。因此，对每个`group`的值进行求和，结果应该分别是`2`和`9`。
- en: 'To express this with pandas, you are going to use the `pd.DataFrame.groupby`
    method, which accepts as an argument the group name(s). In our case, this is the
    `group` column. This technically returns a `pd.core.groupby.DataFrameGroupBy`
    object that exposes a `pd.core.groupby.DataFrameGroupBy.sum` method for summation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 pandas 表达这一点，你将使用`pd.DataFrame.groupby`方法，该方法接受作为参数的分组名称。在我们的例子中，这是`group`列。技术上，它返回一个`pd.core.groupby.DataFrameGroupBy`对象，暴露出一个用于求和的`pd.core.groupby.DataFrameGroupBy.sum`方法：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Don’t worry if you find the method name `pd.core.groupby.DataFrameGroupBy.sum`
    to be verbose; it is, but you will never have to write it out by hand. We are
    going to refer to it here by its technical name for the sake of completeness,
    but as an end user, you will always follow the form you can see here:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得方法名 `pd.core.groupby.DataFrameGroupBy.sum` 太冗长，不用担心；它确实冗长，但你永远不需要手动写出它。我们在这里为了完整性会使用它的技术名称，但作为终端用户，你始终会按照你所看到的形式使用：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is what you will follow to get your `pd.core.groupby.DataFrameGroupBy`
    object.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你用来获取 `pd.core.groupby.DataFrameGroupBy` 对象的方式。
- en: By default, `pd.core.groupby.DataFrameGroupBy.sum` is considered an *aggregation*,
    so each group is *reduced* down to a single row during the *apply* phase of *split-apply-combine*,
    much like we see in *Figure 8.1*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`pd.core.groupby.DataFrameGroupBy.sum` 被视为*聚合*，因此每个组在*应用*阶段会被*归约*为一行，就像我们在*图
    8.1* 中看到的那样。
- en: 'Instead of calling `pd.core.groupby.DataFrameGroupBy.sum` directly, we could
    have alternatively used the `pd.core.groupby.DataFrameGroupBy.agg` method, providing
    it with the argument of `"sum"`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以不直接调用 `pd.core.groupby.DataFrameGroupBy.sum`，而是使用 `pd.core.groupby.DataFrameGroupBy.agg`
    方法，并传递 `"sum"` 作为参数：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The explicitness of `pd.core.groupby.DataFrameGroupBy.agg` is useful when compared
    side by side with the `pd.core.groupby.DataFrameGroupBy.transform` method, which
    will perform a *transformation* (see *Figure 8.2* again) instead of a *reduction*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.core.groupby.DataFrameGroupBy.agg` 的明确性在与 `pd.core.groupby.DataFrameGroupBy.transform`
    方法对比时显得非常有用，后者将执行*转换*（再次见*图 8.2*），而不是*归约*：'
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`pd.core.groupby.DataFrameGroupBy.transform` guarantees to return a like-indexed
    object to the caller, which makes it ideal for performing calculations like `%
    of group`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.core.groupby.DataFrameGroupBy.transform` 保证返回一个具有相同索引的对象给调用者，这使得它非常适合进行诸如“%
    of group”之类的计算：'
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When applying a reduction algorithm, `pd.DataFrame.groupby` will take the unique
    values of the group(s) and use them to form a new row `pd.Index` (or `pd.MultiIndex`,
    in the case of multiple groups). If you would prefer not to have the grouped labels
    create a new index, keeping them as columns instead, you can pass `as_index=False`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用归约算法时，`pd.DataFrame.groupby` 会取出组的唯一值，并利用它们来形成一个新的行 `pd.Index`（或者在多个分组的情况下是
    `pd.MultiIndex`）。如果你不希望分组标签创建新的索引，而是将它们保留为列，你可以传递 `as_index=False`：
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You should also note that the name of any non-grouping columns will not be
    altered when performing a group by operation. For example, even though we start
    with a `pd.DataFrame` containing a column named *value*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该注意，在执行分组操作时，任何非分组列的名称不会改变。例如，即使我们从一个包含名为 *value* 的列的 `pd.DataFrame` 开始：
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The fact that we then group by the `group` column and sum the `value` column
    does not change its name in the result; it is still just `value`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们随后按 `group` 列分组并对 `value` 列求和，这并不会改变结果中的列名；它仍然叫做 `value`：
- en: '[PRE15]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This can be confusing or ambiguous if you apply other algorithms to your groups,
    like `min`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对组应用其他算法，比如 `min`，这可能会让人困惑或产生歧义：
- en: '[PRE17]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Our column is still just called `value`, even though in one instance, we are
    taking the *sum of value* and in the other instance, we are taking the *min of
    value*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的列仍然叫做 `value`，即使在某个实例中，我们是在计算*value的总和*，而在另一个实例中，我们是在计算*value的最小值*。
- en: Fortunately, there is a way to control this by using the `pd.NamedAgg` class.
    When calling `pd.core.groupby.DataFrameGroupBy.agg`, you can provide keyword arguments
    where each argument key dictates the desired column name and the argument value
    is a `pd.NamedAgg`, which dictates the aggregation as well as the original column
    it is applied to.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一种方法可以通过使用 `pd.NamedAgg` 类来控制这一点。当调用 `pd.core.groupby.DataFrameGroupBy.agg`
    时，你可以提供关键字参数，其中每个参数键决定了所需的列名，而参数值是 `pd.NamedAgg`，它决定了聚合操作以及它应用的原始列。
- en: 'For instance, if we wanted to apply a `sum` aggregation to our `value` column,
    and have the result shown as `sum_of_value`, we could write the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想对 `value` 列应用 `sum` 聚合，并且将结果显示为 `sum_of_value`，我们可以写出以下代码：
- en: '[PRE19]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: There’s more…
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Although this recipe focused mainly on summation, pandas offers quite a few
    other built-in *reduction* algorithms that can be applied to a `pd.core.groupby.DataFrameGroupBy`
    object, such as the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这篇教程主要关注求和，但 pandas 提供了许多其他内置的*归约*算法，可以应用于 `pd.core.groupby.DataFrameGroupBy`
    对象，例如以下几种：
- en: '| `any` | `all` | `sum` | `prod` |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `any` | `all` | `sum` | `prod` |'
- en: '| `idxmin` | `idxmax` | `min` | `max` |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `idxmin` | `idxmax` | `min` | `max` |'
- en: '| `mean` | `median` | `var` | `std` |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `mean` | `median` | `var` | `std` |'
- en: '| `sem` | `skew` | `first` | `last` |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `sem` | `skew` | `first` | `last` |'
- en: 'Table 8.1: Commonly used GroupBy reduction algorithms'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1：常用的 GroupBy 减少算法
- en: 'Likewise, there are some built-in *transformation* functions that you can use:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以使用一些内置的*转换*函数：
- en: '| cumprod | cumsum | cummin |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| cumprod | cumsum | cummin |'
- en: '| cummax | rank |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| cummax | rank |  |'
- en: 'Table 8.2: Commonly used GroupBy transformation algorithms'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.2：常用的 GroupBy 转换算法
- en: 'Functionally, there is no difference between calling these functions directly
    as methods of `pd.core.groupby.DataFrameGroupBy` versus providing them as an argument
    to `pd.core.groupby.DataFrameGroupBy.agg` or `pd.core.groupby.DataFrameGroupBy.transform`.
    You will get the same performance and result by doing the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 功能上，直接调用这些函数作为`pd.core.groupby.DataFrameGroupBy`的方法与将它们作为参数提供给`pd.core.groupby.DataFrameGroupBy.agg`或`pd.core.groupby.DataFrameGroupBy.transform`没有区别。你将通过以下方式获得相同的性能和结果：
- en: '[PRE21]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code snippet will yield the same results as this one:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段将得到与以下代码相同的结果：
- en: '[PRE23]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You could argue that the latter approach signals a clearer intent, especially
    considering that `max` can be used as a transformation just as well as an aggregation:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以说，后者的方法更明确，特别是考虑到`max`可以作为转换函数使用，就像它作为聚合函数一样：
- en: '[PRE25]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In practice, both styles are commonplace, so you should be familiar with the
    different approaches.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这两种风格都很常见，因此你应该熟悉不同的方法。
- en: Grouping and calculating multiple columns
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对多个列进行分组和计算
- en: Now that we have the basics down, let’s take a look at a `pd.DataFrame` that
    contains more columns of data. Generally, your `pd.DataFrame` objects will contain
    many columns with potentially different data types, so knowing how to select and
    work with them all through the context of `pd.core.groupby.DataFrameGroupBy` is
    important.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们掌握了基本概念，接下来让我们看一个包含更多数据列的`pd.DataFrame`。通常情况下，你的`pd.DataFrame`对象将包含许多列，且每列可能有不同的数据类型，因此了解如何通过`pd.core.groupby.DataFrameGroupBy`来选择并处理它们非常重要。
- en: How to do it
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'Let’s create a `pd.DataFrame` that shows the `sales` and `returns` of a hypothetical
    `widget` across different `region` and `month` values:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个`pd.DataFrame`，展示一个假设的`widget`在不同`region`和`month`值下的`销售`和`退货`数据：
- en: '[PRE27]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To calculate the total `sales` and `returns` for each `widget`, your first
    attempt at doing so may look like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算每个`widget`的`销售`和`退货`总额，你的第一次尝试可能会是这样的：
- en: '[PRE29]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'While `sales` and `returns` look good, the `region` and `month` columns also
    ended up being summed, using the same summation logic that Python would when working
    with strings:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`sales`和`returns`看起来很好，但`region`和`month`列也被汇总了，使用的是Python在处理字符串时的相同求和逻辑：
- en: '[PRE31]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Unfortunately, this default behavior is usually undesirable. I personally find
    it rare to ever want strings to be concatenated like this, and when dealing with
    large `pd.DataFrame` objects, it can be prohibitively expensive to do so.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这种默认行为通常是不可取的。我个人认为很少会希望字符串以这种方式连接，而且在处理大型`pd.DataFrame`对象时，执行此操作的成本可能非常高。
- en: 'One way to avoid this issue is to be more explicit about the columns you would
    like to aggregate by selecting them after the `df.groupby("widget")` call:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这个问题的一种方法是更加明确地选择你希望聚合的列，可以在`df.groupby("widget")`调用后进行选择：
- en: '[PRE33]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Alternatively, you could reach for the `pd.NamedAgg` class we introduced back
    in the *Group by basics* recipe. Though more verbose, the use of `pd.NamedAgg`
    gives you the benefit of being able to rename the columns you would like to see
    in the output (i.e., instead of `sales`, you may want to see `sales_total`):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用我们在*Group by basics*配方中介绍的`pd.NamedAgg`类。虽然它更加冗长，但使用`pd.NamedAgg`可以让你重命名你希望在输出中看到的列（例如，`sales`可以改为`sales_total`）：
- en: '[PRE35]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Another feature of `pd.core.groupby.DataFrameGroupBy` worth reviewing here
    is its ability to deal with multiple `group` arguments. By providing a list, you
    can expand your grouping to cover both `widget` and `region`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.core.groupby.DataFrameGroupBy`的另一个值得注意的特性是其能够处理多个`group`参数。通过提供一个列表，你可以扩展分组，涵盖`widget`和`region`：'
- en: '[PRE37]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'With `pd.core.groupby.DataFrameGroupBy.agg`, there is no limitation on how
    many functions can be applied. For instance, if you want to see the `sum`, `min`,
    and `mean` of `sales` and `returns` within each `widget` and `region`, you could
    simply write the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pd.core.groupby.DataFrameGroupBy.agg`时，没有对可以应用多少个函数的限制。例如，如果你想查看每个`widget`和`region`中的`销售`和`退货`的`sum`、`min`和`mean`，你可以简单地写出如下代码：
- en: '[PRE39]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: There’s more…
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: While the built-in reduction functions and transformation functions that work
    out of the box with a group by are useful, there may still be times when you need
    to roll with your own custom function. This can be particularly useful when you
    find an algorithm to be `good enough` for what you are attempting in your local
    analysis, but when it may be difficult to generalize to all use cases.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然内置的分组聚合和转换函数在默认情况下非常有用，但有时你可能需要使用自己的自定义函数。当你发现某个算法在本地分析中“足够好”时，这尤其有用，尽管这个算法可能很难推广到所有使用场景。
- en: 'A commonly requested function in pandas that is not provided out of the box
    with a group by is `mode`, even though there is a `pd.Series.mode` method. With
    `pd.Series.mode`, the type returned is always a `pd.Series`, regardless of whether
    there is only one value that appears most frequently:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 中一个常见的请求函数是 `mode`，但是在分组操作中并没有开箱即用的提供，尽管有 `pd.Series.mode` 方法。使用 `pd.Series.mode`
    时，返回的类型始终是 `pd.Series`，无论是否只有一个值出现频率最高：
- en: '[PRE41]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This is true even if there are two or more elements that appear most frequently:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有两个或更多元素出现频率相同，这一切仍然成立：
- en: '[PRE43]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Given that there is a `pd.Series.mode`, why does pandas not offer a similar
    function when doing a group by? From a pandas developer perspective, the reason
    is simple; there is no single way to interpret what a group by should return.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于`pd.Series.mode`已存在，为什么 pandas 在进行分组时没有提供类似的功能？从 pandas 开发者的角度来看，原因很简单；没有一种单一的方式来解释分组操作应该返回什么。
- en: 'Let’s think through this in more detail with the following example, where `group_a`
    contains two values that appear with the same frequency (42 and 555), whereas
    `group_b` only contains the value 0:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下示例更详细地思考这个问题，其中 `group_a` 包含两个出现频率相同的值（42 和 555），而 `group_b` 只包含值 0：
- en: '[PRE45]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The question we need to answer is *what should the mode return for group_a?*
    One possible solution would be to return a list (or any Python sequence) that
    holds both 42 and 555\. The downside to this approach is that your returned dtype
    would be `object`, the pitfalls of which we covered back in *Chapter 3*, *Data
    Types*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要回答的问题是，*对于 group_a，mode 应该返回什么？* 一种可能的解决方案是返回一个列表（或任何 Python 序列），其中包含 42
    和 555。然而，这种方法的缺点是，返回的 dtype 会是 `object`，这种类型的陷阱我们在 *第 3 章*，*数据类型* 中已经讨论过。
- en: '[PRE47]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: A second expectation would be for pandas to just *choose one* of the values.
    Of course, this begs the question as to *how* pandas should make that decision
    – would the value 42 or 555 be more appropriate for `group_a` and how can that
    be determined in a general case?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种期望是，pandas 只是*选择一个*值。当然，这就引出了一个问题，*pandas 应该如何做出这个决定*——对于 `group_a`，值 42
    或 555 哪个更合适，如何在一般情况下做出判断呢？
- en: 'A third expectation would be to return something where the label `group_a`
    appears twice in the resulting row index after aggregation. However, no other
    group by aggregations work this way, so we would be introducing new and potentially
    unexpected behavior by reducing to this:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个期望是，在聚合后的结果行索引中，`group_a` 标签会出现两次。然而，没有其他的分组聚合是这样工作的，所以我们会引入新的并可能是意外的行为，通过简化为此：
- en: '[PRE49]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Rather than trying to solve for all of these expectations and codify it as part
    of the API, pandas leaves it entirely up to you how you would like to implement
    a `mode` function, as long as you adhere to the expectations that aggregations
    reduce to a single value per group. This eliminates the third expectation we just
    outlined as a possibility, at least until we talk about **Group by** apply later
    in this chapter.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 并没有试图解决所有这些期望并将其编码为 API 的一部分，而是完全交给你来决定如何实现 `mode` 函数，只要你遵循聚合操作每个分组返回单一值的预期。这排除了我们刚才概述的第三种期望，至少在本章后续谈论
    **Group by** apply 时才会重新讨论。
- en: 'To that end, if we wanted to roll with our own custom mode functions, they
    may end up looking something like:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，如果我们想使用自定义的众数函数，它们可能最终看起来像这样：
- en: '[PRE51]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Since these are both aggregations, we can use them in the context of a `pd.core.groupby.DataFrameGroupBy.agg`
    operation:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些都是聚合操作，我们可以在 `pd.core.groupby.DataFrameGroupBy.agg` 操作的上下文中使用它们：
- en: '[PRE52]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Group by apply
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Group by apply
- en: During our discussion on algorithms and how to apply them back in *Chapter 5*,
    *Algorithms and How to Apply Them*, we came across the Apply function, which is
    both powerful and terrifying at the same time. An equivalent function for group
    by exists as `pd.core.groupby.DataFrameGroupBy.apply` with all of the same caveats.
    Generally, this function is overused, and you should opt for `pd.core.groupby.DataFrameGroupBy.agg`
    or `pd.core.groupby.DataFrameGroupBy.transform` instead. However, for the cases
    where you don’t really want an *aggregation* or a *transformation*, but something
    in between, using `apply` is your only option.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论算法以及如何应用它们的 *第5章*，*算法及其应用* 中，我们接触到了 Apply 函数，它既强大又令人恐惧。一个与 group by 等效的函数是
    `pd.core.groupby.DataFrameGroupBy.apply`，并且有着相同的注意事项。通常，这个函数被过度使用，您应该选择 `pd.core.groupby.DataFrameGroupBy.agg`
    或 `pd.core.groupby.DataFrameGroupBy.transform`。然而，对于那些您既不想要 *聚合* 也不想要 *转换*，但又希望得到介于两者之间的功能的情况，使用
    `apply` 是唯一的选择。
- en: Generally, `pd.core.groupby.DataFrameGroupBy.apply` should only be used as a
    last resort. It can produce sometimes ambiguous behavior and is rather prone to
    breakage across releases of pandas.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，`pd.core.groupby.DataFrameGroupBy.apply` 应该仅在不得已时使用。它有时会产生模糊的行为，并且在 pandas
    的不同版本之间容易出现破裂。
- en: How to do it
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做
- en: 'In the *There’s more…* section of the previous recipe, we mentioned how it
    is not possible to start with a `pd.DataFrame` of the following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个食谱的 *还有更多…* 部分中，我们提到过从以下的 `pd.DataFrame` 开始是不可能的：
- en: '[PRE54]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'And to produce the following output, using a custom `mode` algorithm supplied
    to `pd.core.groupby.DataFrameGroupBy.agg`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用自定义的 `mode` 算法，提供给 `pd.core.groupby.DataFrameGroupBy.agg` 以生成以下输出：
- en: '[PRE56]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The reason for this is straightforward; an aggregation expects you to reduce
    to a single value per group label. Repeating the label `group_a` twice in the
    output is a non-starter for an aggregation. Similarly, a transformation would
    expect you to produce a result that shares the same row index as the calling `pd.DataFrame`,
    which is not what we are after either.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的原因很简单；聚合期望你将每个分组标签减少到一个单一值。输出中重复 `group_a` 标签两次对于聚合来说是不可接受的。同样，转换期望你生成的结果与调用的
    `pd.DataFrame` 具有相同的行索引，而这并不是我们想要的结果。
- en: '`pd.core.groupby.DataFrameGroupBy.apply` is the in-between method that can
    get us closer to the desired result, which you can see in the following code.
    As a technical aside, the `include_groups=False` argument is passed to suppress
    any deprecation warnings about behavior in pandas 2.2\. In subsequent versions,
    you may not need this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.core.groupby.DataFrameGroupBy.apply` 是一个介于两者之间的方法，可以让我们更接近所期望的结果，正如接下来的代码所示。作为一个技术性的旁注，`include_groups=False`
    参数被传递以抑制关于 pandas 2.2 版本行为的弃用警告。在后续版本中，您可能不需要这个参数：'
- en: '[PRE58]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'It is important to note that we annotated the parameter of the `mode_for_apply`
    function as a `pd.DataFrame`. With aggregations and transformations, user-defined
    functions receive just a single `pd.Series` of data at a time, but with apply,
    you get an entire `pd.DataFrame`. For a more detailed look at what is going on,
    you can add `print` statements to the user-defined function:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们将 `mode_for_apply` 函数的参数注解为 `pd.DataFrame`。在聚合和转换中，用户定义的函数每次只会接收一个
    `pd.Series` 类型的数据，但使用 apply 时，您将获得整个 `pd.DataFrame`。如果想更详细地了解发生了什么，可以在用户定义的函数中添加
    `print` 语句：
- en: '[PRE60]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Essentially, `pd.core.groupby.DataFrameGroupBy.apply` passes a `pd.DataFrame`
    of data to the user-defined function, excluding the column(s) that are used for
    grouping. From there, it will look at the return type of the user-defined function
    and try to infer the best possible output shape it can. In this particular instance,
    because our `mode_for_apply` function returns a `pd.Series`, `pd.core.groupby.DataFrameGroupBy.apply`
    has determined that the best output shape should have a `pd.MultiIndex`, where
    the first level of the index is the group value and the second level contains
    the row index from the `pd.Series` returned by the `mode_for_apply` function.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，`pd.core.groupby.DataFrameGroupBy.apply` 将数据传递给用户定义的函数，传递的数据是一个 `pd.DataFrame`，并排除了用于分组的列。从那里，它会查看用户定义的函数的返回类型，并尝试推断出最合适的输出形状。在这个特定的例子中，由于我们的
    `mode_for_apply` 函数返回的是一个 `pd.Series`，`pd.core.groupby.DataFrameGroupBy.apply`
    已经确定最佳的输出形状应该是一个 `pd.MultiIndex`，其中索引的第一层是组值，第二层包含由 `mode_for_apply` 函数返回的 `pd.Series`
    的行索引。
- en: 'Where `pd.core.groupby.DataFrameGroupBy.apply` gets overused is in the fact
    that it can change its shape to look like an aggregation when it detects that
    the functions it applies reduce to a single value:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.core.groupby.DataFrameGroupBy.apply` 被过度使用的地方在于，当它检测到所应用的函数可以减少为单个值时，它会改变形状，看起来像是一个聚合操作：'
- en: '[PRE62]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: It is a trap to use it in this way, however. Even if it can infer a reasonable
    shape for some outputs, the rules for how it determines that are implementation
    details, for which you pay a performance penalty or run the risk of code breakage
    across pandas releases. If you know your functions will reduce to a single value,
    always opt for `pd.core.groupby.DataFrameGroupBy.agg` in lieu of `pd.core.groupby.DataFrameGroupBy.apply`,
    leaving the latter only for extreme use cases.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，以这种方式使用它是一个陷阱。即使它能够推断出一些输出的合理形状，确定这些形状的规则是实现细节，这会导致性能损失，或者在不同版本的 pandas 中可能导致代码破裂。如果你知道你的函数将减少为单个值，始终选择使用
    `pd.core.groupby.DataFrameGroupBy.agg` 来代替 `pd.core.groupby.DataFrameGroupBy.apply`，后者只应在极端用例中使用。
- en: Window operations
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口操作
- en: Window operations allow you to calculate values over a sliding partition (or
    “window”) of values. Commonly, these operations are used to calculate things like
    “rolling 90-day average,” but they are flexible enough to extend to any algorithm
    of your choosing.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口操作允许你在一个滑动的分区（或“窗口”）内计算值。通常，这些操作用于计算“滚动的 90 天平均值”等，但它们足够灵活，可以扩展到你选择的任何算法。
- en: While not technically a group by operation, window operations are included here
    as they share a similar API and work with “groups” of data. The only difference
    to a group by call is that, instead of forming groups from unique value sets,
    a window operation creates its group by iterating over each value of a pandas
    object and looking at a particular number of preceding (and sometimes following)
    values.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从技术上讲这不是一个分组操作，但窗口操作在这里被包含进来，因为它们共享类似的 API 并且可以与“数据组”一起工作。与分组操作的唯一不同之处在于，窗口操作并不是通过唯一值集来形成分组，而是通过遍历
    pandas 对象中的每个值，查看特定数量的前后（有时是后续）值来创建其组。
- en: How to do it
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'To get a feel for how window operations work, let’s start with a simple `pd.Series`
    where each element is an increasing power of 2:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解窗口操作如何工作，让我们从一个简单的 `pd.Series` 开始，其中每个元素是 2 的递增幂：
- en: '[PRE64]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The first type of window operation you will come across is the “rolling window,”
    accessed via the `pd.Series.rolling` method. When calling this method, you need
    to tell pandas the desired size of your window *n*. The pandas library starts
    at each element and looks backward *n-1* records to form the “window”:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你将遇到的第一种窗口操作是“滚动窗口”，通过 `pd.Series.rolling` 方法访问。当调用此方法时，你需要告诉 pandas 你希望的窗口大小
    *n*。pandas 会从每个元素开始，向后查看 *n-1* 个记录来形成“窗口”：
- en: '[PRE66]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'You may notice that we started with a `pd.Int64Dtype()` but ended up with a
    `float64` type after the rolling window operation. Unfortunately, the pandas window
    operations do not work well with the pandas extension system in at least version
    2.2 (see issue #50449), so for the time being, we need to cast the result back
    into the proper data type:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '你可能注意到，我们开始时使用了 `pd.Int64Dtype()`，但在滚动窗口操作后，最终得到了 `float64` 类型。不幸的是，pandas
    窗口操作至少在 2.2 版本中与 pandas 扩展系统的兼容性不好（参见问题 #50449），因此目前，我们需要将结果转换回正确的数据类型：'
- en: '[PRE68]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: So, what is going on here? Essentially, you can think of a rolling window operation
    as iterating through the `pd.Series` values. While doing so, it looks backward
    to try and collect enough values to fulfill the desired window size, which we
    have specified as 2.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这里发生了什么？本质上，你可以将滚动窗口操作看作是遍历 `pd.Series` 的值。在此过程中，它向后查看，试图收集足够的值以满足所需的窗口大小，我们指定的窗口大小是
    2。
- en: 'After collecting two elements in each window, pandas will apply the specified
    aggregation function (in our case, summation). The result of that aggregation
    in each window is then used to piece back together the result:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个窗口中收集两个元素后，pandas 会应用指定的聚合函数（在我们的例子中是求和）。每个窗口中的聚合结果将用于将结果拼接回去：
- en: '![](img/B31091_08_03.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_08_03.png)'
- en: 'Figure 8.3: Rolling window with sum aggregation'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：滚动窗口与求和聚合
- en: 'In the case of our very first record, which cannot form a window with two elements,
    pandas returns a missing value. If you want the rolling calculation to just sum
    up as many elements as it can, even if the window size cannot be reached, you
    can pass an argument to `min_periods=` that dictates the minimum number of elements
    within each window required to perform the aggregation:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的第一个记录，由于无法形成包含两个元素的窗口，pandas会返回缺失值。如果你希望滚动计算即使窗口大小无法满足也能尽可能地求和，你可以向`min_periods=`传递一个参数，指定每个窗口中进行聚合所需的最小元素数量：
- en: '[PRE70]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: By default, rolling window operations look backward to try and fulfill your
    window size requirements. You can also “center” them instead so that pandas looks
    both forward and backward.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，滚动窗口操作会向后查找以满足你的窗口大小要求。你也可以将它们“居中”，让pandas同时向前和向后查找。
- en: 'The effect of this is better seen with an odd window size. Note the difference
    when we expand our call so far with a window size of `3`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这种效果在使用奇数窗口大小时更为明显。当我们将窗口大小扩展为`3`时，注意到的区别如下：
- en: '[PRE72]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Compared to the same call with an argument of `center=True`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用`center=True`参数的相同调用进行比较：
- en: '[PRE74]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Instead of looking at the current and preceding two values, usage of `center=True`
    tells pandas to take the current value, one prior, and one following to form a
    window.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与查看当前值及前两个值不同，使用`center=True`告诉pandas在窗口中包含当前值、前一个值和后一个值。
- en: 'Another type of window function is the “expanding window”, which looks at all
    prior values encountered. The syntax for that is straightforward; simply replace
    your call to `pd.Series.rolling` with `pd.Series.expanding` and follow that up
    with your desired aggregation function. An expanding summation is similar to the
    `pd.Series.cumsum` method you have seen before, so for demonstration purposes,
    let’s pick a different aggregation function, like `mean`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种窗口函数是“扩展窗口”，它会查看所有先前遇到的值。其语法非常简单；只需将调用`pd.Series.rolling`替换为`pd.Series.expanding`，然后跟随你想要的聚合函数。扩展求和类似于你之前看到的`pd.Series.cumsum`方法，因此为了演示，我们选择一个不同的聚合函数，比如`mean`：
- en: '[PRE76]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Visually represented, an expanding window calculation looks as follows (for
    brevity, not all of the `pd.Series` elements are shown):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以可视化方式表示，扩展窗口计算如下（为了简洁，未显示所有`pd.Series`元素）：
- en: '![](img/B31091_08_04.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_08_04.png)'
- en: 'Figure 8.4: Expanding window with mean aggregation'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：扩展窗口与均值聚合
- en: There’s more…
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: In *Chapter 9*, *Temporal Data Types and Algorithms*, we will dive deeper into
    some of the very nice features pandas can offer when dealing with temporal data.
    Before we get there, it is worth noting that group by and rolling/expanding window
    functions work very naturally with such data, allowing you to concisely perform
    calculations like, “N day moving averages” “year-to-date X,” “quarter-to-date
    X,” etc.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第9章*，*时间数据类型与算法*中，我们将更深入地探讨pandas在处理时间数据时提供的一些非常有用的功能。在我们深入探讨之前，值得注意的是，分组和滚动/扩展窗口函数与此类数据非常自然地配合使用，让你能够简洁地执行诸如“X天移动平均”、“年初至今X”、“季度至今X”等计算。
- en: 'To see how that works, let’s take another look at the Nvidia stock performance
    dataset we started with back in *Chapter 5*, *Algorithms and How to Apply Them*,
    originally as part of the *Calculating a trailing stop order price* recipe:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看看这如何运作，我们再来看一下在*第5章*，*算法及如何应用它们*中，我们最初使用的Nvidia股票表现数据集，该数据集作为*计算追踪止损价格*食谱的一部分：
- en: '[PRE78]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'With rolling window functions, we can easily add 30, 60, and 90-day moving
    averages. A subsequent call to `pd.DataFrame.plot` also makes this easy to visualize:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用滚动窗口函数，我们可以轻松地计算30天、60天和90天的移动平均。随后调用`pd.DataFrame.plot`也让这种可视化变得简单：
- en: '[PRE80]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '![](img/B31091_08_05.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_08_05.png)'
- en: 'For “year-to-date” and “quarter-to-date” calculations, we can use a combination
    of group by and expanding window functions. For “year-to-date” min, max, and mean
    close values, we can start by forming a group by object to split our data into
    yearly buckets, and from there, we can make a call to `.expanding()`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“年初至今”和“季度至今”计算，我们可以使用分组与扩展窗口函数的组合。对于“年初至今”的最小值、最大值和均值，我们可以首先形成一个分组对象，将数据分成按年划分的桶，然后可以调用`.expanding()`：
- en: '[PRE81]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The `pd.Grouper(freq="YS")` takes our row index, which contains datetimes,
    and groups them by the start of the year within which they fall. After the grouping,
    the call to `.expanding()` performs the min/max aggregations, only looking as
    far back as the start of each year. The effects of this are once again easier
    to see with a visualization:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.Grouper(freq="YS")`将我们的行索引（包含日期时间）按年份的开始进行分组。分组后，调用`.expanding()`执行最小值/最大值聚合，只看每年开始时的值。这个效果再次通过可视化更容易看出：'
- en: '[PRE83]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '![](img/B31091_08_06.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_08_06.png)'
- en: 'For a more granular view, you can calculate the expanding min/max close prices
    per quarter by changing the `freq=` argument from `YS` to `QS` in `pd.Grouper`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更详细的视图，你可以通过将`freq=`参数从`YS`改为`QS`，计算每个季度的扩展最小/最大收盘价格：
- en: '[PRE84]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '![](img/B31091_08_07.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_08_07.png)'
- en: 'A `MS` `freq=` argument gets you down to the monthly level:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`MS` `freq=`参数可以将时间精度降低到月份级别：
- en: '[PRE85]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '![](img/B31091_08_08.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_08_08.png)'
- en: Selecting the highest rated movies by year
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按年份选择评分最高的电影
- en: One of the most basic and common operations to perform during data analysis
    is to select rows containing the largest value of some column within a group.
    Applied to our movie dataset, this could mean finding the highest-rated film of
    each year or the highest-grossing film by content rating. To accomplish these
    tasks, we need to sort the groups as well as the column used to rank each member
    of the group, and then extract the highest member of each group.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析中最基本和常见的操作之一是选择某个列在组内具有最大值的行。应用到我们的电影数据集，这可能意味着找出每年评分最高的电影或按内容评级找出最高票房的电影。为了完成这些任务，我们需要对组以及用于排名每个组成员的列进行排序，然后提取每个组中的最高成员。
- en: In this recipe, we will find the highest-rated film of each year using a combination
    of `pd.DataFrame.sort_values` and `pd.DataFrame.drop_duplicates`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用`pd.DataFrame.sort_values`和`pd.DataFrame.drop_duplicates`的组合，找出每年评分最高的电影。
- en: How to do it
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'Start by reading in the movie dataset and slim it down to just the three columns
    we care about: `movie_title`, `title_year`, and `imdb_score`:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 开始时，读取电影数据集并将其精简为我们关心的三列：`movie_title`、`title_year`和`imdb_score`：
- en: '[PRE86]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'As you can see, the `title_year` column gets interpreted as a floating point
    value, but years should always be whole numbers. We could correct that by assigning
    the proper data type directly to our column:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`title_year`列被解释为浮动小数点值，但年份应始终是整数。我们可以通过直接为列分配正确的数据类型来纠正这一点：
- en: '[PRE88]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Alternatively, we could have passed the desired data type as the `dtype=` argument
    in `pd.read_csv`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们也可以在`pd.read_csv`中通过`dtype=`参数传递所需的数据类型：
- en: '[PRE90]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: With our data cleansing out of the way, we can now turn our focus to answering
    the question of “what is the highest rated movie each year?”. There are a few
    ways we can calculate this, but let’s start with the approach you see most commonly.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数据清洗工作完成后，我们现在可以专注于回答“每年评分最高的电影是什么？”这个问题。我们可以通过几种方式来计算，但让我们从最常见的方法开始。
- en: 'When you perform a group by in pandas, the order in which rows appear in the
    original `pd.DataFrame` is respected as rows are bucketed into different groups.
    Knowing this, many users will answer this question by first sorting their dataset
    across `title_year` and `imdb_score`. After the sort, you can group by the `title_year`
    column, select just the `movie_title` column, and chain in a call to `pd.DataFrameGroupBy.last`
    to select the last value from each group:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在pandas中执行分组操作时，原始`pd.DataFrame`中行的顺序会被保留，行会根据不同的组进行分配。知道这一点后，很多用户会通过首先按`title_year`和`imdb_score`对数据集进行排序来回答这个问题。排序后，你可以按`title_year`列进行分组，仅选择`movie_title`列，并链式调用`pd.DataFrameGroupBy.last`来选择每个组的最后一个值：
- en: '[PRE92]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'A slightly more succinct approach can be had if you use `pd.DataFrameGroupBy.idxmax`,
    which selects the row index value corresponding to the highest movie rating each
    year. This would require you to set the index to the `movie_title` up front:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用`pd.DataFrameGroupBy.idxmax`，它会选择每年评分最高的电影的行索引值，这是一种更简洁的方法。这要求你事先将索引设置为`movie_title`：
- en: '[PRE94]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Our results appear mostly the same, although we can see that the two approaches
    disagreed on what the highest rated movie was in the years 2012 and 2014\. A closer
    look at these titles reveals the root cause:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果大致相同，尽管我们可以看到在2012年和2014年，两个方法在选择评分最高的电影时存在不同。仔细查看这些电影标题可以揭示出根本原因：
- en: '[PRE96]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: In case of a tie, each method has its own way of choosing a value. Neither approach
    is right or wrong per se, but if you wanted finer control over that, you would
    have to reach for **Group by apply**.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在发生平局的情况下，每种方法都有自己选择值的方式。没有哪种方法本身是对错的，但如果你希望对这一点进行更精细的控制，你将不得不使用**按组应用**。
- en: Let’s assume we wanted to aggregate the values so that when there is no tie,
    we get back a string, but in case of a tie we get a sequence of strings. To do
    this, you should define a function that accepts a `pd.DataFrame`. This `pd.DataFrame`
    will contain the values associated with each unique grouping column, which is
    `title_year` in our case.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要汇总这些值，以便在没有平局的情况下返回一个字符串，而在发生平局时返回一组字符串。为此，您应该定义一个接受`pd.DataFrame`的函数。这个`pd.DataFrame`将包含与每个唯一分组列相关的值，在我们的例子中，这个分组列是`title_year`。
- en: 'Within the body of the function, you can figure out what the top movie rating
    is, find all movies with that rating, and return back either a single movie title
    (when there are no ties) or a set of movies (in case of a tie):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数体内，您可以找出最高的电影评分，找到所有具有该评分的电影，并返回一个单一的电影标题（当没有平局时）或一组电影（在发生平局时）：
- en: '[PRE98]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Comparing the best hitter in baseball across years
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较棒球历史上各年最佳击球手
- en: In the *Finding the baseball* *players best at…* recipe back in *Chapter 5*,
    *Algorithms and How to Apply Them*, we worked with a dataset that had already
    aggregated the performance of players from the years 2020-2023\. However, comparing
    players based on their performance across multiple years is rather difficult.
    Even on a year-to-year basis, statistics that appear elite one year can be considered
    just “very good” in other years. The reasons for the variation in statistics across
    years can be debated, but likely come down to some combination of strategy, equipment,
    weather, and just pure statistical chance.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5章，*算法及其应用* 中的*寻找棒球* *最擅长的球员…*食谱中，我们处理了一个已经汇总了2020至2023年球员表现的数据集。然而，基于球员在多个年份之间的表现进行比较相当困难。即使是逐年比较，某一年看似精英的统计数据，其他年份可能也仅被认为是“非常好”。统计数据跨年份的变化原因可以进行辩论，但可能归结为战略、设备、天气以及纯粹的统计运气等多种因素的组合。
- en: For this recipe, we are going to work with a more granular dataset that goes
    down to the game level. From there, we are going to aggregate the data up to a
    yearly summary, and from there calculate a common baseball statistic known as
    the *batting average*.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，我们将使用一个更精细的数据集，该数据集细化到游戏层级。从那里，我们将把数据汇总到年度总结，然后计算一个常见的棒球统计数据——*打击率*。
- en: For those unfamiliar, a batting average is calculated by taking the number of
    *hits* a player produces (i.e., how many times they swung a bat at a baseball,
    and reached base as a result) as a percentage of their total *at bats* (i.e.,
    how many times they came to bat, excluding *walks*).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不熟悉的人，打击率是通过将球员的*击球*次数（即他们击打棒球并成功上垒的次数）作为总*打席*次数（即他们上场打击的次数，不包括*保送*）的百分比来计算的。
- en: So what constitutes a good batting average? As you will see, the answer to that
    question is a moving target, having shifted even within the past twenty years.
    In the early 2000s, a batting average between .260-.270 (i.e., getting a hit in
    26%-27% of at bats) was considered middle of the road for professionals. Within
    recent years, that number has fallen somewhere in the range of .240-.250.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么样的打击率才算好呢？正如你所见，答案是一个不断变化的目标，甚至在过去的二十年里也发生了变化。在2000年代初，打击率在.260-.270之间（即每26%-27%的打击中能击中一次）被认为是职业选手的中等水平。近年来，这个数字已经下降到了.240-.250的范围。
- en: As such, to try and compare the *best hitters* from each year to one another,
    we cannot solely look at the batting average. A league-leading batting average
    of .325 in a year when the league itself averaged .240 is likely more impressive
    than a league-leading batting average of .330 in a year where the overall league
    averaged around .260.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了尝试将每年*最佳击球手*进行比较，我们不能仅仅看打击率。在一个联盟整体打击率为.240的年份，打击率为.325的球员可能比在联盟整体打击率为.260的年份，打击率为.330的球员更具震撼力。
- en: How to do it
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做
- en: 'Once again, we are going to use data collected from `retrosheet.org`, with
    the following legal disclaimer:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们将使用从`retrosheet.org`收集的数据，并附上以下法律声明：
- en: The information used here was obtained free of charge from and is copyrighted
    by Retrosheet. Interested parties may contact Retrosheet at [www.retrosheet.org](https://www.retrosheet.org).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的信息是从Retrosheet免费获得的，并且受版权保护。感兴趣的各方可以通过 [www.retrosheet.org](https://www.retrosheet.org)
    联系Retrosheet。
- en: 'For this recipe we are going to use “box score” summaries from every regular
    season game played in the years 2000-2023:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用2000-2023年每场常规赛的“比赛记录”摘要：
- en: '[PRE100]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'A box score summarizes the performance of every player in a *game*. We could
    therefore single in on a particular game that was played in Baltimore on April
    10, 2015, and see how batters performed:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛记录总结了每个球员在*比赛*中的表现。因此，我们可以专注于2015年4月10日在巴尔的摩进行的某场比赛，并查看击球手的表现：
- en: '[PRE102]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'In that game alone we see a total of 75 at bats (*ab*), 29 hits (*h*) and two
    home runs (*hr*):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在那场比赛中，我们看到了总共 75 次打击（*ab*）、29 次安打（*h*）和两支本垒打（*hr*）：
- en: '[PRE104]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'With a basic understanding of what a box score is and what it shows, let’s
    turn our focus toward calculating the batting average every player produces each
    year. The individual player is notated in the `id` column of our dataset, and
    since we want to see the batting average over the course of an entire season,
    we can use the combination of `year` and `id` as our argument to `pd.DataFrame.groupby`.
    Afterward, we can apply a summation to the at bats (`ab`) and hits (`h`) columns:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对比赛记录的基本理解，我们可以将焦点转向计算每个球员每年产生的打击率。每个球员在我们的数据集中都有一个 `id` 列的标注，而由于我们想查看整个赛季的打击率，因此我们可以使用
    `year` 和 `id` 的组合作为 `pd.DataFrame.groupby` 的参数。然后，我们可以对打击次数（`ab`）和安打次数（`h`）列进行求和：
- en: '[PRE106]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'To turn those totals into a batting average, we can chain in a division using
    `pd.DataFrame.assign`. After that, a call to `pd.DataFrame.drop` will let us solely
    focus on the batting average, dropping the `total_ab` and `total_h` columns we
    no longer need:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些总数转化为打击率，我们可以使用 `pd.DataFrame.assign` 链接一个除法操作。之后，调用 `pd.DataFrame.drop`
    将让我们专注于打击率，删除我们不再需要的 `total_ab` 和 `total_h` 列：
- en: '[PRE108]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Before we continue, we have to consider some data quality issues that may arise
    when calculating averages. Over the course of a baseball season, teams may use
    players who only appear in very niche situations, yielding a low number of plate
    appearances. In some instances, a batter may not even register an “at bat” for
    the season, so using that as a divisor has a chance of dividing by 0, which will
    produce `NaN`. In cases where a batter has a non-zero amount of at bats on the
    season, but still has relatively few, a small sample size can severely skew their
    batting average.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们必须考虑在计算平均值时可能出现的一些数据质量问题。在一整个棒球赛季中，球队可能会使用一些只在非常特殊情况下出现的球员，导致其打席次数很低。在某些情况下，击球手甚至可能在整个赛季中没有记录一个“打击机会”，所以使用它作为除数时，可能会导致除以
    0，从而产生 `NaN`。在一些击球手打击次数不为零但仍然相对较少的情况下，小样本量可能会严重扭曲他们的打击率。
- en: 'Major League Baseball has strict rules for determining how many plate appearances
    it takes for a batter to qualify for records within a given year. Without following
    the rule exactly, and without having to calculate plate appearances in our dataset,
    we can proxy this by setting a requirement of at least 400 at bats over the course
    of a season:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 美国职棒大联盟有严格的规定，确定一个击球手需要多少次打击机会才能在某一年内资格入选记录。我们不必完全遵循这个规则，也不需要在我们的数据集中计算打击机会，但我们可以通过设置至少
    400 次打击机会的要求来作为替代：
- en: '[PRE110]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'We can summarize this further by finding the average and maximum `batting_average`
    per season, and we can even use `pd.core.groupby.DataFrameGroupBy.idxmax` to identify
    the player who achieved the best average:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步总结，通过查找每个赛季的平均值和最大值 `batting_average`，甚至可以使用 `pd.core.groupby.DataFrameGroupBy.idxmax`
    来识别出打击率最高的球员：
- en: '[PRE112]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: As we can see, the mean batting average fluctuates each year, with those numbers
    having been higher back toward the year 2000\. In the year 2005, the mean batting
    average was .277, with the best hitter (lee-d002, or Derrek Lee) having hit .335\.
    The best hitter in 2019 (andet001, or Tim Anderson) also averaged .335, but the
    overall league was down around .269\. Therefore, a strong argument could be made
    that Tim Anderson’s 2019 season was more impressive than Derrek Lee’s 2005 season,
    at least through the lens of batting average.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，平均击球率每年都有波动，而这些数字在2000年左右较高。在2005年，平均击球率为0.277，最佳击球手（lee-d002，或德雷克·李）击出了0.335的成绩。2019年最佳击球手（andet001，或蒂姆·安德森）同样打出了0.335的平均击球率，但整个联盟的平均击球率约为0.269。因此，有充分的理由认为，蒂姆·安德森2019赛季的表现比德雷克·李2005赛季的表现更为出色，至少从击球率的角度来看。
- en: While taking the mean can be useful, it doesn’t tell the full story of what
    goes on within a given season. We would probably like to get a better feel for
    the overall distribution of batting averages across each season, for which a visualization
    is in order. The violin plot we discovered back in the *Plotting movie ratings
    by decade with seaborn* recipe can help us understand this in more detail.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算均值很有用，但它并没有完整地展示一个赛季内发生的所有情况。我们可能更希望了解每个赛季击球率的整体分布，这就需要可视化图表来呈现。我们在*用seaborn按十年绘制电影评分*的食谱中发现的小提琴图，可以帮助我们更详细地理解这一点。
- en: 'First let’s set up our seaborn import, and have Matplotlib draw plots as soon
    as possible:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们设置好seaborn的导入，并尽快让Matplotlib绘制图表：
- en: '[PRE114]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: Next, we will want to make a few considerations for seaborn. Seaborn does not
    make use of `pd.MultiIndex`, so we are going to move our index values to columns
    with a call to `pd.DataFrame.reset_index`. Additionally, seaborn can easily misinterpret
    discrete *year* values like 2000, 2001, 2002, and so on for a continuous range,
    which we can solve by turning that column into a categorical data type.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要为seaborn做一些调整。seaborn不支持`pd.MultiIndex`，因此我们将使用`pd.DataFrame.reset_index`将索引值移到列中。此外，seaborn可能会误将离散的*年份*值（如2000、2001、2002等）解释为一个连续的范围，我们可以通过将该列转化为类别数据类型来解决这个问题。
- en: 'The `pd.CategoricalDtype` we want to construct is also ideally ordered, so
    that pandas can ensure the year 2000 is followed by 2001, which is followed by
    2002, and so on:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望构建的`pd.CategoricalDtype`是有序的，这样pandas才能确保2000年之后是2001年，2001年之后是2002年，依此类推：
- en: '[PRE115]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '23 years of data on a single plot may take up a lot of space, so let’s just
    look at the years 2000-2009 first:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 23年的数据绘制在一张图上可能会占据大量空间，所以我们先来看2000-2009年这段时间的数据：
- en: '[PRE117]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '![](img/B31091_08_09.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_08_09.png)'
- en: 'We intentionally made the call to `plt.subplots()` and used `ax.set_xlim(0.15,
    0.4)` so that the x-axis would not change when plotting the remaining years:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们故意调用了`plt.subplots()`并使用`ax.set_xlim(0.15, 0.4)`，以确保在绘制其余年份时x轴不会发生变化：
- en: '[PRE118]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '![](img/B31091_08_10.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_08_10.png)'
- en: While some years show skew in the data (e.g., 2014 skewing right and 2018 skewing
    left), we can generally imagine the distribution of this data as an approximation
    of a normal distribution. Therefore, to try and better compare the peak performances
    across different years, we can use a technique whereby we *normalize* data within
    each season. Rather than thinking in terms of absolute batting averages like .250,
    we instead think of how far beyond the norm within a season a batter’s performance
    is.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然某些年份的数据表现出偏斜（例如2014年向右偏斜，2018年向左偏斜），但我们通常可以将这些数据的分布视为接近正态分布。因此，为了更好地比较不同年份的最佳表现，我们可以使用一种技术，即在每个赛季内*标准化*数据。我们不再用绝对的击球率（如0.250）来思考，而是考虑击球手的表现偏离赛季常态的程度。
- en: 'More specifically, we can use Z-score normalization, which would appear as
    follows when mathematically represented:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，我们可以使用Z-score标准化，数学表示如下：
- en: '![](img/B31091_08_001.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31091_08_001.png)'
- en: Here, `![](img/B31091_08_002.png)` is the mean and `![](img/B31091_08_003.png)`
    is the standard deviation.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`![](img/B31091_08_002.png)`是均值，`![](img/B31091_08_003.png)`是标准差。
- en: 'Calculating this in pandas is rather trivial; all we need to do is define our
    custom `normalize` function and use that as an argument to `pd.core.groupby.DataFrameGroupBy.transform`
    to assign each combination of year and player their normalized batting average.
    Using that in subsequent group by operations allows us to better compare the peak
    performance each year across different years:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas中计算这一点相当简单；我们需要做的就是定义一个自定义的`normalize`函数，并将其作为参数传递给`pd.core.groupby.DataFrameGroupBy.transform`，以为每一组年和球员的组合分配标准化的击球率。在随后的group
    by操作中使用它，可以帮助我们更好地比较不同年份间的最佳表现：
- en: '[PRE119]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: According to this analysis, the 2023 season by Luis Arráez is the most impressive
    batting average performance since the year 2000\. His `league_max_avg` achieved
    that year may appear as the lowest out of our top five, but so was the `league_mean_avg`
    in 2023.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这一分析，路易斯·阿雷兹（Luis Arráez）在2023赛季的击球率表现是自2000年以来最为出色的。他当年创下的`league_max_avg`可能看起来是我们前五名中的最低值，但2023年的`league_mean_avg`也正是如此。
- en: As you can see from this recipe, effective use of pandas’ Group By functionality
    allows you to more fairly evaluate records within different groups. Our example
    used professional baseball players within a season, but that same methodology
    could be extended to evaluate users within different age groups, products within
    different product lines, stocks within different sectors, and so on. Simply put,
    the possibilities for exploring your data with group by are endless!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个示例所示，合理使用pandas的Group By功能可以帮助你更公平地评估不同组别中的记录。我们的示例使用了一个赛季内的职业棒球运动员，但同样的方法也可以扩展到评估不同年龄组的用户、不同产品线的产品、不同领域的股票等。简而言之，通过Group
    By探索你的数据的可能性是无穷无尽的！
- en: Join our community on Discord
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/pandas](https://packt.link/pandas)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/pandas](https://packt.link/pandas)'
- en: '![](img/QR_Code5040900042138312.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5040900042138312.png)'
