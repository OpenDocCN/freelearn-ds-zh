- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Architecture Optimization of Deep Learning Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习网络的架构优化
- en: This chapter describes how genetic algorithms can be used to improve the performance
    of **artificial neural network** (**ANN**)-based models by optimizing the **network
    architecture** of these models. We will start with a brief introduction to **neural
    networks** (**NNs**) and **deep learning** (**DL**). After introducing the *Iris
    dataset* and **Multilayer Perceptron** (**MLP**) classifiers, we will demonstrate
    **network architecture optimization** using a genetic algorithm-based solution.
    Then, we will extend this approach to combine network architecture optimization
    with model **hyperparameter tuning**, which will be jointly carried out by a genetic
    algorithm-based solution.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了如何通过优化**人工神经网络**（**ANN**）模型的**网络架构**，利用遗传算法来提高这些模型的性能。我们将首先简要介绍**神经网络**（**NNs**）和**深度学习**（**DL**）。在介绍了*鸢尾花数据集*和**多层感知器**（**MLP**）分类器后，我们将展示如何通过基于遗传算法的解决方案来进行**网络架构优化**。随后，我们将扩展此方法，将网络架构优化与模型**超参数调优**相结合，二者将通过基于遗传算法的解决方案共同完成。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: Understanding the basic concepts of ANNs and DL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解人工神经网络和深度学习的基本概念
- en: Enhancing the performance of a DL classifier using network architecture optimization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过网络架构优化来提升深度学习分类器的性能
- en: Further enhancing the performance of the DL classifier by combining network
    architecture optimization with hyperparameter tuning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将网络架构优化与超参数调优相结合，进一步增强深度学习分类器的性能
- en: We will start this chapter with an overview of ANNs. If you are a seasoned data
    scientist, feel free to skip the introductory sections.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从人工神经网络的概述开始。如果你是经验丰富的数据科学家，可以跳过介绍部分。
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using Python 3 with the following supporting libraries:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用 Python 3，并配合以下支持库：
- en: '**deap**'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**deap**'
- en: '**numpy**'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**numpy**'
- en: '**scikit-learn**'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scikit-learn**'
- en: Important note
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you use the **requirements.txt** file we provide (see [*Chapter 3*](B20851_03.xhtml#_idTextAnchor091)),
    these libraries are already included in your environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用我们提供的**requirements.txt**文件（见[*第3章*](B20851_03.xhtml#_idTextAnchor091)），这些库已经包含在你的环境中。
- en: In addition, we will be using the UCI Iris flower dataset ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用 UCI 鸢尾花数据集（[https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)）。
- en: 'The programs that will be used in this chapter can be found in this book’s
    GitHub repository at the following link:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用的程序可以在本书的 GitHub 仓库中找到，链接如下：
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_09](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_09)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_09](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_09)'
- en: 'Check out the following video to see the code in action: [https://packt.link/OEBOd](https://packt.link/OEBOd)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，查看代码实际操作：[https://packt.link/OEBOd](https://packt.link/OEBOd)
- en: ANNs and DL
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络和深度学习
- en: 'Inspired by the structure of the human brain, NNs are among the most commonly
    used models in **machine learning** (**ML**). The basic building blocks of these
    networks are nodes, or **neurons**, which are based on the biological neuron cell,
    as depicted in the following diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 受人脑结构的启发，神经网络是**机器学习**（**ML**）中最常用的模型之一。这些网络的基本构建块是节点或**神经元**，它们基于生物神经元细胞，如下图所示：
- en: '![Figure 9.1: Biological neuron model ](img/B20851_09_1.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1：生物神经元模型](img/B20851_09_1.jpg)'
- en: 'Figure 9.1: Biological neuron model'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：生物神经元模型
- en: 'Source: [https://simple.wikipedia.org/wiki/Neuron#/media/File:Neuron.svg](https://simple.wikipedia.org/wiki/Neuron#/media/File:Neuron.svg)
    by Dhp1080'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://simple.wikipedia.org/wiki/Neuron#/media/File:Neuron.svg](https://simple.wikipedia.org/wiki/Neuron#/media/File:Neuron.svg)
    由Dhp1080提供
- en: The neuron cell’s **dendrites**, which surround the **cell body** on the left-hand
    side of the preceding diagram, are used as inputs from multiple similar cells,
    while the long **axon**, coming out of the **cell body**, serves as output and
    can be connected to multiple other cells via its **terminals**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元细胞的**树突**，在前图左侧围绕**细胞体**，用作来自多个相似细胞的输入，而从**细胞体**出来的长**轴突**则作为输出，可以通过其**末端**连接到多个其他细胞。
- en: 'This structure is mimicked by an artificial model called a **perceptron**,
    illustrated as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构通过一个人工模型——**感知器**来模拟，如下所示：
- en: '![Figure 9.2: Artificial neuron model – the perceptron](img/B20851_09_2.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2：人工神经元模型——感知器](img/B20851_09_2.jpg)'
- en: 'Figure 9.2: Artificial neuron model – the perceptron'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：人工神经元模型——感知器
- en: The perceptron calculates the output by multiplying each of the input values
    by a certain **weight**; the results are accumulated, and a **bias** value is
    added to the sum. A non-linear **activation function** then maps the result to
    the output. This functionality emulates the operation of the biological neuron,
    which fires (sends a series of pulses from its output) when the weighted sum of
    the inputs is above a certain threshold.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器通过将每个输入值与一定的**权重**相乘来计算输出；结果会累加，然后加上一个**偏置**值。一个非线性的**激活函数**随后将结果映射到输出。这种功能模仿了生物神经元的运作，当输入的加权和超过某个阈值时，神经元会“激发”（从其输出端发送一系列脉冲）。
- en: The perceptron model can be used for simple classification and regression tasks
    if we adjust its weight and bias values so that they map certain inputs to the
    desired output levels. However, a much more capable model can be constructed when
    connecting multiple perceptron units in a structure called an MLP, which will
    be described in the next subsection.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们调整感知器的权重和偏置值，使其将某些输入映射到期望的输出水平，则可以使用感知器模型进行简单的分类和回归任务。然而，通过将多个感知器单元连接成一个叫做MLP的结构，可以构建一个功能更强大的模型，下一小节将对其进行描述。
- en: MLP
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLP
- en: 'An MLP extends the idea of the perceptron by using numerous nodes, each one
    implementing a perceptron. The nodes in an MLP are arranged in **layers**, and
    each layer is connected to the next. The basic structure of an MLP is illustrated
    in the following diagram:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MLP通过使用多个节点扩展了感知器的概念，每个节点实现一个感知器。MLP中的节点按**层**排列，每一层与下一层相连接。MLP的基本结构如下图所示：
- en: '![Figure 9.3: The basic structure of an MLP](img/B20851_09_3.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3：MLP的基本结构](img/B20851_09_3.jpg)'
- en: 'Figure 9.3: The basic structure of an MLP'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：MLP的基本结构
- en: 'An MLP consists of three main parts:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: MLP由三个主要部分组成：
- en: '**Input layer**: Receives the input values and connects each of them to every
    neuron in the next layer.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：接收输入值，并将每个输入值与下一个层中的每个神经元相连接。'
- en: '**Output layer**: Delivers the results calculated by the MLP. When the MLP
    is used as a **classifier**, each of the outputs represents one of the classes.
    When the MLP is used for **regression**, there will be a single output node, producing
    a continuous value.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：传递MLP计算的结果。当MLP用作**分类器**时，每个输出表示一个类别。当MLP用于**回归**时，将只有一个输出节点，产生一个连续值。'
- en: '**Hidden layer(s)**: Provide the true power and complexity of this model. While
    the preceding diagram shows only two hidden layers, there can be numerous hidden
    layers, each an arbitrary size, that are placed between the input and output layers.
    As the number of hidden layers grows, the network becomes deeper and is capable
    of performing an increasingly more complex and non-linear mapping between the
    inputs and the outputs.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：提供该模型的真正力量和复杂性。尽管前面的图示只显示了两个隐藏层，但可以有多个隐藏层，每个隐藏层的大小可以是任意的，这些隐藏层位于输入层和输出层之间。随着隐藏层数量的增加，网络变得更深，能够执行越来越复杂的非线性映射，连接输入和输出。'
- en: Training this model involves adjusting the weight and bias values for each of
    the nodes. This is typically done using a family of algorithms called **backpropagation**.
    The basic principle of backpropagation is to minimize the error between the actual
    outputs and the desired ones by propagating the output error through the layers
    of the MLP model, from the output layer inward. The process begins by defining
    a cost (or “loss”) function, typically a measure of the difference between the
    predicted outputs and the actual target values. The weights and biases of the
    various nodes are adjusted so that those that contributed the most to the error
    see the greatest adjustments. By iteratively reducing the cost function, the algorithm
    refines the model parameters to improve performance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个模型涉及调整每个节点的权重和偏置值。通常，这通过一类被称为**反向传播**的算法来实现。反向传播的基本原理是通过将输出误差从输出层向内传播到MLP模型的各个层，最小化实际输出与期望输出之间的误差。该过程从定义一个成本（或“损失”）函数开始，通常是预测输出与实际目标值之间差异的度量。通过调整各个节点的权重和偏置，使得那些对误差贡献最大节点的调整最大。通过迭代减少成本函数，算法逐步优化模型参数，提高性能。
- en: For many years, the computational limitations of backpropagation algorithms
    restricted MLPs to no more than two or three hidden layers, until new developments
    changed matters dramatically. These will be explained in the next section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，反向传播算法的计算限制使得MLP的隐藏层数不超过两层或三层，直到新的发展极大地改变了这一局面。相关内容将在下一节中详细说明。
- en: DL and convolutional NNs
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习和卷积神经网络
- en: In recent years, backpropagation algorithms have made a leap forward, enabling
    the use of a large number of hidden layers in a single network. In these **deep
    NNs** (**DNNs**), each layer can interpret a combination of several simpler abstract
    concepts that were learned by the nodes of the previous layer and produce higher-level
    concepts. For example, when implementing a face recognition task, the first layer
    will process the pixels of an image and learn to detect edges in different orientations.
    The next layer may assemble these into lines, corners, and so on, up to a layer
    that detects facial features such as nose and lips, and finally, one that combines
    these into the complete concept of a face.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，反向传播算法取得了突破，允许在单个网络中使用大量的隐藏层。在这些**深度神经网络**（**DNNs**）中，每一层可以解释前一层节点所学到的多个更简单的抽象概念，并产生更高层次的概念。例如，在实现人脸识别任务时，第一层将处理图像的像素并学习检测不同方向的边缘。下一层可能将这些边缘组合成线条、角点等，直到某一层能够检测面部特征，如鼻子和嘴唇，最后，一层将这些特征结合成完整的“面部”概念。
- en: Further advancements have brought about the idea of **convolutional NNs** (**CNNs**).
    These structures can reduce the count of nodes in DNNs that process two-dimensional
    information (such as images) by treating nearby inputs differently compared to
    inputs that are far apart. As a result, these models have proved especially successful
    when it comes to image and video processing tasks. Besides fully connected layers,
    similar to the hidden layers in the MLP, these networks utilize pooling (down-sampling)
    layers, which aggregate outputs of neurons from preceding layers, and convolutional
    layers, which are used for detecting specific features, such as edges in various
    orientations, by effectively sliding a filter over the input image.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的发展催生了**卷积神经网络**（**CNNs**）的概念。这些结构通过对相邻输入与远离的输入进行不同处理，能够减少处理二维信息（如图像）的深度神经网络（DNNs）中的节点数量。因此，这些模型在图像和视频处理任务中尤为成功。除了与多层感知器（MLP）中的隐藏层类似的全连接层外，这些网络还使用池化（下采样）层，池化层将前面层的神经元输出进行汇总，以及卷积层，卷积层通过在输入图像上有效滑动滤波器来检测特定特征，例如各种方向的边缘。
- en: Training such `scikit-learn` library and a simple dataset. The principles that
    will be used, however, still apply to more complex networks and datasets.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`scikit-learn`库和一个简单的数据集进行训练。然而，所使用的原理仍适用于更复杂的网络和数据集。
- en: In the next section, we will find out how the architecture of an MLP can be
    optimized using a genetic algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何使用遗传算法优化多层感知器（MLP）的架构。
- en: Optimizing the architecture of a DL classifier
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化深度学习分类器的架构
- en: When creating a NN model for a given ML task, one crucial design decision that
    needs to be made is the configuration of the **network architecture**. In the
    case of an MLP, the number of nodes in the input and output layers is determined
    by the characteristics of the problem at hand. Therefore, the choices to be made
    are about the hidden layers—how many layers, and how many nodes are in each layer.
    Some rules of thumb can be employed for making these decisions, but in many cases,
    identifying the best choices can turn into a cumbersome trial-and-error process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在为给定的机器学习任务创建神经网络模型时，一个关键的设计决策是**网络架构**的配置。对于多层感知机（MLP）而言，输入层和输出层的节点数由问题的特征决定。因此，待决策的部分是隐藏层——有多少层，每一层有多少个节点。可以使用一些经验法则来做出这些决策，但在许多情况下，识别最佳选择可能会变成一个繁琐的试错过程。
- en: One way to handle network architecture parameters is to consider them as hyperparameters
    of the model since they need to be determined before training is done and, consequently,
    affect the training’s results. In this section, we are going to apply this approach
    and use a genetic algorithm to search for the best combination of hidden layers,
    in a similar manner to the way we went about choosing the best hyperparameter
    values in the previous chapter. Let’s start with the task we want to tackle –
    the **Iris** **flower classification**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 处理网络架构参数的一种方法是将它们视为模型的超参数，因为它们需要在训练之前确定，并且因此会影响训练结果。在本节中，我们将应用这种方法，并使用遗传算法来搜索最佳的隐藏层组合，类似于我们在上一章选择最佳超参数值的方式。我们从我们想要解决的任务开始——**鸢尾花**
    **分类**。
- en: The Iris flower dataset
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 鸢尾花数据集
- en: Perhaps the most well-studied dataset, the *Iris flower dataset* ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris))
    contains measurements of the `sepal`and `petal` parts of three Iris species (Iris
    setosa, Iris virginica, and Iris versicolor), as taken by biologists in 1936.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是研究得最透彻的数据集，*鸢尾花数据集*（[https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)）包含了三种鸢尾花（鸢尾花、弗吉尼亚鸢尾花、和变色鸢尾花）的`萼片`和`花瓣`的测量数据，这些数据由生物学家在1936年收集。
- en: 'The dataset contains 50 samples from each of the three species, and consists
    of the following four features:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含来自三种物种的每种50个样本，并由以下四个特征组成：
- en: '**sepal_length (cm)**'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**萼片长度 (cm)**'
- en: '**sepal_width (cm)**'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**萼片宽度 (cm)**'
- en: '**petal_length (cm)**'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**花瓣长度 (cm)**'
- en: '**petal_width (cm)**'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**花瓣宽度 (cm)**'
- en: 'This dataset is directly available via the `scikit-learn` library and can be
    initialized as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集可以通过`scikit-learn`库直接获得，并可以通过如下方式初始化：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In our experiments, we will be using an MLP classifier in conjunction with this
    dataset and harness the power of genetic algorithms to find the network architecture—the
    number of hidden layers and the number of nodes in each layer—that will yield
    the best classification accuracy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将使用多层感知机（MLP）分类器与此数据集结合，并利用遗传算法的力量来寻找最佳的网络架构——隐藏层的数量和每层节点的数量——以获得最佳的分类准确率。
- en: Since we are using the genetic algorithms approach, the first thing we need
    to do is find a way to represent this architecture using a chromosome, as described
    in the next subsection.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用遗传算法的方法，首先需要做的是找到一种方法，通过染色体表示这种架构，如下一个小节所述。
- en: Representing the hidden layer configuration
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示隐藏层配置
- en: Since the architecture of an MLP is determined by the hidden layer configuration,
    let’s explore how this configuration can be represented in our solution. The hidden
    layer configuration of the `sklearn` MLP ([https://scikit-learn.org/stable/modules/neural_networks_supervised.html](https://scikit-learn.org/stable/modules/neural_networks_supervised.html))
    model is conveyed via the `hidden_layer_sizes` tuple, which is sent as a parameter
    to the model’s constructor. By default, the value of this tuple is `(100,)`, which
    means a single hidden layer of 100 nodes. If we wanted, for example, to configure
    the MLP with three hidden layers of 20 nodes each, this parameter’s value would
    be `(20, 20, 20)`. Before we implement our genetic algorithm-based optimizer for
    the hidden layer configuration, we need to define a chromosome that can be translated
    into this pattern.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于多层感知器（MLP）的结构由隐藏层配置决定，让我们来探讨如何在我们的解决方案中表示这一配置。`sklearn` MLP 的隐藏层配置（[https://scikit-learn.org/stable/modules/neural_networks_supervised.html](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)）通过
    `hidden_layer_sizes` 元组传递，这个元组作为参数传递给模型的构造函数。默认情况下，这个元组的值为 `(100,)`，意味着只有一个包含
    100 个节点的隐藏层。如果我们想要将 MLP 配置为三个每个包含 20 个节点的隐藏层，则该参数的值应为 `(20, 20, 20)`。在我们实现基于遗传算法的优化器来调整隐藏层配置之前，我们需要定义一个可以转换为这种模式的染色体。
- en: 'To accomplish this, we need to come up with a chromosome that can both express
    the number of layers and the number of nodes in each layer. A variable-length
    chromosome that can be directly translated into the variable-length tuple that’s
    used as the model’s `hidden_layer_sizes` parameter is one option; however, this
    approach would require custom, possibly cumbersome, genetic operators. To be able
    to use our standard genetic operators, we will use a fixed-length representation.
    When using this approach, the maximum number of layers is decided in advance,
    and all the layers are always represented, but not necessarily expressed in the
    solution. For example, if we decide to limit the network to four hidden layers,
    the chromosome will look as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，我们需要设计一种染色体，既能够表示层的数量，又能表示每层的节点数。一种可行的方案是使用一个可变长度的染色体，该染色体可以直接转换为用作模型
    `hidden_layer_sizes` 参数的可变长度元组；然而，这种方法需要定制且可能繁琐的遗传操作符。为了能够使用我们的标准遗传操作符，我们将使用一个固定长度的表示法。采用这种方法时，最大层数是预先确定的，所有层始终被表示，但不一定会在解决方案中得到体现。例如，如果我们决定将网络限制为四个隐藏层，染色体将如下所示：
- en: '[n 1, n 2, n 3, n 4]'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[n 1, n 2, n 3, n 4]'
- en: 'Here, n i denotes the number of nodes in the layer i . However, to control
    the actual number of hidden layers in the network, some of these values may be
    zero, or negative. Such a value means that no more layers will be added to the
    network. The following examples illustrate this method:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，n i 表示第 i 层的节点数。然而，为了控制网络中实际的隐藏层数量，这些值中的一些可能是零或负数。这样的值意味着不会再添加更多的层到网络中。以下示例展示了这种方法：
- en: The chromosome [10, 20, -5, 15] is translated into the tuple (10, 20) since
    -5 terminates the layer count
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 染色体 [10, 20, -5, 15] 被转换为元组 (10, 20)，因为 -5 终止了层的计数
- en: The chromosome [10, 0, -5, 15] is translated into the tuple (10, ) since 0 terminates
    the layer count
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 染色体 [10, 0, -5, 15] 被转换为元组 (10, )，因为 0 终止了层的计数
- en: The chromosome [10, 20, 5, -15] is translated into the tuple (10, 20, 5) since
    -15 terminates the layer count
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 染色体 [10, 20, 5, -15] 被转换为元组 (10, 20, 5)，因为 -15 终止了层的计数
- en: The chromosome [10, 20, 5, 15] is translated into the tuple (10, 20, 5, 15)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 染色体 [10, 20, 5, 15] 被转换为元组 (10, 20, 5, 15)
- en: To guarantee that there is at least one hidden layer, we can make sure that
    the first parameter is always greater than zero. The other parameters can have
    varying distributions around zero so that we can control their chances of being
    the terminating parameters.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保证至少有一个隐藏层，我们可以确保第一个参数始终大于零。其他参数可以围绕零分布变化，以便我们能够控制它们作为终止参数的可能性。
- en: 'In addition, even though this chromosome is made up of integers, we chose to
    utilize float numbers instead, just like we did in the previous chapter for various
    types of variables. Using a list of float numbers is convenient as it allows us
    to use existing genetic operators while being able to easily extend the chromosome
    so that it includes other parameters of different types, which we will do later
    on. The float numbers can be translated back into integers using the `round()`
    function. A couple of examples of this generalized approach are as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管该染色体由整数构成，我们选择改用浮动数字，就像我们在上一章中对各种类型的变量所做的那样。使用浮动数字的列表非常方便，因为它允许我们使用现有的遗传算子，同时能够轻松扩展染色体，以便包含其他不同类型的参数，稍后我们会这样做。浮动数字可以通过`round()`函数转换回整数。以下是这种通用方法的几个示例：
- en: The chromosome [9.35, 10.71, -2.51, 17.99] is translated into the tuple (9,
    11)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 染色体[9.35, 10.71, -2.51, 17.99]被转换为元组(9, 11)
- en: The chromosome [9.35, 10.71, 2.51, -17.99] is translated into the tuple (9,
    11, 3)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 染色体[9.35, 10.71, 2.51, -17.99]被转换为元组(9, 11, 3)
- en: To evaluate a given architecture-representing chromosome, we will need to translate
    it back into the tuple of layers, create an MLP classifier implementing these
    layers, train it, and evaluate it. We will learn how to do this in the next subsection.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估给定的表示架构的染色体，我们需要将其转换回层的元组，创建一个实现这些层的MLP分类器，训练它并进行评估。我们将在下一小节中学习如何做到这一点。
- en: Evaluating the classifier’s accuracy
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估分类器的准确性
- en: 'Let’s start with a Python class that encapsulates the MLP classifier’s accuracy
    evaluation for the Iris dataset. The class is called `MlpLayersTest` and can be
    found in the `mlp_layers_test.py` file, which is located at the following link:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个Python类开始，该类封装了Iris数据集的MLP分类器准确性评估。该类被称为`MlpLayersTest`，可以在以下链接的`mlp_layers_test.py`文件中找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_layers_test.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_layers_test.py)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_layers_test.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_layers_test.py)'
- en: 'The main functionality of this class is highlighted as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的主要功能如下所示：
- en: 'The **convertParam()** method of the class takes a list called **params**.
    This is actually the chromosome that we described in the previous subsection and
    contains the float values that represent up to four hidden layers. The method
    transforms this list of floats into the **hidden_layer_sizes** tuple:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该类的**convertParam()**方法接收一个名为**params**的列表。实际上，这就是我们在上一小节中描述的染色体，它包含表示最多四个隐藏层的浮动值。该方法将这些浮动值的列表转换为**hidden_layer_sizes**元组：
- en: '[PRE1]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The **getAccuracy()** method takes the **params** list representing the configuration
    of the hidden layers, uses the **convertParam()** method to transform it into
    a **hidden_layer_sizes** tuple, and initializes an MLP classifier with this tuple:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**getAccuracy()**方法接受表示隐藏层配置的**params**列表，使用**convertParam()**方法将其转换为**hidden_layer_sizes**元组，并使用该元组初始化MLP分类器：'
- en: '[PRE2]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, it finds the accuracy of the classifier using the same *k-fold cross-validation*
    calculation that we created for the *Wine dataset* in [*Chapter 8*](B20851_08.xhtml#_idTextAnchor238),
    *Hyperparameter Tuning of Machine* *Learning Models*:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，它使用与我们在[ *第8章*](B20851_08.xhtml#_idTextAnchor238)中为*葡萄酒数据集*创建的相同*k折交叉验证*计算来找到分类器的准确性，*机器学习模型的超参数调优*：
- en: '[PRE3]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `MlpLayersTest` class is utilized by the genetic algorithm-based optimizer.
    We will explain this part in the next section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`MlpLayersTest`类被基于遗传算法的优化器所使用。我们将在下一节中解释这一部分。'
- en: Optimizing the MLP architecture using genetic algorithms
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用遗传算法优化MLP架构
- en: 'Now that we have a way to represent the architecture configuration of the MLP
    that’s used to classify the Iris flower dataset and a way to determine the accuracy
    of the MLP for each configuration, we can move on and create a genetic algorithm-based
    optimizer to search for the configuration – the number of hidden layers (up to
    4, in our case) and the number of nodes in each layer – that will yield the best
    accuracy. This solution is implemented by the `01_optimize_mlp_layers.py` Python
    program, which is located at the following link:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种表示用于分类Iris花卉数据集的MLP架构配置的方法，并且有了一种确定每个配置的MLP准确性的方法，我们可以继续并创建一个基于遗传算法的优化器，来搜索最佳的配置——隐藏层的数量（在我们的例子中最多为4层）以及每层的节点数量——以获得最佳的准确性。这个解决方案通过位于以下链接的`01_optimize_mlp_layers.py`
    Python程序实现：
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/01_optimize_mlp_layers.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/01_optimize_mlp_layers.py)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/01_optimize_mlp_layers.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/01_optimize_mlp_layers.py)'
- en: 'The following steps describe the main parts of this program:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了该程序的主要部分：
- en: 'We start by setting the lower and upper boundary for each of the float values
    representing a hidden layer. The first hidden layer is given the range [5, 15],
    while the rest of the layers start from increasingly larger negative values, which
    increases their chances of terminating the layer count:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先为表示隐藏层的每个浮动值设置上下边界。第一个隐藏层的范围为[5, 15]，而其余的层从逐渐增大的负值开始，这增加了它们终止层数的几率：
- en: '[PRE4]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we create an instance of the **MlpLayersTest** class, which will allow
    us to test the various combinations of the hidden layers’ architecture:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个**MlpLayersTest**类的实例，这将允许我们测试不同的隐藏层架构组合：
- en: '[PRE5]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Since our goal is to maximize the accuracy of the classifier, we define a single
    objective, maximizing fitness strategy:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的目标是最大化分类器的准确性，我们定义了一个单一的目标，即最大化适应度策略：
- en: '[PRE6]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we employ the same approach we used in the previous chapter—since the
    solution is represented by a list of float values, each of a different range,
    we use the following loop to iterate over all pairs of lower-bound, upper-bound
    values, and for each range, we create a separate **toolbox** operator, **layer_size_attribute**,
    that will later be used to generate random float values in the appropriate range:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们采用与上一章相同的方法——由于解决方案由一个浮动数值列表表示，每个数值的范围不同，我们使用以下循环遍历所有的下限和上限值对，并且对于每个范围，我们创建一个单独的**toolbox**操作符**layer_size_attribute**，该操作符将用于在适当的范围内生成随机浮动数值：
- en: '[PRE7]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we create a **layer_size_attributes** tuple, which contains the separate
    float number generators we just created for each hidden layer:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个**layer_size_attributes**元组，其中包含我们为每个隐藏层刚刚创建的单独浮动数值生成器：
- en: '[PRE8]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we can use this **layer_size_attributes** tuple in conjunction with DEAP’s
    built-in **initCycle()** operator to create a new **individualCreator** operator
    that fills up an individual instance with a combination of randomly generated
    hidden layer-size values:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以将这个**layer_size_attributes**元组与DEAP内置的**initCycle()**操作符结合使用，来创建一个新的**individualCreator**操作符，该操作符将通过随机生成的隐藏层大小值的组合填充一个个体实例：
- en: '[PRE9]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we instruct the genetic algorithm to use the **getAccuracy()** method
    of the **MlpLayersTest** instance for fitness evaluation. As a reminder, the **getAccuracy()**
    method, which we described in the previous subsection, converts the given individual—a
    list of four floats—into a tuple of hidden layer sizes. These are used to configure
    the MLP classifier. Then, we train the classifier and evaluate its accuracy using
    k-fold cross-validation:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们指示遗传算法使用**MlpLayersTest**实例的**getAccuracy()**方法进行适应度评估。提醒一下，**getAccuracy()**方法（我们在上一小节中描述过）将给定个体——一个包含四个浮动数值的列表——转换为一个隐藏层大小的元组。这些元组将用于配置MLP分类器。然后，我们训练分类器并使用k折交叉验证评估其准确性：
- en: '[PRE10]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As for the genetic operators, we repeat the configuration from the previous
    chapter. While for the *selection* operator, we use the usual tournament selection
    with a tournament size of 2, we choose *crossover* and *mutation* operators that
    are specialized for bounded float-list chromosomes and provide them with the boundaries
    we defined for each hidden layer:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 至于遗传操作符，我们重复了上一章的配置。对于*选择*操作符，我们使用常规的锦标赛选择，锦标赛大小为2，选择适用于有界浮动列表染色体的*交叉*和*变异*操作符，并为每个隐藏层提供我们定义的边界：
- en: '[PRE11]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In addition, we continue to use the *elitist* approach, where the **hall-of-fame**
    (**HOF**) members—the current best individuals—are always passed untouched to
    the next generation:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们继续使用*精英*方法，其中**名人堂**（**HOF**）成员——当前最佳个体——总是被不加修改地传递到下一代：
- en: '[PRE12]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When running the algorithm for 10 generations with a population size of 20,
    we get the following outcome:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们用20个个体运行算法10代时，得到的结果如下：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The preceding results indicate that, within the ranges we defined, the best
    combination that was found was of three hidden layers of size 15, 5, and 8, respectively.
    The classification accuracy that we achieved with these values is about 86.7%.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的结果表明，在我们定义的范围内，找到的最佳组合是三个隐藏层，大小分别为15、5和8。我们使用这些值所获得的分类准确率大约为86.7%。
- en: This accuracy seems to be a reasonable result for the problem at hand. However,
    there’s more we can do to improve it even further.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这个准确率似乎是对于当前问题的合理结果。然而，我们还有更多的工作可以做，以进一步提高其准确性。
- en: Combining architecture optimization with hyperparameter tuning
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将架构优化与超参数调优结合
- en: While optimizing the network architecture configuration—the hidden layer parameters—we
    have been using the default (hyper) parameters of the MLP classifier. However,
    as we saw in the previous chapter, tuning the various hyperparameters has the
    potential to increase the classifier’s performance. Can we incorporate hyperparameter
    tuning into our optimization? As you may have guessed, the answer is yes. But
    first, let’s take a look at the hyperparameters we would like to optimize.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化网络架构配置——即隐藏层参数——时，我们一直使用MLP分类器的默认（超）参数。然而，正如我们在上一章中所看到的，调优各种超参数有可能提高分类器的性能。我们能否将超参数调优纳入我们的优化中？如你所猜测的，答案是肯定的。但在此之前，让我们先来看一下我们希望优化的超参数。
- en: 'The `scikit-learn` implementation of the MLP classifier contains numerous tunable
    hyperparameters. For our demonstration, we will concentrate on the following hyperparameters:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`实现的MLP分类器包含许多可调的超参数。为了展示，我们将集中在以下超参数上：'
- en: '| **Name** | **Type** | **Description** | **Default Value** |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **类型** | **描述** | **默认值** |'
- en: '| activation | enumerated | Activation function for the hidden layers:`{''identity'',
    ''logistic'', ''``tanh'', ''relu''}` | `''``relu''` |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | 枚举类型 | 隐藏层的激活函数：`{''identity'', ''logistic'', ''tanh'', ''relu''}`
    | `''relu''` |'
- en: '| solver | enumerated | The solver for weight optimization:`{''lbfgs'', ''``sgd'',
    ''adam''}` | `''``adam''` |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 求解器 | 枚举类型 | 权重优化的求解器：`{''lbfgs'', ''sgd'', ''adam''}` | `''adam''` |'
- en: '| alpha | float | Strength of the L2 regularization term | `0.0001` |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| alpha | 浮动型 | L2正则化项的强度 | `0.0001` |'
- en: '| learning_rate | enumerated | Learning rate schedule for weight updates:{‘constant’,
    ‘invscaling’,’adaptive’} | `''``constant''` |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 枚举类型 | 权重更新的学习率计划：{‘constant’, ‘invscaling’, ‘adaptive’} | `''constant''`
    |'
- en: 'Table 9.1: MLP hyperparameters'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1：MLP超参数
- en: As we saw in the previous chapter, a floating point-based chromosome representation
    allows us to combine various types of hyperparameters into the genetic algorithm-based
    optimization process. Since we already used a floating-point-based chromosome
    to represent the configuration of the hidden layers, we can now incorporate other
    hyperparameters into the optimization process by augmenting the chromosome accordingly.
    Let’s find out how we can do this.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章所看到的，基于浮动点的染色体表示使我们能够将各种类型的超参数结合到基于遗传算法的优化过程中。由于我们已经使用基于浮动点的染色体来表示隐藏层的配置，我们现在可以通过相应地扩展染色体，将其他超参数纳入优化过程中。让我们来看看我们如何做到这一点。
- en: Solution representation
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解的表示
- en: To the existing four floats, representing our network architecture configuration—
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于现有的四个浮动值，表示我们的网络架构配置——
- en: '[n 1, n 2, n 3, n 4]—we can add the following four hyperparameters:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[n 1, n 2, n 3, n 4]——我们可以添加以下四个超参数：'
- en: '**activation** can have one of four values: **''tanh''**, **''relu''**, **''logistic''**,
    or **''identity''**. This can be achieved by representing it as a float number
    in the range of [0, 3.99]. To transform the float value into one of the aforementioned
    values, we need to apply the **floor()** function to it, which will yield either
    0, 1, 2, or 3\. We then replace a value of 0 with **''tanh''**, a value of 1 with
    **''relu''**, a value of 2 with **''logistic''**, and a value of 3 with **''identity''**.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**activation**可以有四个值之一：**''tanh''**、**''relu''**、**''logistic''**或**''identity''**。可以通过将其表示为[0,
    3.99]范围内的浮点数来实现。为了将浮点值转换为上述值之一，我们需要对其应用**floor()**函数，这将得到0、1、2或3。然后，我们将0替换为**''tanh''**，将1替换为**''relu''**，将2替换为**''logistic''**，将3替换为**''identity''**。'
- en: '**solver** can have one of three values: **''sgd''**, **''adam''**, or **''lbfgs''**.
    Just as with the activation parameter, it can be represented using a float number
    in the range of [0, 2.99].'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**solver**可以有三个值之一：**''sgd''**、**''adam''**或**''lbfgs''**。与激活参数一样，它可以使用[0,
    2.99]范围内的浮点数表示。'
- en: '**alpha** is already a float, so no conversion is needed. It will be bound
    to the range of [0.0001, 2.0].'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**alpha**已经是一个浮点数，因此无需转换。它将被限制在[0.0001, 2.0]的范围内。'
- en: '**learning_rate** can have one of three values: **''constant''**, **''invscaling''**,
    or **''adaptive''**. Once again, we can use a float number in the range of [0,
    2.99] to represent its value.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**learning_rate**可以有三个值之一：**''constant''**、**''invscaling''**或**''adaptive''**。同样，我们可以使用[0,
    2.99]范围内的浮点数来表示其值。'
- en: Evaluating the classifier’s accuracy
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估分类器的准确性
- en: 'The class that will be used to evaluate the MLP classifier’s accuracy for the
    given combination of hidden layers and hyperparameters is called `MlpHyperparametersTest`
    and is contained in the `mlp_hyperparameters_test.py` file, which is located at
    the following link:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 用于评估给定隐藏层和超参数组合的MLP分类器准确性的类叫做`MlpHyperparametersTest`，并包含在`mlp_hyperparameters_test.py`文件中，该文件位于以下链接：
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_hyperparameters_test.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_hyperparameters_test.py)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_hyperparameters_test.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_hyperparameters_test.py)'
- en: 'This class is based on the one we used to optimize the configuration of the
    hidden layers, `MlpLayersTest`, but with a few modifications. Let’s go over these:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类基于我们用于优化隐藏层配置的类`MlpLayersTest`，但做了一些修改。我们来看看这些修改：
- en: 'The **convertParam()** method now handles a **params** list, where the first
    four entries (**params[0]** through **params[3]**) represent the sizes of the
    hidden layers, just as before, but in addition, **params[4]** through **params[7]**
    represent the four hyperparameters we added to the evaluation. Consequently, the
    method has been augmented with the following lines of code, allowing it to transform
    the rest of the given parameters (**params[4]** through **params[7]**) into their
    corresponding values, which can then be fed to the MLP classifier:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**convertParam()**方法现在处理一个**params**列表，其中前四个条目（**params[0]**到**params[3]**）代表隐藏层的大小，和之前一样，但另外，**params[4]**到**params[7]**代表我们为评估添加的四个超参数。因此，方法已通过以下代码行进行了扩展，允许它将其余给定的参数（**params[4]**到**params[7]**）转换为相应的值，然后可以传递给MLP分类器：'
- en: '[PRE14]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Similarly, the **getAccuracy()** method now handles the augmented **params**
    list. It configures the MLP classifier with the converted values of all these
    parameters rather than just the hidden layer’s configuration:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，**getAccuracy()**方法现在处理扩展后的**params**列表。它使用所有这些参数的转换值来配置MLP分类器，而不是仅仅配置隐藏层的设置：
- en: '[PRE15]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This `MlpHyperparametersTest` class is utilized by the genetic algorithm-based
    optimizer. We will look at this in the next section.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`MlpHyperparametersTest`类被基于遗传算法的优化器使用。我们将在下一节中讨论这个内容。
- en: Optimizing the MLP’s combined configuration using genetic algorithms
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用遗传算法优化MLP的组合配置
- en: 'The genetic algorithm-based search for the best combination of hidden layers
    and hyperparameters is implemented by the `02_ptimize_mlp_hyperparameters.py`
    Python program, which is located at the following link:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 基于遗传算法的最佳隐藏层和超参数组合搜索由`02_ptimize_mlp_hyperparameters.py` Python程序实现，该程序位于以下链接：
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/02_optimize_mlp_hyperparameters.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/02_optimize_mlp_hyperparameters.py)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/02_optimize_mlp_hyperparameters.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/02_optimize_mlp_hyperparameters.py)'
- en: 'Thanks to the unified floating number representation that’s used for all parameters,
    this program is almost identical to the one we used in the previous section to
    optimize the network architecture. The main difference is in the definition of
    the `BOUNDS_LOW` and `BOUNDS_HIGH` lists, which contain the ranges of the parameters.
    To the four ranges we defined previously—one for each hidden layer—we now add
    another four, representing the additional hyperparameters that we discussed earlier
    in this section:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有参数都使用统一的浮点数表示，这个程序与我们在前一节中用来优化网络架构的程序几乎相同。主要的区别在于`BOUNDS_LOW`和`BOUNDS_HIGH`列表的定义，它们包含了参数的范围。除了之前定义的四个范围（每个隐藏层一个），我们现在添加了另外四个范围，代表我们在本节中讨论的额外超参数：
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: And that’s all it takes—the program is able to handle the added parameters without
    any further changes.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单——程序能够处理新增的参数而无需进一步修改。
- en: 'Running this program produces the following outcome:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此程序将产生以下结果：
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Important note
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Please be aware that, due to variations between operating systems, the results
    that will be produced when you run this program on your system may be somewhat
    different from what’s being shown here.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于操作系统之间的差异，当你在自己的系统上运行该程序时，可能会得到与此处展示的结果略有不同的输出。
- en: 'The preceding results indicate that, within the ranges we defined, the best
    combination that we found for the hidden layer configuration and hyperparameters
    was as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 前述结果表明，在我们定义的范围内，找到的最佳组合如下：
- en: Three hidden layers of size 7, 4, and 6, respectively.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个隐藏层，分别为7、4和6个节点。
- en: An **activation** parameter of the **'tanh'** type—instead of the default **'relu'**
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**''tanh''**类型的**激活函数**参数——而不是默认的**''relu''**'
- en: A **solver** parameter of the **'lbfgs'** type—rather than the default **'adam'**
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**''lbfgs''**类型的**求解器**参数——而不是默认的**''adam''**'
- en: An **alpha** value of about **1.279** – considerably larger than the default
    value of 0.0001
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**alpha**值约为**1.279**——比默认值0.0001大得多'
- en: A **learning_rate** parameter of the **'constant'** type—the same as the default
    value
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**''constant''**类型的**learning_rate**参数——与默认值相同'
- en: This combined optimization resulted in a classification accuracy of about 94.7%—a
    significant improvement over the previous results, all while using fewer nodes
    than before.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种联合优化最终达到了约94.7%的分类准确率——比之前的结果有了显著提升，而且使用的节点比之前更少。
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you were introduced to the basic concepts of ANNs and DL. After
    getting acquainted with the Iris dataset and the MLP classifier, you were presented
    with the notion of network architecture optimization. Next, we demonstrated a
    genetic algorithm-based optimization of network architecture for the MLP classifier.
    Finally, we were able to combine network architecture optimization with model
    hyperparameter tuning using the same genetic algorithms approach, thereby enhancing
    the performance of the classifier even further.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了人工神经网络（ANN）和深度学习（DL）的基本概念。熟悉了Iris数据集和MLP分类器后，我们介绍了网络架构优化的概念。接下来，我们演示了基于遗传算法的MLP分类器网络架构优化。最后，我们能够将网络架构优化与模型超参数调优结合起来，使用相同的遗传算法方法，从而进一步提升分类器的性能。
- en: So far, we have concentrated on **supervised learning** (**SL**). In the next
    chapter, we will look into applying genetic algorithms to **reinforcement learning**
    (**RL**), an exciting and fast-developing branch of ML.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们集中讨论了**监督学习**（**SL**）。在下一章中，我们将探讨将遗传算法应用于**强化学习**（**RL**），这是一个令人兴奋且快速发展的机器学习分支。
- en: Further reading
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'For more information on the topics that we covered in this chapter, please
    refer to the following resources:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解本章内容的更多信息，请参考以下资源：
- en: '*Python Deep Learning—Second Edition*, *Gianmario Spacagna, Daniel Slater,
    et al.*, *January* *16, 2019*'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python深度学习——第二版*, *Gianmario Spacagna, Daniel Slater, 等*, *2019年1月16日*'
- en: '*Neural Network Projects with Python,* *James Loy*, *February* *28, 2019*'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用Python的神经网络项目*, *James Loy*, *2019年2月28日*'
- en: '**scikit-learn** MLP classifier:'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scikit-learn** MLP 分类器：'
- en: '[https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)'
- en: '*UCI Machine Learning* *Repository*: [https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*UCI 机器学习* *数据集库*：[https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)'
