- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Population Genetics
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 群体遗传学
- en: Population genetics is the study of the changes in the frequency of alleles
    in a population on the basis of selection, drift, mutation, and migration. The
    previous chapters focused mainly on data processing and cleanup; this is the first
    chapter in which we will actually infer interesting biological results.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 群体遗传学是研究群体中等位基因频率变化的学科，这些变化基于选择、漂变、突变和迁徙。前几章主要集中在数据处理和清理上；这是我们第一次真正推断有趣的生物学结果。
- en: There is a lot of interesting population genetics analysis based on sequence
    data, but as we already have quite a few recipes for dealing with sequence data,
    we will divert our attention elsewhere. Also, we will not cover genomic structural
    variations such as **Copy Number Variations** (**CNVs**) or inversions here. We
    will concentrate on analyzing SNP data, which is one of the most common data types.
    We will perform many standard population genetic analyses with Python, such as
    using the **Fixation Index** (**FST**) with computing F-statistics, **Principal
    Components Analysis** (**PCA**), and studying population structure.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 基于序列数据的群体遗传学分析有很多有趣的内容，但由于我们已经有了不少处理序列数据的操作，我们将把注意力转向其他地方。此外，我们不会在此讨论基因组结构变异，如**拷贝数变异**（**CNVs**）或倒位。我们将集中分析SNP数据，这是最常见的数据类型之一。我们将使用Python执行许多标准的群体遗传学分析，例如使用**固定指数**（**FST**）计算F统计量、**主成分分析**（**PCA**），并研究群体结构。
- en: We will use Python mostly as a scripting language that glues together applications
    that perform necessary computations, which is the old-fashioned way of doing things.
    Having said that, as the Python software ecology is still evolving, you can at
    least perform the PCA in Python using scikit-learn as we will see in [*Chapter
    11*](B17942_11.xhtml#_idTextAnchor272).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要使用Python作为脚本语言，来将执行必要计算的应用程序连接在一起，这是一种传统的做法。话虽如此，由于Python软件生态系统仍在不断发展，你至少可以使用scikit-learn在Python中执行PCA，如我们在[*第11章*](B17942_11.xhtml#_idTextAnchor272)中将看到的那样。
- en: There is no such thing as a default file format for population genetics data.
    The bleak reality of this field is that there is a plenitude of formats, most
    of them developed with a specific application in mind; therefore, none are generically
    applicable. Some of the efforts to create a more general format (or even just
    a file converter to support many formats) had limited success. Furthermore, as
    our knowledge of genomics increases, we will require new formats anyway (for example,
    to support some kind of previously unknown genomic structural variation). Here,
    we will work with PLINK ([https://www.cog-genomics.org/plink/2.0/](https://www.cog-genomics.org/plink/2.0/)),
    which was originally developed to perform **Genome-Wide Association Studies**
    (**GWAS**) with human data but has many more applications. If you have **Next-Generation
    Sequencing** (**NGS**) data, you may question, why not use the **Variant Call
    Format** (**VCF**)? Well, a VCF file is normally annotated to help with sequencing
    analysis, which you do not need at this stage (you should now have a filtered
    dataset). If you convert your **Single-Nucleotide Polymorphism** (**SNP**) calls
    from VCF to PLINK, you will get roughly a 95 percent reduction in terms of size
    (this is in comparison to a compressed VCF). More importantly, the computational
    cost of processing a VCF file is much bigger (think of processing all this highly
    structured text) than the cost of the other two formats. If you use Docker, use
    the image tiagoantao/bioinformatics_popgen.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在群体遗传学数据中并没有所谓的默认文件格式。这个领域的严峻现实是，存在大量的文件格式，其中大多数是为特定应用而开发的；因此，没有任何一种是通用的。尽管有一些尝试创建更通用的格式（或者只是开发一个支持多种格式的文件转换器），但这些努力的成功是有限的。更重要的是，随着我们对基因组学理解的不断深入，我们将无论如何需要新的格式（例如，支持某种之前未知的基因组结构变异）。在这里，我们将使用PLINK（[https://www.cog-genomics.org/plink/2.0/](https://www.cog-genomics.org/plink/2.0/)），该工具最初是为在人类数据上执行**全基因组关联研究**（**GWAS**）而开发的，但它有更多的应用。如果你有**下一代测序**（**NGS**）数据，可能会问，为什么不使用**变异调用格式**（**VCF**）？嗯，VCF文件通常会进行注释，以帮助测序分析，而在这个阶段你并不需要这些（此时你应该已经拥有一个经过过滤的数据集）。如果你将**单核苷酸多态性**（**SNP**）的调用从VCF转换为PLINK，你会大约节省95%的存储空间（这是与压缩后的VCF相比的结果）。更重要的是，处理VCF文件的计算成本要远高于其他两种格式的处理成本（想象一下处理所有这些高度结构化的文本）。如果你使用Docker，请使用镜像tiagoantao/bioinformatics_popgen。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将覆盖以下内容：
- en: Managing datasets with PLINK
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PLINK管理数据集
- en: Using sgkit for population genetics analysis with xarray
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用sgkit和xarray进行群体遗传学分析
- en: Exploring a dataset with sgkit
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用sgkit探索数据集
- en: Analyzing population structure
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析人口结构
- en: Performing a PCA
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行PCA分析
- en: Investigating population structure with admixture
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过混合分析调查人口结构
- en: First, let’s start with a discussion on file format issues and then continue
    to discuss interesting data analysis.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从文件格式问题的讨论开始，然后继续讨论有趣的数据分析。
- en: Managing datasets with PLINK
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PLINK管理数据集
- en: Here, we will manage our dataset using PLINK. We will create subsets of our
    main dataset (from the HapMap project) that are suitable for analysis in the following
    recipes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用PLINK来管理我们的数据集。我们将从HapMap项目中的主数据集中创建适合以下食谱分析的子集。
- en: Warning
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Note that neither PLINK nor any similar programs were developed for their file
    formats. There was probably no objective to create a default file standard for
    population genetics data. In this field, you will need to be ready to convert
    from format to format (for this, Python is quite appropriate) because every application
    that you will use will probably have its own quirky requirements. The most important
    point to learn from this recipe is that it’s not formats that are being used,
    although these are relevant, but a ‘file conversion mentality’. Beyond this, some
    of the steps in this recipe also convey genuine analytical techniques that you
    may want to consider using, for example, subsampling or **Linkage Disequilibrium-**
    (**LD-**) pruning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，PLINK及其他类似程序并不是为了他们的文件格式而开发的。创建人口遗传学数据的默认文件标准可能并不是一个目标。在这个领域，你需要做好格式转换的准备（为此，Python非常适合），因为你将使用的每个应用程序可能都有自己的独特需求。从这个食谱中你要学到的最重要的一点是，使用的不是文件格式（尽管这些是相关的），而是一种“文件转换思维”。除此之外，本食谱中的一些步骤还传达了你可能希望使用的真正的分析技巧，例如，子抽样或**连锁不平衡**（**LD-**）修剪。
- en: Getting ready
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Throughout this chapter, we will use data from the International HapMap Project.
    You may recall that we used data from the 1,000 Genomes Project in [*Chapter 3*](B17942_03.xhtml#_idTextAnchor068),
    *Next-Generation Sequencing*, and that the HapMap project is in many ways the
    precursor to the 1,000 Genomes Project; instead of whole genome sequencing, genotyping
    was used. Most of the samples of the HapMap project were used in the 1,000 Genomes
    Project, so if you have read the recipes in [*Chapter 3*](B17942_03.xhtml#_idTextAnchor068),
    *Next-Generation Sequencing*, you will already have an idea of the dataset (including
    the available population). I will not introduce the dataset much more, but you
    can refer to [*Chapter 3*](B17942_03.xhtml#_idTextAnchor068), *Next-Generation
    Sequencing*, and, of course, the HapMap site ([https://www.genome.gov/10001688/international-hapmap-project](https://www.genome.gov/10001688/international-hapmap-project))
    for more information. Remember that we have genotyping data for many individuals
    split across populations around the globe. We will refer to these populations
    by their acronyms. Here is the list taken from [http://www.sanger.ac.uk/resources/downloads/human/hapmap3.xhtml](http://www.sanger.ac.uk/resources/downloads/human/hapmap3.xhtml):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用国际HapMap项目的数据。你可能记得我们在[*第3章*](B17942_03.xhtml#_idTextAnchor068)中使用了1,000基因组项目的数据，*下一代测序*，而HapMap项目在许多方面是1,000基因组项目的前身；它使用的是基因分型，而非全基因组测序。HapMap项目的大多数样本都用于1,000基因组项目，因此如果你已经阅读了[*第3章*](B17942_03.xhtml#_idTextAnchor068)中的内容，*下一代测序*，你就已经对该数据集（包括可用的人群）有所了解。我不会再对数据集做更多介绍，但你可以参考[*第3章*](B17942_03.xhtml#_idTextAnchor068)中的内容，*下一代测序*，以及HapMap官网([https://www.genome.gov/10001688/international-hapmap-project](https://www.genome.gov/10001688/international-hapmap-project))获取更多信息。请记住，我们有来自世界各地不同人群的基因分型数据。我们将按人群的缩写来引用这些人群。以下是从[http://www.sanger.ac.uk/resources/downloads/human/hapmap3.xhtml](http://www.sanger.ac.uk/resources/downloads/human/hapmap3.xhtml)获取的列表：
- en: '| **Acronym** | **Population** |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **缩写** | **人群** |'
- en: '| ASW | African ancestry in Southwest USA |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| ASW | 美国西南部的非洲血统人群 |'
- en: '| CEU | Utah residents with Northern and Western European ancestry from the
    CEPH collection |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| CEU | 来自CEPH收藏的北欧和西欧血统的犹他州居民 |'
- en: '| CHB | Han Chinese in Beijing, China |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| CHB | 中国北京的汉族人 |'
- en: '| CHD | Chinese in Metropolitan Denver, Colorado |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| CHD | 科罗拉多州丹佛市的华裔居民 |'
- en: '| GIH | Gujarati Indians in Houston, Texas |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| GIH | 德克萨斯州休斯顿的古吉拉特印度人 |'
- en: '| JPT | Japanese in Tokyo, Japan |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| JPT | 日本东京的日本人 |'
- en: '| LWK | Luhya in Webuye, Kenya |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| LWK | 肯尼亚韦布耶的卢希亚人 |'
- en: '| MXL | Mexican ancestry in Los Angeles, California |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| MXL | 加利福尼亚州洛杉矶的墨西哥裔人 |'
- en: '| MKK | Maasai in Kinyawa, Kenya |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| MKK | 肯尼亚金亚瓦的马赛人 |'
- en: '| TSI | Toscani in Italy |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| TSI | 意大利托斯卡纳地区的人 |'
- en: '| YRI | Yoruba in Ibadan, Nigeria |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| YRI | 尼日利亚伊巴丹的约鲁巴人 |'
- en: Table 6.1 - The populations in the Genome Project
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 - 基因组计划中的群体
- en: Note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will be using data from the HapMap project that has, in practice, been replaced
    by the 1,000 Genomes Project. For the purpose of teaching population genetics
    programming techniques in Python, the HapMap Project dataset is more manageable
    than the 1,000 Genomes Project, as the data is considerably smaller. The HapMap
    samples are a subset of the 1,000 Genomes samples. If you do research in human
    population genetics, you are strongly advised to use the 1,000 Genomes Project
    as a base dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用HapMap项目的数据，该项目实际上已被1,000基因组计划所取代。为了教学目的，教授Python中的群体遗传学编程技术，HapMap项目的数据比1,000基因组项目更易于处理，因为数据要小得多。HapMap样本是1,000基因组样本的子集。如果你从事人类群体遗传学研究，强烈建议使用1,000基因组计划作为基础数据集。
- en: This will require a fairly big download (approximately 1 GB), which will have
    to be uncompressed. Make sure that you have approximately 20 GB of disk space
    for this chapter. The files can be found at [https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3/plink_format/](https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3/plink_format/).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要一个相当大的下载（大约1GB），并且需要解压。确保你有大约20GB的磁盘空间用于本章。文件可以在[https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3/plink_format/](https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3/plink_format/)找到。
- en: 'Decompress the PLINK file using the following commands:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令解压PLINK文件：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, we have PLINK files; the MAP file has information on the marker position
    across the genome, whereas the PED file has actual markers for each individual,
    along with some pedigree information. We also downloaded a metadata file that
    contains information about each individual. Take a look at all these files and
    familiarize yourself with them. As usual, this is also available in the `Chapter06/Data_Formats.py`
    Notebook file, where everything has been taken care of.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了PLINK文件；MAP文件包含了整个基因组中标记的位置，而PED文件包含了每个个体的实际标记，以及一些家谱信息。我们还下载了一个元数据文件，其中包含了每个个体的信息。查看这些文件并熟悉它们。像往常一样，这些内容也可以在`Chapter06/Data_Formats.py`
    Notebook文件中找到，所有内容都已处理好。
- en: Finally, most of this recipe will make heavy usage of PLINK ([https://www.cog-genomics.org/plink/2.0/](https://www.cog-genomics.org/plink/2.0/)).
    Python will mostly be used as the glue language to call PLINK.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这个教程的大部分内容将会大量使用PLINK（[https://www.cog-genomics.org/plink/2.0/](https://www.cog-genomics.org/plink/2.0/)）。Python主要作为连接语言来调用PLINK。
- en: How to do it...
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Take a look at the following steps:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下步骤：
- en: 'Let’s get the metadata for our samples. We will load the population of each
    sample and note all the individuals that are offspring of others in the dataset:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们获取我们样本的元数据。我们将加载每个样本的人口信息，并记录数据集中所有其他个体的后代：
- en: '[PRE1]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will load a dictionary where the population is the key (`CEU`, `YRI`, and
    so on) and its value is the list of individuals in that population. This dictionary
    will also store information on whether the individual is the offspring of another.
    Each individual is identified by the family and individual ID (information that
    can be found in the PLINK file). The file provided by the HapMap project is a
    simple tab-delimited file, which is not difficult to process. While we are reading
    the files using standard Python text processing, this is a typical example where
    pandas would help.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这将加载一个字典，其中人口是键（`CEU`，`YRI`等），而其值是该人口中个体的列表。该字典还将存储个体是否为其他人的后代信息。每个个体通过家族和个体ID进行标识（这些信息可以在PLINK文件中找到）。HapMap项目提供的文件是一个简单的制表符分隔文件，不难处理。虽然我们使用标准的Python文本处理读取文件，但这是一个典型的例子，pandas会有所帮助。
- en: 'There is an important point to make here: the reason this information is provided
    in a separate, ad hoc file is that the PLINK format makes no provision for the
    population structure (this format makes provision only for the case and control
    information for which PLINK was designed). This is not a flaw of the format, as
    it was never designed to support standard population genetic studies (it’s a GWAS
    tool). However, this is a general feature of data formats in population genetics:
    whichever you end up working with, there will be something important missing.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个重要的点：之所以将此信息提供在一个单独的临时文件中，是因为 PLINK 格式没有考虑到种群结构（该格式仅为 PLINK 设计时所用的病例与对照信息提供了支持）。这并不是格式的缺陷，因为它从未被设计用来支持标准的种群遗传学研究（它是一个
    GWAS 工具）。然而，这是种群遗传学中数据格式的普遍特征：无论你最终使用什么格式，都会有所缺失。
- en: We will use this metadata in other recipes in this chapter. We will also perform
    some consistency analysis between the metadata and the PLINK file, but we will
    defer this to the next recipe.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的其他例子中使用这些元数据。我们还将进行元数据与 PLINK 文件之间的一致性分析，但我们将把这部分推迟到下一个例子。
- en: 'Now, let’s subsample the dataset at 10 percent and 1 percent of the number
    of markers, as follows:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们以 10% 和 1% 的标记数对数据集进行子抽样，具体如下：
- en: '[PRE2]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With Jupyter Notebook, you can just do this instead:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 中，你只需要这样做：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note the subtlety that you will not really get 1 or 10 percent of the data;
    each marker will have a 1 or 10 percent chance of being selected, so you will
    get approximately 1 or 10 percent of the markers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意微妙之处，你实际上并不会得到 1% 或 10% 的数据；每个标记有 1% 或 10% 的机会被选中，因此你将得到大约 1% 或 10% 的标记。
- en: Obviously, as the process is random, different runs will produce different marker
    subsets. This will have important implications further down the road. If you want
    to replicate the exact same result, you can nonetheless use the `--seed` option.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，由于过程是随机的，不同的运行会产生不同的标记子集。这将在后续的分析中产生重要的影响。如果你想复现完全相同的结果，你仍然可以使用 `--seed`
    选项。
- en: We will also remove all SNPs that have a genotyping rate lower than 90 percent
    (with the `--geno 0.1` parameter).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将去除所有基因型率低于 90% 的 SNP（使用 `--geno 0.1` 参数）。
- en: Note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There is nothing special about Python in this code, but there are two reasons
    you may want to subsample your data. First, if you are performing an exploratory
    analysis of your own dataset, you may want to start with a smaller version because
    it will be easy to process. Also, you will have a broader view of your data. Second,
    some analytical methods may not require all your data (indeed, some methods might
    not be even able to use all of your data). Be very careful with the last point
    though; that is, for every method that you use to analyze your data, be sure that
    you understand the data requirements for the scientific questions you want to
    answer. Feeding too much data may be okay normally (even if you pay a time and
    memory penalty) but feeding too little will lead to unreliable results.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与 Python 本身没有什么特殊之处，但你可能有两个原因希望对数据进行子抽样。首先，如果你正在对自己的数据集进行探索性分析，你可能希望从一个较小的版本开始，因为这样更易处理。而且，你将能更全面地查看你的数据。第二，一些分析方法可能不需要全部数据（实际上，一些方法甚至可能无法使用全部数据）。不过，对于最后一点要非常小心；也就是说，对于每种分析方法，确保你理解要回答的科学问题对数据的要求。通常提供过多的数据是可以的（即便你需要支付时间和内存的代价），但提供过少的数据将导致不可靠的结果。
- en: 'Now, let’s generate subsets with just the autosomes (that is, let’s remove
    the sex chromosomes and mitochondria), as follows:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们只使用常染色体生成子集（也就是说，我们将去除性染色体和线粒体），具体如下：
- en: '[PRE4]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s create a function that generates a list with all the SNPs not belonging
    to autosomes. With human data, that means all non-numeric chromosomes. If you
    use another species, be careful with your chromosome coding because PLINK is geared
    toward human data. If your species are diploid, have less than 23 autosomes, and
    a sex determination system, that is, X/Y, this will be straightforward; if not,
    refer to [https://www.cog-genomics.org/plink2/input#allow_extra_chr](https://www.cog-genomics.org/plink2/input#allow_extra_chr)
    for some alternatives (such as the `--allow-extra-chr` flag).
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个函数，生成一个包含所有不属于常染色体的 SNP 列表。对于人类数据，这意味着所有非数字染色体。如果你使用的是其他物种，要小心染色体编码，因为
    PLINK 是面向人类数据的。如果你的物种是二倍体，具有少于 23 条常染色体，并且有性别决定系统，即 X/Y，这将很简单；如果不是，请参考 [https://www.cog-genomics.org/plink2/input#allow_extra_chr](https://www.cog-genomics.org/plink2/input#allow_extra_chr)
    以获取一些替代方案（例如 `--allow-extra-chr` 标志）。
- en: We then create autosome-only PLINK files for subsample datasets of 10 and 1
    percent (prefixed as `hapmap10_auto` and `hapmap1_auto`).
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们为10%和1%的子样本数据集创建仅包含常染色体的PLINK文件（分别以`hapmap10_auto`和`hapmap1_auto`为前缀）。
- en: 'Let’s create some datasets without offspring. These will be needed for most
    population genetic analysis, which requires unrelated individuals to a certain
    degree:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一些没有后代的数据集。这些数据集将用于大多数种群遗传学分析，这些分析要求个体之间在一定程度上没有亲缘关系：
- en: '[PRE5]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This step is representative of the fact that most population genetic analyses
    require samples to be unrelated to a certain degree. Obviously, as we know that
    some offspring are in HapMap, we remove them.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步代表了大多数种群遗传学分析的事实，即这些分析要求样本之间有一定程度的非亲缘关系。显然，由于我们知道一些后代存在于HapMap中，因此我们需要将其去除。
- en: However, note that with your dataset, you are expected to be much more refined
    than this. For instance, run `plink --genome` or use another program to detect
    related individuals. The fundamental point here is that you have to dedicate some
    effort to detect related individuals in your samples; this is not a trivial task.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，使用你的数据集时，你需要比这更加精细。例如，运行`plink --genome`或者使用其他程序来检测相关个体。这里的关键是，你必须花费一些精力去检测样本中的相关个体；这并不是一项微不足道的任务。
- en: 'We will also generate an LD-pruned dataset, as required by many PCA and admixture
    algorithms, as follows:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将生成一个LD修剪后的数据集，这是许多PCA和混合分析算法所要求的，具体如下：
- en: '[PRE6]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first step generates a list of markers to be kept if the dataset is LD-pruned.
    This uses a sliding window of `50` SNPs, advancing by `10` SNPs at a time with
    a cut value of `0.1`. The second step extracts SNPs from the list that was generated
    earlier.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步生成一个标记列表，供数据集进行LD修剪时使用。这使用一个`50`个SNP的滑动窗口，每次推进`10`个SNP，切割值为`0.1`。第二步从之前生成的列表中提取SNP。
- en: 'Let’s recode a couple of cases in different formats:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将几个案例以不同格式重新编码：
- en: '[PRE7]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The first operation will convert a PLINK format that uses nucleotide letters
    from the ACTG to another, which recodes alleles with `1` and `2`. We will use
    this in the *Performing a PCA* recipe later.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个操作将把一个使用ACTG核苷酸字母的PLINK格式转换为另一种格式，这种格式将等位基因重新编码为`1`和`2`。我们将在*执行PCA*的配方中稍后使用这个。
- en: The second operation recodes a file in a binary format. If you work inside PLINK
    (using the many useful operations that PLINK has), the binary format is probably
    the most appropriate format (offering, for example, a smaller file size). We will
    use this in the admixture recipe.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个操作将文件重新编码为二进制格式。如果你在PLINK中工作（使用PLINK提供的许多有用操作），二进制格式可能是最合适的格式（例如，提供更小的文件大小）。我们将在混合分析配方中使用此格式。
- en: 'We will also extract a single chromosome (`2`) for analysis. We will start
    with the autosome dataset, which has been subsampled at 10 percent:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将提取单一的染色体（`2`）进行分析。我们将从10%子样本的常染色体数据集开始：
- en: '[PRE8]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There’s more...
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: There are many reasons why you might want to create different datasets for analysis.
    You may want to perform some fast initial exploration of data – for example, if
    the analysis algorithm that you plan to use has some data format requirements
    or a constraint on the input, such as the number of markers or relationships between
    individuals. Chances are that you will have lots of subsets to analyze (unless
    your dataset is very small to start with, for instance, a microsatellite dataset).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能有许多理由想要创建不同的数据集进行分析。你可能想对数据进行一些快速的初步探索——例如，如果你计划使用的分析算法对数据格式有要求，或对输入有某些约束，例如标记数量或个体之间的关系。很可能你会有许多子集需要分析（除非你的数据集本身就非常小，例如微卫星数据集）。
- en: 'This may seem to be a minor point, but it’s not: be very careful with file
    naming (note that I have followed some simple conventions while generating filenames).
    Make sure that the name of the file gives some information about the subset options.
    When you perform the downstream analysis, you will want to be sure that you choose
    the correct dataset; you will want your dataset management to be agile and reliable,
    above all. The worst thing that can happen is that you create an analysis with
    an erroneous dataset that does not obey the constraints required by the software.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来是一个小细节，但实际上并非如此：在命名文件时要非常小心（请注意，我在生成文件名时遵循了一些简单的约定）。确保文件名能提供有关子集选项的一些信息。在进行后续分析时，你需要确保选择正确的数据集；你希望你的数据集管理是灵活且可靠的，最重要的是。最糟糕的情况是，你创建了一个包含错误数据集的分析，而这个数据集不符合软件要求的约束条件。
- en: The LD-pruning that we used is somewhat standard for human analysis, but be
    sure to check the parameters, especially if you are using non-human data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的 LD 剪枝对于人类分析来说是标准的，但如果你使用非人类数据，请务必检查参数。
- en: The HapMap file that we downloaded is based on an old version of the reference
    genome (build 36). As stated in the previous chapter, [*Chapter 5*](B17942_05.xhtml#_idTextAnchor122),
    *Working with Genomes*, be sure to use annotations from build 36 if you plan to
    use this file for more analysis of your own.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下载的 HapMap 文件是基于旧版参考基因组（构建 36）。如前一章 [*第 5 章*](B17942_05.xhtml#_idTextAnchor122)
    所述，*与基因组合作*，如果你计划使用此文件进行更多分析，请确保使用构建 36 的注释。
- en: This recipe sets the stage for the following recipes and its results will be
    used extensively.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例为接下来的示例做了准备，其结果将被广泛使用。
- en: See also
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: The Wikipedia page [http://en.wikipedia.org/wiki/Linkage_disequilibrium](http://en.wikipedia.org/wiki/Linkage_disequilibrium)
    on LD is a good place to start.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科关于 LD 的页面 [http://en.wikipedia.org/wiki/Linkage_disequilibrium](http://en.wikipedia.org/wiki/Linkage_disequilibrium)
    是一个很好的起点。
- en: The website of PLINK [https://www.cog-genomics.org/plink/2.0/](https://www.cog-genomics.org/plink/2.0/)
    is very well documented, something lacking in much of genetics software.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PLINK 网站 [https://www.cog-genomics.org/plink/2.0/](https://www.cog-genomics.org/plink/2.0/)
    有非常详细的文档，这是许多遗传学软件所缺乏的。
- en: Using sgkit for population genetics analysis with xarray
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 sgkit 进行群体遗传学分析与 xarray
- en: Sgkit is the most advanced Python library for doing population genetics analysis.
    It’s a modern implementation, leveraging almost all of the fundamental data science
    libraries in Python. When I say almost all, I am not exaggerating; it uses NumPy,
    pandas, xarray, Zarr, and Dask. NumPy and pandas were introduced in [*Chapter
    2*](B17942_02.xhtml#_idTextAnchor040). Here, we will introduce xarray as the main
    data container for sgkit. Because I feel that I cannot ask you to get to know
    data engineering libraries to an extreme level, I will gloss over the Dask part
    (mostly by treating Dask structures as equivalent NumPy structures). You can find
    more advanced details about out-of-memory Dask data structures in [*Chapter 11*](B17942_11.xhtml#_idTextAnchor272).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Sgkit 是进行群体遗传学分析的最先进的 Python 库。它是一个现代实现，利用了几乎所有 Python 中的基础数据科学库。当我说几乎所有时，我并没有夸张；它使用了
    NumPy、pandas、xarray、Zarr 和 Dask。NumPy 和 pandas 在 [*第 2 章*](B17942_02.xhtml#_idTextAnchor040)
    中已介绍。在这里，我们将介绍 xarray 作为 sgkit 的主要数据容器。因为我觉得不能要求你对数据工程库有极为深入的了解，所以我会略过 Dask 部分（主要是将
    Dask 结构当作等价的 NumPy 结构来处理）。你可以在 [*第 11 章*](B17942_11.xhtml#_idTextAnchor272) 中找到有关超大内存
    Dask 数据结构的更详细信息。
- en: Getting ready
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You will need to run the previous recipe because its output is required for
    this one: we will be using one of the PLINK datasets. You will need to install
    sgkit.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要运行之前的示例，因为它的输出是本示例所需的：我们将使用其中一个 PLINK 数据集。你需要安装 sgkit。
- en: As usual, this is available in the `Chapter06/Sgkit.py` Notebook file, but it
    will still require you to run the previous Notebook file in order to generate
    the required files.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，这可以在 `Chapter06/Sgkit.py` 笔记本文件中找到，但你仍然需要运行之前的笔记本文件以生成所需的文件。
- en: How to do it...
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Take a look at the following steps:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下步骤：
- en: 'Let’s load the `hapmap10_auto_noofs_ld` dataset generated in the previous recipe:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载在之前的示例中生成的 `hapmap10_auto_noofs_ld` 数据集：
- en: '[PRE9]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Remember that we are loading a set of PLINK files. It turns out that sgkit creates
    a very rich and structured representation for that data. That representation is
    based on an xarray dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们正在加载一组 PLINK 文件。事实证明，sgkit 为该数据创建了非常丰富且结构化的表示。这种表示基于 xarray 数据集。
- en: 'Let’s check the structure of our data – if you are in a notebook, just enter
    the following:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一下数据的结构 —— 如果你在笔记本中，只需输入以下内容：
- en: '[PRE10]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`sgkit` – if in a notebook – will generate the following representation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`sgkit` – 如果在笔记本中 – 将生成以下表示：'
- en: '![Figure 6.1 - An overview of the xarray data loaded by sgkit for our PLINK
    file ](img/B17942_06_1.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 - sgkit 加载的 xarray 数据概览，适用于我们的 PLINK 文件](img/B17942_06_1.jpg)'
- en: Figure 6.1 - An overview of the xarray data loaded by sgkit for our PLINK file
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 - sgkit 加载的 xarray 数据概览，适用于我们的 PLINK 文件
- en: '`data` is an xarray DataSet. An xarray DataSet is essentially a dictionary
    in which each value is a Dask array. For our purposes, you can assume it is a
    NumPy array. In this case, we can see that we have **56241** variants for **1198**
    samples. We have **2** alleles per variant and a ploidy of **2**.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`data` 是一个 xarray 数据集。xarray 数据集本质上是一个字典，其中每个值都是一个 Dask 数组。就我们而言，可以假设它是一个 NumPy
    数组。在这种情况下，我们可以看到我们有 **56241** 个变异，涵盖了 **1198** 个样本。每个变异有 **2** 个等位基因，且倍性为 **2**。'
- en: In the notebook, we can expand each entry. In our case, we expanded `call_genotype`.
    This is a three-dimensional array, with `variants`, `samples`, and `ploidy` dimensions.
    The type of the array is `int8`. After this, we can find some metadata relevant
    to the entry, `mixed_ploidy`, and comment. Finally, you have a summary of the
    Dask implementation. The **Array** column presents details about the size and
    shape of the array. For the **Chunk** column, see [*Chapter 11*](B17942_11.xhtml#_idTextAnchor272)
    – but you can safely ignore it for now.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中，我们可以展开每一项。在我们的案例中，我们展开了 `call_genotype`。这是一个三维数组，包含 `variants`、`samples`
    和 `ploidy` 维度。该数组的类型是 `int8`。接下来，我们可以找到一些与条目相关的元数据，如 `mixed_ploidy` 和注释。最后，你将看到
    Dask 实现的摘要。**Array** 列展示了数组的大小和形状的详细信息。对于 **Chunk** 列，请参阅 [*第11章*](B17942_11.xhtml#_idTextAnchor272)——但现在可以安全忽略它。
- en: 'Another way to get summary information, which is especially useful if you are
    not using notebooks, is by inspecting the `dims` field:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取摘要信息的另一种方法，尤其在你没有使用笔记本时，便是检查 `dims` 字段：
- en: '[PRE11]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output should be self-explanatory:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该不言自明：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s extract some information about the samples:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们提取一些关于样本的信息：
- en: '[PRE13]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We have `1198` samples. The first one has a sample ID of `NA19916`, a family
    ID of `2431`, and a sex of `1` (Male). Remember that, given PLINK as the data
    source, a sample ID is not enough to be a primary key (you can have different
    samples with the same sample ID). The primary key is a composite of the sample
    ID and sample family ID.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 `1198` 个样本。第一个样本的样本 ID 是 `NA19916`，家庭 ID 是 `2431`，性别为 `1`（男性）。请记住，考虑到 PLINK
    作为数据源，样本 ID 并不足以作为主键（你可能会有多个样本具有相同的样本 ID）。主键是样本 ID 和样本家庭 ID 的复合键。
- en: TIP
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'You might have noticed that we add `.values` to all the data fields: this is
    actually rendering a lazy Dask array into a materialized NumPy one. For now, I
    suggest that you ignore it, but if you revisit this chapter after reading [*Chapter
    11*](B17942_11.xhtml#_idTextAnchor272), `.values` is akin to the `compute` method
    in Dask.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到我们在所有数据字段后加了 `.values`：这实际上是将一个懒加载的 Dask 数组渲染成一个具象的 NumPy 数组。现在，我建议你忽略它，但如果你在阅读完
    [*第11章*](B17942_11.xhtml#_idTextAnchor272) 后重新回到这一章，`.values` 类似于 Dask 中的 `compute`
    方法。
- en: The `.values` call is no nuisance – the reason our code works is that our dataset
    is small enough to fit into memory, which is great for our teaching example. But
    if you have a very large dataset, the preceding code is too naive. Again, [*Chapter
    11*](B17942_11.xhtml#_idTextAnchor272) will help you with this. For now, the simplicity
    is pedagogical.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`.values` 调用并不麻烦——我们的代码之所以能工作，是因为我们的数据集足够小，可以适应内存，这对于我们的教学示例来说非常好。但如果你有一个非常大的数据集，前面的代码就过于简单了。再次强调，
    [*第11章*](B17942_11.xhtml#_idTextAnchor272) 会帮助你解决这个问题。现在，简单性是为了教学目的。'
- en: 'Before we look at the variant data, we have to be aware of how sgkit stores
    `contigs`:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在查看变异数据之前，我们必须了解 sgkit 如何存储 `contigs`：
- en: '[PRE15]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `contigs` here are the human autosomes (you will not be so lucky if your
    data is based on most other species – you will probably have some ugly identifier
    here).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `contigs` 是人类的常染色体（如果你的数据来自于大多数其他物种，可能就不那么幸运了——你可能会看到一些丑陋的标识符）。
- en: 'Now, let’s look at the variants:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们来看看变异数据：
- en: '[PRE17]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is an abridged version of the output:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出的简化版本：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We have `56241` variants. The `contig` index is `0`, which if you look at the
    step from the previous recipe, is chromosome `1`. The variant is in position `557616`
    (against build 36 of the human genome) and has possible alleles `G` and `A`. It
    has an SNP ID of `rs11510103`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 `56241` 个变异。`contig` 索引是 `0`，如果你查看前一步的步骤，它代表染色体 `1`。变异位于位置 `557616`（参照人类基因组版本
    36），可能的等位基因是 `G` 和 `A`。它有一个 SNP ID，`rs11510103`。
- en: 'Finally, let’s look at the `genotype` data:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们看看 `genotype` 数据：
- en: '[PRE19]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`call_genotype` has a shape of 56,241 x 1,1198,2, which is its dimensioned
    variants, samples, and ploidy.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`call_genotype` 的形状为 56,241 x 1,1198,2，这是它的变异、样本和倍性维度。'
- en: To get all variants for the first individual, you fixate the second dimension.
    To get all the samples for the first variant, you fixate the first dimension.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取第一个个体的所有变异数据，你需要固定第二个维度。要获取第一个变异的所有样本数据，你需要固定第一个维度。
- en: If you print the first individual’s details (sample and family ID), you get
    `2431` and `NA19916` – as expected, exactly as in the first case in the previous
    sample exploration.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打印出第一个个体的详细信息（样本和家庭 ID），你会得到 `2431` 和 `NA19916` ——如预期的那样，正如在上一个样本探索中的第一个案例。
- en: There’s more...
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: This recipe is mostly an introduction to xarray, disguised as a sgkit tutorial.
    There is much more to be said about xarray – be sure to check [https://docs.xarray.dev/](https://docs.xarray.dev/).
    It is worth reiterating that xarray depends on a plethora of Python data science
    libraries and that we are glossing over Dask for now.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱主要是对 xarray 的介绍，伪装成 sgkit 教程。关于 xarray 还有很多内容要说——一定要查看 [https://docs.xarray.dev/](https://docs.xarray.dev/)。值得重申的是，xarray
    依赖于大量的 Python 数据科学库，而我们现在暂时忽略了 Dask。
- en: Exploring a dataset with sgkit
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 sgkit 探索数据集
- en: In this recipe, we will perform an initial exploratory analysis of one of our
    generated datasets. Now that we have some basic knowledge of xarray, we can actually
    try to do some data analysis. In this recipe, we will ignore population structure,
    an issue we will return to in the following one.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将对其中一个生成的数据集进行初步的探索性分析。现在我们对 xarray 有了一些基本了解，可以实际尝试进行一些数据分析。在这个食谱中，我们将忽略群体结构问题，这一问题将在下一个食谱中回到。
- en: Getting ready
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to have run the first recipe and should have the `hapmap10_auto_noofs_ld`
    files available. There is a Notebook file with this recipe called `Chapter06/Exploratory_Analysis.py`.
    You will need the software that you installed for the previous recipe.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要先运行第一个食谱，并确保你有 `hapmap10_auto_noofs_ld` 文件。此食谱中有一个 Notebook 文件，名为 `Chapter06/Exploratory_Analysis.py`。你还需要为上一个食谱安装的软件。
- en: How to do it...
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Take a look at the following steps:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下步骤：
- en: 'We start by loading the PLINK data with sgkit, exactly as in the previous recipe:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先使用 sgkit 加载 PLINK 数据，方法与上一个食谱完全相同：
- en: '[PRE20]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s ask sgkit for `variant_stats`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们请求 sgkit 提供 `variant_stats`：
- en: '[PRE21]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.2 - The variant statistics provided by sgkit’s variant_stats ](img/B17942_06_2.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 - sgkit 提供的变异统计数据](img/B17942_06_2.jpg)'
- en: Figure 6.2 - The variant statistics provided by sgkit’s variant_stats
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 - sgkit 提供的变异统计数据
- en: 'Let’s now look at the statistic, `variant_call_rate`:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来看一下统计数据，`variant_call_rate`：
- en: '[PRE22]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'There is more to unpack here than it may seem. The fundamental part is the
    `to_series()` call. Sgkit is returning a Pandas series to you – remember that
    sgkit is highly integrated with Python data science libraries. After you get the
    Series object, you can call the Pandas `describe` function and get the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有更多需要解释的内容，可能看起来不太明显。关键部分是 `to_series()` 调用。Sgkit 返回给你的是一个 Pandas 序列——记住，sgkit
    与 Python 数据科学库高度集成。获得 Series 对象后，你可以调用 Pandas 的 `describe` 函数并得到以下结果：
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Our variant call rate is quite good, which is not shocking because we are looking
    at array data – you would have worse numbers if you had a dataset based on NGS.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的变异呼叫率相当不错，这并不令人惊讶，因为我们在查看的是阵列数据——如果你有一个基于 NGS 的数据集，数据质量可能会更差。
- en: 'Let’s now look at sample statistics:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们来看一下样本统计数据：
- en: '[PRE24]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Again, sgkit provides a lot of sample statistics out of the box:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，sgkit 提供了许多现成的样本统计数据：
- en: '![Figure 6.3 - The sample statistics obtained by calling sample_stats  ](img/B17942_06_3.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 - 通过调用 sample_stats 获得的样本统计数据](img/B17942_06_3.jpg)'
- en: Figure 6.3 - The sample statistics obtained by calling sample_stats
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 - 通过调用 sample_stats 获得的样本统计数据
- en: 'We will now have a look at sample call rates:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来看一下样本的呼叫率：
- en: '[PRE25]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This time, we plot a histogram of sample call rates. Again, sgkit gets this
    for free by leveraging Pandas:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们绘制了样本呼叫率的直方图。同样，sgkit 通过利用 Pandas 自动实现这一功能：
- en: '![Figure 6.4 - The histogram of sample call rates ](img/B17942_06_4.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 - 样本呼叫率的直方图](img/B17942_06_4.jpg)'
- en: Figure 6.4 - The histogram of sample call rates
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 - 样本呼叫率的直方图
- en: There’s more...
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: The truth is that for population genetic analysis, nothing beats R; you are
    definitely encouraged to take a look at the existing R libraries for population
    genetics. Do not forget that there is a Python-R bridge, which was discussed in
    [*Chapter 1*](B17942_01.xhtml#_idTextAnchor020), *Python and the Surrounding Software
    Ecology*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，对于人口遗传学分析，R 是无可比拟的；我们强烈建议你查看现有的 R 人口遗传学库。不要忘记，Python 和 R 之间有一个桥接，在 [*第1章*](B17942_01.xhtml#_idTextAnchor020)《Python
    和周边软件生态》一章中已作讨论。
- en: Most of the analysis presented here will be computationally costly if done on
    bigger datasets. Indeed, sgkit is prepared to deal with that because it leverages
    Dask. It would be too complex to introduce Dask at this stage, but for large datasets,
    [*Chapter 11*](B17942_11.xhtml#_idTextAnchor272) will discuss ways to address
    those.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在更大的数据集上执行，大多数在此展示的分析将会消耗大量计算资源。实际上，sgkit 已经准备好应对这个问题，因为它利用了 Dask。虽然在这个阶段介绍
    Dask 过于复杂，但对于大数据集，[*第11章*](B17942_11.xhtml#_idTextAnchor272) 会讨论如何解决这些问题。
- en: See also
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: A list of R packages for statistical genetics is available at [http://cran.r-project.org/web/views/Genetics.xhtml](http://cran.r-project.org/web/views/Genetics.xhtml).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计遗传学的 R 包列表可以在 [http://cran.r-project.org/web/views/Genetics.xhtml](http://cran.r-project.org/web/views/Genetics.xhtml)
    找到。
- en: If you need to know more about population genetics, I recommend the book *Principles
    of Population Genetics*, by *Daniel L. Hartl and Andrew G. Clark*, *Sinauer Associates.*
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你需要了解更多人口遗传学的内容，我推荐阅读 *Daniel L. Hartl 和 Andrew G. Clark* 合著的《*人口遗传学原理*》一书，出版商是
    *Sinauer Associates*。
- en: Analyzing population structure
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析人群结构
- en: Previously, we introduced data analysis with sgkit ignoring the population structure.
    Most datasets, including the one we are using, actually do have a population structure.
    Sgkit provides functionality to analyze genomic datasets with population structure
    and that is what we are going to investigate here.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们介绍了不考虑人群结构的 sgkit 数据分析。事实上，大多数数据集，包括我们正在使用的这个数据集，都有一定的人群结构。sgkit 提供了分析具有群体结构的基因组数据集的功能，我们将在这里进行探讨。
- en: Getting ready
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to have run the first recipe, and should have the `hapmap10_auto_noofs_ld`
    data we produced and also the original population meta data `relationships_w_pops_041510.txt`
    file downloaded. There is a Notebook file with the `06_PopGen/Pop_Stats.py` recipe
    in it.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要先运行第一个配方，并且应该已经下载了我们生成的 `hapmap10_auto_noofs_ld` 数据和原始的人群元数据 `relationships_w_pops_041510.txt`
    文件。配方文件中有一个 Notebook 文件，包含了 `06_PopGen/Pop_Stats.py`。
- en: How to do it...
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Take a look at the following steps:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请看以下步骤：
- en: 'First, let’s load the PLINK data with sgkit:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们用 sgkit 加载 PLINK 数据：
- en: '[PRE26]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, let’s load the data assigning individuals to populations:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们加载数据并将个体分配到各个群体：
- en: '[PRE27]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We end up with a dictionary, `pop_ind`, where the key is the population code,
    and the value is a list of samples. Remember that a sample primary key is the
    family ID and the sample ID.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到一个字典 `pop_ind`，其中键是人群代码，值是样本列表。请记住，样本的主键是家庭 ID 和样本 ID。
- en: We also have a list of populations in the `pops` variable.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在 `pops` 变量中列出了人群列表。
- en: 'We now need to inform sgkit about to which population or cohort each sample
    belongs:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在需要告知 sgkit 每个样本属于哪个人群或队列：
- en: '[PRE28]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Remember that each sample in sgkit has a position in an array. So, we have
    to create an array where each element refers to a specific population or cohort
    within a sample. The `assign_cohort` function does exactly that: it takes the
    metadata that we loaded from the `relationships` file and the list of samples
    from the sgkit file, and gets the population index for each sample.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，sgkit 中的每个样本都有一个在数组中的位置。因此，我们需要创建一个数组，其中每个元素都指向样本内的特定人群或队列。`assign_cohort`
    函数正是做了这件事：它获取我们从 `relationships` 文件加载的元数据和来自 sgkit 文件的样本列表，并为每个样本获取人群索引。
- en: 'Now that we have loaded population information structure into the sgkit dataset,
    we can start computing statistics at the population or cohort level. Let’s start
    by getting the number of monomorphic loci per population:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经将人群信息结构加载到 sgkit 数据集中，接下来可以开始在人口或队列层级计算统计数据。首先，让我们计算每个人群的单一等位基因位点数量：
- en: '[PRE29]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We start by asking sgkit to calculate the allele frequencies per cohort or
    population. After that, we filter all loci per population where the allele frequency
    of the first allele is either `0` or `1` (that is, there is the fixation of one
    of the alleles). Finally, we print it. Incidentally, we use the `pprint.pprint`
    function to make it look a bit better (the function is quite useful for more complex
    structures if you want to render the output in a readable way):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先请求sgkit计算每个队列或人群的等位基因频率。然后，我们筛选每个人群中的所有位点，其中第一个等位基因的频率为`0`或`1`（即其中一个等位基因已经固定）。最后，我们打印出来。顺便提一下，我们使用`pprint.pprint`函数让输出看起来更清晰（如果你希望以可读的方式渲染输出，且结构较复杂时，这个函数非常有用）：
- en: '[PRE30]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let’s get the minimum allele frequency for all loci per population. This is
    still based in `cohort_allele_frequency` – so no need to call sgkit again:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们获取每个人群所有位点的最小等位基因频率。这仍然是基于`cohort_allele_frequency`——因此不需要再次调用sgkit：
- en: '[PRE31]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We create Pandas `Series` objects for each population, as this permits lots
    of helpful functions, such as plotting.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个人群创建Pandas `Series`对象，因为这样可以使用许多有用的功能，例如绘图。
- en: 'We will now print the MAF histograms for the `YRI` and `JPT` populations. We
    will leverage Pandas and Matplotlib for this:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将打印`YRI`和`JPT`人群的MAF直方图。我们将利用Pandas和Matplotlib来完成此操作：
- en: '[PRE32]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We get Pandas to generate the histograms and put the results in a Matplotlib
    plot. The result is the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们让Pandas生成直方图，并将结果放入Matplotlib图中。结果如下所示：
- en: '![Figure 6.5 - A MAF histogram for the YRI and JPT populations ](img/B17942_06_5.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 - YRI和JPT人群的MAF直方图](img/B17942_06_5.jpg)'
- en: Figure 6.5 - A MAF histogram for the YRI and JPT populations
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 - YRI和JPT人群的MAF直方图
- en: 'We are now going to concentrate on computing the FST. The FST is a widely used
    statistic that tries to represent the genetic variation created by population
    structure. Let’s compute it with sgkit:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将集中计算FST。FST是一个广泛使用的统计量，旨在表示由人群结构产生的遗传变异。让我们使用sgkit来计算它：
- en: '[PRE33]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The first line computes `fst`, which, in this case, will be pairwise `fst` across
    cohorts or populations. The second line assigns names to each cohorts by using
    the xarray coordinates feature. This makes it easier and more declarative.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行计算`fst`，在这种情况下，它将是队列或人群之间的配对`fst`。第二行通过使用xarray坐标功能为每个队列分配名称。这使得代码更简洁，更具声明性。
- en: 'Let’s compare `fst` between the `CEU` and `CHB` populations with `CHB` and
    `CHD`:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们比较`CEU`和`CHB`人群与`CHB`和`CHD`人群之间的`fst`：
- en: '[PRE34]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We take the pairwise results returned by the `sel` function from `stat_FST`
    to both compare and create a Pandas Series with it. Note that we can refer to
    populations by name, as we have prepared the coordinates in the previous step.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`stat_FST`中`sel`函数返回的配对结果用于比较，并创建一个Pandas `Series`。请注意，我们可以按名称引用人群，因为我们已经在前一步中准备好了坐标。
- en: 'Let’s plot the distance matrix across populations based on the multi-locus
    pairwise FST. Before we do it, we will prepare the computation:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们基于多位点配对FST绘制人群间的距离矩阵。在此之前，我们将准备计算：
- en: '[PRE35]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We compute all the FST values for the population pairs. The execution of this
    code will be demanding in terms of time and memory, as we are actually requiring
    Dask to perform a lot of computations to render our NumPy arrays.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算所有人群对之间的FST值。执行这段代码时将消耗大量时间和内存，因为我们实际上要求Dask进行大量计算，以呈现我们的NumPy数组。
- en: 'We can now do a pairwise plot of all mean FSTs across populations:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以对所有人群的平均FST进行配对绘图：
- en: '[PRE36]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In the following diagram, we will draw an upper triangular matrix, where the
    background color of a cell represents the measure of differentiation; white means
    less different (a lower FST) and blue means more different (a higher FST). The
    lowest value between **CHB** and **CHD** is represented in yellow, and the biggest
    value between **JPT** and **YRI** is represented in magenta. The value on each
    cell is the average pairwise FST between these two populations:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图表中，我们将绘制一个上三角矩阵，其中单元格的背景颜色表示分化的度量；白色表示差异较小（较低的FST），蓝色表示差异较大（较高的FST）。**CHB**和**CHD**之间的最小值用黄色表示，而**JPT**和**YRI**之间的最大值用洋红色表示。每个单元格中的值是这两个人群之间的平均配对FST：
- en: '![Figure 6.6 - The average pairwise FST across the 11 populations in the HapMap
    project for all autosomes ](img/B17942_06_6.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 - HapMap项目中11个人群所有常染色体的平均配对FST](img/B17942_06_6.jpg)'
- en: Figure 6.6 - The average pairwise FST across the 11 populations in the HapMap
    project for all autosomes
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 - HapMap项目中11个人群所有常染色体的平均配对FST
- en: See also
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: F-statistics is an immensely complex topic, so I will direct you firstly to
    the Wikipedia page at [http://en.wikipedia.org/wiki/F-statistics](http://en.wikipedia.org/wiki/F-statistics).
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F统计量是一个非常复杂的话题，所以我首先将引导你到维基百科页面：[http://en.wikipedia.org/wiki/F-statistics](http://en.wikipedia.org/wiki/F-statistics)。
- en: 'A very good explanation can be found in Holsinger and Weir’s paper (*Genetics
    in geographically structured populations: defining, estimating, and interpreting
    FST*) in *Nature Reviews Genetics*, at [http://www.nature.com/nrg/journal/v10/n9/abs/nrg2611.xhtml](http://www.nature.com/nrg/journal/v10/n9/abs/nrg2611.xhtml).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Holsinger和Weir的论文中提供了一个很好的解释，论文标题为《*Genetics in geographically structured populations:
    defining, estimating, and interpreting FST*》，刊登在《*Nature Reviews Genetics*》杂志上，链接：[http://www.nature.com/nrg/journal/v10/n9/abs/nrg2611.xhtml](http://www.nature.com/nrg/journal/v10/n9/abs/nrg2611.xhtml)。'
- en: Performing a PCA
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行PCA
- en: PCA is a statistical procedure that’s used to perform a reduction of the dimension
    of a number of variables to a smaller subset that is linearly uncorrelated. Its
    practical application in population genetics is assisting with the visualization
    of the relationships between the individuals that are being studied.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种统计方法，用于将多个变量的维度降低到一个较小的子集，这些子集是线性无关的。它在群体遗传学中的实际应用是帮助可视化被研究个体之间的关系。
- en: 'While most of the recipes in this chapter make use of Python as a *glue language*
    (Python calls external applications that actually do most of the work), with PCA,
    we have an option: we can either use an external application (for example, EIGENSOFT
    SmartPCA) or use scikit-learn and perform everything on Python. In this recipe,
    we will use SmartPCA – for a native machine learning experience with scikit-learn,
    see [*Chapter 10*](B17942_10.xhtml#_idTextAnchor255).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的大多数食谱都使用Python作为*粘合语言*（Python调用外部应用程序来完成大部分工作），而在PCA中，我们有一个选择：我们可以使用外部应用程序（例如，EIGENSOFT
    SmartPCA），也可以使用scikit-learn并在Python中执行所有操作。在本食谱中，我们将使用SmartPCA——如果你想体验使用scikit-learn的原生机器学习方法，可以参考[*第10章*](B17942_10.xhtml#_idTextAnchor255)。
- en: TIP
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'You actually have a third option: using sgkit. However, I want to show you
    alternatives on how to perform computations. There are two good reasons for this.
    Firstly, you might prefer not to use sgkit – while I recommend it, I don’t want
    to force it – and secondly, you might be required to run an alternative method
    that is not implemented in sgkit. PCA is actually a good example of this: a reviewer
    on a paper might require you to run a published and widely used method such as
    EIGENSOFT SmartPCA.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，你还有第三个选择：使用sgkit。然而，我想向你展示如何执行计算的替代方案。这样做有两个很好的理由。首先，你可能不想使用sgkit——尽管我推荐它，但我不想强迫你；其次，你可能需要使用一个在sgkit中没有实现的替代方法。PCA实际上就是一个很好的例子：论文的审稿人可能要求你运行一个已发布且广泛使用的方法，例如EIGENSOFT
    SmartPCA。
- en: Getting ready
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to run the first recipe in order to make use of the `hapmap10_auto_noofs_ld_12`
    PLINK file (with alleles recoded as `1` and `2`). PCA requires LD-pruned markers;
    we will not risk using the offspring here because it will probably bias the result.
    We will use the recoded PLINK file with alleles as `1` and `2` because this makes
    processing with SmartPCA and scikit-learn easier.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要先运行第一个食谱，才能使用`hapmap10_auto_noofs_ld_12` PLINK文件（其中等位基因已重新编码为`1`和`2`）。PCA需要LD修剪后的标记；我们不会在这里使用后代数据，因为这可能会偏向结果。我们将使用重新编码后的PLINK文件，其中等位基因为`1`和`2`，因为这样可以使SmartPCA和scikit-learn的处理更加方便。
- en: 'I have a simple library to help with some genomics processing. You can find
    this code at [https://github.com/tiagoantao/pygenomics](https://github.com/tiagoantao/pygenomics).
    You can install it with the following command:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个简单的库来帮助进行一些基因组处理。你可以在[https://github.com/tiagoantao/pygenomics](https://github.com/tiagoantao/pygenomics)找到这个代码。你可以使用以下命令进行安装：
- en: '[PRE37]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: For this recipe, you will need to download EIGENSOFT ([http://www.hsph.harvard.edu/alkes-price/software/](http://www.hsph.harvard.edu/alkes-price/software/)),
    which includes the SmartPCA application that we will use.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本食谱，你需要下载EIGENSOFT（[http://www.hsph.harvard.edu/alkes-price/software/](http://www.hsph.harvard.edu/alkes-price/software/)），其中包含我们将使用的SmartPCA应用程序。
- en: There is a Notebook file in the `Chapter06/PCA.py` recipe, but you will still
    need to run the first recipe.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Chapter06/PCA.py`食谱中有一个Notebook文件，但你仍然需要先运行第一个食谱。
- en: How to do it...
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Take a look at the following steps:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下步骤：
- en: 'Let’s load the metadata, as follows:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载元数据，步骤如下：
- en: '[PRE38]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In this case, we will add an entry that is consistent with what is available
    in the PLINK file.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将添加一个与PLINK文件中已有内容一致的条目。
- en: 'Let’s convert the PLINK file into the EIGENSOFT format:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将 PLINK 文件转换为 EIGENSOFT 格式：
- en: '[PRE39]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This uses a function that I have written to convert from PLINK to the EIGENSOFT
    format. This is mostly text manipulation—not exactly the most exciting code.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用了我编写的一个函数，将 PLINK 数据转换为 EIGENSOFT 格式。这主要是文本处理——并不是最激动人心的代码。
- en: 'Now, we will run `SmartPCA` and parse its results, as follows:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将运行 `SmartPCA` 并解析其结果，如下所示：
- en: '[PRE40]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Again, this will use a couple of functions from `pygenomics` to control `SmartPCA`
    and then parse the output. The code is typical for this kind of operation, and
    while you are invited to inspect it, it’s quite straightforward.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这将使用 `pygenomics` 中的几个函数来控制 `SmartPCA`，然后解析输出。代码是此类操作的典型代码，尽管你可以查看它，但实际上它非常直接。
- en: The `parse` function will return the PCA weights (which we will not use, but
    you should inspect), normalized weights, and then the principal components (usually
    up to PC `10`) per individual.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`parse` 函数将返回 PCA 权重（我们不会使用这些权重，但你应该检查一下）、归一化权重，以及每个个体的主成分（通常最多到 PC `10`）。'
- en: 'Then, we plot PC `1` and PC `2`, as shown in the following code:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们绘制 PC `1` 和 PC `2`，如下所示的代码：
- en: '[PRE41]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This will produce the following diagram. We will supply the plotting function
    and the population information retrieved from the metadata, which allows you to
    plot each population with a different color. The results are very similar to published
    results; we will find four groups. Most Asian populations are located at the top,
    the African populations are located on the right-hand side, and the European populations
    are located at the bottom. Two more admixed populations (**GIH** and **MEX**)
    are located in the middle:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图示。我们将提供绘图函数和从元数据中检索的种群信息，这样可以用不同的颜色绘制每个种群。结果与已发布的结果非常相似；我们将找到四个群体。大多数亚洲种群位于顶部，非洲种群位于右侧，欧洲种群位于底部。还有两个混合种群（**GIH**
    和 **MEX**）位于中间：
- en: '![Figure 6.7 - PC 1 and PC 2 of the HapMap data, as produced by SmartPCA ](img/B17942_06_7.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 - 由 SmartPCA 生成的 HapMap 数据的 PC 1 和 PC 2](img/B17942_06_7.jpg)'
- en: Figure 6.7 - PC 1 and PC 2 of the HapMap data, as produced by SmartPCA
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 - 由 SmartPCA 生成的 HapMap 数据的 PC 1 和 PC 2
- en: Note
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that PCA plots can be symmetrical in any axis across runs, as the signal
    does not matter. What matters is that the clusters should be the same and that
    the distances between individuals (and these clusters) should be similar.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，PCA 图可以在任何轴上对称，因为信号并不重要。重要的是簇应该相同，并且个体之间（以及这些簇之间）的距离应该相似。
- en: There’s more...
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: An interesting question here is which method you should use – SmartPCA or scikit-learn,
    which we will use in [*Chapter 10*](B17942_10.xhtml#_idTextAnchor255). The results
    are similar, so if you are performing your own analysis, you are free to choose.
    However, if you publish your results in a scientific journal, SmartPCA is probably
    a safer choice because it’s based on the published piece of software in the field
    of genetics; reviewers will probably prefer this.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的问题，你应该使用哪种方法——SmartPCA 还是 scikit-learn？我们将在[*第 10 章*](B17942_10.xhtml#_idTextAnchor255)中使用
    scikit-learn。结果是相似的，所以如果你正在进行自己的分析，可以自由选择。然而，如果你将结果发表在科学期刊上，SmartPCA 可能是更安全的选择，因为它基于遗传学领域已发布的软件；审稿人可能更倾向于选择这一方法。
- en: See also
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The paper that probably popularized the use of PCA in genetics was Novembre
    et al.’s *Genes mirror geography within Europe* on *Nature*, where a PCA of Europeans
    mapped almost perfectly to a map of Europe. This can be found at [http://www.nature.com/nature/journal/v456/n7218/abs/nature07331.xhtml](http://www.nature.com/nature/journal/v456/n7218/abs/nature07331.xhtml).
    Note that there is nothing about PCA that assures it will map to geographical
    features (just check our PCA earlier).
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能最先普及 PCA 在遗传学中应用的论文是 Novembre 等人的 *Genes mirror geography within Europe*，发表在
    *Nature* 上，文中通过 PCA 映射欧洲人的数据，几乎完美地与欧洲地图对齐。你可以在 [http://www.nature.com/nature/journal/v456/n7218/abs/nature07331.xhtml](http://www.nature.com/nature/journal/v456/n7218/abs/nature07331.xhtml)
    找到这篇文章。请注意，PCA 本身并不保证它会映射到地理特征（只需检查我们之前的 PCA）。
- en: The SmartPCA is described in Patterson et al.’s *Population Structure and Eigenanalysis*,
    *PLoS Genetics*, at [http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190](http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190).
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SmartPCA 在 Patterson 等人的 *Population Structure and Eigenanalysis* 中有描述，发表在 *PLoS
    Genetics*，网址为 [http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190](http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190)。
- en: A discussion of the meaning of PCA can be found in McVean’s paper on *A Genealogical
    Interpretation of Principal Components Analysis*, *PLoS Genetics*, at [http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1000686](http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1000686).
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA的含义讨论可以在McVean的论文《*A Genealogical Interpretation of Principal Components
    Analysis*》，*PLoS Genetics*中找到，链接为[http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1000686](http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1000686)。
- en: Investigating population structure with admixture
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用混合分析研究种群结构
- en: A typical analysis in population genetics was the one popularized by the program
    structure ([https://web.stanford.edu/group/pritchardlab/structure.xhtml](https://web.stanford.edu/group/pritchardlab/structure.xhtml)),
    which is used to study population structure. This type of software is used to
    infer how many populations exist (or how many ancestral populations generated
    the current population), and to identify potential migrants and admixed individuals.
    The structure was developed quite some time ago, when far fewer markers were genotyped
    (at that time, this was mostly a handful of microsatellites), and faster versions
    were developed, including one from the same laboratory called `fastStructure`
    ([http://rajanil.github.io/fastStructure/](http://rajanil.github.io/fastStructure/)).
    Here, we will use Python to interface with a program of the same type that was
    developed at UCLA, called admixture ([https://dalexander.github.io/admixture/download.xhtml](https://dalexander.github.io/admixture/download.xhtml)).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在种群遗传学中，一个典型的分析是由结构程序（[https://web.stanford.edu/group/pritchardlab/structure.xhtml](https://web.stanford.edu/group/pritchardlab/structure.xhtml)）推广的，这个程序用于研究种群结构。此类软件用于推断存在多少个种群（或有多少个祖先种群生成了当前的种群），并识别潜在的迁徙者和混合个体。结构程序是在很久以前开发的，当时基因型标记数量较少（当时主要是少量微卫星标记），之后开发出了更快的版本，包括同一实验室开发的`fastStructure`（[http://rajanil.github.io/fastStructure/](http://rajanil.github.io/fastStructure/)）。在这里，我们将使用Python与在UCLA开发的同类程序——admixture（[https://dalexander.github.io/admixture/download.xhtml](https://dalexander.github.io/admixture/download.xhtml)）进行接口。
- en: Getting ready
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to run the first recipe in order to use the `hapmap10_auto_noofs_ld`
    binary PLINK file. Again, we will use a 10 percent subsampling of autosomes that
    have been LD-pruned with no offspring.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要运行第一个步骤才能使用`hapmap10_auto_noofs_ld`二进制PLINK文件。同样，我们将使用经过LD修剪且没有后代的10%自动体样本。
- en: 'As in the previous recipe, you will use the `pygenomics` library to help; you
    can find these code files at [https://github.com/tiagoantao/pygenomics](https://github.com/tiagoantao/pygenomics).
    You can install it with the following command:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如同前面的步骤，你将使用`pygenomics`库来协助；你可以在[https://github.com/tiagoantao/pygenomics](https://github.com/tiagoantao/pygenomics)找到这些代码文件。你可以使用以下命令安装它：
- en: '[PRE42]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In theory, for this recipe, you will need to download admixture ([https://www.genetics.ucla.edu/software/admixture/](https://www.genetics.ucla.edu/software/admixture/)).
    However, in this case, I will provide the outputs of running admixture on the
    HapMap data that we will use, because running admixture takes a lot of time. You
    can either use the results available or run admixture yourself. There is a Notebook
    file for this in the `Chapter06/Admixture.py` recipe, but you will still need
    to run the recipe first.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，对于本步骤，你需要下载admixture（[https://www.genetics.ucla.edu/software/admixture/](https://www.genetics.ucla.edu/software/admixture/)）。然而，在本案例中，我将提供我们将使用的HapMap数据上运行admixture的输出，因为运行admixture需要很长时间。你可以使用已提供的结果，或者自己运行admixture。此过程的Notebook文件位于`Chapter06/Admixture.py`中，但你仍然需要先运行该步骤。
- en: How to do it...
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Take a look at the following steps:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下步骤：
- en: 'First, let’s define our `k` (a number of ancestral populations) range of interest,
    as follows:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们定义我们感兴趣的`k`（祖先种群的数量）范围，如下所示：
- en: '[PRE43]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let’s run admixture for all our `k` (alternatively, you can skip this step
    and use the example data provided):'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们对所有的`k`进行admixture分析（或者，你也可以跳过此步骤，使用提供的示例数据）：
- en: '[PRE44]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This is the worst possible way of running admixture and will probably take
    more than 3 hours if you do it this way. This is because it will run all `k` from
    `2` to `9` in a sequence. There are two things that you can do to speed this up:
    use the multithreaded option (`-j`), which admixture provides, or run several
    applications in parallel. Here, I have to assume a worst-case scenario where you
    only have a single core and thread available, but you should be able to run this
    more efficiently by parallelizing. We will discuss this issue at length in [*Chapter
    11*](B17942_11.xhtml#_idTextAnchor272).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这是进行混合分析的最糟糕方式，如果你按这种方式进行，可能需要超过 3 小时。这是因为它会按顺序运行所有的`k`值，从`2`到`9`。有两种方法可以加速这一过程：使用混合工具提供的多线程选项（`-j`），或者并行运行多个应用程序。在这里，我假设最坏的情况，你只有一个核心和线程可用，但你应该能通过并行化更高效地运行。我们将在[*第
    11 章*](B17942_11.xhtml#_idTextAnchor272)中详细讨论这个问题。
- en: 'We will need the order of individuals in the PLINK file, as admixture outputs
    individual results in this order:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将需要 PLINK 文件中个体的顺序，因为混合工具以此顺序输出个体结果：
- en: '[PRE45]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The cross-validation error gives a measure of the “best” `k`, as follows:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交叉验证误差给出了“最佳”`k`的衡量标准，如下所示：
- en: '[PRE46]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The following graph plots the `CV` between a `K` of `2` and `9`, the lower,
    the better. It should be clear from this graph that we should maybe run some more
    `K` (indeed, we have 11 populations; if not more, we should at least run up to
    11), but due to computation costs, we stopped at `9`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表绘制了`K`值从`2`到`9`之间的交叉验证误差，越低越好。从这个图表中应该可以看出，我们可能需要运行更多的`K`值（事实上，我们有 11 个种群；如果不是更多的话，至少应该运行到
    11），但由于计算成本问题，我们停在了`9`。
- en: 'It would be a very technical debate on whether there is such thing as the “best”
    `K`. Modern scientific literature suggests that there may not be a “best” `K`;
    these results are worthy of some interpretation. I think it’s important that you
    are aware of this before you go ahead and interpret the `K` results:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 关于是否存在“最佳”`K`，这将是一个非常技术性的讨论。现代科学文献表明，可能没有“最佳”`K`；这些结果值得一些解释。我认为在解释`K`结果之前，你应该意识到这一点：
- en: '![Figure 6.8 - The error by K ](img/B17942_06_8.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 - K 的误差](img/B17942_06_8.jpg)'
- en: Figure 6.8 - The error by K
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 - K 的误差
- en: 'We will need the metadata for the population information:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要人口信息的元数据：
- en: '[PRE47]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We will ignore individuals that are not in the PLINK file.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将忽略 PLINK 文件中没有的个体。
- en: 'Let’s load the individual component, as follows:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载个体组成部分，如下所示：
- en: '[PRE48]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Admixture produces a file with the ancestral component per individual (for an
    example, look at any of the generated `Q` files); there will be as many components
    as the number of `k` that you decided to study. Here, we will load the `Q` file
    for all `k` that we studied and store them in a dictionary where the individual
    ID is the key.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 混合工具会生成一个文件，包含每个个体的祖先组成部分（例如，查看任何生成的`Q`文件）；将会有与您选择研究的`k`值相等的组成部分数量。在这里，我们将加载我们研究的所有`k`的`Q`文件，并将其存储在一个字典中，个体
    ID 作为键。
- en: 'Then, we cluster individuals, as follows:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将对个体进行聚类，如下所示：
- en: '[PRE49]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Remember that individuals were given components of ancestral populations by
    admixture; we would like to order them per their similarity in terms of ancestral
    components (not by their order in the PLINK file). This is not a trivial exercise
    and requires a clustering algorithm.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，个体是通过混合获得祖先种群的组成部分；我们希望根据它们在祖先组成部分上的相似性对它们进行排序（而不是按 PLINK 文件中的顺序）。这并不是一项简单的任务，需要使用聚类算法。
- en: Furthermore, we do not want to order all of them; we want to order them in each
    population and then order each population accordingly.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们并不希望对所有个体进行排序；我们希望按每个种群排序，然后再对每个种群进行排序。
- en: For this purpose, I have some clustering code available at [https://github.com/tiagoantao/pygenomics/blob/master/genomics/popgen/admix/__init__.py](https://github.com/tiagoantao/pygenomics/blob/master/genomics/popgen/admix/__init__.py).
    This is far from perfect but allows you to perform some plotting that still looks
    reasonable. My code makes use of the SciPy clustering code. I suggest you take
    a look (by the way, it’s not very difficult to improve upon it).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我在[https://github.com/tiagoantao/pygenomics/blob/master/genomics/popgen/admix/__init__.py](https://github.com/tiagoantao/pygenomics/blob/master/genomics/popgen/admix/__init__.py)提供了一些聚类代码。这并不完美，但允许你执行一些看起来合理的绘图。我的代码使用了
    SciPy 的聚类代码。我建议你看看（顺便说一句，改进它并不难）。
- en: 'With a sensible individual order, we can now plot the admixture:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在合理的个体顺序下，我们现在可以绘制混合图：
- en: '[PRE50]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This will produce two charts; the second chart is shown in the following diagram
    (the first chart is actually a variation of the third admixture plot from the
    top).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生两个图表；第二个图表显示在下图中（第一个图表实际上是从顶部的第三个混合图的变体）。
- en: The first figure of `K` = `4` requires the components per individual and their
    order. It will plot all individuals, ordered and split by population.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个图 `K` = `4` 需要每个个体的组分及其顺序。它将按种群排序并分割所有个体。
- en: The second chart will perform a set of stacked plots of admixture from `K` =
    `2` to `9`. It requires a `figure` object (as the dimension of this figure can
    vary widely with the number of stacked admixtures that you require). The individual
    order will typically follow one of the `K` (we have chosen a `K` of `7` here).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个图将绘制从 `K` = `2` 到 `9` 的一组堆叠混合图。它需要一个 `figure` 对象（由于堆叠混合物的数量可能很多，图的尺寸会有所不同）。个体的顺序通常会遵循其中一个
    `K`（这里我们选择了 `K` = `7`）。
- en: 'Note that all `K` are worthy of some interpretation (for example, `K` = `2`
    separates the African population from others, and `K` = `3` separates the European
    population and shows the admixture of **GIH** and **MEX**):'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有的 `K` 都值得一些解释（例如，`K` = `2` 将非洲人口与其他人群分离开来，而 `K` = `3` 则将欧洲人口分离，并展示了 **GIH**
    和 **MEX** 的混合）：
- en: '![Figure 6.9 - A stacked admixture plot (between K of 2 and 9) for the HapMap
    example ](img/B17942_06_9.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 - HapMap 示例的堆叠混合图（K 值介于 2 和 9 之间）](img/B17942_06_9.jpg)'
- en: Figure 6.9 - A stacked admixture plot (between K of 2 and 9) for the HapMap
    example
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 - HapMap 示例的堆叠混合图（K 值介于 2 和 9 之间）
- en: There’s more...
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: Unfortunately, you cannot run a single instance of admixture to get a result.
    The best practice is to actually run 100 instances and get the one with the best
    log likelihood (which is reported in the admixture output). Obviously, I cannot
    ask you to run 100 instances for each of the 7 different `K` for this recipe (we
    are talking about two weeks of computation), but you will probably have to perform
    this if you want to have publishable results. A cluster (or at least a very good
    machine) is required to run this. You can use Python to go through outputs and
    select the best log likelihood. After selecting the result with the best log likelihood
    for each `K`, you can easily apply this recipe to plot the output.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，你不能仅运行单个混合实例来获得结果。最佳做法实际上是运行 100 个实例，并获得具有最佳对数似然的结果（这在混合输出中报告）。显然，我无法要求你为此配方的每个
    7 个不同的 `K` 运行 100 个实例（我们在谈论两周的计算时间），但如果你想要有可发布的结果，你可能必须执行这些步骤。需要集群（或至少非常好的机器）来运行这个。你可以使用
    Python 处理输出并选择最佳对数似然的结果。选择每个 `K` 的最佳结果后，你可以轻松应用此配方来绘制输出。
