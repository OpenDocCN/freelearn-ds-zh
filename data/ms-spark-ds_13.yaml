- en: Chapter 13. Secure Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章。安全数据
- en: Throughout this book, we have visited many areas of data science, often straying
    into those that are not traditionally associated with a data scientist's core
    working knowledge. In particular, we dedicated an entire chapter, [Chapter 2](ch02.xhtml
    "Chapter 2. Data Acquisition"), *Data Acquisition*, to data ingestion, which explains
    how to solve an issue that is always present, but rarely acknowledged or addressed
    adequately. In this chapter, we will visit another of those often overlooked fields,
    secure data. More specifically, how to protect your data and analytic results
    at all stages of the data life cycle. This ranges from ingestion, right through
    to presentation, at all times considering the important architectural and scalability
    requirements that naturally form the Spark paradigm.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们访问了许多数据科学领域，通常涉及那些传统上与数据科学家的核心工作知识不太相关的领域。特别是，我们专门在[第2章](ch02.xhtml "第2章。数据获取")
    *数据获取*中，解释了如何解决一个始终存在但很少被承认或充分解决的问题，即数据摄取。在本章中，我们将访问另一个经常被忽视的领域，即安全数据。更具体地说，如何在数据生命周期的所有阶段保护您的数据和分析结果。这从摄取开始，一直到呈现，始终考虑到自然形成Spark范例的重要架构和可扩展性要求。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: How to implement coarse-grained data access controls using HDFS ACLs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用HDFS ACL实现粗粒度数据访问控制
- en: A guide to fine-grained security, with explanations using the Hadoop ecosystem
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hadoop生态系统进行细粒度安全的指南和解释
- en: How to ensure data is always encrypted, with an example using Java KeyStore
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何确保数据始终加密，以Java KeyStore为例
- en: Techniques for obfuscating, masking, and tokenizing data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆、掩码和令牌化数据的技术
- en: How Spark implements Kerberos
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark如何实现Kerberos
- en: Data security - the ethical and technical issues
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据安全-道德和技术问题
- en: Data security
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据安全
- en: The final piece to our data architecture is security, and in this chapter we
    will discover that data security is always important, and the reasons for this.
    Given the huge increase in the volume and variety of data in recent times, caused
    by many factors, but in no small part due to the popularity of the Internet and
    related technologies, there is a growing need to provide fully scalable and secure
    solutions. We are going to explore those solutions along with the confidentiality,
    privacy, and legal concerns associated with the storing, processing, and handling
    of data; we will relate these to the tools and techniques introduced in previous
    chapters.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据架构的最后一部分是安全性，在本章中我们将发现数据安全性总是重要的，以及其原因。由于最近由许多因素引起的数据量和种类的巨大增加，但其中不乏因互联网及相关技术的普及所致，因此需要提供完全可扩展和安全的解决方案。我们将探讨这些解决方案以及与数据的存储、处理和处理相关的机密性、隐私和法律问题；我们将把这些与之前章节介绍的工具和技术联系起来。
- en: We will continue on by explaining the technical issues involved in securing
    data at scale and introduce ideas and techniques that tackle these concerns using
    a variety of access, classification, and obfuscation strategies. As in previous
    chapters, ideas are demonstrated with examples using the Hadoop ecosystem, and
    public cloud infrastructure strategies will also be present.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续解释涉及在规模上保护数据的技术问题，并介绍使用各种访问、分类和混淆策略来解决这些问题的想法和技术。与之前的章节一样，这些想法将通过Hadoop生态系统的示例进行演示，并且公共云基础设施策略也将被介绍。
- en: The problem
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: We have explored many and varied topics in previous chapters, usually concentrating
    on the specifics of a particular issue and the approaches that can be taken to
    solve them. In all of these cases, there has been the implicit idea that the data
    that is being used, and the content of the insights gathered, does not need protecting
    in any way; or at least the protection provided at the operating system level,
    such as login credentials, is sufficient.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们探讨了许多不同的主题，通常集中在特定问题的细节和解决方法上。在所有这些情况下，都存在一个隐含的想法，即正在使用的数据和收集的见解内容不需要以任何方式进行保护；或者至少操作系统级别提供的保护，如登录凭据，是足够的。
- en: In any environment, whether it is a home or a commercial one, data security
    is a huge issue that must always be considered. Perhaps, in a few instances, it
    is enough to write the data to a local hard drive and take no further steps; this
    is rarely an acceptable course of action and certainly should be a conscious decision
    rather than default behavior. In a commercial environment, computing resources
    are often provided with built-in security. In this case, it is still important
    for the user to understand those implications and decide whether further steps
    should be taken; data security is not just about protection from malicious entities
    or accidental deletion, but also everything in-between.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何环境中，无论是家庭还是商业环境，数据安全都是一个必须始终考虑的重大问题。也许，在某些情况下，将数据写入本地硬盘并不再采取进一步的步骤就足够了；这很少是一种可以接受的做法，而且肯定应该是一种有意识的决定，而不是默认行为。在商业环境中，计算资源通常具有内置的安全性。在这种情况下，用户仍然重要理解这些影响，并决定是否应采取进一步的步骤；数据安全不仅仅是关于保护免受恶意实体或意外删除，还包括其中的一切。
- en: As an example, if you work in a secure, regulated, commercial, air-gapped environment
    (no access to the Internet) and within a team of like-minded data scientists,
    individual security responsibilities are still just as important as in an environment
    where no security exists at all; you may have access to data that must not be
    viewed by any of your peers and you may need to produce analytical results that
    are available to different and diversified user groups, all of whom are not to
    see each other's data. The emphasis may be explicitly or implicitly on you to
    ensure that the data is not compromised; therefore, a strong understanding of
    the security layers in your software stack is imperative.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您在一个安全、受监管的商业空隙环境（无法访问互联网）中工作，并且在一个志同道合的数据科学家团队中工作，个人安全责任仍然与根本不存在安全性的环境中一样重要；您可能可以访问不得被同行查看的数据，并且可能需要生成可供不同和多样化用户组使用的分析结果，所有这些用户组都不得查看彼此的数据。强调可能明确或隐含地放在您身上，以确保数据不受损害；因此，对软件堆栈中的安全层有深刻的理解是至关重要的。
- en: The basics
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础知识
- en: Security considerations are everywhere, even in places that you probably hadn't
    even thought of. For example, when Spark is running a parallel job on a cluster,
    do you know the points at which data may touch physical disk during that life
    cycle? If you are thinking that everything is done in RAM, then you have a potential
    security issue right there, as data can be spilled to disk. More on the implications
    of this further on in this chapter. The point here is that you cannot always delegate
    security responsibility to the frameworks you are using. Indeed, the more varied
    the software you use, the more security concerns increase, both user and data
    related.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 安全考虑无处不在，甚至在您可能根本没有考虑过的地方。例如，当Spark在集群上运行并行作业时，您知道数据在生命周期中可能触及物理磁盘的时刻吗？如果您认为一切都是在RAM中完成的，那么您可能会在那里遇到潜在的安全问题，因为数据可能会泄漏到磁盘上。在本章的后面将更多地讨论这一点的影响。这里的要点是，您不能总是将安全责任委托给您正在使用的框架。事实上，您使用的软件越多，安全问题就越多，无论是用户还是数据相关的安全问题。
- en: 'Security can be broadly split into three areas:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性可以大致分为三个领域：
- en: '**Authentication**: determining the legitimacy of the identity of a user'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**认证**：确定用户身份的合法性'
- en: '**Authorization**: the privileges that a user holds to perform specific actions'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**授权**：用户执行特定操作的权限'
- en: '**Access**: the security mechanisms used to protect data, both in transit and
    at rest'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问**：用于保护数据的安全机制，无论是在传输过程中还是在静止状态下'
- en: There are important differences between these points. A user may have full permissions
    to access and edit a file, but if the file has been encrypted outside of the user
    security realm, then the file may still not be readable; user authorization intervenes.
    Equally, a user may send data across a secure link to be processed on a remote
    server before a result is returned, but this does not guarantee that the data
    has not left a footprint on that remote server; the security mechanisms are unknown.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观点之间存在重要差异。用户可能具有访问和编辑文件的完全权限，但如果文件在用户安全领域之外加密，则文件可能仍然无法读取；用户授权会介入。同样，用户可能通过安全链接发送数据到远程服务器进行处理，然后返回结果，但这并不保证数据没有留下痕迹在远程服务器上；安全机制是未知的。
- en: Authentication and authorization
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 认证和授权
- en: Authentication is related to the mechanisms used to ensure that the user is
    who they say they are and operates at two key levels, namely, local and remote.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 认证与机制有关，用于确保用户是其所说的人，并在两个关键级别上运行，即本地和远程。
- en: Authentication can take various forms, the most common is user login, but other
    examples include fingerprint reading, iris scanning, and PIN number entry. User
    logins can be managed on a local basis, as you would on your personal computer,
    for example, or on a remote basis using a tool such as **Lightweight Directory
    Access Protocol** (**LDAP**). Managing users remotely provides roaming user profiles
    that are independent of any particular hardware and can be managed independently
    of the user. All of these methods execute at the operating system level. There
    are other mechanisms that sit at the application layer and provide authentication
    for services, such as Google OAuth.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 认证可以采用各种形式，最常见的是用户登录，但其他例子包括指纹识别、虹膜扫描和PIN码输入。用户登录可以在本地基础上进行管理，例如在个人计算机上，或者在远程基础上使用诸如**轻量级目录访问协议**（**LDAP**）之类的工具。远程管理用户提供了独立于任何特定硬件的漫游用户配置文件，并且可以独立于用户进行管理。所有这些方法都在操作系统级别执行。还有其他机制位于应用程序层，并为服务提供认证，例如Google
    OAuth。
- en: Alternative authentication methods have their own pros and cons, a particular
    implementation should be understood thoroughly before declaring a secure system;
    for example, a fingerprint system may seem very secure, but this is not always
    the case. For more information, refer to [http://www.cse.msu.edu/rgroups/biometrics/Publications/Fingerprint/CaoJain_HackingMobilePhonesUsing2DPrintedFingerprint_MSU-CSE-16-2.pdf](http://www.cse.msu.edu/rgroups/biometrics/Publications/Fingerprint/CaoJain_HackingMobilePhonesUsing2DPrintedFingerprint_MSU-CSE-16-2.pdf).
    We are not going to explore authentication any further here, as we have made the
    assumption that most systems will only be implementing user logins; a feature,
    by the way, that is often not a secure solution in its own right and indeed, in
    many cases, provides no security at all. For more information, refer to [http://www.cs.arizona.edu/~collberg/Teaching/466-566/2012/Resources/presentations/2012/topic7-final/report.pdf](http://www.cs.arizona.edu/~collberg/Teaching/466-566/2012/Resources/presentations/2012/topic7-final/report.pdf).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 替代的身份验证方法各有优缺点，应该在宣称一个安全系统之前充分了解特定的实现方式；例如，指纹系统可能看起来非常安全，但情况并非总是如此。有关更多信息，请参阅[http://www.cse.msu.edu/rgroups/biometrics/Publications/Fingerprint/CaoJain_HackingMobilePhonesUsing2DPrintedFingerprint_MSU-CSE-16-2.pdf](http://www.cse.msu.edu/rgroups/biometrics/Publications/Fingerprint/CaoJain_HackingMobilePhonesUsing2DPrintedFingerprint_MSU-CSE-16-2.pdf)。我们不会在这里进一步探讨身份验证，因为我们已经假设大多数系统只会实现用户登录；顺便说一句，这通常并不是一个安全的解决方案，实际上，在许多情况下根本没有提供安全性。有关更多信息，请参阅[http://www.cs.arizona.edu/~collberg/Teaching/466-566/2012/Resources/presentations/2012/topic7-final/report.pdf](http://www.cs.arizona.edu/~collberg/Teaching/466-566/2012/Resources/presentations/2012/topic7-final/report.pdf)。
- en: 'Authorization is an area that is of great interest to us as it forms a critical
    part of basic security, is an area that we most often have greatest control over,
    and is something that we can use natively in any modern operating system. There
    are various different ways of implementing resource authorization, the two main
    ones being:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 授权是我们非常感兴趣的一个领域，因为它构成了基本安全的关键部分，是我们最常控制的领域，并且是我们可以在任何现代操作系统中原生使用的东西。有各种不同的资源授权实现方式，其中两种主要方式是：
- en: '**Access control lists** (**ACL**)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问控制列表**（**ACL**）'
- en: '**Role-based access control** (**RBAC**)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于角色的访问控制**（**RBAC**）'
- en: We'll discuss each of these in turn.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依次讨论每一条规则。
- en: Access control lists (ACL)
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问控制列表（ACL）
- en: 'In Unix, ACLs are used throughout the filesystem. If we list directory contents
    at the command line:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在Unix中，ACL在整个文件系统中都被使用。如果我们在命令行中列出目录内容：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can see there is a directory called resources that has an assigned owner
    (`mrh`) and group (`mygroup`), has `6` links, a size of `204` bytes, and was last
    modified on the `16 June 2015`. The ACLs `drwxr-xr-x` indicate:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到有一个名为资源的目录，分配了所有者（`mrh`）和组（`mygroup`），有`6`个链接，大小为`204`字节，最后修改日期为`2015年6月16日`。ACL
    `drwxr-xr-x`表示：
- en: '`d` this is a directory (- if it is not)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d`这是一个目录（-如果不是）'
- en: '`rwx` the owner(`mrh`) has read, write, and executable rights'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rwx`所有者（`mrh`）具有读取、写入和执行权限'
- en: '`r-x` anyone in the group (`mygroup`) has read and execute rights'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r-x`组中的任何人（`mygroup`）都有读取和执行权限'
- en: '`r-x` everyone else has read and execute rights'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r-x`其他所有人都有读取和执行权限'
- en: 'Using ACLs is an excellent first step towards securing our data. It should
    always be the first thing considered, and should always be correct; if we do not
    ensure these settings are correct at all times, then we are potentially making
    it easy for other users to access this data, and we don''t necessarily know who
    the other users on the system are. Always avoid providing full access in the *all*
    part of the ACL:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ACL是保护我们的数据的一个很好的第一步。它应该始终是首要考虑的事情，并且应该始终是正确的；如果我们不始终确保这些设置是正确的，那么我们可能会让其他用户轻松访问这些数据，而我们并不一定知道系统上的其他用户是谁。始终避免在ACL的*all*部分提供完全访问权限：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It doesn''t matter how secure our system is, any user with access to the filesystem
    can read, write, and delete this file! A far more appropriate setting would be:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们的系统有多安全，只要有权限访问文件系统的用户都可以读取、写入和删除这个文件！一个更合适的设置是：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Which provides full owner access and read-only access for the group.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这为所有者提供了完全的访问权限，并为组提供了只读权限。
- en: 'HDFS implements ACLs natively; these can be administered using the command
    line:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS原生实现了ACL；这些可以使用命令行进行管理：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This gives full permissions to the file in HDFS for everyone, assuming the file
    already had sufficient permissions for us to make the change.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这为HDFS中的文件提供了所有人的完全权限，假设文件已经具有足够的权限让我们进行更改。
- en: Note
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: When Apache released Hadoop in 2008, it was often not understood that a cluster
    set at all of its defaults did not do any authentication of users. The superuser
    in Hadoop, `hdfs`, could be accessed by any user if the cluster had not been correctly
    configured, by simply creating an `hdfs` user on a client machine (`sudo useradd
    hdfs`).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当Apache在2008年发布Hadoop时，人们经常不理解，集群设置为所有默认值时不会对用户进行任何身份验证。如果集群没有正确配置，Hadoop中的超级用户`hdfs`可以被任何用户访问，只需在客户机上创建一个`hdfs`用户（`sudo
    useradd hdfs`）。
- en: Role-based access control (RBAC)
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于角色的访问控制（RBAC）
- en: RBAC takes a different approach, by assigning users one or more roles. These
    roles are related to common tasks or job functions, such that they can be easily
    added or removed dependent upon the user's responsibilities. For example, in a
    company, there may be many roles, including accounts, stock, and deliveries. An
    accountant may be given all three roles, so that they can compile the end of year
    finances, whereas an administrator booking deliveries would just have the deliveries
    role. This makes it much easier to add new users and manage users when they change
    departments or leave the organization.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC采用了一种不同的方法，通过为用户分配一个或多个角色。这些角色与常见任务或工作职能相关，因此可以根据用户的责任轻松添加或删除。例如，在公司中可能有许多角色，包括账户、库存和交付。会计可能会被赋予这三个角色，以便他们可以编制年终财务报表，而负责交付预订的管理员只会有交付角色。这样可以更轻松地添加新用户，并在他们更换部门或离开组织时管理用户。
- en: 'Three key rules are defined for RBAC:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC定义了三条关键规则：
- en: '**Role assignment**: a user can exercise a permission only if the user has
    selected or been assigned a role'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**角色分配**：用户只有在选择或被分配角色后才能行使权限。'
- en: '**Role authorization**: a user''s active role must be authorized for the user'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**角色授权**：用户的活动角色必须经过授权。'
- en: '**Permission authorization**: a user can exercise a permission only if the
    permission is authorized for the user''s active role'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权限授权**：用户只能行使权限，如果该权限已经为用户的活动角色授权。'
- en: 'The relationships between users and roles can be summarized as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 用户和角色之间的关系可以总结如下：
- en: '**Role-Permissions**: a particular role grants specific permissions to the
    user.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**角色-权限**：特定角色向用户授予特定权限。'
- en: '**User-Role**: the relationships between types of users and specific roles.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户-角色**：用户类型和特定角色之间的关系。'
- en: '**Role-Role**: the relationships between roles. These can be hierarchical,
    so *role1 => role2* could mean that, if a user has *role1*, then they automatically
    have *role2*, but if they have *role2*, this does not necessarily mean they have
    *role1*.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**角色-角色**：角色之间的关系。这些关系可以是层次化的，所以*role1 => role2*可能意味着，如果用户有*role1*，那么他们自动拥有*role2*，但如果他们有*role2*，这并不一定意味着他们有*role1*。'
- en: RBAC is realized in Hadoop through Apache Sentry. Organizations can define the
    privileges for datasets that will be enforced from multiple access paths, including
    HDFS, Apache Hive, Impala, as well as Apache Pig and Apache MapReduce/Yarn via
    HCatalog. As an example, each Spark application runs as the requesting user and
    requires access to the underlying files. Spark cannot enforce access control directly,
    since it is running as the requesting user and is untrusted. Therefore, it is
    restricted to filesystem permissions (ACLs). Apache Sentry provides role-based
    control to resources in this case.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC通过Apache Sentry在Hadoop中实现。组织可以定义对数据集的特权，这些特权将从多个访问路径（包括HDFS、Apache Hive、Impala，以及通过HCatalog的Apache
    Pig和Apache MapReduce/Yarn）强制执行。例如，每个Spark应用程序都作为请求用户运行，并需要访问底层文件。Spark无法直接执行访问控制，因为它作为请求用户运行并且不受信任。因此，它受限于文件系统权限（ACL）。在这种情况下，Apache
    Sentry为资源提供基于角色的控制。
- en: Access
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问
- en: 'We have thus far concentrated only on the specific ideas of ensuring that a
    user is who they say they are and that only the correct users can view and use
    data. However, once we have taken the appropriate steps and confirmed these details,
    we still need to ensure that this data is secure when the user is actually using
    it; there are a number of areas to consider:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只集中在确保用户是他们所说的人，只有正确的用户才能查看和使用数据的具体想法上。然而，一旦我们采取了适当的步骤并确认了这些细节，我们仍然需要确保用户在实际使用数据时数据是安全的；有许多方面需要考虑：
- en: Is the user allowed to see all of the information in the data? Perhaps they
    are to be limited to certain rows, or even certain parts of certain rows.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户是否被允许查看数据中的所有信息？也许他们只能限制在某些行，甚至是某些行的某些部分。
- en: Is the data secure when the user runs analytics across it? We need to ensure
    that the data isn't transmitted as plain text and therefore open to man-in-the-middle
    attacks.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当用户在数据上运行分析时，数据是否安全？我们需要确保数据不以明文传输，因此容易受到中间人攻击。
- en: Is the data secure once the user has completed their task? There's no point
    in ensuring that the data is super secure at all stages, only to write plain text
    results to an insecure area.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户完成任务后，数据是否安全？确保数据在所有阶段都非常安全是没有意义的，只有将明文结果写入不安全的区域。
- en: Can conclusions be made from the aggregation of data? Even if the user only
    has access to certain rows of a dataset, let's say to protect an individual's
    anonymity in this case, it is sometimes possible to make links between apparently
    unrelated information, for example. If the user knows that *A=>B* and *B=>C*,
    they can guess that, probably, *A=>C*, even if they are not allowed to see this
    in the data. In practice, this kind of issue can be very difficult to avoid, as
    data aggregation problems can be very subtle, occurring in unforeseen situations
    and often involving information gleaned over an extended period of time.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以从数据的聚合中得出结论吗？即使用户只能访问数据集的某些行，比如在这种情况下保护个人隐私，有时也可能在看似无关的信息之间建立联系。例如，如果用户知道*A=>B*和*B=>C*，他们可能猜测，*A=>C*，即使他们不被允许在数据中看到这一点。实际上，这种问题很难避免，因为数据聚合问题可能非常微妙，发生在意想不到的情况下，通常涉及在较长时间内获取的信息。
- en: There are a number of mechanisms that we can use to help us protect against
    the preceding scenarios.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一些机制来帮助我们防止上述情况。
- en: Encryption
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加密
- en: Arguably the most obvious and well known method of protecting data is encryption.
    We would use this whether our data is in transit or at rest, so, virtually all
    of the time, apart from when the data is actually being processed inside memory.
    The mechanics of encryption are different depending upon the state of the data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，保护数据最明显和最知名的方法是加密。无论我们的数据是在传输中还是静止状态，我们都会使用它，所以除了数据实际在内存中被处理时，几乎所有时间都会使用。加密的机制取决于数据的状态。
- en: Data at rest
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 静态数据
- en: Our data will always need to be stored somewhere, whether it be HDFS, S3, or
    local disk. If we have taken all of the precautions of ensuring that users are
    authorized and authenticated, there is still the issue of plain text actually
    existing on the disk. With direct access to the disk, either physically or by
    accessing it through a lower level in the OSI stack, it is fairly trivial to stream
    the entire contents and glean the plain text data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据总是需要存储在某个地方，无论是HDFS、S3还是本地磁盘。如果我们已经采取了所有必要的预防措施，确保用户已经得到授权和认证，仍然存在明文实际存在于磁盘上的问题。通过物理方式或通过OSI堆栈中的较低级别访问磁盘，非常容易流式传输整个内容并获取明文数据。
- en: If we encrypt data, then we are protected from this type of attack. The encryption
    can also exist at different levels, either by encrypting the data at the application
    layer using software, or by encrypting it at the hardware level, that is, the
    disk itself.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们加密数据，那么我们就可以免受这种类型的攻击。加密也可以存在于不同的层面，可以通过软件在应用程序层对数据进行加密，也可以通过硬件级别对数据进行加密，也就是磁盘本身。
- en: 'Encrypting the data at the application layer is the most common route, as it
    enables the user to make informed choices about the trade-off decisions that need
    to be made, thereby making the right choice of product for their situation. Because
    encryption adds an extra level of processing overhead (the data needs to be encrypted
    at write and decrypted at read), there is a key decision to make regarding the
    processor time versus security strength trade-off. The principal decisions for
    consideration are:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序层对数据进行加密是最常见的路线，因为它使用户能够对需要做出的权衡决策做出明智的选择，从而为他们的情况做出正确的产品选择。因为加密增加了额外的处理开销（数据需要在写入时加密并在读取时解密），因此在处理器时间与安全强度之间需要做出关键决策。需要考虑的主要决策有：
- en: '**Encryption algorithm type**: the algorithm to use to perform encryption,
    that is, AES, RSA, and so on'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加密算法类型**：用于执行加密的算法，即AES、RSA等'
- en: '**Encryption key bit length**: the size of the encryption key roughly equates
    to how difficult it is to crack, but also influences the size of the result (possible
    storage consideration), that is, 64 bit, 128 bit, and so on'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加密密钥位长度**：加密密钥的大小大致相当于破解的难度，但也影响结果的大小（可能的存储考虑），即64位、128位等。'
- en: '**Processor time allowed**: longer encryption keys generally mean greater processing
    time; this can have a serious impact on processing, given data of sufficient volume'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理器时间允许的时间**：较长的加密密钥通常意味着更长的处理时间；鉴于足够大的数据量，这可能会对处理产生严重影响'
- en: Once we have decided upon the correct combination of factors for our use case,
    bearing in mind that some algorithm key length combinations are no longer considered
    safe, we need the software to actually do the encryption. This could be a bespoke
    Hadoop plugin or a commercial application. As mentioned, Hadoop now has a native
    HDFS encryption plugin, so you will not need to write your own! This plugin uses
    a Java KeyStore to safely store the encryption keys, which can be accessed through
    Apache Ranger. Encryption takes place entirely within HDFS, and is essentially
    linked to the ACLs on files. Therefore, when accessing HDFS files in Spark, the
    process is seamless (apart from some extra time to encrypt/decrypt files).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了我们的用例的正确因素组合，要记住，一些算法密钥长度组合不再被认为是安全的，我们需要软件来实际进行加密。这可以是一个定制的Hadoop插件或商业应用程序。正如前面提到的，Hadoop现在有一个本地的HDFS加密插件，因此您不需要编写自己的插件！该插件使用Java
    KeyStore安全存储加密密钥，可以通过Apache Ranger访问。加密完全在HDFS内部进行，并且基本上与文件的ACLs相关联。因此，在Spark中访问HDFS文件时，该过程是无缝的（除了加密/解密文件需要额外的时间）。
- en: If you wish to implement encryption in Spark to write data to somewhere that
    is not covered in the aforementioned scenarios, then the Java javax.crypto package
    can be used. The weakest link here is now the fact that the key itself must be
    recorded somewhere; therefore, we have potentially simply moved our security issue
    elsewhere. Using a suitable KeyStore, such as Java KeyStore would address this
    issue.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望在Spark中实现加密以将数据写入未在上述情况中涵盖的地方，则可以使用Java javax.crypto包。这里最薄弱的环节现在是密钥本身必须被记录在某个地方；因此，我们可能只是把我们的安全问题简单地转移到了其他地方。使用适当的KeyStore，例如Java
    KeyStore，可以解决这个问题。
- en: At the time of writing, there is no obvious way of encrypting data when writing
    from Spark to local disk. In the next section, we'll write our own!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，尚无明显的方法可以在从Spark写入本地磁盘时加密数据。在下一节中，我们将自己编写！
- en: 'The idea is to replace the `rdd.saveAsTextFile(filePath)` function with something
    as close as possible to the original, with the further capability of encrypting
    the data. However, that''s not the whole story, as we''ll need to be able to read
    the data back too. To do this, we''ll take advantage of an alternative to `rdd.saveAsTextFile(filePath)`
    function, which also accepts a compression codec argument:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是用尽可能接近原始的方式替换`rdd.saveAsTextFile(filePath)`函数，并进一步具备加密数据的能力。然而，这还不是全部，因为我们还需要能够读取数据。为此，我们将利用`rdd.saveAsTextFile(filePath)`函数的替代方案，该函数还接受压缩编解码器参数：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'On the face of it, the way Spark uses the compression codec appears to be similar
    to what we''d want for data encryption. So, let''s adapt one of the existing Hadoop
    compression implementations for our purposes. Looking at a few different existing
    implementations (`GzipCodec`, `BZip2Codec`), we find that we must extend the `CompressionCodec`
    interface to derive our encryption codec, named `CryptoCodec` from here on. Let''s
    look at an implementation in Java:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从表面上看，Spark使用压缩编解码器的方式似乎与我们对数据加密的要求相似。因此，让我们为我们的目的调整现有的Hadoop压缩实现之一。查看几种不同的现有实现（`GzipCodec`、`BZip2Codec`），我们发现我们必须扩展`CompressionCodec`接口以派生我们的加密编解码器，从现在起命名为`CryptoCodec`。让我们看一个Java实现：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It''s worth noting here that this codec class just serves as a wrapper for
    integrating our encryption and decryption routines with the Hadoop API; this class
    provides the entry points for the Hadoop framework to use when the crypto codec
    is called. The two main methods of interest are `createCompressor` and `createDeompressor`,
    which both perform the same initialization:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这个编解码器类只是作为一个包装器，用于将我们的加密和解密例程与Hadoop API集成；当调用加密编解码器时，这个类提供了Hadoop框架使用的入口点。两个主要感兴趣的方法是`createCompressor`和`createDeompressor`，它们都执行相同的初始化：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We have used plain text passwords to make things simpler. When using this code,
    the encryption key should be pulled from a secure store; this is discussed in
    detail further on in this chapter:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用明文密码使事情变得更简单。在使用此代码时，加密密钥应该从安全存储中提取；这在本章后面将详细讨论：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we define the encryption methods themselves:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义加密方法本身：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Each time a file is encrypted, the *Initialization Vector* (IV) should be random.
    Randomization is crucial for encryption schemes to achieve semantic security,
    a property whereby repeated usage of the scheme under the same key does not allow
    an attacker to infer relationships between segments of the encrypted message.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每次加密文件时，*初始化向量*（IV）都应该是随机的。随机化对于加密方案实现语义安全至关重要，这是一种属性，即在相同密钥下重复使用方案不允许攻击者推断加密消息段之间的关系。
- en: The main issue when implementing encryption paradigms is the mishandling of
    byte arrays. A correctly encrypted file size will usually be a multiple of the
    key size when using padding, 16 (bytes) in this case. The encryption/decryption
    process will fail with padding exceptions if the file size is incorrect. In the
    Java libraries used previously, data is fed to the internal encryption routine
    in stages, size `ciphertext.length`, which are encrypted in chunks of 16 bytes.
    If there is a remainder, this is prepended to the data given in the next update.
    If a `doFinal` call is made, the remainder is again prepended and the data is
    padded to the end of the 16 byte block before encryption, whereby the routine
    completes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现加密范例时的主要问题是对字节数组的错误处理。正确加密的文件大小通常是密钥大小的倍数，当使用填充时，本例中为16（字节）。如果文件大小不正确，加密/解密过程将因填充异常而失败。在先前使用的Java库中，数据以阶段方式提供给内部加密例程，大小为`ciphertext.length`，这些数据以16字节的块进行加密。如果有余数，这将被预先放置到下一个更新的数据中。如果进行`doFinal`调用，则余数再次被预先放置，并且数据在加密之前被填充到16字节块的末尾，从而完成例程。
- en: 'We can now proceed to complete the rest of our `CryptoCodec`, that is, the
    compress and decompress implementations that will implement the preceding code.
    These methods are located in the `CryptoCompressor` and `CryptoDecompressor` classes
    and are called by the Hadoop framework:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续完成我们的`CryptoCodec`的其余部分，即实现前面代码的压缩和解压实现。这些方法位于`CryptoCompressor`和`CryptoDecompressor`类中，并由Hadoop框架调用：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can see the full implementation for the `CryptoCodec` class in our code
    repository.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在我们的代码存储库中看到`CryptoCodec`类的完整实现。
- en: 'Now that we have our working `CryptoCodec` class, the Spark driver code is
    then straightforward:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了工作的`CryptoCodec`类，那么Spark驱动程序代码就很简单了：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And we have local disk encryption! To read an encrypted file, we simply define
    the `codec` class within the configuration:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了本地磁盘加密！要读取加密文件，我们只需在配置中定义`codec`类：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Spark will automatically use the `CryptoCodec` class when it recognizes an appropriate
    file and our implementation ensures a unique IV is used for each file; the IV
    is read from the beginning of the encrypted file.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当Spark识别到适当的文件时，将自动使用`CryptoCodec`类，并且我们的实现确保每个文件使用唯一的IV；IV是从加密文件的开头读取的。
- en: Java KeyStore
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Java KeyStore
- en: 'Depending upon your environment, the preceding code may be enough to keep your
    data secure. However, there is a flaw, in that the key used to encrypt/decrypt
    the data has to be provided in plain text. We can solve this issue by creating
    a Java KeyStore. This can be done via the command line or programmatically. We
    can implement a function to create a `JCEKS` KeyStore and add a key:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的环境，上述代码可能足以保护您的数据安全。但是，存在一个缺陷，即用于加密/解密数据的密钥必须以明文形式提供。我们可以通过创建Java KeyStore来解决这个问题。这可以通过命令行或以编程方式完成。我们可以实现一个函数来创建`JCEKS`
    KeyStore并添加一个密钥：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can achieve the same via the command line:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过命令行实现相同的功能：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And check it exists:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 检查它是否存在：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To retrieve the key from this KeyStore:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从KeyStore中检索密钥：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We have hardcoded the specifics for ease of reading, this should not be done
    in practice, as Java byte code is relatively simple to reverse engineer and, therefore,
    a malicious third party could easily obtain this secret information.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经硬编码了具体内容以便阅读，但在实践中不应该这样做，因为Java字节码相对简单，容易被逆向工程，因此，恶意第三方可以轻松获取这些秘密信息。
- en: Our secret key is now protected in a KeyStore and is only accessible using the
    KeyStore password and secret key alias. These still need to be protected, but
    would usually be stored in a database, where they are accessible only to authorized
    users.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的秘钥现在受到KeyStore的保护，只能使用KeyStore密码和秘钥别名访问。这些仍然需要受到保护，但通常会存储在数据库中，只有授权用户才能访问。
- en: 'We can now modify our `EncryptionUtils.getPassword` method to retrieve the
    `JCEKS` key rather than the plain text version, like so:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以修改我们的`EncryptionUtils.getPassword`方法，以检索`JCEKS`密钥而不是明文版本，如下所示：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now that we have a `CryptoCodec` class, we can use it throughout Spark to secure
    data anytime we need data encryption. For example, if we set the Spark configuration
    `spark.shuffle.spill.compress` to true, and set `spark.io.compression.codec` to
    `org.apache.hadoop.io.compress.CryptoCodec`, then any spill to disk will be encrypted.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了`CryptoCodec`类，我们可以在整个Spark中使用它来保护数据，无论何时我们需要数据加密。例如，如果我们将Spark配置`spark.shuffle.spill.compress`设置为true，并将`spark.io.compression.codec`设置为`org.apache.hadoop.io.compress.CryptoCodec`，那么任何溢出到磁盘的数据都将被加密。
- en: S3 encryption
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: S3加密
- en: 'HDFS encryption is great for providing what is essentially a managed service.
    If we now look at S3, this can do the same, but it also offers the ability to
    provide server-side encryption with:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS加密非常适合提供基本上是托管服务的功能。如果我们现在看S3，它也可以做到同样的功能，但它还提供了使用以下功能进行服务器端加密的能力：
- en: AWS KMS-Managed keys (SSE-KMS)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS KMS管理的密钥（SSE-KMS）
- en: Customer-Provided keys (SSE-C)
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户提供的密钥（SSE-C）
- en: Server-side encryption can provide more flexibility should you be in an environment
    where the encryption keys need to be explicitly managed.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器端加密可以提供更多灵活性，如果您处于需要明确管理加密密钥的环境中。
- en: Hardware encryption is handled within the physical disk architecture. Generally,
    this has the advantage of being quicker (due to the bespoke hardware designated
    for encryption) and being easier to secure, as physical access to the machine
    is required in order to circumvent. The downside being that all data written to
    disk is encrypted, which can result in reduced I/O performance for heavily utilized
    disks.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件加密是在物理磁盘架构内处理的。一般来说，这具有更快的优势（由于专门用于加密的定制硬件）并且更容易保护，因为需要物理访问机器才能规避。缺点是所有写入磁盘的数据都是加密的，这可能会导致高度利用的磁盘的I/O性能下降。
- en: Data in transit
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据在传输中
- en: If end-to-end security is your goal, an area that is often of concern is that
    of data in transit. This could be the reading/writing from disk or transportation
    of data around a network during analytics processing. In all cases, it is important
    to be aware of the weaknesses of your environment. It is not enough to assume
    that the framework or network administrator have covered these potential issues
    for you, even if your environment does not allow changes to be made directly.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果端到端的安全性是您的目标，一个经常关注的领域是数据在传输中的问题。这可能是从磁盘读取/写入或在分析处理期间在网络中传输数据。在所有情况下，重要的是要意识到您的环境的弱点。不能仅仅假设框架或网络管理员已经为您解决了这些潜在问题，即使您的环境不允许直接进行更改。
- en: A common mistake is to assume that data is secure when it is not human-readable.
    Although binary data itself isn't human-readable, it is often readily translated
    to the readable content and it can be captured over the network using tools such
    as Wireshark ([www.wireshark.org](http://www.wireshark.org)). So, never assume
    data security on the wire, regardless of whether it's human-readable.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的错误是假设数据在不可读时是安全的。尽管二进制数据本身不可读，但它通常可以轻松转换为可读内容，并且可以使用诸如Wireshark（[www.wireshark.org](http://www.wireshark.org)）之类的工具在网络上捕获。因此，无论数据是否可读，都不要假设数据在传输过程中是安全的。
- en: As we have seen previously, even when encrypting data on disk, we cannot assume
    it is necessarily secure. For example, if the data is encrypted at hardware level,
    then it is unencrypted as soon as it leaves the disk itself. In other words, the
    plain text is readable as it traverses the network to any machine and, therefore,
    completely open to being read by unknown entities at any point on that journey.
    Data encrypted at software level is generally not decrypted until it is used by
    the analytic, therefore, generally making it the safer option if the network topology
    is not known.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所看到的，即使在磁盘上加密数据，我们也不能假设它一定是安全的。例如，如果数据在硬件级别加密，那么一旦离开磁盘，它就会解密。换句话说，纯文本在穿越网络到任何机器时都是可读的，因此完全可以被未知实体在旅程中的任何时候读取。在软件级别加密的数据通常在被分析使用之前不会解密，因此通常是更安全的选择，如果网络拓扑未知的话。
- en: 'When considering the security of processing systems themselves, such as Spark,
    there are issues here too. Data is constantly moved between nodes with no direct
    control from the user. So, it is vital that we understand where the data may be
    available in plain text at any given time. Consider the following diagram that
    shows the interactions between entities during a Spark YARN job:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑处理系统本身的安全性时，例如Spark，这里也存在问题。数据不断在节点之间移动，用户无法直接控制。因此，我们必须了解数据在任何给定时间可能以纯文本形式可用的位置。考虑以下图表，显示了Spark
    YARN作业期间实体之间的交互：
- en: '![Data in transit](img/image_13_001.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![数据在传输中](img/image_13_001.jpg)'
- en: We can see that every connection transmits and receives data. Spark input data
    is transferred via broadcast variables and all channels support encryption apart
    from UI and local shuffle/cache files (see JIRA SPARK-5682 for more information).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每个连接都传输和接收数据。Spark输入数据通过广播变量传输，所有通道都支持加密，除了UI和本地shuffle/cache文件（有关更多信息，请参见JIRA
    SPARK-5682）。
- en: Furthermore, there is a weakness here, in that cached files are stored as plain
    text. The fix is either to implement the preceding solution, or to set up YARN
    local directories to point to local encrypted disks. To do this, we need to ensure
    that `yarn.nodemanager.local-dirs` in yarn-default.xml are encrypted directories
    on all DataNodes, either using a commercial product or hosting these directories
    on encrypted disks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这里存在一个弱点，即缓存文件以纯文本形式存储。修复方法要么是实施前面的解决方案，要么是设置YARN本地目录指向本地加密磁盘。为此，我们需要确保yarn-default.xml中的`yarn.nodemanager.local-dirs`是所有DataNodes上的加密目录，可以使用商业产品或将这些目录托管在加密磁盘上。
- en: Now that we have considered the data as a whole, we should address the individual
    parts of the data itself. It is very possible that data may contain sensitive
    information, for example, names, addresses, and credit card numbers. There are
    a number of ways to handle this type of information.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经考虑了整体数据，我们应该处理数据本身的各个部分。很可能数据中包含敏感信息，例如姓名、地址和信用卡号码。有许多处理此类信息的方法。
- en: Obfuscation/Anonymizing
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混淆/匿名化
- en: 'With obfuscation, the sensitive parts of the data are transformed into something
    that can never be linked back to the original content - providing security through
    obscurity. For example, a CSV file containing fields: `Forename`, `Surname`, `Address
    line 1`, `Address line 2`, `Postcode`, `Phone Number`, `Credit Card Number` might
    be obfuscated like so:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过混淆，数据的敏感部分被转换成永远无法追溯到原始内容的形式 - 通过模糊提供安全性。例如，包含字段：“名”，“姓”，“地址行1”，“地址行2”，“邮政编码”，“电话号码”，“信用卡号”的CSV文件可能会被混淆如下：
- en: Original
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原文
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Obfuscated
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Obfuscating data is great for analytics, as it protects sensitive data while
    still allowing useful calculations, such as counting completed fields. We can
    also be intelligent about the way we obfuscate the data in order to preserve certain
    details while protecting others. For example, a credit card number: `4659 42XX
    XXXX XXXX` can give us a surprising amount of information, as the first six digits
    of payments cards, called the **Bank Identification Number** (**BIN**), tell us
    the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 数据混淆对于分析非常有用，因为它在保护敏感数据的同时仍允许有用的计算，比如计算完成的字段数量。我们还可以在混淆数据的方式上做得更智能，以保留某些细节同时保护其他细节。例如，信用卡号：`4659
    42XX XXXX XXXX`可以给我们提供大量信息，因为付款卡的前六位数字，称为**银行识别号**（**BIN**），告诉我们以下信息：
- en: BIN 465942
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BIN 465942
- en: 'Card brand: VISA'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡品牌：VISA
- en: 'Issuing bank: HSBC'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发卡银行：汇丰银行
- en: 'Card type: debit'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡类型：借记卡
- en: 'Card level: classic'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡级别：经典
- en: ISO country number 826 (Great Britain)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ISO国家编号826（英国）
- en: Data obfuscation should not necessarily be random, but should be carefully tailored
    to ensure that sensitive data is definitely removed. The definition of sensitive
    will entirely depend upon the requirements. In the preceding example, it may be
    very useful to be able to summarize the distribution of customer payments cards
    by type, or it could be deemed as sensitive information that should be removed.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 数据混淆不一定要是随机的，但应该经过精心设计，以确保敏感数据被彻底删除。敏感的定义将完全取决于需求。在前面的例子中，能够按类型总结客户付款卡的分布可能非常有用，或者可以被视为应该删除的敏感信息。
- en: Another phenomenon to be aware of here, as you may recall from previous chapters,
    is data aggregation. For example, if we know that the name of the individual is
    John Smith AND that his credit card starts with 465942, then we know that John
    Smith has an account with HSBC in the UK, a great piece of information for a malicious
    entity to start building on. Therefore, care must be taken to ensure that the
    right amount of obfuscation is applied, bearing in mind that we can never recover
    the original data, unless we have another copy stored elsewhere. Non-recovery
    of data can be a costly event, so data obfuscation should be implemented wisely.
    Indeed, if storage allows, it is not unreasonable to want to store several versions
    of data, each with a different level of obfuscation and different levels of access.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个现象需要注意，正如您可能从之前的章节中记得的那样，那就是数据聚合。例如，如果我们知道个人的姓名是约翰·史密斯，并且他的信用卡号以465942开头，那么我们就知道约翰·史密斯在英国汇丰银行有一个账户，这对于一个恶意实体来说是一个很好的信息基础。因此，必须小心确保应用了正确数量的混淆，要牢记我们永远无法恢复原始数据，除非我们在其他地方有另一个副本存储。数据的不可恢复可能是一个昂贵的事件，因此应明智地实施数据混淆。确实，如果存储允许，想要存储几个版本的数据，每个版本都有不同程度的混淆和不同级别的访问，这并不是不合理的。
- en: When thinking about implementing this in Spark, it is most likely that we will
    have a scenario where there are many input records that require transformation.
    Thus, our starting point is to write something that works on a single record,
    and then wrap this in an RDD so that the functions can be run across many records
    in parallel.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑在Spark中实现这一点时，最有可能的情况是我们将有许多需要转换的输入记录。因此，我们的起点是编写一个适用于单个记录的函数，然后将其包装在RDD中，以便可以并行运行这些函数。
- en: 'Taking our preceding example, let''s express its schema in Scala as an enumeration.
    Along with the definition, we''ll include in our `Enumeration` class information
    about how any particular field should be obfuscated:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们前面的例子为例，让我们在Scala中将其架构表达为一个枚举。除了定义之外，我们还将在我们的`Enumeration`类中包含有关如何混淆任何特定字段的信息：
- en: '*x*, *y* mask the char positions from *x* to *y*'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*，*y*掩盖了从*x*到*y*的字符位置'
- en: '`0`, `len` mask the entire field from 0 to the length of the field text'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`，`len`掩盖从字段文本的0到字段长度的整个字段'
- en: '`prefix` mask everything before the last space character'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix`掩盖最后一个空格字符之前的所有内容'
- en: '`suffix` mask everything after the first space character'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suffix`掩盖第一个空格字符之后的所有内容'
- en: '`""` do nothing'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`""`什么都不做'
- en: 'This information is encoded in the enumeration as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息在枚举中编码如下：
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we can split the input string and write a function that applies the correct
    obfuscation argument to the correct field:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以拆分输入字符串并编写一个函数，将正确的混淆参数应用于正确的字段：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: To keep things simple, we have hardcoded some of the items that you may want
    to change later, for example the split argument (`,`), and also made the obfuscation
    symbol constant in all cases (`X`).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简单，我们已经硬编码了一些您可能希望以后更改的项目，例如分割参数（`,`），并且在所有情况下都使混淆符号保持不变（`X`）。
- en: 'And finally, the actual obfuscation code:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，实际的混淆代码：
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Again, we have kept things simple and do not go to great lengths to check for
    exceptions or edge cases. Here is a practical example:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们保持了简单，没有过多地检查异常或边缘情况。这里是一个实际的例子：
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'It provides the desired result:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了期望的结果：
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This handy bit of code provides a great basis for obfuscating at scale. We
    can easily extend it to more complicated scenarios, such as the obfuscation of
    different parts of the same field. For example, by changing `StringObfuscator`,
    we could mask the house number and road name differently in the `Address Line
    1` field:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方便的代码片段为大规模混淆提供了一个很好的基础。我们可以很容易地将其扩展到更复杂的场景，比如同一字段的不同部分的混淆。例如，通过更改`StringObfuscator`，我们可以在`地址行1`字段中以不同的方式掩盖门牌号和街道名称：
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Of course, if you were wishing to scale this out for many different use cases,
    you could also apply the strategy pattern over `StringObfuscator` to allow an
    obfuscation function to be provided at runtime.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果您希望将其扩展到许多不同的用例，您也可以在`StringObfuscator`上应用策略模式，以允许在运行时提供混淆函数。
- en: 'A key software engineering technique, the strategy pattern is described here:
    [https://sourcemaking.com/design_patterns/strategy](https://sourcemaking.com/design_patterns/strategy).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一种关键的软件工程技术，策略模式在这里描述：[https://sourcemaking.com/design_patterns/strategy](https://sourcemaking.com/design_patterns/strategy)。
- en: At this point, it's worth thinking about obfuscating data using an algorithm,
    such as one-way hashing function or digest, rather than simply replacing with
    characters (`XXX`). This is a versatile technique and is applicable in a wide
    range of use cases. It relies on the computational complexity of performing an
    inverse calculation for some calculations, such as finding factors and modular
    squaring, meaning that once applied they are impractical to reverse. However,
    care should be taken when using hashes because, despite digest calculations being
    NP-complete, there are some scenarios where hashing is still susceptible to compromise
    using implicit knowledge. For example, the predictability of credit card numbers
    means that they have been proved to be cracked quickly by a brute-force approach,
    even using MD5 or SHA-1 hashes.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，值得考虑使用算法对数据进行混淆，例如单向哈希函数或摘要，而不仅仅是用字符（`XXX`）替换。这是一种多功能的技术，在各种用例中都适用。它依赖于执行某些计算的逆计算的计算复杂性，例如找因子和模平方，这意味着一旦应用，它们是不可逆的。然而，在使用哈希时应该小心，因为尽管摘要计算是NP完全的，但在某些情况下，哈希仍然容易受到使用隐含知识的威胁。例如，信用卡号的可预测性意味着它们已经被证明可以很快地通过穷举法破解，即使使用MD5或SHA-1哈希。
- en: For more information, refer to [https://www.integrigy.com/security-resources/hashing-credit-card-numbers-unsafe-application-practices](https://www.integrigy.com/security-resources/hashing-credit-card-numbers-unsafe-application-practices).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅[https://www.integrigy.com/security-resources/hashing-credit-card-numbers-unsafe-application-practices](https://www.integrigy.com/security-resources/hashing-credit-card-numbers-unsafe-application-practices)。
- en: Masking
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩码
- en: Data masking is about creating a functional substitute of data while ensuring
    that important content is hidden. This is another anonymization method whereby
    the original contents are lost once the masking process has taken place. Therefore,
    it is important to ensure that the changes are carefully planned, as they are
    effectively final. Of course, an original version of the data could be stored
    for emergencies, but this would add additional burden to the security considerations.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 数据掩码是关于创建数据的功能替代，同时确保重要内容被隐藏。这是另一种匿名化方法，原始内容一旦经过掩码处理就会丢失。因此，确保变更经过仔细规划是非常重要的，因为它们实际上是最终的。当然，原始版本的数据可以存储以备紧急情况，但这会给安全考虑增加额外的负担。
- en: 'Masking is a simple process and it relies on generating random data to replace
    any sensitive data. For example, applying a mask to our previous example gives:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码是一个简单的过程，它依赖于生成随机数据来替换任何敏感数据。例如，对我们之前的例子应用掩码会得到：
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We now have a row, which is functionally equivalent to the original data. We
    have a full name, address, phone number, and credit card number, but they are
    *different*, such that they cannot be linked to the original.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一行数据，它在功能上等同于原始数据。我们有一个全名、地址、电话号码和信用卡号码，但它们是*不同的*，因此它们不能与原始数据关联起来。
- en: Partial masking is very useful for processing purposes, as we can keep some
    data while masking the rest. In this way, we can perform a number of data auditing
    tasks that are not necessarily possible using obfuscation. For example, we could
    mask data actually present, allowing us to guarantee that populated fields will
    always be valid whilst also being able to detect empty fields.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 部分掩码对于处理目的非常有用，因为我们可以保留一些数据，同时掩盖其余部分。通过这种方式，我们可以执行许多数据审计任务，这些任务可能无法通过混淆来实现。例如，我们可以掩盖实际存在的数据，从而可以保证填充字段始终有效，同时也能够检测到空字段。
- en: It is also possible to use complete masking in order to generate mock data without
    having seen the original data at all. In this case, data could be completely generated,
    say, for testing or profiling purposes.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用完全掩码来生成模拟数据，而根本没有看到原始数据。在这种情况下，数据可以完全生成，比如用于测试或分析的目的。
- en: Whatever the use case, care should be taken when using masking, as it is possible
    to unwittingly insert real information into a record. For example, `Simon Jones`
    might actually be a real person. This being the case, it is certainly a good idea
    to store the data provenance, that is, the source and historical record for all
    data held. Therefore, should the real "`Simon, Jones`" submit a **request for
    information** (**RFI**) under the data protection act you have the necessary information
    in order to provide the relevant justifications.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 无论用例如何，使用掩码时都应该小心，因为可能会无意中将真实信息插入记录中。例如，`Simon Jones`可能实际上是一个真实的人。在这种情况下，存储数据来源和历史记录是一个好主意。因此，如果真正的“Simon,
    Jones”根据数据保护法案提交了**信息请求**（**RFI**），您就有必要的信息来提供相关的理由。
- en: 'Let''s extend our previously built code to implement a basic masking approach
    using a completely random selection. We have seen that the masking method requires
    that we replace fields with some meaningful alternative. To have something working
    quickly we could simply provide arrays of alternatives:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们扩展我们之前构建的代码，使用完全随机选择来实现基本的掩码方法。我们已经看到，掩码方法要求我们用一些有意义的替代内容替换字段。为了快速实现一些功能，我们可以简单地提供替代内容的数组：
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Later, we can extend these to read from a file containing many more alternatives.
    We can even replace multiple fields in one go using a composite mask:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，我们可以扩展这些内容，从包含更多替代方案的文件中读取。我们甚至可以使用复合掩码一次性替换多个字段：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The processing code is then straightforward:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，处理代码就很简单了：
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can define a `RandomCCNumber` function to generate a random credit card
    number. Here''s a simple function that provides four sets of randomly generated
    integers using recursion:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义一个`RandomCCNumber`函数来生成一个随机的信用卡号码。下面是一个简单的函数，它使用递归提供四组随机生成的整数：
- en: '[PRE29]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Putting this code together and running against our original example, gives
    the following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些代码放在一起，并对我们的原始示例运行，得到以下结果：
- en: '[PRE30]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '[PRE31]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Alternatively:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 或者：
- en: '[PRE32]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Again, there are many ways we could develop this code. For example, we could
    generate a credit card number that is valid under the BIN scheme, or we could
    ensure that name selection doesn't randomly choose the same name that it's trying
    to replace. However, the outlined framework is presented here as a demonstration
    of the technique and can be easily extended and generalized to account for any
    additional requirements you might have.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以以许多方式开发这段代码。例如，我们可以生成一个在BIN方案下有效的信用卡号码，或者确保名称选择不会随机选择与其尝试替换的相同名称。然而，所述的框架在这里作为该技术的演示呈现，并且可以很容易地扩展和泛化以满足您可能有的任何额外要求。
- en: Tokenization
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记化
- en: '**Tokenization** is the process of substituting sensitive information with
    a token that can be later used to retrieve the actual data, if required, subject
    to the relevant authentication and authorization. Using our previous example,
    tokenized text might look like:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**标记化**是用标记替换敏感信息的过程，如果需要，可以稍后使用该标记检索实际数据，前提是经过相关的认证和授权。使用我们之前的示例，标记化文本可能如下所示：'
- en: '[PRE33]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Where the bracketed values are tokens that can be exchanged for the actual values
    when the requesting user fulfills the correct security criteria. This method is
    the most secure of those discussed and allows us to recover the exact original
    underlying data. However, it comes with a significant processing overhead to tokenize
    and detokenize data and, of course, the tokenizer system will require administration
    and careful maintenance.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 其中括号中的值是可以在请求用户满足正确的安全标准时用于交换实际值的标记。这种方法是讨论过的方法中最安全的方法，允许我们恢复精确的原始基础数据。然而，标记化和去标记化数据需要大量的处理开销，当然，标记系统需要管理和仔细维护。
- en: 'This also means that there is a single point of failure in the tokenization
    system itself and, therefore, it must be subject to the important security processes
    we have discussed: audit, authentication, and authorization.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着标记化系统本身存在单点故障，因此必须遵守我们讨论过的重要安全流程：审计、认证和授权。
- en: Due to the complexities and security issues with tokenization, the most popular
    implementations are commercial products covered by extensive patents. The bulk
    of the effort with this type of system, particularly with big data, is in ensuring
    that the tokenizer system can provide a full, completely secure, robust and scalable
    service at very high levels of throughput. We can, however, build a simple tokenizer
    using Accumulo. In [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"),
    *Building Communities*, there is a section on setting up Apache Accumulo such
    that we can use cell-level security. Apache Accumulo, is an implementation of
    the Google BigTable paper, but it adds the additional security functionality.
    This means that a user can have all of the advantages of loading and retrieving
    data in parallel and at scale, but also be able to control the visibility of that
    data to a very fine degree. The chapter describes all of the information required
    to set up an instance, configure it for multiple users, and load and retrieve
    data with the required security labels (achieved through Accumulo Mutations).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于标记化的复杂性和安全问题，最流行的实现是商业产品，受到广泛的专利保护。这种类型的系统的大部分工作，特别是在大数据方面，是确保标记化系统能够以非常高的吞吐量提供完全安全、稳健和可扩展的服务。然而，我们可以使用Accumulo构建一个简单的标记化器。在[第7章](ch07.xhtml
    "第7章。建立社区")，*建立社区*中，有一个关于设置Apache Accumulo以便我们可以使用单元级安全的部分。Apache Accumulo是Google
    BigTable论文的实现，但它增加了额外的安全功能。这意味着用户可以在并行和规模上加载和检索数据的所有优势，同时能够以非常精细的程度控制数据的可见性。该章描述了设置实例、为多个用户配置实例以及通过Accumulo
    Mutations加载和检索数据所需的所有信息。
- en: 'For our purposes, we want to take a field and create a token; this could be
    a GUID, hash, or some other object. We can then write an entry to Accumulo using
    the token as the RowID and the field data itself as the contents:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们希望获取一个字段并创建一个标记；这可以是GUID、哈希或其他对象。然后，我们可以使用标记作为RowID并将字段数据本身作为内容写入Accumulo：
- en: '[PRE34]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We then write the `uuid` to the related field in the output data. When tokenized
    data is read back in, anything starting with `[` is assumed to be a token and
    the Accumulo read procedure is used to obtain the original field data, assuming
    that the user invoking the Accumulo read has the correct permissions:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将`uuid`写入输出数据中的相关字段。当读取标记化数据时，任何以`[`开头的内容都被假定为标记，并使用Accumulo读取过程来获取原始字段数据，假设调用Accumulo读取的用户具有正确的权限：
- en: '[PRE35]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Using a Hybrid approach
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用混合方法
- en: 'Obfuscation and masking can be used very effectively together to maximize the
    advantages of both methods. Using this hybrid approach, our example might become:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆和掩码可以有效地结合使用，以最大化两种方法的优势。使用这种混合方法，我们的示例可能变成：
- en: '[PRE36]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Using a combination of masking and tokenization is the emerging banking standard
    for securing credit card transactions. The **Primary Account Number** (**PAN**)
    is replaced with a token made up of a unique, randomly generated sequence of numbers,
    alphanumeric characters, or a combination of a truncated PAN and a random alphanumeric
    sequence. This enables the information to be processed as if it were the actual
    data, for example, audit checks or data quality reports, but it does not allow
    the true information to exist in plain text. Should the original information be
    required, the token can be used to request it and the user is only successful
    if they meet the authorization and authentication requirements.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用掩码和标记化的组合是保护信用卡交易的新兴银行标准。**主帐号号码**（**PAN**）被替换为一个由一组唯一的、随机生成的数字、字母数字字符或截断的PAN和随机字母数字序列组成的标记。这使得信息可以被处理，就好像它是实际数据，例如审计检查或数据质量报告，但它不允许真实信息以明文存在。如果需要原始信息，可以使用标记来请求，只有在满足授权和认证要求的情况下用户才能成功。
- en: 'We can refactor our code to perform this task; we will define a new function
    that mixes obfuscation and masking together:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重构我们的代码来执行这个任务；我们将定义一个新的函数，将混淆和掩码混合在一起：
- en: '[PRE37]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Once again, our example becomes:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们的例子变成了：
- en: '[PRE38]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: As with all tokenization, you will need to be careful to avoid side effects
    with the generated data, for example, `0264` is not a real BIN code. Again, requirements
    will dictate as to whether this is an issue, that is, it's not an issue if we
    are only trying to ensure that the field is populated in the correct format.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有标记化一样，您需要小心避免生成数据的副作用，例如，“0264”不是一个真实的BIN代码。再次，要求将决定这是否是一个问题，也就是说，如果我们只是想确保字段以正确的格式填充，那么这就不是一个问题。
- en: 'In order to run any of these processes at scale, we simply need to wrap them
    in an RDD:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以规模运行任何这些过程，我们只需要将它们包装在一个RDD中：
- en: '[PRE39]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Data disposal
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处置
- en: Secure data should have an agreed life cycle. This will be set by a data authority
    when working in a commercial context, and it will dictate what state the data
    should be in at any given point during that life cycle. For example, a particular
    dataset may be labeled as *sensitive - requires encryption* for the first year
    of its life, followed by *private - no encryption*, and finally, *disposal*. The
    lengths of time and the rules applied will entirely depend upon the organization
    and the data itself - some data expires after just a few days, some after fifty
    years. The life cycle ensures that everyone knows exactly how the data should
    be treated, and it also ensures that older data is not needlessly taking up valuable
    disk space or breaching any data protection laws.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 安全数据应该有一个约定的生命周期。在商业环境中工作时，这将由数据管理机构确定，并且它将决定数据在生命周期的任何给定时刻应处于什么状态。例如，特定数据集在其生命周期的第一年可能被标记为*敏感
    - 需要加密*，然后是*私人 - 无加密*，最后是*处置*。时间长度和适用的规则完全取决于组织和数据本身 - 一些数据在几天后到期，一些在五十年后到期。生命周期确保每个人都清楚地知道数据应该如何处理，它还确保旧数据不会不必要地占用宝贵的磁盘空间或违反任何数据保护法律。
- en: 'The correct disposal of data from secure systems is perhaps one of the most
    mis-understood areas of data security. Interestingly, it doesn''t always involve
    a complete and/or destructive removal process. Examples where no action is required
    include:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从安全系统中正确处置数据可能是数据安全中最被误解的领域之一。有趣的是，这并不总是涉及完全和/或破坏性的移除过程。不需要采取任何行动的例子包括：
- en: If data is simply out of date, it may no longer hold any intrinsic value - a
    good example is government records that are released to the public after their
    expiry date; what was top secret during World War Two is generally of no sensitivity
    now due to the elapsed time.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据只是过时了，可能不再具有任何内在价值 - 一个很好的例子是政府记录在过期后向公众发布；在二战期间是绝密的东西现在由于经过的时间通常不再具有敏感性。
- en: If data is encrypted, and no longer required, simply throw the keys away!
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据已加密，并且不再需要，只需丢弃密钥！
- en: 'As opposed to the examples where some effort is required, leading to the potential
    for mistakes to be made:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 与需要一些努力的例子相反，导致可能会出现错误：
- en: '**Physical destruction**: we often hear of disks being destroyed with a hammer
    or similar, even this is unsafe if not completed thoroughly.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理破坏：我们经常听说使用锤子或类似工具摧毁硬盘，即使这样做也是不安全的，如果不彻底完成的话。
- en: '**Multiple writes**: relies upon writing over data blocks multiple times to
    ensure that the original data is physically overwritten. Utilities such as shred
    and scrub on Linux achieve this; however, they still have limited effectiveness
    depending upon the underlying filesystem. For example, RAID and cache type systems
    will not necessarily be overwritten beyond all retrieval with these tools. Overwriting
    tools should be treated with caution and used only with a complete understanding
    of their limitations.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多次写入：依赖多次写入数据块以确保原始数据被物理覆盖。Linux上的shred和scrub等实用程序可以实现这一点；然而，它们在底层文件系统上的效果有限。例如，RAID和缓存类型系统不一定会被这些工具覆盖以至于无法检索。覆盖工具应该谨慎对待，并且只有在完全了解其局限性的情况下才能使用。
- en: When you secure your data, start thinking about your disposal strategy. Even
    if you are not made aware of any organizational rules in existence (in a commercial
    environment), you should still be thinking about how you are going to make sure
    the data is unrecoverable when access is no longer required.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当您保护您的数据时，开始考虑您的处置策略。即使您没有意识到存在任何组织规则（在商业环境中），您仍应考虑在不再需要访问时如何确保数据不可恢复。
- en: Kerberos authentication
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kerberos认证
- en: Many installations of Apache Spark use Kerberos to provide security and authentication
    to services such as HDFS and Kafka. It's also especially common when integrating
    with third-party databases and legacy systems. As a commercial data scientist,
    at some point, you'll probably find yourself in a situation where you'll have
    to work with data in a Kerberized environment, so, in this part of the chapter,
    we'll cover the basics of Kerberos - what it is, how it works, and how to use
    it.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 许多Apache Spark的安装使用Kerberos为HDFS和Kafka等服务提供安全性和认证。在与第三方数据库和传统系统集成时也特别常见。作为一名商业数据科学家，您可能会发现自己在必须在Kerberized环境中处理数据的情况下，因此，在本章的这一部分，我们将介绍Kerberos的基础知识
    - 它是什么，它是如何工作的，以及如何使用它。
- en: 'Kerberos is a third-party authentication technique that''s particularly useful
    where the primary form of communication is over a network, which makes it ideal
    for Apache Spark. It''s used in preference to alternative methods of authentication,
    for example, username and password, because it provides the following benefits:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Kerberos是一种第三方认证技术，特别适用于主要通信方式是通过网络的情况，这使其非常适合Apache Spark。它被用于替代其他认证方法，例如用户名和密码，因为它提供以下好处：
- en: No passwords are stored in plain text in application configuration files
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用程序配置文件中不以明文存储密码
- en: Facilitates centralized management of services, identities, and permissions
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 促进了服务、身份和权限的集中管理
- en: Establishes a mutual trust, so both entities are identified
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立相互信任，因此两个实体都被识别
- en: Prevents spoofing - trust is only established temporarily, just for a timed
    session, meaning replay attacks are not possible, but sessions are renewable for
    convenience
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止欺骗 - 信任仅在有限时间内建立，仅用于定时会话，这意味着无法进行重放攻击，但会话可以为了方便而续订
- en: Let's look at how it works with Apache Spark.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何与Apache Spark一起工作的。
- en: 'Use case 1: Apache Spark accessing data in secure HDFS'
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例1：Apache Spark访问安全HDFS中的数据
- en: 'In the most basic use case, once you''re logged on to an edge node (or similar)
    of your secure Hadoop cluster and before running your Spark program, Kerberos
    must be initialized. This is done by using the `kinit` command that comes with
    Hadoop and entering your user''s password when prompted:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在最基本的用例中，一旦您登录到安全Hadoop集群的边缘节点（或类似节点）并在运行Spark程序之前，必须初始化Kerberos。这是通过使用Hadoop提供的`kinit`命令并在提示时输入用户密码来完成的。
- en: '[PRE40]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: At this point, you will be fully authenticated and able to access any data within
    HDFS, subject to the standard permissions model.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您将完全经过身份验证，并且可以访问HDFS中的任何数据，受标准权限模型的约束。
- en: 'So, the process seems simple enough, let''s take a deeper look at what happened
    here:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个过程似乎足够简单，让我们更深入地看看这里发生了什么：
- en: When the `kinit` command runs, it immediately sends a request to the Kerberos
    **key distribution centre** (**KDC**), to acquire a **ticket granting ticket**
    (**TGT**). The request is sent in plain text, and it essentially contains what
    is known as the **principal**, which is basically the "username@kerberosdomain"
    in this case (you can find out this string using the `klist` command). The **Authentication
    Server (AS)** responds to this request, with a TGT that has been signed using
    client's private key, a key that was shared ahead of time and is already known
    to the AS. This ensures secure transfer of the TGT.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当运行`kinit`命令时，它立即向Kerberos密钥分发中心（KDC）发送请求，以获取票据授予票据（TGT）。该请求以明文形式发送，基本上包含了所谓的主体，这在本例中基本上是“username@kerberosdomain”（可以使用`klist`命令找到此字符串）。认证服务器（AS）对此请求做出响应，使用客户端的私钥签署了TGT，这是事先共享并已知于AS的密钥。这确保了TGT的安全传输。
- en: The TGT is cached locally on the client, along with a **Keytab** file - which
    is a container for Kerberos keys and it is accessible to any Spark processes running
    as the same user.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TGT与Keytab文件一起在客户端本地缓存，Keytab文件是Kerberos密钥的容器，对于以相同用户身份运行的任何Spark进程都是可访问的。
- en: Next, when the spark-shell is started, Spark uses the cached TGT to request
    that the **Ticket Granting Server** (**TGS**), provide a **session ticket** for
    accessing the HDFS service. This Ticket is signed using the HDFS NameNode's private
    key. In this way, the secure transfer of the Ticket is guaranteed, ensuring that
    only the NameNode can read it.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，当启动spark-shell时，Spark使用缓存的TGT请求票据授予服务器（TGS）提供用于访问HDFS服务的会话票据。此票据使用HDFS NameNode的私钥进行签名。通过这种方式，保证了票据的安全传输，确保只有NameNode可以读取它。
- en: Armed with a ticket, Spark attempts to retrieve a **delegation toke**n from
    the NameNode. The purpose of this token is to prevent a flood of requests into
    the TGT when the executors start reading data (as the TGT was not designed with
    big data in mind!), but it also helps overcome problems Spark has with delayed
    execution times and ticket session expiry.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有票据后，Spark尝试从NameNode检索委托令牌。此令牌的目的是防止执行程序开始读取数据时向TGT发送大量请求（因为TGT并不是为大数据而设计的！），但它还有助于克服Spark在延迟执行时间和票据会话过期方面的问题。
- en: Spark ensures that all executors have access to the delegation token by placing
    it on the distributed cache so that it's available as a YARN local file.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark确保所有执行程序都可以访问委托令牌，方法是将其放在分布式缓存中，以便作为YARN本地文件可用。
- en: When each executor makes a request to the NameNode for access to a block stored
    in HDFS, it passes across the delegation token it was given previously. The NameNode
    replies with the location of the block, along with a **block token** that is signed
    by the NameNode with a private secret. This key is shared by all of the DataNodes
    in the cluster and is only known by them. The purpose of this added block token
    is to ensure that the access is fully secured and, as such, it is only issued
    to authenticated users and it can only be read by verified DataNodes.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当每个执行程序向NameNode请求访问存储在HDFS中的块时，它传递了先前获得的委托令牌。NameNode回复块的位置，以及由NameNode使用私密签名的块令牌。此密钥由集群中的所有DataNode共享，并且只有它们知道。添加块令牌的目的是确保访问完全安全，并且仅发放给经过身份验证的用户，并且只能由经过验证的DataNode读取。
- en: The last step is for the executors to supply the block token to the relevant
    DataNode and receive the requested block of data.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是执行程序向相关的DataNode提供块令牌并接收所请求的数据块。
- en: '![Use case 1: Apache Spark accessing data in secure HDFS](img/image_13_002.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![用例1：Apache Spark访问安全HDFS中的数据](img/image_13_002.jpg)'
- en: 'Use case 2: extending to automated authentication'
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例2：扩展到自动身份验证
- en: By default, Kerberos tickets last for 10 hours and then expire, making them
    useless after this time, but they can be renewed. Therefore, when executing long-running
    Spark jobs or Spark Streaming Jobs (or jobs where a user is not directly involved
    and `kinit` cannot be run manually), it is possible to pass enough information
    upon starting a Spark process in order to automate the renewal of tickets issued
    during the previously discussed handshake.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Kerberos票据的有效期为10小时，然后过期，此后变得无用，但可以进行续订。因此，在执行长时间运行的Spark作业或Spark流作业（或者用户没有直接参与且无法手动运行`kinit`的作业）时，可以在启动Spark进程时传递足够的信息，以自动续订在先前讨论的握手期间发放的票据。
- en: 'This is done by passing in the location of the keytab file and associated principal
    using the command line options provided, like so:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用提供的命令行选项传递密钥表文件的位置和相关主体来完成此操作，如下所示：
- en: '[PRE41]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: When attempting to execute a long running job as your local user, the principal
    name can be found using `klist` otherwise, dedicated **service principals** can
    be configured within Kerberos using `ktutils` and `ktadmin`.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 当尝试以本地用户身份执行长时间运行的作业时，可以使用`klist`找到主体名称，否则，可以使用`ktutils`和`ktadmin`在Kerberos中配置专用的**服务主体**。
- en: 'Use case 3: connecting to secure databases from Spark'
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例3：从Spark连接到安全数据库
- en: When working in a corporate setting, it may be necessary to connect to a third-party
    database that has been secured with Kerberos, such as PostgreSQL or Microsoft
    SQLServer.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业环境中工作时，可能需要连接到使用Kerberos进行安全保护的第三方数据库，例如PostgreSQL或Microsoft SQLServer。
- en: In this situation, it is possible to use JDBC RDD to connect directly to the
    database and have Spark issue an SQL query to ingest data in parallel. Care should
    be taken when using this approach, as traditional databases are not built for
    high levels of parallelism, but if used sensibly, it is sometimes a very useful
    technique, particularly well-suited to rapid data exploration.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可以使用JDBC RDD直接连接到数据库，并让Spark并行发出SQL查询以摄取数据。在使用这种方法时需要小心，因为传统数据库并不是为高并行性而构建的，但如果使用得当，有时这是一种非常有用的技术，特别适合快速数据探索。
- en: Firstly, you will need the native JDBC drivers for your particular database
    - here we've used Microsoft SQLServer as an example, but drivers should be available
    for all modern databases that support Kerberos (see RFC 1964).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要为特定数据库获取本机JDBC驱动程序 - 在这里，我们以Microsoft SQLServer为例，但是对于支持Kerberos的所有现代数据库应该都可以获得驱动程序（参见RFC
    1964）。
- en: 'You''ll need to configure spark-shell to use the JDBC drivers on startup, like
    so:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在启动时配置spark-shell以使用JDBC驱动程序，如下所示：
- en: '[PRE42]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, in the shell, type or paste the following (replacing the environment
    specific variables, which are highlighted):'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在shell中，输入或粘贴以下内容（替换环境特定变量，这些变量已突出显示）：
- en: '[PRE43]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Spark runs the SQL passed into the constructor of `JdbcRDD`, but instead of
    running it as a single query, it is able to chunk it using the last three parameters
    as a guide.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Spark运行传递给`JdbcRDD`构造函数的SQL，但不是作为单个查询运行，而是使用最后三个参数作为指南进行分块。
- en: 'So, in this example, in fact, four queries would be run in parallel:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个例子中，实际上会并行运行四个查询：
- en: '[PRE44]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As you can see, Kerberos is a huge and complicated subject. The level of knowledge
    required for a data scientist can vary depending upon the role. Some organizations
    will have a DevOps team to ensure that everything is implemented correctly. However,
    in the current climate, where there is a big skills shortage in the market, it
    could well be the case that data scientists will have to solve these issues themselves.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，Kerberos是一个庞大而复杂的主题。数据科学家所需的知识水平可能会因角色而异。一些组织将拥有一个DevOps团队来确保一切都得到正确实施。然而，在当前市场上存在技能短缺的情况下，数据科学家可能不得不自己解决这些问题。
- en: Security ecosystem
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全生态系统
- en: We will conclude with a brief rundown of some of the popular security tools
    we may encounter while developing with Apache Spark - and some advice about when
    to use them.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将简要介绍一些在使用Apache Spark开发时可能遇到的流行安全工具，并提供一些建议何时使用它们。
- en: Apache sentry
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Sentry
- en: As the Hadoop ecosystem grows ever larger, products such as Hive, HBase, HDFS,
    Sqoop, and Spark all have different security implementations. This means that
    duplicate policies are often required across the product stack in order to provide
    the user with a seamless experience, as well as enforce the overarching security
    manifest. This can quickly become complicated and time consuming to manage, which
    often leads to mistakes and even security breaches (whether intentional or otherwise).
    Apache Sentry pulls many of the mainstream Hadoop products together, particularly
    with Hive/HS2, to provide fine-grained (up to column level) controls.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Hadoop生态系统的不断扩大，产品如Hive、HBase、HDFS、Sqoop和Spark都有不同的安全实现。这意味着通常需要在产品堆栈中重复使用策略，以便为用户提供无缝体验，并强制执行全面的安全清单。这很快就会变得复杂和耗时，通常会导致错误甚至安全漏洞（无论是有意还是无意）。Apache
    Sentry将许多主流的Hadoop产品整合在一起，特别是与Hive/HS2，以提供细粒度（高达列级）的控制。
- en: Using ACLs is simple, but high maintenance. The setting of permissions for a
    large number of new files and amending umasks is very cumbersome and time consuming.
    As abstractions are created, authorization becomes more complicated. For example,
    the fusing of files and directories can become tables, columns, and partitions.
    Therefore, we need a trusted entity to enforce access control. Hive has a trusted
    service - **HiveServer2** (**HS2**), which parses queries and ensures that users
    have access to the data they are requesting. HS2 runs as a trusted user with access
    to the whole data warehouse. Users don't run code directly in HS2, so there is
    no risk of code bypassing access checks.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ACL很简单，但维护成本很高。为大量新文件设置权限和修改umask非常繁琐和耗时。随着抽象的创建，授权变得更加复杂。例如，文件和目录的融合可以变成表、列和分区。因此，我们需要一个可信的实体来执行访问控制。Hive有一个可信的服务
    - **HiveServer2**（**HS2**），它解析查询并确保用户可以访问他们请求的数据。HS2以可信用户的身份运行，可以访问整个数据仓库。用户不直接在HS2中运行代码，因此不存在代码绕过访问检查的风险。
- en: To bridge Hive and HDFS data, we can use the Sentry HDFS plugin, which synchronizes
    HDFS file permissions with higher level abstractions. For example, permissions
    to read a table = permission to read table's files and, similarly, permissions
    to create a table = permission to write to a database's directory. We still use
    HDFS ACL's for fine-grained user permissions, however we are restricted to the
    Filesystem view of the world and therefore cannot provide column-level and row-level
    access, it's "all or nothing". As mentioned previously, Accumulo provides a good
    alternative when this scenario is important. There is a product, however, that
    also addresses this issue - see the RecordService section.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为了桥接Hive和HDFS数据，我们可以使用Sentry HDFS插件，它将HDFS文件权限与更高级别的抽象同步。例如，读取表的权限=读取表的文件的权限，同样，创建表的权限=写入数据库目录的权限。我们仍然使用HDFS
    ACL来进行细粒度的用户权限控制，但是我们受限于文件系统视图，因此无法提供列级和行级访问权限，只能是“全有或全无”。如前所述，当这种情况很重要时，Accumulo提供了一个很好的替代方案。然而，还有一个产品也解决了这个问题-请参阅RecordService部分。
- en: The quickest and easiest way to implement Apache Sentry is to use Apache Hue.
    Apache Hue has been developed over the last few years, starting life as a simple
    GUI to pull together a few of the basic Hadoop services, such as HDFS, and has
    grown into a hub for many of the key building blocks in the Hadoop stack; HDFS.
    Hive, Pig, HBase, Sqoop, Zookeeper, and Oozie all feature together with integrated
    Sentry to handle the security. A demonstration of Hue can be found at [http://demo.gethue.com/](http://demo.gethue.com/),
    providing a great introduction to the feature set. We can also see many of the
    ideas discussed in this chapter in practice, including HDFS ACLs, RBACs, and Hive
    HS2 access.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 实施Apache Sentry的最快最简单的方法是使用Apache Hue。Apache Hue在过去几年中得到了发展，最初是作为一个简单的GUI，用于整合一些基本的Hadoop服务，如HDFS，现在已经发展成为Hadoop堆栈中许多关键构建块的中心；HDFS、Hive、Pig、HBase、Sqoop、Zookeeper和Oozie都与集成的Sentry一起提供安全性。可以在[http://demo.gethue.com/](http://demo.gethue.com/)找到Hue的演示，为功能集提供了很好的介绍。我们还可以在实践中看到本章讨论的许多想法，包括HDFS
    ACL、RBAC和Hive HS2访问。
- en: RecordService
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RecordService
- en: One of the key aspects of the Hadoop ecosystem is decoupling storage managers
    (for example, HDFS and Apache HBase) and compute frameworks (for example, MapReduce,
    Impala, and Apache Spark). Although this decoupling allows for far greater flexibility,
    thus allowing the user to choose their framework components, it leads to excessive
    complexity due to the compromises required to ensure that everything works together
    seamlessly. As Hadoop becomes an increasingly critical infrastructure component
    for users, the expectations for compatibility, performance, and security also
    increase.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop生态系统的一个关键方面是解耦存储管理器（例如HDFS和Apache HBase）和计算框架（例如MapReduce、Impala和Apache
    Spark）。尽管这种解耦允许更大的灵活性，从而允许用户选择他们的框架组件，但由于需要做出妥协以确保一切无缝协同工作，这导致了过多的复杂性。随着Hadoop成为用户日益关键的基础设施组件，对兼容性、性能和安全性的期望也在增加。
- en: RecordService is a new core security layer for Hadoop that sits between the
    storage managers and compute frameworks to provide a unified data access path,
    fine-grained data permissions, and enforcement across the stack.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: RecordService是Hadoop的一个新的核心安全层，位于存储管理器和计算框架之间，提供统一的数据访问路径，细粒度的数据权限，并在整个堆栈中执行。
- en: '![RecordService](img/image_13_003.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![RecordService](img/image_13_003.jpg)'
- en: RecordService is only compatible with Cloudera 5.4 or later and, thus, cannot
    be used in a standalone capacity, or with Hortonworks, although HDP uses Ranger
    to achieve the same goals. More information can be found at [www.recordservice.io](http://www.recordservice.io).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: RecordService只兼容Cloudera 5.4或更高版本，因此不能独立使用，也不能与Hortonworks一起使用，尽管HDP使用Ranger来实现相同的目标。更多信息可以在[www.recordservice.io](http://www.recordservice.io)找到。
- en: Apache ranger
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache ranger
- en: 'The aims of Apache ranger are broadly the same as RecordService, the primary
    goals being:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Apache ranger的目标与RecordService大致相同，主要目标包括：
- en: Centralized security administration to manage all security related tasks in
    a central UI, or using REST APIs
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中安全管理，以在中央UI中管理所有安全相关任务，或使用REST API
- en: Fine-grained authorization to perform a specific action and/or operation with
    a Hadoop component/tool and manage through a central administration tool
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过集中管理工具执行特定操作和/或操作的细粒度授权，管理Hadoop组件/工具
- en: Standardize authorization methods across all Hadoop components
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有Hadoop组件中标准化授权方法
- en: Enhanced support for different authorization methods including role-based access
    control and attribute based access control
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强对不同授权方法的支持，包括基于角色的访问控制和基于属性的访问控制
- en: Centralized auditing of user access and administrative actions (security related)
    within all components of Hadoop
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop所有组件中用户访问和管理操作（与安全相关）的集中审计
- en: At the time of writing, Ranger is an Apache Incubator project and, therefore,
    is not at a major point release. Although, it is fully integrated with Hortonworks
    HDP supporting HDFS, Hive, HBase, Storm, Knox, Solr, Kafka, NiFi, YARN, and, crucially,
    a scalable cryptographic key management service for HDFS encryption. Full details
    can be found at [http://ranger.incubator.apache.org/](http://ranger.incubator.apache.org/)
    and [http://hortonworks.com/apache/ranger/](http://hortonworks.com/apache/ranger/).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Ranger是Apache孵化器项目，因此尚未发布重要版本。尽管如此，它已完全集成到Hortonworks HDP中，支持HDFS、Hive、HBase、Storm、Knox、Solr、Kafka、NiFi、YARN，以及HDFS加密的可扩展加密密钥管理服务。完整的详细信息可以在[http://ranger.incubator.apache.org/](http://ranger.incubator.apache.org/)和[http://hortonworks.com/apache/ranger/](http://hortonworks.com/apache/ranger/)找到。
- en: Apache Knox
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Knox
- en: We have discussed many of the security areas of the Spark/Hadoop stack, but
    they are all related to securing individual systems or data. An area that has
    not been mentioned in any detail is that of securing a cluster itself from unauthorized
    external access. Apache Knox fulfills this role by "ring fencing" a cluster and
    providing a REST API Gateway through which all external transactions must pass.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了Spark/Hadoop堆栈的许多安全领域，但它们都与保护单个系统或数据有关。一个没有详细提到的领域是保护集群本身免受未经授权的外部访问。Apache
    Knox通过“环形围栏”来履行这一角色，并提供一个REST API网关，所有外部交易都必须经过该网关。
- en: Coupled with a Kerberos secured Hadoop cluster, Knox provides authentication
    and authorization, protecting the specifics of the cluster deployment. Many of
    the common services are catered for, including HDFS (via WEBHDFS), YARN Resource
    Manager, and Hive.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Knox与Kerberos安全的Hadoop集群相结合，提供身份验证和授权，保护集群部署的具体细节。许多常见服务都得到了满足，包括HDFS（通过WEBHDFS）、YARN资源管理器和Hive。
- en: Knox is another project that is heavily contributed to by Hortonworks and, therefore,
    is fully integrated into the Hortonworks HDP platform. Whilst Knox can be deployed
    into virtually any Hadoop cluster, it can be done with a fully integrated approach
    in HDP. More information can be found at [knox.apache.org](http://knox.apache.org).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Knox是另一个由Hortonworks大力贡献的项目，因此完全集成到Hortonworks HDP平台中。虽然Knox可以部署到几乎任何Hadoop集群中，但在HDP中可以采用完全集成的方法。更多信息可以在[knox.apache.org](http://knox.apache.org)找到。
- en: Your Secure Responsibility
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您的安全责任
- en: 'Now that we''ve covered the common security use cases and discussed some of
    the tools that a data scientist needs to be aware of in their everyday activities,
    there''s one last important item to note. *While in their custody, the responsibility
    for data, including its security and integrity, lies with the data scientist*.
    This is usually true whether or not you are explicitly told. Therefore, it is
    crucial that you take this responsibility seriously and take all the necessary
    precautions when handling and processing data. If needed, also be ready to communicate
    to others their responsibility. We all need to ensure that we are not held responsible
    for a breach off-site; this can be achieved by highlighting the issue or, indeed,
    even having a written contract with the off-site service provider outlining their
    security arrangements. To see a real-world example of what can go wrong when you
    don''t pay proper attention to due diligence, have a look at some security notes
    regarding the Ashley-Madison hack here: [http://blog.erratasec.com/2015/08/notes-on-ashley-madison-dump.html#.V-AGgT4rIUv](http://blog.erratasec.com/2015/08/notes-on-ashley-madison-dump.html#.V-AGgT4rIUv).'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了常见的安全用例，并讨论了数据科学家在日常活动中需要了解的一些工具，还有一个重要的事项需要注意。*在他们的监管下，数据的责任，包括其安全性和完整性，都属于数据科学家*。这通常是真实的，无论是否有明确的告知。因此，您需要认真对待这一责任，并在处理和处理数据时采取一切必要的预防措施。如果需要，还要准备好向他人传达他们的责任。我们都需要确保自己不因违反场外责任而受到责备；这可以通过强调这个问题，甚至与场外服务提供商签订书面合同来实现他们的安全安排。要了解当您不注意尽职调查时可能出现的问题的真实案例，请查看有关Ashley-Madison黑客攻击的一些安全说明：[http://blog.erratasec.com/2015/08/notes-on-ashley-madison-dump.html#.V-AGgT4rIUv](http://blog.erratasec.com/2015/08/notes-on-ashley-madison-dump.html#.V-AGgT4rIUv)。
- en: Another area of interest is that of removable media, most commonly DVDs and
    memory sticks. These should be treated in the same way as hard drives, but with
    the assumption that the data is always unsafe and at risk. The same options exist
    for these types of media, meaning data can be secured at the application level
    or at the hardware level (excepting optical disks, for example, DVD/CD). With
    USB key storage, there exists examples that implement hardware encryption. The
    data is always secure when written to them, therefore removing the bulk of the
    responsibility from the user. These types of drive should always be certified
    to **Federal Information Processing Standards** (**FIPS**); generally, FIPS 140
    (Cryptographic modules) or FIPS 197 (AES Cipher).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个感兴趣的领域是可移动介质，最常见的是DVD和存储卡。这些应该与硬盘的处理方式相同，但要假设数据始终不安全且有风险。这些类型的介质存在相同的选项，意味着数据可以在应用程序级别或硬件级别（例如光盘，如DVD/CD）上进行保护。对于USB存储设备，存在实现硬件加密的示例。数据写入它们时始终是安全的，因此减轻了用户的大部分责任。这些类型的驱动器应始终获得联邦信息处理标准（FIPS）的认证；通常是FIPS
    140（密码模块）或FIPS 197（AES密码）。
- en: If an FIPS standard is not required, or the media is optical in nature, then
    data can be encrypted at the application layer, that is, encrypted by software.
    There are a number of ways to do this, including encrypted partitions, encrypted
    files, or raw data encryption. All of these methods involve using third-party
    software to perform the encrypt/decrypt functions at read/write time. Therefore,
    passwords are needed, introducing the issues around password strength, safety,
    and so on. The authors have experienced situations where an encrypted disk was
    handed from one company to another, and the handwritten password handed over at
    the same time! Apart from a risk to data security, there are also possible consequences
    in respect of disciplinary action against the individuals involved. If data is
    put at risk, it's always worth checking best practice and highlighting issues;
    it is very easy to become lax in this area, and sooner or later, data will be
    compromised and someone will have to take responsibility - it may not necessarily
    be the individual who lost the media itself.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不需要FIPS标准，或者媒体是光学性质的，那么数据可以在应用层加密，也就是由软件加密。有许多方法可以做到这一点，包括加密分区、加密文件或原始数据加密。所有这些方法都涉及使用第三方软件在读/写时执行加密/解密功能。因此，需要密码，引入了密码强度、安全性等问题。作者曾经经历过这样的情况，即加密磁盘被从一家公司交接到另一家公司，同时交接的是手写密码！除了对数据安全的风险，还可能涉及对涉及个人的纪律行动的后果。如果数据面临风险，值得检查最佳实践并强调问题；在这个领域变得懈怠是非常容易的，迟早，数据将受到损害，某人将不得不承担责任——这可能不一定是丢失媒体本身的个人。
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have explored the topic of data security and explained some
    of the surrounding issues. We have discovered that not only is there technical
    knowledge to master, but also that a data security mindset is just as important.
    Data security is often overlooked and, therefore, taking a systematic approach,
    and educating others, is a key responsibility for mastering data science.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了数据安全的主题，并解释了一些相关问题。我们发现，不仅需要掌握技术知识，而且数据安全意识同样重要。数据安全经常被忽视，因此，采取系统化的方法并教育他人是掌握数据科学的一个重要责任。
- en: We have explained the data security life cycle and outlined the most important
    areas of responsibility, including authorization, authentication and access, along
    with related examples and use cases. We have also explored the Hadoop security
    ecosystem and described the important open source solutions currently available.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解释了数据安全生命周期，并概述了授权、认证和访问等最重要的责任领域，以及相关示例和用例。我们还探讨了Hadoop安全生态系统，并描述了目前可用的重要开源解决方案。
- en: A significant part of this chapter was dedicated to building a Hadoop `InputFormat`
    *compressor* that operates as a data encryption utility that can be used with
    Spark. Appropriate configuration allows the codec to be used in a variety of key
    areas, crucially when spilling shuffled records to local disk where *currently
    no solution exists*.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的一个重要部分是致力于构建一个Hadoop“InputFormat”*压缩器*，它可以作为与Spark一起使用的数据加密实用程序。适当的配置允许在各种关键领域使用编解码器，特别是在将洗牌记录溢出到本地磁盘时，*目前没有解决方案*。
- en: In the next chapter, we will explore Scalable Algorithms, demonstrating the
    key techniques that we can master to enable performance at a truly "big data"
    scale.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨可扩展算法，展示我们可以掌握的关键技术，以实现真正的“大数据”规模的性能。
