- en: K-Nearest Neighbors and Naive Bayes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-最近邻和朴素贝叶斯
- en: 'In the previous chapter, we have learned about computationally intensive methods.
    In contrast, this chapter discusses the simple methods to balance it out! We will
    be covering the two techniques, called **k-nearest neighbors** (**KNN**)and Naive
    Bayes here. Before touching on KNN, we explained the issue with the curse of dimensionality
    with a simulated example. Subsequently, breast cancer medical examples have been
    utilized to predict whether the cancer is malignant or benign using KNN. In the
    final section of the chapter, Naive Bayes has been explained with spam/ham classification,
    which also involves the application of the **natural language processing** (**NLP**)
    techniques consisting of the following basic preprocessing and modeling steps:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们学习了计算量大的方法。相比之下，本章讨论了平衡这一点的简单方法！我们将介绍两种技术，分别是**K-最近邻**（**KNN**）和朴素贝叶斯。在讨论KNN之前，我们通过一个模拟示例解释了维度灾难的问题。随后，利用乳腺癌医学实例来预测癌症是恶性还是良性，并使用KNN进行分类。在本章的最后部分，介绍了朴素贝叶斯方法，并通过垃圾邮件/正常邮件分类进行讲解，这也涉及到应用**自然语言处理**（**NLP**）技术，包括以下基本的预处理和建模步骤：
- en: Punctuation removal
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标点符号移除
- en: Word tokenization and lowercase conversion
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词语分词和小写转换
- en: Stopwords removal
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用词移除
- en: Stemming
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取
- en: Lemmatization with POS tagging
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词形还原与词性标注
- en: Conversion of words into TF-IDF to create numerical representation of words
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将单词转换为 TF-IDF 以创建单词的数值表示
- en: Application of the Naive Bayes model on TF-IDF vectors to predict if the message
    is either spam or ham on both train and test data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将朴素贝叶斯模型应用于 TF-IDF 向量，以预测消息是垃圾邮件还是正常邮件，适用于训练和测试数据
- en: K-nearest neighbors
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-最近邻
- en: K-nearest neighbors is a non-parametric machine learning model in which the
    model memorizes the training observation for classifying the unseen test data.
    It can also be called instance-based learning. This model is often termed as lazy
    learning, as it does not learn anything during the training phase like regression,
    random forest, and so on. Instead, it starts working only during the testing/evaluation
    phase to compare the given test observations with the nearest training observations,
    which will take significant time in comparing each test data point. Hence, this
    technique is not efficient on big data; also, performance does deteriorate when
    the number of variables is high due to the **curse of dimensionality**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: K-最近邻是一种非参数的机器学习模型，其中模型记住训练观察数据用于分类未知的测试数据。它也可以称为基于实例的学习。这个模型通常被称为懒惰学习，因为它在训练阶段并不像回归、随机森林等那样学习任何东西。相反，它只在测试/评估阶段工作，通过将给定的测试观察数据与最接近的训练观察数据进行比较，这在比较每个测试数据点时会消耗大量时间。因此，该技术在大数据上并不高效；此外，由于**维度灾难**，当变量数量很高时，性能也会下降。
- en: KNN voter example
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN 投票者示例
- en: 'KNN is explained better with the following short example. The objective is
    to predict the party for which voter will vote based on their neighborhood, precisely
    geolocation (latitude and longitude). Here we assume that we can identify the
    potential voter to which political party they would be voting based on majority
    voters did vote for that particular party in that vicinity so that they have a
    high probability to vote for the majority party. However, tuning the k-value (number
    to consider, among which majority should be counted) is the million-dollar question
    (as same as any machine learning algorithm):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: KNN通过以下简短示例进行更好的说明。目标是根据选民的邻里情况，精确到地理位置（纬度和经度），预测选民将投给哪个政党。这里我们假设可以通过观察该地区大多数选民投票给哪个政党来判断潜在选民将投给哪个政党，从而使其有很高的可能性投给大多数党派。然而，调整k值（需要考虑的数量，在这些数量中统计大多数投票）是百万美元的问题（与任何机器学习算法一样）：
- en: '![](img/b6e598c7-2e69-459d-b77a-32f57fb361f4.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6e598c7-2e69-459d-b77a-32f57fb361f4.png)'
- en: In the preceding diagram, we can see that the voter of the study will vote for
    **Party 2**. As within the vicinity, one neighbor has voted for **Party 1** and
    the other voter voted for **Party 3**. But three voters voted for **Party 2**.
    In fact, by this way, KNN solves any given classification problem. Regression
    problems are solved by taking mean of its neighbors within the given circle or
    vicinity or k-value.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们可以看到该研究中的选民将投票给**党派 2**。因为在附近，一个邻居投票给了**党派 1**，另一个选民投票给了**党派 3**。但有三个选民投票给**党派
    2**。事实上，通过这种方式，KNN解决了任何给定的分类问题。回归问题通过在给定的圆圈或邻域内取邻居的均值或k值来解决。
- en: Curse of dimensionality
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度灾难
- en: KNN completely depends on distance. Hence, it is worth studying about the curse
    of dimensionality to understand when KNN deteriorates its predictive power with
    the increase in the number of variables required for prediction. This is an obvious
    fact that high-dimensional spaces are vast. Points in high-dimensional spaces
    tend to be dispersing from each other more compared with the points in low-dimensional
    space. Though there are many ways to check the curve of dimensionality, here we
    are using uniform random values between zero and one generated for 1D, 2D, and
    3D space to validate this hypothesis.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 完全依赖于距离。因此，研究“维度灾难”是值得的，目的是了解在预测所需的变量数量增加时，KNN 是如何随着维度增加而降低其预测能力的。这是一个显而易见的事实：高维空间是非常广阔的。与低维空间中的点相比，高维空间中的点通常会更加分散。虽然有很多方法可以检查维度曲线，但在这里我们使用了介于零和一之间的均匀随机值，分别生成
    1D、2D 和 3D 空间中的数据，以验证这一假设。
- en: 'In the following lines of codes, the mean distance between 1,000 observations
    has been calculated with the change in dimensions. It is apparent that with the
    increase in dimensions, distance between points increases logarithmically, which
    gives us the hint that we need to have an exponential increase in data points
    with increase in dimensions in order to make machine learning algorithms work
    correctly:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码行中，我们计算了1000个观察点在不同维度下的平均距离。显而易见，随着维度的增加，点与点之间的距离呈对数增长，这提示我们需要在维度增加时，数据点以指数级增长，才能让机器学习算法正确工作：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following code generates random numbers between zero and one from uniform
    distribution with the given dimension, which is equivalent of length of array
    or list:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码生成从零到一之间均匀分布的随机数，维度相当于数组或列表的长度：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following function calculates root mean sum of squares of Euclidean distances
    (2-norm) between points by taking the difference between points and sum the squares
    and finally takes the square root of total distance:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数计算了点之间的欧几里得距离（2-范数）的均方根和，通过计算点之间的差异并将平方求和，最后取总距离的平方根：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Both dimension and number of pairs are utilized for calculating the distances
    with the following code:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，使用维度和对数来计算距离：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The experiment has been done by changing dimensions from 1 to 201 with an increase
    of 5 dimensions to check the increase in distance:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 实验通过将维度从1改变到201，维度每次增加5来检查距离的增加：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Both minimum and average distances have been calculated to check, however,
    both illustrate the similar story:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了最小距离和平均距离，结果都表明了相似的趋势：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/befd95be-272c-43f4-81a1-8595f9aa4d17.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/befd95be-272c-43f4-81a1-8595f9aa4d17.png)'
- en: From the preceding graph, it is proved that with the increase in dimensions,
    mean distance increases logarithmically. Hence the higher the dimensions, the
    more data is needed to overcome the curse of dimensionality!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中可以证明，随着维度的增加，平均距离呈对数增长。因此，维度越高，就需要更多的数据来克服维度灾难！
- en: Curse of dimensionality with 1D, 2D, and 3D example
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1D、2D和3D维度灾难示例
- en: 'A quick analysis has been done to see how distance 60 random points are expanding
    with the increase in dimensionality. Initially, random points are drawn for one-dimension:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快速分析了60个随机点的距离，随着维度增加，这些点是如何扩展的。最初，随机点被绘制为一维数据：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If we observe the following graph, all 60 data points are very nearby in one-dimension:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们观察下面的图表，可以发现所有60个数据点在一维空间中非常接近：
- en: '![](img/e05c54d8-cedf-4930-90ac-361d35736b77.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e05c54d8-cedf-4930-90ac-361d35736b77.png)'
- en: 'Here we are repeating the same experiment in a 2D space, by taking 60 random
    numbers with *x* and *y* coordinate space and plotted them visually:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们在2D空间中重复相同的实验，通过选取60个带有 *x* 和 *y* 坐标的随机数，并将其可视化：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'By observing the 2D graph we can see that more gaps have been appearing for
    the same 60 data points:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察2D图表，我们可以看到相同的60个数据点之间出现了更多的间隙：
- en: '![](img/f9e699fd-c223-4755-948a-53ccbfef48c3.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9e699fd-c223-4755-948a-53ccbfef48c3.png)'
- en: 'Finally, 60 data points are drawn for 3D space. We can see a further increase
    in spaces, which is very apparent. This has proven to us visually by now that
    with the increase in dimensions, it creates a lot of space, which makes a classifier
    weak to detect the signal:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们绘制了60个数据点用于3D空间。我们可以看到空间的增加更加明显。到目前为止，这在视觉上已经证明，随着维度的增加，空间会变得非常大，这使得分类器难以检测到信号：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/d36eb8b9-0634-45b7-8c12-41eb2711b1a6.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d36eb8b9-0634-45b7-8c12-41eb2711b1a6.png)'
- en: KNN classifier with breast cancer Wisconsin data example
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用乳腺癌威斯康星数据的KNN分类器示例
- en: 'Breast cancer data has been utilized from the UCI machine learning repository [http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)
    for illustration purposes. Here the task is to find whether the cancer is malignant
    or benign based on various collected features such as clump thickness and so on
    using the KNN classifier:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌数据来自UCI机器学习库 [http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)，用于示范目的。在这里，任务是基于各种收集的特征，如肿块厚度等，使用KNN分类器判断癌症是恶性还是良性：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following are the first few rows to show how the data looks like. The `Class`
    value has class `2` and `4`. Value `2` and `4` represent benign and malignant
    class, respectively. Whereas all the other variables do vary between value `1`
    and `10`, which are very much categorical in nature:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前几行数据，展示数据的样子。`Class`值为`2`和`4`。值`2`和`4`分别代表良性和恶性类别。而所有其他变量的值在`1`和`10`之间变化，这些值在本质上是类别型的：
- en: '![](img/66458e11-de44-4ba8-9bbd-d96eacae2b68.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66458e11-de44-4ba8-9bbd-d96eacae2b68.png)'
- en: 'Only the `Bare_Nuclei` variable has some missing values, here we are replacing
    them with the most frequent value (category value `1`) in the following code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 只有`Bare_Nuclei`变量有一些缺失值，在以下代码中，我们将这些缺失值替换为最常见的值（类别值`1`）：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Use the following code to convert the classes to a `0` and `1` indicator for
    using in the classifier:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码将类转换为`0`和`1`的指示符，以用于分类器：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the following code, we are dropping non-value added variables from analysis:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们正在从分析中剔除无价值的变量：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As KNN is very sensitive to distances, here we are standardizing all the columns
    before applying algorithms:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于KNN对距离非常敏感，在应用算法之前，我们在这里对所有列进行了标准化处理：
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'KNN classifier is being applied with neighbor value of `3` and `p` value indicates
    it is 2-norm, also known as Euclidean distance for computing classes:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: KNN分类器的邻居值为`3`，`p`值表示2范数，也就是欧几里得距离，用于计算类别：
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/915405ac-f99e-4459-9f70-e50a7cd73812.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/915405ac-f99e-4459-9f70-e50a7cd73812.png)'
- en: From the results, it is appearing that KNN is working very well in classifying
    malignant and benign classes well, obtaining test accuracy of 97.6 percent with
    96 percent of recall on malignant class. The only deficiency of KNN classifier
    would be, it is computationally intensive during test phase, as each test observation
    will be compared with all the available observations in train data, which practically
    KNN does not learn a thing from training data. Hence, we are also calling it a
    lazy classifier!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果来看，KNN在分类恶性和良性类别方面表现非常好，测试准确率达到了97.6%，恶性类别的召回率为96%。KNN分类器唯一的缺点是，在测试阶段它计算密集，因为每个测试样本都要与训练数据中的所有可用样本进行比较，实际上，KNN并没有从训练数据中“学习”任何东西。因此，我们也称其为懒惰分类器！
- en: 'The R code for KNN classifier is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是KNN分类器的R代码：
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Tuning of k-value in KNN classifier
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN分类器中的k值调整
- en: 'In the previous section, we just checked with only the k-value of three. Actually,
    in any machine learning algorithm, we need to tune the knobs to check where the
    better performance can be obtained. In the case of KNN, the only tuning parameter
    is k-value. Hence, in the following code, we are determining the best k-value
    with grid search:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们仅使用了k值为3的情况。实际上，在任何机器学习算法中，我们都需要调节参数以检查在哪些情况下可以获得更好的性能。对于KNN，唯一需要调节的参数是k值。因此，在以下代码中，我们通过网格搜索来确定最佳k值：
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/d729f290-0d04-4a49-9cd4-540513d9c272.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d729f290-0d04-4a49-9cd4-540513d9c272.png)'
- en: 'It appears that with less value of k-value, it has more overfitting problems
    due to the very high value of accuracy on train data and less on test data, with
    the increase in k-value more the train and test accuracies are converging and
    becoming more robust. This phenomenon illustrates the typical machine learning
    phenomenon. As for further analysis, readers are encouraged to try k-values higher
    than five and see how train and test accuracies are changing. The R code for tuning
    of k-value in KNN classifier is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，较小的k值会导致更多的过拟合问题，因为训练数据的准确性非常高，而测试数据的准确性较低。随着k值的增大，训练和测试的准确性逐渐趋于一致并变得更加稳定。这一现象展示了典型的机器学习现象。对于进一步分析，建议读者尝试比五更高的k值，看看训练和测试准确性是如何变化的。以下是KNN分类器中k值调整的R代码：
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Naive Bayes
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Bayes algorithm concept is quite old and exists from the 18th century. Thomas
    Bayes developed the foundational mathematical principles for determining the probability
    of unknown events from the known events. For example, if all apples are red in
    color and average diameter would be about 4 inches then, if at random one fruit
    is selected from the basket with red color and diameter of 3.7 inches, what is
    the probability that the particular fruit would be an apple? Naive term does assume
    independence of particular features in a class with respect to others. In this
    case, there would be no dependency between color and diameter. This independence
    assumption makes the Naive Bayes classifier most effective in terms of computational
    ease for particular tasks such as email classification based on words in which
    high dimensions of vocab do exist, even after assuming independence between features.
    Naive Bayes classifier performs surprisingly really well in practical applications.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯算法的概念非常古老，源自18世纪。托马斯·贝叶斯（Thomas Bayes）开发了用于从已知事件中确定未知事件概率的基础数学原理。例如，如果所有的苹果都是红色的，且其平均直径约为4英寸，那么如果随机从一个篮子中挑选一个红色且直径为3.7英寸的水果，这个水果是苹果的概率是多少？朴素贝叶斯假设特定特征之间的独立性。在这种情况下，颜色和直径之间没有依赖关系。这种独立性假设使得朴素贝叶斯分类器在计算上对于某些任务（如基于单词的电子邮件分类）最为高效，即便在假设特征之间相互独立的情况下，词汇的高维度依然能够处理。朴素贝叶斯分类器在实际应用中表现得出奇地好。
- en: Bayesian classifiers are best applied to problems in which information from
    a very high number of attributes should be considered simultaneously to estimate
    the probability of final outcome. Bayesian methods utilize all available evidence
    to consider for prediction even features have weak effects on the final outcome
    to predict. However, we should not ignore the fact that a large number of features
    with relatively minor effects, taken together its combined impact would form strong
    classifiers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯分类器最适用于那些需要同时考虑大量属性信息来估计最终结果概率的问题。贝叶斯方法利用所有可用的证据进行预测，即使某些特征对最终结果的影响较弱，依然会被考虑。然而，我们不应忽视这样一个事实：大量相对较小影响的特征，结合在一起会形成强大的分类器。
- en: Probability fundamentals
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率基础
- en: Before diving into Naive Bayes, it would be good to reiterate the fundamentals.
    Probability of an event can be estimated from observed data by dividing the number
    of trails in which an event occurred with the total number of trails. For instance,
    if a bag contains red and blue balls and randomly picked *10* balls one by one
    with replacement and out of *10*, *3* red balls appeared in trails we can say
    that probability of red is *0.3*, *p[red] = 3/10 = 0.3*. Total probability of
    all possible outcomes must be 100 percent.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解朴素贝叶斯之前，回顾一下基本原理会很有帮助。一个事件的概率可以通过观察数据来估算，即将事件发生的次数除以总试验次数。例如，如果一个袋子里有红色和蓝色的球，且每次都从袋中随机抽取*10*个球，且每次抽取后会放回，那么在这*10*次试验中，出现*3*个红球，那么我们可以说红球的概率是*0.3*，*p[红球]
    = 3/10 = 0.3*。所有可能结果的总概率必须为100%。
- en: 'If a trail has two outcomes such as email classification either it is spam
    or ham and both cannot occur simultaneously, these events are considered as mutually
    exclusive with each other. In addition, if those outcomes cover all possible events,
    it would be called as **exhaustive events**. For example, in email classification
    if *P (spam) = 0.1*, we will be able to calculate *P (ham) = 1- 0.1 = 0.9*, these
    two events are mutually exclusive. In the following Venn diagram, all the email
    possible classes are represented (the entire universe) with the type of outcomes:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个试验有两个可能的结果，例如电子邮件分类问题中，要么是垃圾邮件，要么是正常邮件，且这两个结果不能同时发生，那么这些事件被称为互斥事件。此外，如果这些结果涵盖了所有可能的事件，它们就被称为**穷尽事件**。例如，在电子邮件分类中，如果*P（垃圾邮件）
    = 0.1*，我们可以计算出*P（正常邮件） = 1 - 0.1 = 0.9*，这两个事件是互斥的。在下面的维恩图中，所有电子邮件的可能类别（整个宇宙）以及结果类型都被表示出来：
- en: '![](img/0c902cba-e1f7-4e92-a1e4-79ad6f2499b8.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c902cba-e1f7-4e92-a1e4-79ad6f2499b8.png)'
- en: Joint probability
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联合概率
- en: Though mutually exclusive cases are simple to work upon, most of the actual
    problems do fall under the category of non-mutually exclusive events. By using
    the joint appearance, we can predict the event outcome. For example, if emails
    messages present the word like *lottery*, which is very highly likely of being
    spam rather than ham. The following Venn diagram indicates the joint probability
    of spam with *lottery*. However, if you notice in detail, lottery circle is not
    contained completely within the spam circle. This implies that not all spam messages
    contain the word *lottery* and not every email with the word *lottery* is spam.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管互斥事件的处理比较简单，但大多数实际问题属于非互斥事件的范畴。通过使用联合出现的情况，我们可以预测事件的结果。例如，如果邮件中出现了像 *彩票* 这样的词语，那么它很可能是垃圾邮件而非非垃圾邮件。下图显示了垃圾邮件与
    *彩票* 的联合概率。然而，如果你仔细观察，你会发现“彩票”圆并没有完全包含在垃圾邮件圆内。这意味着并非所有垃圾邮件都包含“彩票”一词，也并非每封包含“彩票”一词的邮件都是垃圾邮件。
- en: '![](img/a4b84585-55e4-4155-8fec-f8d907e9e476.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4b84585-55e4-4155-8fec-f8d907e9e476.png)'
- en: 'In the following diagram, we have expanded the spam and ham category in addition
    to the *lottery* word in Venn diagram representation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们扩展了垃圾邮件和非垃圾邮件类别，并用维恩图表示了*彩票*一词：
- en: '![](img/6499a1f6-038f-48c8-ba33-b628ef50fb8b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6499a1f6-038f-48c8-ba33-b628ef50fb8b.png)'
- en: We have seen that 10 percent of all the emails are spam and 4 percent of emails
    have the word *lottery* and our task is to quantify the degree of overlap between
    these two proportions. In other words, we need to identify the joint probability
    of both *p(spam)* and *p(lottery)* occurring, which can be written as *p(spam
    ∩ lottery)*. In case if both the events are totally unrelated, they are called
    **independent events** and their respective value is *p(spam ∩ lottery) = p(spam)
    * p(lottery) = 0.1 * 0.04 = 0.004*, which is 0.4 percent of all messages are spam
    containing the word Lottery. In general, for independent events *P(A∩ B) = P(A)
    * P(B)*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，10%的邮件是垃圾邮件，4%的邮件包含“彩票”一词，我们的任务是量化这两者之间的重叠程度。换句话说，我们需要确定同时发生 *p(spam)*
    和 *p(lottery)* 的联合概率，可以写作 *p(spam ∩ lottery)*。如果这两个事件完全无关，则它们被称为**独立事件**，它们的值为
    *p(spam ∩ lottery) = p(spam) * p(lottery) = 0.1 * 0.04 = 0.004*，即0.4%的邮件是包含“彩票”一词的垃圾邮件。一般来说，对于独立事件，*P(A∩
    B) = P(A) * P(B)*。
- en: Understanding Bayes theorem with conditional probability
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解贝叶斯定理与条件概率
- en: 'Conditional probability provides a way of calculating relationships between
    dependent events using Bayes theorem. For example, *A* and *B* are two events
    and we would like to calculate *P(A\B)* can be read as the probability of an event
    occurring *A* given the fact that event *B* already occurred, in fact, this is
    known as **conditional probability**, the equation can be written as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 条件概率提供了一种利用贝叶斯定理计算依赖事件之间关系的方法。例如，*A* 和 *B* 是两个事件，我们希望计算 *P(A\B)*，它可以理解为在事件 *B*
    已经发生的情况下，事件 *A* 发生的概率，实际上，这就是**条件概率**，该方程可以写成如下形式：
- en: '![](img/0123470a-764e-4575-a679-9e30c8340e0e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0123470a-764e-4575-a679-9e30c8340e0e.png)'
- en: To understand better, we will now talk about the email classification example.
    Our objective is to predict whether an email is a spam given the word lottery
    and some other clues. In this case, we already knew the overall probability of
    spam, which is 10 percent also known as **prior probability**. Now suppose you
    have obtained an additional piece of information that probability of word lottery
    in all messages, which is 4 percent, also known as **marginal likelihood**. Now,
    we know the probability that *lottery* was used in previous spam messages and
    is called the **likelihood**.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，我们现在来讨论一个邮件分类的例子。我们的目标是预测给定“彩票”一词和其他一些线索的情况下，一封邮件是否为垃圾邮件。在这个例子中，我们已经知道了垃圾邮件的总体概率，即10%，也被称为**先验概率**。现在假设你得到了一个额外的信息，即“彩票”一词在所有邮件中的概率是4%，也叫做**边际似然性**。现在，我们知道“彩票”一词出现在过去垃圾邮件中的概率，这被称为**似然性**。
- en: '![](img/1d3a04f4-4cd8-42e8-b577-71e1f8e778e5.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d3a04f4-4cd8-42e8-b577-71e1f8e778e5.png)'
- en: By applying the Bayes theorem to the evidence, we can calculate the posterior
    probability that calculates the probability that the message is how likely a spam;
    given the fact that lottery was appearing in the message. On average if the probability
    is greater than 50 percent it indicates that the message is spam rather than ham.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用贝叶斯定理到证据上，我们可以计算后验概率，从而计算消息是垃圾邮件的可能性；给定“彩票”一词出现在该消息中的事实。如果该概率大于50%，则表示这封邮件是垃圾邮件而非非垃圾邮件。
- en: '![](img/a22f2b5e-d947-4383-a267-42b02ebbc4d6.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a22f2b5e-d947-4383-a267-42b02ebbc4d6.png)'
- en: In the previous table, the sample frequency table that records the number of
    times *Lottery* appeared in spam and ham messages and its respective likelihood
    has been shown. Likelihood table reveals that *P(Lottery\Spam)= 3/22 = 0.13*,
    indicating that probability is 13 percent that a spam message contains the term
    *Lottery*. Subsequently we can calculate the *P(Spam ∩ Lottery) = P(Lottery\Spam)
    * P(Spam) = (3/22) * (22/100) = 0.03*. In order to calculate the posterior probability,
    we divide *P(Spam ∩ Lottery)* with *P(Lottery)*, which means *(3/22)*(22/100)
    / (4/100) = 0.75*. Therefore, the probability is 75 percent that a message is
    spam, given that message contains the word *Lottery*. Hence, don't believe in
    quick fortune guys!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表格中，记录了*彩票*在垃圾邮件和正常邮件中出现的次数及其相应的可能性。可能性表明 *P(彩票\垃圾邮件)= 3/22 = 0.13*，这表明垃圾邮件中包含“彩票”一词的概率为13%。随后，我们可以计算
    *P(垃圾邮件 ∩ 彩票) = P(彩票\垃圾邮件) * P(垃圾邮件) = (3/22) * (22/100) = 0.03*。为了计算后验概率，我们将
    *P(垃圾邮件 ∩ 彩票)* 除以 *P(彩票)*，即 *(3/22)*(22/100) / (4/100) = 0.75*。因此，给定一条信息中包含“彩票”一词，消息为垃圾邮件的概率为75%。所以，不要相信快速致富的家伙们！
- en: Naive Bayes classification
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类
- en: 'In the past example, we have seen with a single word called *lottery*, however,
    in this case, we will be discussing with a few more additional words such as *Million* and
    *Unsubscribe* to show how actual classifiers do work. Let us construct the likelihood
    table for the appearance of the three words (*W1*, *W2*, and *W3*), as shown in
    the following table for *100* emails:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的示例中，我们看到的是单个单词 *彩票*，然而在本例中，我们将讨论更多的额外单词，如 *百万* 和 *退订*，以展示实际分类器是如何工作的。让我们构建一个包含三个单词（*W1*，*W2*
    和 *W3*）出现频率的可能性表，如下表所示，基于 *100* 封电子邮件：
- en: '![](img/a213ad61-ceab-4f9a-b03a-1d86c3651334.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a213ad61-ceab-4f9a-b03a-1d86c3651334.png)'
- en: When a new message is received, the posterior probability will be calculated
    to determine that email message is spam or ham. Let us assume that we have an
    email with terms *Lottery* and *Unsubscribe*, but it does not have word *Million*
    in it, with this details, what is the probability of spam?
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当收到新邮件时，将计算后验概率，以确定该电子邮件是垃圾邮件还是正常邮件。假设我们收到一封包含 *彩票* 和 *退订* 的邮件，但没有包含 *百万* 一词，基于这些信息，垃圾邮件的概率是多少？
- en: 'By using Bayes theorem, we can define the problem as *Lottery = Yes*, *Million
    = No* and *Unsubscribe = Yes*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理，我们可以定义问题为 *彩票 = 是*，*百万 = 否* 和 *退订 = 是*：
- en: '![](img/7c004c3e-e6bc-43d3-9163-710c9b5798be.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c004c3e-e6bc-43d3-9163-710c9b5798be.png)'
- en: 'Solving the preceding equations will have high computational complexity due
    to the dependency of words with each other. As a number of words are added, this
    will even explode and also huge memory will be needed for processing all possible
    intersecting events. This finally leads to intuitive turnaround with independence
    of words (**cross-conditional independence**) for which it got name of the Naive
    prefix for Bayes classifier. When both events are independent we can write *P(A
    ∩ B) = P(A) * P(B)*. In fact, this equivalence is much easier to compute with
    less memory requirement:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于单词之间相互依赖，解决上述方程将具有较高的计算复杂度。随着单词数量的增加，这种计算复杂度将呈爆炸性增长，同时处理所有可能交集事件所需的内存也会非常庞大。最终，这导致了基于单词独立性的直观转变（**条件独立性交叉**），因此贝叶斯分类器被称为“朴素贝叶斯”。当两个事件相互独立时，我们可以写作
    *P(A ∩ B) = P(A) * P(B)*。实际上，这种等式的计算更为简单，且对内存的需求更低：
- en: '![](img/7c4917c8-5737-4273-bf9d-8e906149c672.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c4917c8-5737-4273-bf9d-8e906149c672.png)'
- en: 'In a similar way, we will calculate the probability for ham messages as well,
    as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，我们也会计算正常邮件的概率，具体如下：
- en: '![](img/e0510142-46a4-4834-8928-885388599293.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0510142-46a4-4834-8928-885388599293.png)'
- en: 'By substituting the preceding likelihood table in the equations, due to the
    ratio of spam/ham we can just simply ignore the denominator terms in both the
    equations. Overall likelihood of spam is:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将前述可能性表代入方程，由于垃圾邮件和正常邮件的比例，我们可以简单地忽略两个方程中的分母项。垃圾邮件的总体可能性为：
- en: '![](img/3008e654-9d2b-42b9-9f29-4cdd2f619bd8.png)![](img/f26373a6-03d9-45f1-839a-e033b6d9ded8.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3008e654-9d2b-42b9-9f29-4cdd2f619bd8.png)![](img/f26373a6-03d9-45f1-839a-e033b6d9ded8.png)'
- en: 'After calculating the ratio, *0.008864/0.004349 = 2.03*, which means that this
    message is two times more likely to be spam than ham. But we can calculate the
    probabilities as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 计算比率后，*0.008864/0.004349 = 2.03*，这意味着这条信息比正常邮件更有可能是垃圾邮件，概率是两倍。但我们可以按照以下方式计算概率：
- en: '*P(Spam) = 0.008864/(0.008864+0.004349) = 0.67*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(垃圾邮件) = 0.008864/(0.008864+0.004349) = 0.67*'
- en: '*P(Ham) = 0.004349/(0.008864+0.004349) = 0.33*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(Ham) = 0.004349/(0.008864+0.004349) = 0.33*'
- en: By converting likelihood values into probabilities, we can show in a presentable
    way for either to set-off some thresholds, and so on.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将似然值转换为概率，我们可以以易于展示的方式呈现，无论是设置某些阈值，等等。
- en: Laplace estimator
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拉普拉斯估计
- en: In the previous calculation, all the values are nonzeros, which makes calculations
    well. Whereas in practice some words never appear in past for specific category
    and suddenly appear at later stages, which makes entire calculations as zeros.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述计算中，所有值都是非零的，这使得计算顺利进行。然而，在实际情况中，有些单词在特定类别中从未出现，突然在后期阶段出现，这会导致整个计算结果为零。
- en: 'For example, in the previous equation *W[3]* did have a *0* value instead of
    *13*, and it will convert entire equations to *0* altogether:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在前面的方程式中，*W[3]*的值是*0*而不是*13*，这会导致整个方程式的结果都变为*0*：
- en: '![](img/4613c8e9-71b9-43dd-95e1-607855b3edf2.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4613c8e9-71b9-43dd-95e1-607855b3edf2.png)'
- en: 'In order to avoid this situation, Laplace estimator essentially adds a small
    number to each of the counts in the frequency table, which ensures that each feature
    has a nonzero probability of occurring with each class. Usually, Laplace estimator
    is set to *1*, which ensures that each class-feature combination is found in the
    data at least once:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，拉普拉斯估计通过在频率表中的每个计数上加上一个小的常数，确保每个特征在每个类别中都有非零的出现概率。通常，拉普拉斯估计设为*1*，这确保每个类别-特征组合在数据中至少出现一次：
- en: '![](img/4ed79e3d-5253-46f6-9efb-7ba0d5737898.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ed79e3d-5253-46f6-9efb-7ba0d5737898.png)'
- en: If you observe the equation carefully, value *1* is added to all three words
    in the numerator and at the same time, three has been added to all denominators
    to provide equivalence.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细观察方程，分子中的三个单词都加了*1*，同时，所有分母都加了*3*，以保持等式平衡。
- en: Naive Bayes SMS spam classification example
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯SMS垃圾邮件分类示例
- en: 'Naive Bayes classifier has been developed using the SMS spam collection data
    available at [http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).
    In this chapter, various techniques available in NLP techniques have been discussed
    to preprocess prior to build the Naive Bayes model:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SMS垃圾邮件数据集（可在[http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)找到）开发了朴素贝叶斯分类器。本章讨论了在构建朴素贝叶斯模型之前，NLP技术中可用的各种预处理技术：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following `sys` package lines code can be used in case of any `utf-8` errors
    encountered while using older versions of Python, or else does not necessary with
    the latest version of Python 3.6:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`sys`包的代码可以在遇到任何`utf-8`错误时使用，尤其是在使用旧版本Python时，否则在最新版本的Python 3.6中不必要：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Normal coding starts from here as usual:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正常编码从此开始，像往常一样：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following code prints the top 5 lines:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码会打印出前五行：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/f57add7c-995c-4e5c-8979-de794701d32e.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f57add7c-995c-4e5c-8979-de794701d32e.png)'
- en: 'After getting preceding output run following code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 获取前述输出后，运行以下代码：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/99598fba-a2c0-42ac-ae0e-f3cd9e01cccd.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99598fba-a2c0-42ac-ae0e-f3cd9e01cccd.png)'
- en: Out of 5,572 observations, 4,825 are ham messages, which are about 86.5 percent
    and 747 spam messages are about remaining 13.4 percent.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在5,572个样本中，4,825个是正常邮件，约占86.5%，而747个是垃圾邮件，占余下的13.4%。
- en: 'Using NLP techniques, we have preprocessed the data for obtaining finalized
    word vectors to map with final outcomes spam or ham. Major preprocessing stages
    involved are:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NLP技术，我们已对数据进行预处理，以获得最终的词向量，进而映射到最终结果：垃圾邮件或正常邮件。主要的预处理阶段包括：
- en: '**Removal of punctuations**: Punctuations needs to be removed before applying
    any further processing. Punctuations from the `string` library are *!"#$%&\''()*+,-./:;<=>?@[\\]^_`{|}~*,
    which are removed from all the messages.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去除标点符号**：在进行进一步处理之前，需要去除标点符号。`string`库中的标点符号包括*!"#$%&\''()*+,-./:;<=>?@[\\]^_`{|}~*，这些标点符号会从所有消息中移除。'
- en: '**Word tokenization**: Words are chunked from sentences based on white space
    for further processing.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇分割**：根据空格将句子中的单词拆分，以便进一步处理。'
- en: '**Converting words into lowercase**: Converting to all lower case provides
    removal of duplicates, such as *Run* and *run*, where the first one comes at start
    of the sentence and the later one comes in the middle of the sentence, and so
    on, which all needs to be unified to remove duplicates as we are working on bag
    of words technique.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将单词转换为小写**：将所有字母转换为小写可以去除重复项，例如*Run*和*run*，前者出现在句子的开头，而后者出现在句子的中间，等等，这些都需要统一处理以去除重复项，因为我们正在使用词袋模型技术。'
- en: '**Stop word removal**: Stop words are the words that repeat so many times in
    literature and yet are not a differentiator in the explanatory power of sentences.
    For example: *I*, *me*, *you*, *this*, *that*, and so on, which needs to be removed
    before further processing.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停用词移除**：停用词是指在文学中重复出现多次，但对句子的解释力没有太大作用的词。例如：*I*、*me*、*you*、*this*、*that*等，需要在进一步处理之前将其移除。'
- en: '**of length at least three**: Here we have removed words with length less than
    three.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长度至少为三**：在这里我们移除了长度小于三的单词。'
- en: '**Stemming of words**: Stemming process stems the words to its respective root
    words. Example of stemming is bringing down running to run or runs to run. By
    doing stemming we reduce duplicates and improve the accuracy of the model.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单词的词干提取**：词干提取过程将单词简化为其根词。词干提取的例子是将running简化为run，或将runs简化为run。通过词干提取，我们减少了重复并提高了模型的准确性。'
- en: '**Part-of-speech (POS) tagging**:  This applies the speech tags to words, such
    as noun, verb, adjective, and so on. For example, POS tagging for *running* is
    verb, whereas for *run* is noun. In some situation *running* is noun and lemmatization
    will not bring down the word to root word *run*, instead, it just keeps the *running*
    as it is. Hence, POS tagging is a very crucial step necessary for performing prior
    to applying the lemmatization operation to bring down the word to its root word.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词性标注（POS tagging）**：这一步骤为单词应用词性标签，如名词、动词、形容词等。例如，*running*的词性标注是动词，而*run*的词性标注是名词。在某些情况下，*running*是名词，词形还原不会将其简化为根词*run*，而是将*running*保留原样。因此，词性标注是执行词形还原操作之前必须进行的关键步骤。'
- en: '**Lemmatization of words**: Lemmatization is another different process to reduce
    the dimensionality. In lemmatization process, it brings down the word to root
    word rather than just truncating the words. For example, bring *ate* to its root
    word as *eat* when we pass the *ate* word into lemmatizer with the POS tag as
    verb.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单词的词形还原**：词形还原是另一种减少维度的过程。在词形还原过程中，它将单词归结为根词，而不是仅仅截断单词。例如，将*ate*还原为其根词*eat*，当我们将*ate*传递给词形还原器时，词性标签为动词。'
- en: 'The `nltk` package has been utilized for all the preprocessing steps, as it
    consists of all the necessary NLP functionality in one single roof:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk`包已经在所有预处理步骤中使用，因为它包含了所有必要的自然语言处理功能：'
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Function has been written (preprocessing) consists of all the steps for convenience.
    However, we will be explaining all the steps in each section:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 已编写（预处理）函数，包含所有步骤，便于使用。然而，我们将在每个部分中解释这些步骤：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following line of the code splits the word and checks each character if
    it is in standard punctuations if so it will be replaced with blank and or else
    it just does not replace with blanks:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码行会将单词拆分并检查每个字符是否为标准标点符号，如果是，则将其替换为空格，否则不进行替换：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following code tokenizes the sentences into words based on white spaces
    and put them together as a list for applying further steps:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将句子根据空格分割为单词，并将它们组合成一个列表，以便应用进一步的步骤：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Converting all the cases (upper, lower, and proper) into lowercase reduces
    duplicates in corpus:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有的大小写（大写、小写和首字母大写）转换为小写，以减少语料库中的重复：
- en: '[PRE27]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As mentioned earlier, stop words are the words that do not carry much weight
    in understanding the sentence; they are used for connecting words, and so on.
    We have removed them with the following line of code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，停用词是指那些在理解句子时没有太大意义的词；它们通常用于连接其他词等。我们已经通过以下代码行将它们移除：
- en: '[PRE28]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Keeping only the words with length greater than `3` in the following code for
    removing small words, which hardly consists of much of a meaning to carry:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将只保留长度大于`3`的单词，用于去除那些几乎没有实际意义的小单词：
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Stemming is applied on the words using `PorterStemmer` function, which stems
    the extra suffixes from the words:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对单词应用了`PorterStemmer`函数进行词干提取，该函数从单词中去除额外的后缀：
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'POS tagging is a prerequisite for lemmatization, based on whether the word
    is noun or verb, and so on, it will reduce it to the root word:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注是词形还原的前提，基于单词是名词还是动词等，它将单词简化为根词：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `pos_tag` function returns the part of speed in four formats for noun and
    six formats for verb. `NN` (noun, common, singular), `NNP` (noun, proper, singular),
    `NNPS` (noun, proper, plural), `NNS` (noun, common, plural), `VB` (verb, base
    form), `VBD` (verb, past tense), `VBG` (verb, present participle), `VBN` (verb,
    past participle), `VBP` (verb, present tense, not third person singular), `VBZ` (verb,
    present tense, third person singular):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`pos_tag`函数为名词返回四种格式，为动词返回六种格式。名词包括`NN`（普通名词，单数）、`NNP`（专有名词，单数）、`NNPS`（专有名词，复数）、`NNS`（普通名词，复数）；动词包括`VB`（动词原形）、`VBD`（动词过去式）、`VBG`（动词现在分词）、`VBN`（动词过去分词）、`VBP`（动词现在时，非第三人称单数）、`VBZ`（动词现在时，第三人称单数）：'
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The `prat_lemmatize` function has been created only for the reasons of mismatch
    between the `pos_tag` function and intake values of the lemmatize function. If
    the tag for any word falls under the respective noun or verb tags category, `n`
    or `v` will be applied accordingly in the lemmatize function:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`prat_lemmatize`函数的创建仅为了解决`pos_tag`函数和词形还原函数（lemmatize）的输入值不匹配的问题。如果某个词的标签属于相应的名词或动词标签类别，则在词形还原函数中会应用`n`或`v`：'
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'After performing tokenization and applied all the various operations, we need
    to join it back to form stings and the following function performs the same:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行了分词和各种操作后，我们需要将其重新连接起来形成字符串，以下函数实现了这一操作：
- en: '[PRE34]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following step applies the preprocessing function to the data and generates
    new corpus:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是对数据应用预处理函数，并生成新的语料库：
- en: '[PRE35]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Data will be split into train and test based on 70-30 split and converted to
    the NumPy array for applying machine learning algorithms:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 数据将按70-30的比例分为训练集和测试集，并转换为NumPy数组，以应用机器学习算法：
- en: '[PRE36]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following code converts the words into a vectorizer format and applies
    **term frequency-inverse document frequency** (**TF-IDF**) weights, which is a
    way to increase weights to words with high frequency and at the same time penalize
    the general terms such as *the*, *him*, *at*, and so on. In the following code,
    we have restricted to most frequent 4,000 words in the vocabulary, none the less
    we can tune this parameter as well for checking where the better accuracies are
    obtained:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将单词转换为向量化格式，并应用**词频-逆文档频率**（**TF-IDF**）权重，这是通过增加高频词的权重并同时惩罚常见词（如*the*，*him*，*at*等）的一种方法。在以下代码中，我们限制了词汇表中最频繁的4,000个单词，尽管我们也可以调整此参数，以检查在哪些参数下获得更好的准确率：
- en: '[PRE37]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The TF-IDF transformation has been shown as follows on both train and test
    data. The `todense` function is used to create the data to visualize the content:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF转换在训练数据和测试数据上展示如下。使用`todense`函数将数据转换成可视化格式：
- en: '[PRE38]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Multinomial Naive Bayes classifier is suitable for classification with discrete
    features (example word counts), which normally requires large feature counts.
    However, in practice, fractional counts such as TF-IDF will also work well. If
    we do not mention any Laplace estimator, it does take the value of *1.0* means
    and it will add *1.0* against each term in numerator and total for denominator:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯分类器适用于具有离散特征（例如单词计数）的分类，这通常需要大量的特征。然而，实际上，像TF-IDF这样的分数计数也能很好地工作。如果我们没有提及任何拉普拉斯估计器，它的默认值为*1.0*，意味着它将在每个词项的分子和分母总和中添加*1.0*：
- en: '[PRE39]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/34b61efc-2814-4234-8d08-21c2e68c1264.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34b61efc-2814-4234-8d08-21c2e68c1264.png)'
- en: From the previous results, it is appearing that Naive Bayes has produced excellent
    results of 96.6 percent test accuracy with significant recall value of 76 percent for
    spam and almost 100 percent for ham.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的结果，似乎朴素贝叶斯算法在垃圾邮件的测试准确率达到了96.6%，且召回值显著，为76%，而在正常邮件（ham）的准确率几乎达到了100%。
- en: 'However, if we would like to check what are the top 10 features based on their
    coefficients from Naive Bayes, the following code will be handy for this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们想查看基于朴素贝叶斯的前10个特征及其系数，以下代码将非常有用：
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](img/b31d330c-79e0-405f-a60d-08a3940ea1b8.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b31d330c-79e0-405f-a60d-08a3940ea1b8.png)'
- en: 'Though the R language is not a popular choice for NLP processing, here we have
    presented the code. Readers are encouraged to change the code and see how accuracies
    are changing for a better understanding of concepts. The R code for Naive Bayes
    classifier on SMS spam/ham data is as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管R语言并不是进行自然语言处理（NLP）的热门选择，但这里我们展示了相关代码。我们鼓励读者修改代码，并观察准确率如何变化，从而更好地理解相关概念。以下是用于处理短信垃圾邮件/正常邮件数据的朴素贝叶斯分类器的R代码：
- en: '[PRE41]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Summary
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you have learned about KNN and Naive Bayes techniques, which
    require somewhat a little less computational power. KNN, in fact, is called a
    lazy learner, as it does not learn anything apart from comparing with training
    data points to classify them into class. Also, you have seen how to tune the k-value
    using grid search technique. Whereas explanation has been provided for Naive Bayes
    classifier, NLP examples have been provided with all the famous NLP processing
    techniques to give you a flavor of this field in a very crisp manner. Though in
    text processing, either Naive Bayes or SVM techniques could be used as these two
    techniques can handle data with high dimensionality, which is very relevant in
    NLP, as the number of word vectors is relatively high in dimensions and sparse
    at the same time.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经学习了KNN和朴素贝叶斯技术，这些技术所需的计算能力相对较低。事实上，KNN被称为懒惰学习器，因为它除了与训练数据点进行比较以进行分类外，不会进行任何学习。此外，您还了解了如何使用网格搜索技术调整k值。对于朴素贝叶斯分类器，我们提供了相关解释，并结合所有著名的NLP处理技术，简洁地向您展示了这个领域的一些示例。尽管在文本处理过程中，朴素贝叶斯或SVM技术都可以使用，因为这两种技术能够处理具有高维度的数据，这在NLP中非常重要，因为单词向量的维度相对较高且通常是稀疏的。
- en: In the next chapter, we will be covering the details of unsupervised learning,
    more precisely, clustering and principal component analysis models.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将详细介绍无监督学习，特别是聚类和主成分分析模型。
