- en: Chapter 9.  Designing Spark Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 设计Spark应用程序
- en: Think functionally. Think application functionality designed like a pipeline
    with each piece plumbed together doing some part of the whole job in hand. It
    is all about processing data, and that is what Spark does in a highly versatile
    manner. Data processing starts with the seed data that gets into the processing
    pipeline. The seed data can be a new piece of data that is ingested into the system,
    or it can be some kind of master dataset that lives in the enterprise data store
    and needs to be sliced and diced to produce different views to serve various purposes
    and business needs. It is this slicing and dicing that is going to be the norm
    when designing and developing data processing applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从功能性角度思考。设想一种应用功能，它被设计成一个管道，每个部分通过管道连接，共同完成手头工作的某一部分。这一切都关乎数据处理，而Spark正是以高度灵活的方式进行数据处理的。数据处理始于进入处理管道的种子数据。种子数据可以是系统摄取的新数据片段，也可以是企业数据存储中的某种主数据集，需要对其进行切片和切块以生成不同的视图，以满足各种目的和业务需求。在设计和开发数据处理应用程序时，这种切片和切块将成为常态。
- en: Any application development exercise starts with a study of the domain, the
    business requirement `s, and the technical tool selection. It is not going to
    be different here. Even though this chapter is going to see the design and development
    of a Spark application, the initial focus will be on the overall architecture
    of data processing applications, use cases, the data, and the applications that
    transform the data from one state to another. Spark is just a driver that assembles
    data processing logic and data together, using its highly powerful infrastructure
    to produce the desired results.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 任何应用程序开发实践都始于对领域的研究、业务需求的`分析`以及技术工具的选择。在这里也不例外。尽管本章将探讨Spark应用程序的设计和开发，但最初的焦点将放在数据处理应用程序的整体架构、用例、数据以及将数据从一种状态转换为另一种状态的应用程序上。Spark只是一个驱动程序，它利用其强大的基础设施将数据处理逻辑和数据组合在一起，以产生期望的结果。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Lambda Architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lambda架构
- en: Microblogging with Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行微博
- en: Data dictionaries
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据字典
- en: Coding style
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码风格
- en: Data ingestion
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取
- en: Lambda Architecture
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lambda架构
- en: Application architecture is very important for any kind of software development.
    It is the blueprint that decides how the software has to be built with a good
    amount of generality and the capability to customize when needed. For common application
    needs, some popular architectures are available, and there is no need for any
    ground-up architecture effort in order to use them. These public architecture
    frameworks are designed by some of the best minds for the benefit of the general
    public. These popular architectures are very useful because there is no barrier
    to entry, and they are used by so many people. There are popular architectures
    available for web application development, data processing, and so on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序架构对于任何类型的软件开发都至关重要。它是决定软件如何构建的蓝图，具有一定程度的通用性，并在需要时具备定制能力。对于常见的应用需求，已有一些流行的架构可供选择，无需从头开始构建架构。这些公共架构框架由一些顶尖人才设计，旨在惠及大众。这些流行的架构非常有用，因为它们没有准入门槛，并被许多人使用。对于Web应用程序开发、数据处理等，都有流行的架构可供选择。
- en: 'Lambda Architecture is a recent and popular architecture that''s ideal for
    developing data processing applications. There are many tools and technologies
    available in the market to develop data processing applications. But independent
    of the technology, how the data processing application components are layered
    and composed together is driven by the architectural framework. That is why Lambda
    Architecture is a technology-agnostic architecture framework and, depending on
    the need, the appropriate technology choice can be made to develop the individual
    components. *Figure 1* captures the essence of Lambda Architecture:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda架构是一种新兴且流行的架构，非常适合开发数据处理应用程序。市场上有许多工具和技术可用于开发数据处理应用程序。但无论采用何种技术，数据处理应用程序组件的分层和组合方式都由架构框架驱动。这就是为什么Lambda架构是一种与技术无关的架构框架，根据需要，可以选择适当的技术来开发各个组件。*图1*捕捉了Lambda架构的精髓：
- en: '![Lambda Architecture](img/image_09_001.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Lambda架构](img/image_09_001.jpg)'
- en: Figure 1
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: 'Lambda Architecture consists of three layers:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda架构由三个层次组成：
- en: The batch layer is the main data store. Any kind of processing happens on this
    dataset. This is the golden dataset.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理层是主要的数据存储。任何类型的处理都在此数据集上进行。这是黄金数据集。
- en: The serving layer processes the master dataset and prepares views for specific
    purposes, and they are termed purposed views here. This intermediate step of processing
    is required to serve the queries, or for generating outputs for specific needs.
    The queries and the specific dataset preparation don't directly access the master
    dataset.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务层处理主数据集并准备特定目的的视图，在此称为有目的的视图。此中间处理步骤对于服务查询或为特定需求生成输出是必要的。查询和特定数据集准备不直接访问主数据集。
- en: The speed layer is all about data stream processing. The stream of data is processed
    in a real-time fashion and volatile real-time views are prepared if that is a
    business need. The queries or specific processes generating outputs may consume
    data from both the purposed data views and real-time views.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度层关注数据流处理。数据流以实时方式处理，如果业务需要，会准备易变的实时视图。查询或生成输出的特定过程可能同时消耗有目的的数据视图和实时视图中的数据。
- en: Using the principles of Lambda Architecture to architect a big data processing
    system, Spark is going to be used here as a data processing tool. Spark fits nicely
    into all the data processing requirements in all three distinct layers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 利用Lambda架构原理来构建大数据处理系统，此处将使用Spark作为数据处理工具。Spark完美适配所有三层不同数据处理需求。
- en: This chapter is going to discuss some selected data processing use cases of
    a microblogging application. The application functionality, its deployment infrastructure,
    and the scalability factors are beyond the scope of this work. In a typical batch
    layer, the master dataset can be plain splittable serialization formats or NoSQL
    data stores, depending on the data access methods. If the application use cases
    are all batch operations, then standard serialization formats will be sufficient.
    But if the use cases mandate random access, NoSQL data stores will be ideal. Here,
    for the sake of simplicity, all the data files are stored in plain text files
    locally.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论一个微博应用的若干精选数据处理用例。应用功能、部署基础设施及可扩展性因素不在本工作讨论范围之内。在典型的批处理层中，主数据集可以是简单的可分割序列化格式或NoSQL数据存储，具体取决于数据访问方法。如果应用用例均为批处理操作，则标准序列化格式已足够。但如果用例要求随机访问，NoSQL数据存储将是理想选择。为简化起见，此处所有数据文件均本地存储为纯文本文件。
- en: Typical application development culminates in a completely functional application.
    But here, the use cases are realized in Spark data processing applications. Data
    processing always works as a part of the functionality of the main application
    and it is scheduled to run in batch mode or run as a listener waiting for data
    and processing it. So, corresponding to each of the use cases, individual Spark
    applications are developed, and they can be scheduled or made to run in listener
    mode as the case may be.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的应用开发以完全功能性应用告终。但在此，用例通过Spark数据处理应用实现。数据处理始终作为主应用功能的一部分，并按计划以批处理模式运行或作为监听器等待数据并进行处理。因此，针对每个用例，开发了独立的Spark应用，并根据情况安排其运行或处于监听模式。
- en: Microblogging with Lambda Architecture
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于Lambda架构的微博
- en: Blogging has been around for couple of decades in various forms. In the initial
    days of blogging as a medium of publication, only professional or aspiring writers
    published articles through the medium of blogs. It spread the false notion that
    only serious content is published through blogs. In recent years, the concept
    of microblogging included the general public in the culture of blogging. Microblogs
    are sudden outbursts of the thought processes of people in the form of a few sentences,
    photos, videos, or links. Sites such as Twitter and Tumblr popularized this culture
    at the biggest scale possible with hundreds of millions of active users using
    the site.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 博客作为一种出版媒介已有数十年历史，形式多样。在博客初期，只有专业或有抱负的作家通过博客发表文章。这传播了一个错误观念，即只有严肃内容才通过博客发布。近年来，微博客的概念将公众纳入了博客文化。微博客是人们思维过程的突然爆发，以几句话、照片、视频或链接的形式呈现。Twitter和Tumblr等网站以最大规模推广了这种文化，拥有数亿活跃用户。
- en: An overview of SfbMicroBlog
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SfbMicroBlog概览
- en: '**SfbMicroBlog**is a microblogging application with millions of users posting
    short messages. A new user who is going to use this application needs to sign
    up with a username and password. To post messages, users have to sign in first.
    The only thing users can do without signing in is read public messages posted
    by users. Users can follow other users. The act of following is a one-way relationship.
    If user A follows user B, user A can see all the messages posted by user B; at
    the same time, user B cannot see the messages posted by user A, because user B
    is not following user A. By default, all the messages posted by all the users
    are public messages and can be seen by everybody. But users have settings to make
    messages visible only to users who are following the message owner. After becoming
    a follower, unfollowing is also allowed.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**SfbMicroBlog**是一个拥有数百万用户发布短消息的微博应用。新用户若要使用此应用，需先注册用户名和密码。要发布消息，用户必须先登录。用户在不登录的情况下唯一能做的就是阅读其他用户发布的公开消息。用户可以关注其他用户。关注是一种单向关系。如果用户A关注用户B，用户A可以看到用户B发布的所有消息；同时，用户B看不到用户A发布的消息，因为用户B没有关注用户A。默认情况下，所有用户发布的所有消息都是公开的，任何人都可以看到。但用户可以设置，使消息仅对其关注者可见。成为关注者后，也可以取消关注。'
- en: Usernames have to be unique across all users. A username and password are required
    to sign in. Every user must have a primary e-mail address, and without that the
    signup process will not be complete. For extra security and password recovery,
    an alternate e-mail address or mobile phone number can be saved in the profile.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用户名必须在所有用户中唯一。登录需要用户名和密码。每个用户必须有一个主要电子邮件地址，否则注册过程将无法完成。为了额外的安全性和密码恢复，可以在个人资料中保存备用电子邮件地址或手机号码。
- en: 'Messages cannot exceed the a of 140 characters. Messages can contain words
    prefixed with the # symbol to group them under various topics. Messages can contain
    usernames prefixed with the @ symbol to directly address users through messages
    that are posted. In other words, users can address any other user in their messages
    without being a follower.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 消息不得超过140个字符。消息可以包含以#符号为前缀的单词，以便将它们归类到各种话题下。消息可以包含以@符号为前缀的用户名，以便通过发布的消息直接向用户发送消息。换句话说，用户可以在他们的消息中提及任何其他用户，而无需成为其关注者。
- en: Once posted, the messages cannot be changed. Once posted, the messages cannot
    be deleted.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦发布，消息无法更改。一旦发布，消息无法删除。
- en: Getting familiar with data
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熟悉数据
- en: 'All pieces of data that come to the master dataset come through a stream. The
    data stream is processed, an appropriate header for each message is inspected,
    and the right action to store it in the data store is done. The following list
    contains the important data items that come into the store through the same stream:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所有进入主数据集的数据都通过一个数据流。数据流经过处理，对每条消息的适当头部进行检查，并采取正确的行动将其存储在数据存储中。以下列表包含了通过同一数据流进入存储的重要数据项：
- en: '**User**: This dataset contains the user details when a user signs in or when
    a user''s data gets changed'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户**：此数据集包含用户登录时或用户数据发生变更时的用户详细信息。'
- en: '**Follower**: This dataset contains the relationship data that gets captured
    when a user opts to follow another user'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注者**：此数据集包含当用户选择关注另一用户时捕获的关系数据。'
- en: '**Message**: This dataset contains the messages posted by registered users'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消息**：此数据集包含注册用户发布的消息。'
- en: 'This list of datasets forms the golden dataset. Based on this master dataset,
    various views are created that cater to the needs of the vital business functions
    in the application. The following list contains the important views of the master
    dataset:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集构成了黄金数据集。基于此主数据集，创建了各种视图，以满足应用中关键业务功能的需求。以下列表包含了主数据集的重要视图：
- en: '**Messages by users**: This view contains messages posted by each user in the
    system. When a given user wants to see the messages posted by him/her, the data
    generated by this view is used. This is also used by the given user''s followers.
    This is a situation where the main dataset is used for a specific purpose. The
    message dataset gives all the required data for this view.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户发布的消息**：此视图包含系统中每个用户发布的消息。当特定用户想要查看自己发布的消息时，会使用此视图生成的数据。这也被该用户的关注者使用。这是一种特定目的使用主数据集的情况。消息数据集为此视图提供了所有必需的数据。'
- en: '**Messages to users**: In the messages, specific users can be addressed by
    prefixing the @ symbol followed by the addressee''s username. This data view contains
    the users addressed with the @ symbol and the corresponding messages. There is
    a limitation enforced in the implementation: you can only have one addressee in
    one message.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向用户发送消息**：在消息中，可以通过在@符号后加上收件人的用户名来指定特定用户。此数据视图包含被@符号标记的用户及其对应的消息。在实现中有一个限制：一条消息只能有一个收件人。'
- en: '**Tagged messages**: In the messages, words prefixed with the # symbol becomes
    searchable messages. For example, the word #spark in a message signifies that
    the message is searchable by the word #spark. For a given hashtag, users can see
    all the public messages and the messages of users whom he/she is following in
    one list. This view contains pairs of the hashtag and the corresponding messages.
    There is a limitation enforced in the implementation: you can only have one tag
    in one message.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签消息**：在消息中，以#符号开头的单词成为可搜索的消息。例如，消息中的#spark一词表示该消息可通过#spark进行搜索。对于给定的标签，用户可以在一个列表中查看所有公开消息以及他/她所关注用户的消息。此视图包含标签与相应消息的配对。在实现中有一个限制：一条消息只能有一个标签。'
- en: '**Follower users**: This view contains the list of users who are following
    a given user. In *Figure 2*, users **U1** and **U3** are in the list of users
    following **U4**.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注者用户**：此视图包含关注特定用户的用户列表。在*图2*中，用户**U1**和**U3**位于关注**U4**的用户列表中。'
- en: '**Followed users**: This view contains the list of users who are followed by
    a given user. In *Figure 2*, users **U2** and **U4** are in the list of users
    who are followed by user **U1**:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**被关注用户**：此视图包含被特定用户关注的用户列表。在*图2*中，用户**U2**和**U4**位于被用户**U1**关注的用户列表中：'
- en: '![Getting familiar with data](img/image_09_002.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![熟悉数据](img/image_09_002.jpg)'
- en: Figure 2
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: 'In a nutshell, *Figure 3* gives the Lambda Architecture view of the solution
    and gives details of the datasets and the corresponding views:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，*图3*给出了解决方案的Lambda架构视图，并详细说明了数据集及其对应的视图：
- en: '![Getting familiar with data](img/image_09_003-1.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![熟悉数据](img/image_09_003-1.jpg)'
- en: Figure 3
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3
- en: Setting the data dictionary
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置数据字典
- en: The data dictionary describes the data, its meaning, and its relationship with
    other data items. For the SfbMicroBlog application, the data dictionary is going
    to be a very minimalistic one to implement the selected use cases. Using this
    as a base, readers can expand and implement their own data items and include data
    processing use cases. The data dictionary is given for all the master datasets,
    as well as the data views.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据字典描述了数据、其含义以及与其他数据项的关系。对于SfbMicroBlog应用程序，数据字典将是一个非常简约的实现所选用例的工具。以此为基础，读者可以扩展并实现自己的数据项，并包含数据处理用例。数据字典为所有主数据集以及数据视图提供。
- en: 'The following table shows the data items of the user dataset:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了用户数据集的数据项：
- en: '| **User data** | **Type** | **Purpose** |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **用户数据** | **类型** | **用途** |'
- en: '| Id | Long | Used to uniquely identify a user, as well as being the vertex
    identifier in the user relationship graph |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 用户ID | 长整型 | 用于唯一标识用户，同时也是用户关系图中的顶点标识 |'
- en: '| Username | String | Used to uniquely identify users of the system |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 用户名 | 字符串 | 用于系统中用户的唯一标识 |'
- en: '| First name | String | Used to capture the first name of the user |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 名字 | 字符串 | 用于记录用户的名 |'
- en: '| Last name | String | Used to capture the last name of the user |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 姓氏 | 字符串 | 用于记录用户的姓 |'
- en: '| E-mail | String | Used to communicate with users |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 邮箱 | 字符串 | 用于与用户沟通 |'
- en: '| Alternate e-mail | String | Used for password recovery |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 备用邮箱 | 字符串 | 用于密码找回 |'
- en: '| Primary phone | String | Used for password recovery |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 主电话 | 字符串 | 用于密码找回 |'
- en: 'The following table captures the data items of the follower dataset:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下表捕捉了关注者数据集的数据项：
- en: '| **Follower data** | **Type** | **Purpose** |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| **关注者数据** | **类型** | **用途** |'
- en: '| Follower username | String | Used to identify who the follower is |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 关注者用户名 | 字符串 | 用于识别关注者身份 |'
- en: '| Followed username | String | Used to identify who is being followed |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 被关注用户名 | 字符串 | 用于识别被关注者 |'
- en: 'The following table captures the data items of the message dataset:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下表捕捉了消息数据集的数据项：
- en: '| **Message data** | **Type** | **Purpose** |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| **消息数据** | **类型** | **用途** |'
- en: '| Username | String | Used to capture the user who posted the message |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 用户名 | 字符串 | 用于记录发布消息的用户 |'
- en: '| Message Id | Long | Used to uniquely identify a message |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 消息ID | 长整型 | 用于唯一标识一条消息 |'
- en: '| Message | String | Used to capture the message that is being posted |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 消息 | 字符串 | 用于记录正在发布的消息 |'
- en: '| Timestamp | Long | Used to capture the time at which the message is posted
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 时间戳 | 长整型 | 用于记录消息发布的时间 |'
- en: 'The following table captures the data items of the Message to users view:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下表记录了用户查看消息的数据项：
- en: '| **Message to users data** | **Type** | **Purpose** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **消息至用户数据** | **类型** | **目的** |'
- en: '| From username | String | Used to capture the user who posted the message
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 来自用户名 | 字符串 | 用于记录发布消息的用户 |'
- en: '| To username | String | Used to capture the user to whom the message is addressed;
    it is the username that is prefixed with the @ symbol |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 目标用户名 | 字符串 | 用于记录消息的接收者；它是前缀带有@符号的用户名 |'
- en: '| Message Id | Long | Used to uniquely identify a message |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 消息ID | 长整型 | 用于唯一标识一条消息 |'
- en: '| Message | String | Used to capture the message that is being posted |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 消息 | 字符串 | 用于记录正在发布的消息 |'
- en: '| Timestamp | Long | Used to capture the time at which the message is posted
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 时间戳 | 长整型 | 用于记录消息发布的时间 |'
- en: 'The following table captures the data items of the Tagged messages view:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下表记录了标记消息视图的数据项：
- en: '| **Tagged messages data** | **Type** | **Purpose** |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **标记消息数据** | **类型** | **目的** |'
- en: '| Hashtag | String | The word that is prefixed with the # symbol |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 字符串 | 前缀带有#符号的单词 |'
- en: '| Username | String | Used to capture the user who posted the message |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 用户名 | 字符串 | 用于记录发布消息的用户 |'
- en: '| Message Id | Long | Used to uniquely identify a message |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 消息ID | 长整型 | 用于唯一标识一条消息 |'
- en: '| Message | String | Used to capture the message that is being posted |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 消息 | 字符串 | 用于记录正在发布的消息 |'
- en: '| Timestamp | Long | Used to capture the time at which the message is posted
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 时间戳 | 长整型 | 用于记录消息发布的时间 |'
- en: The follower relationship of the users is pretty straightforward and consists
    of the pairs of user identification numbers persisted in a data store.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 用户的关注关系相当直接，由存储在数据存储中的一对用户标识号组成。
- en: Implementing Lambda Architecture
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Lambda架构
- en: The concept of Lambda Architecture was introduced in the beginning of this chapter.
    Since it is a technology-agnostic architecture framework, when designing applications
    with it, it is imperative to capture the technology choices used in specific implementations.
    The following sections do exactly that.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开头介绍了Lambda架构的概念。由于它是一种与技术无关的架构框架，因此在设计应用程序时，必须记录特定实现中使用的技术选择。以下各节正是这样做的。
- en: Batch layer
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理层
- en: The core of the batch layer is a data store. For big data applications, there
    are plenty of choices for data stores. Typically, **Hadoop Distributed File System**
    (**HDFS**) in conjunction with Hadoop YARN is the current and accepted platform
    in which data is stored, mainly because of the ability to partition and distribute
    data across the Hadoop cluster.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理层的核心是一个数据存储。对于大数据应用，数据存储有很多选择。通常，**Hadoop分布式文件系统**（**HDFS**）与Hadoop YARN结合使用是目前公认的平台，主要是因为它能够在Hadoop集群中划分和分布数据。
- en: 'There are two types of data access any persistence store supports:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 任何持久存储支持的两种数据访问类型：
- en: Batch write/read
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量写入/读取
- en: Random write/read
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机写入/读取
- en: Both of these need separate data storage solutions. For batch data operations,
    typically splittable serialization formats such as Avro and Parquet are used.
    For random data operations, typically NoSQL data stores are used. Some of these
    NoSQL solutions sit on top of HDFS and some don't. It doesn't matter whether they
    are on top of HDFS or not, they provide partitioning and distribution of data.
    So depending on the use case and the distributed platform that is in use, appropriate
    solutions can be used.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种类型都需要单独的数据存储解决方案。对于批量数据操作，通常使用可分割的序列化格式，如Avro和Parquet。对于随机数据操作，通常使用NoSQL数据存储。其中一些NoSQL解决方案位于HDFS之上，而有些则不是。无论它们是否位于HDFS之上，它们都提供数据的划分和分布。因此，根据用例和使用的分布式平台，可以选择适当的解决方案。
- en: When it comes to the storage of the data in HDFS, commonly used formats such
    as XML and JSON fail because HDFS partitions and distributes the files. When that
    happens, these formats have opening tags and ending tags, and splits at random
    locations in the file make the data dirty. Because of that, splittable file formats
    such as Avro or Parquet are efficient for storing in HDFS.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到HDFS中的数据存储时，常用的格式如XML和JSON会失败，因为HDFS会对文件进行分区并分布。当这种情况发生时，这些格式具有开始标签和结束标签，文件中随机位置的分割会使数据变得脏乱。因此，可分割的文件格式如Avro或Parquet在HDFS中存储效率更高。
- en: When it comes to the NoSQL data store solutions, there are many choices in the
    market, especially from the open source world. Some of these NoSQL data stores,
    such as Hbase, sit on top of HDFS. Some of the NoSQL data stores, such as Cassandra
    and Riak, don't need HDFS, can be deployed on regular operating systems, and can
    be deployed in master-less fashion so that there is no single point of failure
    in a cluster. The choice of the NoSQL store is again dependent on the usage of
    a particular technology within the organization, the production support contracts
    in place, and many other parameters.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在NoSQL数据存储解决方案方面，市场上有许多选择，特别是在开源世界中。其中一些NoSQL数据存储，如Hbase，位于HDFS之上。其他一些NoSQL数据存储，如Cassandra和Riak，不需要HDFS，可以在常规操作系统上部署，并且可以以无主模式部署，从而在集群中没有单点故障。NoSQL存储的选择再次取决于组织内特定技术的使用、现有的生产支持合同以及其他许多参数。
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: This book doesn't recommend a given set of data store technologies for usage
    in conjunction with Spark because Spark drivers are available in abundance for
    most popular serialization formats and NoSQL data stores. In other words, most
    of the data store vendors have started supporting Spark in big way. Another interesting
    trend these days is that many of the prominent ETL tools have started supporting
    Spark, and because of that, those who are using such ETL tools may use Spark applications
    within their ETL processing pipelines.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本书并不推荐特定的数据存储技术与Spark结合使用，因为Spark驱动程序对于大多数流行的序列化格式和NoSQL数据存储都十分丰富。换句话说，大多数数据存储供应商已经开始大力支持Spark。另一个有趣的趋势是，许多主流的ETL工具已经开始支持Spark，因此使用这些ETL工具的用户可能会在其ETL处理管道中使用Spark应用程序。
- en: In this application, neither an HDFS-based nor any NoSQL-based data store is
    being used in order to maintain simplicity and to avoid the complex infrastructure
    setup required to run the application for the readers. Throughout, the data is
    stored on the local system in text file formats. Readers who are interested in
    trying out the examples on HDFS or other NoSQL data stores may go ahead and try
    them, with some changes to the data write/read part of the application.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本应用程序中，为了保持简单并避免运行应用程序所需的复杂基础设施设置，既没有使用基于HDFS的数据存储，也没有使用任何基于NoSQL的数据存储。整个过程中，数据以文本文件格式存储在本地系统上。对在HDFS或其他NoSQL数据存储上尝试示例感兴趣的读者可以继续尝试，只需对应用程序的数据写入/读取部分进行一些更改。
- en: Serving layer
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务层
- en: The serving layer can be implemented in Spark using various methods. If the
    data is not structured and is purely object-based, then the low-level RDD-based
    method is suitable. If the data is structured, a DataFrame is ideal. The use case
    that is being discussed here is dealing with structured data and hence wherever
    possible the Spark SQL library is going to be used. From the data stores, data
    is read and RDDs are created. The RDDs are converted to DataFrames and all the
    serving needs are accomplished using Spark SQL. In this way, the code is going
    to be succinct and easy to understand.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 服务层可以通过Spark使用多种方法实现。如果数据是非结构化的且纯粹基于对象，则适合使用低级别的RDD方法。如果数据是结构化的，DataFrame是理想选择。这里讨论的使用案例涉及结构化数据，因此只要有可能，就会使用Spark
    SQL库。从数据存储中读取数据并创建RDD。将RDD转换为DataFrames，并使用Spark SQL完成所有服务需求。这样，代码将简洁且易于理解。
- en: Speed layer
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速度层
- en: The speed layer is going to be implemented as a Spark Streaming application
    using Kafka as the broker with its own producers producing the messages. The Spark
    Streaming application will act as the consumer to the Kafka topics and receive
    the data that is getting produced. As discussed in the chapter covering Spark
    Streaming, the producers can be the Kafka console producer or any other producer
    supported by Kafka. But the Spark Streaming application working as the consumer
    here is not going to implement the logic of persisting the processed messages
    to the text file as they are not generally used in real-world use cases. Using
    this application as a base, readers can implement their own persistence mechanism.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 速度层将作为Spark Streaming应用程序实现，使用Kafka作为代理，其自己的生产者生成消息。Spark Streaming应用程序将作为Kafka主题的消费者，接收正在生产的数据。正如在涵盖Spark
    Streaming的章节中所讨论的，生产者可以是Kafka控制台生产者或Kafka支持的任何其他生产者。但这里的Spark Streaming应用程序作为消费者，不会实现将处理过的消息持久化到文本文件的逻辑，因为这在现实世界的用例中并不常见。以这个应用程序为基础，读者可以实现自己的持久化机制。
- en: Queries
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询
- en: 'The queries are all generated from the speed layer and serving layer. Since
    the data is available in the form of DataFrames, as mentioned before, all the
    queries for the use case are implemented using Spark SQL. The obvious reason is
    that Spark SQL works as a consolidation technology that unifies the data sources
    and destinations. When readers are using the samples from this book and when they
    are ready to implement it in their real-world use cases, the overall methodology
    can remain the same, but the data sources and destinations may differ. The following
    are some of the queries that can be generated from the serving layer. It is up
    to the imagination of the readers to make the required changes to the data dictionary
    and be able to write these views or queries:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所有查询都来自速度层和服务层。由于数据以DataFrames的形式提供，如前所述，该用例的所有查询都是使用Spark SQL实现的。显而易见的原因是Spark
    SQL作为一种整合技术，统一了数据源和目的地。当读者使用本书中的示例，并准备将其应用于现实世界的用例时，整体方法可以保持不变，但数据源和目的地可能会有所不同。以下是服务层可以生成的一些查询。读者可以根据需要修改数据字典，并能够编写这些视图或查询，这取决于他们的想象力：
- en: Find the messages that are grouped by a given hashtag
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找按给定标签分组的消息
- en: Find the messages that are addressed to a given user
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找发送给指定用户的消息
- en: Find the followers of a given user
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找指定用户的关注者
- en: Find the followed users of a given user
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找指定用户关注的用户
- en: Working with Spark applications
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark应用程序
- en: 'The workhorse of this application is the data processing engine consisting
    of many Spark applications. In general, they can be classified into the following
    types:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个应用程序的工作主力是数据处理引擎，由多个Spark应用程序组成。一般来说，它们可以分为以下类型：
- en: 'A Spark Streaming application to ingest data: This is the main listener application
    that receives the data coming as a stream and stores it in the appropriate master
    dataset.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄取数据的Spark Streaming应用程序：这是主要的监听应用程序，接收作为流过来的数据，并将其存储在适当的主数据集中。
- en: 'A Spark application to create purposed views and queries: This is the application
    that is used to create various purposed views from the master datasets. Apart
    from that, the queries are also included in this application.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建目的视图和查询的Spark应用程序：这是用于从主数据集中创建各种目的视图的应用程序。除此之外，查询也包含在这个应用程序中。
- en: 'A Spark GraphX application to do custom data processing: This is the application
    that is used to process the user-follower relationship.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行自定义数据处理的Spark GraphX应用程序：这是用于处理用户-关注者关系的应用程序。
- en: All these applications are developed independently and they are submitted independently,
    but the stream processing application will be always running as a listener application
    to process the incoming messages. Apart from the main data streaming application,
    all the other applications are scheduled like regular jobs, such as cron jobs
    in a UNIX system. In this application, all these applications are producing various
    purposed views. The scheduling depends on the kind of application and how much
    delay is affordable between the main dataset and the views. It completely depends
    on the business functions. So this chapter is going to focus on Spark application
    development rather than scheduling, to keep the focus on the lessons learned in
    the earlier chapters.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些应用程序都是独立开发的，并且独立提交，但流处理应用程序将始终作为侦听应用程序运行，以处理传入的消息。除了主要的数据流应用程序外，所有其他应用程序都像常规作业一样进行调度，例如UNIX系统中的cron作业。在此应用程序中，所有这些应用程序都在生成各种目的的视图。调度取决于应用程序的类型以及主数据集和视图之间可以承受多少延迟。这完全取决于业务功能。因此，本章将重点放在Spark应用程序开发上，而不是调度上，以保持对前面章节中学到的内容的专注。
- en: Tip
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: It is not ideal to persist the data from the speed layer into text files when
    implementing real-world use cases. For simplicity, all the data is stored in text
    files to empower all levels of reader with the simplest setup. The speed layer
    implementation using Spark Streaming is a skeleton implementation without the
    persistence logic. Readers can enhance this to introduce persistence to their
    desired data stores.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现现实世界的用例时，将速度层的数据持久化到文本文件中并不是理想的选择。为了简化，所有数据都存储在文本文件中，以便为所有级别的读者提供最简单的设置。使用Spark
    Streaming实现的速度层是一个没有持久化逻辑的骨架实现。读者可以对其进行增强，以将持久化引入到他们所需的数据存储中。
- en: Coding style
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码风格
- en: Coding style has been discussed and lots of Spark application programming has
    been  done in the earlier chapters. By now, it has been proven in this book that
    Spark application development can be done in Scala, Python, and R. In most of
    the earlier chapters, the languages of choice were Scala and Python. In this chapter,
    the same trend will continue. Only for the Spark GraphX application, since there
    is no Python support, will the application be developed in Scala alone.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 编码风格已在前面章节中讨论过，并且已经完成了大量Spark应用程序编程。到目前为止，本书已经证明Spark应用程序开发可以使用Scala、Python和R进行。在大多数前面章节中，首选语言是Scala和Python。在本章中，这一趋势将继续。仅对于Spark
    GraphX应用程序，由于没有Python支持，应用程序将仅使用Scala开发。
- en: The style of coding is going to be simple and to the point. The error handling
    and other best practices of application development are avoided deliberately to
    focus on the Spark features. In this chapter, wherever possible, the code is run
    from the appropriate language's Spark REPL. Since the anatomy of the complete
    application and the scripts to compile, build, and run them as applications have
    already been covered in the chapter that discussed Spark Streaming, the source
    code download will have it available as complete ready-to-run applications. Moreover,
    the chapter covering Spark Streaming discussed the anatomy of a complete Spark
    application, including the scripts to build and run Spark applications. The same
    methodology will be used in the applications that are going to be developed in
    this chapter too. When running such standalone Spark applications, as discussed
    in the initial chapters of this book, readers can enable Spark monitoring and
    see how the application is behaving. For the sake of brevity, these discussions
    will not be taken up again here.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 编码风格将简单明了。为了专注于Spark特性，故意避免了应用程序开发中的错误处理和其他最佳实践。在本章中，只要有可能，代码都是从相应语言的Spark REPL运行的。由于完整应用程序的结构以及编译、构建和运行它们作为应用程序的脚本已经在讨论Spark
    Streaming的章节中涵盖，源代码下载将提供完整的即用型应用程序。此外，涵盖Spark Streaming的章节讨论了完整Spark应用程序的结构，包括构建和运行Spark应用程序的脚本。本章中将要开发的应用程序也将采用相同的方法。当运行本书初始章节中讨论的此类独立Spark应用程序时，读者可以启用Spark监控，并查看应用程序的行为。为了简洁起见，这些讨论将不再在此处重复。
- en: Setting up the source code
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置源代码
- en: '*Figure 4* shows the structure of the source code and the data directories
    that are being used in this chapter. A description of each of them is not provided
    here as the reader should be familiar with them, and they have been covered in
    [Chapter 6](ch06.html "Chapter 6.  Spark Stream Processing"), *Spark Stream Processing*.
    There are external library file dependency requirements for running the programs
    using Kafka. For that, the instructions to download the JAR file are in the `TODO.txt`
    file in the `lib`folders. The `submitPy.sh` and `submit.sh` files use some of
    the `Kafka` libraries in the Kafta installation as well. All these external JAR
    file dependencies have already been covered in [Chapter 6](ch06.html "Chapter 6. 
    Spark Stream Processing"), *Spark Stream Processing*.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4*展示了本章使用的源代码和数据目录的结构。由于读者应已熟悉这些内容，且已在[第6章](ch06.html "第6章 Spark流处理")，*Spark流处理*中涵盖，此处不再赘述。运行使用Kafka的程序需要外部库文件依赖。为此，下载JAR文件的说明位于`lib`文件夹中的`TODO.txt`文件。`submitPy.sh`和`submit.sh`文件也使用了Kafka安装中的一些`Kafka`库。所有这些外部JAR文件依赖已在[第6章](ch06.html
    "第6章 Spark流处理")，*Spark流处理*中介绍。'
- en: '![Setting up the source code](img/image_09_004.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![设置源代码](img/image_09_004.jpg)'
- en: Figure 4
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图4
- en: Understanding data ingestion
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据摄取
- en: The Spark Streaming application works as the listener application that receives
    the data from its producers. Since Kafka is going to be used as the message broker,
    the Spark Streaming application will be its consumer application, listening to
    the topics for the messages sent by its producers. Since the master dataset in
    the batch layer has the following datasets, it is ideal to have individual Kafka
    topics for each of the topics, along with the datasets.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming应用作为监听应用，接收来自其生产者的数据。由于Kafka将用作消息代理，Spark Streaming应用将成为其消费者应用，监听主题以接收生产者发送的消息。由于批处理层的母数据集包含以下数据集，因此为每个主题及其数据集分别设置Kafka主题是理想的。
- en: User dataset:  User
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户数据集：用户
- en: 'Follower dataset: Follower'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注者数据集：关注者
- en: 'Message dataset: Message'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息数据集：消息
- en: '*Figure 5* provides an overall picture of the Kafka-based Spark Streaming application
    structure:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5*展示了基于Kafka的Spark Streaming应用的整体结构：'
- en: '![Understanding data ingestion](img/image_09_005.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![理解数据摄取](img/image_09_005.jpg)'
- en: Figure 5
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图5
- en: Since the Kafka setup has already been covered in [Chapter 6](ch06.html "Chapter 6. 
    Spark Stream Processing"), *Spark Stream Processing*, only the application code
    is covered here.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于已在[第6章](ch06.html "第6章 Spark流处理")，*Spark流处理*中介绍了Kafka设置，此处仅涵盖应用代码。
- en: The following scripts are run from a terminal window. Make sure that the `$KAFKA_HOME`
    environment variable is pointing to the directory where Kafka is installed. Also,
    it is very important to start Zookeeper, the Kafka server, the Kafka producer,
    and the Spark Streaming log event data processing application in separate terminal
    windows. Once the necessary Kafka topics are created as shown in the scripts,
    the appropriate producers have to start producing messages. Refer to the Kafka
    setup details that have already been covered in [Chapter 6](ch06.html "Chapter 6. 
    Spark Stream Processing"), *Spark Stream Processing*, before proceeding further.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本从终端窗口运行。确保`$KAFKA_HOME`环境变量指向Kafka安装目录。同时，在单独的终端窗口中启动Zookeeper、Kafka服务器、Kafka生产者以及Spark
    Streaming日志事件数据处理应用非常重要。一旦按照脚本创建了必要的Kafka主题，相应的生产者就必须开始发送消息。在进一步操作之前，请参考已在[第6章](ch06.html
    "第6章 Spark流处理")，*Spark流处理*中介绍的Kafka设置细节。
- en: 'Try the following commands in the terminal window prompt:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在终端窗口提示符下执行以下命令：
- en: '[PRE0]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This section provides the details of the Scala code for the Kafka topic consumer
    application that processes the messages produced by the Kafka producer. The assumption
    before running the following code snippet is that Kafka is up and running, the
    required producers are producing messages, and then, if the application is run,
    it will start consuming the messages. The Scala program for the data ingestion
    is run by submitting it to the Spark cluster. Starting from the Scala directory,
    as shown in *Figure 4*, first compile the program and then run it. The `README.txt`
    file is to be consulted for additional instructions. The two following commands
    are to be executed to compile and run the program:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了Scala代码的详细信息，该代码用于Kafka主题消费者应用程序，该应用程序处理Kafka生产者产生的消息。在运行以下代码片段之前，假设Kafka已启动并运行，所需的生产者正在产生消息，然后，如果运行该应用程序，它将开始消费这些消息。Scala数据摄取程序通过将其提交到Spark集群来运行。从Scala目录开始，如*图4*所示，首先编译程序，然后运行它。应查阅`README.txt`文件以获取额外指令。执行以下两条命令以编译和运行程序：
- en: '[PRE1]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following code is the program listing that is to be compiled and run using
    the preceding commands:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是将要使用前面命令编译和运行的程序清单：
- en: '[PRE2]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The Python program for the data ingestion is run by submitting it to the Spark
    cluster. Starting from the Python directory, as shown in *Figure 4*, run the program.
    The `README.txt`file is to be consulted for additional instructions. All the Kafka
    installation requirements are valid, even when running this Python program. The
    following command is to be followed for running the program. Since Python is an
    interpreted language, there is no compilation required here:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Python数据摄取程序通过将其提交到Spark集群来运行。从Python目录开始，如*图4*所示，运行程序。应查阅`README.txt`文件以获取额外指令。运行此Python程序时，所有Kafka安装要求仍然有效。以下命令用于运行程序。由于Python是解释型语言，此处无需编译：
- en: '[PRE3]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following code snippet is the Python implementation of the same application:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段是同一应用程序的Python实现：
- en: '[PRE4]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Generating purposed views and queries
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成目标视图和查询
- en: 'The following implementations in Scala and Python are the application that
    creates the purposed views and queries discussed in the earlier sections of this
    chapter. At the Scala REPL prompt, try the following statements:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Scala和Python的实现是本章前面部分讨论的创建目标视图和查询的应用程序。在Scala REPL提示符下，尝试以下语句：
- en: '[PRE5]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: These steps complete the process of loading all the required data from persistent
    stores into DataFrames. Here, the data comes from text files. In real-world use
    cases, it may come from popular NoSQL data stores, traditional RDBMS tables, or
    from Avro or Parquet serialized data stores loaded from HDFS.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤完成了将所有必需数据从持久存储加载到DataFrames的过程。这里，数据来自文本文件。在实际应用中，数据可能来自流行的NoSQL数据存储、传统RDBMS表，或是从HDFS加载的Avro或Parquet序列化数据存储。
- en: 'The following section uses these DataFrames and creates various purposed views
    and queries:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分使用这些DataFrames创建了各种目标视图和查询：
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the preceding Scala code snippet, the dataset and DataFrame-based programming
    model is being used because the programming language of choice was Scala. Now,
    since Python is not a strongly typed language, the Dataset API is not supported
    in Python, hence the following Python code uses the traditional RDD-based programming
    model of Spark in conjunction with the DataFrame-based programming model. At the
    Python REPL prompt, try the following statements:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述的Scala代码片段中，由于所选编程语言为Scala，因此使用了基于数据集和DataFrame的编程模型。现在，由于Python不是强类型语言，Python不支持Dataset
    API，因此下面的Python代码结合使用了Spark的传统RDD基础编程模型和基于DataFrame的编程模型。在Python REPL提示符下，尝试以下语句：
- en: '[PRE7]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'These steps complete the process of loading all the required data from persistent
    stores into DataFrames. Here, the data comes from text files. In real-world use
    cases, it may come from popular NoSQL data stores, traditional RDBMS tables, or
    from Avro or Parquet serialized data stores loaded from HDFS. The following section
    uses these DataFrames and creates various purposed views and queries:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤完成了将所有必需数据从持久存储加载到DataFrames的过程。这里，数据来自文本文件。在实际应用中，数据可能来自流行的NoSQL数据存储、传统RDBMS表，或是从HDFS加载的Avro或Parquet序列化数据存储。以下部分使用这些DataFrames创建了各种目标视图和查询：
- en: '[PRE8]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The purposed views and queries required to implement the use cases are developed
    as a single application. But in reality, it is not a good design practice to have
    all the views and queries in one application. It is good to separate them by persisting
    the views and refreshing them at regular intervals. If using only one application,
    caching and the use of custom-made context objects that are broadcasted to the
    Spark cluster could be employed to access the views.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现用例，提议的视图和查询被开发为一个单一的应用程序。但实际上，将所有视图和查询集中在一个应用程序中并不是一个好的设计实践。通过持久化视图并在定期间隔内刷新它们来分离它们是更好的做法。如果仅使用一个应用程序，可以通过缓存和使用自定义制作的环境对象来访问视图，这些对象会被广播到Spark集群。
- en: Understanding custom data processes
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解自定义数据处理流程
- en: The views created here were created to serve various queries and to produce
    desired outputs. There are some other classes of data processing applications
    that are often developed to implement real-world use cases. From the Lambda Architecture
    perspective, this also falls into the serving layer. The reason why these custom
    data processes fall into the serving layer is mainly because most of these use
    or process data from the master dataset and create views or outputs. It is also
    very possible for the custom processed data to remain as a view, and the following
    use case is one of such cases.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此处创建的视图旨在服务于各种查询并产生期望的输出。还有其他一些数据处理应用程序类别，通常是为了实现现实世界的用例而开发的。从Lambda架构的角度来看，这也属于服务层。这些自定义数据处理流程之所以属于服务层，主要是因为它们大多使用或处理来自主数据集的数据，并创建视图或输出。自定义处理的数据也很可能保持为视图，下面的用例就是其中之一。
- en: 'In the SfbMicroBlog microblogging application, it is a very common requirement
    to see whether a given user A is in some way connected to user B in a direct follower
    relationship or in a transitive way. This use case can be implemented using a
    graph data structure to see whether the two users in question are in the same
    connected component, whether they are connected in a transitive way, or whether
    they are not connected at all. For this, a graph is constructed with all the users
    as the vertices and the follow relationship as edges using a Spark GraphX library-based
    Spark application. At the Scala REPL prompt, try the following statements:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在SfbMicroBlog微博应用程序中，一个非常常见的需求是查看给定用户A是否以某种方式与用户B直接或间接相连。此用例可以通过使用图数据结构来实现，以查看这两个用户是否在同一连接组件中，是否以传递方式相连，或者是否根本不相连。为此，使用Spark
    GraphX库构建了一个图，其中所有用户作为顶点，关注关系作为边。在Scala REPL提示符下，尝试以下语句：
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The user graph with the users in the vertices and the connection relationship
    forming the edges is done. On this graph data structure, run the graph processing
    algorithm, the connected component algorithm. The following code snippet does
    this:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 用户图已经完成，其中用户位于顶点，连接关系形成边。在此图数据结构上，运行图处理算法，即连接组件算法。以下代码片段实现了这一点：
- en: '[PRE10]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The connected component graph, `cc`, and its triplets, `ccTriplets`, are created,
    and this can now be used to run various queries. Since the graph is an RDD-based
    data structure, if it is necessary to do queries, converting the graph RDD to
    DataFrames is a common practice. The following code demonstrates this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了连接组件图`cc`及其三元组`ccTriplets`，现在可以使用它们来运行各种查询。由于图是基于RDD的数据结构，如果需要进行查询，将图RDD转换为DataFrames是一种常见做法。以下代码演示了这一点：
- en: '[PRE11]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using the preceding implementation of a purposed view to get a list of users
    and their connected component identification numbers, if there is a need to find
    out whether two users are connected, just read the records of those two users
    and see whether they have the same connected component identification number.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述有目的的视图实现来获取用户列表及其连接组件标识号，如果需要查明两个用户是否相连，只需读取这两个用户的记录，并查看它们是否具有相同的连接组件标识号。
- en: References
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'For more information, visit the following links:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请访问以下链接：
- en: '[http://lambda-architecture.net/](http://lambda-architecture.net/)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lambda 架构网](http://lambda-architecture.net/)'
- en: '[https://www.dre.vanderbilt.edu/~schmidt/PDF/Context-Object-Pattern.pdf](https://www.dre.vanderbilt.edu/~schmidt/PDF/Context-Object-Pattern.pdf)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Context-Object-Pattern.pdf](https://www.dre.vanderbilt.edu/~schmidt/PDF/Context-Object-Pattern.pdf)'
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter concludes the book with one single application's use cases, implemented
    using the Spark concepts learned in the earlier chapters of the book. From a data
    processing application architecture perspective, this chapter covered the Lambda
    Architecture as a technology-agnostic architectural framework for data processing
    applications, which has huge applicability in the big data application development
    space.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以单一应用程序的使用案例作为全书的结尾，这些案例是利用本书前面章节学到的Spark概念实现的。从数据处理应用架构的角度来看，本章介绍了Lambda架构作为一种与技术无关的数据处理应用架构框架，在大数据应用开发领域具有巨大的适用性。
- en: From a data processing application development perspective, RDD-based Spark
    programming, Dataset-based Spark programming, Spark SQL-based DataFrames to process
    structured data, the Spark Streaming-based listener program that constantly listens
    to the incoming messages and processes them, and the Spark GraphX-based application
    to process follower relationships have been covered. The use cases covered so
    far have immense scope for readers to add their own functionalities and enhance
    the application use cases discussed in this chapter.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据处理应用开发的角度来看，涵盖了基于RDD的Spark编程、基于Dataset的Spark编程、基于Spark SQL的DataFrames处理结构化数据、基于Spark
    Streaming的监听程序持续监听传入消息并处理它们，以及基于Spark GraphX的应用程序处理关注者关系。到目前为止所涵盖的使用案例为读者提供了广阔的空间，以添加自己的功能并增强本章讨论的应用程序用例。
