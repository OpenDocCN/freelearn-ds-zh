- en: A First Taste and What's New in Apache Spark V2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初探Apache Spark V2的新特性
- en: '**Apache Spark** is a distributed and highly scalable in-memory data analytics
    system, providing you with the ability to develop applications in Java, Scala,
    and Python, as well as languages such as R. It has one of the highest contribution/involvement
    rates among the Apache top-level projects at this time. Apache systems, such as
    Mahout, now use it as a processing engine instead of MapReduce. It is also possible
    to use a Hive context to have the Spark applications process data directly to
    and from Apache Hive.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Spark**是一个分布式且高度可扩展的内存数据分析系统，为你提供了使用Java、Scala、Python以及R等语言开发应用程序的能力。它是当前Apache顶级项目中贡献/参与度最高的项目之一。Apache系统，如Mahout，现在将其作为处理引擎，而非MapReduce。还可以使用Hive上下文让Spark应用程序直接处理Apache
    Hive中的数据。'
- en: Initially, Apache Spark provided four main submodules--SQL, MLlib, GraphX, and
    Streaming. They will all be explained in their own chapters, but a simple overview
    would be useful here. The ...
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，Apache Spark提供了四个主要子模块——SQL、MLlib、GraphX和Streaming。它们将在各自的章节中进行解释，但在此之前，一个简单的概述将是有益的。...
- en: Spark machine learning
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark机器学习
- en: Machine learning is the real reason for Apache Spark because, at the end of
    the day, you don't want to just ship and transform data from A to B (a process
    called **ETL** (**Extract Transform Load**)). You want to run advanced data analysis
    algorithms on top of your data, and you want to run these algorithms at scale.
    This is where Apache Spark kicks in.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是Apache Spark的真正原因，因为归根结底，你不仅仅希望将数据从A地运送到B地（这一过程称为**ETL**（**提取、转换、加载**））。你希望在你的数据上运行高级数据分析算法，并且希望这些算法能够扩展。这正是Apache
    Spark发挥作用的地方。
- en: Apache Spark, in its core, provides the runtime for massive parallel data processing,
    and different parallel machine learning libraries are running on top of it. This
    is because there is an abundance of machine learning algorithms for popular programming
    languages like R and Python but they are not scalable. As soon as you load more
    data to the available main memory of the system, they crash.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark的核心提供了大规模并行数据处理的运行时环境，不同的并行机器学习库在其上运行。这是因为流行的编程语言如R和Python有大量机器学习算法，但它们不具备可扩展性。一旦你向系统可用主内存加载更多数据，它们就会崩溃。
- en: Apache Spark, in contrast, can make use of multiple computer nodes to form a
    cluster and even on a single node can spill data transparently to disk, therefore,
    avoiding the main memory bottleneck. Two interesting machine learning libraries
    are shipped with Apache Spark, but in this work, we'll also cover third-party
    machine learning libraries.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，Apache Spark可以利用多个计算机节点形成集群，并且即使在单个节点上，也能透明地将数据溢出到磁盘，从而避免主内存瓶颈。Apache Spark自带了两个有趣的机器学习库，但本工作还将涵盖第三方机器学习库。
- en: The Spark MLlib module, Classical MLlib, offers a growing but incomplete list
    of machine learning algorithms. Since the introduction of the **DataFrame**-based
    machine learning API called **SparkML**, the destiny of MLlib is clear. It is
    only kept for backward compatibility reasons.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib模块，即经典MLlib，提供了一个不断增长但尚不完整的机器学习算法列表。自从基于**DataFrame**的机器学习API——**SparkML**推出以来，MLlib的命运已定。它仅因向后兼容的原因而被保留。
- en: In SparkML, we have a machine learning library in place that can take advantage
    of these improvements out of the box, using it as an underlying layer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在SparkML中，我们已有一个机器学习库，该库开箱即用，可利用这些改进作为底层架构。
- en: SparkML will eventually replace MLlib. Apache SystemML introduces the first
    library running on top of Apache Spark that is not shipped with the Apache Spark
    distribution. SystemML provides you with an execution environment of R-style syntax
    with a built-in cost-based optimizer. Massive parallel machine learning is an
    area of constant change at a high frequency. It is hard to say where that the
    journey goes, but it is the first time where advanced machine learning at scale
    is available to everyone using open source and cloud computing.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: SparkML最终将取代MLlib。Apache SystemML推出了首个运行在Apache Spark之上的库，该库并非随Apache Spark发行版一同提供。SystemML为你提供了一个具有内置成本优化器的R风格语法执行环境。大规模并行机器学习是一个不断变化的高频领域。很难预测这一旅程将走向何方，但这是首次，使用开源和云计算的每个人都能获得大规模的高级机器学习。
- en: Deep learning on Apache Spark uses **H20**, **Deeplearning4j**, and **Apache
    SystemML**, which are other examples of very interesting third-party machine learning
    libraries that are not shipped with the Apache Spark distribution.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark上的深度学习使用**H2O**、**Deeplearning4j**和**Apache SystemML**，这些都是非常有趣的第三方机器学习库的例子，它们并未随Apache
    Spark分发。
- en: While H20 is somehow complementary to MLlib, Deeplearning4j only focuses on
    deep learning algorithms. Both use Apache Spark as a means for parallelization
    of data processing. You might wonder why we want to tackle different machine learning
    libraries.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管H2O在某种程度上与MLlib互补，但Deeplearning4j仅专注于深度学习算法。两者都使用Apache Spark作为数据处理并行化的手段。您可能会好奇为什么我们要研究不同的机器学习库。
- en: The reality is that every library has advantages and disadvantages with the
    implementation of different algorithms. Therefore, it often depends on your data
    and Dataset size which implementation you choose for best performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，每个库在实现不同算法时都有其优缺点。因此，通常取决于您的数据和数据集大小，您会选择哪种实现以获得最佳性能。
- en: However, it is nice that there is so much choice and you are not locked in a
    single library when using Apache Spark. Open source means openness, and this is
    just one example of how we are all benefiting from this approach in contrast to
    a single vendor, single product lock-in. Although recently Apache Spark integrated
    GraphX, another Apache Spark library into its distribution, we don't expect this
    will happen too soon. Therefore, it is most likely that Apache Spark as a central
    data processing platform and additional third-party libraries will co-exist, like
    Apache Spark being the big data operating system and the third-party libraries
    are the software you install and run on top of it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，令人高兴的是，使用Apache Spark时有如此多的选择，您不会被锁定在一个单一的库中。开源意味着开放性，这只是我们如何从与单一供应商、单一产品锁定相反的方法中受益的一个例子。尽管最近Apache
    Spark将另一个库GraphX集成到其分发中，但我们不期望这种情况会很快发生。因此，最有可能的是，Apache Spark作为一个中央数据处理平台和额外的第三方库将共存，就像Apache
    Spark是大数据操作系统，而第三方库是您在其上安装和运行的软件一样。
- en: Spark Streaming
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: '**Stream processing** is another big and popular topic for Apache Spark. It
    involves the processing of data in Spark as streams and covers topics such as
    input and output operations, transformations, persistence, and checkpointing,
    among others.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**流处理**是Apache Spark的另一个重大且流行的话题。它涉及在Spark中以流的形式处理数据，并涵盖了输入和输出操作、转换、持久性和检查点等主题。'
- en: Apache Spark Streaming will cover the area of processing, and we will also see
    practical examples of different types of stream processing. This discusses batch
    and window stream configuration and provides a practical example of checkpointing.
    It also covers different examples of stream processing, including Kafka and Flume.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark Streaming将涵盖处理领域，我们还将看到不同类型流处理的实际示例。这讨论了批处理和窗口流配置，并提供了一个检查点设置的实际示例。它还涵盖了包括Kafka和Flume在内的不同流处理示例。
- en: There are many ways in which stream data can be used. Other Spark module functionality
    (for example, SQL, MLlib, and GraphX) can be used to process the stream. You ...
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据有许多用途。其他Spark模块功能（例如，SQL、MLlib和GraphX）可用于处理流。您...
- en: Spark SQL
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: From Spark version 1.3, data frames have been introduced in Apache Spark, so
    that Spark data can be processed in a tabular form and tabular functions (such
    as `select`, `filter`, and `groupBy`) can be used to process data. The Spark SQL
    module integrates with Parquet and JSON formats, to allow data to be stored in
    formats, that better represent the data. This also offers more options to integrate
    with external systems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark版本1.3开始，Apache Spark引入了数据帧，使得Spark数据可以以表格形式处理，并可以使用表格函数（如`select`、`filter`和`groupBy`）来处理数据。Spark
    SQL模块与Parquet和JSON格式集成，允许数据以更好地表示数据的格式存储。这也提供了更多与外部系统集成的选项。
- en: The idea of integrating Apache Spark into the Hadoop Hive big data database
    can also be introduced. Hive context-based Spark applications can be used to manipulate
    Hive-based table data. This brings Spark's fast in-memory distributed processing
    to Hive's big data storage capabilities. It effectively lets Hive use Spark as
    a processing engine.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 将Apache Spark集成到Hadoop Hive大数据数据库的想法也可以引入。基于Hive上下文的Spark应用程序可用于操作基于Hive的表数据。这使得Hive能够利用Spark的快速内存分布式处理能力，有效地让Hive使用Spark作为处理引擎。
- en: Additionally, there is an abundance of additional connectors to access NoSQL
    databases outside the Hadoop ecosystem directly from Apache Spark.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有大量额外的连接器，可以直接从Apache Spark访问Hadoop生态系统之外的NoSQL数据库。
- en: Spark graph processing
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark图处理
- en: Graph processing is another very important topic when it comes to data analysis.
    In fact, a majority of problems can be expressed as a graph.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图处理是数据分析中另一个非常重要的主题。事实上，大多数问题都可以表示为图。
- en: A **graph** is basically, a network of items and their relationships to each
    other. Items are called **nodes** and relationships are called **edges**. Relationships
    can be directed or undirected. Relationships, as well as items, can have properties.
    So a map, for example, can be represented as a graph as well. Each city is a node
    and the streets between the cities are edges. The distance between the cities
    can be assigned as properties on the edge.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**图**基本上是一个项目及其相互关系的网络。项目称为**节点**，关系称为**边**。关系可以是定向的或非定向的。关系以及项目可以具有属性。因此，例如，地图也可以表示为图。每个城市是一个节点，城市之间的街道是边。城市之间的距离可以作为边上的属性分配。'
- en: The **Apache Spark GraphX** module allows Apache Spark to offer fast big data
    in-memory graph processing. This allows you to run graph algorithms ...
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Spark GraphX**模块使Apache Spark能够提供快速的大数据内存图处理。这使您能够运行图算法...'
- en: Extended ecosystem
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展生态系统
- en: When examining big data processing systems, we think it is important to look
    at, not just the system itself, but also how it can be extended and how it integrates
    with external systems so that greater levels of functionality can be offered.
    In a book of this size, we cannot cover every option, but by introducing a topic,
    we can hopefully stimulate the reader's interest so that they can investigate
    further.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在审视大数据处理系统时，我们认为不仅要关注系统本身，还要关注它如何扩展以及如何与外部系统集成，以便提供更高级别的功能。在这本书的篇幅中，我们无法涵盖每一种选择，但通过引入一个主题，我们希望能够激发读者的兴趣，使他们能够进一步研究。
- en: What's new in Apache Spark V2?
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Apache Spark V2中有哪些新变化？
- en: Since Apache Spark V2, many things have changed. This doesn't mean that the
    API has been broken. In contrast, most of the V1.6 Apache Spark applications will
    run on Apache Spark V2 with or without very little changes, but under the hood,
    there have been a lot of changes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自Apache Spark V2以来，许多事情都发生了变化。这并不意味着API已被破坏。相反，大多数V1.6的Apache Spark应用程序将在Apache
    Spark V2上运行，无论是否需要很少的更改，但在幕后，已经发生了很多变化。
- en: Although the **Java Virtual Machine** (**JVM**) is a masterpiece on its own,
    it is a general-purpose bytecode execution engine. Therefore, there is a lot of
    JVM object management and **garbage collection** (**GC**) overhead. So, for example,
    to store a 4-byte string, 48 bytes on the JVM are needed. The GC optimizes on
    object lifetime estimation, but Apache Spark often knows this better than JVM.
    Therefore, Tungsten disables the JVM GC for a subset of privately ...
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管**Java虚拟机**（**JVM**）本身是一件杰作，但它是一个通用的字节码执行引擎。因此，存在大量的JVM对象管理和**垃圾回收**（**GC**）开销。例如，存储一个4字节的字符串，在JVM上需要48字节。GC基于对象生命周期估计进行优化，但Apache
    Spark通常比JVM更了解这一点。因此，Tungsten对私有子集禁用了JVM GC...
- en: Cluster design
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群设计
- en: As we have already mentioned, Apache Spark is a distributed, in-memory, parallel
    processing system, which needs an associated storage system. So, when you build
    a big data cluster, you will probably use a distributed storage system such as
    Hadoop, as well as tools to move data such as Sqoop, Flume, and Kafka.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经提到的，Apache Spark是一个分布式、内存内并行处理系统，需要一个关联的存储系统。因此，当您构建大数据集群时，您可能会使用分布式存储系统，如Hadoop，以及用于移动数据的工具，如Sqoop、Flume和Kafka。
- en: 'We wanted to introduce the idea of edge nodes in a big data cluster. These
    nodes in the cluster will be client-facing, on which reside the client-facing
    components such as Hadoop NameNode or perhaps the Spark master. Majority of the
    big data cluster might be behind a firewall. The edge nodes would then reduce
    the complexity caused by the firewall as they would be the only points of contact
    accessible from outside. The following figure shows a simplified big data cluster:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在大数据集群中引入边缘节点的概念。这些集群中的节点将面向客户端，上面驻留着如Hadoop NameNode或可能是Spark master等客户端面向组件。大多数大数据集群可能位于防火墙后面。边缘节点将减少由防火墙引起的复杂性，因为它们将是外部可访问的唯一接触点。下图展示了一个简化的大数据集群：
- en: '![](img/c67b8c64-37f1-45fb-99f3-ee2d80a03065.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c67b8c64-37f1-45fb-99f3-ee2d80a03065.png)'
- en: It shows five simplified cluster nodes with executor JVMs, one per CPU core,
    and the Spark Driver JVM sitting outside the cluster. In addition, you see the
    disk directly attached to the nodes. This is called the **JBOD** (**just a bunch
    of disks**) approach. Very large files are partitioned over the disks and a virtual
    filesystem such as HDFS makes these chunks available as one large virtual file.
    This is, of course, stylized and simplified, but you get the idea.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 它展示了五个简化的集群节点，每个CPU核心有一个执行器JVM，以及位于集群外部的Spark驱动程序JVM。此外，您可以看到直接连接到节点的磁盘。这被称为**JBOD**（**只是一堆磁盘**）方法。非常大的文件在磁盘上分区，虚拟文件系统（如HDFS）将这些块作为一个大虚拟文件提供。当然，这是风格化和简化的，但您可以理解这个概念。
- en: The following simplified component model shows the driver JVM sitting outside
    the cluster. It talks to the Cluster Manager in order to obtain permission to
    schedule tasks on the worker nodes, because the Cluster Manager keeps track of
    resource allocation of all processes running on the cluster.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的简化组件模型展示了驱动程序JVM位于集群外部。它与集群管理器通信，以获得在worker节点上调度任务的许可，因为集群管理器负责跟踪集群上运行的所有进程的资源分配。
- en: 'As we will see later, there is a variety of different cluster managers, some
    of them also capable of managing other Hadoop workloads or even non-Hadoop applications
    in parallel to the Spark Executors. Note that the Executor and Driver have bidirectional
    communication all the time, so network-wise, they should also be sitting close
    together:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们稍后将看到的，存在多种不同的集群管理器，其中一些还能够管理其他Hadoop工作负载，甚至与Spark执行器并行运行的非Hadoop应用程序。请注意，执行器和驱动程序之间始终保持双向通信，因此从网络角度来看，它们也应该彼此靠近：
- en: '![](img/6e9505fc-7541-41a5-94b3-1b823fe6c1b4.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e9505fc-7541-41a5-94b3-1b823fe6c1b4.png)'
- en: 'Figure source: https://spark.apache.org/docs/2.0.2/cluster-overview.html'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图源：https://spark.apache.org/docs/2.0.2/cluster-overview.html
- en: Generally, firewalls, while adding security to the cluster, also increase the
    complexity. Ports between system components need to be opened up so that they
    can talk to each other. For instance, Zookeeper is used by many components for
    configuration. Apache Kafka, the publish/subscribe messaging system, uses Zookeeper
    to configure its topics, groups, consumers, and producers. So, client ports to
    Zookeeper, potentially across the firewall, need to be open.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，虽然防火墙为集群增加了安全性，但也增加了复杂性。系统组件之间的端口需要打开，以便它们可以相互通信。例如，Zookeeper被许多组件用于配置。Apache
    Kafka，发布/订阅消息系统，使用Zookeeper来配置其主题、组、消费者和生产者。因此，需要打开到Zookeeper的客户端端口，可能跨越防火墙。
- en: Finally, the allocation of systems to cluster nodes needs to be considered.
    For instance, if Apache Spark uses Flume or Kafka, then in-memory channels will
    be used. The size of these channels, and the memory used, caused by the data flow,
    need to be considered. Apache Spark should not be competing with other Apache
    components for memory usage. Depending on your data flows and memory usage, it
    might be necessary to have Spark, Hadoop, Zookeeper, Flume, and other tools on
    distinct cluster nodes. Alternatively, resource managers such as YARN, Mesos,
    or Docker can be used to tackle this problem. In standard Hadoop environments,
    YARN is most likely.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，需要考虑将系统分配给集群节点的方案。例如，如果Apache Spark使用Flume或Kafka，则会使用内存通道。这些通道的大小以及数据流导致的内存使用量需要考虑。Apache
    Spark不应与其他Apache组件竞争内存使用。根据您的数据流和内存使用情况，可能需要在不同的集群节点上部署Spark、Hadoop、Zookeeper、Flume和其他工具。或者，可以使用YARN、Mesos或Docker等资源管理器来解决此问题。在标准的Hadoop环境中，YARN最有可能被采用。
- en: Generally, the edge nodes that act as cluster NameNode servers or Spark master
    servers will need greater resources than the cluster processing nodes within the
    firewall. When many Hadoop ecosystem components are deployed on the cluster, all
    of them will need extra memory on the master server. You should monitor edge nodes
    for resource usage and adjust in terms of resources and/or application location
    as necessary. YARN, for instance, is taking care of this.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，作为集群NameNode服务器或Spark主服务器的边缘节点将需要比防火墙内的集群处理节点更多的资源。当许多Hadoop生态系统组件部署在集群上时，它们都需要在主服务器上额外内存。您应该监控边缘节点的资源使用情况，并根据需要调整资源和/或应用程序位置。例如，YARN正在处理这个问题。
- en: This section has briefly set the scene, for the big data cluster in terms of
    Apache Spark, Hadoop, and other tools. However, how might the Apache Spark cluster
    itself, within the big data cluster, be configured? For instance, it is possible
    to have many types of Spark cluster manager. The next section will examine this
    and describe each type of the Apache Spark cluster manager.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要介绍了大数据集群中的 Apache Spark、Hadoop 及其他工具。但是，大数据集群内部，Apache Spark 集群本身可能如何配置呢？例如，可以有多种类型的
    Spark 集群管理器。下一节将探讨这一点，并描述每种 Apache Spark 集群管理器的类型。
- en: Cluster management
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群管理
- en: The Spark context, as you will see in many of the examples in this book, can
    be defined via a Spark configuration object and Spark URL. The Spark context connects
    to the Spark cluster manager, which then allocates resources across the worker
    nodes for the application. The cluster manager allocates executors across the
    cluster worker nodes. It copies the application JAR file to the workers and finally
    allocates tasks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 上下文，正如你在本书的许多示例中看到的，可以通过 Spark 配置对象和 Spark URL 来定义。Spark 上下文连接到 Spark
    集群管理器，后者随后在集群的工作节点之间分配资源给应用程序。集群管理器在集群的工作节点上分配执行器。它将应用程序 JAR 文件复制到工作节点，并最终分配任务。
- en: The following subsections describe the possible Apache Spark cluster manager
    options available at this time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节描述了目前可用的 Apache Spark 集群管理器的各种选项。
- en: Local
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地
- en: By specifying a Spark configuration local URL, it is possible to have the application
    run locally. By specifying `local[n]`, it is possible to have Spark use *n* threads
    to run the application locally. This is a useful development and test option because
    you can also test some sort of parallelization scenarios but keep all log files
    on a single machine.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定一个本地 Spark 配置 URL，可以使应用程序在本地运行。通过指定 `local[n]`，可以使 Spark 使用 *n* 个线程在本地运行应用程序。这是一个有用的开发和测试选项，因为你还可以测试某种并行化场景，但将所有日志文件保留在单个机器上。
- en: Standalone
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Standalone
- en: 'Standalone mode uses a basic cluster manager that is supplied with Apache Spark.
    The spark master URL will be as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Standalone 模式使用 Apache Spark 自带的基本集群管理器。Spark 主节点的 URL 将如下所示：
- en: '`Spark://<hostname>:7077`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`Spark://<hostname>:7077`'
- en: Here, `<hostname>` is the name of the host on which the Spark master is running.
    We have specified `7077` as the port, which is the default value, but this is
    configurable. This simple cluster manager currently supports only **FIFO** (**first-in-first-out**)
    scheduling. You can contrive to allow concurrent application scheduling by setting
    the resource configuration options for each application; for instance, using `spark.core.max`
    to share cores between applications.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，`<hostname>` 表示运行 Spark 主节点的宿主机的名称。我们已将端口指定为 `7077`，这是默认值，但可配置。当前这种简单的集群管理器仅支持
    **FIFO**（**先进先出**）调度策略。你可以通过为每个应用程序设置资源配置选项来设法实现并发应用调度；例如，使用 `spark.core.max`
    在应用程序之间共享核心。
- en: Apache YARN
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache YARN
- en: At a larger scale, when integrating with Hadoop YARN, the Apache Spark cluster
    manager can be YARN and the application can run in one of two modes. If the Spark
    master value is set as `yarn-cluster`, then the application can be submitted to
    the cluster and then terminated. The cluster will take care of allocating resources
    and running tasks. However, if the application master is submitted as `yarn-client`,
    then the application stays alive during the life cycle of processing, and requests
    resources from YARN.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在更大规模上，当与 Hadoop YARN 集成时，Apache Spark 集群管理器可以是 YARN，应用程序可以运行在两种模式之一。如果将 Spark
    主节点值设置为 `yarn-cluster`，则可以将应用程序提交到集群并随后终止。集群将负责分配资源和运行任务。然而，如果应用程序主节点以 `yarn-client`
    方式提交，则应用程序在处理周期内保持活动状态，并向 YARN 请求资源。
- en: Apache Mesos
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Mesos
- en: '**Apache Mesos** is an open source system, for resource sharing across a cluster.
    It allows multiple frameworks, to share a cluster by managing and scheduling resources.
    It is a cluster manager, that provides isolation using Linux containers and allowing
    multiple systems such as Hadoop, Spark, Kafka, Storm, and more to share a cluster
    safely. It is highly scalable to thousands of nodes. It is a master/slave-based
    system and is fault tolerant, using Zookeeper for configuration management.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Mesos** 是一个开源系统，用于集群间的资源共享。它允许多个框架通过管理和调度资源来共享集群。作为一个集群管理器，它利用 Linux
    容器提供隔离，并允许 Hadoop、Spark、Kafka、Storm 等多种系统安全地共享集群。它高度可扩展至数千个节点。它是一个基于主/从的系统，并具有故障容忍性，使用
    Zookeeper 进行配置管理。'
- en: 'For a single master node Mesos cluster, the Spark master URL will be in this
    form:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个主节点的Mesos集群，Spark主URL将采用以下形式：
- en: '`mesos://<hostname>:5050`.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`mesos://<hostname>:5050`.'
- en: Here, `<hostname>` is the hostname of the Mesos master server; the port is defined
    as `5050,` which is the default Mesos master port (this is ...
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，`<hostname>`是Mesos主服务器的hostname；端口定义为`5050`，这是默认的Mesos主端口（...）
- en: Cloud-based deployments
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于云的部署
- en: There are three different abstraction levels of cloud systems--**Infrastructure
    as a Service** (**IaaS**), **Platform as a Service** (**PaaS**), and **Software
    as a Service** (**SaaS**). We will see how to use and install Apache Spark on
    all of these.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 云系统有三种不同的抽象层次——**基础设施即服务**（**IaaS**）、**平台即服务**（**PaaS**）和**软件即服务**（**SaaS**）。我们将探讨如何在所有这些层面上使用和安装Apache
    Spark。
- en: The new way to do IaaS is Docker and Kubernetes as opposed to virtual machines,
    basically providing a way to automatically set up an Apache Spark cluster within
    minutes. The advantage of Kubernetes is that it can be used among multiple different
    cloud providers as it is an open standard and also based on open source.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 新的IaaS方式是Docker和Kubernetes，与虚拟机相对，基本上提供了一种在几分钟内自动设置Apache Spark集群的方法。Kubernetes的优势在于，由于它是开放标准且基于开源，因此可以在多个不同的云提供商之间使用。
- en: You even can use Kubernetes, in a local data center and transparently and dynamically
    move workloads between local, dedicated, and public cloud data centers. PaaS,
    in contrast, takes away from you the burden of installing and operating an Apache
    Spark cluster because this is provided as a service.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以使用Kubernetes，在本地数据中心内透明且动态地移动工作负载，跨越本地、专用和公共云数据中心。相比之下，PaaS为你减轻了安装和操作Apache
    Spark集群的负担，因为这作为一项服务提供。
- en: There is an ongoing discussion, whether Docker is IaaS or PaaS but, in our opinion,
    this is just a form of a lightweight preinstalled virtual machine. This is particularly
    interesting because the offering is completely based on open source technologies,
    which enables you to replicate the system on any other data center.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Docker是IaaS还是PaaS的讨论仍在进行中，但在我们看来，它只是一种轻量级预装虚拟机形式。这一点特别有趣，因为其完全基于开源技术，使得你能够在任何其他数据中心复制该系统。
- en: One of the open source components, we'll introduce is Jupyter notebooks; a modern
    way to do data science, in a cloud-based collaborative environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍的开源组件之一是Jupyter笔记本；一种在基于云的协作环境中进行数据科学的现代方式。
- en: Performance
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能
- en: 'Before moving on to the rest of the chapters, covering functional areas of
    Apache Spark and extensions, we will examine the area of performance. What issues
    and areas need to be considered? What might impact the Spark application performance,
    starting at the cluster level and finishing with actual Scala code? We don''t
    want to just repeat, what the Spark website says, so take a look at this URL:
    `http://spark.apache.org/docs/<version>/tuning.html`.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入涵盖Apache Spark功能区域和扩展的其余章节之前，我们将审视性能领域。需要考虑哪些问题和领域？从集群级别到实际Scala代码，哪些因素可能影响Spark应用程序性能？我们不想仅仅重复Spark网站上的内容，因此请查看此URL：`http://spark.apache.org/docs/<version>/tuning.html`。
- en: Here, `<version>` relates to the version of Spark that you are using; that is,
    either the latest or something like `1.6.1` for a specific version. So, having
    looked at this page, we will briefly mention some of the topic areas. We will
    list some general points in this section without implying ...
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，`<version>`对应于你正在使用的Spark版本；即，最新版本或类似`1.6.1`的特定版本。因此，浏览此页面后，我们将简要提及一些主题领域。本节中，我们将列出一些一般性要点，但不暗示...
- en: The cluster structure
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群结构
- en: The size and structure of your big data cluster are going to affect performance.
    If you have a cloud-based cluster, your IO and latency will suffer, in comparison
    to an unshared hardware cluster. You will be sharing the underlying hardware,
    with multiple customers and the cluster hardware may be remote. There are some
    exceptions to this. The IBM cloud, for instance, offers dedicated bare metal high-performance
    cluster nodes, with an InfiniBand network connection, which can be rented on an
    hourly basis.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据集群的规模和结构将影响性能。如果你拥有一个基于云的集群，相比非共享硬件集群，你的IO和延迟将会受到影响。你将与多个客户共享底层硬件，且集群硬件可能位于远程。当然，也有例外。例如，IBM云提供按小时租赁的专用裸金属高性能集群节点，配备InfiniBand网络连接。
- en: Additionally, the positioning of cluster components on servers may cause resource
    contention. For instance, think carefully about locating Hadoop NameNodes, Spark
    servers, Zookeeper, Flume, and Kafka servers in large clusters. With high workloads,
    you might consider segregating servers to individual systems. You might also consider
    using an Apache system such as Mesos that provides better distributions and assignment
    of resources to the individual processes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，集群组件在服务器上的位置可能导致资源争用。例如，在大规模集群中仔细考虑Hadoop NameNodes、Spark服务器、Zookeeper、Flume和Kafka服务器的布局。在高负载情况下，您可能需要将服务器隔离到单独的系统中。您还可以考虑使用Apache
    Mesos等系统，它为各个进程提供更好的资源分配和分配。
- en: Consider potential parallelism as well. The greater the number of workers in
    your Spark cluster for large Datasets, the greater the opportunity for parallelism.
    One rule of thumb is one worker per hyper-thread or virtual core respectively.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同时考虑潜在的并行性。对于大型数据集，您的Spark集群中的工作者数量越多，实现并行处理的机会就越大。一个经验法则是每个超线程或虚拟核心分别对应一个工作者。
- en: Hadoop Distributed File System
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop分布式文件系统
- en: You might consider using an alternative to HDFS, depending upon your cluster
    requirements. For instance, IBM has the **GPFS** (**General Purpose File System**)
    for improved performance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的集群需求，您可能考虑使用HDFS的替代方案。例如，IBM提供了**GPFS**（**通用目的文件系统**）以提高性能。
- en: The reason why GPFS might be a better choice is that coming from the high-performance
    computing background, this filesystem has a full read-write capability, whereas
    HDFS is designed as a write once, read many filesystems. It offers an improvement
    in performance over HDFS because it runs at the kernel level as opposed to HDFS,
    which runs in a **Java Virtual Machine** (**JVM**) that in turn runs as an operating
    system process. It also integrates with Hadoop and the Spark cluster tools. IBM
    runs setups with several hundred petabytes using GPFS. ...
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: GPFS可能是更好选择的原因在于，它源自高性能计算背景，这种文件系统具有完整的读写能力，而HDFS设计为一次写入、多次读取的文件系统。它在性能上优于HDFS，因为它在核心级别运行，而HDFS在**Java虚拟机**（**JVM**）中运行，后者又作为操作系统进程运行。它还与Hadoop和Spark集群工具集成。IBM使用GPFS配置了数百PB的系统。...
- en: Data locality
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据局部性
- en: The key for good data processing performance is avoidance of network transfers.
    This was very true a couple of years ago, but is less relevant for tasks with
    high demands on CPU and low I/O, but for low demand on CPU and high I/O demand
    data processing algorithms, this still holds.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 良好数据处理性能的关键是避免网络传输。这在几年前是非常正确的，但对于CPU需求高、I/O需求低的任务来说，这不太相关，但对于CPU需求低、I/O需求高的数据处理算法，这仍然适用。
- en: We can conclude from this, that HDFS is one of the best ways to achieve data
    locality, as chunks of files are distributed on the cluster nodes, in most of
    the cases, using hard drives directly attached to the server systems. This means
    that those chunks can be processed in parallel using the CPUs on the machines
    where individual data chunks are located in order to avoid network transfer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由此我们可以得出结论，HDFS是实现数据局部性的最佳方式之一，因为文件块分布在集群节点上，在大多数情况下，使用直接连接到服务器系统的硬盘。这意味着可以在包含个别数据块的机器上使用CPU并行处理这些块，以避免网络传输。
- en: Another way to achieve data locality is using `ApacheSparkSQL`. Depending on
    the connector implementation, SparkSQL can make use of the data processing capabilities
    of the source engine. So, for example, when using MongoDB in conjunction with
    SparkSQL, parts of the SQL statement are preprocessed by MongoDB before data is
    sent upstream to Apache Spark.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种实现数据局部性的方法是使用`ApacheSparkSQL`。根据连接器实现的不同，SparkSQL可以利用源引擎的数据处理能力。例如，当结合使用MongoDB和SparkSQL时，SQL语句的部分内容在数据发送到Apache
    Spark之前由MongoDB预处理。
- en: Memory
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存
- en: 'In order to avoid **OOM** (**Out of Memory**) messages for the tasks on your
    Apache Spark cluster, please consider a number of questions for the tuning:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免Apache Spark集群上的任务出现**内存不足**（**OOM**）消息，请考虑以下调优问题：
- en: Consider the level of physical memory available on your Spark worker nodes.
    Can it be increased? Check on the memory consumption of operating system processes
    during high workloads in order to get an idea of free memory. Make sure that the
    workers have enough memory.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑您的Spark工作节点上可用的物理内存级别。是否可以增加？在高负载期间检查操作系统进程的内存消耗，以了解可用内存的情况。确保工作者有足够的内存。
- en: Consider data partitioning. Can you increase the number of partitions? As a
    rule of thumb, you should have at least as many partitions as you have available
    CPU cores on the cluster. Use the `repartition` function on the RDD API.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑数据分区。你能增加分区数量吗？一般而言，分区的数量应至少与集群中可用的CPU核心数相等。可使用RDD API中的`repartition`函数。
- en: Can you modify the storage fraction and the memory used by the JVM for storage
    and caching of RDDs? ...
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能调整用于存储和缓存RDD的JVM内存比例吗？...
- en: Coding
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码
- en: Try to tune your code, to improve the Spark application performance. For instance,
    filter your application-based data early in your ETL cycle. One example is, when
    using raw HTML files, detag them and crop away unneeded parts at an early stage.
    Tune your degree of parallelism, try to find the resource-expensive parts of your
    code, and find alternatives.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试优化你的代码，以提升Spark应用程序的性能。例如，在你的ETL周期早期基于应用程序数据进行过滤。一个例子是，当使用原始HTML文件时，在早期阶段去除标签并裁剪掉不需要的部分。调整并行度，尝试找出代码中资源消耗大的部分，并寻找替代方案。
- en: '**ETL** is one of the first things you are doing in an analytics project. So
    you are grabbing data, from third-party systems, either by directly accessing
    relational or NoSQL databases or by reading exports in various file formats such
    as, CSV, TSV, JSON or even more exotic ones from local or remote filesystems or
    from a staging area in HDFS: after some inspections and sanity checks on the files
    an ETL process in Apache Spark basically reads in the files and creates RDDs or
    DataFrames/Datasets out of them.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**ETL**是分析项目中首先要做的事情之一。因此，你正在从第三方系统抓取数据，要么直接访问关系型或NoSQL数据库，要么通过读取各种文件格式的导出，如CSV、TSV、JSON，甚至是来自本地或远程文件系统或HDFS中暂存区的更奇特的格式：在对文件进行一些检查和合理性检查后，Apache
    Spark中的ETL过程基本上读取这些文件并从中创建RDD或DataFrames/Datasets。'
- en: They are transformed, so that they fit the downstream analytics application,
    running on top of Apache Spark or other applications and then stored back into
    filesystems as either JSON, CSV or PARQUET files, or even back to relational or
    NoSQL databases.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 它们被转换以适应下游的分析应用程序，这些应用程序运行在Apache Spark或其他应用程序之上，然后存储回文件系统，格式可以是JSON、CSV或PARQUET文件，甚至返回到关系型或NoSQL数据库。
- en: 'Finally, I can recommend the following resource for any performance-related
    problems with Apache Spark: [https://spark.apache.org/docs/latest/tuning.html](https://spark.apache.org/docs/latest/tuning.html).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于任何与Apache Spark性能相关的问题，我推荐以下资源：[https://spark.apache.org/docs/latest/tuning.html](https://spark.apache.org/docs/latest/tuning.html)。
- en: Cloud
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云
- en: Although parts of this book will concentrate on examples of Apache Spark installed
    on physically server-based clusters, we want to make a point, that there are multiple
    cloud-based options out there that imply many benefits. There are cloud-based
    systems, that use Apache Spark as an integrated component and cloud-based systems
    that offer Spark as a service.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书的部分内容将专注于Apache Spark在物理服务器集群上安装的示例，但我们想强调，市面上存在多种基于云的选项，它们带来了许多好处。有些云系统将Apache
    Spark作为集成组件，而有些则提供Spark作为服务。
- en: Errors and recovery
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误与恢复
- en: 'Generally, the question that needs to be asked for your application is: is
    it critical that you receive and process all the data? If not, then on failure,
    you might just be able to restart the application and discard the missing or lost
    data. If this is not the case, then you will need to use checkpointing, which
    will be described in the next section.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于你的应用程序，需要问的问题是：是否必须接收并处理所有数据？如果不是，那么在失败时，你可能只需重启应用程序并丢弃缺失或丢失的数据。如果情况并非如此，那么你需要使用将在下一节中描述的检查点机制。
- en: It is also worth noting that your application's error management should be robust
    and self-sufficient. What we mean by this is that if an exception is non-critical,
    then manage the exception, perhaps log it, and continue processing. For instance,
    when a task reaches the maximum number of failures (specified by `spark.task.maxFailures`),
    it will terminate processing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 同样值得注意的是，你的应用程序的错误管理应该是健壮且自给自足的。我们的意思是，如果异常不是关键性的，那么管理该异常，可能记录它，并继续处理。例如，当任务达到最大失败次数（由`spark.task.maxFailures`指定）时，它将终止处理。
- en: This property, among others, can be set during creation of the `SparkContext`
    object or as additional command line parameters when invoking `spark-shell` or
    `spark-submit`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这一属性及其他属性，可以在创建`SparkContext`对象时设置，或者在调用`spark-shell`或`spark-submit`时作为额外的命令行参数。
- en: Summary
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In closing this chapter, we invite you to work your way, through each of the
    Scala code-based examples in the following chapters. The rate at which Apache
    Spark has evolved is impressive, and important to note is the frequency of the
    releases. So even though, at the time of writing, Spark has reached 2.2, we are
    sure that you will be using a later version.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之际，我们邀请你逐步学习后续章节中基于Scala代码的示例。Apache Spark的发展速度令人印象深刻，值得注意的是其发布的频繁程度。因此，尽管在撰写本文时Spark已达到2.2版本，但我们确信你将使用更新的版本。
- en: 'If you encounter problems, report them at [www.stackoverflow.com](http://www.stackoverflow.com)
    and tag them accordingly; you''ll receive feedback within minutes--the user community
    is very active. Another way of getting information and help is subscribing to
    the Apache Spark mailing list: `user@apachespark.org`.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到问题，请在[www.stackoverflow.com](http://www.stackoverflow.com)上报并相应地标记它们；你将在几分钟内收到反馈——用户社区非常活跃。获取信息和帮助的另一种方式是订阅Apache
    Spark邮件列表：`user@apachespark.org`。
- en: By the end of this chapter, you should have a good idea what's waiting for you
    in this book. We've dedicated ...
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你应该对本书中等待你的内容有了一个清晰的认识。我们专门...
