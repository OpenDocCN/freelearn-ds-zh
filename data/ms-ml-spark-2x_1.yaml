- en: Introduction to Large-Scale Machine Learning and Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模机器学习和Spark简介
- en: '"Information is the oil of the 21^(st) century, and analytics is the combustion
    engine."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '"信息是21世纪的石油，分析是内燃机。"'
- en: --Peter Sondergaard, Gartner Research
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: --彼得·桑德加德，高德纳研究
- en: By 2018, it is estimated that companies will spend $114 billion on big data-related
    projects, an increase of roughly 300%, compared to 2013 ([https://www.capgemini-consulting.com/resource-file-access/resource/pdf/big_data_pov_03-02-15.pdf](https://www.capgemini-consulting.com/resource-file-access/resource/pdf/big_data_pov_03-02-15.pdf)).
    Much of this increase in expenditure is due to how much data is being created
    and how we are better able to store such data by leveraging distributed filesystems
    such as Hadoop.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到2018年，预计公司将在大数据相关项目上花费1140亿美元，比2013年增长了大约300%（[https://www.capgemini-consulting.com/resource-file-access/resource/pdf/big_data_pov_03-02-15.pdf](https://www.capgemini-consulting.com/resource-file-access/resource/pdf/big_data_pov_03-02-15.pdf)）。支出增加的很大程度上是由于正在创建的数据量以及我们如何更好地利用分布式文件系统（如Hadoop）来存储这些数据。
- en: However, collecting the data is only half the battle; the other half involves
    data extraction, transformation, and loading into a computation system, which
    leverage the power of modern computers to apply various mathematical methods in
    order to learn more about data and patterns, and extract useful information to
    make relevant decisions. The entire data workflow has been boosted in the last
    few years by not only increasing the computation power and providing easily accessible
    and scalable cloud services (for example, Amazon AWS, Microsoft Azure, and Heroku)
    but also by a number of tools and libraries that help to easily manage, control,
    and scale infrastructure and build applications. Such a growth in the computation
    power also helps to process larger amounts of data and to apply algorithms that
    were impossible to apply earlier. Finally, various computation-expensive statistical
    or machine learning algorithms have started to help extract nuggets of information
    from data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，收集数据只是一半的战斗；另一半涉及数据提取、转换和加载到计算系统中，利用现代计算机的能力应用各种数学方法，以了解数据和模式，并提取有用信息以做出相关决策。在过去几年里，整个数据工作流程得到了提升，不仅增加了计算能力并提供易于访问和可扩展的云服务（例如，Amazon
    AWS，Microsoft Azure和Heroku），还有一些工具和库，帮助轻松管理、控制和扩展基础设施并构建应用程序。计算能力的增长还有助于处理更大量的数据，并应用以前无法应用的算法。最后，各种计算昂贵的统计或机器学习算法开始帮助从数据中提取信息。
- en: One of the first well-adopted big data technologies was Hadoop, which allows
    for the  MapReduce computation by saving intermediate results on a disk. However,
    it still lacks proper big data tools for information extraction. Nevertheless,
    Hadoop was just the beginning. With the growing size of machine memory, new in-memory
    computation frameworks appeared, and they also started to provide basic support
    for conducting data analysis and modeling—for example, SystemML or Spark ML for
    Spark and FlinkML for Flink. These frameworks represent only the tip of the iceberg—there
    is a lot more in the big data ecosystem, and it is permanently evolving, since
    the volume of data is constantly growing, demanding new big data algorithms and
    processing methods. For example, the **Internet of Things** (**IoT**) represents
    a new domain that produces huge amount of streaming data from various sources
    (for example, home security system, Alexa Echo, or vital sensors) and brings not
    only an unlimited potential to mind useful information from data, but also demands
    new kind of data processing and modeling methods.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最早被广泛采用的大数据技术之一是Hadoop，它允许通过将中间结果保存在磁盘上进行MapReduce计算。然而，它仍然缺乏适当的大数据工具来进行信息提取。然而，Hadoop只是一个开始。随着机器内存的增长，出现了新的内存计算框架，它们也开始提供基本支持进行数据分析和建模，例如，SystemML或Spark的Spark
    ML和Flink的FlinkML。这些框架只是冰山一角-大数据生态系统中还有很多，它在不断发展，因为数据量不断增长，需要新的大数据算法和处理方法。例如，物联网代表了一个新的领域，它从各种来源产生大量的流数据（例如，家庭安全系统，Alexa
    Echo或重要传感器），不仅带来了从数据中挖掘有用信息的无限潜力，还需要新的数据处理和建模方法。
- en: 'Nevertheless, in this chapter, we will start from the beginning and explain
    the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在本章中，我们将从头开始解释以下主题：
- en: Basic working tasks of data scientists
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家的基本工作任务
- en: Aspect of big data computation in distributed environment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布环境中大数据计算的方面
- en: The big data ecosystem
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据生态系统
- en: Spark and its machine learning support
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark及其机器学习支持
- en: Data science
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学
- en: Finding a uniform definition of data science, however, is akin to tasting wine
    and comparing flavor profiles among friends—everyone has their own definition
    and no one description is *more accurate* than the other. At its core, however,
    data science is the art of asking intelligent questions about data and receiving
    intelligent answers that matter to key stakeholders. Unfortunately, the opposite
    also holds true—ask lousy questions of the data and get lousy answers! Therefore,
    careful formulation of the question is the key for extracting valuable insights
    from your data. For this reason, companies are now hiring *data scientists* to
    help formulate and ask these questions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 找到数据科学的统一定义，就像品尝葡萄酒并在朋友中比较口味一样-每个人都有自己的定义，没有一个描述比其他更准确。然而，在其核心，数据科学是关于对数据提出智能问题并获得对关键利益相关者有意义的智能答案的艺术。不幸的是，相反的也是真的-对数据提出糟糕的问题会得到糟糕的答案！因此，仔细制定问题是从数据中提取有价值见解的关键。因此，公司现在正在聘请数据科学家来帮助制定并提出这些问题。
- en: '![](img/00005.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00005.jpeg)'
- en: Figure 1 - Growing Google Trend of big data and data science
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 - 大数据和数据科学的增长谷歌趋势
- en: The sexiest role of the 21st century – data scientist?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21世纪最性感的角色-数据科学家？
- en: 'At first, it''s easy to paint a stereotypical picture of what a typical data
    scientist looks like: t-shirt, sweatpants, thick-rimmed glasses, and debugging
    a chunk of code in IntelliJ... you get the idea. Aesthetics aside, what are some
    of the traits of a data scientist? One of our favorite posters describing this
    role is shown here in the following diagram:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，很容易对典型的数据科学家的形象有一个刻板印象：T恤，运动裤，厚框眼镜，正在用IntelliJ调试一段代码……你懂的。除了审美外，数据科学家的一些特质是什么？我们最喜欢的一张海报描述了这个角色，如下图所示：
- en: '![](img/00006.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00006.jpeg)'
- en: Figure 2 - What is a data scientist?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2 - 什么是数据科学家？
- en: 'Math, statistics, and general knowledge of computer science is given, but one
    pitfall that we see among practitioners has to do with understanding the business
    problem, which goes back to asking intelligent questions of the data. It cannot
    be emphasized enough: asking more intelligent questions of the data is a function
    of the data scientist''s understanding of the business problem and the limitations
    of the data; without this fundamental understanding, even the most intelligent
    algorithm would be unable to come to solid conclusions based on a wobbly foundation.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数学、统计学和计算机科学的一般知识是必备的，但我们在从业者中看到的一个陷阱与理解业务问题有关，这又回到了对数据提出智能问题。无法再强调：对数据提出更多智能问题取决于数据科学家对业务问题和数据限制的理解；没有这种基本理解，即使是最智能的算法也无法基于摇摇欲坠的基础得出坚实的结论。
- en: A day in the life of a data scientist
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个数据科学家的一天
- en: This will probably come as a shock to some of you—being a data scientist is
    more than reading academic papers, researching new tools, and model building until
    the wee hours of the morning, fueled on espresso; in fact, this is only a small
    percentage of the time that a data scientist gets to truly *play* (the espresso
    part however is 100% true for everyone)! Most part of the day, however, is spent
    in meetings, gaining a better understanding of the business problem(s), crunching
    the data to learn its limitations (take heart, this book will expose you to a
    ton of different feature engineering or feature extractions tasks), and how best
    to present the findings to non data-sciencey people. This is where the true *sausage
    making* process takes place, and the best data scientists are the ones who relish
    in this process because they are gaining more understanding of the requirements
    and benchmarks for success. In fact, we could literally write a whole new book
    describing this process from top-to-tail!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会让你们中的一些人感到震惊——成为一名数据科学家不仅仅是阅读学术论文、研究新工具和模型构建直到清晨，靠浓缩咖啡提神；事实上，这只是数据科学家真正*玩耍*的时间的一小部分（然而，对于每个人来说，咖啡因的部分是100%真实的）！然而，大部分时间都是在开会中度过的，更好地了解业务问题，分析数据以了解其限制（放心，本书将让您接触到大量不同的特征工程或特征提取任务），以及如何最好地向非数据科学人员呈现发现。这就是真正的*香肠制作*过程所在，最优秀的数据科学家是那些热爱这个过程的人，因为他们更多地了解了成功的要求和基准。事实上，我们可以写一本全新的书来描述这个过程的始终！
- en: 'So, what (and who) is involved in asking questions about data? Sometimes, it
    is process of saving data into a relational database and running SQL queries to
    find insights into data: "for the millions of users that bought this particular
    product, what are the top 3 OTHER products also bought?" Other times, the question
    is more complex, such as, "Given the review of a movie, is this a positive or
    negative review?" This book is mainly focused on complex questions, like the latter.
    Answering these types of questions is where businesses really get the most impact
    from their big data projects and is also where we see a proliferation of emerging
    technologies that look to make this Q and A system easier, with more functionality.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，关于数据的提问涉及什么（和谁）？有时，这是将数据保存到关系数据库中，并运行SQL查询以找到数据的见解的过程：“对于购买了这种特定产品的数百万用户，还购买了哪3种其他产品？”其他时候，问题更复杂，比如，“鉴于一部电影的评论，这是一个积极的还是消极的评论？”本书主要关注复杂的问题，比如后者。回答这些类型的问题是企业从其大数据项目中真正获得最大影响的地方，也是我们看到新兴技术大量涌现，旨在使这种问答系统更容易，功能更多。
- en: 'Some of the most popular, open source frameworks that look to help answer data
    questions include R, Python, Julia, and Octave, all of which perform reasonably
    well with small (X < 100 GB) datasets. At this point, it''s worth stopping and
    pointing out a clear distinction between big versus small data. Our general rule
    of thumb in the office goes as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最受欢迎的开源框架，旨在帮助回答数据问题，包括R、Python、Julia和Octave，所有这些框架在小型（X < 100 GB）数据集上表现得相当不错。在这一点上，值得停下来指出大数据与小数据之间的明显区别。我们办公室的一般经验法则如下：
- en: '*If you can open your dataset using Excel, you are working with small data.*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果您可以使用Excel打开数据集，那么您正在处理小数据。*'
- en: Working with big data
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理大数据
- en: 'What happens when the dataset in question is so vast that it cannot fit into
    the memory of a single computer and must be distributed across a number of nodes
    in a large computing cluster? Can''t we just rewrite some R code, for example,
    and extend it to account for more than a single-node computation? If only things
    were that simple! There are many reasons why the scaling of algorithms to more
    machines is difficult. Imagine a simple example of a file containing a list of
    names:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当所讨论的数据集如此庞大，以至于无法适应单台计算机的内存，并且必须分布在大型计算集群中的多个节点上时，会发生什么？难道我们不能简单地重写一些R代码，例如，扩展它以适应多于单节点的计算？如果事情只是那么简单就好了！有许多原因使得算法扩展到更多机器变得困难。想象一个简单的例子，一个文件包含一个名单：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We would like to compute the number of occurrences of individual words in the
    file. If the file fits into a single machine, you can easily compute the number
    of occurrences by using a combination of the Unix tools, `sort` and `uniq`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要计算文件中各个单词的出现次数。如果文件适合在一台机器上，您可以轻松地使用Unix工具`sort`和`uniq`来计算出现次数：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as shown ahead:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: However, if the file is huge and distributed over multiple machines, it is necessary
    to adopt a slightly different computation strategy. For example, compute the number
    of occurrences of individual words for every part of the file that fits into the
    memory and merge the results together. Hence, even simple tasks, such as counting
    the occurrences of names, in a distributed environment can become more complicated.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果文件很大并分布在多台机器上，就需要采用略有不同的计算策略。例如，计算每个适合内存的文件部分中各个单词的出现次数，并将结果合并在一起。因此，即使是简单的任务，比如在分布式环境中计算名称的出现次数，也会变得更加复杂。
- en: The machine learning algorithm using a distributed environment
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用分布式环境的机器学习算法
- en: 'Machine learning algorithms combine simple tasks into complex patterns, that
    are even more complicated in distributed environment. Let''s take a simple decision
    tree algorithm (reference), for example. This particular algorithm creates a binary
    tree that tries to fit training data and minimize prediction errors. However,
    in order to do this, it has to decide about the branch of tree it has to send
    every data point to (don''t worry, we''ll cover the mechanics of how this algorithm
    works along with some very useful parameters that you can learn in later in the
    book). Let''s demonstrate it with a simple example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法将简单的任务组合成复杂的模式，在分布式环境中更加复杂。例如，让我们以简单的决策树算法为例。这个特定的算法创建一个二叉树，试图拟合训练数据并最小化预测错误。然而，为了做到这一点，它必须决定将每个数据点发送到树的哪个分支（不用担心，我们将在本书的后面介绍这个算法的工作原理以及一些非常有用的参数）。让我们用一个简单的例子来演示：
- en: '![](img/00007.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00007.jpeg)'
- en: Figure 3 - Example of red and blue data points covering 2D space.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3 - 覆盖2D空间的红色和蓝色数据点的示例。
- en: 'Consider the situation depicted in preceding figure. A two-dimensional board
    with many points colored in two colors: red and blue. The goal of the decision
    tree is to learn and generalize the shape of data and help decide about the color
    of a new point. In our example, we can easily see that the points almost follow
    a chessboard pattern. However, the algorithm has to figure out the structure by
    itself. It starts by finding the best position of a vertical or horizontal line,
    which would separate the red points from the blue points.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑前面图中描述的情况。一个二维棋盘，上面有许多点涂成两种颜色：红色和蓝色。决策树的目标是学习和概括数据的形状，并帮助决定一个新点的颜色。在我们的例子中，我们很容易看出这些点几乎遵循着象棋盘的模式。然而，算法必须自己找出结构。它首先要找到一个垂直或水平线的最佳位置，这条线可以将红点与蓝点分开。
- en: 'The found decision is stored in the tree root and the steps are recursively
    applied on both the partitions. The algorithm ends when there is a single point
    in the partition:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 找到的决策存储在树根中，并且步骤递归地应用在两个分区上。当分区中只有一个点时，算法结束：
- en: '![](img/00008.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00008.jpeg)'
- en: Figure 4 - The final decision tree and projection of its prediction to the original
    space of points.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图4 - 最终的决策树及其预测在原始点空间中的投影。
- en: Splitting of data into multiple machines
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据分割成多台机器
- en: For now, let's assume that the number of points is huge and cannot fit into
    the memory of a single machine. Hence, we need multiple machines, and we have
    to partition data in such a way that each machine contains only a subset of data.
    This way, we solve the memory problem; however, it also means that we need to
    distribute the computation around a cluster of machines. This is the first difference
    from single-machine computing. If your data fits into a single machine memory,
    it is easy to make decisions about data, since the algorithm can access them all
    at once, but in the case of a distributed algorithm, this is not true anymore
    and the algorithm has to be "clever" about accessing the data. Since our goal
    is to build a decision tree that predicts the color of a new point in the board,
    we need to figure out how to make the tree that will be the same as a tree built
    on a single machine.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们假设点的数量很大，无法适应单台机器的内存。因此，我们需要多台机器，并且我们必须以这样的方式对数据进行分区，使得每台机器只包含数据的一个子集。这样，我们解决了内存问题；然而，这也意味着我们需要在机器集群中分布计算。这是与单机计算的第一个不同之处。如果您的数据适合单台机器的内存，那么很容易做出关于数据的决策，因为算法可以一次性访问所有数据，但在分布式算法的情况下，这不再成立，算法必须在访问数据方面变得“聪明”。由于我们的目标是构建一个决策树，以预测棋盘上一个新点的颜色，我们需要找出如何制作与单机上构建的树相同的树。
- en: The naive solution is to build a trivial tree that separates the points based
    on machine boundaries. But this is obviously a bad solution, since data distribution
    does not reflect color points at all.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素的解决方案是构建一个基于机器边界分隔点的平凡树。但这显然是一个糟糕的解决方案，因为数据分布根本不反映颜色点。
- en: Another solution tries all the possible split decisions in the direction of
    the *X* and *Y* axes and tries to do the best in separating both colors, that
    is, divides the points into two groups and minimizes the number of points of another
    color. Imagine that the algorithm is testing the split via the line, *X = 1.6*.
    This means that the algorithm has to ask each machine in the cluster to report
    the result of splitting the machine's local data, merge the results, and decide
    whether it is the right splitting decision. If it finds an optimal split, it needs
    to inform all the machines about the decision in order to record which partition
    each point belongs to.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个解决方案尝试在*X*和*Y*轴的方向上尝试所有可能的分割决策，并尽量在分离两种颜色时做得最好，也就是将点分成两组并最小化另一种颜色的点数。想象一下，算法正在通过线*X
    = 1.6*测试分割。这意味着算法必须询问集群中的每台机器报告分割机器的本地数据的结果，合并结果，并决定是否是正确的分割决策。如果找到了最佳分割，它需要通知所有机器关于决策，以记录每个点属于哪个分区。
- en: Compared with the single machine scenario, the distributed algorithm constructing
    decision tree is more complex and requires a way of distributing the computation
    among machines. Nowadays, with easy access to a cluster of machines and an increasing
    demand for the analysis of larger datasets, it becomes a standard requirement.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与单机场景相比，构建决策树的分布式算法更复杂，需要一种在多台机器之间分配计算的方式。如今，随着对大型数据集分析需求的增加以及对机器群集的轻松访问，这成为了标准要求。
- en: 'Even these two simple examples show that for a larger data, proper computation
    and distributed infrastructure is required, including the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这两个简单的例子表明，对于更大的数据，需要适当的计算和分布式基础设施，包括以下内容：
- en: A distributed data storage, that is, if the data cannot fit into a single node,
    we need a way to distribute and process them on multiple machines
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式数据存储，即，如果数据无法放入单个节点，我们需要一种在多台机器上分发和处理数据的方式
- en: A computation paradigm to process and transform the distributed data and to
    apply mathematical (and statistical) algorithms and workflows
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种处理和转换分布式数据并应用数学（和统计）算法和工作流的计算范式
- en: Support to persist and reuse defined workflows and models
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持持久化和重用定义的工作流和模型
- en: Support to deploy statistical models in production
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持在生产中部署统计模型
- en: In short, we need a framework that will support common data science tasks. It
    can be considered an unnecessary requirement, since data scientists prefer using
    existing tools, such as R, Weka, or Python's scikit. However, these tools are
    neither designed for large-scale distributed processing nor for the parallel processing
    of large data. Even though there are libraries for R or Python that support limited
    parallel or distributed programming, their main limitation is that the base platforms,
    that is R and Python, were not designed for this kind of data processing and computation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们需要一个支持常见数据科学任务的框架。这可能被认为是一个不必要的要求，因为数据科学家更喜欢使用现有工具，如R、Weka或Python的scikit。然而，这些工具既不是为大规模分布式处理设计的，也不是为大数据的并行处理设计的。尽管有支持有限并行或分布式编程的R或Python库，但它们的主要局限是基础平台，即R和Python，不是为这种数据处理和计算设计的。
- en: From Hadoop MapReduce to Spark
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Hadoop MapReduce到Spark
- en: 'With a growing amount of data, the single-machine tools were not able to satisfy
    the industry needs and thereby created a space for new data processing methods
    and tools, especially Hadoop MapReduce, which is based on an idea originally described
    in the Google paper, *MapReduce: Simplified Data Processing on Large Clusters* ([https://research.google.com/archive/mapreduce.html](https://research.google.com/archive/mapreduce.html)).
    On the other hand, it is a generic framework without any explicit support or libraries
    to create machine learning workflows. Another limitation of classical MapReduce
    is that it performs many disk I/O operations during the computation instead of
    benefiting from machine memory.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '随着数据量的增长，单机工具无法满足行业需求，因此为新的数据处理方法和工具创造了空间，特别是基于最初在Google论文中描述的想法的Hadoop MapReduce，*MapReduce:
    Simplified Data Processing on Large Clusters* ([https://research.google.com/archive/mapreduce.html](https://research.google.com/archive/mapreduce.html))。另一方面，它是一个通用框架，没有任何明确支持或库来创建机器学习工作流。经典MapReduce的另一个局限是，在计算过程中执行了许多磁盘I/O操作，而没有从机器内存中受益。'
- en: As you have seen, there are several existing machine learning tools and distributed
    platforms, but none of them is an exact match for performing machine learning
    tasks with large data and distributed environment. All these claims open the doors
    for Apache Spark.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，存在多种现有的机器学习工具和分布式平台，但没有一个完全匹配于在大数据和分布式环境中执行机器学习任务。所有这些说法为Apache Spark打开了大门。
- en: Enter the room, Apache Spark!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 进入房间，Apache Spark！
- en: '![](img/00009.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00009.jpeg)'
- en: Created in 2010 at the UC Berkeley AMP Lab (Algorithms, Machines, People), the
    Apache Spark project was built with an eye for speed, ease of use, and advanced
    analytics. One key difference between Spark and other distributed frameworks such
    as Hadoop is that datasets can be cached in memory, which lends itself nicely
    to machine learning, given its iterative nature (more on this later!) and how
    data scientists are constantly accessing the same data many times over.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark项目于2010年在加州大学伯克利分校AMP实验室（算法、机器、人）创建，其目标是速度、易用性和高级分析。Spark与Hadoop等其他分布式框架的一个关键区别是，数据集可以缓存在内存中，这非常适合机器学习，因为它的迭代性质（稍后会详细介绍！）以及数据科学家经常多次访问相同的数据。
- en: 'Spark can be run in a variety of ways, such as the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以以多种方式运行，例如以下方式：
- en: '**Local mode:** This entails a single **Java Virtual Machine** (**JVM**) executed
    on a single host'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地模式：**这涉及在单个主机上执行的单个**Java虚拟机**（**JVM**）'
- en: '**Standalone Spark cluster:** This entails multiple JVMs on multiple hosts'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立的Spark集群：**这涉及多个主机上的多个JVM'
- en: '**Via resource manager such as Yarn/Mesos:** This application deployment is
    driven by a resource manager, which controls the allocation of nodes, application,
    distribution, and deployment'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过资源管理器，如Yarn/Mesos：**这种应用部署是由资源管理器驱动的，它控制节点、应用程序、分发和部署的分配'
- en: What is Databricks?
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Databricks？
- en: If you know about the Spark project, then chances are high that you have also
    heard of a company called *Databricks*. However, you might not know how Databricks
    and the Spark project are related to one another. In short, Databricks was founded
    by the creators of the Apache Spark project and accounts for over 75% of the code
    base for the Spark project. Aside from being a huge force behind the Spark project
    with respect to development, Databricks also offers various certifications in
    Spark for developers, administrators, trainers, and analysts alike. However, Databricks
    is not the only main contributor to the code base; companies such as IBM, Cloudera,
    and Microsoft also actively participate in Apache Spark development.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您了解Spark项目，那么很可能您也听说过一个名为*Databricks*的公司。然而，您可能不知道Databricks和Spark项目之间的关系。简而言之，Databricks是由Apache
    Spark项目的创建者成立的，并占据了Spark项目超过75%的代码库。除了在开发方面对Spark项目有着巨大的影响力之外，Databricks还为开发人员、管理员、培训师和分析师提供各种Spark认证。然而，Databricks并不是代码库的唯一主要贡献者；像IBM、Cloudera和微软这样的公司也积极参与Apache
    Spark的开发。
- en: As a side note, Databricks also organizes the Spark Summit (in both Europe and
    the US), which is the premier Spark conference and a great place to learn about
    the latest developments in the project and how others are using Spark within their
    ecosystem.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，Databricks还组织了Spark Summit（在欧洲和美国举办），这是首屈一指的Spark会议，也是了解项目最新发展以及其他人如何在其生态系统中使用Spark的绝佳场所。
- en: Throughout this book, we will give recommended links that we read daily that
    offer great insights and also important changes with respect to the new versions
    of Spark. One of the best resources here is the Databricks blog, which is constantly
    being updated with great content. Be sure to regularly check this out at [https://databricks.com/blog](https://databricks.com/blog).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将提供推荐的链接，这些链接每天都会提供很好的见解，同时也会介绍关于新版本Spark的重要变化。其中最好的资源之一是Databricks博客，该博客不断更新着优质内容。一定要定期查看[https://databricks.com/blog](https://databricks.com/blog)。
- en: 'Also, here is a link to see the past Spark Summit talks, which you may find
    helpful:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这里还有一个链接，可以查看过去的Spark Summit讲座，可能会对您有所帮助：
- en: '[http://slideshare.net/databricks](http://slideshare.net/databricks).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://slideshare.net/databricks](http://slideshare.net/databricks).'
- en: Inside the box
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 盒子里
- en: So, you have downloaded the latest version of Spark (depending on how you plan
    on launching Spark) and you have run the standard *Hello, World!* example....what
    now?!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，您已经下载了最新版本的Spark（取决于您计划如何启动Spark），并运行了标准的*Hello, World!*示例....现在呢？
- en: 'Spark comes equipped with five libraries, which can be used separately--or
    in unison--depending on the task we are trying to solve. Note that in this book,
    we plan on using a variety of different libraries, all within the same application
    so that you will have the maximum exposure to the Spark platform and better understand
    the benefits (and limitations) of each library. These five libraries are as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Spark配备了五个库，可以单独使用，也可以根据我们要解决的任务一起使用。请注意，在本书中，我们计划使用各种不同的库，都在同一个应用程序中，以便您能最大程度地接触Spark平台，并更好地了解每个库的优势（和局限性）。这五个库如下：
- en: '**Core**: This is the Spark core infrastructure, providing primitives to represent
    and store data called **Resilient Distributed Dataset** (**RDDs**) and manipulate
    data with tasks and jobs.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心**：这是Spark的核心基础设施，提供了用于表示和存储数据的原语，称为**弹性分布式数据集**（**RDDs**），并使用任务和作业来操作数据。'
- en: '**SQL** : This library provides user-friendly API over core RDDs by introducing
    DataFrames and SQL to manipulate with the data stored.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SQL**：该库通过引入DataFrames和SQL来提供用户友好的API，以操作存储的数据。'
- en: '**MLlib (Machine Learning Library)** : This is Spark''s very own machine learning
    library of algorithms developed in-house that can be used within your Spark application.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLlib（机器学习库）**：这是Spark自己的机器学习库，其中包含了内部开发的算法，可以在Spark应用程序中使用。'
- en: '**Graphx** : This is used for graphs and graph-calculations; we will explore
    this particular library in depth in a later chapter.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Graphx**：用于图形和图形计算；我们将在后面的章节中深入探讨这个特定的库。'
- en: '**Streaming** : This library allows real-time streaming of data from various
    sources, such as Kafka, Twitter, Flume, and TCP sockets, to name a few. Many of
    the applications we will build in this book will leverage the MLlib and Streaming
    libraries to build our applications.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Streaming**：该库允许从各种来源实时流式传输数据，例如Kafka、Twitter、Flume和TCP套接字等。本书中许多应用程序将利用MLlib和Streaming库来构建我们的应用程序。'
- en: '![](img/00010.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00010.jpeg)'
- en: The Spark platform can also be extended by third-party packages. There are many
    of them, for example, support for reading CSV or Avro files, integration with
    Redshift, and Sparkling Water, which encapsulates the H2O machine learning library.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Spark平台也可以通过第三方软件包进行扩展。例如，支持读取CSV或Avro文件，与Redshift集成以及Sparkling Water，它封装了H2O机器学习库。
- en: Introducing H2O.ai
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍H2O.ai
- en: H2O is an open source, machine learning platform that plays extremely well with
    Spark; in fact, it was one of the first third-party packages deemed "Certified
    on Spark".
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: H2O是一个开源的机器学习平台，与Spark非常兼容；事实上，它是最早被认定为“在Spark上认证”的第三方软件包之一。
- en: '![](img/00011.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00011.jpeg)'
- en: Sparkling Water (H2O + Spark) is H2O's integration of their platform within
    the Spark project, which combines the machine learning capabilities of H2O with
    all the functionality of Spark. This means that users can run H2O algorithms on
    Spark RDD/DataFrame for both exploration and deployment purposes. This is made
    possible because H2O and Spark share the same JVM, which allows for seamless transitions
    between the two platforms. H2O stores data in the H2O frame, which is a columnar-compressed
    representation of your dataset that can be created from Spark RDD and/or DataFrame.
    Throughout much of this book, we will be referencing algorithms from Spark's MLlib
    library and H2O's platform, showing how to use both the libraries to get the best
    results possible for a given task.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Sparkling Water（H2O + Spark）是 H2O 在 Spark 项目中集成其平台的一部分，它将 H2O 的机器学习能力与 Spark
    的所有功能结合在一起。这意味着用户可以在 Spark RDD/DataFrame 上运行 H2O 算法，用于探索和部署。这是可能的，因为 H2O 和 Spark
    共享相同的 JVM，这允许在两个平台之间无缝切换。H2O 将数据存储在 H2O 框架中，这是您的数据集的列压缩表示，可以从 Spark RDD 和/或 DataFrame
    创建。在本书的大部分内容中，我们将引用 Spark 的 MLlib 库和 H2O 平台的算法，展示如何使用这两个库来为给定任务获得尽可能好的结果。
- en: 'The following is a summary of the features Sparkling Water comes equipped with:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Sparkling Water 配备的功能摘要：
- en: Use of H2O algorithms within a Spark workflow
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Spark 工作流中使用 H2O 算法
- en: Transformations between Spark and H2O data structures
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Spark 和 H2O 数据结构之间的转换
- en: Use of Spark RDD and/or DataFrame as inputs to H2O algorithms
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Spark RDD 和/或 DataFrame 作为 H2O 算法的输入
- en: Use of H2O frames as inputs into MLlib algorithms (will come in handy when we
    do feature engineering later)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 H2O 框架用作 MLlib 算法的输入（在进行特征工程时会很方便）
- en: Transparent execution of Sparkling Water applications on top of Spark (for example,
    we can run a Sparkling Water application within a Spark stream)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sparkling Water 应用程序在 Spark 顶部的透明执行（例如，我们可以在 Spark 流中运行 Sparkling Water 应用程序）
- en: The H2O user interface to explore Spark data
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 Spark 数据的 H2O 用户界面
- en: Design of Sparkling Water
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sparkling Water 的设计
- en: Sparkling Water is designed to be executed as a regular Spark application. Consequently,
    it is launched inside a Spark executor created after submitting the application.
    At this point, H2O starts services, including a distributed key-value (K/V) store
    and memory manager, and orchestrates them into a cloud. The topology of the created
    cloud follows the topology of the underlying Spark cluster.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Sparkling Water 被设计为可执行的常规 Spark 应用程序。因此，它在提交应用程序后在 Spark 执行器内启动。此时，H2O 启动服务，包括分布式键值（K/V）存储和内存管理器，并将它们编排成一个云。创建的云的拓扑结构遵循底层
    Spark 集群的拓扑结构。
- en: 'As stated previously, Sparkling Water enables transformation between different
    types of RDDs/DataFrames and H2O''s frame, and vice versa. When converting from
    a hex frame to an RDD, a wrapper is created around the hex frame to provide an
    RDD-like API. In this case, data is not duplicated but served directly from the
    underlying hex frame. Converting from an RDD/DataFrame to a H2O frame requires
    data duplication because it transforms data from Spark into H2O-specific storage.
    However, data stored in an H2O frame is heavily compressed and does not need to
    be preserved as an RDD anymore:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Sparkling Water 可以在不同类型的 RDD/DataFrame 和 H2O 框架之间进行转换，反之亦然。当从 hex 框架转换为
    RDD 时，会在 hex 框架周围创建一个包装器，以提供类似 RDD 的 API。在这种情况下，数据不会被复制，而是直接从底层的 hex 框架提供。从 RDD/DataFrame
    转换为 H2O 框架需要数据复制，因为它将数据从 Spark 转换为 H2O 特定的存储。但是，存储在 H2O 框架中的数据被大量压缩，不再需要作为 RDD
    保留：
- en: '![](img/00012.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00012.jpeg)'
- en: Data sharing between sparkling water and Spark
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Sparkling Water 和 Spark 之间的数据共享
- en: What's the difference between H2O and Spark's MLlib?
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: H2O 和 Spark 的 MLlib 有什么区别？
- en: 'As stated previously, MLlib is a library of popular machine learning algorithms
    built using Spark. Not surprisingly, H2O and MLlib share many of the same algorithms
    but differ in both their implementation and functionality. One very handy feature
    of H2O is that it allows users to visualize their data and perform feature engineering
    tasks, which we will cover in depth in later chapters. The visualization of data
    is accomplished by a web-friendly GUI and allows users a friendly interface to
    seamlessly switch between a code shell and a notebook-friendly environment. The
    following is an example of the H2O notebook - called *Flow* - that you will become
    familiar with soon:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，MLlib 是使用 Spark 构建的流行机器学习算法库。毫不奇怪，H2O 和 MLlib 共享许多相同的算法，但它们在实现和功能上有所不同。H2O
    的一个非常方便的功能是允许用户可视化其数据并执行特征工程任务，我们将在后面的章节中深入介绍。数据的可视化是通过一个友好的网络 GUI 完成的，并允许用户在代码
    shell 和笔记本友好的环境之间无缝切换。以下是 H2O 笔记本的示例 - 称为 *Flow* - 您很快将熟悉的：
- en: '![](img/00013.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00013.jpeg)'
- en: One other nice addition is that H2O allows data scientists to grid search many
    hyper-parameters that ship with their algorithms. Grid search is a way of optimizing
    all the hyperparameters of an algorithm to make model configuration easier. Often,
    it is difficult to know which hyperparameters to change and how to change them;
    the grid search allows us to explore many hyperparameters simultaneously, measure
    the output, and help select the best models based on our quality requirements.
    The H2O grid search can be combined with model cross-validation and various stopping
    criteria, resulting in advanced strategies such as *picking 1000 random parameters
    from a huge parameters hyperspace and finding the best model that can be trained
    under two minutes and with AUC greater than 0.7*
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个很好的补充是，H2O 允许数据科学家对其算法附带的许多超参数进行网格搜索。网格搜索是一种优化算法的所有超参数的方法，使模型配置更加容易。通常，很难知道要更改哪些超参数以及如何更改它们；网格搜索允许我们同时探索许多超参数，测量输出，并根据我们的质量要求帮助选择最佳模型。H2O
    网格搜索可以与模型交叉验证和各种停止标准结合使用，从而产生高级策略，例如*从巨大的参数超空间中选择 1000 个随机参数，并找到可以在两分钟内训练且 AUC
    大于 0.7 的最佳模型*
- en: Data munging
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据整理
- en: Raw data for problems often comes from multiple sources with different and often
    incompatible formats. The beauty of the Spark programming model is its ability
    to define data operations that process the incoming data and transform it into
    a regular form that can be used for further feature engineering and model building.
    This process is commonly referred to as data munging and is where much of the
    battle is won with respect to data science projects. We keep this section intentionally
    brief because the best way to showcase the power--and necessity!--of data munging
    is by example. So, take heart; we have *plenty* of practice to go through in this
    book, which emphasizes this essential process.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的原始数据通常来自多个来源，格式不同且通常不兼容。Spark编程模型的美妙之处在于其能够定义数据操作，处理传入的数据并将其转换为常规形式，以便用于进一步的特征工程和模型构建。这个过程通常被称为数据整理，这是数据科学项目中取得胜利的关键。我们故意将这一部分简短，因为展示数据整理的力量和必要性最好的方式是通过示例。所以，放心吧；在这本书中，我们有很多实践要做，重点是这个基本过程。
- en: Data science - an iterative process
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学-一个迭代的过程
- en: Often, the process flow of many big data projects is iterative, which means
    a lot of back-and-forth testing new ideas, new features to include, tweaking various
    hyper-parameters, and so on, with a *fail fast* attitude. The end result of these
    projects is usually a model that can answer a question being posed. Notice that
    we didn't say *accurately* answer a question being posed! One pitfall of many
    data scientists these days is their inability to generalize a model for new data,
    meaning that they have overfit their data so that the model provides poor results
    when given new data. Accuracy is extremely task-dependent and is usually dictated
    by the business needs with some sensitivity analysis being done to weigh the cost-benefits
    of the model outcomes. However, there are a few standard accuracy measures that
    we will go over throughout this book so that you can compare various models to
    see *how* changes to the model impact the result.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 很多大数据项目的流程是迭代的，这意味着不断地测试新的想法，包括新的特征，调整各种超参数等等，态度是“快速失败”。这些项目的最终结果通常是一个能够回答提出的问题的模型。请注意，我们没有说*准确地*回答提出的问题！如今许多数据科学家的一个缺陷是他们无法将模型泛化到新数据，这意味着他们已经过度拟合了数据，以至于当给出新数据时，模型会提供糟糕的结果。准确性极大地取决于任务，并且通常由业务需求决定，同时进行一些敏感性分析以权衡模型结果的成本效益。然而，在本书中，我们将介绍一些标准的准确性度量，以便您可以比较各种模型，看看对模型的更改如何影响结果。
- en: H2O is constantly giving meetup talks and inviting others to give machine learning
    meetups around the US and Europe. Each meetup or conference slides is available
    on SlideShare ([http://www.slideshare.com/0xdata](http://www.slideshare.com/0xdata))
    or YouTube. Both the sites serve as great sources of information not only about
    machine learning and statistics but also about distributed systems and computation.
    For example, one of the most interesting presentations highlights the "Top 10
    pitfalls in a data scientist job" ([http://www.slideshare.net/0xdata/h2o-world-top-10-data-science-pitfalls-mark-landry](http://www.slideshare.net/0xdata/h2o-world-top-10-data-science-pitfalls-mark-landry))
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: H2O经常在美国和欧洲举办见面会，并邀请其他人参加机器学习见面会。每个见面会或会议的幻灯片都可以在SlideShare（[http://www.slideshare.com/0xdata](http://www.slideshare.com/0xdata)）或YouTube上找到。这两个网站不仅是关于机器学习和统计的重要信息来源，也是关于分布式系统和计算的重要信息来源。例如，其中一个最有趣的演示重点介绍了“数据科学家工作中的前10个陷阱”（[http://www.slideshare.net/0xdata/h2o-world-top-10-data-science-pitfalls-mark-landry](http://www.slideshare.net/0xdata/h2o-world-top-10-data-science-pitfalls-mark-landry)）。
- en: Summary
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we wanted to give you a brief glimpse into the life of a data
    scientist, what this entails, and some of the challenges that data scientists
    consistently face. In light of these challenges, we feel that the Apache Spark
    project is ideally positioned to help tackle these topics, which range from data
    ingestion and feature extraction/creation to model building and deployment. We
    intentionally kept this chapter short and light on verbiage because we feel working
    through examples and different use cases is a better use of time as opposed to
    speaking abstractly and at length about a given data science topic. Throughout
    the rest of this book, we will focus solely on this process while giving best-practice
    tips and recommended reading along the way for users who wish to learn more. Remember
    that before embarking on your next data science project, be sure to clearly define
    the problem beforehand, so you can ask an intelligent question of your data and
    (hopefully) get an intelligent answer!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们想要简要地让您了解数据科学家的生活，这意味着什么，以及数据科学家经常面临的一些挑战。鉴于这些挑战，我们认为Apache Spark项目理想地定位于帮助解决这些主题，从数据摄入和特征提取/创建到模型构建和部署。我们故意将本章保持简短，言辞轻松，因为我们认为通过示例和不同的用例来工作是比抽象地和冗长地谈论某个数据科学主题更好的利用时间。在本书的其余部分，我们将专注于这个过程，同时给出最佳实践建议和推荐阅读，以供希望学习更多的用户参考。请记住，在着手进行下一个数据科学项目之前，一定要在前期清晰地定义问题，这样您就可以向数据提出一个明智的问题，并（希望）得到一个明智的答案！
- en: One awesome website for all things data science is KDnuggets ([http://www.kdnuggets.com](http://www.kdnuggets.com)).
    Here's a great article on the language all data scientists must learn in order
    to be successful ([http://www.kdnuggets.com/2015/09/one-language-data-scientist-must-master.html](http://www.kdnuggets.com/2015/09/one-language-data-scientist-must-master.html)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关于数据科学的很棒的网站是KDnuggets（[http://www.kdnuggets.com](http://www.kdnuggets.com)）。这里有一篇关于所有数据科学家必须学习的语言的好文章，以便取得成功（[http://www.kdnuggets.com/2015/09/one-language-data-scientist-must-master.html](http://www.kdnuggets.com/2015/09/one-language-data-scientist-must-master.html)）。
