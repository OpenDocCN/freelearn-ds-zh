- en: Chapter 3.  Data Processing Using the Batch Processing API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。使用批处理API进行数据处理
- en: Even though many people appreciate the potential value of streaming data processing
    in most industries, there are many use cases where people don't feel it is necessary
    to process the data in a streaming manner. In all such cases, batch processing
    is the way to go. So far Hadoop has been the default choice for data processing.
    However, Flink also supports batch data processing by DataSet API.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多人欣赏流数据处理在大多数行业中的潜在价值，但也有许多用例，人们认为不需要以流式方式处理数据。在所有这些情况下，批处理是前进的方式。到目前为止，Hadoop一直是数据处理的默认选择。但是，Flink也通过DataSet
    API支持批处理数据处理。
- en: For Flink, batch processing is a special case of stream processing. Here is
    a very interesting article explaining this thought in detail at [http://data-artisans.com/batch-is-a-special-case-of-streaming/](http://data-artisans.com/batch-is-a-special-case-of-streaming/).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Flink，批处理是流处理的一种特殊情况。在[http://data-artisans.com/batch-is-a-special-case-of-streaming/](http://data-artisans.com/batch-is-a-special-case-of-streaming/)上有一篇非常有趣的文章详细解释了这个想法。
- en: 'In this chapter, we are going to look at the details regarding DataSet API.
    This includes the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细了解DataSet API的详细信息。这包括以下主题：
- en: Data sources
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源
- en: Transformations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换
- en: Data sinks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据接收器
- en: Connectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接器
- en: 'As we learnt in the previous chapter, any Flink program works on a certain
    defined anatomy as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中学到的，任何Flink程序都遵循以下定义的解剖结构：
- en: '![Data Processing Using the Batch Processing API](img/image_03_001.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![使用批处理API进行数据处理](img/image_03_001.jpg)'
- en: The DataSet API is not an exception to this flow. We will look at each step
    in detail. We already discussed in the previous chapter how to obtain the execution
    environment. So we will directly move to the details of data sources supported
    by DataSet API.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: DataSet API也不例外。我们将详细了解每个步骤。我们已经在上一章中讨论了如何获取执行环境。因此，我们将直接转向DataSet API支持的数据源的详细信息。
- en: Data sources
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据源
- en: Sources are places where the DataSet API expects to get its data from. It could
    in the form of a file or from Java collections. This is the second step in the
    Flink program's anatomy. DataSet API supports a number of pre-implemented data
    source functions. It also supports writing custom data source functions so anything
    that is not supported can be programmed easily. First let's try to understand
    the built-in source functions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 源是DataSet API期望从中获取数据的地方。它可以是文件形式，也可以是来自Java集合。这是Flink程序解剖的第二步。DataSet API支持许多预先实现的数据源函数。它还支持编写自定义数据源函数，因此可以轻松地编程任何不受支持的内容。首先让我们尝试理解内置的源函数。
- en: File-based
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于文件的
- en: 'Flink supports reading data from files. It reads data line by line and returns
    it as strings. The following are built-in functions you can use to read data:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持从文件中读取数据。它逐行读取数据并将其作为字符串返回。以下是您可以使用的内置函数来读取数据：
- en: '`readTextFile(Stringpath)`: This reads data from a file specified in the path.
    By default it will read `TextInputFormat` and will read strings line by line.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readTextFile(Stringpath)`: 从指定路径读取文件中的数据。默认情况下，它将读取`TextInputFormat`并逐行读取字符串。'
- en: '`readTextFileWithValue(Stringpath)`: This reads data from a file specified
    in the path. It returns `StringValues`. `StringValues` are mutable strings.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readTextFileWithValue(Stringpath)`: 从指定路径读取文件中的数据。它返回`StringValues`。`StringValues`是可变字符串。'
- en: '`readCsvFile(Stringpath`): This reads data from comma separated files. It returns
    the Java POJOs or tuples.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readCsvFile(Stringpath)`: 从逗号分隔的文件中读取数据。它返回Java POJOs或元组。'
- en: '`readFileofPremitives(path, delimiter, class)`: This parses the new line into
    primitive data types such as strings or integers.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readFileofPremitives(path, delimiter, class)`: 将新行解析为原始数据类型，如字符串或整数。'
- en: '`readHadoopFile(FileInputFormat, Key, Value, path)`: This reads files from
    a specified path with the given `FileInputFormat`, `Key` class and `Value` class.
    It returns the parsed values as tuples `Tuple2<Key,Value>`.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readHadoopFile(FileInputFormat, Key, Value, path)`: 从指定路径使用给定的`FileInputFormat`、`Key`类和`Value`类读取文件。它将解析后的值返回为元组`Tuple2<Key,Value>`。'
- en: '`readSequenceFile(Key, Value, path)`: This reads files from a specified path
    with the given `SequenceFileInputFormat`, `Key` class and `Value` class. It returns
    the parsed values as tuples `Tuple2<Key,Value>`.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readSequenceFile(Key, Value, path)`: 从指定路径使用给定的`SequenceFileInputFormat`、`Key`类和`Value`类读取文件。它将解析后的值返回为元组`Tuple2<Key,Value>`。'
- en: Note
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For file-based inputs, Flink supports the recursive traversal of folders specified
    in a given path. In order to use this facility, we need to set an environment
    variable and pass it as a parameter while reading the data. The variable to set
    is `recursive.file.enumeration`. We need to set this variable to `true` in order
    to enable recursive traversal.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于文件的输入，Flink支持递归遍历指定路径中的文件夹。为了使用这个功能，我们需要设置一个环境变量，并在读取数据时将其作为参数传递。要设置的变量是`recursive.file.enumeration`。我们需要将此变量设置为`true`以启用递归遍历。
- en: Collection-based
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于集合的
- en: 'With Flink DataSet API, we can also read data from Java-based collections.
    The following are some functions we can use to read the data:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flink DataSet API，我们还可以从基于Java的集合中读取数据。以下是一些我们可以使用的函数来读取数据：
- en: '`fromCollection(Collection)`: This creates a dataset from Java-based collections.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fromCollection(Collection)`: 从基于Java的集合创建数据集。'
- en: '`fromCollection(Iterator, Class)`: This creates a dataset from an iterator.
    The elements of the iterator are of a type given by the class parameter.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fromCollection(Iterator, Class)`: 从迭代器创建数据集。迭代器的元素由类参数给定的类型。'
- en: '`fromElements(T)`: This creates a dataset of a sequence of objects. The object
    type is specified in the function itself.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fromElements(T)`: 创建一个包含一系列对象的数据集。对象类型在函数本身中指定。'
- en: '`fromParallelCollection(SplittableIterator, Class)`: This creates a dataset
    from the iterator in parallel. Class represents the object types.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fromParallelCollection(SplittableIterator, Class)`: 这将并行从迭代器创建数据集。Class代表对象类型。'
- en: '`generateSequence(from, to)`: This generates the sequence of numbers between
    given limits.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generateSequence(from, to)`: 生成给定范围内的数字序列。'
- en: Generic sources
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用源
- en: 'DataSet API supports a couple of generic functions to read data:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: DataSet API支持一些通用函数来读取数据：
- en: '`readFile(inputFormat, path)`: This creates a dataset of the type `FileInputFormat`
    from a given path'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readFile(inputFormat, path)`: 这将从给定路径创建一个`FileInputFormat`类型的数据集'
- en: '`createInput(inputFormat)`: This creates a dataset of the generic input format'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`createInput(inputFormat)`: 这将创建一个通用输入格式的数据集'
- en: Compressed files
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压缩文件
- en: Flink supports the decompression of files while reading if they are marked with
    proper extensions. We don't need to do any different configurations to read the
    compressed files. If a file with a proper extension is detected then Flink automatically
    decompresses it and sends it for further processing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持在读取文件时解压缩文件，如果它们标有适当的扩展名。我们不需要对读取压缩文件进行任何不同的配置。如果检测到具有适当扩展名的文件，则Flink会自动解压缩并发送到进一步处理。
- en: One thing to note here is that decompression of files cannot be done in parallel
    so this might take a bit of time before the actual data processing starts.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一点是，文件的解压缩不能并行进行，因此在实际数据处理开始之前可能需要一些时间。
- en: At this stage, it is recommended to avoid using compressed files as decompression
    is not a scalable activity in Flink.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，建议避免使用压缩文件，因为在Flink中解压缩不是可扩展的活动。
- en: 'The following are supported algorithms:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 支持以下算法：
- en: '| **Compression algorithm** | **Extension** | **Is parallel?** |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **压缩算法** | **扩展名** | **是否并行？** |'
- en: '| Gzip | `.gz`, `.gzip` | No |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Gzip | `.gz`, `.gzip` | 否 |'
- en: '| Deflate | `.deflate` | No |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Deflate | `.deflate` | 否 |'
- en: Transformations
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换
- en: Data transformations transform the dataset from one form into another. The input
    could be one or more datasets and the output could also be zero, or one or more
    data streams. Now let's try to understand each transformation one by one.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换将数据集从一种形式转换为另一种形式。输入可以是一个或多个数据集，输出也可以是零个、一个或多个数据流。现在让我们逐个了解每个转换。
- en: Map
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 映射
- en: This is one of the simplest transformations where the input is one dataset and
    output is also one dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的转换之一，输入是一个数据集，输出也是一个数据集。
- en: 'In Java:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In Scala:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In Python:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Flat map
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flat map
- en: The flat map takes one record and outputs zero, or one or more than one records.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: flat map接受一个记录并输出零个、一个或多个记录。
- en: 'In Java:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In Scala:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In Python:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Filter
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤
- en: Filter functions evaluate the conditions and then if returned `true` only emit
    the record. Filter functions can output zero records.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤函数评估条件，然后如果返回“true”则只发出记录。过滤函数可以输出零个记录。
- en: 'In Java:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In Python:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Project
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目
- en: Project transformations remove or move the elements of a tuple into another.
    This can be used to do selective processing on specific elements.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 项目转换删除或移动元组的元素到另一个元组。这可以用来对特定元素进行选择性处理。
- en: 'In Java:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In Scala, this transformation is not supported.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中，不支持这种转换。
- en: 'In Python:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Reduce on grouped datasets
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对分组数据集进行减少
- en: Reduce transformations reduce each group into a single element based on the
    user-defined reduce function.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 减少转换根据用户定义的减少函数将每个组减少为单个元素。
- en: 'In Java:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In Scala:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In Python, the code is not supported.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，代码不受支持。
- en: Reduce on grouped datasets by field position key
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 按字段位置键对分组数据集进行减少
- en: For datasets with tuples, we can also group by the field positions. The following
    is an example.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于元组数据集，我们也可以按字段位置进行分组。以下是一个例子。
- en: 'In Java:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In Scala:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In Python:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Group combine
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组合组
- en: In some applications, it is important to do intermediate operations before doing
    some more transformations. Group combine operations can be very handy in this
    case. Intermediate transformations could be reducing the size and so on.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些应用中，在进行一些更多的转换之前进行中间操作非常重要。在这种情况下，组合操作非常方便。中间转换可以减小大小等。
- en: This is performed in memory with a greedy strategy that gets performed in multiple
    steps.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用贪婪策略在内存中执行的，需要进行多个步骤。
- en: 'In Java:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In Scala:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In Python, this code is not supported.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，不支持这段代码。
- en: Aggregate on a grouped tuple dataset
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对分组元组数据集进行聚合
- en: Aggregate transformations are very common. We can easily perform common aggregations
    such as `sum`, `min`, and `max` on tuple datasets. The following is the way we
    do it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合转换非常常见。我们可以很容易地对元组数据集执行常见的聚合，如`sum`、`min`和`max`。以下是我们执行的方式。
- en: 'In Java:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In Scala:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In Python:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Please note here that in DataSet API, if we need to apply multiple aggregations,
    we need to use the `and` keyword.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在DataSet API中，如果我们需要应用多个聚合，我们需要使用“and”关键字。
- en: MinBy on a grouped tuple dataset
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对分组元组数据集进行MinBy
- en: The `minBy` function selects a single tuple from each group of tuple datasets
    for which the value is the minimum. The fields used for comparison must be comparable.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`minBy`函数从元组数据集的每个组中选择一个元组，其值为最小值。用于比较的字段必须是可比较的。'
- en: 'In Java:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In Scala:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In Python, this code is not supported.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，不支持这段代码。
- en: MaxBy on a grouped tuple dataset
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对分组元组数据集进行MaxBy
- en: The `MaxBy` function selects a single tuple from each group of tuple datasets
    for which the value is the maximum. The fields used for comparison must be comparable.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`MaxBy`函数从元组数据集的每个组中选择一个元组，其值为最大值。用于比较的字段必须是可比较的。'
- en: 'In Java:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In Scala:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In Python, this code is not supported.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，不支持这段代码。
- en: Reduce on full dataset
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对完整数据集进行减少
- en: The reduce transformation allows for the application of a user-defined function
    on a full dataset. Here is an example.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 减少转换允许在整个数据集上应用用户定义的函数。以下是一个例子。
- en: 'In Java:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In Scala:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In Python:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中：
- en: '[PRE26]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Group reduce on a full dataset
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对完整数据集进行组减少
- en: The group reduce transformation allows for the application of a user-defined
    function on a full dataset. Here is an example.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 组减少转换允许在整个数据集上应用用户定义的函数。以下是一个例子。
- en: 'In Java:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In Scala:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In Python:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中：
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Aggregate on a full tuple dataset
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对完整元组数据集进行聚合
- en: We can run common aggregation functions on full datasets. So far Flink supports
    `MAX`, `MIN`, and `SUM`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对完整数据集运行常见的聚合函数。到目前为止，Flink支持`MAX`、`MIN`和`SUM`。
- en: 'In Java:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE30]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In Scala:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE31]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In Python:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中：
- en: '[PRE32]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: MinBy on a full tuple dataset
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在完整元组数据集上的MinBy
- en: The `MinBy` function selects a single tuple from the full dataset for which
    the value is the minimum. The fields used for comparison must be comparable.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`MinBy`函数从完整数据集中选择一个数值最小的元组。用于比较的字段必须是可比较的。'
- en: 'In Java:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE33]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In Scala:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE34]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In Python, this code is not supported.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，此代码不受支持。
- en: MaxBy on a full tuple dataset
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在完整元组数据集上的MaxBy
- en: '`MaxBy` selects a single tuple full dataset for which the value is maximum.
    The fields used for comparison must be comparable.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`MaxBy`选择数值最大的单个元组完整数据集。用于比较的字段必须是可比较的。'
- en: 'In Java:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE35]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In Scala:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE36]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In Python, this code is not supported.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，此代码不受支持。
- en: Distinct
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同
- en: The distinct transformation emits distinct values from the source dataset. This
    is used for removing duplicate values from the source.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: distinct转换从源数据集中发出不同的值。这用于从源中删除重复的值。
- en: 'In Java:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE37]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In Scala:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE38]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In Python, this code is not supported.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，此代码不受支持。
- en: Join
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接
- en: The join transformation joins two datasets into one dataset. The joining condition
    can be defined as one of the keys from each dataset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: join转换将两个数据集连接成一个数据集。连接条件可以定义为每个数据集的一个键。
- en: 'In Java:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE39]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In Scala:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE40]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In Python
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中
- en: '[PRE41]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'There are various other ways in which two datasets can be joined. Here is a
    link where you can read more about all such joining options: [https://ci.apache.org/projects/flink/flink-docs-master/dev/batch/dataset_transformations.html#join](https://ci.apache.org/projects/flink/flink-docs-master/dev/batch/dataset_transformations.html#join).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种其他方式可以连接两个数据集。在这里有一个链接，您可以阅读更多关于所有这些连接选项的信息：[https://ci.apache.org/projects/flink/flink-docs-master/dev/batch/dataset_transformations.html#join](https://ci.apache.org/projects/flink/flink-docs-master/dev/batch/dataset_transformations.html#join)。
- en: Cross
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉
- en: The cross transformation does the cross product of two datasets by applying
    a user-defined function.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉转换通过应用用户定义的函数对两个数据集进行交叉乘积。
- en: 'In Java:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In Scala:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE43]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In Python:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中：
- en: '[PRE44]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Union
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联合
- en: The union transformation combines two similar datasets. We can also union multiple
    datasets in one go.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: union转换结合了两个相似的数据集。我们也可以一次联合多个数据集。
- en: 'In Java:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE45]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In Scala:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE46]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In Python:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中：
- en: '[PRE47]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Rebalance
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新平衡
- en: This transformation evenly rebalances parallel partitions. This helps in achieving
    better performance as it helps in removing data skews.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换均匀地重新平衡并行分区。这有助于提高性能，因为它有助于消除数据倾斜。
- en: 'In Java:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE48]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'In Scala:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE49]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In Python, this code is not supported.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，此代码不受支持。
- en: Hash partition
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哈希分区
- en: This transformation partitions the dataset on a given key.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换在给定的键上对数据集进行分区。
- en: 'In Java:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE50]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In Scala:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE51]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In Python, this code is not supported.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，此代码不受支持。
- en: Range partition
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 范围分区
- en: This transformation range partitions the dataset on a given key.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换在给定的键上对数据集进行范围分区。
- en: 'In Java:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE52]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'In Scala:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE53]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: In Python, this code is not supported.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，此代码不受支持。
- en: Sort partition
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排序分区
- en: This transformation locally sorts the partitions dataset on a given key and
    in the given order.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换在给定的键和给定的顺序上本地对分区数据集进行排序。
- en: 'In Java:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE54]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'In Scala:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE55]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In Python, this code is not supported.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，此代码不受支持。
- en: First-n
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 首n
- en: This transformation arbitrarily returns the first-n elements of the dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换任意返回数据集的前n个元素。
- en: 'In Java:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE56]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'In Scala:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE57]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In Python, this code is not supported.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，此代码不受支持。
- en: Broadcast variables
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广播变量
- en: Broadcast variables allow user to access certain dataset as collection to all
    operators. Generally, broadcast variables are used when we you want to refer a
    small amount of data frequently in a certain operation. Those who are familiar
    with Spark broadcast variables will be able use the same feature in Flink as well.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 广播变量允许用户将某些数据集作为集合访问到所有操作符。通常，当您希望在某个操作中频繁引用少量数据时，可以使用广播变量。熟悉Spark广播变量的人也可以在Flink中使用相同的功能。
- en: We just need to broadcast a dataset with a specific name and it will be available
    on each executors handy. The broadcast variables are kept in memory so we have
    to be cautious in using them. The following code snippet shows how to broadcast
    a dataset and use it as needed.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要广播一个具有特定名称的数据集，它将在每个执行器上都可用。广播变量保存在内存中，因此在使用它们时必须谨慎。以下代码片段显示了如何广播数据集并根据需要使用它。
- en: '[PRE58]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Broadcast variables are useful when we have look up conditions to be used for
    transformation and the lookup dataset is comparatively small.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有查找条件要用于转换时，广播变量非常有用，查找数据集相对较小。
- en: Data sinks
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据接收器
- en: 'After the data transformations are done, we need to save the results somewhere.
    The following are some options that Flink DataSet API provides to save the results:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换完成后，我们需要将结果保存在某个地方。以下是Flink DataSet API提供的一些选项，用于保存结果：
- en: '`writeAsText()`: This writes records one line at a time as strings.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`writeAsText()`: 这将记录一行一行地写入字符串。'
- en: '`writeAsCsV()`: This writes tuples as comma-separated value files. Row and
    field delimiters can also be configured.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`writeAsCsV()`: 这将元组写为逗号分隔值文件。还可以配置行和字段分隔符。'
- en: '`print()`/`printErr()`: This writes records to the standard output. You can
    also choose to write to a standard error.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`print()`/`printErr()`: 这将记录写入标准输出。您也可以选择写入标准错误。'
- en: '`write()`: This supports writing data in a custom `FileOutputFormat`.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write()`: 这支持在自定义`FileOutputFormat`中写入数据。'
- en: '`output()`: This is used for datasets which are not file-based. This can be
    used where we want to write data to some database.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output()`: 这用于不基于文件的数据集。这可以用于我们想要将数据写入某个数据库的地方。'
- en: Connectors
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接器
- en: Apache Flink's DataSet API supports various connectors, allowing data read/writes
    across various systems. Let's try to explore more on this.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink的DataSet API支持各种连接器，允许在各种系统之间读取/写入数据。让我们尝试更多地探索这一点。
- en: Filesystems
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件系统
- en: Flink allows connecting to various distributed filesystems such as HDFS, S3,
    Google Cloud Storage, Alluxio, and so on, by default. In this section, we will
    see how to connect to these filesystems.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Flink允许默认连接到各种分布式文件系统，如HDFS、S3、Google Cloud Storage、Alluxio等。在本节中，我们将看到如何连接到这些文件系统。
- en: 'In order to connect to these systems, we need to add the following dependency
    in `pom.xml`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了连接到这些系统，我们需要在`pom.xml`中添加以下依赖项：
- en: '[PRE59]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This allows us to use Hadoop data types, input formats, and output formats.
    Flink supports writable and writable comparable out-of-the-box, so we don't need
    the compatibility dependency for that.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够使用Hadoop数据类型、输入格式和输出格式。Flink支持开箱即用的可写和可比较可写，因此我们不需要兼容性依赖项。
- en: HDFS
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HDFS
- en: To read data from an HDFS file, we create a data source using the `readHadoopFile()`
    or `createHadoopInput()` method. In order to use this connector, we first need
    to configure `flink-conf.yaml` and set `fs.hdfs.hadoopconf` to the proper Hadoop
    configuration directory.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要从HDFS文件中读取数据，我们使用`readHadoopFile()`或`createHadoopInput()`方法创建数据源。为了使用此连接器，我们首先需要配置`flink-conf.yaml`并将`fs.hdfs.hadoopconf`设置为正确的Hadoop配置目录。
- en: The resulting dataset would be a tuple of the type that matches with the HDFS
    data types. The following code snippet shows how to do this.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的数据集将是与HDFS数据类型匹配的元组类型。以下代码片段显示了如何做到这一点。
- en: 'In Java:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE60]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'In Scala:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE61]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We can also use this connector to write back the processed data to HDFS. The
    `OutputFormat` wrapper expects the dataset to be in `Tuple2` format. The following
    code snippet shows how to write back processed data to HDFS.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用此连接器将处理后的数据写回HDFS。`OutputFormat`包装器期望数据集以`Tuple2`格式。以下代码片段显示了如何将处理后的数据写回HDFS。
- en: 'In Java:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE62]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'In Scala:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE63]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Amazon S3
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Amazon S3
- en: 'As stated earlier, Flink supports reading data from Amazon S3 by default. But
    we need to do some configurations in Hadoop''s `core-site.xml`. We need to set
    the following properties:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Flink默认支持从Amazon S3读取数据。但是，我们需要在Hadoop的`core-site.xml`中进行一些配置。我们需要设置以下属性：
- en: '[PRE64]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Once done, we can access the S3 filesystem as shown here:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以像这样访问S3文件系统：
- en: '[PRE65]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Alluxio
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Alluxio
- en: 'Alluxio is an open source, memory speed virtual distributed storage. Many companies
    have been using Alluxio for high-speed data storage and processing. You can read
    more about Alluxio at: [http://www.alluxio.org/](http://www.alluxio.org/).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: Alluxio是一个开源的、内存速度的虚拟分布式存储。许多公司都在使用Alluxio进行高速数据存储和处理。您可以在[http://www.alluxio.org/](http://www.alluxio.org/)上了解更多关于Alluxio的信息。
- en: 'Flink supports reading data from Alluxio by default. But we need to do some
    configurations in Hadoop `core-site.xml`. We need to set the following properties:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Flink默认支持从Alluxio读取数据。但是，我们需要在Hadoop的`core-site.xml`中进行一些配置。我们需要设置以下属性：
- en: '[PRE66]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Once done, we can access the Alluxio filesystem as shown here:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以像这样访问Alluxio文件系统：
- en: '[PRE67]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Avro
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Avro
- en: 'Flink has built-in support for Avro files. It allows easy reads and writes
    to Avro files. In order to read Avro files, we need to use `AvroInputFormat`.
    The following code snippet shows how to read Avro files:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Flink内置支持Avro文件。它允许轻松读写Avro文件。为了读取Avro文件，我们需要使用`AvroInputFormat`。以下代码片段显示了如何读取Avro文件：
- en: '[PRE68]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Once the dataset is ready we can easily perform various transformations, such
    as:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集准备好后，我们可以轻松执行各种转换，例如：
- en: '[PRE69]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Microsoft Azure storage
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Microsoft Azure存储
- en: Microsoft Azure Storage is a cloud-based storage that allows storing data in
    a durable and scalable manner. Flink supports managing data stored on Microsoft
    Azure table storage. The following explains how we do this.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft Azure Storage是一种基于云的存储，允许以持久且可扩展的方式存储数据。Flink支持管理存储在Microsoft Azure表存储上的数据。以下解释了我们如何做到这一点。
- en: 'First of all, we need to download the `azure-tables-hadoop` project from `git`
    and then compile it:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从`git`下载`azure-tables-hadoop`项目，然后编译它：
- en: '[PRE70]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Next we add the following dependencies in `pom.xml`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在`pom.xml`中添加以下依赖项：
- en: '[PRE71]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Next we write the following code to access Azure storage:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写以下代码来访问Azure存储：
- en: '[PRE72]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Now we are all set do any processing of the dataset.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好处理数据集了。
- en: MongoDB
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MongoDB
- en: Through open source contributions, developers have been able to connect Flink
    to MongoDB. In this section, we are going to talk about one such project.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 通过开源贡献，开发人员已经能够将Flink连接到MongoDB。在本节中，我们将讨论这样一个项目。
- en: 'The project is open source and can be downloaded from GitHub:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目是开源的，可以从GitHub下载：
- en: '[PRE73]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Next we use the preceding connector in the Java program to connect to MongoDB:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在Java程序中使用前面的连接器连接到MongoDB：
- en: '[PRE74]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Once the data is available as a dataset, we can easily do the desired transformation.
    We can also write back the data to the MongoDB collection as shown here:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据作为数据集可用，我们可以轻松进行所需的转换。我们还可以像这样将数据写回MongoDB集合：
- en: '[PRE75]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Iterations
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代
- en: One of the unique features Flink supports is iterations. These days a lot developers
    want to run iterative machine-learning and graph-processing algorithms using big
    data technologies. To cater to these needs, Flink supports running iterative data
    processing by defining a step function.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持的一个独特功能是迭代。如今，许多开发人员希望使用大数据技术运行迭代的机器学习和图处理算法。为了满足这些需求，Flink支持通过定义步骤函数来运行迭代数据处理。
- en: Iterator operator
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代器操作符
- en: 'An iterator operator consists of the following components:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代器操作符由以下组件组成：
- en: '![Iterator operator](img/image_03_002.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![迭代器操作符](img/image_03_002.jpg)'
- en: '**Iteration Input**: This is either the initial dataset received or the output
    of the previous iteration'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代输入**：这是接收到的初始数据集或上一次迭代的输出'
- en: '**Step Function**: This is the function that needs to be applied on input dataset'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤函数**：这是需要应用于输入数据集的函数'
- en: '**Next Partial Solution**: This is the output of the step function which needs
    to be fed back to the next iteration'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一个部分解**：这是需要反馈到下一次迭代的步骤函数的输出'
- en: '**Iteration Result**:After all iterations are completed, we get the result
    of iterations'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代结果**：在完成所有迭代后，我们得到迭代的结果'
- en: The number of iterations can be controlled by various ways. One way could be
    setting up the number of iterations to perform or we can also put conditional
    termination.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数可以通过各种方式进行控制。一种方式可以是设置要执行的迭代次数，或者我们也可以进行条件终止。
- en: Delta iterator
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增量迭代器
- en: The delta operator iterates over the set of elements for incremental iteration
    operations. The main difference between that the delta iterator and regular iterator
    is, delta iterator works on updating the solution set rather than fully re-computing
    it every iteration.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 增量运算符对一组元素进行增量迭代操作。增量迭代器和常规迭代器之间的主要区别在于，增量迭代器在更新解决方案集而不是在每次迭代中完全重新计算解决方案集上工作。
- en: This leads to more efficient operations as it allows us to focus on the important
    parts of the solution in less time. The following diagram shows flow of delta
    iterator in Flink.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了更高效的操作，因为它使我们能够在更短的时间内专注于解决方案的重要部分。下图显示了Flink中增量迭代器的流程。
- en: '![Delta iterator](img/image_03_003.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![增量迭代器](img/image_03_003.jpg)'
- en: '**Iteration Input**: We have to read the work set and solution set for the
    delta iterator from some files'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代输入**：我们必须从某些文件中读取增量迭代器的工作集和解决方案集'
- en: '**Step Function**: Step function is the function that needs to be applied on
    the input dataset'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤函数**：步骤函数是需要应用于输入数据集的函数'
- en: '**Next Work Set/ Update Solution**: Here after every iteration solution set
    it is updated with the latest results and the next work set is fed to the next
    iteration'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一个工作集/更新解决方案**：在每次迭代解决方案集之后，它会根据最新结果进行更新，并将下一个工作集提供给下一个迭代'
- en: '**Iteration Result**: After all iterations are completed, we get the result
    of the iterations in the form of a solution set'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代结果**：在完成所有迭代后，我们以解决方案集的形式获得迭代的结果'
- en: Since delta iterators work on hot dataset itself, the performance and efficiency
    are great. Here is a detailed article that talks about using Flink iterations
    for the PageRank algorithm. [http://data-artisans.com/data-analysis-with-flink-a-case-study-and-tutorial/](http://data-artisans.com/data-analysis-with-flink-a-case-study-and-tutorial/).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 由于增量迭代器在热数据集本身上运行，因此性能和效率非常好。以下是一篇详细的文章，讨论了使用Flink迭代器进行PageRank算法。[http://data-artisans.com/data-analysis-with-flink-a-case-study-and-tutorial/](http://data-artisans.com/data-analysis-with-flink-a-case-study-and-tutorial/)。
- en: Use case - Athletes data insights using Flink batch API
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例 - 使用Flink批处理API进行运动员数据洞察
- en: 'Now that we have learnt the details of DataSet API, let''s try to apply this
    knowledge to a real-life use case. Let''s say we have a dataset with us, which
    has information about the Olympics athletes and their performance in various games.
    The sample data looks like the following table:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了DataSet API的细节，让我们尝试将这些知识应用到一个真实的用例中。假设我们手头有一个数据集，其中包含有关奥运会运动员及其在各种比赛中表现的信息。示例数据如下表所示：
- en: '| **Player** | **Country** | **Year** | **Game** | **Gold** | **Silver** |
    **Bronze** | **Total** |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| **球员** | **国家** | **年份** | **比赛** | **金牌** | **银牌** | **铜牌** | **总计** |'
- en: '| Yang Yilin | China | 2008 | Gymnastics | 1 | 0 | 2 | 3 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: 杨伊琳 | 中国 | 2008 | 体操 | 1 | 0 | 2 | 3
- en: '| Leisel Jones | Australia | 2000 | Swimming | 0 | 2 | 0 | 2 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 利塞尔·琼斯 | 澳大利亚 | 2000 | 游泳 | 0 | 2 | 0 | 2'
- en: '| Go Gi-Hyeon | South Korea | 2002 | Short-Track Speed Skating | 1 | 1 | 0
    | 2 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 高基贤 | 韩国 | 2002 | 短道速滑 | 1 | 1 | 0 | 2'
- en: '| Chen Ruolin | China | 2008 | Diving | 2 | 0 | 0 | 2 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 陈若琳 | 中国 | 2008 | 跳水 | 2 | 0 | 0 | 2'
- en: '| Katie Ledecky | United States | 2012 | Swimming | 1 | 0 | 0 | 1 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 凯蒂·莱德基 | 美国 | 2012 | 游泳 | 1 | 0 | 0 | 1'
- en: '| Ruta Meilutyte | Lithuania | 2012 | Swimming | 1 | 0 | 0 | 1 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: 鲁塔·梅卢蒂特 | 立陶宛 | 2012 | 游泳 | 1 | 0 | 0 | 1
- en: '| DAiniel Gyurta | Hungary | 2004 | Swimming | 0 | 1 | 0 | 1 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 达尼尔·吉尔塔 | 匈牙利 | 2004 | 游泳 | 0 | 1 | 0 | 1'
- en: '| Arianna Fontana | Italy | 2006 | Short-Track Speed Skating | 0 | 0 | 1 |
    1 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: 阿里安娜·方塔纳 | 意大利 | 2006 | 短道速滑 | 0 | 0 | 1 | 1
- en: '| Olga Glatskikh | Russia | 2004 | Rhythmic Gymnastics | 1 | 0 | 0 | 1 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: 奥尔加·格拉茨基赫 | 俄罗斯 | 2004 | 韵律体操 | 1 | 0 | 0 | 1
- en: '| Kharikleia Pantazi | Greece | 2000 | Rhythmic Gymnastics | 0 | 0 | 1 | 1
    |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 卡里克莱亚·潘塔齐 | 希腊 | 2000 | 韵律体操 | 0 | 0 | 1 | 1'
- en: '| Kim Martin | Sweden | 2002 | Ice Hockey | 0 | 0 | 1 | 1 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: 金·马丁 | 瑞典 | 2002 | 冰球 | 0 | 0 | 1 | 1
- en: '| Kyla Ross | United States | 2012 | Gymnastics | 1 | 0 | 0 | 1 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 凯拉·罗斯 | 美国 | 2012 | 体操 | 1 | 0 | 0 | 1'
- en: '| Gabriela Dragoi | Romania | 2008 | Gymnastics | 0 | 0 | 1 | 1 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 加布里埃拉·德拉戈伊 | 罗马尼亚 | 2008 | 体操 | 0 | 0 | 1 | 1'
- en: '| Tasha Schwikert-Warren | United States | 2000 | Gymnastics | 0 | 0 | 1 |
    1 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 塔莎·施维克特-沃伦 | 美国 | 2000 | 体操 | 0 | 0 | 1 | 1'
- en: Now we want to get answers to the questions such as, How many players per country
    participated in the games? Or how many players participated for each game? As
    the data is at rest, we will be using Flink Batch API to analyze it.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要得到答案，比如，每个国家有多少运动员参加了比赛？或者每个比赛有多少运动员参加了？由于数据处于静止状态，我们将使用Flink批处理API进行分析。
- en: The data available is in the CSV format. So we will be using a CSV reader provided
    by Flink API as shown in the following code snippet.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的数据以CSV格式存在。因此，我们将使用Flink API提供的CSV读取器，如下面的代码片段所示。
- en: '[PRE76]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Once the data is parse properly, it is easy to move ahead and use it as required.
    The following code snippet shows how to get information of no. of players per
    country:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被正确解析，就很容易继续使用它。以下代码片段显示了如何获取每个国家的球员数量的信息：
- en: '[PRE77]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'In the preceding code snippet, we are first creating datasets with the key
    as the player''s country and value as `1` and then we group it and sum up the
    values to get the total count. Once we execute the code, here is how the output
    looks:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们首先创建了以球员国家为键，值为`1`的数据集，然后对其进行分组并求和以获得总数。一旦我们执行了代码，输出如下所示：
- en: '[PRE78]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Similarly we can apply the same logic to find the number of players per game
    as shown in the following code snippet:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以应用相同的逻辑来查找每场比赛的球员数量，如下面的代码片段所示：
- en: '[PRE79]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The output of the preceding code snippet would look as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码片段的输出如下：
- en: '[PRE80]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: This way you can run various other transformations to get the desired output.
    The complete code for this use case is available at [https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter03/flink-batch](https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter03/flink-batch).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，您可以运行各种其他转换以获得所需的输出。此用例的完整代码可在[https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter03/flink-batch](https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter03/flink-batch)上找到。
- en: Summary
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learnt about DataSet API. It enabled us to do the batch
    processing. We learnt various transformations in order to do data processing.
    Later we also explored the various file-based connectors to read/write data from
    HDFS, Amazon S3, MS Azure, Alluxio, and so on.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了DataSet API。它使我们能够进行批处理。我们学习了各种转换以进行数据处理。后来，我们还探索了各种基于文件的连接器，以从HDFS、Amazon
    S3、MS Azure、Alluxio等读取/写入数据。
- en: In the last section, we looked a use case where we applied the knowledge learnt
    in the earlier sections.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们看了一个用例，在这个用例中，我们应用了在前几节中学到的知识。
- en: In the next chapter, we are going to learn another very important API from Flink's
    ecosystem point of view that is, Table API.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习另一个非常重要的API，即Table API，从Flink的生态系统角度来看。
