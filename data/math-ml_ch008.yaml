- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: The Geometric Structure of Vector Spaces
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 向量空间的几何结构
- en: 'Let’s revisit the Iris dataset introduced in the previous chapter! I want to
    test your intuition. I plotted the petal widths against the petal lengths while
    hiding the class labels in Figure 2.1:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视上一章中介绍的鸢尾花数据集！我想考察一下你的直觉。我将花瓣宽度与花瓣长度绘制在一起，并在图 2.1 中隐藏了类别标签：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![PIC](img/file69.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file69.png)'
- en: 'Figure 2.1: The “petal width” and “petal length” features of the Iris dataset'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：鸢尾花数据集中的“花瓣宽度”和“花瓣长度”特征
- en: Even without knowing any labels, we can intuitively point out that there are
    probably at least two classes. Can you summarize your reasoning in a single sentence?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有知道任何标签，我们也可以凭直觉指出，可能至少有两个类别。你能用一句话总结你的推理吗？
- en: There are many valid arguments, but the most prevalent one is that the two clusters
    are far away from each other. As this example illustrates, the concept of distance
    plays an essential role in machine learning. In this chapter, we will translate
    the notion of distance into the language of mathematics and put it into the context
    of vector spaces.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多有效的论点，但最常见的观点是，两个簇之间相距很远。正如这个例子所示，距离的概念在机器学习中起着至关重要的作用。在本章中，我们将把距离的概念翻译成数学语言，并将其置于向量空间的背景下。
- en: 2.1 Norms and distances
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 范数与距离
- en: Previously, we saw that vectors are essentially arrows, starting from the null
    vector. In addition to their direction, vectors also have magnitude. For example,
    as we have learned in high school mathematics, the magnitude in the Euclidean
    plane is defined by
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们看到向量本质上是从零向量开始的箭头。除了方向，向量还有大小。例如，正如我们在高中数学中学到的，欧几里得平面中的大小定义为
- en: '![ ∘ ------- 2 2 ∥x ∥ = x1 + x 2, x = (x1,x2), ](img/file70.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![ ∘ ------- 2 2 ∥x ∥ = x1 + x 2, x = (x1,x2), ](img/file70.png)'
- en: while we can calculate the distance between x and y as
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 同时我们可以计算 x 和 y 之间的距离为
- en: '![ ∘ --------2-----------2 d(x,y) = (x1 − y1) + (x2 − y2) . ](img/file71.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![ ∘ --------2-----------2 d(x,y) = (x1 − y1) + (x2 − y2) . ](img/file71.png)'
- en: (The function ∥⋅∥ simply denotes the magnitude of a vector.)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: （函数 ∥⋅∥ 仅表示向量的大小。）
- en: '![PIC](img/file73.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file73.png)'
- en: 'Figure 2.2: Magnitude in the Euclidean plane'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：欧几里得平面中的大小
- en: The magnitude formula ![∘ ------- x21 + x22](img/file74.png) can be simply generalized
    to higher dimensions by
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大小公式 ![∘ ------- x21 + x22](img/file74.png) 可以简单地推广到更高维度，方法是
- en: '![ ∘ ------------ 2 2 n ∥x ∥ = x1 + ⋅⋅⋅+ xn, x = (x1,...,xn) ∈ ℝ . ](img/file75.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![ ∘ ------------ 2 2 n ∥x ∥ = x1 + ⋅⋅⋅+ xn, x = (x1,...,xn) ∈ ℝ . ](img/file75.png)'
- en: However, just from looking at this formula, it is not clear why it is defined
    this way. What does the square root of a sum of squares have to do with distance
    and magnitude? Behind the scenes, it is just the Pythagorean theorem.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅从这个公式看，并不清楚为什么它是这样定义的。平方和的平方根与距离和大小有什么关系呢？实际上，它背后只是勾股定理。
- en: Recall that the Pythagorean theorem states that in right triangles, the squared
    length of the hypotenuse equals the sum of the squared lengths of the other sides,
    as illustrated by Figure [2.3](#).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，勾股定理指出，在直角三角形中，斜边的平方等于其他两边的平方和，如图 [2.3](#) 所示。
- en: '![PIC](img/file76.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file76.png)'
- en: 'Figure 2.3: The Pythagorean theorem'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：勾股定理
- en: To put this into an algebraic form, it states that a² + b² = c², when c is the
    hypotenuse of the right triangle, and a and b are its two other sides. If we apply
    this to a two-dimensional vector x = (x[1],x[2]), we can see that the Pythagorean
    theorem gives its magnitude ∥x∥ = ![∘ -2----2 x1 + x2](img/file77.png) .
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 将其转换为代数形式时，它表明 a² + b² = c²，当 c 是直角三角形的斜边，a 和 b 是它的两个其他边。如果我们将此应用于二维向量 x = (x[1],x[2])，我们可以看到，勾股定理给出了其大小
    ∥x∥ = ![∘ -2----2 x1 + x2](img/file77.png)。
- en: This can be generalized to higher dimensions. To see what is happening, we are
    going to check the three-dimensional case, as illustrated by Figure [1.4](#).
    Here, we can apply the Pythagorean theorem twice to obtain the magnitude!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以推广到更高维度。为了看看发生了什么，我们将检查三维情况，如图 [1.4](#) 所示。在这里，我们可以将勾股定理应用两次来得到大小！
- en: '![PIC](img/file78.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file78.png)'
- en: 'Figure 2.4: The Pythagorean theorem in three dimensions'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：三维空间中的勾股定理
- en: For each vector x = (x[1],x[2],x[3]), we can take a look at the triangle determined
    by (0,0,0),(x[1],0,0), and (x[1],x[2],0) first. The length of the hypotenuse can
    be calculated by ![∘ ------- x21 + x22](img/file79.png). However, the points (0,0,0),
    (x[1],x[2],0), and (x[1],x[2],x[3]) form a right triangle. Applying the Pythagorean
    theorem once again, we obtain
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个向量x = (x[1],x[2],x[3])，我们可以首先查看由(0,0,0)、(x[1],0,0)和(x[1],x[2],0)确定的三角形。斜边的长度可以通过![∘
    ------- x21 + x22](img/file79.png)来计算。然而，点(0,0,0)、(x[1],x[2],0)和(x[1],x[2],x[3])形成一个直角三角形。再次应用毕达哥拉斯定理，我们得到
- en: '![ ∘ ------------ ∥x∥ = x2 + x2+ x2, 1 2 3 ](img/file80.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![ ∘ ------------ ∥x∥ = x2 + x2+ x2, 1 2 3](img/file80.png)'
- en: which is called the Euclidean norm. This is exactly what is going on in the
    general n-dimensional case.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为欧几里得范数。这正是一般n维情况下发生的事情。
- en: The notions of magnitude and distance are critical in machine learning, as we
    can use them to determine the similarity between data points, measure and control
    the complexity of neural networks, and much more.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，大小和距离的概念至关重要，因为我们可以利用它们来确定数据点之间的相似度，衡量并控制神经网络的复杂度，等等。
- en: Is the Pythagorean theorem the only viable way to measure magnitude and distance?
    Certainly not.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 毕达哥拉斯定理是唯一可行的度量大小和距离的方法吗？当然不是。
- en: Because Manhattan’s street layout is essentially a rectangular grid, its residents
    are famed for measuring distances in blocks. If something is two blocks to the
    north and three blocks east, it means that you have to travel two intersections
    to the north and three to the east to find it. This gives rise to a mathematically
    perfectly valid notion of measurement called Manhattan distance, defined by
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于曼哈顿的街道布局本质上是一个矩形网格，其居民以按街区测量距离而闻名。如果某个地方位于北方两街区和东方三街区，那么就意味着你需要先向北走两个交叉口，再向东走三个交叉口才能到达。由此产生了一种数学上完全有效的度量概念，称为曼哈顿距离，定义为
- en: '![d(x,y ) = |x1 − y1| + |x2 − y2|. ](img/file81.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![d(x,y) = |x1 − y1| + |x2 − y2|.](img/file81.png)'
- en: When using the Manhattan distance, the shortest path between two points is not
    unique.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用曼哈顿距离时，两个点之间的最短路径不是唯一的。
- en: '![PIC](img/file82.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file82.png)'
- en: 'Figure 2.5: For the Manhattan distance, the shortest path between two points
    is not unique'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：对于曼哈顿距离，两个点之间的最短路径不是唯一的
- en: Besides the Euclidean and Manhattan distances, there are several other metrics.
    Once again, we are going to step away from the concrete examples to take an abstract
    viewpoint.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了欧几里得距离和曼哈顿距离外，还有其他几种度量。我们将再次从具体实例中抽离，采用抽象的视角来讨论。
- en: 'If we talk about measurements and metrics in general, what are the properties
    that we expect from all of them? What makes a measurement distance? Essentially,
    there are three such traits:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们谈论一般的测量和度量，那么我们期望它们具备哪些属性呢？什么构成了测量距离？本质上，有三个这样的特征：
- en: the distance should be nonnegative,
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 距离应该是非负的，
- en: it should preserve scaling (that is, ![d(cx,cy) = cd(x,y ) ](img/file83.png)
    for all scalars ![c ](img/file84.png)),
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该保持缩放性（即，![d(cx,cy) = cd(x,y)](img/file83.png) 对所有标量![c](img/file84.png)成立），
- en: the distance straight from point ![x ](img/file85.png) to ![y ](img/file86.png)
    is always equal to or smaller than touching any other point ![z ](img/file87.png).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从点![x](img/file85.png)到![y](img/file86.png)的直线距离始终等于或小于触及任何其他点![z](img/file87.png)的距离。
- en: These are formalized by the notion of norms.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都通过范数的概念来形式化。
- en: Definition 7\. (Norms)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 7\.（范数）
- en: 'Let ![V ](img/file88.png) be a vector space. A function ![∥⋅∥ : V → [0,∞ )
    ](img/file89.png) is said to be a norm if for all ![x, y ∈ V ](img/file90.png),
    the following properties hold:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '设![V](img/file88.png)是一个向量空间。若函数![∥⋅∥ : V → [0,∞ )](img/file89.png)满足以下条件，则称其为范数：对于所有![x,
    y ∈ V](img/file90.png)，以下性质成立：'
- en: 'Positive definiteness: ![∥x ∥ ≥ 0 ](img/file91.png) and ![∥x∥ = 0 ](img/file92.png)
    if and only if ![x = 0 ](img/file93.png).'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正定性：![∥x∥ ≥ 0](img/file91.png) 且![∥x∥ = 0](img/file92.png) 当且仅当![x = 0](img/file93.png)。
- en: 'Positive homogeneity: ![∥cx∥ = |c|∥x ∥ ](img/file94.png) for all ![c ∈ ℝ ](img/file95.png).'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正齐次性：![∥cx∥ = |c|∥x∥](img/file94.png) 对所有![c ∈ ℝ](img/file95.png)成立。
- en: 'Triangle inequality: ![∥x + y ∥ ≤ ∥x ∥+ ∥y ∥ ](img/file96.png) for all ![x,y
    ∈ V ](img/file97.png).'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 三角不等式：![∥x + y∥ ≤ ∥x∥ + ∥y∥](img/file96.png) 对所有![x,y ∈ V](img/file97.png)成立。
- en: A vector space equipped with a norm is called a normed space.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 配备范数的向量空间称为范数空间。
- en: Let’s see some examples!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些例子吧！
- en: Example 1\. Let p ∈ [1,∞) and define
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 1\. 设p ∈ [1,∞)，并定义
- en: '![ ∑n p 1∕p ∥x∥p = ( |xi|) , x = (x1,...,xn ) i=1 ](img/file98.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n p 1∕p ∥x∥p = ( |xi|) , x = (x1,...,xn ) i=1](img/file98.png)'
- en: on ℝ^n. The function ∥⋅∥[p] is called the p-norm. Showing that ∥⋅∥[p] is indeed
    a norm is a bit technical. Thus, we won’t go into the details. (The triangle inequality
    requires some work, but the other two properties are easy to see.)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ℝ^n 上，函数 ∥⋅∥[p] 被称为 p-范数。证明 ∥⋅∥[p] 确实是一个范数有点技术性。因此，我们不会进入详细的讨论。（三角不等式需要一些工作，但其他两个性质很容易看出。）
- en: 'We have already seen two special cases: the Euclidean norm (p = 2) and the
    Manhattan norm (p = 1). Both of them frequently appear in machine learning. For
    instance, the familiar mean squared error is just the scaled Euclidean distance
    between prediction and ground truth:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了两个特殊情况：欧几里得范数（p = 2）和曼哈顿范数（p = 1）。它们在机器学习中经常出现。例如，常见的均方误差就是预测值和真实值之间的缩放欧几里得距离：
- en: '![ n MSE (y,yˆ) =-1∥y − ˆy∥2 = 1-∑ (y − ˆy )2 n 2 n i i i=1 ](img/file99.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![ n MSE (y,yˆ) =-1∥y − ˆy∥2 = 1-∑ (y − ˆy )2 n 2 n i i i=1 ](img/file99.png)'
- en: 'As mentioned before, the 2-norm, along with the 1-norm, is commonly used to
    control the complexity of models during training. To give a concrete example,
    suppose that we are fitting a polynomial f(x) = ∑ [i=0]^mq[i]x^i to the data {(x[1],y[1]),…,(x[n],y[n])}
    . To obtain a model that generalizes well to new data, we prefer our models to
    be as simple as possible. Thus, instead of using the plain mean squared error,
    we might consider minimizing the loss:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，2-范数与 1-范数通常一起用于训练过程中控制模型的复杂度。举个具体的例子，假设我们正在拟合一个多项式 f(x) = ∑ [i=0]^mq[i]x^i
    到数据 {(x[1],y[1]),…,(x[n],y[n])}。为了得到一个对新数据有良好泛化能力的模型，我们希望模型尽可能简单。因此，我们可能考虑最小化损失：
- en: '![Loss(y,ˆy,q) = MSE (y, ˆy)+ λ∥q ∥p, q = (q0,q1,...,qm), λ ∈ [0,∞ ) ](img/file100.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![Loss(y,ˆy,q) = MSE (y, ˆy)+ λ∥q ∥p, q = (q0,q1,...,qm), λ ∈ [0,∞ ) ](img/file100.png)'
- en: where the term ∥q∥[p] is responsible for keeping the coefficients of the polynomial
    f(x) small, while λ controls the strength of regularization. Usually, p is either
    1 or 2, but other values from [1,∞) are also valid.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中项 ∥q∥[p] 负责保持多项式 f(x) 的系数较小，而 λ 控制正则化的强度。通常，p 的值为 1 或 2，但 [1,∞) 区间内的其他值也是有效的。
- en: Example 2\. Let’s stay in ℝ^n for a bit longer! The so-called ∞-norm is defined
    by
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2\. 让我们再在 ℝ^n 上停留一会儿！所谓的 ∞-范数定义为
- en: '![∥x ∥ = max {|x |,...,|x |}. ∞ 1 n ](img/file101.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![∥x ∥ = max {|x |,...,|x |}. ∞ 1 n ](img/file101.png)'
- en: Showing that ∥⋅∥[∞] is indeed a norm is a simple task and left to you for practice.
    (This is perhaps one of the most notorious sentences written in mathematical textbooks,
    but trust me, this is truly easy. Give it a shot! If you don’t see it, try the
    special case ℝ².)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 证明 ∥⋅∥[∞] 确实是一个范数是一个简单的任务，留给你自己练习。（这或许是数学教材中最臭名昭著的一句话，但相信我，这真的很简单。试试看！如果你看不懂，可以尝试特殊情况
    ℝ²。）
- en: This is called the ∞-norm, and is strongly related to the p-norm that we have
    just seen. In fact, if we let the value p grow infinitely, ∥x∥[p] will be very
    close to ∥x∥[∞] , ultimately reaching it at the limit.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的 ∞-范数，并且与我们刚才看到的 p-范数有着密切的关系。事实上，如果我们让 p 的值无限增大，∥x∥[p] 将非常接近 ∥x∥[∞]，最终在极限时达成一致。
- en: Remark 2\. (The ∞-norm as the limit of p-norm)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 备注 2\. （∞-范数作为 p-范数的极限）
- en: If you are already familiar with convergent sequences and limits, you can see
    that this is called the ∞-norm because
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉收敛序列和极限，你会看到这是所谓的 ∞-范数，因为
- en: '![lim ∥x∥ = ∥x∥ . p→ ∞ p ∞ ](img/file102.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![lim ∥x∥ = ∥x∥ . p→ ∞ p ∞ ](img/file102.png)'
- en: To show this, consider that
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这一点，考虑到
- en: '![ ∑n p 1∕p ∑n -|xi|-p 1∕p pli→m∞ ∥x ∥p = pl→im∞ ( |xi|) = pli→m∞ ∥x∥∞ ( (∥x
    ∥∞ )) . i=1 i=1 ](img/file103.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n p 1∕p ∑n -|xi|-p 1∕p pli→m∞ ∥x ∥p = pl→im∞ ( |xi|) = pli→m∞ ∥x∥∞ ( (∥x
    ∥∞ )) . i=1 i=1 ](img/file103.png)'
- en: Since ![-|xi|- ∥x∥∞](img/file104.png) ≤ 1 by definition,
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于根据定义！[-|xi|- ∥x∥∞](img/file104.png) ≤ 1，
- en: '![ ∑n |xi| 1 ≤ ( (------)p)1∕p ≤ n1 ∕p i=1 ∥x ∥∞ ](img/file105.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n |xi| 1 ≤ ( (------)p)1∕p ≤ n1 ∕p i=1 ∥x ∥∞ ](img/file105.png)'
- en: holds. Because
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 成立。因为
- en: '![lim n1∕p = 1, p→∞ ](img/file106.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![lim n1∕p = 1, p→∞ ](img/file106.png)'
- en: we can conclude that
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论：
- en: '![lim ∥x∥p = ∥x∥∞. p→ ∞ ](img/file107.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![lim ∥x∥p = ∥x∥∞. p→ ∞ ](img/file107.png)'
- en: This is the reason why the ∞-norm is considered a p-norm with p = ∞.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么 ∞-范数被视为一个 p-范数，其中 p = ∞。
- en: If you are not familiar with taking limits of sequences, don’t worry. We’ll
    cover everything in detail when studying single-variable calculus.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉序列极限的概念，不用担心。我们将在学习单变量微积分时详细讲解这一点。
- en: Example 3\. ∞-norms can be generalized for function spaces. Remember C([0,1]),
    the vector space of functions continuous on [0,1]? We introduced this when talking
    about examples of vector spaces in Section [1.1.1](ch007.xhtml#examples-of-vector-spaces).
    There, ∥⋅∥[∞] can be defined as
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 3\. ∞-范数可以推广到函数空间。还记得C([0,1])吗？这是在第 [1.1.1](ch007.xhtml#examples-of-vector-spaces)
    节中介绍的连续函数的向量空间示例。在那里，∥⋅∥[∞]可以定义为
- en: '![∥f∥∞ = sup |f (x )|. x∈[0,1] ](img/file108.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![∥f∥∞ = sup |f (x )|. x∈[0,1] ](img/file108.png)'
- en: This norm can be defined on other function spaces, like C(ℝ), the space of continuous
    real functions. Since the maximum is not guaranteed to exist (as for the sigmoid
    function in C(ℝ)), the maximum is replaced with supremum. Hence, the ∞-norm is
    often called the supremum norm.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这种范数可以在其他函数空间中定义，例如C(ℝ)，即连续实函数的空间。由于最大值不能保证存在（例如在C(ℝ)中的S形函数），最大值被替换为上确界。因此，∞-范数通常被称为上确界范数。
- en: If you imagine the function as a landscape, the supremum norm is the height
    of the highest peak or the depth of the deepest trench (whichever is larger in
    absolute value).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将函数想象为一种景观，则上确界范数是最高峰的高度或最深槽的深度（无论绝对值较大者）。
- en: '![PIC](img/file109.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file109.png)'
- en: 'Figure 2.6: The supremum norm'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：上确界范数
- en: When encountering this norm for the first time, it might seem challenging to
    understand what this has to do with any notion of magnitude. However, ∥f −g∥[∞]
    is a natural way to measure the distance between two functions f and g, and in
    general, magnitude is just the distance from 0.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当第一次遇到这种范数时，可能会觉得难以理解它与任何大小概念有什么关系。然而，∥f −g∥[∞]是衡量两个函数f和g之间距离的一种自然方式，一般来说，大小只是与0的距离。
- en: '![PIC](img/file110.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file110.png)'
- en: 'Figure 2.7: The distance between two functions, given by the supremum norm'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：由上确界范数给出的两个函数之间的距离
- en: 2.1.1 Defining distances from norms
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 定义从范数到距离的距离
- en: Besides measuring the magnitude of vectors, we are also interested in measuring
    the distance between them. If you are at the location x in some normed space,
    how far is y? In normed vector spaces, we can define the distance between any
    x and y by
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了测量向量的大小外，我们还对测量它们之间的距离感兴趣。如果您在某个诱导范数空间中的位置为x，则y有多远？在诱导范数向量空间中，我们可以通过以下方式定义任何x和y之间的距离
- en: '![d(x,y) = ∥x− y∥. ](img/file111.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![d(x,y) = ∥x− y∥. ](img/file111.png)'
- en: This is called the norm-induced metric. Thus, norms measure the distance from
    the zero vector, and the metric d measures the norm of the difference.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为诱导范数的度量。因此，范数衡量了与零向量的距离，而度量d衡量了差异的范数。
- en: 'In general, we say that a function d : V ×V → [0,∞) is a metric if the following
    hold.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们说函数d：V ×V → [0,∞)是一种度量，如果以下条件对于所有x,y,z ∈ V 都成立。
- en: Definition 8\. (Metrics)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 8\. （度量）
- en: 'Let ![V ](img/file112.png) be a vector space and d : V ×V → [0,∞) be a function.
    d is a metric if the following conditions hold for all x,y,z ∈ V:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让 ![V ](img/file112.png) 是一个向量空间，d：V ×V → [0,∞) 是一个函数。如果以下条件对于所有的x,y,z ∈ V 都成立，d就是一个度量：
- en: Whenever d(x,y ) = 0 , we have x = y (positive definiteness).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当d(x,y ) = 0时，我们有x = y（正定性）。
- en: d (x, y) = d (y, x) (symmetry).
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: d (x, y) = d (y, x)（对称性）。
- en: d (x, z) ≤ d(x,y )+ d(y,z)(triangle inequality).
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: d (x, z) ≤ d(x,y )+ d(y,z)(三角不等式)。
- en: One of the immediate consequences of the definition is that if x≠y, then d(x,y)/span>0\.
    (As the positive definiteness gives that d(x,y) = 1 implies x = y.)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 定义的一个直接结果是，如果x≠y，则d(x,y)/span>0。 （由于正定性给出了d(x,y) = 1意味着x = y。）
- en: Given the properties of norms, we can quickly check that d(x,y) = ∥x−y∥ is indeed
    a metric. Due to the linear structure of vector spaces, the norm-generated metric
    is invariant to translation. That is, for any x,y,z ∈V , we have
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于向量空间的线性结构，我们可以快速检查d(x,y) = ∥x−y∥确实是一种度量。这是因为范数生成的度量对平移是不变的。也就是说，对于任意的x,y,z
    ∈V ，我们有
- en: '![d(x,y) = d(x + z,y + z). ](img/file120.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![d(x,y) = d(x + z,y + z). ](img/file120.png)'
- en: 'In other words, it doesn’t matter where you start: the distance only depends
    on your displacement. This is not true for any metric. Thus, norm-induced metrics
    are special. In our studies, we only deal with these special cases. Because of
    this, we won’t even talk about metrics, just norms.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，你从哪里出发并不重要：距离只取决于你的位移。这对于任何度量来说都不成立。因此，诱导范数度量是特殊的。在我们的研究中，我们只处理这些特殊情况。因此，我们甚至不会讨论度量，只讨论范数。
- en: In itself, a vector space is just a skeleton that provides a way to represent
    data. On top of this, norms define a geometric structure that reveals properties
    such as magnitude and distance. Both of these are essential in machine learning.
    For instance, some unsupervised learning algorithms separate data points into
    clusters based on their mutual distances from each other.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 向量空间本身只是一个骨架，它提供了一种表示数据的方式。在此基础上，范数定义了一种几何结构，揭示了大小和距离等属性。这两者在机器学习中都是至关重要的。例如，一些无监督学习算法根据数据点之间的相互距离将数据点分成不同的簇。
- en: 'There is yet another way to enhance the geometric structure of vector spaces:
    inner products, also called dot products. We are going to put this concept under
    our magnifying glass in the next section.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种方法可以增强向量空间的几何结构：内积，也称为点积。我们将在下一节中对这个概念进行详细探讨。
- en: 2.2 Inner products, angles, and lots of reasons to care about them
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 内积、角度及其重要性
- en: In the previous section, we imbued our vector spaces with norms, measuring the
    magnitude of vectors and the distance between points. In machine learning, these
    concepts can be used, for instance, to identify clusters in unlabeled datasets.
    However, without context, distance is often not enough. Following our geometric
    intuition, we can aspire to measure the similarity of data points. This is done
    by the inner product (also known as the dot product).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们给向量空间赋予了范数，用来衡量向量的大小和点之间的距离。在机器学习中，这些概念可以用于，例如，识别无标签数据集中的簇。然而，在没有上下文的情况下，距离往往是不够的。根据我们的几何直觉，我们可以希望测量数据点之间的相似性。通过内积（也称为点积）可以实现这一点。
- en: You can recall the inner product as a quantity that we used to measure the angle
    between two vectors in high school geometry classes. Given two vectors x = (x[1],x[2])
    and y = (y[1],y[2]) from the plane, we defined their inner product by
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以回想起内积是我们用来测量两向量之间角度的量，就像在高中几何课上学到的那样。给定平面上的两个向量 x = (x[1],x[2]) 和 y = (y[1],y[2])，我们通过以下方式定义它们的内积：
- en: '![⟨x, y⟩ = x y + x y , 1 1 2 2 ](img/file121.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![⟨x, y⟩ = x y + x y , 1 1 2 2 ](img/file121.png)'
- en: for which it can be shown that
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明
- en: ⟨x,y⟩ = ∥x∥∥y∥ cosα (2.1)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ⟨x,y⟩ = ∥x∥∥y∥ cosα （2.1）
- en: holds, where α is the angle between x and y. (In fact, there are two such angles,
    but their cosine is equal.) Thus, the angle itself can be extracted by
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其成立，其中 α 是 x 和 y 之间的角度。（事实上，存在两个这样的角度，但它们的余弦值相等。）因此，角度本身可以通过以下方式提取：
- en: '![ -⟨x,y-⟩ α = arccos∥x ∥∥y∥, ](img/file122.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![ -⟨x,y-⟩ α = arccos∥x ∥∥y∥, ](img/file122.png)'
- en: where arccosx is the inverse of the cosine function. We can use the inner products
    to determine whether two vectors are orthogonal, as this happens if and only if
    ⟨x,y⟩ = 0 holds. During our earlier encounters with mathematics, geometric intuition
    (such as orthogonality) came first, on which we built tools such as the inner
    product. However, if we zoom out and take an abstract viewpoint, things are exactly
    the opposite. As we’ll see soon, inner products emerge quite naturally, giving
    rise to the general concept of orthogonality.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 arccosx 是余弦函数的反函数。我们可以使用内积来确定两个向量是否正交，因为只有当 ⟨x,y⟩ = 0 时，才成立正交性。在我们早期接触到的数学中，几何直觉（比如正交性）是首先出现的，然后我们才构建了像内积这样的工具。然而，如果我们放远来看，并采取一种抽象的观点，事情恰恰相反。正如我们很快会看到的，内积的出现非常自然，推动了正交性的普遍概念的形成。
- en: In general, this is the formal definition of an inner product.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，这是内积的正式定义。
- en: Definition 9\. (Inner products and inner product spaces)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 9. （内积与内积空间）
- en: 'Let V be a real vector space. The function ⟨⋅,⋅⟩ : V × V → ℝ is called an inner
    product if the following holds for all x,y, z ∈ V and a ∈ ℝ:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '设 V 为实向量空间。函数 ⟨⋅,⋅⟩ : V × V → ℝ 称为内积，如果对所有 x, y, z ∈ V 和 a ∈ ℝ，以下条件成立：'
- en: ⟨ax + y,z⟩ = a⟨x,z⟩ + ⟨y, z⟩ (linearity of the first variable).
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ⟨ax + y,z⟩ = a⟨x,z⟩ + ⟨y, z⟩ （第一个变量的线性性）。
- en: ⟨x,y ⟩ = ⟨y,x ⟩ (symmetry).
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ⟨x,y ⟩ = ⟨y,x ⟩ （对称性）。
- en: ⟨x,x ⟩ >0 for all ![x ⁄= 0 ](img/file130.png) (positive definiteness).
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ⟨x,x ⟩ > 0 对所有 ![x ⁄= 0 ](img/file130.png) （正定性）。
- en: Vector spaces with an inner product are called inner product spaces.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 带有内积的向量空间称为内积空间。
- en: Right off the bat, we can immediately deduce two properties. First,
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 立刻，我们可以立即推导出两个性质。首先，
- en: '![⟨0,x ⟩ = ⟨0x,x ⟩ = 0⟨x,x⟩ = 0.](img/file131.png) (2.2)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![⟨0,x ⟩ = ⟨0x,x ⟩ = 0⟨x,x⟩ = 0.](img/file131.png) （2.2）'
- en: 'As a special case, ⟨0,0⟩ = 0\. Just like we have seen for norms, a bit more
    is true: if ⟨x,x⟩ = 0, then x = 0\. This follows from positive definiteness and
    ([2.2](ch008.xhtml#x1-42011x3.2)).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 作为特例，⟨0,0⟩ = 0。就像我们之前看到的范数一样，有更多的性质成立：如果 ⟨x,x⟩ = 0，那么 x = 0。这是由正定性和（[2.2](ch008.xhtml#x1-42011x3.2)）推导出来的。
- en: In addition, due to the symmetry and linearity of the first variable, inner
    products are also linear in the second variable. Because of this, they are called
    bilinear.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于第一个变量的对称性和线性，内积在第二个变量上也是线性的。因此，它们被称为双线性的。
- en: To familiarize ourselves with the concept, let’s see some examples!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了熟悉这个概念，让我们看一些例子！
- en: Example 1\. As usual, the canonical and most prevalent example of inner product
    spaces is ℝ^n, where the inner product ⟨⋅,⋅⟩ is defined by
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 1\. 像往常一样，内积空间的典型且最普遍的例子是 ℝ^n，其中内积 ⟨⋅,⋅⟩ 被定义为
- en: '![ ∑n ⟨x,y⟩ = xiyi, x = (x1,...,xn ), y = (y1,...,yn). i=1 ](img/file132.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n ⟨x,y⟩ = xiyi, x = (x1,...,xn ), y = (y1,...,yn). i=1 ](img/file132.png)'
- en: This bilinear function is often called the dot product. Equipped with this,
    ℝ^n is called the n-dimensional Euclidean space. This is a central concept in
    machine learning, as data is most frequently represented in Euclidean spaces.
    Thus, we are going to explore the structure of this space in great detail throughout
    this book.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个双线性函数通常被称为点积。配备了这个，ℝ^n 被称为 n 维欧几里得空间。这是机器学习中的一个核心概念，因为数据通常表示为欧几里得空间中的向量。因此，我们将在本书中详细探索这个空间的结构。
- en: Example 2\. Besides Euclidean spaces, there are other inner product spaces that
    play a significant role in mathematics and machine learning. If you are familiar
    with integration, in certain function spaces, the bilinear function
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2\. 除了欧几里得空间，还有其他在数学和机器学习中扮演重要角色的内积空间。如果你熟悉积分，在某些函数空间中，双线性函数
- en: '![ ∫ ∞ ⟨f,g⟩ = f(x)g(x)dx −∞ ](img/file133.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ ∞ ⟨f,g⟩ = f(x)g(x)dx −∞ ](img/file133.png)'
- en: defines an inner product space with a very rich and beautiful structure.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了一个具有非常丰富和美丽结构的内积空间。
- en: The symmetry and linearity of ⟨f,g⟩ is clear. Only the positive definiteness
    seems to be an issue.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ⟨f,g⟩ 的对称性和线性是显而易见的。只有正定性似乎是一个问题。
- en: For instance, if f is defined by
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 f 被定义为
- en: '![ ( |{1 if x = 0, f(x) = |(0 otherwise, ](img/file134.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![ ( |{1 if x = 0, f(x) = |(0 otherwise, ](img/file134.png)'
- en: then f≠0, but ⟨f,f⟩ = 0\. This problem can be circumvented by “overloading”
    the equality operator and letting f = g if and only if ∫ [−∞]^∞|f(x) −g(x)|² dx
    = 0\. Even though function spaces such as this play an important role in mathematics
    and machine learning, their study falls outside of our scope.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 f ≠ 0，但 ⟨f,f⟩ = 0\. 这个问题可以通过“重载”相等操作符并让 f = g 当且仅当 ∫ [−∞]^∞|f(x) −g(x)|²
    dx = 0 来避免。尽管像这样的函数空间在数学和机器学习中扮演着重要角色，但它们的研究超出了我们讨论的范围。
- en: 2.2.1 The generated norm
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 生成的范数
- en: Recall that the 2-norm in ℝ^n was defined by ∥x∥[2] = (∑ [i=1]^nx[i]²)^(1∕2)
    , which, according to our definition of the inner product there, equals ![∘ ------
    ⟨x,x⟩](img/file137.png). This is not a coincidence. Inner products can be used
    to define norms on vector spaces.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，ℝ^n 中的 2-范数定义为 ∥x∥[2] = (∑ [i=1]^nx[i]²)^(1∕2)，根据我们在此处对内积的定义，它等于 ![∘ ------
    ⟨x,x⟩](img/file137.png)。这不是巧合。内积可以用来定义向量空间上的范数。
- en: 'To show exactly how, we need a simple tool: the Cauchy-Schwarz inequality.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确切地展示这一点，我们需要一个简单的工具：柯西-施瓦茨不等式。
- en: Theorem 8\. (Cauchy-Schwarz inequality)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 8\.（柯西-施瓦茨不等式）
- en: Let V be an inner product space. Then, for any x,y ∈ V, the inequality
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 设 V 为一个内积空间。那么，对于任何 x,y ∈ V，不等式
- en: '![|⟨x, y⟩|2 ≤ ⟨x,x⟩⟨y,y⟩ ](img/file140.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![|⟨x, y⟩|2 ≤ ⟨x,x⟩⟨y,y⟩ ](img/file140.png)'
- en: holds.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 成立。
- en: Proof. At this point, we don’t know much about the inner product except its
    core defining properties. So, we are going to use a little trick. For any λ ∈
    ℝ, the positive definiteness implies that
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。此时，我们对内积了解得还不多，只知道它的核心定义特性。因此，我们将使用一个小技巧。对于任何 λ ∈ ℝ，正定性意味着
- en: '![⟨x + λy,x + λy ⟩ ≥ 0\. ](img/file142.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![⟨x + λy,x + λy ⟩ ≥ 0\. ](img/file142.png)'
- en: On the other hand, because of bilinearity (that is, linearity in both variables)
    and symmetry, we have
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于双线性（即，两个变量的线性）和对称性，我们有
- en: '![2 ⟨x + λy, x+ λy ⟩ = ⟨x,x ⟩+ 2λ⟨x,y ⟩+ λ ⟨y,y⟩,](img/file143.png) (2.3)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![2 ⟨x + λy, x+ λy ⟩ = ⟨x,x ⟩+ 2λ⟨x,y ⟩+ λ ⟨y,y⟩,](img/file143.png) (2.3)'
- en: which is a quadratic polynomial in λ. In general, we know that for any quadratic
    polynomial of the form ax2 + bx + c, the roots are given by the formula
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关于λ的二次多项式。一般来说，我们知道，对于任何形如 ax² + bx + c 的二次多项式，其根由公式给出：
- en: '![ √ -------- − b-±--b2 −-4ac- x1,2 = 2a . ](img/file146.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![ √ -------- − b-±--b2 −-4ac- x1,2 = 2a . ](img/file146.png)'
- en: Since
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于
- en: '![⟨x + λy,x + λy ⟩ ≥ 0, ](img/file147.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![⟨x + λy,x + λy ⟩ ≥ 0, ](img/file147.png)'
- en: the polynomial defined by (2.3) must have at most one real root. Thus, the discriminant
    b² − 4ac is non-positive. Plugging in the coefficients of the polynomial (2.3)
    into the discriminant formula, we obtain
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由（2.3）定义的多项式最多只有一个实根。因此，判别式 b² − 4ac 非正。将多项式（2.3）的系数代入判别式公式，我们得到
- en: '![|⟨x, y⟩|2 − ⟨x, x⟩⟨y,y⟩ ≤ 0, ](img/file148.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![|⟨x, y⟩|² − ⟨x, x⟩⟨y, y⟩ ≤ 0, ](img/file148.png)'
- en: which completes the proof.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了证明。
- en: The Cauchy-Schwarz inequality is probably one of the most useful tools in studying
    inner product spaces. One application we are going to see next is to show how
    inner products define norms.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 柯西-施瓦茨不等式可能是研究内积空间中最有用的工具之一。接下来我们将看到的一个应用是展示内积如何定义范数。
- en: Theorem 9\. (The norm generated by the inner product)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 9\.（由内积生成的范数）
- en: 'Let V be an inner product space. Then, the function ∥⋅∥ : V → [0,∞ ) defined
    by'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '设 V 为内积空间。那么，定义函数 ∥⋅∥ : V → [0, ∞) 由内积诱导。则，'
- en: '![ ∘ ------ ∥x∥ = ⟨x,x⟩ ](img/file151.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![ ∘ ------ ∥x∥ = ⟨x, x⟩ ](img/file151.png)'
- en: is a norm on V.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 是 V 上的一个范数。
- en: 'Proof. According to the definition of norms, we have to show that three properties
    hold: positive definiteness, homogeneity, and the triangle inequality. The first
    two follow easily from the properties of inner products. The triangle inequality
    follows from the Cauchy-Schwarz inequality:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。根据范数的定义，我们必须证明三个性质成立：正定性、齐次性和三角不等式。前两个性质可以通过内积的性质轻松推导出来。三角不等式则来自柯西-施瓦茨不等式：
- en: ∥x + y∥²  = ⟨x + y,x + y⟩
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ∥x + y∥² = ⟨x + y, x + y⟩
- en: = ∥x∥² + ∥y∥² + 2⟨x,y⟩
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: = ∥x∥² + ∥y∥² + 2⟨x, y⟩
- en: ≤ ∥x∥² + ∥y∥² + 2∥x∥∥y∥
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ≤ ∥x∥² + ∥y∥² + 2∥x∥∥y∥
- en: = (∥x∥ + ∥y∥)²,
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: = (∥x∥ + ∥y∥)²，
- en: from which the triangle inequality follows.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 从中得出三角不等式。
- en: Thus, inner product spaces are normed spaces as well. They have the proper algebraic
    and geometric structure that we need to represent, manipulate, and transform data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，内积空间也是带有范数的空间。它们具有适当的代数和几何结构，我们需要这些结构来表示、操作和转换数据。
- en: Most importantly, Theorem [9](ch008.xhtml#x1-43006r9) can be reversed! That
    is, given a norm ∥⋅∥, we can define a matching inner product.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，定理 [9](ch008.xhtml#x1-43006r9) 是可以反向的！也就是说，给定一个范数 ∥⋅∥，我们可以定义一个匹配的内积。
- en: Theorem 10\. (The polarization identity)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 10\.（极化恒等式）
- en: Let V be an inner product space, and let ∥⋅∥be the norm induced by the inner
    product. Then,
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 设 V 为内积空间，∥⋅∥ 为由内积诱导的范数。那么，
- en: '![1 ⟨x, y⟩ = -(∥x+ y∥2 − ∥x ∥2 − ∥y ∥2). 2](img/file155.png) (2.4)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![1 ⟨x, y⟩ = -(∥x + y∥² − ∥x∥² − ∥y∥²). 2](img/file155.png) （2.4）'
- en: In other words, one can generate an inner product from a norm, not just the
    other way around.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，可以从范数生成内积，而不仅仅是反过来。
- en: Proof. As the inner product is bilinear, we have
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。由于内积是双线性的，我们有
- en: '![⟨x + y,x + y⟩ = ⟨x,x⟩+ 2⟨x, y⟩+ ⟨y,y ⟩, ](img/file156.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![⟨x + y, x + y⟩ = ⟨x, x⟩ + 2⟨x, y⟩ + ⟨y, y⟩, ](img/file156.png)'
- en: from which the polarization identity (2.4) follows.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 从中得出极化恒等式（2.4）。
- en: 2.2.2 Orthogonality
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 正交性
- en: In vector spaces other than ℝ², the concept of enclosed angles is not clear
    at all. For instance, in spaces where vectors are functions, there is no intuitive
    way to define the angles between two functions. However, as (2.1) suggests, in
    the special case ℝ², these can be generalized.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在除 ℝ² 外的向量空间中，包含角度的概念完全不明确。例如，在向量是函数的空间中，没有直观的方法定义两个函数之间的角度。然而，正如（2.1）所示，在特例
    ℝ² 中，这些可以被推广。
- en: Definition 10\. (Orthogonality of vectors)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 10\.（向量的正交性）
- en: Let V be an inner product space, and let x,y ∈V . We say that x and y are orthogonal
    if
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 设 V 为内积空间，x, y ∈ V 。我们说 x 和 y 互为正交，当且仅当
- en: '![⟨x,y ⟩ = 0\. ](img/file157.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![⟨x, y⟩ = 0\. ](img/file157.png)'
- en: Orthogonality is denoted as x ⊥y.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 正交性用 x ⊥ y 表示。
- en: To illustrate how inner products and orthogonality define geometry on vector
    spaces, let’s see how the classic Pythagorean theorem looks in this new form.
    Recall that the “original” version states that in right triangles, a² + b² = c²,
    where c is the length of the hypotenuse, while a and b are the lengths of the
    other two sides.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明内积和正交性如何定义向量空间上的几何，我们来看看经典的毕达哥拉斯定理在这个新形式中的表现。回想一下，原版的定理表示，在直角三角形中，a² + b²
    = c²，其中 c 是斜边的长度，而 a 和 b 是其他两边的长度。
- en: In inner product spaces, this generalizes in the following way.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在内积空间中，这个概念以以下方式进行推广。
- en: Theorem 11\. (The Pythagorean theorem)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 11\.（毕达哥拉斯定理）
- en: Let V be an inner product space, and let x,y ∈V . Then, x and y are orthogonal
    if and only if
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 设 V 为内积空间，x, y ∈ V 。那么，x 和 y 互为正交，当且仅当
- en: ⟨x + y,x + y⟩ = ⟨x,x⟩ + ⟨y,y⟩. (2.5)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ⟨x + y, x + y⟩ = ⟨x, x⟩ + ⟨y, y⟩。 （2.5）
- en: Proof. Given the definition of inner products and orthogonality, the proof is
    straightforward. Due to bilinearity, we have
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。根据内积和正交性的定义，证明过程很直接。由于双线性，我们有
- en: '![⟨x+ y, x+ y ⟩ = ⟨x,x + y⟩ + ⟨y,x + y⟩ = ⟨x,x ⟩+ 2⟨x,y⟩ + ⟨y,y⟩. ](img/file158.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![⟨x + y, x + y⟩ = ⟨x, x + y⟩ + ⟨y, x + y⟩ = ⟨x, x⟩ + 2⟨x, y⟩ + ⟨y, y⟩. ](img/file158.png)'
- en: 'Since x and y are orthogonal, we have ⟨x,y⟩ = 0\. Thus, the equation simplifies
    to:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于x和y正交，我们有⟨x, y⟩ = 0。因此，方程简化为：
- en: '![⟨x + y, x+ y ⟩ = ⟨x,x ⟩+ ⟨y,y⟩. ](img/file159.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![⟨x + y, x + y⟩ = ⟨x, x⟩ + ⟨y, y⟩. ](img/file159.png)'
- en: This completes the proof.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了证明。
- en: Why is this the Pythagorean theorem in another form? Because the norm and the
    inner product is related by ⟨x,x⟩ = ∥x∥², ([11](ch008.xhtml#x1-44003r11)) is equivalent
    to
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这是毕达哥拉斯定理的另一种形式？因为范数和内积通过⟨x, x⟩ = ∥x∥²相联系，（[11](ch008.xhtml#x1-44003r11)）是等价的
- en: '![∥x + y∥2 = ∥x∥2 + ∥y∥2, ](img/file160.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![∥x + y∥² = ∥x∥² + ∥y∥², ](img/file160.png)'
- en: which is exactly the famous “![ 2 2 2 a + b = c ](img/file161.png)”.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是著名的“![ 2 2 2 a + b = c ](img/file161.png)”。
- en: 2.2.3 The geometric interpretation of inner products
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 内积的几何解释
- en: Looking at the general definition, it is hard to get an insight into the inner
    product. However, by using the concept of orthogonality, we can visualize what
    ⟨x,y⟩ represents for any x and y.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 从一般定义来看，很难对内积有直观的理解。然而，通过使用正交性这一概念，我们可以直观地了解⟨x, y⟩对任何x和y的含义。
- en: Intuitively, any x can be decomposed into the sum of two vectors x[o] + x[p],
    where x[o] is orthogonal to y and x[p] is parallel to it.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，任何x都可以分解为两个向量x[o] + x[p]的和，其中x[o]与y正交，而x[p]与y平行。
- en: '![PIC](img/file162.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file162.png)'
- en: 'Figure 2.8: Decomposition of x into components parallel and orthogonal to y'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：x分解为与y平行和正交的分量
- en: Let’s make the intuition precise. How can we find x[p] and x[o]? Since x[p]
    has the same direction as y, it can be written in the form x[p] = cy for some
    scalar c ∈ℝ. Because x[p] and x[o] sum up to x, we also have x[o] = x −x[p] =
    x −cy.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把直觉弄得更精确一些。我们如何找到x[p]和x[o]呢？由于x[p]与y的方向相同，因此可以写成x[p] = cy，其中c是某个标量c ∈ℝ。因为x[p]和x[o]的和等于x，所以我们也有x[o]
    = x − x[p] = x − cy。
- en: Since x[o] is orthogonal to y, the constant c can be determined by solving the
    equation
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 由于x[o]与y正交，因此常数c可以通过解方程来确定。
- en: '![⟨x − cy,y⟩ = 0\. ](img/file163.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![⟨x − cy, y⟩ = 0\. ](img/file163.png)'
- en: By using the bilinearity of the inner product, we can express c from this equation.
    Thus, we have
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用内积的双线性性质，我们可以从这个方程中求解出c。因此，我们得到
- en: '![c = ⟨x,y⟩. ⟨y,y⟩ ](img/file164.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![c = ⟨x, y⟩. ⟨y, y⟩ ](img/file164.png)'
- en: So,
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，
- en: '![xp = ⟨x,y⟩-y, ⟨y,y ⟩ ⟨x,y⟩- xo = x − ⟨y,y⟩y.](img/file165.png) (2.6)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![xp = ⟨x, y⟩-y, ⟨y, y⟩ ⟨x, y⟩- xo = x − ⟨y, y⟩y.](img/file165.png) (2.6)'
- en: We call x[p] the orthogonal projection of x onto y. This is a common transformation,
    so we are going to introduce the notation
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称x[p]为x在y上的正交投影。这是一个常见的变换，因此我们将引入符号
- en: proj[y](x) = ![⟨x,y-⟩ ⟨y,y ⟩](img/file166.png) y. (2.7)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: proj[y](x) = ![⟨x, y⟩ ⟨y, y⟩](img/file166.png) y. (2.7)
- en: From this, we can see that the scaling ratio between y and proj[y](x) can be
    described by inner products.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 从中，我们可以看出y与proj[y](x)之间的比例关系可以通过内积来描述。
- en: So far, we have seen that we can use inner products to define the orthogonality
    relation between two vectors. Can we use it to measure (and, in some cases, even
    define) the angle? The answer is yes! In the following, we are going to see how,
    arriving at the formula (2.1) already familiar from basic geometry.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到可以使用内积来定义两个向量之间的正交关系。那么，我们能否用它来测量（并且在某些情况下，甚至定义）角度呢？答案是肯定的！接下来，我们将看到如何做到这一点，最终得出从基本几何学中已经熟悉的公式（2.1）。
- en: To build our intuition, let’s select two arbitrary n-dimensional vectors x,y
    ∈ℝ^n. The inner product of the sum x + y can be calculated using the bilinearity
    property.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的直觉，让我们选择两个任意的n维向量x，y ∈ℝ^n。x + y的内积可以通过双线性性质来计算。
- en: '![PIC](img/file167.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file167.png)'
- en: 'Figure 2.9: The sum of x and y'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：x与y的和
- en: With this, we obtain
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们得到
- en: '![⟨x + y,x + y⟩ = ∥x+ y∥2 = ∥x∥2 + ∥y∥2 + 2⟨x,y ⟩. ](img/file168.png) (2.8)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![⟨x + y, x + y⟩ = ∥x + y∥² = ∥x∥² + ∥y∥² + 2⟨x, y⟩. ](img/file168.png) (2.8)'
- en: On the other hand, considering that x, y, and x + y form a triangle, we can
    use the law of cosines ( [https://en.wikipedia.org/wiki/Law_of_cosines](https://en.wikipedia.org/wiki/Law_of_cosines))
    to express ⟨x + y,x + y⟩ = ∥x + y∥² in a different form.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，考虑到x、y和x + y形成一个三角形，我们可以使用余弦定律（[https://en.wikipedia.org/wiki/Law_of_cosines](https://en.wikipedia.org/wiki/Law_of_cosines)）将⟨x
    + y, x + y⟩ = ∥x + y∥²以另一种形式表示。
- en: Here, the law of cosines implies
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，余弦定律意味着
- en: ∥x + y∥²  = ∥x∥² + ∥y∥² − 2∥x∥∥y∥ ![c◟os(π◝ −◜-α-)◞](img/file169.png) [=−cos α]
    . (2.9)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ∥x + y∥² = ∥x∥² + ∥y∥² − 2∥x∥∥y∥ ![c◟os(π◝ −◜-α-)◞](img/file169.png) [=−cos
    α] 。(2.9)
- en: By combining (2.8) and (2.9), we get
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合 (2.8) 和 (2.9)，我们得到
- en: '![⟨x,y⟩ = ∥x∥∥y ∥cosα. ](img/file170.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![⟨x,y⟩ = ∥x∥∥y ∥cosα.](img/file170.png)'
- en: That is, in ℝ^n, the angle enclosed by x and y can be extracted by
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，在 ℝ^n 中，x 和 y 之间的夹角可以通过以下方式提取：
- en: α = arccos ![ ⟨x,y⟩ ------- ∥x∥∥y ∥](img/file171.png) . (2.10)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: α = arccos ![ ⟨x,y⟩ ------- ∥x∥∥y ∥](img/file171.png) 。(2.10)
- en: '![PIC](img/file172.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file172.png)'
- en: 'Figure 2.10: The triangle formed by x, y, and x + y'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10：由 x、y 和 x + y 组成的三角形
- en: What about vector spaces where the angle between vectors is not defined? We
    have seen instances of vector spaces (Section [1.1.1](ch007.xhtml#examples-of-vector-spaces))
    where the elements are polynomials, functions, and other mathematical objects.
    There, (2.10) can be used to define the angle!
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在向量空间中没有定义向量之间的角度怎么办？我们已经看到过一些向量空间（第[1.1.1](ch007.xhtml#examples-of-vector-spaces)节），其中元素是多项式、函数以及其他数学对象。在这些情况下，(2.10)可以用来定义角度！
- en: Let’s explore this idea further and see how to use inner products to measure
    similarity.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步探讨这个想法，看看如何使用内积来度量相似性。
- en: Given our geometric interpretation of inner products as orthogonal projections,
    let’s focus on the case when both x and y have unit norms. In this special case,
    the orthogonal projection equals
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 结合我们将内积解释为正交投影的几何理解，让我们关注当 x 和 y 都是单位范数时的情况。在这种特殊情况下，正交投影等于
- en: '![projy(x) = ⟨x,y ⟩y (∥x∥ = ∥y∥ = 1). ](img/file173.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![projy(x) = ⟨x,y ⟩y (∥x∥ = ∥y∥ = 1).](img/file173.png)'
- en: Thus, ⟨x,y⟩ precisely describes the signed magnitude of the orthogonal projection.
    (It can be negative when ![projy(x ) ](img/file174.png) and y have an opposite
    direction.)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，⟨x,y⟩ 精确地描述了正交投影的符号大小。（当![projy(x)](img/file174.png) 和 y 方向相反时，它可能是负数。）
- en: With this in mind, we can see that the inner product is equal to the cosine
    of the angle enclosed by the two vectors. Let’s draw a picture to illustrate!
    (Recall that in right triangles, the cosine is the ratio of the length of the
    adjacent side and the hypotenuse. In this case, the adjacent side has a length
    of ⟨x,y⟩, while the hypotenuse is of unit length.)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，我们可以看到，内积等于两个向量之间夹角的余弦值。让我们画一张图来说明！（回想一下，在直角三角形中，余弦是邻边和斜边长度的比值。在这种情况下，邻边的长度是
    ⟨x,y⟩，而斜边的长度是单位长度。）
- en: '![PIC](img/file175.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file175.png)'
- en: 'Figure 2.11: The inner product of two unit vectors equals the cosine of their
    angle'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11：两个单位向量的内积等于它们夹角的余弦值
- en: In machine learning, this quantity is frequently used to measure the similarity
    of two vectors.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，这个量常用于衡量两个向量的相似性。
- en: Because any vector x can be scaled to unit norm with the transformation x→x∕∥x∥,
    we define the cosine similarity by
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 因为任何向量 x 都可以通过变换 x → x∕∥x∥ 缩放为单位范数，我们定义余弦相似度为
- en: cos(x,y) = ![⟨](img/file177.png) ![‑x‑‑ ∥x∥](img/file178.png) , ![‑y‑‑ ∥y∥](img/file179.png)
    ![⟩](img/file180.png) . (2.11)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: cos(x,y) = ![⟨](img/file177.png) ![‑x‑‑ ∥x∥](img/file178.png) , ![‑y‑‑ ∥y∥](img/file179.png)
    ![⟩](img/file180.png) 。(2.11)
- en: If x and y represent the feature vectors of two data samples, cos(x,y) tells
    us how much the features move together. Note that because of the scaling, two
    samples with a high cosine similarity can be far from each other. So, this reveals
    nothing about their relative positions in the feature space.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 x 和 y 表示两个数据样本的特征向量，cos(x,y) 告诉我们这些特征是如何共同变化的。注意，由于缩放的原因，两个具有较高余弦相似度的样本可能相距很远。因此，这并不能揭示它们在特征空间中的相对位置。
- en: 2.2.4 Orthogonal and orthonormal bases
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 正交与正交归一基
- en: Through the lens of similarity, orthogonality means that one vector does not
    contain “information” about the other. We will make this notion more precise when
    learning about correlation, but there are clear implications regarding the structure
    of inner product spaces. Recall that during the introduction of basis vectors
    (Section [1.2](ch007.xhtml#the-basis)), our motivation was to find a minimal set
    of vectors that can be used to express any other vector. With the introduction
    of orthogonality, we can go one step further.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 从相似性的角度来看，正交意味着一个向量不包含另一个向量的“信息”。当我们学习相关性时，我们将更加精确地定义这个概念，但它对于内积空间的结构有着明确的影响。回想一下，在介绍基向量时（第[1.2](ch007.xhtml#the-basis)节），我们的动机是找到一个最小的向量集合，可以用来表示其他任何向量。通过引入正交性，我们可以更进一步。
- en: Definition 11\. (Orthogonal and orthonormal bases)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 11.（正交与正交归一基）
- en: Let ![V ](img/file181.png) be a vector space and S = {v1,...,vn } its basis.
    We say that ![S ](img/file183.png) is an orthogonal basis if ⟨vi,vj⟩ = 0 whenever
    ![i ⁄= j ](img/file185.png).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 设 ![V ](img/file181.png) 是一个向量空间，S = {v1,...,vn } 是其基。我们说 ![S ](img/file183.png)
    是正交基，如果 ⟨vi,vj⟩ = 0 当且仅当 ![i ⁄= j ](img/file185.png)。
- en: Moreover, ![S ](img/file186.png) is called orthonormal if
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此外， ![S ](img/file186.png) 被称为正交归一，如果
- en: '![ ( |{ 1, if i = j, ⟨vi,vj⟩ = |( 0, if i ⁄= j. ](img/file187.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![ ( |{ 1, 如果 i = j, ⟨vi,vj⟩ = |( 0, 如果 i ⁄= j. ](img/file187.png)'
- en: In other words, ![S ](img/file188.png) is orthonormal if, in addition to being
    orthogonal, each vector has unit norm.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说， ![S ](img/file188.png) 是正交归一的，如果，除了是正交的外，每个向量都有单位范数。
- en: Orthogonal and orthonormal bases are extremely convenient to use. If a basis
    is orthogonal, we can easily obtain an orthonormal basis by simply scaling its
    vectors to unit norm. Thus, we’ll use orthonormal basis vectors most of the time.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 正交和正交归一基非常方便使用。如果一个基是正交的，我们可以通过简单地将其向量缩放到单位范数，轻松获得一个正交归一基。因此，我们大多数时候会使用正交归一基向量。
- en: Why do we love orthonormal bases so much? To see this, let {v[1],…,v[n]} be
    an arbitrary basis and let ![x ](img/file189.png) be an arbitrary vector. We know
    that
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们如此喜欢正交归一基？为了看到这一点，设 {v[1],…,v[n]} 是任意基，且 ![x ](img/file189.png) 是任意向量。我们知道
- en: '![ ∑n x = xivi, i=1 ](img/file190.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n x = xivi, i=1 ](img/file190.png)'
- en: but how do we find the coefficients x[i]? There is a general method involving
    linear equations that we will see later in Chapter [6](ch011.xhtml#matrices-and-equations)
    , but if {v[i]}[i=1]^n is orthonormal, the situation is much simpler.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们如何找到系数 x[i] 呢？有一种涉及线性方程的一般方法，我们将在第[6](ch011.xhtml#matrices-and-equations)章中看到，不过如果
    {v[i]}[i=1]^n 是正交归一的，情况就简单多了。
- en: This is made more precise in the following theorem.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点在以下定理中得到了更精确的阐述。
- en: Theorem 12\.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 12\。
- en: Let ![V ](img/file191.png) be a vector space and ![S = {v1,...,vn } ](img/file192.png)be
    an orthonormal basis of ![V ](img/file193.png). Then, for any ![x ∈ V ](img/file194.png),
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 设 ![V ](img/file191.png) 是一个向量空间，且 ![S = {v1,...,vn } ](img/file192.png) 是 ![V
    ](img/file193.png) 的正交归一基。那么，对于任意的 ![x ∈ V ](img/file194.png)，
- en: x = ∑ [i=1] ^n ⟨x,v[i]⟩ v[i] (2.12)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: x = ∑ [i=1] ^n ⟨x,v[i]⟩ v[i] (2.12)
- en: holds.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 保持成立。
- en: Proof. Since ![v1,...,vn ](img/file195.png) form a basis, we can express ![x
    ](img/file196.png) as
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。由于 ![v1,...,vn ](img/file195.png) 形成一个基，我们可以将 ![x ](img/file196.png) 表示为
- en: '![ ∑n x = xivi i=1 ](img/file197.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n x = xivi i=1 ](img/file197.png)'
- en: for some scalars ![xi ](img/file198.png). By the linearity of the inner product,
    we obtain
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对某些标量 ![xi ](img/file198.png)。通过内积的线性性，我们得到
- en: '![ ∑n ∑n ⟨x,vj⟩ = ⟨ xivi,vj⟩ = xi⟨vi,vj⟩. i=1 i=1 ](img/file199.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n ∑n ⟨x,vj⟩ = ⟨ xivi,vj⟩ = xi⟨vi,vj⟩. i=1 i=1 ](img/file199.png)'
- en: Since ![v ,...,v 1 n ](img/file200.png) form an orthonormal basis, we have
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ![v ,...,v 1 n ](img/file200.png) 形成一个正交归一基，我们有
- en: '![ (| {1, if i = j, ⟨vi,vj⟩ = | (0, if i ⁄= j. ](img/file201.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![ (| {1, 如果 i = j, ⟨vi,vj⟩ = | (0, 如果 i ⁄= j. ](img/file201.png)'
- en: Thus, the sum reduces to
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，求和简化为
- en: '![⟨x,vj ⟩ = xj. ](img/file202.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![⟨x,vj ⟩ = xj. ](img/file202.png)'
- en: This proves the result.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了这个结果。
- en: Thus, the coefficients can be calculated by taking the inner product. In other
    words, for orthonormal bases, x[j] depends only on the j-th basis vector.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，系数可以通过内积计算得到。换句话说，对于正交归一基，x[j] 仅依赖于第 j 个基向量。
- en: As another consequence of the orthonormality, calculating the norm is also easier,
    as we can always express it in terms of the coefficients. To be more precise,
    we have
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 由于正交归一性的另一个结果，计算范数也变得更加容易，因为我们总是可以用系数来表示它。更准确地说，我们有
- en: '![2 ∥x∥ = ⟨x,x⟩ ∑n ∑n = ⟨ xivi, xjvj ⟩ i=1 j=1 ∑n ∑n = xixj⟨vi,vj⟩ i=1j=1 n
    = ∑ x2\. i i=1](img/file203.png) (2.13)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![2 ∥x∥ = ⟨x,x⟩ ∑n ∑n = ⟨ xivi, xjvj ⟩ i=1 j=1 ∑n ∑n = xixj⟨vi,vj⟩ i=1j=1 n
    = ∑ x2\. i i=1](img/file203.png) (2.13)'
- en: This is called Parseval’s identity. In other words, if x is given in terms of
    an orthonormal basis, its norm is easy to find. It is not a coincidence that this
    formula resembles the Euclidean norm so much! (Note that, here, ∥⋅∥ is a general
    norm.) In fact, the squared Euclidean norm
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为帕尔塞瓦尔恒等式。换句话说，如果 x 是用正交归一基表示的，那么它的范数很容易找到。这个公式与欧几里得范数如此相似并非偶然！(请注意，这里 ∥⋅∥
    是一个一般范数。) 事实上，平方的欧几里得范数
- en: '![ n ∥x ∥2= ∑ x2, x = (x ,...,x ) ∈ ℝn 2 i 1 n i=1 ](img/file204.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![ n ∥x ∥2= ∑ x2, x = (x ,...,x ) ∈ ℝn 2 i 1 n i=1 ](img/file204.png)'
- en: is just (2.13) using the standard basis.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是使用标准基的 (2.13)。
- en: 2.2.5 The Gram-Schmidt orthogonalization process
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.5 格拉姆-施密特正交化过程
- en: Orthogonal bases are awesome and all, but how do we find them?
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 正交基很棒，但我们如何找到它们呢？
- en: There is a general method called the Gram-Schmidt orthogonalization process
    that solves this problem. The algorithm takes any set of basis vectors {v[1],…,v[n]}
    and outputs an orthonormal basis {e[1],…,e[n]}
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个通用的方法叫做 Gram-Schmidt 正交化过程，它解决了这个问题。该算法接受任何一组基向量 {v[1],…,v[n]}，并输出一个正交规范基
    {e[1],…,e[n]}
- en: such that
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 使得
- en: '![span(v1,...,vk) = span (e1,...,ek), k = 1,...,n, ](img/file205.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![span(v1,...,vk) = span (e1,...,ek), k = 1,...,n, ](img/file205.png)'
- en: that is, the subspaces generated by the first k vectors of both sets match.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，两组中前 k 个向量生成的子空间相匹配。
- en: How do we do that? The process is straightforward. Let’s focus on finding an
    orthogonal system first, which we can normalize later to achieve orthonormality.
    We are going to build our set {e[1],…,e[n]} iteratively. It is clear that
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们怎么做呢？这个过程很简单。让我们首先集中精力找到一个正交系统，我们可以稍后将其归一化以达到正交规范化。我们将迭代地构建我们的集合 {e[1],…,e[n]}。很明显，
- en: '![e1 := v1 ](img/file206.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![e1 := v1 ](img/file206.png)'
- en: is a good choice. Now, our goal is to find e[2] such that e[2] ⊥e[1] and, together,
    they span the same subspace as {v[1],v[2]}. Remember when we talked about the
    geometric interpretation of orthogonality in Section [2.2.3](ch008.xhtml#the-geometric-interpretation-of-inner-products)?
    The orthogonal component of v[2] with respect to e[1] will be a good choice for
    e[2]. Thus, let
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个不错的选择。现在，我们的目标是找到 e[2]，使得 e[2] ⊥ e[1]，并且它们共同生成与 {v[1],v[2]} 相同的子空间。还记得我们在第
    [2.2.3](ch008.xhtml#the-geometric-interpretation-of-inner-products) 节讨论过的正交性的几何解释吗？v[2]
    相对于 e[1] 的正交分量将是 e[2] 的一个好选择。因此，设
- en: '![ ⟨v2,e1⟩ e2 := v2 − proje1(v2) = v2 −-------e1\. ⟨e1,e1⟩ ](img/file207.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![ ⟨v2,e1⟩ e2 := v2 − proje1(v2) = v2 −-------e1\. ⟨e1,e1⟩ ](img/file207.png)'
- en: From the definition, it is clear that e[2] ⊥e[1], and it is also clear that
    {e[1],e[2]} spans the same subspace as {v[1],v[2]}.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，显然有 e[2] ⊥ e[1]，同时也可以看出 {e[1],e[2]} 生成的子空间与 {v[1],v[2]} 相同。
- en: In the next step, we perform the same process. We project v[3] onto the subspace
    generated by e[1] and e[2], then define e[3] as the difference of v[3] and the
    projection. That is,
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们执行相同的过程。我们将 v[3] 投影到由 e[1] 和 e[2] 生成的子空间上，然后将 e[3] 定义为 v[3] 和投影的差异。即，
- en: '![e3 := v3 − proje1,e2(v3 ) = v3 − ⟨v3,e1⟩e1 − ⟨v3,e2⟩e2\. ⟨e1,e1⟩ ⟨e2,e2⟩
    ](img/file208.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![e3 := v3 − proje1,e2(v3 ) = v3 − ⟨v3,e1⟩e1 − ⟨v3,e2⟩e2\. ⟨e1,e1⟩ ⟨e2,e2⟩
    ](img/file208.png)'
- en: With this, we essentially remove the “contributions” of e[1] and e[2] toward
    v[3], thus obtaining an e[3] that is orthogonal to the previous ones.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们本质上去除了 e[1] 和 e[2] 对 v[3] 的“贡献”，从而得到一个与之前的向量正交的 e[3]。
- en: In general, if we have e[1],…,e[k], the vector e[k+1] can be found by
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，如果我们已经有 e[1],…,e[k]，那么可以通过以下方式找到向量 e[k+1]：
- en: '![ek+1 := vk+1 − proj (vk+1), e1,...,ek ](img/file209.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![ek+1 := vk+1 − proj (vk+1), e1,...,ek ](img/file209.png)'
- en: where
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![∑k ⟨x, ei⟩ proje1,...,ek(x) = -------ei i=1⟨ei,ei⟩](img/file210.png) (2.14)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![∑k ⟨x, ei⟩ proje1,...,ek(x) = -------ei i=1⟨ei,ei⟩](img/file210.png) (2.14)'
- en: is the generalized orthogonal projection operator, projecting a vector to the
    subspace generated by {e[1],…,e[k]}.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 是广义正交投影算子，将一个向量投影到由 {e[1],…,e[k]} 生成的子空间上。
- en: To check that e[k+1] ⊥e[1],…,e[k] , we have
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证 e[k+1] ⊥ e[1],…,e[k]，我们有
- en: '![∑ k ⟨vk+1,ei⟩ ⟨ek+1,ej⟩ = ⟨vk+1 − ⟨ei,ei⟩ ei,ej⟩ i=1 ∑k ⟨vk+1,ei⟩ = ⟨vk+1,ej⟩−
    ---------⟨ei,ej⟩ i=1 ⟨ei,ei⟩ = ⟨v ,e ⟩− ⟨v ,e ⟩ k+1 j k+1 j = 0.](img/file211.png)
    (2.15)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![∑ k ⟨vk+1,ei⟩ ⟨ek+1,ej⟩ = ⟨vk+1 − ⟨ei,ei⟩ ei,ej⟩ i=1 ∑k ⟨vk+1,ei⟩ = ⟨vk+1,ej⟩−
    ---------⟨ei,ej⟩ i=1 ⟨ei,ei⟩ = ⟨v ,e ⟩− ⟨v ,e ⟩ k+1 j k+1 j = 0.](img/file211.png)
    (2.15)'
- en: due to the orthogonality of the e[i]-s and the linearity of the inner product.
    Since {e[1],…,e[k]} spans the same subspace as {v[1],…,v[k]} and e{k + 1} is a
    linear combination of v[k+1] and e[1],…,e[k] (where the coefficient of v[k+1]
    is nonzero),
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 e[i] 的正交性和内积的线性性质。由于 {e[1],…,e[k]} 和 {v[1],…,v[k]} span 同一子空间，并且 e{k + 1}
    是 v[k+1] 和 e[1],…,e[k] 的线性组合（其中 v[k+1] 的系数非零），
- en: '![span(v ,...,v ) = span (e ,...,e ) 1 k+1 1 k+1 ](img/file212.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![span(v ,...,v ) = span (e ,...,e ) 1 k+1 1 k+1 ](img/file212.png)'
- en: also follows. This can be repeated until we run out of vectors and find {e[1],…,e[n]}.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以推导出来。这可以重复进行，直到我们用完所有向量并找到 {e[1],…,e[n]}。
- en: For the sake of further reference, mathematical correctness, and a tiny bit
    of perfectionism, let’s summarize all the above in a single theorem.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步参考、数学正确性，以及一点点完美主义，我们将上述所有内容总结成一个定理。
- en: Theorem 13\. (Gram-Schmidt orthogonalization process)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 13.（Gram-Schmidt 正交化过程）
- en: Let V be an inner product vector space and {v[1],…,v[n]} ⊆ V be a set of linearly
    independent vectors. Then, there exists an orthonormal set {e[1],…,e[n]}⊆V such
    that
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 设 V 为一个内积向量空间，{v[1],…,v[n]} ⊆ V 是一组线性无关的向量。那么，存在一个正交规范集 {e[1],…,e[n]} ⊆ V，使得
- en: '![span(e1,...,ek) = span (v1,...,vk) ](img/file213.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![span(e1,...,ek) = span(v1,...,vk)](img/file213.png)'
- en: holds for any k = 1,…,n.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 k = 1,…,n，公式成立。
- en: As a consequence, we can state that each finite inner product space has an orthonormal
    basis. We can even construct it explicitly via the Gram-Schmidt process.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以断言，每个有限维的内积空间都有一个正交归一化基。我们甚至可以通过 Gram-Schmidt 过程显式构造它。
- en: Corollary 1\. (Existence of orthonormal bases)
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 推论 1.（正交归一化基的存在）
- en: Let V be a finite-dimensional inner product space. Then, there exists an orthonormal
    basis in V .
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 设 V 为有限维内积空间。那么，V 中存在一个正交归一化基。
- en: Going one step further, we can view Theorem [13](ch008.xhtml#x1-47004r13) and
    its proof as an algorithm.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步，我们可以将定理[13](ch008.xhtml#x1-47004r13)及其证明视为一个算法。
- en: '(Gram-Schmidt orthogonalization process) Inputs: A set of linearly independent
    vectors {v[1],…,v[n]}⊆V . Output: A set of orthonormal vectors {e[1],…,e[n]} such
    that![span(e1,...,ek) = span (v1, ...,vk) ](img/file214.png)holds for any k =
    1,…,n.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: （Gram-Schmidt正交化过程）输入：一组线性无关的向量 {v[1],…,v[n]}⊆V。输出：一组正交归一化的向量 {e[1],…,e[n]}，使得![span(e1,...,ek)
    = span(v1,...,vk)](img/file214.png) 对于任何 k = 1,…,n 均成立。
- en: Remark 3\. (Linearly dependent inputs in the Gram-Schmidt process) What happens
    if we apply the Gram-Schmidt orthogonalization process to a set of linearly *dependent*
    vectors?
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 注释 3.（Gram-Schmidt 过程中的线性相关输入）如果我们将 Gram-Schmidt 正交化过程应用于一组线性*相关*的向量，会发生什么？
- en: 'To get a grip on the problem, let’s consider a simple case of two vectors:
    {v[1],v[2] = cv[1]}, where c is an arbitrary scalar. e[1] is chosen to be v[1],
    and e[2] is defined by'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个问题，让我们考虑一个简单的情况：两个向量 {v[1],v[2] = cv[1]}，其中 c 是一个任意的标量。e[1] 被选择为 v[1]，而
    e[2] 定义为
- en: '![e = v − proj (v ). 2 2 e1 2 ](img/file215.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![e = v − proj (v ). 2 2 e1 2 ](img/file215.png)'
- en: By expanding the projection term, we obtain
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 通过展开投影项，我们得到
- en: '![e2 = v2 − ⟨v2,e1⟩e1\. ⟨e1,e1⟩ ](img/file216.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![e2 = v2 − ⟨v2,e1⟩e1\. ⟨e1,e1⟩ ](img/file216.png)'
- en: Since v[1] = e[1] and v[2] = cv[1], we get that
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 v[1] = e[1] 且 v[2] = cv[1]，我们得到
- en: e[2] = cv[1] −![c⟨v1,v1-⟩ ⟨v1,v1⟩](img/file217.png)v[1] = cv[1] −cv[1]= 0
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: e[2] = cv[1] −![c⟨v1,v1-⟩ ⟨v1,v1⟩](img/file217.png)v[1] = cv[1] −cv[1]= 0
- en: 'This result generalizes: when the Gram-Schmidt process encounters an input
    vector that is linearly dependent from the previous ones, a zero vector is produced
    in the output.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果可以推广：当 Gram-Schmidt 过程遇到与之前的向量线性相关的输入向量时，输出会生成一个零向量。
- en: 2.2.6 The orthogonal complement
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.6 正交补
- en: Earlier, we saw that given a fixed vector y ∈V , we can decompose any x ∈V as
    x = x[o] + x[p], where x[o] is orthogonal to y, while x[p] is parallel to it.
    (We used this to provide a geometric motivation for inner products in Section [2.2.3](ch008.xhtml#the-geometric-interpretation-of-inner-products).)
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们看到，给定一个固定向量 y ∈V，我们可以将任何 x ∈V 分解为 x = x[o] + x[p]，其中 x[o] 与 y 正交，而 x[p]
    与 y 平行。（我们用这个来提供内积几何解释的动机，见[2.2.3节](ch008.xhtml#the-geometric-interpretation-of-inner-products)。）
- en: This is an essential tool, and in this section, we will see that an analogue
    of this decomposition still holds true when y is replaced with an arbitrary subspace
    S ⊂V . To see this, let’s talk about the orthogonality of subspaces. (If you want
    to recall the definition of subspaces, check out Definition [5](ch007.xhtml#x1-29003r5).)
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个重要工具，在这一节中，我们将看到当 y 被任意子空间 S ⊂V 替代时，这一分解的类似物仍然成立。为了理解这一点，让我们讨论子空间的正交性。（如果你想回顾子空间的定义，查看定义[5](ch007.xhtml#x1-29003r5)。）
- en: Definition 12\. (Orthogonal subspaces)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 12.（正交子空间）
- en: Let V be an arbitrary inner product space. We say that the subspaces S[1],S[2]
    ⊆V are orthogonal if, for every pair of vectors x ∈S[1] and y ∈S[2], we have ⟨x,y⟩
    = 0\. This is denoted as S[1] ⊥S[2].
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 设 V 为任意的内积空间。我们说子空间 S[1] 和 S[2] ⊆V 是正交的，当且仅当对于每一对向量 x ∈S[1] 和 y ∈S[2]，有 ⟨x,y⟩
    = 0。记作 S[1] ⊥S[2]。
- en: For example, the x-axis and the y-axis are orthogonal subspaces in ℝ². (Just
    as the xy plane and the z-axis in ℝ³.)
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，x 轴和 y 轴是 ℝ² 中的正交子空间。（就像在 ℝ³ 中，xy 平面与 z 轴正交一样。）
- en: 'Similarly, we can talk about the orthogonality of a vector and a subspace:
    x is orthogonal to the subspace S, or x ⊥S in symbols, if x is orthogonal to all
    vectors of S.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以讨论一个向量与子空间的正交性：如果 x 正交于 S 中的所有向量，则称 x 与子空间 S 正交，符号为 x ⊥S。
- en: One of the most straightforward and essential ways to construct orthogonal subspaces
    is to take the orthogonal complement.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 构造正交子空间最直接且重要的方法之一是取正交补。
- en: Definition 13\. (Orthogonal complement)
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 13.（正交补）
- en: Let V be an arbitrary inner product space and let S ⊆ V be a subspace. The set
    defined by
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 设 V 为一个任意的内积空间，S ⊆ V 为子空间。定义的集合为
- en: '![S⊥ := {x ∈ V | x ⊥ S }](img/file218.png) (2.16)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![S⊥ := {x ∈ V | x ⊥ S }](img/file218.png) (2.16)'
- en: is called the orthogonal complement of S.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 称为 S 的正交补。
- en: S^⊥ is not just any set; it is a subspace, as we are about to see.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: S^⊥ 不仅仅是一个集合，它是一个子空间，正如我们即将看到的那样。
- en: Theorem 14\.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 14\。
- en: Let ![V ](img/file219.png) be an arbitrary inner product space and ![S ⊆ V ](img/file220.png)
    one of its subspaces. Then
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 设 ![V ](img/file219.png) 为任意内积空间，![
- en: '![S ⊥ ](img/file221.png) is orthogonal to ![S ](img/file222.png),'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![S ⊥ ](img/file221.png) 与 ![S ](img/file222.png) 正交，'
- en: '![S ⊥ ](img/file223.png) is a subspace of ![V ](img/file224.png),'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![S ⊥ ](img/file223.png) 是 ![V ](img/file224.png) 的一个子空间，'
- en: and ![ ⊥ S ∩ S = {0} ](img/file225.png).
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 且 ![ ⊥ S ∩ S = {0} ](img/file225.png)。
- en: 'Proof. According to the definition of subspaces, we only have to show that
    S^⊥ is closed with respect to addition and scalar multiplication. As the inner
    product is bilinear, this is straightforward:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：根据子空间的定义，我们只需证明 S^⊥ 在加法和标量乘法下是封闭的。由于内积是双线性的，这一点很容易证明：
- en: '![⟨ax + by,z⟩ = a⟨x,z⟩+ b⟨y,z⟩ = 0 ](img/file226.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![⟨ax + by,z⟩ = a⟨x,z⟩+ b⟨y,z⟩ = 0 ](img/file226.png)'
- en: holds for any vectors x,y ∈S^⊥, z ∈S, and scalars a,b.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对任意向量 x,y ∈S^⊥，z ∈S 和标量 a,b，成立。
- en: To see that S ∩S^⊥ = {0}, let’s take an arbitrary x ∈S ∩S^⊥. By the definition
    of S^⊥, we have ⟨x,x⟩ = 0\. As the inner product is positive definite per definition,
    x must be the zero vector 0.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明 S ∩ S^⊥ = {0}，我们取任意 x ∈ S ∩ S^⊥。根据 S^⊥ 的定义，我们有 ⟨x,x⟩ = 0。由于内积按定义是正定的，x
    必须是零向量 0。
- en: Recall the decomposition of any x ∈V into a parallel and an orthogonal component
    with respect to a fixed vector y? In terms of subspaces, we can restate this as
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，任意 x ∈V 如何相对于固定向量 y 被分解为平行分量和正交分量？从子空间的角度来看，我们可以将其重新表述为
- en: '![ ⊥ V = span(y) + span (y) , ](img/file227.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![ ⊥ V = span(y) + span (y) , ](img/file227.png)'
- en: that is, V can be written as the direct sum of the vector space spanned by y,
    and its orthogonal complement. This is an extremely powerful result, as this allows
    us to decouple x from y. For instance, if we think of vectors as a collection
    of features (just like the sepal and petal width and length measurements in our
    favorite Iris dataset), y can represent a certain trait that we want to exclude
    from our analysis.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，V 可以写成由 y 张成的向量空间与其正交补的直和形式。这是一个非常强大的结果，因为它使我们能够将 x 从 y 中解耦。例如，如果我们将向量看作特征的集合（就像我们最喜爱的鸢尾花数据集中的花萼和花瓣宽度与长度测量一样），y
    可以表示我们希望从分析中排除的某个特征。
- en: With the notion of orthogonal complements, we can make this mathematically precise.
    We can also be more general. In fact, the decomposition
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 利用正交补的概念，我们可以使这个命题在数学上更加精确。我们还可以更加一般化。事实上，这个分解
- en: '![V = S + S⊥ ](img/file228.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![V = S + S⊥ ](img/file228.png)'
- en: holds for any subspace S! Let’s see the proof!
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 对任意子空间 S 成立！让我们来看一下证明！
- en: Theorem 15\.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 15\。
- en: Let V be an arbitrary finite-dimensional inner product space and S ⊂V its subspace.
    Then,
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 设 V 为任意有限维内积空间，S ⊂V 为其子空间。那么，
- en: '![V = S + S⊥ ](img/file229.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![V = S + S⊥ ](img/file229.png)'
- en: holds.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 成立。
- en: Proof. Let v[1],…,v[k] ∈S be a basis of S. As during the Gram-Schmidt process,
    we can define the generalized orthogonal projection ([3.2.5](ch008.xhtml#the-gramschmidt-orthogonalization-process)),
    given by
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：设 v[1],…,v[k] ∈S 是 S 的一组基。像在 Gram-Schmidt 过程中的做法一样，我们可以定义广义的正交投影（[3.2.5](ch008.xhtml#the-gramschmidt-orthogonalization-process)），其形式为
- en: '![ ∑k proj (x ) = ⟨x,vi⟩vi. v1,...,vk i=1 ](img/file230.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑k proj (x ) = ⟨x,vi⟩vi. v1,...,vk i=1 ](img/file230.png)'
- en: Using this, we can decompose any x ∈V as
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这个，我们可以将任意 x ∈V 分解为
- en: '![x = (x − proj_{v1,...,vk}(x)) + proj_{v1,...,vk}(x)](img/file231.png) (2.17)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![x = (x − proj_{v1,...,vk}(x)) + proj_{v1,...,vk}(x)](img/file231.png) (2.17)'
- en: Since proj[v[1],…,v[k]](x) is the linear combination of v[i]-s, it belongs to
    S. On the other hand, the bilinearity of the inner product gives that x − proj[v[1],…,v[k]](x)
    ∈S^⊥. Indeed, as we have
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 proj[v[1],…,v[k]](x) 是 v[i] 的线性组合，它属于 S。另一方面，内积的双线性性质使得 x − proj[v[1],…,v[k]](x)
    ∈S^⊥。事实上，正如我们所见
- en: '![ ∑k proj (x ) = ⟨x,vi⟩vi. v1,...,vk i=1 ](img/equation.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑k proj (x ) = ⟨x,vi⟩vi. v1,...,vk i=1 ](img/equation.png)'
- en: the vector x − proj[v[1],…,v[k]](x) is orthogonal to each v[j]. Thus, since
    v[1],…,v[k] is a basis of S, x is also orthogonal to S, hence x − proj[e[1],…,v[k]](x)
    ∈S^⊥.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 x − proj[v[1],…,v[k]](x) 与每个 v[j] 正交。因此，由于 v[1],…,v[k] 是 S 的一组基，x 也正交于 S，因此
    x − proj[e[1],…,v[k]](x) ∈S^⊥。
- en: The fact that every x ∈V can be decomposed as the sum of a vector from S and
    a vector from S^⊥, as given by ([3.2.6](ch008.xhtml)), means that V = S + S^⊥,
    which is what we had to prove.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 x ∈V 都可以分解为 S 中一个向量与 S^⊥ 中一个向量的和，正如 ([3.2.6](ch008.xhtml)) 所给出的，这意味着 V =
    S + S^⊥，这是我们要证明的。
- en: 2.3 Summary
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 摘要
- en: In this chapter, we have learned that, besides the algebraic structure given
    by addition and scalar multiplication, vectors have a beautiful geometry that
    rises from the inner product. From the inner product, we have norms; from norms,
    we have metrics; and from metrics, we have geometry and topology.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学到了，除了由加法和标量乘法给出的代数结构，向量还具有从内积中产生的美丽几何。通过内积，我们得到了范数；通过范数，我们得到了度量；通过度量，我们得到了几何和拓扑。
- en: Distance, similarity, angles, and orthogonality all arise from the simple concept
    of inner products. These are all extremely useful in both theory and practice.
    For instance, inner products give us a way to quantify the similarity of two vectors
    via the so-called cosine similarity, but they also provide a means to find optimal
    bases through the notion of orthogonality.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 距离、相似性、角度和正交性都来源于内积这一简单概念。这些在理论和实践中都非常有用。例如，内积为我们提供了通过所谓的余弦相似性来量化两个向量相似度的方法，但它们也为我们提供了通过正交性的概念来寻找最优基的方法。
- en: To summarize, we’ve learned what norms and distances are, the definition of
    the inner product, how inner products give angles and norms, and why all of these
    are useful in machine learning.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们已经学习了什么是范数和距离，内积的定义，内积如何给出角度和范数，以及这些在机器学习中的应用为何如此重要。
- en: 'Besides the basic definitions and properties, we’ve encountered our very first
    algorithm: the Gram-Schmidt process, turning a set of vectors into an orthonormal
    basis. This is the best kind of basis.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基本的定义和性质，我们还遇到了第一个算法：格拉姆-施密特过程，将一组向量转化为正交标准基。这是最好的基。
- en: In the next section, we’ll take all that theory and put it into practice, taking
    our first steps in computational linear algebra. Let’s go!
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把所有这些理论付诸实践，迈出计算线性代数的第一步。我们开始吧！
- en: 2.4 Problems
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 问题
- en: 'Problem 1\. Let V be a vector space and define the function d : V ×V → [0,∞)
    by'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '问题 1\. 设 V 为一个向量空间，定义函数 d : V ×V → [0,∞) 为：'
- en: '![ ( |{ 0 if x = y, d(x,y) = |( 1 otherwise. ](img/file234.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![ ( |{ 0 如果 x = y, d(x,y) = |( 1 否则。 ](img/file234.png)'
- en: (a) Show that d is a metric (see Definition [8](ch008.xhtml#x1-41003r8)).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 证明 d 是一个度量（见定义 [8](ch008.xhtml#x1-41003r8)）。
- en: (b) Show that d cannot come from a norm.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 证明 d 无法来自于范数。
- en: Problem 2\. Let S[n] be the set of all ASCII strings of n character length and
    define the Hamming distance h(x,y) for any two x,y ∈S[n] by the number of corresponding
    positions where x and y are different.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 2\. 设 S[n] 为所有长度为 n 的 ASCII 字符串集合，并定义任意两个 x,y ∈S[n] 的汉明距离 h(x,y) 为 x 和 y
    在相应位置不同的个数。
- en: For instance,
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，
- en: '![h(”001101”,”101110”) = 2, h(”metal”,”petal”) = 1\. ](img/file235.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![h(”001101”,”101110”) = 2, h(”metal”,”petal”) = 1\. ](img/file235.png)'
- en: Show that h satisfies the three defining properties of a metric. (Note that
    S[n] is not a vector space so, technically, the Hamming distance is not a metric.)
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 证明 h 满足度量的三个定义性质。（注意，S[n] 不是一个向量空间，因此从严格意义上说，汉明距离不是度量。）
- en: 'Problem 3\. Let ∥⋅∥ be a norm on the vector space ℝ^n, and define the mapping
    f : ℝ^n →ℝ^n,'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '问题 3\. 设 ∥⋅∥ 为向量空间 ℝ^n 上的一个范数，定义映射 f : ℝ^n →ℝ^n，'
- en: '![f : (x1,x2,...,xn ) ↦→ (x1,2x2,...,nxn ). ](img/file236.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![f : (x1,x2,...,xn ) ↦→ (x1,2x2,...,nxn ). ](img/file236.png)'
- en: Show that
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：
- en: '![∥x ∥∗ := ∥f(x)∥ ](img/file237.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![∥x ∥∗ := ∥f(x)∥ ](img/file237.png)'
- en: is a norm on ℝ^n.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 是 ℝ^n 上的一个范数。
- en: Problem 4\. Let a[1],…,a[n]/span>0 be arbitrary positive numbers. Show that
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 4\. 设 a[1],…,a[n] > 0 为任意正数。证明：
- en: '![ n ∑ n ⟨x,y ⟩ := aixiyi, x,y ∈ ℝ . i=1 ](img/file238.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![ n ∑ n ⟨x,y ⟩ := aixiyi, x,y ∈ ℝ . i=1 ](img/file238.png)'
- en: is an inner product, where x = (x[1],…,x[n]) and y = (y[1],…,y[n]).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个内积，其中 x = (x[1],…,x[n]) 和 y = (y[1],…,y[n])。
- en: Problem 5\. Let V be a finite-dimensional inner product space, let v[1],…,v[n]
    ∈V be a basis in V , and define
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 5\. 设 V 为有限维内积空间，v[1],…,v[n] ∈V 为 V 中的一个基，定义：
- en: '![ai,j := ⟨vi,vj⟩. ](img/file239.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![ai,j := ⟨vi,vj⟩. ](img/file239.png)'
- en: Show that for any x,y ∈V ,
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 证明对于任意 x,y ∈V，
- en: '![ ∑n ⟨x,y ⟩ = ai,jxiyj, i,j=1 ](img/file240.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n ⟨x,y ⟩ = ai,jxiyj, i,j=1 ](img/file240.png)'
- en: where x = ∑ [i=1]^nx[i]v[i] and y = ∑ [i=1]^ny[i]v[i].
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 x = ∑ [i=1]^n x[i]v[i]，y = ∑ [i=1]^n y[i]v[i]。
- en: Problem 6\. Let V be a finite-dimensional real inner product space.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 6\. 设 V 为有限维实内积空间。
- en: (a) Let y ∈V be an arbitrary vector. Show that
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 设 y ∈V 为任意向量。证明：
- en: '![f : V → ℝ, x ↦→ ⟨x,y⟩ ](img/file241.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![f : V → ℝ, x ↦→ ⟨x,y⟩ ](img/file241.png)'
- en: is a linear function (that is, f(αu + βv) = αf(u) + βf(v) holds for all u,v
    ∈V and α,β ∈ℝ).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个线性函数（即，对于所有 u,v ∈V 和 α,β ∈ℝ，满足 f(αu + βv) = αf(u) + βf(v)）。
- en: '(b) Let f : V →ℝ an arbitrary linear function. Show that there exists a y ∈V
    such that'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '(b) 设 f : V →ℝ 为任意线性函数。证明存在一个 y ∈V，使得：'
- en: '![f(x) = ⟨x,y⟩. ](img/file242.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![f(x) = ⟨x,y⟩. ](img/file242.png)'
- en: (Note that (b) is the reverse of (a), and a much more interesting result.)
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: （注意，(b) 是 (a) 的反向，是一个更有趣的结果。）
- en: Problem 7\. Let V be a real inner product space and let ∥x∥ = ![∘ ------ ⟨x,x
    ⟩](img/file243.png) be the generated norm. Show that
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 7. 设 V 为实数内积空间，且 ∥x∥ = ![∘ ------ ⟨x,x ⟩](img/file243.png) 为生成的范数。证明
- en: '![Equation 2.16](img/file244.png) (2.18)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![方程 2.16](img/file244.png) (2.18)'
- en: This is called the parallelogram law, because if we think of x and y as the
    two sides determining a parallelogram, ([3.4](ch008.xhtml#problems1)) relates
    the length of its sides to the length of its diagonals.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为平行四边形法则，因为如果我们将 x 和 y 看作确定平行四边形的两条边，([3.4](ch008.xhtml#problems1)) 将平行四边形两条边的长度与对角线的长度联系起来。
- en: Problem 8\. Let V be a real inner product space and let u,v ∈V . Show that if
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 8. 设 V 为实数内积空间，u,v ∈V 。证明若
- en: '![⟨u, x⟩ = ⟨v, x⟩ ](img/file245.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![⟨u, x⟩ = ⟨v, x⟩ ](img/file245.png)'
- en: holds for all x ∈V , then u = v.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有的 x ∈V，若成立，则 u = v。
- en: Problem 9\. Apply the Gram-Schmidt process to the input vectors
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 9. 对输入向量应用格拉姆-施密特过程
- en: '![v1 = (2,1,1), v2 = (1,1,1), v3 = (1,0,1). ](img/file246.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![v1 = (2,1,1), v2 = (1,1,1), v3 = (1,0,1). ](img/file246.png)'
- en: Join our community on Discord
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们在 Discord 上的社区
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他用户、机器学习专家以及作者本人一起阅读本书。提问、为其他读者提供解决方案、通过问我任何问题的环节与作者互动，等等。扫描二维码或访问链接加入社区。[https://packt.link/math](https://packt.link/math)
- en: '![PIC](img/file1.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1.png)'
