- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Parallel Processing with Dask and Zarr
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Dask 和 Zarr 进行并行处理
- en: Bioinformatics datasets are growing at an exponential rate. Data analysis strategies
    based on standard tools such as Pandas assume that datasets are able to fit in
    memory (though with some provision for out-of-core analysis) or that a single
    machine is able to efficiently process all the data. This is, unfortunately, not
    realistic for many modern datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 生物信息学数据集正在以指数速度增长。基于标准工具（如 Pandas）的数据分析策略假设数据集能够装入内存（尽管会有一些外部存储分析的处理），或者假设单台机器能够高效地处理所有数据。不幸的是，这对于许多现代数据集来说并不现实。
- en: 'In this chapter, we will introduce two libraries that are able to deal with
    very large datasets and expensive computations:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍两种能够处理非常大数据集和昂贵计算的库：
- en: Dask is a library that allows parallel computing that can scale from a single
    computer to very large cloud and cluster environments. Dask provides interfaces
    that are similar to Pandas and NumPy while allowing you to deal with large datasets
    spread over many computers.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 是一个支持并行计算的库，可以扩展到从单台计算机到非常大的云环境和集群环境。Dask 提供了与 Pandas 和 NumPy 类似的接口，同时允许你处理分布在多台计算机上的大数据集。
- en: Zarr is a library that stores compressed and chunked multidimensional arrays.
    As we will see, these arrays are tailored to deal with very large datasets processed
    in large computer clusters, while still being able to process data on a single
    computer if need be.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zarr 是一个存储压缩和分块多维数组的库。正如我们将看到的，这些数组专为处理在大型计算机集群中处理的大数据集而设计，同时在需要时也能在单台计算机上处理数据。
- en: Our recipes will introduce these advanced libraries using data from mosquito
    genomics. You should look at this code as a starting point to get you on the path
    to processing large datasets. Parallel processing of large datasets is a complex
    topic, and this is the beginning—not the end—of your journey.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的食谱将使用蚊子基因组学的数据介绍这些高级库。你应该将这段代码作为起点，帮助你走上处理大数据集的道路。大数据集的并行处理是一个复杂的话题，而这只是你旅程的开始——而非结束。
- en: Because all these libraries are fundamental for data analysis, if you are using
    Docker, they all can be found on the `tiagoantao/bioinformatics_dask` Docker image.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因为所有这些库对于数据分析都至关重要，如果你正在使用 Docker，它们都可以在 `tiagoantao/bioinformatics_dask` Docker
    镜像中找到。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下食谱：
- en: Reading genomics data with Zarr
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Zarr 读取基因组学数据
- en: Parallel processing of data using Python multiprocessing
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Python 多进程进行数据并行处理
- en: Using Dask to process genomic data based on NumPy arrays
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Dask 基于 NumPy 数组处理基因组数据
- en: Scheduling tasks with `dask.distributed`
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `dask.distributed` 调度任务
- en: Reading genomics data with Zarr
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Zarr 读取基因组学数据
- en: Zarr ([https://zarr.readthedocs.io/en/stable/](https://zarr.readthedocs.io/en/stable/))
    stores array-based data—such as NumPy —in a hierarchical structure on disk and
    cloud storage. The data structures used by Zarr to represent arrays are not only
    very compact but also allow for parallel reading and writing, something we will
    see in the next recipes. In this recipe, we will be reading and processing genomics
    data from the Anopheles gambiae 1000 Genomes project ([https://malariagen.github.io/vector-data/ag3/download.xhtml](https://malariagen.github.io/vector-data/ag3/download.xhtml)).
    Here, we will simply do sequential processing to ease the introduction to Zarr;
    in the following recipe, we will do parallel processing. Our project will be computing
    the missingness for all genomic positions sequenced for a single chromosome.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Zarr ([https://zarr.readthedocs.io/en/stable/](https://zarr.readthedocs.io/en/stable/))
    将基于数组的数据（如 NumPy）存储在磁盘和云存储的层次结构中。Zarr 用来表示数组的数据结构不仅非常紧凑，而且还支持并行读取和写入，这一点我们将在接下来的食谱中看到。在本食谱中，我们将读取并处理来自按蚊基因组
    1000 基因组计划的数据（[https://malariagen.github.io/vector-data/ag3/download.xhtml](https://malariagen.github.io/vector-data/ag3/download.xhtml)）。在这里，我们将仅进行顺序处理，以便引入
    Zarr；在接下来的食谱中，我们将进行并行处理。我们的项目将计算单一染色体上所有基因位置的缺失数据。
- en: Getting ready
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The Anopheles 1000 Genomes data is available from `gsutil`, available from
    [https://cloud.google.com/storage/docs/gsutil_install](https://cloud.google.com/storage/docs/gsutil_install).
    After you have `gsutil` installed, download the data (~2 **gigabytes** (**GB**))
    with the following lines of code:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 [https://cloud.google.com/storage/docs/gsutil_install](https://cloud.google.com/storage/docs/gsutil_install)
    上提供的指导，从 `gsutil` 中获取按蚊 1000 基因组数据。在安装了 `gsutil` 后，使用以下代码行下载数据（约 2 **千兆字节** (**GB**)）：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We download a subset of samples from the project. After downloading the data,
    the code to process it can be found in `Chapter11/Zarr_Intro.py`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从项目中下载了一个样本子集。下载数据后，处理它的代码可以在`Chapter11/Zarr_Intro.py`中找到。
- en: How to do it...
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Take a look at the following steps to get started:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下步骤以开始：
- en: 'Let’s start by checking the structure made available inside the Zarr file:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先检查一下Zarr文件中提供的结构：
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We start by opening the Zarr file (as we will soon see, this might not actually
    be a file). After that, we print the tree of data available inside it:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从打开Zarr文件开始（正如我们很快会看到的，这可能实际上并不是一个文件）。之后，我们会打印出里面可用的数据树：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The Zarr file has five arrays: four correspond to chromosomes in the mosquito—`2L`,
    `2R`, `3L`, `3R`, and `X` (`Y` is not included)—and one has a list of 81 samples
    included in the file. The last array has the sample names included—we have 81
    samples in this file. The chromosome data is made of 8-bit integers (`int8`),
    and the sample names are strings.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Zarr文件包含五个数组：四个对应蚊子的染色体——`2L`、`2R`、`3L`、`3R`和`X`（`Y`不包括在内）——另一个包含文件中包含的81个样本。最后一个数组包含样本名称——我们在这个文件中有81个样本。染色体数据由8位整数（`int8`）组成，样本名称则是字符串。
- en: 'Now, let’s explore the data for chromosome `2L`. Let’s start with some basic
    information:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们探索`2L`染色体的数据。首先来看一些基本信息：
- en: '[PRE3]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is the output:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We have an array of `4852547` `81` samples. For each SNP and sample, we have
    `2` alleles.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含`4852547`个SNP和`81`个样本的数组。对于每个SNP和样本，我们有`2`个等位基因。
- en: 'Let’s now inspect how the data is stored:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来检查数据是如何存储的：
- en: '[PRE5]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output looks like this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来是这样的：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There is a lot to unpack here, but for now, we will concentrate on the store
    type, bytes stored, and storage ratio. The `Store type` value is `zarr.storage.DirectoryStore`,
    so the data is not in a single file but inside a directory. The raw size of the
    data is `7.3` GB! But Zarr uses a compressed format that reduces the size to `426.2`
    `17.6`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多内容需要解析，但现在我们将专注于存储类型、存储的字节数和存储比率。`Store type`的值是`zarr.storage.DirectoryStore`，所以数据并不在一个单独的文件中，而是存储在一个目录内。数据的原始大小是`7.3`
    GB！但是Zarr使用压缩格式，将数据的大小压缩到`426.2` `17.6`。
- en: 'Let’s peek at how the data is stored inside the directory. If you list the
    contents of the `AG1000G-AO` directory, you will find the following structure:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看数据是如何存储在目录中的。如果你列出`AG1000G-AO`目录的内容，你会发现以下结构：
- en: '[PRE7]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If you list the contents of `2L/calldata/GT`, you will find plenty of files
    encoding the array:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你列出`2L/calldata/GT`目录的内容，你会发现很多文件在编码该数组：
- en: '[PRE8]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There are 324 files inside the `2L/calldata/GT` directory. Remember from a previous
    step that we have a parameter called `Chunk shape` with a value of `(300000, 50,
    2)`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`2L/calldata/GT`目录中有324个文件。记住，在前一步中我们有一个叫做`Chunk shape`的参数，它的值是`(300000, 50,
    2)`。'
- en: Zarr splits the array into chunks—bits that are easier to process in memory
    than loading the whole array. Each chunk has 30000x50x2 elements. Given that we
    have 48525747 SNPs, we need 162 chunks to represent the number of SNPs (48525747/300000
    = 161.75) and then multiply it by 2 for the number of samples (81 samples/50 per
    chunk = 1.62). Hence, we end up with 162*2 chunks/files.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Zarr将数组拆分成多个块——这些块比加载整个数组更容易在内存中处理。每个块包含30000x50x2个元素。考虑到我们有48525747个SNP，我们需要162个块来表示这些SNP的数量（48525747/300000
    = 161.75），然后乘以2以表示样本的数量（81个样本/每块50个 = 1.62）。因此，我们最终会得到162*2个块/文件。
- en: Tip
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Chunking is a technique widely used to deal with data that cannot be fully loaded
    into memory in a single pass. This includes many other libraries such as Pandas
    or Zarr. We will see an example with Zarr later. The larger point is that you
    should be aware of the concept of chunking as it is applied in many cases requiring
    big data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 分块是一个广泛应用的技术，用于处理不能完全一次性加载到内存中的数据。这包括许多其他库，如Pandas或Zarr。稍后我们将看到一个Zarr的例子。更大的观点是，你应该意识到分块的概念，因为它在许多需要大数据的场景中都有应用。
- en: 'Before we load the Zarr data for processing, let’s create a function to compute
    some basic genomic statistics for a chunk. We will compute missingness, the number
    of ancestral homozygotes, and the number of heterozygotes:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们加载Zarr数据进行处理之前，先创建一个函数来计算一个块的基本基因组统计信息。我们将计算缺失值、祖先纯合子数量和异合子数量：
- en: '[PRE9]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you look at the previous function, you will notice that there is nothing
    Zarr-related: it’s just NumPy code. Zarr has a very light **application programming
    interface** (**API**) that exposes most of the data inside NumPy, making it quite
    easy to use if you know NumPy.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看前面的函数，你会注意到没有任何与Zarr相关的内容：它只是NumPy代码。Zarr有一个非常轻量的**应用程序接口**（**API**），它将NumPy中的大多数数据暴露出来，使得如果你熟悉NumPy，它非常容易使用。
- en: 'Finally, let’s traverse our data—that is, traverse our chunks to compute our
    statistics:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们遍历数据——也就是遍历我们的数据块来计算统计信息：
- en: '[PRE10]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Most of the code takes care of the management of chunks and involves arithmetic
    to decide which part of the array to access. The important part in terms of ready
    Zarr data is the `my_chunk = gt_2l[start_pos:end_pos, :, :]` line. When you slice
    a Zarr array, it will automatically return a NumPy array.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数代码负责管理数据块，并涉及算术运算来决定访问数组的哪部分。就准备好的Zarr数据而言，重要的部分是`my_chunk = gt_2l[start_pos:end_pos,
    :, :]`这一行。当你切片Zarr数组时，它会自动返回一个NumPy数组。
- en: Tip
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Be very careful with the amount of data that you bring into memory. Remember
    that most Zarr arrays will be substantially bigger than the memory that you have
    available, so if you try to load it, your application and maybe even your computer
    will crash. For example, if you do `all_data = gt_2l[:, :, :]`, you will need
    around 8 GB of free memory to load it—as we have seen earlier, the data is 7.3
    GB in size.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据加载到内存时要非常小心。记住，大多数Zarr数组的大小将远大于你可用的内存，因此如果尝试加载，可能会导致应用程序甚至计算机崩溃。例如，如果你执行`all_data
    = gt_2l[:, :, :]`，你将需要大约8 GB的空闲内存来加载它——正如我们之前看到的，数据大小为7.3 GB。
- en: There’s more...
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Zarr has many more features than those presented here, and while we will explore
    some more in the next recipes, there are some possibilities that you should be
    aware of. For example, Zarr is one of the only libraries that allow for concurrent
    writing of data. You can also change the internal format of a Zarr representation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Zarr具有比这里展示的更多功能，虽然我们将在接下来的示例中探索一些其他功能，但仍有一些可能性是你应该了解的。例如，Zarr是少数几个允许并发写入数据的库之一。你还可以更改Zarr表示的内部格式。
- en: As we have seen here, Zarr is able to compress data in very efficient ways—this
    is made possible by using the Blosc library ([https://www.blosc.org/](https://www.blosc.org/)).
    You can change the internal compression algorithm of Zarr data owing to the flexibility
    of Blosc.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这里看到的，Zarr能够以非常高效的方式压缩数据——这是通过使用Blosc库实现的（[https://www.blosc.org/](https://www.blosc.org/)）。由于Blosc的灵活性，你可以更改Zarr数据的内部压缩算法。
- en: See also
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: There are alternative formats to Zarr—for example, **Hierarchical Data Format
    5** (**HDF5**) ([https://en.wikipedia.org/wiki/Hierarchical_Data_Format](https://en.wikipedia.org/wiki/Hierarchical_Data_Format))
    and **Network Common Data Form** (**NetCDF**) ([https://en.wikipedia.org/wiki/NetCDF](https://en.wikipedia.org/wiki/NetCDF)).
    While these are more common outside the bioinformatics space, they have less functionality—for
    example, a lack of concurrent writes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Zarr有替代格式——例如，**分层数据格式5**（**HDF5**）（[https://en.wikipedia.org/wiki/Hierarchical_Data_Format](https://en.wikipedia.org/wiki/Hierarchical_Data_Format)）和**网络公共数据格式**（**NetCDF**）（[https://en.wikipedia.org/wiki/NetCDF](https://en.wikipedia.org/wiki/NetCDF)）。虽然这些格式在生物信息学领域之外更为常见，但它们功能较少——例如，缺乏并发写入功能。
- en: Parallel processing of data using Python multiprocessing
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python多处理进行数据并行处理
- en: When dealing with lots of data, one strategy is to process it in parallel so
    that we make use of all available **central processing unit** (**CPU**) power,
    given that modern machines have many cores. In a theoretical best-case scenario,
    if your machine has eight cores, you can get an eight-fold increase in performance
    if you do parallel processing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大量数据时，一种策略是并行处理，以便利用所有可用的**中央处理单元**（**CPU**）的计算能力，因为现代计算机通常有多个核心。在理论上的最佳情况下，如果你的计算机有八个核心，你可以通过并行处理获得八倍的性能提升。
- en: Unfortunately, typical Python code only makes use of a single core. That being
    said, Python has built-in functionality to use all available CPU power; in fact,
    Python provides several avenues for that. In this recipe, we will be using the
    built-in `multiprocessing` module. The solution presented here works well in a
    single computer and if the dataset fits into memory, but if you want to scale
    it in a cluster or the cloud, you should consider Dask, which we will introduce
    in the next two recipes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，典型的Python代码只能使用一个核心。话虽如此，Python具有内置功能来利用所有可用的CPU资源；事实上，Python提供了几种方法来实现这一点。在本配方中，我们将使用内置的`multiprocessing`模块。这里提供的解决方案在单台计算机上运行良好，并且如果数据集能适应内存的话也没有问题，但如果你想要在集群或云端扩展，应该考虑使用Dask，我们将在接下来的两篇配方中介绍它。
- en: Our objective here will again be to compute some statistics around missingness
    and heterozygosity.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里的目标仍然是计算与缺失值和杂合度相关的统计信息。
- en: Getting ready
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will be using the same data as in the previous recipe. The code for this
    recipe can be found in `Chapter11/MP_Intro.py`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与之前的配方相同的数据。该配方的代码可以在`Chapter11/MP_Intro.py`中找到。
- en: How to do it...
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Follow these steps to get started:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照以下步骤开始：
- en: 'We will be using the exact same function as in the previous recipe to calculate
    statistics—this is heavily NumPy-based:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用与之前的配方完全相同的函数来计算统计信息——这是一个高度依赖NumPy的函数：
- en: '[PRE11]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s access our mosquito data:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们访问我们的蚊子数据：
- en: '[PRE12]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'While we are using the same function to calculate statistics, our approach
    will be different for the whole dataset. First, we compute all the intervals for
    which we will call `calc_stats`. The intervals will be devised to match perfectly
    with the chunk division for variants:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管我们使用相同的函数来计算统计信息，但我们对整个数据集的处理方式将有所不同。首先，我们计算所有将调用`calc_stats`的区间。这些区间将被设计成与变异体的块划分完美匹配：
- en: '[PRE13]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It is important that our interval list is related to the chunking on disk. The
    computation will be efficient as long as this mapping is as close as possible.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的区间列表必须与磁盘上的块划分相关。这项计算会很高效，只要这个映射尽可能接近。
- en: 'We are now going to separate the code to compute each interval in a function.
    This is important as the `multiprocessing` module will execute this function many
    times on each process that it creates:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将把计算每个区间的代码分离到一个函数中。这一点很重要，因为`multiprocessing`模块将在它创建的每个进程中多次执行这个函数：
- en: '[PRE14]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We are now finally going to execute our code over several cores:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在终于将让代码在多个核心上执行：
- en: '[PRE15]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first line creates a context manager using the `multiprocessing.Pool` object.
    The `Pool` object, by default, creates several processes numbered `os.cpu_count()`.
    The pool provides a `map` function that will call our `compute_interval` function
    across all processes created. Each call will take one of the intervals.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行使用`multiprocessing.Pool`对象创建一个上下文管理器。`Pool`对象默认会创建多个编号为`os.cpu_count()`的进程。池提供了一个`map`函数，能够在所有创建的进程中调用我们的`compute_interval`函数。每次调用将处理一个区间。
- en: There’s more...
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: This recipe provides a small introduction to parallel processing with Python
    without the need to use external libraries. That being said, it presents the most
    important building block for concurrent parallel processing with Python.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方简要介绍了如何在Python中进行并行处理，而无需使用外部库。话虽如此，它展示了并发并行处理的最重要构建块。
- en: Due to the way thread management is implemented in Python, threading is not
    a viable alternative for real parallel processing. Pure Python code cannot be
    run in parallel using multithreading.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Python中的线程管理方式，线程并不是实现真正并行处理的可行替代方案。纯Python代码无法通过多线程并行执行。
- en: 'Some libraries that you might use—and this is normally the case with NumPy—are
    able to make use of all underlying processors even when executing a sequential
    piece of code. Make sure that when making use of external libraries, you are not
    overcommitting processor resources: this happens when you have multiple processes,
    and underlying libraries also make use of many cores.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一些你可能使用的库——通常NumPy就是这样的——能够在执行顺序代码时利用所有底层处理器。确保在使用外部库时，不要过度占用处理器资源：当你有多个进程时，底层库也会使用多个核心。
- en: See also
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: There is way more to be discussed about the `multiprocessing` module. You can
    start with the standard documentation at [https://docs.python.org/3/library/multiprocessing.xhtml](https://docs.python.org/3/library/multiprocessing.xhtml).
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关 `multiprocessing` 模块的讨论还有很多。你可以从标准文档开始了解：[https://docs.python.org/3/library/multiprocessing.xhtml](https://docs.python.org/3/library/multiprocessing.xhtml)
- en: To understand why Python-based multithreading doesn’t make use of all CPU resources,
    read about the **Global Interpreter Lock** (**GIL**) at [https://realpython.com/python-gil/.](https://realpython.com/python-gil/.%0D)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要理解为什么基于 Python 的多线程不能充分利用所有 CPU 资源，请阅读有关 **全局解释器锁** (**GIL**) 的内容：[https://realpython.com/python-gil/.](https://realpython.com/python-gil/.%0D)
- en: Using Dask to process genomic data based on NumPy arrays
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Dask 处理基于 NumPy 数组的基因组数据
- en: Dask is a library that provides advanced parallelism that can scale from a single
    computer to very large clusters or a cloud operation. It also provides the ability
    to process datasets that are larger than memory. It is able to provide interfaces
    that are similar to common Python libraries such as NumPy, Pandas, or scikit-learn.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 是一个提供高级并行处理的库，可以从单个计算机扩展到非常大的集群或云操作。它还提供了处理比内存更大的数据集的能力。它能够提供与常见 Python
    库如 NumPy、Pandas 或 scikit-learn 相似的接口。
- en: We are going to repeat a subset of the example from previous recipes—namely,
    compute missingness for the SNPs in our dataset. We will be using an interface
    similar to NumPy that is offered by Dask.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重复之前配方中的一个子集——即计算数据集中 SNP 的缺失情况。我们将使用 Dask 提供的类似于 NumPy 的接口。
- en: 'Before we start, be aware that the semantics of Dask are quite different from
    libraries such as NumPy or Pandas: it is a lazy library. For example, when you
    specify a call equivalent to—say—`np.sum`, you are not actually calculating a
    sum, but adding a task that in the future will eventually calculate it. Let’s
    get into the recipe to make things clearer.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，请注意 Dask 的语义与 NumPy 或 Pandas 等库有很大不同：它是一个懒加载库。例如，当你指定一个等效于 `np.sum`
    的调用时，你实际上并没有计算和求和，而是在未来会计算它的任务。让我们进入配方来进一步澄清这一点。
- en: Getting ready
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'We are going to rechunk the Zarr data in a completely different way. The reason
    we do that is so that we can visualize task graphs during the preparation of our
    algorithm. Task graphs with five operations are easier to visualize than task
    graphs with hundreds of nodes. For practical purposes, you should not rechunk
    in so little chunks as we do here. In fact, you will be perfectly fine if you
    don’t rechunk this dataset at all. We are only doing it for visualization purposes:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以一种完全不同的方式重新分块 Zarr 数据。我们这么做的原因是为了在准备算法时能够可视化任务图。包含五个操作的任务图比包含数百个节点的任务图更容易可视化。为了实际目的，你不应该像我们这里做的那样将数据重新分块为如此小的块。实际上，如果你根本不重新分块这个数据集，也是完全可以的。我们这么做只是为了可视化的目的：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We will end up with very large chunks, and while that is good for our visualization
    purpose, they might be too big to fit in memory.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终会得到非常大的块，虽然这对我们的可视化目的很有用，但它们可能太大而无法放入内存中。
- en: The code for this recipe can be found in `Chapter11/Dask_Intro.py`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方的代码可以在 `Chapter11/Dask_Intro.py` 中找到。
- en: How to do it...
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Let’s first load the data and inspect the size of the DataFrame:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先加载数据并检查 DataFrame 的大小：
- en: '[PRE17]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is the output if you are executing inside Jupyter:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在 Jupyter 中执行，这将是输出结果：
- en: '![Figure 11.1 - Jupyter output for a Dask array, summarizing our data ](img/B17942_11_001.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 - Dask 数组的 Jupyter 输出，汇总我们的数据](img/B17942_11_001.jpg)'
- en: Figure 11.1 - Jupyter output for a Dask array, summarizing our data
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 - Dask 数组的 Jupyter 输出，汇总我们的数据
- en: 'The full array takes up `7.32` GB. The most important number is the chunk size:
    `1.83` GB. Each worker will need to have enough memory to process a chunk. Remember
    that we are only using such a smaller number of chunks to be able to plot the
    tasks here.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的数组占用 `7.32` GB。最重要的数字是块的大小：`1.83` GB。每个工作节点需要有足够的内存来处理一个块。记住，我们这里只是使用了较少的块数，以便能够在这里绘制任务。
- en: Because of the large chunk sizes, we end up with just four chunks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大块数据的大小，我们最终只得到了四个块。
- en: 'We did not load anything in memory yet: we just specified that we want to eventually
    do it. We are creating a task graph to be executed, not executing—at least for
    now.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未将任何内容加载到内存中：我们只是指定了最终想要执行的操作。我们正在创建一个任务图来执行，而不是立即执行——至少目前如此。
- en: 'Let’s see which tasks we have to execute to load the data:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看我们需要执行哪些任务来加载数据：
- en: '[PRE18]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is the output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 11.2 - Tasks that need to be executed to load our Zarr array ](img/B17942_11_002.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 - 加载我们的 Zarr 数组所需执行的任务](img/B17942_11_002.jpg)'
- en: Figure 11.2 - Tasks that need to be executed to load our Zarr array
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 - 加载我们的 Zarr 数组所需执行的任务
- en: We thus have four tasks to execute, one for each chunk.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有四个任务要执行，每个块对应一个任务。
- en: 'Now, let’s look at the function to compute missingness per chunk:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看计算每个块缺失值的函数：
- en: '[PRE19]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The function per chunk will operate on NumPy arrays. Note the difference: the
    code that we use to work on the main loop works with Dask arrays, but at the chunk
    level the data is presented as a NumPy array. Hence, the chunks have to fit in
    memory as NumPy requires that.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 每个块的函数将在 NumPy 数组上操作。请注意区别：我们在主循环中使用的代码是针对 Dask 数组的，但在块级别，数据以 NumPy 数组的形式呈现。因此，这些块必须适配内存，因为
    NumPy 需要如此。
- en: 'Later, when we actually use the function, we need to have a **two-dimensional**
    (**2D**) array. Given that the array is **three-dimensional** (**3D**), we will
    need to reshape the array:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 后面，当我们实际使用这个函数时，我们需要一个**二维**（**2D**）数组。由于数组是**三维**（**3D**）的，我们需要对数组进行重塑：
- en: '[PRE20]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here is the task graph as it currently stands:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是当前的任务图：
- en: '![Figure 11.3 - The task graph to load genomic data and reshape it ](img/B17942_11_003.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 - 加载基因组数据并重塑的任务图](img/B17942_11_003.png)'
- en: Figure 11.3 - The task graph to load genomic data and reshape it
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 - 加载基因组数据并重塑的任务图
- en: The `reshape` operation is happening at the `dask.array` level, not at the NumPy
    level, so it just added nodes to the task graph. There is still no execution.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`reshape` 操作发生在 `dask.array` 层，而不是 NumPy 层，因此它仅向任务图中添加了节点。仍然没有执行。'
- en: 'Let’s now prepare to execute the function—meaning adding tasks to our task
    graph—over all our dataset. There are many ways to execute it; here, we are going
    to use the `apply_along_axis` function that `dask.array` provides and is based
    on the equally named function from NumPy:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们准备执行这个函数——意味着在整个数据集上向我们的任务图中添加任务。有很多种执行方式；在这里，我们将使用 `dask.array` 提供的 `apply_along_axis`
    函数，它基于 NumPy 中同名的函数：
- en: '[PRE21]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'For now, we are only going to study the first million positions. As you can
    see in the task graph, Dask is smart enough to only add an operation to the chunk
    involved in the computation:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们只打算研究前百万个位置。正如你在任务图中看到的，Dask 足够智能，只会对参与计算的块添加操作：
- en: '![Figure 11.4 - The complete task graph including statistical computing ](img/B17942_11_004.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.4 - 包括统计计算在内的完整任务图](img/B17942_11_004.jpg)'
- en: Figure 11.4 - The complete task graph including statistical computing
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 - 包括统计计算在内的完整任务图
- en: 'Remember that we haven’t computed anything until now. It is now time to actually
    execute the task graph:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记住，在此之前我们还没有进行任何计算。现在是时候真正执行任务图了：
- en: '[PRE22]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This will start the computation. Precisely how the computation is done is something
    we will discuss in the next recipe.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动计算。计算的具体方式是我们将在下一个配方中讨论的内容。
- en: WARNING
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Because of the chunk size, this code might crash your computer. You will be
    safe with at least 16 GB of memory. Remember that you can use smaller chunk sizes—and
    you *should use* smaller chunk sizes. We just used chunk sizes like this in order
    to be able to generate the task graphs shown earlier (if not, they would have
    possibly hundreds of nodes and would be unprintable).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于块大小的问题，这段代码可能会导致你的计算机崩溃。至少需要 16 GB 内存才能保证安全。记住，你可以使用更小的块大小——而且你*应该使用*更小的块大小。我们之所以使用这样的块大小，是为了能够生成前面展示的任务图（否则，它们可能会有数百个节点，无法打印出来）。
- en: There’s more...
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: We didn’t spend any time here discussing strategies to optimize the code for
    Dask—that would be a book of its own. For very complex algorithms, you will need
    to research further into the best practices.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有在这里讨论如何优化 Dask 代码的策略——那将是另一本书的内容。对于非常复杂的算法，你需要进一步研究最佳实践。
- en: Dask provides interfaces similar to other known Python libraries such as Pandas
    or scikit-learn that can be used for parallel processing. You can also use it
    for general algorithms that are not based on existing libraries.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 提供的接口类似于其他常见的 Python 库，如 Pandas 或 scikit-learn，可以用于并行处理。你也可以将它用于不依赖现有库的通用算法。
- en: See also
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For best practices with Dask, your best starting point is the Dask documentation
    itself, especially [https://docs.dask.org/en/latest/best-practices.xhtml](https://docs.dask.org/en/latest/best-practices.xhtml).
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Dask 的最佳实践，最好的起点是 Dask 文档本身，尤其是 [https://docs.dask.org/en/latest/best-practices.xhtml](https://docs.dask.org/en/latest/best-practices.xhtml)。
- en: Scheduling tasks with dask.distributed
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 dask.distributed 调度任务
- en: 'Dask is extremely flexible in terms of execution: we can execute locally, on
    a scientific cluster, or on the cloud. That flexibility comes at a cost: it needs
    to be parameterized. There are several alternatives to configure a Dask schedule
    and execution, but the most generic is `dask.distributed` as it is able to manage
    different kinds of infrastructure. Because I cannot assume you have access to
    a cluster or a cloud such as `dask.distributed` on very different kinds of platforms.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 在执行方面非常灵活：我们可以在本地执行、在科学集群上执行，或者在云上执行。这种灵活性是有代价的：它需要被参数化。有多种配置 Dask 调度和执行的方式，但最通用的是
    `dask.distributed`，因为它能够管理不同种类的基础设施。因为我不能假设你能够访问像 `dask.distributed` 这样存在于不同平台上的集群或云服务。
- en: Here, we will again compute simple statistics over variants of the Anopheles
    1000 Genomes project.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将再次计算 Anopheles 1000 基因组项目的不同变体的简单统计数据。
- en: Getting ready
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Before we start with `dask.distributed`, we should note that Dask has a default
    scheduler that actually can change depending on the library you are targeting.
    For example, here is the scheduler for our NumPy example:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用`dask.distributed`之前，我们需要注意，Dask 有一个默认的调度器，这个调度器实际上会根据你所使用的库而有所变化。例如，以下是我们
    NumPy 示例的调度器：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output will be as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Dask uses a threaded scheduler here. This makes sense for a NumPy array: NumPy
    implementation is itself multithreaded (real multithreaded with parallelism).
    We don’t want lots of processes running when the underlying library is running
    in parallel to itself. If you had a Pandas DataFrame, Dask would probably choose
    a multiprocessor scheduler. As Pandas is not parallel, it makes sense for Dask
    to run in parallel itself.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 在这里使用了一个线程调度器。对于 NumPy 数组来说，这样做是有道理的：NumPy 实现本身是多线程的（真正的多线程，带有并行性）。当底层库并行运行时，我们不希望有大量进程在后台运行。如果你使用的是
    Pandas DataFrame，Dask 可能会选择一个多进程调度器。因为 Pandas 本身不支持并行，所以让 Dask 自己并行运行是有意义的。
- en: OK—now that we have that important detail out of the way, let’s get back to
    preparing our environment.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 好的——既然我们已经解决了这个重要细节，现在让我们回到环境准备工作。
- en: '`dask.distributed` has a centralized scheduler and a set of workers, and we
    need to start those. Run this code in the command line to start the scheduler:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`dask.distributed` 有一个集中式调度器和一组工作节点，我们需要启动它们。可以在命令行中运行以下代码来启动调度器：'
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can start workers on the same machine as the scheduler, like this:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在与调度器相同的机器上启动工作节点，方法如下：
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: I specified two processes with a single thread per process. This is reasonable
    for NumPy code, but the actual configuration will depend on your workload (and
    be completely different if you are on a cluster or a cloud).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我指定了每个进程使用一个线程。对于 NumPy 代码来说，这个配置是合理的，但实际配置将取决于你的工作负载（如果你在集群或云上，配置可能完全不同）。
- en: Tip
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: You actually do not need to start the whole process manually as I did here.
    `dask.distributed` will start something for you—not really optimized for your
    workload—if you don’t prepare the system yourself (see the following section for
    details). But I wanted to give you a flavor of the effort as in many cases, you
    will have to set up the infrastructure yourself.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上不需要像我这里所做的那样手动启动整个进程。`dask.distributed` 会为你启动一些东西——虽然它不会完全优化你的工作负载——如果你没有自己准备好系统（详情请见下一部分）。但我想给你一个概念，因为在很多情况下，你必须自己设置基础设施。
- en: Again, we will be using data from the first recipe. Be sure you download and
    prepare it, as explained in its *Getting ready* section. We won’t be using the
    rechunked part—we will be doing it in our Dask code in the following section.
    Our code is available in `Chapter11/Dask_distributed.py`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将使用第一部分食谱中的数据。请确保按照*准备工作*部分的说明下载并准备好数据。我们不会使用重新分块的部分——我们将在下一部分的 Dask 代码中进行处理。我们的代码可以在`Chapter11/Dask_distributed.py`中找到。
- en: How to do it...
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Follow these steps to get started:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤开始：
- en: 'Let’s start by connecting to the scheduler that we created earlier:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从连接到之前创建的调度器开始：
- en: '[PRE27]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If you are on Jupyter, you will get a nice output summarizing the configuration
    you created in the *Getting ready* part of this recipe:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是 Jupyter，你将看到一个很好的输出，汇总了你在此食谱的*准备工作*部分所创建的配置：
- en: '![Figure 11.5 - Summary of your execution environment with dask.distributed
    ](img/B17942_11_005.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.5 - 使用 dask.distributed 时的执行环境摘要](img/B17942_11_005.jpg)'
- en: Figure 11.5 - Summary of your execution environment with dask.distributed
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 - 使用 dask.distributed 时的执行环境摘要
- en: You will notice the reference to a dashboard here. `dask.distributed` provides
    a real-time dashboard over the web that allows you to track the state of the computation.
    Point your browser to http://127.0.0.1:8787/ to find it, or just follow the link
    provided in *Figure 11.5*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到这里提到了一个仪表板。`dask.distributed` 提供了一个实时仪表板，允许你跟踪计算的状态。你可以在浏览器中输入 http://127.0.0.1:8787/
    来访问它，或者直接点击 *图 11.5* 中提供的链接。
- en: 'As we still haven’t done any computations, the dashboard is mostly empty. Be
    sure to explore the many tabs along the top:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们还没有进行任何计算，仪表板大部分是空的。一定要探索顶部的许多标签：
- en: '![Figure 11.6 - The starting state of the dask.distributed dashboard ](img/B17942_11_006.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 - dask.distributed 仪表板的初始状态](img/B17942_11_006.jpg)'
- en: Figure 11.6 - The starting state of the dask.distributed dashboard
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 - dask.distributed 仪表板的初始状态
- en: 'Let’s load the data. More rigorously, let’s prepare the task graph to load
    the data:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载数据。更严格地说，让我们准备任务图以加载数据：
- en: '[PRE28]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is the output on Jupyter:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在 Jupyter 上的输出：
- en: '![Figure 11.7 - Summary of the original Zarr array for chromosome 2L ](img/B17942_11_007.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.7 - 原始 Zarr 数组（2L 染色体）的汇总](img/B17942_11_007.jpg)'
- en: Figure 11.7 - Summary of the original Zarr array for chromosome 2L
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 - 原始 Zarr 数组（2L 染色体）的汇总
- en: 'To facilitate visualization, let’s rechunk again. We are also going to have
    a single chunk for the second dimension, which is the samples. This is because
    our computation of missingness requires all the samples, and it makes little sense—in
    our specific case—to have two chunks per sample:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了方便可视化，让我们再次进行分块。我们还将为第二个维度——样本——创建一个单一的块。这是因为我们缺失值的计算需要所有样本，而在我们的特定情况下，为每个样本创建两个块是没有意义的：
- en: '[PRE29]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As a reminder, we have very large chunks, and you might end up with memory problems.
    If that is the case, then you can run it with the original chunks. It’s just that
    the visualization will be unreadable.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，我们有非常大的块，你可能会遇到内存问题。如果是这样，你可以使用原始的块进行运行。只是可视化效果将无法读取。
- en: 'Before we continue, let’s ask Dask to not only execute the rechunking but also
    to have the results of it at the ready in the workers:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在继续之前，让我们要求 Dask 不仅执行重新分块操作，还要确保结果已经准备好并存储在工作节点中：
- en: '[PRE30]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `persist` call makes sure the data is available in the workers. In the
    following screenshot, you can find the dashboard somewhere in the middle of the
    computation. You can find which tasks are executing on each node, a summary of
    tasks done and to be done, and the bytes stored per worker. Of note is the concept
    of **spilled to disk** (see the top left of the screen). If there is not enough
    memory for all chunks, they will temporarily be written to disk:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`persist` 调用确保数据在工作节点中可用。在以下截图中，你可以看到计算过程中的仪表板。你可以查看每个节点上正在执行的任务、已完成和待完成的任务摘要，以及每个工作节点上存储的字节数。需要注意的是
    **溢写到磁盘** 的概念（见屏幕左上角）。如果内存不足以容纳所有块，它们会暂时写入磁盘：'
- en: '![Figure 11.8 - The dashboard state while executing the persist function for
    rechunking the array ](img/B17942_11_008.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 - 执行持久化函数以重新分块数组时的仪表板状态](img/B17942_11_008.jpg)'
- en: Figure 11.8 - The dashboard state while executing the persist function for rechunking
    the array
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 - 执行持久化函数以重新分块数组时的仪表板状态
- en: 'Let’s now compute the statistics. We will use a different approach for the
    last recipe—we will ask Dask to apply a function to each chunk:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们计算统计信息。对于最后一个配方，我们将使用不同的方法——我们将请求 Dask 对每个块应用一个函数：
- en: '[PRE31]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Remember that each chunk is not a `dask.array` instance but a NumPy array,
    so the code works on NumPy arrays. Here is the current task graph. There are no
    operations to load the data, as the function performed earlier executed all of
    those:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，每个块不是 `dask.array` 实例，而是一个 NumPy 数组，因此代码是在 NumPy 数组上运行的。以下是当前的任务图。没有加载数据的操作，因为之前执行的函数已经完成了所有这些操作：
- en: '![Figure 11.9 - Calls to the calc_stats function over chunks starting with
    persisted data ](img/B17942_11_009.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.9 - 从持久化数据开始的每个块对 `calc_stats` 函数的调用](img/B17942_11_009.jpg)'
- en: Figure 11.9 - Calls to the calc_stats function over chunks starting with persisted
    data
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 - 从持久化数据开始的每个块对 `calc_stats` 函数的调用
- en: 'Finally, we can get our results:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以得到我们的结果：
- en: '[PRE32]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: There’s more...
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There is substantially more that can be said about the `dask.distributed` interface.
    Here, we introduced the basic concepts of its architecture and the dashboard.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 `dask.distributed` 接口，还有很多内容可以进一步讲解。在这里，我们介绍了其架构的基本概念和仪表盘。
- en: '`dask.distributed` provides an asynchronous interface based on the standard
    `async` module of Python. Due to the introductory nature of this chapter, we won’t
    address it, but you are recommended to look at it.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`dask.distributed` 提供了基于 Python 标准 `async` 模块的异步接口。由于本章内容的介绍性性质，我们不会详细讨论它，但建议你查看相关内容。'
- en: See also
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: You can start with the documentation of `dask.distributed` at [https://distributed.dask.org/en/stable/](https://distributed.dask.org/en/stable/).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以从 `dask.distributed` 的文档开始，访问 [https://distributed.dask.org/en/stable/](https://distributed.dask.org/en/stable/)。
- en: 'In many cases, you will need to deploy your code in a cluster or the cloud.
    Check out the deployment documentation for resources on different platforms: [https://docs.dask.org/en/latest/deploying.xhtml](https://docs.dask.org/en/latest/deploying.xhtml).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在许多情况下，你需要将代码部署到集群或云端。请查看部署文档，了解不同平台的资源：[https://docs.dask.org/en/latest/deploying.xhtml](https://docs.dask.org/en/latest/deploying.xhtml)。
- en: After you have mastered the content here, studying asynchronous computation
    in Python would be your next step. Check out [https://docs.python.org/3/library/asyncio-task.xhtml](https://docs.python.org/3/library/asyncio-task.xhtml).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在掌握本章内容后，下一步可以学习 Python 中的异步计算。请查看 [https://docs.python.org/3/library/asyncio-task.xhtml](https://docs.python.org/3/library/asyncio-task.xhtml)。
