- en: Start Working with Spark – REPL and RDDs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用Spark - REPL和RDDs
- en: '"All this modern technology just makes people try to do everything at once."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “所有这些现代技术只是让人们试图一次做所有事情。”
- en: '- Bill Watterson'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 比尔·沃特森'
- en: In this chapter, you will learn how Spark works; then, you will be introduced
    to RDDs, the basic abstractions behind Apache Spark, and you'll learn that they
    are simply distributed collections exposing Scala-like APIs. You will then see
    how to download Spark and how to make it run locally via the Spark shell.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解Spark的工作原理；然后，您将介绍RDDs，这是Apache Spark背后的基本抽象，并且您将了解它们只是暴露类似Scala的API的分布式集合。然后，您将看到如何下载Spark以及如何通过Spark
    shell在本地运行它。
- en: 'In a nutshell, the following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章将涵盖以下主题：
- en: Dig deeper into Apache Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解Apache Spark
- en: Apache Spark installation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark安装
- en: Introduction to RDDs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍RDDs
- en: Using the Spark shell
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark shell
- en: Actions and Transformations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作和转换
- en: Caching
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存
- en: Loading and Saving data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载和保存数据
- en: Dig deeper into Apache Spark
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解Apache Spark
- en: Apache Spark is a fast in-memory data processing engine with elegant and expressive
    development APIs to allow data workers to efficiently execute streaming machine
    learning or SQL workloads that require fast interactive access to datasets. Apache
    Spark consists of Spark core and a set of libraries. The core is the distributed
    execution engine and the Java, Scala, and Python APIs offer a platform for distributed
    application development.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个快速的内存数据处理引擎，具有优雅和富有表现力的开发API，允许数据工作者高效地执行流式机器学习或SQL工作负载，这些工作负载需要对数据集进行快速交互式访问。Apache
    Spark由Spark核心和一组库组成。核心是分布式执行引擎，Java，Scala和Python API提供了分布式应用程序开发的平台。
- en: Additional libraries built on top of the core allow the workloads for streaming,
    SQL, Graph processing, and machine learning. SparkML, for instance, is designed
    for Data science and its abstraction makes Data science easier.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 构建在核心之上的附加库允许流处理，SQL，图处理和机器学习的工作负载。例如，SparkML专为数据科学而设计，其抽象使数据科学变得更容易。
- en: In order to plan and carry out the distributed computations, Spark uses the
    concept of a job, which is executed across the worker nodes using Stages and Tasks.
    Spark consists of a driver, which orchestrates the execution across a cluster
    of worker nodes. The driver is also responsible for tracking all the worker nodes
    as well as the work currently being performed by each of the worker nodes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计划和执行分布式计算，Spark使用作业的概念，该作业在工作节点上使用阶段和任务执行。Spark由驱动程序组成，该驱动程序在工作节点集群上协调执行。驱动程序还负责跟踪所有工作节点以及每个工作节点当前执行的工作。
- en: 'Let''s look into the various components a little more. The key components are
    the Driver and the Executors which are all JVM processes (Java processes):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解一下各个组件。关键组件是Driver和Executors，它们都是JVM进程（Java进程）：
- en: '**Driver**: The Driver program contains the applications, main program. If
    you are using the Spark shell, that becomes the Driver program and the Driver
    launches the executors across the cluster and also controls the task executions.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Driver**：Driver程序包含应用程序，主程序。如果您使用Spark shell，那就成为了Driver程序，并且Driver在整个集群中启动执行者，并且还控制任务的执行。'
- en: '**Executor**: Next are the executors which are processes running on the worker
    nodes in your cluster. Inside the executor, the individual tasks or computations
    are run. There could be one or more executors in each worker node and, similarly,
    there could be multiple tasks inside each executor. When Driver connects to the
    cluster manager, the cluster manager assigns resources to run executors.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Executor**：接下来是执行者，它们是在集群中的工作节点上运行的进程。在执行者内部，运行单个任务或计算。每个工作节点中可能有一个或多个执行者，同样，每个执行者内部可能有多个任务。当Driver连接到集群管理器时，集群管理器分配资源来运行执行者。'
- en: The cluster manager could be a standalone cluster manager, YARN, or Mesos.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器可以是独立的集群管理器，YARN或Mesos。
- en: 'The **Cluster Manager** is responsible for the scheduling and allocation of
    resources across the compute nodes forming the cluster. Typically, this is done
    by having a manager process which knows and manages a cluster of resources and
    allocates the resources to a requesting process such as Spark. We will look at
    the three different cluster managers: standalone, YARN, and Mesos further down
    in the next sections.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**集群管理器**负责在形成集群的计算节点之间进行调度和资源分配。通常，这是通过具有了解和管理资源集群的管理进程来完成的，并将资源分配给请求进程，例如Spark。我们将在接下来的章节中更深入地了解三种不同的集群管理器：独立，YARN和Mesos。'
- en: 'The following is how Spark works at a high level:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Spark在高层次上的工作方式：
- en: '![](img/00292.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00292.jpeg)'
- en: The main entry point to a Spark program is called the `SparkContext`. The `SparkContext`
    is inside the **Driver** component and represents the connection to the cluster
    along with the code to run the scheduler and task distribution and orchestration.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark程序的主要入口点称为“SparkContext”。 “SparkContext”位于**Driver**组件内部，表示与集群的连接以及运行调度器和任务分发和编排的代码。
- en: In Spark 2.x, a new variable called `SparkSession` has been introduced. `SparkContext`,
    `SQLContext`, and `HiveContext` are now member variables of the `SparkSession`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.x中，引入了一个名为“SparkSession”的新变量。 “SparkContext”，“SQLContext”和“HiveContext”现在是“SparkSession”的成员变量。
- en: When you start the **Driver** program, the commands are issued to the cluster
    using the `SparkContext`, and then the **executors** will execute the instructions.
    Once the execution is completed, the **Driver** program completes the job. You
    can, at this point, issue more commands and execute more Jobs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当启动**Driver**程序时，使用`SparkContext`向集群发出命令，然后**executors**将执行指令。执行完成后，**Driver**程序完成作业。此时，您可以发出更多命令并执行更多作业。
- en: The ability to maintain and reuse the `SparkContext` is a key advantage of the
    Apache Spark architecture, unlike the Hadoop framework where every `MapReduce`
    job or Hive query or Pig Script starts entire processing from scratch for each
    task we want to execute that too using expensive disk instead of memory.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 保持和重用`SparkContext`的能力是Apache Spark架构的一个关键优势，与Hadoop框架不同，Hadoop框架中每个`MapReduce`作业或Hive查询或Pig脚本都需要从头开始进行整个处理，而且使用昂贵的磁盘而不是内存。
- en: The `SparkContext` can be used to create RDDs, accumulators, and broadcast variables
    on the cluster. Only one `SparkContext` may be active per JVM/Java process. You
    must `stop()` the active `SparkContext` before creating a new one.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkContext`可用于在集群上创建RDD、累加器和广播变量。每个JVM/Java进程只能有一个活动的`SparkContext`。在创建新的`SparkContext`之前，必须`stop()`活动的`SparkContext`。'
- en: The **Driver** parses the code, and serializes the byte level code across to
    the executors to be executed. When we perform any computations, the computations
    will actually be done at the local level by each node, using in-memory processing.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**Driver**解析代码，并将字节级代码序列化传输到执行者以执行。当我们进行任何计算时，实际上是每个节点在本地级别使用内存处理进行计算。'
- en: The process of parsing the code and planning the execution is the key aspect
    implemented by the **Driver** process.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 解析代码并规划执行的过程是由**Driver**进程实现的关键方面。
- en: 'The following is how Spark **Driver** coordinates the computations across the
    cluster:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Spark **Driver**如何协调整个集群上的计算：
- en: '![](img/00298.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00298.jpeg)'
- en: The **Directed Acyclic Graph** (**DAG**) is the secret sauce of Spark framework.
    The **Driver** process creates a DAG of tasks for a piece of code you try to run
    using the distributed processing framework. Then, the DAG is actually executed
    in stages and tasks by the task scheduler by communicating with the **Cluster
    Manager** for resources to run the executors. A DAG represents a job, and a job
    is split into subsets, also called stages, and each stage is executed as tasks
    using one core per task.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**有向无环图**（**DAG**）是Spark框架的秘密武器。**Driver**进程为您尝试使用分布式处理框架运行的代码创建任务的DAG。然后，任务调度程序通过与**集群管理器**通信以获取资源来运行执行者，实际上按阶段和任务执行DAG。DAG代表一个作业，作业被分割成子集，也称为阶段，每个阶段使用一个核心作为任务执行。'
- en: 'An illustration of a simple job and how the DAG is split into stages and tasks
    is shown in the following two illustrations; the first one shows the job itself,
    and the second diagram shows the stages in the job and the tasks:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单作业的示例以及DAG如何分割成阶段和任务的示意图如下两个图示；第一个显示作业本身，第二个图表显示作业中的阶段和任务：
- en: '![](img/00301.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00301.jpeg)'
- en: 'The following diagram now breaks down the job/DAG into stages and tasks:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表将作业/DAG分解为阶段和任务：
- en: '![](img/00304.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00304.jpeg)'
- en: The number of stages and what the stages consist of is determined by the kind
    of operations. Usually, any transformation comes into the same stage as the one
    before, but every operation such as reduce or shuffle always creates a new stage
    of execution. Tasks are part of a stage and are directly related to the cores
    executing the operations on the executors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段的数量和阶段的内容取决于操作的类型。通常，任何转换都会进入与之前相同的阶段，但每个操作（如reduce或shuffle）总是创建一个新的执行阶段。任务是阶段的一部分，与在执行者上执行操作的核心直接相关。
- en: If you use YARN or Mesos as the cluster manager, you can use dynamic YARN scheduler
    to increase the number of executors when more work needs to be done, as well as
    killing idle executors.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用YARN或Mesos作为集群管理器，可以使用动态YARN调度程序在需要执行更多工作时增加执行者的数量，以及终止空闲执行者。
- en: The driver, hence, manages the fault tolerance of the entire execution process.
    Once the job is completed by the Driver, the output can be written to a file,
    database, or simply to the console.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Driver管理整个执行过程的容错。一旦Driver完成作业，输出可以写入文件、数据库，或者简单地输出到控制台。
- en: Remember that the code in the Driver program itself has to be completely serializable
    including all the variables and objects.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，Driver程序本身的代码必须完全可序列化，包括所有变量和对象。
- en: The often seen exception is a not a serializable exception, which is a result
    of including global variables from outside the block.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 经常看到的异常是不可序列化异常，这是由于包含来自块外部的全局变量。
- en: Hence, the Driver process takes care of the entire execution process while monitoring
    and managing the resources used, such as executors, stages, and tasks, making
    sure everything is working as planned and recovering from failures such as task
    failures on executor nodes or entire executor nodes as a whole.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Driver进程负责整个执行过程，同时监视和管理使用的资源，如执行者、阶段和任务，确保一切按计划进行，并从故障中恢复，如执行者节点上的任务故障或整个执行者节点作为整体的故障。
- en: Apache Spark installation
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark安装
- en: Apache Spark is a cross-platform framework, which can be deployed on Linux,
    Windows, and a Mac Machine as long as we have Java installed on the machine. In
    this section, we will look at how to install Apache Spark.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个跨平台框架，可以部署在Linux、Windows和Mac机器上，只要我们在机器上安装了Java。在本节中，我们将看看如何安装Apache
    Spark。
- en: Apache Spark can be downloaded from [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark可以从[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)下载
- en: 'First, let''s look at the pre-requisites that must be available on the machine:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看机器上必须可用的先决条件：
- en: Java 8+ (mandatory as all Spark software runs as JVM processes)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java 8+（作为所有Spark软件都作为JVM进程运行，因此是必需的）
- en: Python 3.4+ (optional and used only when you want to use PySpark)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.4+（可选，仅在使用PySpark时使用）
- en: R 3.1+ (optional and used only when you want to use SparkR)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 3.1+（可选，仅在使用SparkR时使用）
- en: Scala 2.11+ (optional and used only to write programs for Spark)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala 2.11+（可选，仅用于编写Spark程序）
- en: 'Spark can be deployed in three primary deployment modes, which we will look
    at:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以部署在三种主要的部署模式中，我们将会看到：
- en: Spark standalone
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark独立
- en: Spark on YARN
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YARN上的Spark
- en: Spark on Mesos
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesos上的Spark
- en: Spark standalone
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark独立
- en: Spark standalone uses a built-in scheduler without depending on any external
    scheduler such as YARN or Mesos. To install Spark in standalone mode, you have
    to copy the spark binary install package onto all the machines in the cluster.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Spark独立模式使用内置调度程序，不依赖于任何外部调度程序，如YARN或Mesos。要在独立模式下安装Spark，你必须将Spark二进制安装包复制到集群中的所有机器上。
- en: In standalone mode, the client can interact with the cluster, either through
    spark-submit or Spark shell. In either case, the Driver communicates with the
    Spark master Node to get the worker nodes, where executors can be started for
    this application.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立模式下，客户端可以通过spark-submit或Spark shell与集群交互。在任何情况下，Driver都会与Spark主节点通信，以获取可以为此应用程序启动的工作节点。
- en: Multiple clients interacting with the cluster create their own executors on
    the Worker Nodes. Also, each client will have its own Driver component.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与集群交互的多个客户端在Worker节点上创建自己的执行器。此外，每个客户端都将有自己的Driver组件。
- en: 'The following is the standalone deployment of Spark using Master node and worker
    nodes:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用主节点和工作节点的独立部署Spark：
- en: '![](img/00307.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00307.jpeg)'
- en: 'Let''s now download and install Spark in standalone mode using a Linux/Mac:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们下载并安装Spark在独立模式下使用Linux/Mac：
- en: 'Download Apache Spark from the link [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html):'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从链接[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)下载Apache
    Spark：
- en: '![](img/00313.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00313.jpeg)'
- en: 'Extract the package in your local directory:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地目录中解压包：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Change directory to the newly created directory:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到新创建的目录：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Set environment variables for `JAVA_HOME` and `SPARK_HOME` by implementing
    the following steps:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过实施以下步骤设置`JAVA_HOME`和`SPARK_HOME`的环境变量：
- en: '`JAVA_HOME` should be where you have Java installed. On my Mac terminal, this
    is set as:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`JAVA_HOME`应该是你安装Java的地方。在我的Mac终端上，这是设置为：'
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`SPARK_HOME` should be the newly extracted folder. On my Mac terminal, this
    is set as:'
  id: totrans-71
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SPARK_HOME`应该是新解压的文件夹。在我的Mac终端上，这是设置为：'
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Run Spark shell to see if this works. If it does not work, check the `JAVA_HOME`
    and `SPARK_HOME` environment variable: `./bin/spark-shell`'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行Spark shell来查看是否可以工作。如果不工作，检查`JAVA_HOME`和`SPARK_HOME`环境变量：`./bin/spark-shell`
- en: 'You will now see the shell as shown in the following:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你将看到如下所示的shell。
- en: '![](img/00316.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00316.jpeg)'
- en: 'You will see the Scala/ Spark shell at the end and now you are ready to interact
    with the Spark cluster:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将在最后看到Scala/Spark shell，现在你已经准备好与Spark集群交互了：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we have a Spark-shell connected to an automatically setup local cluster
    running Spark. This is the quickest way to launch Spark on a local machine. However,
    you can still control the workers/executors as well as connect to any cluster
    (standalone/YARN/Mesos). This is the power of Spark, enabling you to quickly move
    from interactive testing to testing on a cluster and subsequently deploying your
    jobs on a large cluster. The seamless integration offers a lot of benefits, which
    you cannot realize using Hadoop and other technologies.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个连接到自动设置的本地集群运行Spark的Spark-shell。这是在本地机器上启动Spark的最快方式。然而，你仍然可以控制工作节点/执行器，并连接到任何集群（独立/YARN/Mesos）。这就是Spark的强大之处，它使你能够快速从交互式测试转移到集群测试，随后在大型集群上部署你的作业。无缝集成提供了许多好处，这是你无法通过Hadoop和其他技术实现的。
- en: You can refer to the official documentation in case you want to understand all
    the settings [http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解所有设置，可以参考官方文档[http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)。
- en: 'There are several ways to start the Spark shell as in the following snippet.
    We will see more options in a later section, showing Spark shell in more detail.:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种启动Spark shell的方式，如下面的代码片段所示。我们将在后面的部分中看到更多选项，更详细地展示Spark shell：
- en: 'Default shell on local machine automatically picks local machine as master:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地机器上自动选择本地机器作为主节点的默认shell：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Default shell on local machine specifying local machine as master with `n`
    threads:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地机器上指定本地机器为主节点并使用`n`线程的默认shell：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Default shell on local machine connecting to a specified spark master:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地机器上连接到指定的spark主节点的默认shell：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Default shell on local machine connecting to a YARN cluster using client mode:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地机器上使用客户端模式连接到YARN集群的默认shell：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Default shell on local machine connecting to a YARN cluster using cluster mode:'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地机器上连接到YARN集群使用集群模式的默认shell：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Spark Driver also has a Web UI, which helps you to understand everything about
    the Spark cluster, the executors running, the jobs and tasks, environment variables,
    and cache. The most important use, of course, is to monitor the jobs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Driver也有一个Web UI，可以帮助你了解关于Spark集群、正在运行的执行器、作业和任务、环境变量和缓存的一切。当然，最重要的用途是监视作业。
- en: Launch the Web UI for the local Spark cluster at `http://127.0.0.1:4040/jobs/`
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在`http://127.0.0.1:4040/jobs/`上启动本地Spark集群的Web UI
- en: 'The following is the Jobs tab in the Web UI:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Web UI中的作业选项卡如下：
- en: '![](img/00322.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00322.jpeg)'
- en: 'The following is the tab showing all the executors of the cluster:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是显示集群所有执行器的选项卡：
- en: '![](img/00200.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00200.jpeg)'
- en: Spark on YARN
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark on YARN
- en: In YARN mode, the client communicates with YARN resource manager and gets containers
    to run the Spark execution. You can regard it as something like a mini Spark-cluster
    deployed just for you.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在YARN模式下，客户端与YARN资源管理器通信，并获取容器来运行Spark执行。你可以把它看作是为你部署的一个迷你Spark集群。
- en: Multiple clients interacting with the cluster create their own executors on
    the cluster nodes (node managers). Also, each client will have its own Driver
    component.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与集群交互的多个客户端在集群节点（节点管理器）上创建自己的执行器。此外，每个客户端都将有自己的Driver组件。
- en: When running using YARN, Spark can run either in YARN-client mode or YARN-cluster
    mode.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用YARN时，Spark可以在YARN客户端模式或YARN集群模式下运行。
- en: YARN client mode
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YARN客户端模式
- en: In YARN client mode, the Driver runs on a node outside the cluster (typically
    where the client is). Driver first contacts the resource manager requesting resources
    to run the Spark job. The resource manager allocates a container (container zero)
    and responds to the Driver. The Driver then launches the Spark application master
    in the container zero. The Spark application master then creates the executors
    on the containers allocated by the resource manager. The YARN containers can be
    on any node in the cluster controlled by node manager. So, all allocations are
    managed by resource manager.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在YARN客户端模式中，驱动程序在集群外的节点上运行（通常是客户端所在的地方）。驱动程序首先联系资源管理器请求资源来运行Spark作业。资源管理器分配一个容器（容器零）并回应驱动程序。然后驱动程序在容器零中启动Spark应用程序主节点。Spark应用程序主节点然后在资源管理器分配的容器上创建执行器。YARN容器可以在由节点管理器控制的集群中的任何节点上。因此，所有分配都由资源管理器管理。
- en: Even the Spark application master needs to talk to resource manager to get subsequent
    containers to launch executors.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 即使Spark应用程序主节点也需要与资源管理器通信，以获取后续容器来启动执行器。
- en: 'The following is the YARN-client mode deployment of Spark:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Spark的YARN客户端模式部署：
- en: '![](img/00203.jpeg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00203.jpeg)'
- en: YARN cluster mode
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YARN集群模式
- en: In the YARN cluster mode, the Driver runs on a node inside the cluster (typically
    where the application master is). Client first contacts the resource manager requesting
    resources to run the Spark job. The resource manager allocates a container (container
    zero) and responds to the client. The client then submits the code to the cluster
    and then launches the Driver and Spark application master in the container zero.
    The Driver runs along with the application master and the Spark application master,
    and then creates the executors on the containers allocated by the resource manager.
    The YARN containers can be on any node in the cluster controlled by the node manager.
    So, all allocations are managed by the resource manager.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在YARN集群模式中，驱动程序在集群内的节点上运行（通常是应用程序主节点所在的地方）。客户端首先联系资源管理器请求资源来运行Spark作业。资源管理器分配一个容器（容器零）并回应客户端。然后客户端将代码提交到集群，然后在容器零中启动驱动程序和Spark应用程序主节点。驱动程序与应用程序主节点一起运行，然后在资源管理器分配的容器上创建执行器。YARN容器可以在由节点管理器控制的集群中的任何节点上。因此，所有分配都由资源管理器管理。
- en: Even the Spark application master needs to talk to the resource manager to get
    subsequent containers to launch executors.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 即使Spark应用程序主节点也需要与资源管理器通信，以获取后续容器来启动执行器。
- en: 'The following is the Yarn-cluster mode deployment of Spark:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Spark的Yarn集群模式部署：
- en: '![](img/00206.jpeg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00206.jpeg)'
- en: There is no shell mode in YARN cluster mode, since the Driver itself is running
    inside YARN.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在YARN集群模式中没有shell模式，因为驱动程序本身正在YARN中运行。
- en: Spark on Mesos
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mesos上的Spark
- en: Mesos deployment is similar to Spark standalone mode and the Driver communicates
    with the Mesos Master, which then allocates the resources needed to run the executors.
    As seen in standalone mode, the Driver then communicates with the executors to
    run the job. Thus, the Driver in Mesos deployment first talks to the master and
    then secures the container's request on all the Mesos slave nodes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos部署类似于Spark独立模式，驱动程序与Mesos主节点通信，然后分配所需的资源来运行执行器。与独立模式一样，驱动程序然后与执行器通信以运行作业。因此，Mesos部署中的驱动程序首先与主节点通信，然后在所有Mesos从节点上保证容器的请求。
- en: When the containers are allocated to the Spark job, the Driver then gets the
    executors started up and then runs the code in the executors. When the Spark job
    is completed and Driver exits, the Mesos master is notified, and all the resources
    in the form of containers on the Mesos slave nodes are reclaimed.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当容器分配给Spark作业时，驱动程序然后启动执行器，然后在执行器中运行代码。当Spark作业完成并且驱动程序退出时，Mesos主节点会收到通知，并且在Mesos从节点上以容器的形式的所有资源都会被回收。
- en: Multiple clients interacting with the cluster create their own executors on
    the slave nodes. Also, each client will have its own Driver component. Both client
    and cluster mode are possible just like YARN mode
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与集群交互的多个客户端在从节点上创建自己的执行器。此外，每个客户端都将有自己的驱动程序组件。就像YARN模式一样，客户端模式和集群模式都是可能的
- en: 'The following is the mesos-based deployment of Spark depicting the **Driver**
    connecting to **Mesos Master Node**, which also has the cluster manager of all
    the resources on all the Mesos slaves:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是基于Mesos的Spark部署，描述了**驱动程序**连接到**Mesos主节点**，该主节点还具有所有Mesos从节点上所有资源的集群管理器：
- en: '![](img/00209.jpeg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00209.jpeg)'
- en: Introduction to RDDs
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD介绍
- en: A **Resilient Distributed Dataset** (**RDD**) is an immutable, distributed collection
    of objects. Spark RDDs are resilient or fault tolerant, which enables Spark to
    recover the RDD in the face of failures. Immutability makes the RDDs read-only
    once created. Transformations allow operations on the RDD to create a new RDD
    but the original RDD is never modified once created. This makes RDDs immune to
    race conditions and other synchronization problems.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集**（**RDD**）是不可变的、分布式的对象集合。Spark RDD是具有弹性或容错性的，这使得Spark能够在面对故障时恢复RDD。一旦创建，不可变性使得RDD一旦创建就是只读的。转换允许对RDD进行操作以创建新的RDD，但原始RDD一旦创建就不会被修改。这使得RDD免受竞争条件和其他同步问题的影响。'
- en: The distributed nature of the RDDs works because an RDD only contains a reference
    to the data, whereas the actual data is contained within partitions across the
    nodes in the cluster.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: RDD的分布式特性是因为RDD只包含对数据的引用，而实际数据包含在集群中的节点上的分区中。
- en: Conceptually, a RDD is a distributed collection of elements spread out across
    multiple nodes in the cluster. We can simplify a RDD to better understand by thinking
    of a RDD as a large array of integers distributed across machines.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念上，RDD是分布在集群中多个节点上的元素的分布式集合。我们可以简化RDD以更好地理解，将RDD视为分布在机器上的大型整数数组。
- en: A RDD is actually a dataset that has been partitioned across the cluster and
    the partitioned data could be from **HDFS** (**Hadoop Distributed File System**),
    HBase table, Cassandra table, Amazon S3.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: RDD实际上是一个数据集，已经在集群中进行了分区，分区的数据可能来自HDFS（Hadoop分布式文件系统）、HBase表、Cassandra表、Amazon
    S3。
- en: 'Internally, each RDD is characterized by five main properties:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，每个RDD都具有五个主要属性：
- en: A list of partitions
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区列表
- en: A function for computing each split
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每个分区的函数
- en: A list of dependencies on other RDDs
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对其他RDD的依赖列表
- en: Optionally, a partitioner for key-value RDDs (for example, to say that the RDD
    is hash partitioned)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选地，用于键-值RDD的分区器（例如，指定RDD是哈希分区的）
- en: Optionally, a list of preferred locations to compute each split on (for example,
    block locations for an HDFS file)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选地，计算每个分区的首选位置列表（例如，HDFS文件的块位置）
- en: 'Take a look at the following diagram:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下下面的图表：
- en: '![](img/00212.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00212.jpeg)'
- en: Within your program, the driver treats the RDD object as a handle to the distributed
    data. It is analogous to a pointer to the data, rather than the actual data used,
    to reach the actual data when it is required.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的程序中，驱动程序将RDD对象视为分布式数据的句柄。这类似于指向数据的指针，而不是实际使用的数据，当需要时用于访问实际数据。
- en: The RDD by default uses the hash partitioner to partition the data across the
    cluster. The number of partitions is independent of the number of nodes in the
    cluster. It could very well happen that a single node in the cluster has several
    partitions of data. The number of partitions of data that exist is entirely dependent
    on how many nodes your cluster has and the size of the data. If you look at the
    execution of tasks on the nodes, then a task running on an executor on the worker
    node could be processing the data which is available on the same local node or
    a remote node. This is called the locality of the data, and the executing task
    chooses the most local data possible.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: RDD默认使用哈希分区器在集群中对数据进行分区。分区的数量与集群中节点的数量无关。很可能集群中的单个节点有多个数据分区。存在的数据分区数量完全取决于集群中节点的数量和数据的大小。如果你看节点上任务的执行，那么在worker节点上执行的执行器上的任务可能会处理同一本地节点或远程节点上可用的数据。这被称为数据的局部性，执行任务会选择尽可能本地的数据。
- en: The locality affects the performance of your job significantly. The order of
    preference of locality by default can be shown as
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 局部性会显著影响作业的性能。默认情况下，局部性的优先顺序可以显示为
- en: '`PROCESS_LOCAL > NODE_LOCAL > NO_PREF > RACK_LOCAL > ANY`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`PROCESS_LOCAL > NODE_LOCAL > NO_PREF > RACK_LOCAL > ANY`'
- en: 'There is no guarantee of how many partitions a node might get. This affects
    the processing efficiency of any executor, because if you have too many partitions
    on a single node processing multiple partitions, then the time taken to process
    all the partitions also grows, overloading the cores on the executor, and thus
    slowing down the entire stage of processing, which directly slows down the entire
    job. In fact, partitioning is one of the main tuning factors to improve the performance
    of a Spark job. Refer to the following command:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 节点可能会得到多少分区是没有保证的。这会影响任何执行器的处理效率，因为如果单个节点上有太多分区在处理多个分区，那么处理所有分区所需的时间也会增加，超载执行器上的核心，从而减慢整个处理阶段的速度，直接减慢整个作业的速度。实际上，分区是提高Spark作业性能的主要调优因素之一。参考以下命令：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s look further into what an RDD will look like when we load data. The
    following is an example of how Spark uses different workers to load different
    partitions or splits of the data:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步了解当我们加载数据时RDD会是什么样子。以下是Spark如何使用不同的worker加载数据的示例：
- en: '![](img/00218.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00218.jpeg)'
- en: No matter how the RDD is created, the initial RDD is typically called the base
    RDD and any subsequent RDDs created by the various operations are part of the
    lineage of the RDDs. This is another very important aspect to remember, as the
    secret to fault tolerance and recovery is that the **Driver** maintains the lineage
    of the RDDs and can execute the lineage to recover any lost blocks of the RDDs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 无论RDD是如何创建的，初始RDD通常被称为基础RDD，而由各种操作创建的任何后续RDD都是RDD的血统的一部分。这是另一个非常重要的方面要记住，因为容错和恢复的秘密是**Driver**维护RDD的血统，并且可以执行血统来恢复任何丢失的RDD块。
- en: 'The following is an example showing multiple RDDs created as a result of operations.
    We start with the **Base RDD,** which has 24 items and derive another RDD **carsRDD**
    that contains only items (3) which match cars:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，显示了作为操作结果创建的多个RDD。我们从**Base RDD**开始，它有24个项目，并派生另一个RDD **carsRDD**，其中只包含与汽车匹配的项目（3）：
- en: '![](img/00227.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00227.jpeg)'
- en: The number of partitions does not change during such operations, as each executor
    applies the filter transformation in-memory, generating a new RDD partition corresponding
    to the original RDD partition.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些操作期间，分区的数量不会改变，因为每个执行器都会在内存中应用过滤转换，生成与原始RDD分区对应的新RDD分区。
- en: Next, we will see how to create RDDs
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何创建RDDs
- en: RDD Creation
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD创建
- en: An RDD is the fundamental object used in Apache Spark. They are immutable collections
    representing datasets and have the inbuilt capability of reliability and failure
    recovery. By nature, RDDs create new RDDs upon any operation such as transformation
    or action. RDDs also store the lineage which is used to recover from failures.
    We have also seen in the previous chapter some details about how RDDs can be created
    and what kind of operations can be applied to RDDs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: RDD是Apache Spark中使用的基本对象。它们是不可变的集合，代表数据集，并具有内置的可靠性和故障恢复能力。从本质上讲，RDD在进行任何操作（如转换或动作）时会创建新的RDD。RDD还存储了用于从故障中恢复的血统。我们在上一章中也看到了有关如何创建RDD以及可以应用于RDD的操作的一些细节。
- en: 'An RDD can be created in several ways:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过多种方式创建RDD：
- en: Parallelizing a collection
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行化集合
- en: Reading data from an external source
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从外部源读取数据
- en: Transformation of an existing RDD
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有RDD的转换
- en: Streaming API
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式API
- en: Parallelizing a collection
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行化集合
- en: Parallelizing a collection can be done by calling `parallelize()` on the collection
    inside the driver program. The driver, when it tries to parallelize a collection,
    splits the collection into partitions and distributes the data partitions across
    the cluster.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在驱动程序内部的集合上调用`parallelize()`来并行化集合。当驱动程序尝试并行化集合时，它将集合分割成分区，并将数据分区分布到集群中。
- en: The following is an RDD to create an RDD from a sequence of numbers using the
    SparkContext and the `parallelize()` function. The `parallelize()` function essentially
    splits the Sequence of numbers into a distributed collection otherwise known as
    an RDD.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用SparkContext和`parallelize()`函数从数字序列创建RDD的RDD。`parallelize()`函数基本上将数字序列分割成分布式集合，也称为RDD。
- en: '[PRE11]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Reading data from an external source
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从外部源读取数据
- en: A second method for creating an RDD is by reading data from an external distributed
    source such as Amazon S3, Cassandra, HDFS, and so on. For example, if you are
    creating an RDD from HDFS, then the distributed blocks in HDFS are all read by
    the individual nodes in the Spark cluster.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 创建RDD的第二种方法是从外部分布式源（如Amazon S3、Cassandra、HDFS等）读取数据。例如，如果您从HDFS创建RDD，则Spark集群中的各个节点都会读取HDFS中的分布式块。
- en: Each of the nodes in the Spark cluster is essentially doing its own input-output
    operations and each node is independently reading one or more blocks from the
    HDFS blocks. In general, Spark makes the best effort to put as much RDD as possible
    into memory. There is the capability to `cache` the data to reduce the input-output
    operations by enabling nodes in the spark cluster to avoid repeated reading operations,
    say from the HDFS blocks, which might be remote to the Spark cluster. There are
    a whole bunch of caching strategies that can be used within your Spark program,
    which we will examine later in a section for caching.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Spark集群中的每个节点基本上都在进行自己的输入输出操作，每个节点都独立地从HDFS块中读取一个或多个块。一般来说，Spark会尽最大努力将尽可能多的RDD放入内存中。有能力通过在Spark集群中启用节点来缓存数据，以减少输入输出操作，避免重复读取操作，比如从可能远离Spark集群的HDFS块。在您的Spark程序中可以使用一整套缓存策略，我们将在缓存部分后面详细讨论。
- en: The following is an RDD of text lines loading from a text file using the Spark
    Context and the `textFile()` function. The `textFile` function loads the input
    data as a text file (each newline `\n` terminated portion becomes an element in
    the RDD). The function call also automatically uses HadoopRDD (shown in next chapter)
    to detect and load the data in the form of several partitions as needed, distributed
    across the cluster.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从文本文件加载的文本行RDD，使用Spark Context和`textFile()`函数。`textFile`函数将输入数据加载为文本文件（每个换行符`\n`终止的部分成为RDD中的一个元素）。该函数调用还自动使用HadoopRDD（在下一章中显示）来检测和加载所需的分区形式的数据，分布在集群中。
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Transformation of an existing RDD
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现有RDD的转换
- en: RDDs, by nature, are immutable; hence, your RDDs could be created by applying
    transformations on any existing RDD. Filter is one typical example of a transformation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: RDD本质上是不可变的；因此，可以通过对任何现有RDD应用转换来创建您的RDD。过滤器是转换的一个典型例子。
- en: The following is a simple `rdd` of integers and transformation by multiplying
    each integer by `2`. Again, we use the `SparkContext` and parallelize function
    to create a sequence of integers into an RDD by distributing the Sequence in the
    form of partitions. Then, we use the `map()` function to transform the RDD into
    another RDD by multiplying each number by `2`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的整数`rdd`，通过将每个整数乘以`2`进行转换。同样，我们使用`SparkContext`和`parallelize`函数将整数序列分布为分区形式的RDD。然后，我们使用`map()`函数将RDD转换为另一个RDD，将每个数字乘以`2`。
- en: '[PRE13]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Streaming API
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式API
- en: RDDs can also be created via spark streaming. These RDDs are called Discretized
    Stream RDDs (DStream RDDs).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: RDD也可以通过spark streaming创建。这些RDD称为离散流RDD（DStream RDD）。
- en: We will look at this further in [Chapter 9](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c),
    *Stream Me Up, Scotty - Spark Streaming*.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第9章](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c)中进一步讨论这个问题，*Stream
    Me Up, Scotty - Spark Streaming*。
- en: In the next section, we will create RDDs and explore some of the operations
    using Spark-Shell.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将创建RDD并使用Spark-Shell探索一些操作。
- en: Using the Spark shell
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark shell
- en: Spark shell provides a simple way to perform interactive analysis of data. It
    also enables you to learn the Spark APIs by quickly trying out various APIs. In
    addition, the similarity to Scala shell and support for Scala APIs also lets you
    also adapt quickly to Scala language constructs and make better use of Spark APIs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Spark shell提供了一种简单的方式来执行数据的交互式分析。它还使您能够通过快速尝试各种API来学习Spark API。此外，与Scala shell的相似性和对Scala
    API的支持还让您能够快速适应Scala语言构造，并更好地利用Spark API。
- en: Spark shell implements the concept of **read-evaluate-print-loop** (**REPL**),
    which allows you to interact with the shell by typing in code which is evaluated.
    The result is then printed on the console, without needing to be compiled, so
    building executable code.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Spark shell实现了**读取-求值-打印-循环**（**REPL**）的概念，允许您通过键入要评估的代码与shell进行交互。然后在控制台上打印结果，无需编译即可构建可执行代码。
- en: 'Start it by running the following in the directory where you installed Spark:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装Spark的目录中运行以下命令启动它：
- en: '[PRE14]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Spark shell launches and the Spark shell automatically creates the `SparkSession`
    and `SparkContext` objects. The `SparkSession` is available as a Spark and the
    `SparkContext` is available as sc.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Spark shell启动时，会自动创建`SparkSession`和`SparkContext`对象。`SparkSession`可作为Spark使用，`SparkContext`可作为sc使用。
- en: '`spark-shell` can be launched with several options as shown in the following
    snippet (the most important ones are in bold):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark-shell`可以通过以下片段中显示的几个选项启动（最重要的选项用粗体显示）：'
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can also submit Spark code in the form of executable Java jars so that the
    job is executed in a cluster. Usually, you do this once you have reached a workable
    solution using the shell.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以以可执行的Java jar的形式提交Spark代码，以便在集群中执行作业。通常，您在使用shell达到可行解决方案后才这样做。
- en: Use `./bin/spark-submit` when submitting a Spark job to a cluster (local, YARN,
    and Mesos).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在提交Spark作业到集群（本地、YARN和Mesos）时，请使用`./bin/spark-submit`。
- en: 'The following are Shell Commands (the most important ones are in bold):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Shell命令（最重要的命令用粗体标出）：
- en: '[PRE16]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Using the spark-shell, we will now load some data as an RDD:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 使用spark-shell，我们现在将一些数据加载为RDD：
- en: '[PRE17]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As you see, we are running the commands one by one. Alternately, we can also
    paste the commands:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们正在逐个运行命令。或者，我们也可以粘贴命令：
- en: '[PRE18]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the next section, we will go deeper into the operations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入研究这些操作。
- en: Actions and Transformations
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作和转换
- en: RDDs are immutable and every operation creates a new RDD. Now, the two main
    operations that you can perform on an RDD are **Transformations** and **Actions**.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs是不可变的，每个操作都会创建一个新的RDD。现在，你可以在RDD上执行的两个主要操作是**转换**和**动作**。
- en: '**Transformations** change the elements in the RDD such as splitting the input
    element, filtering out elements, and performing calculations of some sort. Several
    transformations can be performed in a sequence; however no execution takes place
    during the planning.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**转换**改变RDD中的元素，例如拆分输入元素、过滤元素和执行某种计算。可以按顺序执行多个转换；但是在规划期间不会执行任何操作。'
- en: For transformations, Spark adds them to a DAG of computation and, only when
    driver requests some data, does this DAG actually gets executed. This is called
    *lazy* evaluation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于转换，Spark将它们添加到计算的DAG中，只有当驱动程序请求一些数据时，这个DAG才会实际执行。这被称为*延迟*评估。
- en: The reasoning behind the lazy evaluation is that Spark can look at all the transformations
    and plan the execution, making use of the understanding the Driver has of all
    the operations. For instance, if a filter transformation is applied immediately
    after some other transformation, Spark will optimize the execution so that each
    Executor performs the transformations on each partition of data efficiently. Now,
    this is possible only when Spark is waiting until something needs to be executed.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟评估的原因是，Spark可以查看所有的转换并计划执行，利用驱动程序对所有操作的理解。例如，如果筛选转换立即应用于其他一些转换之后，Spark将优化执行，以便每个执行器有效地对数据的每个分区执行转换。现在，只有当Spark等待执行时才有可能。
- en: '**Actions** are operations, which actually trigger the computations. Until
    an action operation is encountered, the execution plan within the spark program
    is created in the form of a DAG and does nothing. Clearly, there could be several
    transformations of all sorts within the execution plan, but nothing happens until
    you perform an action.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**动作**是实际触发计算的操作。在遇到动作操作之前，Spark程序内的执行计划以DAG的形式创建并且不执行任何操作。显然，在执行计划中可能有各种转换，但在执行动作之前什么也不会发生。'
- en: 'The following is a depiction of the various operations on some arbitrary data
    where we just wanted to remove all pens and bikes and just count cars**.** Each
    print statement is an action which triggers the execution of all the transformation
    steps in the DAG based execution plan until that point as shown in the following
    diagram:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对一些任意数据的各种操作的描述，我们只想删除所有的笔和自行车，只计算汽车的数量**。**每个打印语句都是一个动作，触发DAG执行计划中到那一点的所有转换步骤的执行，如下图所示：
- en: '![](img/00230.jpeg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00230.jpeg)'
- en: For example, an action count on a directed acyclic graph of transformations
    triggers the execution of the transformation all the way up to the base RDD. If
    there is another action performed, then there is a new chain of executions that
    could take place. This is a clear case of why any caching that could be done at
    different stages in the directed acyclic graph will greatly speed up the next
    execution of the program. Another way that the execution is optimized is through
    the reuse of the shuffle files from the previous execution.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对转换的有向无环图执行计数动作会触发执行直到基本RDD的所有转换。如果执行了另一个动作，那么可能会发生新的执行链。这清楚地说明了为什么在有向无环图的不同阶段可以进行任何缓存，这将极大地加快程序的下一次执行。另一种优化执行的方式是通过重用上一次执行的洗牌文件。
- en: Another example is the collect action that collects or pulls all the data from
    all the nodes to the driver. You could use a partial function when invoking collect
    to selectively pull the data.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是collect动作，它从所有节点收集或拉取所有数据到驱动程序。在调用collect时，您可以使用部分函数有选择地拉取数据。
- en: Transformations
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换
- en: '**Transformations** creates a new RDD from an existing RDD by applying transformation
    logic to each of the elements in the existing RDD. Some of the transformation
    functions involve splitting the element, filtering out elements, and performing
    calculations of some sort. Several transformations can be performed in a sequence.
    However, no execution takes place during the planning.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**转换**通过将转换逻辑应用于现有RDD中的每个元素，从现有RDD创建新的RDD。一些转换函数涉及拆分元素、过滤元素和执行某种计算。可以按顺序执行多个转换。但是，在规划期间不会执行任何操作。'
- en: Transformations can be divided into four categories, as follows.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 转换可以分为四类，如下所示。
- en: General transformations
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用转换
- en: '**General transformations** are transformation functions that handle most of
    the general purpose use cases, applying the transformational logic to existing
    RDDs and generating a new RDD. The common operations of aggregation, filters and
    so on are all known as general transformations.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用转换**是处理大多数通用用例的转换函数，将转换逻辑应用于现有的RDD并生成新的RDD。聚合、过滤等常见操作都称为通用转换。'
- en: 'Examples of general transformation functions are:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通用转换函数的示例包括：
- en: '`map`'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map`'
- en: '`filter`'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter`'
- en: '`flatMap`'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flatMap`'
- en: '`groupByKey`'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groupByKey`'
- en: '`sortByKey`'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sortByKey`'
- en: '`combineByKey`'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`combineByKey`'
- en: Math/Statistical transformations
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学/统计转换
- en: Mathematical or statistical transformations are transformation functions which
    handle some statistical functionality, and which usually apply some mathematical
    or statistical operation on existing RDDs, generating a new RDD. Sampling is a
    great example of this and is used often in Spark programs.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 数学或统计转换是处理一些统计功能的转换函数，通常对现有的RDD应用一些数学或统计操作，生成一个新的RDD。抽样是一个很好的例子，在Spark程序中经常使用。
- en: 'Examples of such transformations are:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 此类转换的示例包括：
- en: '`sampleByKey`'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampleByKey`'
- en: '``randomSplit``'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '``randomSplit``'
- en: Set theory/relational transformations
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集合理论/关系转换
- en: Set theory/relational transformations are transformation functions, which handle
    transformations like Joins of datasets and other relational algebraic functionality
    such as `cogroup`. These functions work by applying the transformational logic
    to existing RDDs and generating a new RDD.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 集合理论/关系转换是处理数据集的连接和其他关系代数功能（如`cogroup`）的转换函数。这些函数通过将转换逻辑应用于现有的RDD并生成新的RDD来工作。
- en: 'Examples of such transformations are:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 此类转换的示例包括：
- en: '`cogroup`'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cogroup`'
- en: '`join`'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`join`'
- en: '`subtractByKey`'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subtractByKey`'
- en: '`fullOuterJoin`'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fullOuterJoin`'
- en: '`leftOuterJoin`'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`leftOuterJoin`'
- en: '`rightOuterJoin`'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rightOuterJoin`'
- en: Data structure-based transformations
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于数据结构的转换
- en: Data structure-based transformations are transformation functions which operate
    on the underlying data structures of the RDD, the partitions in the RDD. In these
    functions, you can directly work on partitions without directly touching the elements/data
    inside the RDD. These are essential in any Spark program beyond the simple programs
    where you need more control of the partitions and distribution of partitions in
    the cluster. Typically, performance improvements can be realized by redistributing
    the data partitions according to the cluster state and the size of the data, and
    the exact use case requirements.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 基于数据结构的转换是操作RDD的基础数据结构，即RDD中的分区的转换函数。在这些函数中，您可以直接在分区上工作，而不直接触及RDD内部的元素/数据。这些在任何Spark程序中都是必不可少的，超出了简单程序的范围，您需要更多地控制分区和分区在集群中的分布。通常，通过根据集群状态和数据大小以及确切的用例要求重新分配数据分区，可以实现性能改进。
- en: 'Examples of such transformations are:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此类转换的示例包括：
- en: '`partitionBy`'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`partitionBy`'
- en: '`repartition`'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repartition`'
- en: '`zipwithIndex`'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zipwithIndex`'
- en: '`coalesce`'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`coalesce`'
- en: 'The following is the list of transformation functions as available in the latest
    Spark 2.1.1:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是最新Spark 2.1.1中可用的转换函数列表：
- en: '| Transformation | Meaning |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 转换 | 意义 |'
- en: '| `map(func)` | Return a new distributed dataset formed by passing each element
    of the source through a function `func`. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| `map(func)` | 通过将源数据的每个元素传递给函数`func`来返回一个新的分布式数据集。|'
- en: '| `filter(func)` | Return a new dataset formed by selecting those elements
    of the source on which func returns true. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| `filter(func)` | 返回一个由源数据集中func返回true的元素组成的新数据集。|'
- en: '| `flatMap(func)` | Similar to map, but each input item can be mapped to 0
    or more output items (so `func` should return a `Seq` rather than a single item).
    |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| `flatMap(func)` | 类似于map，但每个输入项可以映射到0个或多个输出项（因此`func`应返回`Seq`而不是单个项）。|'
- en: '| `mapPartitions(func)` | Similar to map, but runs separately on each partition
    (block) of the RDD, so `func` must be of type `Iterator<T> => Iterator<U>` when
    running on an RDD of type `T`. |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| `mapPartitions(func)` | 类似于map，但在RDD的每个分区（块）上单独运行，因此当在类型为`T`的RDD上运行时，`func`必须是`Iterator<T>
    => Iterator<U>`类型。|'
- en: '| `mapPartitionsWithIndex(func)` | Similar to `mapPartitions`, but also provides
    `func` with an integer value representing the index of the partition, so `func`
    must be of type `(Int, Iterator<T>) => Iterator<U>` when running on an RDD of
    type `T`. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| `mapPartitionsWithIndex(func)` | 类似于`mapPartitions`，但还为`func`提供一个整数值，表示分区的索引，因此当在类型为`T`的RDD上运行时，`func`必须是`(Int,
    Iterator<T>) => Iterator<U>`类型。|'
- en: '| `sample(withReplacement, fraction, seed)` | Sample a fraction fraction of
    the data, with or without replacement, using a given random number generator seed.
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| `sample(withReplacement, fraction, seed)` | 使用给定的随机数生成器种子，对数据的一部分进行抽样，可以有或没有替换。|'
- en: '| `union(otherDataset)` | Return a new dataset that contains the union of the
    elements in the source dataset and the argument. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `union(otherDataset)` | 返回一个包含源数据集和参数中元素并集的新数据集。|'
- en: '| `intersection(otherDataset)` | Return a new RDD that contains the intersection
    of elements in the source dataset and the argument. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `intersection(otherDataset)` | 返回一个包含源数据集和参数中元素交集的新RDD。|'
- en: '| `distinct([numTasks]))` | Return a new dataset that contains the distinct
    elements of the source dataset. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `distinct([numTasks]))` | 返回一个包含源数据集的不同元素的新数据集。|'
- en: '| `groupByKey([numTasks])` | When called on a dataset of `(K, V)` pairs, returns
    a dataset of `(K, Iterable<V>)` pairs. Note: If you are grouping in order to perform
    an aggregation (such as a sum or average) over each key, using `reduceByKey` or
    `aggregateByKey` will yield much better performance.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '| `groupByKey([numTasks])` | 当在`(K, V)`对的数据集上调用时，返回一个`(K, Iterable<V>)`对的数据集。注意：如果要对每个键执行聚合（例如求和或平均值），使用`reduceByKey`或`aggregateByKey`将获得更好的性能。'
- en: 'Note: By default, the level of parallelism in the output depends on the number
    of partitions of the parent RDD. You can pass an optional `numTasks` argument
    to set a different number of tasks. |'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：默认情况下，输出中的并行级别取决于父RDD的分区数。您可以传递一个可选的`numTasks`参数来设置不同数量的任务。|
- en: '| reduceByKey(func, [numTasks]) | When called on a dataset of `(K, V)` pairs,
    returns a dataset of `(K, V)` pairs where the values for each key are aggregated
    using the given `reduce` function `func`, which must be of type `(V,V) => V`.
    As in `groupByKey`, the number of reduce tasks is configurable through an optional
    second argument. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| reduceByKey(func, [numTasks]) | 当在`(K, V)`对的数据集上调用时，返回一个`(K, V)`对的数据集，其中每个键的值使用给定的`reduce`函数`func`进行聚合，`func`必须是`(V,V)
    => V`类型。与`groupByKey`一样，通过可选的第二个参数可以配置reduce任务的数量。|'
- en: '| `aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])` | When called on a
    dataset of `(K, V)` pairs, returns a dataset of `(K, U)` pairs where the values
    for each key are aggregated using the given combine functions and a neutral *zero*
    value. Allows an aggregated value type that is different than the input value
    type, while avoiding unnecessary allocations. As in `groupByKey`, the number of
    reduce tasks is configurable through an optional second argument. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])` | 当在`(K, V)`对的数据集上调用时，返回使用给定的组合函数和中性“零”值对每个键的值进行聚合的`(K,
    U)`对的数据集。允许聚合值类型与输入值类型不同，同时避免不必要的分配。与`groupByKey`一样，通过可选的第二个参数可以配置减少任务的数量。'
- en: '| `sortByKey([ascending], [numTasks])` | When called on a dataset of `(K, V)`
    pairs where `K` implements ordered, returns a dataset of `(K, V)` pairs sorted
    by keys in ascending or descending order, as specified in the boolean ascending
    argument. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `sortByKey([ascending], [numTasks])` | 当在实现有序的`(K, V)`对的数据集上调用时，返回按键按升序或降序排序的`(K,
    V)`对的数据集，如布尔值升序参数中指定的那样。'
- en: '| `join(otherDataset, [numTasks])` | When called on datasets of type `(K, V)`
    and `(K, W)`, returns a dataset of `(K, (V, W))` pairs with all pairs of elements
    for each key. Outer joins are supported through `leftOuterJoin`, `rightOuterJoin`,
    and `fullOuterJoin`. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `join(otherDataset, [numTasks])` | 当在类型为`(K, V)`和`(K, W)`的数据集上调用时，返回每个键的所有元素对的`(K,
    (V, W))`对的数据集。通过`leftOuterJoin`、`rightOuterJoin`和`fullOuterJoin`支持外连接。'
- en: '| `cogroup(otherDataset, [numTasks])` | When called on datasets of type `(K,
    V)` and `(K, W)`, returns a dataset of `(K, (Iterable<V>, Iterable<W>))` tuples.
    This operation is also called `groupWith`. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `cogroup(otherDataset, [numTasks])` | 当在类型为`(K, V)`和`(K, W)`的数据集上调用时，返回`(K,
    (Iterable<V>, Iterable<W>))`元组的数据集。此操作也称为`groupWith`。'
- en: '| `cartesian(otherDataset)` | When called on datasets of types `T` and `U`,
    returns a dataset of `(T, U)` pairs (all pairs of elements). |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| `cartesian(otherDataset)` | 当在类型为`T`和`U`的数据集上调用时，返回`(T, U)`对的数据集（所有元素的所有对）。
    |'
- en: '| `pipe(command, [envVars])` | Pipe each partition of the RDD through a shell
    command, for example, a Perl or bash script. RDD elements are written to the process''s
    `stdin`, and lines output to its `stdout` are returned as an RDD of strings. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| `pipe(command, [envVars])` | 将RDD的每个分区通过shell命令（例如Perl或bash脚本）进行管道传输。RDD元素被写入进程的`stdin`，并且输出到其`stdout`的行将作为字符串的RDD返回。'
- en: '| `coalesce(numPartitions)` | Decrease the number of partitions in the RDD
    to `numPartitions`. Useful for running operations more efficiently after filtering
    down a large dataset. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| `coalesce(numPartitions)` | 将RDD中的分区数减少到`numPartitions`。在筛选大型数据集后更有效地运行操作时非常有用。
    |'
- en: '| `repartition(numPartitions)` | Reshuffle the data in the RDD randomly to
    create either more or fewer partitions and balance it across them. This always
    shuffles all data over the network. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| `repartition(numPartitions)` | 随机重排RDD中的数据，以创建更多或更少的分区并在它们之间平衡。这总是通过网络洗牌所有数据。'
- en: '| `repartitionAndSortWithinPartitions(partitioner)` | Repartition the RDD according
    to the given partitioner and, within each resulting partition, sort records by
    their keys. This is more efficient than calling `repartition` and then sorting
    within each partition because it can push the sorting down into the shuffle machinery.
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `repartitionAndSortWithinPartitions(partitioner)` | 根据给定的分区器重新分区RDD，并在每个生成的分区内按其键对记录进行排序。这比调用`repartition`然后在每个分区内排序更有效，因为它可以将排序推入洗牌机制中。'
- en: 'We will illustrate the most common transformations:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将说明最常见的转换：
- en: map function
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: map函数
- en: '`map` applies transformation function to input partitions to generate output
    partitions in the output RDD.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '`map`将转换函数应用于输入分区，以生成输出RDD中的输出分区。'
- en: 'As shown in the following snippet, this is how we can map an RDD of a text
    file to an RDD with lengths of the lines of text:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如下面的代码片段所示，这是我们如何将文本文件的RDD映射到文本行的长度的RDD：
- en: '[PRE19]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following diagram explains of how `map()` works. You can see that each
    partition of the RDD results in a new partition in a new RDD essentially applying
    the transformation to all elements of the RDD:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 下图解释了`map()`的工作原理。您可以看到RDD的每个分区都会在新的RDD中产生一个新的分区，从而在RDD的所有元素上应用转换：
- en: '![](img/00236.jpeg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00236.jpeg)'
- en: flatMap function
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: flatMap函数
- en: '`flatMap()` applies transformation function to input partitions to generate
    output partitions in the output RDD just like `map()` function. However, `flatMap()`
    also flattens any collection in the input RDD elements.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMap()`将转换函数应用于输入分区，以生成输出RDD中的输出分区，就像`map()`函数一样。但是，`flatMap()`还会展平输入RDD元素中的任何集合。'
- en: '[PRE20]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following diagram explains how `flatMap()` works. You can see that each
    partition of the RDD results in a new partition in a new RDD, essentially applying
    the transformation to all elements of the RDD:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 下图解释了`flatMap()`的工作原理。您可以看到RDD的每个分区都会在新的RDD中产生一个新的分区，从而在RDD的所有元素上应用转换：
- en: '![](img/00239.jpeg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00239.jpeg)'
- en: filter function
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: filter函数
- en: '`filter` applies transformation function to input partitions to generate filtered
    output partitions in the output RDD.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter` 将转换函数应用于输入分区，以生成输出RDD中的过滤后的输出分区。'
- en: '[PRE22]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The following diagram explains how `filter` works. You can see that each partition
    of the RDD results in a new partition in a new RDD, essentially applying the filter
    transformation on all elements of the RDD.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 下图解释了`filter`的工作原理。您可以看到RDD的每个分区都会在新的RDD中产生一个新的分区，从而在RDD的所有元素上应用过滤转换。
- en: Note that the partitions do not change, and some partitions could be empty too,
    when applying filter
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，分区不会改变，应用筛选时有些分区可能也是空的
- en: '![](img/00242.jpeg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00242.jpeg)'
- en: coalesce
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: coalesce
- en: '`coalesce` applies a `transformation` function to input partitions to combine
    the input partitions into fewer partitions in the output RDD.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`coalesce`将转换函数应用于输入分区，以将输入分区合并为输出RDD中的较少分区。'
- en: 'As shown in the following code snippet, this is how we can combine all partitions
    to a single partition:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如下面的代码片段所示，这是我们如何将所有分区合并为单个分区：
- en: '[PRE24]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following diagram explains how `coalesce` works. You can see that a new
    RDD is created from the original RDD essentially reducing the number of partitions
    by combining them as needed:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表解释了`coalesce`的工作原理。您可以看到，从原始RDD创建了一个新的RDD，基本上通过根据需要组合它们来减少分区的数量：
- en: '![](img/00248.jpeg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00248.jpeg)'
- en: repartition
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新分区
- en: '`repartition` applies a `transformation` function to input partitions to `repartition`
    the input into fewer or more output partitions in the output RDD.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '`repartition`将`transformation`函数应用于输入分区，以将输入重新分区为输出RDD中的更少或更多的输出分区。'
- en: 'As shown in the following code snippet, this is how we can map an RDD of a
    text file to an RDD with more partitions:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如下面的代码片段所示，这是我们如何将文本文件的RDD映射到具有更多分区的RDD：
- en: '[PRE25]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following diagram explains how `repartition` works. You can see that a
    new RDD is created from the original RDD, essentially redistributing the partitions
    by combining/splitting them as needed:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表解释了`repartition`的工作原理。您可以看到，从原始RDD创建了一个新的RDD，基本上通过根据需要组合/拆分分区来重新分配分区：
- en: '![](img/00254.jpeg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00254.jpeg)'
- en: Actions
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作
- en: Action triggers the entire **DAG** (**Directed Acyclic Graph**) of transformations
    built so far to be materialized by running the code blocks and functions. All
    operations are now executed as the DAG specifies.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 动作触发到目前为止构建的所有转换的整个**DAG**（**有向无环图**）通过运行代码块和函数来实现。现在，所有操作都按照DAG指定的方式执行。
- en: 'There are two kinds of action operations:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的动作操作：
- en: '**Driver**: One kind of action is the driver action such as collect count,
    count by key, and so on. Each such action performs some calculations on the remote
    executor and pulls the data back into the driver.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**驱动程序**：一种动作是驱动程序动作，例如收集计数、按键计数等。每个此类动作在远程执行器上执行一些计算，并将数据拉回驱动程序。'
- en: Driver-based action has the problem that actions on large datasets can easily
    overwhelm the memory available on the driver taking down the application, so you
    should use the driver involved actions judiciously
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 基于驱动程序的动作存在一个问题，即对大型数据集的操作可能会轻松地压倒驱动程序上可用的内存，从而使应用程序崩溃，因此应谨慎使用涉及驱动程序的动作
- en: '**Distributed**: Another kind of action is a distributed action, which is executed
    on the nodes in the cluster. An example of such a distributed action is `saveAsTextfile`.
    This is the most common action operation due to the desirable distributed nature
    of the operation.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式**：另一种动作是分布式动作，它在集群中的节点上执行。这种分布式动作的示例是`saveAsTextfile`。由于操作的理想分布式性质，这是最常见的动作操作。'
- en: 'The following is the list of action functions as available in the latest Spark
    2.1.1:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是最新的Spark 2.1.1中可用的动作函数列表：
- en: '| Action | Meaning |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | 意义 |'
- en: '| `reduce(func)` | Aggregate the elements of the dataset using a function `func`
    (which takes two arguments and returns one). The function should be commutative
    and associative so that it can be computed correctly in parallel. |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| `reduce(func)` | 使用函数`func`（接受两个参数并返回一个参数）聚合数据集的元素。该函数应该是可交换和可结合的，以便可以正确并行计算。
    |'
- en: '| `collect()` | Return all the elements of the dataset as an array at the driver
    program. This is usually useful after a filter or other operation that returns
    a sufficiently small subset of the data. |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| `collect()` | 将数据集的所有元素作为数组返回到驱动程序。这通常在过滤或其他返回数据的操作之后非常有用，这些操作返回了数据的足够小的子集。
    |'
- en: '| `count()` | Return the number of elements in the dataset. |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| `count()` | 返回数据集中元素的数量。 |'
- en: '| `first()` | Return the first element of the dataset (similar to `take(1)`).
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| `first()` | 返回数据集的第一个元素（类似于`take(1)`）。 |'
- en: '| `take(n)` | Return an array with the first `n` elements of the dataset. |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| `take(n)` | 返回数据集的前`n`个元素的数组。 |'
- en: '| `takeSample(withReplacement, num, [seed])` | Return an array with a random
    sample of `num` elements of the dataset, with or without replacement, optionally
    pre-specifying a random number generator seed. |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| `takeSample(withReplacement, num, [seed])` | 返回数据集的`num`个元素的随机样本数组，可替换或不可替换，可选择预先指定随机数生成器种子。
    |'
- en: '| `takeOrdered(n, [ordering])` | Return the first `n` elements of the RDD using
    either their natural order or a custom comparator. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| `takeOrdered(n, [ordering])` | 使用它们的自然顺序或自定义比较器返回RDD的前`n`个元素。 |'
- en: '| `saveAsTextFile(path)` | Write the elements of the dataset as a text file
    (or set of text files) in a given directory in the local filesystem, HDFS or any
    other Hadoop-supported file system. Spark will call `toString` on each element
    to convert it to a line of text in the file. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| `saveAsTextFile(path)` | 将数据集的元素作为文本文件（或一组文本文件）写入本地文件系统、HDFS或任何其他支持Hadoop的文件系统中的给定目录。Spark将对每个元素调用`toString`以将其转换为文件中的文本行。
    |'
- en: '| `saveAsSequenceFile(path)` (Java and Scala) | Write the elements of the dataset
    as a Hadoop SequenceFile in a given path in the local filesystem, HDFS, or any
    other Hadoop-supported file system. This is available on RDDs of key-value pairs
    that implement Hadoop''s `Writable` interface. In Scala, it is also available
    on types that are implicitly convertible to `Writable` (Spark includes conversions
    for basic types like `Int`, `Double`, `String`, and so on). |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| `saveAsSequenceFile(path)`（Java和Scala） | 将数据集的元素作为Hadoop SequenceFile写入本地文件系统、HDFS或任何其他支持Hadoop的文件系统中的给定路径。这适用于实现Hadoop的`Writable`接口的键值对RDD。在Scala中，它也适用于隐式转换为`Writable`的类型（Spark包括基本类型如`Int`、`Double`、`String`等的转换）。
    |'
- en: '| `saveAsObjectFile(path)` (Java and Scala) | Write the elements of the dataset
    in a simple format using Java serialization, which can then be loaded using `SparkContext.objectFile()`.
    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| `saveAsObjectFile(path)`（Java和Scala） | 使用Java序列化以简单格式写入数据集的元素，然后可以使用`SparkContext.objectFile()`加载。
    |'
- en: '| `countByKey()` | Only available on RDDs of type `(K, V)`. Returns a hashmap
    of `(K, Int)` pairs with the count of each key. |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| `countByKey()` | 仅适用于类型为`(K, V)`的RDD。返回一个`(K, Int)`对的哈希映射，其中包含每个键的计数。 |'
- en: '| `foreach(func)` | Run a function `func` on each element of the dataset. This
    is usually done for side effects such as updating an accumulator ([http://spark.apache.org/docs/latest/programming-guide.html#accumulators](http://spark.apache.org/docs/latest/programming-guide.html#accumulators))
    or interacting with external storage systems. Note: modifying variables other
    than accumulators outside of the `foreach()` may result in undefined behavior.
    See understanding closures ([http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka](http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka))
    [for more details.](http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka)
    |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| `foreach(func)` | 对数据集的每个元素运行函数`func`。这通常用于诸如更新累加器（[http://spark.apache.org/docs/latest/programming-guide.html#accumulators](http://spark.apache.org/docs/latest/programming-guide.html#accumulators)）或与外部存储系统交互等副作用。注意：在`foreach()`之外修改除累加器之外的变量可能导致未定义的行为。有关更多详细信息，请参见理解闭包（[http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka](http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka)）。'
- en: reduce
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: reduce
- en: '`reduce()` applies the reduce function to all the elements in the RDD and sends
    it to the Driver.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce()`将reduce函数应用于RDD中的所有元素，并将其发送到Driver。'
- en: The following is example code to illustrate this. You can use `SparkContext`
    and the parallelize function to create an RDD from a sequence of integers. Then
    you can add up all the numbers of the RDD using the `reduce` function on the RDD.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例代码，用于说明这一点。您可以使用`SparkContext`和parallelize函数从整数序列创建一个RDD。然后，您可以使用RDD上的`reduce`函数将RDD中所有数字相加。
- en: Since this is an action, the results are printed as soon as you run the `reduce`
    function.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个动作，所以一旦运行`reduce`函数，结果就会被打印出来。
- en: 'Shown below is the code to build a simple RDD from a small array of numbers
    and then perform a reduce operation on the RDD:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示了从一组小数字构建一个简单RDD的代码，然后在RDD上执行reduce操作：
- en: '[PRE26]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The following diagram is an illustration of `reduce()`. Driver runs the reduce
    function on the executors and collects the results in the end.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示是`reduce()`的说明。Driver在执行器上运行reduce函数，并在最后收集结果。
- en: '![](img/00257.jpeg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00257.jpeg)'
- en: count
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: count
- en: '`count()` simply counts the number of elements in the RDD and sends it to the
    Driver.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`count()`简单地计算RDD中的元素数量并将其发送到Driver。'
- en: The following is an example of this function. We created an RDD from a Sequence
    of integers using SparkContext and parallelize function and then called count
    on the RDD to print the number of elements in the RDD.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这个函数的一个例子。我们使用SparkContext和parallelize函数从整数序列创建了一个RDD，然后调用RDD上的count函数来打印RDD中元素的数量。
- en: '[PRE27]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The following is an illustration of `count()`. The Driver asks each of the executor/task
    to count the number of elements in the partition being handled by the task and
    then adds up the counts from all the tasks together at the Driver level.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`count()`的说明。Driver要求每个执行器/任务计算任务处理的分区中元素的数量，然后在Driver级别将所有任务的计数相加。
- en: '![](img/00260.jpeg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00260.jpeg)'
- en: collect
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: collect
- en: '`collect()` simply collects all elements in the RDD and sends it to the Driver.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '`collect()`简单地收集RDD中的所有元素并将其发送到Driver。'
- en: Shown here is an example showing what collect function essentially does. When
    you call collect on an RDD, the Driver collects all the elements of the RDD by
    pulling them into the Driver.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了collect函数的一个例子。当你在RDD上调用collect时，Driver会通过将RDD的所有元素拉到Driver上来收集它们。
- en: Calling collect on large RDDs will cause out-of-memory issues on the Driver.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型RDD上调用collect会导致Driver出现内存不足的问题。
- en: 'Shown below is the code to collect the content of the RDD and display it:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示了收集RDD的内容并显示它的代码：
- en: '[PRE28]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The following is an illustration of `collect()`. Using collect, the Driver is
    pulling all the elements of the RDD from all partitions.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`collect()`的说明。使用collect，Driver从所有分区中拉取RDD的所有元素。
- en: '![](img/00027.jpeg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00027.jpeg)'
- en: Caching
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存
- en: Caching enables Spark to persist data across computations and operations. In
    fact, this is one of the most important technique in Spark to speed up computations,
    particularly when dealing with iterative computations.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存使Spark能够在计算和操作之间持久保存数据。事实上，这是Spark中最重要的技术之一，可以加速计算，特别是在处理迭代计算时。
- en: Caching works by storing the RDD as much as possible in the memory. If there
    is not enough memory then the current data in storage is evicted, as per LRU policy.
    If the data being asked to cache is larger than the memory available, the performance
    will come down because Disk will be used instead of memory.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存通过尽可能多地将RDD存储在内存中来工作。如果内存不足，那么根据LRU策略会将当前存储中的数据清除。如果要缓存的数据大于可用内存，性能将下降，因为将使用磁盘而不是内存。
- en: You can mark an RDD as cached using either `persist()` or `cache()`
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`persist()`或`cache()`将RDD标记为已缓存
- en: '`cache()` is simply a synonym for persist`(MEMORY_ONLY)`'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '`cache()`只是`persist`(MEMORY_ONLY)的同义词'
- en: '`persist` can use memory or disk or both:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '`persist`可以使用内存或磁盘或两者：'
- en: '[PRE29]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following are the possible values for Storage level:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是存储级别的可能值：
- en: '| Storage Level | Meaning |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 存储级别 | 含义 |'
- en: '| `MEMORY_ONLY` | Stores RDD as deserialized Java objects in the JVM. If the
    RDD does not fit in memory, some partitions will not be cached and will be recomputed
    on the fly each time they''re needed. This is the default level. |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_ONLY` | 将RDD存储为JVM中的反序列化Java对象。如果RDD不适合内存，则某些分区将不会被缓存，并且每次需要时都会在飞行中重新计算。这是默认级别。
    |'
- en: '| `MEMORY_AND_DISK` | Stores RDD as deserialized Java objects in the JVM. If
    the RDD does not fit in memory, store the partitions that don''t fit on disk,
    and read them from there when they''re needed. |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_AND_DISK` | 将RDD存储为JVM中的反序列化Java对象。如果RDD不适合内存，则将不适合内存的分区存储在磁盘上，并在需要时从磁盘中读取它们。'
- en: '| `MEMORY_ONLY_SER` (Java and Scala) | Stores RDD as serialized Java objects
    (one byte array per partition). This is generally more space-efficient than deserialized
    objects, especially when using a fast serializer, but more CPU-intensive to read.
    |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_ONLY_SER`（Java和Scala）|将RDD存储为序列化的Java对象（每个分区一个字节数组）。通常情况下，这比反序列化对象更节省空间，特别是在使用快速序列化器时，但读取时更消耗CPU。'
- en: '| `MEMORY_AND_DISK_SER` (Java and Scala) | Similar to `MEMORY_ONLY_SER`, but
    spill partitions that don''t fit in memory to disk instead of recomputing them
    on the fly each time they''re needed. |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_AND_DISK_SER`（Java和Scala）|类似于`MEMORY_ONLY_SER`，但将不适合内存的分区溢出到磁盘，而不是每次需要时动态重新计算它们。'
- en: '| `DISK_ONLY` | Store the RDD partitions only on disk. |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| `DISK_ONLY` | 仅将RDD分区存储在磁盘上。'
- en: '| `MEMORY_ONLY_2`, `MEMORY_AND_DISK_2`, and so on. | Same as the preceding
    levels, but replicate each partition on two cluster nodes. |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_ONLY_2`，`MEMORY_AND_DISK_2`等|与前面的级别相同，但在两个集群节点上复制每个分区。'
- en: '| `OFF_HEAP` (experimental) | Similar to `MEMORY_ONLY_SER`, but store the data
    in off-heap memory. This requires off-heap memory to be enabled. |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| `OFF_HEAP`（实验性）|类似于`MEMORY_ONLY_SER`，但将数据存储在堆外内存中。这需要启用堆外内存。'
- en: Storage level to choose depends on the situation
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的存储级别取决于情况
- en: If RDDs fit into memory, use `MEMORY_ONLY` as that's the fastest option for
    execution performance
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果RDD适合内存，则使用`MEMORY_ONLY`，因为这是执行性能最快的选项
- en: Try `MEMORY_ONLY_SER` is there are serializable objects being used in order
    to make the objects smaller
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试`MEMORY_ONLY_SER`，如果使用了可序列化对象，以使对象更小
- en: '`DISK` should not be used unless your computations are expensive.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除非您的计算成本很高，否则不应使用`DISK`。
- en: Use replicated storage for best fault tolerance if you can spare the additional
    memory needed. This will prevent recomputation of lost partitions for best availability.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果可以承受额外的内存，使用复制存储以获得最佳的容错性。这将防止丢失分区的重新计算，以获得最佳的可用性。
- en: '`unpersist()` simply frees up the cached content.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '`unpersist()`只是释放缓存的内容。'
- en: 'The following are examples of how to call `persist()` function using different
    types of storage (memory or disk):'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用不同类型的存储（内存或磁盘）调用`persist()`函数的示例：
- en: '[PRE30]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The following is an illustration of the performance improvement we get by caching.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是缓存带来的性能改进的示例。
- en: 'First, we will run the code:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将运行代码：
- en: '[PRE31]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You can use the WebUI to look at the improvement achieved as shown in the following
    screenshots:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用WebUI查看所示的改进，如以下屏幕截图所示：
- en: '![](img/00052.jpeg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00052.jpeg)'
- en: Loading and saving data
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载和保存数据
- en: Loading data into an RDD and Saving an RDD onto an output system both support
    several different methods. We will cover the most common ones in this section.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据加载到RDD和将RDD保存到输出系统都支持多种不同的方法。我们将在本节中介绍最常见的方法。
- en: Loading data
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: Loading data into an RDD can be done by using `SparkContext`. Some of the most
    common methods are:.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`SparkContext`可以将数据加载到RDD中。一些最常见的方法是：
- en: '`textFile`'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`textFile`'
- en: '`wholeTextFiles`'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wholeTextFiles`'
- en: '`load` from a JDBC datasource'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从JDBC数据源加载
- en: textFile
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: textFile
- en: '`textFile()` can be used to load textFiles into an RDD and each line becomes
    an element in the RDD.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '`textFile()`可用于将textFiles加载到RDD中，每行成为RDD中的一个元素。'
- en: '[PRE32]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following is an example of loading a `textfile` into an RDD using `textFile()`:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用`textFile()`将`textfile`加载到RDD中的示例：
- en: '[PRE33]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: wholeTextFiles
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: wholeTextFiles
- en: '`wholeTextFiles()` can be used to load multiple text files into a paired RDD
    containing pairs `<filename, textOfFile>` representing the filename and the entire
    content of the file. This is useful when loading multiple small text files and
    is different from `textFile` API because when whole `TextFiles()` is used, the
    entire content of the file is loaded as a single record:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '`wholeTextFiles()`可用于将多个文本文件加载到包含对`<filename，textOfFile>`的配对RDD中，表示文件名和文件的整个内容。这在加载多个小文本文件时很有用，并且与`textFile`
    API不同，因为使用整个`TextFiles()`时，文件的整个内容将作为单个记录加载：'
- en: '[PRE34]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following is an example of loading a `textfile` into an RDD using `wholeTextFiles()`:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用`wholeTextFiles()`将`textfile`加载到RDD中的示例：
- en: '[PRE35]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Load from a JDBC Datasource
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从JDBC数据源加载
- en: 'You can load data from an external data source which supports **Java Database
    Connectivity** (**JDBC**). Using a JDBC driver, you can connect to a relational
    database such as Mysql and load the content of a table into Spark as shown in
    in the following code snippet:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从支持**Java数据库连接**（**JDBC**）的外部数据源加载数据。使用JDBC驱动程序，您可以连接到关系数据库，如Mysql，并将表的内容加载到Spark中，如下面的代码片段所示：
- en: '[PRE36]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following is an example of loading from a JDBC datasource:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从JDBC数据源加载的示例：
- en: '[PRE37]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Saving RDD
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存RDD
- en: 'Saving data from an RDD into a file system can be done by either:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据从RDD保存到文件系统可以通过以下方式之一完成：
- en: '`saveAsTextFile`'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`saveAsTextFile`'
- en: '`saveAsObjectFile`'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`saveAsObjectFile`'
- en: The following is an example of saving an RDD to a text file
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是将RDD保存到文本文件的示例
- en: '[PRE38]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: There are many more ways of loading and saving data, particularly when integrating
    with HBase, Cassandra and so on.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成HBase、Cassandra等时，还有许多其他加载和保存数据的方法。
- en: Summary
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the internals of Apache Spark, what RDDs are,
    DAGs and lineages of RDDs, Transformations, and Actions. We also looked at various
    deployment modes of Apache Spark using standalone, YARN, and Mesos deployments.
    We also did a local install on our local machine and then looked at Spark shell
    and how it can be used to interact with Spark.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了Apache Spark的内部工作原理，RDD是什么，DAG和RDD的血统，转换和操作。我们还看了Apache Spark使用独立、YARN和Mesos部署的各种部署模式。我们还在本地机器上进行了本地安装，然后看了Spark
    shell以及如何与Spark进行交互。
- en: In addition, we also looked at loading data into RDDs and saving RDDs to external
    systems as well as the secret sauce of Spark's phenomenal performance, the caching
    functionality, and how we can use memory and/or disk to optimize the performance.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还研究了将数据加载到RDD中以及将RDD保存到外部系统以及Spark卓越性能的秘密武器，缓存功能以及如何使用内存和/或磁盘来优化性能。
- en: In the next chapter, we will dig deeper into RDD API and how it all works in
    [Chapter 7](part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c), *Special RDD
    Operations*.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入研究RDD API以及它在《第7章》*特殊RDD操作*中的全部工作原理。
