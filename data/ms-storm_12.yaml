- en: Twitter Tweet Collection and Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter推文收集和机器学习
- en: In the previous chapter, we covered how we can create a log processing application
    with Storm and Kafka.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了如何使用Storm和Kafka创建日志处理应用程序。
- en: In this chapter, we are covering another important use case of Storm machine
    learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖Storm机器学习的另一个重要用例。
- en: 'The following are the major topics covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主要主题如下：
- en: Exploring machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索机器学习
- en: Using Kafka producer to store the tweets in a Kafka cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kafka生产者将推文存储在Kafka集群中
- en: Using Kafka Spout to read the data from Kafka
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kafka Spout从Kafka读取数据
- en: Using Storm Bolt to filter the tweets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Storm Bolt来过滤推文
- en: Using Storm Bolt to calculate the sentiments of tweets
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Storm Bolt来计算推文的情感
- en: Deployment of topologies
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拓扑的部署
- en: Exploring machine learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索机器学习
- en: Machine learning is a branch of applied computer science in which we build models
    of real-world phenomenon based on existing data available for analysis, and then
    using that model, predicting certain characteristics of data never seen before
    by the model. Machine learning has become a very important component of real-time
    applications as decisions need to be made in real time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是应用计算机科学的一个分支，在这个分支中，我们基于现有的可供分析的数据构建真实世界现象的模型，然后使用该模型，预测模型以前从未见过的数据的某些特征。机器学习已经成为实时应用程序非常重要的组成部分，因为需要实时做出决策。
- en: 'Graphically, the process of machine learning can be represented by the following
    figure:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从图形上看，机器学习的过程可以用以下图示表示：
- en: '![](img/00064.gif)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00064.gif)'
- en: The process of building the model from data is called **training** in machine
    learning terminology. Training can happen in real time on a stream of data or
    it can be done on historical data. When the training is done in real time, the
    model evolves over time with the changed data. This kind of learning is referred
    to as *online* learning, and when the model is updated every once in a while,
    by running the training algorithm on a new set of data, it is called *offline*
    learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据构建模型的过程在机器学习术语中称为**训练**。训练可以实时在数据流上进行，也可以在历史数据上进行。当训练实时进行时，模型随着数据的变化而随时间演变。这种学习被称为*在线*学习，当模型定期更新，通过在新数据集上运行训练算法时，被称为*离线*学习。
- en: When we talk about machine learning in the context of Storm, more often than
    not we are talking about online learning algorithms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论Storm上的机器学习时，往往我们谈论的是在线学习算法。
- en: 'The following are some of the real-world applications of machine learning:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是机器学习的一些真实应用：
- en: Online ad optimization
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线广告优化
- en: New article clustering
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新文章聚类
- en: Spam detection
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垃圾邮件检测
- en: Computer vision
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: Sentiment analysis
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析
- en: Twitter sentiment analysis
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter情感分析
- en: 'We will be dividing the sentiments use case into two parts:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将情感用例分为两部分：
- en: Collecting tweets from Twitter and storing them in Kafka
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Twitter收集推文并将其存储在Kafka中
- en: Reading the data from Kafka, calculating the sentiments, and storing them in
    HDFS
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Kafka读取数据，计算情感，并将其存储在HDFS中
- en: '![](img/00065.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00065.jpeg)'
- en: Using Kafka producer to store the tweets in a Kafka cluster
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kafka生产者将推文存储在Kafka集群中
- en: In this section, we are going to cover how we can stream the tweets from Twitter
    using the twitter streaming API. We are also going to cover how we can store the
    fetched tweets in Kafka for later processing through Storm.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何使用Twitter流API从Twitter中获取推文。我们还将介绍如何将获取的推文存储在Kafka中，以便通过Storm进行后续处理。
- en: 'We are assuming you already have a twitter account, and that the consumer key
    and access token are generated for your application. You can refer to: [https://bdthemes.com/support/knowledge-base/generate-api-key-consumer-token-access-key-twitter-oauth/](https://bdthemes.com/support/knowledge-base/generate-api-key-consumer-token-access-key-twitter-oauth/)
    to generate a consumer key and access token. Take the following steps:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您已经拥有Twitter账户，并且为您的应用程序生成了消费者密钥和访问令牌。您可以参考：[https://bdthemes.com/support/knowledge-base/generate-api-key-consumer-token-access-key-twitter-oauth/](https://bdthemes.com/support/knowledge-base/generate-api-key-consumer-token-access-key-twitter-oauth/)
    生成消费者密钥和访问令牌。请按照以下步骤进行：
- en: Create a new maven project with `groupId`, `com.stormadvance` and `artifactId`,
    `kafka_producer_twitter`.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`groupId`为`com.stormadvance`和`artifactId`为`kafka_producer_twitter`创建一个新的maven项目。
- en: 'Add the following dependencies to the `pom.xml` file. We are adding the Kafka
    and Twitter streaming Maven dependencies to `pom.xml` to support the Kafka Producer
    and the streaming tweets from Twitter:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下依赖项添加到`pom.xml`文件中。我们正在向`pom.xml`添加Kafka和Twitter流Maven依赖项，以支持Kafka生产者和从Twitter流式传输推文。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, we need to create a class, `TwitterData`, that contains the code to consume/stream
    data from Twitter and publish it to the Kafka cluster. We are assuming you already
    have a running Kafka cluster and topic, `twitterData`, created in the Kafka cluster.
    Please refer to [Chapter 8](part0137.html#42KT20-6149dd15e07b443593cc93f2eb31ee7b),
    *Integration of Storm and Kafka*, for information on the installation of the Kafka
    cluster and the creation of a Kafka topic if they do not exist.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个名为`TwitterData`的类，其中包含从Twitter获取/流式传输数据并将其发布到Kafka集群的代码。我们假设您已经有一个运行中的Kafka集群和在Kafka集群中创建的`twitterData`主题。有关Kafka集群的安装和创建Kafka主题的信息，请参阅[第8章](part0137.html#42KT20-6149dd15e07b443593cc93f2eb31ee7b)，*Storm和Kafka的集成*。
- en: The class contains an instance of the `twitter4j.conf.ConfigurationBuilder`
    class; we need to set the access token and consumer keys in configuration, as
    mentioned in the source code.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该类包含`twitter4j.conf.ConfigurationBuilder`类的一个实例；我们需要在配置中设置访问令牌和消费者密钥，如源代码中所述。
- en: 'The `twitter4j.StatusListener` class returns the continuous stream of tweets
    inside the `onStatus()` method. We are using the Kafka Producer code inside the
    `onStatus()` method to publish the tweets in Kafka. The following is the source
    code for the `TwitterData` class:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`twitter4j.StatusListener`类在`onStatus()`方法中返回推文的连续流。我们在`onStatus()`方法中使用Kafka
    Producer代码来发布推文到Kafka。以下是`TwitterData`类的源代码：'
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Use valid Kafka properties before executing the `TwitterData` class.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行`TwitterData`类之前，请使用有效的Kafka属性。
- en: After executing the preceding class, the user will have a real-time stream of
    Twitter tweets in Kafka. In the next section, we are going to cover how we can
    use Storm to calculate the sentiments of the collected tweets.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行上述类之后，用户将在Kafka中获得Twitter推文的实时流。在下一节中，我们将介绍如何使用Storm来计算收集到的推文的情感。
- en: Kafka spout, sentiments bolt, and HDFS bolt
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka spout，情感bolt和HDFS bolt
- en: 'In this section, we are going to write/configure a Kafka spout to consume the
    tweets from the Kafka cluster. We are going to use the open source Storm spout
    connectors for consuming the data from Kafka:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将编写/配置一个Kafka spout来消费来自Kafka集群的推文。我们将使用开源的Storm spout连接器来从Kafka消费数据：
- en: Create a new maven project with the `groupID` as `com.stormadvance` and `artifactId` as `Kafka_twitter_topology`.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`groupID`为`com.stormadvance`和`artifactId`为`Kafka_twitter_topology`创建一个新的maven项目。
- en: 'Add the following maven dependencies to the `pom.xml` file:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下maven依赖项添加到`pom.xml`文件中：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a `StormHDFSTopology` class inside the `com.stormadvance.Kafka_twitter_topology.topology`
    package and add the following dependencies to specify that the Kafka spout consumes
    the data from the `twitterData` topic:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`com.stormadvance.Kafka_twitter_topology.topology`包内创建一个`StormHDFSTopology`类，并添加以下依赖项以指定Kafka
    spout从`twitterData`主题中消费数据：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create a `JSONParsingBolt` class inside the package''s `com.stormadvance.Kafka_twitter_topology.bolt`
    class to extract the tweet text from the JSON twitter tweet that the JSON received
    from Twitter:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`com.stormadvance.Kafka_twitter_topology.bolt`包内创建一个`JSONParsingBolt`类，以从Twitter接收的JSON推文中提取推文文本：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create a `SentimentBolt` class inside the package''s `com.stormadvance.Kafka_twitter_topology.sentiments`
    class to create the sentiments of each tweet. We are using a dictionary file to
    find out if the words used in tweets are positive or negative and calculate the
    sentiments of an entire tweet. The following is the source code of the class:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`com.stormadvance.Kafka_twitter_topology.sentiments`包内创建一个`SentimentBolt`类，以创建每条推文的情感。我们使用字典文件来查找推文中使用的词语是积极的还是消极的，并计算整条推文的情感。以下是该类的源代码：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We need to store the sentiments in an HDFS for generating charts or feature
    analysis. Next, we add the following code inside the `StormHDFSTopology` class
    to chain the spout and bolts:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要将情感存储在HDFS中以生成图表或特征分析。接下来，在`StormHDFSTopology`类中添加以下代码以链接spout和bolts：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following is the complete code of the `StormHDFSTopology` class:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是`StormHDFSTopology`类的完整代码：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, we can create the JAR for the entire project and deploy it on a Storm cluster
    as defined in [Chapter 2](part0034.html#10DJ40-6149dd15e07b443593cc93f2eb31ee7b),
    *Storm Deployment, Topology Development, and Topology Options* in this book.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以为整个项目创建JAR并根据本书中的[第2章](part0034.html#10DJ40-6149dd15e07b443593cc93f2eb31ee7b)，*Storm部署、拓扑开发和拓扑选项*中定义的方式部署到Storm集群。
- en: Summary
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this section, we covered how we can read Twitter tweets using the Twitter
    streaming API, how we can process the tweets to calculate the tweet text from
    inputted JSON records, calculate the sentiments of the tweets, and store the final
    output in HDFS.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了如何使用Twitter流API读取Twitter推文，如何处理推文以计算输入JSON记录中的推文文本，计算推文的情感，并将最终输出存储在HDFS中。
- en: With this, we come to the end of this book. Over the course of this book, we
    have come a long way from taking our first steps with Apache Storm to developing
    real-world applications with it. Here, we would like to summarize everything that
    we have learned.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们来到了本书的结尾。在本书的过程中，我们已经从开始使用Apache Storm迈出了一大步，发展成为了真实世界应用程序的开发者。在这里，我们想总结一下我们所学到的一切。
- en: We introduced you to the basic concepts and components of Storm, and covered
    how we can write and deploy/run the topology in both local and clustered mode.
    We also walked through the basic commands of Storm, and covered how we can modify
    the parallelism of the Storm topology at runtime. We also dedicated an entire
    chapter to monitoring Storm, which is an area often neglected during development,
    but is a critical part of any production setting. You also learned about Trident,
    which is an abstraction of the low-level Storm API that can be used to develop
    more complex topologies and maintain the application state.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向您介绍了Storm的基本概念和组件，并介绍了如何在本地和集群模式下编写和部署/运行拓扑。我们还介绍了Storm的基本命令，并介绍了如何在运行时修改Storm拓扑的并行性。我们还专门介绍了监控Storm的整个章节，这在开发过程中经常被忽视，但是对于任何生产环境来说都是至关重要的部分。您还了解了Trident，这是低级Storm
    API的抽象，可用于开发更复杂的拓扑并维护应用程序状态。
- en: No enterprise application can be developed in a single technology, and so our
    next step was to see how we could integrate Storm with other big data tools and
    technologies. We saw a specific implementation of Storm with Kafka, Hadoop, HBase,
    and Redis. Most of the big data applications use Ganglia as a centralized monitoring
    tool, hence we also covered how we could monitor the Storm cluster through JMX
    and Ganglia.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何企业应用程序可以仅使用一种技术开发，因此我们的下一步是看看如何将Storm与其他大数据工具和技术集成。我们看到了Storm与Kafka、Hadoop、HBase和Redis的特定实现。大多数大数据应用程序使用Ganglia作为集中监控工具，因此我们还介绍了如何通过JMX和Ganglia监控Storm集群。
- en: You also learned about various patterns used to integrate diverse data sources
    with Storm. Finally, in both [Chapter 11](part0182.html#5DI6C0-6149dd15e07b443593cc93f2eb31ee7b),
    *Apache Log Processing with Storm*, and this chapter, we implemented two case
    studies in Apache Storm that can serve as a starting point for developing more
    complex applications.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习了有关使用各种模式将不同数据源与Storm集成的知识。最后，在[第11章](part0182.html#5DI6C0-6149dd15e07b443593cc93f2eb31ee7b)《使用Storm进行Apache日志处理》和本章中，我们实施了两个Apache
    Storm中的案例研究，这可以作为开发更复杂应用程序的起点。
- en: We hope that reading this book has been a fruitful journey for you, and that
    you developed a basic understanding of Storm and, in general, the various aspects
    of developing a real-time stream processing application. Apache Storm is turning
    into a de-facto standard for stream processing, and we hope that this book will
    act as a catalyst for you to jumpstart the exciting journey of building a real-time
    stream processing application.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望阅读本书对你来说是一次富有成效的旅程，并且你对Storm以及一般实时流处理应用程序开发的各个方面有了基本的了解。Apache Storm正在成为流处理的事实标准，我们希望本书能够成为你启动构建实时流处理应用程序的激动人心旅程的催化剂。
