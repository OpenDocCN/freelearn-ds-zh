- en: Chapter 7. Extending Spark with H2O
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。用H2O扩展Spark
- en: 'H2O is an open source system, developed in Java by [http://h2o.ai/](http://h2o.ai/)
    for machine learning. It offers a rich set of machine learning algorithms, and
    a web-based data processing user interface. It offers the ability to develop in
    a range of languages: Java, Scala, Python, and R. It also has the ability to interface
    to Spark, HDFS, Amazon S3, SQL, and NoSQL databases. This chapter will concentrate
    on H2O''s integration with Apache Spark using the **Sparkling Water** component
    of H2O. A simple example, developed in Scala, will be used, based on real data
    to create a deep-learning model. This chapter will:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: H2O是一个由[http://h2o.ai/](http://h2o.ai/)开发的开源系统，用于机器学习。它提供了丰富的机器学习算法和基于Web的数据处理用户界面。它提供了使用多种语言开发的能力：Java、Scala、Python和R。它还具有与Spark、HDFS、Amazon
    S3、SQL和NoSQL数据库进行接口的能力。本章将集中讨论H2O与Apache Spark的集成，使用H2O的**Sparkling Water**组件。将使用Scala开发一个简单的示例，基于真实数据创建一个深度学习模型。本章将：
- en: Examine the H2O functionality
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查H2O功能
- en: Consider the necessary Spark H2O environment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑必要的Spark H2O环境
- en: Examine the Sparkling Water architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查Sparkling Water架构
- en: Introduce and use the H2O Flow interface
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍并使用H2O Flow界面
- en: Introduce deep learning with an example
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过示例介绍深度学习
- en: Consider performance tuning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑性能调优
- en: Examine data quality
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查数据质量
- en: The next step will be to provide an overview of the H2O functionality, and the
    Sparkling Water architecture that will be used in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步将概述H2O功能和本章中将使用的Sparkling Water架构。
- en: Overview
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'Since it is only possible to examine, and use, a small amount of H2O''s functionality
    in this chapter, I thought that it would be useful to provide a list of all of
    the functional areas that it covers. This list is taken from [http://h2o.ai/](http://h2o.ai/)
    website at [http://h2o.ai/product/algorithms/](http://h2o.ai/product/algorithms/)
    and is based upon munging/wrangling data, modeling using the data, and scoring
    the resulting models:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章只能检查和使用H2O功能的一小部分，我认为提供一个功能区域列表将是有用的。此列表取自[http://h2o.ai/](http://h2o.ai/)网站的[http://h2o.ai/product/algorithms/](http://h2o.ai/product/algorithms/)，基于数据整理、建模和对结果模型进行评分：
- en: '| Process | Model | The score tool |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 过程 | 模型 | 评分工具 |'
- en: '| --- | --- | --- |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Data profiling | Generalized Linear Models (GLM) | Predict |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 数据概要分析 | 广义线性模型（GLM） | 预测 |'
- en: '| Summary statistics | Decision trees | Confusion Matrix |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 摘要统计 | 决策树 | 混淆矩阵 |'
- en: '| Aggregate, filter, bin, and derive columns | Gradient Boosting (GBM) | AUC
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 聚合、过滤、分箱和派生列 | 梯度提升（GBM） | AUC |'
- en: '| Slice, log transform, and anonymize | K-Means | Hit Ratio |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 切片、对数变换和匿名化 | K均值 | 命中率 |'
- en: '| Variable creation | Anomaly detection | PCA Score |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 变量创建 | 异常检测 | PCA得分 |'
- en: '| PCA | Deep learning | Multi Model Scoring |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| PCA | 深度学习 | 多模型评分 |'
- en: '| Training and validation sampling plan | Naïve Bayes |   |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 训练和验证抽样计划 | 朴素贝叶斯 |   |'
- en: '|   | Grid search |   |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|   | 网格搜索 |   |'
- en: The following section will explain the environment used for the Spark and H2O
    examples in this chapter and it will also explain some of the problems encountered.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将解释本章中Spark和H2O示例使用的环境，并解释遇到的一些问题。
- en: The processing environment
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理环境
- en: If any of you have examined my web-based blogs, or read my first book, *Big
    Data Made Easy*, you will see that I am interested in Big Data integration, and
    how the big data tools connect. None of these systems exist in isolation. The
    data will start upstream, be processed in Spark plus H2O, and then the result
    will be stored, or moved to the next step in the ETL chain. Given this idea in
    this example, I will use Cloudera CDH HDFS for storage, and source my data from
    there. I could just as easily use S3, an SQL or NoSQL database.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你们中有人查看过我的基于Web的博客，或者阅读过我的第一本书《大数据简化》，你会发现我对大数据集成和大数据工具的连接很感兴趣。这些系统都不是独立存在的。数据将从上游开始，在Spark加上H2O中进行处理，然后结果将被存储，或者移动到ETL链中的下一步。根据这个想法，在这个示例中，我将使用Cloudera
    CDH HDFS进行存储，并从那里获取我的数据。我也可以很容易地使用S3、SQL或NoSQL数据库。
- en: 'At the point of starting the development work for this chapter, I had a Cloudera
    CDH 4.1.3 cluster installed and working. I also had various Spark versions installed,
    and available for use. They are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本章的开发工作时，我安装并使用了Cloudera CDH 4.1.3集群。我还安装了各种Spark版本，并可供使用。它们如下：
- en: Spark 1.0 installed as CentOS services
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Spark 1.0安装为CentOS服务
- en: Spark 1.2 binary downloaded and installed
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载并安装的Spark 1.2二进制文件
- en: Spark 1.3 built from a source snapshot
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从源快照构建的Spark 1.3
- en: 'I thought that I would experiment to see which combinations of Spark, and Hadoop
    I could get to work together. I downloaded Sparkling water at [http://h2o-release.s3.amazonaws.com/sparkling-water/master/98/index.html](http://h2o-release.s3.amazonaws.com/sparkling-water/master/98/index.html)
    and used the 0.2.12-95 version. I found that the 1.0 Spark version worked with
    H2O, but the Spark libraries were missing. Some of the functionality that was
    used in many of the Sparkling Water-based examples was available. Spark versions
    1.2 and 1.3 caused the following error to occur:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我会进行实验，看看哪些Spark和Hadoop的组合可以一起工作。我在[http://h2o-release.s3.amazonaws.com/sparkling-water/master/98/index.html](http://h2o-release.s3.amazonaws.com/sparkling-water/master/98/index.html)下载了Sparkling
    Water的0.2.12-95版本。我发现1.0版本的Spark与H2O一起工作，但缺少Spark库。许多基于Sparkling Water的示例中使用的一些功能是可用的。Spark版本1.2和1.3导致出现以下错误：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The Spark master port number, although correctly configured in Spark, was not
    being picked up, and so the H2O-based application could not connect to Spark.
    After discussing the issue with the guys at H2O, I decided to upgrade to an H2O
    certified version of both Hadoop and Spark. The recommended system versions that
    should be used are available at [http://h2o.ai/product/recommended-systems-for-h2o/](http://h2o.ai/product/recommended-systems-for-h2o/).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Spark中正确配置了主端口号，但没有被识别，因此H2O应用无法连接到Spark。在与H2O的工作人员讨论了这个问题后，我决定升级到H2O认证版本的Hadoop和Spark。应该使用的推荐系统版本可在[http://h2o.ai/product/recommended-systems-for-h2o/](http://h2o.ai/product/recommended-systems-for-h2o/)上找到。
- en: I upgraded my CDH cluster from version 5.1.3 to version 5.3 using the Cloudera
    Manager interface parcels page. This automatically provided Spark 1.2—the version
    that has been integrated into the CDH cluster. This solved all the H2O-related
    issues, and provided me with an H2O-certified Hadoop and Spark environment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用Cloudera Manager界面的包管理页面将我的CDH集群从版本5.1.3升级到版本5.3。这自动提供了Spark 1.2——这个版本已经集成到CDH集群中。这解决了所有与H2O相关的问题，并为我提供了一个经过H2O认证的Hadoop和Spark环境。
- en: Installing H2O
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装H2O
- en: For completeness, I will show you how I downloaded, installed, and used H2O.
    Although, I finally settled on version 0.2.12-95, I first downloaded and used
    0.2.12-92\. This section is based on the earlier install, but the approach used
    to source the software is the same. The download link changes over time so follow
    the Sparkling Water download option at [http://h2o.ai/download/](http://h2o.ai/download/).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整起见，我将向您展示如何下载、安装和使用H2O。尽管我最终选择了版本0.2.12-95，但我首先下载并使用了0.2.12-92。本节基于早期的安装，但用于获取软件的方法是相同的。下载链接会随时间变化，因此请在[http://h2o.ai/download/](http://h2o.ai/download/)上关注Sparkling
    Water下载选项。
- en: 'This will source the zipped Sparkling water release, as shown by the CentOS
    Linux long file listing here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这将获取压缩的Sparkling Water发布，如下所示的CentOS Linux长文件列表：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This zipped release file is unpacked using the Linux `unzip` command, and it
    results in a sparkling water release file tree:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个压缩的发布文件使用Linux的`unzip`命令解压，得到一个Sparkling Water发布文件树：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'I have moved the release tree to the `/usr/local/` area using the root account,
    and created a simple symbolic link to the release called `h2o`. This means that
    my H2O-based build can refer to this link, and it doesn''t need to change as new
    versions of sparkling water are sourced. I have also made sure, using the Linux
    `chmod` command, that my development account, hadoop, has access to the release:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我已将发布树移动到`/usr/local/`目录下，使用root账户，并创建了一个名为`h2o`的简单符号链接到发布版本。这意味着我的基于H2O的构建可以引用这个链接，并且不需要随着新版本的Sparkling
    Water的获取而更改。我还使用Linux的`chmod`命令确保我的开发账户hadoop可以访问发布版本。
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The release has been installed on all the nodes of my Hadoop CDH clusters.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 发布已安装在我的Hadoop CDH集群的所有节点上。
- en: The build environment
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建环境
- en: 'From past examples, you will know that I favor SBT as a build tool for developing
    Scala source examples. I have created a development environment on the Linux CentOS
    6.5 server called `hc2r1m2` using the hadoop development account. The development
    directory is called `h2o_spark_1_2`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从过去的例子中，您会知道我偏爱SBT作为开发Scala源代码示例的构建工具。我已在Linux CentOS 6.5服务器上使用hadoop开发账户创建了一个名为`hc2r1m2`的开发环境。开发目录名为`h2o_spark_1_2`。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'My SBT build configuration file named `h2o.sbt` is located here; it contains
    the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我的SBT构建配置文件名为`h2o.sbt`，位于这里；它包含以下内容：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: I have provided SBT configuration examples in the previous chapters, so I won't
    go into the line-by line-detail here. I have used the file-based URLs to define
    the library dependencies, and have sourced the Hadoop JAR files from the Cloudera
    parcel path for the CDH install. The Sparkling Water JAR path is defined as `/usr/local/h2o/`
    that was just created.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我在之前的章节中提供了SBT配置示例，所以我不会在这里逐行详细介绍。我使用基于文件的URL来定义库依赖，并从Cloudera parcel路径获取CDH安装的Hadoop
    JAR文件。Sparkling Water JAR路径被定义为`/usr/local/h2o/`，这刚刚创建。
- en: 'I use a Bash script called `run_h2o.bash` within this development directory
    to execute my H2O-based example code. It takes the application class name as a
    parameter, and is shown below:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这个开发目录中使用一个名为`run_h2o.bash`的Bash脚本来执行基于H2O的示例代码。它将应用程序类名作为参数，并如下所示：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This example of Spark application submission has already been covered, so again,
    I won't get into the detail. Setting the executor memory at a correct value was
    critical to avoiding out-of-memory issues and performance problems. This will
    be examined in the *Performance Tuning* section.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Spark应用程序提交的示例已经涵盖过了，所以我不会详细介绍。将执行器内存设置为正确的值对避免内存不足问题和性能问题至关重要。这将在*性能调优*部分进行讨论。
- en: As in the previous examples, the application Scala code is located in the `src/main/scala`
    subdirectory, under the `development` directory level. The next section will examine
    the Apache Spark, and the H2O architecture.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的例子一样，应用Scala代码位于`development`目录级别下的`src/main/scala`子目录中。下一节将检查Apache Spark和H2O的架构。
- en: Architecture
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: The diagrams in this section have been sourced from the [http://h2o.ai/](http://h2o.ai/)
    web site at [http://h2o.ai/blog/2014/09/how-sparkling-water-brings-h2o-to-spark/](http://
    http://h2o.ai/blog/2014/09/how-sparkling-water-brings-h2o-to-spark/) to provide
    a clear method of describing the way in which H2O Sparkling Water can be used
    to extend the functionality of Apache Spark. Both, H2O and Spark are open source
    systems. Spark MLlib contains a great deal of functionality, while H2O extends
    this with a wide range of extra functionality, including deep learning. It offers
    tools to *munge* (transform), model, and score the data. It also offers a web-based
    user interface to interact with.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的图表来自[http://h2o.ai/](http://h2o.ai/)网站，网址为[http://h2o.ai/blog/2014/09/how-sparkling-water-brings-h2o-to-spark/](http://
    http://h2o.ai/blog/2014/09/how-sparkling-water-brings-h2o-to-spark/)，以清晰地描述H2O
    Sparkling Water如何扩展Apache Spark的功能。H2O和Spark都是开源系统。Spark MLlib包含大量功能，而H2O通过一系列额外的功能扩展了这一点，包括深度学习。它提供了用于*转换*（转换）、建模和评分数据的工具。它还提供了一个基于Web的用户界面进行交互。
- en: 'The next diagram, borrowed from [http://h2o.ai/](http://h2o.ai/), shows how
    H2O integrates with Spark. As we already know, Spark has master and worker servers;
    the workers create executors to do the actual work. The following steps occur
    to run a Sparkling water-based application:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图表，来自[http://h2o.ai/](http://h2o.ai/)，显示了H2O如何与Spark集成。正如我们已经知道的，Spark有主服务器和工作服务器；工作服务器创建执行器来执行实际工作。运行基于Sparkling
    water的应用程序发生以下步骤：
- en: Spark's `submit` command sends the sparkling water JAR to the Spark master.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark的`submit`命令将闪亮的水JAR发送到Spark主服务器。
- en: The Spark master starts the workers, and distributes the JAR file.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark主服务器启动工作服务器，并分发JAR文件。
- en: The Spark workers start the executor JVMs to carry out the work.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark工作程序启动执行器JVM来执行工作。
- en: The Spark executor starts an H2O instance.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark执行器启动H2O实例。
- en: The H2O instance is embedded with the Executor JVM, and so it shares the JVM
    heap space with Spark. When all of the H2O instances have started, H2O forms a
    cluster, and then the H2O flow web interface is made available.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: H2O实例嵌入了Executor JVM，因此它与Spark共享JVM堆空间。当所有H2O实例都启动时，H2O形成一个集群，然后H2O流Web界面可用。
- en: '![Architecture](img/B01989_07_01.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![架构](img/B01989_07_01.jpg)'
- en: 'The preceding diagram explains how H2O fits into the Apache Spark architecture,
    and how it starts, but what about data sharing? How does data pass between Spark
    and H2O? The following diagram explains this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上图解释了H2O如何适应Apache Spark架构，以及它是如何启动的，但是数据共享呢？数据如何在Spark和H2O之间传递？下图解释了这一点：
- en: '![Architecture](img/B01989_07_02.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![架构](img/B01989_07_02.jpg)'
- en: A new H2O RDD data structure has been created for H2O and Sparkling Water. It
    is a layer, based at the top of an H2O frame, each column of which represents
    a data item, and is independently compressed to provide the best compression ratio.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为H2O和Sparkling Water创建了一个新的H2O RDD数据结构。它是一个层，位于H2O框架的顶部，其中的每一列代表一个数据项，并且独立压缩以提供最佳的压缩比。
- en: 'In the deep learning example, Scala code presented later in this chapter you
    will see that a data frame has been created implicitly from a Spark schema RDD
    and a columnar data item, income has been enumerated. I won''t dwell on this now
    as it will be explained later but this is a practical example of the above architecture:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面呈现的深度学习示例中，您将看到已经从Spark模式RDD和列数据项隐式创建了一个数据框，并且收入已被枚举。我现在不会详细解释这一点，因为稍后会解释，但这是上述架构的一个实际示例：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the Scala-based example that will be tackled in this chapter, the following
    actions will take place:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中将处理的基于Scala的示例中，将发生以下操作：
- en: Data is being sourced from HDFS, and is being stored in a Spark RDD.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据来自HDFS，并存储在Spark RDD中。
- en: Spark SQL is used to filter data.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark SQL用于过滤数据。
- en: The Spark schema RDD is converted into an H2O RDD.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark模式RDD转换为H2O RDD。
- en: The H2O-based processing and modeling occurs.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于H2O的处理和建模正在进行。
- en: The results are passed back to Spark for accuracy checking.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果被传递回Spark进行准确性检查。
- en: To this point, the general architecture of H2O has been examined, and the product
    has been sourced for use. The development environment has been explained, and
    the process by which H2O and Spark integrate has been considered. Now, it is time
    to delve into a practical example of the use of H2O. First though, some real-world
    data must be sourced for modeling purposes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，已经检查了H2O的一般架构，并且已经获取了用于使用的产品。已经解释了开发环境，并且已经考虑了H2O和Spark集成的过程。现在，是时候深入了解H2O的实际用法了。不过，首先必须获取一些真实世界的数据用于建模。
- en: Sourcing the data
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据来源
- en: Since I have already used the **Artificial Neural Net** (**ANN**) functionality
    in [Chapter 2](ch02.html "Chapter 2. Apache Spark MLlib"), *Apache Spark MLlib*,
    to classify images, it seems only fitting that I use H2O deep learning to classify
    data in this chapter. In order to do this, I need to source data sets that are
    suitable for classification. I need either image data with associated image labels,
    or the data containing vectors and a label that I can enumerate, so that I can
    force H2O to use its classification algorithm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 自从我已经在[第2章](ch02.html "第2章。Apache Spark MLlib")中使用了**人工神经网络**（**ANN**）功能，*Apache
    Spark MLlib*，来对图像进行分类，似乎只有使用H2O深度学习来对本章中的数据进行分类才合适。为了做到这一点，我需要获取适合分类的数据集。我需要包含图像标签的图像数据，或者包含向量和标签的数据，以便我可以强制H2O使用其分类算法。
- en: The MNIST test and training image data was sourced from [ann.lecun.com/exdb/mnist/](http://ann.lecun.com/exdb/mnist/).
    It contains 50,000 training rows, and 10,000 rows for testing. It contains digital
    images of numbers 0 to 9 and associated labels.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST测试和训练图像数据来自[ann.lecun.com/exdb/mnist/](http://ann.lecun.com/exdb/mnist/)。它包含50,000个训练行和10,000个测试行。它包含数字0到9的数字图像和相关标签。
- en: 'I was not able to use this data as, at the time of writing, there was a bug
    in H2O Sparkling water that limited the record size to 128 elements. The MNIST
    data has a record size of *28 x 28 + 1* elements for the image plus the label:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，我无法使用这些数据，因为H2O Sparkling water中存在一个bug，限制了记录大小为128个元素。MNIST数据的记录大小为*28
    x 28 + 1*，包括图像和标签：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This issue should have been fixed and released by the time you read this, but
    in the short term I sourced another data set called income from [http://www.cs.toronto.edu/~delve/data/datasets.html](http://www.cs.toronto.edu/~delve/data/datasets.html),
    which contains Canadian employee income data. The following information shows
    the attributes and the data volume. It also shows the list of columns in the data,
    and a sample row of the data:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在您阅读此文时，这个问题应该已经得到解决并发布，但在短期内，我从[http://www.cs.toronto.edu/~delve/data/datasets.html](http://www.cs.toronto.edu/~delve/data/datasets.html)获取了另一个名为income的数据集，其中包含了加拿大雇员的收入数据。以下信息显示了属性和数据量。它还显示了数据中的列列表和一行样本数据：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: I will enumerate the last column in the data—the income bracket, so `<=50k`
    will enumerate to `0`. This will allow me to force the H2O deep learning algorithm
    to carry out classification rather than regression. I will also use Spark SQL
    to limit the data columns, and filter the data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我将枚举数据中的最后一列——收入等级，所以`<=50k`将枚举为`0`。这将允许我强制H2O深度学习算法进行分类而不是回归。我还将使用Spark SQL来限制数据列，并过滤数据。
- en: Data quality is absolutely critical when creating an H2O-based example like
    that described in this chapter. The next section examines the steps that can be
    taken to improve the data quality, and so save time.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量在创建本章描述的基于H2O的示例时至关重要。下一节将探讨可以采取的步骤来改善数据质量，从而节省时间。
- en: Data Quality
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据质量
- en: 'When I import CSV data files from HDFS to my Spark Scala H2O example code,
    I can filter the incoming data. The following example code contains two filter
    lines; the first checks that a data line is not empty, while the second checks
    that the final column in each data row (income), which will be enumerated, is
    not empty:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当我将HDFS中的CSV数据文件导入到我的Spark Scala H2O示例代码时，我可以过滤传入的数据。以下示例代码包含两行过滤器；第一行检查数据行是否为空，而第二行检查每个数据行中的最后一列（收入）是否为空：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'I also needed to clean my raw data. There are two data sets, one for training
    and one for testing. It is important that the training and testing data have the
    following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我还需要清理原始数据。有两个数据集，一个用于训练，一个用于测试。训练和测试数据必须具备以下特点：
- en: The same number of columns
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同数量的列
- en: The same data types
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同的数据类型
- en: The null values must be allowed for in the code
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码中必须允许空值
- en: The enumerated type values must match—especially for the labels
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 枚举类型的值必须匹配——尤其是标签
- en: I encountered an error related to the enumerated label column income and the
    values that it contained. I found that my test data set rows were terminated with
    a full stop character "`.`" When processed, this caused the training and the test
    data values to mismatch when enumerated.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到了与枚举标签列收入及其包含的值相关的错误。我发现我的测试数据集行以句点字符“。”结尾。处理时，这导致训练和测试数据的值在枚举时不匹配。
- en: So, I think that time and effort should be spent safeguarding the data quality,
    as a pre-step to training, and testing machine learning functionality so that
    time is not lost, and extra cost incurred.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我认为应该花费时间和精力来保障数据质量，作为训练和测试机器学习功能的预备步骤，以免浪费时间和产生额外成本。
- en: Performance tuning
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能调优
- en: 'It is important to monitor the Spark application error and the standard output
    logs in the Spark web user interface if you see errors like the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在Spark网络用户界面中看到以下错误，就需要监控Spark应用程序错误和标准输出日志：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If you encounter instances where application executors seem to hang without
    response, you may need to tune your executor memory. You need to do so if you
    see an error like the following in your executor log:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遇到应用执行器似乎没有响应的情况，可能需要调整执行器内存。如果您在执行器日志中看到以下错误，就需要这样做：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This can cause a loop, as the application requests more memory than is available,
    and so waits until the next iteration retries. The application can seem to hang
    until the executors are killed, and the tasks re-executed on alternate nodes.
    A short task's run time can extend considerably due to such problems.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会导致循环，因为应用程序请求的内存超过了可用内存，因此会等待下一次迭代重试。应用程序似乎会挂起，直到执行器被终止，并在备用节点上重新执行任务。由于这些问题，短任务的运行时间可能会大大延长。
- en: Monitor the Spark logs for these types of error. In the previous example, changing
    the executor memory setting in the `spark-submit` command removes the error, and
    reduces the runtime substantially. The memory value requested has been reduced
    to a figure below that which is available.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 监控Spark日志以查找这些类型的错误。在前面的示例中，更改`spark-submit`命令中的执行器内存设置可以消除错误，并大大减少运行时间。所请求的内存值已经降低到低于可用内存的水平。
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Deep learning
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: Neural networks were introduced in [Chapter 2](ch02.html "Chapter 2. Apache
    Spark MLlib"), *Apache Spark MLlib*. This chapter builds upon this understanding
    by introducing deep learning, which uses deep neural networks. These are neural
    networks that are feature-rich, and contain extra hidden layers, so that their
    ability to extract data features is increased. These networks are generally feed-forward
    networks, where the feature characteristics are inputs to the input layer neurons.
    These neurons then fire and spread the activation through the hidden layer neurons
    to an output layer, which should present the feature label values. Errors in the
    output are then propagated back through the network (at least in back propagation),
    adjusting the neuron connection weight matrices so that classification errors
    are reduced during training.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在[第2章](ch02.html "第2章。Apache Spark MLlib")中介绍，*Apache Spark MLlib*。本章在此基础上介绍了深度学习，它使用深度神经网络。这些是功能丰富的神经网络，包含额外的隐藏层，因此它们提取数据特征的能力增强。这些网络通常是前馈网络，其中特征特性是输入到输入层神经元的输入。然后这些神经元激活并将激活传播到隐藏层神经元，最终到输出层，应该呈现特征标签值。然后通过网络（至少在反向传播中）传播输出中的错误，调整神经元连接权重矩阵，以便在训练期间减少分类错误。
- en: '![Deep learning](img/B01989_07_03.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习](img/B01989_07_03.jpg)'
- en: The previous example image, described in the H2O booklet at [https://leanpub.com/deeplearning/read](https://leanpub.com/deeplearning/read)
    ,shows a deep learning network with four input neurons to the left, two hidden
    layers in the middle, and two output neurons. The arrows show both the connections
    between neurons and the direction that activation takes through the network.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在[H2O手册](https://leanpub.com/deeplearning/read)中描述的前面的示例图显示了一个深度学习网络，左侧有四个输入神经元，中间有两个隐藏层，右侧有两个输出神经元。箭头显示了神经元之间的连接以及激活通过网络的方向。
- en: 'These networks are feature-rich because they provide the following options:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络功能丰富，因为它们提供以下选项：
- en: Multiple training algorithms
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多种训练算法
- en: Automated network configuration
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动网络配置
- en: The ability to configure many options
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够配置许多选项
- en: Structure
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构
- en: Hidden layer structure
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层结构
- en: Training
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练
- en: Learning rate, annealing, and momentum
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率、退火和动量
- en: 'So, after giving this brief introduction to deep learning, it is now time to
    look at some of the sample Scala-based code. H2O provides a great deal of functionality;
    the classes that are needed to build and run the network have been developed for
    you. You just need to do the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在对深度学习进行简要介绍之后，现在是时候看一些基于Scala的示例代码了。H2O提供了大量的功能；构建和运行网络所需的类已经为您开发好了。您只需要做以下事情：
- en: Prepare the data and parameters
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据和参数
- en: Create and train the model
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和训练模型
- en: Validate the model with a second data set
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用第二个数据集验证模型
- en: Score the validation data set output
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对验证数据集输出进行评分
- en: When scoring your model, you must hope for a high value in percentage terms.
    Your model must be able to accurately predict and classify your data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在评分模型时，您必须希望以百分比形式获得高值。您的模型必须能够准确预测和分类您的数据。
- en: Example code – income
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例代码 - 收入
- en: 'This section examines the Scala-based H2O Sparkling Water deep learning example
    using the previous Canadian income data source. First, the Spark (`Context`, `Conf`,
    `mllib`, and `RDD`), and H2O (`h2o`, `deeplearning`, and `water`) classes are
    imported:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将使用之前的加拿大收入数据源，检查基于Scala的H2O Sparkling Water深度学习示例。首先，导入了Spark（`Context`、`Conf`、`mllib`和`RDD`）和H2O（`h2o`、`deeplearning`和`water`）类：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next an application class called `h2o_spark_dl2` is defined, the master URL
    is created, and then a configuration object is created, based on this URL, and
    the application name. The Spark context is then created using the configuration
    object:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来定义了一个名为`h2o_spark_dl2`的应用程序类，创建了主URL，然后基于此URL创建了一个配置对象和应用程序名称。然后使用配置对象创建Spark上下文：
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'An H2O context is created from the Spark context, and also an SQL context:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark上下文创建H2O上下文，还有一个SQL上下文：
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The H2O Flow user interface is started with the `openFlow` command:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`openFlow`命令启动H2O Flow用户界面：
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The training and testing of the data files are now defined (on HDFS) using
    the server URL, path, and the file names:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在定义了数据文件的训练和测试（在HDFS上）使用服务器URL、路径和文件名：
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The CSV based training and testing data is loaded using the Spark context''s
    `textFile` method:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark上下文的`textFile`方法加载基于CSV的训练和测试数据：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, the schema is defined in terms of a string of attributes. Then, a schema
    variable is created by splitting the string using a series of `StructField`, based
    on each column. The data types are left as String, and the true value allows for
    the Null values in the data:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模式是根据属性字符串定义的。然后，通过使用一系列`StructField`，基于每一列拆分字符串，创建了一个模式变量。数据类型保留为字符串，true值允许数据中的空值：
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The raw CSV line `training` and testing data is now split by commas into columns.
    The data is filtered on empty lines to ensure that the last column (`income`)
    is not empty. The actual data rows are created from the fifteen (0-14) trimmed
    elements in the raw CSV data. Both, the training and the test data sets are processed:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 原始CSV行“训练”和测试数据现在通过逗号分割成列。数据被过滤以确保最后一列（“收入”）不为空。实际数据行是从原始CSV数据中的十五个（0-14）修剪的元素创建的。训练和测试数据集都经过处理：
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Spark Schema RDD variables are now created for the training and test data sets
    by applying the schema variable, created previously for the data using the Spark
    context''s `applySchema` method:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用Spark上下文的`applySchema`方法，为训练和测试数据集创建了Spark Schema RDD变量：
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Temporary tables are created for the training and testing data:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为训练和测试数据创建临时表：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, SQL is run against these temporary tables, both to filter the number of
    columns, and to potentially limit the data. I could have added a `WHERE` or `LIMIT`
    clause. This is a useful approach that enables me to manipulate both the column
    and row-based data:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对这些临时表运行SQL，既可以过滤列的数量，也可以潜在地限制数据。我可以添加`WHERE`或`LIMIT`子句。这是一个有用的方法，使我能够操纵基于列和行的数据：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The H2O data frames are now created from the data. The final column in each
    data set (income) is enumerated, because this is the column that will form the
    deep learning label for the data. Also, enumerating this column forces the deep
    learning model to carry out classification rather than regression:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从数据中创建了H2O数据框。每个数据集中的最后一列（收入）是枚举的，因为这是将用于数据的深度学习标签的列。此外，枚举此列会强制深度学习模型进行分类而不是回归：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The enumerated results data income column is now saved so that the values in
    this column can be used to score the tested model prediction values:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在保存了枚举结果数据收入列，以便可以使用该列中的值对测试模型预测值进行评分：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The deep learning model parameters are now set up in terms of the number of
    epochs, or iterations—the data sets for training and validation and the label
    column income, which will be used to classify the data. Also, we chose to use
    variable importance to determine which data columns are most important in the
    data. The deep learning model is then created:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，深度学习模型参数已经设置好，包括迭代次数（或迭代次数）-用于训练和验证的数据集以及标签列收入，这将用于对数据进行分类。此外，我们选择使用变量重要性来确定数据中哪些数据列最重要。然后创建深度学习模型：
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The model is then scored against the test data set for predictions, and these
    income predictions are compared to the previously stored enumerated test data
    income values. Finally, an accuracy percentage is output from the test data:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对模型进行针对测试数据集的评分，进行预测，这些收入预测值与先前存储的枚举测试数据收入值进行比较。最后，从测试数据中输出准确率百分比：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the last step, the application is stopped, the H2O functionality is terminated
    via a `shutdown` call, and then the Spark context is stopped:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步中，应用程序被停止，通过`shutdown`调用终止H2O功能，然后停止Spark上下文：
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Based upon a training data set of 32,000, and a test data set of 16,000 income
    records, this deep learning model is quite accurate. It reaches an accuracy level
    of `83` percent, which is impressive for a few lines of code, small data sets,
    and just 100 epochs, as the run output shows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 基于训练数据集的32,000条记录和测试数据集的16,000条收入记录，这个深度学习模型非常准确。它达到了`83`％的准确度水平，这对于几行代码、小数据集和仅100个迭代次数来说是令人印象深刻的，如运行输出所示：
- en: '[PRE30]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the next section, I will examine some of the coding needed to process the
    MNIST data, even though that example could not be completed due to an H2O limitation
    at the time of coding.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我将检查处理MNIST数据所需的一些编码，尽管由于编码时的H2O限制，该示例无法完成。
- en: The example code – MNIST
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例代码-MNIST
- en: 'Since the MNIST image data record is so big, it presents problems while creating
    a Spark SQL schema, and processing a data record. The records in this data are
    in CSV format, and are formed from a 28 x 28 digit image. Each line is then terminated
    by a label value for the image. I have created my schema by defining a function
    to create the schema string to represent the record, and then calling it:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MNIST图像数据记录非常庞大，在创建Spark SQL模式和处理数据记录时会出现问题。此数据中的记录以CSV格式形成，并由28 x 28数字图像组成。然后，每行以图像的标签值终止。我通过定义一个函数来创建表示记录的模式字符串，然后调用它来创建我的模式：
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The same general approach to deep learning can be taken to data processing
    as the previous example, apart from the actual processing of the raw CSV data.
    There are too many columns to process individually, and they all need to be converted
    into integers to represent their data type. This can be done in one of two ways.
    In the first example, `var args` can be used to process all the elements in the
    row:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前的示例一样，可以采用与深度学习相同的一般方法来处理数据，除了实际处理原始CSV数据。有太多列需要单独处理，并且它们都需要转换为整数以表示它们的数据类型。可以通过两种方式之一来完成。在第一个示例中，可以使用`var
    args`来处理行中的所有元素：
- en: '[PRE32]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The second example uses the `fromSeq` method to process the row elements:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个示例使用`fromSeq`方法来处理行元素：
- en: '[PRE33]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the next section, the H2O Flow user interface will be examined to see how
    it can be used to both monitor H2O and process the data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，将检查H2O Flow用户界面，以了解如何使用它来监视H2O并处理数据。
- en: H2O Flow
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: H2O流
- en: H2O Flow is a web-based open source user interface for H2O, and given that it
    is being used with Spark, Sparkling Water. It is a fully functional H2O web interface
    for monitoring the H2O Sparkling Water cluster plus jobs, and also for manipulating
    data and training models. I have created some simple example code to start the
    H2O interface. As in the previous Scala-based code samples, all I need to do is
    create a Spark, an H2O context, and then call the `openFlow` command, which will
    start the Flow interface.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: H2O Flow是H2O的基于Web的开源用户界面，并且由于它与Spark一起使用，因此也可以使用Sparkling Water。这是一个完全功能的H2O
    Web界面，用于监视H2O Sparkling Water集群和作业，以及操作数据和训练模型。我已经创建了一些简单的示例代码来启动H2O界面。与之前基于Scala的代码示例一样，我所需要做的就是创建一个Spark，一个H2O上下文，然后调用`openFlow`命令，这将启动Flow界面。
- en: 'The following Scala code example just imports classes for Spark context, configuration,
    and H2O. It then defines the configuration in terms of the application name and
    the Spark cluster URL. A Spark context is then created using the configuration
    object:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Scala代码示例仅导入了用于Spark上下文、配置和H2O的类。然后根据应用程序名称和Spark集群URL定义配置。然后使用配置对象创建Spark上下文：
- en: '[PRE34]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'An H2O context is then created, and started using the Spark context. The H2O
    context classes are imported, and the Flow user interface is started with the
    `openFlow` command:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然后创建了一个H2O上下文，并使用Spark上下文启动了它。导入了H2O上下文类，并使用`openFlow`命令启动了Flow用户界面：
- en: '[PRE35]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Note, for the purposes of this example and to enable me to use the Flow application,
    I have commented out the H2O shutdown and the Spark context stop options. I would
    not normally do this, but I wanted to make this application long-running so that
    it gives me plenty of time to use the interface:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了让我能够使用Flow应用程序，我已经注释掉了H2O关闭和Spark上下文停止选项。我通常不会这样做，但我想让这个应用程序长时间运行，这样我就有足够的时间使用界面：
- en: '[PRE36]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'I use my Bash script `run_h2o.bash` with the application class name called
    `h2o_spark_ex2` as a parameter. This script contains a call to the `spark-submit`
    command, which will execute the compiled application:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用我的Bash脚本`run_h2o.bash`，并将应用程序类名称为`h2o_spark_ex2`作为参数。这个脚本包含对`spark-submit`命令的调用，它将执行编译后的应用程序：
- en: '[PRE37]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'When the application runs, it lists the state of the H2O cluster and provides
    a URL by which the H2O Flow browser can be accessed:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序运行时，它会列出H2O集群的状态，并提供一个URL，通过该URL可以访问H2O Flow浏览器：
- en: '[PRE38]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The previous example shows that I can access the H2O interface using the port
    number `54323` on the host IP address `192.168.1.108`. I can simply check my host''s
    file to confirm that the host name is `hc2r1m2`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子表明，我可以使用主机IP地址`192.168.1.108`上的端口号`54323`访问H2O界面。我可以简单地检查我的主机文件，确认主机名是`hc2r1m2`：
- en: '[PRE39]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'So, I can access the interface using the `hc2r1m2:54323` URL. The following
    screenshot shows the Flow interface with no data loaded. There are data processing
    and administration menu options and buttons at the top of the page. To the right,
    there are help options to enable you to learn more about H2O:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我可以使用`hc2r1m2:54323`的URL访问界面。下面的截图显示了Flow界面没有加载数据。页面顶部有数据处理和管理菜单选项和按钮。右侧有帮助选项，让您可以更多地了解H2O：
- en: '![H2O Flow](img/B01989_07_04.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_04.jpg)'
- en: 'The following screenshot shows the menu options and buttons in greater detail.
    In the following sections, I will use a practical example to explain some of these
    options, but there will not be enough space in this chapter to cover all the functionality.
    Check the [http://h2o.ai/](http://h2o.ai/) website to learn about the Flow application
    in detail, available at [http://h2o.ai/product/flow/](http://h2o.ai/product/flow/):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图更详细地显示了菜单选项和按钮。在接下来的章节中，我将使用一个实际的例子来解释其中一些选项，但在本章中没有足够的空间来涵盖所有的功能。请查看[http://h2o.ai/](http://h2o.ai/)网站，详细了解Flow应用程序，可在[http://h2o.ai/product/flow/](http://h2o.ai/product/flow/)找到：
- en: '![H2O Flow](img/B01989_07_05.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_05.jpg)'
- en: 'In greater definition, you can see that the previous menu options and buttons
    allow you to both administer your H2O Spark cluster, and also manipulate the data
    that you wish to process. The following screenshot shows a reformatted list of
    the help options available, so that, if you get stuck, you can investigate solving
    your problem from the same interface:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，前面的菜单选项和按钮允许您管理您的H2O Spark集群，并操纵您希望处理的数据。下面的截图显示了可用的帮助选项的重新格式化列表，这样，如果遇到问题，您可以在同一个界面上调查解决问题：
- en: '![H2O Flow](img/B01989_07_06.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_06.jpg)'
- en: 'If I use the menu option, **Admin** | **Cluster Status**, I will obtain the
    following screenshot, which shows me the status of each cluster server in terms
    of memory, disk, load, and cores. It''s a useful snapshot that provides me with
    a color-coded indication of the status:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我使用菜单选项**Admin** | **Cluster Status**，我将获得以下截图，显示了每个集群服务器的内存、磁盘、负载和核心状态。这是一个有用的快照，为我提供了状态的彩色指示：
- en: '![H2O Flow](img/B01989_07_07.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_07.jpg)'
- en: 'The menu option, **Admin** | **Jobs,** provides details of the current cluster
    jobs in terms of the start, end, and run times, as well as status. Clicking on
    the job name provides further details, as shown next, including data processing
    details, and an estimated run time, which is useful. Also, if you select the **Refresh**
    button, the display will continuously refresh until it is deselected:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 菜单选项**Admin** | **Jobs**提供了当前集群作业的详细信息，包括开始、结束和运行时间，以及状态。单击作业名称会提供更多详细信息，包括数据处理细节和估计的运行时间，这是很有用的。此外，如果选择**Refresh**按钮，显示将持续刷新，直到取消选择为止：
- en: '![H2O Flow](img/B01989_07_08.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_08.jpg)'
- en: 'The **Admin** | **Water Meter** option provides a visual display of the CPU
    usage on each node in the cluster. As you can see in the following screenshot,
    my meter shows that my cluster was idle:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**Admin** | **Water Meter**选项提供了集群中每个节点的CPU使用情况的可视化显示。如下截图所示，我的仪表显示我的集群处于空闲状态：'
- en: '![H2O Flow](img/B01989_07_09.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_09.jpg)'
- en: 'Using the menu option, **Flow** | **Upload File**, I have uploaded some of
    the training data used in the previous deep learning Scala-based example. The
    data has been loaded into a data preview pane; I can see a sample of the data
    that has been organized into cells. Also, an accurate guess has been made of the
    data types so that I can see which columns can be enumerated. This is useful if
    I want to consider classification:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用菜单选项**Flow** | **Upload File**，我已经上传了之前基于Scala的深度学习示例中使用的一些训练数据。数据已加载到数据预览窗格中；我可以看到数据的样本已经组织成单元格。还对数据类型进行了准确的猜测，这样我就可以看到哪些列可以被列举。如果我想考虑分类，这是很有用的：
- en: '![H2O Flow](img/B01989_07_10.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_10.jpg)'
- en: 'Having loaded the data, I am now presented with a **Frame** display, which
    offers me the ability to view, inspect, build a model, create a prediction, or
    download the data. The data display shows information like min, max, and mean.
    It shows data types, labels, and a zero data count, as shown in the following
    screenshot:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 加载完数据后，我现在看到了一个**Frame**显示，它让我能够查看、检查、构建模型、创建预测或下载数据。数据显示了最小值、最大值和平均值等信息。它显示了数据类型、标签和零数据计数，如下截图所示：
- en: '![H2O Flow](img/B01989_07_11.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_11.jpg)'
- en: 'I thought that it would be useful to create a deep learning classification
    model, based on this data, to compare the Scala-based approach to this H2O user
    interface. Using the view and inspect options, it is possible to visually, and
    interactively check the data, as well as create plots relating to the data. For
    instance, using the previous inspect option followed by the plot columns option,
    I was able to create a plot of data labels versus zero counts in the column data.
    The following screenshot shows the result:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为基于这些数据创建深度学习分类模型，以比较基于Scala的方法和H2O用户界面会很有用。使用查看和检查选项，可以直观地交互式地检查数据，并创建与数据相关的图表。例如，使用先前的检查选项，然后选择绘制列选项，我能够创建一个数据标签与列数据中零计数的图表。以下截图显示了结果：
- en: '![H2O Flow](img/B01989_07_12.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_12.jpg)'
- en: 'By selecting the build model option, a menu option is offered that lets me
    choose a model type. I will select deep learning, as I already know that this
    data is suited to this classification approach. The previous Scala-based model
    resulted in an accuracy level of 83 percent:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择构建模型选项，会提供一个菜单选项，让我选择模型类型。我将选择深度学习，因为我已经知道这些数据适合这种分类方法。先前基于Scala的模型的准确度达到了83%：
- en: '![H2O Flow](img/B01989_07_13.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_13.jpg)'
- en: 'I have selected the deep learning option. Having chosen this option, I am then
    able to set model parameters, such as training and validation data sets, as well
    as choosing the data columns that my model should use (obviously, the two data
    sets should contain the same columns). The following screenshot displays the data
    sets, and the model columns being selected:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了深度学习选项。选择了这个选项后，我可以设置模型参数，如训练和验证数据集，以及选择模型应该使用的数据列（显然，两个数据集应该包含相同的列）。以下截图显示了被选择的数据集和模型列：
- en: '![H2O Flow](img/B01989_07_14.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_14.jpg)'
- en: 'There are a large range of basic and advanced model options available. A selection
    of them are shown in the following screenshot. I have set the response column
    to 15 as the income column. I have also set the **VARIABLE_IMPORTANCES** option.
    Note that I don''t need to enumerate the response column, as it has been done
    automatically:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 有大量基本和高级模型选项可供选择。其中一些显示在以下截图中。我已将响应列设置为15作为收入列。我还设置了**VARIABLE_IMPORTANCES**选项。请注意，我不需要枚举响应列，因为它已经自动完成了：
- en: '![H2O Flow](img/B01989_07_15.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_15.jpg)'
- en: Note also that the epochs or iterations option is set to **100** as before.
    Also, the figure `200,200` for the hidden layers indicates that the network has
    two hidden layers, each with 200 neurons. Selecting the build model option causes
    the model to be created from these parameters. The following screenshot shows
    the model being trained, including an estimation of training time and an indication
    of the data processed so far.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，迭代选项设置为**100**与之前一样。此外，隐藏层的`200,200`表示网络有两个隐藏层，每个隐藏层有200个神经元。选择构建模型选项会根据这些参数创建模型。以下截图显示了正在训练的模型，包括训练时间的估计和迄今为止处理的数据的指示。
- en: '![H2O Flow](img/B01989_07_16.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_16.jpg)'
- en: 'Viewing the model, once trained, shows training and validation metrics, as
    well as a list of the important training parameters:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，查看模型会显示训练和验证指标，以及重要训练参数的列表：
- en: '![H2O Flow](img/B01989_07_17.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_17.jpg)'
- en: 'Selecting the **Predict** option allows an alternative validation data set
    to be specified. Choosing the **Predict** option using the new data set causes
    the already trained model to be validated against a new test dataset:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 选择**预测**选项可以指定另一个验证数据集。使用新数据集选择**预测**选项会导致已经训练的模型针对新的测试数据集进行验证：
- en: '![H2O Flow](img/B01989_07_18.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_18.jpg)'
- en: 'Selecting the **Predict** option causes the prediction details for the deep
    learning model, and dataset to be displayed as shown in the following screenshot:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 选择**预测**选项会导致深度学习模型和数据集的预测细节显示如下截图所示：
- en: '![H2O Flow](img/B01989_07_19.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_19.jpg)'
- en: The preceding screenshot shows the test data frame and the model category, as
    well as the validation statistics in terms of AUC, GINI, and MSE.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图显示了测试数据框架和模型类别，以及AUC、GINI和MSE的验证统计数据。
- en: 'The AUC value, or area under the curve, relates to the ROC, or the receiver
    operator characteristics curve, which is also shown in the following screenshot.
    TPR means **True Positive Rate**, and FPR means **False Positive Rate**. AUC is
    a measure of accuracy with a value of one being perfect. So, the blue line shows
    greater accuracy than that of the red line:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: AUC值，即曲线下面积，与ROC曲线相关，ROC曲线也显示在以下截图中。TPR表示**真正率**，FPR表示**假正率**。AUC是一个准确度的度量，值为1表示完美。因此，蓝线显示的准确度比红线高：
- en: '![H2O Flow](img/B01989_07_20.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![H2O Flow](img/B01989_07_20.jpg)'
- en: There is a great deal of functionality available within this interface that
    I have not explained, but I hope that I have given you a feel for its power and
    potential. You can use this interface to inspect your data, and create reports
    before attempting to develop code, or as an application in its own right to delve
    into your data.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这个界面中有很多功能，我没有解释，但我希望我已经让您感受到了它的强大和潜力。您可以使用这个界面来检查数据，并在尝试开发代码之前创建报告，或者作为一个独立的应用程序来深入研究您的数据。
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: My continuing theme, when examining both Apache Hadoop and Spark, is that none
    of these systems stand alone. They need to be integrated to form ETL-based processing
    systems. Data needs to be sourced and processed in Spark, and then passed to the
    next link in the ETL chain, or stored. I hope that this chapter has shown you
    that Spark functionality can be extended with extra libraries, and systems such
    as H2O.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 当我检查Apache Hadoop和Spark时，我的持续主题是，这些系统都不是独立的。它们需要集成在一起形成基于ETL的处理系统。数据需要在Spark中进行源和处理，然后传递到ETL链中的下一个链接，或者存储起来。我希望本章已经向您展示了，Spark功能可以通过额外的库和H2O等系统进行扩展。
- en: Although Apache Spark MLlib (machine learning library) has a lot of functionality,
    the combination of H2O Sparkling Water and the Flow web interface provides an
    extra wealth of data analysis modeling options. Using Flow, you can also visually,
    and interactively process your data. I hope that this chapter shows you, even
    though it cannot cover all that H2O offers, that the combination of Spark and
    H2O widens your data processing possibilities.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Apache Spark MLlib（机器学习库）具有许多功能，但H2O Sparkling Water和Flow web界面的组合提供了额外丰富的数据分析建模选项。使用Flow，您还可以直观、交互式地处理数据。希望本章能向您展示，尽管无法涵盖H2O提供的所有内容，但Spark和H2O的组合扩大了您的数据处理可能性。
- en: I hope that you have found this chapter useful. As a next step, you might consider
    checking the [http://h2o.ai/](http://h2o.ai/) website or the H2O Google group,
    which is available at [https://groups.google.com/forum/#!forum/h2ostream](https://groups.google.com/forum/#!forum/h2ostream).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您觉得本章内容有用。作为下一步，您可以考虑查看[http://h2o.ai/](http://h2o.ai/)网站或H2O Google小组，该小组可在[https://groups.google.com/forum/#!forum/h2ostream](https://groups.google.com/forum/#!forum/h2ostream)上找到。
- en: The next chapter will examine the Spark-based service [https://databricks.com/](https://databricks.com/),
    which will use Amazon AWS storage for Spark cluster creation in the cloud.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将审查基于Spark的服务[https://databricks.com/](https://databricks.com/)，该服务将在云中使用Amazon
    AWS存储来创建Spark集群。
