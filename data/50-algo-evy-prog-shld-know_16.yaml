- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Algorithmic Strategies for Data Handling
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理的算法策略
- en: Data is the New Oil of the Digital Economy.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据是数字经济的新石油。
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Wired Magazine
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —《连线杂志》
- en: In this data-driven era, the ability to extract meaningful information from
    large data sets is fundamentally shaping our decision-making processes. The algorithms
    we delve into throughout this book lean heavily on this reliance on data. Therefore,
    it becomes important to develop tools, methodologies, and strategic plans aimed
    at creating robust and efficient infrastructures for data storage.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据驱动的时代，从大量数据集中提取有意义的信息的能力正在从根本上塑造我们的决策过程。我们在本书中探讨的算法在很大程度上依赖于数据。因此，开发旨在创建强大且高效的数据存储基础设施的工具、方法和战略计划变得尤为重要。
- en: The focus of this chapter is data-centric algorithms to efficiently manage data.
    Integral to these algorithms are operations such as efficient storage and data
    compression. By employing such methodologies, data-centric architectures enable
    data management and efficient resource utilization. By the end of this chapter,
    you should be well-equipped to understand the concepts and trade-offs involved
    in designing and implementing various data-centric algorithms.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是以数据为中心的算法，用于高效地管理数据。这些算法的核心操作包括高效存储和数据压缩。通过运用这些方法，以数据为中心的架构能够实现数据管理和高效的资源利用。通过本章的学习，你应该能够很好地理解设计和实施各种数据中心算法时涉及的概念和权衡。
- en: 'This chapter discusses the following concepts:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论以下概念：
- en: Introduction to data algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据算法简介
- en: Classification of data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分类
- en: Data storage algorithms
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储算法
- en: Data compression algorithms
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据压缩算法
- en: Let’s first introduce the basic concepts.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先介绍基本概念。
- en: Introduction to data algorithms
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据算法简介
- en: Data algorithms are specialized for managing and optimizing data storage. Beyond
    storage, they handle tasks like data compression, ensuring efficient storage space
    utilization, and streamline rapid data retrieval, critical in many applications.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据算法专门用于管理和优化数据存储。除了存储，它们还处理数据压缩等任务，确保高效的存储空间利用率，并简化快速的数据检索，这在许多应用中至关重要。
- en: 'A critical facet in understanding data algorithms, especially in distributed
    systems, is the CAP theorem. Here’s where its significance lies: this theorem
    elucidates the balance among consistency, availability, and partition tolerance.
    In any distributed system, achieving two out of these three guarantees simultaneously
    is all we can hope for. Comprehending CAP’s subtleties aids in discerning the
    challenges and design decisions in modern data algorithms.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 理解数据算法，特别是在分布式系统中，关键的一点是CAP定理。它的重要性在于：该定理阐明了在一致性、可用性和分区容忍性之间的平衡。在任何分布式系统中，同时实现这三者中的两项保障是我们所能期望的。理解CAP的微妙之处有助于识别现代数据算法中的挑战和设计决策。
- en: In the scope of data governance, these algorithms are invaluable. They assure
    data consistency across all distributed system nodes, ensuring data integrity.
    They also assure efficient data availability and manage data partition tolerance,
    enhancing the system’s resilience and security.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据治理的范围内，这些算法是非常宝贵的。它们确保分布式系统中所有节点的数据一致性，从而保证数据的完整性。它们还确保数据的高效可用性，并管理数据分区容忍度，从而增强系统的弹性和安全性。
- en: Significance of CAP theorem in context of data algorithms
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CAP定理在数据算法中的重要性
- en: The CAP theorem doesn’t just set theoretical limits; it has practical implications
    in real-world scenarios where data is manipulated, stored, and retrieved. Imagine,
    for instance, a scenario where an algorithm must retrieve data from a distributed
    system. The choices made around consistency, availability, and partition tolerance
    directly impact the efficiency and reliability of that algorithm. If a system
    prioritizes availability, the data might be easily retrievable but may not be
    the most up-to-date version. Conversely, a system prioritizing consistency might
    sometimes delay data retrieval to ensure that only the most recent data is accessed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CAP定理不仅设定了理论上的极限，它在现实场景中也具有实际的意义，尤其是在数据被操作、存储和检索的过程中。例如，假设一个算法必须从分布式系统中检索数据。关于一致性、可用性和分区容忍性的选择会直接影响该算法的效率和可靠性。如果一个系统优先考虑可用性，数据可能很容易被检索，但可能不是最新的版本。相反，优先考虑一致性的系统可能会延迟数据检索，以确保访问的始终是最新的数据。
- en: The data-centric algorithms we discuss here are, in many ways, influenced by
    these CAP constraints. By intertwining our understanding of CAP theorem with data
    algorithms, we can make more informed decisions when dealing with data challenges.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论的数据中心算法在许多方面都受到这些CAP约束的影响。通过将我们对CAP定理的理解与数据算法相结合，我们可以在处理数据挑战时做出更明智的决策。
- en: Storage in distributed environments
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式环境中的存储
- en: Single-node architecture is effective for smaller data sets. However, with the
    surge in dataset sizes, distributed environment storage has become standard for
    large scale problems. Identifying the right strategy for data storage in such
    environments depends on various factors, including the nature of the data and
    anticipated usage patterns.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 单节点架构适用于较小的数据集。然而，随着数据集规模的激增，分布式环境存储已成为大规模问题的标准解决方案。在此类环境中，确定合适的数据存储策略取决于多个因素，包括数据的性质和预期的使用模式。
- en: The CAP theorem provides a foundational principle for developing these storage
    strategies, helping us tackle challenges linked with managing expansive data sets.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: CAP定理为开发这些存储策略提供了一个基础性原理，帮助我们应对与管理庞大数据集相关的挑战。
- en: Connecting CAP theorem and data compression
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接CAP定理与数据压缩
- en: It might initially seem there’s little overlap between the CAP theorem and data
    compression. But consider the practical implications. If we prioritize consistency
    in our system (as per CAP considerations), our data compression methods would
    need to ensure that data remains consistently compressed across all nodes. In
    a system where availability takes precedence, the compression method might be
    optimized for speed, even if it leads to minor inconsistencies. This interplay
    highlights that our choices around CAP influence even how we compress and retrieve
    our data, demonstrating the theorem’s pervasive influence in data-centric algorithms.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，CAP定理与数据压缩似乎没有什么重叠。但考虑到实际的影响，如果我们在系统中优先考虑一致性（按照CAP的考虑），那么我们的数据压缩方法需要确保数据在所有节点间始终保持一致的压缩状态。在一个以可用性为优先的系统中，即便这可能导致轻微的不一致，压缩方法可能会为了速度进行优化。这种相互作用表明，我们在CAP方面的选择甚至会影响到数据的压缩和检索方式，展示了定理在数据中心算法中的广泛影响。
- en: Presenting the CAP theorem
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展示CAP定理
- en: 'In 1998, Eric Brewer proposed a theorem that later became famous as the CAP
    theorem. It highlights the various trade-offs involved in designing a distributed
    service system. To understand the CAP theorem, first, let’s define the following
    three characteristics of distributed service systems: consistency, availability,
    and partition tolerance. CAP is, in fact, an acronym made up of these three characteristics:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 1998年，Eric Brewer提出了一个定理，后来被称为CAP定理。它突出了设计分布式服务系统中涉及的各种权衡。为了理解CAP定理，首先，我们需要定义分布式服务系统的以下三个特性：一致性、可用性和分区容忍性。CAP实际上是由这三种特性组成的首字母缩写：
- en: '**Consistency** (or simply **C**): The distributed service consists of various
    nodes. Any of these nodes can be used to read, write, or update records in the
    data repository. Consistency guarantees that at a certain time, t1, independent
    of which node we use to read the data, we will get the same result. Every read
    operation either returns the latest data that is consistent across the distributed
    repository or gives an error message.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**（或简称**C**）：分布式服务由多个节点组成。任何一个节点都可以用来读取、写入或更新数据仓库中的记录。一致性保证了在某个特定时间t1，无论我们使用哪个节点来读取数据，都能得到相同的结果。每次读取操作要么返回最新的数据（在分布式数据仓库中保持一致），要么返回错误信息。'
- en: '**Availability** (or simply **A**): In the area of distributed systems, availability
    means that the system as a whole always responds to requests. This ensures that
    users get a reply every time they query the system, even if it might not always
    be the latest piece of data. So, instead of focusing on every single node being
    up-to-date, the emphasis is on the entire system being responsive. It’s about
    guaranteeing that a user’s request never goes unanswered, even if some parts of
    the system have outdated information.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**（或简称**A**）：在分布式系统中，可用性意味着系统整体始终对请求作出响应。这确保了用户每次查询系统时都会得到回复，即使这可能不是最新的数据。因此，重点不在于每个节点是否都保持最新，而在于整个系统的响应能力。它保证了即使系统的某些部分包含过时的信息，用户的请求也永远不会没有回复。'
- en: '**Partition Tolerance** (or simply **P**): In a distributed system, multiple
    nodes are connected via a communication network. Partition tolerance guarantees
    that, in the event of communication failure between a small subset of nodes (one
    or more), the system remains operational. Note that to guarantee partition tolerance,
    data needs to be replicated across a sufficient number of nodes.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区容忍性**（简称**P**）：在分布式系统中，多个节点通过通信网络连接。分区容忍性保证在发生部分节点（一个或多个）之间的通信故障时，系统仍然能够正常运行。需要注意的是，为了保证分区容忍性，数据需要在足够数量的节点之间进行复制。'
- en: 'Using these three characteristics, the CAP theorem carefully summarizes the
    trade-offs involved in the architecture and design of a distributed service system.
    Specifically, the CAP theorem states that, in a distributed storage system, we
    can only have two of the following characteristics: consistency or **C**, availability
    or **A**, and partition tolerance or **P**.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这三个特性，CAP定理仔细总结了分布式服务系统架构和设计中涉及的权衡。具体来说，CAP定理指出，在分布式存储系统中，我们只能拥有以下三个特性中的两个：一致性或**C**，可用性或**A**，分区容忍性或**P**。
- en: 'This is shown in the following diagram:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点在下面的图表中得以展示：
- en: '![Diagram, venn diagram  Description automatically generated](img/B18046_13_01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图表，维恩图 描述自动生成](img/B18046_13_01.png)'
- en: 'Figure 13.1: Visualizing the choices in distributed systems: the CAP theorem'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1：在分布式系统中可视化选择：CAP定理
- en: 'Distributed data storage is increasingly becoming an essential component of
    modern IT infrastructure. Designing distributed data storage should be carefully
    considered, based on the characteristics of the data and the requirements of the
    problem we want to solve. When applied to distributed databases, the CAP theorem
    helps to guide the design and decision-making process by ensuring that developers
    and architects understand the fundamental trade-offs and limitations involved
    in creating distributed database systems. Balancing these three characteristics
    is crucial to achieve the desired performance, reliability, and scalability of
    the distributed database system. When applied to distributed databases, the CAP
    theorem helps to guide the design and decision-making process by ensuring that
    developers and architects understand the fundamental trade-offs. Balancing these
    three characteristics is crucial in order to achieve the desired performance,
    reliability, and scalability of the distributed database system. In the context
    of the CAP theorem, we can assume that there are three types of distributed storage
    systems:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式数据存储正日益成为现代IT基础设施的重要组成部分。设计分布式数据存储时应根据数据的特性和我们要解决的问题的需求来仔细考虑。当应用于分布式数据库时，CAP定理有助于指导设计和决策过程，确保开发人员和架构师理解在创建分布式数据库系统时涉及的基本权衡和限制。平衡这三个特性对于实现分布式数据库系统的期望性能、可靠性和可扩展性至关重要。当应用于分布式数据库时，CAP定理有助于指导设计和决策过程，确保开发人员和架构师理解基本的权衡。平衡这三个特性在实现分布式数据库系统的期望性能、可靠性和可扩展性方面至关重要。在CAP定理的背景下，我们可以假设有三种类型的分布式存储系统：
- en: A **CA** system (implementing Consistency-Availability)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CA**系统（实现一致性-可用性）'
- en: An **AP** system (implementing Availability-Partition Tolerance)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AP**系统（实现可用性-分区容忍性）'
- en: A **CP** system (implementing Consistency-Partition Tolerance)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CP**系统（实现一致性-分区容忍性）'
- en: Classifying data storage into **CA**, **AP**, and **CP** systems helps us to
    understand the various trade-offs involved when designing data storage systems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据存储分类为**CA**、**AP**和**CP**系统有助于我们理解在设计数据存储系统时涉及的各种权衡。
- en: Let’s look into them, one by one.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一了解它们。
- en: CA systems
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CA系统
- en: Traditional single-node systems are CA systems. In non-distributed systems,
    partition tolerance is not a concern as there is no need to manage communication
    between multiple nodes. As a result, these systems can focus on maintaining both
    consistency and availability. In other words, they are CA systems.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的单节点系统是CA系统。在非分布式系统中，分区容忍性不是一个问题，因为无需管理多个节点之间的通信。因此，这些系统可以专注于同时维护一致性和可用性。换句话说，它们是CA系统。
- en: A system can function without partition tolerance by storing and processing
    data on a single node or server. While this approach may not be suitable for handling
    large-scale data sets or high-velocity data streams, it can be effective for smaller
    data sizes or applications with less demanding performance requirements.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个系统可以通过在单个节点或服务器上存储和处理数据来实现没有分区容错的功能。虽然这种方法可能不适用于处理大规模数据集或高速数据流，但对于较小的数据规模或对性能要求不高的应用来说，它是有效的。
- en: Traditional single-node databases, such as Oracle or MySQL, are prime examples
    of CA systems. These systems are well-suited for use cases where data volume and
    velocity are relatively low, and partition tolerance is not a critical factor.
    Examples include small to medium-sized businesses, academic projects, or applications
    with a limited number of users and data sources.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的单节点数据库，如Oracle或MySQL，是CA系统的典型例子。这些系统非常适合数据量和数据流速相对较低且分区容错不是关键因素的使用场景。例子包括中小型企业、学术项目或具有有限用户和数据源的应用。
- en: AP systems
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AP系统
- en: AP systems are distributed storage systems designed to prioritize availability
    and partition tolerance, even at the expense of consistency. These highly responsive
    systems can sacrifice consistency, if necessary, to accommodate high-velocity
    data. In doing so, these distributed storage systems can handle user requests
    immediately, even if it results in temporarily serving slightly outdated or inconsistent
    data across different nodes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: AP系统是设计用来优先考虑可用性和分区容错的分布式存储系统，即使需要牺牲一致性。这些高响应系统可以在必要时牺牲一致性，以适应高速数据流。在这样做的过程中，这些分布式存储系统能够立即处理用户请求，即使这会导致不同节点间暂时提供稍微过时或不一致的数据。
- en: When consistency is sacrificed in AP systems, users might occasionally may get
    slightly outdated information. In some cases, this temporary inconsistency is
    an acceptable trade-off, as the ability to quickly process user requests and maintain
    high availability is deemed more critical than strict data consistency.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当AP系统牺牲一致性时，用户可能会偶尔获取稍微过时的信息。在某些情况下，这种临时的不一致性是可以接受的，因为能够快速处理用户请求并保持高可用性被认为比严格的数据一致性更为关键。
- en: Typical AP systems are used in real-time monitoring systems, such as sensor
    networks. High-velocity distributed databases, like Cassandra, are prime examples
    of AP systems.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的AP系统用于实时监控系统，如传感器网络。高速度的分布式数据库，如Cassandra，是AP系统的典型例子。
- en: An AP system is recommended for implementing distributed data storage in scenarios
    where high availability, responsiveness, and partition tolerance are essential.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要高可用性、响应性和分区容错的场景中，推荐使用AP系统来实现分布式数据存储。
- en: For example, if Transport Canada wants to monitor traffic on one of the highways
    in Ottawa through a network of sensors installed at different locations on the
    highway, an AP system would be the preferred choice. In this context, prioritizing
    real-time data processing and availability is crucial to ensuring that traffic
    monitoring can function effectively, even in the presence of network partitions
    or temporary inconsistencies. This is why an AP system is often the recommended
    choice for such applications, despite the potential trade-off of sacrificing consistency.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果加拿大交通运输部希望通过在渥太华高速公路上不同位置安装的传感器网络来监控交通状况，那么AP系统将是首选。在这种情况下，优先考虑实时数据处理和可用性对于确保交通监控能够有效运行至关重要，即使存在网络分区或临时的不一致性。因此，尽管可能会牺牲一致性，AP系统仍然是这种应用的推荐选择。
- en: CP systems
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CP系统
- en: CP systems prioritize both consistency and partition tolerance, ensuring that
    distributed storage systems guarantee consistency before a read process retrieves
    a value. These systems are specifically designed to maintain data consistency
    and continue operating effectively even in the presence of network partitions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: CP系统优先考虑一致性和分区容错，确保分布式存储系统在读取过程中获取值之前保证一致性。这些系统专门设计用于维护数据一致性，并在存在网络分区的情况下继续有效运行。
- en: The ideal data type for CP systems is data that requires strict consistency
    and accuracy, even if it means sacrificing the immediate availability of the system.
    Examples include financial transactions, inventory management, and critical business
    operations data. In these cases, ensuring that the data remains consistent and
    accurate across the distributed environment is of paramount importance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: CP系统的理想数据类型是那些需要严格一致性和准确性的数据，即使这意味着牺牲系统的即时可用性。例子包括财务交易、库存管理和关键业务操作数据。在这些情况下，确保数据在分布式环境中的一致性和准确性至关重要。
- en: A typical use case for CP systems is when we want to store document files in
    JSON format. Document datastores, such as MongoDB, are CP systems tuned for consistency
    in a distributed environment.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: CP系统的典型使用案例是我们想要以JSON格式存储文档文件。像MongoDB这样的文档数据存储是为分布式环境中的一致性而调整的CP系统。
- en: With an understanding of the different types of distributed storage systems,
    we can now move on to exploring data compression algorithms.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解不同类型的分布式存储系统，我们现在可以继续探索数据压缩算法。
- en: Decoding data compression algorithms
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码数据压缩算法
- en: Data compression is an essential methodology used for data storage. It not only
    enhances storage efficiency and minimizes data transmission times, but it also
    has significant implications for cost savings and performance improvements, particularly
    in the realm of big data and cloud computing. This section presents the details
    data compression techniques, with a special focus on the lossless algorithms Huffman
    and LZ77, and their influence on modern compression schemes, such as Gzip, LZO,
    and Snappy.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据压缩是用于数据存储的重要方法。它不仅提高了存储效率，减少了数据传输时间，而且在成本节省和性能提升方面具有重要意义，特别是在大数据和云计算领域。本节介绍了详细的数据压缩技术，特别关注无损算法哈夫曼和LZ77，以及它们对现代压缩方案（如Gzip、LZO和Snappy）的影响。
- en: Lossless compression techniques
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无损压缩技术
- en: Lossless compression revolves around eliminating redundancy in data to minimize
    storage needs while ensuring perfect reversibility. Huffman and LZ77 are two foundational
    algorithms that have strongly influenced the field.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 无损压缩围绕着消除数据中的冗余，以减少存储需求，同时确保完美的可逆性。哈夫曼和LZ77是两个基础算法，它们在这一领域产生了深远影响。
- en: Huffman coding focuses on variable-length coding, representing frequent characters
    with fewer bits, while LZ77, a dictionary-based algorithm, exploits repeated data
    sequences and represents them with shorter references. Let us look into them one
    by one.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 哈夫曼编码侧重于可变长度编码，使用较少的位表示频繁出现的字符，而LZ77是一种基于字典的算法，利用重复的数据序列并用较短的引用表示它们。让我们一一来看。
- en: 'Huffman coding: Implementing variable-length coding'
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 哈夫曼编码：实现可变长度编码
- en: Huffman coding, a form of entropy encoding, is used widely in lossless data
    compression. The key principle underlying Huffman coding is to assign shorter
    codes to more frequently occurring characters in a dataset, thereby reducing the
    overall data size.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 哈夫曼编码，一种熵编码形式，广泛应用于无损数据压缩。哈夫曼编码的基本原理是为频繁出现的字符分配较短的编码，从而减少整体数据大小。
- en: 'The algorithm uses a specific type of binary tree known as a Huffman tree,
    where each leaf node corresponds to a data element. The frequency of occurrence
    of the element determines the placement in the tree: more frequent elements are
    placed closer to the root. This strategy ensures that the most common elements
    have the shortest codes.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法使用一种特定类型的二叉树，称为哈夫曼树，其中每个叶子节点对应一个数据元素。元素出现的频率决定了在树中的位置：频繁出现的元素更靠近树根。这种策略确保最常见的元素具有最短的编码。
- en: A quick example
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个简单的示例
- en: 'Imagine we have data containing letters **A**, **B**, and **C** with frequencies
    `5`, `9`, and `12` respectively. In Huffman coding:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有包含字母**A**、**B**和**C**的数据，其频率分别为`5`、`9`和`12`。在哈夫曼编码中：
- en: '**C**, being the most frequent, might get a short code like `0`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C**，最常见的，可能会得到像`0`这样的短编码。'
- en: '**B**, the next frequent, could get `10`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**B**，下一个最常见的，可能会得到`10`。'
- en: '**A**, the least frequent, might have `11`.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A**，最不常见的，可能会得到`11`。'
- en: To understand it fully, let us go through an example in Python.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面理解，我们来通过一个Python示例。
- en: Implementing Huffman coding in Python
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在Python中实现哈夫曼编码
- en: We start by creating a node for each character, where the node contains the
    character and its frequency. These nodes are then added to a priority queue, with
    the least frequent elements having the highest priority. For this, we create a
    `Node` class to represent each character in the Huffman tree. Each `Node` object
    contains the character, its frequency, and pointers to its left and right children.
    The `__lt__` method is defined to compare two `Node` objects based on their frequencies.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过为每个字符创建一个节点开始，其中节点包含字符及其频率。然后，将这些节点添加到优先队列中，频率最低的元素具有最高优先级。为此，我们创建一个`Node`类来表示霍夫曼树中的每个字符。每个`Node`对象包含字符、其频率，以及指向其左子节点和右子节点的指针。`__lt__`方法用于根据频率比较两个`Node`对象。
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we build the Huffman tree. The construction of a Huffman tree involves
    a series of insertions and deletions in a priority queue, typically implemented
    as a binary heap. To build the Huffman tree, we create a min-heap of Node objects.
    A min-heap is a specialized tree-based structure that satisfies a simple but important
    condition: the parent node has a value less than or equal to its children. This
    property ensures that the smallest element is always at the root, making it efficient
    for priority operations. We repeatedly pop the two nodes with the lowest frequencies,
    merge them, and push the merged node back into the heap. This process continues
    until there is only one node left, which becomes the root of the Huffman tree.
    The tree can be built by `build_tree` function, which is defined as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构建霍夫曼树。构建霍夫曼树涉及在优先队列中进行一系列的插入和删除操作，通常实现为二叉堆。为了构建霍夫曼树，我们创建一个`Node`对象的最小堆。最小堆是一种特殊的树状结构，满足一个简单而重要的条件：父节点的值小于或等于其子节点的值。这个属性确保最小的元素始终位于根节点，使得优先操作更加高效。我们反复弹出两个频率最低的节点，将它们合并，并将合并后的节点推回堆中。这个过程持续进行，直到只剩下一个节点，它成为霍夫曼树的根节点。树的构建通过以下定义的`build_tree`函数来实现：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once the Huffman tree is constructed, we can generate the Huffman codes by traversing
    the tree. Starting from the root, we append a `0` for every left branch we follow
    and a `1` for every right branch. When we reach a leaf node, the sequence of `0`s
    and `1`s accumulated along the path from the root forms the Huffman code for the
    character at that leaf node. This functionality is achieved by creating `generate_codes`
    function as follows.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦霍夫曼树构建完成，我们可以通过遍历树来生成霍夫曼编码。从根节点开始，每走一条左分支就附加一个`0`，每走一条右分支就附加一个`1`。当我们到达一个叶子节点时，沿着从根到该叶子节点路径上累积的`0`和`1`序列就是该叶子节点对应字符的霍夫曼编码。这个功能通过如下定义的`generate_codes`函数来实现。
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now let us use the Huffman tree. Let us first define data that we will use for
    Huffman’s encoding.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用霍夫曼树。我们首先定义用于霍夫曼编码的数据。
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then, we print out the Huffman codes for each character.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们打印出每个字符的霍夫曼编码。
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we can infer the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以推断出以下内容：
- en: '**Fixed length code**: The fixed-length code for this table is `3`. This is
    because, with six characters, a fixed-length binary representation would need
    a maximum of three bits (2³ = 8 possible combinations, which can cover our 6 characters).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**固定长度编码**：此表的固定长度编码为`3`。这是因为，对于六个字符，固定长度的二进制表示需要最多三个位（2³ = 8种可能的组合，可以涵盖我们的6个字符）。'
- en: '**Variable length code**: The variable-length code for this table is'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可变长度编码**：此表的可变长度编码为'
- en: '`45(1) + .13(3) + .12(3) + .16(3) + .09(4) + .05(4) = 2.24.`'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`45(1) + .13(3) + .12(3) + .16(3) + .09(4) + .05(4) = 2.24.`'
- en: 'The following diagram shows the Huffman tree created from the preceding example:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示显示了从前述示例创建的霍夫曼树：
- en: '![Diagram  Description automatically generated](img/B18046_13_02.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图示 自动生成的描述](img/B18046_13_02.png)'
- en: 'Figure 13.2: The Huffman tree: visualizing the vompression process'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2：霍夫曼树：可视化压缩过程
- en: Note that Huffman encoding is about converting data into a Huffman tree that
    enables compression. Decoding or decompression brings the data back to the original
    format.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，霍夫曼编码是将数据转换为霍夫曼树，从而实现压缩。解码或解压缩则将数据恢复为原始格式。
- en: Having looked at Huffman’s encoding, let us now explore another lossless compression
    technique based on dictionary-based compression.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了霍夫曼编码后，我们接下来将探索另一种基于字典的无损压缩技术。
- en: Next, let us discuss dictionary-based compression.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论基于字典的压缩。
- en: Understanding dictionary-based compression LZ77
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解基于字典的压缩 LZ77
- en: LZ77 belongs to a family of compression algorithms known as dictionary coders.
    Rather than maintaining a static dictionary of code words, as in Huffman coding,
    LZ77 dynamically builds a dictionary of substrings seen in the input data. This
    dictionary isn’t stored separately but is implicitly referred to as a sliding
    window over the already-encoded input, facilitating an elegant and efficient method
    of representing repeating sequences.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: LZ77 属于一种称为字典编码器的压缩算法家族。与霍夫曼编码通过维持静态的代码词典不同，LZ77 会动态构建一个输入数据中已出现的子字符串字典。这个字典不会单独存储，而是通过一个滑动窗口隐式引用已经编码的输入数据，从而提供一种优雅且高效的表示重复序列的方法。
- en: The LZ77 algorithm operates on the principle of replacing repeated occurrences
    of data with references to a single copy. It maintains a “sliding window” of recently
    processed data. When it encounters a substring that has occurred before, it doesn’t
    store the actual substring; instead, it stores a pair of values – the distance
    to the start of the repeated substring in the sliding window, and the length of
    the repeated substring.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: LZ77 算法的原理是将重复的数据替换为指向单一副本的引用。它保持一个“滑动窗口”，用于处理最近的数据。当它遇到已经出现过的子字符串时，它不会存储实际的子字符串；而是存储一对值——指向重复子字符串开始位置的距离，以及重复子字符串的长度。
- en: Understanding with an example
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过一个示例来理解
- en: 'Imagine a scenario where you’re reading the string:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在阅读以下字符串：
- en: '`data_string = "ABABCABABD"`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`data_string = "ABABCABABD"`'
- en: Now, as you process this string left to right, when you encounter the substring
    "`CABAB`", you’ll notice that "`ABAB`" has appeared before, right after the initial
    "`AB`". LZ77 takes advantage of such repetitions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当你从左到右处理这个字符串时，当遇到子字符串 "`CABAB`" 时，你会注意到 "`ABAB`" 之前已经出现过，紧接着最初的 "`AB`" 后面。LZ77利用了这种重复现象。
- en: 'Instead of writing "`ABAB`" again, LZ77 would suggest: “Hey, look back two
    characters and copy the next two characters!” In technical terms, this is a reference
    back of two characters with a length of two characters.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: LZ77不会再写 "`ABAB`"，它会建议：“嘿，回溯两个字符并复制接下来的两个字符！”从技术角度讲，这是指回溯两个字符并复制两个字符。
- en: 'So, compressing our `data_string` using LZ77, it might look something like:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，使用LZ77压缩我们的 `data_string`，它可能看起来像这样：
- en: '`ABABC<2,2>D`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`ABABC<2,2>D`'
- en: Here, `<2,2>` is the LZ77 notation, indicating “go back by two characters and
    copy the next two.”
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`<2,2>` 是 LZ77 的符号表示，意味着“回溯两个字符并复制接下来的两个字符”。
- en: Comparison with Huffman
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与霍夫曼的比较
- en: To appreciate the power and differences between LZ77 and Huffman, it’s helpful
    to use the same data. Let’s stick with our `data_string = "ABABCABABD"`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解LZ77和霍夫曼之间的强大功能和差异，使用相同的数据是有帮助的。让我们继续使用 `data_string = "ABABCABABD"`。
- en: While LZ77 identifies repeated sequences in data and references them, Huffman
    encoding is more about representing frequent characters with shorter codes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 LZ77 识别数据中的重复序列并加以引用，但霍夫曼编码更多的是将频繁出现的字符表示为更短的编码。
- en: For instance, if you were to compress our `data_string` using Huffman, you might
    see certain characters, say '`A`' and '`B`', that are more frequent, represented
    with shorter binary codes than the less frequent '`C`' and '`D`'.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你使用霍夫曼算法压缩我们的 `data_string`，你可能会看到一些字符，比如 '`A`' 和 '`B`'，它们出现频率较高，会用比较少出现的
    '`C`' 和 '`D`' 更短的二进制代码表示。
- en: This comparison showcases that while Huffman is all about frequency-based representation,
    LZ77 is about spotting and referencing patterns. Depending on the data type and
    structure, one might be more efficient than the other.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个比较展示了，尽管霍夫曼编码基于频率来表示字符，但LZ77则是通过识别和引用模式来进行压缩。根据数据的类型和结构，某一种可能比另一种更高效。
- en: Advanced lossless compression formats
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级无损压缩格式
- en: The principles laid out by Huffman and LZ77 have given rise to advanced compression
    formats. We will look into three advanced formats in this chapter.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由霍夫曼和LZ77提出的原则催生了高级压缩格式。本章将探讨三种高级格式。
- en: LZO
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LZO
- en: Snappy
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Snappy
- en: gzip
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: gzip
- en: Let us look into them one by one.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一一来看它们。
- en: 'LZO compression: Prioritizing speed'
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LZO 压缩：优先考虑速度
- en: LZO is a lossless data compression algorithm that emphasizes rapid compression
    and decompression. It replaces repeated occurrences of data with references to
    a single copy. After this initial pass of LZ77 compression, the data is then passed
    through a Huffman coding stage.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: LZO 是一种无损数据压缩算法，强调快速的压缩和解压缩。它将重复的数据替换为指向单一副本的引用。在经过这一次LZ77压缩后，数据会被传递到霍夫曼编码阶段。
- en: While its compression ratio might not be the highest, its processing speed is
    significantly faster than many other algorithms. This makes LZO an excellent choice
    for situations where quick data access is a priority, such as real-time data processing
    and streaming applications.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其压缩比可能不是最高的，但其处理速度明显快于许多其他算法。这使得 LZO 成为在实时数据处理和流媒体应用等需要快速数据访问的场景中非常理想的选择。
- en: 'Snappy compression: Striking a balance'
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Snappy 压缩：寻求平衡
- en: Snappy is another fast compression and decompression library originally developed
    by Google. The primary focus of Snappy is to achieve high speeds and reasonable
    compression, but not necessarily the maximum compression.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Snappy 是另一个由 Google 最初开发的快速压缩和解压缩库。Snappy 的主要关注点是实现高速和合理的压缩，但不一定是最大压缩比。
- en: Snappy’s compression method is based on LZ77 but with a focus on speed and without
    an additional entropy encoding step like Huffman coding. Instead, Snappy utilizes
    a much simpler encoding algorithm that ensures speedy compressions and decompressions.
    The algorithm uses a copy-based strategy where it searches for repeated sequences
    in the data and then encodes them as a length and a reference to the previous
    location.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Snappy 的压缩方法基于 LZ77，但侧重于速度，并且没有像霍夫曼编码那样的额外熵编码步骤。相反，Snappy 使用了一种更简单的编码算法，确保压缩和解压缩过程的快速执行。该算法采用基于复制的策略，寻找数据中重复的序列，并将其编码为长度和对先前位置的引用。
- en: It should be noted that due to this tradeoff for speed, Snappy does not compress
    data as efficiently as algorithms that use Huffman coding or other forms of entropy
    encoding. However, in use-cases where speed is more critical than the compression
    ratio, Snappy can be a very effective choice.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意，由于这种速度上的权衡，Snappy 的数据压缩效率不如使用霍夫曼编码或其他形式的熵编码的算法。然而，在速度比压缩比更为关键的使用场景中，Snappy
    可以是一个非常有效的选择。
- en: 'GZIP compression: Maximizing storage efficiency'
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GZIP 压缩：最大化存储效率
- en: '`GZIP` is a file format and a software application used for file compression
    and decompression. The `GZIP` data format uses a combination of the LZ77 algorithm
    and Huffman coding.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`GZIP` 是一种文件格式和软件应用程序，用于文件压缩和解压缩。`GZIP` 数据格式结合了 LZ77 算法和霍夫曼编码。'
- en: 'Practical example: Data management in AWS: A focus on CAP theorem and compression
    algorithms'
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际示例：AWS 中的数据管理：聚焦于 CAP 定理和压缩算法
- en: Let us consider an example of a global e-commerce platform that runs on multiple
    cloud servers across the world. This platform handles thousands of transactions
    every second, and the data generated from these transactions needs to be stored
    and processed efficiently. We’ll see how the CAP theorem and compression algorithms
    can guide the design of the platform’s data management system.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个全球电子商务平台的例子，该平台运行在全球多个云服务器上。这个平台每秒处理成千上万的交易，来自这些交易的数据需要高效地存储和处理。我们将看到
    CAP 定理和压缩算法如何引导平台数据管理系统的设计。
- en: 1\. Applying the CAP theorem
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 应用 CAP 定理
- en: 'The CAP theorem states that a distributed data store cannot simultaneously
    provide more than two out of the following three guarantees: consistency, availability,
    and partition tolerance.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: CAP 定理指出，分布式数据存储不能同时提供以下三种保证中的两种：一致性、可用性和分区容错性。
- en: In our e-commerce platform scenario, availability and partition tolerance might
    be prioritized. High availability ensures that the system can continue processing
    transactions even if a few servers fail. Partition tolerance means the system
    can still function even if network failures cause some of the servers to be isolated.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的电子商务平台场景中，可用性和分区容错性可能会被优先考虑。高可用性确保即使一些服务器发生故障，系统仍能继续处理交易。分区容错性意味着即使网络故障导致某些服务器被隔离，系统仍然可以继续运行。
- en: While this means the system may not always provide strong consistency (every
    read receives the most recent write), it could use eventual consistency (updates
    propagate through the system and eventually all replicas show the same value)
    to ensure a good user experience. In practice, slight inconsistencies might be
    acceptable, for example, when it takes a few seconds for a user’s shopping cart
    to update across all devices.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这意味着系统可能无法始终提供强一致性（每次读取都获得最新的写入），但它可以使用最终一致性（更新通过系统传播，最终所有副本显示相同的值）来确保良好的用户体验。在实践中，轻微的不一致是可以接受的，例如，当用户的购物车在所有设备上更新需要几秒钟时。
- en: In the AWS ecosystem, we have a variety of data storage services that can be
    chosen based on the needs defined by the CAP theorem. For our e-commerce platform,
    we would prefer availability and partition tolerance over consistency. Amazon
    DynamoDB, a key-value NoSQL database, would be an excellent fit. It offers built-in
    support for multi-region replication and automatic sharding, ensuring high availability
    and partition tolerance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 生态系统中，我们有多种数据存储服务可供选择，可以根据 CAP 定理定义的需求进行选择。对于我们的电商平台，我们更倾向于选择可用性和分区容忍度，而非一致性。亚马逊
    DynamoDB 作为一款键值型 NoSQL 数据库，十分契合这一需求。它内建支持多区域复制和自动分片，确保高可用性和分区容忍度。
- en: For consistency, DynamoDB offers “eventual consistency” and “strong consistency”
    options. In our case, we would opt for eventual consistency to prioritize availability
    and performance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持一致性，DynamoDB 提供了“最终一致性”和“强一致性”选项。在我们的案例中，我们会选择最终一致性，以优先考虑可用性和性能。
- en: 2\. Using compression algorithms
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 使用压缩算法
- en: The platform would generate vast amounts of data, including transaction details,
    user behavior logs, and product information. Storing and transferring this data
    could be costly and time-consuming.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 平台会生成大量的数据，包括交易详情、用户行为日志和产品信息。存储和传输这些数据可能会既昂贵又耗时。
- en: Here, compression algorithms like gzip, Snappy, or LZO can help. For instance,
    the platform might use gzip to compress transaction logs that are archived for
    long-term storage. Given that gzip can typically compress text files to about
    30% of their original size, this could reduce storage costs significantly.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，像 gzip、Snappy 或 LZO 这样的压缩算法可以提供帮助。例如，平台可能会使用 gzip 来压缩那些被归档存储的交易日志。考虑到 gzip
    通常能将文本文件压缩到原始大小的约 30%，这可以显著降低存储成本。
- en: On the other hand, for real-time analytics on user behavior data, the platform
    might use Snappy or LZO. While these algorithms may not compress data as much
    as gzip, they are faster and would allow the analytics system to process data
    more quickly.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对于用户行为数据的实时分析，平台可能会使用 Snappy 或 LZO。虽然这些算法的压缩比可能不如 gzip，但它们更快速，能够让分析系统更快地处理数据。
- en: AWS provides various ways to implement compression depending on the type and
    use of data. For compressing transaction logs for long-term storage, we could
    use Amazon S3 (Simple Storage Service) coupled with gzip compression. S3 supports
    automatic gzip compression for files being uploaded, which can significantly reduce
    storage costs. For real-time analytics on user behavior data, we could use Amazon
    Kinesis Data Streams with Snappy or LZO compression. Kinesis can capture, process,
    and store data streams for real-time analytics, and supports compression to handle
    high-volume data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了多种实现压缩的方法，具体取决于数据的类型和使用场景。对于长时间存储的交易日志，我们可以使用亚马逊 S3（简单存储服务）结合 gzip 压缩。S3
    支持在上传文件时自动进行 gzip 压缩，这可以显著降低存储成本。对于用户行为数据的实时分析，我们可以使用亚马逊 Kinesis 数据流结合 Snappy
    或 LZO 压缩。Kinesis 可以捕获、处理并存储数据流以进行实时分析，并支持压缩以处理大容量数据。
- en: 3\. Quantifying the benefits
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 量化效益
- en: The benefits can be quantified similarly as described earlier.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其效益的量化方式与前述类似。
- en: Let’s take a practical example to demonstrate potential cost savings. Imagine
    our platform produces 1 TB of transaction logs daily. By leveraging gzip compression
    with S3, we can potentially shrink the storage requirement to roughly 300 GB.
    As of August 2023, S3 charges around $0.023 for every GB up to the initial 50
    TB monthly. Doing the math, this equates to a saving of about $485 each month,
    or a significant $5,820 annually, just from the log storage. It’s worth noting
    that the cited AWS pricing is illustrative and specific to August 2023; be sure
    to check current rates as they might vary.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际的例子来演示潜在的成本节省。假设我们的平台每天生成 1 TB 的交易日志。通过利用 gzip 压缩与 S3，我们可能将存储需求缩小到大约
    300 GB。截至 2023 年 8 月，S3 对前 50 TB 每月收费约为每 GB $0.023。算一下，这每月可以节省约 $485，年节省约 $5,820，仅从日志存储方面来看。值得注意的是，引用的
    AWS 定价仅供参考，具体数据可能因 2023 年 8 月而异，使用时请务必查看最新的定价。
- en: Using Snappy or LZO with Kinesis for real-time analytics could improve data
    processing speed. This could lead to more timely and personalized user recommendations,
    potentially increasing sales. The financial gain could be calculated based on
    the increase in conversion rate attributed to improved recommendation speed.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Snappy或LZO与Kinesis进行实时分析可以提高数据处理速度。这可能会导致更及时和个性化的用户推荐，进而有可能增加销售额。财务收益可以根据通过提高推荐速度所带来的转化率提升来计算。
- en: Lastly, by using DynamoDB and adhering to the CAP theorem, we ensure a smooth
    shopping experience for our users even in the event of network partitions or individual
    server failures. The value of this choice could be reflected in the platform’s
    user retention rate and overall customer satisfaction.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过使用DynamoDB并遵循CAP定理，我们可以确保即使在发生网络分区或单个服务器故障的情况下，用户仍能享受到流畅的购物体验。这一选择的价值可以体现在平台的用户留存率和整体客户满意度上。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we examined the design of data-centric algorithms, concentrating
    on three key components: data storage, data governance and data compression. We
    investigated the various issues related to data governance. We analyzed how the
    distinct attributes of data influence the architectural decisions for data storage.
    We investigated different data compression algorithms, each providing specific
    advantages in terms of efficiency and performance. In the next chapter, we will
    look at cryptographic algorithms. We will learn how we can use the power of these
    algorithms to secure exchanged and stored messages.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了以数据为中心的算法设计，重点关注三个关键组成部分：数据存储、数据治理和数据压缩。我们研究了与数据治理相关的各种问题。我们分析了数据的不同属性如何影响数据存储的架构决策。我们探讨了不同的数据压缩算法，每种算法在效率和性能方面提供了特定的优势。在下一章中，我们将研究密码学算法。我们将了解如何利用这些算法的力量来确保交换和存储的信息安全。
- en: Learn more on Discord
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Discord上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入本书的Discord社区——在这里你可以分享反馈、向作者提问，并了解新版本的发布——请扫描下面的二维码：
- en: '[https://packt.link/WHLel](https://packt.link/WHLel)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/WHLel](https://packt.link/WHLel)'
- en: '![](img/QR_Code1955211820597889031.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1955211820597889031.png)'
