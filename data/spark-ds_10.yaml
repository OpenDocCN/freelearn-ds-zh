- en: Chapter 10.  Putting It All Together
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。将所有内容整合在一起
- en: Big data analytics is revolutionizing the way businesses are run and has paved
    the way for several hitherto unimagined opportunities. Almost every enterprise,
    individual researcher, or investigative journalist has lots of data to process.
    We need a concise approach to start from raw data and arrive at meaningful insights
    based on the questions at hand.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据分析正在改变企业经营的方式，并为许多此前无法想象的机会铺平了道路。几乎每个企业、个人研究人员或调查记者都有大量数据需要处理。我们需要一种简洁的方法，从原始数据开始，根据手头的问题得出有意义的见解。
- en: We have covered various aspects of data science using Apache Spark in previous
    chapters. We started off discussing big data analytics requirements and how Apache
    spark fits in. Gradually, we looked into the Spark programming model, RDDs, and
    DataFrame abstractions and learnt how unified data access is enabled by Spark
    datasets along with the streaming aspect of continuous applications. Then we covered
    the entire breadth of the data analysis life cycle using Apache Spark followed
    by machine learning. We learnt structured and unstructured data analytics on Spark
    and explored the visualization aspects for data engineers and scientists, as well
    as business users.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在先前的章节中使用Apache Spark涵盖了数据科学的各个方面。我们开始讨论大数据分析需求以及Apache Spark的适用性。逐渐地，我们深入研究了Spark编程模型、RDD和DataFrame抽象，并学习了Spark数据集实现的统一数据访问以及连续应用的流式方面。然后，我们涵盖了使用Apache
    Spark进行整个数据分析生命周期，随后是机器学习。我们学习了Spark上的结构化和非结构化数据分析，并探索了数据工程师和科学家以及业务用户的可视化方面。
- en: 'All the previously discussed chapters helped us understand one concise aspect
    per chapter. We are now equipped to traverse the entire data science life cycle.
    In this chapter, we shall take up an end-to-end case study and apply all that
    we have learned so far. We will not introduce any new concepts; this will help
    apply the knowledge gained so far and strengthen our understanding. However, we
    have reiterated some concepts without going into too much detail, to make this
    chapter self-contained. The topics covered in this chapter are roughly the same
    as the steps in the data analytics life cycle:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所有先前讨论的章节都帮助我们理解每个章节中的一个简洁方面。我们现在已经具备了遍历整个数据科学生命周期的能力。在本章中，我们将进行一个端到端的案例研究，并应用到目前为止学到的所有知识。我们不会介绍任何新概念；这将有助于应用到目前为止所获得的知识，并加强我们的理解。但是，我们已经重申了一些概念，而没有过多地详细介绍，以使本章内容自成一体。本章涵盖的主题与数据分析生命周期中的步骤大致相同：
- en: A quick recap
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速回顾
- en: Introducing a case study
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入案例研究
- en: Framing the business problem
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建业务问题
- en: Data acquisition and data cleansing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据获取和数据清洗
- en: Developing the hypothesis
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定假设
- en: Data exploration
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据探索
- en: Data preparation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Model building
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型构建
- en: Data visualization
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Communicating the results to business users
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将结果传达给业务用户
- en: Summary
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: A quick recap
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速回顾
- en: 'We already discussed in detail the various steps involved in a typical data
    science project separately in different chapters. Let us quickly glance through
    what we have covered already and touch upon some important aspects. A high-level
    overview of the steps involved may appear as in the following figure:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在不同的章节中详细讨论了典型数据科学项目中涉及的各种步骤。让我们快速浏览一下我们已经涵盖的内容，并触及一些重要方面。所涉步骤的高级概述可能如下图所示：
- en: '![A quick recap](img/image_10_001.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![快速回顾](img/image_10_001.jpg)'
- en: In the preceding pictorial representation, we have tried to explain the steps
    involved in a data science project at a higher level, mostly generic to many data
    science assignments. Many more substeps are actually present at every stage, but
    may differ from project to project.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们试图解释了涉及数据科学项目的步骤，大部分是通用于许多数据科学任务的。实际上，在每个阶段都存在许多子步骤，但可能因项目而异。
- en: It is very difficult for data scientists to find the best approach and steps
    to follow in the beginning. Generally, data science projects do not have a well-defined
    life cycle such as the **Software Development Life Cycle** (**SDLC**). It is usually
    the case that data science projects get tramped into delivery delays with repeated
    hold-ups, as most of the steps in the life cycle are iterative. Also, there could
    be cyclic dependencies across teams that add to the complexity and cause delay
    in execution. However, while working on big data analytics projects, it is important
    as well as advantageous for data scientists to follow a well-defined data science
    workflow, irrespective of different business cases. This not only helps in an
    organized execution, but also helps us stay focused on the objective, as data
    science projects are inherently agile in most cases. Also, it is recommended that
    you plan for some level of research on data, domain, and algorithms for any given
    project.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据科学家来说，很难在开始时找到最佳方法和步骤。通常，数据科学项目没有像**软件开发生命周期**（**SDLC**）那样定义明确的生命周期。通常情况下，数据科学项目会因为生命周期中的大多数步骤都是迭代的而陷入交付延迟。此外，团队之间可能存在循环依赖，增加了复杂性并导致执行延迟。然而，在处理大数据分析项目时，对数据科学家来说，遵循明确定义的数据科学工作流程是重要且有利的，无论不同的业务案例如何。这不仅有助于组织执行，还有助于我们专注于目标，因为在大多数情况下，数据科学项目本质上是敏捷的。此外，建议您计划对任何给定项目的数据、领域和算法进行一定程度的研究。
- en: In this chapter, we may not be able to accommodate all the granular steps in
    a single flow, but will address the important areas to give you a heads-up. We
    will try to look at some different coding examples that we have not covered in
    the previous chapters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们可能无法在单个流程中容纳所有细粒度步骤，但将讨论重要领域，以便提前了解。我们将尝试查看一些在先前章节中未涵盖的不同编码示例。
- en: Introducing a case study
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入案例研究
- en: We will be exploring Academy Awards demographics in this chapter. You can download
    the data from the GitHub repository at [https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv](https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨奥斯卡奖的人口统计学。你可以从GitHub仓库下载数据[https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv](https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv)。
- en: This dataset is based on the data provided at [http://www.crowdflower.com/data-for-everyone](http://www.crowdflower.com/data-for-everyone).
    It contains demographic details such as race, birthplace, and age. Rows are around
    400 and it can be easily processed on a simple home computer, so you can do a
    **Proof of Concept** (**POC**) on executing a data science project on Spark.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是基于[http://www.crowdflower.com/data-for-everyone](http://www.crowdflower.com/data-for-everyone)提供的数据。它包含人口统计学细节，如种族、出生地和年龄。行数大约为400，可以在普通家用电脑上轻松处理，因此你可以在Spark上执行数据科学项目的**概念验证**（**POC**）。
- en: Just start by downloading the file and inspecting the data. The data may look
    fine but as you take a closer look, you will notice that it is not "clean". For
    example, the date of birth column does not follow the same format. Some years
    are in two-digit format whereas some are in four-digit format. Birthplace does
    not have country for locations within the USA.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从下载文件并检查数据开始。数据可能看起来不错，但当你仔细查看时，你会注意到它并不是“干净”的。例如，出生日期列的格式不一致。有些年份是两位数格式，而有些是四位数格式。出生地没有美国境内地点的国家信息。
- en: Likewise, you will also notice that the data looks skewed, with more "white"
    race people from the USA. But you might have felt that the trend has changed toward
    later years. You have not used any tools or techniques so far, just had a quick
    glance at the data. In the real world of data science, this seemingly trivial
    activity can be quite helpful further down the life cycle. You get to develop
    a feel for the data at hand and simultaneously hypothesize about the data. This
    brings you to the very first step in the workflow.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你也会注意到数据看起来有偏差，美国有更多“白人”种族的人。但你可能会感觉到趋势在后来的年份有所改变。到目前为止，你还没有使用任何工具或技术，只是快速浏览了一下数据。在数据科学的现实世界中，这种看似琐碎的活动在生命周期的后期可能会非常有帮助。你可以对手头的数据产生一种感觉，并同时对数据进行假设。这将带你进入工作流程的第一步。
- en: The business problem
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 业务问题
- en: As iterated before, the most important aspect of any data science project is
    the question at hand. Having a clear understanding on *what problem are we trying
    to solve?* This is critical to the success of the project. It also drives what
    is considered as relevant data and what is not. For example, in the current case
    study, if what we want to look at is the demographics, then movie name and person
    name are irrelevant. At times, there is no specific question at hand! *What then?*
    Even when there is no specific question, the business may still have some objective,
    or data scientists and domain experts can work together to find the area of business
    to work on. To understand the business, functions, problem statement, or data,
    the data scientists start with "Questioning". It not only helps in defining the
    workflow, but helps in sourcing the right data to work on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所述，任何数据科学项目最重要的方面是所面临的问题。对于*我们试图解决什么问题？*有清晰的理解至关重要。这对项目的成功至关重要。它还决定了什么是相关数据，什么不是。例如，在当前案例研究中，如果我们想要研究的是人口统计学，那么电影名称和人名就是无关的。有时候，手头上没有具体的问题！*那么呢？*即使没有具体的问题，业务可能仍然有一些目标，或者数据科学家和领域专家可以共同努力找到要处理的业务领域。为了理解业务、功能、问题陈述或数据，数据科学家从“质疑”开始。这不仅有助于定义工作流程，还有助于获取正确的数据。
- en: 'As an example, if the business focus is on demographics information, a formal
    business problem statement can be defined as:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果业务重点是人口统计信息，一个正式的业务问题陈述可以被定义为：
- en: '*What is the impact of the race and country of origin among Oscar award winners?*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*种族和出生国家在奥斯卡奖得主中的影响是什么？*'
- en: In real-world, scenarios this step will not be this straightforward. Framing
    the right question is the collective responsibility of the data scientist, strategy
    team, domain experts, and the project owner. Since the whole exercise is futile
    if it does not serve the purpose, a data scientist has to consult all stakeholders
    and try to elicit as much information as possible from them. However, they may
    end up getting invaluable insights or "hunches". All of these combined form the
    core of the initial hypothesis and also help the data scientist to understand
    what exactly they should look for.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，这一步并不会如此直接。提出正确的问题是数据科学家、战略团队、领域专家和项目所有者的共同责任。如果不符合目的，整个练习就是徒劳的，数据科学家必须咨询所有利益相关者，并尽可能从他们那里获取尽可能多的信息。然而，他们可能最终得到宝贵的见解或“直觉”。所有这些结合起来构成了最初的假设的核心，并帮助数据科学家了解他们应该寻找什么。
- en: The situations where there is no specific question at hand that the business
    is trying to find an answer for are even more interesting to deal with, but can
    be complex in executing!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在业务没有明确问题需要寻找答案的情况下，处理起来更有趣，但在执行上可能更复杂！
- en: Data acquisition and data cleansing
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据获取和数据清洗
- en: '**Data acquisition** is the logical next step. It may be as simple as selecting
    data from a single spreadsheet or it may be an elaborate several months project
    in itself. A data scientist has to collect as much relevant data as possible.
    ''Relevant'' is the keyword here. Remember, more relevant data beats clever algorithms.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据获取**是下一个逻辑步骤。它可能只是从单个电子表格中选择数据，也可能是一个独立的几个月的项目。数据科学家必须收集尽可能多的相关数据。这里的关键词是“相关”。记住，更相关的数据胜过聪明的算法。'
- en: We have already covered how to source data from heterogeneous data sources and
    consolidate it to form a single data matrix, so we will not iterate the same fundamentals
    here. Instead, we source our data from a single source and extract a subset of
    it.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了如何从异构数据源获取数据并 consoli，以形成单个数据矩阵，因此我们将不在这里重复相同的基础知识。相反，我们从单一来源获取数据并提取其子集。
- en: 'Now it is time to view the data and start cleansing it. The scripts presented
    in this chapter tend to be longer than the previous examples but still are no
    means of production quality. Real-world work requires a lot more exception checks
    and performance tuning:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候查看数据并开始清理了。本章中呈现的脚本往往比以前的示例要长，但仍然不是生产质量的。现实世界的工作需要更多的异常检查和性能调优：
- en: '**Scala**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala**'
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Python**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python**'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For the dataset at hand, you might also have noticed that `date_of_birth` and
    `birthplace` require a lot of cleaning. The following code shows two **user-defined
    functions** (**UDFs**) that clean `date_of_birth` and `birthplace` respectively.
    These UDFs work on a single data element at a time and they are just ordinary
    Scala/Python functions. These user defined functions should be registered so that
    they can be used from within a SQL statement. The final step is to create a cleaned
    data frame that will participate in further analysis.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于手头的数据集，您可能还注意到`date_of_birth`和`birthplace`需要大量清理。以下代码显示了分别清理`date_of_birth`和`birthplace`的两个**用户定义函数**（**UDFs**）。这些UDFs一次处理一个数据元素，它们只是普通的Scala/Python函数。这些用户定义函数应该被注册，以便它们可以从SQL语句中使用。最后一步是创建一个经过清理的数据框，以便参与进一步的分析。
- en: 'Notice the following logic for cleaning `birthplace.` It is a weak logic because
    we are assuming that any string ending with two characters is an American state.
    We have to compare them against a list of valid abbreviations. Similarly, assuming
    two-digit years are always from the twentieth century is another error-prone assumption.
    Depending on the use case, a data scientist/data engineer has to take a call whether
    retaining more rows is important or only quality data should be included. All
    such decisions should be neatly documented for reference:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意清理`birthplace`的逻辑。这是一个薄弱的逻辑，因为我们假设任何以两个字符结尾的字符串都是美国州。我们必须将它们与有效缩写列表进行比较。同样，假设两位数年份总是来自二十世纪是另一个容易出错的假设。根据使用情况，数据科学家/数据工程师必须决定保留更多行是否重要，或者只应包含质量数据。所有这些决定都应该被清晰地记录以供参考：
- en: '**Scala:**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Python:**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The UDF to clean date accepts a hyphenated date string and splits it. If the
    last component, which is the year, is two digits long, then it is assumed to be
    a twentieth-century date and 1900 is added to bring it to four-digit format.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 清理日期的UDF接受一个连字符日期字符串并拆分它。如果最后一个组件，即年份，是两位数长，则假定它是二十世纪的日期，并添加1900以将其转换为四位数格式。
- en: 'The following UDF appends the country as USA if the country string is either
    New York City or the last component is two characters long, where it is assumed
    to be a state in the USA:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下UDF附加了国家作为美国，如果国家字符串是纽约市或最后一个组件为两个字符长，那么假定它是美国的一个州：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Python:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 'Python:'
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The UDFs should be registered if you want to access them from SELECT strings:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要从SELECT字符串中访问UDFs，则应注册UDFs：
- en: '**Scala:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Python:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Clean the data frame using the UDFs. Perform the following cleanup operations:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用UDFs清理数据框。执行以下清理操作：
- en: Call UDFs `fncleanDate` and `fncleanBirthplace` to fix birthplace and country.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用UDFs `fncleanDate`和`fncleanBirthplace`来修复出生地和国家。
- en: Subtract birth year from `award_year` to get `age` at the time of receiving
    the award.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`award_year`中减去出生年份以获得获奖时的`age`。
- en: Retain `race` and `award` as they are.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留`race`和`award`。
- en: '**Scala:**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Python:**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The last line requires some explanation. The UDFs are used similar to SQL functions
    and the expressions are aliased to meaningful names. We have added a computed
    column `age` because we would like to validate the impact of age also. The `substring_index`
    function  searches the first argument for the second argument. `-1` indicates
    to look for the first occurrence from the right.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行需要一些解释。UDFs类似于SQL函数，并且表达式被别名为有意义的名称。我们添加了一个计算列`age`，因为我们也想验证年龄的影响。`substring_index`函数搜索第一个参数的第二个参数。`-1`表示从右边查找第一次出现。
- en: Developing the hypothesis
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 制定假设
- en: A hypothesis is your best guess about what the outcome will be. You form your
    initial hypothesis based on the question, conversations with stakeholders, and
    also by looking at the data. You may form one or more hypotheses for a given problem.
    This initial hypothesis serves as a roadmap that guides you through the exploratory
    analysis. Developing a hypothesis is very important to statistically approve or
    not approve a statement, and not just by looking at the data as a data matrix
    or even through visuals. This is because our perception built by just looking
    at the data may be incorrect and rather deceptive at times.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 假设是关于结果的最佳猜测。您根据问题、与利益相关者的对话以及查看数据形成初始假设。对于给定的问题，您可能会形成一个或多个假设。这个初始假设作为指导您进行探索性分析的路线图。制定假设对于统计上批准或不批准一个陈述非常重要，而不仅仅是通过查看数据作为数据矩阵或甚至通过视觉来进行。这是因为我们仅仅通过查看数据建立的认知可能是不正确的，有时甚至是具有欺骗性的。
- en: 'Now you know that your final result may or may not prove the hypothesis to
    be correct. Coming to the case study we have considered for this lesson, we arrive
    at the following initial hypotheses:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道你的最终结果可能证明假设是正确的，也可能不是。来到我们为这节课考虑的案例研究，我们得出以下初始假设：
- en: Award winners are mostly white
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获奖者大多是白人
- en: Most of the award winners are from the USA
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数获奖者来自美国
- en: Best actors and actresses tend to be younger than best directors
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳男演员和女演员往往比最佳导演年轻
- en: Now that we have formalized our hypotheses, we are all set to move forward with
    the next steps in the life cycle..
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经明确了我们的假设，我们已经准备好继续进行生命周期的下一步了。
- en: Data exploration
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: Now that we have a clean data frame with relevant data and the initial hypothesis,
    it is time to really explore what we have. The DataFrames abstraction provides
    functions such as `group by` out of the box for you to look around. You may register
    the cleaned data frame as a table and run the time-tested SQL statements to do
    just the same.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个包含相关数据和初始假设的干净数据框架，是时候真正探索我们拥有的东西了。数据框架抽象提供了`group by`等函数，供您随时查看。您可以将清理后的数据框架注册为表，并运行经过时间考验的SQL语句来执行相同的操作。
- en: This is also the time to plot a few graphs. This phase of visualization is the
    exploratory analysis mentioned in the data visualization chapter. The objectives
    of this exploration are greatly influenced by the initial information you garner
    from the business stakeholders and the hypothesis. In other words, your discussions
    with the stakeholders help you know what to look for.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在也是绘制一些图表的时候了。这个可视化阶段是数据可视化章节中提到的探索性分析。这次探索的目标受到您从业务利益相关者和假设中获得的初始信息的极大影响。换句话说，您与利益相关者的讨论帮助您知道要寻找什么。
- en: 'There are some general guidelines that are applicable for almost all data science
    assignments, but again subjective to different use cases. Let us look at some
    generic ones:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些通用准则适用于几乎所有数据科学任务，但又因不同的使用情况而异。让我们看一些通用的准则：
- en: Look for missing data and treat it. We have already discussed various ways to
    do this in [Chapter 5](ch05.xhtml "Chapter 5. Data Analysis on Spark"), *Data
    Analysis on Spark*.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找缺失数据并处理它。我们已经讨论了在[第5章](ch05.xhtml "第5章。Spark上的数据分析")*Spark上的数据分析*中执行此操作的各种方法。
- en: Find the outliers in the dataset and treat them. We have discussed this aspect
    as well. Please note that there are cases where what we think of as outliers and
    normal data points may change depending on the use case.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找数据集中的异常值并处理它们。我们也讨论了这个方面。请注意，有些情况下，我们认为是异常值和正常数据点的界限可能会根据使用情况而改变。
- en: Perform univariate analysis, wherein you explore each variable in the dataset
    separately. Frequency distribution or percentile distribution are quite common.
    Perhaps plot some graphs to get a better idea. This will also help you prepare
    your data before getting into data modeling.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行单变量分析，其中您单独探索数据集中的每个变量。频率分布或百分位数分布是相当常见的。也许绘制一些图表以获得更好的想法。这也将帮助您在进行数据建模之前准备数据。
- en: Validate your initial hypothesis.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证您的初始假设。
- en: Check minimum and maximum values of numerical data. If the variation is too
    high in any column, that could be a candidate for data normalization or scaling.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查数值数据的最小值和最大值。如果任何列中的变化太大，那可能是数据归一化或缩放的候选项。
- en: Check distinct values in categorical data (string values such as city names)
    and their frequencies. If there are too many distinct values (aka levels) in any
    column, you may have to look for ways to reduce the number of levels. If one level
    is occurring almost always, then this column is not helping the model to differentiate
    between the possible outcomes. Such columns are likely candidates for removal.
    At the exploration stage, you just figure out such candidate columns and let the
    data preparation phase take care of the actual action.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查分类数据（如城市名称等字符串值）中的不同值及其频率。如果任何列中有太多不同的值（也称为级别），则可能需要寻找减少级别数量的方法。如果一个级别几乎总是出现，那么该列对模型区分可能结果没有帮助。这样的列很可能被移除。在探索阶段，您只需找出这样的候选列，让数据准备阶段来处理实际操作。
- en: 'In our current dataset, we do not have any missing data and we do not have
    any numerical data that might create any challenge. However, some missing values
    might creep in when invalid dates are processed. So, the following code covers
    the remaining action items. This code assumes that `cleaned_df` is already created:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的数据集中，我们没有任何缺失数据，也没有任何可能造成挑战的数值数据。但是，当处理无效日期时，可能会出现一些缺失值。因此，以下代码涵盖了剩余的操作项目。此代码假定`cleaned_df`已经创建：
- en: '**Scala/Python:**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala/Python:**'
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following visualizations correspond to the initial hypotheses. Note that
    two of our hypotheses were found to be correct but the third one was not. These
    visualizations are created using zeppelin:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下可视化与初始假设相对应。请注意，我们发现两个假设是正确的，但第三个假设不正确。这些可视化是使用zeppelin创建的：
- en: '![Data exploration](img/image_10_002.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![数据探索](img/image_10_002.jpg)'
- en: Note here that the all hypotheses cannot just be validated through visuals,
    as they can be deceptive at times. So proper statistical tests such as t-tests,
    ANOVA, Chi-squared tests, correlation tests, and so on need to be performed as
    applicable. We will not get into the details in this section. Please refer to
    [Chapter 5](ch05.xhtml "Chapter 5. Data Analysis on Spark"), *Data Analysis on
    Spark*, for further details.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，并非所有假设都可以通过可视化来验证，因为它们有时可能具有欺骗性。因此，需要根据适用情况执行适当的统计检验，如t检验、ANOVA、卡方检验、相关性检验等。我们不会在本节详细介绍。有关详细信息，请参阅[第5章](ch05.xhtml
    "第5章。Spark上的数据分析")*Spark上的数据分析*。
- en: Data preparation
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: The data exploration stage helped us identify all the issues that needed to
    be fixed before proceeding to the modeling stage. Each individual issue requires
    careful thought and deliberation to choose the best fix. Here are some common
    issues and the possible fixes. The best fix is dependent on the problem at hand
    and/or the business context.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索阶段帮助我们确定了在进入建模阶段之前需要修复的所有问题。每个单独的问题都需要仔细思考和审议，以选择最佳的修复方法。以下是一些常见问题和可能的修复方法。最佳修复取决于手头的问题和/或业务背景。
- en: Too many levels in a categorical variable
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类变量中的太多级别
- en: 'This is one of the most common issues we face. The treatment of this issue
    is dependent on multiple factors:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们面临的最常见问题之一。解决此问题取决于多个因素：
- en: If the column is almost always unique, for example, it is a transaction ID or
    timestamp, then it does not participate in modeling unless you are deriving new
    features from it. You may safely drop the column without losing any information
    content. You usually drop it during the data cleansing stage itself.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果列几乎总是唯一的，例如，它是一个交易ID或时间戳，那么它在建模过程中不参与，除非你正在从中派生新特征。您可以安全地删除该列而不会丢失任何信息内容。通常在数据清洗阶段就会删除它。
- en: If it is possible to replace the levels with coarser-grained levels (for example,
    state or country instead of city) that make sense in the current context, then
    usually that is the best way to fix this issue.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果可能用粗粒度级别（例如，州或国家而不是城市）替换级别，这在当前情境下通常是解决此问题的最佳方式。
- en: You may want to add dummy columns with 0 or 1 values for each distinct level.
    For example, if you have 100 levels in a single column, you add 100 columns instead.
    At most, one column will have 1 at any observation (row). This is called **one-hot
    encoding** and Spark provides this out of the box through the `ml.features` package.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能希望为每个不同级别添加具有0或1值的虚拟列。例如，如果单个列中有100个级别，则添加100个列。最多，一个观察（行）中将有一个列具有1。这称为**独热编码**，Spark通过`ml.features`包提供了这个功能。
- en: Another option is to retain the most frequent levels. You may even attach each
    of these levels to one of the dominant levels that is somehow considered "nearer"
    to this level. Also, you may bundle up the remaining into a single bucket, say,
    `Others`.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个选择是保留最频繁的级别。您甚至可以将这些级别中的每一个附加到某个被认为与该级别“更接近”的主导级别中。此外，您可以将其余级别捆绑到一个单一的桶中，例如`Others`。
- en: There is no hard and fast rule for an absolute limit to the number of levels.
    It depends on what granularity you require in each individual feature and the
    performance constraints.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有绝对限制级别的硬性规定。这取决于您对每个单独特征所需的粒度以及性能约束。
- en: 'The current dataset has too many levels in the categorical variable `country`.
    We chose to retain the most frequent levels and bundle the remaining into `Others`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当前数据集在分类变量`country`中有太多级别。我们选择保留最频繁的级别，并将其余级别捆绑到`Others`中：
- en: '**Scala:**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Python:**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Numerical variables with too much variation
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有太多变化的数值变量
- en: Sometimes numerical data values may vary by several orders of magnitude. For
    example, if you are looking at the annual income of individuals, it may vary a
    lot. Z-score normalization (standardization) and min-max scaling are two popular
    choices to deal with such data. Spark includes both of these transformations out
    of the box in the `ml.features` package.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，数值数据值可能相差几个数量级。例如，如果您正在查看个人的年收入，它可能会有很大变化。Z分数标准化（标准化）和最小-最大缩放是处理这种数据的两种常用选择。Spark在`ml.features`包中提供了这两种转换。
- en: Our current dataset does not have any such variable. The only numerical variable
    we have is age and its value is uniformly two digits. That's one less issue to
    fix.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的数据集没有这样的变量。我们唯一的数值变量是年龄，其值均匀为两位数。这是一个问题少了一个问题。
- en: Please note that it is not always necessary to normalize such data. If you are
    comparing two variables that are in two different scales, or if you are using
    a clustering algorithm or SVM classifier, or any other scenario where there is
    really a need to normalize the data, you may normalize the data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，并非总是需要对此类数据进行标准化。如果您正在比较两个不同规模的变量，或者如果您正在使用聚类算法或SVM分类器，或者任何其他真正需要对数据进行标准化的情况，您可以对数据进行标准化。
- en: Missing data
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺失数据
- en: This is a major area of concern. Any observations where the target itself is
    missing should be removed from the training data. The remaining observations may
    be retained with some imputed values or removed as per the requirements. You should
    be very careful in imputing the missing values; it may lead to misleading output
    otherwise! It may seem very easy to just go ahead and substitute average values
    in the blank cells of a continuous variable, but this may not be the right approach.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个重要的关注领域。任何目标本身缺失的观察应该从训练数据中删除。根据要求，剩下的观察可以保留一些填充值或删除。在填充缺失值时，您应该非常小心；否则可能会导致误导性的输出！看起来很容易只需继续并在连续变量的空白单元格中替换平均值，但这可能不是正确的方法。
- en: Our current case study does not have any missing data so there is no scope for
    treating it. However, let us look at an example.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的案例研究没有任何缺失数据，因此没有处理的余地。不过，让我们看一个例子。
- en: Let's assume you have a student's dataset that you are dealing with, and it
    has data from class-1 to class-5\. If there are some missing `Age` values and
    you just find the average of the whole column and substitute, then that would
    rather become an outlier and could lead to vague results. You may choose to find
    the average of only the class that the student is in, and then impute that value.
    This is at least a better approach, but may not be a perfect one. In most of the
    cases, you will have to give weightage to other variables as well. If you do so,
    you may end up building a predictive model to find the missing values and this
    can be a great approach!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您正在处理一个学生数据集，其中包含从一年级到五年级的数据。如果有一些缺失的`Age`值，您只需找到整个列的平均值并替换，那么这将成为一个异常值，并可能导致模糊的结果。您可以选择仅找到学生所在班级的平均值，然后填充该值。这至少是一个更好的方法，但可能不是完美的方法。在大多数情况下，您还必须给其他变量赋予权重。如果这样做，您可能会建立一个预测模型来查找缺失的值，这可能是一个很好的方法！
- en: Continuous data
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续数据
- en: 'Numerical data is often continuous and must be discretized because it is a
    prerequisite to some of the algorithms. It is usually split into different buckets
    or ranges of values. However, there could be cases where you may not just uniformly
    bucket based on the range of your data, you may have to consider the variance
    or standard deviation or any other applicable reason to bucket properly. Now,
    deciding the number of buckets is also at the discretion of the data scientist,
    but that too needs careful analysis. Too few buckets reduces granularity and too
    many buckets is just about the same as having too many categorical levels. In
    our case study, `age` is an example of such data and we need to discretize it.
    We split it into different buckets. For example, look at this pipeline stage,
    which converts `age` to 10 buckets:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数值数据通常是连续的，必须离散化，因为这是一些算法的先决条件。它通常被分成不同的桶或值的范围。然而，可能存在这样的情况，你不仅仅是根据数据的范围均匀地分桶，你可能需要考虑方差或标准差或任何其他适用的原因来正确地分桶。现在，决定桶的数量也取决于数据科学家的判断，但这也需要仔细分析。太少的桶会降低粒度，而太多的桶与拥有太多的分类级别几乎是一样的。在我们的案例研究中，“年龄”就是这样的数据的一个例子，我们需要将其离散化。我们将其分成不同的桶。例如，看看这个管道阶段，它将“年龄”转换为10个桶：
- en: '**Scala:**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Python:**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Categorical data
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类数据
- en: We have discussed the need for discretizing continuous data and converting it
    to categories or buckets. We have also discussed the introduction of dummy variables,
    one for each distinct value of a categorical variable. There is one more common
    data preparation practice where we convert categorical levels to numerical (discrete)
    data. This is required because many machine learning algorithms work with numerical
    data, integers, and real-valued numbers, or some other situation may demand it.
    So, we need to convert categorical data into numerical data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了将连续数据离散化并转换为类别或桶的需要。我们还讨论了引入虚拟变量，每个分类变量的每个不同值都有一个。还有一个常见的数据准备做法，即将分类级别转换为数值（离散）数据。这是因为许多机器学习算法使用数值数据、整数和实值数字，或者其他情况可能需要。因此，我们需要将分类数据转换为数值数据。
- en: There can be downsides to this approach. Introducing an order into inherently
    unordered data may not be logical at times. For example, assigning numbers such
    as 0, 1, 2, 3 to the colors "red", "green", "blue", and "black", respectively,
    does not make sense. This is because we cannot say that red is one unit distant
    from "green" and so is "green" from "blue"! If applicable, introducing dummy variables
    makes more sense in many such cases.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能存在缺点。在本质上无序的数据中引入顺序有时可能是不合逻辑的。例如，将数字0、1、2、3分配给颜色“红色”、“绿色”、“蓝色”和“黑色”是没有意义的。这是因为我们不能说红色距离“绿色”一单位，绿色距离“蓝色”也是如此！如果适用，在许多这种情况下引入虚拟变量更有意义。
- en: Preparing the data
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'Having discussed the common issues and possible fixes, let us see how to prepare
    our current dataset. We have already covered the too many levels issue related
    code fix. The following example shows the rest. It converts all the features into
    a single features column. It also sets aside some data for testing the models.
    This code heavily relies on the `ml.features` package, which was designed to support
    the data preparation phase. Note that this piece of code is just defining what
    needs to be done. The transformations are not carried out as yet. These will become
    stages in subsequently defined pipelines. Execution is deferred as late as possible,
    until the actual model is built. The Catalyst optimizer finds the optimal route
    to implement the pipeline:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了常见问题和可能的修复方法之后，让我们看看如何准备我们当前的数据集。我们已经涵盖了与太多级别问题相关的代码修复。以下示例显示了其余部分。它将所有特征转换为单个特征列。它还将一些数据设置为测试模型。这段代码严重依赖于`ml.features`包，该包旨在支持数据准备阶段。请注意，这段代码只是定义了需要做什么。转换尚未进行。这些将成为随后定义的管道中的阶段。执行被尽可能地推迟，直到实际模型建立。Catalyst优化器找到了实施管道的最佳路径：
- en: '**Scala:**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala:**'
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Python:**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python:**'
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After carrying out all data preparation activity, you will end up with a completely
    numeric data with no missing values and with manageable levels in each attribute.
    You may have already dropped any attributes that may not add much value to the
    analysis on hand. This is what we call the **final data matrix**. You are all
    set now to start modeling your data. So, first you split your source data into
    train data and test data. Models are "trained" using train data and "tested" using
    test data. Note that the split is random and you may end up with different train
    and test partitions if you redo the split.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行所有数据准备活动之后，您将得到一个完全数字化的数据，没有缺失值，并且每个属性中的级别是可管理的。您可能已经删除了可能对手头的分析没有太大价值的任何属性。这就是我们所说的**最终数据矩阵**。现在您已经准备好开始对数据进行建模了。因此，首先将源数据分成训练数据和测试数据。模型使用训练数据进行“训练”，并使用测试数据进行“测试”。请注意，拆分是随机的，如果重新进行拆分，您可能会得到不同的训练和测试分区。
- en: Model building
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型构建
- en: A model is a representation of things, a rendering or description of reality.
    Just like a model of a physical building, data science models attempt to make
    sense of the reality; in this case, the reality is the underlying relationships
    between the features and the predicted variable. They may not be 100 percent accurate,
    but still very useful to give some deep insights into our business space based
    on the data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是事物的表现，现实的描述或描述。就像物理建筑的模型一样，数据科学模型试图理解现实；在这种情况下，现实是特征和预测变量之间的基本关系。它们可能不是100%准确，但仍然非常有用，可以根据数据为我们的业务空间提供一些深刻的见解。
- en: There are several machine learning algorithms that help us model data and Spark
    provides many of them out of the box. However, which model to build is still a
    million dollar question. It depends on various factors, such as interpretability-accuracy
    trade-off, how much data you have at hand, categorical or numerical variables,
    time and memory constraints, and so on. In the following code example, we have
    just trained a few models at random to show you how it can be done.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种机器学习算法可以帮助我们对数据进行建模，Spark提供了其中许多。然而，要构建哪种模型仍然是一个价值百万的问题。这取决于各种因素，比如解释性和准确性的权衡、手头有多少数据、分类或数值变量、时间和内存限制等等。在下面的代码示例中，我们随机训练了一些模型，以展示如何完成这些步骤。
- en: 'We''ll be predicting the award type based on race, age, and country. We''ll
    be using the DecisionTreeClassifier, RandomForestClassifier, and OneVsRest algorithms.
    These three are chosen arbitrarily. All of them work with multiclass labels and
    are simple to understand. We have used the following evaluation metrics provided
    by the `ml` package:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将根据种族、年龄和国家来预测奖项类型。我们将使用DecisionTreeClassifier、RandomForestClassifier和OneVsRest算法。这三个是任意选择的。它们都适用于多类标签，并且易于理解。我们使用了`ml`包提供的以下评估指标：
- en: '**Accuracy**: The ratio of correctly predicted observations.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：正确预测的观察比例。'
- en: '**Weighted Precision**: Precision is the ratio of correct positive observations
    to all positive observations. Weighted precision takes the frequency of individual
    classes into account.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权精确度**：精确度是正确的正例观察值与所有正例观察值的比率。加权精确度考虑了各个类别的频率。'
- en: '**Weighted Recall**: Recall is the ratio of positives to actual positives.
    Actual positives are the sum of true positives and false negatives. Weighted Recall
    takes the frequency of individual classes into account.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权召回率**：召回率是正例与实际正例的比率。实际正例是真正例和假负例的总和。加权召回率考虑了各个类别的频率。'
- en: '**F1**: The default evaluation measure. This is the weighted average of Precision
    and Recall.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1**：默认的评估指标。这是精确度和召回率的加权平均值。'
- en: '**Scala:**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala：**'
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Python:**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python：**'
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'So far, we have tried a few models and found that they gives us roughly the
    same performance. There are various other ways to validate the model performance.
    This again depends on the algorithm you have used, the business context, and the
    outcome produced. Let us look at some metrics that are offered out of the box
    in the `spark.ml.evaluation` package:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们尝试了一些模型，并发现它们大致表现相同。还有其他各种验证模型性能的方法。这再次取决于你使用的算法、业务背景和产生的结果。让我们看看`spark.ml.evaluation`包中提供的一些开箱即用的指标：
- en: '**Scala:**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scala：**'
- en: '[PRE19]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Python:**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python：**'
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Output:**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '|  | **Decision tree** | **Random Forest** | **OneVsRest** |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | **决策树** | **随机森林** | **OneVsRest** |'
- en: '| F1 | 0.29579 | 0.26451 | 0.25649 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| F1 | 0.29579 | 0.26451 | 0.25649 |'
- en: '| WeightedPrecision | 0.32654 | 0.26451 | 0.25295 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 加权精确度 | 0.32654 | 0.26451 | 0.25295 |'
- en: '| WeightedRecall | 0.30827 | 0.29323 | 0.32330 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 加权召回率 | 0.30827 | 0.29323 | 0.32330 |'
- en: Upon validating the model performance, you will have to tune the model as much
    as possible. Now, tuning can happen both ways, at the data level and at the algorithm
    level. Feeding the right data that an algorithm expects is very important. The
    problem is that whatever data you feed in, the algorithm may still give some output
    - it never complains! So, apart from cleaning the data properly by treating missing
    values, treating univariate and multivariate outliers, and so on, you can create
    many more relevant features. This feature engineering is usually treated as the
    most important aspect of data science. Having decent domain expertise helps to
    engineer better features. Now, coming to the algorithmic aspect of tuning, there
    is always scope for working on optimizing the parameters that we pass to an algorithm.
    You may choose to use grid search to find the optimal parameters. Also, data scientists
    should question themselves on which loss function to use and why, and, out of
    GD, SGD, L-BFGS, and so on, which algorithm to use to optimize the loss function
    and why.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证模型性能后，你将不得不尽可能调整模型。现在，调整可以在数据级别和算法级别两种方式进行。提供算法期望的正确数据非常重要。问题在于，无论你提供什么数据，算法可能仍然会给出一些输出-它从不抱怨！因此，除了通过处理缺失值、处理一元和多元异常值等来正确清理数据之外，你还可以创建更多相关的特征。这种特征工程通常被视为数据科学中最重要的方面。具有良好的领域专业知识有助于构建更好的特征。现在，来到调整的算法方面，总是有优化我们传递给算法的参数的空间。你可以选择使用网格搜索来找到最佳参数。此外，数据科学家应该质疑自己要使用哪种损失函数以及为什么，以及在GD、SGD、L-BFGS等中，要使用哪种算法来优化损失函数以及为什么。
- en: Please note that the preceding approach is intended just to demonstrate how
    to perform the steps on Spark. Selecting one algorithm over the other by just
    looking at the accuracy level may not be the best way. Selecting an algorithm
    depends on the type of data you are dealing with, the outcome variable, the business
    problem/requirement, computational challenges, interpretability, and many others.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的方法仅用于演示如何在Spark上执行这些步骤。仅仅通过准确性水平来选择一种算法可能不是最佳方式。选择算法取决于你处理的数据类型、结果变量、业务问题/需求、计算挑战、可解释性等等。
- en: Data visualization
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可视化
- en: '**Data visualization** is something which is needed every now and then from
    the time you take on a data science assignment. Before building any model, preferably,
    you will have to visualize each variable to see their distributions to understand
    their characteristics and also find outliers so you can treat them. Simple tools
    such as scatterplot, box plot, bar chart, and so on are a few versatile, handy
    tools for such purposes. Also, you will have to use the visuals in most of the
    steps to ensure you are heading in the right direction.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据可视化**是在从事数据科学任务时经常需要的东西。在构建任何模型之前，最好是要可视化每个变量，以了解它们的分布，理解它们的特征，并找到异常值，以便进行处理。散点图、箱线图、条形图等简单工具是用于这些目的的一些多功能、方便的工具。此外，您将不得不在大多数步骤中使用可视化工具，以确保您朝着正确的方向前进。'
- en: Every time you want to collaborate with business users or stakeholders, it is
    always a good practice to convey your analysis through visuals. Visuals can accommodate
    more data in them in a more meaningful way and are inherently intuitive in nature.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 每次您想与业务用户或利益相关者合作时，通过可视化传达您的分析总是一个很好的做法。可视化可以更有意义地容纳更多的数据，并且本质上是直观的。
- en: Please note that most data science assignment outcomes are preferably represented
    through visuals and dashboards to business users. We already have a dedicated
    chapter on this topic, so we won't go deeper into it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，大多数数据科学任务的结果最好通过可视化和仪表板向业务用户呈现。我们已经有了一个专门讨论这个主题的章节，所以我们不会深入讨论。
- en: Communicating the results to business users
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向业务用户传达结果
- en: In real-life scenarios, it is mostly the case that you have to keep communicating
    with the business intermittently. You might have to build several models before
    concluding on a final production-ready model and communicate the results to the
    business.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，通常情况下，您必须不断与业务进行沟通。在最终确定生产就绪模型之前，您可能必须构建多个模型，并将结果传达给业务。
- en: 'An implementable model does not always depend on accuracy; you might have to
    bring in other measures such as sensitivity, specificity, or an ROC curve, and
    also represent your results through visuals such as a Gain/Lift chart or an output
    of a K-S test with statistical significance. Note that these techniques require
    business users'' input. This input often guides the way you build the models or
    set thresholds. Let us look at a few examples to better understand how it works:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 可实施的模型并不总是取决于准确性；您可能需要引入其他措施，如灵敏度、特异性或ROC曲线，并通过可视化来表示您的结果，比如增益/提升图表或具有统计显著性的K-S测试的输出。请注意，这些技术需要业务用户的输入。这种输入通常指导您构建模型或设置阈值的方式。让我们看一些例子，以更好地理解它是如何工作的：
- en: If a regressor predicts the probability of an event occurring, then blindly
    setting the threshold to 0.5 and assuming anything above 0.5 is 1 and less than
    0.5 is 0 may not be the best way! You may use an ROC curve and take a rather more
    scientific or logical decision.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个回归器预测事件发生的概率，那么盲目地将阈值设置为0.5，并假设大于0.5的为1，小于0.5的为0可能不是最佳方式！您可以使用ROC曲线，并做出更科学或更合乎逻辑的决定。
- en: False-negative predictions for diagnosis of a cancer test may not be desirable
    at all! This is an extreme case of life risk.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于癌症测试的假阴性预测可能根本不可取！这是一种极端的生命风险。
- en: E-mail campaigning is cheaper compared to delivery of hard copies. So the business
    may decide to send e-mails to the recipients who are predicted with less than
    0.5 (say 0.35) probability.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与寄送硬拷贝相比，电子邮件营销成本更低。因此，企业可能决定向预测概率低于0.5（比如0.35）的收件人发送电子邮件。
- en: Notice that the preceding decisions are influenced heavily by business users
    or the problem owners, and data scientists work closely with them to take a call
    on such cases.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前述决策受到业务用户或问题所有者的严重影响，数据科学家与他们密切合作，以在这些情况下做出决策。
- en: Again, as discussed already, the right visuals are the most preferred way to
    communicate the results to the business.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面已经讨论过的，正确的可视化是向业务传达结果的最佳方式。
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have taken up a case study and completed the data analytics
    life cycle end to end. During the course of building a data product, we have applied
    the knowledge gained so far in the previous chapters. We have stated a business
    problem, formed an initial hypothesis, acquired data, and prepared it for model
    building. We have tried building multiple models and found a suitable model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们进行了一个案例研究，并完成了数据分析的整个生命周期。在构建数据产品的过程中，我们应用了之前章节中所学到的知识。我们提出了一个业务问题，形成了一个初始假设，获取了数据，并准备好了模型构建。我们尝试构建多个模型，并找到了一个合适的模型。
- en: In the next chapter, which is also the final chapter, we will discuss building
    real-world applications using Spark.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，也是最后一章中，我们将讨论使用Spark构建真实世界的应用程序。
- en: References
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[http://www2.sas.com/proceedings/forum2007/073-2007.pdf](http://www2.sas.com/proceedings/forum2007/073-2007.pdf).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www2.sas.com/proceedings/forum2007/073-2007.pdf](http://www2.sas.com/proceedings/forum2007/073-2007.pdf)。'
- en: '[https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/](https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/](https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/)。'
- en: '[http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf](http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf](http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf)。'
