- en: Apache Spark - Machine Learning on Big Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark - 大数据上的机器学习
- en: So far in this book we've talked about a lot of general data mining and machine
    learning techniques that you can use in your data science career, but they've
    all been running on your desktop. As such, you can only run as much data as a
    single machine can process using technologies such as Python and scikit-learn.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们已经讨论了许多通用的数据挖掘和机器学习技术，你可以在数据科学职业中使用，但它们都在你的桌面上运行。因此，你只能使用诸如Python和scikit-learn等技术来处理单台机器可以处理的数据量。
- en: 'Now, everyone talks about big data, and odds are you might be working for a
    company that does in fact have big data to process. Big data meaning that you
    can''t actually control it all, you can''t actually wrangle it all on just one
    system. You need to actually compute it using the resources of an entire cloud,
    a cluster of computing resources. And that''s where Apache Spark comes in. Apache
    Spark is a very powerful tool for managing big data, and doing machine learning
    on large Datasets. By the end of the chapter, you will have an in-depth knowledge
    of the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个人都在谈论大数据，很可能你正在为一家实际上有大数据需要处理的公司工作。大数据意味着你实际上无法控制所有数据，你无法在一个系统上处理所有数据。你需要使用整个云、一组计算资源的集群来计算它。这就是Apache
    Spark的用武之地。Apache Spark是一个非常强大的工具，用于管理大数据，并在大规模数据集上进行机器学习。到本章结束时，你将对以下主题有深入的了解：
- en: Installing and working with Spark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和使用Spark
- en: '**Resilient Distributed Datasets** (**RDDs**)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集**（**RDDs**）'
- en: The **MLlib** (**Machine Learning Library**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLlib**（**机器学习库**）'
- en: Decision Trees in Spark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark中的决策树
- en: K-Means Clustering in Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark中的K均值聚类
- en: Installing Spark
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Spark
- en: In this section, I'm going to get you set up using Apache Spark, and show you
    some examples of actually using Apache Spark to solve some of the same problems
    that we solved using a single computer in the past in this book. The first thing
    we need to do is get Spark set up on your computer. So, we're going to walk you
    through how to do that in the next couple of sections. It's pretty straightforward
    stuff, but there are a few gotchas. So, don't just skip these sections; there
    are a few things you need to pay special attention to get Spark running successfully,
    especially on a Windows system. Let's get Apache Spark set up on your system,
    so you can actually dive in and start playing around with it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我将帮助你使用Apache Spark，并向你展示一些实际使用Apache Spark解决与本书中过去在单台计算机上解决的相同问题的示例。我们需要做的第一件事是在你的计算机上设置Spark。因此，我们将在接下来的几节中为你介绍如何做到这一点。这是相当简单的事情，但有一些需要特别注意的地方。所以，不要只是跳过这些部分；有一些东西你需要特别注意，才能成功地运行Spark，尤其是在Windows系统上。让我们在你的系统上设置Apache
    Spark，这样你就可以真正地投入其中并开始尝试一些东西。
- en: We're going to be running this just on your own desktop for now. But, the same
    programs that we're going to write in this chapter could be run on an actual Hadoop
    cluster. So, you can take these scripts that we're writing and running locally
    on your desktop in Spark standalone mode, and actually run them from the master
    node of an actual Hadoop cluster, then let it scale up to the entire power of
    a Hadoop cluster and process massive Datasets that way. Even though we're going
    to set things up to run locally on your own computer, keep in mind that these
    same concepts will scale up to running on a cluster as well.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在你自己的桌面上运行这个。但是，我们在本章中要编写的相同程序可以在实际的Hadoop集群上运行。因此，你可以将我们正在编写并在Spark独立模式下在你的桌面上运行的这些脚本，实际上从实际的Hadoop集群的主节点上运行它们，然后让它扩展到整个Hadoop集群的强大处理大规模数据集的能力。即使我们要在你自己的计算机上本地运行这些东西，也要记住这些相同的概念也可以扩展到在集群上运行。
- en: Installing Spark on Windows
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Windows上安装Spark
- en: 'Getting Spark installed on Windows involves several steps that we''ll walk
    you through here. I''m just going to assume that you''re on Windows because most
    people use this book at home. We''ll talk a little bit about dealing with other
    operating systems in a moment. If you''re already familiar with installing stuff
    and dealing with environment variables on your computer, then you can just take
    the following little cheat sheet and go off and do it. If you''re not so familiar
    with Windows internals, I will walk you through it one step at a time in the upcoming
    sections. Here are the quick steps for those Windows pros:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows上安装Spark涉及几个步骤，我们将在这里为你逐步介绍。我假设你在Windows上，因为大多数人在家里使用这本书。我们稍后会谈一下如何处理其他操作系统。如果你已经熟悉在计算机上安装东西和处理环境变量，那么你可以使用以下简短的提示表并开始操作。如果你对Windows内部不太熟悉，我将在接下来的几节中逐步为你介绍。以下是那些Windows专家的快速步骤：
- en: '**Install a JDK**: You need to first install a JDK, that''s a Java Development
    Kit. You can just go to Sun''s website and download that and install it if you
    need to. We need the JDK because, even though we''re going to be developing in
    Python during this course, that gets translated under the hood to Scala code,
    which is what Spark is developed in natively. And, Scala, in turn, runs on top
    of the Java interpreter. So, in order to run Python code, you need a Scala system,
    which will be installed by default as part of Spark. Also, we need Java, or more
    specifically Java''s interpreter, to actually run that Scala code. It''s like
    a technology layer cake.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安装JDK**：你需要首先安装JDK，即Java开发工具包。如果需要，你可以直接去Sun的网站下载并安装。我们需要JDK，因为即使在这门课程中我们将使用Python进行开发，但在底层，它会被转换为Scala代码，而Spark就是用Scala原生开发的。而Scala又是在Java解释器之上运行的。因此，为了运行Python代码，你需要一个Scala系统，这将作为Spark的一部分默认安装。此外，我们需要Java，或者更具体地说，需要Java的解释器来实际运行那些Scala代码。就像是一个技术层的蛋糕。'
- en: '**Install Python**: Obviously you''re going to need Python, but if you''ve
    gotten to this point in the book, you should already have a Python environment
    set up, hopefully with Enthought Canopy. So, we can skip this step.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安装Python**：显然，你需要Python，但如果你已经阅读到这本书的这一部分，你应该已经设置好了Python环境，希望是Enthought
    Canopy。所以，我们可以跳过这一步。'
- en: '**Install a prebuilt version of Spark for Hadoop**: Fortunately, the Apache
    website makes available prebuilt versions of Spark that will just run out of the
    box that are precompiled for the latest Hadoop version. You don''t have to build
    anything, you can just download that to your computer and stick it in the right
    place and be good to go for the most part.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安装Hadoop的预编译版本的Spark**：幸运的是，Apache网站提供了预编译版本的Spark，可以直接运行，已经为最新的Hadoop版本进行了预编译。您不需要构建任何东西，只需将其下载到计算机上并放在正确的位置，大部分情况下就可以使用了。'
- en: '**Create a conf/log4j.properties file**: We have a few configuration things
    to take care of. One thing we want to do is adjust our warning level so we don''t
    get a bunch of warning spam when we run our jobs. We''ll walk through how to do
    that. Basically, you need to rename one of the properties files, and then adjust
    the error setting within it.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建conf/log4j.properties文件**：我们有一些配置要处理。我们想要做的一件事是调整警告级别，以便在运行作业时不会收到大量警告信息。我们将介绍如何做到这一点。基本上，您需要重命名一个属性文件，然后在其中调整错误设置。'
- en: '**Add a SPARK_HOME environment variable**: Next, we need to set up some environment
    variables to make sure that you can actually run Spark from any path that you
    might have. We''re going to add a SPARK_HOME environment variable pointing to
    where you installed Spark, and then we will add `%SPARK_HOME%\bin` to your system
    path, so that when you run Spark Submit, or PySpark or whatever Spark command
    you need, Windows will know where to find it.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**添加SPARK_HOME环境变量**：接下来，我们需要设置一些环境变量，以确保您可以从任何路径运行Spark。我们将添加一个指向安装Spark的SPARK_HOME环境变量，然后将`%SPARK_HOME%\bin`添加到系统路径中，这样当您运行Spark
    Submit、PySpark或其他Spark命令时，Windows就知道在哪里找到它。'
- en: '**Set a HADOOP_HOME variable**: On Windows there''s one more thing we need
    to do, we need to set a `HADOOP_HOME` variable as well because it''s going to
    expect to find one little bit of Hadoop, even if you''re not using Hadoop on your
    standalone system.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置HADOOP_HOME变量**：在Windows上，我们还需要做一件事，那就是设置`HADOOP_HOME`变量，因为即使在独立系统上不使用Hadoop，它也会期望找到Hadoop的一小部分。'
- en: '**Install winutils.exe**: Finally, we need to install a file called `winutils.exe`.
    There''s a link to `winutils.exe` within the resources for this book, so you can
    get that there.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安装winutils.exe**：最后，我们需要安装一个名为`winutils.exe`的文件。本书的资源中有`winutils.exe`的链接，您可以从那里获取。'
- en: If you want to walk through the steps in more detail, you can refer to the upcoming
    sections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想更详细地了解步骤，可以参考接下来的部分。
- en: Installing Spark on other operating systems
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在其他操作系统上安装Spark
- en: 'A quick note on installing Spark on other operating systems: the same steps
    will basically apply on them too. The main difference is going to be in how you
    set environment variables on your system, in such a way that they will automatically
    be applied whenever you log in. That''s going to vary from OS to OS. macOS does
    it differently from various flavors of Linux, so you''re going to have to be at
    least a little bit familiar with using a Unix terminal command prompt, and how
    to manipulate your environment to do that. But most macOS or Linux users who are
    doing development already have those fundamentals under their belt. And of course,
    you''re not going to need `winutils.exe` if you''re not on Windows. So, those
    are the main differences for installing on different OSes.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关于在其他操作系统上安装Spark的快速说明：基本上，这些步骤也适用于它们。主要区别在于如何在系统上设置环境变量，以便在您登录时自动应用。这将因操作系统而异。macOS的做法与各种Linux的做法不同，因此您至少需要稍微熟悉使用Unix终端命令提示符，以及如何操纵您的环境来做到这一点。但是，大多数已经掌握这些基本原理的macOS或Linux用户都不需要`winutils.exe`。因此，这些是在不同操作系统上安装的主要区别。
- en: Installing the Java Development Kit
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Java开发工具包
- en: 'For installing the Java Development Kit, go back to the browser, open a new
    tab, and just search for `jdk` (short for Java Development Kit). This will bring
    you to the Oracle site, from where you can download Java:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Java开发工具包，返回浏览器，打开一个新标签页，然后搜索`jdk`（Java开发工具包的简称）。这将带您到Oracle网站，从那里您可以下载Java。
- en: '![](img/dfda3f71-6e92-47de-8042-5809c986c13b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfda3f71-6e92-47de-8042-5809c986c13b.png)'
- en: 'On the Oracle website, click on JDK DOWNLOAD. Now, click on Accept License
    Agreement and then you can select the download option for your operating system:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在Oracle网站上，点击JDK DOWNLOAD。现在，点击Accept License Agreement，然后您可以选择适用于您操作系统的下载选项：
- en: '![](img/874e8bfe-a3b6-413a-a33f-c477b3acb6f3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/874e8bfe-a3b6-413a-a33f-c477b3acb6f3.png)'
- en: 'For me, that''s going to be Windows 64-bit, and a wait for 198 MB of goodness
    to download:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，这将是Windows 64位，等待198MB的好东西下载：
- en: '![](img/4fa3b0cf-a3f7-4032-89e9-81b98807069a.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fa3b0cf-a3f7-4032-89e9-81b98807069a.png)'
- en: 'Once the download is finished, locate the installer and start it running. Note
    that we can''t just accept the default settings in the installer on Windows here.
    So, this is a Windows-specific workaround, but as of the writing of this book,
    the current version of Spark is 2.1.1 and it turns out there''s an issue with
    Spark 2.1.1 with Java on Windows. The issue is that if you''ve installed Java
    to a path that has a space in it, it doesn''t work, so we need to make sure that
    Java is installed to a path that does not have a space in it. This means that
    you can''t skip this step even if you have Java installed already, so let me show
    you how to do that. On the installer, click on Next, and you will see, as in the
    following screen, that it wants to install by default to the `C:\Program Files\Java\jdk`
    path, whatever the version is:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后，找到安装程序并运行它。请注意，我们不能在Windows安装程序中接受默认设置。因此，这是一个特定于Windows的解决方法，但在撰写本书时，当前版本的Spark是2.1.1，结果表明Spark
    2.1.1在Windows上与Java存在问题。问题在于，如果您将Java安装到带有空格的路径中，它将无法工作，因此我们需要确保Java安装到没有空格的路径中。这意味着即使您已经安装了Java，也不能跳过此步骤，所以让我向您展示如何做到这一点。在安装程序上，点击下一步，您将看到如下屏幕，它默认要安装到`C:\Program
    Files\Java\jdk`路径，无论版本是什么：
- en: '![](img/e23f5481-70e9-419a-a294-9d84965a3314.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e23f5481-70e9-419a-a294-9d84965a3314.png)'
- en: 'The space in the `Program Files` path is going to cause trouble, so let''s
    click on the Change... button and install to `c:\jdk`, a nice simple path, easy
    to remember, and with no spaces in it:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: “Program Files”路径中的空格会引起麻烦，因此让我们单击“更改...”按钮并安装到“c:\jdk”，一个简单的路径，易于记忆，并且其中没有空格：
- en: '![](img/b84c94ed-6ede-43cb-82cb-fdee49f881b5.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b84c94ed-6ede-43cb-82cb-fdee49f881b5.png)'
- en: Now, it also wants to install the Java Runtime environment, so just to be safe,
    I'm also going to install that to a path with no spaces.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，它还希望安装Java运行时环境，因此为了安全起见，我也将其安装到没有空格的路径。
- en: 'At the second step of the JDK installation, we should have this showing on
    our screen:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在JDK安装的第二步，我们应该在屏幕上看到这个：
- en: '![](img/7023d123-557d-4ad3-afcc-126c9ca354e0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7023d123-557d-4ad3-afcc-126c9ca354e0.png)'
- en: 'I will change that destination folder as well, and we will make a new folder
    called `C:\jre` for that:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我也将更改目标文件夹，并为其创建一个名为“C:\jre”的新文件夹：
- en: '![](img/05f43aab-96ee-4ed9-acbe-e6c5323e0a8e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05f43aab-96ee-4ed9-acbe-e6c5323e0a8e.png)'
- en: Alright, successfully installed. Woohoo!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，安装成功。哇呼！
- en: Now, you'll need to remember the path that we installed the JDK into, which
    in our case was `C:\jdk`. We still have a few more steps to go here. Next, we
    need to install Spark itself.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您需要记住我们安装JDK的路径，我们的情况下是“C:\jdk”。我们还有一些步骤要走。接下来，我们需要安装Spark本身。
- en: Installing Spark
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Spark
- en: 'Let''s get back to a new browser tab here, head to [spark.apache.org](http://spark.apache.org),
    and click on the Download Spark button:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到一个新的浏览器选项卡，转到[spark.apache.org](http://spark.apache.org)，并单击“下载Spark”按钮：
- en: '![](img/86baa4bb-df53-4641-ae95-0e5de592efc2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86baa4bb-df53-4641-ae95-0e5de592efc2.png)'
- en: Now, we have used Spark 2.1.1 in this book, but anything beyond 2.0 should work
    just fine.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在本书中使用的是Spark 2.1.1，但超过2.0的任何版本都应该可以正常工作。
- en: '![](img/7b1ca113-5535-438e-88fd-dea58a24ef55.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b1ca113-5535-438e-88fd-dea58a24ef55.png)'
- en: Make sure you get a prebuilt version, and select the Direct Download option
    so all these defaults are perfectly fine. Go ahead and click on the link next
    to instruction number 4 to download that package.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您获得了预构建版本，并选择直接下载选项，因此所有这些默认设置都非常好。继续并单击第4条指示旁边的链接以下载该软件包。
- en: 'Now, it downloads a **TGZ** (**Tar in GZip**) file, which you might not be
    familiar with. Windows is kind of an afterthought with Spark quite honestly because
    on Windows, you''re not going to have a built-in utility for actually decompressing
    TGZ files. This means that you might need to install one, if you don''t have one
    already. The one I use is called WinRAR, and you can pick that up from [www.rarlab.com](http://www.rarlab.com).
    Go to the Downloads page if you need it, and download the installer for WinRAR
    32-bit or 64-bit, depending on your operating system. Install WinRAR as normal,
    and that will allow you to actually decompress TGZ files on Windows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，它下载了一个**TGZ**（**Tar in GZip**）文件，您可能不熟悉。坦率地说，Windows实际上对Spark来说有点事后诸葛亮，因为在Windows上，您将没有内置的实用程序来实际解压缩TGZ文件。这意味着您可能需要安装一个，如果您还没有的话。我使用的是WinRAR，您可以从[www.rarlab.com](http://www.rarlab.com)获取。如果需要，转到下载页面，并下载WinRAR
    32位或64位的安装程序，具体取决于您的操作系统。像平常一样安装WinRAR，这将允许您在Windows上实际解压缩TGZ文件：
- en: '![](img/a3e884d4-be31-4274-b858-9c7ed7e14ed6.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3e884d4-be31-4274-b858-9c7ed7e14ed6.jpg)'
- en: 'So, let''s go ahead and decompress the TGZ files. I''m going to open up my
    `Downloads` folder to find the Spark archive that we downloaded, and let''s go
    ahead and right-click on that archive and extract it to a folder of my choosing
    - I''m just going to put it in my `Downloads` folder for now. Again, WinRAR is
    doing this for me at this point:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们继续解压缩TGZ文件。我将打开我的“下载”文件夹，找到我们下载的Spark存档，然后右键单击该存档，并将其提取到我选择的文件夹中-我现在只是将其放在我的“下载”文件夹中。同样，此时WinRAR正在为我执行此操作：
- en: '![](img/edaa679b-44dd-4811-bbed-52b39f6488bc.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/edaa679b-44dd-4811-bbed-52b39f6488bc.png)'
- en: 'So, I should now have a folder in my `Downloads` folder associated with that
    package. Let''s open that up and there is Spark itself. You should see something
    like the folder content shown below. So, you need to install that in some place
    that you can remember:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我现在应该在我的“下载”文件夹中有一个与该软件包相关联的文件夹。让我们打开它，里面就是Spark本身。您应该看到类似下面显示的文件夹内容。因此，您需要将其安装在您可以记住的某个地方：
- en: '![](img/417d9e6f-4b8c-4a51-bb1c-694e1cf23530.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/417d9e6f-4b8c-4a51-bb1c-694e1cf23530.png)'
- en: 'You don''t want to leave it in your `Downloads` folder obviously, so let''s
    go ahead and open up a new file explorer window here. I go to my `C` drive and
    create a new folder, and let''s just call it `spark`. So, my Spark installation
    is going to live in `C:\spark`. Again, nice and easy to remember. Open that folder.
    Now, I go back to my downloaded `spark` folder and use *Ctrl* + *A* to select
    everything in the Spark distribution, *Ctrl* + *C* to copy it, and then go back
    to `C:\spark`, where I want to put it, and *Ctrl* + *V* to paste it in:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，您不希望将其留在“下载”文件夹中，所以让我们打开一个新的文件资源管理器窗口。我转到我的C驱动器并创建一个新文件夹，让我们称之为“spark”。所以，我的Spark安装将位于“C:\spark”中。再次，很容易记住。打开该文件夹。现在，我回到下载的“spark”文件夹，并使用*Ctrl*
    + *A*选择Spark分发中的所有内容，*Ctrl* + *C*将其复制，然后返回到“C:\spark”，*Ctrl* + *V*将其粘贴进去：
- en: '![](img/e5a2e48a-10c4-4849-9ef2-11833945eaa8.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5a2e48a-10c4-4849-9ef2-11833945eaa8.png)'
- en: Remembering to paste the contents of the `spark` folder, not the `spark` folder
    itself is very important. So, what I should have now is my `C` drive with a `spark`
    folder that contains all of the files and folders from the Spark distribution.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 非常重要的是要记住粘贴“spark”文件夹的内容，而不是“spark”文件夹本身。因此，我现在应该有一个包含Spark分发中所有文件和文件夹的“C”驱动器中的“spark”文件夹。
- en: 'Well, there are still a few things we need to configure. So, while we''re in
    `C:\spark` let''s open up the `conf` folder, and in order to make sure that we
    don''t get spammed to death by log messages, we''re going to change the logging
    level setting here. So to do that, right-click on the `log4j.properties.template`
    file and select Rename:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，还有一些东西我们需要配置。所以，当我们在`C:\spark`中时，让我们打开`conf`文件夹，为了确保我们不会被日志消息淹没，我们将在这里更改日志级别设置。因此，右键单击`log4j.properties.template`文件，然后选择重命名：
- en: '![](img/82bf6b9d-9ba1-40c3-b8e0-c60efbc92f77.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82bf6b9d-9ba1-40c3-b8e0-c60efbc92f77.png)'
- en: 'Delete the `.template` part of the filename to make it an actual `log4j.properties`
    file. Spark will use this to configure its logging:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 删除文件名中的`.template`部分，使其成为一个真正的`log4j.properties`文件。Spark将使用这个文件来配置它的日志记录：
- en: '![](img/9277e9dc-cb05-4d4c-95ef-415e6c0ed53d.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9277e9dc-cb05-4d4c-95ef-415e6c0ed53d.png)'
- en: 'Now, open this file in a text editor of some sort. On Windows, you might need
    to right-click there and select Open with and then WordPad. In the file, locate
    `log4j.rootCategory=INFO`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，用某种文本编辑器打开这个文件。在Windows上，你可能需要右键单击，然后选择“打开方式”，然后选择“WordPad”。在文件中，找到`log4j.rootCategory=INFO`：
- en: '![](img/36f4f644-c41d-4647-a05d-2c54a8151c23.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36f4f644-c41d-4647-a05d-2c54a8151c23.png)'
- en: Let's change this to `log4j.rootCategory=ERROR` and this will just remove the
    clutter of all the log spam that gets printed out when we run stuff. Save the
    file, and exit your editor.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把这个改成`log4j.rootCategory=ERROR`，这样就可以消除运行时打印出的所有日志垃圾。保存文件，然后退出编辑器。
- en: So far, we installed Python, Java, and Spark. Now the next thing we need to
    do is to install something that will trick your PC into thinking that Hadoop exists,
    and again this step is only necessary on Windows. So, you can skip this step if
    you're on Mac or Linux.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们安装了Python、Java和Spark。现在我们需要做的下一件事是安装一些东西，让你的电脑认为Hadoop是存在的，这一步在Windows上是必要的。所以，如果你在Mac或Linux上，可以跳过这一步。
- en: 'I have a little file available that will do the trick. Let''s go to [http://media.sundog-soft.com/winutils.exe](http://media.sundog-soft.com/winutils.exe).
    Downloading `winutils.exe` will give you a copy of a little snippet of an executable,
    which can be used to trick Spark into thinking that you actually have Hadoop:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个小文件可以解决问题。让我们去[http://media.sundog-soft.com/winutils.exe](http://media.sundog-soft.com/winutils.exe)。下载`winutils.exe`将给你一个可执行文件的一小部分副本，可以用来欺骗Spark，让它认为你实际上安装了Hadoop：
- en: '![](img/6257dac5-b9be-45d2-a9dc-58ad1ac9d705.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6257dac5-b9be-45d2-a9dc-58ad1ac9d705.png)'
- en: 'Now, since we''re going to be running our scripts locally on our desktop, it''s
    not a big deal, we don''t need to have Hadoop installed for real. This just gets
    around another quirk of running Spark on Windows. So, now that we have that, let''s
    find it in the `Downloads` folder, *Ctrl* + *C* to copy it, and let''s go to our
    `C` drive and create a place for it to live:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，因为我们将在我们的桌面上本地运行我们的脚本，这并不是什么大不了的事，我们不需要真正安装Hadoop。这只是绕过在Windows上运行Spark的另一个怪癖。所以，现在我们有了这个，让我们在“下载”文件夹中找到它，*Ctrl*
    + *C*复制它，然后让我们去我们的`C`驱动器，为它创建一个位置。
- en: '![](img/dccddda5-a8d8-47c0-b4ec-d9c636fe4513.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dccddda5-a8d8-47c0-b4ec-d9c636fe4513.png)'
- en: 'So, create a new folder again in the root `C` drive, and we will call it `winutils`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在`C`驱动器的根目录中再次创建一个新文件夹，我们将称之为`winutils`：
- en: '![](img/ca38647b-576c-4e16-ba62-027fdf776511.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca38647b-576c-4e16-ba62-027fdf776511.png)'
- en: 'Now let''s open this `winutils` folder and create a `bin` folder inside it:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打开这个`winutils`文件夹，并在其中创建一个`bin`文件夹：
- en: '![](img/240ed035-0eef-48dc-b8b7-a0ff9e7bbc98.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/240ed035-0eef-48dc-b8b7-a0ff9e7bbc98.png)'
- en: 'Now in this `bin` folder, I want you to paste the `winutils.exe` file we downloaded.
    So you should have `C:\winutils\bin` and then `winutils.exe`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在这个`bin`文件夹中，我希望你把我们下载的`winutils.exe`文件粘贴进去。所以你应该有`C:\winutils\bin`，然后`winutils.exe`：
- en: '![](img/329665c8-62c3-4959-88f5-6f7a64587d6c.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/329665c8-62c3-4959-88f5-6f7a64587d6c.png)'
- en: This next step is only required on some systems, but just to be safe, open Command
    Prompt on Windows. You can do that by going to your Start menu and going down
    to Windows System, and then clicking on Command Prompt. Here, I want you to type
    `cd c:\winutils\bin`, which is where we stuck our `winutils.exe` file. Now if
    you type `dir`, you should see that file there. Now type `winutils.exe chmod 777
    \tmp\hive`. This just makes sure that all the file permissions you need to actually
    run Spark successfully are in place without any errors. You can close Command
    Prompt now that you're done with that step. Wow, we're almost done, believe it
    or not.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个下一步只在一些系统上需要，但为了安全起见，在Windows上打开命令提示符。你可以通过转到开始菜单，然后转到Windows系统，然后点击命令提示符来做到这一点。在这里，我希望你输入`cd
    c:\winutils\bin`，这是我们放置`winutils.exe`文件的地方。现在如果你输入`dir`，你应该会看到那个文件。现在输入`winutils.exe
    chmod 777 \tmp\hive`。这只是确保你需要成功运行Spark的所有文件权限都已经放置好，没有任何错误。现在你可以关闭命令提示符了，因为你已经完成了这一步。哇，我们几乎完成了，信不信由你。
- en: 'Now we need to set some environment variables for things to work. I''ll show
    you how to do that on Windows. On Windows 10, you''ll need to open up the Start
    menu and go to Windows System | Control Panel to open up Control Panel:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要设置一些环境变量才能让事情正常运行。我将向你展示如何在Windows上做到这一点。在Windows 10上，你需要打开开始菜单，然后转到Windows系统
    | 控制面板来打开控制面板：
- en: '![](img/a2776899-1253-46d2-9de2-19bb4e45e982.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2776899-1253-46d2-9de2-19bb4e45e982.png)'
- en: 'In Control Panel, click on System and Security:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制面板中，点击系统和安全：
- en: '![](img/a0db3138-5c95-463d-be21-087a8b379cfc.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0db3138-5c95-463d-be21-087a8b379cfc.png)'
- en: 'Then, click on System:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，点击系统：
- en: '![](img/b19bf633-a93e-45f9-9614-dae791b66324.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b19bf633-a93e-45f9-9614-dae791b66324.png)'
- en: 'Then click on Advanced system settings from the list on the left-hand side:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后从左侧的列表中点击高级系统设置：
- en: '![](img/a07b0a50-a59b-4438-9d8a-7d876d861c14.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a07b0a50-a59b-4438-9d8a-7d876d861c14.png)'
- en: 'From here, click on Environment Variables...:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，点击环境变量...：
- en: '![](img/4ded259c-ba10-44cf-acdb-da295c9959c2.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ded259c-ba10-44cf-acdb-da295c9959c2.png)'
- en: 'We will get these options:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到这些选项：
- en: '![](img/bbce5217-fcad-4153-a54c-c5ac2fc3a5c7.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbce5217-fcad-4153-a54c-c5ac2fc3a5c7.png)'
- en: 'Now, this is a very Windows-specific way of setting environment variables.
    On other operating systems, you''ll use different processes, so you''ll have to
    look at how to install Spark on them. Here, we''re going to set up some new user
    variables. Click on the first New... button for a new user variable and call it
    `SPARK_HOME`, as shown below, all uppercase. This is going to point to where we
    installed Spark, which for us is `c:\spark`, so type that in as the Variable value
    and click on OK:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是一个非常特定于Windows的设置环境变量的方法。 在其他操作系统上，您将使用不同的进程，因此您需要查看如何在它们上安装Spark。 在这里，我们将设置一些新的用户变量。
    单击第一个New...按钮以创建一个新的用户变量，并将其命名为`SPARK_HOME`，如下所示，全部大写。 这将指向我们安装Spark的位置，对我们来说是`c:\spark`，因此在变量值中键入它，然后单击确定：
- en: '![](img/99ecb254-bc78-45ff-9608-c74514df43f9.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99ecb254-bc78-45ff-9608-c74514df43f9.png)'
- en: 'We also need to set up `JAVA_HOME`, so click on New... again and type in `JAVA_HOME`
    as Variable name. We need to point that to where we installed Java, which for
    us is `c:\jdk`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要设置`JAVA_HOME`，因此再次单击新建...，并键入`JAVA_HOME`作为变量名。 我们需要将其指向我们安装Java的位置，对我们来说是`c:\jdk`。
- en: '![](img/f84bdcce-2d85-4a17-888e-6e7020a1d3ed.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f84bdcce-2d85-4a17-888e-6e7020a1d3ed.png)'
- en: 'We also need to set up `HADOOP_HOME`, and that''s where we installed the `winutils`
    package, so we''ll point that to `c:\winutils`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要设置`HADOOP_HOME`，这是我们安装`winutils`软件包的位置，因此我们将其指向`c:\winutils`：
- en: '![](img/65b2e661-efd4-4c80-b6a6-d7ad02f60df2.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65b2e661-efd4-4c80-b6a6-d7ad02f60df2.png)'
- en: 'So far, so good. The last thing we need to do is to modify our path. You should
    have a PATH environment variable here:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切都很好。 我们需要做的最后一件事是修改我们的路径。 您应该在这里有一个PATH环境变量：
- en: '![](img/24ed1020-3dc3-4d51-b234-8dcb95deb08c.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24ed1020-3dc3-4d51-b234-8dcb95deb08c.png)'
- en: 'Click on the PATH environment variable, then on Edit..., and add a new path.
    This is going to be `%SPARK_HOME%\bin`, and I''m going to add another one, `%JAVA_HOME%\bin`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 单击PATH环境变量，然后单击编辑...，并添加一个新路径。 这将是`%SPARK_HOME%\bin`，我将添加另一个，`%JAVA_HOME%\bin`：
- en: '![](img/cdba2eb0-1f24-483c-a034-b7128e4e0e2d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cdba2eb0-1f24-483c-a034-b7128e4e0e2d.png)'
- en: Basically, this makes all the binary executables of Spark available to Windows,
    wherever you're running it from. Click on OK on this menu and on the previous
    two menus. We have finally everything set up.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这使得Spark的所有二进制可执行文件都可以在Windows上运行。 单击此菜单上的确定以及前两个菜单上的确定。 我们最终设置好了一切。
- en: Spark introduction
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark介绍
- en: Let's get started with a high-level overview of Apache Spark and see what it's
    all about, what it's good for, and how it works.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从高层次概述Apache Spark开始，看看它是什么，它适用于什么，以及它是如何工作的。
- en: What is Spark? Well, if you go to the Spark website, they give you a very high-level,
    hand-wavy answer, "A fast and general engine for large-scale data processing."
    It slices, it dices, it does your laundry. Well, not really. But it is a framework
    for writing jobs or scripts that can process very large amounts of data, and it
    manages distributing that processing across a cluster of computing for you. Basically,
    Spark works by letting you load your data into these large objects called Resilient
    Distributed Data stores, RDDs. It can automatically perform operations that transform
    and create actions based on those RDDs, which you can think of as large data frames.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是Spark？嗯，如果你去Spark的网站，他们会给你一个非常高层次的，模糊的答案，“一个用于大规模数据处理的快速通用引擎。” 它切片，切块，它可以洗你的衣服。
    嗯，不是真的。 但它是一个用于编写可以处理大量数据的作业或脚本的框架，并且它管理将该处理分布到计算集群中。 基本上，Spark通过让你将数据加载到称为弹性分布式数据存储的大型对象中来工作，RDDs。
    它可以自动执行转换和创建基于这些RDD的操作，你可以将其视为大型数据框架。
- en: The beauty of it is that Spark will automatically and optimally spread that
    processing out amongst an entire cluster of computers, if you have one available.
    You are no longer restricted to what you can do on a single machine or a single
    machine's memory. You can actually spread that out to all the processing capabilities
    and memory that's available to a cluster of machines, and, in this day and age,
    computing is pretty cheap. You can actually rent time on a cluster through things
    like Amazon's Elastic MapReduce service, and just rent some time on a whole cluster
    of computers for just a few dollars, and run your job that you couldn't run on
    your own desktop.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 它的美妙之处在于，Spark将自动地并且最优地将处理分布在整个计算机集群中，如果您有一个可用的话。 您不再受限于在单台计算机或单台计算机的内存上可以做什么。
    您实际上可以将其扩展到整个机器集群可用的所有处理能力和内存，而且在今天这个时代，计算是相当便宜的。 您实际上可以通过像亚马逊的弹性MapReduce服务这样的服务租用集群上的时间，并且只需花费几美元就可以在整个计算机集群上租用一些时间，并运行您无法在自己的桌面上运行的作业。
- en: It's scalable
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是可扩展的
- en: How is Spark scalable? Well, let's get a little bit more specific here in how
    it all works.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Spark如何实现可扩展性？ 好吧，让我们在这里更具体一点看看它是如何工作的。
- en: '![](img/3e61f40b-aae0-4ce4-a3bf-9ae6e2948d6c.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e61f40b-aae0-4ce4-a3bf-9ae6e2948d6c.png)'
- en: The way it works is, you write a driver program, which is just a little script
    that looks just like any other Python script really, and it uses the Spark library
    to actually write your script with. Within that library, you define what's called
    a Spark Context, which is sort of the root object that you work within when you're
    developing in Spark.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作原理是，您编写一个驱动程序，它只是一个看起来与任何其他Python脚本非常相似的小脚本，并且它使用Spark库来实际编写您的脚本。 在该库中，您定义了所谓的Spark上下文，这在您在Spark中开发时是您要使用的根对象。
- en: From there, the Spark framework kind of takes over and distributes things for
    you. So if you're running in standalone mode on your own computer, like we're
    going to be doing in these upcoming sections, it all just stays there on your
    computer, obviously. However, if you are running on a cluster manager, Spark can
    figure that out and automatically take advantage of it. Spark actually has its
    own built-in cluster manager, you can actually use it on its own without even
    having Hadoop installed, but if you do have a Hadoop cluster available to you,
    it can use that as well.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里开始，Spark框架会接管并为您分配任务。因此，如果您在自己的计算机上以独立模式运行，就像我们将在接下来的部分中进行的那样，所有任务都会留在您的计算机上。然而，如果您在集群管理器上运行，Spark可以识别并自动利用它。Spark实际上有自己内置的集群管理器，您甚至可以在没有安装Hadoop的情况下单独使用它，但如果您有可用的Hadoop集群，它也可以使用。
- en: Hadoop is more than MapReduce; there's actually a component of Hadoop called
    YARN that separates out the entire cluster management piece of Hadoop. Spark can
    interface with YARN to actually use that to optimally distribute the components
    of your processing amongst the resources available to that Hadoop cluster.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop不仅仅是MapReduce；实际上，Hadoop有一个名为YARN的组件，它将Hadoop的整个集群管理部分分离出来。Spark可以与YARN接口，实际上使用它来在Hadoop集群中有效地分配处理组件的资源。
- en: Within a cluster, you might have individual executor tasks that are running.
    These might be running on different computers, or they might be running on different
    cores of the same computer. They each have their own individual cache and their
    own individual tasks that they run. The driver program, the Spark Context and
    the cluster manager work together to coordinate all this effort and return the
    final result back to you.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中，您可能有正在运行的个别执行器任务。这些可能在不同的计算机上运行，也可能在同一台计算机的不同核心上运行。它们各自有自己的缓存和自己的任务。驱动程序、Spark
    Context和集群管理器共同协调所有这些工作，并将最终结果返回给您。
- en: The beauty of it is, all you have to do is write the initial little script,
    the driver program, which uses a Spark Context to describe at a high level the
    processing you want to do on this data. Spark, working together with the cluster
    manager that you're using, figures out how to spread that out and distribute it
    so you don't have to worry about all those details. Well, if it doesn't work,
    obviously, you might have to do some troubleshooting to figure out if you have
    enough resources available for the task at hand, but, in theory, it's all just
    magic.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 它的美妙之处在于，您只需要编写最初的小脚本，即驱动程序，它使用Spark Context在高层次上描述您想要对这些数据进行的处理。Spark与您使用的集群管理器一起工作，找出如何分散和分发，因此您不必担心所有这些细节。当然，如果不起作用，显然，您可能需要进行一些故障排除，以找出您手头的任务是否有足够的资源可用，但理论上，这都只是魔术。
- en: It's fast
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它很快
- en: What's the big deal about Spark? I mean, there are similar technologies like
    MapReduce that have been around longer. Spark is fast though, and on the website
    they claim that Spark is "up to 100x faster than MapReduce when running a job
    in memory, or 10 times faster on disk." Of course, the key words here are "up
    to," your mileage may vary. I don't think I've ever seen anything, actually, run
    that much faster than MapReduce. Some well-crafted MapReduce code can actually
    still be pretty darn efficient. But I will say that Spark does make a lot of common
    operations easier. MapReduce forces you to really break things down into mappers
    and reducers, whereas Spark is a little bit higher level. You don't have to always
    put as much thought into doing the right thing with Spark.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有什么了不起的？我的意思是，有类似的技术，比如MapReduce已经存在很长时间了。不过，Spark很快，网站上声称Spark在内存中运行作业时比MapReduce快100倍，或者在磁盘上快10倍。当然，这里的关键词是“最多”，您的情况可能有所不同。我从来没有见过任何东西实际上比MapReduce快那么多。一些精心设计的MapReduce代码实际上仍然可以非常高效。但我会说，Spark确实使许多常见操作更容易。MapReduce迫使您真正将事情分解为映射器和减速器，而Spark则更高级一些。您不必总是那么费心地使用Spark做正确的事情。
- en: Part of that leads to another reason why Spark is so fast. It has a DAG engine,
    a directed acyclic graph. Wow, that's another fancy word. What does it mean? The
    way Spark works is, you write a script that describes how to process your data,
    and you might have an RDD that's basically like a data frame. You might do some
    sort of transformation on it, or some sort of action on it. But nothing actually
    happens until you actually perform an action on that data. What happens at that
    point is, Spark will say "hmm, OK. So, this is the end result you want on this
    data. What are all the other things I had to do to get up this point, and what's
    the optimal way to lay out the strategy for getting to that point?" So, under
    the hood, it will figure out the best way to split up that processing, and distribute
    that information to get the end result that you're looking for. So, the key inside
    here, is that Spark waits until you tell it to actually produce a result, and
    only at that point does it actually go and figure out how to produce that result.
    So, it's kind of a cool concept there, and that's the key to a lot of its efficiency.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分原因之一是Spark为何如此快的原因。它有一个DAG引擎，即有向无环图。哇，这是另一个花哨的词。这是什么意思？Spark的工作方式是，您编写一个描述如何处理数据的脚本，您可能有一个RDD，基本上就像一个数据框架。您可能对其进行某种转换或某种操作。但直到您对该数据执行某种操作之前，实际上什么都不会发生。在那一点上发生的是，Spark会说“嗯，好吧。所以，这是您在这些数据上想要的最终结果。我为了达到这一点必须做的所有其他事情是什么，以及达到这一点的最佳策略是什么？”因此，在幕后，它将找出最佳的方式来分割处理，并分发信息以获得您所寻找的最终结果。因此，这里的关键是，Spark等到您告诉它实际产生结果，只有在那一点上它才会去找出如何产生那个结果。因此，这是一个很酷的概念，这是它效率的关键。
- en: It's young
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它很年轻
- en: Spark is a very hot technology, and is relatively young, so it's still very
    much emerging and changing quickly, but a lot of big people are using it. Amazon,
    for example, has claimed they're using it, eBay, NASA's Jet Propulsional Laboratories,
    Groupon, TripAdvisor, Yahoo, and many, many others have too. I'm sure there's
    a lot of companies using it that don't confess up to it, but if you go to the
    Spark Apache Wiki page at [http://spark.apache.org/powered-by.html](http://spark.apache.org/powered-by.html).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一种非常炙手可热的技术，而且相对年轻，所以它仍然在不断发展和迅速变化，但很多大公司都在使用它。例如，亚马逊声称他们在使用它，eBay，NASA的喷气推进实验室，Groupon，TripAdvisor，雅虎，还有许多其他公司也在使用。我相信有很多公司在使用它，但他们不会承认，但如果你去Spark
    Apache Wiki页面[http://spark.apache.org/powered-by.html](http://spark.apache.org/powered-by.html)。
- en: There's actually a list you can look up of known big companies that are using
    Spark to solve real-world data problems. If you are worried that you're getting
    into the bleeding edge here, fear not, you're in very good company with some very
    big people that are using Spark in production for solving real problems. It is
    pretty stable stuff at this point.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上有一个你可以查阅的已知大公司使用Spark解决实际数据问题的列表。如果你担心自己正在接触最前沿的技术，不用担心，有一些非常大的公司正在使用Spark来解决实际问题，你是和一些非常重要的人一起使用Spark来解决实际问题。在这一点上，它是相当稳定的东西。
- en: It's not difficult
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这并不困难
- en: It's also not that hard. You have your choice of programming in Python, Java,
    or Scala, and they're all built around the same concept that I just described
    earlier, that is, the Resilient Distributed Dataset, RDD for short. We'll talk
    about that in a lot more detail in the coming sections of this chapter.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这也不难。你可以选择用Python、Java或Scala编程，它们都是围绕我之前描述的相同概念构建的，即弹性分布式数据集，简称RDD。我们将在本章的后续部分详细讨论这一点。
- en: Components of Spark
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的组件
- en: Spark actually has many different components that it's built up of. So there
    is a Spark Core that lets you do pretty much anything you can dream up just using
    Spark Core functions alone, but there are these other things built on top of Spark
    that are also useful.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Spark实际上有许多不同的组件构成。因此，有一个Spark核心，只需使用Spark核心功能就可以做出几乎任何你可以想象的事情，但还有其他一些构建在Spark之上的东西也很有用。
- en: '![](img/ed75debb-a33c-49d3-99f3-e69d183abf4f.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed75debb-a33c-49d3-99f3-e69d183abf4f.png)'
- en: '**Spark Streaming**: Spark Streaming is a library that lets you actually process
    data in real time. Data can be flowing into a server continuously, say, from weblogs,
    and Spark Streaming can help you process that data in real time as you go, forever.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming**：Spark Streaming是一个库，它让你实际上可以实时处理数据。数据可以持续地流入服务器，比如来自网络日志，Spark
    Streaming可以帮助你实时处理数据，一直进行下去。'
- en: '**Spark SQL**: This lets you actually treat data as a SQL database, and actually
    issue SQL queries on it, which is kind of cool if you''re familiar with SQL already.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**：这让你实际上可以将数据视为SQL数据库，并在其上发出SQL查询，如果你已经熟悉SQL，这是很酷的。'
- en: '**MLlib**: This is what we''re going to be focusing on in this section. It
    is actually a machine learning library that lets you perform common machine learning
    algorithms, with Spark underneath the hood to actually distribute that processing
    across a cluster. You can perform machine learning on much larger Datasets than
    you could have otherwise.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLlib**：这是我们在本节中要重点关注的内容。它实际上是一个机器学习库，让你可以执行常见的机器学习算法，底层使用Spark来实际分布式处理集群中的数据。你可以对比以前能处理的更大的数据集进行机器学习。'
- en: '**GraphX**: This is not for making pretty charts and graphs. It refers to graph
    in the network theory sense. Think about a social network; that''s an example
    of a graph. GraphX just has a few functions that let you analyze the properties
    of a graph of information.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX**：这不是用来制作漂亮的图表和图形的。它是指网络理论意义上的图。想想一个社交网络；这就是图的一个例子。GraphX只有一些函数，让你分析信息图的属性。'
- en: Python versus Scala for Spark
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python与Scala在Spark中的比较
- en: I do get some flack sometimes about using Python when I'm teaching people about
    Apache Spark, but there's a method to my madness. It is true that a lot of people
    use Scala when they're writing Spark code, because that's what Spark is developed
    in natively. So, you are incurring a little bit of overhead by forcing Spark to
    translate your Python code into Scala and then into Java interpreter commands
    at the end of the day.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我在教授Apache Spark时会遇到一些批评，因为我使用Python，但我的做法是有道理的。的确，很多人在编写Spark代码时使用Scala，因为Spark是本地开发的。因此，通过强制Spark将你的Python代码转换为Scala，然后在最后一天转换为Java解释器命令，你会增加一些开销。
- en: 'However, Python''s a lot easier, and you don''t need to compile things. Managing
    dependencies is also a lot easier. You can really focus your time on the algorithms
    and what you''re doing, and less on the minutiae of actually getting it built,
    and running, and compiling, and all that nonsense. Plus, obviously, this book
    has been focused on Python so far, and it makes sense to keep using what we''ve
    learned and stick with Python throughout these lectures. Here''s a quick summary
    of the pros and cons of the two languages:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Python要容易得多，而且你不需要编译东西。管理依赖项也要容易得多。你可以真正把时间集中在算法和你正在做的事情上，而不是在实际构建、运行、编译和所有那些废话上。此外，显然，这本书到目前为止一直都在关注Python，继续使用我们学到的东西并在这些讲座中坚持使用Python是有意义的。以下是两种语言的优缺点的快速总结：
- en: '| **Python** | **Scala** |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **Python** | **Scala** |'
- en: '|'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: No need to compile, manage dependencies, etc.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需编译、管理依赖等
- en: Less coding overhead
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码开销更少
- en: You already know Python
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已经了解Python
- en: Lets us focus on the concepts instead of a new language
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们专注于概念而不是新语言
- en: '|'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Scala is probably a more popular choice with Spark
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala可能是Spark的更受欢迎的选择
- en: Spark is built in Scala, so coding in Scala is "native" to Spark
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark是用Scala构建的，所以在Scala中编码对于Spark来说是“本地”的
- en: New features, libraries tend to be Scala-first
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新功能、库往往是首先使用Scala
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'However, I will say that if you were to do some Spark programming in the real
    world, there''s a good chance people are using Scala. Don''t worry about it too
    much, though, because in Spark the Python and Scala code ends up looking very
    similar because it''s all around the same RDD concept. The syntax is very slightly
    different, but it''s not that different. If you can figure out how to do Spark
    using Python, learning how to use it in Scala isn''t that big of a leap, really.
    Here''s a quick example of the same code in the two languages:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我要说的是，如果您在现实世界中进行一些Spark编程，很有可能人们正在使用Scala。不过不要太担心，因为在Spark中，Python和Scala代码最终看起来非常相似，因为它们都围绕着相同的RDD概念。语法略有不同，但并不是很大的不同。如果您能够弄清楚如何使用Python进行Spark编程，学习如何在Scala中使用它并不是一个很大的飞跃。这里有两种语言中相同代码的快速示例：
- en: '![](img/6a204753-e6f0-415f-a359-b8d9a68c5a8e.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a204753-e6f0-415f-a359-b8d9a68c5a8e.png)'
- en: So, that's the basic concepts of Spark itself, why it's such a big deal, and
    how it's so powerful in letting you run machine learning algorithms on very large
    Datasets, or any algorithm really. Let's now talk in a little bit more detail
    about how it does that, and the core concept of the Resilient Distributed Dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是Spark本身的基本概念，为什么它如此重要，以及它如何在让您在非常大的数据集上运行机器学习算法或任何算法方面如此强大。现在让我们更详细地讨论一下它是如何做到这一点的，以及弹性分布式数据集的核心概念。
- en: Spark and Resilient Distributed Datasets (RDD)
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark和弹性分布式数据集（RDD）
- en: Let's get a little bit deeper into how Spark works. We're going to talk about
    Resilient Distributed Datasets, known as RDDs. It's sort of the core that you
    use when programming in Spark, and we'll have a few code snippets to try to make
    it real. We're going to give you a crash course in Apache Spark here. There's
    a lot more depth to it than what we're going to cover in the next few sections,
    but I'm just going to give you the basics you need to actually understand what's
    going on in these examples, and hopefully get you started and pointed in the right
    direction.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解一下Spark的工作原理。我们将谈论弹性分布式数据集，即RDD。这是您在Spark编程中使用的核心，我们将提供一些代码片段来尝试使其变得真实。我们将在这里为您提供Apache
    Spark的速成课程。比我们接下来要涵盖的内容更加深入，但我只会为您提供实际理解这些示例所需的基础知识，并希望能够让您开始并指向正确的方向。
- en: As mentioned, the most fundamental piece of Spark is called the Resilient Distributed
    Dataset, an RDD, and this is going to be the object that you use to actually load
    and transform and get the answers you want out of the data that you're trying
    to process. It's a very important thing to understand. The final letter in RDD
    stands for Dataset, and at the end of the day that's all it is; it's just a bunch
    of rows of information that can contain pretty much anything. But the key is the
    R and the first D.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Spark最基本的部分称为弹性分布式数据集，即RDD，这将是您实际用来加载、转换和获取您想要的数据的对象。这是一个非常重要的理解。RDD中的最后一个字母代表数据集，最终它只是一堆包含几乎任何内容的信息行。但关键是R和第一个D。
- en: '**Resilient**: It is resilient in that Spark makes sure that if you''re running
    this on a cluster and one of those clusters goes down, it can automatically recover
    from that and retry. Now, that resilience only goes so far, mind you. If you don''t
    have enough resources available to the job that you''re trying to run, it will
    still fail, and you will have to add more resources to it. There''s only so many
    things it can recover from; there is a limit to how many times it will retry a
    given task. But it does make its best effort to make sure that in the face of
    an unstable cluster or an unstable network it will still continue to try its best
    to run through to completion.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性**：它是弹性的，因为Spark确保如果您在集群上运行此任务并且其中一个集群出现故障，它可以自动从中恢复并重试。不过，请注意，这种弹性是有限的。如果您没有足够的资源可用于您要运行的作业，它仍然会失败，您将不得不为其添加更多资源。它只能从许多事情中恢复；它会尝试多少次重新尝试给定的任务是有限的。但它会尽最大努力确保在面对不稳定的集群或不稳定的网络时，仍然会继续尽最大努力运行到完成。'
- en: '**Distributed**: Obviously, it is distributed. The whole point of using Spark
    is that you can use it for big data problems where you can actually distribute
    the processing across the entire CPU and memory power of a cluster of computers.
    That can be distributed horizontally, so you can throw as many computers as you
    want to a given problem. The larger the problem, the more computers; there''s
    really no upper bound to what you can do there.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式**：显然，它是分布式的。使用Spark的整个目的是，您可以将其用于可以横向分布到整个计算机集群的CPU和内存功率的大数据问题。这可以水平分布，因此您可以将尽可能多的计算机投入到给定的问题中。问题越大，使用的计算机就越多；在这方面真的没有上限。'
- en: The SparkContext object
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkContext对象
- en: You always start your Spark scripts by getting a SparkContext object, and this
    is the object that embodies the guts of Spark. It is what is going to give you
    your RDDs to process on, so it is what generates the objects that you use in your
    processing.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您始终通过获取SparkContext对象来启动Spark脚本，这个对象体现了Spark的核心。它将为您提供要在其上处理的RDD，因此它生成了您在处理中使用的对象。
- en: You know, you don't actually think about the SparkContext very much when you're
    actually writing Spark programs, but it is sort of the substrate that is running
    them for you under the hood. If you're running in the Spark shell interactively,
    it has an `sc` object already available for you that you can use to create RDDs.
    In a standalone script, however, you will have to create that SparkContext explicitly,
    and you'll have to pay attention to the parameters that you use because you can
    actually tell the Spark context how you want that to be distributed. Should I
    take advantage of every core that I have available to me? Should I be running
    on a cluster or just standalone on my local computer? So, that's where you set
    up the fundamental settings of how Spark will operate.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道吗，当你实际编写Spark程序时，你并不会非常关注SparkContext，但它实际上是在幕后为你运行它们的基础。如果你在Spark shell中交互式运行，它已经为你提供了一个`sc`对象，你可以用它来创建RDD。然而，在独立脚本中，你将不得不显式创建SparkContext，并且你将不得不注意你使用的参数，因为你实际上可以告诉Spark上下文你希望它如何分布。我应该利用我可用的每个核心吗？我应该在集群上运行还是只在我的本地计算机上独立运行？所以，这就是你设置Spark操作的基本设置的地方。
- en: Creating RDDs
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建RDD
- en: Let's look at some little code snippets of actually creating RDDs, and I think
    it will all start to make a little bit more sense.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些实际创建RDD的小代码片段，我认为这一切都会开始变得更加清晰。
- en: Creating an RDD using a Python list
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python列表创建RDD
- en: 'The following is a very simple example:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个非常简单的例子：
- en: '[PRE0]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If I just want to make an RDD out of a plain old Python list, I can call the
    `parallelize()` function in Spark. That will convert a list of stuff, in this
    case, just the numbers, 1, 2, 3, 4, into an RDD object called `nums`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我只想从一个普通的Python列表中创建RDD，我可以在Spark中调用`parallelize()`函数。这将把一系列东西，比如这里的数字1、2、3、4，转换为一个名为`nums`的RDD对象。
- en: That is the simplest case of creating an RDD, just from a hard-coded list of
    stuff. That list could come from anywhere; it doesn't have to be hard-coded either,
    but that kind of defeats the purpose of big data. I mean, if I have to load the
    entire Dataset into memory before I can create an RDD from it, what's the point?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建RDD的最简单情况，只是从一个硬编码的列表中创建。该列表可以来自任何地方；它也不必是硬编码的，但这有点违背了大数据的目的。我的意思是，如果我必须在创建RDD之前将整个数据集加载到内存中，那还有什么意义呢？
- en: Loading an RDD from a text file
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文本文件加载RDD
- en: I can also load an RDD from a text file, and that could be anywhere.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我还可以从文本文件中加载RDD，它可以是任何地方。
- en: '[PRE1]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this example, I have a giant text file that's the entire encyclopedia or
    something. I'm reading that from my local disk here, but I could also use s3n
    if I want to host this file on a distributed AmazonS3 bucket, or hdfs if I want
    to refer to data that's stored on a distributed HDFS cluster (that stands for
    Hadoop Distributed File System if you're not familiar with HDFS). When you're
    dealing with big data and working with a Hadoop cluster, usually that's where
    your data will live.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我有一个巨大的文本文件，整个百科全书之类的东西。我正在从我的本地磁盘读取它，但如果我想要将这个文件托管在分布式的AmazonS3存储桶上，我也可以使用s3n，或者如果我想引用存储在分布式HDFS集群上的数据，我可以使用hdfs（如果您对HDFS不熟悉，它代表Hadoop分布式文件系统）。当你处理大数据并使用Hadoop集群时，通常你的数据会存储在那里。
- en: That line of code will actually convert every line of that text file into its
    own row in an RDD. So, you can think of the RDD as a database of rows, and, in
    this example, it will load up my text file into an RDD where every line, every
    row, contains one line of text. I can then do further processing in that RDD to
    parse or break out the delimiters in that data. But that's where I start from.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码实际上会将文本文件的每一行转换为RDD中的一行。所以，你可以把RDD看作是一行的数据库，在这个例子中，它将我的文本文件加载到一个RDD中，其中每一行，每一行，包含一行文本。然后我可以在那个RDD中进行进一步的处理，解析或分解数据中的分隔符。但这是我开始的地方。
- en: Remember when we talked about ETL and ELT earlier in the book? This is a good
    example of where you might actually be loading raw data into a system and doing
    the transform on the system itself that you used to query your data. You can take
    raw text files that haven't been processed at all and use the power of Spark to
    actually transform those into more structured data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得我们之前在书中讨论ETL和ELT吗？这是一个很好的例子，你可能实际上正在将原始数据加载到系统中，并在系统本身上进行转换，用于查询数据的系统。你可以拿未经任何处理的原始文本文件，并利用Spark的强大功能将其转换为更结构化的数据。
- en: 'It can also talk to things like Hive, so if you have an existing Hive database
    set up at your company, you can create a Hive context that''s based on your Spark
    context. How cool is that? Take a look at this example code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 它还可以与Hive等东西通信，所以如果你的公司已经设置了现有的Hive数据库，你可以创建一个基于你的Spark上下文的Hive上下文。这是多么酷啊？看看这个例子代码：
- en: '[PRE2]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can actually create an RDD, in this case called rows, that's generated by
    actually executing a SQL query on your Hive database.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上可以创建一个RDD，这里称为rows，它是通过在你的Hive数据库上实际执行SQL查询来生成的。
- en: More ways to create RDDs
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建RDD的更多方法
- en: There are more ways to create RDDs as well. You can create them from a JDBC
    connection. Basically any database that supports JDBC can also talk to Spark and
    have RDDs created from it. Cassandra, HBase, Elasticsearch, also files in JSON
    format, CSV format, sequence files object files, and a bunch of other compressed
    files like ORC can be used to create RDDs. I don't want to get into the details
    of all those, you can get a book and look those up if you need to, but the point
    is that it's very easy to create an RDD from data, wherever it might be, whether
    it's on a local filesystem or a distributed data store.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多创建RDD的方法。您可以从JDBC连接创建它们。基本上，任何支持JDBC的数据库也可以与Spark通信，并从中创建RDD。Cassandra、HBase、Elasticsearch，还有JSON格式、CSV格式、序列文件对象文件以及一堆其他压缩文件（如ORC）都可以用来创建RDD。我不想深入讨论所有这些细节，如果需要，您可以找一本书查看，但重点是很容易从数据中创建RDD，无论数据是在本地文件系统还是分布式数据存储中。
- en: Again, RDD is just a way of loading and maintaining very large amounts of data
    and keeping track of it all at once. But, conceptually within your script, an
    RDD is just an object that contains a bunch of data. You don't have to think about
    the scale, because Spark does that for you.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，RDD只是一种加载和维护大量数据并一次跟踪所有数据的方法。但是，在脚本中，概念上，RDD只是包含大量数据的对象。您不必考虑规模，因为Spark会为您处理。
- en: RDD operations
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD操作
- en: Now, there are two different types of classes of things you can do on RDDs once
    you have them, you can do transformations, and you can do actions.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦您拥有RDD，您可以对其执行两种不同类型的操作，即转换和操作。
- en: Transformations
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换
- en: 'Let''s talk about transformations first. Transformations are exactly what they
    sound like. It''s a way of taking an RDD and transforming every row in that RDD
    to a new value, based on a function you provide. Let''s look at some of those
    functions:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先谈谈转换。转换就是它听起来的样子。这是一种将RDD中的每一行根据您提供的函数转换为新值的方法。让我们看看其中一些函数：
- en: '**map() and flatmap()**: `map` and `flatmap` are the functions you''ll see
    the most often. Both of these will take any function that you can dream up, that
    will take, as input, a row of an RDD, and it will output a transformed row. For
    example, you might take raw input from a CSV file, and your `map` operation might
    take that input and break it up into individual fields based on the comma delimiter,
    and return back a Python list that has that data in a more structured format that
    you can perform further processing on. You can chain map operations together,
    so the output of one `map` might end up creating a new RDD that you then do another
    transformation on, and so on, and so forth. Again, the key is, Spark can distribute
    those transformations across the cluster, so it might take part of your RDD and
    transform it on one machine, and another part of your RDD and transform it on
    another.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**map() 和 flatmap()**: `map`和`flatmap`是您经常看到的函数。这两个函数都将接受您可以想象的任何函数，该函数将以RDD的一行作为输入，并输出一个转换后的行。例如，您可以从CSV文件中获取原始输入，您的`map`操作可能会将该输入根据逗号分隔符拆分为单独的字段，并返回一个包含以更结构化格式的数据的Python列表，以便您可以进行进一步的处理。您可以链接map操作，因此一个`map`的输出可能最终创建一个新的RDD，然后您可以对其进行另一个转换，依此类推。再次强调，关键是，Spark可以在集群上分发这些转换，因此它可能会在一台机器上转换RDD的一部分，然后在另一台机器上转换RDD的另一部分。'
- en: Like I said, `map` and `flatmap` are the most common transformations you'll
    see. The only difference is that `map` will only allow you to output one value
    for every row, whereas `flatmap` will let you actually output multiple new rows
    for a given row. So you can actually create a larger RDD or a smaller RDD than
    you started with using `flatmap.`
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我说的，`map`和`flatmap`是您将看到的最常见的转换。唯一的区别是`map`只允许您为每一行输出一个值，而`flatmap`将允许您实际上为给定的行输出多个新行。因此，您实际上可以使用`flatmap`创建一个比您开始时更大或更小的RDD。
- en: '**filter()**: `filter` can be used if what you want to do is just create a
    Boolean function that says "should this row be preserved or not? Yes or no."'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**filter()**: 如果您只想创建一个布尔函数来判断“是否应该保留此行？是或否。”'
- en: '**distinct()**: `distinct` is a less commonly used transformation that will
    only return back distinct values within your RDD.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**distinct()**: `distinct`是一个不太常用的转换，它将仅返回RDD中的不同值。'
- en: '**sample()**: This function lets you take a random sample from your RDD'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sample()**: 此函数允许您从RDD中随机抽取样本'
- en: '**union(), intersection(), subtract() and Cartesian()**: You can perform intersection
    operations like union, intersection, subtract, or even produce every cartesian
    combination that exists within an RDD.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**union(), intersection(), subtract() 和 Cartesian()**: 您可以执行诸如并集、交集、差集，甚至生成RDD中存在的每个笛卡尔组合的操作。'
- en: Using map()
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用map()
- en: 'Here''s a little example of how you might use the map function in your work:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是您如何在工作中使用map函数的一个小例子：
- en: '[PRE3]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's say I created an RDD just from the list 1, 2, 3, 4\. I can then call `rdd.map()`
    with a lambda function of x that takes in each row, that is, each value of that
    RDD, calls it x, and then it applies the function x multiplied by x to square
    it. If I were to then collect the output of this RDD, it would be 1, 4, 9 and
    16, because it would take each individual entry of that RDD and square it, and
    put that into a new RDD.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我只是从列表1、2、3、4创建了一个RDD。然后我可以使用一个lambda函数x调用`rdd.map()`，该函数接受每一行，也就是RDD的每个值，将其称为x，然后将函数x乘以x应用于平方。如果我然后收集此RDD的输出，它将是1、4、9和16，因为它将获取该RDD的每个单独条目并对其进行平方，然后将其放入新的RDD中。
- en: 'If you don''t remember what lambda functions are, we did talk about it a little
    bit earlier in this book, but as a refresher, the lambda function is just a shorthand
    for defining a function in line. So `rdd.map(lambda x: x*x)` is exactly the same
    thing as a separate function `def squareIt(x): return x*x`, and saying `rdd.map(squareIt)`.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您不记得lambda函数是什么，我们在本书的前面稍微谈到过，但是作为提醒，lambda函数只是定义一个内联函数的简写。因此，`rdd.map(lambda
    x: x*x)`与一个单独的函数`def squareIt(x): return x*x`是完全相同的，并且说`rdd.map(squareIt)`。'
- en: It's just a shorthand for very simple functions that you want to pass in as
    a transformation. It eliminates the need to actually declare this as a separate
    named function of its own. That's the whole idea of functional programming. So
    you can say you understand functional programming now, by the way! But really,
    it's just shorthand notation for defining a function inline as part of the parameters
    to a `map()` function, or any transformation for that matter.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个非常简单的函数的简写，您希望将其作为转换传递。它消除了实际将其声明为自己的单独命名函数的需要。这就是函数式编程的整个理念。所以你现在可以说你理解函数式编程了！但实际上，这只是定义一个内联函数作为`map()`函数的参数之一，或者任何转换的简写符号。
- en: Actions
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行动
- en: 'You can also perform actions on an RDD, when you want to actually get a result.
    Here are some examples of what you can do:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以对RDD执行操作，当您真正想要获得结果时。以下是一些您可以执行的示例：
- en: '`collect()`: You can call collect() on an RDD, which will give you back a plain
    old Python object that you can then iterate through and print out the results,
    or save them to a file, or whatever you want to do.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collect()`: 您可以在RDD上调用collect()，这将为您提供一个普通的Python对象，然后您可以遍历并打印结果，或将其保存到文件，或者您想做的任何其他事情。'
- en: '`count()`: You can also call `count()`, which will force it to actually go
    count how many entries are in the RDD at this point.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count()`: 您还可以调用`count()`，这将强制其实际上计算此时RDD中有多少条目。'
- en: '`countByValue()`: This function will give you a breakdown of how many times
    each unique value within that RDD occurs.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`countByValue()`: 此函数将为您提供RDD中每个唯一值出现的次数的统计。'
- en: '`take()`: You can also sample from the RDD using `take()`, which will take
    a random number of entries from the RDD.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`take()`: 您还可以使用`take()`从RDD中进行抽样，它将从RDD中获取随机数量的条目。'
- en: '`top()`: `top()` will give you the first few entries in that RDD if you just
    want to get a little peek into what''s in there for debugging purposes.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top()`: 如果您只想为了调试目的查看RDD中的前几个条目，`top()`将为您提供这些条目。'
- en: '`reduce()`: The more powerful action is `reduce()` which will actually let
    you combine values together for the same common key value. You can also use RDDs
    in the context of key-value data. The `reduce()` function lets you define a way
    of combining together all the values for a given key. It is very much similar
    in spirit to MapReduce. `reduce()` is basically the analogous operation to a `reducer()`
    in MapReduce, and `map()` is analogous to a `mapper()`. So, it''s often very straightforward
    to actually take a MapReduce job and convert it to Spark by using these functions.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce()`: 更强大的操作是`reduce()`，它实际上允许您将相同的公共键值的值组合在一起。您还可以在键-值数据的上下文中使用RDD。`reduce()`函数允许您定义一种将给定键的所有值组合在一起的方式。它在精神上与MapReduce非常相似。`reduce()`基本上是MapReduce中`reducer()`的类似操作，而`map()`类似于`mapper()`。因此，通过使用这些函数，实际上很容易将MapReduce作业转换为Spark。'
- en: Remember, too, that nothing actually happens in Spark until you call an action.
    Once you call one of those action methods, that's when Spark goes out and does
    its magic with directed acyclic graphs, and actually computes the optimal way
    to get the answer you want. But remember, nothing really occurs until that action
    happens. So, that can sometimes trip you up when you're writing Spark scripts,
    because you might have a little print statement in there, and you might expect
    to get an answer, but it doesn't actually appear until the action is actually
    performed.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得，在Spark中实际上什么都不会发生，直到您调用一个操作。一旦调用其中一个操作方法，Spark就会出去并使用有向无环图进行其魔术，并实际计算获得所需答案的最佳方式。但请记住，直到发生那个操作，实际上什么都不会发生。因此，当您编写Spark脚本时，有时可能会遇到问题，因为您可能在其中有一个小的打印语句，并且您可能期望得到一个答案，但实际上直到执行操作时才会出现。
- en: That is Spark 101 in a nutshell. Those are the basics you need for Spark programming.
    Basically, what is an RDD and what are the things you can do to an RDD. Once you
    get those concepts, then you can write some Spark code. Let's change tack now
    and talk about MLlib, and some specific features in Spark that let you do machine
    learning algorithms using Spark.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Spark编程的基础。基本上，什么是RDD以及您可以对RDD执行哪些操作。一旦掌握了这些概念，您就可以编写一些Spark代码。现在让我们改变方向，谈谈MLlib，以及Spark中一些特定的功能，让您可以使用Spark进行机器学习算法。
- en: Introducing MLlib
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍MLlib
- en: Fortunately, you don't have to do things the hard way in Spark when you're doing
    machine learning. It has a built-in component called MLlib that lives on top of
    Spark Core, and this makes it very easy to perform complex machine learning algorithms
    using massive Datasets, and distributing that processing across an entire cluster
    of computers. So, very exciting stuff. Let's learn more about what it can do.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在进行机器学习时，您不必在Spark中以困难的方式进行操作。它有一个名为MLlib的内置组件，它位于Spark Core之上，这使得使用大规模数据集执行复杂的机器学习算法变得非常容易，并将该处理分布到整个计算机集群中。非常令人兴奋的事情。让我们更多地了解它可以做什么。
- en: Some MLlib Capabilities
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些MLlib功能
- en: So, what are some of the things MLlib can do? Well, one is feature extraction.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，MLlib可以做些什么？其中之一是特征提取。
- en: One thing you can do at scale is term frequency and inverse document frequency
    stuff, and that's useful for creating, for example, search indexes. We will actually
    go through an example of that later in the chapter. The key, again, is that it
    can do this across a cluster using massive Datasets, so you could make your own
    search engine for the web with this, potentially. It also offers basic statistics
    functions, chi-squared tests, Pearson or Spearman correlation, and some simpler
    things like min, max, mean, and variance. Those aren't terribly exciting in and
    of themselves, but what is exciting is that you can actually compute the variance
    or the mean or whatever, or the correlation score, across a massive Dataset, and
    it would actually break that Dataset up into various chunks and run that across
    an entire cluster if necessary.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在规模上执行词频和逆文档频率等操作，这对于创建搜索索引非常有用。我们稍后将实际上通过本章的一个示例来进行说明。关键是，它可以使用大规模数据集在整个集群中执行此操作，因此您可以使用它来为网络创建自己的搜索引擎。它还提供基本的统计函数，卡方检验，皮尔逊或斯皮尔曼相关性，以及一些更简单的东西，如最小值，最大值，平均值和方差。这些本身并不是非常令人兴奋，但令人兴奋的是，您实际上可以计算大规模数据集的方差或平均值，或者相关性得分，如果必要，它实际上会将该数据集分解成各种块，并在整个集群中运行。
- en: So, even if some of these operations aren't terribly interesting, what's interesting
    about it is the scale at which it can operate at. It can also support things like
    linear regression and logistic regression, so if you need to fit a function to
    a massive set of data and use that for predictions, you can do that too. It also
    supports Support Vector Machines. We're getting into some of the more fancy algorithms
    here, some of the more advanced stuff, and that too can scale up to massive Datasets
    using Spark's MLlib. There is a Naive Bayes classifier built into MLlib, so, remember
    that spam classifier that we built earlier in the book? You could actually do
    that for an entire e-mail system using Spark, and scale that up as far as you
    want to.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，即使其中一些操作并不是非常有趣，有趣的是它可以操作的规模。它还支持诸如线性回归和逻辑回归之类的东西，因此如果您需要将函数拟合到大量数据集并用于预测，您也可以这样做。它还支持支持向量机。我们正在进入一些更高级的算法，一些更高级的东西，这也可以使用Spark的MLlib扩展到大规模数据集。MLlib中内置了朴素贝叶斯分类器，因此，还记得我们在本书前面构建的垃圾邮件分类器吗？您实际上可以使用Spark为整个电子邮件系统执行此操作，并根据需要扩展。
- en: Decision trees, one of my favorite things in machine learning, are also supported
    by Spark, and we'll actually have an example of that later in this chapter. We'll
    also look at K-Means clustering, and you can do clustering using K-Means and massive
    Datasets with Spark and MLlib. Even principal component analysis and **SVD** (**Singular
    Value Decomposition**) can be done with Spark as well, and we'll have an example
    of that too. And, finally, there's a built-in recommendations algorithm called
    Alternating Least Squares that's built into MLlib. Personally, I've had kind of
    mixed results with it, you know, it's a little bit too much of a black box for
    my taste, but I am a recommender system snob, so take that with a grain of salt!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树，机器学习中我最喜欢的东西之一，也受到Spark的支持，我们稍后在本章中将有一个示例。我们还将研究K均值聚类，您可以使用Spark和MLlib对大规模数据集进行聚类。甚至主成分分析和奇异值分解也可以使用Spark进行，我们也将有一个示例。最后，MLlib中内置了一种名为交替最小二乘法的推荐算法。就我个人而言，我对它的效果有些参差不齐，您知道，对于我来说，它有点太神秘了，但我是一个推荐系统的挑剔者，所以请带着一颗谨慎的心来看待这一点！
- en: Special MLlib data types
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特殊的MLlib数据类型
- en: Using MLlib is usually pretty straightforward, there are just some library functions
    you need to call. It does introduce a few new data types; however, that you need
    to know about, and one is the vector.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MLlib通常非常简单，只需要调用一些库函数。但是，它确实引入了一些新的数据类型，您需要了解一下，其中之一就是向量。
- en: The vector data type
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量数据类型
- en: Remember when we were doing movie similarities and movie recommendations earlier
    in the book? An example of a vector might be a list of all the movies that a given
    user rated. There are two types of vector, sparse and dense. Let's look at an
    example of those. There are many, many movies in the world, and a dense vector
    would actually represent data for every single movie, whether or not a user actually
    watched it. So, for example, let's say I have a user who watched Toy Story, obviously
    I would store their rating for Toy Story, but if they didn't watch the movie Star
    Wars, I would actually store the fact that there is not a number for Star Wars.
    So, we end up taking up space for all these missing data points with a dense vector.
    A sparse vector only stores the data that exists, so it doesn't waste any memory
    space on missing data, OK. So, it's a more compact form of representing a vector
    internally, but obviously that introduces some complexity while processing. So,
    it's a good way to save memory if you know that your vectors are going to have
    a lot of missing data in them.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得我们在本书前面做电影相似性和电影推荐时吗？向量的一个例子可能是给定用户评分的所有电影的列表。有两种类型的向量，稀疏和密集。让我们看看这两种的例子。世界上有很多很多电影，密集向量实际上会表示每部电影的数据，无论用户是否真的观看了它。所以，例如，假设我有一个用户观看了《玩具总动员》，显然我会存储他们对《玩具总动员》的评分，但如果他们没有观看电影《星球大战》，我实际上会存储没有《星球大战》的数字这一事实。因此，我们最终会占用所有这些缺失数据点的空间。稀疏向量只存储存在的数据，因此不会浪费任何内存空间在缺失数据上。因此，它是一种更紧凑的内部向量表示形式，但显然在处理时会引入一些复杂性。因此，如果您知道您的向量中将有很多缺失数据，这是一种节省内存的好方法。
- en: LabeledPoint data type
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带标签的点数据类型
- en: There's also a `LabeledPoint` data type that comes up, and that's just what
    it sounds like, a point that has some sort of label associated with it that conveys
    the meaning of this data in human readable terms.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个`LabeledPoint`数据类型，它就像它听起来的那样，一个带有某种标签的点，以人类可读的方式传达这些数据的含义。
- en: Rating data type
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评级数据类型
- en: Finally, there is a `Rating` data type that you'll encounter if you're using
    recommendations with MLlib. This data type can take in a rating that represents
    a 1-5 or 1-10, whatever star rating a person might have, and use that to inform
    product recommendations automatically.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您在使用MLlib进行推荐，您将遇到`Rating`数据类型。这种数据类型可以接受代表1-5或1-10的评级，无论一个人可能有什么星级评价，并使用它来自动提供产品推荐。
- en: So, I think you finally have everything you need to get started, let's dive
    in and actually look at some real MLlib code and run it, and then it will make
    a lot more sense.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我认为您终于有了开始的一切，让我们深入实际查看一些真正的MLlib代码并运行它，然后它将变得更加清晰。
- en: Decision Trees in Spark with MLlib
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark中使用MLlib的决策树
- en: 'Alright, let''s actually build some decision trees using Spark and the MLlib
    library, this is very cool stuff. Wherever you put the course materials for this
    book, I want you to go to that folder now. Make sure you''re completely closed
    out of Canopy, or whatever environment you''re using for Python development, because
    I want to make sure you''re starting it from this directory, OK? And find the
    `SparkDecisionTree` script, and double-click that to open up Canopy:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，让我们使用Spark和MLlib库实际构建一些决策树，这是非常酷的东西。无论你把这本书的课程材料放在哪里，我希望你现在就去那个文件夹。确保你完全关闭了Canopy，或者你用于Python开发的任何环境，因为我想确保你是从这个目录开始的，好吗？然后找到`SparkDecisionTree`脚本，双击打开Canopy：
- en: '![](img/ad29a0a7-b494-4b34-9ab7-0430ffb3c225.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad29a0a7-b494-4b34-9ab7-0430ffb3c225.png)'
- en: Now, up until this point we've been using IPython notebooks for our code, but
    you can't really use those very well with Spark. With Spark scripts, you need
    to actually submit them to the Spark infrastructure and run them in a very special
    way, and we'll see how that works shortly.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在这一点上，我们一直在使用IPython笔记本来编写我们的代码，但是你不能真正很好地使用它们与Spark。对于Spark脚本，你需要实际将它们提交到Spark基础设施并以非常特殊的方式运行它们，我们很快就会看到它是如何工作的。
- en: Exploring decision trees code
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索决策树代码
- en: So, we are just looking at a raw Python script file now, without any of the
    usual embellishment of the IPython notebook stuff. let's walk through what's going
    on in the script.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们只是看一个原始的Python脚本文件，没有IPython笔记本的通常修饰。让我们来看看脚本中发生了什么。
- en: '![](img/6d667f6b-c68c-489a-a94b-5582b37634f0.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d667f6b-c68c-489a-a94b-5582b37634f0.png)'
- en: We'll go through it slowly, because this is your first Spark script that you've
    seen in this book.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会慢慢来，因为这是你在本书中看到的第一个Spark脚本。
- en: First, we're going to import, from `pyspark.mllib`, the bits that we need from
    the machine learning library for Spark.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从`pyspark.mllib`中导入我们在Spark机器学习库中需要的部分。
- en: '[PRE4]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We need the `LabeledPoint` class, which is a data type required by the `DecisionTree`
    class, and the `DecisionTree` class itself, imported from `mllib.tree`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要`LabeledPoint`类，这是`DecisionTree`类所需的数据类型，以及从`mllib.tree`导入的`DecisionTree`类本身。
- en: 'Next, pretty much every Spark script you see is going to include this line,
    where we import `SparkConf` and `SparkContext`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你会看到几乎每个Spark脚本都会包含这一行，我们在其中导入`SparkConf`和`SparkContext`：
- en: '[PRE5]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is needed to create the `SparkContext` object that is kind of the root
    of everything you do in Spark.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建`SparkContext`对象所需的，它是你在Spark中做任何事情的根本。
- en: 'And finally, we''re going to import the array library from `numpy`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将从`numpy`中导入数组库：
- en: '[PRE6]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Yes, you can still use `NumPy`, and `scikit-learn`, and whatever you want within
    Spark scripts. You just have to make sure, first of all, that these libraries
    are installed on every machine that you intend to run it on.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，你仍然可以在Spark脚本中使用`NumPy`、`scikit-learn`或者任何你想要的东西。你只需要确保首先这些库在你打算在其上运行的每台机器上都已安装好。
- en: If you're running on a cluster, you need to make sure that those Python libraries
    are already in place somehow, and you also need to understand that Spark will
    not make the scikit-learn methods, for example, magically scalable. You can still
    call these functions in the context of a given map function, or something like
    that, but it's only going to run on that one machine within that one process.
    Don't lean on that stuff too heavily, but, for simple things like managing arrays,
    it's totally an okay thing to do.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在集群上运行，你需要确保这些Python库已经以某种方式安装好了，并且你还需要明白，Spark不会使scikit-learn的方法等变得可扩展。你仍然可以在给定map函数的上下文中调用这些函数，但它只会在那一个机器的一个进程中运行。不要过分依赖这些东西，但是对于像管理数组这样的简单事情，这是完全可以的。
- en: Creating the SparkContext
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建SparkContext
- en: Now, we'll start by setting up our `SparkContext`, and giving it a `SparkConf`,
    a configuration.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将开始设置我们的`SparkContext`，并给它一个`SparkConf`，一个配置。
- en: '[PRE7]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This configuration object says, I'm going to set the master node to "`local`",
    and this means that I'm just running on my own local desktop, I'm not actually
    running on a cluster at all, and I'm just going to run in one process. I'm also
    going to give it an app name of "`SparkDecisionTree`," and you can call that whatever
    you want, Fred, Bob, Tim, whatever floats your boat. It's just what this job will
    appear as if you were to look at it in the Spark console later on.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置对象表示，我将把主节点设置为"`local`"，这意味着我只是在自己的本地桌面上运行，我实际上根本不是在集群上运行，我只会在一个进程中运行。我还会给它一个应用程序名称"`SparkDecisionTree`"，你可以随意命名它，Fred、Bob、Tim，随你喜欢。这只是当你稍后在Spark控制台中查看时，这个作业将显示为什么。
- en: 'And then we will create our `SparkContext` object using that configuration:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用该配置创建我们的`SparkContext`对象：
- en: '[PRE8]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That gives us an `sc` object we can use for creating RDDs.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们一个`sc`对象，我们可以用它来创建RDDs。
- en: 'Next, we have a bunch of functions:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有一堆函数：
- en: '[PRE9]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let's just get down these functions for now, and we'll come back to them later.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在先记住这些函数，稍后我们会回来再讨论它们。
- en: Importing and cleaning our data
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入和清理我们的数据
- en: Let's go to the first bit of Python code that actually gets executed in this
    script.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这个脚本中实际执行的第一部分Python代码。
- en: '![](img/ba509b70-e811-4149-b770-49de93e80b5b.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba509b70-e811-4149-b770-49de93e80b5b.png)'
- en: The first thing we're going to do is load up this `PastHires.csv` file, and
    that's the same file we used in the decision tree exercise that we did earlier
    in this book.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是加载`PastHires.csv`文件，这是我们在本书早期做决策树练习时使用的同一个文件。
- en: Let's pause quickly to remind ourselves of the content of that file. If you
    remember right, we have a bunch of attributes of job candidates, and we have a
    field of whether or not we hired those people. What we're trying to do is build
    up a decision tree that will predict - would we hire or not hire a person given
    those attributes?
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，回顾一下那个文件的内容。如果你记得的话，我们有一堆求职者的属性，还有一个字段，表示我们是否雇佣了这些人。我们要做的是建立一个决策树，来预测
    - 根据这些属性，我们是否会雇佣这个人。
- en: Now, let's take a quick peek at the `PastHires.csv`, which will be an Excel
    file.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们快速查看一下`PastHires.csv`，这将是一个Excel文件。
- en: '![](img/26da7937-34a5-4c2b-9f83-e42cb12a44ab.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26da7937-34a5-4c2b-9f83-e42cb12a44ab.png)'
- en: You can see that Excel actually imported this into a table, but if you were
    to look at the raw text you'd see that it's made up of comma-separated values.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到Excel实际上将其导入为一个表，但如果您查看原始文本，您会发现它由逗号分隔的值组成。
- en: The first line is the actual headings of each column, so what we have above
    are the number of years of prior experience, is the candidate currently employed
    or not, number of previous employers, the level of education, whether they went
    to a top-tier school, whether they had an internship while they were in school,
    and finally, the target that we're trying to predict on, whether or not they got
    a job offer in the end of the day. Now, we need to read that information into
    an RDD so we can do something with it.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行是每列的实际标题，所以上面的内容是先前经验年数，候选人当前是否在职，以及之前的雇主数量，教育水平，是否就读于顶尖学校，是否在学校期间有实习，最后，我们试图在最后一天预测的目标，即他们是否得到了工作机会。现在，我们需要将这些信息读入RDD，以便我们可以对其进行处理。
- en: 'Let''s go back to our script:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的脚本：
- en: '[PRE10]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first thing we need to do is read that CSV data in, and we're going to throw
    away that first row, because that's our header information, remember. So, here's
    a little trick for doing that. We start off by importing every single line from
    that file into a raw data RDD, and I could call that anything I want, but we're
    calling it `sc.textFile`. SparkContext has a `textFile` function that will take
    a text file and create a new RDD, where each entry, each line of the RDD, consists
    of one line of input.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是读取CSV数据，并且我们将丢弃第一行，因为那是我们的标题信息，记住。这里有一个小技巧。我们首先从文件中导入每一行到一个原始数据RDD中，我可以随意命名它，但我们称它为`sc.textFile`。SparkContext有一个`textFile`函数，它将获取一个文本文件并创建一个新的RDD，其中每个条目，RDD的每一行，都包含一个输入行。
- en: Make sure you change the path to that file to wherever you actually installed
    it, otherwise it won't work.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将文件的路径更改为您实际安装的位置，否则它将无法工作。
- en: Now, I'm going to extract the first line, the first row from that RDD, by using
    the `first` function. So, now the header RDD will contain one entry that is just
    that row of column headers. And now, look what's going on in the above code, I'm
    using `filter` on my original data that contains all of the information in that
    CSV file, and I'm defining a `filter` function that will only let lines through
    if that line is not equal to the contents of that initial header row. What I've
    done here is, I've taken my raw CSV file and I've stripped out the first line
    by only allowing lines that do not equal that first line to survive, and I'm returning
    that back to the `rawData` RDD variable again. So, I'm taking `rawData`, filtering
    out that first line, and creating a new `rawData` that only contains the data
    itself. With me so far? It's not that complicated.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我将使用`first`函数从RDD中提取第一行，也就是第一行列标题。现在，头部RDD将包含一个条目，即列标题的那一行。现在，看看上面的代码，我在包含CSV文件中的原始数据上使用`filter`，并定义了一个`filter`函数，只有当该行不等于初始标题行的内容时，才允许该行通过。我在这里所做的是，我从我的原始CSV文件中剥离出了第一行，只允许不等于第一行的行通过，并将其返回给`rawData`
    RDD变量。所以，我从`rawData`中过滤掉了第一行，并创建了一个只包含数据本身的新`rawData`。到目前为止明白了吗？并不复杂。
- en: 'Now, we''re going to use a `map` function. What we need to do next is start
    to make more structure out of this information. Right now, every row of my RDD
    is just a line of text, it is comma-delimited text, but it''s still just a giant
    line of text, and I want to take that comma-separated value list and actually
    split it up into individual fields. At the end of the day, I want each RDD to
    be transformed from a line of text that has a bunch of information separated by
    commas into a Python list that has actual individual fields for each column of
    information that I have. So, that''s what this lambda function does:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要使用`map`函数。接下来，我们需要开始对这些信息进行更多的结构化处理。现在，我的RDD的每一行都只是一行文本，它是逗号分隔的文本，但它仍然只是一行巨大的文本，我想将逗号分隔的值列表实际分割成单独的字段。最终，我希望每个RDD都从一行文本转换为一个Python列表，其中包含我拥有的每个信息列的实际单独字段。这就是这个lambda函数的作用：
- en: '[PRE11]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It calls the built-in Python function `split`, which will take a row of input,
    and split it on comma characters, and divide that into a list of every field delimited
    by commas.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 它调用了内置的Python函数`split`，该函数将获取一行输入，并在逗号字符上进行拆分，并将其分成一个由逗号分隔的每个字段的列表。
- en: The output of this map function, where I passed in a lambda function that just
    splits every line into fields based on commas, is a new RDD called `csvData`.
    And, at this point, `csvData` is an RDD that contains, on every row, a list where
    every element is a column from my source data. Now, we're getting close.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`map`函数的输出，我传入了一个lambda函数，它只是根据逗号将每一行拆分成字段，得到了一个名为`csvData`的新RDD。此时，`csvData`是一个RDD，其中每一行都包含一个列表，其中每个元素都是源数据中的列。现在，我们接近了。
- en: 'It turns out that in order to use a decision tree with MLlib, a couple of things
    need to be true. First of all, the input has to be in the form of LabeledPoint
    data types, and it all has to be numeric in nature. So, we need to transform all
    of our raw data into data that can actually be consumed by MLlib, and that''s
    what the `createLabeledPoints` function that we skipped past earlier does. We''ll
    get to that in just a second, first here''s the call to it:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，为了在MLlib中使用决策树，需要满足一些条件。首先，输入必须是LabeledPoint数据类型，并且所有数据都必须是数字性质的。因此，我们需要将所有原始数据转换为实际可以被MLlib消耗的数据，这就是我们之前跳过的`createLabeledPoints`函数所做的事情。我们马上就会讲到，首先是对它的调用：
- en: '[PRE12]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We''re going to call a map on `csvData`, and we are going to pass it the `createLabeledPoints`
    function, which will transform every input row into something even closer to what
    we want at the end of the day. So, let''s look at what `createLabeledPoints` does:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在`csvData`上调用map，并将其传递给`createLabeledPoints`函数，该函数将将每个输入行转换为最终我们想要的东西。所以，让我们看看`createLabeledPoints`做了什么：
- en: '[PRE13]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It takes in a list of fields, and just to remind you again what that looks
    like, let''s pull up that `.csv` Excel file again:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 它接受一个字段列表，再次提醒您一下它是什么样子，让我们再次打开那个`.csv`的Excel文件：
- en: '![](img/1a544f69-91c3-4053-9c95-790c728122e3.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a544f69-91c3-4053-9c95-790c728122e3.png)'
- en: 'So, at this point, every RDD entry has a field, it''s a Python list, where
    the first element is the years of experience, second element is employed, so on
    and so forth. The problems here are that we want to convert those lists to Labeled
    Points, and we want to convert everything to numerical data. So, all these yes
    and no answers need to be converted to ones and zeros. These levels of experience
    need to be converted from names of degrees to some numeric ordinal value. Maybe
    we''ll assign the value zero to no education, one can mean BS, two can mean MS,
    and three can mean PhD, for example. Again, all these yes/no values need to be
    converted to zeros and ones, because at the end of the day, everything going into
    our decision tree needs to be numeric, and that''s what `createLabeledPoints`
    does. Now, let''s go back to the code and run through it:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，此时每个RDD条目都有一个字段，它是一个Python列表，其中第一个元素是工作经验，第二个元素是就业情况，依此类推。问题在于我们希望将这些列表转换为Labeled
    Points，并且我们希望将所有内容转换为数值数据。因此，所有这些yes和no答案都需要转换为1和0。这些经验水平需要从学位名称转换为某些数值序数值。也许我们将值0分配给没有教育，1表示学士学位，2表示硕士学位，3表示博士学位，例如。同样，所有这些yes/no值都需要转换为0和1，因为归根结底，进入我们的决策树的一切都需要是数值的，这就是`createLabeledPoints`的作用。现在，让我们回到代码并运行它：
- en: '[PRE14]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'First, it takes in our list of `StringFields` ready to convert it into `LabeledPoints`,
    where the label is the target value-was this person hired or not? 0 or 1-followed
    by an array that consists of all the other fields that we care about. So, this
    is how you create a `LabeledPoint` that the `DecisionTree MLlib` class can consume.
    So, you see in the above code that we''re converting years of experience from
    a string to an integer value, and for all the yes/no fields, we''re calling this
    `binary` function, that I defined up at the top of the code, but we haven''t discussed
    yet:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它接受我们的`StringFields`列表，准备将其转换为`LabeledPoints`，其中标签是目标值-这个人是否被雇佣？0或1-后面是由我们关心的所有其他字段组成的数组。因此，这就是您创建`DecisionTree
    MLlib`类可以使用的`LabeledPoint`的方式。因此，您可以在上面的代码中看到，我们将工作经验从字符串转换为整数值，并且对于所有的yes/no字段，我们调用了我在代码顶部定义的`binary`函数，但我们还没有讨论过：
- en: '[PRE15]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'All it does is convert the character yes to 1, otherwise it returns 0\. So,
    Y will become 1, N will become 0\. Similarly, I have a `mapEducation` function:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是将字符yes转换为1，否则返回0。所以，Y将变为1，N将变为0。同样，我有一个`mapEducation`函数：
- en: '[PRE16]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we discussed earlier, this simply converts different types of degrees to
    an ordinal numeric value in exactly the same way as our yes/no fields.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，这只是将不同类型的学位转换为与我们的yes/no字段完全相同的序数数值。
- en: 'As a reminder, this is the line of code that sent us running through those
    functions:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，这是让我们通过这些函数的代码行：
- en: '[PRE17]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: At this point, after mapping our RDD using that `createLabeledPoints` function,
    we now have a `trainingData` RDD, and this is exactly what MLlib wants for constructing
    a decision tree.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`createLabeledPoints`函数映射我们的RDD之后，我们现在有了一个`trainingData` RDD，这正是MLlib构建决策树所需要的。
- en: Creating a test candidate and building our decision tree
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建测试候选人并构建我们的决策树
- en: 'Let''s create a little test candidate we can use, so we can use our model to
    actually predict whether someone new would be hired or not. What we''re going
    to do is create a test candidate that consists of an array of the same values
    for each field as we had in the CSV file:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个小的测试候选人，这样我们就可以使用我们的模型来预测是否会雇佣某个新人。我们要做的是创建一个测试候选人，其中包含与CSV文件中每个字段相同的值的数组：
- en: '[PRE18]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s quickly compare that code with the Excel document so you can see the
    array mapping:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速将该代码与Excel文档进行比较，以便您可以看到数组映射：
- en: '![](img/d0370539-6f0f-49a4-b9c0-5adc827ce00c.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0370539-6f0f-49a4-b9c0-5adc827ce00c.png)'
- en: Again, we need to map these back to their original column representation, so
    that 10, 1, 3, 1, 0, 0 means 10 years of prior experience, currently employed,
    three previous employers, a BS degree, did not go to a top-tier school and did
    not do an internship. We could actually create an entire RDD full of candidates
    if we wanted to, but we'll just do one for now.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们需要将它们映射回它们的原始列表示，以便10、1、3、1、0、0表示10年的工作经验，目前就业，三个以前的雇主，学士学位，没有上过一流学校，也没有做实习。如果我们愿意，我们实际上可以创建一个完整的RDD候选人，但现在我们只做一个。
- en: 'Next, we''ll use parallelize to convert that list into an RDD:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用parallelize将该列表转换为RDD：
- en: '[PRE19]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Nothing new there. Alright, now for the magic let''s move to the next code
    block:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 没有新东西。好了，现在让我们移动到下一个代码块：
- en: '[PRE20]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We are going to call `DecisionTree.trainClassifier`, and this is what will
    actually build our decision tree itself. We pass in our `trainingData`, which
    is just an RDD full of `LabeledPoint` arrays, `numClasses=2`, because we have,
    basically, a yes or no prediction that we''re trying to make, will this person
    be hired or not? The next parameter is called `categoricalFeaturesInfo`, and this
    is a Python dictionary that maps fields to the number of categories in each field.
    So, if you have a continuous range available to a given field, like the number
    of years of experience, you wouldn''t specify that at all in here, but for fields
    that are categorical in nature, such as what degree do they have, for example,
    that would say fieldID3, mapping to the degree attained, which has four different
    possibilities: no education, BS, MS, and PhD. For all of the yes/no fields, we''re
    mapping those to 2 possible categories, yes/no or 0/1 is what we converted those
    to.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将调用`DecisionTree.trainClassifier`，这将实际构建我们的决策树本身。我们传入我们的`trainingData`，这只是一个充满`LabeledPoint`数组的RDD，`numClasses=2`，因为我们基本上是在做一个是或否的预测，这个人会被雇佣吗？下一个参数叫做`categoricalFeaturesInfo`，这是一个Python字典，将字段映射到每个字段中的类别数。因此，如果某个字段有一个连续的范围可用，比如工作经验的年数，你就不需要在这里指定它，但对于那些具有分类特性的字段，比如他们拥有什么学位，例如，那会说字段ID3，映射到所获得的学位，有四种不同的可能性：没有教育、学士、硕士和博士。对于所有的是/否字段，我们将它们映射到2种可能的类别，是/否或0/1是我们将它们转换成的。
- en: Continuing to move through our `DecisionTree.trainClassifier` call, we are going
    to use the `'gini'` impurity metric as we measure the entropy. We have a `maxDepth`
    of 5, which is just an upper boundary on how far we're going to go, that can be
    larger if you wish. Finally, `maxBins` is just a way to trade off computational
    expense if you can, so it just needs to at least be the maximum number of categories
    you have in each feature. Remember, nothing really happens until we call an action,
    so we're going to actually use this model to make a prediction for our test candidate.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 继续通过我们的`DecisionTree.trainClassifier`调用，我们将使用'gini'不纯度度量作为我们测量熵的指标。我们有一个最大深度为5，这只是我们将要走多远的一个上限，如果你愿意，它可以更大。最后，`maxBins`只是一种权衡计算开销的方式，如果可以的话，它只需要至少是每个特征中你拥有的最大类别数。记住，直到我们调用一个操作之前，什么都不会发生，因此我们将实际使用这个模型来为我们的测试候选人做出预测。
- en: 'We use our `DecisionTree` model, which contains a decision tree that was trained
    on our test training data, and we tell that to make a prediction on our test data:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用我们的`DecisionTree`模型，其中包含了在我们的测试训练数据上训练的决策树，并告诉它对我们的测试数据进行预测：
- en: '[PRE21]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We''ll get back a list of predictions that we can then iterate through. So,
    `predict` returns a plain old Python object and is an action that I can `collect`.
    Let me rephrase that a little bit: `collect` will return a Python object on our
    predictions, and then we can iterate through every item in that list and print
    the result of the prediction.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到一个预测列表，然后我们可以进行迭代。因此，`predict`返回一个普通的Python对象，是我可以`collect`的一个操作。让我稍微改一下：`collect`将返回我们预测的Python对象，然后我们可以迭代遍历列表中的每个项目并打印出预测的结果。
- en: 'We can also print out the decision tree itself by using `toDebugString`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过使用`toDebugString`打印出决策树本身：
- en: '[PRE22]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: That will actually print out a little representation of the decision tree that
    it created internally, that you can follow through in your own head. So, that's
    kind of cool too.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这将实际打印出它内部创建的决策树的一个小表示，你可以在自己的头脑中跟踪。所以，这也很酷。
- en: Running the script
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行脚本
- en: Alright, feel free to take some time, stare at this script a little bit more,
    digest what's going on, but, if you're ready, let's move on and actually run this
    beast. So, to do so, you can't just run it directly from Canopy. We're going to
    go to the Tools menu and open up a Canopy Command Prompt, and this just opens
    up a Windows command prompt with all the necessary environment variables in place
    for running Python scripts in Canopy. Make sure that the working directory is
    the directory that you installed all of the course materials into.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，随意花点时间，多看一下这个脚本，消化一下正在发生的事情，但是，如果你准备好了，让我们继续并实际运行这个程序。因此，你不能直接从Canopy运行它。我们将转到工具菜单，打开Canopy命令提示符，这只是打开一个Windows命令提示符，其中包含运行Canopy中Python脚本所需的所有必要环境变量。确保工作目录是你安装所有课程材料的目录。
- en: All we need to do is call `spark-submit`, so this is a script that lets you
    run Spark scripts from Python, and then the name of the script, `SparkDecisionTree.py`.
    That's all I have to do.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的就是调用`spark-submit`，这是一个脚本，可以让你从Python运行Spark脚本，然后是脚本的名称`SparkDecisionTree.py`。这就是我需要做的全部。
- en: '[PRE23]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Hit Return, and off it will go. Again, if I were doing this on a cluster and
    I created my `SparkConf` accordingly, this would actually get distributed to the
    entire cluster, but, for now, we''re just going to run it on my computer. When
    it''s finished, you should see the below output:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 按回车键，然后它就会运行。再次强调，如果我在集群上进行操作，并且相应地创建了我的`SparkConf`，这实际上会分发到整个集群，但是现在，我们只是在我的电脑上运行它。完成后，你应该会看到下面的输出：
- en: '![](img/5982c23d-99f3-4cb4-818a-6d9b5041176f.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5982c23d-99f3-4cb4-818a-6d9b5041176f.png)'
- en: 'So, in the above image, you can see in the test person that we put in above,
    we have a prediction that this person would be hired, and I''ve also printed out
    the decision tree itself, so it''s kind of cool. Now, let''s bring up that Excel
    document once more so we can compare it to the output:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在上面的图像中，你可以看到我们上面输入的测试人员的预测是这个人会被雇佣，我也打印出了决策树本身，所以这很酷。现在，让我们再次打开那个Excel文档，这样我们就可以将其与输出进行比较：
- en: '![](img/ad3fb46a-ef3e-442a-91c1-1971c3616db4.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad3fb46a-ef3e-442a-91c1-1971c3616db4.png)'
- en: 'We can walk through this and see what it means. So, in our output decision
    tree we actually end up with a depth of four, with nine different nodes, and,
    again, if we remind ourselves what these different fields correlate to, the way
    to read this is: If (feature 1 in 0), so that means if the employed is No, then
    we drop down to feature 5\. This list is zero-based, so feature 5 in our Excel
    document is internships. We can run through the tree like that: this person is
    not currently employed, did not do an internship, has no prior years of experience
    and has a Bachelor''s degree, we would not hire this person. Then we get to the
    Else clauses. If that person had an advanced degree, we would hire them, just
    based on the data that we had that we trained it on. So, you can work out what
    these different feature IDs mean back to your original source data, remember,
    you always start counting at 0, and interpret that accordingly. Note that all
    the categorical features are expressed in Boolean in this list of possible categories
    that it saw, whereas continuous data is expressed numerically as less than or
    greater than relationships.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以逐步进行并看看它的意思。所以，在我们的输出决策树中，实际上我们最终得到了一个深度为四的树，有九个不同的节点，再次提醒一下，这些不同的字段是如何相关的，阅读的方式是：如果（特征1为0），这意味着如果受雇者为否，那么我们就会下降到特征5。这个列表是从0开始的，所以在我们的Excel文档中，特征5是实习。我们可以像这样遍历整个树：这个人目前没有工作，没有做实习，没有工作经验，有学士学位，我们不会雇佣这个人。然后我们来到了Else子句。如果这个人有高级学位，我们会雇用他们，仅仅基于我们训练的数据。所以，你可以根据这些不同的特征ID回溯到你的原始数据源，记住，你总是从0开始计数，并据此进行解释。请注意，在这个可能的类别列表中，所有的分类特征都是用布尔值表示的，而连续数据则是用数字表示小于或大于的关系。
- en: And there you have it, an actual decision tree built using Spark and MLlib that
    actually works and makes sense. Pretty awesome stuff.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，使用Spark和MLlib构建的实际决策树确实有效且有意义。非常棒的东西。
- en: K-Means Clustering in Spark
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的K-Means聚类
- en: Alright, let's look at another example of using Spark in MLlib, and this time
    we're going to look at k-means clustering, and just like we did with decision
    trees, we're going to take the same example that we did using scikit-learn and
    we're going to do it in Spark instead, so it can actually scale up to a massive
    Dataset. So, again, I've made sure to close out of everything else, and I'm going
    to go into my book materials and open up the `SparkKMeans` Python script, and
    let's study what's going on in.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，让我们看看在MLlib中使用Spark的另一个例子，这一次我们将看看k-means聚类，就像我们使用决策树一样，我们将采用与使用scikit-learn相同的例子，但这次我们将在Spark中进行，这样它就可以扩展到大规模数据集。所以，我已经确保关闭了其他所有东西，然后我将进入我的书籍材料，打开`SparkKMeans`Python脚本，让我们来研究一下其中的内容。
- en: '![](img/375a2436-ca5b-41bf-94d4-a80bedf814e1.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/375a2436-ca5b-41bf-94d4-a80bedf814e1.png)'
- en: Alright, so again, we begin with some boilerplate stuff.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，再次开始一些样板文件。
- en: '[PRE24]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We're going to import the `KMeans` package from the clustering `MLlib` package,
    we're going to import array and random from `numpy`, because, again, we're free
    to use whatever you want, this is a Python script at the end of the day, and `MLlib`
    often does require `numpy` arrays as input. We're going to import the `sqrt` function
    and the usual boilerplate stuff, we need `SparkConf` and `SparkContext`, pretty
    much every time from `pyspark`. We're also going to import the scale function
    from `scikit-learn`. Again, it's OK to use `scikit-learn` as long as you make
    sure its installed in every machine that you're going to be running this job on,
    and also don't assume that `scikit-learn` will magically scale itself up just
    because you're running it on Spark. But, since I'm only using it for the scaling
    function, it's OK. Alright, let's go ahead and set things up.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从聚类`MLlib`包中导入`KMeans`包，我们将从`numpy`中导入数组和随机数，因为，再次强调，你可以自由使用任何你想要的东西，这是一个Python脚本，`MLlib`通常需要`numpy`数组作为输入。我们将导入`sqrt`函数和通常的样板文件，我们需要从`pyspark`中几乎每次都导入`SparkConf`和`SparkContext`。我们还将从`scikit-learn`中导入缩放函数。再次强调，只要确保在你要运行这个作业的每台机器上都安装了`scikit-learn`，并且不要假设`scikit-learn`会因为在Spark上运行就会自动扩展。但是，因为我只是用它来进行缩放函数，所以没问题。好了，让我们开始设置吧。
- en: 'I''m going to create a global variable first:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我将首先创建一个全局变量：
- en: '[PRE25]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'I''m going to run k-means clustering in this example with a K of 5, meaning
    with five different clusters. I''m then going to go ahead and set up a local `SparkConf`
    just running on my own desktop:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我将使用K为5来运行k-means聚类，意味着有五个不同的簇。然后我将设置一个本地的`SparkConf`，只在我的桌面上运行：
- en: '[PRE26]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: I'm going to set the name of my application to `SparkKMeans` and create a `SparkContext`
    object that I can then use to create RDDs that run on my local machine. We'll
    skip past the `createClusteredData` function for now, and go to the first line
    of code that gets run.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我将把我的应用程序的名称设置为`SparkKMeans`，并创建一个`SparkContext`对象，然后我可以使用它来创建在我的本地机器上运行的RDD。我们暂时跳过`createClusteredData`函数，直接到第一行被运行的代码。
- en: '[PRE27]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The first thing we're going to do is create an RDD by parallelizing in some
    fake data that I'm creating, and that's what the `createClusteredData` function
    does. Basically, I'm telling you to create 100 data points clustered around K
    centroids, and this is pretty much identical to the code that we looked at when
    we played with k-means clustering earlier in the book. If you want a refresher,
    go ahead and look back at that chapter. Basically, what we're going to do is create
    a bunch of random centroids around which we normally distribute some age and income
    data. So, what we're doing is trying to cluster people based on their age and
    income, and we are fabricating some data points to do that. That returns a `numpy`
    array of our fake data.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是通过并行化一些我创建的假数据来创建一个RDD，这就是`createClusteredData`函数所做的。基本上，我告诉你创建100个围绕K个质心聚集的数据点，这与我们在本书早期玩k-means聚类时看到的代码几乎完全相同。如果你需要复习，可以回头看看那一章。基本上，我们要做的是创建一堆随机的质心，围绕它们通常分布一些年龄和收入数据。所以，我们正在尝试根据他们的年龄和收入对人进行聚类，并且我们正在制造一些数据点来做到这一点。这将返回我们的假数据的`numpy`数组。
- en: Once that result comes back from `createClusteredData`, I'm calling `scale`
    on it, and that will ensure that my ages and incomes are on comparable scales.
    Now, remember the section we studied saying you have to remember about data normalization?
    This is one of those examples where it is important, so we are normalizing that
    data with `scale` so that we get good results from k-means.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦`createClusteredData`返回结果，我会在其上调用`scale`，这将确保我的年龄和收入在可比较的尺度上。现在，记住我们学过的关于数据归一化的部分吗？这是一个重要的例子，所以我们正在使用`scale`对数据进行归一化，以便我们从k-means中得到好的结果。
- en: And finally, we parallelize the resulting list of arrays into an RDD using `parallelize`.
    Now our data RDD contains all of our fake data. All we have to do, and this is
    even easier than a decision tree, is call `KMeans.train` on our training data.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用`parallelize`将结果数组列表并行化为RDD。现在我们的数据RDD包含了所有的假数据。我们所要做的，甚至比决策树还要简单，就是在我们的训练数据上调用`KMeans.train`。
- en: '[PRE28]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We pass in the number of clusters we want, our K value, a parameter that puts
    an upper boundary on how much processing it's going to do; we then tell it to
    use the default initialization mode of k-means where we just randomly pick our
    initial centroids for our clusters before we start iterating on them, and back
    comes the model that we can use. We're going to call that `clusters`.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传入我们想要的簇的数量，我们的K值，一个参数，它对它要处理的量设置了一个上限；然后告诉它使用k-means的默认初始化模式，在我们开始迭代之前，我们只是随机选择我们的簇的初始质心，然后我们可以使用返回的模型。我们将称之为`clusters`。
- en: Alright, now we can play with that cluster.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在我们可以玩玩那个簇。
- en: 'Let''s start by printing out the cluster assignments for each one of our points.
    So, we''re going to take our original data and transform it using a lambda function:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从打印出每一个点的簇分配开始。所以，我们将使用一个lambda函数来对我们的原始数据进行转换：
- en: '[PRE29]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This function is just going to transform each point into the cluster number
    that is predicted from our model. Again, we're just taking our RDD of data points.
    We're calling `clusters.predict` to figure out which cluster our k-means model
    is assigning them to, and we're just going to put the results in our `resultRDD`.
    Now, one thing I want to point out here is this cache call, in the above code.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数只是将每个点转换为从我们的模型预测的簇编号。同样，我们只是拿着我们的数据点的RDD。我们调用`clusters.predict`来找出我们的k-means模型分配给它们的簇，然后我们将结果放入我们的`resultRDD`中。现在，我想在上面的代码中指出的一件事是这个缓存调用。
- en: An important thing when you're doing Spark is that any time you're going to
    call more than one action on an RDD, it's important to cache it first, because
    when you call an action on an RDD, Spark goes off and figures out the DAG for
    it, and how to optimally get to that result.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在做Spark时一个重要的事情是，每当你要在RDD上调用多个操作时，首先将其缓存起来是很重要的，因为当你在RDD上调用一个操作时，Spark会去计算它的DAG，以及如何最优地得到结果。
- en: It will go off and actually execute everything to get that result. So, if I
    call two different actions on the same RDD, it will actually end up evaluating
    that RDD twice, and if you want to avoid all of that extra work, you can cache
    your RDD in order to make sure that it does not recompute it more than once.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 它将去执行一切以得到结果。所以，如果我在同一个RDD上调用两个不同的操作，它实际上会评估那个RDD两次，如果你想避免所有这些额外的工作，你可以缓存你的RDD，以确保它不会被计算超过一次。
- en: 'By doing that, we make sure these two subsequent operations do the right thing:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们确保这两个后续操作做了正确的事情：
- en: '[PRE30]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In order to get an actual result, what we're going to do is use `countByValue`,
    and what that will do is give us back an RDD that has how many points are in each
    cluster. Remember, `resultRDD` currently has mapped every individual point to
    the cluster it ended up with, so now we can use `countByValue` to just count up
    how many values we see for each given cluster ID. We can then easily print that
    list out. And we can actually look at the raw results of that RDD as well, by
    calling `collect` on it, and that will give me back every single points cluster
    assignment, and we can print out all of them.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到实际的结果，我们将使用`countByValue`，它将给我们一个包含每个簇中有多少点的RDD。记住，`resultRDD`目前已经将每个单独的点映射到它最终所在的簇，所以现在我们可以使用`countByValue`来计算每个给定簇ID看到多少个值。然后我们可以轻松地打印出那个列表。我们也可以通过在其上调用`collect`来实际查看该RDD的原始结果，并打印出所有的结果。
- en: Within set sum of squared errors (WSSSE)
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在一组平方误差和（WSSSE）内
- en: 'Now, how do we measure how good our clusters are? Well, one metric for that
    is called the Within Set Sum of Squared Errors, wow, that sounds fancy! It''s
    such a big term that we need an abbreviation for it, WSSSE. All it is, we look
    at the distance from each point to its centroid, the final centroid in each cluster,
    take the square of that error and sum it up for the entire Dataset. It''s just
    a measure of how far apart each point is from its centroid. Obviously, if there''s
    a lot of error in our model then they will tend to be far apart from the centroids
    that might apply, so for that we need a higher value of K, for example. We can
    go ahead and compute that value and print it out with the following code:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何衡量我们的簇有多好呢？嗯，其中一个度量标准就是被称为簇内平方和误差（WSSSE），哇，听起来很高级！这个术语如此之大，以至于我们需要一个缩写，WSSSE。它就是我们看每个点到它所在簇的质心的距离，每个簇的最终质心，取这个误差的平方并对整个数据集进行求和。它只是衡量每个点距离它所在簇的质心有多远。显然，如果我们的模型中有很多误差，那么它们很可能会远离可能适用的质心，因此我们需要更高的K值。我们可以继续计算这个值，并用以下代码打印出来：
- en: '[PRE31]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: First of all, we define this `error` function that computes the squared error
    for each point. It just takes the distance from the point to the centroid center
    of each cluster and sums it up. To do that, we're taking our source data, calling
    a lambda function on it that actually computes the error from each centroid center
    point, and then we can chain different operations together here.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义了这个`error`函数，它计算每个点的平方误差。它只是取每个点到每个簇的质心的距离，并将它们相加。为了做到这一点，我们取我们的源数据，在其上调用一个lambda函数，实际上计算每个质心中心点的误差，然后我们可以在这里链接不同的操作。
- en: First, we call `map` to compute the error for each point. Then to get a final
    total that represents the entire Dataset, we're calling `reduce` on that result.
    So, we're doing `data.map` to compute the error for each point, and then `reduce`
    to take all of those errors and add them all together. And that's what the little
    lambda function does. This is basically a fancy way of saying, "I want you to
    add up everything in this RDD into one final result." `reduce` will take the entire
    RDD, two things at a time, and combine them together using whatever function you
    provide. The function I'm providing it above is "take the two rows that I'm combining
    together and just add them up."
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们调用`map`来计算每个点的误差。然后为了得到代表整个数据集的最终总和，我们对该结果调用`reduce`。所以，我们使用`data.map`来计算每个点的误差，然后使用`reduce`将所有这些误差相加在一起。这就是这个小lambda函数的作用。基本上就是一种高级的说法，即“我希望你把这个RDD中的所有东西加起来得到一个最终结果”。`reduce`会一次取整个RDD的两个元素，并使用你提供的任何函数将它们组合在一起。我上面提供的函数是“取我要组合在一起的两行，然后把它们加起来”。
- en: If we do that throughout every entry of the RDD, we end up with a final summed-up
    total. It might seem like a little bit of a convoluted way to just sum up a bunch
    of values, but by doing it this way we are able to make sure that we can actually
    distribute this operation if we need to. We could actually end up computing the
    sum of one piece of the data on one machine, and a sum of a different piece over
    on another machine, and then take those two sums and combine them together into
    a final result. This `reduce` function is saying, how do I take any two intermediate
    results from this operation, and combine them together?
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在RDD的每个条目中都这样做，最终我们会得到一个总和的总数。这可能看起来有点绕，但通过这种方式做，我们能够确保如果需要的话，我们实际上可以分发这个操作。我们实际上可能会在一台机器上计算数据的总和，而在另一台机器上计算不同部分的总和，然后将这两个总和组合在一起得到最终结果。这个`reduce`函数是在问，我如何将这个操作的任何两个中间结果组合在一起？
- en: 'Again, feel free to take a moment and stare at this a little bit longer if
    you want it to sink in. Nothing really fancy going on here, but there are a few
    important points:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果你想让它深入你的脑海中，可以随意花点时间盯着它看一会儿。这里没有什么特别复杂的东西，但有一些重要的要点：
- en: We introduced the use of a cache if you want to make sure that you don't do
    unnecessary recomputations on an RDD that you're going to use more than once.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们介绍了缓存的使用，如果你想确保在一个你将要多次使用的RDD上不进行不必要的重新计算。
- en: We introduced the use of the `reduce` function.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们介绍了`reduce`函数的使用。
- en: We have a couple of interesting mapper functions as well here, so there's a
    lot to learn from in this example.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还有一些有趣的映射函数在这里，所以这个例子中有很多可以学习的地方。
- en: At the end of the day, it will just do k-means clustering, so let's go ahead
    and run it.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，它只会执行k均值聚类，所以让我们继续运行它。
- en: Running the code
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行代码
- en: 'Go to the Tools menu, Canopy Command Prompt, and type in:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 转到工具菜单，Canopy命令提示符，然后输入：
- en: '[PRE32]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Hit Return, and off it will go. In this situation, you might have to wait a
    few moments for the output to appear in front of you, but you should see something
    like this:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 按回车，然后它就会运行。在这种情况下，你可能需要等待一段时间才能看到输出，但你应该会看到类似这样的东西：
- en: '![](img/eebf988f-bcec-4c78-ab13-d495aba80c3a.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eebf988f-bcec-4c78-ab13-d495aba80c3a.png)'
- en: It worked, awesome! So remember, the output that we asked for was, first of
    all, a count of how many points ended up in each cluster. So, this is telling
    us that cluster 0 had 21 points in it, cluster 1 had 20 points in it, and so on
    and so forth. It ended up pretty evenly distributed, so that's a good sign.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 它起作用了，太棒了！所以记住，我们要求的输出首先是每个簇中有多少点的计数。这告诉我们，簇0中有21个点，簇1中有20个点，依此类推。它最终分布得相当均匀，这是一个好迹象。
- en: Next, we printed out the cluster assignments for each individual point, and,
    if you remember, the original data that fabricated this data did it sequentially,
    so it's actually a good thing that you see all of the 3s together, and all the
    1s together, and all the 4s together, it looks like it started to get a little
    bit confused with the 0s and 2s, but by and large, it seems to have done a pretty
    good job of uncovering the clusters that we created the data with originally.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们打印出了每个点的聚类分配，如果你还记得，生成这些数据的原始数据是顺序的，所以看到所有的3都在一起，所有的1都在一起，所有的4都在一起，看起来它开始对0和2有点困惑，但总的来说，它似乎已经很好地揭示了我们最初创建数据的聚类。
- en: And finally, we computed the WSSSE metric, it came out to 19.97 in this example.
    So, if you want to play around with this a little bit, I encourage you to do so.
    You can see what happens to that error metric as you increase or decrease the
    values of K, and think about why that may be. You can also experiment with what
    happens if you don't normalize all the data, does that actually affect your results
    in a meaningful way? Is that actually an important thing to do? And you can also
    experiment with the `maxIterations` parameter on the model itself and get a good
    feel of what that actually does to the final results, and how important it is.
    So, feel free to mess around with it and experiment away. That's k-means clustering
    done with MLlib and Spark in a scalable manner. Very cool stuff.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算了WSSSE指标，在这个例子中为19.97。所以，如果你想玩一下，我鼓励你这样做。你可以看到当你增加或减少K的值时，错误指标会发生什么变化，并思考为什么会这样。你也可以尝试一下如果不对所有数据进行归一化会发生什么，这实际上是否会以一种有意义的方式影响你的结果？这实际上是否是一件重要的事情？你还可以尝试一下在模型本身上调整`maxIterations`参数，了解它对最终结果的实际影响以及它的重要性。所以，随意尝试并进行实验。这是使用MLlib和Spark进行可扩展的k均值聚类。非常酷。
- en: TF-IDF
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF
- en: So, our final example of MLlib is going to be using something called Term Frequency
    Inverse Document Frequency, or TF-IDF, which is the fundamental building block
    of many search algorithms. As usual, it sounds complicated, but it's not as bad
    as it sounds.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们MLlib的最后一个例子将使用一种称为词项频率逆文档频率（TF-IDF）的东西，这是许多搜索算法的基本构建块。像往常一样，听起来很复杂，但实际上并没有听起来那么糟糕。
- en: So, first, let's talk about the concepts of TF-IDF, and how we might go about
    using that to solve a search problem. And what we're actually going to do with
    TF-IDF is create a rudimentary search engine for Wikipedia using Apache Spark
    in MLlib. How awesome is that? Let's get started.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，首先，让我们谈谈TF-IDF的概念，以及我们如何使用它来解决搜索问题。我们实际上要用TF-IDF来为维基百科创建一个基本的搜索引擎，使用Apache
    Spark中的MLlib。多么棒啊？让我们开始吧。
- en: TF-IDF stands for Term Frequency and Inverse Document Frequency, and these are
    basically two metrics that are closely interrelated for doing search and figuring
    out the relevancy of a given word to a document, given a larger body of documents.
    So, for example, every article on Wikipedia might have a term frequency associated
    with it, every page on the Internet could have a term frequency associated with
    it for every word that appears in that document. Sounds fancy, but, as you'll
    see, it's a fairly simple concept.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF代表词项频率和逆文档频率，这基本上是两个密切相关的指标，用于进行搜索并确定给定单词与文档的相关性，给定更大的文档集。所以，例如，维基百科上的每篇文章可能都有与之关联的词项频率，互联网上的每个页面可能都有与之关联的词项频率，对于出现在该文档中的每个单词。听起来很花哨，但是，正如你将看到的那样，这是一个相当简单的概念。
- en: '**All Term Frequency** means is how often a given word occurs in a given document.
    So, within one web page, within one Wikipedia article, within one whatever, how
    common is a given word within that document? You know, what is the ratio of that
    word''s occurrence rate throughout all the words in that document? That''s it.
    That''s all term frequency is.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**所有词项频率**的意思就是给定单词在给定文档中出现的频率。所以，在一个网页内，在一个维基百科文章内，在一个任何地方，给定单词在该文档内有多常见？你知道，该单词在该文档中所有单词中出现率的比率是多少？就是这样。这就是词项频率的全部。'
- en: '**Document frequency**, is the same idea, but this time it is the frequency
    of that word across the entire corpus of documents. So, how often does this word
    occur throughout all of the documents that I have, all the web pages, all of the
    articles on Wikipedia, whatever. For example, common words like "a" or "the" would
    have a very high document frequency, and I would expect them to also have a very
    high term frequency, but that doesn''t necessarily mean they''re relevant to a
    given document.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档频率**，是相同的概念，但这次是该单词在整个文档语料库中的频率。所以，这个单词在我拥有的所有文档，所有网页，所有维基百科文章中出现的频率有多高。例如，像"a"或"the"这样的常见词汇会有很高的文档频率，我也期望它们在特定文档中也有很高的词项频率，但这并不一定意味着它们与给定文档相关。'
- en: You can kind of see where we're going with this. So, let's say we have a very
    high term frequency and a very low document frequency for a given word. The ratio
    of these two things can give me a measure of the relevance of that word to the
    document. So, if I see a word that occurs very often in a given document, but
    not very often in the overall space of documents, then I know that this word probably
    conveys some special meaning to this particular document. It might convey what
    this document is actually about.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看出我们要做什么。所以，假设我们有一个给定单词的词项频率很高，文档频率很低。这两者的比率可以给我一个衡量该单词与文档相关性的指标。所以，如果我看到一个单词在给定文档中经常出现，但在整个文档空间中并不经常出现，那么我知道这个单词可能对这个特定文档传达了一些特殊的含义。它可能传达了这个文档实际上是关于什么。
- en: So, that's TF-IDF. It just stands for Term Frequency x Inverse Document Frequency,
    which is just a fancy way of saying term frequency over document frequency, which
    is just a fancy way of saying how often does this word occur in this document
    compared to how often it occurs in the entire body of documents? It's that simple.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是TF-IDF。它只是词频x逆文档频率的缩写，这只是一种说词频除以文档频率的花哨方式，这只是一种说这个词在这个文档中出现的频率与它在整个文档体中出现的频率相比有多频繁的花哨方式。就是这么简单。
- en: TF-IDF in practice
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的TF-IDF
- en: In practice, there are a few little nuances to how we use this. For example,
    we use the actual log of the inverse document frequency instead of the raw value,
    and that's because word frequencies in reality tend to be distributed exponentially.
    So, by taking the log, we end up with a slightly better weighting of words, given
    their overall popularity. There are some limitations to this approach, obviously,
    one is that we basically assume a document is nothing more than a bagful of words,
    we assume there are no relationships between the words themselves. And, obviously,
    that's not always the case, and actually parsing them out can be a good part of
    the work, because you have to deal with things like synonyms and various tenses
    of words, abbreviations, capitalizations, misspellings, and so on. This gets back
    to the idea of cleaning your data being a large part of your job as a data scientist,
    and it's especially true when you're dealing with natural language processing
    stuff. Fortunately, there are some libraries out there that can help you with
    this, but it is a real problem and it will affect the quality of your results.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们在使用这个方法时有一些小细节。例如，我们使用逆文档频率的实际对数值，而不是原始值，这是因为实际上单词频率往往呈指数分布。因此，通过取对数，我们最终得到了对单词的稍微更好的加权，考虑到它们的整体流行度。显然，这种方法也有一些局限性，其中之一是我们基本上假设一个文档只是一袋词，我们假设词之间没有关系。显然，这并不总是事实，实际上解析它们可能是工作的一大部分，因为你必须处理同义词和各种时态的词、缩写、大写、拼写错误等。这又回到了清理数据作为数据科学家工作的一个重要部分的想法，特别是当你处理自然语言处理的东西时。幸运的是，有一些库可以帮助你解决这个问题，但这确实是一个真正的问题，它会影响你的结果的质量。
- en: Another implementation trick that we use with TF-IDF is, instead of storing
    actual string words with their term frequencies and inverse document frequency,
    to save space and make things more efficient, we actually map every word to a
    numerical value, a hash value we call it. The idea is that we have a function
    that can take any word, look at its letters, and assign that, in some fairly well-distributed
    manner, to a set of numbers in a range. That way, instead of using the word "represented",
    we might assign that a hash value of 10, and we can then refer to the word "represented"
    as "10" from now on. Now, if the space of your hash values isn't large enough,
    you could end up with different words being represented by the same number, which
    sounds worse than it is. But, you know, you want to make sure that you have a
    fairly large hash space so that is unlikely to happen. Those are called hash collisions.
    They can cause issues, but, in reality, there's only so many words that people
    commonly use in the English language. You can get away with 100,000 or so and
    be just fine.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在TF-IDF中使用的另一个实现技巧是，我们不是存储实际的字符串词及其词频和逆文档频率，为了节省空间并使事情更有效率，我们实际上将每个词映射到一个数值，我们称之为哈希值。这个想法是我们有一个函数，可以取任何词，查看它的字母，并以一种相当均匀分布的方式将其分配给一个数字范围内的一组数字。这样，我们可以用“10”来代表“represented”。现在，如果你的哈希值空间不够大，你可能会得到不同的词被同一个数字表示，这听起来比实际情况要糟糕。但是，你要确保你有一个相当大的哈希空间，这样才不太可能发生。这些被称为哈希冲突。它们可能会引起问题，但实际上，人们在英语中常用的词并不多。你可以用10万左右就可以了。
- en: Doing this at scale is the hard part. If you want to do this over all of Wikipedia,
    then you're going to have to run this on a cluster. But for the sake of argument,
    we are just going to run this on our own desktop for now, using a small sample
    of Wikipedia data.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在规模上做到这一点是困难的。如果你想在整个维基百科上做到这一点，那么你将不得不在一个集群上运行这个。但是为了论证，我们现在只是在我们自己的桌面上运行这个，使用维基百科数据的一个小样本。
- en: Using TF- IDF
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TF-IDF
- en: How do we turn that into an actual search problem? Once we have TF-IDF, we have
    this measure of each word's relevancy to each document. What do we do with it?
    Well, one thing you could do is compute TF-IDF for every word that we encounter
    in the entire body of documents that we have, and then, let's say we want to search
    for a given term, a given word. Let's say we want to search for "what Wikipedia
    article in my set of Wikipedia articles is most relevant to Gettysburg?" I could
    sort all the documents by their TF-IDF score for Gettysburg, and just take the
    top results, and those are my search results for Gettysburg. That's it. Just take
    your search word, compute TF-IDF, take the top results. That's it.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将这转化为一个实际的搜索问题？一旦我们有了TF-IDF，我们就有了每个词对每个文档相关性的度量。我们该怎么处理呢？嗯，你可以做的一件事是为我们遇到的整个文档体中的每个词计算TF-IDF，然后，假设我们想搜索一个给定的术语，一个给定的词。比如说我们想搜索“在我的维基百科文章集中，哪篇文章与葛底斯堡最相关？”我可以按照它们对葛底斯堡的TF-IDF得分对所有文档进行排序，然后只取前几个结果，这些就是我对葛底斯堡的搜索结果。就是这样。只需取你的搜索词，计算TF-IDF，取前几个结果。就这样。
- en: Obviously, in the real world there's a lot more to search than that. Google
    has armies of people working on this problem and it's way more complicated in
    practice, but this will actually give you a working search engine algorithm that
    produces reasonable results. Let's go ahead and dive in and see how it all works.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在现实世界中，搜索的内容要比这多得多。谷歌有大批人在解决这个问题，实际上这个问题要复杂得多，但这实际上会给你一个能产生合理结果的工作搜索引擎算法。让我们继续深入了解它是如何工作的。
- en: Searching wikipedia with Spark MLlib
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark MLlib搜索维基百科
- en: We're going to build an actual working search algorithm for a piece of Wikipedia
    using Apache Spark in MLlib, and we're going to do it all in less than 50 lines
    of code. This might be the coolest thing we do in this entire book!
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Apache Spark在MLlib中为维基百科的一部分构建一个实际工作的搜索算法，并且我们将在不到50行的代码中完成所有工作。这可能是我们在整本书中做的最酷的事情！
- en: 'Go into your course materials and open up the `TF-IDF.py` script, and that
    should open up Canopy with the following code:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 进入您的课程材料，打开`TF-IDF.py`脚本，这将打开Canopy，并显示以下代码：
- en: '![](img/87797105-16f4-4546-bb34-a435f27ece1f.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87797105-16f4-4546-bb34-a435f27ece1f.png)'
- en: Now, step back for a moment and let it sink in that we're actually creating
    a working search algorithm, along with a few examples of using it in less than
    50 lines of code here, and it's scalable. I could run this on a cluster. It's
    kind of amazing. Let's step through the code.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，暂停一下，让它沉淀下来，我们实际上正在创建一个工作的搜索算法，以及在不到50行的代码中使用它的一些示例，而且它是可扩展的。我可以在集群上运行这个。这有点令人惊讶。让我们逐步了解代码。
- en: Import statements
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入语句
- en: We're going to start by importing the `SparkConf` and `SparkContext` libraries
    that we need for any Spark script that we run in Python, and then we're going
    to import `HashingTF` and `IDF` using the following commands.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入我们在Python中运行任何Spark脚本所需的`SparkConf`和`SparkContext`库，然后使用以下命令导入`HashingTF`和`IDF`。
- en: '[PRE33]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: So, this is what computes the term frequencies (`TF`) and inverse document frequencies
    (`IDF`) within our documents.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是计算我们文档中的词项频率（`TF`）和逆文档频率（`IDF`）的方法。
- en: Creating the initial RDD
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建初始RDD
- en: We'll start off with our boilerplate Spark stuff that creates a local `SparkConfiguration`
    and a `SparkContext`, from which we can then create our initial RDD.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从创建本地`SparkConfiguration`和`SparkContext`的样板Spark内容开始，然后我们可以从中创建我们的初始RDD。
- en: '[PRE34]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Next, we're going to use our `SparkContext` to create an RDD from `subset-small.tsv`.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用我们的`SparkContext`从`subset-small.tsv`创建一个RDD。
- en: '[PRE35]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This is a file containing tab-separated values, and it represents a small sample
    of Wikipedia articles. Again, you'll need to change your path as shown in the
    preceding code as necessary for wherever you installed the course materials for
    this book.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含制表符分隔值的文件，它代表了维基百科文章的一个小样本。同样，您需要根据前面的代码所示更改路径，以适应您在本书课程材料安装的位置。
- en: That gives me back an RDD where every document is in each line of the RDD. The
    `tsv` file contains one entire Wikipedia document on every line, and I know that
    each one of those documents is split up into tabular fields that have various
    bits of metadata about each article.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '这给我返回了一个RDD，其中每个文档都在RDD的每一行中。`tsv`文件中的每一行都包含一个完整的维基百科文档，我知道每个文档都分成了包含有关每篇文章的各种元数据的表字段。 '
- en: 'The next thing I''m going to do is split those up:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我要做的是将它们分开：
- en: '[PRE36]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: I'm going to split up each document based on their tab delimiters into a Python
    list, and create a new `fields` RDD that, instead of raw input data, now contains
    Python lists of each field in that input data.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我将根据它们的制表符分隔符将每个文档分割成一个Python列表，并创建一个新的`fields` RDD，该RDD不再包含原始输入数据，而是包含该输入数据中每个字段的Python列表。
- en: 'Finally, I''m going to map that data, take in each list of fields, extract
    field number three `x[3]`, which I happen to know is the body of the article itself,
    the actual article text, and I''m in turn going to split that based on spaces:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我将映射这些数据，接收每个字段列表，提取字段编号三`x[3]`，我碰巧知道这是文章正文，实际的文章文本，然后我将根据空格拆分它：
- en: '[PRE37]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: What `x[3]` does is extract the body of the text from each Wikipedia article,
    and split it up into a list of words. My new `documents` RDD has one entry for
    every document, and every entry in that RDD contains a list of words that appear
    in that document. Now, we actually know what to call these documents later on
    when we're evaluating the results.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '`x[3]`的作用是从每篇维基百科文章中提取文本内容，并将其拆分成一个单词列表。我的新`documents` RDD中每个文档都有一个条目，该RDD中的每个条目都包含该文档中出现的单词列表。现在，我们实际上知道在评估结果时如何称呼这些文档。'
- en: 'I''m also going to create a new RDD that stores the document names:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我还将创建一个新的RDD来存储文档名称：
- en: '[PRE38]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: All that does is take that same `fields` RDD and uses this `map` function to
    extract the document name, which I happen to know is in field number one.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 所有它做的就是使用这个`map`函数从相同的`fields` RDD中提取文档名称，我碰巧知道它在字段编号一中。
- en: So, I now have two RDDs, `documents`, which contains lists of words that appear
    in each document, and `documentNames`, which contains the name of each document.
    I also know that these are in the same order, so I can actually combine these
    together later on to look up the name for a given document.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我现在有两个RDD，`documents`，其中包含每个文档中出现的单词列表，以及`documentNames`，其中包含每个文档的名称。我也知道它们是按顺序排列的，所以我实际上可以稍后将它们组合在一起，以便查找给定文档的名称。
- en: Creating and transforming a HashingTF object
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和转换HashingTF对象
- en: 'Now, the magic happens. The first thing we''re going to do is create a `HashingTF`
    object, and we''re going to pass in a parameter of 100,000\. This means that I''m
    going to hash every word into one of 100,000 numerical values:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，魔术发生了。我们要做的第一件事是创建一个`HashingTF`对象，并传入一个参数100,000。这意味着我要将每个单词哈希成100,000个数字值中的一个：
- en: '[PRE39]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Instead of representing words internally as strings, which is very inefficient,
    it's going to try to, as evenly as possible, distribute each word to a unique
    hash value. I'm giving it up to 100,000 hash values to choose from. Basically,
    this is mapping words to numbers at the end of the day.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 它不是将单词内部表示为字符串，这样效率很低，而是尝试尽可能均匀地将每个单词分配给唯一的哈希值。我给了它多达100,000个哈希值可供选择。基本上，这是将单词映射到数字。
- en: 'Next, I''m going to call `transform` on `hashingTF` with my actual RDD of documents:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将在实际的文档RDD上调用`hashingTF`的`transform`：
- en: '[PRE40]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: That's going to take my list of words in every document and convert it to a
    list of hash values, a list of numbers that represent each word instead.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把每个文档中的单词列表转换为哈希值列表，代表每个单词的数字列表。
- en: This is actually represented as a sparse vector at this point to save even more
    space. So, not only have we converted all of our words to numbers, but we've also
    stripped out any missing data. In the event that a word does not appear in a document
    where you're not storing the fact that word does not appear explicitly, it saves
    even more space.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，实际上是以稀疏向量的形式表示，以节省更多的空间。因此，我们不仅将所有单词转换为数字，还剥离了任何缺失的数据。如果一个单词在文档中不存在，您不需要显式存储该单词不存在的事实，这样可以节省更多的空间。
- en: Computing the TF-IDF score
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算TF-IDF分数
- en: To actually compute the TF-IDF score for each word in each document, we first
    cache this `tf` RDD.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算每个文档中每个单词的TF-IDF分数，我们首先缓存这个`tf` RDD。
- en: '[PRE41]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We do that because we''re going to use it more than once. Next, we use `IDF(minDocFreq=2)`,
    meaning that we''re going to ignore any word that doesn''t appear at least twice:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做是因为我们将使用它不止一次。接下来，我们使用`IDF(minDocFreq=2)`，这意味着我们将忽略任何出现次数不到两次的单词：
- en: '[PRE42]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We call `fit` on `tf`, and then in the next line we call `transform` on `tf`:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`tf`上调用`fit`，然后在下一行上调用`transform`：
- en: '[PRE43]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: What we end up with here is an RDD of the TF-IDF score for each word in each
    document.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到的是每个文档中每个单词的TF-IDF分数的RDD。
- en: Using the Wikipedia search engine algorithm
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用维基百科搜索引擎算法
- en: 'Let''s try and put the algorithm to use. Let''s try to look up the best article
    for the word **Gettysburg**. If you''re not familiar with US history, that''s
    where Abraham Lincoln gave a famous speech. So, we can transform the word Gettysburg
    into its hash value using the following code:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试并使用该算法。让我们尝试查找单词**Gettysburg**的最佳文章。如果您对美国历史不熟悉，那就是亚伯拉罕·林肯发表著名演讲的地方。因此，我们可以使用以下代码将单词Gettysburg转换为其哈希值：
- en: '[PRE44]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We will then extract the TF-IDF score for that hash value into a new RDD for
    each document:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将从该哈希值中提取TF-IDF分数到每个文档的新RDD中：
- en: '[PRE45]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: What this does is extract the TF-IDF score for Gettysburg, from the hash value
    it maps to for every document, and stores that in this `gettysburgRelevance` RDD.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的目的是从映射到每个文档的哈希值中提取Gettysburg的TF-IDF分数，并将其存储在`gettysburgRelevance` RDD中。
- en: 'We then combine that with the `documentNames` so we can see the results:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将其与`documentNames`结合起来，以便查看结果：
- en: '[PRE46]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, we can print out the answer:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以打印出答案：
- en: '[PRE47]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Running the algorithm
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行算法
- en: So, let's go run that and see what happens. As usual, to run the Spark script,
    we're not going to just hit the play icon. We have to go to Tools>Canopy Command
    Prompt. In the Command Prompt that opens up, we will type in `spark-submit TF-IDF.py`,
    and off it goes.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们运行一下，看看会发生什么。通常情况下，要运行Spark脚本，我们不会只是点击播放图标。我们需要转到工具>Canopy命令提示符。在打开的命令提示符中，我们将输入`spark-submit
    TF-IDF.py`，然后就可以运行了。
- en: We are asking it to chunk through quite a bit of data, even though it's a small
    sample of Wikipedia it's still a fair chunk of information, so it might take a
    while. Let's see what comes back for the best document match for Gettysburg, what
    document has the highest TF-IDF score?
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这只是维基百科的一个小样本，但我们要求它处理相当多的数据，因此可能需要一些时间。让我们看看为Gettysburg找到的最佳文档匹配是什么，哪个文档具有最高的TF-IDF分数？
- en: '![](img/b7dfb9f6-8d3a-4af9-a230-6d0fccb02e54.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7dfb9f6-8d3a-4af9-a230-6d0fccb02e54.png)'
- en: It's Abraham Lincoln! Isn't that awesome? We just made an actual search engine
    that actually works, in just a few lines of code.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 这是亚伯拉罕·林肯！这不是很棒吗？我们只需几行代码就制作了一个真正有效的搜索引擎。
- en: And there you have it, an actual working search algorithm for a little piece
    of Wikipedia using Spark in MLlib and TF-IDF. And the beauty is we can actually
    scale that up to all of Wikipedia if we wanted to, if we had a cluster large enough
    to run it.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是使用Spark在MLlib和TF-IDF中实际工作的搜索算法。美妙的是，如果我们有足够大的集群来运行它，我们实际上可以将其扩展到整个维基百科。
- en: Hopefully we got your interest up there in Spark, and you can see how it can
    be applied to solve what can be pretty complicated machine learning problems in
    a distributed manner. So, it's a very important tool, and I want to make sure
    you don't get through this book on data science without at least knowing the concepts
    of how Spark can be applied to big data problems. So, when you need to move beyond
    what one computer can do, remember, Spark is at your disposal.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我们引起了您对Spark的兴趣，您可以看到它如何应用于以分布式方式解决相当复杂的机器学习问题。因此，这是一个非常重要的工具，我希望您在阅读本数据科学书籍时，至少要了解Spark如何应用于大数据问题的概念。因此，当您需要超越单台计算机的能力时，请记住，Spark可以为您提供帮助。
- en: Using the Spark 2.0 DataFrame API for MLlib
  id: totrans-420
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0 DataFrame API进行MLlib
- en: This chapter was originally produced for Spark 1, so let's talk about what's
    new in Spark 2, and what new capabilities exist in MLlib now.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最初是为Spark 1制作的，因此让我们谈谈Spark 2中的新功能以及MLlib现在存在的新功能。
- en: So, the main thing with Spark 2 is that they moved more and more toward Dataframes
    and Datasets. Datasets and Dataframes are kind of used interchangeably sometimes.
    Technically a dataframe is a Dataset of row objects, they're kind of like RDDs,
    but the only difference is that, whereas an RDD just contains unstructured data,
    a Dataset has a defined schema to it.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Spark 2的主要特点是它越来越向Dataframes和Datasets迈进。有时Datasets和Dataframes有点交替使用。从技术上讲，Dataframe是一组行对象的Dataset，它们有点像RDD，但唯一的区别在于，RDD只包含非结构化数据，而Dataset具有定义的模式。
- en: A Dataset knows ahead of time exactly what columns of information exists in
    each row, and what types those are. Because it knows about the actual structure
    of that Dataset ahead of time, it can optimize things more efficiently. It also
    lets us think of the contents of this Dataset as a little, mini database, well,
    actually, a very big database if it's on a cluster. That means we can do things
    like issue SQL queries on it.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: Dataset提前知道每行中存在的信息列以及这些信息的类型。因为它提前知道该Dataset的实际结构，所以它可以更有效地优化事物。它还让我们将该Dataset的内容视为一个小型数据库，实际上，如果它在集群上，那就是一个非常大的数据库。这意味着我们可以对其执行SQL查询等操作。
- en: This creates a higher-level API with which we can query and analyze massive
    Datasets on a Spark cluster. It's pretty cool stuff. It's faster, it has more
    opportunities for optimization, and it has a higher-level API that's often easier
    to work with.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个更高级的API，我们可以在Spark集群上查询和分析大型数据集。这是相当酷的东西。它更快，有更多的优化机会，并且有一个更高级的API，通常更容易使用。
- en: How Spark 2.0 MLlib works
  id: totrans-425
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0 MLlib的工作原理
- en: 'Going forward in Spark 2.0, MLlib is pushing dataframes as its primary API.
    This is the way of the future, so let''s take a look at how it works. I''ve gone
    ahead and opened up the `SparkLinearRegression.py` file in Canopy, as shown in
    the following figure, so let''s walk through it a little bit:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0中，MLlib正在将数据框架作为其主要API。这是未来的发展方向，所以让我们看看它是如何工作的。我已经打开了Canopy中的`SparkLinearRegression.py`文件，如下图所示，让我们来看一下：
- en: '![](img/99003c6b-bbd0-4c0a-84c5-9c12af88be01.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99003c6b-bbd0-4c0a-84c5-9c12af88be01.png)'
- en: As you see, for one thing, we're using `ml` instead of `MLlib`, and that's because
    the new dataframe-based API is in there.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，首先，我们使用`ml`而不是`MLlib`，这是因为新的基于数据框架的API在其中。
- en: Implementing linear regression
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施线性回归
- en: In this example, what we're going to do is implement linear regression, and
    linear regression is just a way of fitting a line to a set of data. What we're
    going to do in this exercise is take a bunch of fabricated data that we have in
    two dimensions, and try to fit a line to it with a linear model.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们要做的是实现线性回归，线性回归只是一种将一条线拟合到一组数据的方法。在这个练习中，我们将使用两个维度中的一堆虚构数据，并尝试用线性模型拟合一条线。
- en: 'We''re going to separate our data into two sets, one for building the model
    and one for evaluating the model, and we''ll compare how well this linear model
    does at actually predicting real values. First of all, in Spark 2, if you''re
    going to be doing stuff with the `SparkSQL` interface and using Datasets, you''ve
    got to be using a `SparkSession` object instead of a `SparkContext`. To set one
    up, you do the following:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据分成两组，一组用于构建模型，一组用于评估模型，并比较这个线性模型在实际预测真实值时的表现。首先，在Spark 2中，如果要使用`SparkSQL`接口并使用数据集，你必须使用`SparkSession`对象而不是`SparkContext`。要设置一个，你可以这样做：
- en: '[PRE48]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Note that the middle bit is only necessary on Windows and in Spark 2.0\. It
    kind of works around a little bug that they have, to be honest. So, if you''re
    on Windows, make sure you have a `C:/temp` folder. If you want to run this, go
    create that now if you need to. If you''re not on Windows, you can delete that
    whole middle section to leave: `spark = SparkSession.builder.appName("LinearRegression").getOrCreate()`.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，中间部分只在Windows和Spark 2.0中才需要。说实话，这是为了解决一个小bug。所以，如果你在Windows上，请确保你有一个`C:/temp`文件夹。如果你想运行这个程序，如果需要的话现在就创建它。如果你不在Windows上，你可以删除整个中间部分，留下：`spark
    = SparkSession.builder.appName("LinearRegression").getOrCreate()`。
- en: Okay, so you can say `spark`, give it an `appName` and `getOrCreate()`.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以你可以说`spark`，给它一个`appName`和`getOrCreate()`。
- en: This is interesting, because once you've created a Spark session, if it terminates
    unexpectedly, you can actually recover from that the next time that you run it.
    So, if we have a checkpoint directory, it can actually restart where it left off
    using `getOrCreate`.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣，因为一旦你创建了一个Spark会话，如果它意外终止，你实际上可以在下次运行时从中恢复。所以，如果我们有一个检查点目录，它可以使用`getOrCreate`在上次中断的地方重新启动。
- en: 'Now, we''re going to use this `regression.txt` file that I have included with
    the course materials:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用我提供的`regression.txt`文件：
- en: '[PRE49]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: That is just a text file that has comma-delimited values of two columns, and
    they're just two columns of, more or less randomly, linearly correlated data.
    It can represent whatever you want. Let's imagine that it represents heights and
    weights, for example. So, the first column might represent heights, the second
    column might represent weights.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个文本文件，其中有两列逗号分隔的值，它们只是两列，或多或少地，线性相关的数据。它可以代表任何你想要的东西。比如，我们可以想象它代表身高和体重。所以，第一列可能代表身高，第二列可能代表体重。
- en: In the lingo of machine learning, we talk about labels and features, where labels
    are usually the thing that you're trying to predict, and features are a set of
    known attributes of the data that you use to make a prediction from.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的术语中，我们谈论标签和特征，其中标签通常是你要预测的东西，而特征是数据的一组已知属性，你用它来进行预测。
- en: In this example, maybe heights are the labels and the features are the weights.
    Maybe we're trying to predict heights based on your weight. It can be anything,
    it doesn't matter. This is all normalized down to data between -1 and 1\. There's
    no real meaning to the scale of the data anywhere, you can pretend it means anything
    you want, really.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，也许身高是标签，体重是特征。也许我们试图根据你的体重来预测身高。它可以是任何东西，都无所谓。这一切都被归一化到-1到1之间的数据。数据的规模没有真正的意义，你可以假装它代表任何你想要的东西。
- en: 'To use this with MLlib, we need to transform our data into the format it expects:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 要在MLlib中使用这个，我们需要将我们的数据转换成它期望的格式：
- en: '[PRE50]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The first thing we're going to do is split that data up with this `map` function
    that just splits each line into two distinct values in a list, and then we're
    going to map that to the format that MLlib expects. That's going to be a floating
    point label, and then a dense vector of the feature data.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是使用`map`函数将数据拆分成两个不同的值列表，然后将其映射到MLlib期望的格式。这将是一个浮点标签，然后是特征数据的密集向量。
- en: In this case, we only have one bit of feature data, the weight, so we have a
    vector that just has one thing in it, but even if it's just one thing, the MLlib
    linear regression model requires a dense vector there. This is like a `labeledPoint`
    in the older API, but we have to do it the hard way here.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们只有一个特征数据，即重量，所以我们有一个只包含一个元素的向量，但即使只有一个元素，MLlib线性回归模型也需要一个密集向量。这就像旧API中的`labeledPoint`，但我们必须用更麻烦的方式来做。
- en: 'Next, we need to actually assign names to those columns. Here''s the syntax
    for doing that:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要为这些列实际分配名称。以下是执行此操作的语法：
- en: '[PRE51]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We're going to tell MLlib that these two columns in the resulting RDD actually
    correspond to the label and the features, and then I can convert that RDD to a
    DataFrame object. At this point, I have an actual dataframe or, if you will, a
    Dataset that contains two columns, label and features, where the label is a floating
    point height, and the features column is a dense vector of floating point weights.
    That is the format required by MLlib, and MLlib can be pretty picky about this
    stuff, so it's important that you pay attention to these formats.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将告诉MLlib，结果RDD中的这两列实际上对应于标签和特征，然后我可以将该RDD转换为DataFrame对象。此时，我有一个实际的数据框，或者说，一个包含两列标签和特征的数据集，其中标签是浮点高度，特征列是浮点权重的密集向量。这是MLlib所需的格式，而MLlib对此可能会很挑剔，因此重要的是您注意这些格式。
- en: Now, like I said, we're going to split our data in half.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，就像我说的，我们要把我们的数据分成两半。
- en: '[PRE52]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We're going to do a 50/50 split between training data and test data. This returns
    back two dataframes, one that I'm going to use to actually create my model, and
    one that I'm going to use to evaluate my model.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在训练数据和测试数据之间进行50/50的拆分。这将返回两个数据框，一个用于创建模型，一个用于评估模型。
- en: I will next create my actual linear regression model with a few standard parameters
    here that I've set.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将使用一些标准参数创建我的实际线性回归模型。
- en: '[PRE53]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We''re going to call `lir = LinearRegression`, and then I will fit that model
    to the set of data that I held aside for training, the training data frame:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将调用`lir = LinearRegression`，然后我将把该模型拟合到我留出用于训练的数据集上，即训练数据框：
- en: '[PRE54]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: That gives me back a model that I can use to make predictions from.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我得到一个模型，我可以用它来进行预测。
- en: Let's go ahead and do that.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续做吧。
- en: '[PRE55]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: I will call `model.transform(testDF)`, and what that's going to do is predict
    the heights based on the weights in my testing Dataset. I actually have the known
    labels, the actual, correct heights, and this is going to add a new column to
    that dataframe called predictions, that has the predicted values based on that
    linear model.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 我将调用`model.transform(testDF)`，这将根据我的测试数据集中的权重预测身高。我实际上有已知的标签，即实际的正确身高，这将在该数据框中添加一个名为预测的新列，其中包含基于该线性模型的预测值。
- en: 'I''m going to cache those results, and now I can just extract them and compare
    them together. So, let''s pull out the prediction column, just using `select`
    like you would in SQL, and then I''m going to actually transform that dataframe
    and pull out the RDD from it, and use that to map it to just a plain old RDD full
    of floating point heights in this case:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 我将缓存这些结果，现在我可以提取它们并将它们进行比较。因此，让我们提取预测列，就像在SQL中使用`select`一样，然后我将实际转换该数据框并从中提取RDD，并使用它将其映射到这种情况下的一组浮点高度：
- en: '[PRE56]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'These are the predicted heights. Next, we''re going to get the actual heights
    from the label column:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是预测的身高。接下来，我们将从标签列中获取实际的身高：
- en: '[PRE57]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Finally, we can zip them back together and just print them out side by side
    and see how well it does:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将它们重新组合在一起，然后将它们并排打印出来，看看效果如何：
- en: '[PRE58]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: This is kind of a convoluted way of doing it; I did this to be more consistent
    with the previous example, but a simpler approach would be to just actually select
    prediction and label together into a single RDD that maps out those two columns
    together and then I don't have to zip them up, but either way it works. You'll
    also note that right at the end there we need to stop the Spark session.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有点复杂；我之所以这样做是为了与之前的示例保持一致，但更简单的方法是实际上选择预测和标签，将它们合并成一个RDD，将这两列一起映射出来，然后我就不必将它们合并在一起，但无论哪种方法都可以。您还会注意到，在最后，我们需要停止Spark会话。
- en: So let's see if it works. Let's go up to Tools, Canopy Command Prompt, and we'll
    type in `spark-submit SparkLinearRegression.py` and let's see what happens.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是否有效。让我们转到工具，Canopy命令提示符，然后输入`spark-submit SparkLinearRegression.py`，看看会发生什么。
- en: There's a little bit more upfront time to actually run these APIs with Datasets,
    but once they get going, they're very fast. Alright, there you have it.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，使用数据集运行这些API需要更多的前期时间，但一旦开始，它们就非常快。好了，就是这样。
- en: '![](img/93ec3caf-319d-47fa-8712-266370ee6bcb.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93ec3caf-319d-47fa-8712-266370ee6bcb.png)'
- en: Here we have our actual and predicted values side by side, and you can see that
    they're not too bad. They tend to be more or less in the same ballpark. There
    you have it, a linear regression model in action using Spark 2.0, using the new
    dataframe-based API for MLlib. More and more, you'll be using these APIs going
    forward with MLlib in Spark, so make sure you opt for these when you can. Alright,
    that's MLlib in Spark, a way of actually distributing massive computing tasks
    across an entire cluster for doing machine learning on big Datasets. So, good
    skill to have. Let's move on.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将实际值和预测值并排放在一起，您可以看到它们并不太糟糕。它们往往在同一范围内。就是这样，使用Spark 2.0进行线性回归模型，使用MLlib的基于新数据框的API。今后，您将越来越多地使用这些API来进行Spark中的MLlib，因此请尽量选择这些API。好了，这就是Spark中的MLlib，一种实际上可以在整个集群上分发大规模计算任务以处理大型数据集的机器学习方法。这是一个很好的技能。让我们继续。
- en: Summary
  id: totrans-470
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started with installing Spark, then moved to introducing
    Spark in depth while understanding how Spark works in combination with RDDs. We
    also walked through various ways of creating RDDs while exploring different operations.
    We then introduced MLlib, and stepped through some detailed examples of decision
    trees and K-Means Clustering in Spark. We then pulled off our masterstroke of
    creating a search engine in just a few lines of code using TF-IDF. Finally, we
    looked at the new features of Spark 2.0.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从安装Spark开始，然后深入介绍了Spark，同时了解了Spark与RDD的结合工作原理。我们还通过探索不同的操作方式，介绍了创建RDD的各种方法。然后我们介绍了MLlib，并详细介绍了Spark中决策树和K-Means聚类的一些示例。然后我们通过使用TF-IDF仅需几行代码就创建了一个搜索引擎。最后，我们看了一下Spark
    2.0的新功能。
- en: In the next chapter, we'll take a look at A/B testing and experimental design.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍A/B测试和实验设计。
