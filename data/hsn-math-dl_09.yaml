- en: Feedforward Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈神经网络
- en: In the previous chapter, we covered linear neural networks, which have proven
    to be effective for problems such as regression and so are widely used in the
    industry. However, we also saw that they have their limitations and are unable
    to work effectively on higher-dimensional problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了线性神经网络，这些网络在回归等问题中证明是有效的，因此在工业中得到了广泛应用。然而，我们也看到它们有自己的局限性，无法在高维问题上有效工作。
- en: In this chapter, we will take an in-depth look at the **multilayer perceptron**
    (**MLP**), a type of **feedforward neural network** (**FNN**). We will start by
    taking a look at how biological neurons process information, then we will move
    onto mathematical models of biological neurons. The **artificial neural networks**
    (**ANNs**) we will study in this book are made up of mathematical models of biological
    neurons (we will learn more about this shortly). Once we have built a foundation,
    we will move on to understanding how MLPs—which are the FNNs—work and their involvement
    with deep learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将深入探讨**多层感知器**（**MLP**），这是一种**前馈神经网络**（**FNN**）。我们将首先了解生物神经元如何处理信息，然后转向生物神经元的数学模型。本书中我们将研究的**人工神经网络**（**ANNs**）是由生物神经元的数学模型构成的（我们很快就会深入了解这一点）。一旦我们建立了基础，我们将继续理解MLP（即FNN）如何工作以及它们与深度学习的关系。
- en: What FNNs allow us to do is approximate a function that maps input to output
    and this can be used in a variety of tasks, such as predicting the price of a
    house or a stock or determining whether or not an event will occur.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络（FNN）使我们能够近似一个将输入映射到输出的函数，这可以用于各种任务，例如预测房价或股票价格，或判断某个事件是否会发生。
- en: 'The following topics are covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding biological neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解生物神经网络
- en: Comparing the perceptron and the McCulloch-Pitts neuron
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较感知机与麦卡洛克-皮茨神经元
- en: MLPs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层感知器（MLP）
- en: Training neural networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络训练
- en: Deep neural networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: Understanding biological neural networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解生物神经网络
- en: The human brain is capable of some remarkable feats—it performs very complex
    information processing. The neurons that make up our brains are very densely connected
    and perform in parallel with others. These biological neurons receive and pass
    signals to other neurons through the connections (synapses) between them. These
    synapses have strengths associated with them and increasing or weakening the strength
    of the connections between neurons is what facilitates our learning and allows
    us to continuously learn and adapt to the dynamic environments we live in.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑能够完成一些非凡的壮举——它执行非常复杂的信息处理。构成我们大脑的神经元连接非常密集，并与其他神经元并行工作。这些生物神经元通过它们之间的连接（突触）接收并传递信号。这些突触有各自的强度，增强或削弱神经元之间连接的强度，促进了我们的学习，并使我们能够不断学习并适应我们所生活的动态环境。
- en: As we know, the brain consists of neurons—in fact, according to recent studies,
    it is estimated that the human brain contains roughly 86 billion neurons. That
    is a lot of neurons and a whole lot more connections. A very large number of these
    neurons are used simultaneously every day to allow us to carry out a variety of
    tasks and be functional members of society. Neurons by themselves are said to
    be quite slow, but it is this large-scale parallel operation that gives our brains
    its extraordinary capability.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，大脑由神经元组成——实际上，根据最近的研究，估计人类大脑大约包含860亿个神经元。这是一个庞大的神经元数量，而且还有更多的连接。每天有大量的神经元被同时使用，以帮助我们完成各种任务，并成为社会中的功能性成员。单独的神经元被认为是相当缓慢的，但正是这种大规模的并行操作赋予了大脑非凡的能力。
- en: 'The following is a diagram of a biological neuron:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生物神经元的示意图：
- en: '![](img/2ed8f991-d8d2-4c8d-bedb-1386771c801d.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ed8f991-d8d2-4c8d-bedb-1386771c801d.png)'
- en: As you can see from the preceding diagram, each neuron has three main components—the
    body, an axon, and many dendrites. The synapses connect the axon of one neuron
    to the dendrites of other neurons and determine the weight of the information
    that is received from other neurons. Only when the sum of the weighted inputs
    to the neuron exceeds a certain threshold does the neuron fire (activate); otherwise,
    it is at rest. This communication between neurons is done through electrochemical
    reactions, involving potassium, sodium, and chlorine (which we will not go into
    as it is beyond the scope of this book; however, if this interests you, there
    is a lot of literature you can find on it).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，每个神经元有三个主要组成部分——细胞体、轴突和多个树突。突触将一个神经元的轴突与其他神经元的树突连接，并决定从其他神经元接收到信息的权重。只有当神经元的加权输入之和超过某个阈值时，神经元才会激活（发放）；否则，它处于静止状态。神经元之间的这种通信是通过电化学反应完成的，涉及钾、钠和氯（我们不会深入讨论这一点，因为它超出了本书的范围；不过，如果你对此感兴趣，可以找到很多相关文献）。
- en: The reason we are looking at biological neurons is that the neurons and neural
    networks we will be learning about and developing in this book are largely biologically
    inspired. If we are trying to develop artificial intelligence, where better to
    learn than from actual intelligence?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以研究生物神经元，是因为本书中我们将学习和开发的神经元和神经网络在很大程度上受到生物学的启发。如果我们想要开发人工智能，哪里比学习真正的智能更好呢？
- en: Since the goal of this book is to teach you how to develop ANNs on computers,
    it is relatively important that we take a look at the differences between the
    computational power of our brains as opposed to computers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书的目标是教你如何在计算机上开发人工神经网络（ANNs），因此我们有必要看看我们的大脑与计算机在计算能力上的差异。
- en: Computers have a significant advantage over our brains as they can perform roughly
    10 billion operations per second, whereas the human brain can only perform around
    800 operations per second. However, the brain requires roughly 10 watts to operate,
    which is 10 times less than what a computer requires. Another advantage that computers
    have is their precision; they can perform operations millions of times more accurately.
    Lastly, computers perform operations sequentially and cannot deal with data they
    have not been programmed to deal with, but the brain performs operations in parallel
    and is well equipped to deal with new data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机在计算能力上相较于我们的大脑具有显著的优势，它们每秒能够执行大约100亿次操作，而人脑每秒只能执行约800次操作。然而，大脑运作所需的功率约为10瓦特，仅为计算机所需功率的十分之一。计算机的另一个优势是其精度；它们可以比大脑高出数百万倍的精度进行操作。最后，计算机执行操作是按顺序进行的，无法处理未经过编程的数据，而大脑则能并行执行操作，并能够很好地处理新的数据。
- en: Comparing the perceptron and the McCulloch-Pitts neuron
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较感知机与麦卡洛克-皮茨神经元
- en: In this section, we will cover two mathematical models of biological neurons—the
    **McCulloch-Pitts** (**MP**) neuron and Rosenblatt's perceptron—which create the
    foundation for neural networks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍两种生物神经元的数学模型——**麦卡洛克-皮茨**（**MP**）神经元和罗森布拉特感知机——它们为神经网络的基础奠定了基础。
- en: The MP neuron
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**MP** 神经元'
- en: 'The MP neuron was created in 1943 by Warren McCulloch and Walter Pitts. It
    was modeled after the biological neuron and is the first mathematical model of
    a biological neuron. It was created primarily for classification tasks. The MP
    neuron takes as input binary values and outputs a binary value based on a threshold
    value. If the sum of the inputs is greater than the threshold, then the neuron
    outputs `1` (if it is under the threshold, it outputs `0`). In the following diagram,
    we can see what a basic neuron with three inputs and one output looks like:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**MP** 神经元由沃伦·麦卡洛克和沃尔特·皮茨于1943年创建。它是仿生神经元模型，是第一个生物神经元的数学模型。它主要用于分类任务。**MP**
    神经元接受二进制值作为输入，并根据阈值输出二进制值。如果输入之和大于阈值，则神经元输出 `1`（如果低于阈值，则输出 `0`）。在下图中，我们可以看到一个具有三个输入和一个输出的基本神经元：'
- en: '![](img/46f43996-76b7-40b2-9fcf-a3e8c0a66941.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46f43996-76b7-40b2-9fcf-a3e8c0a66941.png)'
- en: As you can see, this isn't entirely dissimilar to the biological neuron we saw
    earlier.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这与我们之前看到的生物神经元并不完全不同。
- en: 'Mathematically, we can write this as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上看，我们可以将其写成如下形式：
- en: '![](img/4fed4244-c23d-4d05-a473-4d82d7431cbe.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fed4244-c23d-4d05-a473-4d82d7431cbe.png)'
- en: Here, *x[i]* = `0` or `1`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x[i]* = `0` 或 `1`。
- en: We can think of this as outputting Boolean answers; that is, `true` or `false`
    (or `yes` or `no`).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其视为输出布尔值答案；也就是说，`true`或`false`（或者`yes`或`no`）。
- en: While the MP neuron may look simple, it has the ability to model any logic function,
    such as `OR`, `AND`, and `NOT`; but it is unable to classify the `XOR` function.
    Additionally, it does not have the ability to learn, so the threshold (*b*) needs
    to be adjusted analytically to fit our data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然MP神经元看起来简单，但它能够建模任何逻辑函数，如`OR`、`AND`和`NOT`；但是它无法对`XOR`函数进行分类。此外，它没有学习能力，因此阈值（*b*）需要通过解析方法进行调整以适应我们的数据。
- en: Perceptron
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机
- en: 'The perceptron model, created by Frank Rosenblatt in 1958, is an improved version
    of the MP neuron and can take any real value as input. Each input is then multiplied
    by a real-valued weight. If the sum of the weighted inputs is greater than the
    threshold, then the output is `1`, and if it is below the threshold, then the
    output is `0`. The following diagram illustrates a basic perceptron model:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由Frank Rosenblatt于1958年创建的感知机模型是MP神经元的改进版，它可以接受任何实数值作为输入。每个输入都会乘以一个实值权重。如果加权输入的和大于阈值，则输出为`1`，如果低于阈值，则输出为`0`。下图展示了一个基本的感知机模型：
- en: '![](img/1feb9992-9a74-441f-90d7-9319a5c58453.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1feb9992-9a74-441f-90d7-9319a5c58453.png)'
- en: This model shares a lot of similarities with the MP neuron, but it is more similar
    to the biological neuron.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型与MP神经元有很多相似之处，但它更接近生物神经元。
- en: 'Mathematically, we can write this as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，我们可以将其写成如下形式：
- en: '![](img/eabb472d-f1e6-4d0b-ba24-91da720f7cad.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eabb472d-f1e6-4d0b-ba24-91da720f7cad.png)'
- en: Here, [![](img/abe9bae6-e784-4fd1-b427-4eb016761781.png)].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， [![](img/abe9bae6-e784-4fd1-b427-4eb016761781.png)]。
- en: 'Sometimes, we rewrite the perceptron equation in the following form:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们将感知机方程写成以下形式：
- en: '![](img/6db254bd-9049-4af9-bed8-a0e63848161b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6db254bd-9049-4af9-bed8-a0e63848161b.png)'
- en: 'The following diagram shows how the perceptron equation will look like:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了感知机方程的形式：
- en: '![](img/320b5a92-8882-41fe-917a-50f3c47904f1.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/320b5a92-8882-41fe-917a-50f3c47904f1.png)'
- en: Here, [![](img/22c4d4ac-b870-4c16-bc5a-d53df9a04aa6.png)] and [![](img/056f6b67-7dff-452e-aa0e-352cf607637c.png)].
    This prevents us from having to hardcode the threshold, which makes the threshold
    a learnable parameter instead of something we have to manually adjust (as is the
    case with the MP neuron).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， [![](img/22c4d4ac-b870-4c16-bc5a-d53df9a04aa6.png)] 和 [![](img/056f6b67-7dff-452e-aa0e-352cf607637c.png)]。这避免了我们必须硬编码阈值，从而使得阈值成为一个可学习的参数，而不是像MP神经元那样需要手动调整的内容。
- en: Pros and cons of the MP neuron and perceptron
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MP神经元和感知机的优缺点
- en: The advantage the perceptron model has over the MP neuron is that it is able
    to learn through error correction and it linearly separates the problem using
    a hyperplane, so anything that falls below the hyperplane is `0` and anything
    above it is `1`. This error correction allows the perceptron to adjust the weights
    and move the position of the hyperplane so that it can properly classify the data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机模型相对于MP神经元的优势在于，它能够通过错误修正进行学习，并且通过超平面将问题进行线性分割，因此，超平面下的所有点为`0`，超平面上的所有点为`1`。这种错误修正使得感知机可以调整权重并移动超平面的位置，从而正确地分类数据。
- en: Earlier, we mentioned that the perceptron learns to linearly classify a problem—but
    what exactly does it learn? Does it learn the nature of the question that is asked?
    No. It learns the effect of the input on the output. *So, the greater the weight
    associated with a certain input, the greater its impact on the prediction (classification). *
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们提到过感知机学习线性分类一个问题——但是它究竟学习了什么呢？它学习的是问题本身的性质吗？不是。它学习的是输入对输出的影响。*因此，某个输入所关联的权重越大，它对预测（分类）的影响就越大。*
- en: 'The update for the weights (learning) happens as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的更新（学习）过程如下：
- en: '![](img/40e6f30f-a89f-4952-ae9f-8d37a3dde434.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40e6f30f-a89f-4952-ae9f-8d37a3dde434.png)'
- en: Here, *δ* = expected value – predicted value.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*δ* = 预期值 – 预测值。
- en: 'We could also add a learning rate ([![](img/b3ec0dca-3cfe-45b8-ba4c-97f0f728a4cf.png)])
    if we want to speed up the learning; so, the update will be as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想加快学习速度，也可以加入学习率（[![](img/b3ec0dca-3cfe-45b8-ba4c-97f0f728a4cf.png)]）；那么，更新公式将如下所示：
- en: '![](img/801f9c62-f4a2-4ad9-87df-e292a6e5e2ef.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/801f9c62-f4a2-4ad9-87df-e292a6e5e2ef.png)'
- en: 'During these updates, the perceptron calculates the distance of the hyperplane
    from the points to be classified and adjusts itself to find the best position
    that it can perfectly linearly classify the two target classes. So, it maximally
    separates both points on either side, which we can see in the following plot:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些更新过程中，感知机计算超平面与待分类点之间的距离，并调整自身以找到最佳位置，从而能够完美地线性分类这两个目标类别。因此，它会最大限度地将两侧的点分开，这一点可以从以下图中看到：
- en: '![](img/47d7de84-900c-4b21-8635-e037cbe4f383.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47d7de84-900c-4b21-8635-e037cbe4f383.png)'
- en: What is even more fascinating about this is that because of the aforementioned
    learning rule, the perceptron is guaranteed to converge when given a finite number
    of updates and so will work on any binary classification task.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 更加令人着迷的是，由于上述的学习规则，感知机在给定有限次数的更新时保证会收敛，因此能够处理任何二分类任务。
- en: But alas, the perceptron is not perfect either and it also has limitations.
    As it is a linear classifier, it is unable to deal with nonlinear problems, which
    makes up the majority of the problems we usually wish to develop solutions for.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 但遗憾的是，感知机也并不完美，它也有局限性。由于它是一个线性分类器，因此无法处理非线性问题，而非线性问题占我们通常希望开发解决方案的大多数问题。
- en: MLPs
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLPs
- en: As mentioned, both the MP neuron and perceptron models are unable to deal with
    nonlinear problems. To combat this issue, modern-day perceptrons use an activation
    function that introduces nonlinearity to the output.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，MP 神经元和感知机模型都无法处理非线性问题。为了解决这个问题，现代感知机使用一种激活函数，将非线性引入到输出中。
- en: 'The perceptrons (neurons, but we will mostly refer to them as **nodes** going
    forward) we will use are of the following form:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的感知机（神经元，但接下来我们大多数情况下称其为**节点**）形式如下：
- en: '![](img/c7e24253-38b4-46c3-af8b-368270efcc83.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7e24253-38b4-46c3-af8b-368270efcc83.png)'
- en: 'Here, *y* is the output, *φ* is a nonlinear activation function, *x[i]* is
    the inputs to the unit, *w[i]* is the weights, and *b* is the bias. This improved
    version of the perceptron looks as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*y* 是输出，*φ* 是非线性激活函数，*x[i]* 是单元的输入，*w[i]* 是权重，*b* 是偏差。这个改进版的感知机如下所示：
- en: '![](img/fbda6328-f94d-4726-a9c2-156876ef5b39.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbda6328-f94d-4726-a9c2-156876ef5b39.png)'
- en: 'In the preceding diagram, the activation function is generally the sigmoid
    function:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，激活函数通常是 sigmoid 函数：
- en: '![](img/d31b4c77-65c0-4313-a34f-ed9189ce7367.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d31b4c77-65c0-4313-a34f-ed9189ce7367.png)'
- en: What the sigmoid activation function does is squash all the output values into
    the `(0, 1)` range. The sigmoid activation function is largely used for historical
    purposes since the developers of the earlier neurons focused on thresholding.
    When gradient-based learning was introduced, the sigmoid function turned out to
    be the best choice.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数的作用是将所有输出值压缩到`(0, 1)`范围内。Sigmoid 激活函数主要是出于历史原因被广泛使用，因为早期神经元的开发者关注的是阈值处理。当引入基于梯度的学习时，sigmoid
    函数证明是最佳选择。
- en: 'An MLP is the simplest type of FNN. It is basically a lot of nodes combined
    together and the computation is carried out sequentially. The network looks as
    follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 是最简单的 FNN 类型。它基本上是许多节点组合在一起，计算过程是按顺序进行的。网络结构如下：
- en: '![](img/9ed4f81b-6ef0-47d0-abd4-f29af3de7bf8.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ed4f81b-6ef0-47d0-abd4-f29af3de7bf8.png)'
- en: An FNN is essentially a directed acyclic graph; that is, the connections are
    always moving in one direction. There are no connections that feed the outputs
    back into the network.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: FNN 本质上是一个有向无环图；也就是说，连接总是朝着一个方向流动。没有将输出反馈到网络的连接。
- en: As you can see from the preceding diagram, the nodes are arranged in layers
    and the nodes in each layer are connected to each of the neurons in the next layer.
    However, there aren't any connections between nodes in the same layer. We refer
    to networks such as this as being fully connected.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图示所示，节点按层排列，每一层的节点与下一层的每个神经元相连。然而，同一层的节点之间没有连接。我们将这种网络称为全连接。
- en: The first layer is referred to as the input layer, the last layer is referred
    to as the output layer, and all the layers in between are called hidden layers.
    The number of nodes in the output layer depends on the type of problem we build
    our MLP for. It is important that you remember that the inputs to and outputs
    from layers are not the same as the inputs to and outputs from the network.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层称为输入层，最后一层称为输出层，所有中间的层称为隐藏层。输出层中的节点数量取决于我们为MLP构建的任务类型。请记住，层的输入和输出与网络的输入和输出是不同的。
- en: You may also notice that in the preceding architecture, there is only one unit
    in the output layer. This is generally the case when we have a regression or binary
    classification task. So, if we want our network to be able to detect multiple
    classes, then our output layer will have *K* nodes, where *K* is the number of
    classes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会注意到，在上述架构中，输出层只有一个单元。这通常出现在回归或二分类任务中。所以，如果我们希望网络能够检测多个类别，那么输出层将包含*K*个节点，其中*K*是类别的数量。
- en: Note that the depth of the network is the number of layers it has and the width
    is the number of nodes in a layer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，网络的深度是指它的层数，而宽度是指每层的节点数量。
- en: However, what makes neural networks so powerfully effective, and the reason
    we are studying them, is that they are universal function approximators. The universal
    approximation theorem states that "<q>a feedforward neural network with a single
    hidden layer containing a finite number of neurons can approximate continuous
    functions on compact subsets of ![](img/1f059592-7ad0-46d4-89f1-9c34a491c915.png),
    under mild assumptions on the activation function.</q>" What this means is that
    if the hidden layer contains a specific number of neurons, then our neural network
    can reasonably approximate any known function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使得神经网络如此强大且有效的原因，也是我们研究它们的原因，是它们是通用的函数近似器。通用近似定理表明：“一个具有有限个神经元的单隐层前馈神经网络可以在激活函数满足温和假设的条件下，近似定义在紧致子集上的连续函数。”这意味着，如果隐层包含特定数量的神经元，那么我们的神经网络可以合理地近似任何已知的函数。
- en: You will notice that it is unclear exactly how many neurons are needed in the
    hidden layer for it to be able to approximate any function. This could vary greatly,
    depending on the function we want it to learn.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，隐层需要多少神经元才能近似任何函数并不明确。这可能会有很大不同，取决于我们希望它学习的函数。
- en: By now, you might be thinking that if MLPs have been around since the late 1960s,
    why has it taken nearly 50 years for them to take off and be used as widely as
    they are today? This is because the computing power that was available 50 years
    ago was nowhere near as powerful as what is available today, nor was the same
    amount of data that is available now available back then. So, because of the lack
    of results that MLPs were able to achieve back then, they faded into obscurity.
    Because of this, as well as the universal approximation theorem, researchers at
    the time hadn't looked deeper than into a couple of layers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在，你可能在想，既然MLP自1960年代末期就已存在，为什么它们直到今天才广泛应用并取得成功？这是因为50年前的计算能力远不如今天的强大，同时，当时也没有现在如此丰富的数据。因此，由于当时MLP未能取得显著成果，它们逐渐被遗忘。由于这一原因，再加上通用近似定理，当时的研究者并未深入研究超过几层的网络。
- en: Let's break the model down and see how it works.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解模型，看看它是如何工作的。
- en: Layers
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层
- en: We know now that MLPs (and so FNNs) are made of three different kinds of layers—input,
    hidden, and output. We also know what a single neuron looks like. Let's now mathematically
    explore MLPs and how they work.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道，MLP（以及前馈神经网络FNN）由三种不同类型的层组成——输入层、隐藏层和输出层。我们也知道一个单一神经元的样子。现在让我们从数学角度探索MLP及其工作原理。
- en: 'Suppose we have an MLP with ![](img/d94ac979-7201-4477-80eb-3dd96760aa3f.png) input (where ![](img/da399926-f0d2-4ca9-a852-de9c9d2b5785.png)),
    *L* layers, *N* neurons in each layer, an activation function [![](img/b7b71384-b2f7-4c6f-9348-6b7bb274e79c.png)],
    and the network output, *y*. The MLP looks as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含![](img/d94ac979-7201-4477-80eb-3dd96760aa3f.png)输入的多层感知器（MLP）（其中![](img/da399926-f0d2-4ca9-a852-de9c9d2b5785.png)），*L*层，每层有*N*个神经元，一个激活函数[![](img/b7b71384-b2f7-4c6f-9348-6b7bb274e79c.png)]，以及网络输出*y*。该MLP的结构如下：
- en: '![](img/24293b6e-1760-42d5-813a-77d6b949fb16.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24293b6e-1760-42d5-813a-77d6b949fb16.png)'
- en: 'As you can see, this network has four inputs—the first hidden layer has five
    nodes, the second hidden layer has three nodes, the third hidden layer has five
    nodes, and there is one node for the output. Mathematically, we can write this
    as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个网络有四个输入——第一隐藏层有五个节点，第二隐藏层有三个节点，第三隐藏层有五个节点，输出层有一个节点。数学上，我们可以将其写作如下：
- en: '![](img/1d1d4d07-243c-42ee-bd77-db5cb042f6d7.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d1d4d07-243c-42ee-bd77-db5cb042f6d7.png)'
- en: Here, [![](img/a99c32c3-e4c0-4b0d-a607-67feeb0586c6.png)] is the *i^(th)* node
    in the *l^(th)* layer, [![](img/49bdaee9-222a-428a-8b5d-a06e1c30b442.png)] is
    an activation function for the *l**^(th)* layer, *x[j]* is the *j^(th)* input
    to the network, [![](img/2090f186-37fd-445d-b5a1-10d0b59368da.png)] is the bias
    for the *i**^(th)* node in the *l**^(th)* layer, and [![](img/35365769-aa9c-4837-9481-8096a06f8480.png)] is
    the directed weight that connects the *j^(th)* node in the *l–1^(st)* layer to
    the *i^(th)* node in the *l^(th)* layer.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，[![](img/a99c32c3-e4c0-4b0d-a607-67feeb0586c6.png)]是*l^(th)*层中的*i^(th)*节点，[![](img/49bdaee9-222a-428a-8b5d-a06e1c30b442.png)]是*l^(th)*层的激活函数，*x[j]*是输入网络的*j^(th)*个输入，[![](img/2090f186-37fd-445d-b5a1-10d0b59368da.png)]是*l^(th)*层中*i^(th)*节点的偏置，[![](img/35365769-aa9c-4837-9481-8096a06f8480.png)]是将*l-1^(st)*层的*j^(th)*节点与*l^(th)*层的*i^(th)*节点连接起来的有向权重。
- en: Before we move forward, let's take a look at the preceding equations. From them,
    we can easily observe that each hidden node depends on the weights from the previous
    layer. If you take a pencil and draw out the network (or use your fingers to trace
    the connections), you will notice that the deeper we get into the network, the
    more complex the relationship nodes in the later hidden layers have with those
    in the earlier layers.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，先来回顾一下前面的方程式。从这些方程式中，我们可以很容易地观察到，每个隐藏节点都依赖于前一层的权重。如果你拿起一支铅笔，画出这个网络（或者用手指沿着连接线走一遍），你会发现，随着我们深入网络，后面隐藏层中的节点与前面层的节点之间的关系变得越来越复杂。
- en: 'Now that you have an idea of how each neuron is computed in an MLP, you might
    have realized that explicitly writing out the computation on each node in each
    layer can be a daunting task. So, let''s rewrite the preceding equation in a cleaner
    and simpler manner. We generally do not express neural networks in terms of the
    computation that happens on each node. We instead express them in terms of layers
    and because each layer has multiple nodes, we can write the previous equations
    in terms of vectors and matrices. The previous equations can now be written as
    follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了一些关于MLP中每个神经元如何计算的概念，你可能已经意识到，显式地写出每个节点在每一层中的计算过程可能是一项艰巨的任务。所以，让我们以更简洁清晰的方式重写前面的方程式。我们通常不会以每个节点上的计算来表达神经网络，而是通过层来表示，因为每层有多个节点，我们可以用向量和矩阵来表示前面的方程式。现在，前面的方程式可以写作如下：
- en: '![](img/8261ea86-c6a6-4edc-b0be-de27390d03cd.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8261ea86-c6a6-4edc-b0be-de27390d03cd.png)'
- en: This is a whole lot simpler to follow.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这简单多了。
- en: Remember from [Chapter 2](6a34798f-db83-4a32-9222-06ba717fc809.xhtml), *Linear
    Algebra*, that when you multiply a vector or matrix with a scalar value, the scalar
    value is applied to all the entries.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在[第二章](6a34798f-db83-4a32-9222-06ba717fc809.xhtml)《线性代数》中提到的，当你将向量或矩阵与标量值相乘时，标量值会作用于所有的条目。
- en: 'For the networks we want to build, the input more than likely will not be a
    vector, as it is in the preceding examples; it will be a matrix, so we can then
    rewrite it as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们想要构建的网络，输入很可能不再是向量，正如前面的例子中那样；它将是一个矩阵，所以我们可以将其重写如下：
- en: '![](img/2b2e05de-f12e-409f-bb52-d383fbb0be89.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b2e05de-f12e-409f-bb52-d383fbb0be89.png)'
- en: Here, X is the matrix containing all the data we want to train our model on,
    H^([l]) contains the hidden nodes at each layer for all the data samples, and
    everything else is the same as it was earlier.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，X是包含我们希望用来训练模型的所有数据的矩阵，H^([l])包含每一层所有数据样本的隐藏节点，其他的和之前一样。
- en: If you have been paying attention, you will have noticed that the order of the
    multiplication taking place in the matrix is different than what took place earlier.
    Why do you think that is? (I'll give you a hint—transpose.)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有留心的话，你会发现矩阵中的乘法顺序与之前的有所不同。你认为这是为什么呢？（我给你个提示——转置。）
- en: You should now have a decent, high-level understanding of how neural networks
    are constructed. Let's now lift up the hood and take a look at what is going on
    underneath. We know from the previous equations that neural networks are comprised
    of a series of matrix multiplications and matrix additions and scalar multiplications.
    Since we are now dealing with vectors and matrices, their dimensions are important
    because if they don't line up properly, we can't multiply and add them.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该对神经网络的构造有了相当不错的高层次理解。现在，让我们揭开面纱，看看底层发生了什么。我们从前面的方程中知道，神经网络由一系列矩阵乘法、矩阵加法和标量乘法组成。由于我们现在处理的是向量和矩阵，它们的维度非常重要，因为如果它们没有正确对齐，我们就无法进行乘法和加法运算。
- en: Let's view the preceding MLP in its full matrix form. (To keep things simple,
    we will go through it layer by layer and we will use the second form since our
    input is in vector form.) To simplify the view and to properly understand what
    is happening, we will now denote ![](img/ca351150-a3f6-4865-963e-7ee02f9b96da.png) and [![](img/62411681-fe78-4517-bee2-413408f0ed74.png)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以完整的矩阵形式查看前面的MLP。（为了简化，我们将逐层进行，并且由于我们的输入是向量形式，所以使用第二种形式。）为了简化视图并正确理解发生了什么，我们现在表示 ![](img/ca351150-a3f6-4865-963e-7ee02f9b96da.png)和[![](img/62411681-fe78-4517-bee2-413408f0ed74.png)]。
- en: 'Calculate *z^([1])* as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式计算*z^([1])*：
- en: '![](img/bedf5dd0-6d85-47b3-a534-7b9d61fc79c6.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bedf5dd0-6d85-47b3-a534-7b9d61fc79c6.png)'
- en: 'Calculate *h^([1])*  as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式计算*h^([1])*：
- en: '![](img/c63f4068-7687-4f18-bf31-7f525f49e4b0.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c63f4068-7687-4f18-bf31-7f525f49e4b0.png)'
- en: 'Calculate *z^([2])* as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式计算*z^([2])*：
- en: '![](img/2ede43e9-b1e2-4853-acab-4852ba7b9340.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ede43e9-b1e2-4853-acab-4852ba7b9340.png)'
- en: 'Calculate *h^([2])* as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式计算*h^([2])*：
- en: '![](img/c65fd29b-e5c2-4f09-a629-55a199e1c46d.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c65fd29b-e5c2-4f09-a629-55a199e1c46d.png)'
- en: 'Calculate *z^([3])* as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式计算*z^([3])*：
- en: '![](img/4eef0a30-7d90-4f5f-bcfa-dfa210079eed.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4eef0a30-7d90-4f5f-bcfa-dfa210079eed.png)'
- en: 'Calculate *h^([3])* as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式计算*h^([3])*：
- en: '![](img/4511cb88-0b8a-410e-ba8a-43da43868ac0.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4511cb88-0b8a-410e-ba8a-43da43868ac0.png)'
- en: 'Calculate *z^([4])* as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式计算*z^([4])*：
- en: '![](img/e798ced7-1c46-48d6-9c68-aadfa57c6889.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e798ced7-1c46-48d6-9c68-aadfa57c6889.png)'
- en: 'Calculate ***y*** as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式计算***y***：
- en: '![](img/424fa6be-32b2-4d38-999f-e13ffa5c8bc0.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/424fa6be-32b2-4d38-999f-e13ffa5c8bc0.png)'
- en: There we have it. Those are all the operations that take place in our MLP.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。这些是我们MLP中发生的所有操作。
- en: I have slightly tweaked the preceding notation by putting [![](img/6269823e-af7e-4a18-b7ac-49cd77c585ed.png)] in
    brackets and writing *y* as a vector, even though it is clearly a scalar. This
    was only done to keep the flow and to avoid changing the notation. *y* is a vector
    if we use the *k*-class classification (giving us multiple output neurons).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我稍微调整了前面的符号表示，将[![](img/6269823e-af7e-4a18-b7ac-49cd77c585ed.png)]放在括号中，并将*y*表示为向量，尽管它显然是一个标量。这只是为了保持流程并避免更改符号。如果我们使用*k*类分类（得到多个输出神经元），*y*将是一个向量。
- en: Now, if you think back to [Chapter 2](6a34798f-db83-4a32-9222-06ba717fc809.xhtml),
    *Linear Algebra*, where we did matrix multiplication, we learned that when a matrix
    or vector is multiplied by another matrix with differing dimensions, then the
    resulting matrix or vector is of a different shape (except, of course, when we
    multiply by the identity matrix). We call this mapping because our matrix maps
    points in one space to points in another space. Keeping this in mind, let's take
    a look again at the operations that were carried out in our MLP. From this, we
    can deduce that our neural network maps our input vector from one Euclidean space
    to our output vector in another Euclidean space.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你回想一下[第二章](6a34798f-db83-4a32-9222-06ba717fc809.xhtml)，*线性代数*，我们做了矩阵乘法，我们学习到，当一个矩阵或向量与另一个维度不同的矩阵相乘时，得到的矩阵或向量的形状会不同（当然，除非我们与单位矩阵相乘）。我们称这种操作为映射，因为我们的矩阵将一个空间中的点映射到另一个空间中的点。记住这一点，让我们再看一下在MLP中进行的操作。从中我们可以推导出我们的神经网络将输入向量从一个欧几里得空间映射到另一个欧几里得空间中的输出向量。
- en: 'Using this observation, we can generalize and write the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这一观察，我们可以概括并写出以下内容：
- en: '![](img/32adc558-0a11-49ed-9097-85e4291b0b22.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32adc558-0a11-49ed-9097-85e4291b0b22.png)'
- en: Here, [![](img/0b1e6f17-8765-410b-8f0e-8764a5481669.png)] is our MLP, [![](img/d4d6931d-2030-4b68-8c7f-82d126da64e7.png)] is
    the number of nodes in the dimension of the input layer, [![](img/5c4afa24-34e8-41be-9565-176e8bf53239.png)] is
    the number of nodes in the output layer, and *L* is the total number of layers.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，[![](img/0b1e6f17-8765-410b-8f0e-8764a5481669.png)] 是我们的 MLP，[![](img/d4d6931d-2030-4b68-8c7f-82d126da64e7.png)]
    是输入层维度中的节点数，[![](img/5c4afa24-34e8-41be-9565-176e8bf53239.png)] 是输出层中的节点数，而 *L*
    是总层数。
- en: However, there are a number of matrix multiplications that take place in the
    preceding network and each has different dimensions, which tells us that a sequence
    of mappings takes place (from one layer to the next).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在之前的网络中，有多个矩阵乘法操作发生，每个操作的维度不同，这表明映射序列发生了（从一层到下一层）。
- en: 'We can then write the mappings individually, as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以分别写出映射关系，如下所示：
- en: '![](img/461302a4-1d57-4d82-a5a0-cf6d12ea57a9.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/461302a4-1d57-4d82-a5a0-cf6d12ea57a9.png)'
- en: Here, each *f^i* value maps the *l^(th)* layer to the *l+1^(st)* layer. To make
    sure we have covered all of our bases, ![](img/c90c9a0b-7e57-426e-ba83-d952c0292587.png) and ![](img/58a84bec-44f5-4c0a-ac8c-8b2c9b24651d.png).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个 *f^i* 值将 *l^(th)* 层映射到 *l+1^(st)* 层。为了确保我们已覆盖所有内容，![](img/c90c9a0b-7e57-426e-ba83-d952c0292587.png)
    和 ![](img/58a84bec-44f5-4c0a-ac8c-8b2c9b24651d.png)。
- en: 'Now, we can summarize our MLP in the following equation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过以下方程来总结我们的多层感知器（MLP）：
- en: '![](img/79a27b1a-f32a-41e2-85a8-e1b0f1480971.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79a27b1a-f32a-41e2-85a8-e1b0f1480971.png)'
- en: With that done, we can now move on to the next subsection where we will understand
    activation functions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这一部分后，我们可以继续进入下一小节，了解激活函数。
- en: Activation functions
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: We have mentioned activation functions a few times so far and we introduced
    one of them as well—the sigmoid activation function. However, this isn't the only
    activation function that we use in neural networks. In fact, it is an active area
    of research, and today, there are many different types of activation functions.
    They can be classified into two types—linear and non-linear. We will focus on
    the latter because they are differentiable and this property is very important
    for us when we train neural networks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经提到过几次激活函数，并且也介绍过其中一个——Sigmoid 激活函数。然而，这并不是我们在神经网络中使用的唯一激活函数。事实上，激活函数是一个活跃的研究领域，今天有许多不同类型的激活函数。它们可以分为两种类型——线性和非线性。我们将重点讨论后者，因为它们是可微分的，这一特性对于我们训练神经网络时非常重要。
- en: Sigmoid
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid
- en: 'To start, we will take a look at sigmoid since we''ve already encountered it.
    The sigmoid function is written as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看 Sigmoid，因为我们已经遇到过它。Sigmoid 函数的公式如下：
- en: '![](img/65343faf-854d-42ef-b64e-62a778b610be.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65343faf-854d-42ef-b64e-62a778b610be.png)'
- en: 'The function looks as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的形式如下：
- en: '![](img/d904765c-afb9-4ec4-a8bc-71d32c02a4ee.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d904765c-afb9-4ec4-a8bc-71d32c02a4ee.png)'
- en: The sigmoid activation function takes the sum of the weighted inputs and bias
    as input and compresses the value into the `(0, 1)` range.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数将加权输入和偏置的和作为输入，并将其压缩到`(0, 1)` 范围内。
- en: 'Its derivative is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 它的导数如下：
- en: '![](img/b02c4eb1-3e11-4dd7-b385-45cbe316d190.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b02c4eb1-3e11-4dd7-b385-45cbe316d190.png)'
- en: 'The derivative will look as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 该导数的形式如下：
- en: '![](img/fbc8a1ff-d0f2-423a-affc-e76fcd497b49.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbc8a1ff-d0f2-423a-affc-e76fcd497b49.png)'
- en: This activation function is usually used in the output layer for predicting
    a probability-based output. We avoid using it in the hidden layers of deep neural
    networks because it leads to what is known as the vanishing gradient problem.
    When the value of *x* is either greater than `2` or less than `-2`, then the output
    of the sigmoid function is very close to `1` or `0`, respectively. This hinders
    the network's ability to learn or slows it down drastically.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 该激活函数通常用于输出层，用于预测基于概率的输出。我们避免在深度神经网络的隐藏层中使用它，因为它会导致所谓的梯度消失问题。当 *x* 的值大于 `2`
    或小于 `-2` 时，Sigmoid 函数的输出将非常接近 `1` 或 `0`，分别地。这会阻碍网络学习的能力或极大地减慢学习速度。
- en: Hyperbolic tangent
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双曲正切函数
- en: 'Another activation function used instead of the sigmoid is the hyperbolic tangent
    (*tanh*). It is written as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种替代 Sigmoid 的激活函数是双曲正切函数（*tanh*）。其公式如下：
- en: '![](img/29a6dbcc-29d9-48dd-8ce1-5dd8eee8612c.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29a6dbcc-29d9-48dd-8ce1-5dd8eee8612c.png)'
- en: 'The function looks as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的形式如下：
- en: '![](img/c34fe00f-f828-46e6-acd4-0bba19b8686a.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c34fe00f-f828-46e6-acd4-0bba19b8686a.png)'
- en: 'The `tanh` function squashes all the output values into the `(-1, 1)` range.
    Its derivative is as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`tanh`函数将所有输出值压缩到`(-1, 1)`范围内。其导数如下所示：'
- en: '![](img/a78c193d-d87d-4b24-8dbd-245871bc6541.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a78c193d-d87d-4b24-8dbd-245871bc6541.png)'
- en: 'The derivative looks as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 导数如下所示：
- en: '![](img/611ab700-5c35-47a8-ae99-da0ee9ce946e.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/611ab700-5c35-47a8-ae99-da0ee9ce946e.png)'
- en: From the preceding graph you can tell that the `tanh` function is zero-centered,
    which allows us to model values that are very positive, very negative, or neutral.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中可以看出，`tanh`函数是零中心的，这使得我们能够建模非常正、非常负或中性的值。
- en: Softmax
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax
- en: The softmax activation function normalizes a vector containing *K* elements
    into a probability distribution over the *K* elements. For this reason, it is
    generally used in the output layer to predict the probability of it being one
    of the classes.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: softmax激活函数将包含*K*元素的向量标准化为*K*元素的概率分布。因此，它通常用于输出层，以预测它属于某一类别的概率。
- en: 'The softmax function is as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: softmax函数如下所示：
- en: '![](img/078e746d-755c-49e1-ad8e-eb385d1bd08a.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/078e746d-755c-49e1-ad8e-eb385d1bd08a.png)'
- en: 'Its derivative can be found using the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 它的导数可以通过以下方式找到：
- en: '![](img/4bea56c6-9781-4b10-8394-5e5853583f38.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bea56c6-9781-4b10-8394-5e5853583f38.png)'
- en: Rectified linear unit
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整流线性单元
- en: '**Rectified linear unit** (**ReLU**) is one of the most widely used activation
    functions because it is more computationally efficient than the activation functions
    we have already seen; therefore, it allows the network to train a lot faster and
    so converge more quickly.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**整流线性单元**（**ReLU**）是最广泛使用的激活函数之一，因为它比我们之前见过的激活函数在计算上更高效；因此，它使得网络能够更快地训练，从而更快地收敛。'
- en: 'The ReLU function is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU函数如下所示：
- en: '![](img/c8f560b9-eeda-4c48-9b5a-02ced40f9feb.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8f560b9-eeda-4c48-9b5a-02ced40f9feb.png)'
- en: 'The function looks as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数如下所示：
- en: '![](img/abefcb80-ff62-440c-a9b2-b194a6a3b7e0.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/abefcb80-ff62-440c-a9b2-b194a6a3b7e0.png)'
- en: 'As you can see, all the negative values for *x* are clipped off and turn into
    `0`. It may surprise you to know that even though this looks like a linear function,
    it has a derivative that is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，所有负值*x*都被剪切并变为`0`。虽然这看起来像一个线性函数，但它的导数如下所示：
- en: '![](img/50b76861-ee1f-4b43-93ea-a958ad18ec33.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50b76861-ee1f-4b43-93ea-a958ad18ec33.png)'
- en: 'The derivative looks as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 导数如下所示：
- en: '![](img/b4686eec-9555-4d48-b6d4-515c8688ba4a.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4686eec-9555-4d48-b6d4-515c8688ba4a.png)'
- en: This, too, faces some problems in training—particularly, the dying ReLU problem.
    This occurs when the input values are negative and this hinders learning because
    we cannot differentiate `0`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这同样在训练中面临一些问题，特别是“死亡ReLU”问题。当输入值为负时，学习会受到阻碍，因为我们无法对`0`进行求导。
- en: Leaky ReLU
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: Leaky ReLU is a modification of the ReLU function that we saw in the previous
    section and it not only enables the network to learn faster but it is also more
    balanced as it helps deal with vanishing gradients.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU是ReLU函数的一种修改版，它不仅能让网络学习更快，还更加平衡，帮助解决梯度消失问题。
- en: 'The leaky ReLU function is as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 漏斗ReLU函数如下所示：
- en: '![](img/1a63e576-a013-457c-b768-b511947fca9c.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a63e576-a013-457c-b768-b511947fca9c.png)'
- en: 'The function looks as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数如下所示：
- en: '![](img/4d8475c2-09b7-4ab2-9f40-07f8719824a4.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d8475c2-09b7-4ab2-9f40-07f8719824a4.png)'
- en: 'As you can see, the difference here is that the negative values of *x* that
    were clipped off before are now rescaled to ![](img/fe6ed447-d293-4927-8093-23183b8cb1af.png),
    which overcomes the dying ReLU problem. The derivative of this activation function
    is as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，区别在于之前被剪切掉的负值*x*现在被重新缩放为！[](img/fe6ed447-d293-4927-8093-23183b8cb1af.png)，这克服了“死亡ReLU”问题。该激活函数的导数如下所示：
- en: '![](img/e7b04e0f-b2e7-4f90-8da3-de579d948f75.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7b04e0f-b2e7-4f90-8da3-de579d948f75.png)'
- en: 'The derivative looks as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 导数如下所示：
- en: '![](img/c33ce17c-74ce-4b21-98aa-3f6a08ca261d.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c33ce17c-74ce-4b21-98aa-3f6a08ca261d.png)'
- en: Parametric ReLU
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数化ReLU
- en: '**Parametric ReLU** (**PReLU**) is a variation of the leaky ReLU activation
    function and has similar performance improvements to it, except that here, the
    parameters are learnable whereas before they were not.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数化ReLU**（**PReLU**）是漏斗ReLU激活函数的一种变种，具有类似的性能提升，唯一不同的是，在这里，参数是可学习的，而之前不是。'
- en: 'The PReLU function is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: PReLU函数如下所示：
- en: '![](img/d29916bd-ccbb-42a3-b1f8-bf7ab4f3ed13.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d29916bd-ccbb-42a3-b1f8-bf7ab4f3ed13.png)'
- en: 'The function looks as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数如下所示：
- en: '![](img/a26d887b-874d-4131-972b-c714c0c2625b.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a26d887b-874d-4131-972b-c714c0c2625b.png)'
- en: 'The derivative is as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 导数如下所示：
- en: '![](img/ea942e7c-4a0a-4453-8bbe-66e5b96e55de.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea942e7c-4a0a-4453-8bbe-66e5b96e55de.png)'
- en: 'The derivative looks as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 导数如下所示：
- en: '![](img/6903a6f6-6f11-4383-a04c-2a3b1234fdd9.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6903a6f6-6f11-4383-a04c-2a3b1234fdd9.png)'
- en: Exponential linear unit
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指数线性单元
- en: '**Exponential linear unit** (**ELU**) is another variation of the leaky ReLU
    activation function, where instead of having a straight line for all cases of ![](img/e0e33eee-fef0-45c8-9c61-885d9b83f959.png),
    it is a log curve.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**指数线性单元** (**ELU**) 是一种改进版的泄漏ReLU激活函数，不同于所有情况下都有直线的形式，**ELU** 使用了对数曲线。'
- en: 'The ELU activation function is as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**ELU** 激活函数如下所示：'
- en: '![](img/12b5ff24-5b55-422f-a554-2fccbc506809.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12b5ff24-5b55-422f-a554-2fccbc506809.png)'
- en: 'The function looks as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 函数如下所示：
- en: '![](img/13d45385-b514-4f7b-a22e-2925ce8343c2.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13d45385-b514-4f7b-a22e-2925ce8343c2.png)'
- en: 'The derivative of this activation function is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 该激活函数的导数如下所示：
- en: '![](img/c5bb5a77-8275-4ed0-9868-6bfdf38b94a5.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5bb5a77-8275-4ed0-9868-6bfdf38b94a5.png)'
- en: The loss function
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: The loss function is a very critical part of neural networks and their training.
    They give us a means of calculating the error of a network after a forward pass
    has been computed. This error compares the neural network output with the target
    output that was specified in the training data.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是神经网络及其训练中非常关键的一部分。它们为我们提供了一种计算神经网络在前向传播后误差的方法。这个误差将神经网络的输出与训练数据中指定的目标输出进行比较。
- en: There are two errors in particular that are of concern to us—the local error
    and the global error. The local error is the difference between the output expected
    of a neuron and its actual output. The global error, however, is the total error
    (the sum of all the local errors) and it tells us how well our network is performing
    on the training data.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注的主要有两种误差——局部误差和全局误差。局部误差是神经元的预期输出与实际输出之间的差异。然而，全局误差是总误差（所有局部误差的和），它告诉我们网络在训练数据上的表现如何。
- en: There are a number of methods that we use in practice and each has its own use
    cases, advantages, and disadvantages. Conventionally, the loss function is referred
    to as the cost function and is denoted as *J(θ)* (or, equivalently, *J(W,b)*).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，我们有许多方法，每种方法都有其特定的使用场景、优缺点。传统上，损失函数被称为成本函数，表示为 *J(θ)* （或等效地，*J(W,b)*）。
- en: Mean absolute error
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均绝对误差
- en: '**Mean absolute error** (**MAE**) is the same as the L1 loss we saw in [Chapter
    3](719fc119-9e7a-4fce-be04-eb1e49bed753.xhtml), *Probability and Statistics*,
    and it looks as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差** (**MAE**) 就是我们在[第3章](719fc119-9e7a-4fce-be04-eb1e49bed753.xhtml)《概率与统计》中看到的L1损失，它的公式如下：'
- en: '![](img/d0621604-a42a-4f1a-a340-cfee0baf3b8a.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0621604-a42a-4f1a-a340-cfee0baf3b8a.png)'
- en: Here, *N* is the number of samples in our training dataset.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*N* 是我们训练数据集中样本的数量。
- en: What we are doing here is calculating the absolute distance between the prediction
    and the true value and averaging over the sum of the errors.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做的是计算预测值与真实值之间的绝对距离，并对所有误差的总和进行平均。
- en: Mean squared error
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方误差
- en: '**Mean squared error** (**MSE**) is one of the most commonly used loss functions,
    especially for regression tasks (it takes in a vector and outputs a scalar). It
    calculates the square of the difference between the output and the expected output.
    It looks as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差** (**MSE**) 是最常用的损失函数之一，尤其在回归任务中（它输入一个向量并输出一个标量）。它计算的是输出与预期输出之间差异的平方。公式如下：'
- en: '![](img/975006bd-f561-4375-8b79-780d31b47f91.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/975006bd-f561-4375-8b79-780d31b47f91.png)'
- en: Here, *N* is the number of samples in our training dataset.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*N* 是我们训练数据集中样本的数量。
- en: In the preceding equation, we calculate the square of the L2 norm. Intuitively,
    we should be able to tell that when [![](img/6ede8b8e-9f4b-4a57-9be8-409aa2efd1aa.png)] ,
    the error is 0, and the larger the distance between the points, the larger the
    error. The reason we use this is that it always outputs a positive value and by
    squaring the distance between the output and expected output, it allows us to
    differentiate between small and large errors with greater ease and correct them.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，我们计算的是L2范数的平方。直观上，我们应该能够看出，当[![](img/6ede8b8e-9f4b-4a57-9be8-409aa2efd1aa.png)]时，误差为0，且点之间的距离越大，误差越大。我们之所以使用这个，是因为它总是输出一个正值，通过对输出与预期输出之间的距离进行平方处理，它使我们更容易区分小误差和大误差，并加以修正。
- en: Root mean squared error
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方根误差
- en: '**Root mean squared error** (**RMSE**) is simply the square root of the preceding
    MSE function and it looks as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方根误差**（**RMSE**）是前面提到的均方误差（MSE）函数的平方根，表达式如下：'
- en: '![](img/852bea1b-21f1-4452-913a-00c654336837.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/852bea1b-21f1-4452-913a-00c654336837.png)'
- en: The reason we use this is that it scales back the MSE function to the scale
    it was originally at before we squared the errors, which gives us a better idea
    of the error with respect to the target(s).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这个损失的原因是，它将MSE函数缩放回平方误差之前的原始尺度，这使得我们更好地理解相对于目标的误差。
- en: The Huber loss
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Huber损失
- en: 'The Huber loss looks as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Huber损失的表达式如下：
- en: '![](img/f4bdc492-ee0d-4da4-9caa-b13cf5c69164.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4bdc492-ee0d-4da4-9caa-b13cf5c69164.png)'
- en: Here, ε is a constant term that we can configure. The smaller it is, the more
    insensitive the loss is to large errors and outliers, and the larger it is, the
    more sensitive the loss is to large errors and outliers.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，ε 是一个常数项，可以配置。它越小，损失对大误差和异常值的敏感度越低；而它越大，损失对大误差和异常值的敏感度越高。
- en: Now, if you look closely, you should notice that when ε is very small, the Huber
    loss is similar to MAE, but when it is very large, it is similar to MSE.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你仔细观察，你会发现，当ε非常小的时候，Huber损失类似于MAE，而当它非常大的时候，它则类似于MSE。
- en: Cross entropy
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵
- en: Cross entropy loss is used mostly when we have a binary classification problem;
    that is, where the network outputs either 1 or 0.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失主要用于二分类问题，也就是网络输出值为1或0的情况。
- en: 'Suppose we are given a training dataset, [![](img/89873e28-1e09-4bbf-a107-d969446d59c2.png)] and
    [![](img/d21a5ae5-3daf-47d5-bf91-2871160d90d7.png)]. We can then write this in
    the following form:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 假设给定一个训练数据集， [![](img/89873e28-1e09-4bbf-a107-d969446d59c2.png)] 和 [![](img/d21a5ae5-3daf-47d5-bf91-2871160d90d7.png)]。我们可以将其表示为以下形式：
- en: '![](img/e66a7df7-8d7a-44d0-ade2-1bb56a78564f.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e66a7df7-8d7a-44d0-ade2-1bb56a78564f.png)'
- en: 'Here, *θ* is the parameters of the network (weights and biases). We can express
    this in terms of a Bernoulli distribution, as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， *θ* 是网络的参数（权重和偏置）。我们可以通过伯努利分布来表示，表达式如下：
- en: '![](img/29d4cb2b-be80-4664-965b-7dfa4bf1a434.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29d4cb2b-be80-4664-965b-7dfa4bf1a434.png)'
- en: 'The probability, given the entire dataset, is then as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 给定整个数据集的概率如下：
- en: '![](img/f3f8a931-1ad0-4460-a978-83b0a96478f8.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3f8a931-1ad0-4460-a978-83b0a96478f8.png)'
- en: 'If we take its negative-log likelihood, we get the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取其负对数似然，我们得到如下结果：
- en: '![](img/42581895-dccc-430f-98fa-f1bd6b524fa9.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42581895-dccc-430f-98fa-f1bd6b524fa9.png)'
- en: 'So, we have the following:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们得到以下公式：
- en: '![](img/be5d8e99-7a02-44cd-9d97-b26ddd07e808.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be5d8e99-7a02-44cd-9d97-b26ddd07e808.png)'
- en: 'Cross entropy is also used when we have more than two classes. This is known
    as **multiclass cross entropy**. Suppose we have *K* output units, then, we would
    calculate the loss for each class and then sum them together, as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有超过两个类别时，也会使用交叉熵，这种情况称为**多分类交叉熵**。假设我们有 *K* 个输出单元，那么我们会计算每个类别的损失，然后将它们相加，表达式如下：
- en: '![](img/5bbf3b40-e925-4231-91b6-09f8522ec81e.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bbf3b40-e925-4231-91b6-09f8522ec81e.png)'
- en: Here, [![](img/1ec5fd38-ea69-4982-bc7d-e79cd01d8667.png)] is the probability
    that observation (*i*) belongs to class *k*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， [![](img/1ec5fd38-ea69-4982-bc7d-e79cd01d8667.png)] 是观察值 (*i*) 属于类别 *k*
    的概率。
- en: Kullback-Leibler divergence
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 库尔贝克-莱布勒散度
- en: '**Kullback-Leibler** (KL)** divergence** measures the divergence of two probability
    distributions, *p* and *q*. It looks as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**库尔贝克-莱布勒**（KL）**散度** 衡量两个概率分布 *p* 和 *q* 的差异。表达式如下：'
- en: '![](img/73a11d7b-8708-41af-bbab-82d8c5fc26ad.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73a11d7b-8708-41af-bbab-82d8c5fc26ad.png)'
- en: So, when *p(x)=q(x)*, the KL divergence value is 0 at all points. This is usually
    used in generative models.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当 *p(x)=q(x)* 时，KL散度在所有点的值为0。这通常用于生成模型。
- en: Jensen-Shannon divergence
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 詹森-香农散度
- en: 'Like the KL divergence, the **Jensen-Shannon** (**JS**) divergence measures
    how similar two probability distributions are; however, it is smoother. The following
    equation represents the JS divergence:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 像KL散度一样，**詹森-香农**（**JS**）散度衡量两个概率分布的相似度；然而，它更加平滑。以下方程表示JS散度：
- en: '![](img/ec2e7ec1-cfcf-4c33-8cd5-307053b2dfe2.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec2e7ec1-cfcf-4c33-8cd5-307053b2dfe2.png)'
- en: This behaves a lot better than KL divergence when *p(x)* and *q(x)* are both
    small.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *p(x)* 和 *q(x)* 都很小的时候，它表现得比KL散度要好得多。
- en: Backpropagation
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: Now that we know how the forward passes are computed in MLPs, as well as how
    to best initialize them and calculate the loss of the network, it is time for
    us to learn about backpropagation—a method that allows us to calculate the gradient
    of the network using the information from the loss function. This is where our
    knowledge of multivariable calculus and partial derivatives comes in handy.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何计算MLP中的前向传递过程，以及如何初始化它们并计算网络的损失，是时候了解反向传播了——这是一种利用损失函数的信息计算网络梯度的方法。此时，我们需要用到多元微积分和偏导数的知识。
- en: 'If you recall, this network is fully connected, which means all the nodes in
    each layer are connected to—and so have an impact on—the next layer. It is for
    this reason that in backpropagation we take the derivative of the loss with respect
    to the weights of the layer closest to the output, then the one before that, and
    so on, until we reach the first layer. If you don''t yet understand this, don''t
    worry. We will go through backpropagation in detail and use the network from earlier
    as an example. We will assume that the activation function is sigmoid and our
    loss function is cross entropy. We will first calculate the derivative of the
    loss ([![](img/b3c63103-f3cb-42be-84d8-237d3b9f1ba4.png)]) with respect to *W^([4])*,
    which looks as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，这个网络是全连接的，这意味着每一层中的所有节点都与下一层的节点相连，并且对下一层有影响。正因为如此，在反向传播中，我们首先对最靠近输出层的权重求损失函数的导数，然后是前一层的权重，以此类推，直到达到第一层。如果你还不理解这个过程，不用担心，我们将详细讲解反向传播，并以之前的网络为例。我们假设激活函数为sigmoid，损失函数为交叉熵。首先，我们将计算损失函数对*W^([4])*的导数，其表达式如下：
- en: '![](img/5971378d-f9e5-44ed-bfff-00e5806d6bb6.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5971378d-f9e5-44ed-bfff-00e5806d6bb6.png)'
- en: '![](img/74578a41-447c-4c30-9ac3-17dc423088e6.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74578a41-447c-4c30-9ac3-17dc423088e6.png)'
- en: '![](img/fbf77b44-2541-4e37-b7d7-78b0629a59f6.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbf77b44-2541-4e37-b7d7-78b0629a59f6.png)'
- en: '![](img/84e2b8f4-2a9f-485c-a166-30ee953c792f.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84e2b8f4-2a9f-485c-a166-30ee953c792f.png)'
- en: '![](img/2f2a92d2-a163-42b7-ba81-52ac4ac6ce80.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f2a92d2-a163-42b7-ba81-52ac4ac6ce80.png)'
- en: '![](img/732afee6-3253-4cca-ba9c-14258bb9b268.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/732afee6-3253-4cca-ba9c-14258bb9b268.png)'
- en: With that, we have finished computing the first derivative. As you can see,
    it takes quite a bit of work, and calculating the derivative for each layer can
    be a very time-consuming process. So, instead, we can make use of the chain rule
    from calculus.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就完成了第一次导数的计算。正如你所看到的，这个过程需要花费相当多的工作，计算每一层的导数可能是一个非常耗时的过程。所以，我们可以利用微积分中的链式法则来简化计算。
- en: 'For simplicity, let''s say [![](img/a8629f59-6322-4371-9765-a8886500192e.png)] and [![](img/ca778ea0-b62b-4a10-b82b-b7983efea97d.png)] and
    assume that [![](img/d67fa046-f726-435b-a2a2-1457433eadef.png)]. Now, if we want
    to calculate the gradient of the loss with respect to *W^([2])*, we get the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便起见，假设[![](img/a8629f59-6322-4371-9765-a8886500192e.png)]和[![](img/ca778ea0-b62b-4a10-b82b-b7983efea97d.png)]，并且假设[![](img/d67fa046-f726-435b-a2a2-1457433eadef.png)]。现在，如果我们要计算损失对*W^([2])*的梯度，我们得到如下结果：
- en: '![](img/593b0c14-ca82-419c-90e9-b4523b2154f6.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/593b0c14-ca82-419c-90e9-b4523b2154f6.png)'
- en: 'We can rewrite this as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其重写为：
- en: '![](img/444ad371-8e2f-4897-90df-4ec18c0f615e.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/444ad371-8e2f-4897-90df-4ec18c0f615e.png)'
- en: 'Suppose we do want to find the partial of the loss with respect to *b^([4])*;
    this looks as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们确实要计算损失对*b^([4])*的偏导数，其表达式如下：
- en: '![](img/0f02076f-6afa-4cc7-b5d4-b80104251477.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f02076f-6afa-4cc7-b5d4-b80104251477.png)'
- en: Before we move on to the next section, pay close attention to the preceding
    derivative, [![](img/0b1d4d1d-0fa9-4744-b002-edcc2ea7c9a3.png)]. If you look back
    to earlier on in the *Layers* section, [![](img/e03583c2-1673-49bb-acc8-47b9f00164e9.png)] were
    all vectors and matrices. This is still true. Because we are again dealing with
    vectors and matrices, it is important that their dimensions line up.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一部分之前，请密切注意前面的导数，[![](img/0b1d4d1d-0fa9-4744-b002-edcc2ea7c9a3.png)]。如果你回头看看*层*这一节，[![](img/e03583c2-1673-49bb-acc8-47b9f00164e9.png)]仍然是向量和矩阵。这一点仍然成立。因为我们又在处理向量和矩阵，所以确保它们的维度一致非常重要。
- en: We know that ![](img/54559173-808e-482d-bfb0-53c473fad4bc.png), but what about
    the others? I will leave this to you as an exercise to determine whether or not
    the other is correct and if it is not, how would you change the order to ensure
    it is?
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道！[](img/54559173-808e-482d-bfb0-53c473fad4bc.png)，但其他的呢？我将留给你作为练习，看看其他的是否正确，如果不正确，你会如何调整顺序以确保它是正确的？
- en: If you're feeling very confident in your math abilities and are up for a challenge,
    I encourage you to try finding the derivative, [![](img/06d785f2-0ebb-4e1a-9f28-25e3989dd48b.png)].
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对自己的数学能力非常有信心并且愿意挑战一下，我鼓励你尝试求导，[![](img/06d785f2-0ebb-4e1a-9f28-25e3989dd48b.png)]。
- en: Training neural networks
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: Now that we have an understanding of backpropagation and how gradients are computed,
    you might be wondering what purpose it serves and what it has to do with training
    our MLP. If you will recall from [Chapter 1](3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml),
    *Vector Calculus*, when we covered partial derivatives, we learned that we can
    use partial derivatives to check the impact that changing one parameter can have
    on the output of a function. When we use the first and second derivatives to plot
    our graphs, we can analytically tell what the local and global minima and maxima
    are. However, it isn't as straightforward as that in our case as our model doesn't
    know where the optima is or how to get there; so, instead, we use backpropagation
    with the gradient descent as a guide to help us get to the (hopefully global)
    minima.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了反向传播和梯度是如何计算的，你可能会想知道它的作用是什么，以及它与训练我们的MLP有什么关系。如果你还记得在[第1章](3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml)《*向量微积分*》中，当我们讨论偏导数时，我们学到可以使用偏导数来检查改变一个参数对函数输出的影响。当我们使用一阶和二阶导数绘制图形时，我们可以从分析上得知局部和全局最小值与最大值。然而，在我们的例子中并非如此直接，因为我们的模型并不知道最优解在哪里或者如何到达那里；因此，我们使用反向传播和梯度下降作为引导，帮助我们到达（希望是全局的）最小值。
- en: 'In [Chapter 4](feeeb2a4-650e-445a-8f97-8c0ebb2538eb.xhtml), *Optimization*,
    we learned about gradient descent and how we iteratively move from one point on
    the function to a lower point on the function that is in the direction of the
    local/global minima by taking a step in the direction of the negative of the gradient.
    We expressed it in the following form:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](feeeb2a4-650e-445a-8f97-8c0ebb2538eb.xhtml)《*优化*》中，我们学习了梯度下降法，以及我们如何通过迭代地从函数的一个点移动到函数的一个较低点，在指向局部/全局最小值的方向上沿着负梯度的方向迈进一步。我们将其表示为如下形式：
- en: '![](img/2d7750e6-5c01-45c1-8a47-3f2aeb580569.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d7750e6-5c01-45c1-8a47-3f2aeb580569.png)'
- en: 'However, for neural networks, the update rule for the weights, in this case,
    is written as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于神经网络来说，权重的更新规则在此情况下写作如下：
- en: '![](img/4d12e7fe-323f-4fd8-adcc-a9f01f783955.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d12e7fe-323f-4fd8-adcc-a9f01f783955.png)'
- en: Here, *θ* = *(W,b)*.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*θ* = *(W,b)*。
- en: As you can see, while this does look similar, it isn't the optimization we have
    learned. Our goal here is to minimize the total loss of the network and update
    our weights accordingly.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，尽管看起来类似，但这并不是我们学过的优化方法。我们的目标是最小化网络的总损失，并相应地更新我们的权重。
- en: Parameter initialization
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数初始化
- en: In [Chapter 4](feeeb2a4-650e-445a-8f97-8c0ebb2538eb.xhtml), *Optimization*,
    we mentioned that before we start optimizing, we need an initial (starting) point,
    which is the purpose of initialization. This is an extremely important part of
    training neural networks because as mentioned earlier on in this chapter, neural
    networks have a lot of parameters—often, well over tens of millions—which means
    that finding the point in the weight space that minimizes our loss can be very
    time consuming and challenging (because the weight space is non-convex; that is,
    there are lots of local minima and saddle points).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](feeeb2a4-650e-445a-8f97-8c0ebb2538eb.xhtml)《*优化*》中，我们提到在开始优化之前，我们需要一个初始（起始）点，这就是初始化的目的。这是训练神经网络中一个极其重要的部分，因为正如本章前面提到的，神经网络有很多参数——通常超过千万——这意味着找到在权重空间中最小化损失的点可能非常耗时且具有挑战性（因为权重空间是非凸的；也就是说，存在很多局部最小值和鞍点）。
- en: For this reason, finding a good initial point is important because it makes
    it easier to get to the optima and reduce the training time, as well as reducing
    the chances of our weights either vanishing or exploding. Let's now explore the
    various ways that we can initialize our weights and biases.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个原因，找到一个好的初始点非常重要，因为它能让我们更容易找到最优解并减少训练时间，同时减少权重消失或爆炸的可能性。现在让我们探索一下可以初始化权重和偏置的不同方式。
- en: All zeros
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全零初始化
- en: As the name suggests, here we set the initial weights and biases of our model
    to be zeros. I don't recommend doing this because, as you may have guessed, this
    means that all the neurons in our model are dead. In fact, this is the very problem
    we want to avoid when training our network.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名字所示，在这里我们将模型的初始权重和偏置设置为0。我不建议这么做，因为，正如你可能猜到的那样，这意味着我们模型中的所有神经元都“死了”。实际上，这正是我们在训练网络时想要避免的问题。
- en: 'Let''s see what happens anyway. For the sake of simplicity, let''s suppose
    we have the following linear classifier:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看会发生什么。为了简化起见，假设我们有以下的线性分类器：
- en: '![](img/64678005-0a60-463a-b72c-2e0bbf92bff7.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64678005-0a60-463a-b72c-2e0bbf92bff7.png)'
- en: If the weights are initialized as 0, then our output is always 0, which means
    we lost all the information that was part of our training data and the network
    that we put so much effort into building learns nothing.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果权重初始化为0，那么我们的输出将始终是0，这意味着我们丧失了训练数据中所有的信息，并且我们投入大量精力构建的网络什么也学不到。
- en: Random initialization
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机初始化
- en: One way of initializing our weights to be non-zero is to use random initialization
    and for this, we could use one of two distributions—the normal distribution or
    the uniform distribution.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化权重为非零的其中一种方法是使用随机初始化，对于这一点，我们可以使用两种分布之一——正态分布或均匀分布。
- en: To initialize our parameters using the normal distribution, we have to specify
    the mean and the standard deviation. Usually, we choose a mean of 0 and a standard
    deviation of 1\. To initialize using the uniform distribution, we usually use
    the [-1, 1] range (where there is an equal probability of any value in the range
    being picked).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用正态分布初始化我们的参数，我们必须指定均值和标准差。通常，我们选择均值为0，标准差为1。为了使用均匀分布初始化，我们通常使用[-1, 1]的范围（在这个范围内，每个值被选中的概率是相等的）。
- en: While this gives us weights that we can use in training, it is very slow and
    has previously resulted in vanishing and exploding gradients in deep networks,
    resulting in mediocre performance.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这给我们提供了可以用于训练的权重，但它非常慢，并且之前在深度网络中曾导致梯度消失和梯度爆炸，导致表现平平。
- en: Xavier initialization
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Xavier初始化
- en: As we have seen, if our weights are too small, then they vanish, which results
    in dead neurons and, conversely, if our weights are too big, we get exploding
    gradients. We want to avoid both scenarios, which means we need the weights to
    be initialized just right so that our network can learn what it needs to.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，如果我们的权重过小，那么它们会消失，导致神经元死亡；相反，如果我们的权重过大，就会出现梯度爆炸。我们希望避免这两种情况，这意味着我们需要将权重初始化得刚刚好，以便我们的网络能够学习所需的内容。
- en: 'To tackle this problem, Xavier Glorot and Yoshua Bengio created a normalized
    initialization method (generally referred to as Xavier initialization). It is
    as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Xavier Glorot和Yoshua Bengio创建了一种规范化的初始化方法（通常称为Xavier初始化）。它的具体方法如下：
- en: '![](img/457d8e0b-c3d7-4d87-b136-0e25d3a09732.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/457d8e0b-c3d7-4d87-b136-0e25d3a09732.png)'
- en: Here, *n[k]* is the number of neurons in layer *k*.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*n[k]* 是第 *k* 层中的神经元数量。
- en: But why does this work better than randomly initializing our network? The idea
    is that we want to maintain the variance as we propagate through subsequent layers.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为什么这种方法比随机初始化我们的网络效果更好呢？这个想法是，我们希望在通过后续层传播时保持方差。
- en: The data
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: As you will know by now, what we are trying to build here are networks that
    can learn to map an input to an output. For our network to be able to do this,
    it needs to be fed data—and lots of it. Therefore, it is important for us to know
    what the data should look like.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你现在所了解的，我们在这里尝试构建的是能够学习将输入映射到输出的网络。为了让我们的网络能够做到这一点，它需要接受数据——而且是大量数据。因此，我们需要知道数据应该是什么样子的，这一点非常重要。
- en: 'Let''s suppose we have a classification or regression task. Our data will then
    take the following form:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个分类或回归任务。我们的数据将呈现以下形式：
- en: '![](img/d26f3546-527b-4ce8-889d-7fbe1ac0af4a.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d26f3546-527b-4ce8-889d-7fbe1ac0af4a.png)'
- en: 'Here, we assume the following:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设以下内容：
- en: '![](img/1be36326-723b-4d59-b4bc-b2160f8f5a5b.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1be36326-723b-4d59-b4bc-b2160f8f5a5b.png)'
- en: As you can see, each sample in the dataset has the input (*x[i]*) and a corresponding
    output/target (*y[i]*). However, depending on the task, our output will look a
    bit different. In regression, our output can take on any real value, whereas in
    classification, it must be one of the classes we can predict.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，数据集中的每个样本都有输入（*x[i]*）和相应的输出/目标（*y[i]*）。然而，具体表现可能会根据任务有所不同。在回归中，我们的输出可以是任意实数值，而在分类中，它必须是我们可以预测的某个类别。
- en: 'Our data (*x*), as you may expect, contains all the various information we
    want to use to predict our target variables (*y*) and this, of course, depends
    on the problem. As an example, let''s take the Boston Housing dataset, which is
    a regression task. It contains the following features:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所料，我们的数据（*x*）包含了我们希望用来预测目标变量（*y*）的各种信息，这当然取决于问题的具体情况。例如，假设我们使用波士顿住房数据集，这是一个回归任务。它包含以下特征：
- en: The per-capita crime rate by town
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个城镇的犯罪率（按人均计算）
- en: The proportion of residential land zoned for lots over 25,000 square feet
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划为25,000平方英尺以上大块地皮的住宅用地比例
- en: The proportion of non-retail business acres per town
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个城镇非零售商业用地的比例
- en: The Charles River dummy variable (1 if tract bounds river and 0 if not)
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查尔斯河虚拟变量（如果区域边界接壤河流则为1，否则为0）
- en: The nitric oxide concentration value (parts per 10 million)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一氧化氮浓度值（百万分之一）
- en: The average number of rooms per dwelling
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个住宅的平均房间数
- en: The proportion of owner-occupied units built before 1940
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1940年之前建造的自有住宅比例
- en: The weighted distances to five Boston employment centers
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 距离五个波士顿就业中心的加权距离
- en: The index of accessibility to radial highways
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通往径向高速公路的可达性指数
- en: The full-value property tax rate per $10,000
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每$10,000的全额房产税税率
- en: The pupil-to-teacher ratio by town
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个城镇的师生比
- en: The proportion of African Americans by town
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个城镇非裔美国人比例
- en: The percentage of the population that is of a lower status
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处于较低社会地位的群体所占人口比例
- en: The target variable is the median value of owner-occupied homes in $1,000.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 目标变量是自有住房的中位数价值，单位为$1,000。
- en: All the data is numerical (since the machines don't really read or know what
    those labels mean, but they do know how to parse numbers).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据都是数值型的（因为机器并不真正理解这些标签的含义，但它们知道如何解析数字）。
- en: Now, let's look at a classification problem—since we are trying to predict which
    class our data belongs to, the target will become a vector instead of a scalar
    (as it is in the preceding dataset), where the dimension of the target vector
    will be the number of categories. But how do we represent this target vector?
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来看一个分类问题——因为我们正在尝试预测数据属于哪一类，所以目标将变成一个向量，而不是前面数据集中使用的标量，目标向量的维度将是类别的数量。那么我们该如何表示这个目标向量呢？
- en: 'Suppose we have a dataset of images with the corresponding target labels:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含图像及其对应目标标签的数据集：
- en: '![](img/7d724ddb-33aa-4f14-9bc0-858c4cd46694.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d724ddb-33aa-4f14-9bc0-858c4cd46694.png)'
- en: 'As you can see, each label has a digit assigned to it and during training,
    our network could mistake these for trainable parameters, which we obviously would
    want to avoid. Instead, we can one-hot encode this, thereby turning the label
    vector into the following:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，每个标签都有一个数字分配给它，在训练过程中，我们的网络可能会误将这些视为可训练参数，这显然是我们希望避免的。相反，我们可以进行独热编码，将标签向量转化为如下形式：
- en: '![](img/0088e534-5123-4a3b-ad07-723176a09347.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0088e534-5123-4a3b-ad07-723176a09347.png)'
- en: Great! Now we know what is in a dataset and how datasets are structured. But
    what now? We split the dataset into training, testing, and validation sets. How
    we split the data into the three respective sets depends largely on how much data
    we have. In the case of deep learning, we will, more often than not, be dealing
    with very large datasets; that is, millions to tens of millions of samples.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！现在我们知道了数据集的内容以及数据集的结构。那么接下来呢？我们将数据集分为训练集、测试集和验证集。如何将数据划分为这三个集合，很大程度上取决于我们拥有的数据量。在深度学习的情况下，我们通常会处理非常大的数据集，也就是说，数百万到千万级的样本。
- en: As a rule of thumb, we generally select 80-90% of the dataset to train our network,
    and the remaining 10-20% is split into two portions—the validation and test sets.
    The validation set is used during training to determine whether our network has
    overfit or underfit to the data and the test set is used at the end to check how
    well our model generalizes to unseen data.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们通常选择80-90%的数据集用于训练我们的网络，其余的10-20%则分为两个部分——验证集和测试集。在训练过程中，验证集用于确定我们的网络是否出现过拟合或欠拟合，而测试集则用于最后检查我们的模型在处理未见数据时的泛化能力。
- en: Deep neural networks
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: Now, it's time to get into the really fun stuff (and what you picked up this
    book for)—deep neural networks. The depth comes from the number of layers in the
    neural network and for an FNN to be considered deep, it must have more than 10
    hidden layers. A number of today's state-of-the-art FNNs have well over 40 layers.
    Let's now explore some of the properties of deep FNNs and get an understanding
    of why they are so powerful.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，到了真正有趣的部分（也是你拿起这本书的原因）——深度神经网络。深度的来源是神经网络中的层数，要使一个前馈神经网络被认为是深度的，它必须具有超过10个隐藏层。今天许多最先进的前馈神经网络已经有了超过40层。现在，让我们来探索一下深度前馈神经网络的一些特性，并了解它们为何如此强大。
- en: If you recall, earlier on we came across the universal approximation theorem,
    which stated that an MLP with a single hidden layer could approximate any function.
    But if that is the case, why do we need deep neural networks? Simply put, the
    capacity of a neural network increases with each hidden layer (and the brain has
    a deep structure). What this means is that deeper networks have far greater expressiveness
    than shallower networks. This is something we came across earlier when learning
    about MLPs. We saw that by adding hidden layers, we were able to create a network
    that was able to learn to solve a problem that a linear neural network was not
    able to.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，之前我们遇到过普适逼近定理，它指出一个只有单一隐藏层的MLP能够逼近任何函数。但如果是这样，那为什么我们还需要深度神经网络呢？简单来说，神经网络的容量随着每增加一个隐藏层而增加（而大脑本身具有深度结构）。这意味着，深度网络相比浅层网络具有更强的表现力。这一点我们之前在学习MLP时就有所接触。当时我们看到，通过增加隐藏层，我们能够创建一个能够解决线性神经网络无法解决的问题的网络。
- en: Additionally, deeper networks are preferred over wider networks, not because
    they improve the overall performance, but because networks with more hidden layers
    (but less width) have much fewer parameters than wider networks with fewer hidden
    layers.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深型网络比宽型网络更受偏爱，并非因为它们提高了整体性能，而是因为具有更多隐藏层（但宽度较小）的网络比具有较少隐藏层但宽度更大的网络拥有更少的参数。
- en: Let's suppose we have two networks—one that is wide and one that is deep. Both
    networks have 20 inputs and 6 output nodes. Let's calculate the total number of
    parameters for both layers; that is, the number of connections between all the
    layers and biases.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个网络——一个是宽型的，一个是深型的。这两个网络都有20个输入和6个输出节点。让我们计算这两个网络的总参数数量，也就是所有层之间的连接数和偏置。
- en: 'Our wide neural network has two hidden layers, each with 1,024 neurons. The
    total number of parameters is as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的宽型神经网络有两个隐藏层，每个隐藏层包含1,024个神经元。总的参数数量如下：
- en: '![](img/dee8c6bc-e85c-4a0b-a207-5b43642263dc.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dee8c6bc-e85c-4a0b-a207-5b43642263dc.png)'
- en: 'Our deep neural network has 12 hidden layers, each with 150 neurons. The total
    number of parameters is as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的深度神经网络有12个隐藏层，每个隐藏层包含150个神经元。总的参数数量如下：
- en: '![](img/fcca0a2a-0630-4eda-9e99-502be89e3da5.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fcca0a2a-0630-4eda-9e99-502be89e3da5.png)'
- en: As you can see, the deeper network has less than half the parameters that the
    wider network does.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，深型网络的参数比宽型网络少了一半。
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we first learned about a simple FNN, known as the MLP, and
    broke it down into its individual components to get a deeper understanding of
    how they work and are constructed. We then extended these concepts to further
    our understanding of deep neural networks. You should now have intimate knowledge
    of how FNNs work and understand how various models are constructed, as well as
    understand how to build and possibly improve them for yourself.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们首先学习了一个简单的前馈神经网络（FNN），即多层感知机（MLP），并将其拆解为各个组成部分，以更深入地理解它们是如何工作的以及如何构建的。接着，我们将这些概念扩展到更深的神经网络，进一步加深了对它们的理解。现在，你应该已经对前馈神经网络的工作原理有了深刻的理解，并且明白了各种模型是如何构建的，以及如何为自己构建并可能改进它们。
- en: Let's now move on to the next chapter, where we will learn how to improve our
    neural networks so that they generalize better on unseen data.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们进入下一章，学习如何改进我们的神经网络，使其在未见数据上能够更好地泛化。
