- en: Curse of High-Dimensionality in Big Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据中的高维度诅咒
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Two methods of ingesting and preparing a CSV file for processing in Spark
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark中摄取和准备CSV文件进行处理的两种方法
- en: '**Singular Value Decomposition** (**SVD**) to reduce high-dimensionality in
    Spark'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）以减少Spark中的高维度'
- en: '**Principal Component Analysis** (**PCA**) to pick the most effective latent
    factor for machine learning in Spark'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）在Spark中为机器学习选择最有效的潜在因素'
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The curse of dimensionality is not a new term or concept. The term was originally
    coined by R. Bellman when tackling problems in dynamic programming (the Bellman
    equation). The core concepts in machine learning refer to the problem that as
    we increase the number of dimensions (axes or features), the number of training
    data (samples) remains the same (or relatively low), which causes less accuracy
    in our predictions. This phenomenon is also referred to as the *Hughes Effect*,
    named after G. Hughes, which talks about the problem caused by rapid (exponential)
    increase of search space as we introduce more and more dimensions to the problem
    space. It is a bit counterintuitive, but if the number of samples does not expand
    at the same rate as you add more dimensions, you actually end up with a less accurate
    model!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 维度诅咒并不是一个新的术语或概念。这个术语最初是由R.贝尔曼在解决动态规划问题（贝尔曼方程）时创造的。机器学习中的核心概念指的是，随着我们增加维度（轴或特征）的数量，训练数据（样本）的数量保持不变（或相对较低），这导致我们的预测准确性降低。这种现象也被称为*休斯效应*，以G.休斯的名字命名，它讨论了随着我们在问题空间中引入越来越多的维度，搜索空间的迅速（指数级）增加所导致的问题。这有点违直觉，但如果样本数量的增长速度不如添加更多维度的速度，实际上你最终会得到一个更不准确的模型！
- en: In a nutshell, most machine learning algorithms are statistical by nature and
    they attempt to learn the properties of the target space by cutting up the space
    during training and by doing some sort of counting for the number of each classes
    in each subspace. The curse of dimensionality is caused by having fewer and fewer
    data samples, which can help the algorithm to discriminate and learn as we add
    more dimensions. Generally speaking, if we have *N* samples in a dense *D* dimension,
    then we need *(N)^D* samples to keep the sample density constant.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，大多数机器学习算法本质上是统计学的，它们试图通过在训练期间切割空间并对每个子空间中每个类别的数量进行某种计数来学习目标空间的属性。维度诅咒是由于随着维度的增加，能够帮助算法区分和学习的数据样本变得越来越少。一般来说，如果我们在一个密集的*D*维度中有*N*个样本，那么我们需要*(N)^D*个样本来保持样本密度恒定。
- en: For example, let us say that you have 10 patient datasets that are measured
    along two dimensions (height, weight). This results in 10 data points in a two-dimensional
    plane. What happens if we start introducing other dimensions such as region, calorie
    intake, ethnicity, income, and so on? In this case, we still have 10 observation
    points (10 patients) but in a much larger space of six dimensions. This inability
    for the sample data (needed for training) to expand exponentially as new dimensions
    are introduced is called the **Curse of Dimensionality**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有10个患者数据集，这些数据集是沿着两个维度（身高、体重）进行测量的。这导致了一个二维平面上的10个数据点。如果我们开始引入其他维度，比如地区、卡路里摄入量、种族、收入等，会发生什么？在这种情况下，我们仍然有10个观察点（10个患者），但是在一个更大的六维空间中。当新的维度被引入时，样本数据（用于训练）无法呈指数级增长，这就是所谓的**维度诅咒**。
- en: 'Let''s look at a graphical example to show the growth of the search space versus
    data samples. The following figure depicts a set of five data points that are
    being measured in 5 x 5 (25 cells). What happens to the prediction accuracy as
    we add another dimension? We still have five data points in 125 3D-cells, which
    results in a lot of sparse subspace that cannot help the ML algorithm to learn
    better (discriminate) so it results in less accuracy:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个图形示例来展示搜索空间与数据样本的增长。下图描述了一组五个数据点，这些数据点在5 x 5（25个单元格）中被测量。当我们添加另一个维度时，预测准确性会发生什么变化？我们仍然有五个数据点在125个3D单元格中，这导致了大量稀疏子空间，这些子空间无法帮助机器学习算法更好地学习（区分），因此导致了更低的准确性：
- en: '![](img/00254.jpeg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00254.jpeg)'
- en: Our goal should be to strive toward a near-optimal number of features or dimensions
    rather than adding more and more features (maximum features or dimensions). After
    all, shouldn't we have a better classification error if we just add more and more
    features or dimensions? It seems like a good idea at first, but the answer in
    most cases is "no" unless you can increase the samples exponentially, which is
    neither practical nor possible in almost all cases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标应该是努力朝着一个接近最佳特征或维度的数量，而不是不断添加更多特征（最大特征或维度）。毕竟，如果我们只是不断添加更多特征或维度，难道我们不应该有更好的分类错误吗？起初这似乎是个好主意，但在大多数情况下答案是“不”，除非你能指数级增加样本，而这在几乎所有情况下都是不切实际的也几乎不可能的。
- en: 'Let us take a look at the following figure, which depicts learning error versus
    total number of features:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下图，它描述了学习错误与特征总数的关系：
- en: '![](img/00255.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00255.jpeg)'
- en: In the previous section, we examined the core concept beyond the curse of dimensionality,
    but we have not talked about its other side effects or how to deal with the curse
    itself. As we have seen previously, contrary to popular belief, it is not the
    dimensions themselves, but the reduction of the ratio of samples to search space
    which subsequently results in a less accurate forecast.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们研究了维度诅咒背后的核心概念，但我们还没有讨论它的其他副作用或如何处理诅咒本身。正如我们之前所看到的，与普遍观念相反，问题不在于维度本身，而在于样本与搜索空间的比率的减少，随之而来的是更不准确的预测。
- en: 'Imagine a simple ML system, as shown in the following figure. The ML system
    shown here takes the MNIST ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/))
    type handwriting dataset and wants to train itself so it can predict what six-digit
    zip code is used on a parcel:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个简单的ML系统，如下图所示。这里显示的ML系统使用MNIST（[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)）类型的手写数据集，并希望对自己进行训练，以便能够预测包裹上使用的六位邮政编码是什么：
- en: '![](img/00256.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00256.jpeg)'
- en: 'Source: MNIST'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：MNIST
- en: 'Even though the MNIST data is 20 x 20, to make the problem more visible let''s
    assume we have a 40 x 40 pixel patch for each digit that has to be stored, analyzed,
    and then used for future prediction. If we assume black/white, then the *apparent*
    dimensionality is two (40 x 40) or 21,600, which is large. The next question that
    should be asked is: given the 21,600 apparent dimensions for the data, what is
    the actual dimension that we need to do our work? If we look at all the possible
    samples drawn from a 40 x 40 patch, how many of them actually look for digits?
    Once we look at the problem a bit more carefully, we will see that the "actual"
    dimensions (that is, limited to a smaller manifold subspace which is the space
    used by a pen stroke to make the digits. In practice, the actual subspace is much
    smaller and not randomly distributed across the 40 x 40 patch) are actually a
    lot smaller! What is happening here is that the actual data (the digits drawn
    by humans) exists in much smaller dimensions and most likely is confined to a
    small set of manifolds in the subspace (that is, the data lives around a certain
    subspace). To understand this better, draw 1,000 random samples from a 40 x 40
    patch and visually inspect the samples. How many of them actually look alike a
    3, 6, or a 5?'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 即使MNIST数据是20 x 20，为了使问题更加明显，让我们假设每个数字有一个40 x 40像素的补丁需要存储、分析，然后用于未来的预测。如果我们假设是黑/白，那么“表观”维度是两个（40
    x 40）或21,600，这是很大的。接下来应该问的问题是：给定数据的21,600个表观维度，我们需要多少实际维度来完成我们的工作？如果我们看一下从40 x
    40补丁中抽取的所有可能样本，有多少实际上是在寻找数字？一旦我们仔细看一下这个问题，我们会发现“实际”维度（即限制在一个较小的流形子空间中，这是笔画用来制作数字的空间。实际上，实际子空间要小得多，而且不是随机分布在40
    x 40的补丁上）实际上要小得多！这里发生的情况是，实际数据（人类绘制的数字）存在于更小的维度中，很可能局限于子空间中的一小组流形（即，数据存在于某个子空间周围）。为了更好地理解这一点，从40
    x 40的补丁中随机抽取1,000个样本，并直观地检查这些样本。有多少样本实际上看起来像3、6或5？
- en: 'When we add dimensions, we can unintentionally increase the error rate by introducing
    noise to the system due to the fact that there would not be enough samples to
    predict accurately or simply act if the measurement introduces noise by itself.
    Common problems with adding more dimensions are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们增加维度时，我们可能会无意中增加错误率，因为由于没有足够的样本来准确预测，或者由于测量本身引入了噪声，系统可能会引入噪声。增加更多维度的常见问题如下：
- en: Longer compute time
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更长的计算时间
- en: Increased noise
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加噪声
- en: More samples needed to keep the same learning/prediction rate
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要更多样本以保持相同的学习/预测速率
- en: Overfitting of the data due to lack of actionable samples in sparse space
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于稀疏空间中缺乏可操作样本而导致数据过拟合
- en: 'A pictorial presentation can help us understand the difference between *apparent
    dimensions* versus *actual dimensions* and why *less is more* in this case:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图片展示可以帮助我们理解“表观维度”与“实际维度”的差异，以及在这种情况下“少即是多”的原因：
- en: '![](img/00257.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00257.jpeg)'
- en: 'The reasons we want to reduce dimensions can be expressed as the ability to:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望减少维度的原因可以表达为：
- en: Visualize data better
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好地可视化数据
- en: Compress the data and reduce storage requirements
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩数据并减少存储需求
- en: Increase signal to noise ratio
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加信噪比
- en: Achieve faster running time
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现更快的运行时间
- en: Feature selection versus feature extraction
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择与特征提取
- en: We have two options, feature selection and feature extraction, at our disposal
    for reducing the dimensions to a more manageable space. Each of these techniques
    is a distinct discipline and has its own methods and complexity. Even though they
    sound the same, they are very different and require a separate treatment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个选择，特征选择和特征提取，可以用来将维度减少到一个更易管理的空间。这些技术各自是一个独立的学科，有自己的方法和复杂性。尽管它们听起来相同，但它们是非常不同的，需要单独处理。
- en: 'The following figure provides a mind map, which compares the feature selection
    versus feature extraction for reference. While the feature selection, also referred
    to as feature engineering, is beyond the scope of this book, we cover the two
    most common feature extraction techniques (PCA and SVD) via detailed recipes:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下图提供了一个思维导图，比较了特征选择和特征提取。虽然特征选择，也称为特征工程，超出了本书的范围，但我们通过详细的配方介绍了两种最常见的特征提取技术（PCA和SVD）：
- en: '![](img/00258.gif)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00258.gif)'
- en: 'The two techniques available for picking a set of features or inputs to a ML
    algorithm are:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 用于选择ML算法的一组特征或输入的两种可用技术是：
- en: '**Feature selection**: In this technique, we use our domain knowledge to select
    a subset of features that best describes the variance in the data. What we are
    trying to do is to select the best dependent variables (features) that can help
    us predict the outcome. This method is often referred to as "feature engineering"
    and requires a data engineer or domain expertise to be effective.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：在这种技术中，我们利用我们的领域知识选择最能描述数据方差的特征子集。我们试图做的是选择最能帮助我们预测结果的最佳因变量（特征）。这种方法通常被称为“特征工程”，需要数据工程师或领域专业知识才能有效。'
- en: For example, we might look at 200 independent variables (dimensions, features)
    that are proposed for a logistics classifier to predict whether a house would
    sell or not in the city of Chicago. After talking to real-estate experts with
    20+ years of experience in buying/selling houses in the Chicago market, we found
    out that only 4 of the 200 initially proposed dimensions, such as number of bedrooms,
    price, total square foot area, and quality of schools, are adequate for predictions.
    While this is great, it is usually very expensive, time-consuming, and requires
    a domain expert to analyze and provide direction.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能会查看为物流分类器提出的200个独立变量（维度、特征），以预测芝加哥市的房屋是否会出售。在与在芝加哥市购买/销售房屋有20多年经验的房地产专家交谈后，我们发现最初提出的200个维度中只有4个是足够的，例如卧室数量、价格、总平方英尺面积和学校质量。虽然这很好，但通常非常昂贵、耗时，并且需要领域专家来分析和提供指导。
- en: '**Feature extraction**: This refers to a more algorithmic approach that uses
    a mapping function to map high-dimensional data to a lower-dimensional space.
    For example, mapping a three-dimensional space (for example, height, weight, eye
    color) to a one-dimensional space (for example, latent factors) that can capture
    almost all variances in the dataset.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：这是指一种更算法化的方法，使用映射函数将高维数据映射到低维空间。例如，将三维空间（例如，身高、体重、眼睛颜色）映射到一维空间（例如，潜在因素），可以捕捉数据集中几乎所有的变化。'
- en: What we are trying to do here is to come up with a set of latent factors that
    are a combination (usually linear) of the original factor but can capture and
    explain the data in an accurate way. For example, we use words to describe documents
    which usually end in 10⁶ to 10⁹ space, but wouldn't it be nice to describe the
    documents by topics (for example, romance, war, peace, science, art, and so on)
    that are more abstract and high level? Do we really need to look at or include
    every word to do a better job with text analytics? At what cost?
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里尝试的是提出一组潜在因素，这些因素是原始因素的组合（通常是线性的），可以以准确的方式捕捉和解释数据。例如，我们使用单词来描述文档，通常以10⁶到10⁹的空间结束，但是用主题（例如，浪漫、战争、和平、科学、艺术等）来描述文档会更抽象和高层次，这不是很好吗？我们真的需要查看或包含每个单词来更好地进行文本分析吗？以什么代价？
- en: Feature extraction is about an algorithmic approach to dimensionality reduction
    which itself is a proxy for mapping from "apparent dimensionality" to "actual
    dimensionality".
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取是一种从“表观维度”到“实际维度”映射的降维算法方法。
- en: Two methods of ingesting and preparing a CSV file for processing in Spark
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两种在Spark中摄取和准备CSV文件进行处理的方法
- en: In this recipe, we explore reading, parsing, and preparing a CSV file for a
    typical ML program. A **comma-separated values** (**CSV**) file normally stores
    tabular data (numbers and text) in a plain text file. In a typical CSV file, each
    row is a data record, and most of the time, the first row is also called the header
    row, which stores the field's identifier (more commonly referred to as a column
    name for the field). Each record consists of one or more fields, separated by
    commas.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们探讨了读取、解析和准备CSV文件用于典型的ML程序。**逗号分隔值**（**CSV**）文件通常将表格数据（数字和文本）存储在纯文本文件中。在典型的CSV文件中，每一行都是一个数据记录，大多数情况下，第一行也被称为标题行，其中存储了字段的标识符（更常见的是字段的列名）。每个记录由一个或多个字段组成，字段之间用逗号分隔。
- en: How to do it...
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: The sample CSV data file is from movie ratings. The file can be retrieved at [http://files.grouplens.org/datasets/movielens/ml-latest-small.zip](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip).
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 示例CSV数据文件来自电影评分。该文件可在[http://files.grouplens.org/datasets/movielens/ml-latest-small.zip](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip)中获取。
- en: 'Once the file is extracted, we will use the `ratings.csv` file for our CSV
    program to load the data into Spark. The CSV files will look like the following:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件提取后，我们将使用`ratings.csv`文件来加载数据到Spark中。CSV文件将如下所示：
- en: '| **userId** | **movieId** | **rating** | **timestamp** |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **userId** | **movieId** | **rating** | **timestamp** |'
- en: '| 1 | 16 | 4 | 1217897793 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 16 | 4 | 1217897793 |'
- en: '| 1 | 24 | 1.5 | 1217895807 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 24 | 1.5 | 1217895807 |'
- en: '| 1 | 32 | 4 | 1217896246 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 32 | 4 | 1217896246 |'
- en: '| 1 | 47 | 4 | 1217896556 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 47 | 4 | 1217896556 |'
- en: '| 1 | 50 | 4 | 1217896523 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 50 | 4 | 1217896523 |'
- en: '| 1 | 110 | 4 | 1217896150 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 110 | 4 | 1217896150 |'
- en: '| 1 | 150 | 3 | 1217895940 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 150 | 3 | 1217895940 |'
- en: '| 1 | 161 | 4 | 1217897864 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 161 | 4 | 1217897864 |'
- en: '| 1 | 165 | 3 | 1217897135 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 165 | 3 | 1217897135 |'
- en: '| 1 | 204 | 0.5 | 1217895786 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 204 | 0.5 | 1217895786 |'
- en: '| ... | ... | ... | ... |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... |'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '`package spark.ml.cookbook.chapter11`.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`package spark.ml.cookbook.chapter11`。'
- en: 'Import the necessary packages for Spark to get access to the cluster and `Log4j.Logger`
    to reduce the amount of output produced by Spark:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark所需的包，以便访问集群和`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create Spark''s configuration and Spark session so we can have access to the
    cluster:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和Spark会话，以便我们可以访问集群：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We read in the CSV files as a text file:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将CSV文件读入为文本文件：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We process the dataset:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们处理数据集：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It should be mentioned that the `split` function here is for demonstration purposes
    only and a more robust tokenizer technique should be used in production.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里应该提到，`split`函数仅用于演示目的，生产中应该使用更健壮的标记技术。
- en: First, we trim the line, remove any empty spaces, and load the CSV file into
    the `headerAndData` RDD since `ratings.csv` does have a header row.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们修剪行，删除任何空格，并将CSV文件加载到`headerAndData` RDD中，因为`ratings.csv`确实有标题行。
- en: We then read the first row as the header, and read the rest of the data into
    the data RDD. Any further computing could use the data RDD to perform the machine
    learning algorithm. For demo purposes, we mapped the header row to the data RDD
    and printed out the first 10 rows.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将第一行读取为标题，将其余数据读入数据RDD中。任何进一步的计算都可以使用数据RDD来执行机器学习算法。为了演示目的，我们将标题行映射到数据RDD并打印出前10行。
- en: 'In the application console, you will see the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序控制台中，您将看到以下内容：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: There is also another option to load the CSV file into Spark with the help of
    the Spark-CSV package.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还有另一种选项可以使用Spark-CSV包将CSV文件加载到Spark中。
- en: 'To utilize this feature, you will need to download the following JAR file and
    place them on the classpath: [http://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.4.0/spark-csv_2.10-1.4.0.jar](http://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.4.0/spark-csv_2.10-1.4.0.jar)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此功能，您需要下载以下JAR文件并将它们放在类路径上：[http://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.4.0/spark-csv_2.10-1.4.0.jar](http://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.4.0/spark-csv_2.10-1.4.0.jar)
- en: Since the Spark-CSV package is also dependent on `common-csv`, you will need
    to get the `common-csv` JAR file from the following location: [https://commons.apache.org/proper/commons-csv/download_csv.cgi](https://commons.apache.org/proper/commons-csv/download_csv.cgi)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark-CSV包也依赖于`common-csv`，您需要从以下位置获取`common-csv` JAR文件：[https://commons.apache.org/proper/commons-csv/download_csv.cgi](https://commons.apache.org/proper/commons-csv/download_csv.cgi)
- en: We get the `common-csv-1.4-bin.zip` and extract the `commons-csv-1.4.jar` out,
    and put the preceding two jars on the classpath.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取`common-csv-1.4-bin.zip`并提取`commons-csv-1.4.jar`，然后将前两个jar放在类路径上。
- en: 'We load the CSV file using the Databricks `spark-csv` package with the following
    code. It will create a DataFrame object after successfully loading the CSV file:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用Databricks的`spark-csv`包加载CSV文件，使用以下代码。成功加载CSV文件后，它将创建一个DataFrame对象：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We register a temp in-memory view named `ratings` from the DataFrame:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从DataFrame中注册一个名为`ratings`的临时内存视图：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then use a SQL query against the table and display 10 rows. In the console,
    you will see the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对表使用SQL查询并显示10行。在控制台上，您将看到以下内容：
- en: '![](img/00259.gif)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00259.gif)'
- en: Further machine learning algorithms could be performed on the DataFrame that
    was created previously.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步的机器学习算法可以在之前创建的DataFrame上执行。
- en: 'We then close the program by stopping the Spark session:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过停止Spark会话来关闭程序：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works...
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'In the older version of Spark, we needed to use a special package to read in
    CSV, but we now can take advantage of `spark.sparkContext.textFile(dataFile)`
    to ingest the file. The `Spark` which starts the statement is the Spark session
    (handle to cluster) and can be named anything you like via the creation phase,
    as shown here:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在旧版本的Spark中，我们需要使用特殊包来读取CSV，但现在我们可以利用`spark.sparkContext.textFile(dataFile)`来摄取文件。开始该语句的`Spark`是Spark会话（集群句柄），可以在创建阶段通过任何您喜欢的名称来命名，如下所示：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Spark 2.0+ uses `spark.sql.warehouse.dir` to set the warehouse location to store
    tables rather than `hive.metastore.warehouse.dir`. The default value for `spark.sql.warehouse.dir`
    is `System.getProperty("user.dir")`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0+使用`spark.sql.warehouse.dir`来设置存储表的仓库位置，而不是`hive.metastore.warehouse.dir`。`spark.sql.warehouse.dir`的默认值是`System.getProperty("user.dir")`。
- en: Also see `spark-defaults.conf` for more details.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另请参阅`spark-defaults.conf`以获取更多详细信息。
- en: 'Going forward, we prefer this method as opposed to obtaining the special package
    and the dependent JAR, as explained in step 9 of this recipe, followed by step
    10:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在以后的工作中，我们更喜欢这种方法，而不是按照本示例的第9步和第10步所解释的获取特殊包和依赖JAR的方法：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This demonstrates how to consume the file.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这演示了如何使用文件。
- en: There's more...
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The CSV file format has a lot of variations. The basic idea of separating fields
    with a comma is clear, but it could also be a tab, or other special character.
    Sometimes even the header row is optional.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: CSV文件格式有很多变化。用逗号分隔字段的基本思想是清晰的，但它也可以是制表符或其他特殊字符。有时甚至标题行是可选的。
- en: A CSV file is widely used to store raw data due to its portability and simplicity.
    It's portable across different applications. We will introduce two simple and
    typical ways to load a sample CSV file into Spark, and it can be easily modified
    to fit your use case.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其可移植性和简单性，CSV文件广泛用于存储原始数据。它可以在不同的应用程序之间进行移植。我们将介绍两种简单而典型的方法来将样本CSV文件加载到Spark中，并且可以很容易地修改以适应您的用例。
- en: See also
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: For more information regarding the Spark-CSV package, visit [https://github.com/databricks/spark-csv](https://github.com/databricks/spark-csv)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关Spark-CSV包的更多信息，请访问[https://github.com/databricks/spark-csv](https://github.com/databricks/spark-csv)
- en: Singular Value Decomposition (SVD) to reduce high-dimensionality in Spark
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Singular Value Decomposition（SVD）在Spark中降低高维度
- en: In this recipe, we will explore a dimensionality reduction method straight out
    of the linear algebra, which is called **SVD** (**Singular Value Decomposition**).
    The key focus here is to come up with a set of low-rank matrices (typically three)
    that approximates the original matrix but with much less data, rather than choosing
    to work with a large *M* by *N* matrix.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将探讨一种直接来自线性代数的降维方法，称为**SVD**（**奇异值分解**）。这里的重点是提出一组低秩矩阵（通常是三个），它们可以近似原始矩阵，但数据量要少得多，而不是选择使用大型*M*乘以*N*矩阵。
- en: SVD is a simple linear algebra technique that transforms the original data to
    eigenvector/eigenvalue low rank matrices that can capture most of the attributes
    (the original dimensions) in a much more efficient low rank matrix system.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: SVD是一种简单的线性代数技术，它将原始数据转换为特征向量/特征值低秩矩阵，可以捕捉大部分属性（原始维度）在一个更有效的低秩矩阵系统中。
- en: 'The following figure depicts how SVD can be used to reduce dimensions and then
    use the S matrix to keep or eliminate higher-level concepts derived from the original
    data (that is, a low rank matrix with fewer columns/features than the original):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示了SVD如何用于降低维度，然后使用S矩阵来保留或消除从原始数据派生的更高级概念（即，具有比原始数据更少列/特征的低秩矩阵）：
- en: '![](img/00260.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00260.jpeg)'
- en: How to do it...
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: We will use the movie rating data for the SVD analysis. The movieLens 1M dataset
    contains around 1 million records which consist of anonymous ratings of around
    3,900 movies made by 6,000 movieLens users.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用电影评分数据进行SVD分析。MovieLens 1M数据集包含大约100万条记录，由6000个MovieLens用户对约3900部电影的匿名评分组成。
- en: 'The dataset can be retrieved at: [http://files.grouplens.org/datasets/movielens/ml-1m.zip](http://files.grouplens.org/datasets/movielens/ml-1m.zip)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以在以下位置检索：[http://files.grouplens.org/datasets/movielens/ml-1m.zip](http://files.grouplens.org/datasets/movielens/ml-1m.zip)
- en: 'The dataset contains the following files:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含以下文件：
- en: '`ratings.dat`: Contains the user ID, movie ID, ratings, and timestamp'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ratings.dat`：包含用户ID、电影ID、评分和时间戳'
- en: '`movies.dat`: Contains the movie ID, titles, and genres'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movies.dat`：包含电影ID、标题和类型'
- en: '`users.dat`: Contains the user ID, genders, ages, occupations, and zip code'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`users.dat`：包含用户ID、性别、年龄、职业和邮政编码'
- en: 'We will use the `ratings.dat` for our SVD analysis. Sample data for the `ratings.dat`
    looks like the following:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`ratings.dat`进行SVD分析。`ratings.dat`的样本数据如下：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will use the following program to convert the data into a ratings matrix
    and fit it into the SVD algorithm model (in this case, we have 3,953 columns in
    total):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下程序将数据转换为评分矩阵，并将其适应SVD算法模型（在本例中，总共有3953列）：
- en: '|  | **Movie 1** | **Movie 2** | **Movie ...** | **Movie 3953** |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | **电影1** | **电影2** | **电影...** | **电影3953** |'
- en: '| user 1 | 1 | 4 | - | 3 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 用户1 | 1 | 4 | - | 3 |'
- en: '| user 2 | 5 | - | 2 | 1 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 用户2 | 5 | - | 2 | 1 |'
- en: '| user ... | - | 3 | - | 2 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 用户... | - | 3 | - | 2 |'
- en: '| user N | 2 | 4 | - | 5 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 用户N | 2 | 4 | - | 5 |'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '`package spark.ml.cookbook.chapter11`.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`package spark.ml.cookbook.chapter11`。'
- en: 'Import the necessary packages for the Spark session:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark会话所需的包：
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Create Spark''s configuration and Spark session so we can have access to the
    cluster:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和Spark会话，以便我们可以访问集群：
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We read in the original raw data file:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '我们读取原始的原始数据文件： '
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We preprocess the dataset:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们预处理数据集：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Since we are more interested in the ratings, we extract the `userId`, `movieId`,
    and rating values from the data file, `fields(0)`, `fields(1)`, and `fields(2)`,
    and create a ratings RDD based on the records.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对评分更感兴趣，我们从数据文件中提取`userId`，`movieId`和评分值，即`fields(0)`，`fields(1)`和`fields(2)`，并基于记录创建一个评分RDD。
- en: 'We then find out how many movies are available in the ratings data and calculate
    the max movie index:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们找出评分数据中有多少部电影，并计算最大电影索引：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In total, we get 3,953 movies based on the dataset.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 总共，我们根据数据集得到3953部电影。
- en: 'We put all the user movie item ratings together, using RDD''s `groupByKey`
    function, so a single user''s movie ratings are grouped together:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将所有用户的电影项目评分放在一起，使用RDD的`groupByKey`函数，所以单个用户的电影评分被分组在一起：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We then print out the top two records to see the collection. Since we might
    have a large dataset, we cache RDD to improve performance.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们打印出前两条记录以查看集合。由于我们可能有一个大型数据集，我们缓存RDD以提高性能。
- en: 'In the console, you will see the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中，您将看到以下内容：
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the preceding records, the user ID is `4904`. For the movie ID `2054`, the
    rating is `4.0`, movie ID is `588`, rating is `4`, and so on.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述记录中，用户ID为`4904`。对于电影ID`2054`，评分为`4.0`，电影ID为`588`，评分为`4`，依此类推。
- en: 'We then create a sparse vector to host the data:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们创建一个稀疏向量来存储数据：
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We then convert the data into a more useful format. We use the `userID` as the
    key (sorted), and create a sparse vector to host the movie rating data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将数据转换为更有用的格式。我们使用`userID`作为键（排序），并创建一个稀疏向量来存储电影评分数据。
- en: 'In the console, you will see the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中，您将看到以下内容：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding printout, for user `1`, in total, there are `3,953` movies.
    For movie ID `1`, the rating is `5.0`. The sparse vector contains a `movieID`
    array together with a rating value array.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述打印输出中，对于用户`1`，总共有`3,953`部电影。对于电影ID`1`，评分为`5.0`。稀疏向量包含一个`movieID`数组和一个评分值数组。
- en: 'We just need the rating matrix for our SVD analysis:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需要评分矩阵进行SVD分析：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The preceding code will get the sparse vector part out and create a row RDD.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将提取稀疏向量部分并创建一个行RDD。
- en: 'We then create a RowMatrix based on the RDD. Once the RowMatrix object is created,
    we can call Spark''s `computeSVD` function to compute the SVD out of the matrix:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们基于RDD创建一个RowMatrix。一旦创建了RowMatrix对象，我们就可以调用Spark的`computeSVD`函数来计算矩阵的SVD：
- en: '[PRE21]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The preceding parameters could also be adjusted to fit our needs. Once we have
    the SVD computed, we can get the model data out.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述参数也可以调整以适应我们的需求。一旦我们计算出SVD，就可以获取模型数据。
- en: 'We print the singular values out:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印出奇异值：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You will see the following output on the console:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在控制台上看到以下输出：
- en: '![](img/00261.gif)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00261.gif)'
- en: 'From the Spark Master (`http://localhost:4040/jobs/`), you should see the tracking
    as shown in the following screenshot:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Spark Master（`http://localhost:4040/jobs/`）中，您应该看到如下截图所示的跟踪：
- en: '![](img/00262.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00262.jpeg)'
- en: 'We then close the program by stopping the Spark session:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过停止Spark会话来关闭程序：
- en: '[PRE23]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: How it works...
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The core of the work gets done by declaring a `RowMatrix()` and then invoking
    the `computeSVD()` method to decompose the matrix into subcomponents that are
    much smaller, but approximate the original with uncanny accuracy:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 工作的核心是声明一个`RowMatrix()`，然后调用`computeSVD()`方法将矩阵分解为更小的子组件，但以惊人的准确度近似原始矩阵：
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'SVD is a factorization technique for a real or complex matrix. At its core,
    it is a straight linear algebra which was actually derived from PCA itself. The
    concept is used extensively in recommender systems (ALS, SVD), topic modeling
    (LDA), and text analytics, to derive concepts from primitive high-dimensional
    matrices. Let''s try to outline this without getting into the mathematical details
    of what goes in and what comes out in an SVD decomposition. The following figure
    depicts how this dimensionality reduction recipe and its dataset (`MovieLens`)
    relate to an SVD decomposition:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: SVD是一个用于实数或复数矩阵的因式分解技术。在其核心，它是一种直接的线性代数，实际上是从PCA中导出的。这个概念在推荐系统（ALS，SVD），主题建模（LDA）和文本分析中被广泛使用，以从原始的高维矩阵中推导出概念。让我们尝试概述这个降维的方案及其数据集（`MovieLens`）与SVD分解的关系，而不深入讨论SVD分解中的数学细节。以下图表描述了这个降维方案及其数据集（`MovieLens`）与SVD分解的关系：
- en: '![](img/00263.jpeg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00263.jpeg)'
- en: There's more...
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We will end up with much more efficient (low-ranked) matrices for computation
    based on the original dataset.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到基于原始数据集的更高效（低秩）的矩阵。
- en: The following equation depicts the decomposition of an array of *m x n*, which
    is large and hard to work with. The right-hand side of the equation helps to solve
    the decomposition problem which is the basis of the SVD technique.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程描述了一个*m x n*数组的分解，这个数组很大，很难处理。方程的右边帮助解决了分解问题，这是SVD技术的基础。
- en: '![](img/00264.jpeg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00264.jpeg)'
- en: 'The following steps provides a concrete example of the SVD decomposition step
    by step:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤逐步提供了SVD分解的具体示例：
- en: Consider a matrix of 1,000 x 1,000 which provides 1,000,000 data points (M=
    users, N = Movies).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑一个1,000 x 1,000的矩阵，提供1,000,000个数据点（M=用户，N=电影）。
- en: Assume there are 1,000 rows (number of observations) and 1,000 columns (number
    of movies).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设有1,000行（观测数量）和1,000列（电影数量）。
- en: Let's assume we use Spark's SVD method to decompose A into three new matrices.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们使用Spark的SVD方法将A分解为三个新矩阵。
- en: Matrix `U [m x r]` has 1,000 rows, but only 5 columns now (`r=5`; `r` can be
    thought of as concepts)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵`U [m x r]`有1,000行，但现在只有5列（`r=5`；`r`可以被看作是概念）
- en: Matrix `S [ r x r ]` holds the singular values, which are the strength of each
    concept (only interested in diagonals)
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵`S [r x r]`保存了奇异值，它们是每个概念的强度（只对对角线感兴趣）
- en: Matrix `V [n x r ]` has the right singular value vectors (`n= Movies`, `r =
    concepts`, such as romance, sci-fi, and so on)
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵`V [n x r]`具有右奇异值向量（`n=电影`，`r=概念`，如浪漫，科幻等）
- en: Let's assume that after decomposition, we end up with five concepts (romantic,
    sci-fi-drama, foreign, documentary, and adventure)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设在分解后，我们得到了五个概念（浪漫，科幻剧，外国，纪录片和冒险）
- en: How did the low rank help?
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低秩如何帮助？
- en: Originally we had 1,000,000 points of interest
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最初我们有1,000,000个兴趣点
- en: After SVD and even before we started selecting what we want to keep using singular
    values (diagonals of matrix S), we ended up with total points of interest = membership
    in U (1,000 x 5) + S (5 x 5) + V(1,000 x 5)
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在SVD之后，甚至在我们开始使用奇异值（矩阵S的对角线）选择我们想要保留的内容之前，我们得到了总的兴趣点数= U（1,000 x 5）+ S（5 x 5）+
    V（1,000 x 5）
- en: Rather than working with 1 million data points (matrix A, which is 1,000 x 1,000),
    we now have 5,000+25+5,000, which is about 10,000 data points, which is considerably
    less
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们不再使用1百万个数据点（矩阵A，即1,000 x 1,000），而是有了5,000+25+5,000，大约有10,000个数据点，这要少得多
- en: The act of selecting singular values allows us to decide how much we want to
    keep and how much do we want to throw away (do you really want to show the user
    the lowest 900 movie recommendations--does it have any value?)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择奇异值的行为允许我们决定我们想要保留多少，以及我们想要丢弃多少（你真的想向用户展示最低的900部电影推荐吗？这有价值吗？）
- en: See also
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: Documentation for RowMatrix can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix)
    and [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition)
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RowMatrix的文档可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix)和[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition)找到
- en: Principal Component Analysis (PCA) to pick the most effective latent factor
    for machine learning in Spark
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）用于在Spark中为机器学习选择最有效的潜在因子
- en: In this recipe, we use **PCA** (**Principal Component Analysis**) to map the
    higher-dimension data (the apparent dimensions) to a lower-dimensional space (actual
    dimensions). It is hard to believe, but PCA has its root as early as 1901(see
    K. Pearson's writings) and again independently in the 1930s by H. Hotelling.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们使用**PCA**（主成分分析）将高维数据（表面维度）映射到低维空间（实际维度）。难以置信，但PCA的根源早在1901年（参见K. Pearson的著作）和1930年代由H.
    Hotelling独立提出。
- en: PCA attempts to pick new components in a manner that maximizes the variance
    along perpendicular axes and effectively transforms high-dimensional original
    features to a lower-dimensional space with derived components that can explain
    the variation (discriminate classes) in a more concise form.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: PCA试图以最大化垂直轴上的方差的方式选择新的组件，并有效地将高维原始特征转换为一个具有派生组件的低维空间，这些组件可以以更简洁的形式解释变化（区分类别）。
- en: 'The intuition beyond PCA is depicted in the following figure. Let''s assume
    for now that our data has two dimensions (x, y) and the question we are going
    to ask the data is whether most of the variation (and discrimination) can be explained
    by only one dimension or more precisely with a linear combination of original
    features:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: PCA背后的直觉如下图所示。现在假设我们的数据有两个维度（x，y），我们要问的问题是，大部分变化（和区分）是否可以用一个维度或更准确地说是原始特征的线性组合来解释：
- en: '![](img/00265.jpeg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00265.jpeg)'
- en: How to do it...
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: The Cleveland Heart Disease database is a published dataset used by ML researchers.
    The dataset contains more than a dozen fields, and experiments with the Cleveland
    database have concentrated on simply attempting to distinguish presence (value
    1,2,3) and absence (value 0) of the disease (in the goal column, 14th column).
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克利夫兰心脏病数据库是机器学习研究人员使用的一个已发布的数据集。该数据集包含十几个字段，对克利夫兰数据库的实验主要集中在试图简单地区分疾病的存在（值1,2,3）和不存在（值0）（在目标列，第14列）。
- en: The Cleveland Heart Disease dataset is available at [http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data](http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data).
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克利夫兰心脏病数据集可在[http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data](http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data)找到。
- en: 'The dataset contains the following attributes (age, sex, cp, trestbps, chol,
    fbs, restecg, thalach, exang, oldpeak, slope, ca, thal, num) that are depicted
    as the header  of the table below:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集包含以下属性（年龄，性别，cp，trestbps，chol，fbs，restecg，thalach，exang，oldpeak，slope，ca，thal，num），如下表的标题所示：
- en: 'For a detailed explanation on the individual attributes, refer to: [http://archive.ics.uci.edu/ml/datasets/Heart+Disease](http://archive.ics.uci.edu/ml/datasets/Heart+Disease)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 有关各个属性的详细解释，请参阅：[http://archive.ics.uci.edu/ml/datasets/Heart+Disease](http://archive.ics.uci.edu/ml/datasets/Heart+Disease)
- en: 'The dataset will look like the following:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集将如下所示：
- en: '| **age** | **sex** | **cp** | **trestbps** | **chol** | **fbs** | **restecg**
    | **thalach** | **exang** | **oldpeak** | **slope** | **ca** | **thal** | **num**
    |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| **age** | **sex** | **cp** | **trestbps** | **chol** | **fbs** | **restecg**
    | **thalach** | **exang** | **oldpeak** | **slope** | **ca** | **thal** | **num**
    |'
- en: '| 63 | 1 | 1 | 145 | 233 | 1 | 2 | 150 | 0 | 2.3 | 3 | 0 | 6 | 0 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 63 | 1 | 1 | 145 | 233 | 1 | 2 | 150 | 0 | 2.3 | 3 | 0 | 6 | 0 |'
- en: '| 67 | 1 | 4 | 160 | 286 | 0 | 2 | 108 | 1 | 1.5 | 2 | 3 | 3 | 2 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 67 | 1 | 4 | 160 | 286 | 0 | 2 | 108 | 1 | 1.5 | 2 | 3 | 3 | 2 |'
- en: '| 67 | 1 | 4 | 120 | 229 | 0 | 2 | 129 | 1 | 2.6 | 2 | 2 | 7 | 1 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 67 | 1 | 4 | 120 | 229 | 0 | 2 | 129 | 1 | 2.6 | 2 | 2 | 7 | 1 |'
- en: '| 37 | 1 | 3 | 130 | 250 | 0 | 0 | 187 | 0 | 3.5 | 3 | 0 | 3 | 0 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 37 | 1 | 3 | 130 | 250 | 0 | 0 | 187 | 0 | 3.5 | 3 | 0 | 3 | 0 |'
- en: '| 41 | 0 | 2 | 130 | 204 | 0 | 2 | 172 | 0 | 1.4 | 1 | 0 | 3 | 0 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 41 | 0 | 2 | 130 | 204 | 0 | 2 | 172 | 0 | 1.4 | 1 | 0 | 3 | 0 |'
- en: '| 56 | 1 | 2 | 120 | 236 | 0 | 0 | 178 | 0 | 0.8 | 1 | 0 | 3 | 0 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 56 | 1 | 2 | 120 | 236 | 0 | 0 | 178 | 0 | 0.8 | 1 | 0 | 3 | 0 |'
- en: '| 62 | 0 | 4 | 140 | 268 | 0 | 2 | 160 | 0 | 3.6 | 3 | 2 | 3 | 3 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 62 | 0 | 4 | 140 | 268 | 0 | 2 | 160 | 0 | 3.6 | 3 | 2 | 3 | 3 |'
- en: '| 57 | 0 | 4 | 120 | 354 | 0 | 0 | 163 | 1 | 0.6 | 1 | 0 | 3 | 0 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 57 | 0 | 4 | 120 | 354 | 0 | 0 | 163 | 1 | 0.6 | 1 | 0 | 3 | 0 |'
- en: '| 63 | 1 | 4 | 130 | 254 | 0 | 2 | 147 | 0 | 1.4 | 2 | 1 | 7 | 2 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 63 | 1 | 4 | 130 | 254 | 0 | 2 | 147 | 0 | 1.4 | 2 | 1 | 7 | 2 |'
- en: '| 53 | 1 | 4 | 140 | 203 | 1 | 2 | 155 | 1 | 3.1 | 3 | 0 | 7 | 1 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 53 | 1 | 4 | 140 | 203 | 1 | 2 | 155 | 1 | 3.1 | 3 | 0 | 7 | 1 |'
- en: '| 57 | 1 | 4 | 140 | 192 | 0 | 0 | 148 | 0 | 0.4 | 2 | 0 | 6 | 0 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 57 | 1 | 4 | 140 | 192 | 0 | 0 | 148 | 0 | 0.4 | 2 | 0 | 6 | 0 |'
- en: '| 56 | 0 | 2 | 140 | 294 | 0 | 2 | 153 | 0 | 1.3 | 2 | 0 | 3 | 0 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 56 | 0 | 2 | 140 | 294 | 0 | 2 | 153 | 0 | 1.3 | 2 | 0 | 3 | 0 |'
- en: '| 56 | 1 | 3 | 130 | 256 | 1 | 2 | 142 | 1 | 0.6 | 2 | 1 | 6 | 2 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 56 | 1 | 3 | 130 | 256 | 1 | 2 | 142 | 1 | 0.6 | 2 | 1 | 6 | 2 |'
- en: '| 44 | 1 | 2 | 120 | 263 | 0 | 0 | 173 | 0 | 0 | 1 | 0 | 7 | 0 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 44 | 1 | 2 | 120 | 263 | 0 | 0 | 173 | 0 | 0 | 1 | 0 | 7 | 0 |'
- en: '| 52 | 1 | 3 | 172 | 199 | 1 | 0 | 162 | 0 | 0.5 | 1 | 0 | 7 | 0 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 52 | 1 | 3 | 172 | 199 | 1 | 0 | 162 | 0 | 0.5 | 1 | 0 | 7 | 0 |'
- en: '| 57 | 1 | 3 | 150 | 168 | 0 | 0 | 174 | 0 | 1.6 | 1 | 0 | 3 | 0 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 57 | 1 | 3 | 150 | 168 | 0 | 0 | 174 | 0 | 1.6 | 1 | 0 | 3 | 0 |'
- en: '| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ...
    | ... |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ...
    | ... |'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '`package spark.ml.cookbook.chapter11`.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`package spark.ml.cookbook.chapter11`.'
- en: 'Import the necessary packages for the Spark session:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Spark会话所需的包：
- en: '[PRE25]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create Spark''s configuration and Spark session so we can have access to the
    cluster:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建Spark的配置和Spark会话，以便我们可以访问集群：
- en: '[PRE26]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We read in the original raw data file and count the raw data:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取原始数据文件并计算原始数据：
- en: '[PRE27]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the console, we get the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中，我们得到以下内容：
- en: '[PRE28]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We pre-process the dataset (see the preceding code for details):'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对数据集进行预处理（详细信息请参见前面的代码）：
- en: '[PRE29]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In the preceding code, we filter the missing data record, and use Spark DenseVector
    to host the data. After filtering the missing data, we get the following count
    of data in the console:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们过滤了缺失的数据记录，并使用Spark DenseVector来托管数据。在过滤缺失数据后，我们在控制台中得到以下数据计数：
- en: '[PRE30]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The record print, `2`, will look like the following:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 记录打印，`2`，将如下所示：
- en: '[PRE31]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We create a DataFrame from the data RDD, and create a PCA object for computing:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从数据RDD创建一个DataFrame，并创建一个用于计算的PCA对象：
- en: '[PRE32]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The parameters for the PCA model are shown in the preceding code. We set the
    `K` value to `4`. `K` represents the number of top K principal components that
    we are interested in after completing the dimensionality reduction algorithm.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA模型的参数如前面的代码所示。我们将`K`值设置为`4`。`K`代表在完成降维算法后我们感兴趣的前K个主成分的数量。
- en: 'An alternative is also available via the Matrix API: `mat.computePrincipalComponents(4)`.
    In this case, the `4` represents the top K principal components after the dimensionality
    reduction is completed.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一种选择也可以通过矩阵API实现：`mat.computePrincipalComponents(4)`。在这种情况下，`4`代表了在完成降维后的前K个主成分。
- en: 'We use the transform function to do computing and show the result in the console:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用transform函数进行计算，并在控制台中显示结果：
- en: '[PRE33]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The following will be displayed on the console.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容将显示在控制台上。
- en: 'What you are seeing are the four new PCA components (PC1, PC2, PC3, and PC4),
    which can be substituted for the original 14 features. We have successfully mapped
    the high-dimensional space (14 dimensions) to a lower-dimensional space (four
    dimensions):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 您所看到的是四个新的PCA组件（PC1、PC2、PC3和PC4），可以替代原始的14个特征。我们已经成功地将高维空间（14个维度）映射到了一个低维空间（四个维度）：
- en: '![](img/00266.gif)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00266.gif)'
- en: 'From the Spark Master (`http://localhost:4040/jobs`), you can also track the
    job, as shown in the following figure:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Spark Master（`http://localhost:4040/jobs`）中，您还可以跟踪作业，如下图所示：
- en: '![](img/00267.jpeg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00267.jpeg)'
- en: 'We then close the program by stopping the Spark session:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过停止Spark会话来关闭程序：
- en: '[PRE34]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How it works...
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'After loading and processing the data, the core of the work for PCA is done
    via the following code:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载和处理数据之后，通过以下代码完成了PCA的核心工作：
- en: '[PRE35]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `PCA()` call allows us to select how many components we need (`setK(4)`).
    In the case of this recipe, we selected the first four components.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`PCA()`调用允许我们选择需要多少个组件（`setK(4)`）。在这个配方的情况下，我们选择了前四个组件。'
- en: The goal is to find a lower-dimension space (a reduced PCA space) from the original
    higher-dimension data while preserving the structural properties (variance of
    data along principal component axis) in such a way that allows for maximum discrimination
    of labeled data without the original high-dimensional space requirement.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是从原始的高维数据中找到一个较低维度的空间（降低的PCA空间），同时保留结构属性（沿主成分轴的数据方差），以便最大限度地区分带标签的数据，而无需原始的高维空间要求。
- en: 'A sample PCA chart is shown in the following figure. After dimension reduction,
    it will look like the following--in this case, we can easily see that most of
    the variance is explained by the first four principal components. If you quickly
    examine the graph (red line), you see how fast the variance disappears after the
    fourth component. This type of knee chart (variance versus number of components)
    helps us to quickly pick the number of components that are needed (in this case,
    four components) to explain most of the variance. To recap, almost all the variance
    (green line) can be cumulatively attributed to the first four components, since
    it reaches almost 1.0 while the amount of contribution from each individual component
    can be traced via the red line at the same time:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个样本PCA图表。在降维后，它将看起来像下面这样--在这种情况下，我们可以很容易地看到大部分方差由前四个主成分解释。如果您快速检查图表（红线），您会看到第四个组件后方差如何迅速消失。这种膝盖图（方差与组件数量的关系）帮助我们快速选择所需的组件数量（在这种情况下，四个组件）来解释大部分方差。总之，几乎所有的方差（绿线）可以累积地归因于前四个组件，因为它几乎达到了1.0，同时可以通过红线追踪每个单独组件的贡献量：
- en: '![](img/00268.jpeg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00268.jpeg)'
- en: The chart above is a depiction of ‘Kaiser Rule’ which is the most commonly used
    approach to selecting the number of components. To produce the chart, one can
    use R to plot eigenvalues against principal components or write your own using
    Python.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表是“凯撒法则”的描述，这是选择组件数量最常用的方法。要生成图表，可以使用R来绘制特征值与主成分的关系，或者使用Python编写自己的代码。
- en: 'See the following link from university of Missouri for plotting a chart in
    R:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见密苏里大学的以下链接以在R中绘制图表：
- en: '[http://web.missouri.edu/~huangf/data/mvnotes/Documents/pca_in_r_2.html](http://web.missouri.edu/~huangf/data/mvnotes/Documents/pca_in_r_2.html).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://web.missouri.edu/~huangf/data/mvnotes/Documents/pca_in_r_2.html](http://web.missouri.edu/~huangf/data/mvnotes/Documents/pca_in_r_2.html)。'
- en: As stated, the chart relates to Kaiser rule which states the more correlation
    variables loaded in a particular principal component, the more important that
    factor is in summarizing the data. The eigenvalue in this case can be thought
    of as a sort of index that measures how good a component is summarizing the data
    (in direction of maximum variance).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，图表与凯撒法则有关，凯撒法则指出在特定主成分中加载的更多相关变量，该因子在总结数据方面就越重要。在这种情况下，特征值可以被认为是一种衡量组件在总结数据方面的好坏的指标（在最大方差方向上）。
- en: Using PCA is similar to other methods in which we try to learn the distribution
    for the data. We still need the average of each attribute and K (the number of
    components to keep), which is simply an estimated covariance. In short, dimension
    reduction occurs because we are ignoring the directions (the PCA components) that
    have the least variance. Keep in mind that PCA can be difficult, but you are in
    control of what happens and how much you keep (use knee charts to select K or
    the number of components to keep).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA类似于其他方法，我们试图学习数据的分布。我们仍然需要每个属性的平均值和K（要保留的组件数量），这只是一个估计的协方差。简而言之，降维发生是因为我们忽略了具有最小方差的方向（PCA组件）。请记住，PCA可能很困难，但您可以控制发生的事情以及保留多少（使用膝盖图表来选择K或要保留的组件数量）。
- en: 'There are two methods for calculating PCA:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种计算PCA的方法：
- en: Covariance method
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协方差方法
- en: '**Singular Value Decomposition** (**SVD**)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）'
- en: We will outline the covariance matrix method (the straight eigenvector and eigenvalue
    plus centering) here, but feel free to refer to the SVD recipe (*Singular Value
    Decomposition (SVD) to reduce high-dimensionality in Spark*) for the inner workings
    of SVD as it relates to PCA.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里概述协方差矩阵方法（直接特征向量和特征值加上居中），但是请随时参考SVD配方（*Singular Value Decomposition（SVD）在Spark中减少高维度*）以了解SVD与PCA的内部工作原理。
- en: 'The PCA algorithm using the covariance matrix method, in a nutshell, involves
    the following:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 用协方差矩阵方法进行PCA算法，简而言之，涉及以下内容：
- en: 'Given a matrix of N by M:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个N乘以M的矩阵：
- en: N = total number of training data
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: N = 训练数据的总数
- en: M is a particular dimension (or feature)
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M是特定的维度（或特征）
- en: Intersection of M x N is a call with the sample value
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M x N的交集是一个带有样本值的调用
- en: 'Compute the mean:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算平均值：
- en: '![](img/00269.jpeg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00269.jpeg)'
- en: Center (normalize) the data by subtracting the average from each observation:![](img/00270.jpeg)
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从每个观察中减去平均值来对数据进行中心化（标准化）：![](img/00270.jpeg)
- en: Construct the covariance matrix:![](img/00271.jpeg)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建协方差矩阵：![](img/00271.jpeg)
- en: Compute eigenvectors and eigenvalues of the covariance matrix (it's straightforward,
    but bear in mind that not all matrices can be decomposed).
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵的特征向量和特征值（这很简单，但要记住并非所有矩阵都可以分解）。
- en: Pick the eigenvectors that have the largest eigenvalues.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择具有最大特征值的特征向量。
- en: The larger the eigenvalue, the more contribution to the variance of a component.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征值越大，对组件的方差贡献越大。
- en: There's more...
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The net result of using PCA in this recipe is that the original search space
    of 14 dimensions (the same as saying14 features) is reduced to 4 dimensions that
    explain almost all the variations in the original dataset.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA在这个案例中的净结果是，原始的14维搜索空间（也就是说14个特征）被减少到解释原始数据集中几乎所有变化的4个维度。
- en: PCA is not purely a ML concept and has been in use in finance for many years
    prior to the ML movement. At its core, PCA uses an orthogonal transformation (each
    component is perpendicular to the other component) to map the original features
    (apparent dimensions) to a set of newly derived dimensions so that most of the
    redundant and co-linear attributes are removed. The derived (actual latent dimension)
    components are linear combinations of the original attributes.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: PCA并不纯粹是一个机器学习概念，在机器学习运动之前，它在金融领域已经使用了很多年。在本质上，PCA使用正交变换（每个组件都与其他组件垂直）将原始特征（明显的维度）映射到一组新推导的维度，以便删除大部分冗余和共线性属性。推导的（实际的潜在维度）组件是原始属性的线性组合。
- en: While it is easy to program PCA from scratch using RDD, the best way to learn
    it is to try to do PCA with a neuron network implementation and look at the intermediate
    result. You can do this in Café (on Spark), or just Torch, to see that it is a
    straight linear transformation despite the mystery surrounding it. At its core,
    PCA is a straight exercise in linear algebra regardless of whether you use the
    covariance matrix or SVD for decomposition.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用RDD从头开始编程PCA很容易，但学习它的最佳方法是尝试使用神经网络实现PCA，并查看中间结果。您可以在Café（在Spark上）中进行此操作，或者只是Torch，以查看它是一个直线转换，尽管围绕它存在一些神秘。在本质上，无论您使用协方差矩阵还是SVD进行分解，PCA都是线性代数的基本练习。
- en: Spark provides examples for PCA via source code on GitHub under both the dimensionality
    reduction and feature extraction sections.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在GitHub上提供了PCA的源代码示例，分别在降维和特征提取部分。
- en: See also
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Documentation for PCA can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCA) [and ](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCA)[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCAModel)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA的文档可以在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCA)和[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCAModel)找到。
- en: 'Some words of caution regarding PCA usage and shortcomings:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 关于PCA的使用和缺点的一些建议：
- en: Some datasets are mutually exclusive so that eigenvalues do not dropoff, (every
    single value is needed for the matrix). For example, the following vectors (.5,0,0),
    (0,.5,0,0), (0,0,.5,0), and (0,0,0,.5) ...... will not allow any eigenvalue to
    drop.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些数据集是相互排斥的，因此特征值不会下降（矩阵中每个值都是必需的）。例如，以下向量(.5,0,0), (0,.5,0,0), (0,0,.5,0),
    and (0,0,0,.5)......不会允许任何特征值下降。
- en: PCA is linear in nature and attempts to learn a Gaussian distribution by using
    mean and the covariance matrix.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA是线性的，试图通过使用均值和协方差矩阵来学习高斯分布。
- en: Sometimes two Gaussian distributions parallel to each other will not allow PCA
    to find the right direction. In this case, PCA will eventually terminate and find
    some directions and output them, but are they the best?
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，两个彼此平行的高斯分布不会允许PCA找到正确的方向。在这种情况下，PCA最终会终止并找到一些方向并输出它们，但它们是否是最好的呢？
