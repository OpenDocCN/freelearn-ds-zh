- en: Spark for Big Data Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据分析中的Spark
- en: As the use of Hadoop and related technologies in the respective ecosystem gained
    prominence, a few obvious and salient deficiencies of the Hadoop operational model
    became apparent. In particular, the ingrained reliance on the MapReduce paradigm,
    and other facets related to MapReduce, made a truly functional use of the Hadoop
    ecosystem possible only for major firms that were invested deeply in the respective
    technologies.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Hadoop及其相关技术在各自生态系统中的使用日益突出，Hadoop操作模型的一些明显和显著的缺陷也变得显而易见。特别是对MapReduce范式的固有依赖以及与MapReduce相关的其他方面，使得仅有那些深度投资于相应技术的主要公司才能真正功能化地使用Hadoop生态系统。
- en: At the **UC Berkeley Electrical Engineering and Computer Sciences** (**EECS**)
    Annual Research Symposium of 2011, a vision for a new research group at the university
    was announced during a presentation by Prof. Ian Stoica ([https://amplab.cs.berkeley.edu/about/](https://amplab.cs.berkeley.edu/about/)).
    It laid out the foundation of what was to become a pivotal unit that would profoundly
    change the landscape of Big Data. The **AMPLab**, launched in February 2011, aimed
    to deliver a scalable and unified solution by integrating Algorithms, Machines,
    and People that could cater to future needs without requiring any major re-engineering
    efforts.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在2011年的**加州大学伯克利分校电子工程与计算机科学系**（**EECS**）年度研究研讨会上，由伊恩·斯托伊卡教授在演讲中宣布了一项新的研究小组的愿景（[https://amplab.cs.berkeley.edu/about/](https://amplab.cs.berkeley.edu/about/)）。它奠定了未来将深刻改变大数据格局的关键单位的基础。**AMPLab**于2011年2月成立，旨在通过集成算法、机器和人力提供可扩展且统一的解决方案，以满足未来需求，而无需进行任何重大的重新工程化工作。
- en: The most well-known and most widely used project to evolve from the AMPLab initiative
    was Spark, arguably a superior alternative - or more precisely, *extension* -
    of the Hadoop ecosystem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从AMPLab倡议中演化而来的最著名和最广泛使用的项目之一是Spark，可以说是Hadoop生态系统的一个更优越的选择或更确切地说是*扩展*。
- en: 'In this chapter, we will visit some of the salient characteristics of Spark
    and end with a real-world tutorial on how to use Spark. The topics we will cover
    are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍Spark的一些显著特征，并以一个关于如何使用Spark的实际教程结束。我们将涵盖的主题包括：
- en: The advent of Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的出现
- en: Theoretical concepts in Spark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark中的理论概念
- en: Core components of Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的核心组件
- en: The Spark architecture
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark架构
- en: Spark solutions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark解决方案
- en: Spark tutorial
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark教程
- en: The advent of Spark
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的出现
- en: When the first release of Spark became available in 2014, Hadoop had already
    enjoyed several years of growth since 2009 onwards in the commercial space. Although
    Hadoop solved a major hurdle in analyzing large terabyte-scale datasets efficiently,
    using distributed computing methods that were broadly accessible, it still had
    shortfalls that hindered its wider acceptance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当2014年首次发布Spark时，自2009年以来，Hadoop在商业空间已经享有多年的增长。尽管Hadoop有效地解决了分析大规模TB级数据集的主要难题，并使用广泛可访问的分布式计算方法，但仍然存在一些短板，限制了其更广泛的接受度。
- en: Limitations of Hadoop
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop的局限性
- en: 'A few of the common limitations with Hadoop were as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop存在一些常见的局限性，其中几个如下：
- en: '**I/O Bound operations**: Due to the reliance on local disk storage for saving
    and retrieving data, any operation performed in Hadoop incurred an I/O overhead.
    The problem became more acute in cases of larger datasets that involved thousands
    of blocks of data across hundreds of servers. To be fair, the ability to co-ordinate
    concurrent I/O operations (via HDFS) formed the foundation of distributed computing
    in Hadoop world. However, leveraging the capability and *tuning* the Hadoop cluster
    in an efficient manner across different use cases and datasets required an immense
    and perhaps disproportionate level of expertise. Consequently, the I/O bound nature
    of workloads became a deterrent factor for using Hadoop against extremely large
    datasets. As an example, machine learning use cases that required hundreds of
    iterative operations meant that the system would incur an I/O overhead for each
    pass of the iteration.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**I/O Bound操作**：由于依赖于本地磁盘存储来保存和检索数据，Hadoop中执行的任何操作都会产生I/O开销。在涉及跨数百台服务器的数千个数据块的大型数据集的情况下，问题变得更加严重。公平地说，通过HDFS协调并发I/O操作（在Hadoop世界中）构建了分布式计算的基础。然而，利用这种能力并在不同的用例和数据集中有效地*调整*
    Hadoop集群，需要极高且可能不成比例的专业水平。因此，工作负载的I/O绑定特性成为阻碍使用Hadoop处理极大数据集的因素。例如，需要数百次迭代操作的机器学习用例意味着系统每次迭代都会产生I/O开销。'
- en: '**MapReduce programming (MR) Model**: As discussed in the earlier parts of
    this book, all operations in Hadoop require expressing problems in terms of the
    MapReduce Programming Model - namely, the user would have to express the problem
    in terms of key-value pairs where each pair can be independently computed. In
    Hadoop, coding efficient MapReduce programs, mainly in Java, was non-trivial,
    especially for those new to Java or to Hadoop (or both).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MapReduce 编程（MR）模型**：正如本书前面的部分所讨论的，Hadoop 中的所有操作都需要使用 MapReduce 编程模型来表达问题——即用户必须通过键值对来表达问题，每个键值对可以独立计算。在
    Hadoop 中，编写高效的 MapReduce 程序（主要是用 Java）并不简单，尤其对于那些不熟悉 Java 或 Hadoop（或两者）的新手来说。'
- en: '**Non-MR Use Cases**: Due to the reliance on MapReduce, other more common and
    simpler concepts such as filters, joins, and so on would have to also be expressed
    in terms of a MapReduce program. Thus, a join across two files across a primary
    key would have to adopt a key-value pair approach. This meant that operations,
    both simple and complex, were hard to achieve without significant programming
    efforts.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非 MR 用例**：由于依赖于 MapReduce，其他更常见和简单的概念（如过滤器、连接等）也必须通过 MapReduce 程序来表达。因此，在两个文件之间通过主键进行连接时，也必须采用键值对方法。这意味着，无论是简单操作还是复杂操作，都难以在没有显著编程努力的情况下实现。'
- en: '**Programming APIs**: The use of Java as the central programming language across
    Hadoop meant that to be able to properly administer and use Hadoop, developers
    had to have a strong knowledge of Java and related topics such as JVM tuning,
    Garbage Collection, and others. This also meant that developers in other popular
    languages such as R, Python, and Scala had very little recourse for re-using or
    at least implementing their solution in the language they knew best.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编程 API**：Hadoop 采用 Java 作为主要编程语言，这意味着为了能够正确管理和使用 Hadoop，开发人员必须具备扎实的 Java
    知识以及相关主题（如 JVM 调优、垃圾回收等）的掌握。这也意味着其他流行语言如 R、Python 和 Scala 的开发人员在重新使用或至少用自己最擅长的语言实现解决方案时，几乎没有选择。'
- en: On the whole, even though the Hadoop world had championed the Big Data revolution,
    it fell short of being able to democratize the use of the technology for Big Data
    on a broad scale.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总的来说，尽管 Hadoop 世界推动了大数据革命，但它未能广泛实现将大数据技术普及化的目标。
- en: The team at AMPLab recognized these shortcomings early on, and set about creating
    Spark to address these and, in the process, hopefully develop a new, superior
    alternative.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: AMPLab 团队早期就意识到这些不足，并着手创建 Spark 来解决这些问题，并在此过程中，希望能开发出一种新的、更优越的替代方案。
- en: Overcoming the limitations of Hadoop
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 克服 Hadoop 的局限性
- en: We'll now look at some of the limitations discussed in the earlier section and
    understand how Spark addresses these areas, by virtue of which it provides a superior
    alternative to the Hadoop ecosystem.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将回顾之前部分讨论的一些局限性，了解 Spark 如何解决这些问题，正因如此，它提供了一个优于 Hadoop 生态系统的替代方案。
- en: A key difference to bear in mind at the onset is that Spark does NOT need Hadoop
    in order to operate. In fact, the underlying backend from which Spark accesses
    data can be technologies such as HBase, Hive and Cassandra in addition to HDFS.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个关键区别是，Spark 并不需要 Hadoop 才能运行。实际上，Spark 访问数据的底层后端可以是 HBase、Hive 和 Cassandra
    等技术，除了 HDFS。
- en: This means that organizations that wish to leverage a standalone Spark system
    can do so without building a separate Hadoop infrastructure if one does not already
    exist.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，希望利用独立 Spark 系统的组织，可以在没有现有 Hadoop 基础设施的情况下，直接使用 Spark。
- en: 'The Spark solutions are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的解决方案如下：
- en: '**I/O Bound operations**: Unlike Hadoop, Spark can store and access data stored
    in *memory*, namely RAM - which, as discussed earlier, is 1,000+ times faster
    than reading data from a disk. With the emergence of SSD drives, the standard
    in today''s enterprise systems, the difference has gone down significantly. Recent
    NVMe drives can deliver up to 3-5 GB (Giga Bytes) of bandwidth per second. Nevertheless,
    RAM, which averages about 25-30 GB per second in read speed, is still 5-10x faster
    compared to reading from the newer storage technologies. As a result, being able
    to store data in RAM provides a 5x or more improvement to the time it takes to
    read data for Spark operations. This is a significant improvement over the Hadoop
    operating model which relies on disk read for all operations. In particular, tasks
    that involve iterative operations as in machine learning benefit immensely from
    the Spark''s facility to store and read data from memory.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**I/O 绑定操作**：与 Hadoop 不同，Spark 可以存储和访问存储在*内存*中的数据，也就是 RAM——如前所述，这比从磁盘读取数据要快
    1,000 多倍。随着 SSD 驱动器的出现，这已成为今天企业系统中的标准，这种差异已经显著减少。近期的 NVMe 驱动器能够提供每秒 3-5 GB（千兆字节）的带宽。然而，RAM
    的读取速度大约为每秒 25-30 GB，仍然比新型存储技术快 5-10 倍。因此，能够将数据存储在 RAM 中为 Spark 操作读取数据的时间提供了 5
    倍或更多的提升。这是对 Hadoop 操作模型的显著改进，后者依赖磁盘读取来完成所有操作。特别是涉及迭代操作的任务（如机器学习）从 Spark 提供的内存存储和读取能力中受益匪浅。'
- en: '**MapReduce programming (MR) Model**: While MapReduce is the primary programming
    model through which users can benefit from a Hadoop platform, Spark does not have
    the same requirement. This is particularly helpful for more complex use cases
    such as quantitative analysis involving calculations that cannot be easily *parallelized,*
    such as machine learning algorithms. By decoupling the programming model from
    the platform, Spark allows users to write and execute code written in various
    languages without forcing any specific programming model as a pre-requisite.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MapReduce 编程（MR）模型**：虽然 MapReduce 是用户通过 Hadoop 平台受益的主要编程模型，但 Spark 并不需要遵循这一要求。这对于更复杂的用例特别有帮助，例如涉及无法轻松*并行化*的计算的定量分析（如机器学习算法）。通过将编程模型与平台解耦，Spark
    允许用户使用多种语言编写和执行代码，而无需强制要求任何特定的编程模型作为前提条件。'
- en: '**Non-MR use cases**: Spark SQL, Spark Streaming and other components of the
    Spark ecosystem provide a rich set of functionalities that allow users to perform
    common tasks such as SQL joins, aggregations, and related database-like operations
    without having to leverage other, external solutions. Spark SQL queries are generally
    executed against data stored in Hive (JSON is another option), and the functionality
    is also available in other Spark APIs such as R and Python.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非 MapReduce 用例**：Spark SQL、Spark Streaming 以及 Spark 生态系统中的其他组件提供了一整套丰富的功能，使用户可以执行常见任务，如
    SQL 连接、聚合及相关的数据库操作，而无需依赖其他外部解决方案。Spark SQL 查询通常执行的是存储在 Hive 中的数据（JSON 也是一种选择），这些功能也可以在其他
    Spark API 中使用，如 R 和 Python。'
- en: '**Programming APIs**: The most commonly used APIs in Spark are Python, Scala
    and Java. For R programmers, there is a separate package called `SparkR` that
    permits direct access to Spark data from R. This is a major differentiating factor
    between Hadoop and Spark, and by exposing APIs in these languages, Spark becomes
    immediately accessible to a much larger community of developers. In Data Science
    and Analytics, Python and R are the most prominent languages of choice, and hence,
    any Python or R programmer can leverage Spark with a much simpler learning curve
    relative to Hadoop. In addition, Spark also includes an interactive shell for
    ad-hoc analysis.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编程 API**：Spark 中最常用的 API 是 Python、Scala 和 Java。对于 R 程序员，还有一个名为 `SparkR` 的单独包，可以直接通过
    R 访问 Spark 数据。这是 Hadoop 和 Spark 之间的一个主要区别，通过暴露这些语言的 API，Spark 能够立刻为更大范围的开发者社区所使用。在数据科学和分析领域，Python
    和 R 是最受欢迎的编程语言，因此，任何 Python 或 R 程序员都可以更轻松地使用 Spark，相较于 Hadoop，其学习曲线更为平缓。此外，Spark
    还包含一个用于临时分析的交互式 shell。'
- en: Theoretical concepts in Spark
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 中的理论概念
- en: 'The following are the core concepts in Spark:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Spark 中的核心概念：
- en: Resilient distributed datasets
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性分布式数据集
- en: Directed acyclic graphs
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有向无环图
- en: SparkContext
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkContext
- en: Spark DataFrames
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 数据帧
- en: Actions and transformations
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作和转换
- en: Spark deployment options
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 部署选项
- en: Resilient distributed datasets
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性分布式数据集
- en: '**Resilient distributed datasets**, more commonly known as **RDD**s, are the
    primary data structure used in Spark. RDDs are essentially a collection of records
    that are stored across a Spark cluster in a distributed manner. RDDs are *immutable*,
    which is to say, they cannot be altered once created. RDDs that are stored across
    nodes can be accessed in parallel, and hence support parallel operations natively.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集**，通常被称为**RDD**，是Spark中使用的主要数据结构。RDD本质上是一组记录，这些记录以分布式方式存储在Spark集群中。RDD是*不可变*的，意味着它们一旦创建就不能修改。分布在各个节点上的RDD可以并行访问，因此本地支持并行操作。'
- en: The user does not need to write separate code to get the benefits of parallelization
    but can get the benefits of *actions and transformations* of data simply by running
    specific commands that are native to the Spark platform. Because RDDs can be also
    stored in memory, as an additional benefit, the parallel operations can act on
    the data directly in memory without incurring expensive I/O access penalties.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 用户无需编写单独的代码即可获得并行化的好处，只需通过运行Spark平台本身的特定命令，就能享受到数据的*操作和转换*带来的好处。由于RDD也可以存储在内存中，作为额外的好处，并行操作可以直接在内存中的数据上进行，而无需产生昂贵的I/O访问延迟。
- en: Directed acyclic graphs
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定向无环图
- en: In computer science and mathematics parlance, a directed acyclic graph represents
    pairs of nodes (also known as **vertices**) connected with edges (or **lines**)
    that are unidirectional. Namely, given Node A and Node B, the edge can connect
    A à B or B à A but not both. In other words, there isn't a circular relationship
    between any pair of nodes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学和数学术语中，定向无环图表示一对节点（也称为**顶点**）通过边（或**线**）连接，且这些边是单向的。也就是说，给定节点A和节点B，边可以连接A
    → B或B → A，但不能同时连接两者。换句话说，任意一对节点之间不会形成循环关系。
- en: Spark leverages the concept of DAG to build an internal workflow that delineates
    the different stages of processing in a Spark job. Conceptually, this is akin
    to creating a virtual flowchart of the series of steps needed to obtain a certain
    output. For instance, if the required output involves producing a count of words
    in a document, the intermediary steps map-shuffle-reduce can be represented as
    a series of actions that lead to the final result. By maintaining such a **map**,
    Spark is able to keep track of the dependencies involved in the operation. More
    specifically, RDDs are the **nodes**, and transformations, which are discussed
    later in this section, are the **edges** of the DAG.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Spark利用DAG的概念来构建一个内部工作流程，划定Spark作业中不同的处理阶段。从概念上讲，这类似于创建一个虚拟流程图，描述获取特定输出所需的各个步骤。例如，如果所需输出是计算文档中的单词数量，那么中间步骤map-shuffle-reduce可以表示为一系列操作，最终得出结果。通过维护这样的**map**，Spark能够跟踪操作中的依赖关系。更具体地说，RDD是**节点**，而后面会讨论的转换操作是DAG的**边**。
- en: SparkContext
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkContext
- en: A SparkContext is the entry point for all Spark operations and means by which
    the application connects to the resources of the Spark cluster. It initializes
    an instance of Spark and can thereafter be used to create RDDs, perform actions
    and transformations on the RDDs, and extract data and other Spark functionalities.
    A SparkContext also initializes various properties of the process, such as the
    application name, number of cores, memory usage parameters, and other characteristics.
    Collectively, these properties are contained in the object SparkConf, which is
    passed to SparkContext as a parameter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SparkContext是所有Spark操作的入口点，它是应用程序连接到Spark集群资源的方式。它初始化一个Spark实例，之后可以用来创建RDD、对RDD进行操作和转换，并提取数据和其他Spark功能。SparkContext还会初始化进程的各种属性，例如应用程序名称、核心数、内存使用参数以及其他特性。这些属性总称为SparkConf对象，并作为参数传递给SparkContext。
- en: '`SparkSession` is the new abstraction through which users initiate their connection
    to Spark. It is a superset of the functionality provided in `SparkContext` prior
    to Spark 2.0.0\. However, practitioners still use `SparkSession` and `SparkContext`
    interchangeably to mean one and the same entity; namely, the primary mode of interacting
    with `Spark.SparkSession` has essentially combined the functionalities of both
    SparkContext and `HiveContext`.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`是用户启动连接到Spark的全新抽象，它是Spark 2.0.0之前`SparkContext`功能的超集。然而，实践者仍然常常互换使用`SparkSession`和`SparkContext`，指代相同的实体；即与`Spark.SparkSession`交互的主要方式，实际上结合了SparkContext和`HiveContext`的功能。'
- en: Spark DataFrames
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark DataFrames
- en: A DataFrame in Spark is the raw data organized into rows and columns. This is
    conceptually similar to CSV files or SQL tables. Using R, Python and other Spark
    APIs, the user can interact with a DataFrame using common Spark commands used
    for filtering, aggregating, and more generally manipulating the data. The data
    contained in DataFrames are physically located across the multiple nodes of the
    Spark cluster. However, by representing them in a **DataFrame** they appear to
    be a cohesive unit of data without exposing the complexity of the underlying operations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，DataFrame 是将原始数据组织成行和列的数据结构。从概念上讲，这类似于 CSV 文件或 SQL 表格。通过 R、Python
    和其他 Spark API，用户可以使用常见的 Spark 命令来操作 DataFrame，实现过滤、聚合和更一般的数据处理。DataFrame 中的数据物理上分布在
    Spark 集群的多个节点上。然而，通过将数据表示为 **DataFrame**，它们看起来是一个统一的数据单元，而无需暴露底层操作的复杂性。
- en: Note that DataFrames are not the same as Datasets, another common term used
    in Spark. Datasets refer to the actual data that is held across the Spark cluster.
    A DataFrame is the tabular representation of the Dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，DataFrame 与 Dataset 是不同的，Dataset 是 Spark 中的另一个常用术语。Dataset 指的是分布在 Spark
    集群中的实际数据。而 DataFrame 则是 Dataset 的表格表示。
- en: Starting with Spark 2.0, the DataFrame and Dataset APIs were merged and a DataFrame
    in essence now represents a Dataset of Row. That said, DataFrame still remains
    the primary abstraction for users who want to leverage Python and R for interacting
    with Spark data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Spark 2.0 开始，DataFrame 和 Dataset 的 API 被合并，本质上，DataFrame 现在表示一个包含行的 Dataset。也就是说，DataFrame
    仍然是用户使用 Python 和 R 与 Spark 数据交互时的主要抽象。
- en: Actions and transformations
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作和转换
- en: 'There are 2 types of Spark operations:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 操作有 2 种类型：
- en: Transformations
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换
- en: Actions
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作
- en: '**Transformations** specify general data manipulation operations such as filtering
    data, joining data, performing aggregations, sampling data, and so on. Transformations
    do not return any result when the line containing the transformation operation
    in the code is executed. Instead, the command, upon execution, supplements Spark''s
    internal DAG with the corresponding operation request. Examples of common transformations
    include: `map`, `filter`, `groupBy`, `union`, `coalesce`, and many others.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**转换**指定了常见的数据操作，例如过滤数据、连接数据、进行聚合、采样数据等等。转换在执行包含转换操作的代码行时不会返回任何结果。相反，执行时，该命令会将相应的操作请求添加到
    Spark 的内部 DAG 中。常见的转换包括：`map`、`filter`、`groupBy`、`union`、`coalesce` 等等。'
- en: '**Actions**, on the other hand, return results. Namely, they execute the series
    of transformations (if any) that the user may have specified on the corresponding
    RDD and produce an output. In other words, actions trigger the execution of the
    steps in the DAG. Common Actions include: `reduce`, `collect`, `take`, `aggregate`,
    `foreach`, and many others.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**操作**，另一方面，会返回结果。即，它们会执行用户可能在相应 RDD 上指定的一系列转换（如果有的话），并生成输出。换句话说，操作触发了 DAG
    中步骤的执行。常见的操作包括：`reduce`、`collect`、`take`、`aggregate`、`foreach` 等等。'
- en: Note that RDDs are immutable. They cannot be changed; transformations and actions
    will always produce new RDDs, but never modify existing ones.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，RDD 是不可变的。它们不能被更改；转换和操作总是会生成新的 RDD，而不会修改现有的 RDD。
- en: Spark deployment options
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 部署选项
- en: 'Spark can be deployed in various modes. The most important ones are:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 可以在多种模式下部署。最重要的模式包括：
- en: '**Standalone mode**: As an independent cluster not dependent upon any external
    cluster manager'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立模式**：作为一个独立的集群，不依赖任何外部集群管理器'
- en: '**Amazon EC2**: On EC2 instances of Amazon Web Services where it can access
    data from S3'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon EC2**：在亚马逊 Web 服务的 EC2 实例上，它可以访问来自 S3 的数据'
- en: '**Apache YARN**: The Hadoop ResourceManager'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache YARN**：Hadoop 资源管理器'
- en: Other options include **Apache Mesos** and **Kubernetes.**
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其他选项包括 **Apache Mesos** 和 **Kubernetes**。
- en: Further details can be found at the Spark documentation website, [https://spark.apache.org/docs/latest/index.html](https://spark.apache.org/docs/latest/index.html).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详情可以在 Spark 文档网站上找到，[https://spark.apache.org/docs/latest/index.html](https://spark.apache.org/docs/latest/index.html)。
- en: Spark APIs
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark API
- en: The Spark platform is easily accessible through Spark APIs available in Python,
    Scala, R, and Java. Together they make working with data in Spark simple and broadly
    accessible. During the inception of the Spark project, it only supported Scala/Java
    as the primary API. However, since one of the overarching objectives of Spark
    was to provide an easy interface to a diverse set of developers, the Scala API
    was followed by a Python and R API.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 平台通过 Python、Scala、R 和 Java 中可用的 Spark API 轻松访问。它们一起使得在 Spark 中处理数据变得简单且广泛可用。在
    Spark 项目开始时，它只支持 Scala/Java 作为主要 API。然而，鉴于 Spark 的一个主要目标是为不同开发者提供简易的接口，因此 Scala
    API 后便推出了 Python 和 R API。
- en: In Python, the PySpark package has become a widely used standard for writing
    Spark applications by the Python developer community. In R, users interact with
    Spark via the SparkR package. This is useful for R developers who may also be
    interested in working with data stored in a Spark ecosystem. Both of these languages
    are very prevalent in the Data Science community, and hence, the introduction
    of the Python and R APIs set the groundwork for democratizing **Big Data** Analytics
    on Spark for analytical use cases.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，PySpark 包已经成为 Python 开发者社区编写 Spark 应用程序的广泛使用标准。在 R 中，用户通过 SparkR
    包与 Spark 进行交互。这对那些可能也有兴趣使用存储在 Spark 生态系统中的数据的 R 开发者非常有用。这两种语言在数据科学社区中都非常流行，因此
    Python 和 R API 的引入为在 Spark 上进行 **大数据** 分析的分析性用例奠定了基础。
- en: Core components in Spark
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 中的核心组件
- en: 'The following components are quite important in Spark:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下组件在 Spark 中非常重要：
- en: Spark Core
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Core
- en: Spark SQL
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Spark Streaming
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: GraphX
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GraphX
- en: MLlib
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLlib
- en: Spark Core
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Core
- en: Spark Core provides fundamental functionalities in Spark, such as working with
    RDDs, performing actions, and transformations, in addition to more administrative
    tasks such as storage, high availability, and other topics.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Core 提供了 Spark 中的基本功能，如操作 RDD、执行动作和转换，以及更多的管理任务，如存储、高可用性和其他话题。
- en: Spark SQL
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Spark SQL provides the user with the ability to query data stored in Apache
    Hive using standard SQL commands. This adds an additional level of accessibility
    by providing developers with a means to interact with datasets via the Spark SQL
    interface using common SQL terminologies. The platform hosting the underlying
    data is not limited to Apache Hive, but can also include JSON, Parquet, and others.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 为用户提供了通过标准 SQL 命令查询存储在 Apache Hive 中的数据的能力。这为开发者提供了一种通过 Spark SQL
    接口使用常见 SQL 术语与数据集互动的方式，增加了额外的可访问性。底层数据的平台不仅限于 Apache Hive，还可以包括 JSON、Parquet 等。
- en: Spark Streaming
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: The streaming component of Spark allows users to interact with streaming data
    such as web-related content and others. It also includes enterprise characteristics
    such as high availability. Spark can read data from various middleware and data
    streaming services such as Apache Kafka, Apache Flume, and Cloud based solutions
    from vendors such as Amazon Web Services.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的流处理组件允许用户与流数据进行交互，如与网络相关的内容等。它还包括企业级特性，如高可用性。Spark 可以从各种中间件和数据流服务读取数据，例如
    Apache Kafka、Apache Flume 以及 Amazon Web Services 等云解决方案提供商。
- en: GraphX
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GraphX
- en: The GraphX component of Spark supports graph-based operations, similar to technologies
    such as graph databases that support specialized data structures. These make it
    easy to use, access, and represent inter-connected points of data, such as social
    networks. Besides analytics, the Spark GraphX platform supports graph algorithms
    that are useful for business use cases that require relationships to be represented
    at scale. As an example, credit card companies use Graph based databases similar
    to the GraphX component of Spark to build recommendation engines that detect users
    with similar characteristics. These characteristics may include buying habits,
    location, demographics, and other qualitative and quantitative factors. Using
    Graph systems in these cases allows companies to build networks with nodes representing
    individuals and edges representing relationship metrics to find common features
    amongst them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的 GraphX 组件支持基于图的操作，类似于支持专用数据结构的图数据库等技术。这些技术使得使用、访问和表示互联数据点变得容易，例如社交网络。除了数据分析外，Spark
    GraphX 平台还支持图算法，适用于需要大规模表示关系的商业用例。例如，信用卡公司使用类似于 Spark 的 GraphX 组件的基于图的数据库来构建推荐引擎，检测具有相似特征的用户。这些特征可能包括购买习惯、位置、人口统计信息以及其他定性和定量因素。在这些案例中使用图系统可以让公司构建包含代表个人的节点和代表关系度量的边的网络，从而找到它们之间的共同特征。
- en: MLlib
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLlib
- en: 'MLlib is one of the flagship components of the Spark ecosystem. It provides
    a scalable, high-performance interface to perform resource intensive machine learning
    tasks in Spark. Additionally, MLlib can natively connect to HDFS, HBase, and other
    underlying storage systems supported in Spark. Due to this versatility, users
    do not need to rely on a pre-existing Hadoop environment to start using the algorithms
    built into MLlib. Some of the supported algorithms in MLlib include:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 是 Spark 生态系统的旗舰组件之一。它提供了一个可扩展的高性能接口，用于在 Spark 中执行资源密集型的机器学习任务。此外，MLlib
    可以本地连接到 HDFS、HBase 和 Spark 支持的其他底层存储系统。由于这种多功能性，用户无需依赖现有的 Hadoop 环境即可开始使用内置于 MLlib
    中的算法。MLlib 中支持的一些算法包括：
- en: '**Classification**: logistic regression'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：逻辑回归'
- en: '**Regression**: generalized linear regression, survival regression and others'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：广义线性回归、生存回归等'
- en: Decision trees, random forests, and gradient-boosted trees
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树、随机森林和梯度提升树
- en: '**Recommendation**: Alternating least squares'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐**：交替最小二乘法'
- en: '**Clustering**: K-means, Gaussian mixtures and others'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：K-means、高斯混合等'
- en: '**Topic modeling**: Latent Dirichlet allocation'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题建模**：潜在狄利克雷分配'
- en: '**Apriori**: Frequent Itemsets, Association Rules'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apriori**：频繁项集，关联规则'
- en: 'ML workflow utilities include:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ML 工作流实用工具包括：
- en: '**Feature transformations**: Standardization, normalization and others'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征转换**：标准化、归一化等'
- en: ML Pipeline construction
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML 管道构建
- en: Model evaluation and hyper-parameter tuning
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估和超参数调整
- en: '**ML persistence**: Saving and loading models and Pipelines'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ML 持久化**：保存和加载模型与管道'
- en: The architecture of Spark
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 的架构
- en: 'Spark consists of 3 primary architectural components:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 由三个主要的架构组件组成：
- en: The SparkSession / SparkContext
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkSession / SparkContext
- en: The Cluster Manager
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群管理器
- en: The Worker Nodes (that hosts executor processes)
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行进程所在的工作节点
- en: The **SparkSession/SparkContext**, or more generally the Spark Driver, is the
    entry point for all Spark applications as discussed earlier. The SparkContext
    will be used to create RDDs and perform operations against RDDs. The SparkDriver
    sends instructions to the worker nodes to schedule tasks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**SparkSession/SparkContext**，或更广义的 Spark Driver，是所有 Spark 应用程序的入口点。如前所述，SparkContext
    将用于创建 RDD 并对 RDD 执行操作。SparkDriver 向工作节点发送指令以调度任务。'
- en: The **Cluster manager** is conceptually similar to Resource Managers in Hadoop
    and indeed, one of the supported solutions is YARN. Other Cluster Managers include
    Mesos. Spark can also operate in a Standalone mode in which case YARN/Mesos are
    not required. Cluster Managers co-ordinate communications between the Worker Nodes,
    manage the nodes (such as starting, stopping, and so on), and perform other administration
    tasks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**集群管理器** 在概念上类似于 Hadoop 中的资源管理器，事实上，支持的解决方案之一是 YARN。其他集群管理器包括 Mesos。Spark
    还可以在独立模式下运行，此时不需要 YARN/Mesos。集群管理器协调工作节点之间的通信，管理节点（如启动、停止等），并执行其他管理任务。'
- en: '**Worker nodes** are servers where Spark applications are hosted. Each application
    gets its own unique **executor process**, namely, processes that perform the actual
    action and transformation tasks. By assigning dedicated executor processes, Spark
    ensures that an issue in any particular application does not impact other applications.
    Worker Nodes consist of the Executor, the JVM, and the Python/R/other application
    process required by the Spark application. Note that in the case of Hadoop, the
    Worker Node and Data Nodes are one and the same:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**工作节点**是托管 Spark 应用程序的服务器。每个应用程序都有一个独特的 **执行器进程**，即执行实际操作和转换任务的进程。通过分配专用的执行器进程，Spark
    确保某个特定应用程序中的问题不会影响其他应用程序。工作节点由执行器、JVM 和 Spark 应用程序所需的 Python/R/其他应用进程组成。请注意，在
    Hadoop 中，工作节点和数据节点是同一个概念：'
- en: '![](img/820c15f2-b785-4035-9227-69df5fcfba24.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/820c15f2-b785-4035-9227-69df5fcfba24.png)'
- en: Spark solutions
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 解决方案
- en: Spark is directly available from [spark.apache.org](http://spark.apache.org/)
    as an open-source solution. **Databricks** is the leading provider of the commercial
    solution of Spark. For those who are familiar with programming in Python, R, Java,
    or Scala, the time required to start using Spark is minimal due to efficient interfaces,
    such as the PySpark API that allows users to work in Spark using just Python.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 作为开源解决方案，可以直接从 [spark.apache.org](http://spark.apache.org/) 获取。**Databricks**
    是 Spark 商业解决方案的领先提供商。对于那些熟悉 Python、R、Java 或 Scala 编程的人来说，由于高效的接口（如 PySpark API
    允许用户仅用 Python 即可在 Spark 中工作），开始使用 Spark 所需的时间最小。
- en: Cloud-based Spark platforms, such as the Databricks Community Edition, provide
    an easy and simple means to work on Spark without the additional work of installing
    and configuring Spark. Hence, users who wish to use Spark for programming and
    related tasks can get started much more rapidly without spending time on administrative
    tasks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 基于云的 Spark 平台，如 Databricks 社区版，提供了一种简单易用的方式来使用 Spark，而无需额外的安装和配置工作。因此，想要使用 Spark
    进行编程及相关任务的用户，可以在不花费时间进行管理任务的情况下迅速上手。
- en: Spark practicals
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 实践
- en: In this section, we will create an account on Databricks' Community Edition
    and complete a hands-on exercise that will walk the reader through the basics
    of actions, transformations, and RDD concepts in general.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个 Databricks 社区版账户，并完成一个实践练习，带领读者了解一般的操作、转换以及 RDD 概念的基础。
- en: Signing up for Databricks Community Edition
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注册 Databricks 社区版
- en: 'The following steps outline the process of signing up for the **Databricks
    Community Edition**:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤概述了注册 **Databricks 社区版** 的过程：
- en: 'Go to [https://databricks.com/try-databricks](https://databricks.com/try-databricks):'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [https://databricks.com/try-databricks](https://databricks.com/try-databricks)：
- en: '![](img/b32b5600-60df-4eb1-8bc7-fbef99fc7302.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b32b5600-60df-4eb1-8bc7-fbef99fc7302.png)'
- en: 'Click on the START TODAY button and enter your information:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“今天开始”按钮并输入您的信息：
- en: '![](img/24ce51e3-d0a6-4572-a80c-226886b59f14.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24ce51e3-d0a6-4572-a80c-226886b59f14.png)'
- en: 'Confirm that you have read and agree to the terms in the popup menu (scroll
    down to the bottom for the **Agree** button):'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认您已阅读并同意弹出菜单中的条款（向下滚动至底部以找到**同意**按钮）：
- en: '![](img/08c99e13-c3ff-49b6-932a-8786ce3112d8.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08c99e13-c3ff-49b6-932a-8786ce3112d8.png)'
- en: 'Check your email for a confirmation email from Databricks and click on the
    link to confirm your account:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查您的电子邮件，查收来自 Databricks 的确认邮件，并点击其中的链接确认您的账户：
- en: '![](img/fa9ccec1-86a7-4b4e-898b-ebcf8fe525a7.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa9ccec1-86a7-4b4e-898b-ebcf8fe525a7.png)'
- en: 'Once you click on the link to confirm your account, you''ll be taken to a login
    screen where you can log on using the email address and password you used to sign
    up for the account:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击确认账户链接后，您将被引导到一个登录界面，您可以使用注册账户时的电子邮件地址和密码进行登录：
- en: '![](img/b77a0d6c-f43a-4646-a1f2-6cac2abd0f82.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b77a0d6c-f43a-4646-a1f2-6cac2abd0f82.png)'
- en: 'After logging in, click on Cluster to set up a Spark cluster, as shown in the
    following figure:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，点击“集群”以设置 Spark 集群，如下图所示：
- en: '![](img/ec9a4931-343a-4856-8453-81359dc3cdc2.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec9a4931-343a-4856-8453-81359dc3cdc2.png)'
- en: 'Enter `Packt_Exercise` as the Cluster Name and click on the Create Cluster
    button at the top of the page:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入 `Packt_Exercise` 作为集群名称，然后点击页面顶部的“创建集群”按钮：
- en: '![](img/4c630ae4-a0ae-42e5-8e38-9be37c9b24a5.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c630ae4-a0ae-42e5-8e38-9be37c9b24a5.png)'
- en: This will initiate the process of starting up a Spark Cluster on which we will
    execute our Spark commands using an iPython notebook. An iPython Notebook is the
    name given to a commonly used IDE - a web-based development application used for
    writing and testing Python code. The notebook can also support other languages
    through the use of kernels, but for the purpose of this exercise, we will focus
    on the Python kernel.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将启动一个Spark集群，我们将在上面使用iPython笔记本执行Spark命令。iPython笔记本是一个常用的集成开发环境（IDE）——一个基于Web的开发应用程序，用于编写和测试Python代码。该笔记本还可以通过使用内核支持其他语言，但在本教程中，我们将专注于Python内核。
- en: 'After a while, the Status will change from Pending to Running:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 稍等一会儿，状态将从“待处理”变为“运行中”：
- en: '![](img/c97289e0-9db2-4a45-849f-3062f7f69040.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c97289e0-9db2-4a45-849f-3062f7f69040.png)'
- en: 'Status changes to Running after a few minutes:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 状态将在几分钟后变为“运行中”：
- en: '![](img/c7149a7e-15b2-4c4f-99ce-332c20841de1.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7149a7e-15b2-4c4f-99ce-332c20841de1.png)'
- en: 'Click on **Workspace** (on the left hand bar) and select **options**, **Users**
    | (`Your userid`) and click on the drop-down arrow next to your email address.
    Select Create | Notebook:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**工作区**（在左侧栏），选择**选项**，**用户** | (`您的用户ID`)，然后点击您电子邮件地址旁边的下拉箭头。选择“创建”|“笔记本”：
- en: '![](img/972a70c7-9846-48d0-83bf-c64f87eb3e3f.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/972a70c7-9846-48d0-83bf-c64f87eb3e3f.png)'
- en: 'In the popup screen, enter `Packt_Exercise` as the name of the notebook and
    click on the Create button:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在弹出窗口中，输入`Packt_Exercise`作为笔记本名称，并点击“创建”按钮：
- en: '![](img/05754e5e-5029-4729-bff4-41b883ced6c8.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05754e5e-5029-4729-bff4-41b883ced6c8.png)'
- en: 'Once you click on the **Create** button, you''ll be taken directly to the Notebook
    as shown in the following screenshot. This is the Spark Notebook, where you''ll
    be able to execute the rest of the code given in the next few sections. The code
    should be typed in the cells of the notebook as shown. After entering your code,
    press *Shift + Enter* to execute the corresponding cell:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦点击**创建**按钮，您将直接进入如下所示的笔记本。这是Spark笔记本，您将在这里执行接下来的代码。代码应输入在笔记本的单元格中，如下所示。输入代码后，按*Shift
    + Enter*来执行对应的单元格：
- en: '![](img/fa59787f-7c2f-4ed5-a0e6-62c84d6f76a9.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa59787f-7c2f-4ed5-a0e6-62c84d6f76a9.png)'
- en: For the next few exercises, you can copy-paste the text into the cells of the
    Notebook. Alternatively, you can also import the notebook and load it directly
    in your workspace. If you do so, you'll not need to type in the commands (although
    typing in the commands will provide more hands-on familiarity).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的几个练习中，您可以将文本复制粘贴到笔记本的单元格中。或者，您也可以导入笔记本并直接加载到您的工作区。如果这样做，您将不需要输入命令（尽管输入命令会让您更熟悉操作）。
- en: 'An alternative approach to copy-pasting commands: You can import the notebook
    by clicking on Import as shown in the following screenshot:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制粘贴命令的替代方法：您可以通过点击“导入”来导入笔记本，如下所示：
- en: '![](img/8e1545cd-bf1c-43a7-9309-4eb40b781d71.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e1545cd-bf1c-43a7-9309-4eb40b781d71.png)'
- en: 'Enter the following **URL** in the popup menu (select **URL** as the **Import
    from** option):'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在弹出菜单中输入以下**URL**（选择**从**导入为**URL**选项）：
- en: '![](img/db82dd15-c748-4433-bf0e-96f8a1d820d8.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db82dd15-c748-4433-bf0e-96f8a1d820d8.png)'
- en: 'The notebook will then show up under your email ID. Click on the name of the
    notebook to load it:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 笔记本将显示在您的电子邮件ID下。点击笔记本名称以加载它：
- en: '![](img/e3ab20dd-cf19-40cd-9e8e-e32c3faed7e5.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3ab20dd-cf19-40cd-9e8e-e32c3faed7e5.png)'
- en: Spark exercise - hands-on with Spark (Databricks)
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark练习 - 与Spark动手实践（Databricks）
- en: This notebook is based on tutorials conducted by Databricks ([https://databricks.com/](https://databricks.com/)).
    The tutorial will be conducted using the Databricks' Community Edition of Spark,
    available to sign up to at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
    Databricks is a leading provider of the commercial and enterprise supported version
    of Spark.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本笔记本基于Databricks提供的教程 ([https://databricks.com/](https://databricks.com/))。教程将使用Databricks的社区版Spark进行，用户可以通过[https://databricks.com/try-databricks](https://databricks.com/try-databricks)进行注册。Databricks是Spark的商业版和企业版的领先提供商。
- en: In this tutorial, we will introduce a few basic commands used in Spark. Users
    are encouraged to try out more extensive Spark tutorials and notebooks that are
    available on the web for more detailed examples.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将介绍一些Spark中的基本命令。我们鼓励用户尝试更多的Spark教程和笔记本，网上有许多更详细的示例。
- en: Documentation for Spark's Python API can be found at [https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.sql](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.sql).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的Python API文档可以在[https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.sql](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.sql)找到。
- en: The data for this book was imported into the Databricks' Spark Platform. For
    more information on importing data, go to **Importing Data** - **Databricks**
    ([https://docs.databricks.com/user-guide/importing-data.html](https://docs.databricks.com/user-guide/importing-data.html)).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的数据已导入Databricks的Spark平台。有关导入数据的更多信息，请参阅**导入数据** - **Databricks** ([https://docs.databricks.com/user-guide/importing-data.html](https://docs.databricks.com/user-guide/importing-data.html))。
- en: '[PRE0]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Summary
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we read about some of the core features of Spark, one of the
    most prominent technologies in the Big Data landscape today. Spark has matured
    rapidly since its inception in 2014, when it was released as a Big Data solution
    that alleviated many of the shortcomings of Hadoop, such as I/O contention and
    others.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们阅读了关于Spark的一些核心特性，Spark是当今大数据领域最突出的一项技术之一。自2014年推出以来，Spark迅速成熟，成为一种大数据解决方案，缓解了Hadoop的许多不足之处，如I/O争用等问题。
- en: Today, Spark has several components, including dedicated ones for streaming
    analytics and machine learning, and is being actively developed. Databricks is
    the leading provider of the commercially supported version of Spark and also hosts
    a very convenient cloud-based Spark environment with limited resources that any
    user can access at no charge. This has dramatically lowered the barrier to entry
    as users do not need to install a complete Spark environment to learn and use
    the platform.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，Spark拥有多个组件，包括专门用于流式分析和机器学习的组件，并且正在积极开发中。Databricks是Spark商业支持版本的领先提供商，还提供了一个非常方便的基于云的Spark环境，任何用户都可以免费访问，尽管资源有限。这大大降低了入门门槛，因为用户无需安装完整的Spark环境就能学习和使用该平台。
- en: In the next chapter, we will begin our discussion on machine learning. Most
    of the text, until this section, has focused on the management of large scale
    data. Making use of the data effectively and gaining *insights* from the data
    is always the final aim. In order to do so, we need to employ the advanced algorithmic
    techniques that have become commonplace today. The next chapter will discuss the
    basic tenets of machine learning, and thereafter we will delve deeper into the
    subject area in the subsequent chapter.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始讨论机器学习。在本节之前的大部分内容都集中在大规模数据的管理上。有效利用数据并从中获取*洞察*始终是最终目标。为了做到这一点，我们需要采用今天已经普及的高级算法技术。下一章将讨论机器学习的基本原则，之后我们将在随后的章节中深入探讨该领域。
