- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Probabilistic Forecasting and More
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率预测及更多
- en: Throughout the book, we have learned different techniques to generate a forecast,
    including some classical methods, using machine learning, and some deep learning
    architectures as well. But we have been focusing on one typical type of forecasting
    problem—generating a point forecast for continuous time series with no hierarchy
    and a good amount of history. We have been doing that because this is the most
    popular kind of problem you will face. But in this chapter, we will take some
    time to look at a few niche topics that, although less popular, are no less important.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们学习了生成预测的不同技术，包括一些经典方法，使用机器学习以及一些深度学习架构。但我们一直在关注一种典型的预测问题——为连续时间序列生成点预测，并且没有层级关系且历史数据足够丰富。我们之所以这样做，是因为这是你会遇到的最常见的问题。但在本章中，我们将花一些时间讨论几个利基话题，尽管这些话题的受关注程度较低，但同样重要。
- en: 'In this chapter, we will focus on these topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点讨论以下主题：
- en: Probabilistic forecasting
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率预测
- en: Probability Density Functions
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率密度函数
- en: Quantile functions
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分位数函数
- en: Monte Carlo Dropout
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡洛 Dropout
- en: Conformal Prediction
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一致性预测
- en: Intermittent/sparse time series forecasting
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 间歇性/稀疏时间序列预测
- en: Interpretability
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性
- en: Cold-start forecasting
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冷启动预测
- en: Pre-trained models like TimeGPT
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练模型，如 TimeGPT
- en: Similarity-based forecasting
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于相似度的预测
- en: Hierarchical forecasting
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层级预测
- en: Probabilistic forecasting
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率预测
- en: So far, we have been talking about the forecast as a single number. We have
    been projecting our DL models to a single dimension or training our machine learning
    models to output a single number. Subsequently, we were training the model using
    a loss, such as mean squared loss. This paradigm is what we call a *point forecast*.
    But we are not considering one important aspect. We are using the history to train
    our model to make the best guess. But how sure is the model about its prediction?
    Those of you who are aware of machine learning and classification problems would
    recognize that for classification problems, besides getting a prediction of which
    class the sample belongs to, we also get a notion of the uncertainty of the model.
    But our forecasting is a regression problem and we don’t get the uncertainty for
    free.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在讨论将预测作为一个单一的数字。我们一直在将我们的深度学习模型投射到一个单一维度，或者训练我们的机器学习模型输出一个单一的数字。随后，我们使用如均方误差之类的损失函数来训练模型。这种范式就是我们所说的*点预测*。但是我们没有考虑到一个重要方面。我们正在使用历史数据来训练模型，以做出最佳预测。但是模型对于其预测有多确定呢？那些熟悉机器学习和分类问题的人会意识到，对于分类问题，除了得到样本属于哪个类别的预测，我们还会得到模型的不确定性。可是我们的预测是回归问题，我们并没有免费的不确定性。
- en: But why is quantifying uncertainty important in forecasting? Any forecast is
    created for some purpose, some downstream task for which the forecasted information
    is being used. In other words, there is some decision that has to be taken using
    the forecast we generate. And when making a decision, we usually would like to
    have the maximum amount of information available to us.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么在预测中量化不确定性是重要的呢？任何预测都是为了某个目的创建的，即某个下游任务，需要使用预测信息。换句话说，我们生成的预测是为了做出某些决策。而在做决策时，我们通常希望能够获得尽可能多的信息。
- en: Let’s look at an example to really drive home the point. You have recorded your
    monthly grocery consumption for the last 5 years and, using the techniques in
    the book, created a super accurate forecast and an app that tells you how much
    to shop for in any month. You open up the app and it tells you that you need to
    buy two bread loaves this month. You head out to the supermarket, bag two loaves
    of bread, and return home. And a week before the end of the month, the bread loaves
    ran out, and you starved the rest of the time. At that peak of starvation, you
    started questioning your decisions and the forecast. You analyzed the data to
    figure out where you went wrong and realized that your consumption of bread per
    month varied a lot. In some months you consumed 4 loaves and, in some other months,
    it was just 1\. So, there is a good chance that this forecast will leave you with
    no bread for a few months and with excess bread for a few other months.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，来真正强调这一点。你已经记录了过去5年每月的食品消费量，并使用书中的技术，创建了一个超精准的预测和一个应用程序，告诉你每个月需要购买多少食品。你打开应用程序，它告诉你这个月需要购买2个面包。你前往超市，拿了两个面包，然后回家。结果在月底前一周，面包就用完了，剩下的时间你饿着肚子。就在饿得快不行的时候，你开始质疑自己的决定和预测。你分析了数据，发现自己哪里出错了，意识到每个月的面包消费量变化很大。有些月份你吃了4个面包，而有些月份则只有1个。所以，很有可能这个预测会导致你某些月份没有面包，而其他月份则面包过剩。
- en: Then, you read this chapter and convert your forecast to a probabilistic forecast
    and now it tells you that 50% of the time your next month’s consumption of bread
    is 2 loaves. But now, there is an additional feature in the app, which asks you
    whether you prefer to starve or have excess bread at the end of the day. So, depending
    on your appetite to starve or save money, you decide on an option. Let’s say you
    don’t want to starve, but are okay if bread runs out like 10% of the time. Once
    you enter this preference into the app, it revises its forecast and tells you
    that you should get 3 loaves of bread, and you never starve again (also because
    you smartened up and bought other stuff from the supermarket).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你阅读了这一章，并将你的预测转化为概率预测，现在它告诉你，50%的情况下你下个月的面包消费量是2个。可是，现在应用程序中有一个附加功能，它询问你是更愿意挨饿，还是希望每天结束时有剩余的面包。所以，依据你对挨饿或节省金钱的接受程度，你做出了选择。假设你不想挨饿，但如果面包偶尔缺货，比如10%的情况，你也能接受。一旦你将这个偏好输入应用程序，它就会修正预测，并告诉你，你应该买3个面包，这样你就再也不会挨饿了（当然，也因为你更聪明了，从超市里买了其他东西）。
- en: The ability to use the uncertainty in the forecast and revise it according to
    our appetite for risk is one of the main utilities for probabilistic forecasting.
    It also helps our forecast to be more transparent and trustworthy to the users.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 能够利用预测中的不确定性，并根据我们对风险的接受度进行修正，是概率预测的主要实用功能之一。它还帮助我们的预测变得更加透明，并使用户更加信任。
- en: Now, let’s quickly look at the types of uncertainty one would have in a prediction
    problem using learned models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们快速看一下使用学习模型进行预测问题时可能出现的不确定性类型。
- en: Types of Predictive Uncertainty
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测不确定性类型
- en: We saw in *Chapter 5* that supervised machine learning is nothing but learning
    a function, ![](img/B22389_17_001.png), where *h*, along with ![](img/B22389_07_003.png),
    is the model that we learn and is the input data. So, if we think about the sources
    of uncertainty, it can be from two of these components.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第5章*中看到，监督式机器学习无非就是学习一个函数，![](img/B22389_17_001.png)，其中*h*以及![](img/B22389_07_003.png)是我们学习的模型，而它们的输入数据则是这些内容。所以，如果我们考虑不确定性的来源，它可以来自这两个组成部分中的任意一个。
- en: 'The model, *h*, we learned is an approximation using a dataset, *X*, which
    may or may not cover all the cases completely and some uncertainty can be introduced
    to the system from this. We call this **Epistemic Uncertainty**. In the context
    of machine learning, epistemic uncertainty can occur when the model has not been
    exposed to enough data, the model itself is insufficient to learn the complexity
    of the problem, or the data it has been trained on does not represent all possible
    scenarios. This is also known as systemic or reducible uncertainty because this
    is the portion of the total predictive uncertainty that can be actively reduced
    by having better models, better data, and so on; in other words, by gaining more
    knowledge about the system. Let’s see a few examples to make the concept clearer:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所学习的模型，*h*，是使用数据集*X*进行近似的，这个数据集可能并没有完全覆盖所有情况，因此可能会从中引入一些不确定性。我们称之为**认识性不确定性**。在机器学习的背景下，当模型没有接触到足够的数据、模型本身不足以学习问题的复杂性，或者它所训练的数据不能代表所有可能的情境时，就会出现认识性不确定性。这也被称为系统性或可减少不确定性，因为这是总预测不确定性中可以通过更好的模型、更好的数据等方式主动减少的部分；换句话说，就是通过获得更多关于系统的知识来减少它。让我们通过几个例子来更清楚地理解这一概念：
- en: If a weather forecast model has less data from a particular region (maybe because
    of a faulty sensor), there is going to be less knowledge about that region, and
    this increases uncertainty.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个天气预测模型来自某个特定区域的数据较少（可能是因为传感器故障），那么对该区域的了解将会减少，从而增加不确定性。
- en: If a linear model was used for a non-linear problem, we would be introducing
    some uncertainty because of the simpler model having less knowledge about the
    system.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果将线性模型应用于非线性问题，那么由于更简单的模型对系统了解较少，我们就会引入一些不确定性。
- en: If an economic forecasting model is not trained using a few key influencing
    factors like change in economic policies, climate change influencing the decisions,
    and so on, this also creates some uncertainty.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个经济预测模型没有使用一些关键的影响因素进行训练，例如经济政策的变化、气候变化对决策的影响等等，这也会产生一些不确定性。
- en: The good thing about this kind of uncertainty is that this is something that
    we can actively reduce by collecting better data, training better models, and
    so on.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不确定性的一大优点是，我们可以通过收集更好的数据、训练更好的模型等方式主动减少它。
- en: Now there is another kind of uncertainty in the total predictive uncertainty—**Aleatoric
    Uncertainty**. This refers to the inherent randomness in the data which cannot
    be explained away. This is also known as statistical or irreducible uncertainty.
    Although our universe appears deterministic to us, there is an ever-prevalent
    layer of uncertainty just beneath the surface.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在总预测不确定性中还有另一种不确定性——**Aleatoric不确定性**。这指的是数据中固有的随机性，是无法通过解释来消除的。这也被称为统计性或不可约不确定性。尽管我们的宇宙对我们来说似乎是决定论的，但在表面之下始终存在一层普遍的不确定性。
- en: 'For example, the motion of the celestial objects can be calculated accurately
    (thanks to General Relativity and Einstein), but still, a random asteroid can
    hit any of the bodies and cause alterations to the calculated trajectory. This
    kind of irreducible and unavoidable uncertainty is referred to as aleatoric uncertainty.
    Let’s see a few examples:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，天体的运动可以准确计算（感谢广义相对论和爱因斯坦），但仍然有可能随机的小行星撞击任何天体并改变计算出的轨迹。这种不可约且无法避免的不确定性被称为Aleatoric不确定性。让我们来看几个例子：
- en: Weather predictions, no matter how accurate the measurements and the models
    are, are still variable. There is an inherent randomness in weather that we might
    never be able to completely explain.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论天气预测的测量和模型多么精确，仍然存在可变性。天气中有一种固有的随机性，我们可能永远无法完全解释清楚。
- en: The performance of an athlete, no matter how much they have trained and followed
    the rules, is still not completely deterministic. There are a lot of factors,
    like weather, health, and other random events during or before the game, which
    will affect the performance of the athlete.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个运动员的表现，无论他们训练得多么努力并遵循规则，仍然不是完全决定性的。很多因素，如天气、健康以及比赛期间或之前的其他随机事件，都可能影响运动员的表现。
- en: Now that we understand the different types of uncertainty, and why we need uncertainty
    quantification, let’s see what it means in the context of forecasting.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了不同类型的不确定性，以及为什么我们需要不确定性量化，让我们看看在预测的背景下它意味着什么。
- en: What are probabilistic forecasts and Prediction Intervals?
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是概率预测和预测区间？
- en: A probabilistic forecast is when the forecast, instead of having a single-point
    prediction, captures the uncertainty of that forecast as well. Probabilistic forecasting
    is a method of predicting future events or outcomes by providing a range of possible
    values along with associated probabilities or confidence levels. This approach
    captures the uncertainty in the prediction process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 概率预测是指预测不仅给出单点预测结果，还能够捕捉预测的不确定性。概率预测是一种通过提供一系列可能值及其相关概率或置信水平来预测未来事件或结果的方法。这种方法捕捉了预测过程中的不确定性。
- en: In the econometrics and classical time series world, the prediction intervals
    were already baked into the formulation. The statistical grounding and strong
    assumptions of those methods made sure that the output of those models was readily
    interpreted in a probabilistic way as well (so long as you could satisfy the assumptions
    that were stipulated by those models). But in the modern machine learning/deep
    learning world, probabilistic forecasting is not an afterthought. A combination
    of factors, such as fewer rigid assumptions and the way we train the models, leads
    to this predicament.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在计量经济学和经典时间序列领域，预测区间已经内嵌在公式中。这些方法的统计基础和强假设确保了模型输出能够以概率的方式进行解释（只要满足这些模型规定的假设）。但在现代机器学习/深度学习领域，概率预测不是事后考虑的。多种因素的组合，如较少的严格假设以及我们训练模型的方式，导致了这种局面。
- en: There are different methods with which we can add the probabilistic dimension
    to our forecast, and we will go through a few of them in this chapter. But before
    that, let’s also understand one of the most useful manifestations of probabilistic
    forecasts—**Prediction Intervals**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过不同的方法将概率维度加入到预测中，本章将讨论其中的一些方法。但在此之前，让我们首先理解概率预测的一种最有用的表现形式——**预测区间**。
- en: 'A Prediction Interval is a range within which a future observation is expected
    to fall with a specified probability. For instance, if we have a 95% prediction
    interval for a time step of `[5,8]`, we say that 95% of the time, the actual value
    will lie between 5 and 8\. Let’s take an example of a normal distribution with
    mean, ![](img/B22389_17_003.png), and variance, ![](img/B22389_17_004.png) as
    the forecast at timestep, *t* (one of the techniques we will talk about gives
    us just that). So, the prediction interval at time, *t*, with a significance level
    (the probability with which the forecast can fall outside the interval), ![](img/B22389_04_009.png),
    can be written as:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 预测区间是一个范围，未来的观测值预计会以特定概率落在该范围内。例如，如果我们有一个95%的预测区间为 `[5,8]`，我们可以说，95%的情况下，实际值会落在5到8之间。我们以一个正态分布为例，假设其均值为
    ![](img/B22389_17_003.png)，方差为 ![](img/B22389_17_004.png)，这是我们在时间步长 *t* 时的预测（我们将在接下来的内容中讨论其中一种方法，可以给我们这样的结果）。因此，时间
    *t* 时的预测区间，假设显著性水平（即预测值可能落在区间外的概率）为 ![](img/B22389_04_009.png)，可以表示为：
- en: '![](img/B22389_17_006.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_006.png)'
- en: where *z* is the z-score of the normal distribution.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *z* 是正态分布的 z 分数。
- en: '**Prediction Interval** (**PI**) vs **Confidence Interval** (**CI**)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测区间** (**PI**) 与 **置信区间** (**CI**)'
- en: One of the most confusing topics is prediction intervals and confidence intervals.
    Let’s demystify them here. These are both ways we quantify uncertainty, but they
    serve different purposes and are interpreted differently in the context of forecasting.
    A confidence interval also provides a range, but for a population parameter (like
    the mean) of the sample data, whereas a prediction interval focuses on providing
    a range for a future observation. One of the key differences between the two is
    that CIs are, typically, narrower than PIs because PIs also account for the uncertainty
    of the new point. On the other hand, CIs only account for the uncertainty accounted
    to the model parameters. So, we use PIs when we need to give a range for when
    a future observation is likely to fall, like the next month’s sales. CIs are used
    when we need to provide a range for an estimated parameter, like the estimated
    average demand over a year.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最令人困惑的主题之一是预测区间和置信区间。让我们在这里解开它们的迷雾。它们都是量化不确定性的方法，但在预测的背景下，它们服务于不同的目的，并且解释方式不同。置信区间提供一个范围，但针对样本数据的总体参数（如均值），而预测区间则专注于为未来的观测值提供一个范围。两者的一个关键区别是，置信区间通常比预测区间更窄，因为预测区间还考虑了新点的不确定性。另一方面，置信区间只考虑模型参数的不确定性。因此，当我们需要给出未来某个观测值可能落入的范围（比如下个月的销售额）时，我们使用预测区间。当我们需要为一个估计参数（比如年均需求量）提供一个范围时，我们使用置信区间。
- en: There are a few terms and concepts we need to clarify before we begin the discussion
    in detail.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细讨论之前，我们需要澄清一些术语和概念。
- en: Confidence levels, error rates, and quantiles
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 置信水平、误差率和分位数
- en: When working with prediction intervals, it’s crucial to understand the relationship
    between confidence levels, error rates, and quantiles. These concepts help in
    determining the range within which future observations are expected to fall with
    a certain probability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理预测区间时，了解置信水平、误差率和分位数之间的关系至关重要。这些概念有助于确定未来观测值在一定概率范围内的预期区间。
- en: '*Error Rate* (![](img/B22389_04_009.png)) is the allowable probability that
    the prediction interval will not contain the future observation. It is typically
    expressed in percentages or as a decimal between 0 and 1\. If we say ![](img/B22389_17_008.png)
    or ![](img/B22389_17_009.png), it means that there is a 10% chance that the future
    observation will not be in the prediction interval.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*误差率* (![](img/B22389_04_009.png)) 是预测区间不包含未来观测值的允许概率。它通常以百分比或0到1之间的小数表示。如果我们说
    ![](img/B22389_17_008.png) 或 ![](img/B22389_17_009.png)，这意味着未来的观测值有10%的机会不在预测区间内。'
- en: '*Confidence Level* (![](img/B22389_17_010.png)) is the complement of error
    rate and is the probability that the prediction interval contains the future observation.
    ![](img/B22389_17_011.png). If we say that the error rate is 10%, the confidence
    level would be 90%.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*置信水平* (![](img/B22389_17_010.png)) 是误差率的补数，它是预测区间包含未来观测值的概率。![](img/B22389_17_011.png)。如果我们说误差率是10%，那么置信水平将是90%。'
- en: '*Quantiles* are points that divide the data into intervals with equal probabilities.
    In simpler terms, a quantile shows the value below which a certain percentage
    of data falls. For instance, the 5th percentile or 0.05th quantile marks the point
    where 5% of data lies below it. Therefore, we can use quantiles as well to define
    the prediction intervals where we don’t have an analytical way to get the prediction
    intervals based on distributional assumption.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*分位数* 是将数据分成具有相等概率的区间的点。简单来说，分位数表示一个值，低于该值的数据占某个百分比。例如，第5百分位数或0.05分位数标志着5%的数据位于该值以下。因此，当我们没有基于分布假设的解析方法来获得预测区间时，也可以使用分位数来定义预测区间。'
- en: In *Figure 17.1*, we show the prediction intervals for a standard normal distribution.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图17.1*中，我们展示了标准正态分布的预测区间。
- en: '![](img/B22389_17_01.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_01.png)'
- en: 'Figure 17.1: Prediction intervals for a standard normal distribution'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1：标准正态分布的预测区间
- en: There is a strong link between error rates, confidence levels, and quantiles.
    Error rates and confidence levels are direct complements to each other and can
    be used interchangeably to define the confidence that we want our prediction intervals
    to have or the error rate we are okay with from the prediction intervals. Another
    way to look at it is by the area under the curve. In *Figure 17.1*, the area of
    the green-shaded region denotes the confidence level, and the area of the red
    area denotes the error rate.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 错误率、置信水平和分位数之间有着密切的关系。错误率和置信水平是彼此的直接补充，可以互换使用，以定义我们希望预测区间具有的置信度或我们可以接受的预测区间的误差率。另一种看法是通过曲线下的面积。在*图17.1*中，绿色阴影区域的面积表示置信水平，而红色区域的面积表示误差率。
- en: 'For a standard normal distribution, we can directly get the prediction intervals
    by using the analytical formula:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标准正态分布，我们可以直接通过使用解析公式获得预测区间：
- en: '![](img/B22389_17_012.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_012.png)'
- en: where ![](img/B22389_03_003.png) is the mean of the distribution, ![](img/B22389_03_004.png)
    is the standard deviation of the distribution, and ![](img/B22389_17_015.png)
    is the critical value from the standard normal distribution corresponding to the
    desired confidence level, ![](img/B22389_17_016.png). ![](img/B22389_17_017.png)
    is taken because we are allowing for the error rate to be spread on both sides
    (red shaded area on both sides of the curve in *Figure 17.1*).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中！[](img/B22389_03_003.png)是分布的均值，！[](img/B22389_03_004.png)是分布的标准差，！[](img/B22389_17_015.png)是对应所需置信水平的标准正态分布临界值，！[](img/B22389_17_016.png)。取！[](img/B22389_17_017.png)是因为我们允许误差率在两侧分布（*图17.1*中的曲线两侧的红色阴影区域）。
- en: 'Now let’s look at how error rates and confidence levels are linked with quantiles
    because if we don’t know what the distribution is (and we don’t want to assume
    any distribution), we can’t go by the analytical formula to get prediction intervals.
    In such cases, we can use quantiles to get the same. Just like we did with the
    analytical formula, the error rate, ![](img/B22389_04_009.png), should be equally
    divided across both sides. And therefore, the prediction interval would be:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看错误率和置信水平是如何与分位数相联系的，因为如果我们不知道分布是什么（并且我们不想假设任何分布），我们就无法通过解析公式获得预测区间。在这种情况下，我们可以使用分位数来获得相同的结果。就像我们使用解析公式时一样，错误率！[](img/B22389_04_009.png)应该在两侧均匀分配。因此，预测区间将是：
- en: '![](img/B22389_17_019.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_019.png)'
- en: where ![](img/B22389_17_020.png) is the *t*^(th) quantile. So, from the definition
    of quantile, we know ![](img/B22389_17_021.png) would have ![](img/B22389_17_017.png)
    % of data below it and ![](img/B22389_17_023.png) would have ![](img/B22389_17_017.png)
    % of data above it, thus making the area outside the intervals to be ![](img/B22389_04_009.png).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中！[](img/B22389_17_020.png)是*t*^(th)分位数。所以，从分位数的定义，我们知道！[](img/B22389_17_021.png)将有！[](img/B22389_17_017.png)
    %的数据低于它，！[](img/B22389_17_023.png)将有！[](img/B22389_17_017.png) %的数据高于它，从而使区间外的面积为！[](img/B22389_04_009.png)。
- en: 'Using this relation, we can go from error rates to quantiles or confidence
    levels to quantiles. If the error rate is ![](img/B22389_04_009.png) we have already
    seen what the corresponding quantiles denoting the prediction interval are. Let’s
    also see one more quick formula to go from confidence levels (declared as percentages)
    to quantiles:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这一关系，我们可以从错误率转化为分位数，或者从置信水平转化为分位数。如果错误率是！[](img/B22389_04_009.png)，我们已经看到对应的分位数表示预测区间。让我们再看一个简单的公式，将置信水平（以百分比表示）转化为分位数：
- en: '![](img/B22389_17_027.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_027.png)'
- en: 'In Python code, this is simply:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python代码中，这仅仅是：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, let’s look at how to measure the goodness of prediction intervals.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下如何衡量预测区间的优良性。
- en: Measuring the goodness of prediction intervals
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 衡量预测区间的优良性
- en: We know what a probabilistic forecast is and what prediction intervals are.
    But before we look at techniques to generate prediction intervals, we need a way
    to measure the goodness of such an interval. Standard metrics like Mean Absolute
    Error or Mean Squared Error hold no more because they are point forecast measuring
    metrics.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道什么是概率预测，以及什么是预测区间。但在我们查看生成预测区间的技术之前，我们需要一种方法来衡量这种区间的优良性。标准指标如平均绝对误差或均方误差不再适用，因为它们是点预测衡量指标。
- en: What do we want from a prediction interval? If we have a prediction interval
    with 90% confidence, we would expect the data points to lie between the interval
    at least 90% of the time. This can easily be obtained by having very wide prediction
    intervals, but that again becomes a useless prediction interval. So, we want our
    prediction interval to be as narrow as possible and still have the 90% confidence
    criteria respected. To measure these two distinct aspects, we can use two metrics—**Coverage**
    and **Average Length of Prediction Intervals**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从预测区间中想要得到什么？如果我们有一个90%置信度的预测区间，我们希望数据点至少有90%的时间落在该区间内。这可以通过设置非常宽的预测区间来轻松获得，但那样预测区间就变得没有用。因此，我们希望预测区间尽可能窄，同时仍能满足90%的置信度要求。为了衡量这两个不同的方面，我们可以使用两个指标——**覆盖率**和**预测区间的平均长度**。
- en: '**Coverage** is the proportion of true values that fall within the prediction
    intervals. Mathematically, if we denote the Prediction Interval by ![](img/B22389_17_028.png)
    for each observation *i*, and true value by ![](img/B22389_17_029.png), coverage
    can be defined as:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**覆盖率**是指真实值落入预测区间的比例。从数学上讲，如果我们用![](img/B22389_17_028.png)表示每个观察值*i*的预测区间，用![](img/B22389_17_029.png)表示真实值，则覆盖率可以定义为：'
- en: '![](img/B22389_17_030.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_030.png)'
- en: where ![](img/B22389_17_031.png) is an indicator function that equals 1 if the
    condition inside is true and 0 otherwise, and *N* is the total number of observations.
    A coverage metric close to the desired confidence level (e.g., 95% for a 95% prediction
    interval) indicates that the model’s uncertainty estimates are well-calibrated.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/B22389_17_031.png)是一个指示函数，如果内部条件为真，则值为1，否则为0，*N*是观察值的总数。接近所需置信水平（例如，95%的预测区间）的覆盖率指标表明模型的不确定性估计得到了很好的校准。
- en: '**Average Length of Prediction Intervals** is calculated by averaging the lengths
    of the prediction intervals across all observations. Using the same notations
    as above, it can be mathematically written as:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测区间的平均长度**是通过对所有观察值的预测区间长度取平均值来计算的。使用与上面相同的符号，它可以用数学方式写成：'
- en: '![](img/B22389_17_032.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_032.png)'
- en: 'This metric helps in understanding the trade-off between the coverage of the
    intervals and their precision. Let’s also look at Python functions for both of
    these metrics:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指标有助于理解区间的覆盖率与其精度之间的权衡。我们还将看一下这两个指标的Python函数：
- en: '**Coverage**:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**覆盖率**：'
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Average Length**:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均长度**：'
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Both of these Python functions can be found in `src/utils/ts_utils.py` and we
    will be using them to measure the quality of prediction intervals generated in
    the chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个Python函数可以在`src/utils/ts_utils.py`中找到，我们将在本章中使用它们来衡量生成的预测区间的质量。
- en: Now, let’s look at different techniques we can use to get probabilistic forecasts
    and how we can use them practically.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下可以用来获得概率预测的不同技术，以及如何实际使用它们。
- en: Probability Density Function (PDF)
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率密度函数（PDF）
- en: This is one of the most common techniques for probabilistic forecasting, especially
    in the deep learning space, because of the ease of implementation. The forecast
    at time *t*, ![](img/B22389_17_033.png), can be seen as the realization of a probability
    distribution, ![](img/B22389_17_034.png). And instead of estimating ![](img/B22389_17_033.png),
    we can estimate ![](img/B22389_17_034.png). If we assume ![](img/B22389_17_034.png)
    is one of the parametrized distributions, ![](img/B22389_17_038.png), with parameters
    ![](img/B22389_17_039.png), then we can estimate the parameters ![](img/B22389_17_039.png),
    instead of ![](img/B22389_17_033.png), directly.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是概率预测中最常用的技术之一，尤其是在深度学习领域，因为它的实现简单。在时间*t*的预测![](img/B22389_17_033.png)可以看作是概率分布![](img/B22389_17_034.png)的实现。与其估计![](img/B22389_17_033.png)，我们可以估计![](img/B22389_17_034.png)。如果我们假设![](img/B22389_17_034.png)是一个参数化的分布之一，![](img/B22389_17_038.png)，其参数为![](img/B22389_17_039.png)，那么我们可以直接估计参数![](img/B22389_17_039.png)，而不是估计![](img/B22389_17_033.png)。
- en: For instance, if we assume the forecast is drawn from normal distribution, we
    can model
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们假设预测来自正态分布，则我们可以建模为
- en: '![](img/B22389_17_042.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_042.png)'
- en: 'So, instead of getting our model to output ![](img/B22389_17_033.png), we can
    get it to output ![](img/B22389_17_003.png) and ![](img/B22389_17_045.png). And
    with ![](img/B22389_17_003.png) and ![](img/B22389_17_045.png), we can easily
    calculate the prediction intervals at the given ![](img/B22389_04_009.png):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，代替让我们的模型输出！[](img/B22389_17_033.png)，我们可以让它输出！[](img/B22389_17_003.png)和！[](img/B22389_17_045.png)。通过！[](img/B22389_17_003.png)和！[](img/B22389_17_045.png)，我们可以轻松计算给定！[](img/B22389_04_009.png)的预测区间：
- en: '![](img/B22389_17_049.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_049.png)'
- en: '![](img/B22389_17_050.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_050.png)'
- en: where *Z* is the critical value from the standard normal distribution corresponding
    to the desired confidence level. For a 90% confidence level, ![](img/B22389_17_051.png).
    Simple enough, right? Not so fast!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*Z*是来自标准正态分布的临界值，对应于所需的置信水平。对于90%的置信水平，！[](img/B22389_17_051.png)。够简单吧？等等，别急！
- en: Now that we are modeling the parameters of a distribution, how do we train the
    model? We still have the actual point forecast as the target. In the normal distribution
    case, the targets are still actual ![](img/B22389_17_052.png) and not the means
    and standard deviations. We get over this problem by using a loss function like
    **log likelihood**, instead of losses like Squared Error.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们正在对一个分布的参数进行建模，那么我们如何训练模型呢？我们仍然有实际的点预测作为目标。在正态分布的情况下，目标仍然是实际的！[](img/B22389_17_052.png)，而不是均值和标准差。我们通过使用像**对数似然度**这样的损失函数，而不是像平方误差这样的损失函数来解决这个问题。
- en: For instance, let’s say we have a set of *i.i.d* observations (in our case,
    the target), ![](img/B22389_17_053.png). With the predicted parameters of the
    assumed distribution (![](img/B22389_17_054.png)), we will be able to calculate
    the probability of each of the target, ![](img/B22389_17_055.png). The *i.i.d*
    assumption means each sample is independent of each other. And high-school mathematics
    tells us that when two independent events happen, we can calculate their joint
    probability by multiplying the two independent probabilities together. Using the
    same logic, we can calculate the joint probability or likelihood of all *n* *i.i.d*
    observations (probability that all these events occur) by just multiplying all
    the individual probabilities together.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一组*i.i.d*观测值（在我们这个例子中是目标值），！[](img/B22389_17_053.png)。通过假定分布的预测参数（！[](img/B22389_17_054.png)），我们将能够计算出每个目标值的概率，！[](img/B22389_17_055.png)。*i.i.d*假设意味着每个样本相互独立。高中数学告诉我们，当两个独立事件发生时，我们可以通过将两个独立的概率相乘来计算它们的联合概率。使用相同的逻辑，我们可以通过将所有独立概率相乘来计算所有*n*
    *i.i.d*观测值的联合概率或似然度（即所有这些事件发生的概率）。
- en: '![](img/B22389_17_056.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_056.png)'
- en: Maximizing the likelihood helps the model learn the right parameters for each
    sample such that the probability under the assumed distribution maximizes. We
    can intuitively think about this as follows. For an assumed distribution like
    normal distribution, maximizing the likelihood makes sure that the target falls
    in the center of the distribution defined by the predicted parameters for each
    sample.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化似然度有助于模型学习每个样本的正确参数，使得在假定分布下的概率最大化。我们可以直观地这样理解：对于一个假定的分布，比如正态分布，最大化似然度可以确保目标值位于由预测参数定义的分布中心。
- en: 'However, this operation is not numerically stable. Since probabilities are
    ![](img/B22389_17_057.png), multiplying them together makes the result progressively
    smaller and can soon lead to numerical underflow issues. Therefore, we use the
    log likelihood, which is nothing but the likelihood but log transformed. We do
    it because:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个操作并不是数值稳定的。由于概率是！[](img/B22389_17_057.png)，将它们相乘会使结果逐渐变小，并且很快可能导致数值下溢问题。因此，我们使用对数似然度，它只是对似然度进行对数变换。我们这样做是因为：
- en: Being a strictly monotonic transformation, optimizing a function is equivalent
    to optimizing the log transform of the function. Therefore, optimizing likelihood
    and log likelihood is the same thing.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为严格单调变换，优化一个函数等同于优化该函数的对数变换。因此，优化似然度和对数似然度是一样的。
- en: Log transformation converts the multiplication into an addition, which is a
    much more numerically stable operation.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数变换将乘法转换为加法，这是一种更加数值稳定的操作。
- en: '![](img/B22389_17_058.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_058.png)'
- en: The major disadvantage of this approach is that this relies on parametrized
    probability distributions whose log likelihood computation is tractable. Therefore,
    we are forced to make assumptions about the output and pick a distribution that
    might fit ahead of time.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要缺点是它依赖于参数化的概率分布，其对数似然计算是可行的。因此，我们被迫对输出做出假设，并预先选择一个可能适合的分布。
- en: This is a double-edged sword. On one hand, we can inject some domain knowledge
    into the problem and regularize the model training, but on the other hand, if
    we aren’t clear on whether choosing a normal distribution is the right choice,
    it can lead to an unnecessarily constrained model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一把双刃剑。一方面，我们可以将一些领域知识注入到问题中，并且对模型训练进行正则化，但另一方面，如果我们不清楚选择正态分布是否是正确的选择，就可能导致模型过于受限。
- en: Many popular distributions, such as Normal, Poisson, Negative Binomial, Exponential,
    LogNormal, Tweedie, and so on, can be used for generating probabilistic forecasts
    with this technique.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 许多流行的分布，如正态分布、泊松分布、负二项分布、指数分布、对数正态分布、Tweedie分布等，都可以用来通过这种技术生成概率预测。
- en: Now, we have all the components for training and learning a model and can make
    it predict a full probabilistic distribution instead of a point forecast. With
    all that theory set, let’s switch gears and see how we can use this technique.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经具备了训练和学习模型的所有组件，并能够使其预测完整的概率分布，而不仅仅是点预测。既然理论已经讲解完毕，让我们换个角度，看看如何使用这项技术。
- en: Forecasting with PDF—machine learning models
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PDF进行预测——机器学习模型
- en: We have seen how to use standard machine learning models for forecasting in
    *Part 2*, *Machine Learning for Time Series*. But all of that was for point forecast.
    Can we easily convert all of that into probabilistic forecasts using the PDF approach?
    In theory, yes. But practically, it’s not that easy. All the popular implementations
    of machine learning models like `sci-kit learn`, `xgboost`, `lightgbm`, and so
    on take a point prediction paradigm. And as users of such open-source libraries,
    it isn’t easy for us to tweak and re-write the code to make it optimize the log
    likelihood as a regression loss. But fear not, it is not impossible. `NGBoost`
    is a distant cousin of the very popular gradient boosting models, like `xgboost`
    and `lightgbm`, and it is implemented such that it predicts the PDF, instead of
    the point prediction.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在*第二部分*《时间序列的机器学习》中看到如何使用标准的机器学习模型进行预测。但那时我们讨论的都是点预测。那么，能否通过PDF方法轻松地将所有内容转换为概率预测呢？从理论上讲，是的。但实际上，事情并没有那么简单。像`sci-kit
    learn`、`xgboost`、`lightgbm`等流行的机器学习模型实现采用的是点预测范式。作为这些开源库的用户，我们很难轻易地调整和重写代码，以使其优化对数似然作为回归损失。但不要担心，这并不是不可能的。`NGBoost`是流行的梯度提升模型，如`xgboost`和`lightgbm`的远亲，它的实现方式是预测PDF，而不是点预测。
- en: There are other techniques like Quantile Forecast or Conformal Prediction, which
    are more widely applicable (and recommended) to the machine learning models we
    discussed in the book if the end goal is to have a prediction interval. NGBoost
    is discussed for completeness and for the cases where a full probability distribution
    is needed as an output.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他技术，如分位数预测（Quantile Forecast）或保形预测（Conformal Prediction），这些方法在我们书中讨论的机器学习模型中具有更广泛的应用性（并且是推荐的），尤其是在最终目标是获取预测区间时。为了完整性考虑，书中提到的NGBoost适用于需要输出完整概率分布的情况。
- en: We aren’t going too deep into what NGBoost is and how it differs from the regular
    gradient boosting models here, but just know that it is a model that predicts
    probability distributions instead of point predictions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里深入探讨NGBoost是什么以及它与常规梯度提升模型的区别，但只需知道，它是一种预测概率分布而不是点预测的模型。
- en: '**Reference check**:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The research paper by Duan et.al., which proposed `NGBoost`, is cited in *References*
    under reference *1*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由Duan等人提出的研究论文，介绍了`NGBoost`，在*参考文献*中以参考文献*1*列出。
- en: '*Further* *reading* has a link to a blog about NGBoost, which goes into a bit
    more depth on what the model is.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*进一步的* *阅读*提供了关于NGBoost的博客链接，里面更深入地讨论了该模型的细节。'
- en: '**Notebook alert**:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒**：'
- en: To follow along with the complete code, use the notebook named `01-NGBoost_prediction_intervals.ipynb`
    in the `Chapter17` folder.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟着完整的代码走，请使用`Chapter17`文件夹中的`01-NGBoost_prediction_intervals.ipynb`笔记本。
- en: Let’s use a sample of eight time series from the M4 competition (which has 100,000
    time series, references *5*) for the probabilistic forecasts. The data is easily
    available online and the download script is included in the notebook. We are using
    this simpler dataset than the one we have been working on because I want to avoid
    complicating the narrative with exogenous variables and such. Here is the plot
    of the last 100 time steps of the eight sampled time series.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用来自M4竞赛的8条时间序列样本（该竞赛有100,000条时间序列，参考文献*5*）来进行概率预测。数据可以轻松在线获得，下载脚本也包含在笔记本中。我们使用这个比我们之前使用的数据集更简单的时间序列，因为我想避免因外生变量等问题而使叙述变得复杂。以下是8条抽样时间序列的最后100个时间步的图示。
- en: '![](img/B22389_17_02_01.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_02_01.png)'
- en: '![](img/B22389_17_02_02.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_02_02.png)'
- en: 'Figure 17.2: Last 100 timesteps of 8 sampled time series from the M4 Competition.
    Test period is drawn in a dotted purple line.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.2：来自M4竞赛的8条抽样时间序列的最后100个时间步。测试期用虚线紫色线表示。
- en: Let’s use mlforecast to create some features quickly in order to convert this
    into a regression problem. We had a bonus notebook back in *Chapter 6*, which
    showed how to use `mlforecast` as an alternative for the feature engineering that
    is included in the book’s repository. For the detailed code, refer to the full
    notebook, but for now, let’s assume that we have a dataframe called `data`, which
    has all the features necessary to run a machine learning model. We have split
    it into `train` and `val` and then subsequently to `X_train`, `y_train`, `X_val`,
    and `y_val`. Now, let’s see how we can train a model, assuming the output is a
    Normal distribution.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用mlforecast快速创建一些特征，将问题转换为回归问题。在*第6章*中，我们有一个额外的笔记本，展示了如何使用`mlforecast`作为特征工程的替代方法，这些特征工程也包括在书的代码库中。详细的代码可以参考完整的笔记本，但现在假设我们有一个名为`data`的数据框，里面包含了运行机器学习模型所需的所有特征。我们已经将其拆分为`train`和`val`，然后进一步拆分为`X_train`、`y_train`、`X_val`和`y_val`。接下来，让我们看看如何训练一个模型，假设输出服从正态分布。
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: NGBoost doesn’t have a lot of parameters to tune and therefore isn’t as flexible
    as other **gradient-boosting decision trees** (**GBDT**). And it’s also not as
    fast as the other GBDTs. This is a model that you use only for special use cases
    when you need probabilistic outputs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: NGBoost没有很多需要调整的参数，因此不像其他**梯度提升决策树**（**GBDT**）那样灵活。而且它的速度也不如其他GBDT模型。这个模型只有在需要概率输出的特殊应用场景下才会使用。
- en: 'These are a few parameters that NGBoost has:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是NGBoost的一些参数：
- en: '`Dist`: This is the assumed distributional form of the output. The package
    currently supports `Normal`, `LogNormal`, and `Exponential`—all of which can be
    imported from `ngboost.distns`'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dist`：这是输出的假定分布形式。该包目前支持`Normal`、`LogNormal`和`Exponential`，这些都可以从`ngboost.distns`导入。'
- en: '`Score`: This is any valid scoring function that is used to compare the predicted
    distributions to observations. The log likelihood score that we discussed earlier
    is called `LogScore` in the package and is the default value. All scoring functions
    can be imported from `ngboost.scores`.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Score`：这是任何有效的评分函数，用来将预测的分布与实际观测值进行比较。我们之前讨论的对数似然评分在包中称为`LogScore`，并且是默认值。所有评分函数都可以从`ngboost.scores`导入。'
- en: '`n_estimators`: This is the number of estimators that are used in the boosted
    trees.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：这是在提升树中使用的估计器数量。'
- en: '`learning_rate`: This is the learning rate used for combining the boosted trees.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：这是用于结合提升树的学习率。'
- en: '`mini_batch_frac`: The percent of rows which is sub-sampled for each iteration.
    This is set to `1.0` as the default value.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mini_batch_frac`：每次迭代时子采样的行数百分比。默认值为`1.0`。'
- en: Now that we have a trained NGBoost model, let’s see how we can use it to generate
    predictions and prediction intervals.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了NGBoost模型，接下来让我们看看如何使用它来生成预测值和预测区间。
- en: To get point predictions, the syntax is exactly the same as the sci-kit learn
    API.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得点预测，语法与scikit-learn API完全相同。
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is nothing but a wrapper method that calculates the location parameter
    of the assumed distribution. For instance, for the Normal distribution, the mean
    of the predicted distribution is the point prediction.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅是一个包装方法，用来计算假定分布的定位参数。例如，对于正态分布，预测分布的均值即为点预测值。
- en: 'Now to get the underlying probabilistic prediction and subsequently the prediction
    intervals, we need to use a different method:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了获得底层的概率预测以及随后的预测区间，我们需要使用不同的方法：
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Each point in `y_pred_dists` is a complete distribution. If we want to instead
    the first five predicted points, we can do the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`y_pred_dists`中的每个点都是一个完整的分布。如果我们只想预测前五个点，可以这样做：'
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, to get the prediction interval, we can use `y_pred_dist` and call a method,
    giving the level of confidence we expect. This, in turn, calls the `scipy` distribution
    (like `scipy.stats.norm`), which has a method, `interval`, to get the intervals
    given the confidence level.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了获取预测区间，我们可以使用`y_pred_dist`并调用一个方法，给出我们期望的置信度水平。这反过来又调用`scipy`分布（如`scipy.stats.norm`），其具有一个`interval`方法，可以在给定置信度水平的情况下获取区间。
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, we have a window around `y_pred` wide enough to envelope the uncertainty
    expected at each data point—the prediction interval. Let’s look at the forecast
    and the metrics.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们围绕`y_pred`有一个足够宽的窗口，能够包围每个数据点的预期不确定性——即预测区间。让我们看看预测结果和度量指标。
- en: '![](img/B22389_17_03_01.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_03_01.png)'
- en: '![](img/B22389_17_03_02.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_03_02.png)'
- en: 'Figure 17.3: Forecast with prediction intervals from NGBoost'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.3：NGBoost的预测区间
- en: 'And below are the metrics we calculated for these eight time series (Mean Absolute
    Error, Coverage, and Average Length):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们为这八个时间序列计算的度量指标（平均绝对误差、覆盖率和平均长度）：
- en: '![](img/B22389_17_04.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_04.png)'
- en: 'Figure 17.4: Metrics for NGBoost'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.4：NGBoost的度量指标
- en: Although for some time series, the intervals seem to be good, some others (like
    time series H103) seem to have way too narrow a prediction interval, which is
    also evident in the low coverage as well.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对于某些时间序列，区间看起来很好，但一些其他序列（如时间序列H103）的预测区间似乎过于狭窄，这在低覆盖率中也很明显。
- en: Forecasting with PDF—deep learning models
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PDF进行预测——深度学习模型
- en: 'Unlike machine learning models, it’s very easy to convert all the deep learning
    models we have learned about in the book to their PDF version. Remember the discussion
    we had before we started the practical application? The major changes we needed
    to do were these:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与机器学习模型不同，转换我们在书中学到的所有深度学习模型为它们的PDF版本非常简单。记得我们开始实际应用前的讨论吗？我们需要做的主要改动就是这些：
- en: Instead of predicting point forecast (single number), we predict the parameters
    of a probability distribution (one or more numbers).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与预测单一的点预测（单个数字）不同，我们预测的是概率分布的参数（一个或多个数字）。
- en: Instead of using a point loss like Mean Squared Error, use a probabilistic scoring
    function like log likelihood.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其使用像均方误差这样的点损失，不如使用像对数似然这样的概率评分函数。
- en: In the deep learning paradigm, these are very simple changes to make, aren’t
    they? In almost all the deep learning models we have learned to use in the book,
    there is a linear projection at the end, which projects the output into the required
    dimensions. Conceptually, it’s simple enough to make these output multiple numbers
    (parameters of the assumed distribution) by changing the linear projection. Similarly,
    it’s simple enough to change the loss function as well. Notably, *DeepAR* (Reference
    *3*) is a well-known deep learning model that uses this technique for probabilistic
    forecasting.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习范式下，这些改动非常简单，不是吗？在我们书中学到的几乎所有深度学习模型中，最后都有一个线性投影，将输出投影到所需的维度上。从概念上讲，通过改变线性投影，让输出变为多个数字（假设分布的参数）是足够简单的。同样，改变损失函数也很简单。值得注意的是，*DeepAR*（参考文献*3*）是一个使用这种技术进行概率预测的知名深度学习模型。
- en: '**Notebook alert**:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒**：'
- en: To follow along with the complete code, use the notebook named `02-NeuralForecast_prediction_intervals_PDF.ipynb`
    in the `Chapter17` folder.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随完整的代码，请使用`Chapter17`文件夹中的`02-NeuralForecast_prediction_intervals_PDF.ipynb`笔记本。
- en: Let’s see how we can do this in `neuralforecast` (the library we were using
    *Chapter 16*). In our example here, we will take a simple model like an LSTM,
    but we can do the same with any model because all we are doing is switching the
    loss function to `DistributionLoss`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在`neuralforecast`（我们在*第16章*中使用的库）中实现这个功能。在这里的示例中，我们将使用一个简单的模型，如LSTM，但我们可以对任何模型执行相同操作，因为我们所做的只是将损失函数切换为`DistributionLoss`。
- en: And, for this example, we are going to use the M4 competition dataset, which
    is freely available (the code to download the dataset is included in the notebook).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用M4竞赛数据集，该数据集可以自由获取（下载数据集的代码已包含在笔记本中）。
- en: Let’s start from the point where we have the data formatted the way `neuralforecast`
    expects in `Y_train_df` and `Y_test_df`. The first thing we need to do is import
    the necessary classes.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据已经按照`neuralforecast`期望的格式在`Y_train_df`和`Y_test_df`中整理好的地方开始。我们需要做的第一件事是导入必要的类。
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The only class that we haven’t looked at before in the book is `DistributionLoss`.
    This is a class that wraps `torch.distribution` classes and implements the negative
    log likelihood loss we discussed earlier. At the time of writing this book, the
    `DistributionLoss` class supports these underlying distributions:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中唯一我们之前没有涉及的类是`DistributionLoss`。这是一个封装了`torch.distribution`类并实现我们之前讨论的负对数似然损失的类。在写本书时，`DistributionLoss`类支持以下这些底层分布：
- en: Poisson
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poisson
- en: Normal
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Normal
- en: StudentT
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StudentT
- en: NegativeBinomial
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NegativeBinomial
- en: Tweedie
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tweedie
- en: Bernoulli (Temporal Classifiers)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bernoulli（时间分类器）
- en: ISQF (Incremental Spline Quantile Function)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ISQF（增量样条分位数函数）
- en: 'The choice between these different distributions is totally up to the modeler
    and is a key assumption in the model. If the output we are modeling is expected
    to be in a normal distribution, then we can choose `Normal`. These are the major
    parameters of `DistributionLoss`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些不同的分布之间的选择完全取决于模型构建者，并且是模型中的关键假设。如果我们建模的输出预期符合正态分布，那么我们可以选择`Normal`。以下是`DistributionLoss`的主要参数：
- en: '`distribution`: This is a string that identifies which distribution we are
    assuming. It can be any one from the list we saw before.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distribution`：这是一个字符串，用于标识我们假设的分布类型。它可以是我们之前看到的任何一个分布。'
- en: '`level`: This is a list of floats that defines the different confidence levels
    we are interested in modeling. For instance, if we want to model 80% and 90% confidence,
    we should give the values as `[80,90]`.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`level`：这是一个浮动值列表，定义我们希望建模的不同置信水平。例如，如果我们想建模80%和90%的置信度，我们应该给出值为`[80, 90]`。'
- en: '`quantiles`: This is an alternate way of defining the level. Instead of 95%
    confidence, you can define them in quantiles → `[0.1, 0.9]`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quantiles`：这是定义水平的另一种方式。你可以不使用95%的置信度，而是使用分位数 → `[0.1, 0.9]`。'
- en: '**Practitioner’s tip**:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**实践者提示**：'
- en: Making a distributional assumption requires a deep study of the domain and data.
    But if you are not completely familiar with these distributions, `Normal` or `StudentT`
    is a good starting point as a lot of data resembles normal distribution. But before
    you jump into using `Normal` distributions everywhere, you should do a bit of
    literature study in the domain of the problem and choose the distribution accordingly.
    For instance, intermittent demand or sparse demand, which is very common in retail,
    is better modeled using a `Poisson` distribution. If the forecast is a count data
    (positive integers), `Negative Binomial` is a good option.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 做出分布假设需要对领域和数据进行深入研究。但如果你对这些分布不完全熟悉，`Normal`或`StudentT`是一个很好的起点，因为许多数据类似于正态分布。但在你将`Normal`分布到处使用之前，应该在问题领域中进行一些文献研究，并根据实际情况选择合适的分布。例如，间歇性需求或稀疏需求，在零售业中非常常见，使用`Poisson`分布建模会更好。如果预测的是计数数据（正整数），`Negative
    Binomial`是一个不错的选择。
- en: Now let’s set a horizon, the levels we need, and a few hyperparameters for LSTM
    (we’ve chosen some small and simple hyperparameters to make the training faster.
    In real-world problems, it is advisable to do a hyperparameter search to find
    the best parameters).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们设置一个预测期限、需要的水平以及一些LSTM的超参数（我们选择了一些简单且较小的超参数来加速训练。在实际问题中，建议进行超参数搜索，以找到最佳参数）。
- en: '[PRE9]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we need to define the models we are going to use and the `NeuralForecast`
    class. Let’s define two models—one using `Normal` and another using `StudentT`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要定义将要使用的模型和`NeuralForecast`类。我们定义两个模型——一个使用`Normal`，另一个使用`StudentT`。
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Notice how the syntax is exactly the same as for point forecast, except for
    the Distribution Loss we chose. Now all that’s left is to train the models.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，语法与点预测完全相同，唯一不同的是我们选择的分布损失。现在剩下的就是训练模型了。
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once the model is trained, we can predict using the predict method. This output
    will have the point forecast under the alias we have defined and the high and
    low intervals for all the levels we have defined.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以使用预测方法进行预测。该输出将在我们定义的别名下提供点预测，并提供我们定义的所有置信水平的高低区间。
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that we have the probabilistic forecasts, let’s take a look at them and
    also calculate the metrics.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了概率预测，让我们来看一下它们，并计算相关指标。
- en: '![](img/B22389_17_05_01.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_05_01.png)'
- en: '![](img/B22389_17_05_02.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_05_02.png)'
- en: 'Figure 17.5: Forecast with prediction intervals from LSTM with StudentT distribution
    as output'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.5：使用LSTM和StudentT分布作为输出的预测带置信区间
- en: '![](img/B22389_17_06.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_06.png)'
- en: 'Figure 17.6: Metrics for the LSTM with Normal and StudentT distribution outputs'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.6：LSTM与正态分布和StudentT分布输出的度量标准
- en: If we compare the coverages with the ones with NGBoost, we can see that the
    deep learning approach increased the coverage, but also has wider than necessary
    intervals in most cases (as evidenced by larger average widths).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将覆盖率与NGBoost的覆盖率进行比较，我们可以看到深度学习方法提高了覆盖率，但在大多数情况下间隔也比必要的更宽（如更大的平均宽度所示）。
- en: The biggest disadvantage of this method is that we restrict the output to one
    of the parametrized distributions. In many real-world cases, the data might not
    conform to any parametric distributions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的最大缺点是我们将输出限制在参数化分布之一。在许多实际情况下，数据可能不符合任何参数化分布。
- en: Now, let’s see a method that doesn’t require the assumption of any parametric
    distribution, but still gets the prediction intervals.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一个不需要假设任何参数分布的方法，但仍然可以得到预测区间。
- en: Quantile function
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分位数函数
- en: If our only aim is to get prediction intervals, we can also do the same using
    quantiles. Let’s look at the PDF method in a slightly different light. In the
    PDF method, we have a full probability distribution as the output at each timestep
    and we use that distribution to get the quantiles, which are the prediction intervals.
    Although for most parametric distributions, there are analytical formulae to get
    the quantiles, we can also find the quantiles numerically. We just draw N samples,
    where N is sufficiently large, and then calculate the quantiles of the drawn samples.
    The point is that even though we have a full probability distribution, for prediction
    intervals, all we need are the quantiles. And calculating quantiles given N samples
    is not dependent on the kind of distribution.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的唯一目的是获得预测区间，我们也可以使用分位数来做同样的事情。让我们从稍微不同的角度看PDF方法。在PDF方法中，我们在每个时间步有一个完整的概率分布作为输出，并使用该分布来获得分位数，这些分位数就是预测区间。尽管对于大多数参数分布，有获取分位数的解析公式，但我们也可以通过数值方法找到分位数。我们只需绘制足够数量的N个样本，然后计算绘制样本的分位数。关键在于，即使我们有完整的概率分布，对于预测区间，我们所需的只是分位数。而给定N个样本计算分位数并不依赖于分布的种类。
- en: So, what if we can train our models to predict the specified quantiles directly,
    having no assumption on the underlying probability distribution? This is exactly
    what we do with quantile functions.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们可以训练我们的模型直接预测指定的分位数，而不假设潜在的概率分布会怎样？这正是我们用分位数函数所做的。
- en: 'Before talking about quantile functions, let’s spend a minute on the **Cumulative
    Distribution Function** (**CDF**). This, again, is high school probability. In
    simple words, a CDF returns the probability of some random variable, *X* being
    less than or equal to some value, *x*:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论分位数函数之前，让我们花一分钟了解**累积分布函数**（**CDF**）。再次强调，这是高中的概率知识。简单来说，CDF返回某个随机变量*X*小于或等于某个值*x*的概率：
- en: '![](img/B22389_17_059.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_059.png)'
- en: where *F* is the CDF. This function takes in an input, *x*, and returns a value
    between 0 and 1\. Let’s call this value ![](img/B22389_16_143.png).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*F*是CDF。该函数接受一个输入*x*，并返回一个介于0和1之间的值。我们称这个值为![](img/B22389_16_143.png)。
- en: A *quantile function* is an inverse of CDF. This function tells you what value
    of *x* would make ![](img/B22389_17_061.png) return a particular value, ![](img/B22389_16_143.png).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*分位数函数*是CDF的反函数。该函数告诉你使![](img/B22389_17_061.png)返回特定值![](img/B22389_16_143.png)的*x*的值。'
- en: '![](img/B22389_17_063.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_063.png)'
- en: This function, ![](img/B22389_17_064.png), is the *quantile function*.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数，![](img/B22389_17_064.png)，就是*分位数函数*。
- en: From the implementation perspective, for models which are capable of multi-output
    prediction (like the deep learning models), we can use a single model and predict
    all the quantiles we want to by changing the output layer. And for models that
    are restricted to a single output (like machine learning models), we can learn
    separate quantile models for each quantile.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从实施的角度来看，对于能够进行多输出预测的模型（如深度学习模型），我们可以使用一个模型，并通过更改输出层来预测我们想要的所有分位数。对于仅限于单输出的模型（如机器学习模型），我们可以为每个分位数学习单独的分位数模型。
- en: Now, just like before, we can’t use point losses like the mean squared error.
    We got over this problem in PDFs by using the log likelihood function. And here,
    we can use quantile loss or pinball loss.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，就像之前一样，我们不能使用均方误差这样的点损失。我们通过使用对数似然函数在 PDF 中克服了这个问题。在这里，我们可以使用分位损失或针球损失。
- en: '**Reference check**:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The paper that proposed quantile loss and regression is cited in the *References*
    under reference *4*.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 提出分位损失和回归的论文已在 *参考文献* 中第 *4* 条进行了引用。
- en: 'The quantile loss can be defined as below:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 分位损失可以定义如下：
- en: '![](img/B22389_17_065.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_065.png)'
- en: 'where ![](img/B22389_17_052.png) is the target value at time *t*, ![](img/B22389_17_067.png)
    is the quantile forecast, and *q* is the quantile we are forecasting. The formula
    looks daunting, but bear with me for a second; it’s quite simple. Let’s try and
    get some intuitions about the loss. We know the median is the 0.5 quantile, and
    that is a measure of central tendency. But if we want our predictions to approximate
    the 75^(th) percentile or 0.75 quantile, then we would have to urge the model
    to overestimate, right? And if we want the model to overestimate, we need to penalize
    the model more if it’s underestimating. And vice versa, if we want to predict
    the 0.25 quantile, we need to underestimate. Quantile loss does exactly that:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B22389_17_052.png) 是时刻 *t* 的目标值，![](img/B22389_17_067.png) 是分位预测值，而
    *q* 是我们预测的分位数。公式看起来很复杂，但请耐心等一下，其实很简单。让我们尝试理解一下损失的直觉。我们知道中位数是 0.5 分位数，这是一个集中趋势的度量。但是，如果我们想让预测值接近第
    75 百分位数或 0.75 分位数，我们就必须促使模型做出高估，对吗？如果我们想让模型高估，我们需要在模型低估时给予更高的惩罚。反之，如果我们想预测 0.25
    分位数，我们需要低估。分位损失正是做到了这一点：
- en: For ![](img/B22389_17_068.png) (under estimation), the loss is ![](img/B22389_17_069.png)
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 ![](img/B22389_17_068.png)（低估），损失为 ![](img/B22389_17_069.png)
- en: For ![](img/B22389_17_070.png) (over estimation), the loss is ![](img/B22389_17_071.png)
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 ![](img/B22389_17_070.png)（高估），损失为 ![](img/B22389_17_071.png)
- en: The asymmetry is derived from the term *q* or 1 - *q*. The other term is just
    the difference between the actual and predicted values.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不对称性来源于 *q* 或 1 - *q*。另一个项只是实际值和预测值之间的差异。
- en: Let’s try to understand this with an example. Suppose we have the true value
    ![](img/B22389_17_072.png), and we want to estimate 0.75 quantile (![](img/B22389_17_073.png)).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这一点。假设我们有真实值 ![](img/B22389_17_072.png)，并且我们想要估计 0.75 分位数（![](img/B22389_17_073.png)）。
- en: '**Case 1: Overestimation**: ![](img/B22389_17_074.png). Since ![](img/B22389_17_070.png),
    our quantile loss would be:'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例 1：高估**：![](img/B22389_17_074.png)。由于 ![](img/B22389_17_070.png)，我们的分位损失将是：'
- en: '![](img/B22389_17_076.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_076.png)'
- en: '**Case 2: Underestimation**: ![](img/B22389_17_077.png). Since ![](img/B22389_17_078.png),
    our quantile loss would be:'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例 2：低估**：![](img/B22389_17_077.png)。由于 ![](img/B22389_17_078.png)，我们的分位损失将是：'
- en: '![](img/B22389_17_079.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_079.png)'
- en: '**Notebook alert**:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒**：'
- en: To follow along with the complete code, use the notebook named `03-Understanding_Quantile_Loss.ipynb`
    in the `Chapter17` folder.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 若要跟随完整代码，请使用 `Chapter17` 文件夹中的名为 `03-Understanding_Quantile_Loss.ipynb` 的笔记本。
- en: Therefore, by varying the value of *q*, we can make the loss more or less asymmetric
    and toward either side. *Figure 17.7* below shows the loss curves for *q* = 0.5
    and *q* = 0.75 with these example predictions marked on it.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过调整 *q* 的值，我们可以使损失变得更具不对称性，且倾向于任一方向。下面的 *图 17.7* 显示了 *q* = 0.5 和 *q* = 0.75
    的损失曲线，并标出了这些示例预测。
- en: '![](img/B22389_17_07.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_07.png)'
- en: 'Figure 17.7: Quantile loss curves for q=0.5 and q=0.75'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.7：q=0.5 和 q=0.75 的分位损失曲线
- en: We can see that the quantile loss for *q* = 0.5 is symmetric as it represents
    the median, whereas the loss curve for *q* = 0.75 is asymmetric, penalizing underestimation
    a lot more than overestimation.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，*q* = 0.5 的分位损失是对称的，因为它代表了中位数，而 *q* = 0.75 的损失曲线是非对称的，对低估的惩罚远高于高估。
- en: 'Although the formula has a branching structure, a maximum operation can be
    easily avoided when implementing it in code. The quantile loss in Python looks
    like this:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管公式具有分支结构，但在实现代码时可以轻松避免使用最大操作。Python 中的分位损失代码如下：
- en: '[PRE13]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, let’s get into the practical side of things and learn how to use quantile
    loss with the different techniques (both machine learning and deep learning) we
    have covered in this book.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入实际操作，学习如何在本书中涵盖的不同技术（包括机器学习和深度学习）中使用分位损失。
- en: Forecasting with quantile loss (machine learning)
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用分位损失进行预测（机器学习）
- en: Regular machine learning models are typically capable of modeling one output.
    Therefore, we will need to train a different model for each of the quantiles we
    are interested in using quantile loss. So, if we want to predict the 0.5, 0.05,
    and 0.95 quantiles for a problem, we will have to train three separate models,
    one for each quantile.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 常规机器学习模型通常只能建模一个输出。因此，我们需要为每个我们感兴趣的分位数训练不同的模型，使用分位数损失。所以，如果我们想预测一个问题的 0.5、0.05
    和 0.95 分位数，我们必须训练三个独立的模型，每个分位数一个模型。
- en: '**Notebook alert**:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提示**：'
- en: To follow along with the complete code, use the notebook named `04-LightGBM_Prediction_Interval_Quantile_Loss.ipynb`
    in the `Chapter17` folder.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随完整代码，请使用 `Chapter17` 文件夹中的 `04-LightGBM_Prediction_Interval_Quantile_Loss.ipynb`
    笔记本。
- en: Let’s see how we can do that. Just like in the PDF section, we are using `mlforecast`
    to quickly whip up a synthetic problem and create some features. For the detailed
    code, refer to the full notebook, but for now, let’s assume that we have a dataframe
    `data`, which has all the features necessary to run a machine learning model.
    We have split it into `train` and `val` and then subsequently into `X_train`,
    `y_train`, `X_val`, and `y_val`.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何实现这一点。就像在 PDF 部分中一样，我们使用 `mlforecast` 快速生成一个合成问题并创建一些特征。有关详细代码，请参考完整的笔记本，但现在假设我们有一个数据框
    `data`，其中包含运行机器学习模型所需的所有特征。我们将其分为 `train` 和 `val`，然后分别划分为 `X_train`、`y_train`、`X_val`
    和 `y_val`。
- en: The first step is to import the **LGBMRegressor** and set some parameters and
    quantiles we want to train.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是导入 **LGBMRegressor** 并设置我们希望训练的某些参数和分位数。
- en: '[PRE14]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The key parameter to note here is that the `objective` is set as `quantile`
    and the `metric` is also set as `quantile`. The rest of the LightGBM parameters
    can be tuned and tweaked for each use case. Now, let’s train all the quantile
    models.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的关键参数是 `objective` 设置为 `quantile`，并且 `metric` 也设置为 `quantile`。其余的 LightGBM
    参数可以根据每个用例进行调优。现在，让我们训练所有的分位数模型。
- en: '[PRE15]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now that the model is trained, we can get the point prediction and prediction
    intervals from the quantile models.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经训练完成，我们可以从分位数模型中获取点预测和预测区间。
- en: '[PRE16]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, let’s see what the forecast and metrics look like.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看预测和度量指标的结果。
- en: '![](img/B22389_17_08_01.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_08_01.png)'
- en: '![](img/B22389_17_08_02.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_08_02.png)'
- en: 'Figure 17.8: Forecast with prediction intervals using Quantile Regression with
    LightGBM'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.8：使用 Quantile 回归和 LightGBM 进行带预测区间的预测
- en: '![](img/B22389_17_09.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_09.png)'
- en: 'Figure 17.9: Metrics for Quantile Regression with LightGBM'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.9：使用 LightGBM 的分位数回归的度量指标
- en: The disadvantage of this is that we are training a model for each quantile.
    This can become unwieldy quickly. Instead of training one model, training three
    models would make the total training time increase. Another issue is that since
    all three models are trained differently, they might have different properties,
    and the way they have learned to solve the problem can also be very different.
    And because of this inconsistency, the prediction intervals can also suffer from
    some issues. We can see it clearly in the jagged prediction intervals in many
    of the time series in *Figure 17.7*; they seem disconnected from the median prediction.
    This is a problem that we don’t have in the deep learning world.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的缺点是我们为每个分位数训练一个模型。这会很快变得难以管理。与训练一个模型不同，训练三个模型会导致总训练时间增加。另一个问题是，由于三个模型的训练方式不同，它们可能具有不同的属性，它们解决问题的方式也可能有很大不同。由于这种不一致性，预测区间也可能存在一些问题。我们可以清楚地看到这一点，在许多时间序列中的*图
    17.7*，它们似乎与中位预测断开连接。这是深度学习领域没有的问题。
- en: Forecasting with quantile loss (deep learning)
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用分位数损失进行预测（深度学习）
- en: 'In the deep learning models, we use a common learning structure and just use
    different linear projections on the shared projection for the different quantiles.
    This ensures that the underlying representation and learning are common across
    all the quantiles and can cause a more coherent set of quantile predictions. So,
    for all the deep learning models we have learned about in the book, we can make
    them into a Quantile Forecast model by doing two things:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习模型中，我们使用一个共同的学习结构，并对不同的分位数在共享投影上使用不同的线性投影。这确保了所有分位数的基础表示和学习是相同的，从而可以产生更一致的分位数预测。因此，对于我们在本书中学到的所有深度学习模型，我们可以通过做两件事将它们转换为分位数预测模型：
- en: Instead of predicting point forecast (single number), we predict parameters
    of a probability distribution (one or more numbers).
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不是预测单一数字的点预测，而是预测概率分布的参数（一个或多个数字）。
- en: Instead of using a point loss like Mean Squared Error, use a probabilistic scoring
    function like log likelihood.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与其使用像均方误差这样的点损失，不如使用像对数似然这样的概率评分函数。
- en: And just like we did in the PDF section, all we need to do is to switch out
    the loss function in `neuralforecast`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在PDF部分做的那样，我们所需要做的只是将`neuralforecast`中的损失函数切换掉。
- en: '**Notebook alert**:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本警告**：'
- en: To follow along with the complete code, use the notebook named `05-NeuralForecast_prediction_intervals_Quantile_Loss.ipynb`
    in the `Chapter17` folder.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随完整代码，请使用`Chapter17`文件夹中的`05-NeuralForecast_prediction_intervals_Quantile_Loss.ipynb`笔记本。
- en: Let’s see how we can do this in `neuralforecast` (the library we were using
    *Chapter 16*). Just like before, we will take a simple model like an LSTM and
    the M4 competition dataset, but we can do the same with any model or any dataset
    because all we are doing is switching the loss function to `MQLoss` (multi-quantile
    loss).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在`neuralforecast`中实现这一点（我们在*第16章*中使用的库）。就像之前一样，我们将使用一个简单的模型，如LSTM和M4竞赛数据集，但我们可以对任何模型或任何数据集执行相同的操作，因为我们所做的只是将损失函数切换为`MQLoss`（多分位数损失）。
- en: Let’s start from the point where we have the data formatted the way `neuralforecast`
    expects in a `Y_train_df` and `Y_test_df`. The first thing we need to do is import
    the necessary classes.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据已经按照`neuralforecast`所需的格式（即`Y_train_df`和`Y_test_df`）开始。首先，我们需要导入必要的类。
- en: '[PRE17]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The only class that we haven’t looked at before is `MQLoss`. This class just
    calculates the quantile loss we discussed just now for multiple quantiles (which
    is how you would want to typically train the model). These are the major parameters
    of `MQLoss`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前没有查看过的唯一类是`MQLoss`。这个类计算我们刚刚讨论的多分位数的分位数损失（这通常是你训练模型的方式）。这些是`MQLoss`的主要参数：
- en: '`level`: This is a list of floats that defines the different confidence levels
    we are interested in modeling. For instance, if we want to model 80% and 90% confidence,
    we should give the values as `[80,90]`.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`level`：这是一个浮动数值列表，定义了我们感兴趣的不同置信度水平。例如，如果我们想建模80%和90%的置信度，我们应该给出`[80,90]`。'
- en: '`quantiles`: This is an alternate way of defining the level. Instead of 95%
    confidence, you can define them in quantiles → `[0.1, 0.9]`.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quantiles`：这是一种定义水平的替代方法。你可以将其定义为分位数，而不是95%的置信度 → `[0.1, 0.9]`。'
- en: Now, let’s set a horizon, the levels we need, and a few hyperparameters for
    LSTM.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们设置一个预测范围，确定所需的水平，并为LSTM设置一些超参数。
- en: '[PRE18]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now, we need to define the models we are going to use and the `NeuralForecast`
    class. Let’s define just one model.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要定义我们将要使用的模型和`NeuralForecast`类。让我们只定义一个模型。
- en: '[PRE19]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Notice how the syntax is exactly the same as the point forecast, except for
    the multi-quantile loss we chose. Now, all that’s left is to train the models.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，语法与点预测完全相同，唯一不同的是我们选择了多分位数损失。现在，剩下的就是训练模型了。
- en: '[PRE20]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Once the model is trained, we can predict using the `predict` method. This output
    will have the point forecast under the alias we have defined and the high and
    low intervals for all the levels we have defined.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们就可以使用`predict`方法进行预测。这个输出将包含我们定义的别名下的点预测，以及我们所定义的所有水平的高低区间。
- en: '[PRE21]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, we have a way of getting prediction intervals without making an assumption
    about the output distribution and that is valuable in real-world cases where we
    are not sure what the underlying output distribution would be. Let’s look at the
    generated probabilistic forecasts and its metrics.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一种不对输出分布做假设的预测区间方法，这在现实世界中非常有价值，特别是在我们不确定基础输出分布时。让我们看看生成的概率预测及其指标。
- en: '![](img/B22389_17_10_01.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_10_01.png)'
- en: '![](img/B22389_17_10_02.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_10_02.png)'
- en: 'Figure 17.10: Forecast with prediction intervals using Quantile Regression
    (deep learning)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.10：使用分位数回归（深度学习）进行预测并生成预测区间
- en: '![](img/B22389_17_11.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_11.png)'
- en: 'Figure 17.11: Metrics for the Quantile Regression (deep learning)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.11：分位数回归（深度学习）的指标
- en: We can see that the prediction intervals are quite in sync with each other and
    the median prediction and not disconnected like separate models with LightGBM.
    This is because the same learning is happening for all the quantiles, just the
    final projection heads are different. The coverage, in general, is also better.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，预测区间彼此之间非常一致，与中位预测结果没有脱节，像 LightGBM 中的不同模型那样断开。这是因为对于所有分位数，都在进行相同的学习，只是最终的投影头不同。总体而言，覆盖率也更好。
- en: There is another way to get prediction intervals that are dead simple, but not
    that easy to implement because of the way PyTorch is typically used. That’s what
    we are going to see next.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种获取预测区间的方法，它非常简单，但由于 PyTorch 的使用方式，实际上并不容易实现。这就是我们接下来要看到的内容。
- en: Monte Carlo Dropout
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡洛丢弃
- en: Dropout is a hugely popular regularization layer in deep learning. Without going
    into details, dropout regularization is when we randomly make some part of the
    weights of the network while training (dropout is turned off during inference).
    Intuitively, this forces the model to not rely on a few weights but rather to
    distribute the relevance of the weights across the network. From another perspective,
    we are applying a sort of regularization (very similar to Ridge regularization),
    which makes sure none of the weights are single-handedly too high to influence
    the output drastically.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃是深度学习中一种非常流行的正则化层。简单来说，丢弃正则化是指在训练过程中随机使网络的部分权重为零（在推理时，丢弃会被关闭）。直观地看，这迫使模型不依赖少数几个权重，而是将权重的相关性分布在整个网络中。从另一个角度看，我们正在应用一种类似于岭回归（Ridge
    regularization）的正则化方法，确保没有任何权重单独过高，以至于会剧烈地影响输出。
- en: 'Technically, besides making a random part of the weights zero, we also debias
    each layer by normalizing by the fraction of nodes/weights that were retained
    (not zeroed out). Let’s formalize this layer now. If the probability of dropout
    is ![](img/B22389_16_143.png), and is applied on an intermediate activation, *h*
    then the activation after dropout, ![](img/B22389_17_081.png) will be:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，除了将部分权重设为零，我们还通过按保留（未置零）节点/权重的比例进行归一化，来消除每一层的偏差。现在让我们对这一层进行公式化。如果丢弃的概率是
    ![](img/B22389_16_143.png)，并应用于一个中间激活值 *h*，那么丢弃后的激活值 ![](img/B22389_17_081.png)
    将是：
- en: '![](img/B22389_17_082.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_082.png)'
- en: Why do we need to scale/normalize the output when dropout is applied? The intuitive
    answer is to make sure the output has the same scale during training (when dropout
    is active) and during inference (when dropout is turned off). The long answer
    is as follows.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么在应用丢弃时，我们需要缩放/归一化输出？直观的答案是为了确保在训练期间（丢弃启用时）和推理期间（丢弃关闭时），输出的尺度一致。更长的答案如下。
- en: 'Let’s say without dropout, the output of a node is *h*. Now, with dropout,
    with a probability of ![](img/B22389_17_083.png), this output becomes 0 and with
    a probability of ![](img/B22389_17_084.png), it is *h*. Therefore, the expected
    value of the node would be: ![](img/B22389_17_085.png). This means the average
    value of the output is reduced by a factor of ![](img/B22389_17_084.png), which
    is not desirable as this would change the scale of the values during training
    and inference. Therefore, the solution is to scale the output of the nodes that
    are retained by dropout by ![](img/B22389_17_084.png).'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在没有丢弃（dropout）的情况下，一个节点的输出是 *h*。现在，使用丢弃时，节点的输出有 ![](img/B22389_17_083.png)
    的概率变为 0，且有 ![](img/B22389_17_084.png) 的概率保持为 *h*。因此，该节点的期望值为：![](img/B22389_17_085.png)。这意味着输出的平均值会被
    ![](img/B22389_17_084.png) 缩减，这在训练和推理过程中是不希望发生的，因为它会改变数值的尺度。因此，解决方案是将被丢弃保留的节点的输出按
    ![](img/B22389_17_084.png) 进行缩放。
- en: Now, we know what dropout is. But remember that we were using dropout only during
    training as a regularization. In 2015, Yarin Gal et al. proposed that the good
    old dropout also doubles as a *Bayesian Approximation of Gaussian Processes*.
    That’s a lot of terms that we haven’t come across in a single phrase. Let’s take
    a short detour to understand these terms at a high level, and I’ll include other
    links in *Further reading*.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道了什么是丢弃。但请记住，我们仅在训练期间使用丢弃作为正则化手段。在2015年，Yarin Gal 等人提出了经典的丢弃法也可以作为 *贝叶斯高斯过程近似*。这是一堆我们之前没接触过的术语。让我们简要地绕个路，从高层次上理解这些术语，我会在
    *进一步阅读* 中提供更多链接。
- en: '**Reference check**:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查**：'
- en: The paper by Yarin Gal et al. about Monte Carlo Dropout is cited in *References*
    under reference *2*.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Yarin Gal 等人的关于蒙特卡洛丢弃的论文在 *参考文献* 中以参考文献 *2* 的形式被引用。
- en: '*Bayesian inference* is a statistical method that updates the probability of
    a hypothesis as more evidence or information becomes available. It is based on
    Bayes’ theorem, which mathematically expresses the relationship between the prior
    probability, the likelihood, and the posterior probability. Formally, Bayes’ theorem
    is given by:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '*贝叶斯推断*是一种统计方法，当更多证据或信息变得可用时，它会更新假设的概率。它基于贝叶斯定理，该定理在数学上表达了先验概率、似然性和后验概率之间的关系。形式上，贝叶斯定理表示为：'
- en: '![](img/B22389_17_088.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_088.png)'
- en: where ![](img/B22389_17_089.png) is the *posterior probability* of hypothesis
    ![](img/B22389_04_016.png) given the data *D*, ![](img/B22389_17_091.png) is the
    *likelihood* (probability of observing the data *D* given hypothesis ![](img/B22389_04_016.png)),
    ![](img/B22389_17_093.png) is the *prior probability* of the hypothesis ![](img/B22389_04_016.png)
    before observing the data, and ![](img/B22389_17_095.png) is the marginal likelihood
    or *evidence* (the total probability of observing the data under all possible
    hypotheses).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/B22389_17_089.png)是给定数据*D*的假设![](img/B22389_04_016.png)的*后验概率*，![](img/B22389_17_091.png)是*似然性*（给定假设![](img/B22389_04_016.png)观察到数据*D*的概率），![](img/B22389_17_093.png)是观察数据之前假设![](img/B22389_04_016.png)的*先验概率*，而![](img/B22389_17_095.png)是边际似然性或*证据*（在所有可能的假设下观察到数据的总概率）。
- en: Although it has some specific terminology, this is very intuitive and provides
    a structured way to update our prior beliefs in the face of evidence or data.
    We start with a prior distribution ![](img/B22389_17_096.png) that represents
    our initial beliefs about the parameters. As we observe data *D*, we update our
    beliefs getting the posterior distribution ![](img/B22389_17_089.png). This posterior
    distribution combines the prior information and the likelihood of the observed
    data, providing a new, updated belief about the parameters. *Further reading*
    has a more detailed explanation for those who are interested. It also has a page
    from *Seeing Theory* that helps you visualize these in an intuitive way.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它有一些特定的术语，但这个方法非常直观，并提供了一种在面对证据或数据时更新我们先前信念的结构化方式。我们从一个先验分布![](img/B22389_17_096.png)开始，表示我们对参数的初始信念。当我们观察到数据*D*时，我们更新我们的信念，得到后验分布![](img/B22389_17_089.png)。这个后验分布结合了先验信息和观察到的数据的似然性，提供了一个关于参数的新、更新的信念。对于感兴趣的人，*进一步阅读*提供了更详细的解释。它还包含了*Seeing
    Theory*中的一页，帮助你以直观的方式可视化这些内容。
- en: Now, let’s move on to the **Gaussian Process** (**GP**). As we have seen in
    *Chapter 5*, supervised learning is learning a function ![](img/B22389_17_098.png)
    where ![](img/B22389_05_001.png) is the quantity we are interested in predicting,
    *h*, is the function we learn, *X* is the input data, and ![](img/B22389_07_003.png)
    represents the model parameters. So, GPs assume this function as a probability
    distribution and use Bayesian inference to update the posterior of the function
    using the data we have available for training. It’s a drastically different way
    of learning from data and is inherently probabilistic.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续讲解**高斯过程**（**GP**）。正如我们在*第5章*中看到的，监督学习是学习一个函数![](img/B22389_17_098.png)，其中![](img/B22389_05_001.png)是我们感兴趣的预测量，*h*是我们学习的函数，*X*是输入数据，而![](img/B22389_07_003.png)表示模型参数。因此，高斯过程将这个函数假设为一个概率分布，并使用贝叶斯推断，通过我们可用于训练的数据来更新该函数的后验。这是一种与数据学习截然不同的方式，并且本质上是概率性的。
- en: There is one more term left, which is approximation. In many cases, the complex
    posterior distributions in Bayesian models make them intractable. So, we have
    a technique called *Variational Inference* in which we use a known parametric
    distribution family, *q*, and find a member that is closest to the true posterior.
    Getting into details of Variational Inference and GPs is out of the scope of the
    book, but I have added a few links in *Further reading* for those of you who are
    interested.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个术语，就是近似。在许多情况下，贝叶斯模型中的复杂后验分布使其无法直接计算。因此，我们有一种称为*变分推断*的技术，其中我们使用一个已知的参数分布族*q*，并找到最接近真实后验的成员。详细讨论变分推断和高斯过程（GP）超出了本书的范围，但我在*进一步阅读*中为有兴趣的人提供了一些链接。
- en: So, coming back to dropouts, Yarin Gal et al. showed that a neural network defined
    with dropout layers before each of the weight layers is in fact a Bayesian approximation
    of a GP. So, if the model with dropout is a GP and GP is a posterior over functions,
    this should give us a probabilistic output, right? But here, we don’t have a well-defined
    parametric probability distribution like the Normal distribution for us to analytically
    calculate the properties (like the mean, standard deviation, or quantiles) of
    the distribution. How do we do that?
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 回到dropout，Yarin Gal等人展示了在每个权重层之前定义带有dropout层的神经网络，实际上是GP的贝叶斯近似。因此，如果带有dropout的模型是GP，并且GP是一个函数的后验分布，那么这应该给我们一个概率输出，对吧？但是在这里，我们没有像正态分布这样的明确定义的参数化概率分布，可以让我们分析地计算分布的属性（例如均值、标准差或分位数）。那我们该怎么做呢？
- en: Remember the discussion at the start of the Quantile Function section where
    we said that if we have *N* samples drawn from a distribution and if that *N*
    is sufficiently large, we can approximate the properties of the distribution?
    We have a name for that, and it’s called *Monte Carlo sampling*. *Monte Carlo
    sampling* is a computational technique used to estimate the statistical properties
    of a distribution by generating many random samples from that distribution. Bringing
    this idea to the dropout-enabled neural network, we can assess the properties
    of the posterior probability distribution of the function using Monte Carlo sampling,
    which means we need to keep dropout turned on during inference and sample from
    the posterior by executing the forward pass *N* times.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们在量化函数章节开始时讨论过的内容：如果我们从一个分布中抽取*N*个样本，并且*N*足够大，我们可以近似地估计该分布的属性？我们把这个过程叫做*蒙特卡洛采样*。*蒙特卡洛采样*是一种计算技术，用于通过从分布中生成许多随机样本来估计分布的统计属性。将这一理念应用到启用了dropout的神经网络中，我们可以通过蒙特卡洛采样评估函数后验概率分布的属性，这意味着我们需要在推理时保持dropout开启，并通过执行*N*次前向传播从后验中进行采样。
- en: Theoretical justification allows us to apply dropout to any neural network,
    getting uncertainty estimates through a simple operation. Isn’t the simplicity
    of that beautiful?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上的依据使我们可以将dropout应用于任何神经网络，通过简单的操作获得不确定性估计。难道这份简洁性不美吗？
- en: 'So, all of that boils down to these simple steps to get prediction intervals
    for our forecasts:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，所有这些归结为以下简单步骤来获取预测区间：
- en: Pick any deep learning architecture.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择任意一种深度学习架构。
- en: Insert Dropout Layers before every major operation and set them to a value ![](img/B22389_16_143.png),
    where ![](img/B22389_17_102.png).
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个重要操作之前插入Dropout层，并将其设置为一个值 ![](img/B22389_16_143.png)，其中 ![](img/B22389_17_102.png)。
- en: After training the model, with dropout enabled, do *N* forward passes.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练完模型并启用dropout后，执行*N*次前向传播。
- en: Using the *N* samples, estimate the median (for the point prediction), and the
    quantiles corresponding to the defined confidence levels for prediction intervals.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*N*个样本，估计中位数（用于点预测），以及对应于定义的置信水平的分位数，以获得预测区间。
- en: This sounds simple enough, but it’s complicated for just one reason. Both `PyTorch`
    and `Tensorflow` are designed in a way that dropouts are turned off during inference.
    In `PyTorch`, we can indicate if the model is in the training phase or inference
    phase by doing `model.train()` or `model.eval()`, respectively. And most popular
    implementations that wrap `PyTorch` to make training easy and automated (like
    `PyTorch Lightning`) do this `model.eval()` step in the backend before predicting.
    So, when using libraries like `neuralforecast` (which uses `PyTorch Lightning`
    in the background), turning on dropout during prediction isn’t easy.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来足够简单，但因为一个原因，它其实很复杂。`PyTorch`和`Tensorflow`的设计方式是dropout在推理阶段会被关闭。在`PyTorch`中，我们可以通过`model.train()`或`model.eval()`来分别指示模型处于训练阶段或推理阶段。而大多数流行的实现（例如`PyTorch
    Lightning`）在后台会在预测之前执行`model.eval()`步骤。因此，当使用像`neuralforecast`这样的库（它在后台使用`PyTorch
    Lightning`）时，在预测时启用dropout并不容易。
- en: 'Before we learn how to implement MC Dropout for neuralforecast models, let’s
    take a slight detour and learn how to define custom models in neuralforecast.
    It is useful when you want to tweak any model for your use case. And we are doing
    it here because of two reasons:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们学习如何为neuralforecast模型实现MC Dropout之前，让我们稍作绕行，学习如何在neuralforecast中定义自定义模型。这在你需要根据自己的使用案例调整任何模型时非常有用。我们之所以在这里做，是有两个原因：
- en: I wanted to show you how to define a new model in neuralforecast.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我想向你展示如何在neuralforecast中定义一个新模型。
- en: I wanted a model that is ideal for the MC Dropout technique, i.e., the model
    needs to have dropouts before every weight/layer.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我希望模型能很好地适配MC Dropout技术，也就是说，模型需要在每一层/权重之前都有dropout操作。
- en: Creating a custom model in neuralforecast
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在neuralforecast中创建自定义模型
- en: 'Now it’s time to have some fun and define a custom `PyTorch` model that works
    with `neuralforecast`. To keep things simple, let’s use a model that is a small
    tweak on **D-Linear** we learned about in *Chapter 16*. Besides the linear trend
    and seasonality, we also add a component for non-linear trend. Let’s give it a
    wacky name as well—**D-NonLinear**. The architecture would be something like this:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候开始一些有趣的事情了，定义一个自定义的`PyTorch`模型，使其能够与`neuralforecast`一起使用。为了简化起见，我们将使用一个基于我们在*第16章*中学到的**D-Linear**的小改进模型。除了线性趋势和季节性之外，我们还添加了一个非线性趋势的组件。让我们也给它取个奇怪的名字——**D-NonLinear**。模型架构大致如下：
- en: '![](img/B22389_17_12.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_12.png)'
- en: 'Figure 17.12: D-NonLinear model architecture'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.12：D-NonLinear 模型架构
- en: Now, let’s understand how to write a model that would work with `neuralforecast`.
    All models in `neuralforecast` are inherited from one of three classes—`BaseWindows`,
    `BaseRecurrent`, or `BaseMultivariate`. The documentation clearly explains the
    purpose of `BaseWindows`, which is exactly what we require for our use case. We
    need to sample windows from a time series while training.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解如何编写一个与`neuralforecast`兼容的模型。所有`neuralforecast`中的模型都继承自三种类之一——`BaseWindows`、`BaseRecurrent`或`BaseMultivariate`。文档清楚地解释了`BaseWindows`的目的，这正是我们在此用例中所需的。我们需要在训练时从时间序列中采样窗口。
- en: 'One more thing we need to keep in mind is that `neuralforecast` uses `PyTorch
    Lightning` under the hood for training. This link has more details on how to define
    a new model for `neuralforecast`: [https://nixtlaverse.nixtla.io/neuralforecast/docs/tutorials/adding_models.html](https://nixtlaverse.nixtla.io/neuralforecast/docs/tutorials/adding_models.html).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要记住的一点是，`neuralforecast`在后台使用`PyTorch Lightning`进行训练。这个链接提供了有关如何为`neuralforecast`定义新模型的更多详细信息：[https://nixtlaverse.nixtla.io/neuralforecast/docs/tutorials/adding_models.html](https://nixtlaverse.nixtla.io/neuralforecast/docs/tutorials/adding_models.html)。
- en: 'If you aren’t aware of **Object-Oriented Programming** (**OOP**) and **inheritance**,
    then it might be difficult for you to understand what we are doing here. Inheritance
    allows a child class to inherit all the attributes and methods defined in a parent
    class. This allows developers to define common functionalities in a base class
    and then inherit that class to get all the functionality and then add on top of
    that any specific functionality you want to add to a class. It is highly recommended
    that you understand inheritance, not only for this example but also to become
    a better developer in general. There are hundreds of tutorials on the internet,
    and I’m linking one here: [https://ioflood.com/blog/python-inheritance](https://ioflood.com/blog/python-inheritance).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉**面向对象编程**（**OOP**）和**继承**，那么你可能会很难理解我们在这里做的事情。继承允许子类继承父类中定义的所有属性和方法。这使得开发人员可以在基类中定义通用功能，然后继承该类以获得所有功能，再在此基础上添加任何你想要添加的特定功能。强烈建议你理解继承，不仅仅是为了这个例子，更是为了成为一个更好的开发者。网上有成百上千的教程，我这里链接一个：[https://ioflood.com/blog/python-inheritance](https://ioflood.com/blog/python-inheritance)。
- en: The full code for the model can be found in `src/dl/nf_models.py`, but we will
    look at key portions of the model definition right here.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的完整代码可以在`src/dl/nf_models.py`中找到，但我们将在这里查看模型定义的关键部分。
- en: Let’s start by defining the `__init__` function (only including relevant portions
    here; refer to the Python file for the full class definition).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从定义`__init__`函数开始（这里只包含相关部分；完整的类定义请参见Python文件）。
- en: '[PRE22]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We have now defined part of the `__init__` function. Now, let’s initialize the
    different layers needed in the rest of the method. We have a series decomposition
    layer that uses a moving average to split the input into a trend and seasonal
    component, a linear trend predictor and seasonality predictor that takes the linear
    trend and seasonality and projects it into the future, and a non-linear predictor
    that takes in the original input and projects into the future.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经定义了`__init__`函数的一部分。接下来，让我们初始化在方法的其余部分中需要的不同层。我们有一个系列分解层，它使用移动平均将输入分割为趋势和季节性分量，还有一个线性趋势预测器和季节性预测器，负责将线性趋势和季节性分量投影到未来，最后是一个非线性预测器，它接受原始输入并将其投影到未来。
- en: '[PRE23]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, let’s define the `forward` method. The `forward` method should have just
    one argument that is a dictionary of different inputs:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义 `forward` 方法。`forward` 方法应该只有一个参数，它是一个包含不同输入的字典：
- en: '`insample_y`: The context window of the target time series we have to predict'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`insample_y`：我们需要预测的目标时间序列的上下文窗口'
- en: '`futr_exog`: The exogenous variables for the future'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`futr_exog`：未来的外生变量'
- en: '`hist_exog`: The exogenous variables for the context window'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hist_exog`：上下文窗口的外生变量'
- en: '`stat_exog`: The static variables'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stat_exog`：静态变量'
- en: 'For this use case, we only need the `insample_y` since our model doesn’t use
    any other information. So, this is the `forward` method implementation:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个用例，我们只需要 `insample_y`，因为我们的模型不使用其他信息。所以，这是 `forward` 方法的实现：
- en: '[PRE24]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The code is pretty straightforward. The only thing we need to ensure to align
    to `neuralforecast` models is to take the data we need from the input dictionary
    and call `self.loss.domain_map` at the end so that it is mapped to the right output
    size depending on the loss. Now, this model will function just like any other
    model in the neuralforecast library.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 代码非常直观。我们唯一需要确保与 `neuralforecast` 模型对齐的地方是，从输入字典中提取我们需要的数据，并在最后调用 `self.loss.domain_map`，这样它就会根据损失函数映射到正确的输出大小。现在，这个模型将像
    neuralforecast 库中的任何其他模型一样运行。
- en: Now, let’s get back to MC Dropout and its implementation.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到 MC Dropout 及其实现。
- en: Forecasting with MC Dropout (neuralforecast)
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 MC Dropout 进行预测（neuralforecast）
- en: We said it wasn’t easy to implement MC Dropout in frameworks like `neuralforecast`
    and `PyTorch Lightning`, but just because something isn’t easy shouldn’t stop
    us from doing it. All we need to do is to make sure the dropouts are enabled during
    prediction and take multiple samples. If you are writing your own `PyTorch` training
    code, then it’s as simple as not calling `model.eval()` before predicting. But
    the best practice is to just make the dropouts into train mode and not the whole
    model. There may be layers like batch normalization, which also behave differently
    during inference, which might be affected. Let’s see a handy method that makes
    all dropout layers into train mode.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，像 `neuralforecast` 和 `PyTorch Lightning` 这样的框架中实现 MC Dropout 并不容易，但仅仅因为某件事情不容易并不应该阻止我们去做。我们只需要确保在预测过程中启用
    dropout，并进行多次采样。如果你正在编写自己的 `PyTorch` 训练代码，那么只需确保在预测前不调用 `model.eval()`。但最佳做法是将
    dropout 层置于训练模式，而不是整个模型。可能会有像批量归一化这样的层，它们在推理时的行为也不同，可能会受到影响。让我们看看一个便捷的方法，将所有 dropout
    层都设置为训练模式。
- en: '[PRE25]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: For `neuralforecast`, we have put together a recipe with which we can use MC
    Dropout for any of their models (provided they have enough dropouts). Now, we
    are going to use the custom `DNonLinear` model we just defined. Note that each
    component in the definition starts with a dropout layer so that we can apply MC
    Dropout with no qualms.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `neuralforecast`，我们已经准备了一份配方，可以使用 MC Dropout 处理他们的任何模型（前提是模型有足够的 dropout）。现在，我们将使用我们刚定义的自定义
    `DNonLinear` 模型。请注意，定义中的每个组件都以 dropout 层开始，这样我们就可以毫无顾虑地应用 MC Dropout。
- en: '**Notebook alert**:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本警告**：'
- en: To follow along with the complete code, use the notebook named `06-Prediction_Intervals_MCDropout.ipynb`
    in the `Chapter17` folder.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随完整的代码，请使用 `Chapter17` 文件夹中的名为 `06-Prediction_Intervals_MCDropout.ipynb` 的笔记本。
- en: If you remember, we used `PyTorch Lightning` back in *Chapter 13* and explained
    that it’s pretty much standard `PyTorch` code, but organized in a specified form—`training_step`,
    `validation_step`, `predict_step`, `configure_optimizers`, etc. For a refresher,
    head back to *Chapter 13* and *Further reading* in the chapter to learn more about
    how to migrate from `PyTorch` to `PyTorch Lightning`. Since neuralforecast is
    already using `PyTorch Lightning` in the backend, the `BaseWindows` that we are
    inheriting is already a `PyTorch Lightning` model. This information is essential
    because we need to essentially modify the `predict_step` method to implement our
    MC Dropout.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得的话，我们在 *第13章* 使用了 `PyTorch Lightning`，并解释了它基本上是标准的 `PyTorch` 代码，但以指定的形式组织——`training_step`、`validation_step`、`predict_step`、`configure_optimizers`
    等。如果需要复习，请回顾 *第13章* 和本章中的 *进一步阅读* 部分，了解更多关于如何从 `PyTorch` 迁移到 `PyTorch Lightning`
    的信息。由于 neuralforecast 已经在后台使用了 `PyTorch Lightning`，我们继承的 `BaseWindows` 本身就是一个
    `PyTorch Lightning` 模型。这个信息非常重要，因为我们实际上需要修改 `predict_step` 方法来实现我们的 MC Dropout。
- en: Using the same inheritance we used to inherit `BaseWindows`, we can inherit
    the `DNonLinear` class we defined earlier and make a few changes so that it becomes
    an MC Dropout model. And for that, all we need to re-define is the `predict_step`
    method. The `predict_step` method is the method `PyTorch Lightning` calls every
    time it has to get a prediction for a batch. So, instead of taking the predictions
    as is, we need to keep the dropout enabled, take *N* samples from *N* forward
    passes, calculate the prediction intervals and median (point forecast), and return
    it.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前用来继承`BaseWindows`的相同方法，我们可以继承我们早前定义的`DNonLinear`类，并做一些修改，使其成为一个MC Dropout模型。为此，我们只需要重新定义`predict_step`方法。`predict_step`是`PyTorch
    Lightning`每次需要获取批次预测时调用的方法。所以，我们不直接使用现有的预测，而是需要保持dropout启用，从*N*次前向传递中获取*N*个样本，计算预测区间和中位数（点预测），并返回它。
- en: '[PRE26]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Are we done yet? Not quite. There is just one little thing to be done. In *Chapter
    16*, we used a class called `NeuralForecast` for all the fitting and predicting
    of `neuralforecast` models. This is like a wrapper class that does the heavy lifting
    of preparing the inputs and outputs in the right way before calling the underlying
    models. This class has to be aware that we have tweaked the `predict_step` and
    therefore, we need to make a small change there. The solution is more of a hack
    than a principled way of editing, but a hack is just as good if it achieves the
    purpose. I have done the snooping around the implementation to figure out the
    best way to hack `NeuralForecast` to enable our MCDropout inference. There is
    no short way of explaining the hack, but just understand that I have misused the
    way `neuralforecast` flexibly produces point forecasts and prediction intervals
    based on different losses. So, here is the re-defined `NeuralForecast` class with
    a hack in the `predict` method.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了吗？还没有。只剩下一个小事情需要做。在*第16章*中，我们使用了一个名为`NeuralForecast`的类来进行`neuralforecast`模型的所有拟合和预测。这个类就像一个包装类，在调用底层模型之前，负责以正确的方式准备输入和输出。这个类必须知道我们已经修改了`predict_step`，因此我们需要在这里做一个小改动。这个解决方案更多的是一个“黑客”方法，而不是一种有原则的编辑方式，但如果能达到目的，黑客方法也不失为一个好方法。我已经对实现进行了探索，以找出修改`NeuralForecast`类以支持我们的MCDropout推断的最佳方法。没有简短的解释方法，但请理解，我错误地使用了`neuralforecast`根据不同损失灵活生成点预测和预测区间的方式。所以，下面是重新定义的`NeuralForecast`类，其中`predict`方法做了一个黑客修改。
- en: '[PRE27]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: That’s it. We have successfully “hacked” the library to do our bidding. In addition
    to this being an MC Dropout tutorial, it’s also a tutorial on how to hack a library
    to do what you want it to do. It is important to note that this doesn’t make you
    a “hacker,” so stop before you update your LinkedIn title.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们成功地“黑进”了这个库，让它按我们的意愿工作。除了这篇文章是一个MC Dropout教程外，它还是一个关于如何“黑进”一个库让它做你想让它做的事的教程。需要注意的是，这并不会让你成为一个“黑客”，所以在更新你的LinkedIn头衔之前请先停手。
- en: Now, on to training the model. This is pretty much the same as you train other
    models in `neuralforecast`, but instead of the `NeuralForecast` class, you need
    to use the new `MCNeuralForecast` class we defined.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开始训练模型。这与在`neuralforecast`中训练其他模型基本相同，只是你需要使用我们定义的新`MCNeuralForecast`类，而不是`NeuralForecast`类。
- en: '[PRE28]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once the training is finished, we can generate the predictions like this:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们可以像这样生成预测：
- en: '[PRE29]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The output is exactly like any other model from `neuralforecast` with the prediction
    intervals formatted as `<ModelName>-lo-<level>` and `<ModelName>-hi-<level>`.
    The point forecast can be found under `<ModelName>-median`. In this case, `<ModelName>`
    would be `MCDropoutDNonLinear`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 输出与`neuralforecast`的其他模型完全一样，预测区间的格式为`<ModelName>-lo-<level>`和`<ModelName>-hi-<level>`。点预测可以在`<ModelName>-median`下找到。在这种情况下，`<ModelName>`是`MCDropoutDNonLinear`。
- en: Let’s look at the plot of the forecasts and the metrics.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看预测结果和指标的图表。
- en: '![](img/B22389_17_13_01.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_13_01.png)'
- en: '![](img/B22389_17_13_02.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_13_02.png)'
- en: 'Figure 17.13: Forecast with prediction intervals using MC Dropout'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.13：使用MC Dropout的预测和预测区间
- en: '![](img/B22389_17_14.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_14.png)'
- en: 'Figure 17.14: Metrics for MC Dropout'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.14：MC Dropout的指标
- en: Our forecasting model does decent enough on the data; it’s nothing to write
    home about, but decent. If we do an ablation study, we might even realize that
    the non-linear component we added does absolutely nothing. But, as long as we
    had fun doing it and learned something from it, I’m happy. Now, look at the prediction
    intervals. They aren’t really smooth and have quite a bit of “noise” when you
    look at them, right? This is because of the inherent randomness in the methodology
    and may be because of insufficient learning. When we do MC Dropout, we are essentially
    relying on *N* sub-models or sub-networks and calculating the quantiles based
    on these *N* forecasts. Maybe a few of these sub-networks haven’t learned very
    well, and those outputs can skew the quantiles and thereby the prediction intervals.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测模型在数据上的表现还算不错；虽然不能说是出类拔萃，但也算得上中规中矩。如果我们做一个消融研究，可能会发现我们添加的非线性组件根本没有任何作用。但只要我们在过程中感到愉快，并且从中学到了东西，我就很满足了。现在，看看预测区间吧。它们并不平滑，观察时会有相当多的“噪声”，对吧？这是因为该方法本身存在随机性，也可能是因为学习不足。当我们使用MC
    Dropout时，我们实际上是依赖于*N*个子模型或子网络，并基于这*N*个预测计算分位数。也许其中有些子网络并没有很好地学习，而这些输出可能会扭曲分位数，从而影响预测区间。
- en: There are many criticisms of the MC Dropout method. Many in the Bayesian community
    don’t consider MC Dropout as Bayesian and consider the Variational Approximation
    that was proposed such a poor approximation that we can’t refer to what it measures
    as Bayesian uncertainty. There is an unpublished Arxiv paper by Loic Le Folgoc
    et al. called “Is MC Dropout Bayesian?” (Reference *6*), which claims that it
    isn’t. But it still doesn’t take away the fact that MC Dropout is a cheap way
    of getting uncertainty quantified. But when used in fields like medical studies,
    where uncertainty quantification is of paramount importance, we may want to take
    on something more principled.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: MC Dropout方法有许多批评声音。贝叶斯社区中的许多人不认为MC Dropout是贝叶斯方法，认为其提出的变分逼近是如此差劲的近似，以至于我们不能将其度量的东西称为贝叶斯不确定性。Loic
    Le Folgoc等人有一篇未发表的Arxiv论文，名为“MC Dropout是贝叶斯方法吗？”（参考文献*6*），声称MC Dropout并非贝叶斯方法。但这并不改变MC
    Dropout作为一种量化不确定性的廉价方法的事实。然而，在医学研究等领域，不确定性量化至关重要的情况下，我们可能希望采用更加原理化的方法。
- en: We can also notice that the coverage is quite bad across all time series. And
    this is, again, something that is exhibited across different studies. In 2023,
    Nicolas Dewolf et al. published a study comparing different ways of uncertainty
    quantification for regression problems (Reference *7*). They found that MC Dropout
    has one of the worst performances in both coverage and average length, underlying
    the claim that MC Dropout is a very dirty approximation of the uncertainty.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以注意到，在所有时间序列中，覆盖率表现得相当糟糕。这同样是在不同研究中展现出来的。在2023年，Nicolas Dewolf等人发表了一项研究，比较了回归问题中不同的不确定性量化方法（参考文献*7*）。他们发现，MC
    Dropout在覆盖率和平均长度方面表现最差，进一步验证了MC Dropout是对不确定性的非常粗糙的近似。
- en: Now, let’s look at another technique for probabilistic forecasting that promises
    theoretical guarantees for perfect coverage and has become quite the rage in the
    last few years.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看另一种有理论保证完美覆盖的概率预测技术，近年来已经成为炙手可热的趋势。
- en: Conformal Prediction
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适应性预测
- en: What if I tell you that there is a technique for generating prediction intervals
    that statistically guarantees perfect coverage, can work on any model, and doesn’t
    require us to make any assumptions about the output distribution? Conformal Prediction
    is just that. Conformal Prediction is a method that helps machine learning models
    make reliable predictions by estimating how uncertain the model is. Conformal
    Prediction provides robust, statistically valid measures of uncertainty for any
    machine learning model, ensuring reliable and trustworthy predictions in critical
    applications.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我告诉你有一种技术能够生成预测区间，并且从统计学上保证完美覆盖，能够适用于任何模型，而且无需我们对输出分布做任何假设呢？适应性预测就是这样的一种方法。适应性预测是一种帮助机器学习模型做出可靠预测的技术，通过估算模型的不确定性来实现。适应性预测为任何机器学习模型提供了稳健、统计上有效的不确定性量化方法，确保在关键应用中做出可靠且可信的预测。
- en: Although it was proposed as early as 2005 by Vladmir Vovk *(*Reference *8*),
    it picked up interest in the last couple of years. Let’s first understand the
    basic principles of Conformal Prediction using a classification example and then
    see how we can do it for regression and time series examples.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管早在2005年，Vladmir Vovk就提出了这一方法*（*参考文献*8*），但它在过去几年受到了更多关注。让我们首先理解一下使用分类示例的符合预测的基本原理，然后看看如何将其应用于回归和时间序列示例。
- en: Conformal Prediction for classification
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类的符合预测
- en: 'Let’s start with a trained model, ![](img/B22389_17_103.png), which outputs
    estimated probabilities (*softmax* scores) for *K* output classes (![](img/B22389_17_104.png)).
    It doesn’t matter what this model is; it can be a machine learning model, a deep
    learning model, or even a rule-based model. We have training data, ![](img/B22389_17_105.png),
    and test data, ![](img/B22389_17_106.png). Now, we need a small amount of additional
    data (other than training and test) called *calibration data*, ![](img/B22389_17_107.png).
    Now, what do we want from this? Using ![](img/B22389_17_103.png) and ![](img/B22389_17_109.png),
    we want to create a prediction set of possible labels, ![](img/B22389_17_110.png)
    that makes sure that the probability that a test data point is part of that set
    is almost exactly the user-defined error rate, ![](img/B22389_04_009.png) (we
    will be talking about error rates throughout this discussion. A 10% error rate
    is a 90% confidence level). This is exactly what Conformal Prediction guarantees.
    It’s called the *marginal coverage* guarantee and it can be written more formally
    as:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个已训练的模型开始，![](img/B22389_17_103.png)，它输出*K*个输出类别的估计概率（*softmax*得分）（![](img/B22389_17_104.png)）。这个模型是什么并不重要；它可以是机器学习模型、深度学习模型，甚至是基于规则的模型。我们有训练数据，![](img/B22389_17_105.png)，和测试数据，![](img/B22389_17_106.png)。现在，我们需要一些额外的数据（除了训练和测试数据）叫做*校准数据*，![](img/B22389_17_107.png)。那么，我们从这些数据中想要什么呢？通过使用![](img/B22389_17_103.png)和![](img/B22389_17_109.png)，我们想要创建一个可能的标签预测集合，![](img/B22389_17_110.png)，确保测试数据点属于该集合的概率几乎正好是用户定义的误差率，![](img/B22389_04_009.png)（在整个讨论中，我们将讨论误差率。10%的误差率意味着90%的置信水平）。这正是符合预测所保证的内容。它被称为*边际覆盖*保证，可以更正式地写为：
- en: '![](img/B22389_17_112.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_112.png)'
- en: Naturally, you may have this question in your mind. What is this ![](img/B22389_17_113.png)?
    This term signifies that the coverage guarantee is derived from a finite sample
    of size *n*. Since *n* is in the denominator, we know as and when *n* increases,
    this term becomes smaller and smaller. Extending this to the limit, we know that
    if ![](img/B22389_17_114.png), this term would be zero and the coverage would
    be exactly ![](img/B22389_17_016.png).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，你可能会在心中产生这个问题。这个 ![](img/B22389_17_113.png) 是什么？这个术语表示覆盖保证是从一个有限大小为*n*的样本中得出的。由于*n*在分母中，我们知道当*n*增大时，这个术语会变得越来越小。将其扩展到极限，我们知道如果
    ![](img/B22389_17_114.png)，这个术语将为零，覆盖度将正好是 ![](img/B22389_17_016.png)。
- en: 'We know what we want, but how do we get it? The core idea in conformal prediction
    is very simple and can be laid out in four steps:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们想要什么，但我们该如何实现呢？符合预测的核心思想非常简单，可以分为四个步骤：
- en: Identify a heuristic notion of uncertainty using the trained model. In our classification
    example, this can be the softmax scores.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的模型识别不确定性的启发式概念。在我们的分类示例中，这可以是softmax得分。
- en: Define a score function, ![](img/B22389_17_116.png), which is also called *Non-Conformity
    Scores*. This can be a score that takes in the prediction, ![](img/B22389_05_001.png),
    and actual value, ![](img/B22389_17_118.png), and gives a score that encodes the
    disagreement between them. The higher the score is, the larger the disagreement
    is. In the classification example, this would be something as simple as ![](img/B22389_17_119.png).
    In simple English, this means taking the softmax score of the correct class and
    doing ![](img/B22389_17_120.png).
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个得分函数，![](img/B22389_17_116.png)，也称为*非一致性得分*。这个得分可以是输入预测值，![](img/B22389_05_001.png)，和实际值，![](img/B22389_17_118.png)，并给出一个表示它们之间不一致的得分。得分越高，不一致性越大。在分类示例中，这可以是像
    ![](img/B22389_17_119.png) 这样简单的东西。用简单的英语来说，这意味着计算正确类别的softmax得分，并做出 ![](img/B22389_17_120.png)。
- en: Compute ![](img/B22389_17_121.png) as the ![](img/B22389_17_122.png) quantile
    of the calibration scores. We use the calibration data and score function to calculate
    calibration scores and calculate the quantile on that data. The ![](img/B22389_17_123.png)
    is the quantile calculation is again derived from the finite sample correction.
    As ![](img/B22389_17_124.png) tends to infinity, the term tends to zero.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算![](img/B22389_17_121.png)作为校准得分的![](img/B22389_17_122.png)分位数。我们使用校准数据和得分函数来计算校准得分，并计算该数据上的分位数。![](img/B22389_17_123.png)的分位数计算仍然来源于有限样本修正。当![](img/B22389_17_124.png)趋近于无穷大时，该项趋近于零。
- en: 'Use this quantile to form prediction sets for new examples: ![](img/B22389_17_125.png).
    This means selecting all the items from the output set that has a score (according
    to the score function) greater than the threshold, ![](img/B22389_17_121.png).'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个分位数为新的示例形成预测集：![](img/B22389_17_125.png)。这意味着从输出集中选择所有得分（根据得分函数）大于阈值的项，![](img/B22389_17_121.png)。
- en: This simple technique will give us prediction sets that guarantee to satisfy
    the marginal coverage, no matter what model is used or what the distribution of
    the data is. Let’s see how simple this is using `Python` code and assuming that
    the model we are talking about is a `scikit-learn` classifier.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的技术将为我们提供预测集，保证满足边际覆盖要求，无论使用什么模型或数据分布如何。让我们看看使用`Python`代码时，这有多简单，并假设我们讨论的模型是一个`scikit-learn`分类器。
- en: We have a trained model, `model`, calibration data, `X_calib`, and test data,
    `X_test`. For full code and some visualizations, check the notebook.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个训练好的模型，`model`，校准数据，`X_calib`，以及测试数据，`X_test`。完整的代码和一些可视化请参阅笔记本。
- en: '**Notebook alert**:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒**：'
- en: To follow along with the complete code, use the notebook named `07-Understanding_Conformal_Prediction.ipynb`
    in the `Chapter17` folder.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随完整的代码，使用位于`Chapter17`文件夹中的名为`07-Understanding_Conformal_Prediction.ipynb`的笔记本。
- en: '[PRE30]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now, let’s think about the prediction set, ![](img/B22389_17_127.png). We have
    been defining it as set-valued with discrete classes for the classification scenario.
    This set becomes larger or smaller based on how confident the initial heuristic
    estimate of uncertainty is.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们思考预测集，![](img/B22389_17_127.png)。我们一直将其定义为在分类场景下具有离散类别的集合值。这个集合的大小会根据初始启发式不确定性估计的置信度而增大或减小。
- en: Conformal Prediction for regression
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归的符合性预测
- en: 'Let’s extend this notion of prediction sets to regression. In regression, the
    output space is continuous rather than discrete, and we aim to construct continuous
    prediction sets, which are typically a continuous interval in ![](img/B22389_17_128.png).
    The idea is to maintain the same principle of coverage: the prediction interval
    should contain the true value with high probability. So, now the prediction set,
    ![](img/B22389_17_127.png), that we saw earlier is also the prediction interval
    in the regression context. But along with the change in the interpretation of
    prediction sets, we will also need to change the score function, which calculates
    non-conformity scores. A common score function that is used is the distance to
    the conditional mean, ![](img/B22389_17_130.png) (Reference *10*). When we have
    a trained model, ![](img/B22389_17_103.png), we can consider the output of the
    model as the conditional mean, which will make this the absolute residual value
    for each point.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将预测集的概念扩展到回归问题。在回归中，输出空间是连续的，而不是离散的，我们的目标是构建连续的预测集，通常是![](img/B22389_17_128.png)中的一个连续区间。这个想法是保持相同的覆盖原则：预测区间应该以较高的概率包含真实值。因此，现在我们之前看到的预测集，![](img/B22389_17_127.png)，也是回归背景下的预测区间。但是随着预测集解释的变化，我们也需要更改得分函数，用于计算不一致性得分。一个常用的得分函数是与条件均值的距离，![](img/B22389_17_130.png)（参考*10*）。当我们有一个训练好的模型，![](img/B22389_17_103.png)，我们可以将模型的输出视为条件均值，这将使得每个点的绝对残差值。
- en: '![](img/B22389_17_132.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_132.png)'
- en: Note that this score satisfies the condition. The larger the deviation, the
    larger the “heuristic” measure of uncertainty is. The rest of the procedure remains
    almost the same—calculating the quantile, ![](img/B22389_17_121.png), and forming
    the prediction intervals, ![](img/B22389_17_134.png)
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个得分满足条件。偏差越大，“启发式”不确定性度量越大。其余的过程几乎保持不变——计算分位数，![](img/B22389_17_121.png)，并形成预测区间，![](img/B22389_17_134.png)。
- en: Let’s check how the Python code changes (full code is in the notebook).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下Python代码如何变化（完整代码见笔记本）。
- en: '[PRE31]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: It’s as simple as that. We can check coverage and see that it will be greater
    than 90%, which is the error rate we defined with ![](img/B22389_17_009.png).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单。我们可以检查覆盖情况，看到它将大于 90%，这是我们用 ![](img/B22389_17_009.png) 定义的误差率。
- en: '**Practitioner’s Note**'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '**实践者注意事项**'
- en: In many use cases, we will be training a single model for multiple entities
    or groups we care about. For instance, for the global forecasting models we talked
    about in *Chapter 10*, we use a single regression model for multiple time series.
    In such cases, we can also run Conformal Prediction on each time series or groups
    of time series separately to be more adaptive to the errors in that subset. This
    would allow for coverage guarantees at the group/time series level.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用场景中，我们将为多个实体或我们关心的群体训练一个单一模型。例如，对于我们在*第10章*中讨论的全球预测模型，我们使用一个回归模型来处理多个时间序列。在这种情况下，我们也可以对每个时间序列或时间序列组分别进行共形预测，以更好地适应该子集中的误差。这将允许在群体/时间序列层面提供覆盖保证。
- en: But you might have noticed something. In this method, we have the same width
    of the interval all throughout. But we would expect the intervals to be tighter
    when the model is more confident and wider when it isn’t (let’s call it *adaptive
    prediction intervals* from now on). Let’s look at another technique that has this
    property.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可能注意到了一些问题。在这种方法中，我们始终使用相同宽度的区间。但我们期望当模型更有信心时，区间应该更紧，而当模型不确定时，区间应该更宽（从现在起我们称之为*自适应预测区间*）。让我们看看另一种具有此特性的技术。
- en: Conformalized Quantile Regression
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共形化分位回归
- en: We learned about Quantile Regression as one of the methods for probabilistic
    forecast (or regression in a general case). Quantile Regression is powerful in
    the sense that it does not require us to have any prior assumption about the underlying
    output distribution. But it does not enjoy the coverage guarantees that Conformal
    Prediction offers. In 2019, Yaniv Roano et al. (Reference *11*) brought the best
    of both worlds into **Conformalized Quantile Regression** (**CQR**). They proposed
    a way to take the quantile forecasts and conformalize them such that they have
    the coverage guarantees that conformal prediction assures.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解了分位回归作为概率预测方法（或一般回归的一种形式）。分位回归之所以强大，是因为它不要求我们对潜在的输出分布做任何先验假设。但它并没有享受共形预测所提供的覆盖保证。在2019年，Yaniv
    Roano 等人（参考文献 *11*）将两者的优点结合到了**共形化分位回归**（**CQR**）中。他们提出了一种方法，可以将分位预测进行共形化，使其具备共形预测所保证的覆盖保证。
- en: In this case, the model to be used has a restriction. It should be a model that
    outputs quantile predictions. And, from our earlier discussion, we know that if
    the error rate is ![](img/B22389_04_009.png), then the quantiles we need for the
    prediction intervals are ![](img/B22389_17_137.png) and ![](img/B22389_17_138.png).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，所使用的模型有一个限制。它应该是一个能够输出分位预测的模型。并且，结合我们之前的讨论，我们知道如果误差率为 ![](img/B22389_04_009.png)，那么我们需要的预测区间的分位数是
    ![](img/B22389_17_137.png) 和 ![](img/B22389_17_138.png)。
- en: So, the quantile model predicts ![](img/B22389_17_139.png) and ![](img/B22389_17_138.png).
    By definition, if ![](img/B22389_17_139.png) and ![](img/B22389_17_138.png) are
    true estimations of the real quantiles, a quantile regression alone will have
    perfect coverage. But the model fit may not be perfect and that would result in
    sub-par coverage. We will use conformal prediction to correct the quantiles based
    on calibration data such that we get the perfect coverage promised by conformal
    prediction.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，分位模型预测 ![](img/B22389_17_139.png) 和 ![](img/B22389_17_138.png)。根据定义，如果 ![](img/B22389_17_139.png)
    和 ![](img/B22389_17_138.png) 是真实分位数的准确估计，单独使用分位回归将具有完美的覆盖率。但模型拟合可能并不完美，这将导致低于标准的覆盖率。我们将使用共形预测来根据校准数据修正分位数，从而实现共形预测所承诺的完美覆盖。
- en: Yaniv Roano et al. proposed to use a new non-conformity score function for Quantile
    Regression.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: Yaniv Roano 等人提出使用一种新的非共形得分函数来处理分位回归。
- en: '![](img/B22389_17_143.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_143.png)'
- en: Let’s take a beat and explore the score function using the diagram in *Figure
    17.15*.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍作停顿，使用*图 17.15*中的示意图来探索得分函数。
- en: '![](img/B22389_17_15.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_15.png)'
- en: 'Figure 17.15: Illustration of score function for Conformalized Quantile Regression'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.15：用于共形化分位回归的得分函数示意图
- en: There are two terms in the max operator. If the true value, ![](img/B22389_17_118.png),
    is between the two quantiles, ![](img/B22389_17_137.png) and ![](img/B22389_17_138.png),
    both the terms inside will be negative and will be the distance to the nearest
    prediction interval (see points B and C in *Figure 17.15*).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 最大操作符中有两个项。如果真实值，![](img/B22389_17_118.png)，位于两个分位数之间，![](img/B22389_17_137.png)和![](img/B22389_17_138.png)，那么两个项的值都为负数，并表示到最近预测区间的距离（参见*图
    17.15*中的B点和C点）。
- en: 'Now, let’s look at point A (3), which is above the higher quantile. The quantiles
    are [1, 2]. The two terms in the max operator would be:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下A点（3），它位于较高的分位数之上。分位数为[1, 2]。最大操作符中的两个项将是：
- en: '![](img/B22389_17_147.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_147.png)'
- en: '![](img/B22389_17_148.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_148.png)'
- en: This makes the score function 3 because of the max operator. And now, we see
    point D (1.6), which is below the lower quantile (quantiles are [3.5, 2]). This
    makes the score function max{0.4, -1.9}, which would be 0.4\. So, the max operator
    ensures that the score is positive if it falls outside the quantiles.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得得分函数为3，因为最大操作符的作用。现在，我们看到了D点（1.6），它位于较低的分位数之下（分位数为[3.5, 2]）。这使得得分函数为max{0.4,
    -1.9}，结果为0.4。因此，最大操作符确保得分为正，如果它位于分位数之外。
- en: Therefore, what we have is a score function that assigns positive values to
    points where the actual value falls outside the intervals and negative to points
    within. And for the points that fall outside the intervals, the way the score
    function is constructed will choose the worse error. This satisfies our requirements
    from the score function. A larger score shows larger uncertainty, and it also
    encodes a heuristic notion of uncertainty.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到的得分函数会将正值分配给实际值落在区间外的点，将负值分配给落在区间内的点。而对于那些落在区间外的点，得分函数的构造方式将选择较差的误差。这满足了我们对得分函数的要求。较大的得分表示较大的不确定性，并且它还编码了一种不确定性的启发式概念。
- en: 'Now that we have the scores, the rest of the steps are almost identical:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了得分，接下来的步骤几乎是相同的：
- en: Compute ![](img/B22389_17_121.png) as the ![](img/B22389_17_122.png) quantile
    of the calibration scores.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算![](img/B22389_17_121.png)作为标定得分的![](img/B22389_17_122.png)分位数。
- en: 'Use this quantile to form prediction sets for new examples: ![](img/B22389_17_151.png),
    i.e., we widen the exiting quantiles by ![](img/B22389_17_121.png) and get coverage
    guarantees.'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个分位数为新示例形成预测集：![](img/B22389_17_151.png)，即我们通过![](img/B22389_17_121.png)扩大现有的分位数，并获得覆盖保证。
- en: Quantile regression is one of the better ways of getting *adaptive prediction
    intervals*. Let’s look at one such technique now.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 分位回归是获得*自适应预测区间*的更好方法之一。现在，让我们来看一下这种技术。
- en: Conformalizing uncertainty estimates
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 符合化不确定性估计
- en: If we think about Conformalized Quantile Regression (from the last section)
    a bit deeply, we can realize that what the underlying quantile regression is doing
    is capturing the *uncertainty estimate* at each point in our prediction. And we
    conformalize those estimates for better coverage. If we can capture this *uncertainty
    estimate*, ![](img/B22389_17_153.png), we have some hope of conformalizing this
    to get better *adaptive prediction intervals*.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们深入思考一下符合性分位回归（来自上一节），我们可以意识到，基础的分位回归所做的事情是捕捉我们预测中每个点的*不确定性估计*。然后，我们对这些估计进行符合化处理，以获得更好的覆盖。如果我们能够捕捉到这个*不确定性估计*，![](img/B22389_17_153.png)，我们就有希望将其符合化，从而获得更好的*自适应预测区间*。
- en: 'Let’s say that we have a trained model, ![](img/B22389_17_103.png), and some
    uncertainty scalar, ![](img/B22389_17_153.png), that has high values when uncertainty
    is high, and vice versa. We can define our non-conformity score as:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个训练好的模型，![](img/B22389_17_103.png)，以及一个不确定性标量，![](img/B22389_17_153.png)，当不确定性较高时，该标量值较大，反之亦然。我们可以将我们的非符合性得分定义为：
- en: '![](img/B22389_17_156.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_156.png)'
- en: The natural interpretation of this score is that we are multiplying a correction
    factor to the standard ![](img/B22389_17_157.png). Once we have this new score,
    the rest of the process is exactly the same—taking ![](img/B22389_17_121.png)
    from the scores and forming the prediction intervals as ![](img/B22389_17_159.png).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这个得分的自然解释是，我们正在将一个修正因子乘到标准的![](img/B22389_17_157.png)上。一旦我们有了这个新得分，剩下的过程与之前完全相同——从得分中取![](img/B22389_17_121.png)，并形成预测区间，表示为![](img/B22389_17_159.png)。
- en: So, what are some ways to capture this uncertainty? (Note that this uncertainty
    measure should be capturing it at a data point level.)
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，有哪些方法可以捕捉这种不确定性呢？（请注意，这种不确定性度量应该在数据点层面捕捉它。）
- en: Assume a probability distribution and modeling their parameters (**Probability
    Density Function**). In the case of Gaussian Distribution, we would have an estimate
    of uncertainty as the standard deviation, ![](img/B22389_17_160.png), and we consider
    that as ![](img/B22389_17_153.png). ![](img/B22389_17_162.png).
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设一个概率分布，并对其参数进行建模（**概率密度函数**）。以高斯分布为例，我们可以通过标准差来估算不确定性，![](img/B22389_17_160.png)，我们认为它是![](img/B22389_17_153.png)。![](img/B22389_17_162.png)。
- en: 'Use the MC Dropout technique to generate samples and calculate the standard
    deviation of ![](img/B22389_17_163.png) from the samples: ![](img/B22389_17_164.png).'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MC Dropout技术生成样本，并计算来自样本的![](img/B22389_17_163.png)的标准差：![](img/B22389_17_164.png)。
- en: Along with the main model prediction ![](img/B22389_17_165.png), train another
    model to predict the residuals, ![](img/B22389_17_166.png), and set ![](img/B22389_17_167.png).
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在主模型预测![](img/B22389_17_165.png)的基础上，训练另一个模型来预测残差![](img/B22389_17_166.png)，并设置![](img/B22389_17_167.png)。
- en: 'Use an ensemble of models to generate multiple predictions for each data point
    and take the standard deviation of different predictions at each data point: ![](img/B22389_17_168.png).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型集成生成每个数据点的多个预测，并对每个数据点的不同预测取标准差：![](img/B22389_17_168.png)。
- en: Although the list above is not exhaustive, it does show that we can apply conformal
    prediction to almost any uncertainty estimates (including the ones we have already
    seen in the chapter). This makes the conformal prediction paradigm a very flexible
    toolkit to get coverage guarantees with a wide variety of problems. Even with
    this flexible nature, there are cases that mess with the coverage guarantees that
    the framework promises. And for our time series context, this is important to
    understand.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上面的列表并不详尽，但它表明我们可以将符合预测应用于几乎所有的不确定性估计（包括我们在本章中已经看到的那些）。这使得符合预测范式成为一个非常灵活的工具包，可以为各种问题提供覆盖保证。即便具有这种灵活性，仍然存在一些情况会影响框架所承诺的覆盖保证。在我们讨论时间序列的背景下，理解这一点非常重要。
- en: Exchangeability in Conformal Prediction and time series forecasting
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 符合预测中的可交换性与时间序列预测
- en: '*Exchangeability* is a fundamental assumption in Conformal Prediction. Exchangeability
    means that the data points are identically distributed and their joint probability
    distribution does not change when the order of the data points is changed. This
    concept ensures that past data points can reliably predict future data points.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '*可交换性*是符合预测中的一个基本假设。可交换性意味着数据点是同质分布的，并且当数据点的顺序发生变化时，它们的联合概率分布不变。这个概念确保了过去的数据点能够可靠地预测未来的数据点。'
- en: Imagine a chocolate factory producing chocolates with consistent weights. If
    the production process is highly controlled, with the same conditions and ingredients,
    the weights of the chocolates are exchangeable because their order of production
    does not affect their weight. You can sample 100 chocolates, measure their weights,
    and calculate nonconformity scores based on deviations from the predicted weight.
    Using these scores, you can form prediction intervals for future chocolates. Because
    the chocolates are exchangeable, the sample distribution represents the future
    distribution, making prediction intervals reliable.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一家巧克力工厂生产重量一致的巧克力。如果生产过程高度可控，且使用相同的条件和原料，那么巧克力的重量是可交换的，因为生产顺序不会影响其重量。你可以抽取100颗巧克力，测量它们的重量，并根据与预测重量的偏差计算不合规分数。利用这些分数，你可以为未来的巧克力制定预测区间。由于巧克力是可交换的，样本分布代表了未来的分布，从而使预测区间可靠。
- en: However, if the production process changes over time—due to machinery wear or
    different ingredient batches—the weights become non-exchangeable. The order of
    production affects the weights, making the sample distribution unrepresentative
    of future weights, leading to unreliable prediction intervals.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果生产过程随着时间的推移发生变化——由于机器磨损或不同的原料批次——那么权重就不再是可交换的。生产顺序会影响权重，使得样本分布无法代表未来的权重，从而导致预测区间不可靠。
- en: In time series data, observations are typically dependent on previous observations,
    violating the exchangeability assumption. For example, in a sales forecast, today’s
    sales may influence tomorrow’s sales due to trends, or the sales a year ago may
    influence tomorrow’s sales due to seasonal effects. This dependency means that
    the distribution of past data does not represent the distribution of future data
    accurately.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列数据中，观测值通常依赖于之前的观测值，这违反了可交换性假设。例如，在销售预测中，今天的销售可能会影响明天的销售，这可能是由于趋势因素，或者一年前的销售数据可能会影响明天的销售，这是由于季节性效应。这种依赖性意味着过去数据的分布并不能准确代表未来数据的分布。
- en: But what does this mean for us? The most obvious answer is that our coverage
    guarantees will suffer. But can we still apply these techniques for time series?
    Of course we can. Empirically, the community has seen that this framework works
    for time series data as well, but with some loss in coverage guarantees. For most
    practical purposes, there shouldn’t be an issue in using regular conformal prediction
    for time series data. In 2023, Barber et al. (Reference *12*) studied this issue
    and derived theoretical coverage guarantees for non-exchangeable data (like time
    series). They defined the Coverage Gap as the difference between expected coverage
    (![](img/B22389_17_016.png)) and actual coverage and derived an upper bound on
    this gap to show how much the exchangeability assumption on the scores is violated.
    For this bound, they considered the scores of the calibration data with our original
    model, ![](img/B22389_17_170.png), and an alternate model, which was trained on
    the same data but after swapping one randomly selected datapoint in the training
    data with the test datapoint, ![](img/B22389_17_171.png).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这对我们意味着什么？最明显的答案是我们的覆盖率保证将会受到影响。但我们还能将这些技术应用于时间序列数据吗？当然可以。从经验上看，社区已经发现这个框架也适用于时间序列数据，但会有一些覆盖率保证的损失。在大多数实际应用中，使用常规的符合性预测方法来处理时间序列数据不会有问题。2023年，Barber等人（参考文献*12*）研究了这个问题，并为不可交换数据（如时间序列）推导了理论覆盖率保证。他们定义了覆盖率差距，作为预期覆盖率(![](img/B22389_17_016.png))与实际覆盖率之间的差异，并推导了这个差距的上界，以显示得分的可交换性假设被违反的程度。对于这个界限，他们考虑了我们原始模型的校准数据得分！[](img/B22389_17_170.png)与一个替代模型的得分，后者是在同一数据上训练的，但将训练数据中的一个随机选择的数据点与测试数据点交换后得到的！[](img/B22389_17_171.png)。
- en: The bound was shown to be directly proportional to ![](img/B22389_17_172.png),
    which is the distributional distance between these two scores. In most algorithms
    we use, swapping one data point may not change the model drastically and therefore,
    we can still use conformal prediction for time series data with minimal loss in
    coverage.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 该界限被证明与![](img/B22389_17_172.png)成正比，这是这两个分数之间的分布距离。在我们使用的大多数算法中，交换一个数据点可能不会显著改变模型，因此我们仍然可以在时间序列数据上使用符合性预测，并且对覆盖率的损失最小。
- en: But on the other hand, if we want to be really accurate with the prediction
    intervals, or if we are using a model that is particularly impacted by the swapping
    of a datapoint, then we would need some techniques to overcome this degradation
    due to a shift in distributions. There are many ways to deal with this, and it
    is an active area of research at the time of writing the book. There are two very
    simple methods that are worth mentioning here.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 但另一方面，如果我们希望预测区间更准确，或者我们使用的模型特别容易受到数据点交换的影响，那么我们需要一些技术来克服由于分布变化所造成的性能下降。目前有很多方法可以解决这个问题，并且这是撰写本书时一个活跃的研究领域。这里有两种非常简单的方法值得提及。
- en: Weighted conformal prediction
  id: totrans-402
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加权符合性预测
- en: Suppose we have slowly varying changes in the data distribution in a time series,
    ![](img/B22389_17_173.png), and we are using a calibration set, ![](img/B22389_17_174.png),
    taking the last *k* timesteps. And we are interested in predicting the test set,
    ![](img/B22389_17_175.png), where *H* is the horizon of forecast.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在时间序列中有缓慢变化的数据分布！[](img/B22389_17_173.png)，并且我们使用一个校准集！[](img/B22389_17_174.png)，该集取自最后的*k*时间步。我们感兴趣的是预测测试集！[](img/B22389_17_175.png)，其中*H*是预测的时间范围。
- en: So, it stands to reason that the most recent timestep in ![](img/B22389_17_176.png)
    would be closest to the distribution of values we would observe in the test time
    period. So, what if we assign weights to the non-conformity scores in the calibration
    data such that the most recent time step gets higher weights, and calculates a
    weighted quantile instead of a regular quantile? Apparently, that’s a very good
    idea, with some solid theoretical backing as well.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，合理的推测是，![](img/B22389_17_176.png) 中的最近时间步将最接近我们在测试时间段中观察到的值分布。那么，如果我们给校准数据中的不一致性分数分配权重，使得最近的时间步获得更高的权重，并计算加权分位数而不是常规分位数呢？显然，这是一个非常好的想法，并且有坚实的理论支持。
- en: And weighing the calibration data using recency is just one of the ways we can
    use the weights to tackle the distribution shift. More generally, any weight schedule,
    ![](img/B22389_17_177.png) can be used here. Maybe for a strongly seasonal time
    series, it makes sense to use seasonal periods to define the weights, or there
    may be some other known criteria that makes different instances of the calibration
    data more or less relevant to future prediction.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最近性来加权校准数据只是我们利用权重解决分布偏移问题的方式之一。更一般地说，任何权重调度，![](img/B22389_17_177.png) 都可以在这里使用。也许对于一个强季节性的时间序列，使用季节性周期来定义权重是有意义的，或者可能有其他已知的标准，使得不同的校准数据实例与未来预测的相关性高低不同。
- en: Before we get into the actual mechanics of this, we need to understand what
    weighed quantiles are. If you are already comfortable with the concept, feel free
    to go ahead. If you need some intuition about what it is, I strongly advise you
    to check out the notebook in the chapter folder named `08-Quantiles_and_Weighted_Quantiles.ipynb`.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解这一点的实际机制之前，我们需要理解什么是加权分位数。如果你已经熟悉这个概念，可以直接跳过。如果你需要一些直观的理解，我强烈建议你查看章节文件夹中名为`08-Quantiles_and_Weighted_Quantiles.ipynb`的笔记本。
- en: Now, let’s come back to our Weighted Conformal Prediction method.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到我们的加权一致性预测方法。
- en: 'So, as we discussed earlier, for any normalized weight schedule, ![](img/B22389_17_178.png),
    and calibration scores, *s*[i] the weighted quantile can be formally defined as:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，对于任何归一化的权重调度，![](img/B22389_17_178.png)，以及校准分数，*s*[i]，加权分位数可以正式定义为：
- en: '![](img/B22389_17_179.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_179.png)'
- en: where *inf* is the infimum and ![](img/B22389_17_031.png) is an indicator function,
    which is 1 when the condition is true and 0 otherwise. In this context, the *infimum*
    is the smallest value of *q* such that the inequality holds true. This is just
    a more rigorous way of defining the weighted quantile that we saw in the aforementioned
    notebook. The rest of the process is exactly the same as before.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *inf* 是下确界，![](img/B22389_17_031.png) 是一个指示函数，当条件为真时值为 1，否则为 0。在这种情况下，*下确界*
    是满足不等式成立的最小 *q* 值。这只是定义我们在前述笔记本中看到的加权分位数的一种更严谨的方式。其余过程与之前完全相同。
- en: 'Practically, there are a few different ways we can use this for time series
    problems. For instance:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以通过几种不同的方式将其应用于时间序列问题。例如：
- en: We can consider a moving window of length *K* and have a fixed weight vector
    of length *K*. Under this scheme, we will apply the weights for each point in
    the time series to the last *K* points and calculate the prediction interval for
    that point. These weights can be equal weights or even decayed weights, capturing
    the temporal element.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以考虑一个长度为 *K* 的滑动窗口，并且有一个长度为 *K* 的固定权重向量。在这个方案下，我们将对时间序列中的每个点应用权重，直到最近的 *K*
    个点，并为该点计算预测区间。这些权重可以是相等的权重，甚至是衰减的权重，捕捉时间元素。
- en: When we model multiple time series together, we can make sure the weights reflect
    how close another time series is to the time series we are generating the intervals
    for, along with the temporal context.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们一起建模多个时间序列时，我们可以确保权重反映了另一个时间序列与我们为其生成区间的时间序列之间的接近程度，以及时间上下文。
- en: The bottom line is that we can be as creative as we can when coming up with
    the weights. The guideline is that the weight should reflect how different the
    calibration datapoints are from the datapoint you are generating the prediction
    interval for. Remember, we talked about the upper bound earlier, ![](img/B22389_17_172.png).
    The weight we choose would counteract this term. When we have smaller weights
    for datapoints that are “farther” away from the datapoint we care about, this
    brings down the upper bound of the coverage gap and makes it tighter.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是，我们在确定权重时可以尽可能地发挥创造力。基本原则是，权重应当反映校准数据点与正在生成预测区间的数据点之间的差异。记住，我们之前提到过上限，![](img/B22389_17_172.png)。我们选择的权重将抵消这个项。当我们对“距离”我们关注的数据点较远的数据点赋予较小的权重时，会降低覆盖差距的上限，从而使其更加紧密。
- en: Now, let’s learn about another very simple modification to account for distribution
    shift.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解另一种非常简单的修改方式，用于应对分布漂移。
- en: Adaptive Conformal Inference (ACI)
  id: totrans-416
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自适应保形推断（ACI）
- en: In 2021, Gibbs et al. (Reference *13*) proposed another way to deal with distribution
    shift (especially in time series) in an online setting. Time series data usually
    comes in one datapoint at a time, and this way to deal with distribution shift
    relies on this online aspect as it proposes to keep adjusting the prediction intervals
    based on the data that keeps trickling in, thus making the prediction intervals
    adapt to changing distributions. This method, called **Adaptive Conformal Inference**
    (**ACI**), can be integrated with any prediction algorithm to provide robust prediction
    sets under non-stationary conditions.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在2021年，Gibbs等人（参考文献*13*）提出了另一种处理分布漂移（特别是在时间序列中的分布漂移）的方法，适用于在线环境。时间序列数据通常是一个数据点一个数据点地到达，这种处理分布漂移的方法依赖于在线特性，提出基于持续流入的数据不断调整预测区间，从而使预测区间适应变化的分布。这个方法被称为**自适应保形推断**（**ACI**），可以与任何预测算法结合使用，在非平稳条件下提供稳健的预测集。
- en: In traditional Conformal Prediction, we have a score function, ![](img/B22389_17_182.png),
    and a quantile function, ![](img/B22389_17_121.png), which gives us the prediction
    intervals, ![](img/B22389_17_184.png). Note that the underlying uncertainty model,
    which was conformalized as ![](img/B22389_17_121.png), can be any way of estimating
    uncertainties, like quantile regression, PDF, MC Dropouts, and so on. When the
    data is exchangeable, the ![](img/B22389_17_121.png) we calculated on calibration
    data will hold good on future test datapoints. But when the distribution shifts,
    this ![](img/B22389_17_121.png) will also start to become less and less relevant.
    To address this, the authors propose regularly re-estimating these functions to
    align with the most recent data observations. Specifically, at each time point,
    *t*, a new score function, ![](img/B22389_17_188.png), and a new quantile function,
    ![](img/B22389_17_189.png)(.), are fitted based on the most recent data.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的保形预测中，我们有一个评分函数，![](img/B22389_17_182.png)，和一个分位数函数，![](img/B22389_17_121.png)，它们给我们提供预测区间，![](img/B22389_17_184.png)。请注意，基础的不确定性模型，经过保形处理后为![](img/B22389_17_121.png)，可以是任何估计不确定性的方法，例如分位回归、PDF、MC
    Dropouts等等。当数据是可交换时，我们在校准数据上计算的![](img/B22389_17_121.png)会在未来的测试数据点上保持有效。但当分布发生漂移时，这个![](img/B22389_17_121.png)也会逐渐变得不再相关。为了解决这个问题，作者建议定期重新估计这些函数，以与最新的数据观测对齐。具体来说，在每个时间点*t*，基于最新数据拟合一个新的评分函数，![](img/B22389_17_188.png)，以及一个新的分位数函数，![](img/B22389_17_189.png)(.)。
- en: For this, they defined the *miscoverage rate*, ![](img/B22389_17_190.png), as
    the probability that the true label, ![](img/B22389_17_191.png), lies outside
    the prediction intervals, ![](img/B22389_17_184.png), where probability is calculated
    over the calibration data and the test datapoint. We want the *Miscoverage rate*,
    ![](img/B22389_17_190.png), to be equal to ![](img/B22389_04_009.png) (expected
    error rate). But since the data distribution is shifting, ![](img/B22389_17_190.png)
    is not expected to remain constant over time, and it may not equal the target
    level, ![](img/B22389_04_009.png). The authors hypothesize that for each time,
    ![](img/B22389_16_126.png), there may exist an optimal coverage level, ![](img/B22389_17_198.png)
    such that the miscoverage rate, ![](img/B22389_17_199.png), is approximately ![](img/B22389_04_009.png).
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，他们将 *误覆盖率*，![](img/B22389_17_190.png)，定义为真实标签 ![](img/B22389_17_191.png)
    落在预测区间 ![](img/B22389_17_184.png) 外的概率，其中概率是通过校准数据和测试数据点计算的。我们希望 *误覆盖率*，![](img/B22389_17_190.png)，等于
    ![](img/B22389_04_009.png)（期望误差率）。但由于数据分布正在变化，![](img/B22389_17_190.png) 不太可能在时间上保持不变，并且它可能不等于目标水平
    ![](img/B22389_04_009.png)。作者假设，对于每个时间点 ![](img/B22389_16_126.png)，可能存在一个最佳覆盖水平
    ![](img/B22389_17_198.png)，使得误覆盖率 ![](img/B22389_17_199.png) 约等于 ![](img/B22389_04_009.png)。
- en: 'To estimate this ![](img/B22389_17_198.png), the authors propose a simple online
    update equation. This update takes into consideration the empirical miscoverage
    rate of the previous observations and then decreases or increases our estimate
    of ![](img/B22389_17_202.png). Concretely, if we set ![](img/B22389_17_203.png),
    we can define error as:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计这个 ![](img/B22389_17_198.png)，作者提出了一个简单的在线更新方程。该更新考虑了前一个观察值的经验误覆盖率，然后增加或减少我们对
    ![](img/B22389_17_202.png) 的估计。具体而言，如果我们设置 ![](img/B22389_17_203.png)，则可以定义误差为：
- en: '![](img/B22389_17_204.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_204.png)'
- en: 'Now, we can define the update step recursively as:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以递归地定义更新步骤为：
- en: '![](img/B22389_17_205.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_205.png)'
- en: Here, ![](img/B22389_17_206.png), serves as an estimate of the historical miscoverage
    rate and ![](img/B22389_17_207.png) is the step-size (a hyperparameter; more on
    this later). So, when ![](img/B22389_17_208.png) (prediction was within the interval),
    ![](img/B22389_17_209.png) will be positive and thus updating ![](img/B22389_17_210.png)
    to be higher than ![](img/B22389_17_211.png). This, in turn, makes the prediction
    interval narrower (according to the ![](img/B22389_04_011.png) we have defined).
    With the same logic, when ![](img/B22389_17_213.png) (prediction was outside the
    interval), the prediction interval becomes wider.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B22389_17_206.png) 作为历史误覆盖率的估计值，![](img/B22389_17_207.png) 是步长（一个超参数；稍后会详细讨论）。因此，当
    ![](img/B22389_17_208.png)（预测在区间内）时，![](img/B22389_17_209.png) 将为正值，从而使得更新后的 ![](img/B22389_17_210.png)
    高于 ![](img/B22389_17_211.png)。这反过来会使预测区间变窄（根据我们定义的 ![](img/B22389_04_011.png)）。按照相同的逻辑，当
    ![](img/B22389_17_213.png)（预测在区间外）时，预测区间会变宽。
- en: 'A natural alternative to this update, which also takes into account the history
    a bit more, is using a weighted average of past timesteps:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的替代更新方法，稍微更多地考虑历史信息，是使用过去时间步的加权平均：
- en: '![](img/B22389_17_214.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_214.png)'
- en: 'where ![](img/B22389_17_215.png) is a sequence of increasing weights with ![](img/B22389_17_216.png).
    Instead of looking at just the last time step for the estimate of miscoverage,
    this update looks at the recent history. This makes it slightly more robust, at
    least in theory. The paper reported no significant difference between the two
    strategies. One of the strategies they have used to decide the weights is:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B22389_17_215.png) 是一个递增的权重序列，满足 ![](img/B22389_17_216.png)。与仅仅查看最后一个时间步来估计误覆盖不同，这种更新方法会查看最近的历史。这使得它在理论上稍微更加稳健。论文报告称，两种策略之间没有显著差异。他们用来决定权重的策略之一是：
- en: '![](img/B22389_17_217.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_217.png)'
- en: They reported that the trajectories of prediction intervals that they obtained
    from the simple update and weighted update were almost the same, but the weighted
    one was considerably smoother with less local variation in ![](img/B22389_17_211.png).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 他们报告说，从简单更新和加权更新中获得的预测区间轨迹几乎相同，但加权更新的轨迹显得更加平滑，且在 ![](img/B22389_17_211.png)
    中具有更少的局部变化。
- en: Now, let’s also spend some time understanding the effect of the step-size parameter,
    ![](img/B22389_04_011.png). The intuition is very similar to the learning rate
    in deep learning models. ![](img/B22389_04_011.png) decides the magnitude with
    which we update the ![](img/B22389_04_009.png). The larger the value, the quicker
    the update, and vice versa. The paper also gives us an intuition that the greater
    the distributional shift greater the value of ![](img/B22389_04_011.png). For
    all their experiments, they used ![](img/B22389_17_223.png) with a justification
    that they found this value to make the trajectories relatively smooth while still
    being large enough to allow ![](img/B22389_17_211.png) to adapt to distributional
    shifts. We can see this as a parameter controlling the strength of “adapting”
    and letting us move in the spectrum between non-adaptive intervals and strongly
    adaptive intervals.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们也花点时间了解步长参数的效果，![](img/B22389_04_011.png)。直觉上与深度学习模型中的学习率非常相似。![](img/B22389_04_011.png)决定我们更新![](img/B22389_04_009.png)的幅度。值越大，更新速度越快，反之亦然。该论文还向我们解释了，分布偏移越大，![](img/B22389_04_011.png)的值就越大。对于所有的实验，他们使用了![](img/B22389_17_223.png)，并且他们证明这个值使得轨迹相对平滑，同时又足够大以允许![](img/B22389_17_211.png)适应分布偏移。我们可以把这看作是一个控制“适应”强度并让我们在非自适应间隔和强自适应间隔之间移动的参数。
- en: Now, let’s see how to apply these in practice.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何将这些应用到实践中。
- en: Forecasting with Conformal Prediction
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用符合预测进行预测
- en: 'We didn’t find any ready-to-use implementations of all the techniques we wanted
    to show here, especially one that has these properties:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有找到所有我们想展示的技术的现成实现，特别是具有以下特性的实现：
- en: Complete separation of model layer and conformal prediction layer (being model-agnostic
    is one of the most exciting features of Conformal Prediction)
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型层与符合预测层的完全分离（成为模型无关的符合预测的最令人兴奋的特性之一）
- en: Out-of-the box compatibility with `neuralforecast` predictions
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与`neuralforecast`预测的即插即用兼容性
- en: Time series focus
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列焦点
- en: Pedagogical ease
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教学便利
- en: Therefore, we have included a file (`src/conformal/conformal_predictions.py`)
    with the necessary implementations that would work with `neuralforecast` forecasts
    and have a unified API. It is also simple enough to understand. We will go through
    major parts of the code, but to see how it all fits together, you should just
    take a look at the file.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经包含了一个文件（`src/conformal/conformal_predictions.py`），其中包含与`neuralforecast`预测兼容且具有统一API的必要实现。它也足够简单易懂。我们将详细讨论代码的主要部分，但要看到所有内容如何组合在一起，你应该直接查看该文件。
- en: 'All the methods we discussed, like Conformal Prediction for Regression, Conformalized
    Quantile Regression, and Conformalizing uncertainty estimates, have been coded
    out in the same API. Let’s look at the most basic Conformal Prediction for Regression
    to understand the API. It can be found in the `ConformalPrediction` class in the
    file. The rest of the techniques inherit this class and make slight tweaks. And
    all these classes are coded in such a way that they take in the prediction dataframe
    from `neuralforecast` (or `statsforecast`) and use the same naming conventions
    to conformalize those predictions. In theory, any forecast that can be made into
    the expected format can be used with these classes. The expected columns in the
    format are:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的所有方法，如回归的符合预测、符合化分位数回归和符合化不确定性估计，都已在同一API中编码出来。让我们看看最基本的回归符合预测以了解API。它可以在文件中的`ConformalPrediction`类中找到。其他技术继承了这个类并进行了轻微调整。所有这些类的编码方式都是以相同的方式接受来自`neuralforecast`（或`statsforecast`）的预测数据框架，并使用相同的命名约定来进行符合化预测。理论上，任何可以转换为预期格式的预测都可以与这些类一起使用。格式中预期的列包括：
- en: '`ds`: This column should contain dates or the numerical equivalent of time.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ds`：此列应包含日期或时间的数值等效项。'
- en: '`y`: For train and calibration datasets, this column is necessary and it represents
    the actual value of that time series.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y`：对于训练和校准数据集，此列是必需的，它代表该时间序列的实际值。'
- en: '`unique_id`: This column is the unique identifier for different time series.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unique_id`：此列是不同时间序列的唯一标识符。'
- en: In addition to these columns, we would also have a column (or multiple columns)
    of forecast, named accordingly.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些列外，我们还将有一个（或多个）相应命名的预测列。
- en: 'Before we start generating conformal predictions, we also need some data and
    forecasts. We are using the same data that we have been using in this chapter
    (M4), with one additional split (calibration) created. And using the new train
    data, we have created these three forecasts with `level = 90`:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始生成保形预测之前，还需要一些数据和预测。我们使用的是本章中已经使用的数据（M4），并创建了一个额外的拆分（校准）。利用新的训练数据，我们已经使用`level
    = 90`生成了这三个预测：
- en: LSTM point forecast (`LSTM`)
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM点预测（`LSTM`）
- en: LSTM with Quantile Regression (`LSTM_QR`)
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带分位数回归的LSTM（`LSTM_QR`）
- en: LSTM with PDF (normal distribution) (`LSTM_PDF`)
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带PDF（正态分布）的LSTM（`LSTM_PDF`）
- en: '**Notebook alert**'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒**'
- en: To follow along with the complete code, use the notebook named `09-Conformal_Techniques.ipynb`
    in the `Chapter17` folder.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 若要跟随完整的代码，请使用`Chapter17`文件夹中的笔记本`09-Conformal_Techniques.ipynb`。
- en: The notebook has the entire code, but we can start from the point where we have
    already split the data into `Y_train_df`, `Y_calib_df`, and `Y_test_df`, and generated
    and stored forecasts in a dictionary, `prediction_dict`. Let’s take a look at
    the top five rows of the prepared data frame to see what kind of data we are working
    with.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本包含了完整的代码，但我们可以从数据已经被分割为`Y_train_df`、`Y_calib_df`和`Y_test_df`，并且预测已生成并存储在字典`prediction_dict`中这一点开始。让我们先查看准备好的数据框的前五行，看看我们正在使用的数据是什么样的。
- en: '![](img/B22389_17_16.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_16.png)'
- en: 'Figure 17.16: Top five rows of the Y_calib_df we are working with. This is
    the format that the conformal prediction classes we have coded expect'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.16：我们正在使用的Y_calib_df的前五行。这是我们编写的保形预测类所期望的格式。
- en: Now, let’s get down to business and start creating prediction intervals.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始实际操作，创建预测区间。
- en: Conformal prediction for regression
  id: totrans-454
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 回归的保形预测
- en: 'The `ConformalPrediction` class provides a structured way to calculate prediction
    intervals based on a chosen model’s predictions from a calibration dataset. It
    includes the following input parameters:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConformalPrediction`类提供了一种基于选定模型的校准数据集预测，计算预测区间的结构化方式。它包括以下输入参数：'
- en: '`model` (str): The name of the column with the forecast you want to conformalize.
    This is a required parameter.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（str）：包含你想进行保形化的预测的列名。这是一个必填参数。'
- en: '`level` (float): The confidence level for the prediction intervals, expressed
    as a percentage (e.g., 95 for a 95% confidence interval). The level must be between
    1 and 100\. This is a required parameter.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`level`（float）：预测区间的置信度水平，以百分比表示（例如，95表示95%的置信区间）。该值必须在1和100之间。这是必填参数。'
- en: '`alias` (str, optional): An optional string to provide an alias for the model.
    This can be useful when working with multiple models or versions and you want
    to call the output something else other than the model. If not provided, `model`
    is used.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alias`（str，选填）：一个可选的字符串，用于为模型提供别名。当使用多个模型或版本时，如果想要给输出命名为与模型不同的名称时，这非常有用。如果未提供别名，则使用`model`作为默认值。'
- en: 'The major functions for using the class are:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该类的主要功能包括：
- en: '`fit(Y_calib_df)`: This method orchestrates the entire calibration process.
    It first calculates the calibration scores using `calculate_scores` and then determines
    the quantiles for each `unique_id` using `get_quantile`. The resulting quantiles
    (`q_hat`) are stored as an attribute of the class, making them available for subsequent
    prediction intervals.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit(Y_calib_df)`：该方法协调整个校准过程。它首先使用`calculate_scores`计算校准分数，然后使用`get_quantile`为每个`unique_id`确定分位数。结果的分位数（`q_hat`）作为类的属性存储，供后续预测区间使用。'
- en: '`predict(Y_test_df)`: This method applies the prediction intervals to the test
    data. It uses the `calc_prediction_interval` method to compute the intervals and
    then adds them to the DataFrame as new columns. The new columns are created in
    this format: `f"{self.alias or self.model}-{self._mthd}-lo-{self.level}"`. For
    example, the higher interval for Conformal Prediction using LSTM would have `LSTM-CP-hi-90`
    as the column name in the dataframe.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict(Y_test_df)`：该方法将预测区间应用于测试数据。它使用`calc_prediction_interval`方法来计算区间，然后将它们作为新列添加到DataFrame中。这些新列的格式为：`f"{self.alias
    or self.model}-{self._mthd}-lo-{self.level}"`。例如，使用LSTM的保形预测的高区间会在DataFrame中显示为`LSTM-CP-hi-90`。'
- en: These classes are the external API. Internally, there are some methods that
    actually define how it’s done. Let’s look at the major methods in the context
    of regular Conformal Prediction.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类是外部API。内部有一些方法实际定义了如何进行操作。让我们在常规保形预测的上下文中看看主要方法。
- en: 'The `calculate_scores` method is defined below, where we just calculate the
    absolute residuals using the calibration dataset as the scores:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '`calculate_scores`方法如下所示，在这里我们只是使用校准数据集计算绝对残差作为得分：'
- en: '[PRE32]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The `get_quantile` method calculates the quantile using the defined ![](img/B22389_04_009.png)
    for each `unique_id`:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_quantile`方法使用定义的![](img/B22389_04_009.png)为每个`unique_id`计算分位数：'
- en: '[PRE33]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `calc_prediction_interval` method uses the calculated `q_hat` and mean
    prediction to generate the prediction intervals. For regular Conformal Prediction,
    it goes like this:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '`calc_prediction_interval`方法使用计算出的`q_hat`和平均预测值生成预测区间。对于常规的符合预测，它的流程如下：'
- en: '[PRE34]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now, let’s use it for forecasting.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用它来进行预测。
- en: '[PRE35]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The generated dataframe (*Figure 17.15*) with prediction intervals will have
    two columns—`LSTM-CP-lo-90` and `LSTM-CP-hi-90` for the lower and upper prediction
    interval, respectively. `CP` is the method tag we have assigned the conformal
    prediction class.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的带有预测区间的数据框（*图17.15*）将有两列——`LSTM-CP-lo-90`和`LSTM-CP-hi-90`，分别对应下限和上限预测区间。`CP`是我们为符合预测类分配的方法标签。
- en: 'We can check the method name of any object as follows:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式检查任何对象的方法名称：
- en: '[PRE36]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let’s see what the forecast dataframe looks like (CP is the method tag we have
    assigned the conformal prediction class. We can check the method name of any object
    by doing `cp.method_name`):'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看预测数据框的样子（CP是我们为符合预测类分配的方法标签。我们可以通过执行`cp.method_name`来检查任何对象的方法名称）：
- en: '![](img/B22389_17_17.png)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_17.png)'
- en: 'Figure 17.17: The generated dataframe with prediction intervals'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.17：带有预测区间的生成数据框
- en: Now, we also have to calculate the coverage and average length of the intervals
    to assess how these prediction intervals are. We use the same methods we used
    earlier to do that. Instead of looking at the performance of each method, let’s
    save the discussion for the end and look at creating the prediction intervals
    for now.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们还需要计算区间的覆盖率和平均长度，以评估这些预测区间的表现。我们使用之前使用过的相同方法来实现这一点。与其讨论每种方法的性能，不如把讨论留到最后，先看看如何创建预测区间。
- en: So, let’s move on to the next technique.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们继续下一种技术。
- en: Conformalized Quantile Regression
  id: totrans-479
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 符合分位数回归
- en: The first condition of applying this method of conformal prediction is that
    there should already be a set of prediction intervals from the underlying Quantile
    Regression. Therefore, we use the `LSTM_QR` model we trained here.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这种符合预测方法的第一个条件是，必须已经从基础的分位数回归中得到一组预测区间。因此，我们使用了这里训练的`LSTM_QR`模型。
- en: The main difference between vanilla Conformal Prediction and CQR is the way
    the scores are calculated and the prediction intervals. So, we can inherit `ConformalPrediction`
    and just redefine these two methods.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的符合预测和CQR之间的主要区别在于得分和预测区间的计算方式。因此，我们可以继承`ConformalPrediction`并重新定义这两种方法。
- en: Let’s look at the `calculate_scores` method.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下`calculate_scores`方法。
- en: '[PRE37]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We have just implemented the formula we saw earlier. `self.lower_quantile_model`
    and `self.upper_quantile_model` are the column names of the already-generated
    intervals from CQR.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚实现了之前看到的公式。`self.lower_quantile_model`和`self.upper_quantile_model`是从CQR生成的已存在区间的列名。
- en: Now, we also need to define the `calc_prediction_interval` method.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们还需要定义`calc_prediction_interval`方法。
- en: '[PRE38]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`q_hat` is a dictionary with the quantiles calculated for each `unique_id`.
    So, all we do here is take the existing prediction interval from CQR and adjust
    it by mapping the quantile using the `unique_id` in the input dataframe.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '`q_hat`是一个字典，包含为每个`unique_id`计算的分位数。因此，我们要做的就是拿到CQR生成的现有预测区间，并通过映射输入数据框中的`unique_id`来调整它。'
- en: Now, let’s use this for forecasting. The API is exactly the same as before.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用这个进行预测。API与之前完全相同。
- en: '[PRE39]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now, let’s look at the third technique we discussed.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们讨论过的第三种技术。
- en: Conformalizing uncertainty estimates
  id: totrans-491
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 符合化不确定性估计
- en: If you remember the discussion we had earlier, to use this technique, we need
    an estimate of uncertainty that can be further conformalized. This is why we picked
    one of the other techniques we used earlier, PDF. But we can also do this with
    the MC Dropout just as easily. All we need is the standard deviation or something
    similar that captures the uncertainty at each data point.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得我们之前的讨论，使用这种技术需要对不确定性的估计，并且该估计可以进一步符合化。这就是为什么我们选择了之前使用的其他技术之一，PDF。但我们也可以轻松地使用MC
    Dropout来实现这一点。我们需要的只是标准差或类似的东西，以捕捉每个数据点的不确定性。
- en: We are using the `LSTM_PDF` forecast that we generated earlier for this purpose.
    Although the model predicts the mean and standard deviation of the normal distribution,
    it is used internally to generate the prediction intervals. So, the output from
    the PDF model we defined earlier would be the prediction intervals, but we want
    the standard deviation. Fear not. We know the prediction intervals were created
    with the normal distribution. So, it’s not hard to re-engineer the standard deviation
    from the prediction intervals.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用我们之前生成的`LSTM_PDF`预测模型来完成这个任务。虽然模型预测的是正态分布的均值和标准差，但它在内部用于生成预测区间。因此，我们之前定义的PDF模型的输出将是预测区间，但我们需要的是标准差。别担心，我们知道预测区间是使用正态分布生成的。因此，从预测区间中重新计算标准差并不难。
- en: '![](img/B22389_17_049.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_049.png)'
- en: '![](img/B22389_17_050.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_050.png)'
- en: 'Using basic math, we can derive:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基本的数学，我们可以推导出：
- en: '![](img/B22389_17_228.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_228.png)'
- en: And *Z* is very straightforward to get. We can use `scipy.stats.norm` for this.
    Below is a method that will get you the standard deviation from the prediction
    intervals (do keep in mind that this is only for PDFs created using a normal distribution).
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 而*Z*的获取非常简单。我们可以使用`scipy.stats.norm`来实现。下面是一个方法，可以从预测区间中获得标准差（请记住，这仅适用于使用正态分布创建的PDF）。
- en: '[PRE40]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now, we add this to our `Y_calib_df` and `Y_test_df`.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将其添加到`Y_calib_df`和`Y_test_df`中。
- en: '[PRE41]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, let’s look at how we can define the class. We need additional information
    in here that we didn’t need earlier—the column name of the uncertainty estimate.
    So, we define our new class (still inheriting `ConformalPrediction`) as below:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何定义这个类。这里我们需要一些之前不需要的额外信息——不确定性估计的列名。因此，我们定义我们的新类（仍然继承`ConformalPrediction`）如下：
- en: '[PRE42]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We define an additional parameter, `uncertainty_model`, and pass on the other
    parameters to the parent class.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个额外的参数`uncertainty_model`，并将其他参数传递给父类。
- en: 'Now, it’s pretty straightforward. We need to define how the scores are calculated:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，很简单。我们需要定义分数是如何计算的：
- en: '[PRE43]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'And the `calc_prediction_interval` method:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 以及`calc_prediction_interval`方法：
- en: '[PRE44]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: That’s it. Now, we have a new class that conformalizes uncertainty estimates.
    Let’s use it to get the forecast for the dataset we have been working with.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。现在，我们有了一个新的类，用于符合化不确定性估计。让我们用它来获取我们正在使用的数据集的预测。
- en: '[PRE45]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now, let’s also look at the two techniques we saw, which were more suited for
    time series problems where there is a distribution shift.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们之前看到的两种技术，它们更适合处理时间序列问题，尤其是当存在分布漂移时。
- en: Weighted conformal prediction
  id: totrans-512
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加权符合预测
- en: We saw earlier that weighted conformal prediction was just about applying the
    right kind of weights to the calibration data such that the points similar to
    the test point get more weight than the dissimilar ones. And the key difference
    occurs only in the way the quantiles are calculated.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，加权符合预测只是将正确类型的权重应用于校准数据，使得与测试点相似的点比不相似的点获得更多的权重。关键区别仅在于分位数的计算方式。
- en: What this means is that we can use any conformal prediction technique underneath,
    but instead of calculating a simple quantile, we need to calculate a weighted
    one. So, from an implementation perspective, we can look at this class as a wrapper
    class around other techniques we have defined and convert them into weighted conformal
    predictions.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以使用任何符合预测技术，但不是计算简单的分位数，而是需要计算加权分位数。所以，从实现的角度来看，我们可以将这个类视为对我们定义的其他技术的封装类，并将它们转换为加权符合预测。
- en: Although the weighted conformal prediction can be implemented in many ways,
    taking in different kinds of weights (across time, across `unique_id`, and so
    on), we are going to implement a simpler look-back window-based weighted conformal
    prediction. We choose the last *K* timesteps and calculate the weighted quantile
    on these *K* scores using the weights given. The weights can either be simple
    uniform weights across all *K* steps, or have decayed weights giving the highest
    weightage to the most recent score. They can even have a completely custom weight.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然加权符合预测可以通过多种方式实现，采用不同种类的权重（跨时间、跨`unique_id`等等），但我们将实现一种基于简单回溯窗口的加权符合预测。我们选择最后的*K*时间步，并在这*K*个分数上使用给定的权重计算加权分位数。这些权重可以是对所有*K*步的简单均匀权重，或者是逐渐衰减的权重，给最新的分数赋予最高的权重。它们甚至可以具有完全自定义的权重。
- en: 'So, let’s define the `__init__` of the class as follows:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们可以按如下方式定义类的`__init__`方法：
- en: '[PRE46]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Here, `K` is the window, and `conformal_predictor` is the underlying conformal
    prediction class we should be using (this should be one of the three classes we
    have defined). We can define the weight strategy to be either `uniform`, `decay`,
    or `custom` for uniform weights, decayed weights, or custom weights, respectively.
    `decay_factor` decides how fast the decay is applied to the weights for the decayed
    weighting strategy, and `custom_weights` lets you specify exactly the weight on
    these `K` timesteps.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`K`是窗口，`conformal_predictor`是我们应该使用的底层一致性预测类（它应该是我们定义的三个类之一）。我们可以将权重策略定义为`uniform`（均匀）、`decay`（衰减）或`custom`（自定义）权重，分别对应均匀权重、衰减权重或自定义权重。`decay_factor`决定衰减权重策略的衰减速度，而`custom_weights`让你精确指定这些`K`时间步上的权重。
- en: While we won’t look at the entire code in the text here, we will go through
    the usual suspects so that you can understand what’s happening. But I would definitely
    urge you to take some time to digest the code in the file.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这里不会查看完整的代码，但我们会浏览一些常见部分，以便你理解发生了什么。但我确实强烈建议你花时间消化文件中的代码。
- en: First up, we have our `fit` method. In this method, we just use the score calculation
    of the underlying conformal predictor and store the calibration dataframe.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有我们的`fit`方法。在此方法中，我们仅使用基础一致性预测器的得分计算，并存储校准数据框。
- en: '[PRE47]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now, let’s look at the main parts of the `predict` method.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下`predict`方法的主要部分。
- en: '[PRE48]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let’s now take one of the methods we saw earlier and apply the weighted conformal
    predictor wrapper on it. For our example, let’s choose the simple `ConformalPrediction`.
    Let’s see how we can use this class:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们拿一个我们之前看到的方法，并在其上应用加权一致性预测器包装器。对于我们的示例，让我们选择简单的`ConformalPrediction`。让我们看看如何使用这个类：
- en: '[PRE49]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This would create the prediction intervals with the tag `CP_Wtd`. We can always
    check the tag by doing `weighted_cp.method_name`.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建带有标签`CP_Wtd`的预测区间。我们可以通过执行`weighted_cp.method_name`随时检查标签。
- en: Now, there is one small shortcoming with the implementation. Although we are
    considering the temporal order in the scores, we still have a fixed calibration
    set. So, until we “re-fit” or calibrate with the latest data points, we will still
    be working on the same calibrated dataset. So, if you think about it, this should
    ideally be applied in an online way where each time we predict the new timestep,
    the previous timestep (with actual value) should be added to the calibration data.
    We have also provided an alternative implementation that is able to do this in
    an online fashion. We won’t go into details of the implementation because the
    core logic is the same, but the API is different, making it possible to update
    the calibration data. The full implementation can be found in the `OnlineWeightedConformalPredictor`
    class in the file.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个实现有一个小小的缺陷。尽管我们在得分中考虑了时间顺序，但我们仍然有一个固定的校准集。因此，直到我们“重新拟合”或使用最新数据点进行校准之前，我们仍将使用相同的校准数据集。所以，如果你仔细想想，这理想情况下应该以在线方式应用，每次我们预测新的时间步时，前一个时间步（带有实际值）应该添加到校准数据中。我们还提供了一个替代实现，能够以在线方式执行此操作。我们不会详细讨论实现细节，因为核心逻辑是相同的，但
    API 是不同的，这使得更新校准数据成为可能。完整的实现可以在文件中的`OnlineWeightedConformalPredictor`类中找到。
- en: Let’s see how it can be used. First, we define the setup, initialize the class,
    and fit the calibration.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它如何使用。首先，我们定义设置，初始化类并进行校准拟合。
- en: '[PRE50]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, during inference, we can do something like this for each timestep:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在推断过程中，我们可以为每个时间步做类似这样的事情：
- en: '[PRE51]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'For our special case where we are evaluating on test data where we know the
    actuals, there is another method that does similar online prediction for the data:
    `offline_predict`.'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的特殊情况，在测试数据上进行评估并且已知实际值时，还有另一个方法可以为数据执行类似的在线预测：`offline_predict`。
- en: '[PRE52]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Now, let’s look at one last method.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看最后一个方法。
- en: Adaptive Conformal Inference
  id: totrans-535
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自适应一致性推断
- en: Lastly, we look at the Adaptive Conformal Inference. This can also be implemented
    as a wrapper over other conformal prediction methods because this technique involves
    updating ![](img/B22389_04_009.png) such that coverage is maintained in spite
    of the shift in distribution. And because of the nature of the technique, we can
    only apply this in an online manner, i.e., updating alpha at every time step with
    the available data. So, this will have the same API as the `OnlineWeightedConformalPredictor`
    we saw earlier.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看一下自适应符合性推断。这也可以作为其他符合性预测方法的封装器来实现，因为该技术涉及更新![](img/B22389_04_009.png)，以确保在分布发生变化时仍然维持覆盖率。由于该技术的性质，我们只能以在线方式应用它，即在每个时间步根据可用数据更新alpha。因此，它将拥有与我们之前看到的`OnlineWeightedConformalPredictor`相同的API。
- en: 'The full class is available in `src/conformal/conformal_predictions.py`, but
    here, we will look at some main parts so that you understand it. Let’s take a
    look at the `__init__` function first:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的类可以在`src/conformal/conformal_predictions.py`中找到，但在这里，我们将先看一些主要部分，以便你理解。让我们先看看`__init__`函数：
- en: '[PRE53]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Similar to `WeightedConformalPredictor`, we take in an underlying conformal
    predictor (`conformal_predictor`). In addition, we have `gamma`, which is the
    step size (![](img/B22389_04_011.png)), `update_method`, which can either be `simple`
    (taking only the last timestep for update) or `momentum` (taking the running average
    of the trajectory of errors). Finally, we can also define `momentum_bw`, which
    is the momentum backweight that is used to calculate the weighted average of past
    errors. A higher momentum (e.g., `0.95`) makes the trajectory smoother, having
    the effect of past miscoverage rates decay slower. Going to the other extreme
    (`0.05`) makes the weighted average more reactive and closer to the “simple” method.
    Lastly, we also have a parameter to do the error calculation for each `unique_id`
    separately or pool all errors together.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 与`WeightedConformalPredictor`类似，我们传入一个基础的符合性预测器（`conformal_predictor`）。此外，我们还有`gamma`，即步长（![](img/B22389_04_011.png)），`update_method`，它可以是`simple`（只使用最后一个时间步进行更新）或`momentum`（使用误差轨迹的滚动平均）。最后，我们还可以定义`momentum_bw`，它是用于计算过去误差加权平均的动量反向权重。较高的动量（例如`0.95`）使得轨迹更平滑，表现为过去的错误覆盖率衰减得较慢。若选择另一极端值（`0.05`），加权平均将更加敏感，接近于“简单”方法。最后，我们还有一个参数，用于决定是单独计算每个`unique_id`的误差，还是将所有误差汇总在一起。
- en: As usual, we have a `fit` method that uses a calibration dataset to calculate
    the scores and keep them ready. We can also see this as a warm-up period in an
    online implementation. The ![](img/B22389_04_009.png) update will use the calibration
    data as an initial history to start off.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们有一个`fit`方法，它使用一个标定数据集来计算分数，并将其保留以备后续使用。我们也可以将其视为在线实现中的一个预热阶段。![](img/B22389_04_009.png)更新将使用标定数据作为初始历史记录开始。
- en: '[PRE54]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Now, let’s see the `predict_one` method, which predicts one timestep.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看`predict_one`方法，它用于预测一个时间步。
- en: '[PRE55]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The code is well-commented and you should be able to understand it. For each
    `unique_id`, we get the scores, get the appropriate ![](img/B22389_04_009.png),
    calculate the quantile, use this information to calculate the prediction interval
    using the underlying conformal predictor, and store the prediction for later use.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 代码已经有详细注释，你应该能够理解它。对于每个`unique_id`，我们获取分数，获取相应的 ![](img/B22389_04_009.png)，计算分位数，利用这些信息通过基础的符合性预测器计算预测区间，并将预测结果存储起来以供后续使用。
- en: Now, we look at the `update` method, which we can use once the actual value
    for the timestep becomes available.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来看一下`update`方法，我们可以在时间步的实际值可用时使用它。
- en: '[PRE56]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Let’s see how it can be used. First, we define the setup, initialize the class,
    and fit the calibration.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何使用的。首先，我们定义设置，初始化类，并进行标定。
- en: '[PRE57]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, during inference, we can do something like below for each timestep:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在推理过程中，我们可以对每个时间步执行如下操作：
- en: '[PRE58]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'For our special case where we are evaluating on test data where we know the
    actuals, there is another method that does similar online prediction for the data:
    `offline_predict`.'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在测试数据上评估且已知真实值的特殊情况，还有一个方法可以执行类似的在线预测：`offline_predict`。
- en: '[PRE59]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Now that we have seen all the techniques in action, let’s take a look at how
    they have been performing.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了所有的技术实现，接下来让我们看看它们的表现如何。
- en: Evaluating the results
  id: totrans-554
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估结果
- en: If you have been following the notebooks, you would know that we have been calculating
    coverage and average length for each of these techniques. To recap, coverage measures
    the percentage of time the actual value fell between the intervals we predicted
    and average length measures the average width of the intervals we predicted. For
    `level=90`, we expect the coverage to be around 90% or 0.9 and the average length
    to be as small as possible, without compromising on coverage.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您一直在关注这些笔记本，您会知道我们一直在计算每种技术的覆盖率和平均长度。简而言之，覆盖率衡量实际值落在我们预测的区间内的百分比，而平均长度则衡量我们预测的区间宽度的平均值。对于`level=90`，我们期望覆盖率约为90%或0.9，且平均长度尽可能小，而不会影响覆盖率。
- en: 'Let’s take a look at the following figures summarizing the coverage and average
    length:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下图表，总结了覆盖率和平均长度：
- en: '![](img/B22389_17_18.png)'
  id: totrans-557
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_18.png)'
- en: 'Figure 17.18: Coverage for all the conformal techniques (for each unique_id),
    colored according to how close they are to 0.9 (we had set level=90)'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.18：所有保形技术的覆盖率（针对每个`unique_id`），根据它们与0.9的接近程度着色（我们设置了`level=90`）
- en: '![](img/B22389_17_19.png)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_17_19.png)'
- en: 'Figure 17.19: Average length for all the conformal techniques (for each unique_id),
    colored according to how small they are'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.19：所有保形技术的平均长度（针对每个`unique_id`），根据其大小着色
- en: 'Here’s a quick recap of the legend to understand the different columns, which
    are just a combination of these tags based on the application:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个快速回顾图例，以帮助理解不同的列，这些列只是根据应用的不同，结合了这些标签：
- en: '`LSTM`: Underlying model that we trained to get point prediction'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LSTM`：我们训练的基础模型，用于获取点预测'
- en: '`LSTM_QR`: Underlying Quantile Regression model we trained'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LSTM_QR`：我们训练的基础分位回归模型'
- en: '`LSTM_Normal`: Underlying PDF model we trained using Normal distribution assumption'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LSTM_Normal`：我们基于正态分布假设训练的基础PDF模型'
- en: '`CP`: Regular Conformal Prediction'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CP`：常规保形预测'
- en: '`CQR`: Conformalized Quantile Regression'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CQR`：保形化分位回归'
- en: '`CUE`: Conformalizing uncertainty Estimates'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CUE`：保形化不确定性估计'
- en: '`CP_Wtd`: Weighted corrections over Conformal Prediction'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CP_Wtd`：对保形预测的加权修正'
- en: '`CP_Wtd_O`: Online Weighted corrections over Conformal Prediction'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CP_Wtd_O`：对保形预测的在线加权修正'
- en: '`ACI`: Adaptive Conformal Inference'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ACI`：自适应保形推断'
- en: Right off the bat, we can see that the techniques that corrected for distribution
    shift are performing the best. There are more “greens” on the right side of both
    tables (closer to 0.9 coverage and lower average lengths). Remember that we started
    with regular conformal predictions and then corrected them for distributional
    shifts. We can note that for most `unique_id`, regular **Conformal Prediction**
    has wider than necessary intervals for `level = 90`. The coverages are greater
    than 0.9, in most cases 1.0\. But both **Weighted Correction** (**CP_Wtd**) and
    **Adaptive Conformal Inference** (**ACI**) made the prediction intervals narrower
    and got closer to our expected level. There is no clear winner between the two
    and that has to be evaluated on your dataset.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，我们可以看到修正了分布偏移的技术表现最佳。两张表格的右侧有更多的“绿色”（接近0.9的覆盖率和较低的平均长度）。记住，我们从常规的保形预测开始，然后对其进行了分布偏移修正。我们可以注意到，对于大多数`unique_id`，常规**保形预测**在`level
    = 90`时的区间比必要的宽。覆盖率大于0.9，在大多数情况下是1.0。但**加权修正**（**CP_Wtd**）和**自适应保形推断**（**ACI**）使预测区间变得更窄，并且更接近我们期望的水平。这两者之间没有明显的优胜者，需根据您的数据集进行评估。
- en: And this concludes our discussion about Probabilistic Forecasts. We hope that
    with this, you can steer into this lesser-known territory with confidence and
    deliver more value to whoever you are forecasting for.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 这也结束了我们关于概率预测的讨论。我们希望通过这部分内容，您能够自信地进入这一较少被关注的领域，为您所预测的对象提供更多价值。
- en: Now, let’s take a very high-level look at some niche topics in time series forecasting
    that get very little attention, but are quite relevant in many domains.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们简要地看一下时间序列预测中一些很少被关注，但在许多领域都非常相关的小众话题。
- en: Road less traveled in time series forecasting
  id: totrans-574
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列预测中较少走的路
- en: In the spirit of Robert Frost’s *The Road Not Taken*, this section explores
    the lesser-known, yet highly impactful, techniques in time series forecasting.
    Just as Frost chooses a path that is less traveled, we delve into niche methods
    that, while not mainstream, offer unique insights and potential breakthroughs
    in various domains.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将以罗伯特·弗罗斯特的《未选择的路》为灵感，探讨一些不太为人知但具有深远影响的时间序列预测技术。正如弗罗斯特选择了一条不常走的路，我们也深入探讨那些虽然不主流，但在各个领域中提供独特见解和潜在突破的小众方法。
- en: Intermittent or sparse demand forecasting
  id: totrans-576
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 间歇性或稀疏需求预测
- en: Intermittent time series forecasting handles data with sporadic, irregular events,
    often seen in retail, where products may have infrequent sales. It’s crucial for
    managing inventory and avoiding stockouts or overstocking, especially for slow-moving
    items. Traditional methods struggle with these patterns because of the fact that
    for most of these items, the expectation of demand falls to zero, but specialized
    techniques are needed to improve accuracy, making them essential for retail forecasting.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 间歇性时间序列预测处理的是具有间歇性、不规则事件的数据，这类数据常见于零售行业，其中产品可能会有不规律的销售。它对库存管理至关重要，可以避免缺货或过度库存，尤其是对于慢速移动的商品。传统方法在处理这些模式时存在困难，因为大多数这类商品的需求预期接近零，但需要专业技术来提高预测的准确性，使得这些技术在零售预测中变得不可或缺。
- en: Here, we will quickly name-drop a few alternative algorithms that were designed
    for intermittent forecasting and where you can find the implementations for it.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将快速列举一些为间歇性预测设计的替代算法，并指出它们的实现位置。
- en: '**Croston and its friends**: The Croston model, developed in 1972, is used
    to forecast intermittent demand by separately estimating two components: the demand
    rate (when sales occur) and the time interval between sales, ignoring periods
    with zero sales. These estimates are combined to forecast future demand, making
    it useful for industries with infrequent sales. However, the model doesn’t account
    for external factors or changes in demand patterns, which can affect its accuracy
    in more complex situations. Although this was a method developed all the way back
    in 1972, it has stayed with us in many forms and led to many modifications and
    extensions. In *Nixtla*’s `statsforecast`, we can find `CrostonClassic`, `CrostonOptimized`,
    `CrostonSBA`, and `TSB` as four models that can be used just like the other models
    we saw in *Chapter 4*, *Setting a Strong Baseline Forecast*.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Croston及其变种**：1972年开发的Croston模型通过分别估计两个组成部分来预测间歇性需求：需求率（即销售发生时）和销售间隔时间，忽略零销售的时期。这些估计值被结合起来，预测未来的需求，因此对于销售不频繁的行业非常有用。然而，该模型没有考虑外部因素或需求模式的变化，这可能会影响其在更复杂情况中的准确性。尽管这是1972年开发的方法，但它以多种形式延续至今，并且经过许多修改和扩展。在*Nixtla*的`statsforecast`中，我们可以找到`CrostonClassic`、`CrostonOptimized`、`CrostonSBA`和`TSB`这四种模型，可以像我们在*第4章：设定强基线预测*中看到的其他模型一样使用。'
- en: '**ADIDA**: The **Aggregate-Disaggregate Intermittent Demand Approach** (**ADIDA**)
    uses **Simple Exponential Smoothing** (**SES**) combined with temporal aggregation
    to forecast intermittent demand. The method starts by aggregating demand data
    into non-overlapping time buckets of a size equal to the **mean inter-demand**
    interval (**MI**). SES is then applied to these aggregated values to generate
    forecasts, which are subsequently disaggregated back to the original time scale,
    providing demand predictions for each time period. This model is available in
    `statsforecast` as `ADIDA`.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ADIDA**：**聚合-拆解间歇性需求方法**（**ADIDA**）结合了**简单指数平滑**（**SES**）和时间聚合来预测间歇性需求。该方法首先将需求数据聚合成不重叠的时间段，每个时间段的长度等于**平均需求间隔**（**MI**）。然后，将SES应用于这些聚合值以生成预测，接着将预测拆解回原始的时间尺度，从而为每个时间段提供需求预测。此模型在`statsforecast`中以`ADIDA`的形式提供。'
- en: '**IMAPA**: The **Intermittent Multiple Aggregation Prediction Algorithm** (**IMAPA**)
    forecasts intermittent time series by aggregating values at regular intervals
    and applying optimized **Simple exponential smoothing** (**SES**) to predict future
    values. IMAPA is efficient, robust to missing data, and effective across various
    intermittent time series, making it a practical and easy-to-implement choice.
    This model is available in `statsforecast` as `IMAPA`.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IMAPA**：**间歇性多重聚合预测算法**（**IMAPA**）通过在规律的时间间隔内聚合数据，并应用优化的**简单指数平滑**（**SES**）来预测未来值。IMAPA高效、对缺失数据具有鲁棒性，并且在各种间歇性时间序列中有效，因此它是一种实际且易于实施的选择。此模型在`statsforecast`中以`IMAPA`的形式提供。'
- en: Interpretability
  id: totrans-582
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性
- en: 'We directed you toward a few interpretability techniques for machine learning
    models back in *Chapter 10*, *Global Forecasting Models*. While some of them,
    such as SHAP and LIME, can still be applied to deep learning models, none of them
    consider the temporal aspect by design. This is because all those techniques were
    developed for more general purposes, such as classification and regression. That
    being said, there has been some work in the interpretability of DL models and
    time series models. Here, I’ll list a few promising papers that tackle the temporal
    aspect head-on:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第10章*《*全局预测模型*》中为你推荐了一些机器学习模型的解释性技术。尽管其中一些方法，如SHAP和LIME，仍可应用于深度学习模型，但它们都没有考虑时间维度，因为这些技术最初是为更通用的任务（如分类和回归）而开发的。尽管如此，深度学习模型和时间序列模型的解释性工作已经有所进展。在这里，我列出了一些专门处理时间维度的前沿论文：
- en: '**TimeSHAP**: This is a model-agnostic recurrent explainer that builds upon
    **KernelSHAP** and extends it to the time series domain. Research paper: [https://dl.acm.org/doi/10.1145/3447548.3467166](https://dl.acm.org/doi/10.1145/3447548.3467166).
    GitHub: [https://github.com/feedzai/timeshap](https://github.com/feedzai/timeshap).'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TimeSHAP**：这是一个与模型无关的递归解释器，基于**KernelSHAP**，并将其扩展到时间序列领域。研究论文：[https://dl.acm.org/doi/10.1145/3447548.3467166](https://dl.acm.org/doi/10.1145/3447548.3467166)。GitHub：[https://github.com/feedzai/timeshap](https://github.com/feedzai/timeshap)。'
- en: '**SHAPTime**: This technique uses the Shapley Value to provide stable explanations
    in the temporal dimension, improving forecasting performance. ShapTime’s model-agnostic
    nature allows it to be deployed on any forecasting model at a lower cost, demonstrating
    significant performance improvements across various datasets. Research paper:
    [https://link.springer.com/chapter/10.1007/978-3-031-47721-8_45](https://link.springer.com/chapter/10.1007/978-3-031-47721-8_45).
    GitHub: [https://github.com/Zhangyuyi-0825/ShapTime](https://github.com/Zhangyuyi-0825/ShapTime).'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SHAPTime**：该技术利用Shapley值在时间维度上提供稳定的解释，从而提高预测性能。ShapTime的模型无关性使其可以在任何预测模型上低成本部署，在多个数据集上展示了显著的性能提升。研究论文：[https://link.springer.com/chapter/10.1007/978-3-031-47721-8_45](https://link.springer.com/chapter/10.1007/978-3-031-47721-8_45)。GitHub：[https://github.com/Zhangyuyi-0825/ShapTime](https://github.com/Zhangyuyi-0825/ShapTime)。'
- en: '**Instance-wise Feature Importance in Time** (**FIT**): This is an interpretability
    technique that relies on the distribution shift between the predictive distribution
    and a counterfactual, where all but the feature under inspection is unobserved.
    Research paper: [https://proceedings.neurips.cc/paper/2020/file/08fa43588c2571ade19bc0fa5936e028-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/08fa43588c2571ade19bc0fa5936e028-Paper.pdf).
    GitHub: [https://github.com/sanatonek/time_series_explainability](https://github.com/sanatonek/time_series_explainability).'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于实例的时间特征重要性** (**FIT**)：这是一种解释性技术，它依赖于预测分布与反事实之间的分布偏移，其中除了被检查的特征之外，其他所有特征都未观察到。研究论文：[https://proceedings.neurips.cc/paper/2020/file/08fa43588c2571ade19bc0fa5936e028-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/08fa43588c2571ade19bc0fa5936e028-Paper.pdf)。GitHub：[https://github.com/sanatonek/time_series_explainability](https://github.com/sanatonek/time_series_explainability)。'
- en: '**Time Interpret**: `time_interpret` is a library extending `Captum`, focused
    on temporal data, providing feature attribution methods to explain predictions
    from any PyTorch model. It includes synthetic and real-world time series datasets,
    various PyTorch models, and evaluation methods, with some components also applicable
    to language model predictions. Research paper: [https://arxiv.org/abs/2306.02968](https://arxiv.org/abs/2306.02968).
    GitHub: [https://github.com/josephenguehard/time_interpret](https://github.com/josephenguehard/time_interpret).'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Time Interpret**：`time_interpret`是一个扩展`Captum`的库，专注于时间序列数据，提供特征归因方法，以解释任何PyTorch模型的预测。它包括合成和真实世界的时间序列数据集、各种PyTorch模型和评估方法，一些组件还可以应用于语言模型的预测。研究论文：[https://arxiv.org/abs/2306.02968](https://arxiv.org/abs/2306.02968)。GitHub：[https://github.com/josephenguehard/time_interpret](https://github.com/josephenguehard/time_interpret)。'
- en: While this is not an exhaustive list, these are a few works that we feel are
    important and promising. This is an area of active research, and new techniques
    will come up as time goes on.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不是一个详尽无遗的清单，但这些是我们认为重要且有前景的一些工作。这个领域正在积极研究，随着时间的推移，会有新的技术出现。
- en: Cold-start forecasting
  id: totrans-589
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 冷启动预测
- en: Cold-start forecasting addresses the challenge of predicting demand for products
    with no historical data, a common issue in industries like retail, manufacturing,
    and consumer goods. It arises when launching new products, onboarding brands,
    or expanding into new regions. Traditional statistical forecasting models like
    ARIMA or exponential smoothing will not be able to tackle this as they require
    historical data, a good amount of it as well.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 冷启动预测解决了缺乏历史数据的产品需求预测难题，这是零售、制造业和消费品等行业的常见问题。它通常出现在新产品发布、品牌接入或扩展到新地区时。传统的统计预测模型，如ARIMA或指数平滑方法，无法解决这个问题，因为它们需要大量的历史数据。
- en: 'But all hope is not lost. We do have some ways to handle such cases:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 但并非所有希望都破灭。我们确实有一些方法来处理这种情况：
- en: '**Manual Substitution Mapping**: If we know that a new product is substituting
    another product, we can do manual alignment, which moves the history of the other
    product to the new product and uses regular techniques to forecast.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手动替代映射**：如果我们知道一个新产品正在替代另一个产品，我们可以进行手动对齐，将旧产品的历史数据转移到新产品上，并使用常规技术进行预测。'
- en: '**Global Machine Learning Models**: We saw how we can model multiple time series
    using global models in *Chapter 10*. There, we used some constructed features
    like lags, rolling aggregations, and so on. But if we train such a model without
    such features that rely on history and with features characterizing the item (item
    features that will enable us to cross-learn with other longer time series), we
    can utilize cross-learning to learn the forecast from similar items. But this
    works best for new launches that substitute another product.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全球机器学习模型**：我们在*第10章*中看到过如何使用全球模型对多个时间序列进行建模。那时，我们使用了一些构造特征，如滞后、滚动聚合等。但如果我们在没有依赖历史的特征的情况下训练这样的模型，并且使用一些描述产品特征的特征（这些特征可以帮助我们与其他更长时间的时间序列进行交叉学习），我们可以利用交叉学习从类似产品中学习预测。但这种方法最适用于替代其他产品的新发布产品。'
- en: '**Launch Profile Models**: If the new product is brand-new, we can use a very
    similar setup to Global Machine Learning Models but with a small tweak. We can
    convert our time series into a date/time agnostic manner and consider each time
    series to start from the date of launch. For instance, the first timestep after
    the launch of every product can be 1, the next can be 2, and so on. Once we convert
    all our time series in this manner, we have a dataset that considers how a new
    item is launched and ramps up. And this can be used to train models that consider
    the initial ramp-up during launch.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发布档案模型**：如果新产品是全新的，我们可以使用与全球机器学习模型非常相似的设置，但稍作调整。我们可以将时间序列转换为不依赖日期/时间的方式，并将每个时间序列视为从发布日期开始。例如，每个产品发布后的第一个时间步可以是1，接下来的是2，依此类推。一旦我们以这种方式转换所有时间序列，我们就得到一个数据集，考虑了新产品发布及其逐步增长的过程。这可以用来训练考虑发布初期增长的模型。'
- en: '**Foundational Time Series Models**: Foundation models for time series leverage
    large-scale pre-trained models to capture generalized patterns across various
    time series tasks. These models are highly effective in applications like cold
    start forecasting, where little or no historical data is available. By using foundational
    models, practitioners can apply pre-trained knowledge to new scenarios, improving
    accuracy in forecasting tasks across industries such as retail, healthcare, and
    finance. The models offer flexibility, allowing fine-tuning or zero-shot applications,
    making them particularly useful for complex, sparse, or intermittent data. Many
    foundational models for time series are already available for use—some are commercial
    and some others are open source. This is a nice survey of the foundational models
    at the time of writing the book: Foundation Models for Time Series Analysis: A
    Tutorial and Survey: [https://arxiv.org/abs/2403.14735](https://arxiv.org/abs/2403.14735).
    The performance of such models is average at best, but in the absence of any data
    (cold-start), this presents as a good option to try out. A few popular ways of
    doing this practically are:'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础时间序列模型**：时间序列的基础模型利用大规模预训练模型捕捉不同时间序列任务中的通用模式。这些模型在冷启动预测等应用中非常有效，因为在这种情况下几乎没有历史数据可用。通过使用基础模型，实践者可以将预训练的知识应用于新场景，提高在零售、医疗和金融等行业中的预测任务准确性。基础模型具有灵活性，可以进行微调或零样本应用，因此在处理复杂、稀疏或间歇性数据时尤为有用。许多时间序列的基础模型已可供使用——其中一些是商业产品，另一些是开源的。以下是编写本书时关于基础模型的一个不错的综述：时间序列分析的基础模型：教程与综述：[https://arxiv.org/abs/2403.14735](https://arxiv.org/abs/2403.14735)。此类模型的性能最好也只是一般，但在没有任何数据的情况下（冷启动），它仍然是一个值得尝试的不错选择。一些流行的实际应用方法包括：'
- en: '*TimeGPT* (Commercial) from Nixtla: [https://nixtlaverse.nixtla.io/nixtla/docs/getting-started/introduction.html](https://nixtlaverse.nixtla.io/nixtla/docs/getting-started/introduction.html)'
  id: totrans-596
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自Nixtla的*TimeGPT*（商业）：[https://nixtlaverse.nixtla.io/nixtla/docs/getting-started/introduction.html](https://nixtlaverse.nixtla.io/nixtla/docs/getting-started/introduction.html)
- en: '*Chronos* from Amazon: [https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html)'
  id: totrans-597
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自Amazon的*Chronos*：[https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html)
- en: '*Moirai* from Salesforce: [https://huggingface.co/Salesforce/moirai-1.0-R-large](https://huggingface.co/Salesforce/moirai-1.0-R-large)'
  id: totrans-598
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自Salesforce的*Moirai*：[https://huggingface.co/Salesforce/moirai-1.0-R-large](https://huggingface.co/Salesforce/moirai-1.0-R-large)
- en: '*TimesFM* from Google: [https://github.com/google-research/timesfm](https://github.com/google-research/timesfm)'
  id: totrans-599
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自Google的*TimesFM*：[https://github.com/google-research/timesfm](https://github.com/google-research/timesfm)
- en: Hierarchical forecasting
  id: totrans-600
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层级预测
- en: Hierarchical forecasting deals with time series that can be disaggregated into
    nested levels, such as product categories or geographic regions. These structures
    require forecasts that maintain coherence, meaning predictions at lower levels
    should sum up to higher levels, reflecting the aggregation. Grouped time series,
    which combine different hierarchies (e.g., product type and geography), add complexity.
    The goal is to produce consistent, accurate forecasts that align with the data’s
    natural aggregation, making hierarchical forecasting essential for businesses
    managing large collections of time series across multiple dimensions.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 层级预测处理的是可以分解为嵌套层级的时间序列，例如产品类别或地理区域。这些结构要求预测保持一致性，即较低层级的预测应该加起来与较高层级一致，反映出聚合效果。组合不同层级（例如产品类型和地理位置）的时间序列增加了复杂性。目标是生成一致且准确的预测，符合数据的自然聚合，这使得层级预测在处理多个维度的大规模时间序列数据的企业中至关重要。
- en: 'There are some techniques of disaggregating, aggregating, or reconciling all
    forecasts so that they add up in a logical manner. *Chapter 10* from Rob Hyndman’s
    free Bible of time series forecasting (*Forecasting: Principles and Practice*)
    talks about this at length and is a very good resource to get up to speed (fair
    warning: it’s quite math-intensive). You can find the chapter here: [https://otexts.com/fpp2/hierarchical.html](https://otexts.com/fpp2/hierarchical.html).
    For a more practical approach, you can check out Nixtla’s `hierarchicalforecast`
    library here: [https://nixtlaverse.nixtla.io/hierarchicalforecast/index.html](https://nixtlaverse.nixtla.io/hierarchicalforecast/index.html).'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些技术用于分解、聚合或调和所有预测，使它们以逻辑的方式加总起来。Rob Hyndman 的《时间序列预测圣经》第*10章*（《预测：原理与实践》）详细讨论了这一点，是加速掌握这一主题的非常好资源（特别提醒：内容偏数学）。你可以在这里找到该章节：[https://otexts.com/fpp2/hierarchical.html](https://otexts.com/fpp2/hierarchical.html)。对于更实用的做法，你可以查看
    Nixtla 的 `hierarchicalforecast` 库，地址为：[https://nixtlaverse.nixtla.io/hierarchicalforecast/index.html](https://nixtlaverse.nixtla.io/hierarchicalforecast/index.html)。
- en: And with that, one of the longest chapters of the book comes to an end. Congrats
    on making it and digesting all the information. Do feel free to use the notebooks
    and play around with the code, different options, and so on to get a better grasp
    of what’s happening.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，本书最长的章节之一终于结束了。恭喜你成功阅读并消化了所有信息。欢迎随时使用笔记本，并尝试不同的代码选项等，以更好地理解发生了什么。
- en: Summary
  id: totrans-604
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored different techniques for generating probabilistic
    forecasts like Probability Density Functions, Quantile functions, Monte Carlo
    Dropout, and Conformal Prediction. We deep-dived into each of them and learned
    how to apply them to real data. For Conformal Predictions, an actively researched
    field, we learned different ways to conformalize prediction intervals with different
    underlying mechanisms, like Conformalized Quantile Regression, conformalizing
    uncertainty estimates, and so on. And finally, we saw a few tweaks to make conformal
    prediction work even better for time series problems.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了生成概率预测的不同技术，如概率密度函数、分位数函数、蒙特卡罗 Dropout 和一致性预测。我们深入研究了每一种技术，并学习了如何将其应用于实际数据。在一致性预测这一活跃研究领域中，我们学习了不同的方法来使预测区间符合不同的基本机制，比如一致性分位回归、一致性不确定性估计等。最后，我们看到了一些调整，使一致性预测在时间序列问题中表现得更好。
- en: To wrap it up, we also looked at a few topics that are on the road less traveled,
    like intermittent demand forecasting, interpretability, cold-start forecasting,
    and hierarchical forecasting.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们还讨论了一些不太常见的主题，如间歇性需求预测、可解释性、冷启动预测和层次预测。
- en: In the next part of this book, we will look at a few mechanics of forecasting,
    such as multi-step forecasting, cross-validation, and evaluation.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的下一部分，我们将探讨一些预测的机制，如多步预测、交叉验证和评估。
- en: References
  id: totrans-608
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The following are the references for this chapter:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章的参考文献：
- en: 'Tony Duan, A. Avati, D. Ding, S. Basu, A. Ng, and Alejandro Schuler. (2019).
    *NGBoost: Natural Gradient Boosting for Probabilistic Prediction*. International
    Conference on Machine Learning. [https://proceedings.mlr.press/v119/duan20a/duan20a.pdf](https://proceedings.mlr.press/v119/duan20a/duan20a.pdf).'
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Tony Duan, A. Avati, D. Ding, S. Basu, A. Ng, 和 Alejandro Schuler. (2019).
    *NGBoost: 用于概率预测的自然梯度提升*。国际机器学习大会。[https://proceedings.mlr.press/v119/duan20a/duan20a.pdf](https://proceedings.mlr.press/v119/duan20a/duan20a.pdf)。'
- en: 'Y. Gal and Zoubin Ghahramani. (2015). *Dropout as a Bayesian Approximation:
    Representing Model Uncertainty in Deep Learning*. International Conference on
    Machine Learning. [https://proceedings.mlr.press/v48/gal16.html](https://proceedings.mlr.press/v48/gal16.html).'
  id: totrans-611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Gal 和 Zoubin Ghahramani. (2015). *Dropout 作为贝叶斯近似：在深度学习中表示模型不确定性*。国际机器学习大会。[https://proceedings.mlr.press/v48/gal16.html](https://proceedings.mlr.press/v48/gal16.html)。
- en: 'Valentin Flunkert, David Salinas, and Jan Gasthaus. (2017). *DeepAR: Probabilistic
    Forecasting with Autoregressive Recurrent Networks*. International Journal of
    Forecasting. [https://www.sciencedirect.com/science/article/pii/S0169207019301888](https://www.sciencedirect.com/science/article/pii/S0169207019301888).'
  id: totrans-612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Valentin Flunkert, David Salinas, 和 Jan Gasthaus. (2017). *DeepAR: 使用自回归递归网络进行概率预测*。国际预测期刊。[https://www.sciencedirect.com/science/article/pii/S0169207019301888](https://www.sciencedirect.com/science/article/pii/S0169207019301888)。'
- en: Koenker, Roger. (2005). *Quantile Regression*. Cambridge University Press. pp.
    146–7\. ISBN 978-0-521-60827-5\. [http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf](http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf).
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Koenker, Roger. (2005). *分位数回归*. 剑桥大学出版社. 第146-147页\. ISBN 978-0-521-60827-5\.
    [http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf](http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf).
- en: 'Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos. (2020). *The
    M4 Competition: 100,000 time series and 61 forecasting methods*. International
    Journal of Forecasting. [https://www.sciencedirect.com/science/article/pii/S0169207019301128](https://www.sciencedirect.com/science/article/pii/S0169207019301128).'
  id: totrans-614
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos. (2020). *M4竞赛：100,000个时间序列和61种预测方法*.
    国际预测学杂志. [https://www.sciencedirect.com/science/article/pii/S0169207019301128](https://www.sciencedirect.com/science/article/pii/S0169207019301128).
- en: 'Loic Le Folgoc and Vasileios Baltatzis and Sujal Desai and Anand Devaraj and
    Sam Ellis and Octavio E. Martinez Manzanera and Arjun Nair and Huaqi Qiu and Julia
    Schnabel and Ben Glocker. (2021). *Is MC Dropout Bayesian?*. arXiv preprint arXiv:
    Arxiv-2110.04286\. [https://arxiv.org/abs/2110.04286](https://arxiv.org/abs/2110.04286).'
  id: totrans-615
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Loic Le Folgoc 和 Vasileios Baltatzis 和 Sujal Desai 和 Anand Devaraj 和 Sam Ellis
    和 Octavio E. Martinez Manzanera 和 Arjun Nair 和 Huaqi Qiu 和 Julia Schnabel 和 Ben
    Glocker. (2021). *MC Dropout 是贝叶斯的吗？*. arXiv预印本 arXiv: Arxiv-2110.04286\. [https://arxiv.org/abs/2110.04286](https://arxiv.org/abs/2110.04286).'
- en: Nicolas Dewolf and Bernard De Baets and Willem Waegeman. (2023). *Valid prediction
    intervals for regression problems*. Artif. Intell. Rev. [https://doi.org/10.1007/s10462-022-10178-5](https://doi.org/10.1007/s10462-022-10178-5).
  id: totrans-616
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nicolas Dewolf 和 Bernard De Baets 和 Willem Waegeman. (2023). *回归问题的有效预测区间*.
    人工智能评论. [https://doi.org/10.1007/s10462-022-10178-5](https://doi.org/10.1007/s10462-022-10178-5).
- en: V. Vovk, A. Gammerman, and G. Shafer. (2005). *Algorithmic learning in a random
    world*. Springer. [https://link.springer.com/book/10.1007/b106715](https://link.springer.com/book/10.1007/b106715).
  id: totrans-617
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: V. Vovk, A. Gammerman, 和 G. Shafer. (2005). *随机世界中的算法学习*. Springer. [https://link.springer.com/book/10.1007/b106715](https://link.springer.com/book/10.1007/b106715).
- en: 'Anastasios N. Angelopoulos and Stephen Bates. (2021). *A Gentle Introduction
    to Conformal Prediction and Distribution-Free Uncertainty Quantification*. arXiv
    preprint arXiv: Arxiv-2107.07511\. [https://arxiv.org/abs/2107.07511](https://arxiv.org/abs/2107.07511).'
  id: totrans-618
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Anastasios N. Angelopoulos 和 Stephen Bates. (2021). *关于符合预测与无分布不确定性量化的温和介绍*.
    arXiv预印本 arXiv: Arxiv-2107.07511\. [https://arxiv.org/abs/2107.07511](https://arxiv.org/abs/2107.07511).'
- en: 'Harris Papadopoulos, Kostas Proedrou, Volodya Vovk, and Alex Gammerman. (2002).
    *Inductive confidence machines for regression*. Machine Learning: ECML 2002\.
    ECML 2002\. Lecture Notes in Computer Science(), vol 2430\. Springer, Berlin,
    Heidelberg. [https://doi.org/10.1007/3-540-36755-1_29](https://doi.org/10.1007/3-540-36755-1_29).'
  id: totrans-619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Harris Papadopoulos, Kostas Proedrou, Volodya Vovk, 和 Alex Gammerman. (2002).
    *回归的归纳置信度机器*. 机器学习: ECML 2002\. ECML 2002\. 计算机科学讲义系列(), 第2430卷\. Springer, Berlin,
    Heidelberg. [https://doi.org/10.1007/3-540-36755-1_29](https://doi.org/10.1007/3-540-36755-1_29).'
- en: Romano, Yaniv and Patterson, Evan and Candes, Emmanuel. (2019). *Conformalized
    Quantile Regression*. Advances in Neural Information Processing Systems. [https://proceedings.neurips.cc/paper_files/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf).
  id: totrans-620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Romano, Yaniv 和 Patterson, Evan 和 Candes, Emmanuel. (2019). *标准化分位数回归*. 神经信息处理系统进展.
    [https://proceedings.neurips.cc/paper_files/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf).
- en: R. Barber, E. Candès, Aaditya Ramdas, and R. Tibshirani. (2022). *Conformal
    prediction beyond exchangeability*. Annals of Statistics. [https://projecteuclid.org/journals/annals-of-statistics/volume-51/issue-2/Conformal-prediction-beyond-exchangeability/10.1214/23-AOS2276.full](https://projecteuclid.org/journals/annals-of-statistics/volume-51/issue-2/Conformal-prediction-beyond-exchangeability/10.1214/23-AOS2276.full).
  id: totrans-621
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. Barber, E. Candès, Aaditya Ramdas, 和 R. Tibshirani. (2022). *超越可交换性的符合预测*.
    统计年鉴. [https://projecteuclid.org/journals/annals-of-statistics/volume-51/issue-2/Conformal-prediction-beyond-exchangeability/10.1214/23-AOS2276.full](https://projecteuclid.org/journals/annals-of-statistics/volume-51/issue-2/Conformal-prediction-beyond-exchangeability/10.1214/23-AOS2276.full).
- en: Gibbs, Isaac and Candes, Emmanuel. (2021). *Adaptive Conformal Inference Under
    Distribution Shift*. Advances in Neural Information Processing Systems. [https://proceedings.neurips.cc/paper_files/paper/2021/file/0d441de75945e5acbc865406fc9a2559-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/0d441de75945e5acbc865406fc9a2559-Paper.pdf).
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gibbs, Isaac 和 Candes, Emmanuel. (2021). *分布变化下的自适应符合推断*. 神经信息处理系统进展. [https://proceedings.neurips.cc/paper_files/paper/2021/file/0d441de75945e5acbc865406fc9a2559-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/0d441de75945e5acbc865406fc9a2559-Paper.pdf).
- en: Further reading
  id: totrans-623
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: To learn more about the topics that were covered in this chapter, take a look
    at the following resources.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多本章涉及的主题，请查看以下资源。
- en: '*The Gradient Boosters VI(B): NGBoost*: [https://deep-and-shallow.com/2020/06/27/the-gradient-boosters-vib-ngboost/](https://deep-and-shallow.com/2020/06/27/the-gradient-boosters-vib-ngboost/)'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《梯度提升 VI(B): NGBoost》*：[https://deep-and-shallow.com/2020/06/27/the-gradient-boosters-vib-ngboost/](https://deep-and-shallow.com/2020/06/27/the-gradient-boosters-vib-ngboost/)'
- en: '*Dive into Deep Learning, Chapter 5.6:* [https://d2l.ai/chapter_multilayer-perceptrons/dropout.html](https://d2l.ai/chapter_multilayer-perceptrons/dropout.html)'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《深入深度学习，第5.6章：》* [https://d2l.ai/chapter_multilayer-perceptrons/dropout.html](https://d2l.ai/chapter_multilayer-perceptrons/dropout.html)'
- en: '*Bayesian Inference by Marco Taboga*: [https://www.statlect.com/fundamentals-of-statistics/Bayesian-inference](https://www.statlect.com/fundamentals-of-statistics/Bayesian-inference)'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《贝叶斯推断》* 作者：Marco Taboga：[https://www.statlect.com/fundamentals-of-statistics/Bayesian-inference](https://www.statlect.com/fundamentals-of-statistics/Bayesian-inference)'
- en: '*Seeing Theory: Bayesian Inference*: [https://seeing-theory.brown.edu/bayesian-inference/index.html](https://seeing-theory.brown.edu/bayesian-inference/index.html)'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《理论可视化：贝叶斯推断》*：[https://seeing-theory.brown.edu/bayesian-inference/index.html](https://seeing-theory.brown.edu/bayesian-inference/index.html)'
- en: '*A Tutorial on Sparse Gaussian Processes and Variational Inference by Felix
    Leibfried et al.*: [https://arxiv.org/pdf/2012.13962](https://arxiv.org/pdf/2012.13962)'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《稀疏高斯过程与变分推断教程》* 作者：Felix Leibfried 等.：[https://arxiv.org/pdf/2012.13962](https://arxiv.org/pdf/2012.13962)'
- en: '*A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty
    Quantification* *by Anastasios N. Angelopoulos and Stephen Bates. (2021):* [https://arxiv.org/abs/2107.07511](https://arxiv.org/abs/2107.07511  )'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《符合预测与无分布不确定性量化的温和入门》* 作者：Anastasios N. Angelopoulos 和 Stephen Bates. (2021):
    [https://arxiv.org/abs/2107.07511](https://arxiv.org/abs/2107.07511)'
- en: Join our community on Discord
  id: totrans-631
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with authors and other readers:'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者及其他读者进行讨论：
- en: '[https://packt.link/mts](https://packt.link/mts)'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mts](https://packt.link/mts)'
- en: '![](img/QR_Code15080603222089750.png)'
  id: totrans-634
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code15080603222089750.png)'
- en: Leave a Review!
  id: totrans-635
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留下您的评论！
- en: Thank you for purchasing this book from Packt Publishing—we hope you enjoy it!
    Your feedback is invaluable and helps us improve and grow. Once you’ve completed
    reading it, please take a moment to leave an Amazon review; it will only take
    a minute, but it makes a big difference for readers like you.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您从 Packt 出版社购买本书——我们希望您喜欢它！您的反馈对我们非常重要，它能帮助我们不断改进和成长。阅读完后，请花一点时间在亚马逊上留下评论；这只需要一分钟，但对像您这样的读者来说意义重大。
- en: Scan the QR or visit the link to receive a free ebook of your choice.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描二维码或访问链接以获取您选择的免费电子书。
- en: '[https://packt.link/NzOWQ](Chapter_17.xhtml)'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/NzOWQ](Chapter_17.xhtml)'
- en: '![A qr code with black squares  Description automatically generated](img/review1.jpg)'
  id: totrans-639
  prefs: []
  type: TYPE_IMG
  zh: '![一个带有黑色方块的二维码，描述自动生成](img/review1.jpg)'
