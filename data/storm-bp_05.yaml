- en: Chapter 5. Real-time Graph Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 实时图分析
- en: In this chapter, we will introduce you to graph analysis using Storm to persist
    data to a graph database and query that data to discover relationships. Graph
    databases are databases that store data as graph structures with vertices, edges,
    and properties, and focus primarily on relationships between the entities.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍使用Storm进行图分析，将数据持久化到图数据库并查询数据以发现关系。图数据库是将数据存储为顶点、边和属性的图结构的数据库，主要关注实体之间的关系。
- en: With the advent of social media sites such as Twitter, Facebook, and LinkedIn,
    social graphs have become ubiquitous. Analyzing relationships between people,
    the products they buy, the recommendations they make, and even the words they
    use can be analyzed to reveal patterns that would be difficult with traditional
    data models. For example, when LinkedIn shows that you are four steps away from
    another person based on your network, when Twitter offers suggestions of people
    to follow, or when Amazon suggests products you may be interested in, they are
    leveraging what they know about your relationship graph. Graph databases are designed
    for this type of relationship analysis.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Twitter、Facebook和LinkedIn等社交媒体网站的出现，社交图已经变得无处不在。分析人与人之间的关系、他们购买的产品、他们做出的推荐，甚至他们使用的词语，都可以被分析以揭示传统数据模型难以发现的模式。例如，当LinkedIn显示你与另一个人相隔四步时，基于你的网络，当Twitter提供关注的人的建议时，或者当亚马逊建议你可能感兴趣的产品时，它们都在利用他们对你的关系图的了解。图数据库就是为这种关系分析而设计的。
- en: 'In this chapter, we will build an application that ingests a subset of the
    Twitter firehose (the real-time feed of all tweets made by Twitter users) and
    based on the content of each message, creates nodes (vertices) and relationships
    (edges) in a graph database that we can then analyze. The most obvious graph structure
    within Twitter is based on the follows / followed by relationship between users,
    but we can infer additional relationships by looking beyond these explicit relationships.
    By looking at the content of messages, we can use message metadata (hashtags,
    user mentions, and so on) to identify, for example, users who mention the same
    subjects or tweet related hashtags. In this chapter, we will cover the following
    topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个应用程序，摄取Twitter firehose的一个子集（Twitter用户发布的所有推文的实时源），并根据每条消息的内容，在图数据库中创建节点（顶点）和关系（边），然后进行分析。在Twitter中最明显的图结构是基于用户之间的关注/被关注关系，但是我们可以通过超越这些显式关系来推断出额外的关系。通过查看消息的内容，我们可以使用消息元数据（标签、用户提及等）来识别例如提到相同主题或发布相关标签的用户。在本章中，我们将涵盖以下主题：
- en: Basic graph database concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本图数据库概念
- en: The TinkerPop graph APIs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TinkerPop图形API
- en: Graph data modeling
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图数据建模
- en: Interacting with the Titan-distributed graph database
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Titan分布式图数据库交互
- en: Writing a Trident state implementation backed by a graph database
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写由图数据库支持的Trident状态实现
- en: Use case
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例
- en: 'Today''s social media websites capture a wealth of information. Many social
    media services such as Twitter, Facebook, and LinkedIn are based largely on relationships:
    who you follow, are friends with, or have a business connection to. Beyond the
    obvious and explicit relationships, social media interactions also create a persistent
    set of implicit connections that can be easily taken for granted. With Twitter,
    for example, the obvious relationships consist of those one follows and who one
    is followed by. The less obvious relationships are the connections created, perhaps
    unknowingly, just by using the service. Have you directly messaged someone on
    Twitter? If yes, then you''ve formed a connection. Tweeted a URL? If yes, again
    a connection. Liked a product, service, or comment on Facebook? Connection. Even
    the act of using a specific word or phrase in a tweet or post can be thought of
    as creating a connection. By using that word, you are forming a connection with
    it, and by using it repeatedly, you are strengthening that connection.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的社交媒体网站捕获了大量的信息。许多社交媒体服务，如Twitter、Facebook和LinkedIn，主要基于人际关系：你关注谁，与谁交友，或者与谁有业务联系。除了明显和显式的关系之外，社交媒体互动还会产生一组持久的隐式连接，这些连接很容易被忽视。例如，对于Twitter来说，明显的关系包括关注的人和被关注的人。不太明显的关系是通过使用服务而可能无意中创建的连接。你在Twitter上直接给某人发过私信吗？如果是，那么你们之间就建立了连接。发过URL的推文吗？如果是，也是一种连接。在Facebook上点赞产品、服务或评论吗？连接。甚至在推文或帖子中使用特定词语或短语也可以被视为创建连接。通过使用那个词，你正在与它建立连接，并且通过反复使用它，你正在加强那个连接。
- en: 'If we look at data as "everything is a connection," then we can build a structured
    dataset and analyze it to expose broader patterns. If Bob does not know Alice,
    but both Bob and Alice have tweeted the same URL, we can infer a connection from
    this fact. As our dataset grows, its value will also grow as the number of connections
    in the network increases (similar to Metcalfe''s law: [http://en.wikipedia.org/wiki/Metcalfe''s_law](http://en.wikipedia.org/wiki/Metcalfe''s_law)).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将数据视为“一切都是连接”，那么我们可以构建一个结构化的数据集并对其进行分析，以揭示更广泛的模式。如果Bob不认识Alice，但Bob和Alice都发推文相同的URL，我们可以从这个事实推断出一个连接。随着我们的数据集增长，其价值也将随着网络中连接的数量增加而增长（类似于梅特卡夫定律：[http://en.wikipedia.org/wiki/Metcalfe's_law](http://en.wikipedia.org/wiki/Metcalfe's_law)）。
- en: 'When we begin querying our dataset, the value for storing data in a graph database
    will quickly become evident as we glean patterns from the growing network of connections.
    The graph analysis we perform is applicable to a number of real-world use cases
    that include the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始查询我们的数据集时，将很快意识到将数据存储在图数据库中的价值，因为我们可以从不断增长的连接网络中获取模式。我们进行的图分析适用于许多现实世界的用例，包括以下内容：
- en: Targeted advertising
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定向广告
- en: Recommendation engines
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐引擎
- en: Sentiment analysis
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析
- en: Architecture
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: The architecture for our application is relatively simple. We will create a
    Twitter client application that reads a subset of the Twitter firehose and writes
    each message to a Kafka queue as a JSON data structure. We'll then use the Kafka
    spout to feed that data into our storm topology. Finally, our storm topology will
    analyze the incoming messages and populate the graph database.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用程序的架构相对简单。我们将创建一个Twitter客户端应用程序，读取Twitter firehose的子集，并将每条消息作为JSON数据结构写入Kafka队列。然后，我们将使用Kafka
    spout将数据输入到我们的storm拓扑中。最后，我们的storm拓扑将分析传入的消息并填充图数据库。
- en: '![Architecture](img/8294_05_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![架构](img/8294_05_01.jpg)'
- en: The Twitter client
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Twitter客户端
- en: Twitter provides a comprehensive RESTful API that in addition to a typical request-response
    interface also provides a streaming API that supports long-lived connections.
    The Twitter4J Java library ([http://twitter4j.org/](http://twitter4j.org/)) offers
    full compatibility with the latest version of the Twitter API and takes care of
    all the low-level details (connection management, OAuth authentication, and JSON
    parsing) with a clean Java API. We will use Twitter4J to connect to the Twitter-streaming
    API.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter提供了一个全面的RESTful API，除了典型的请求-响应接口外，还提供支持长连接的流API。Twitter4J Java库 ([http://twitter4j.org/](http://twitter4j.org/))
    完全兼容最新版本的Twitter API，并通过清晰的Java API处理所有底层细节（连接管理、OAuth认证和JSON解析）。我们将使用Twitter4J连接到Twitter流API。
- en: Kafka spout
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka spout
- en: In the previous chapter, we developed a Logback Appender extension that allowed
    us to easily publish data to a Kafka queue, and we used Nathan Marz's Kafka spout
    ([https://github.com/nathanmarz/storm-contrib](https://github.com/nathanmarz/storm-contrib))
    to consume the data in a Storm topology. While it would be easy enough to write
    a Storm spout using Twitter4J and the Twitter streaming API, using Kafka and the
    Kafka Spout gives us transactional, exactly-once semantics, and built-in fault
    tolerance that we would otherwise have to implement ourselves. For more information
    on installing and running Kafka refer to [Chapter 4](ch04.html "Chapter 4. Real-time
    Trend Analysis"), *Real-time Trend Analysis*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们开发了一个Logback Appender扩展，使我们能够轻松地将数据发布到Kafka队列，并且我们使用了Nathan Marz的Kafka
    spout ([https://github.com/nathanmarz/storm-contrib](https://github.com/nathanmarz/storm-contrib))
    来消费Storm拓扑中的数据。虽然使用Twitter4J和Twitter流API编写Storm spout会很容易，但使用Kafka和Kafka Spout可以给我们提供事务性、精确一次语义和内置的容错性，否则我们将不得不自己实现。有关安装和运行Kafka的更多信息，请参阅[第4章](ch04.html
    "第4章。实时趋势分析") *实时趋势分析*。
- en: A titan-distributed graph database
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Titan分布式图数据库
- en: 'Titan is a distributed graph database optimized for storing and querying graph
    structures. Like Storm and Kafka, Titan databases can run as a cluster and can
    scale horizontally to accommodate increasing data volume and user load. Titan
    stores its data in one of the three configurable storage backends: Apache Cassandra,
    Apache HBase, and Oracle Berkely Database. The choice of storage backend depends
    on which two properties of the CAP theorem are desired. In respect to a database,
    the CAP theorem stipulates that a distributed system cannot simultaneously make
    all of the following guarantees:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Titan是一个优化用于存储和查询图结构的分布式图数据库。像Storm和Kafka一样，Titan数据库可以作为集群运行，并且可以水平扩展以容纳不断增加的数据量和用户负载。Titan将其数据存储在三种可配置的存储后端之一：Apache
    Cassandra、Apache HBase和Oracle Berkely数据库。存储后端的选择取决于CAP定理的哪两个属性是期望的。就数据库而言，CAP定理规定分布式系统不能同时满足以下所有保证：
- en: '**Consistency**: All clients see the current data regardless of modifications'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**：所有客户端看到当前数据，无论修改如何'
- en: '**Availability**: The system continues to operate as expected despite node
    failures'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**：系统在节点故障时仍然按预期运行'
- en: '**Partition Tolerance**: The system continues to operate as expected despite
    network or message failure'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区容错性**：系统在网络或消息故障时仍然按预期运行'
- en: '![A titan-distributed graph database](img/8294_05_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![Titan分布式图数据库](img/8294_05_02.jpg)'
- en: For our use case, consistency is not critical to our application. We are far
    more concerned with scalability and fault tolerance. If we look at the CAP theorem
    triangle, shown in the preceding diagram, it becomes clear that Cassandra is the
    storage backend of choice.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的用例，一致性对我们的应用程序并不重要。我们更关心的是可伸缩性和容错性。如果我们看一下CAP定理三角形，在前面的图中显示，就会清楚地看到Cassandra是首选的存储后端。
- en: A brief introduction to graph databases
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图数据库简介
- en: 'A graph is a network of objects (vertices) with directed connections (edges)
    between them. The following diagram illustrates a simple social graph similar
    to what one might find on Twitter:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图是一个对象（顶点）的网络，它们之间有定向连接（边）。下图说明了一个简单的社交图，类似于在Twitter上找到的图：
- en: '![A brief introduction to graph databases](img/8294_05_03.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图数据库简介](img/8294_05_03.jpg)'
- en: In this example, users are represented by vertices (nodes), and relationships
    are expressed as edges (connection). Note that the edges in the graph are directed,
    allowing an additional degree of expressiveness. This allows, for example, to
    express the fact that Bob and Alice follow one another, and Alice follows Ted
    but Ted does not follow Alice. This relationship would be more cumbersome to model
    without directed edges.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，用户由顶点（节点）表示，关系表示为边（连接）。请注意，图中的边是有向的，允许额外的表达度。例如，这允许表达Bob和Alice互相关注，Alice关注Ted但Ted不关注Alice。如果没有有向边，这种关系将更难建模。
- en: 'Many graph databases follow a property graph model. A property graph extends
    the basic graph model by allowing a set of properties (key-value pairs) to be
    assigned to vertices and edges as shown in the following diagram:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 许多图数据库遵循属性图模型。属性图通过允许一组属性（键值对）分配给顶点和边来扩展基本图模型，如下图所示：
- en: '![A brief introduction to graph databases](img/8294_05_04.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图数据库简介](img/8294_05_04.jpg)'
- en: The ability to associate property metadata to objects and relationships in a
    graph model provides powerful support metadata for graph algorithms and queries.
    For example, adding the **since** property to the **Follows** edge would enable
    us to efficiently query for all the users who started following a particular user
    in a given year.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在图模型中将属性元数据与对象和关系关联起来，为图算法和查询提供了强大的支持元数据。例如，将**Follows**边缘添加**since**属性将使我们能够有效地查询在特定年份开始关注特定用户的所有用户。
- en: In contrast to relational databases, relationships in a graph database are explicit
    as opposed to implicit. Relationships in a graph database are full-blown data
    structures rather than implied connections (that is, foreign keys). Under the
    hood, graph databases' underlying data structures are heavily optimized for graph
    traversal. While it is entirely possible to model a graph in a relational database,
    it is often less efficient than a graph-centric model. In a relational data model,
    traversing a graph structure can be computationally expensive as it involves joining
    many tables. In a graph database, it is a more natural process of traversing links
    between nodes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与关系数据库相比，图数据库中的关系是显式的，而不是隐式的。图数据库中的关系是完整的数据结构，而不是暗示的连接（即外键）。在底层，图数据库的基础数据结构经过了大量优化，用于图遍历。虽然在关系数据库中完全可以对图进行建模，但通常比图中心模型效率低。在关系数据模型中，遍历图结构可能会涉及连接许多表，因此计算成本高昂。在图数据库中，遍历节点之间的链接是一个更自然的过程。
- en: Accessing the graph – the TinkerPop stack
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问图 - TinkerPop堆栈
- en: TinkerPop is a group of open source projects focused on graph technologies such
    as database access, data flow, and graph traversal. Blueprints, the foundation
    of the TinkerPop stack, is a generic Java API for interacting with property graphs
    in much the same way JDBC provides a generic interface to relational databases.
    Other projects in the stack add additional functionalities on top of that foundation
    so that they can be used with any graph database that implements the Blueprints
    API.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: TinkerPop是一组专注于图技术的开源项目，如数据库访问、数据流和图遍历。Blueprints是TinkerPop堆栈的基础，是一个通用的Java
    API，用于与属性图进行交互，方式与JDBC提供关系数据库的通用接口类似。堆栈中的其他项目在该基础上添加了额外的功能，以便它们可以与实现Blueprints
    API的任何图数据库一起使用。
- en: '![Accessing the graph – the TinkerPop stack](img/8294_05_05.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![访问图 - TinkerPop堆栈](img/8294_05_05.jpg)'
- en: 'The components of the TinkerPop stack include the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: TinkerPop堆栈的组件包括以下内容：
- en: '**Blueprints**: Graph API Blueprints is a collection of interfaces that provide
    access to a property graph data model. Implementations are available for graph
    databases including Titan, Neo4J, MongoDB, and many others.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Blueprints**：图API Blueprints是一组接口，提供对属性图数据模型的访问。可用于包括Titan、Neo4J、MongoDB等图数据库的实现。'
- en: '**Pipes**: Dataflow Processing Pipes is a dataflow framework for defining and
    connecting various data operations as a process graph. Manipulating data with
    Pipes'' primitives closely resembles data processing in Storm. Pipes dataflow
    are **directed acyclic graphs** (**DAG**), much like a Storm topology.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pipes**：数据流处理管道是一个用于定义和连接各种数据操作的数据流框架。使用Pipes的基本操作与Storm中的数据处理非常相似。Pipes数据流是**有向无环图**（**DAG**），就像Storm拓扑结构一样。'
- en: '**Gremlin**: Gremlin is a graph traversal language. It is a Java-based **domain
    specific language** (**DSL**) for graph traversal, query, analysis, and manipulation.
    The Gremlin distribution comes with a Groovy-based shell that allows the use of
    interactive analysis and modification of a Blueprints graph.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gremlin**：Gremlin是一种图遍历语言。它是用于图遍历、查询、分析和操作的基于Java的**领域特定语言**（**DSL**）。Gremlin分发版附带了一个基于Groovy的shell，允许对Blueprints图进行交互式分析和修改。'
- en: '**Frames**: Frames is an object-to-graph mapping framework analogous to an
    ORM but tailored for graphs.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Frames**：Frames是一个对象到图映射框架，类似于ORM，但专为图设计。'
- en: '**Furnace**: The Furnace project aims to provide implementations of many common
    graph algorithms for Blueprints property graphs.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Furnace**：Furnace项目旨在为Blueprints属性图提供许多常见图算法的实现。'
- en: '**Rexster**: Rexster is a graph server that exposes Blueprints graphs through
    a REST API, as well as a binary protocol.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rexster**：Rexster是一个通过REST API和二进制协议公开Blueprints图的图服务器。'
- en: For our purposes, we will be focusing on the Blueprints API for populating a
    graph from a Storm topology and Gremlin for graph queries and analysis.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们将专注于使用Blueprints API从Storm拓扑中填充图以及使用Gremlin进行图查询和分析。
- en: Manipulating the graph with the Blueprints API
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Blueprints API操作图
- en: 'The Blueprints API is very straightforward. The following code listing uses
    the Blueprints API to create the graph depicted in the previous diagram:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Blueprints API非常简单。以下代码清单使用Blueprints API创建了前面图表中所示的图：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first line of code instantiates an implementation of the `com.tinkerpop.blueprints.Graph`
    interface. In this case, we're creating an in-memory, toy graph (`com.tinkerpop.blueprints.impls.tg.TinkerGraph`)
    for exploration. Later, we will demonstrate how to connect to a distributed graph
    database.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第一行实例化了`com.tinkerpop.blueprints.Graph`接口的实现。在这种情况下，我们创建了一个内存中的玩具图（`com.tinkerpop.blueprints.impls.tg.TinkerGraph`）进行探索。稍后，我们将演示如何连接到分布式图数据库。
- en: Tip
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: You may be wondering why we are passing `null` as a parameter to the `addVertex()`
    and `addEdge()` methods at the first argument. This argument is essentially a
    suggestion to the underlying Blueprints implementation for a unique ID for the
    object. Passing in `null` as the ID simply has the effect of letting the underlying
    implementation assign an ID to the new object.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么我们将`null`作为参数传递给`addVertex()`和`addEdge()`方法的第一个参数。这个参数实质上是对底层Blueprints实现提供对象的唯一ID的建议。将`null`作为ID传递只是让底层实现为新对象分配一个ID。
- en: Manipulating the graph with the Gremlin shell
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Gremlin shell操作图
- en: 'Gremlin is a high-level Java API built on the top of the Pipes and Blueprints
    APIs. In addition to the Java API, Gremlin also includes a Groovy-based API and
    ships with an interactive shell (or REPL) that allows you to directly interact
    with a Blueprints graph. The Gremlin shell allows you to create and/or connect
    to the shell and query virtually any Blueprints graph. The following code listing
    illustrates the process of executing the Gremlin shell:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Gremlin是建立在Pipes和Blueprints API之上的高级Java API。除了Java API外，Gremlin还包括基于Groovy的API，并附带一个交互式shell（或REPL），允许您直接与Blueprints图交互。Gremlin
    shell允许您创建和/或连接到shell，并查询几乎任何Blueprints图。以下代码清单说明了执行Gremlin shell的过程：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In addition to querying a graph, it is also easy to create and manipulate graphs
    using Gremlin. The following code listing consists of Gremlin Groovy code that
    will create the same graph illustrated in the previous diagram and is the Groovy
    equivalent of the Java code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 除了查询图之外，使用Gremlin还可以轻松创建和操作图。以下代码清单包括将创建与前面图示相同的图的Gremlin Groovy代码，是Java代码的Groovy等价物：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You will learn more about using the Gremlin API and DSL later in the chapter
    once we've built a topology to populate a graph and are ready to analyze the graph
    data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们构建了一个拓扑图来填充图并准备好分析图数据，您将在本章后面学习如何使用Gremlin API和DSL。
- en: Software installation
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件安装
- en: The application we're building will utilize Apache Kafka and its dependencies
    (Apache ZooKeeper). If you haven't done so already, set up ZooKeeper and Kafka
    according to the instructions in the *ZooKeeper installation* section in [Chapter
    2](ch02.html "Chapter 2. Configuring Storm Clusters"), *Configuring Storm Clusters*,
    and the *Installing Kafka* section in [Chapter 4](ch04.html "Chapter 4. Real-time
    Trend Analysis"), *Real-time Trend Analysis*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建的应用程序将利用Apache Kafka及其依赖项（Apache ZooKeeper）。如果您还没有这样做，请根据[第2章](ch02.html
    "第2章。配置风暴集群")中“ZooKeeper安装”部分的说明设置ZooKeeper和Kafka，以及[第4章](ch04.html "第4章。实时趋势分析")中“安装Kafka”部分的说明，进行配置风暴集群和实时趋势分析。
- en: Titan installation
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Titan安装
- en: 'To install Titan, download the Titan 0.3.x complete package from Titan''s downloads
    page ([https://github.com/thinkaurelius/titan/wiki/Downloads](https://github.com/thinkaurelius/titan/wiki/Downloads)),
    and extract it to a convenient location by using the following command:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Titan，请从Titan的下载页面（[https://github.com/thinkaurelius/titan/wiki/Downloads](https://github.com/thinkaurelius/titan/wiki/Downloads)）下载Titan
    0.3.x完整包，并使用以下命令将其提取到方便的位置：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Titan''s complete distribution package includes everything that is necessary
    for running Titan with any of the supported storage backends: Cassandra, HBase,
    and BerkelyDB. There are also backend-specific distributions if you are only interested
    in using a specific storage backend.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Titan的完整分发包包括运行Titan所需的一切支持的存储后端：Cassandra、HBase和BerkelyDB。如果您只对使用特定存储后端感兴趣，还有特定于后端的分发。
- en: Note
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Both Storm and Titan use the Kryo ([https://code.google.com/p/kryo/](https://code.google.com/p/kryo/))
    library for Java object serialization. At the time of writing, Storm and Titan
    use different versions of the Kryo library, which will cause problems when the
    two are used in conjunction.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Storm和Titan都使用Kryo（[https://code.google.com/p/kryo/](https://code.google.com/p/kryo/)）库进行Java对象序列化。在撰写本文时，Storm和Titan使用不同版本的Kryo库，这将在两者同时使用时引起问题。
- en: 'To patch Titan in order to properly enable serialization between Storm and
    Titan, replace the `kryo.jar` file in the Titan distribution with the `kryo.jar`
    file that comes with Storm:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确启用Storm和Titan之间的序列化，需要对Titan进行补丁，将Titan分发中的`kryo.jar`文件替换为Storm提供的`kryo.jar`文件：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At this point, you can test the installation by running the Gremlin shell:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可以通过运行Gremlin shell来测试安装：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`GraphOfTheGodsFactory` is a class included with Titan that will create and
    populate a Titan database with a sample graph that represents the relationships
    between the characters and places in the Roman pantheon. Passing a directory path
    to the `create()` method will return a Blueprints graph implementation, specifically
    a `com.thinkaurelius.titan.graphdb.database.StandardTitanGraph` instance that
    uses a combination of BerkelyDB and Elasticsearch for a storage backend. Since
    the Gremlin shell is a Groovy REPL, we can easily verify this by looking at the
    class of the `g` variable:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`GraphOfTheGodsFactory`是Titan中包含的一个类，它将使用样本图创建和填充一个Titan数据库，该图表示罗马万神殿中角色和地点之间的关系。将目录路径传递给`create()`方法将返回一个Blueprints图实现，具体来说是一个使用BerkelyDB和Elasticsearch组合作为存储后端的`com.thinkaurelius.titan.graphdb.database.StandardTitanGraph`实例。由于Gremlin
    shell是一个Groovy REPL，我们可以通过查看`g`变量的类轻松验证这一点：'
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Setting up Titan to use the Cassandra storage backend
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Titan以使用Cassandra存储后端
- en: We've seen that Titan supports different storage backends. Exploring all three
    options is beyond the scope of this chapter (you can learn more about Titan and
    its configuration options at [http://thinkaurelius.github.io/titan/](http://thinkaurelius.github.io/titan/)),
    so we will focus on using the Cassandra ([http://cassandra.apache.org](http://cassandra.apache.org))
    storage backend.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到Titan支持不同的存储后端。探索所有三个选项超出了本章的范围（您可以在[http://thinkaurelius.github.io/titan/](http://thinkaurelius.github.io/titan/)了解有关Titan及其配置选项的更多信息），因此我们将专注于使用Cassandra（[http://cassandra.apache.org](http://cassandra.apache.org)）存储后端。
- en: Installing Cassandra
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Cassandra
- en: 'In order to download and run Cassandra, we need to execute the following commands:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了下载和运行Cassandra，我们需要执行以下命令：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The default file that comes with the Cassandra distribution will create a single-node
    Cassandra database running locally. If there is an error during the startup, you
    may need to configure Cassandra by editing the `${CASSANDRA_HOME}/conf/cassandra.yaml`
    and/or `${CASSANDRA_HOME}/conf/log4j-server.properties` files. The most common
    problems are usually related to the lack of file-write permissions on `/var/lib/cassandra`
    (where, by default, Cassandra stores its data) and `/var/log/cassandra` (the default
    Cassandra log location).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra分发的默认文件将创建一个在本地运行的单节点Cassandra数据库。如果在启动过程中出现错误，您可能需要通过编辑`${CASSANDRA_HOME}/conf/cassandra.yaml`和/或`${CASSANDRA_HOME}/conf/log4j-server.properties`文件来配置Cassandra。最常见的问题通常与在`/var/lib/cassandra`（默认情况下，Cassandra存储其数据的位置）和`/var/log/cassandra`（默认Cassandra日志位置）上缺乏文件写入权限有关。
- en: Starting Titan with the Cassandra backend
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Cassandra后端启动Titan
- en: 'To run Titan with Cassandra, we need to configure it to connect to our Cassandra
    server. Create a new file called `storm-blueprints-cassandra.yaml` with the following
    contents:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Cassandra运行Titan，我们需要配置它连接到我们的Cassandra服务器。创建一个名为`storm-blueprints-cassandra.yaml`的新文件，内容如下：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can probably surmise, this configures Titan to connect to the Cassandra
    instance running locally.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能推测的那样，这配置Titan连接到本地运行的Cassandra实例。
- en: Note
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For this project, we may not need to actually run the Titan server. Since we're
    using Cassandra, Storm and Gremlin should be able to share the backend without
    any issues.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们可能不需要实际运行Titan服务器。由于我们使用的是Cassandra，Storm和Gremlin应该能够在没有任何问题的情况下共享后端。
- en: With the Titan backend configured, we are ready to create our data model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有了Titan后端配置，我们准备创建我们的数据模型。
- en: Graph data model
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图数据模型
- en: 'The primary entity in our data model is a Twitter user. A Twitter user can
    perform the following relationship-forming actions when posting a tweet:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据模型中的主要实体是Twitter用户。当发布一条推文时，Twitter用户可以执行以下关系形成的操作：
- en: Use a word
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个单词
- en: Mention a hashtag
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提及一个标签
- en: Mention another user
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提及另一个用户
- en: Mention a URL
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提及URL
- en: Retweet another user
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转推另一个用户
- en: '![Graph data model](img/8294_05_06.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图数据模型](img/8294_05_06.jpg)'
- en: 'This concept maps very naturally into a graph model. In the model, we will
    have four different entity types (vertices):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念非常自然地映射到图模型中。在模型中，我们将有四种不同的实体类型（顶点）：
- en: '**User**: This represents a Twitter user account'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户：这代表了一个Twitter用户账户
- en: '**Word**: This represents any word contained in a tweet'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单词**：这代表推文中包含的任何单词'
- en: '**URL**: This represents any URL contained in a tweet'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**URL**：这代表推文中包含的任何URL'
- en: '**Hashtag**: This represents any hashtag contained in a tweet'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签**：这代表推文中包含的任何标签'
- en: 'Relationships (edges) will consist of the following actions:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 关系（边）将包括以下操作：
- en: '**mentions_user**: Using this action, a user mentions another user'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提及用户**：使用此操作，用户提及另一个用户'
- en: '**retweets_user**: Using this action, a user retweets another user''s post'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转推用户**：使用此操作，用户转推另一个用户的帖子'
- en: '**follows_user**: Using this action, a user follows another user'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注用户**：使用此操作，用户关注另一个用户'
- en: '**mentions_hashtag**: Using this action, a user mentions a hashtag'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提及标签**：使用此操作，用户提及一个标签'
- en: '**uses_word**: Using this action, the user uses a specific word in a tweet'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用单词**：使用此操作，用户在推文中使用特定的单词'
- en: '**mentions_url**: Using this action, a user tweets a specific URL'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提及URL**：使用此操作，用户推文特定的URL'
- en: 'The user vertex models a user''s Twitter account information, which is shown
    in the following table:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 用户顶点模拟了用户的Twitter账户信息，如下表所示：
- en: '| User [vertex] |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 用户 [顶点] |'
- en: '| --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| type | String | `"user"` |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 字符串 | `"用户"` |'
- en: '| user | String | Twitter screen name |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 字符串 | Twitter用户名 |'
- en: '| name | String | Twitter name |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 字符串 | Twitter名称 |'
- en: '| location | String | Twitter location |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 位置 | 字符串 | Twitter位置 |'
- en: 'The URL vertex provides a reference point for unique URLs:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: URL顶点提供了唯一URL的参考点：
- en: '| URL [vertex] |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| URL [顶点] |'
- en: '| --- |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| type | String | `"url"` |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 字符串 | `"url"` |'
- en: '| value | String | URL |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 值 | 字符串 | URL |'
- en: 'The hashtag vertex allows us to store unique hashtags:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 标签顶点允许我们存储唯一的标签：
- en: '| Hashtag [vertex] |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 标签 [顶点] |'
- en: '| --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| type | String | `"hashtag"` |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 字符串 | `"标签"` |'
- en: '| value | String |   |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 值 | 字符串 |   |'
- en: 'We store individual words in the word vertex:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在单词顶点中存储单个单词：
- en: '| Word [vertex] |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 单词 [顶点] |'
- en: '| --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| type | String | `"word"` |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 字符串 | `"单词"` |'
- en: '| value | String |   |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 值 | 字符串 |   |'
- en: 'The `mentions_user` edge is used for relationships between user objects:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`提及用户`边用于用户对象之间的关系：'
- en: '| mentions_user [edge] |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 提及用户 [边] |'
- en: '| --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| user | String | The ID of the user mentioned |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 字符串 | 被提及用户的ID |'
- en: 'The `mentions_url` edge represents a relationship between the User and URL
    objects:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`提及URL`边表示用户和URL对象之间的关系：'
- en: '| mentions_url [edge] |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 提及URL [边] |'
- en: '| --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| user | String | The ID of the user mentioned |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 字符串 | 被提及用户的ID |'
- en: Connecting to the Twitter stream
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接到Twitter流
- en: 'In order to connect to the Twitter API, we must first generate a set of OAuth
    tokens that will enable our application to authenticate with Twitter. This is
    done by creating a Twitter application that is associated with your account and
    then authorizing that application to access your account. If you do not already
    have a Twitter account, create one now and log in to it. Once you are logged in
    to Twitter, generate the OAuth tokens by following these steps:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了连接到Twitter API，我们必须首先生成一组OAuth令牌，这将使我们的应用程序能够与Twitter进行身份验证。这是通过创建一个与您的账户关联的Twitter应用程序，然后授权该应用程序访问您的账户来完成的。如果您还没有Twitter账户，请立即创建一个并登录。登录到Twitter后，按照以下步骤生成OAuth令牌：
- en: Go to [https://dev.twitter.com/apps/new](https://dev.twitter.com/apps/new) and
    log in if necessary.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往[https://dev.twitter.com/apps/new](https://dev.twitter.com/apps/new)，如果需要，请登录。
- en: Enter a name and description for your application.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为你的应用程序输入一个名称和描述。
- en: Enter a URL for your application. In our case, the URL is unimportant since
    we're not creating an app that will be distributed like a mobile app. Entering
    a placeholder URL here is fine.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的情况下，输入一个应用程序的URL是不重要的，因为我们不是在创建一个像移动应用程序那样会被分发的应用程序。在这里输入一个占位符URL是可以的。
- en: Submit the form. The next page will display the details of the OAuth settings
    for your application. Note the **Consumer key** and **Consumer secret** values
    since we will need those for our application.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提交表单。下一页将显示您的应用程序的OAuth设置的详细信息。请注意**消费者密钥**和**消费者密钥**的值，因为我们需要这些值用于我们的应用程序。
- en: At the bottom of the page, click on the **Create my access token** button. This
    will generate an OAuth Access token and a secret key that will allow an application
    to access your account on your behalf. We will also need these values for our
    application. Do not share these values as they would allow someone else to authenticate
    as you.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在页面底部，点击**创建我的访问令牌**按钮。这将生成一个OAuth访问令牌和一个密钥，允许应用程序代表您访问您的帐户。我们也需要这些值用于我们的应用程序。不要分享这些值，因为它们会允许其他人以您的身份进行认证。
- en: Setting up the Twitter4J client
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置Twitter4J客户端
- en: 'The Twitter4J client is broken down into a number of different modules that
    can be pieced together depending on our needs. For our purposes, we need the `core`
    module that provides essential functionalities such as HTTP transport, OAuth,
    and access to the basic Twitter API. We will also use the `stream` module for
    accessing the streaming API. These modules can be included in the project by adding
    the following Maven dependencies:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter4J客户端被分解为许多不同的模块，可以根据我们的需求组合在一起。对于我们的目的，我们需要`core`模块，它提供了基本功能，如HTTP传输、OAuth和对基本Twitter
    API的访问。我们还将使用`stream`模块来访问流API。这些模块可以通过添加以下Maven依赖项包含在项目中：
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The OAuth configuration
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OAuth配置
- en: 'By default, Twitter4J will search the classpath for a `twitter4j.properties`
    file and load OAuth tokens from that file. The easiest way to do this is to create
    the file in the `resources` folder of your Maven project. Add the tokens you generated
    earlier to this file:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Twitter4J将在类路径中搜索`twitter4j.properties`文件，并从该文件加载OAuth令牌。这样做的最简单方法是在Maven项目的`resources`文件夹中创建该文件。将之前生成的令牌添加到这个文件中：
- en: '[PRE10]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We're now ready to use the Twitter4J client to connect to Twitter's streaming
    API to consume tweets in real time.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备使用Twitter4J客户端连接到Twitter的流API，实时消费推文。
- en: The TwitterStreamConsumer class
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TwitterStreamConsumer类
- en: 'The purpose of our Twitter client is straightforward; it will perform the following
    functions:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Twitter客户端的目的很简单；它将执行以下功能：
- en: Connect to the Twitter streaming API
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到Twitter流API
- en: Request a stream of tweets filtered by a set of keywords
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求通过一组关键字过滤的推文流
- en: Create a JSON data structure based on the status message
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据状态消息创建一个JSON数据结构
- en: Write the JSON data to Kafka for consumption by the Kafka spout
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将JSON数据写入Kafka以供Kafka spout消费
- en: 'The `main()` method of the `TwitterStreamConsumer` class creates a `TwitterStream`
    object and registers an instance of `StatusListener` as a listener. The `StatusListener`
    interface is used as an asynchronous event handler that is notified whenever a
    stream-related event occurs:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`TwitterStreamConsumer`类的`main()`方法创建一个`TwitterStream`对象，并注册`StatusListener`的一个实例作为监听器。`StatusListener`接口用作异步事件处理程序，每当发生与流相关的事件时就会通知它：'
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: After registering the listener, we create a `FilterQuery` object to filter the
    stream based on a set of keywords. For convenience, we use the program arguments
    as the list of keywords so the filter criteria can be easily changed from the
    command line.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注册监听器后，我们创建一个`FilterQuery`对象来根据一组关键字过滤流。为了方便起见，我们使用程序参数作为关键字列表，因此过滤条件可以很容易地从命令行更改。
- en: The TwitterStatusListener class
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TwitterStatusListener类
- en: 'The `TwitterStatusListener` class performs most of the heavy lifting in our
    application. The `StatusListener` class defines several callback methods for events
    that can occur during the lifetime of a stream. The `onStatus()` method is our
    primary interest, since it is the method that gets calls whenever a new Tweet
    arrives. The following is the code for the `TwitterStatusListener` class:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`TwitterStatusListener`类在我们的应用程序中承担了大部分的重活。`StatusListener`类定义了几个回调方法，用于在流的生命周期中可能发生的事件。我们主要关注`onStatus()`方法，因为这是每当有新推文到达时调用的方法。以下是`TwitterStatusListener`类的代码：'
- en: '[PRE12]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In addition to the raw text of the status message, the `Status` object includes
    convenient methods for accessing all the associated metadata, such as user information,
    the hashtags, URLs, and user mentions contained in the tweet. The bulk of our
    `onStatus()` method builds up the JSON structure before finally logging it to
    the Kafka queue via the Logback Kafka Appender.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 除了状态消息的原始文本之外，`Status`对象还包括方便的方法，用于访问所有相关的元数据，例如包含在推文中的用户信息、标签、URL和用户提及。我们的`onStatus()`方法的大部分内容在最终通过Logback
    Kafka Appender将其记录到Kafka队列之前构建JSON结构。
- en: Twitter graph topology
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter图拓扑
- en: The Twitter graph topology will read raw tweet data from the Kafka queue, parse
    out the relevant information, and then create nodes and relationships in the Titan
    graph database. Instead of writing to the graph database individually for each
    tuple received, we will implement a trident state implementation for performing
    persistence operations in bulk using Trident's transaction mechanism.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter图拓扑将从Kafka队列中读取原始推文数据，解析出相关信息，然后在Titan图数据库中创建节点和关系。我们将使用Trident的事务机制实现一个trident状态实现，以便批量执行持久性操作，而不是为每个接收到的元组单独写入图数据库。
- en: This approach offers several benefits. First, for graph databases, such as Titan
    that supports transactions, we can leverage this capability to provide additional
    exactly-once processing guarantees. Second, it allows us to perform a bulk-write
    followed by a bulk-commit (when supported) for an entire batch of tuples rather
    than a write-commit operation for each individual tuple. Finally, by using the
    generic Blueprints API, our Trident state implementation will be largely agnostic
    to the underlying graph database implementation, allowing any Blueprints graph
    database backend to be easily swapped in and out.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法提供了几个好处。首先，对于支持事务的图数据库，比如Titan，我们可以利用这个能力提供额外的一次性处理保证。其次，它允许我们执行批量写入，然后进行批量提交（如果支持）来处理整个批处理的元组，而不是对每个单独的元组进行写入提交操作。最后，通过使用通用的Blueprints
    API，我们的Trident状态实现将在很大程度上对基础图数据库实现保持不可知，从而可以轻松地替换任何Blueprints图数据库后端。
- en: '![Twitter graph topology](img/8294_05_07.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![Twitter graph topology](img/8294_05_07.jpg)'
- en: The first component of the topology consists of `JSONProjectFunction,which`
    we developed in [Chapter 7](ch07.html "Chapter 7. Integrating Druid for Financial
    Analytics"), *Integrating Druid for Financial Analytics*, which simply parses
    the raw JSON data to extract only the information we are interested in. In this
    case, we are mainly interested in the timestamp of the message and the JSON representation
    of the Twitter status message.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑的第一个组件包括我们在[第7章](ch07.html "第7章。集成Druid进行金融分析")中开发的`JSONProjectFunction`，*集成Druid进行金融分析*，它简单地解析原始JSON数据，提取我们感兴趣的信息。在这种情况下，我们主要关注消息的时间戳和Twitter状态消息的JSON表示。
- en: The JSONProjectFunction class
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JSONProjectFunction类
- en: 'The following is a code snippet explaining the `JSONProjectFunction` class:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个解释`JSONProjectFunction`类的代码片段：
- en: '[PRE13]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Implementing GraphState
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现GraphState
- en: 'The heart of the topology will be a Trident state implementation responsible
    for translating Trident tuples into graph structures and persisting them. Recall
    that a Trident state implementation consists of three components:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑的核心将是一个Trident状态实现，负责将Trident元组转换为图结构并将其持久化。回想一下，Trident状态实现由三个组件组成：
- en: '`StateFactory`: The `StateFactory` interface defines the method Trident uses
    to create the persistent `State` objects.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StateFactory`：`StateFactory`接口定义了Trident用来创建持久`State`对象的方法。'
- en: '`State`: The Trident `State` interface defines the `beginCommit()` and `commit()`
    methods that are called before and after a Trident batch partition is written
    to the backing store. If the write succeeds (that is, all tuples are processed
    without error), Trident will call the `commit()` method.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`State`：Trident `State`接口定义了在Trident批处理分区写入到后端存储之前和之后调用的`beginCommit()`和`commit()`方法。如果写入成功（即，所有元组都被处理而没有错误），Trident将调用`commit()`方法。'
- en: '`StateUpdater`: The `StateUpdater` interface defines the `updateState()` method
    that is called to update the state, given that there is a batch of tuples. Trident
    passes three arguments to this method: the `State` object to be updated, a list
    of `TridentTuple` objects that represents a batch partition, and a `TridentCollector`
    instance that can be used to optionally emit additional tuples as a result of
    the state update.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StateUpdater`：`StateUpdater`接口定义了`updateState()`方法，用于更新状态，假设有一批元组。Trident将三个参数传递给这个方法：要更新的`State`对象，代表批处理的`TridentTuple`对象列表，以及可以用来可选地发出额外元组的`TridentCollector`实例作为状态更新的结果。'
- en: In addition to these abstractions provided by Trident, we will introduce two
    additional interfaces that will support the use of any Blueprints graph database
    (`GraphFactory`) and isolate any use-case-specific business logic (`GraphTupleProcessor`).
    Before diving in to the Trident state implementation, let's quickly look at these
    interfaces.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Trident提供的这些抽象，我们还将介绍两个额外的接口，支持任何Blueprints图数据库的使用（`GraphFactory`），并隔离任何特定用例的业务逻辑（`GraphTupleProcessor`）。在深入研究Trident状态实现之前，让我们快速看一下这些接口。
- en: GraphFactory
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphFactory
- en: 'The `GraphFactory` interface contract is simple: given a `Map` object that
    represents the Storm and topology configuration, return a `com.tinkerpop.blueprints.Graph`
    implementation.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`GraphFactory`接口的合同很简单：给定一个代表风暴和拓扑配置的`Map`对象，返回一个`com.tinkerpop.blueprints.Graph`实现。'
- en: '[PRE14]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This interface allows us to plug in any Blueprints-compatible graph implementation
    simply by providing an implementation of the `makeGraph()` method. Later, we will
    implement this interface to return a connection to a Titan graph database.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这个接口允许我们通过提供`makeGraph()`方法的实现来简单地插入任何兼容Blueprints的图实现。稍后，我们将实现这个接口，返回到Titan图数据库的连接。
- en: GraphTupleProcessor
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphTupleProcessor
- en: The `GraphTupleProcessor` interface provides an abstraction between the Trident
    state implementation and any use-case-specific business logic.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`GraphTupleProcessor`接口在Trident状态实现和任何特定用例的业务逻辑之间提供了一个抽象。'
- en: '[PRE15]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Given a graph object, `TridentTuple`, and `TridentCollector`, manipulating the
    graph and optionally emitting additional tuples is the job of a `GraphTupleProcessor`.
    Later in the chapter, we will implement this interface to populate a graph based
    on the content of a Twitter status message.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个图对象、`TridentTuple`和`TridentCollector`，操作图并可选择发出额外的元组是`GraphTupleProcessor`的工作。在本章后面，我们将实现这个接口，根据Twitter状态消息的内容填充图。
- en: GraphStateFactory
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphStateFactory
- en: Trident's `StateFactory` interface represents the entry point for a state implementation.
    When a Trident topology using state components (via the `Stream.partitionPersist()`
    and `Stream.persistentAggregate()` methods) initializes, Storm calls the `StateFactory.makeState()`
    method to create a State instance for each batch partition. The number of batch
    partitions is determined by the parallelism of the stream. Storm passes this information
    to the `makeState()` method via the `numPartitions` and `partitionIndex` parameters,
    allowing state implementations to perform partition-specific logic if necessary.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Trident的`StateFactory`接口代表了状态实现的入口点。当使用状态组件的Trident拓扑（通过`Stream.partitionPersist()`和`Stream.persistentAggregate()`方法）初始化时，Storm调用`StateFactory.makeState()`方法为每个批处理分区创建一个状态实例。批处理分区的数量由流的并行性确定。Storm通过`numPartitions`和`partitionIndex`参数将这些信息传递给`makeState()`方法，允许状态实现在必要时执行特定于分区的逻辑。
- en: In our use case, we're not concerned with partitions, so the `makeState()` method
    just uses a `GraphFactory` instance to instantiate a `Graph` instance used to
    construct a `GraphState` instance.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的用例中，我们不关心分区，所以`makeState()`方法只是使用`GraphFactory`实例来实例化一个用于构建`GraphState`实例的`Graph`实例。
- en: '[PRE16]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: GraphState
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphState
- en: Our `GraphState` class provides implementations for `State.beginCommit()` and
    `State.commit()` methods that will be called when a batch partition is about to
    take place and when it has successfully completed, respectively. In our case,
    we override the `commit()` method to check if the internal `Graph` object supports
    transactions, and if so, call the `TransactionalGraph.commit()` method to complete
    the transaction.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`GraphState`类提供了`State.beginCommit()`和`State.commit()`方法的实现，当批处理分区即将发生和成功完成时将被调用。在我们的情况下，我们重写`commit()`方法来检查内部的`Graph`对象是否支持事务，如果是，就调用`TransactionalGraph.commit()`方法来完成事务。
- en: Note
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `State.beginCommit()` method may be called multiple times if there are failures
    within a Trident batch and the batch is replayed, while the `State.commit()` method
    will only get called once when all partition state updates have completed successfully.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在Trident批处理中出现故障并且批处理被重播，`State.beginCommit()`方法可能会被多次调用，而`State.commit()`方法只会在所有分区状态更新成功完成时被调用一次。
- en: 'The code snippet of the `GraphState` class is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`GraphState`类的代码片段如下：'
- en: '[PRE17]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `GraphState.update()` method does the core processing of the transaction
    between the calls to the `State.beginCommit()` and `State.commit()` methods. If
    the `update()` method succeeds for all batch partitions, the Trident transaction
    will complete and the `State.commit()` method will be called.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`GraphState.update()`方法在调用`State.beginCommit()`和`State.commit()`方法之间进行事务的核心处理。如果`update()`方法对所有批处理分区都成功，Trident事务将完成，并且将调用`State.commit()`方法。'
- en: Notice that the `update()` method that actually updates the graph state is simply
    a public method of the `GraphState` class and not overridden. As you will see,
    we will have the opportunity to call this method directly in our `StateUpdater`
    implementation.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，实际更新图状态的`update()`方法只是`GraphState`类的一个公共方法，而不是被覆盖。正如您将看到的，我们将有机会在我们的`StateUpdater`实现中直接调用这个方法。
- en: GraphUpdater
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphUpdater
- en: The `GraphUpdater` class implements the `updateState()` method that Storm will
    call (potentially repeatedly in the case of batch failures/replays) just after
    the call to `State.beginCommit()`. The first argument to the `StateUpdater.updateState()`
    method is a Java generics-typed instance of our state implementation that we use
    to call our `GraphState.update()` method.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`GraphUpdater`类实现了Storm将调用的`updateState()`方法（在批处理失败/重播的情况下可能会重复调用）。`StateUpdater.updateState()`方法的第一个参数是我们用来调用`GraphState.update()`方法的Java泛型类型实例。'
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Implementing GraphFactory
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现GraphFactory
- en: 'The `GraphFactory` interface we defined earlier creates a TinkerPop Graph implementation,
    where a `Map` object represents a Storm configuration. The following code illustrates
    how to create `TitanGraph` backed by Cassandra:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前定义的`GraphFactory`接口创建了一个TinkerPop图实现，其中`Map`对象表示了一个Storm配置。以下代码说明了如何创建由Cassandra支持的`TitanGraph`：
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Implementing GraphTupleProcessor
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现GraphTupleProcessor
- en: In order to populate the graph database with relationships gleaned from Twitter
    status messages, we need to implement the `GraphTupleProcessor` interface. The
    following code illustrates parsing the Twitter status message's JSON object and
    creating `"user"` and `"hashtag"` vertices with `"mentions"` relationships.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用从Twitter状态消息中获取的关系填充图数据库，我们需要实现`GraphTupleProcessor`接口。以下代码说明了解析Twitter状态消息的JSON对象并创建带有`"mentions"`关系的`"user"`和`"hashtag"`顶点。
- en: '[PRE20]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Putting it all together – the TwitterGraphTopology class
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容放在一起 - TwitterGraphTopology类
- en: 'Creating our final topology consists of the following steps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 创建我们的最终拓扑包括以下步骤：
- en: Consume raw JSON from the Kafka spout
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Kafka喷嘴中消耗原始JSON
- en: Extract and project only the data we are interested in
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取和投影我们感兴趣的数据
- en: Build and connect the Trident `GraphState` implementation to our stream
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建并连接Trident的`GraphState`实现到我们的流
- en: The TwitterGraphTopology class
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TwitterGraphTopology类
- en: Let's look at the TwitterGraphTopology class in detail.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看一下TwitterGraphTopology类。
- en: '[PRE21]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To run the application, begin by executing the `TwitterStreamConsumer` class,
    passing in a list of keywords you want to use to query the Twitter firehose. For
    example, if we want to build a graph of users discussing big data, we might use
    `bigdata` and `hadoop` as query parameters:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行应用程序，首先执行`TwitterStreamConsumer`类，传入您想要用来查询Twitter firehose的关键字列表。例如，如果我们想要构建一个讨论大数据的用户图，我们可以使用`bigdata`和`hadoop`作为查询参数：
- en: '[PRE22]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `TwitterStreamConsumer` class will connect to the Twitter Streaming API
    and begin queuing data to Kafka. With the `TwitterStreamConsumer` application
    running, we can then deploy `TwitterGraphTopology` to begin populating the Titan
    database.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`TwitterStreamConsumer`类将连接到Twitter Streaming API并开始将数据排队到Kafka。运行`TwitterStreamConsumer`应用程序后，我们可以部署`TwitterGraphTopology`来开始填充Titan数据库。'
- en: Let `TwitterStreamConsumer` and `TwitterGraphTopology` run for a while. Depending
    on the popularity of the keywords used for the query, it may take some time for
    the dataset to grow to a meaningful level. We can then connect to Titan with the
    Gremlin shell to analyze the data with graph queries.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让`TwitterStreamConsumer`和`TwitterGraphTopology`运行一段时间。根据查询使用的关键词的流行程度，数据集可能需要一些时间才能增长到一个有意义的水平。然后我们可以使用Gremlin
    shell连接到Titan来分析图查询中的数据。
- en: Querying the graph with Gremlin
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Gremlin查询图形
- en: 'To query the graph, we need to launch the Gremlin shell and create a `TitanGraph`
    instance connected to the local Cassandra backend:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要查询图形，我们需要启动Gremlin shell并创建连接到本地Cassandra后端的`TitanGraph`实例：
- en: '[PRE23]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `g` variable now contains a `Graph` object we can use to issue graph traversal
    queries. The following are a few sample queries you can use to get started:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`g`变量现在包含一个我们可以使用来发出图遍历查询的`Graph`对象。以下是一些示例查询，您可以使用它们来开始：'
- en: 'To find all the users who have tweeted `#hadoop hashtag` and to show the number
    of times they have done this, use the following code:'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要查找所有发推`#hadoop标签`的用户，并显示他们这样做的次数，请使用以下代码：
- en: '[PRE24]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To count the number of times the `#hadoop hashtag` has been tweeted, use the
    following code:'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要计算`#hadoop标签`被发推文的次数，请使用以下代码：
- en: '[PRE25]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The Gremlin DSL is very powerful; covering the complete API could fill an entire
    chapter (if not a whole book). To further explore the Gremlin language, we encourage
    you to explore the following online documentation:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Gremlin DSL非常强大；覆盖完整API可能需要填满整整一章（甚至一本整书）。要进一步探索Gremlin语言，我们鼓励您探索以下在线文档：
- en: The official Gremlin Wiki at [https://github.com/tinkerpop/gremlin/wiki](https://github.com/tinkerpop/gremlin/wiki)
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 官方Gremlin Wiki在[https://github.com/tinkerpop/gremlin/wiki](https://github.com/tinkerpop/gremlin/wiki)
- en: GremlinDocs reference guide at [http://gremlindocs.com](http://gremlindocs.com)
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GremlinDocs参考指南在[http://gremlindocs.com](http://gremlindocs.com)
- en: SQL2Gremlin (sample SQL queries and their Gremlin equivalents) at [http://sql2gremlin.com](http://sql2gremlin.com)
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL2Gremlin（示例SQL查询及其Gremlin等效查询）在[http://sql2gremlin.com](http://sql2gremlin.com)
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced you to graph databases by creating a topology
    that monitors a subset of the Twitter firehose and persists that information to
    the Titan graph database for further analysis. We've also demonstrated the reuse
    of generic components by using generic building blocks from earlier chapters such
    as the Logback Kafka appender.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过创建一个监视Twitter firehose子集并将信息持久化到Titan图数据库以供进一步分析的拓扑图，向您介绍了图数据库。我们还演示了通过使用早期章节的通用构建块（如Logback
    Kafka appender）来重复使用通用组件。
- en: While graph databases are not perfect for every use case, they represent a powerful
    weapon in your arsenal of polyglot persistence tools. Polyglot persistence is
    a term often used to describe a software architecture that involves multiple types
    of data stores such as relational, key-value, graph, document, and so on. Polyglot
    persistence is all about choosing the right database for the right job. In this
    chapter, we introduced you to graph data models, and have hopefully inspired you
    to explore situations where a graph may be the best data model to support a given
    use case. Later in the book, we will create a Storm application that persists
    data to multiple data stores, each for a specific purpose.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图数据库并非适用于每种用例，但它们代表了您多语言持久性工具库中的强大武器。多语言持久性是一个经常用来描述涉及多种数据存储类型（如关系型、键值、图形、文档等）的软件架构的术语。多语言持久性是关于为正确的工作选择正确的数据库。在本章中，我们向您介绍了图形数据模型，并希望激发您探索图形可能是支持特定用例的最佳数据模型的情况。在本书的后面，我们将创建一个Storm应用程序，将数据持久化到多个数据存储中，每个存储都有特定的目的。
