- en: Installing Pyspark and Setting up Your Development Environment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Pyspark并设置开发环境
- en: In this chapter, we are going to introduce Spark and learn the core concepts,
    such as, SparkContext, and Spark tools such as SparkConf and Spark shell. The
    only prerequisite is the knowledge of basic Python concepts and the desire to
    seek insight from big data. We will learn how to analyze and discover patterns
    with Spark SQL to improve our business intelligence. Also, you will be able to
    quickly iterate through your solution by setting to PySpark for your own computer. By
    the end of the book, you will be able to work with real-life messy data sets using
    PySpark to get practical big data experience.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍Spark并学习核心概念，如SparkContext，以及Spark工具，如SparkConf和Spark shell。唯一的先决条件是对基本Python概念的了解，并且希望从大数据中寻求洞察力。我们将学习如何使用Spark
    SQL分析和发现模式，以改进我们的业务智能。此外，您将能够通过设置PySpark来快速迭代解决方案。在本书结束时，您将能够使用PySpark处理真实的混乱数据集，从而获得实际的大数据经验。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: An overview of PySpark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark概述
- en: Setting up Spark on Windows and PySpark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Windows上设置Spark和PySpark
- en: Core concepts in Spark and PySpark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark和PySpark中的核心概念
- en: An overview of PySpark
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark概述
- en: Before we start with installing PySpark, which is the Python interface for Spark,
    let's go through some core concepts in Spark and PySpark. Spark is the latest
    big data tool from Apache, which can be found by simply going to [http://spark.apache.org/](http://spark.apache.org/).
    It's a unified analytics engine for large-scale data processing. This means that,
    if you have a lot of data, you can feed that data into Spark to create some analytics
    at a good speed. If we look at the running times between Hadoop and Spark, Spark
    is more than a hundred times faster than Hadoop. It is very easy to use because
    there are very good APIs for use with Spark.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始安装PySpark之前，PySpark是Spark的Python接口，让我们先了解一些Spark和PySpark的核心概念。Spark是Apache的最新大数据工具，可以通过简单地转到[http://spark.apache.org/](http://spark.apache.org/)找到。它是用于大规模数据处理的统一分析引擎。这意味着，如果您有大量数据，您可以将这些数据输入Spark以快速创建一些分析。如果我们比较Hadoop和Spark的运行时间，Spark比Hadoop快一百倍以上。它非常易于使用，因为有非常好的API可用于与Spark一起使用。
- en: 'The four major components of the Spark platform are as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Spark平台的四个主要组件如下：
- en: '**Spark SQL**: A clearing language for Spark'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**：Spark的清理语言'
- en: '**Spark Streaming**: Allows you to feed in real-time streaming data'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming**：允许您提供实时流数据'
- en: '**MLlib (machine learning)**: The machine learning library for Spark'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLlib（机器学习）**：Spark的机器学习库'
- en: '**GraphX (graph)**: The graphing library for Spark'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX（图形）**：Spark的图形库'
- en: The core concept in Spark is an RDD, which is similar to the pandas DataFrame,
    or a Python dictionary or list. It is a way for Spark to store large amounts of
    data on the infrastructure for us. The key difference of an RDD versus something
    that is in your local memory, such as a pandas DataFrame, is that an RDD is distributed
    across many machines, but it appears like one unified dataset. What this means
    is, if you have large amounts of data that you want to operate on in parallel,
    you can put it in an RDD and Spark will handle parallelization and the clustering
    of the data for you.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的核心概念是RDD，它类似于pandas DataFrame，或Python字典或列表。这是Spark用来在基础设施上存储大量数据的一种方式。RDD与存储在本地内存中的内容（如pandas
    DataFrame）的关键区别在于，RDD分布在许多机器上，但看起来像一个统一的数据集。这意味着，如果您有大量数据要并行操作，您可以将其放入RDD中，Spark将为您处理并行化和数据的集群。
- en: 'Spark has three different interfaces, as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有三种不同的接口，如下所示：
- en: Scala
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala
- en: Java
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java
- en: Python
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: 'Python is similar to PySpark integration, which we will cover soon. For now,
    we will import some libraries from the PySpark package to help us work with Spark.
    The best way for us to understand Spark is to look at an example, as shown in
    the following screenshot:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Python类似于PySpark集成，我们将很快介绍。现在，我们将从PySpark包中导入一些库，以帮助我们使用Spark。我们理解Spark的最佳方式是查看示例，如下面的屏幕截图所示：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, we have created a new variable called `lines` by calling `SC.textFile
    ("data.txt")`. `sc` is our Python objects that represent our Spark cluster. A
    Spark cluster is a series of instances or cloud computers that store our Spark
    processes. By calling a `textFile` constructor and feeding in `data.text`, we
    have potentially fed in a large text file and created an RDD just using this one
    line. In other words, what we are trying to do here is to feed a large text file
    into a distributed cluster and Spark, and Spark handles this clustering for us.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们通过调用`SC.textFile("data.txt")`创建了一个名为`lines`的新变量。`sc`是代表我们的Spark集群的Python对象。Spark集群是一系列存储我们的Spark进程的实例或云计算机。通过调用`textFile`构造函数并输入`data.text`，我们可能已经输入了一个大型文本文件，并仅使用这一行创建了一个RDD。换句话说，我们在这里要做的是将一个大型文本文件输入到分布式集群和Spark中，而Spark会为我们处理这个集群。
- en: In line two and line three, we have a MapReduce function. In line two, we have
    mapped the length function using a `lambda` function to each line of `data.text`.
    In line three, we have called a reduction function to add all `lineLengths` together
    to produce the total length of the documents. While Python's `lines` is a variable
    that contains all the lines in `data.text`, under the hood, Spark is actually
    handling the distribution of fragments of `data.text` in two different instances
    on the Spark cluster, and is handling the MapReduce computation over all of these
    instances.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二行和第三行，我们有一个MapReduce函数。在第二行，我们使用`lambda`函数将长度函数映射到`data.text`的每一行。在第三行，我们调用了一个减少函数，将所有`lineLengths`相加，以产生文档的总长度。虽然Python的`lines`是一个包含`data.text`中所有行的变量，但在幕后，Spark实际上正在处理`data.text`的片段在Spark集群上的两个不同实例上的分布，并处理所有这些实例上的MapReduce计算。
- en: Spark SQL
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: 'Spark SQL is one of the four components on top of the Spark platform, as we
    saw earlier in the chapter. It can be used to execute SQL queries or read data
    from any existing Hive insulation, where Hive is a database implementation also
    from Apache. Spark SQL looks very similar to MySQL or Postgres. The following
    code snippet is a good example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是Spark平台上的四个组件之一，正如我们在本章中之前看到的。它可以用于执行SQL查询或从任何现有的Hive绝缘中读取数据，其中Hive也是来自Apache的数据库实现。Spark
    SQL看起来非常类似于MySQL或Postgres。以下代码片段是一个很好的例子：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You'll need to select all the columns from a certain table, such as `people`,
    and using the Spark objects, you'll feed in a very standard-looking SQL statement,
    which is going to show an SQL result much like what you would expect from a normal
    SQL implementation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要从某个表中选择所有列，例如`people`，并使用Spark对象，您将输入一个非常标准的SQL语句，这将显示一个SQL结果，就像您从正常的SQL实现中所期望的那样。
- en: Let's now look at datasets and DataFrames. A dataset is a distributed collection
    of data. It is an interface added in Spark 1.6 that provides benefits on top of
    RDDs. A DataFrame, on the other hand, is very familiar to those who have used
    pandas or R. A DataFrame is simply a dataset organized into named columns, which
    is similar to a relational database or a DataFrame in Python. The main difference
    between a dataset and a DataFrame is that DataFrames have column names. As you
    can imagine, this would be very convenient for machine learning work and feeding
    into things such as scikit-learn.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看数据集和数据框。数据集是分布式数据集合。它是在Spark 1.6中添加的一个接口，提供了RDD的优势。另一方面，数据框对于那些使用过pandas或R的人来说非常熟悉。数据框只是一个组织成命名列的数据集，类似于关系数据库或Python中的数据框。数据集和数据框之间的主要区别在于数据框有列名。可以想象，这对于机器学习工作和输入到诸如scikit-learn之类的东西非常方便。
- en: 'Let''s look at how DataFrames can be used. The following code snippet is a
    quick example of a DataFrame:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用数据框。以下代码片段是数据框的一个快速示例：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the same way, as pandas or R would do, `read.json` allows us to feed in some
    data from a JSON file, and `df.show` shows us the contents of the DataFrame in
    a similar way to pandas.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与pandas或R一样，`read.json`允许我们从JSON文件中输入一些数据，而`df.show`以类似于pandas的方式显示数据框的内容。
- en: MLlib, as we know, is used to make machine learning scalable and easy. MLlib
    allows you to do common machine learning tasks, such as featurization; creating
    pipelines; saving and loading algorithms, models, and pipelines; and also some
    utilities, such as linear algebra, statistics, and data handling. The other thing
    to note is that Spark and RDD are almost inseparable concepts. If your main use
    case for Spark is machine learning, Spark now actually encourages you to use the
    DataFrame-based API for MLlib, which is quite beneficial to us as we are already
    familiar with pandas, which means a smooth transition into Spark.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，MLlib用于使机器学习变得可扩展和简单。MLlib允许您执行常见的机器学习任务，例如特征化；创建管道；保存和加载算法、模型和管道；以及一些实用程序，例如线性代数、统计和数据处理。另一件事需要注意的是，Spark和RDD几乎是不可分割的概念。如果您对Spark的主要用例是机器学习，Spark现在实际上鼓励您使用基于数据框的MLlib
    API，这对我们来说非常有益，因为我们已经熟悉pandas，这意味着平稳过渡到Spark。
- en: In the next section, we will see how we can set up Spark on Windows, and set
    up PySpark as the interface.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何在Windows上设置Spark，并设置PySpark作为接口。
- en: Setting up Spark on Windows and PySpark
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Windows上设置Spark和PySpark
- en: 'Complete the following steps to install PySpark on a Windows machine:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 完成以下步骤，在Windows计算机上安装PySpark：
- en: Download **Gnu on Windows** (**GOW**) from [https://github.com/bmatzelle/gow/releases/download/v0.8.0/Gow-0.8.0.exe](https://github.com/bmatzelle/gow/releases/download/v0.8.0/Gow-0.8.0.exe).
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://github.com/bmatzelle/gow/releases/download/v0.8.0/Gow-0.8.0.exe](https://github.com/bmatzelle/gow/releases/download/v0.8.0/Gow-0.8.0.exe)下载**Gnu
    on Windows**（**GOW**）。
- en: 'GOW allows the use of Linux commands on Windows. We can use the following command
    to see the basic Linux commands allowed by installing GOW:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GOW允许在Windows上使用Linux命令。我们可以使用以下命令来查看通过安装GOW允许的基本Linux命令：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives the following output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![](img/fa145c7d-15ef-487b-8867-4a1b54fdf2bb.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa145c7d-15ef-487b-8867-4a1b54fdf2bb.png)'
- en: Download and install Anaconda. If you need help, you can go through the following
    tutorial: [https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444](https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444).
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并安装Anaconda。如果需要帮助，可以参考以下教程：[https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444](https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444)。
- en: Close the previous command line and open a new command line.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭先前的命令行，打开一个新的命令行。
- en: Go to the Apache Spark website ([https://spark.apache.org/](https://spark.apache.org/)).
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到Apache Spark网站（[https://spark.apache.org/](https://spark.apache.org/)）。
- en: 'To download Spark, choose the following from the drop-down menu:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要下载Spark，请从下拉菜单中选择以下内容：
- en: A recent Spark release
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近的Spark版本
- en: A proper package type
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适当的软件包类型
- en: 'The following screenshot shows the download page of Apache Spark:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了Apache Spark的下载页面：
- en: '![](img/6ed179f0-ad5c-4927-a36f-d43a63b2c7b8.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ed179f0-ad5c-4927-a36f-d43a63b2c7b8.png)'
- en: Then, download Spark. Once it is downloaded, move the file to the folder where
    you want to unzip it.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，下载Spark。下载完成后，将文件移动到您想要解压缩的文件夹中。
- en: 'You can either unzip it manually or use the following commands:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以手动解压缩，也可以使用以下命令：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, download `winutils.exe` into your `spark-2.1.0-bin-hadoop2.7\bin` folder
    using the following command:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令将`winutils.exe`下载到您的`spark-2.1.0-bin-hadoop2.7\bin`文件夹中：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Make sure you have Java installed on your machine. You can use the following
    command to see the Java version:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您的计算机上已安装Java。您可以使用以下命令查看Java版本：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This gives the following output:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![](img/f08c68fa-631b-4c66-984b-f63e77052d5d.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f08c68fa-631b-4c66-984b-f63e77052d5d.png)'
- en: 'Check for the Python version by using the following command:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令检查Python版本：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This gives the following output:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![](img/42925ce3-f91e-40dd-be15-3be663c8e818.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42925ce3-f91e-40dd-be15-3be663c8e818.png)'
- en: 'Let''s edit our environmental variables so that we can open Spark in any directory,
    as follows:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编辑我们的环境变量，这样我们可以在任何目录中打开Spark，如下所示：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Add `C:\opt\spark\spark-2.1.0-bin-hadoop2.7\bin` to your path.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将`C:\opt\spark\spark-2.1.0-bin-hadoop2.7\bin`添加到你的路径中。
- en: 'Close the Terminal, open a new one, and type the following command:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭终端，打开一个新的终端，并输入以下命令：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `PYSPARK_DRIVER_PYTHON` and the `PYSPARK_DRIVER_PYTHON_OPTS` parameters
    are used to launch the PySpark shell in Jupyter Notebook. The `--master` parameter
    is used for setting the master node address.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`PYSPARK_DRIVER_PYTHON`和`PYSPARK_DRIVER_PYTHON_OPTS`参数用于在Jupyter Notebook中启动PySpark
    shell。`--master`参数用于设置主节点地址。'
- en: 'The next thing to do is to run the PySpark command in the `bin` folder:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来要做的是在`bin`文件夹中运行PySpark命令：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives the following output:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/6e33e968-011e-4439-9616-2457bc36493d.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e33e968-011e-4439-9616-2457bc36493d.png)'
- en: Core concepts in Spark and PySpark
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark和PySpark中的核心概念
- en: 'Let''s now look at the following core concepts in Spark and PySpark:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看Spark和PySpark中的以下核心概念：
- en: SparkContext
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkContext
- en: SparkConf
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkConf
- en: Spark shell
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark shell
- en: SparkContext
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkContext
- en: SparkContext is an object or concept within Spark. It is a big data analytical
    engine that allows you to programmatically harness the power of Spark.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: SparkContext是Spark中的一个对象或概念。它是一个大数据分析引擎，允许你以编程方式利用Spark的强大功能。
- en: The power of Spark can be seen when you have a large amount of data that doesn't fit into
    your local machine or your laptop, so you need two or more computers to process
    it. You also need to maintain the speed of processing this data while working
    on it. We not only want the data to be split among a few computers for computation;
    we also want the computation to be parallel. Lastly, you want this computation
    to look like one single computation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有大量数据无法放入本地机器或笔记本电脑时，Spark的强大之处就显现出来了，因此你需要两台或更多计算机来处理它。在处理数据的同时，你还需要保持处理速度。我们不仅希望数据在几台计算机上进行计算，还希望计算是并行的。最后，你希望这个计算看起来像是一个单一的计算。
- en: 'Let''s consider an example where we have a large contact database that has
    50 million names, and we might want to extract the first name from each of these
    contacts. Obviously, it is difficult to fit 50 million names into your local memory,
    especially if each name is embedded within a larger contacts object. This is where
    Spark comes into the picture. Spark allows you to give it a big data file, and
    will help in handling and uploading this data file, while handling all the operations
    carried out on this data for you. This power is managed by Spark''s cluster manager,
    as shown in the following diagram:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子，我们有一个包含5000万个名字的大型联系人数据库，我们可能想从每个联系人中提取第一个名字。显然，如果每个名字都嵌入在一个更大的联系人对象中，将5000万个名字放入本地内存中是困难的。这就是Spark发挥作用的地方。Spark允许你给它一个大数据文件，并将帮助处理和上传这个数据文件，同时为你处理在这个数据上进行的所有操作。这种能力由Spark的集群管理器管理，如下图所示：
- en: '![](img/de3d8722-4e52-4873-aa4c-216c7dbc627c.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de3d8722-4e52-4873-aa4c-216c7dbc627c.png)'
- en: The cluster manager manages multiple workers; there could be 2, 3, or even 100\.
    The main point is that Spark's technology helps in managing this cluster of workers,
    and you need a way to control how the cluster is behaving, and also pass data
    back and forth from the clustered rate.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器管理多个工作节点；可能有2个、3个，甚至100个。关键是Spark的技术有助于管理这个工作节点集群，你需要一种方法来控制集群的行为，并在工作节点之间传递数据。
- en: A **SparkContext** lets you use the power of Spark's cluster manager as with Python
    objects. So with a **SparkContext**, you can pass jobs and resources, schedule
    tasks, and complete tasks the downstream from the **SparkContext** down to the **Spark
    Cluster Manager**, which will then take the results back from the **Spark Cluster
    Manager** once it has completed its computation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**SparkContext** 让你可以像使用Python对象一样使用Spark集群管理器的功能。因此，有了**SparkContext**，你可以传递作业和资源，安排任务，并完成从**SparkContext**到**Spark集群管理器**的下游任务，然后**Spark集群管理器**完成计算后将结果带回来。'
- en: 'Let''s see what this looks like in practice and see how to set up a SparkContext:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这在实践中是什么样子，以及如何设置SparkContext：
- en: First, we need to import `SparkContext`.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入`SparkContext`。
- en: Create a new object in the `sc` variable standing for the SparkContext using
    the `SparkContext` constructor.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新对象，将其赋给变量`sc`，代表使用`SparkContext`构造函数的SparkContext。
- en: 'In the `SparkContext` constructor, pass a `local` context. We are looking at `hands
    on PySpark` in this context, as follows:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`SparkContext`构造函数中，传递一个`local`上下文。在这种情况下，我们正在研究`PySpark`的实际操作，如下所示：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After we''ve established this, all we need to do is then use `sc` as an entry
    point to our Spark operation, as demonstrated in the following code snippet:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们建立了这一点，我们只需要使用`sc`作为我们Spark操作的入口点，就像下面的代码片段中所演示的那样：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let's take an example; if we were to analyze the synthetic datasets of visitor
    counts to our clothing store, we might have a list of `visitors` denoting the
    daily visitors to our store. We can then create a parallelized version of the
    DataFrame, call `sc.parallelize(visitors)`, and feed in the `visitors` datasets. `df_visitors` then
    creates for us a DataFrame of visitors. We can then map a function; for example,
    making the daily numbers and extrapolating them into a yearly number by mapping
    a `lambda` function that multiplies the daily number (`x`) by `365`, which is
    the number of days in a year. Then, we call a `collect()` function to make sure
    that Spark executes on this `lambda` call. Lastly, we print out `df_ visitors_yearly`. Now,
    we have Spark working on this computation on our synthetic data behind the scenes,
    while this is simply a Python operation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子；如果我们要分析我们服装店的虚拟数据集的访客数量，我们可能有一个表示每天访客数量的`visitors`列表。然后，我们可以创建一个DataFrame的并行版本，调用`sc.parallelize(visitors)`，并输入`visitors`数据集。`df_visitors`然后为我们创建了一个访客的DataFrame。然后，我们可以映射一个函数；例如，通过映射一个`lambda`函数，将每日数字（`x`）乘以`365`，即一年中的天数，将其推断为一年的数字。然后，我们调用`collect()`函数，以确保Spark执行这个`lambda`调用。最后，我们打印出`df_visitors_yearly`。现在，我们让Spark在幕后处理我们的虚拟数据的计算，而这只是一个Python操作。
- en: Spark shell
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark shell
- en: We will go back into our Spark folder, which is `spark-2.3.2-bin-hadoop2.7`,
    and start our PySpark binary by typing `.\bin\pyspark`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将返回到我们的Spark文件夹，即`spark-2.3.2-bin-hadoop2.7`，然后通过输入`.\bin\pyspark`来启动我们的PySpark二进制文件。
- en: 'We can see that we''ve started a shell session with Spark in the following
    screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们已经在以下截图中启动了一个带有Spark的shell会话：
- en: '![](img/ba96adf0-562b-47ec-9924-986c14b08156.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba96adf0-562b-47ec-9924-986c14b08156.png)'
- en: 'Spark is now available to us as a `spark` variable. Let''s try a simple thing
    in Spark. The first thing to do is to load a random file. In each Spark installation,
    there is a `README.md` markdown file, so let''s load it into our memory as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Spark对我们来说是一个`spark`变量。让我们在Spark中尝试一件简单的事情。首先要做的是加载一个随机文件。在每个Spark安装中，都有一个`README.md`的markdown文件，所以让我们将其加载到内存中，如下所示：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If we use `spark.read.text` and then put in `README.md`, we get a few warnings,
    but we shouldn't be too concerned about that at the moment, as we will see later
    how we are going to fix these things. The main thing here is that we can use Python
    syntax to access Spark.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`spark.read.text`然后输入`README.md`，我们会得到一些警告，但目前我们不必太担心这些，因为我们将在稍后看到如何解决这些问题。这里的主要问题是我们可以使用Python语法来访问Spark。
- en: 'What we have done here is put `README.md` as text data read by `spark` into
    Spark, and we can use `text_file.count()` can get Spark to count how many characters
    are in our text file as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的是将`README.md`作为`spark`读取的文本数据放入Spark中，然后我们可以使用`text_file.count()`来让Spark计算我们的文本文件中有多少个字符，如下所示：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'From this, we get the following output:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 从中，我们得到以下输出：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can also see what the first line is with the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过以下方式查看第一行是什么：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will get the following output:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can now count a number of lines that contain the word `Spark` by doing the
    following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过以下方式计算包含单词`Spark`的行数：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we have filtered for lines using the `filter()` function, and within the `filter()` function,
    we have specified that `text_file_value.contains` includes the word `"Spark"`,
    and we have put those results into the `lines_with_spark` variable.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`filter()`函数过滤了行，并在`filter()`函数内部指定了`text_file_value.contains`包含单词`"Spark"`，然后将这些结果放入了`lines_with_spark`变量中。
- en: 'We can modify the preceding command and simply add `.count()`, as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以修改上述命令，简单地添加`.count()`，如下所示：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We will now get the following output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将得到以下输出：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can see that `20` lines in the text file contain the word `Spark`. This is
    just a simple example of how we can use the Spark shell.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到文本文件中有`20`行包含单词`Spark`。这只是一个简单的例子，展示了我们如何使用Spark shell。
- en: SparkConf
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkConf
- en: SparkConf allows us to configure a Spark application. It sets various Spark
    parameters as key-value pairs, and so will usually create a `SparkConf` object
    with a `SparkConf()` constructor, which would then load values from the `spark.*`
    underlying Java system.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: SparkConf允许我们配置Spark应用程序。它将各种Spark参数设置为键值对，通常会使用`SparkConf()`构造函数创建一个`SparkConf`对象，然后从`spark.*`底层Java系统中加载值。
- en: There are a few useful functions; for example, we can use the `sets()` function
    to set the configuration property. We can use the `setMaster()` function to set
    the master URL to connect to. We can use the `setAppName()` function to set the
    application name, and `setSparkHome()` in order to set the path where Spark will
    be installed on worker nodes.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些有用的函数；例如，我们可以使用`sets()`函数来设置配置属性。我们可以使用`setMaster()`函数来设置要连接的主URL。我们可以使用`setAppName()`函数来设置应用程序名称，并使用`setSparkHome()`来设置Spark将安装在工作节点上的路径。
- en: You can learn more about SparkConf at [https://spark.apache.org/docs/0.9.0/api/pyspark/pysaprk.conf.SparkConf-class.html](https://spark.apache.org/docs/0.9.0/api/pyspark/pysaprk.conf.SparkConf-class.html).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://spark.apache.org/docs/0.9.0/api/pyspark/pysaprk.conf.SparkConf-class.html](https://spark.apache.org/docs/0.9.0/api/pyspark/pysaprk.conf.SparkConf-class.html)了解更多关于SparkConf的信息。
- en: Summary
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about the core concepts in Spark and PySpark. We
    learned about setting up Spark and using PySpark on Windows. We also went through
    the three main pillars of Spark, which are SparkContext, Spark shell, and SparkConf.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了Spark和PySpark中的核心概念。我们学习了在Windows上设置Spark和使用PySpark。我们还介绍了Spark的三大支柱，即SparkContext、Spark
    shell和SparkConf。
- en: In the next chapter, we're going to look at getting your big data into Spark
    environments using RDDs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何使用RDD将大数据导入Spark环境。
