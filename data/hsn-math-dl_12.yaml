- en: Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: In this chapter, we will take an in-depth look at **Recurrent Neural Networks**
    (**RNNs**). In the previous chapter, we looked at **Convolutional Neural Networks**
    (**CNNs**), which are a powerful class of neural networks for computer vision
    tasks because of their ability to capture spatial relationships. The neural networks
    we will be studying in this chapter, however, are very effective for sequential
    data and are used in applications such as algorithmic trading, image captioning,
    sentiment classification, language translation, video classification, and so on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将深入探讨**循环神经网络**（**RNNs**）。在上一章中，我们学习了**卷积神经网络**（**CNNs**），这种神经网络在计算机视觉任务中非常有效，因为它能够捕捉空间关系。然而，本章我们要研究的神经网络在处理顺序数据方面非常有效，广泛应用于算法交易、图像描述、情感分类、语言翻译、视频分类等领域。
- en: In regular neural networks, all the inputs and outputs are assumed to be independent,
    but in RNNs, each output is dependent on the previous one, which allows them to
    capture dependencies in sequences, such as in language, where the next word depends
    on the previous word and the one before that.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在常规神经网络中，所有输入和输出被假设为独立的，但在RNN中，每个输出都依赖于前一个输出，这使得它们能够捕捉序列中的依赖关系，例如语言中，接下来的单词依赖于前一个单词及其之前的单词。
- en: We will start by taking a look at the vanilla RNN, then the bidirectional RNN,
    deep RNNs, **long short-term memory** (**LSTM**), and **gated recurrent units**
    (**GRUs**), as well as some of the state-of-the-art architectures used in industry
    today.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先了解基础RNN，然后是双向RNN、深度RNN、**长短期记忆网络**（**LSTM**）和**门控循环单元**（**GRU**），以及一些目前工业界使用的最先进架构。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: The need for RNNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN的需求
- en: The types of data used in RNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在RNN中使用的数据类型
- en: Understanding RNNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解RNN
- en: Long short-term memory
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: Gated recurrent units
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 门控循环单元
- en: Deep RNNs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度RNN
- en: Training and optimization
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练与优化
- en: Popular architecture
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的架构
- en: The need for RNNs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN的需求
- en: In the previous chapter, we learned about CNNs and their effectiveness on image-
    and time series-related tasks that have data with a grid-like structure. We also
    saw how CNNs are inspired by how the human visual cortex processes visual input.
    Similarly, the RNNs that we will learn about in this chapter are also biologically
    inspired.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了CNN及其在图像和时间序列任务中的有效性，尤其是处理具有网格状结构的数据时。我们还看到CNN的灵感来源于人类视觉皮层如何处理视觉输入。类似地，本章我们要学习的RNN也是受到生物学启发的。
- en: The need for this form of neural network arises from the fact that **fuzzy neural
    networks** (**FNNs**) are unable to capture time-based dependencies in data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种神经网络的需求源于**模糊神经网络**（**FNNs**）无法捕捉数据中的时间依赖性。
- en: The first model of an RNN was created by John Hopfield in 1982 in an attempt
    to understand how associative memory in our brains works. This is known as a **Hopfield
    network**. It is a fully connected single-layer recurrent network and it stores
    and accesses information similarly to how we think our brains do.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个RNN模型由约翰·霍普菲尔德（John Hopfield）于1982年创建，旨在理解我们大脑中联想记忆的工作原理。这个模型被称为**霍普菲尔德网络**。它是一个完全连接的单层循环网络，存储和访问信息的方式类似于我们大脑的处理方式。
- en: The types of data used in RNNs
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在RNN中使用的数据类型
- en: As mentioned in the introduction to this chapter, RNNs are used frequently for—and
    have brought about tremendous results in—tasks such as natural language processing,
    machine translation, and algorithmic trading. For these tasks, we need sequential
    or time-series data—that is, the data has a fixed order. For example, languages
    and music have a fixed order. When we speak or write sentences, they follow a
    framework, which is what enables us to understand them. If we break the rules
    and mix up words that do not correlate, then the sentence no longer makes sense.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章介绍所述，RNN被广泛应用于自然语言处理、机器翻译和算法交易等任务，并取得了显著成果。对于这些任务，我们需要顺序或时间序列数据——即数据有固定的顺序。例如，语言和音乐都有固定的顺序。当我们说或写句子时，它们遵循一定的结构，这使得我们能够理解它们。如果我们打破规则，混淆那些没有关联的单词，那么句子就不再有意义。
- en: Suppose we have the sentence `The greatest glory in living lies not in never
    falling, but in rising every time we fall` and we pass it through a sentence randomizer.
    The output that we get is `fall. falling, in every in not time but in greatest
    lies The we living glory rising never`, which clearly doesn't make sense. Another
    example is ordering stock prices by date and prices at opening and closing or
    daily prices at fixed time intervals (possibly every hour).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一句话`The greatest glory in living lies not in never falling, but in rising
    every time we fall`，并将其通过句子随机化处理。我们得到的输出是`fall. falling, in every in not time
    but in greatest lies The we living glory rising never`，这显然没有意义。另一个例子是按日期和开盘收盘价格或每天固定时间间隔（可能是每小时）的股价进行排序。
- en: Other examples of sequential data are rainfall measurements over a number of
    successive days, nucleotide base pairs in a DNA strand, or the daily tick values
    for a stock.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其他顺序数据的例子包括连续几天的降水量测量、DNA链中的核苷酸碱基对，或者股票的每日波动值。
- en: We would structure this sort of data in a similar way to how we would for one-dimensional
    convolutions. However, instead of having a kernel that convolves over the data,
    the RNN (which we will become well acquainted with shortly) will take the same
    input, where the node corresponds to the time step of the data (this will become
    clearer momentarily).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以类似于一维卷积的方式组织这类数据。不过，不同的是，RNN（我们很快就会熟悉的）会接收相同的输入，其中每个节点对应数据的时间步长（稍后会更清楚）。
- en: Understanding RNNs
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解RNN
- en: The word **recurrent** in the name of this neural network comes from the fact
    that it has cyclic connections and the same computation is performed on each element
    of the sequence. This allows it to learn (or memorize) parts of the data to make
    predictions about the future. An RNN's advantage is that it can scale to much
    longer sequences than non-sequence based models are able to.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络名称中的**递归**（recurrent）一词来源于其具有循环连接的特点，并且对序列中的每个元素执行相同的计算。这使得它能够学习（或记住）数据的部分信息，从而对未来进行预测。RNN的优势在于，它可以处理比非序列模型更长的序列。
- en: Vanilla RNNs
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准RNN
- en: 'Without further ado, let''s take a look at the most basic version of an RNN,
    referred to as a vanilla RNN. It looks as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不再赘述，让我们先来看一下最基本的RNN版本，称为标准RNN。其结构如下：
- en: '![](img/1562a2fd-dd46-44a3-8579-2b8502c3d12d.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1562a2fd-dd46-44a3-8579-2b8502c3d12d.png)'
- en: 'This looks somewhat familiar, doesn''t it? It should. If we were to remove
    the loop, this would be the same as a traditional neural network, but with one
    hidden layer, which we''ve encountered already. Now, if we unroll the loop and
    view the full network, it looks as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来有点熟悉，是吧？应该是。如果我们去掉循环，这将与传统神经网络相同，但只有一个隐藏层，而我们已经见过这种情况。现在，如果我们展开循环并查看完整的网络，它的结构如下：
- en: '![](img/f1194bb6-7d82-43c8-ac46-154e3fdd42ea.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1194bb6-7d82-43c8-ac46-154e3fdd42ea.png)'
- en: 'Here, we have the following parameters:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下参数：
- en: '*x[t]* is the input at time step *t*'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x[t]*是时间步长*t*的输入'
- en: '*h[t]* is the hidden state at time step *t*'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h[t]*是时间步长*t*的隐藏状态'
- en: '*o[t]* is the output at time step *t*'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*o[t]*是时间步长*t*的输出'
- en: From the preceding diagram, we can observe that the same calculation is performed
    on the input at each time step and this is what differentiates it from the FNNs
    we came across earlier. The parameters (weights and biases) at each layer of an
    FNN are different, but in this architecture, the parameters (*U*, *V*, and *W*)
    remain the same at each time step. Because of this, RNNs are more memory intensive
    and need to be trained for longer in comparison to CNNs. It is also important
    that you note that in RNNs, the time step doesn't necessarily correspond to the
    time in the real world; it merely means the input sequence is of length *t*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图示中可以看出，在每个时间步上对输入执行相同的计算，这也是它与我们之前遇到的FNN（前馈神经网络）的区别。FNN每一层的参数（权重和偏置）是不同的，但在这种架构中，参数（*U*、*V*和*W*）在每个时间步保持不变。因此，RNN相比CNN来说，内存占用更大，并且需要更长的训练时间。值得注意的是，在RNN中，时间步并不一定与现实世界中的时间对应；它只是意味着输入序列的长度是*t*。
- en: But why do these weights remain the same across all the time steps? Why can't
    we have separate parameters that need to be learned at different time steps? The
    reason for this is that separate parameters are unable to generalize to sequence
    lengths that aren't encountered during the training process. Having the same three
    weights shared across the sequence and at different time steps enables the network
    to deal with information that can occur at multiple positions, as it tends to
    in language. For example, `the` can appear at a number of positions in a given
    sentence and the RNN should be able to recognize and extract it regardless of
    the position(s) it is in. This shared statistical strength property is advantageous
    in comparison to an FNN because an FNN would need to learn the language's rules
    at every position, which—as you can imagine—can be very challenging to train.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么这些权重在所有时间步长中保持相同呢？为什么不能在不同的时间步长中使用需要学习的单独参数？原因在于，单独的参数无法对训练过程中未遇到的序列长度进行泛化。在整个序列中共享相同的三个权重，并且在不同的时间步长中使用这些权重，使得网络能够处理可能出现在多个位置的信息，这在语言中是常见的。例如，`the`
    可以出现在给定句子的多个位置，RNN 应该能够识别并提取它，而不论它处于哪个位置。这种共享的统计强度特性相较于 FNN 是有优势的，因为 FNN 需要在每个位置学习语言规则，正如你能想象的那样，这对于训练来说是非常具有挑战性的。
- en: Intuitively, we can think of this as having a sequence [![](img/1c9d1ac3-2fcc-400e-b78c-9ef9f0e41a05.png)] where
    we are trying to find [![](img/21a4f626-7aba-4c9e-a583-e31fdc52b49b.png)], which
    we are already familiar with from [Chapter 3](719fc119-9e7a-4fce-be04-eb1e49bed753.xhtml),
    *Probability and Statistics*. This is not exactly what is happening; we have simplified
    it to help you understand what the RNN is trying to learn to do.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，我们可以将其看作是有一个序列 [![](img/1c9d1ac3-2fcc-400e-b78c-9ef9f0e41a05.png)]，我们试图找到
    [![](img/21a4f626-7aba-4c9e-a583-e31fdc52b49b.png)]，这是我们在[第3章](719fc119-9e7a-4fce-be04-eb1e49bed753.xhtml)
    *概率与统计* 中已经熟悉的内容。这并不完全是发生的情况；我们将其简化，以帮助你理解 RNN 试图学习做什么。
- en: 'Using the knowledge we have now gained, we can create some very complex RNNs
    for a variety of tasks, such as language translation or converting audio into
    text. Depending on the type of task we want to build our model for, we can choose
    from one of the following types of RNNs:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们现在获得的知识，我们可以为各种任务创建非常复杂的 RNN，例如语言翻译或将音频转换为文本。根据我们想要为之构建模型的任务类型，我们可以从以下几种类型的
    RNN 中选择：
- en: One-to-one (one input and one output)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对一（一个输入和一个输出）
- en: One-to-many (one input and many outputs)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对多（一个输入和多个输出）
- en: Many-to-one (many inputs and one output)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对一（多个输入和一个输出）
- en: Many-to-many (multiple inputs and outputs, where the number of inputs and outputs
    are equal)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对多（多个输入和多个输出，输入和输出的数量相等）
- en: Many-to-many (multiple inputs and outputs, where the number of inputs and outputs
    are not equal)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对多（多个输入和多个输出，输入和输出的数量不相等）
- en: Let's take a deeper dive into RNNs and see what is happening at each time step
    from the input to the output through all the hidden layers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨 RNN，看看在每个时间步长从输入到输出经过所有隐藏层时发生了什么。
- en: 'Mathematically, we can calculate the hidden state at each time step using the
    following equation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们可以使用以下方程式计算每个时间步长的隐藏状态：
- en: '![](img/c8e66371-838c-4dcf-a551-67f7bcd43fb0.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8e66371-838c-4dcf-a551-67f7bcd43fb0.png)'
- en: 'Here, *f[1]* is a non-linearity, such as ReLU, tanh, or sigmoid. The output
    is calculated as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， *f[1]* 是一个非线性函数，例如 ReLU、tanh 或 sigmoid。输出计算如下：
- en: '![](img/2b338fbf-0c33-4e8a-ad0e-ffccb997bbc2.png).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/2b338fbf-0c33-4e8a-ad0e-ffccb997bbc2.png)。'
- en: 'We can calculate the probability vector of the output using a nonlinear function, *f[2]*,
    (such as softmax), as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用非线性函数 *f[2]*（例如 softmax）计算输出的概率向量，如下所示：
- en: '![](img/d140200d-2dda-40fb-9925-94d69f95e944.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d140200d-2dda-40fb-9925-94d69f95e944.png)'
- en: By using these equations and applying them repeatedly, we can calculate the
    hidden states and outputs at each time step.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这些方程并重复应用它们，我们可以计算每个时间步长的隐藏状态和输出。
- en: 'So, the RNN then looks as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，RNN 看起来如下：
- en: '![](img/e7d358a3-33ee-45ee-a0b6-518ca0c71a83.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7d358a3-33ee-45ee-a0b6-518ca0c71a83.png)'
- en: By looking at the preceding diagram and the equations, you should be able to
    venture a guess as to the shape of our weight matrices and our bias vectors—[![](img/b13ffd01-6acb-4470-aa97-d5b9a4d5e587.png)] (connects
    the input to the hidden layer), [![](img/b5da4763-2964-4cba-a61e-444c8286e49b.png)] (connects
    the previously hidden layer to the current hidden layer), [![](img/b1cc40f4-682f-43cf-8c01-6f1a12ef5abd.png)] (connects
    the hidden layer and the output), [![](img/58631c14-e746-4cb9-a213-66e672e4cb83.png)] (the
    bias vector for the hidden layer), and [![](img/26884a78-3b83-493e-9299-aed75440ed44.png)] (the
    bias vector for the output layer).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察前面的图示和方程式，你应该能够大致猜测出我们的权重矩阵和偏置向量的形状——[![](img/b13ffd01-6acb-4470-aa97-d5b9a4d5e587.png)]（连接输入层和隐藏层），[![](img/b5da4763-2964-4cba-a61e-444c8286e49b.png)]（连接先前的隐藏层和当前隐藏层），[![](img/b1cc40f4-682f-43cf-8c01-6f1a12ef5abd.png)]（连接隐藏层和输出层），[![](img/58631c14-e746-4cb9-a213-66e672e4cb83.png)]（隐藏层的偏置向量），以及[![](img/26884a78-3b83-493e-9299-aed75440ed44.png)]（输出层的偏置向量）。
- en: From the preceding equations, we can clearly tell that the hidden state at time
    step *t* depends on the current input and the previous hidden state. However,
    the initial hidden state, *h[0]*, must be initialized in a similar way to how
    we initialized the weights and kernels in FNNs and CNNs. The output at each time
    step, on the other hand, is dependent on the current hidden state. Additionally,
    *a* and *b* are biases and so they are trainable parameters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的方程式中，我们可以清楚地看出，时间步* t *的隐藏状态依赖于当前的输入和先前的隐藏状态。然而，初始隐藏状态*h[0]*必须以类似初始化FNN和CNN中的权重和卷积核的方式进行初始化。另一方面，每个时间步的输出则依赖于当前的隐藏状态。此外，*a*和*b*是偏置，它们是可训练的参数。
- en: In RNNs, *h[t]* contains information about everything that has occurred at the
    previous time steps (but in practice, we limit this to a few time steps, instead
    of all previous ones, because of the vanishing/exploding gradient problem) and
    *o[t]* is calculated based on the most recent information. This allows the RNN
    to exploit relationships between the sequence and use it to predict the most likely
    output, which is not entirely dissimilar to how CNNs capture spatial relationships
    in sequential data using one-dimensional convolutions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNN中，*h[t]*包含了之前所有时间步发生的事情的信息（但实际上，我们通常只限制在少数几个时间步，而不是所有之前的时间步，这是因为梯度消失/爆炸的问题），而*o[t]*则是基于最新的信息计算出来的。这使得RNN能够利用序列之间的关系，并用它来预测最可能的输出，这与CNN如何通过一维卷积捕捉序列数据中的空间关系并不完全不同。
- en: 'However, this isn''t the only way to construct RNNs. Instead of passing outputs
    from hidden layer to hidden layer ([![](img/91060362-403a-472c-9c7d-40c23d4df645.png)])
    as in the preceding diagram, we could pass the output from the previous output
    into the next hidden state ([![](img/31706299-f030-4ca8-b85c-1dd61fb22ff2.png)]),
    changing how we calculate the hidden state. It now becomes the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是构建RNN的唯一方式。我们可以改变计算隐藏状态的方式，而不是像前面的图示那样将输出从一个隐藏层传递到另一个隐藏层（[![](img/91060362-403a-472c-9c7d-40c23d4df645.png)]），我们可以将上一个输出的结果传递到下一个隐藏状态（[![](img/31706299-f030-4ca8-b85c-1dd61fb22ff2.png)]），这样就改变了计算隐藏状态的方式。新的计算公式变为如下：
- en: '![](img/c54622b9-245b-4978-9d37-8caedfd5c212.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c54622b9-245b-4978-9d37-8caedfd5c212.png)'
- en: 'In the following diagram, we can see the various operations taking place in
    a hidden state cell at time step *t*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到在时间步* t *的隐藏状态单元中发生的各种操作：
- en: '![](img/f7c2f057-de92-4542-95e9-b04988aa507a.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7c2f057-de92-4542-95e9-b04988aa507a.png)'
- en: 'When working with FNNs, we calculate the loss at the end of each forward pass
    through the network and then backpropagate the errors to update the weights. However,
    in RNNs, we calculate the loss at each time step, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用FNN时，我们会在每次通过网络的前向传递后计算损失，并进行误差反向传播以更新权重。然而，在RNN中，我们会在每个时间步计算损失，如下所示：
- en: '![](img/ef54fcf0-8729-4b96-b4be-9345d20d4bf1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef54fcf0-8729-4b96-b4be-9345d20d4bf1.png)'
- en: Here, *L* is the cross-entropy loss function (which we are already familiar
    with), *y[t]* is the target, [![](img/4076ec47-3e82-4d91-89e4-39470c086152.png)] is
    a probability vector, and *n* is the number of outputs/targets.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*L*是交叉熵损失函数（我们已经熟悉它了），*y[t]*是目标，[![](img/4076ec47-3e82-4d91-89e4-39470c086152.png)]是一个概率向量，*n*是输出/目标的数量。
- en: While effective, these vanilla RNNs aren't perfect. They do have a few problems
    that we usually encounter during training, particularly the vanishing gradient
    problem. This occurs when the weights become very small, preventing the neuron
    from firing, which prevents hidden neurons at later time steps from firing because
    each one depends on the last, and the one before that, and so on.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有效，这些普通的RNN并不完美。它们确实存在一些我们在训练过程中通常会遇到的问题，特别是消失梯度问题。消失梯度问题发生在权重变得非常小，导致神经元无法激活，从而阻止了后续时间步的隐藏神经元激活，因为每个神经元依赖于上一个神经元，依此类推。
- en: To get a better understanding of this, let's consider the following example.
    Suppose we have a very simple vanilla RNN without any nonlinear activations and
    or inputs. We can express this network as [![](img/98c4a2d5-7a0a-4d2f-b683-36bf669d44e9.png)].
    As you can see, we are applying the same weight over and over again at each unit
    from time step to time step. However, let's focus our attention on the weights.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，我们来考虑以下例子。假设我们有一个非常简单的普通RNN，没有任何非线性激活函数或输入。我们可以将这个网络表示为[![](img/98c4a2d5-7a0a-4d2f-b683-36bf669d44e9.png)]。正如你所看到的，我们在每个单元上每个时间步都会重复应用相同的权重。然而，让我们把注意力集中在权重上。
- en: 'To understand the vanishing and exploding gradient problem, let''s suppose
    that our weight matrix has a shape of 2 × 2 and is diagonalizable. You should
    remember from [Chapter 2](6a34798f-db83-4a32-9222-06ba717fc809.xhtml), *Linear
    Algebra*, that if our matrix is diagonalizable, then it can be decomposed into
    the form [![](img/893fef47-6833-4bd9-9e88-0bbbaf2989ac.png)], where *Q* is a matrix
    containing the eigenvectors and Λ is a square matrix that contains eigenvalues
    along the diagonal. As previously, if we have eight hidden layers, then our weight
    would be [![](img/dc392677-29f6-45e0-bc98-08e55db5158d.png)]. We can see this
    as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解消失和爆炸梯度问题，假设我们的权重矩阵的形状为2 × 2，并且是可对角化的。你应该还记得在[第二章](6a34798f-db83-4a32-9222-06ba717fc809.xhtml)《线性代数》中提到，如果矩阵是可对角化的，那么它可以分解成形如[![](img/893fef47-6833-4bd9-9e88-0bbbaf2989ac.png)]的形式，其中*Q*是包含特征向量的矩阵，Λ是包含特征值的方阵，特征值位于对角线上。如前所述，如果我们有八个隐藏层，那么我们的权重矩阵会是[![](img/dc392677-29f6-45e0-bc98-08e55db5158d.png)]。我们可以如下理解：
- en: '![](img/5ab20487-fcfb-410a-83f5-6c7ea439ea0b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ab20487-fcfb-410a-83f5-6c7ea439ea0b.png)'
- en: In the preceding equation, we get a good glimpse of both the vanishing and the
    exploding gradient problems. We assumed we have eight hidden units and by multiplying
    them over and over, we can see that the values become either very small or very
    large, which makes training RNNs rather challenging because of their instability.
    The small weights make it difficult for our RNN to learn long-term dependencies
    and is why innovations in the cells, such as **Long short-term models** (**LSTMs**)
    and **gated recurrent units** (**GRUs**) were created (we'll learn about these
    two RNN cell variants shortly).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的方程中，我们可以清楚地看到消失梯度和爆炸梯度问题。我们假设有八个隐藏单元，通过不断相乘，我们可以看到值变得非常小或非常大，这使得训练RNN变得相当困难，因为它们不稳定。小的权重使得RNN很难学习长期依赖关系，这也是**长短期记忆网络**（**LSTM**）和**门控循环单元**（**GRU**）等单元创新的原因（稍后我们将学习这两种RNN单元变体）。
- en: Now, if we have an RNN with 20 time steps or more and we want our network to
    remember the first, second, or third input, it is more than likely it won't be
    able to remember them, but it will remember the most recent inputs. For example,
    we could have the sentence `I remember when I visited Naples a few years ago...I
    had the best pizza of my life`. In this case, we need to understand the context
    of Naples from further back to understand where this magnificent pizza is from,
    but looking this far back is challenging for RNNs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们的RNN有20个时间步或更多，并且我们希望网络记住第一个、第二个或第三个输入，那么它很可能无法记住这些输入，但它会记住最近的输入。例如，我们可能有一句话`我记得几年前去那不勒斯旅行的时候...我吃到了人生中最好吃的披萨`。在这种情况下，我们需要理解那不勒斯的背景，以便知道这块美味的披萨是来自哪里，但RNN很难回溯到这么远的历史。
- en: 'Similarly, if our weight is greater than 1, it can get much larger, which results
    in exploding gradients. We can, however, deal with this by using gradient clipping,
    where we rescale the weights so that the norm is at most η. We use the following formula
    to do so:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果我们的权重大于1，它可能会变得非常大，导致梯度爆炸。然而，我们可以通过使用梯度裁剪来解决这个问题，梯度裁剪是通过重新缩放权重，使其范数最多为η。我们使用以下公式来实现：
- en: '![](img/fb36263c-c264-4cc7-938a-be318291fbf1.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb36263c-c264-4cc7-938a-be318291fbf1.png)'
- en: Bidirectional RNNs
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双向RNN（Bidirectional RNNs）
- en: Now that we know how RNNs work at their most basic level, let's take a look
    at a variant of them—the bidirectional RNN. The preceding RNN is feedforward;
    that is, the data passes through the network from left (![](img/b2e68ba7-20c7-4893-ab76-ce2ff624bc5d.png))
    to right (![](img/53ae4383-5d6f-4ba4-a6fe-0d587d0cfe4a.png)), which creates a
    dependency on the past. However, for some of the problems that we may want to
    work with, it could help to look into the future, as well.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了基本的 RNN 工作原理，让我们来看一下它的一个变种——双向 RNN。前面的 RNN 是前馈型的；也就是说，数据从左边 (![](img/b2e68ba7-20c7-4893-ab76-ce2ff624bc5d.png))
    到右边 (![](img/53ae4383-5d6f-4ba4-a6fe-0d587d0cfe4a.png)) 传递，这会产生对过去的依赖。然而，对于一些我们可能希望处理的问题，展望未来也是有帮助的。
- en: This allows us to feed the network training data both forward and backward into
    two separate recurrent layers, respectively. It is important to note that both
    of these layers share the same output layer. This approach allows us to contextualize
    the input data with respect to the past and the future, which produces much better
    results than the previous, unidirectional RNN for tasks relating to speech and
    translation. Naturally, however, bidirectional RNNs are not the answer to every
    time-series task, such as predicting stock prices, because we don't know what
    will happen in the future.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够分别将训练数据正向和反向地输入到两个独立的递归层中。需要注意的是，这两个层共享相同的输出层。这种方法允许我们将输入数据与过去和未来的上下文相结合，从而在涉及语音和翻译的任务中产生比之前的单向
    RNN 更好的结果。然而，双向 RNN 并不是每个时间序列任务的解决方案，例如股票价格预测，因为我们无法知道未来会发生什么。
- en: 'In the following diagram, we can see what a bidirectional RNN looks like:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图示中，我们可以看到一个双向 RNN 的结构：
- en: '![](img/a61fb748-6287-47d3-82c1-9d490b0f47b3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a61fb748-6287-47d3-82c1-9d490b0f47b3.png)'
- en: As you can see, the network now contains two parallel layers running in opposite
    directions and there are now six different sets of weights applied at every time
    step; namely, input-to-hidden (*U* and *C*), hidden-to-hidden (*A* and *W*), and
    hidden-to-output (*V* and *B*). It is important that we note that there is no
    information shared between the forward layer and the backward layer.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，网络现在包含两个平行的层，分别朝相反的方向运行，并且在每个时间步上应用六组不同的权重；即，输入到隐藏层的权重 (*U* 和 *C*)，隐藏到隐藏层的权重
    (*A* 和 *W*)，以及隐藏到输出层的权重 (*V* 和 *B*)。需要注意的是，前向层和后向层之间没有共享信息。
- en: 'Now, the operations taking place at each of the hidden states at time ![](img/f7176f55-6205-4278-88ed-b8b8e5821873.png) are
    as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个隐藏状态在时间点 ![](img/f7176f55-6205-4278-88ed-b8b8e5821873.png) 处的操作如下：
- en: '[![](img/c3d40ed6-caba-4a59-ba60-aba2a621cf87.png)]'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/c3d40ed6-caba-4a59-ba60-aba2a621cf87.png)]'
- en: '![](img/a471826e-2d60-4223-9050-c103b13ff3e2.png)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/a471826e-2d60-4223-9050-c103b13ff3e2.png)'
- en: 'Here, *f[1]* is a non-linearity and *a* and *b* are biases. The output unit
    can be calculated as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*f[1]* 是一个非线性函数，*a* 和 *b* 是偏置项。输出单元可以按以下公式计算：
- en: '![](img/b277585d-dd9c-4e51-b936-17f2bb0f2a91.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b277585d-dd9c-4e51-b936-17f2bb0f2a91.png)'
- en: 'Here, *d* is a bias. Then, we can find the probability vector using the following
    equation:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*d* 是偏置项。然后，我们可以通过以下公式找到概率向量：
- en: '![](img/751358e0-c8e3-4885-a90d-027a4b775462.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/751358e0-c8e3-4885-a90d-027a4b775462.png)'
- en: The preceding equations show us that the hidden states in the forward layer
    receive information from the previous hidden states and the hidden states in the
    backward layer receive information from the future states.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的公式告诉我们，前向层的隐藏状态从先前的隐藏状态获取信息，而后向层的隐藏状态则从未来的状态中获取信息。
- en: Long short-term memory
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆
- en: As we saw earlier, the standard RNN does have some limitations; in particular,
    they suffer from the vanishing gradient problem. The LSTM architecture was proposed
    by Jürgen Schmidhuber ([ftp://ftp.idsia.ch/pub/juergen/lstm.pdf](ftp://ftp.idsia.ch/pub/juergen/lstm.pdf))
    as a solution to the long-term dependency problem that RNNs face.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所见，标准的 RNN 确实存在一些局限性；尤其是它们受到梯度消失问题的影响。LSTM 架构由 Jürgen Schmidhuber 提出（[ftp://ftp.idsia.ch/pub/juergen/lstm.pdf](ftp://ftp.idsia.ch/pub/juergen/lstm.pdf)），作为解决
    RNN 所面临的长期依赖问题的一种方案。
- en: LSTM cells differ from vanilla RNN cells in a few ways. Firstly, they contain
    what we call a memory block, which is basically a set of recurrently connected
    subnets. Secondly, each of the memory blocks contains not only self-connected
    memory cells but also three multiplicative units that represent the input, output,
    and forget gates.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM单元与传统的RNN单元在几个方面有所不同。首先，它们包含我们所谓的记忆块，实际上是一个递归连接的子网络集合。其次，每个记忆块不仅包含自连接的记忆单元，还包含三个乘法单元，分别代表输入门、输出门和遗忘门。
- en: 'Let''s take a look at what a single LSTM cell looks like, then we will dive
    into the nitty-gritty of it to gain a better understanding. In the following diagram,
    you can see what an LSTM block looks like and the operations that take place inside
    it:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个LSTM单元的样子，然后我们将深入研究它，帮助更好地理解。在以下图示中，你可以看到LSTM块的外观以及其中发生的操作：
- en: '![](img/076993fb-4bc3-4461-9abb-600ab3523f8e.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/076993fb-4bc3-4461-9abb-600ab3523f8e.png)'
- en: 'As you can see in the preceding LSTM cell, a bunch of operations take place
    at each time step and it has the following components:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在前面的LSTM单元中，每个时间步都会发生一系列操作，并且它包含以下组件：
- en: '*f*: The forget gate (an NN with sigmoid)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f*：遗忘门（一个带有sigmoid的神经网络）'
- en: '[![](img/ad68a289-373f-44b4-a0fd-3026563cdbf8.png)]: The candidate layer (an
    NN with tanh)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/ad68a289-373f-44b4-a0fd-3026563cdbf8.png)]: 候选层（一个带有tanh的神经网络）'
- en: '*I*: The input gate (an NN with sigmoid)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*I*：输入门（一个带有sigmoid的神经网络）'
- en: '*O*: The output gate (an NN with sigmoid)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*O*：输出门（一个带有sigmoid的神经网络）'
- en: '*H*: The hidden state (a vector)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*H*：隐藏状态（一个向量）'
- en: '*C*: The memory state (a vector)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*C*：记忆状态（一个向量）'
- en: '*W* and *U*: The weights for the forget gate, candidate, input gate, and output
    gate'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W* 和 *U*：遗忘门、候选层、输入门和输出门的权重'
- en: At each time step, the memory cell takes the current input (*X[t]*), the previous
    hidden state (*H[t-1]*), and the previous memory state (*C[t-1]*) as input and
    it outputs the current hidden state (*H[t]*) and the current memory state (*C[t]*).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，记忆单元接收当前输入（*X[t]*）、前一个隐藏状态（*H[t-1]*）和前一个记忆状态（*C[t-1]*）作为输入，并输出当前的隐藏状态（*H[t]*）和当前的记忆状态（*C[t]*）。
- en: As you can see in the preceding diagram, there are a lot more operations happening
    here than were taking place in the hidden cell of the vanilla RNN. The significance
    of this is that it preserves the gradients throughout the network and allows longer-term
    dependencies, as well as providing a solution to the vanishing gradient problem.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在前面的图示中所见，这里发生的操作比传统RNN的隐藏单元中要多得多。这意味着它能够在整个网络中保持梯度，允许更长时间的依赖关系，并提供了解决梯度消失问题的方法。
- en: 'But how exactly do LSTMs do this? Let''s find out. The memory state stores
    information and continues to do so until the old information is overridden by
    the new information. Each cell can make a decision as to whether or not it wants
    to output this information or store it. Before we go deeper into the explanations,
    let''s first take a look at the mathematical operations that take place in each
    LSTM cell. They are as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，LSTM到底是如何做到这一点的呢？让我们来看看。记忆状态存储信息，并一直这样做，直到新信息覆盖旧信息。每个单元都可以决定是否要输出这些信息或存储它。在深入解释之前，我们首先来看一下每个LSTM单元中发生的数学操作。它们如下：
- en: '[![](img/d6efa777-e4c4-40e6-802c-e655c8816c48.png)]'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/d6efa777-e4c4-40e6-802c-e655c8816c48.png)]'
- en: '[![](img/6d35903b-cab4-40d1-9076-23f9ec363b89.png)]'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/6d35903b-cab4-40d1-9076-23f9ec363b89.png)]'
- en: '[![](img/39d6e2ef-fba3-4d73-829f-0666e30a640e.png)]'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/39d6e2ef-fba3-4d73-829f-0666e30a640e.png)]'
- en: '[![](img/f3b38e96-c284-45fa-95bd-b303a7cec238.png)]'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/f3b38e96-c284-45fa-95bd-b303a7cec238.png)]'
- en: '[![](img/c6ca1e21-a5f5-4585-9bfc-103cd8a47a6d.png)]'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/c6ca1e21-a5f5-4585-9bfc-103cd8a47a6d.png)]'
- en: '[![](img/2b8795af-0a2c-4fa8-aa4e-cae69a6b754b.png)]'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/2b8795af-0a2c-4fa8-aa4e-cae69a6b754b.png)]'
- en: 'Now that we know the different operations that take place in each cell, let''s
    really understand what each of the preceding equations represents. They are as
    follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了每个单元中发生的不同操作，让我们真正理解一下前面方程的含义。它们如下：
- en: The candidate layer ([![](img/43ea863f-463b-4a35-bd3f-97eecbaa0784.png)]) takes
    as input a word (*X[t]*) and the output from the previous hidden state *H[t-1]* and
    creates a new memory, which includes the new word.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选层([![](img/43ea863f-463b-4a35-bd3f-97eecbaa0784.png)])的输入是一个单词（*X[t]*）和来自前一个隐藏状态的输出
    *H[t-1]*，并生成一个新的记忆，其中包括新的单词。
- en: The input gate (*I*) performs a very important function. It determines whether
    or not the new input word is worth preserving based on the output of the previous
    hidden state.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门 (*I*) 执行一个非常重要的功能。它根据前一个隐藏状态的输出，决定当前的新输入词是否值得保留。
- en: The forget gate (*f*), even though it looks very similar to the input gate,
    performs a different function. It determines the relevance (or usefulness) of
    the previous memory cell when computing the current one.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记门 (*f*)，尽管它看起来与输入门非常相似，但执行的是不同的功能。它决定在计算当前记忆时，前一个记忆单元的相关性（或有用性）。
- en: The memory state (sometimes referred to as the final memory) is produced after
    taking in the forget gate and the input gate as input and then gates the new memory
    and sums the output to product *C[t]*.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记忆状态（有时称为最终记忆）是在接收忘记门和输入门作为输入后生成的，然后它将新记忆进行门控并将输出加和，最终生成 *C[t]*。
- en: The output gate differentiates the memory from the hidden state and determines
    how much of the information present in the memory should be present in the hidden
    state. This produces *O[t]*, which we then use to gate tanh (*C[t]*).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出门将记忆与隐藏状态区分开，并决定在隐藏状态中应该保留多少记忆中的信息。这个过程产生 *O[t]*，我们接着用它来门控 tanh (*C[t]*)。
- en: Gated recurrent units
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 门控循环单元
- en: Similar to the LSTM, GRUs are also an improvement on the hidden cells in vanilla
    RNNs. GRUs were also created to address the vanishing gradient problem by storing
    memory from the past to help make better future decisions. The motivation for
    the GRU stemmed from questioning whether all the components that are present in
    the LSTM are necessary for controlling the forgetfulness and time scale of units.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LSTM 类似，GRU 也是对传统 RNN 中隐藏单元的改进。GRU 的设计也是为了应对梯度消失问题，通过存储过去的记忆来帮助做出更好的未来决策。GRU
    的动机源于对 LSTM 中所有组成部分是否都必要的质疑，尤其是控制遗忘性和单位时间尺度的部分。
- en: The main difference here is that this architecture uses one gating unit to decide
    what to forget and when to update the state, which gives it a more persistent
    memory.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要区别在于，这种架构使用一个门控单元来决定忘记什么以及何时更新状态，这使得它拥有更持久的记忆。
- en: 'In the following diagram, you can see what the GRU architecture looks like:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，你可以看到 GRU 架构的样子：
- en: '![](img/284492cc-e1ec-4ab3-b760-836388db7399.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/284492cc-e1ec-4ab3-b760-836388db7399.png)'
- en: 'As you can see in the preceding diagram, it takes in the current input (*X[t]*) and
    the previous hidden state (*H[t-1]*), and there are a lot fewer operations that
    take place here in comparison to the preceding LSTM. It has the following components:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的图示中看到的，它接收当前输入 (*X[t]*) 和前一个隐藏状态 (*H[t-1]*)，并且与前面的 LSTM 相比，这里进行的操作要少得多。它包含以下几个组成部分：
- en: '*Z[t]*: The update gate'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Z[t]*：更新门'
- en: '*R[t]*: The reset gate'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R[t]*：重置门'
- en: '[![](img/5872a37f-3bd5-42f2-83eb-e8035dd1559d.png)]: The new memory'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/5872a37f-3bd5-42f2-83eb-e8035dd1559d.png)]：新的记忆'
- en: '*H[t]*: The hidden state'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*H[t]*：隐藏状态'
- en: 'To produce the current hidden state, the GRU uses the following operations:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成当前隐藏状态，GRU 使用以下操作：
- en: '[![](img/7d8ea1bc-1315-44e8-abc0-3f7274128e34.png)]'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/7d8ea1bc-1315-44e8-abc0-3f7274128e34.png)]'
- en: '[![](img/0cb747b3-a746-41b3-bf8e-76342e121253.png)]'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/0cb747b3-a746-41b3-bf8e-76342e121253.png)]'
- en: '[![](img/607d3719-69fd-4c0c-bfcb-06ca2632a1c3.png)]'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/607d3719-69fd-4c0c-bfcb-06ca2632a1c3.png)]'
- en: '[![](img/cafab2cc-ceee-4fea-b2ae-d3a0800e2800.png)]'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/cafab2cc-ceee-4fea-b2ae-d3a0800e2800.png)]'
- en: 'Now, let''s break down the preceding equations to get a better idea of what
    the GRU is doing to its two inputs. They are as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们解析之前的方程式，更好地了解 GRU 是如何处理其两个输入的。它们如下：
- en: The GRU takes in the current input (*X[t]*) and the previous hidden state (*H[t-1]*)
    and contextualizes the word based on the information it has about the previous
    words to produce [![](img/c5fa89f4-32d3-48aa-b737-e505b9a81e00.png)]—the new memory.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRU 接收当前输入 (*X[t]*) 和前一个隐藏状态 (*H[t-1]*)，并根据它所拥有的关于前一个单词的信息来上下文化当前词语，从而生成 [![](img/c5fa89f4-32d3-48aa-b737-e505b9a81e00.png)]——新的记忆。
- en: The reset gate (*R[t]*) decides the importance of the previous hidden state
    in computing the current hidden state; that is, whether or not it is relevant
    to obtaining the new memory, which helps capture short-term dependencies.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重置门 (*R[t]*) 决定在计算当前隐藏状态时，前一个隐藏状态的重要性；也就是说，它决定前一个隐藏状态是否与获得新记忆相关，这有助于捕捉短期依赖关系。
- en: The update gate (*Z[t]*) determines how much of the previous hidden state should
    be passed on to the next state to capture long-term dependencies. In a nutshell,
    if *Z[t]≈1*, then most of the previous hidden state is incorporated into the current
    hidden state; but if *Z[t]≈0*, then most of the new memory is passed forward.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新门 (*Z[t]*) 决定了应将多少先前的隐藏状态传递给下一个状态，以捕捉长期依赖关系。简而言之，如果 *Z[t]≈1*，则大部分先前的隐藏状态会被纳入当前的隐藏状态；但如果
    *Z[t]≈0*，则大部分新记忆会被传递向前。
- en: Finally, the present hidden state (*H[t]*) is computed using the new memory
    and the previous hidden state, contingent on the results of the update gate.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终，当前的隐藏状态 (*H[t]*) 是通过新记忆和之前的隐藏状态计算得到的，取决于更新门的结果。
- en: Deep RNNs
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 RNN
- en: In the previous chapters, we saw how adding depth to our neural networks helps
    achieve much greater results; the same is true with RNNs, where adding more layers
    allows us to learn even more complex information.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们看到将深度添加到神经网络中有助于取得更好的结果；对于 RNN 来说也是如此，增加更多的层可以帮助我们学习更复杂的信息。
- en: Now that we have seen what RNNs are and have an understanding of how they work,
    let's go deeper and see what deep RNNs look like and what kind of benefits we
    gain from adding additional layers. Going deeper into RNNs is not as straightforward
    as it was when we were dealing with FNNs and CNNs; we have to make a few different
    kinds of considerations here, particularly about how and where we should add the
    nonlinearity between layers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 RNN 的基本概念，并且理解了它们是如何工作的，让我们深入探讨一下深度 RNN 长什么样子，并且从中获得的好处。深入研究 RNN 并不像处理
    FNN 和 CNN 时那么简单；我们在这里需要考虑一些不同的因素，特别是关于我们应该如何以及在哪里在层之间添加非线性。
- en: If we want to go deeper, we can stack more hidden recurrent layers on top of
    each other, which allows our architecture to capture and learn complex information
    at multiple timescales, and before the information is passed from layer to layer,
    we can add either non-linearity or gates.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想更深一些，我们可以将更多的隐藏递归层堆叠在一起，这样我们的架构就可以在多个时间尺度上捕捉并学习复杂信息，并且在信息从一层传递到另一层之前，我们可以加入非线性或门控机制。
- en: 'Let''s start with a two-hidden-layer bidirectional RNN, which is shown in the
    following diagram:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个两层隐藏层的双向 RNN 开始，如下图所示：
- en: '![](img/4d5d6afe-3202-4192-a0fb-7f894280db2b.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d5d6afe-3202-4192-a0fb-7f894280db2b.png)'
- en: As you can see, it looks like three **multilayer perceptrons** (**MLPs**) stacked
    together side by side, and the hidden layers are connected, as before, forming
    a lattice. There are also no connections between the forward and backward hidden
    units in the same layer. Each hidden node feeds into the node directly above it
    at the same time step and each hidden node takes two parameters from the correlated
    hidden node in the previous layer as input—one from the forward layer and the
    other from the backward layer.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它看起来像是三个**多层感知机**（**MLP**）并排堆叠在一起，隐藏层依然是互相连接的，形成一个网格。同时，同一层的前向和反向隐藏单元之间没有连接。每个隐藏节点在相同时间步长下同时输入直接位于其上方的节点，并且每个隐藏节点从上一层相关的隐藏节点获取两个参数作为输入——一个来自前向层，另一个来自反向层。
- en: 'We can generalize and write the equations for the deep bidirectional RNN as
    follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以概括并写出深度双向 RNN 的方程，如下所示：
- en: 'In the forward layer, we have the following:'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前向层中，我们有以下内容：
- en: '![](img/3e071034-b2cb-4682-a766-726a012088d5.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e071034-b2cb-4682-a766-726a012088d5.png)'
- en: 'In the backward layer, we have the following:'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在反向层中，我们有以下内容：
- en: '![](img/b8b25b2c-ae7a-4e9d-9754-443447208983.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8b25b2c-ae7a-4e9d-9754-443447208983.png)'
- en: 'In the output layer, we have the following:'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在输出层中，我们有以下内容：
- en: '![](img/9147e982-3c41-4ba3-a1de-11326a0dfc23.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9147e982-3c41-4ba3-a1de-11326a0dfc23.png)'
- en: Using this as a guideline, we can do the same for LSTMs or GRUs and add them
    in.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以此为指导，我们可以对 LSTM 或 GRU 进行相同的操作并将其添加进去。
- en: Training and optimization
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练与优化
- en: As in the neural networks we have already encountered, RNNs also update their
    parameters using backpropagation by finding the gradient of the error (loss) with
    respect to the weights. Here, however, it is referred to as **Backpropagation
    Through Time** (**BPTT**) because each node in the RNN has a time step. I know
    the name sounds cool, but it has nothing to do with time travel—it's still just
    good old backpropagation with gradient descent for the parameter updates.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在之前的神经网络中遇到的，RNN也通过反向传播来更新其参数，通过计算误差（损失）相对于权重的梯度。然而，在这里，它被称为**时间反向传播**（**BPTT**），因为RNN中的每个节点都有一个时间步长。我知道这个名字听起来很酷，但它与时间旅行无关——它仍然是经典的反向传播，使用梯度下降进行参数更新。
- en: Here, using BPTT, we want to find out how much the hidden units and output affect
    the total error, as well as how much changing the weights (*U, V, W*) affects
    the output. *W*, as we know, is constant throughout the network, so we need to
    traverse all the way back to the initial time step to make an update to it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，使用BPTT，我们想要找出隐藏单元和输出对总误差的影响，以及更改权重（*U, V, W*）对输出的影响。*W*，如我们所知，在整个网络中是常量，因此我们需要一直回溯到初始时间步长进行更新。
- en: When backpropagating in RNNs, we again apply the chain rule. What makes training
    RNNs tricky is that the loss function is dependent not only on the activation
    of the output layer but also on the activation of the current hidden layer and
    its effect on the hidden layer at the next time step. In the following equation,
    we can see how backpropagation works in RNNs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNN中进行反向传播时，我们再次应用链式法则。训练RNN之所以棘手，是因为损失函数不仅依赖于输出层的激活，还依赖于当前隐藏层的激活及其对下一个时间步长中隐藏层的影响。在下列方程中，我们可以看到RNN中反向传播的工作原理。
- en: 'As you can see, we first find the cross-entropy loss (defined in the *Vanilla
    RNNs* section); our total error is as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们首先找到交叉熵损失（在*Vanilla RNNs*部分定义）；我们的总误差如下：
- en: '![](img/088da816-0209-435f-9a9d-fe81b4e4d512.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/088da816-0209-435f-9a9d-fe81b4e4d512.png)'
- en: 'We can expand the preceding equation using the chain rule with respect to the
    losses and hidden layers, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用链式法则展开前述方程，相对于损失和隐藏层，如下所示：
- en: '![](img/a8fd0426-7be3-4e72-8ade-7febd2e04c2e.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8fd0426-7be3-4e72-8ade-7febd2e04c2e.png)'
- en: 'Let''s focus in on the third term on the right-hand side and expand on it:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们聚焦于右侧第三项并对其展开：
- en: '![](img/ba47a8d3-6f32-48fa-857e-fc4b39805f67.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba47a8d3-6f32-48fa-857e-fc4b39805f67.png)'
- en: You should note that each partial here is a Jacobian matrix.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意到这里的每个偏导数都是一个雅可比矩阵。
- en: 'We can now combine the preceding equations together to get a holistic view
    of how to calculate the error, which looks as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将前述方程结合起来，全面了解如何计算误差，结果如下所示：
- en: '![](img/9cf21858-6ec5-4dca-a0db-ccc21bc3664e.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9cf21858-6ec5-4dca-a0db-ccc21bc3664e.png)'
- en: 'We know from earlier on in this chapter that *h[t]* is calculated using the
    following equation:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道在本章前面的部分，*h[t]*是使用以下公式计算的：
- en: '![](img/5e7067dd-9b64-4c9c-97bd-a1e177c271b4.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e7067dd-9b64-4c9c-97bd-a1e177c271b4.png)'
- en: 'So, we can calculate the gradient of *h[t]*, as shown:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以计算*h[t]*的梯度，如下所示：
- en: '![](img/4408bf9d-c095-4d80-82c9-f9d56dded2ec.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4408bf9d-c095-4d80-82c9-f9d56dded2ec.png)'
- en: 'Since the hidden neurons also take *x[t]* as input, we need to take the derivative
    with respect to *U* as well. We can do this as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 由于隐藏神经元也将*x[t]*作为输入，我们还需要对*U*进行求导。我们可以按如下方式操作：
- en: '![](img/d8c2f879-a50e-471c-9489-b2d6274bac29.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8c2f879-a50e-471c-9489-b2d6274bac29.png)'
- en: 'But wait—the hidden units, as we have seen, take in two inputs. So, let''s
    backpropagate one time step using what we have just seen and see how it works:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 等等——正如我们所看到的，隐藏单元接收两个输入。那么，接下来让我们使用刚才所学的内容回传一步，看看它是如何工作的：
- en: '![](img/6d74dac3-3b1a-4da1-bdbc-8a12c9ca83a0.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d74dac3-3b1a-4da1-bdbc-8a12c9ca83a0.png)'
- en: 'Using this, we can now sum over all the previous gradients up to the present
    one, as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个，我们现在可以对所有先前的梯度进行求和，直到当前为止，如下所示：
- en: '![](img/948d3884-d535-47a1-bae2-028506d42ea1.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/948d3884-d535-47a1-bae2-028506d42ea1.png)'
- en: The backward pass in LSTM or GRU is much like what we did with regular RNNs,
    but there are some additional complexities here because of the gates (we will
    not go through the differences between the backward passes in LSTMs or GRUs here).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM或GRU中的反向传递与我们在普通RNN中所做的非常相似，但由于门控机制的存在，情况变得更加复杂（这里我们不会深入讨论LSTM或GRU中的反向传递差异）。
- en: Popular architecture
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流行架构
- en: Now that we have learned about all the components that are used to contrast
    RNNs, let's explore a popular architecture that has been developed by researchers
    in the field—the **clockwork RNN **(**CW-RNN**).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了所有用于对比RNN的组件，让我们来探索一种由该领域研究人员开发的流行架构——**时钟工作RNN**（**CW-RNN**）。
- en: Clockwork RNNs
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时钟工作RNN
- en: As we have learned, it is very challenging to discover long-term dependencies
    in RNNs, and LSTMs and GRUs were designed to overcome this limitation. CW-RNN,
    created by a group at IDSIA led by Jürgen Schmidhuber, modifies the vanilla RNN
    module such that the hidden layer is partitioned into separate modules, each of
    which processes its inputs at different temporal granularities. What this means
    is that the hidden layers perform computations at their preset clock rate (which
    is where the name comes from). The effect of this is a reduced number of trainable
    parameters and greater accuracy when compared to regular RNNs and LSTMs.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所了解的那样，在RNN中发现长期依赖关系是非常具有挑战性的，LSTM和GRU被设计用来克服这个限制。CW-RNN由IDSIA团队在Jürgen
    Schmidhuber的带领下开发，它修改了传统的RNN模块，使隐藏层被分割成多个独立的模块，每个模块以不同的时间粒度处理其输入。这意味着隐藏层以预设的时钟速率执行计算（这也是该名称的来源）。其效果是，相比于传统的RNN和LSTM，训练参数数量减少并且精度更高。
- en: Just as our earlier RNNs had input-to-hidden and hidden-to-output connections,
    a CW-RNN also has the same, except the neurons in the hidden layer are partitioned
    into *g* modules of size *k*, each of which has an assigned clock period, [![](img/5e608460-a533-437b-b23b-8bf58c320341.png)].
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前的RNN具有输入到隐藏和隐藏到输出的连接一样，CW-RNN也有这些连接，只不过隐藏层的神经元被分割成* g *个大小为*k*的模块，每个模块都有一个指定的时钟周期，[![](img/5e608460-a533-437b-b23b-8bf58c320341.png)]。
- en: These modules are fully connected together, but the recurrent connections from
    modules *j* to *i* are not if period [![](img/17d47dfc-b3eb-4917-9a75-5543a327bfc5.png)].
    These modules are sorted by increasing period and, therefore, the connections
    move from slower to faster (right to left).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模块是完全连接在一起的，但模块*j*到*i*的递归连接在周期[![](img/17d47dfc-b3eb-4917-9a75-5543a327bfc5.png)]的情况下是不成立的。这些模块按周期递增排序，因此连接是从较慢的到较快的（从右到左）。
- en: 'In the following diagram, we can see the architecture of the CW-RNN:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，我们可以看到CW-RNN的架构：
- en: '![](img/92317b85-89c9-49d9-a965-8b89ecdfc2cf.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92317b85-89c9-49d9-a965-8b89ecdfc2cf.png)'
- en: 'The clock period of module *i* can be calculated as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 模块*i*的时钟周期可以通过以下公式计算：
- en: '![](img/34b764e1-b9a1-4c40-bb95-d76ce35b41ea.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34b764e1-b9a1-4c40-bb95-d76ce35b41ea.png)'
- en: 'The input and hidden weight matrices are partitioned into *g* block rows, as
    follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和隐藏权重矩阵被分割成* g *个块行，如下所示：
- en: '![](img/e9fd2f57-215a-4b89-a645-2ef863af3e32.png) and ![](img/d1b93cb4-09ba-4375-9777-bf337318b18f.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9fd2f57-215a-4b89-a645-2ef863af3e32.png) 和 ![](img/d1b93cb4-09ba-4375-9777-bf337318b18f.png)'
- en: 'In the preceding equation, *W* is an upper-triangular matrix and each *W[i] *valueis
    partitioned into block columns:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，* W *是一个上三角矩阵，每个* W[i] *值被分割成块列：
- en: '![](img/4112c3b1-df4f-4059-a5fe-deccb3e037e4.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4112c3b1-df4f-4059-a5fe-deccb3e037e4.png)'
- en: 'During the forward pass, only the block rows of the hidden weight matrix and
    the input weight matrix correspond to the executed modules, where the following
    is true:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播过程中，只有隐藏权重矩阵和输入权重矩阵的块行对应于已执行的模块，其中以下条件成立：
- en: '![](img/bf6576cc-a983-4923-a2dd-576e32f5b693.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf6576cc-a983-4923-a2dd-576e32f5b693.png)'
- en: The modules with lower clock rates learn and maintain long-term information
    from the input and the modules with the higher clock rates learn local information.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 时钟速率较低的模块从输入中学习并保持长期信息，而时钟速率较高的模块则学习局部信息。
- en: 'We mentioned in the preceding equation that each hidden layer is partitioned
    into *g* modules of size *k*, which means there are a total of *n = kg* neurons.
    Since neurons are only connected to those that have a similar or larger period,
    the number of parameters within the hidden-to-hidden matrix is as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，我们提到每个隐藏层被分割成* g *个大小为*k*的模块，这意味着总共有*n = kg*个神经元。由于神经元仅与具有相似或更长周期的神经元相连，因此隐藏层到隐藏层矩阵中的参数数量如下所示：
- en: '![](img/7a7eb4fd-7f60-4ad2-8e97-13cda5f91413.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a7eb4fd-7f60-4ad2-8e97-13cda5f91413.png)'
- en: 'Let''s compare this with our vanilla RNNs, which have *n²* parameters. Let''s
    see how this is the case:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其与我们的传统RNN进行比较，后者有*n²*个参数。让我们看看这是如何实现的：
- en: '![](img/354d64a6-bb01-49e1-8720-5cc70f819a29.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/354d64a6-bb01-49e1-8720-5cc70f819a29.png)'
- en: The CW-RNN has approximately half as many parameters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: CW-RNN的参数数量大约是普通RNN的一半。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered a very powerful type of neural network—RNNs. We
    also learned about several variations of the RNN cell, such as LSTM cells and
    GRUs. Like the neural networks in prior chapters, these too can be extended to
    deep neural networks, which have several advantages. In particular, they can learn
    a lot more complex information about sequential data, for example, in language.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 本章，我们介绍了一种非常强大的神经网络——RNN。我们还学习了几种RNN单元的变体，如LSTM单元和GRU。像前几章中的神经网络一样，这些也可以扩展到深度神经网络，这具有许多优势。特别是，它们可以学习关于序列数据的更复杂的信息，例如在语言处理中的应用。
- en: In the next chapter, we will learn about attention mechanisms and their increasing
    popularity in language- and vision-related tasks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习注意力机制及其在语言和视觉相关任务中日益增长的应用。
