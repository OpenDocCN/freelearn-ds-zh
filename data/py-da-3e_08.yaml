- en: Retrieving, Processing, and Storing Data
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 检索、处理和存储数据
- en: Data can be found everywhere, in all shapes and forms. We can get it from the
    web, IoT sensors, emails, FTP, and databases. We can also collect it ourselves
    in a lab experiment, election polls, marketing polls, and social surveys. As a
    data professional, you should know how to handle a variety of datasets as that
    is a very important skill. We will discuss retrieving, processing, and storing
    various types of data in this chapter. This chapter offers an overview of how
    to acquire data in various formats, such as CSV, Excel, JSON, HDF5, Parquet, and
    `pickle`.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据无处不在，呈现各种形态和形式。我们可以从网络、物联网传感器、电子邮件、FTP 和数据库中获取数据。我们还可以通过实验室实验、选举民调、市场调查和社会调查等方式收集数据。作为数据专业人员，你应该知道如何处理各种数据集，因为这是一个非常重要的技能。本章将讨论如何检索、处理和存储不同类型的数据。本章概述了如何获取不同格式的数据，如
    CSV、Excel、JSON、HDF5、Parquet 和 `pickle`。
- en: Sometimes, we need to store or save the data before or after the data analysis.
    We will also learn how to access data from relational and **NoSQL** (**Not Only
    SQL**) databases such as `sqlite3`, MySQL, MongoDB, Cassandra, and Redis. In the
    world of the21st-century web, NoSQL databases are undergoing substantial growth
    in big data and web applications. They provide a more flexible, faster, and schema-free
    database. NoSQL databases can store data in various formats, such as document
    style, column-oriented, objects, graphs, tuples, or a combination.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们需要在数据分析之前或之后存储或保存数据。我们还将学习如何访问来自关系型和**NoSQL**（**非关系型数据库**）数据库的数据，如 `sqlite3`、MySQL、MongoDB、Cassandra
    和 Redis。在 21 世纪的网络世界中，NoSQL 数据库在大数据和 Web 应用程序中正在经历快速增长。它们提供了更灵活、更快速、无需架构的数据库。NoSQL
    数据库可以存储各种格式的数据，如文档风格、列式存储、对象、图形、元组或它们的组合。
- en: 'The topics covered in this chapter are listed as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题如下：
- en: Reading and writing CSV files with NumPy
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 NumPy 读取和写入 CSV 文件
- en: Reading and writing CSV files with pandas
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pandas 读取和写入 CSV 文件
- en: Reading and writing data from Excel
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Excel 读取和写入数据
- en: Reading and writing data from JSON
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 JSON 读取和写入数据
- en: Reading and writing data from HDF5
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 HDF5 读取和写入数据
- en: Reading and writing data from HTML tables
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 HTML 表格读取和写入数据
- en: Reading and writing data from Parquet
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Parquet 读取和写入数据
- en: Reading and writing data from a `pickle pandas` object
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 `pickle pandas` 对象读取和写入数据
- en: Lightweight access with `sqllite3`
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `sqlite3` 进行轻量级访问
- en: Reading and writing data from MySQL
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 MySQL 读取和写入数据
- en: Reading and writing data from MongoDB
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 MongoDB 读取和写入数据
- en: Reading and writing data from Cassandra
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Cassandra 读取和写入数据
- en: Reading and writing data from Redis
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Redis 读取和写入数据
- en: PonyORM
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PonyORM
- en: Technical requirements
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章有以下技术要求：
- en: 'You can find the code and the dataset at the following GitHub link: [https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter06](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter06).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在以下 GitHub 链接中找到代码和数据集：[https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter06](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter06)。
- en: All the code blocks are available in the `ch6.ipynb` file.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有代码块都可以在 `ch6.ipynb` 文件中找到。
- en: This chapter uses CSV files (`demo.csv`, `product.csv`, `demo_sample_df.csv`,
    `my_first_demo.csv`, and `employee.csv`), Excel files (`employee.xlsx`, `employee_performance.xlsx`,
    and `new_employee_details.xlsx`), JSON files (`employee.json` and `employee_demo.json`),
    an HTML file (`country.html`), a `pickle` file (`demo_obj.pkl`), an HDF5 file
    (`employee.h5`), and a Parquet file (`employee.parquet`) for practice purposes.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章使用以下文件进行实践：CSV 文件（`demo.csv`、`product.csv`、`demo_sample_df.csv`、`my_first_demo.csv`
    和 `employee.csv`）、Excel 文件（`employee.xlsx`、`employee_performance.xlsx` 和 `new_employee_details.xlsx`）、JSON
    文件（`employee.json` 和 `employee_demo.json`）、HTML 文件（`country.html`）、`pickle` 文件（`demo_obj.pkl`）、HDF5
    文件（`employee.h5`）和 Parquet 文件（`employee.parquet`）。
- en: In this chapter, we will use the `pandas`, `pickle`, `pyarrow`, `sqlite3`, `pymysql`,
    `mysql-connector`, `pymongo`, `cassandra-driver`, and `redis` Python libraries.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章将使用以下 Python 库：`pandas`、`pickle`、`pyarrow`、`sqlite3`、`pymysql`、`mysql-connector`、`pymongo`、`cassandra-driver`
    和 `redis`。
- en: Reading and writing CSV files with NumPy
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 NumPy 读取和写入 CSV 文件
- en: 'In [Chapter 2](2519d92c-1aff-40f7-a6ee-e36d32b77096.xhtml), *NumPy and pandas*,
    we looked at the NumPy library in detail and explored lots of functionality. NumPy
    also has functions to read and write CSV files and get output in a NumPy array.
    The `genfromtxt()` function will help us to read the data and the `savetxt()`
    function will help us to write the data into a file. The `genfromtxt()` function
    is slow compared to other functions due to its two-stage operation. In the first
    stage, it reads the data in a string type, and in the second stage, it converts
    the string type into suitable data types. `genfromtxt()` has the following parameters:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](2519d92c-1aff-40f7-a6ee-e36d32b77096.xhtml)，《*NumPy和pandas*》中，我们详细研究了NumPy库并探索了许多功能。NumPy也有读取和写入CSV文件的功能，并能以NumPy数组的形式输出结果。`genfromtxt()`函数将帮助我们读取数据，而`savetxt()`函数将帮助我们将数据写入文件。由于其两阶段操作，`genfromtxt()`函数比其他函数慢。第一阶段，它将数据以字符串类型读取，第二阶段，它将字符串类型转换为适当的数据类型。`genfromtxt()`具有以下参数：
- en: '`fname`: String; filename or path of the file.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fname`：字符串；文件名或文件路径。'
- en: '`delimiter`: String; optional, separate string value. By default, it takes
    consecutive white spaces.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delimiter`：字符串；可选，分隔字符串值。默认情况下，它采用连续的空白字符。'
- en: '`skip_header`: Integer; optional, number of lines you want to skip from the
    start of the file.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_header`：整数；可选，表示要跳过的文件开头的行数。'
- en: 'Let''s see an example of reading and writing CSV files:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个读取和写入CSV文件的例子：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This results in the following output:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding code example, we are reading the `demo.csv` file using the
    `genfromtxt()` method of the NumPy module:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用NumPy模块的`genfromtxt()`方法读取了`demo.csv`文件：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code example, we are writing the `my_first_demo.csv` file using
    the `savetxt()` method of the NumPy module.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用NumPy模块的`savetxt()`方法写入了`my_first_demo.csv`文件。
- en: Let's see how can we read CSV files using the `pandas` module in the next section.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节中看看如何使用`pandas`模块读取CSV文件。
- en: Reading and writing CSV files with pandas
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pandas读取和写入CSV文件
- en: 'The `pandas` library provides a variety of file reading and writing options.
    In this section, we will learn about reading and writing CSV files. In order to
    read a CSV file, we will use the `read_csv()` method. Let''s see an example:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas`库提供了多种文件读取和写入选项。在本节中，我们将学习如何读取和写入CSV文件。为了读取CSV文件，我们将使用`read_csv()`方法。让我们看一个例子：'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This results in the following output:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/2f3288ff-42bd-4c76-9674-b5c9d94c0888.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f3288ff-42bd-4c76-9674-b5c9d94c0888.png)'
- en: 'We can now save the dataframe as a CSV file using the following code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下代码将数据框保存为CSV文件：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding sample code, we have read and saved the CSV file using the
    `read_csv()` and `to_csv(0)` methods of the `pandas` module.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例代码中，我们使用`pandas`模块的`read_csv()`和`to_csv(0)`方法读取和保存了CSV文件。
- en: 'The `read_csv()` method has the following important arguments:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv()`方法有以下重要参数：'
- en: '`filepath_or_buffer`: Provides a file path or URL as a string to read a file.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filepath_or_buffer`：提供作为字符串的文件路径或URL，用于读取文件。'
- en: '`sep`: Provides a separator in the string, for example, comma as `'',''` and
    semicolon as `'';''`. The default separator is a comma `'',''`.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep`：提供字符串中的分隔符，例如逗号为`'',''`，分号为`'';''`。默认分隔符是逗号`'',''`。'
- en: '`delim_whitespace`: Alternative argument for a white space separator. It is
    a Boolean variable. The default value for `delim_whitespace` is `False`.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delim_whitespace`：用于空白分隔符的备用参数。它是一个布尔变量。`delim_whitespace`的默认值为`False`。'
- en: '`header`: This is used to identify the names of columns. The default value
    is `infer`.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`header`：用于标识列名。默认值为`infer`。'
- en: '`names`: You can pass a list of column names. The default value for `names`
    is `None`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`names`：可以传递一个列名列表。`names`的默认值为`None`。'
- en: 'In `pandas`, a DataFrame can also be exported in a CSV file using the `to_csv()`
    method. CSV files are comma-separated values files. This method can run with only
    a single argument (filename as a string):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在`pandas`中，可以使用`to_csv()`方法将DataFrame导出为CSV文件。CSV文件是逗号分隔值文件。此方法可以仅使用一个参数（文件名作为字符串）运行：
- en: '`path_or_buf`: The file path or location where the file will export.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path_or_buf`：文件将导出的文件路径或位置。'
- en: '`sep`: This is a delimiter used for output files.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep`：用于输出文件的分隔符。'
- en: '`header`: To include column names or a list of column aliases (default value:
    `True`).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`header`：包含列名或列别名列表（默认值：`True`）。'
- en: '`index`: To write an index to the file (default value: `True`).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index`：将索引写入文件（默认值：`True`）。'
- en: For more parameters and detailed descriptions, visit [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html).
    Let's see how can we read Excel files using the `pandas` module in the next section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 更多参数和详细描述，请访问[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html)。接下来，我们将看看如何使用`pandas`模块读取Excel文件。
- en: Reading and writing data from Excel
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Excel读取和写入数据
- en: 'Excel files are widely used files in the business domain. Excel files can be
    easily read in Python''s `pandas` using the `read_excel()` function. The `read_excel()`
    function takes a file path and `sheet_name` parameters to read the data:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Excel文件是商业领域中广泛使用的文件类型。在Python中，Excel文件可以通过`pandas`的`read_excel()`函数轻松读取。`read_excel()`函数需要传入文件路径和`sheet_name`参数来读取数据：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This results in the following output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/9458050f-19fc-47fd-95a9-0f411cfb3ef6.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9458050f-19fc-47fd-95a9-0f411cfb3ef6.png)'
- en: 'DataFrame objects can be written on Excel sheets. We can use the `to_excel()`
    function to export DataFrame objects into an Excel sheet. Mostly, the `to_excel()`
    function arguments are the same as `to_csv()` except for the `sheet_name` argument:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame对象可以写入Excel工作表中。我们可以使用`to_excel()`函数将DataFrame对象导出到Excel工作表中。通常，`to_excel()`函数的参数与`to_csv()`相同，除了`sheet_name`参数：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the preceding code example, we have exported a single DataFrame into an
    Excel sheet. We can also export multiple DataFrames in a single file with different
    sheet names. We can also write more than one DataFrame in a single Excel file
    (each DataFrame on different sheets) using `ExcelWriter`, as shown:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们将一个单独的DataFrame导出到了Excel工作表中。我们还可以将多个DataFrame导出到一个文件中，并使用不同的工作表名称。我们还可以使用`ExcelWriter`在一个Excel文件中写入多个DataFrame（每个DataFrame在不同的工作表上），如下所示：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding code example, we have written multiple DataFrames to a single
    Excel file. Here, each DataFrame store on a different sheet using the `ExcelWriter`
    function. Let's see how can we read the JSON files using the `pandas` module in
    the next section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们将多个DataFrame写入了一个Excel文件。这里，每个DataFrame存储在不同的工作表上，使用了`ExcelWriter`函数。接下来，我们将看看如何使用`pandas`模块读取JSON文件。
- en: Reading and writing data from JSON
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从JSON读取和写入数据
- en: '**JSON** (**JavaScript Object Notation**) files are a widely used format for
    interchanging data among web applications and servers. It acts as a data interchanger
    and is more readable compared to XML. `pandas` offers the `read_json` function
    for reading JSON data and `to_json()` for writing JSON data:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**JSON**（**JavaScript对象表示法**）文件是用于Web应用和服务器之间交换数据的广泛使用的格式。它作为数据交换格式，比XML更易读。`pandas`提供了`read_json`函数来读取JSON数据，以及`to_json()`函数来写入JSON数据：'
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/b694891d-1d89-4264-bf15-5412ba97b2fc.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b694891d-1d89-4264-bf15-5412ba97b2fc.png)'
- en: 'In the preceding code example, we have read the JSON file using the `read_json()`
    method. Let''s see how to write a JSON file:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用`read_json()`方法读取了JSON文件。接下来，我们将看看如何写入一个JSON文件：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding code example, we have written the JSON file using the `to_json()`
    method. In the `to_json()` method, the `orient` parameter is used to handle the
    output string format. `orient` offers record, column, index, and value kind of
    formats. You can explore it in more detail on the official web page, at [https://pandas.pydata.org/pandas-docs/version/0.24.2/reference/api/pandas.DataFrame.to_json.html](https://pandas.pydata.org/pandas-docs/version/0.24.2/reference/api/pandas.DataFrame.to_json.html).
    It's time to jump into HDF5 files. In the next section, we will see how to read
    and write HDF5 files using the `pandas` module.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用`to_json()`方法写入了JSON文件。在`to_json()`方法中，`orient`参数用于处理输出字符串格式。`orient`提供了record、column、index和value等格式。你可以在官方网页上查看更多详细信息，网址是：[https://pandas.pydata.org/pandas-docs/version/0.24.2/reference/api/pandas.DataFrame.to_json.html](https://pandas.pydata.org/pandas-docs/version/0.24.2/reference/api/pandas.DataFrame.to_json.html)。接下来，我们将进入HDF5文件部分，在下一节中，我们将学习如何使用`pandas`模块读取和写入HDF5文件。
- en: Reading and writing data from HDF5
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从HDF5读取和写入数据
- en: '**HDF** stands for **Hierarchical Data Format**. HDF is designed to store and
    manage large amounts of data with high performance. It offers fast I/O processing
    and storage of heterogeneous data. There are various HDF file formats available,
    such as HDF4 and HDF5\. HDF5 is the same as a dictionary object that reads and
    writes `pandas` DataFrames. It uses the PyTables library''s `read_hdf()` function
    for reading the HDF5 file and the `to_hdf()` function for writing:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**HDF** 代表 **层次化数据格式**。HDF 旨在以高性能存储和管理大量数据。它提供了快速的 I/O 处理和异构数据的存储。有多种 HDF 文件格式可用，如
    HDF4 和 HDF5。HDF5 与读取和写入 `pandas` DataFrame 的字典对象相同。它使用 PyTables 库的 `read_hdf()`
    函数来读取 HDF5 文件，使用 `to_hdf()` 函数来写入：'
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the preceding code example, we have written the HDF file format using the
    `to_hdf()` method. `''table''` is a format parameter used for the table format.
    Table format may perform slower but offers more flexible operations, such as searching
    and selecting. The `append` parameter is used to append input data onto the existing
    data file:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用 `to_hdf()` 方法写入了 HDF 文件格式。`'table'` 是用于表格格式的格式参数。表格格式可能会较慢，但提供了更灵活的操作，例如搜索和选择。`append`
    参数用于将输入数据追加到现有数据文件中：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This results in the following output:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![](img/3ea05ef4-fb90-4f8e-ab93-0d15496e2aef.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ea05ef4-fb90-4f8e-ab93-0d15496e2aef.png)'
- en: In the preceding code example, we have read the HDF file format using the `read_hdf()`
    method. Let's see how to read and write HTML tables from a website in the next
    section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用 `read_hdf()` 方法读取了 HDF 文件格式。接下来让我们看看如何从网站读取和写入 HTML 表格。
- en: Reading and writing data from HTML tables
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 HTML 表格读取和写入数据
- en: 'HTML tables store rows in the `<tr>...</tr>` tag and each row has corresponding
    `<td>...</td>` cells for holding values. In `pandas`, we can also read the HTML
    tables from a file or URL. The `read_html()` function reads an HTML table from
    a file or URL and returns HTML tables into a list of `pandas` DataFrames:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: HTML 表格将行存储在 `<tr>...</tr>` 标签中，每行都有相应的 `<td>...</td>` 单元格来存储值。在 `pandas` 中，我们还可以从文件或
    URL 中读取 HTML 表格。`read_html()` 函数从文件或 URL 中读取 HTML 表格，并将 HTML 表格返回为 `pandas` DataFrame
    列表：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in the following output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the preceding code example, we have read the HTML table from a given web
    page using the `read_html()` method. `read_html()` will return all the tables
    as a list of DataFrames. Let''s check one of the DataFrames from the list:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用 `read_html()` 方法从给定的网页读取了 HTML 表格。`read_html()` 将返回所有表格，作为 DataFrame
    列表。接下来让我们检查列表中的一个 DataFrame：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This results in the following output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![](img/12053630-a477-4f75-9354-f324711171b0.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12053630-a477-4f75-9354-f324711171b0.png)'
- en: 'In the preceding code example, we have shown the initial five records of the
    first table available on the given web page. Similarly, we can also write DataFrame
    objects as HTML tables using `to_html()`. `to_html()` renders the content as an
    HTML table:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们展示了给定网页上第一个表格的前五条记录。类似地，我们还可以使用 `to_html()` 将 DataFrame 对象写入 HTML
    表格。`to_html()` 将内容渲染为 HTML 表格：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With the preceding code example, we can convert any DataFrame into an HTML page
    that contains the DataFrame as a table.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面的代码示例，我们可以将任何 DataFrame 转换为一个包含 DataFrame 作为表格的 HTML 页面。
- en: Reading and writing data from Parquet
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 Parquet 读取和写入数据
- en: The Parquet file format provides columnar serialization for `pandas` DataFrames.
    It reads and writes DataFrames efficiently in terms of storage and performance
    and shares data across distributed systems without information loss. The Parquet
    file format does not support duplicate and numeric columns.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件格式为 `pandas` DataFrame 提供了列式序列化。它在存储和性能方面高效地读取和写入 DataFrame，并且能够在分布式系统之间共享数据而不丢失信息。Parquet
    文件格式不支持重复列和数值列。
- en: 'There are two engines used to read and write Parquet files in `pandas`: `pyarrow`
    and the `fastparquet` engine. `pandas`''s default Parquet engine is `pyarrow`;
    if `pyarrow` is unavailable, then it uses `fastparquet`. In our example, we are
    using `pyarrow`. Let''s install `pyarrow` using `pip`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas` 使用两个引擎来读取和写入 Parquet 文件：`pyarrow` 和 `fastparquet` 引擎。`pandas` 的默认
    Parquet 引擎是 `pyarrow`；如果 `pyarrow` 不可用，则使用 `fastparquet`。在我们的示例中，我们使用的是 `pyarrow`。让我们使用
    `pip` 安装 `pyarrow`：'
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can also install the `pyarrow` engine in the Jupyter Notebook by putting
    an `!` before the `pip` keyword. Here is an example:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过在 `pip` 关键词前加上 `!` 来在 Jupyter Notebook 中安装 `pyarrow` 引擎。以下是一个示例：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s write a file using the `pyarrow` engine:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `pyarrow` 引擎写入一个文件：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code example, we have written the `using to_parquet()` Parquet
    file and the `pyarrow` engine:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用`to_parquet()` Parquet文件和`pyarrow`引擎进行写入：
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This results in the following output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/b44e908a-ac9e-43fc-9a94-86fbb6d1abb0.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b44e908a-ac9e-43fc-9a94-86fbb6d1abb0.png)'
- en: In the preceding code example, we have read the Parquet file using `read_parquet()`
    and the `pyarrow` engine. `read_parquet()` helps to read the Parquet file formats.
    Let's see how to read and write the data using `pickle` files in the next section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用`read_parquet()`和`pyarrow`引擎读取了Parquet文件。`read_parquet()`有助于读取Parquet文件格式。接下来，我们将在下一部分了解如何读取和写入`pickle`文件中的数据。
- en: Reading and writing data from a pickle pandas object
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Pickle pandas对象中读取和写入数据
- en: 'In the data preparation step, we will use various data structures such as dictionaries,
    lists, arrays, or DataFrames. Sometimes, we might want to save them for future
    reference or send them to someone else. Here, a `pickle` object comes into the
    picture. `pickle` serializes the objects to save them and can be loaded again
    any time. `pandas` offer two functions: `read_pickle()` for loading `pandas` objects
    and `to_pickle()` for saving Python objects:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据准备步骤中，我们将使用各种数据结构，如字典、列表、数组或DataFrame。有时，我们可能希望将它们保存以备将来参考，或将它们发送给其他人。在这种情况下，`pickle`对象就派上用场了。`pickle`将对象序列化以保存它们，并可以随时加载。`pandas`提供了两个函数：`read_pickle()`用于加载`pandas`对象，`to_pickle()`用于保存Python对象：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the preceding code, we read the `demo.csv` file using the `read_csv()` method
    with `sep` and `header` parameters. Here, we have assigned `sep` with a comma
    and `header` with `None`. Finally, we have written the dataset to a `pickle` object
    using the `to_pickle()` method. Let''s see how to read `pickle` objects using
    the `pandas` library:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`read_csv()`方法读取了`demo.csv`文件，并设置了`sep`和`header`参数。在这里，我们将`sep`设置为逗号，将`header`设置为`None`。最后，我们使用`to_pickle()`方法将数据集写入`pickle`对象。接下来，让我们看看如何使用`pandas`库读取`pickle`对象：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This results in the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/e6244c42-b825-4193-886c-eee9ae5d3c8e.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6244c42-b825-4193-886c-eee9ae5d3c8e.png)'
- en: In the preceding code, we have read the `pickle` objects using the `read_pickle()`
    method.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`read_pickle()`方法读取了`pickle`对象。
- en: Lightweight access with sqllite3
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用sqllite3轻量级访问
- en: SQLite is an open-source database engine. It offers various features such as
    faster execution, lightweight processing, serverless architecture, ACID compliance,
    less administration, high stability, and reliable transactions. It is the most
    popular and widely deployed database in the mobile and computer world. It is also
    known as an embedded relational database because it runs as part of your application.
    SQLite is a lighter database and does not offer full-fledged features. It is mainly
    used for small data to store and process locally, such as mobile and desktop applications.
    The main advantages of SQLite are that it is easy to use, efficient, and light,
    and can be embedded into the application.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: SQLite是一个开源数据库引擎。它提供了多种特性，如更快的执行速度、轻量级处理、无服务器架构、ACID兼容性、更少的管理工作、高稳定性和可靠的事务。它是移动设备和计算机世界中最流行、最广泛部署的数据库。它也被称为嵌入式关系型数据库，因为它作为应用程序的一部分运行。SQLite是一个更轻量的数据库，功能上没有完整的功能。它主要用于存储和处理小型数据，比如移动应用和桌面应用。SQLite的主要优点是易于使用、高效、轻便，并且可以嵌入到应用程序中。
- en: 'We can read and write data in Python from the `sqlite3` module. We don''t need
    to download and install `sqlite3` as it is already available in all the standard
    Python distributions. With `sqlite3`, we can either store the database in a file
    or keep it in RAM. `sqlite3` allows us to write any database using SQL without
    any third-party application server. Let''s look at the following example to understand
    database connectivity:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`sqlite3`模块在Python中读取和写入数据。我们不需要下载和安装`sqlite3`，因为它已经包含在所有标准Python发行版中。使用`sqlite3`，我们可以将数据库存储在文件中或保存在内存中。`sqlite3`允许我们使用SQL编写任何数据库，而不需要第三方应用程序服务器。让我们通过以下示例了解数据库连接：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, we are using the `sqlite3` module. First, we import the module and create
    a connection using the `connect()` method. The `connect()` method will take the
    database name and path; if the database does not exist, it will create the database
    with the given name and on the given location path. Once you have established
    a connection with the database, then you need to create the `Cursor` object and
    execute the SQL query using the `execute()` method. We can create a table in the
    `execute()` method, as given in the example `emp` table, which is created in the
    employee database. Similarly, we can write the data using the `execute()` method
    with an `insert` query argument and commit the data into the database using the
    `commit()` method. Data can also be extracted using the `execute()` method by
    passing the `select` query as an argument and fetched using `fetchall()` and the
    `fetchone()` method. `fetchone()` extracts a single record and `fetchall()` extracts
    all the records from a database table.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用`sqlite3`模块。首先，我们导入该模块并使用`connect()`方法创建一个连接。`connect()`方法将接收数据库名称和路径；如果数据库不存在，它将使用给定的名称和路径创建该数据库。一旦与数据库建立连接，您需要创建`Cursor`对象，并使用`execute()`方法执行SQL查询。我们可以在`execute()`方法中创建表，例如在员工数据库中创建的`emp`表。类似地，我们可以使用`execute()`方法和`insert`查询参数写入数据，并使用`commit()`方法将数据提交到数据库。也可以通过`execute()`方法传递`select`查询作为参数来提取数据，使用`fetchall()`和`fetchone()`方法进行提取。`fetchone()`提取一条记录，`fetchall()`提取数据库表中的所有记录。
- en: Reading and writing data from MySQL
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从MySQL读取和写入数据
- en: 'MySQL is a fast, open-source, and easy-to-use relational or tabular database.
    It is suitable for small and large business applications. It is very friendly
    with database-driven web development applications. There are lots of ways to access
    data in Python from MySQL. Connectors such as MySQLdb, `mysqlconnector`, and `pymysql`
    are available for MySQL database connectivity. For this connectivity purpose,
    you should install a MySQL relational database and the `mysql-python` connector.
    The MySQL setup details are available on its website: [https://www.mysql.com/downloads/](https://www.mysql.com/downloads/).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: MySQL是一个快速、开源、易于使用的关系型数据库，适用于小型和大型商业应用。它与数据库驱动的Web开发应用非常兼容。Python中有很多方式可以访问MySQL中的数据。像MySQLdb、`mysqlconnector`和`pymysql`等连接器可用于MySQL数据库连接。为了实现这种连接，您需要安装MySQL关系型数据库和`mysql-python`连接器。MySQL的安装详情可以在其官网上找到：[https://www.mysql.com/downloads/](https://www.mysql.com/downloads/)。
- en: 'You can use the `pymysql` connector as the client library and it can be installed
    using `pip`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`pymysql`连接器作为客户端库，可以通过`pip`安装：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can establish a connection with the following steps:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下步骤建立连接：
- en: Import the library.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库。
- en: Create a database connection.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据库连接。
- en: Create a cursor object.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建游标对象。
- en: Execute the SQL query.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行SQL查询。
- en: Fetch the records or response for the update or insert the record.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取记录或响应以更新或插入记录。
- en: Close the connection.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭连接。
- en: In our examples, we are trying database connectivity using `mysqlconnecter`
    and `pymysql`. Before running the database connectivity script, the first step
    is to design and create a database and then create a table in MySQL.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们尝试使用`mysqlconnecter`和`pymysql`进行数据库连接。在运行数据库连接脚本之前，第一步是设计并创建一个数据库，然后在MySQL中创建一个表。
- en: 'Let''s create a database using the following query:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下查询来创建一个数据库：
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Change the database to the employee database:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据库更改为员工数据库：
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create a table in the database:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据库中创建表：
- en: '[PRE26]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we can insert and fetch the records from a table in MySQL. Let''s look
    at the following example to understand the database connectivity:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以插入和获取MySQL表中的记录。让我们看一下以下示例来理解数据库连接：
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here, we are using the `pymysql` module. First, we import the module and create
    a connection. The `connect()` function will take the host address, which is `localhost`,
    in our case (we can also use the IP address of the remote database), username,
    password, database name, character set, and cursor class.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用`pymysql`模块。首先，我们导入该模块并创建一个连接。`connect()`函数将接收主机地址，在我们的案例中是`localhost`（我们也可以使用远程数据库的IP地址）、用户名、密码、数据库名称、字符集和游标类。
- en: After establishing the connection, we can read or write the data. In our example,
    we are writing the data using the `insert` SQL query and retrieving it using the
    `select` query. In the insert query, we are executing the query and passing the
    argument that we want to enter into the database, and committing the results into
    the database using the `commit()` method. When we read the records using the select
    query, we will get some number of records. We can extract those records using
    the `fetchone()` and `fetchall()` functions. The `fetchone()` method extracts
    only single records and the `fetchall()` method extracts multiple records from
    a database table.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立连接之后，我们可以读取或写入数据。在我们的例子中，我们使用 `insert` SQL 查询写入数据，并使用 `select` 查询检索数据。在插入查询中，我们执行查询并传递要插入数据库的参数，然后使用
    `commit()` 方法将结果提交到数据库。当我们使用选择查询读取记录时，将得到一些记录。我们可以使用 `fetchone()` 和 `fetchall()`
    函数提取这些记录。`fetchone()` 方法仅提取单条记录，而 `fetchall()` 方法则提取数据库表中的多条记录。
- en: 'One more thing; here, all the read-write operations are performed in a `try`
    block and the connection is closed in the final block. We can also try one more
    module `mysql.connector` for MySQL and Python connectivity. It can be installed
    using `pip`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事；在这里，所有的读写操作都在 `try` 块中执行，连接则在最终块中关闭。我们还可以尝试另一个模块 `mysql.connector` 来实现
    MySQL 和 Python 的连接。它可以通过 `pip` 安装：
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s look at the following example to understand the database connectivity:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子来理解数据库连接：
- en: '[PRE29]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the preceding code example, we are connecting to Python with the MySQL database
    using the `mysql.connector` module and the approach and steps for retrieving data
    are the same as with the `pymysql` module. We are also writing the extracted records
    into a `pandas` DataFrame by just passing fetched records into the DataFrame object
    and assigning column names from the cursor description.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用 `mysql.connector` 模块连接 Python 和 MySQL 数据库，提取数据的方式和步骤与使用 `pymysql`
    模块相同。我们还通过将提取的记录传递到 `pandas` DataFrame 对象中，并从游标描述中分配列名来写入数据。
- en: Inserting a whole DataFrame into the database
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将整个 DataFrame 插入数据库
- en: 'In the preceding program, a single record is inserted using the `insert` command.
    If we want to insert multiple records, we need to run a loop to insert the multiple
    records into the database. We can also use the `to_sql()` function to insert multiple
    records in a single line of code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的程序中，使用 `insert` 命令插入单个记录。如果我们想要插入多个记录，则需要运行一个循环，将多个记录插入数据库。我们还可以使用 `to_sql()`
    函数在一行代码中插入多个记录：
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding code example, we will create an engine for a database connection
    with username, password, and database parameters. The `to_sql()` function writes
    multiple records from the DataFrame to a SQL database. It will take the table
    name, the `con` parameter for the connection engine object, the `if_exists` parameter
    for checking whether data will append to a new table or replace with a new table,
    and `chunksize` for writing data in batches.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们将为数据库连接创建一个引擎，包含用户名、密码和数据库参数。`to_sql()` 函数将多个记录从 DataFrame 写入 SQL
    数据库。它需要表名、`con` 参数（用于连接引擎对象）、`if_exists` 参数（检查数据是追加到新表还是替换为新表），以及 `chunksize`（用于批量写入数据）。
- en: Reading and writing data from MongoDB
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 MongoDB 读取和写入数据
- en: 'MongoDB is a document-oriented non-relational (NoSQL) database. It uses JSON-like
    notation, **BSON** (**Binary Object Notation**) to store the data. MongoDB offers
    the following features:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB 是一个面向文档的非关系型（NoSQL）数据库。它使用类似 JSON 的符号，**BSON**（**二进制对象符号**）来存储数据。MongoDB
    提供以下功能：
- en: It is a free, open-source, and cross-platform database software.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一个免费的、开源的、跨平台的数据库软件。
- en: It is easy to learn, can build faster applications, supports flexible schemas,
    handles diverse data types, and has the capability to scale in a distributed environment.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它易于学习，能够构建更快的应用程序，支持灵活的模式，处理多种数据类型，并且具备在分布式环境中扩展的能力。
- en: It works on concepts of documents.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它基于文档的概念工作。
- en: It has a database, collection, document, field, and primary key.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含数据库、集合、文档、字段和主键。
- en: 'We can read and write data in Python from MongoDB using the `pymongo` connector.
    For this connectivity purpose, we need to install MongoDB and the `pymongo` connector.
    You can download MongoDB from its official web portal: [https://www.mongodb.com/download-center/community](https://www.mongodb.com/download-center/community).
    PyMongo is a pure Python MongoDB client library that can be installed using `pip`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`pymongo`连接器在Python中从MongoDB读取和写入数据。为了实现这个连接，我们需要安装MongoDB和`pymongo`连接器。你可以从其官方网站下载MongoDB：[https://www.mongodb.com/download-center/community](https://www.mongodb.com/download-center/community)。PyMongo是一个纯Python的MongoDB客户端库，可以通过`pip`安装：
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s try database connectivity using `pymongo`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用`pymongo`进行数据库连接：
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, we are trying to extract data from database collection in MongoDB by creating
    a Mongo client, inserting data, extracting collection details, and assigning it
    to the DataFrame. Let's see how to create a database connection with the columnar
    database Cassandra in the next section.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过创建Mongo客户端、插入数据、提取集合详情并将其分配给DataFrame，尝试从MongoDB的数据库集合中提取数据。接下来，我们将展示如何在下一节中使用列式数据库Cassandra建立数据库连接。
- en: Reading and writing data from Cassandra
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Cassandra读取和写入数据
- en: Cassandra is scalable, highly available, durable, and fault-tolerant, has lower
    admin overhead, has faster read-write, and is a resilient column-oriented database.
    It is easier to learn and configure. It provides solutions for quite complex problems.
    It also supports replication across multiple data centers. Plenty of big companies,
    for example, Apple, eBay, and Netflix use Cassandra.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra是一个可扩展、高可用、耐久和容错的列式数据库，具有较低的管理开销、更快的读写速度，并且能够提供强大的弹性。它容易学习和配置，能够为复杂问题提供解决方案，并且支持跨多个数据中心的复制。许多大型公司，例如苹果、eBay和Netflix，都在使用Cassandra。
- en: 'We can read and write data in Python from Cassandra using the `cassandra-driver`
    connector. For this connectivity purpose, we need to install Cassandra and `cassandra-driver`
    connectors. You can download Cassandra from its official website: [http://cassandra.apache.org/download/](http://cassandra.apache.org/download/).
    `cassandra-driver` is a pure Python Cassandra client library that can be installed
    using `pip`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`cassandra-driver`连接器在Python中从Cassandra读取和写入数据。为了实现这个连接，我们需要安装Cassandra和`cassandra-driver`连接器。你可以从其官方网站下载Cassandra：[http://cassandra.apache.org/download/](http://cassandra.apache.org/download/)。`cassandra-driver`是一个纯Python的Cassandra客户端库，可以通过`pip`安装：
- en: '[PRE33]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s try database connectivity using `cassandra-driver`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用`cassandra-driver`进行数据库连接：
- en: '[PRE34]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here, we are trying to extract data from the Cassandra database by creating
    a cluster object, creating a connection using the `connect()` method, executing
    an insert, and selecting query data. After running the query, we are printing
    the results and assigning the extracted records to the `pandas` DataFrame. Let''s
    move on now to another NoSQL database: Redis.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过创建一个集群对象、使用`connect()`方法建立连接、执行插入操作以及选择查询数据，尝试从Cassandra数据库中提取数据。运行查询后，我们打印结果并将提取的记录分配给`pandas`
    DataFrame。接下来，让我们进入另一个NoSQL数据库：Redis。
- en: Reading and writing data from Redis
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Redis读取和写入数据
- en: Redis is an open-source NoSQL database. It is a key-value database, in-memory,
    extremely fast, and highly available. It can also be employed as a cache or act
    as a message broker. In-memory means it uses RAM for the storage of data and handles
    bigger-sized data using virtual memory. Redis offers a cache service or permanent
    storage. Redis supports a variety of data structures, such as string, set, list,
    bitmap, geospatial indexes, and hyperlogs. Redis can deal with geospatial, streaming,
    and time-series data. It is offered with cloud services such as AWS and Google
    Cloud.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Redis是一个开源的NoSQL数据库。它是一个键值数据库，存储在内存中，速度极快，并且高可用。它还可以用作缓存或消息代理。内存存储意味着它使用RAM来存储数据，并通过虚拟内存处理更大的数据。Redis提供缓存服务或永久存储。Redis支持多种数据结构，如字符串、集合、列表、位图、地理空间索引和超日志。Redis能够处理地理空间、流式数据和时间序列数据。它还可以与云服务如AWS和Google
    Cloud一起使用。
- en: 'We can read and write data in Python from Redis using the Redis connector.
    For this connectivity purpose, we need to install Redis and the Redis connector.
    You can download Redis from the following link: [https://github.com/rgl/redis/downloads](https://github.com/rgl/redis/downloads).
    Redis is a pure Python Redis client library that can be installed using `pip`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过Redis连接器在Python中从Redis读取和写入数据。为了实现这个连接，我们需要安装Redis和Redis连接器。你可以通过以下链接下载Redis：[https://github.com/rgl/redis/downloads](https://github.com/rgl/redis/downloads)。Redis是一个纯Python的Redis客户端库，可以通过`pip`安装：
- en: '[PRE35]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let''s try database connectivity using Redis:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用 Redis 进行数据库连接：
- en: '[PRE36]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, we are trying to extract data from the Redis key-value database. First,
    we have created a connection with the database. We are setting the key-value pairs
    into the Redis database using the `set()` method and we have also extracted the
    value using the `get()` method with the given key parameter.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们尝试从 Redis 键值数据库中提取数据。首先，我们与数据库建立了连接。我们使用 `set()` 方法将键值对设置到 Redis 数据库中，并且通过给定的键参数使用
    `get()` 方法提取值。
- en: Finally, its time to shift to the last topic of this chapter, which is PonyORM
    for **object-relational mapping** (**ORM**).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，进入本章的最后一个主题，即用于**对象关系映射**（**ORM**）的 PonyORM。
- en: PonyORM
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PonyORM
- en: 'PonyORM is a powerful ORM package that is written in pure Python. It is fast
    and easy to use and performs operations with minimum effort. It provides automatic
    query optimization and a GUI database schema editor. It also supports automatic
    transaction management, automatic caching, and composite keys. PonyORM uses Python
    generator expressions, which are translated in SQL. We can install it using `pip`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: PonyORM 是一个强大的 ORM 包，使用纯 Python 编写。它快速、易于使用，并且能以最小的努力执行操作。它提供自动查询优化和图形界面数据库模式编辑器。它还支持自动事务管理、自动缓存和复合键。PonyORM
    使用 Python 生成器表达式，这些表达式会被翻译成 SQL。我们可以通过 `pip` 安装它：
- en: '[PRE37]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let''s see an example of ORM using `pony`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个使用 `pony` 的 ORM 示例：
- en: '[PRE38]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In the preceding code example, we are performing ORM. First, we have created
    a `Database` object and defined entities using an `Emp` class. After that, we
    have attached the entities to the database using `db.bind()`. We can bind it with
    four databases: `sqlite`, `mysql`, `postgresql`, and `oracle`. In our example,
    we are using MySQL and passing its credential details, such as username, password,
    and database name. We can perform the mapping of entities with data using `generate_mapping()`.
    The `create_tables=True` argument creates the tables if it does not exist. `sql_debug(True)`
    will turn on the debug mode. The `select()` function translates a Python generator
    into a SQL query and returns a `pony` object. This `pony` object will be converted
    into a list of entities using the slice operator (`[:]`) and the `show()` function
    will display all the records in a tabular fashion.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们正在执行 ORM。首先，我们创建了一个 `Database` 对象，并使用 `Emp` 类定义了实体。接着，我们通过 `db.bind()`
    方法将实体绑定到数据库。我们可以将其绑定到四个数据库：`sqlite`、`mysql`、`postgresql` 和 `oracle`。在我们的示例中，我们使用
    MySQL，并传递其凭证信息，如用户名、密码和数据库名称。我们可以使用 `generate_mapping()` 来执行实体与数据的映射。`create_tables=True`
    参数在表格不存在时会创建表。`sql_debug(True)` 会启用调试模式。`select()` 函数将 Python 生成器翻译成 SQL 查询，并返回一个
    `pony` 对象。这个 `pony` 对象将通过切片操作符（`[:]`）转换为实体列表，`show()` 函数将以表格形式显示所有记录。
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about retrieving, processing, and storing data in
    different formats. We have looked at reading and writing data from various file
    formats and sources, such as CSV, Excel, JSON, HDF5, HTML, `pickle`, table, and
    Parquet files. We also learned how to read and write from various relational and
    NoSQL databases, such as SQLite3, MySQL, MongoDB, Cassandra, and Redis.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了如何以不同格式检索、处理和存储数据。我们查看了如何从各种文件格式和来源读取和写入数据，如 CSV、Excel、JSON、HDF5、HTML、`pickle`、表格和
    Parquet 文件。我们还学习了如何从各种关系型和 NoSQL 数据库中读取和写入数据，如 SQLite3、MySQL、MongoDB、Cassandra
    和 Redis。
- en: The next chapter, [Chapter 7](5e8db48a-32f4-4b31-ba39-33376d7cafa3.xhtml), *Cleaning
    Messy Data*, is about the important topic of data preprocessing and feature engineering
    with Python. The chapter starts with exploratory data analysis, and leads to filtering,
    handling missing values, and outliers. After cleaning, the focus will be on data
    transformation, such as encoding, scaling, and splitting.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章，[第7章](5e8db48a-32f4-4b31-ba39-33376d7cafa3.xhtml)，*清理混乱数据*，讲述了数据预处理和特征工程在
    Python 中的关键主题。本章从探索性数据分析开始，接着介绍了数据过滤、处理缺失值和异常值。清理之后，将重点讨论数据转换，如编码、缩放和拆分。
