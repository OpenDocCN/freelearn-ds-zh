- en: Chapter 9.  Advanced Machine Learning with Streaming and Graph Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。流式和图形数据的高级机器学习
- en: 'This chapter guides the reader on how to apply machine learning techniques
    with the help of Spark MLlib, Spark ML and Spark Streaming to streaming and graph
    data using GraphX. For example, topic modeling from the real-time tweets data
    from Twitter. The readers will be able to use available APIs to build real-time
    and predictive applications from streaming data sources such as Twitter. Through
    the Twitter data analysis, we will show how to perform large scale social sentiment
    analysis. We will also show how to develop a large-scale movie recommendation
    using Spark MLlib, which is an implicit part of social network analysis. In a
    nutshell, the following topics will be covered throughout this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将指导读者如何借助Spark MLlib、Spark ML和Spark Streaming应用机器学习技术到流式和图形数据，使用GraphX。例如，从Twitter的实时推文数据中进行主题建模。读者将能够使用现有的API从Twitter等流数据源构建实时和预测性应用程序。通过Twitter数据分析，我们将展示如何进行大规模社交情感分析。我们还将展示如何使用Spark
    MLlib开发大规模电影推荐系统，这是社交网络分析的一个隐含部分。简而言之，本章将涵盖以下主题：
- en: Developing real-time ML pipelines
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发实时ML管道
- en: Time series and social network analysis
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列和社交网络分析
- en: Movie recommendation using Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行电影推荐
- en: Developing a real-time ML pipeline from streaming
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从流式数据开发实时ML管道
- en: ML pipeline on graph data and semi-supervised graph-based learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图形数据和半监督图形学习上的ML管道
- en: However, what it really needs in order to be an effective and emerging ML application
    is a continuous flow of labeled data. Consequently, preprocessing the large-scale
    unstructured data and accurate labeling to that data essentially introduces many
    unwanted latencies.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要成为一个有效和新兴的机器学习应用程序，实际上需要持续流动的标记数据。因此，对大规模非结构化数据进行预处理和准确标记数据本质上引入了许多不必要的延迟。
- en: Nowadays, we hear and read a lot about real-time machine learning. More or less,
    people usually provide this appealing business scenario when discussing sentiment
    analysis from **Social Network Services** (**SNS**), credit card fraud detection
    systems, or mining purchase rules related to the customer from business oriented
    transactional data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，我们经常听到和阅读关于实时机器学习的内容。人们通常在讨论从社交网络服务（SNS）、信用卡欺诈检测系统或从面向业务的交易数据中挖掘与客户相关的购买规则时，提供这种吸引人的业务场景。
- en: According to many ML experts, it is possible to continuously update the credit
    card fraud detection model in real time. It's fantastic, but not realistic to
    me, for several reasons. Firstly, ensuring the continuous flow of this kind of
    data is not needed for model retraining. Secondly, creating labeled data would
    probably be the slowest and the most expensive step in most of the machine learning
    systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 根据许多机器学习专家的说法，实时不断地更新信用卡欺诈检测模型是可能的。这很棒，但对我来说并不现实，有几个原因。首先，确保这种数据的持续流动对于模型的重新训练并不是必要的。其次，在大多数机器学习系统中，创建标记数据可能是最慢、最昂贵的步骤。
- en: Developing real-time ML pipelines
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发实时ML管道
- en: In order to develop a real-time machine learning application, we need to have
    access to a continuous flow of data. The data might include transactional data,
    simple texts, tweets from Twitter, messaging or streaming from Flume or Kafka,
    and so on, as this is mostly unstructured data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发实时机器学习应用程序，我们需要持续获取数据。这些数据可能包括交易数据、简单文本、来自Twitter的推文、来自Flume或Kafka的消息或流式数据等，因为这些数据大多是非结构化的。
- en: To deploy these kinds of ML applications, we need to go through a series of
    steps. The most unreliable source of the data that would serve our purpose is
    the real-time data from several sources. Often networks are a performance bottleneck.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署这些类型的机器学习应用程序，我们需要经历一系列步骤。为我们的目的服务的数据最不可靠的来源是来自多个来源的实时数据。通常网络是性能瓶颈。
- en: For example, it's not guaranteed that you will always receive a bunch of tweets
    from Twitter. Moreover, labeling this data towards building an ML model on the
    fly is not a realistic idea. Nevertheless, here we provide a real insight on how
    we could develop and deploy an ML pipeline from real-time streaming data. *Figure
    1* shows the workflow of a real-time ML application development.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，并不保证您总是能够从Twitter收到大量推文。此外，即时对这些数据进行标记以构建一个ML模型并不是一个现实的想法。尽管如此，我们在这里提供了一个关于如何从实时流数据中开发和部署ML管道的真实见解。*图1*显示了实时ML应用程序开发的工作流程。
- en: Streaming data collection as unstructured text data
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式数据收集作为非结构化文本数据
- en: 'We would like to stress here that real time stream data collection depends
    on:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里要强调的是，实时流数据收集取决于：
- en: The purpose of the data collection. If the purpose is to develop a credit card
    fraud detection online, then the data should be collected from your own network
    through the web API. If the purpose is to collect social media sentiment analysis
    then data could be collected from Twitter, LinkedIn, Facebook, or newspaper sites,
    and if the purpose is to network anomaly detection, data could be collected from
    the network data.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集的目的。如果目的是开发在线信用卡欺诈检测系统，那么数据应该通过网络API从您自己的网络中收集。如果目的是收集社交媒体情感分析，那么数据可以从Twitter、LinkedIn、Facebook或报纸网站收集，如果目的是网络异常检测，数据可以从网络数据中收集。
- en: Data availability is an issue since not all social media platforms provide public
    APIs for collecting data. The network condition is important since stream data
    is huge and needs very fast network connectivity.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可用性是一个问题，因为并非所有社交媒体平台都提供公共API来收集数据。网络条件很重要，因为流数据量很大，需要非常快的网络连接。
- en: Storage capability is an important consideration since a collection of a few
    minutes of tweets data, for example, could contribute to several GB of data.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储能力是一个重要考虑因素，因为几分钟的推文数据集合可能会产生数GB的数据。
- en: Moreover, we should wait at least a couple of days before marking the transactions
    as *Fraud* or *Not Fraud*, for example. In contrast, if somebody reported a fraud
    transaction, we can immediately label this transaction as *Fraud* for the sake
    of simplicity.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们应该至少等待几天，然后将交易标记为*欺诈*或*非欺诈*。相反，如果有人报告了欺诈交易，我们可以立即将该交易标记为*欺诈*以简化操作。
- en: Labeling the data towards making the supervised machine learning
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记数据以进行监督式机器学习
- en: The labeled data set plays a central role in the whole process. It ensures that
    it is very easy to change the parameters of an algorithm such as the feature normalization
    or loss function. In that case, we would have several options of choosing the
    algorithm itself from logistic regression, to **Support Vector Machine** (**SVM**),
    or random forest, for example.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 标记的数据集在整个过程中起着核心作用。它确保很容易改变算法的参数，比如特征归一化或损失函数。在这种情况下，我们可以从逻辑回归、**支持向量机**（**SVM**）或随机森林等多种算法中选择算法本身。
- en: However, we cannot change the labeled data set since this information is predefined
    and your model should predict the labels that you already have. In previous chapters,
    we have shown that labeling the structured data takes a considerable amount of
    time.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不能改变标记的数据集，因为这些信息是预定义的，你的模型应该预测你已经拥有的标签。在之前的章节中，我们已经表明标记结构化数据需要相当长的时间。
- en: Now think about the fully unstructured stream data that we would be receiving
    from streaming or real-time sources. In that case, labeling the data would take
    a considerable amount of time. Nevertheless, we will also have to do the pre-processing,
    such as tokenization, cleaning, indexing, removing stop words, and removing special
    characters from the unstructured data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想想我们将从流媒体或实时来源接收到的完全非结构化的流数据。在这种情况下，标记数据将需要相当长的时间。然而，我们还必须进行预处理，如标记化、清理、索引、去除停用词和去除非结构化数据中的特殊字符。
- en: Now, essentially, there would be a question of *how long does the data labeling
    process take?* The final thing about the labeled dataset is that we should understand
    that the labeled dataset might be biased sometimes if we don't do the labeling
    carefully, which might lead to a lot of issues with the model's performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基本上会有一个问题，*数据标记过程需要多长时间？*关于标记的数据集的最后一件事是，我们应该明白，如果我们不仔细进行标记，标记的数据集有时可能会存在偏见，这可能会导致模型性能出现很多问题。
- en: Creating and building the model
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建和构建模型
- en: 'For training the sentiment analytics, the credit card fraud detection model,
    and the association rule mining model, we need to have a lot of examples of transaction
    data that is as accurate as possible. Once we have the labeled dataset, we are
    ready to train and build the model:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练情感分析、信用卡欺诈检测模型和关联规则挖掘模型，我们需要尽可能准确的交易数据示例。一旦我们有了标记的数据集，我们就可以开始训练和构建模型：
- en: '![Creating and building the model](img/00142.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![创建和构建模型](img/00142.jpeg)'
- en: 'Figure 1: Real-time machine learning workflow.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：实时机器学习工作流程。
- en: In [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*,
    we discussed how to choose appropriate models and ML algorithms in order to produce
    better predictive analytics. The model can be presented as a binary or multiclass
    classifier with several classes. Alternatively, use an LDA model for the sentiment
    analysis using the topic modeling concept. In a nutshell, *Figure 1* shows a real-time
    machine learning workflow.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d "第7章。调整机器学习模型")中，*调整机器学习模型*，我们讨论了如何选择适当的模型和ML算法，以产生更好的预测分析。该模型可以呈现为具有多个类的二进制或多类分类器。或者，使用LDA模型进行情感分析，使用主题建模概念。简而言之，*图1*展示了实时机器学习工作流程。
- en: Real-time predictive analytics
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实时预测分析
- en: When your ML model is properly trained and built, your model is ready for doing
    the real-time predictive analytics. If you get a good prediction from the model
    it would be fantastic. However, as we previously mentioned when discussing some
    accuracy issues such as true positives and false positives, if the number of false
    positives is high then that means the performance of the model is not satisfactory.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的ML模型经过适当的训练和构建后，您的模型已经准备好进行实时预测分析。如果您从模型中得到一个好的预测，那将是很棒的。然而，正如我们之前提到的，当讨论一些准确性问题时，例如真正的阳性和假阳性，如果假阳性的数量很高，那意味着模型的性能不尽人意。
- en: 'This essentially means three things: we have not properly labeled the stream
    dataset, in that case iterate step two (labeling the data in *Figure 1*), or have
    not selected the proper ML algorithm to train the model, and finally we have not
    tuned what would eventually help us to find the appropriate hyperparameters or
    model selection; in that case, go straight to step seven (model deployment in
    *Figure 1*).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上意味着三件事：我们没有正确标记流数据集，在这种情况下迭代第二步（标记*图1*中的数据），或者没有选择适当的ML算法来训练模型，最后我们没有调整最终能帮助我们找到适当超参数或模型选择；在这种情况下，直接进入第七步（*图1*中的模型部署）。
- en: Tuning the ML model for improvement and model evaluation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整ML模型以改进和模型评估
- en: As mentioned in step four (model evaluation in *Figure 1*), if the performance
    of the model is not satisfactory or convincing enough, then we need to tune the
    model. As discussed in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*,
    we learned about how to choose the appropriate model and ML algorithms in order
    to produce better predictive analytics. There are several techniques for tuning
    the models, performance and we can go for them based on requirements and the situation.
    When we have done the tuning, finally we should do the model evaluation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第四步（*图1*中的模型评估）中提到的，如果模型的性能不令人满意或令人信服，那么我们需要调整模型。正如在[第7章](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "第7章。调整机器学习模型")中讨论的，我们学习了如何选择适当的模型和ML算法，以产生更好的预测分析。有几种调整模型性能的技术，我们可以根据需求和情况选择其中的一种。当我们完成调整后，最后我们应该进行模型评估。
- en: Model adaptability and deployment
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型的适应性和部署
- en: When we have the tuned and found best model, the machine learning model has
    to be prepared in order to learn incrementally over the new data types when the
    model is updated each time it sees a new training instance. When we have our model
    ready for making the accurate and reliable prediction for the large-scale streaming
    data, we can deploy it in real life.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们拥有调整和找到最佳模型时，机器学习模型必须准备好在每次看到新的训练实例时逐渐学习新的数据类型。当我们的模型准备好为大规模流数据进行准确可靠的预测时，我们可以将其部署到现实生活中。
- en: Time series and social network analysis
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列和社交网络分析
- en: In this section, we will try to provide some insights and challenges of dealing
    and developing a large-scale ML pipeline from the time series and social network
    data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试提供一些见解和挑战，以处理和开发来自时间序列和社交网络数据的大规模ML管道。
- en: Time series analysis
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列分析
- en: Time series data often arises, when, for example, monitoring industrial processes
    or tracking corporate business metrics. One of the fundamental differences between
    modeling data via time series methods is that time series analysis accounts for
    the fact that data points taken over time may have an internal structure.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据通常出现在监控工业过程或跟踪企业业务指标的情况下。通过时间序列方法对数据进行建模的一个基本区别是，时间序列分析考虑到随时间采集的数据点可能具有内部结构。
- en: This might include autocorrelation, trends, or seasonal variation that should
    be taken into account. Regression analysis in this regard is mostly used to test
    the theories. The target is to test to make sure that the current values of one
    or more independent times series parameters are correlated to the current properties
    of other time series data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能包括应该考虑的自相关、趋势或季节性变化。在这方面，回归分析主要用于测试理论。目标是测试以确保一个或多个独立时间序列参数的当前值与其他时间序列数据的当前属性相关联。
- en: To develop large scale predictive analytics applications, time series analysis
    techniques can be applied to real-valued, categorical variables, continuous data,
    discrete numeric data, or even discrete symbolic data. A time series is a sequence
    of floating-point values, each linked to a timestamp. In particular, we try as
    hard as possible to stick with *time series* as meaning a univariate time series,
    although in other contexts, it sometimes refers to a series of multiple values
    at the same timestamp.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发大规模的预测分析应用程序，时间序列分析技术可以应用于实值、分类变量、连续数据、离散数值数据，甚至离散符号数据。时间序列是一系列浮点值，每个值都与时间戳相关联。特别是，我们尽可能坚持*时间序列*的含义是指单变量时间序列，尽管在其他情境中，它有时指的是在同一时间戳上的多个值的系列。
- en: 'An instant in the time series data is the vector of values in a collection
    of time series corresponding to a single point in time. An observation is a tuple
    (timestamp, key, value), that is, a single value in a time series or instant.
    In a nutshell, a time series has mainly four characteristics:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据中的一个瞬时是与单个时间点对应的一系列时间序列中的值向量。一个观察是一个元组（时间戳、键、值），即时间序列或瞬时中的单个值。简而言之，时间序列主要具有四个特征：
- en: '**Series with trends**: Since observations increase or decrease over time,
    although the trends are persistent and have long term movement.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带有趋势的系列**：由于观察结果随时间增加或减少，尽管趋势是持久的并且具有长期运动。'
- en: '**Series data with seasonality**: Since observations stay high then drop off
    and some patterns repeat from one period to the next, and contain regular periodic
    fluctuations, say within a 12 month period.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带有季节性的系列数据**：由于观察结果保持高位然后下降，并且一些模式从一个周期重复到下一个周期，并且包含定期的周期性波动，比如在12个月的周期内。'
- en: '**Series data with cyclic component**: Since the business model changes periodically,
    that is, recessions in the business occur in a cyclic order sometimes. It also
    might contain the repeating swings or movements over more than one year.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带有循环成分的系列数据**：由于业务模型定期变化，即，商业中的衰退有时是循环的。它还可能包含超过一年的重复波动或运动。'
- en: '**Random variation**: An unpredictable component that gives time series graphs
    an irregular or zigzag appearance. It also contains erratic or residual fluctuations.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机变化**：给时间序列图表带来不规则或锯齿状外观的不可预测的组成部分。它还包含不规则或残余的波动。'
- en: Because of these kinds of challenging characteristics, it really becomes difficult
    to develop practical machine learning applications for practical purposes. Hence,
    until now, there is only one package available for time series data analysis,
    developed by Cloudera; it is called the Spark-TS library. Here each time series
    is typically labeled with a key that enables identifying it among a collection
    of time series.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些具有挑战性的特征，为了实际目的开发实际的机器学习应用变得非常困难。因此，到目前为止，只有一个用于时间序列数据分析的软件包可用，由Cloudera开发；它被称为Spark-TS库。这里每个时间序列通常都带有一个键，可以在一系列时间序列中识别它。
- en: However, the current implementation of Spark does not provide any implemented
    algorithm for the time series data analysis. However, since it is an emerging
    and trending topic of interest, hopefully, we will have at least some algorithms
    implemented in Spark in coming releases. In [Chapter 10](part0079_split_000.html#2BASE2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 10.  Configuring and Working with External Libraries"), *Configuring
    and Working with External Libraries*, we will provide more insight into how to
    use these kinds of third-party packages with Spark.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Spark的当前实现没有为时间序列数据分析提供任何实现的算法。然而，由于这是一个新兴和热门的话题，希望在未来的版本中至少会有一些算法在Spark中实现。在[第10章](part0079_split_000.html#2BASE2-0b803698e2de424b8aa3c56ad52b005d
    "第10章。配置和使用外部库")中，*配置和使用外部库*，我们将更深入地介绍如何使用这些类型的第三方包与Spark。
- en: Social network analysis
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社交网络分析
- en: A social network is made up of **nodes** (points) and associated **links**,where
    nodes, links, or edges are then identifiable categories of analysis. These nodes
    might include the information about the people, groups, and organizations. Typically,
    this information is usually the main priority and concern for any type of social
    experimentation and analysis. The links in this kind of analysis focus on the
    collective way to include social contacts and exchangeable information to expand
    social interaction, such as Facebook, LinkedIn, Twitter, and so on. Therefore,
    it is obvious that organizations that are embedded in networks of larger social
    processes, links, and nodes, influence the others.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 社交网络由**节点**（点）和相关**链接**组成，其中节点、链接或边缘是可识别的分析类别。这些节点可能包括有关人、群体和组织的信息。通常，这些信息通常是任何类型的社交实验和分析的主要重点和关注点。这种分析中的链接侧重于以集体方式包括社交联系和可交换信息，以扩大社交互动，例如Facebook、LinkedIn、Twitter等。因此，很明显，嵌入在更大社交过程、链接和节点网络中的组织会影响其他人。
- en: 'On the other hand, according to Otte E.et al. (*Social network analysis: A
    powerful strategy, also for the information sciences*, Journal of Information
    Science, 28: 441-453), **Social Network Analysis** (**SNA**) is the study of finding
    the mapping and measuring the relationships between the connected people or groups.
    It is also used to find the flows between people, groups, organizations, and information
    processing entities.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，根据Otte E.et al.（*社交网络分析：信息科学的强大策略，也用于信息科学*，信息科学杂志，28：441-453），**社交网络分析**（**SNA**）是研究发现连接的人或群体之间关系的映射和测量。它还用于找到人、群体、组织和信息处理实体之间的流动。
- en: 'A proper SNA analysis could be used to show the distinction between the three
    most popular individual centrality measures: degree centrality, betweenness centrality,
    and closeness centrality. **Degree centrality** signifies how many links or incidents
    a node has or how many ties a node has.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 适当的SNA分析可用于显示三种最流行的个体中心性度量之间的区别：度中心性、中介中心性和亲近中心性。**度中心性**表示节点具有多少链接或事件，或者节点有多少联系。
- en: The **betweenness centrality** is a centrality measure of a vertex within a
    graph. This also considers the edge of betweenness. Moreover, the betweenness
    centrality signifies the number of times a node acts as a bridge by considering
    the shortest paths between other nodes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**中介中心性**是图中顶点的中心性度量。这也考虑了中介边缘。此外，中介中心性表示节点作为桥梁的次数，通过考虑其他节点之间的最短路径。'
- en: On the other hand, the **closeness centrality** of a node is the average length
    of the shortest path between a particular node and all other nodes in a connected
    graph, such as a social network.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，节点的**亲近中心性**是连接图中特定节点与所有其他节点之间最短路径的平均长度，例如社交网络。
- en: Tip
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Interested readers are recommended to read more about the eigenvector centrality,
    Katz centrality, PageRank centrality, Percolation centrality, Cross-clique centrality,
    and alpha centrality for the proper understanding of statistical as well as social
    network centrality.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 建议感兴趣的读者阅读更多关于特征向量中心性、Katz中心性、PageRank中心性、渗流中心性、交叉团中心性和alpha中心性，以便正确理解统计和社交网络中心性。
- en: 'The social network is often represented as connected graphs (directed or undirected).
    As a result, it also involves graph data analysis, where people act as nodes and
    the connections or links act as the edges. Moreover, collecting and analyzing
    large-scale data and later on developing predictive and descriptive analytics
    applications from the social network, such as Facebook, Twitter, and LinkedIn
    also involve social network data analysis, including: link prediction such as
    predicting relations or friendship, determining communities in social networks
    such as clustering on graphs, and determining opinion leaders in networks, which
    is essentially a PageRank problem if the proper structure is done on a graph data.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 社交网络通常表示为连接的图（有向或无向）。因此，它还涉及图数据分析，其中人们充当节点，而连接或链接充当边缘。此外，从社交网络收集和分析大规模数据，然后开发预测性和描述性分析应用程序，例如Facebook、Twitter和LinkedIn，也涉及社交网络数据分析，包括：链接预测，如预测关系或友谊，社交网络中的社区确定，如图上的聚类，以及确定网络中的意见领袖，如果在图数据上进行了适当的结构，则本质上是一个PageRank问题。
- en: Spark has its dedicated API for the analysis of graphs, which is called GraphX.
    This API can be used, for instance, to search for spam, rank search results, determine
    communities in social networks, or search for opinion leaders, and it's not a
    complete list of applying methods for analyzing graphs. We will discuss using
    the GraphX later in this chapter in more detail.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有专门用于图分析的API，称为GraphX。例如，可以使用此API搜索垃圾邮件，对搜索结果进行排名，在社交网络中确定社区，或搜索意见领袖，这并不是分析图形的应用方法的完整列表。我们将在本章后面更详细地讨论如何使用GraphX。
- en: Movie recommendation using Spark
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行电影推荐
- en: Model-based collaborative filtering is commonly being used by many companies,
    such as Netflix, as a recommender system for a real-time movie recommendation.
    In this section, we will see a complete example of how it works towards recommending
    movies for new users.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的协同过滤通常被许多公司使用，比如Netflix，作为实时电影推荐的推荐系统。在本节中，我们将看到一个完整的示例，说明它是如何为新用户推荐电影的。
- en: Model-based movie recommendation using Spark MLlib
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark MLlib进行基于模型的电影推荐
- en: 'The implementation in Spark MLlib supports the model-based collaborative filtering.
    In the model based collaborative filtering technique, users and products are described
    by a small set of factors, also called the **latent factors** (**LFs**).The LFs
    are then used for predicting the missing entries. Spark API provides the implementation
    of the **Alternating Least Squares** (also known as the **ALS** widely) algorithm,
    which is used to learn these latent factors by considering six parameters, including:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib中的实现支持基于模型的协同过滤。在基于模型的协同过滤技术中，用户和产品由一小组因子描述，也称为**潜在因子**（**LFs**）。然后使用这些LFs来预测缺失的条目。Spark
    API提供了**交替最小二乘**（也被广泛称为**ALS**）算法的实现，用于通过考虑六个参数来学习这些潜在因子，包括：
- en: '`numBlocks`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numBlocks`'
- en: '`rank`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank`'
- en: '`iterations`'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iterations`'
- en: '`lambda`'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lambda`'
- en: '`implicitPrefs`'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`implicitPrefs`'
- en: '`alpha`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`'
- en: 'To learn more about these parameters, refer to the recommendation system section
    in [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples*. Note that to construct an ALS instance with
    default parameters, you can set the value based on your requirements. The default
    values are as follows: `numBlocks`: -1, `rank`: 10, `iterations`: 10, `lambda`:
    0.01, `implicitPrefs`: false, `alpha`: 1.0.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于这些参数的信息，请参考[第5章](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "第5章。通过示例进行监督和无监督学习")中的推荐系统部分，*通过示例进行监督和无监督学习*。请注意，要使用默认参数构建ALS实例，可以根据您的需求设置值。默认值如下：`numBlocks`：-1，`rank`：10，`iterations`：10，`lambda`：0.01，`implicitPrefs`：false，`alpha`：1.0。
- en: 'The construction of an ALS instance, in short, is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，构建ALS实例的方法如下：
- en: At first, the ALS, which is an iterative algorithm, is used to model the rating
    matrix as the multiplication of low-ranked users and product factors
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，ALS是一个迭代算法，用于将评分矩阵建模为低排名用户和产品因子的乘积
- en: After that, the learning task is done by using these factors by minimizing the
    reconstruction error of the observed ratings
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，通过最小化观察到的评分的重建误差，使用这些因子进行学习任务。
- en: However, the unknown ratings can successively be calculated by multiplying these
    factors together.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，未知的评分可以通过将这些因子相乘来逐步计算。
- en: The approach for a movie recommendation or any other recommendation based on
    the collaborative filtering technique used in the Spark MLlib has been proven
    to be a high performer with high prediction accuracy and scalable for the billions
    of ratings on commodity clusters used by companies such as Netflix. By following
    this approach, a company such as Netflix can recommend movies to its subscriber
    based on the predicted ratings. The ultimate target is to increase the sales,
    and of course, the customer satisfaction.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Spark MLlib中使用的协同过滤技术进行电影推荐或其他推荐的方法已被证明具有高预测准确性，并且在像Netflix这样的公司使用的商品集群上可扩展到数十亿的评分。通过遵循这种方法，Netflix这样的公司可以根据预测的评分向其订阅者推荐电影。最终目标是增加销售，当然也是客户满意度。
- en: Data exploration
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据探索
- en: The movie and the corresponding rating dataset were downloaded from the MovieLens
    website ([https://movielens.org](https://movielens.org/movies/1)). According to
    the data description on the MovieLens website, all the ratings are described in
    the `ratings.csv` file. Each row of this file followed by the header represents
    one rating for one movie by one user.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 电影及其对应的评分数据集是从MovieLens网站([https://movielens.org](https://movielens.org/movies/1))下载的。根据MovieLens网站上的数据描述，所有评分都在`ratings.csv`文件中描述。该文件的每一行（包括标题）代表一个用户对一部电影的评分。
- en: 'The CSV dataset has the following columns: `userId`, `movieId`, `rating`, and
    `timestamp` as shown in *Figure 2*. The rows are ordered first by the `userId`
    within the user, by `movieId`. Ratings are made on a five-star scale; with half-star
    increments (0.5 stars up to 5.0 stars). The timestamps represent the seconds since
    midnight **Coordinated Universal Time** (**UTC**) of January 1, 1970, where we
    have 105,339 ratings from the 668 users on 10,325 movies:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: CSV数据集包含以下列：`userId`，`movieId`，`rating`和`timestamp`，如*图2*所示。行首先按`userId`在用户内部排序，然后按`movieId`排序。评分采用五星制，可以增加0.5星（0.5星至5.0星）。时间戳表示自1970年1月1日**协调世界时**（**UTC**）午夜以来的秒数，我们有105,339个评分，来自668个用户对10,325部电影进行评分：
- en: '![Data exploration](img/00123.jpeg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![数据探索](img/00123.jpeg)'
- en: 'Figure 2: Sample ratings for the top 20 movies.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：前20部电影的样本评分。
- en: 'On the other hand, the movie information is contained in the `movies.csv` file.
    Each row apart from the header information represents one movie containing the
    columns: `movieId`, `title`, and `genres`.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，电影信息包含在`movies.csv`文件中。除了标题信息之外，每一行代表一个电影，包含列：`movieId`，`title`和`genres`。
- en: Movie titles are either created or inserted manually or imported from the website
    of the movie database at [https://www.themoviedb.org/](https://www.themoviedb.org/).
    The release year, however, is shown in the bracket.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 电影标题可以手动创建、插入，或者从[https://www.themoviedb.org/](https://www.themoviedb.org/)电影数据库网站导入。然而，发行年份显示在括号中。
- en: Since movie titles are inserted manually, some errors or inconsistencies may
    exist in these titles. Readers are therefore recommended to check the IMDb database
    ([http://www.ibdb.com/](http://www.ibdb.com/)) to make sure if there are no inconsistencies
    or incorrect titles with their corresponding release year.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于电影标题是手动插入的，因此这些标题可能存在错误或不一致。因此，建议读者检查IMDb数据库（[http://www.ibdb.com/](http://www.ibdb.com/)）以确保没有不一致或不正确的标题及其对应的发行年份。
- en: 'Genres are a separated list, and are selected from the following genre categories:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 类型是一个分隔的列表，从以下类型类别中选择：
- en: Action, Adventure, Animation, Children's, Comedy, Crime
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作，冒险，动画，儿童，喜剧，犯罪
- en: Documentary, Drama, Fantasy, Film-Noir, Horror, Musical
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纪录片，戏剧，奇幻，黑色电影，恐怖，音乐
- en: Mystery, Romance, Sci-Fi, Thriller, Western, War
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神秘，浪漫，科幻，惊悚，西部，战争
- en: '![Data exploration](img/00065.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![数据探索](img/00065.jpeg)'
- en: 'Figure 3: The title and genres for the top 20 movies.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：前20部电影的标题和类型。
- en: Movie recommendation using Spark MLlib
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark MLlib进行电影推荐
- en: In this subsection, we will show you how to recommend the movie for other users
    through a step-by-step example from data collection to movie recommendation. Download
    the `movies.csv` and `ratings.csv` files from Packt supplementary documents and
    place them in your project directory.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将通过逐步示例从数据收集到电影推荐来向您展示如何为其他用户推荐电影。从Packt的补充文件中下载`movies.csv`和`ratings.csv`文件，并将它们放在您的项目目录中。
- en: '**Step 1: Configure your Spark environment**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：配置您的Spark环境**'
- en: 'Here is the code to configure your Spark environment:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是配置您的Spark环境的代码：
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2: Load, parse, and explore the movie and rating Dataset**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：加载、解析和探索电影和评分数据集**'
- en: 'Here is the code illustrated:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码示例：
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This code segment should return you the `Dataset<Row>` of the ratings same
    as in *Figure 2*. On the other hand, the following code segment shows you the
    `Dataset<Row>` of movies, same as in *Figure 3*:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码应该返回与*图2*中相同的评分`Dataset<Row>`。另一方面，以下代码段显示了与*图3*中相同的电影`Dataset<Row>`：
- en: '[PRE2]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Step 3: Register both Datasets as temp tables**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：将两个数据集都注册为临时表**'
- en: 'To register both Datasets, we can use the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要注册这两个数据集，我们可以使用以下代码：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will help to make the in-memory querying faster by creating a temporary
    view as a table in min-memory. The lifetime of the temporary table using the `createOrReplaceTempView()`
    method is tied to the `[[SparkSession]]` that was used to create this Dataset.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这将通过在内存中创建一个临时视图作为表来加快内存查询的速度。使用`createOrReplaceTempView()`方法创建的临时表的生命周期与用于创建此数据集的`[[SparkSession]]`相关联。
- en: '**Step 4: Explore and query for related statistics**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：探索和查询相关统计数据**'
- en: 'Let''s check the ratings related statistics. Just use the following code lines:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来检查与评分相关的统计数据。只需使用以下代码行：
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You should find 105,339 ratings from 668 users on 10,325 movies. Now, let's
    get the maximum and minimum ratings along with the count of users who have rated
    a movie.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该发现来自668个用户对10,325部电影的105,339个评分。现在，让我们获取最高和最低评分以及对电影进行评分的用户数量。
- en: However, you need to perform a SQL query on the rating table we just created
    in-memory in the previous step. Making a query here is simple, and it is similar
    to making a query from a MySQL database or RDBMS.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您需要在上一步中刚刚在内存中创建的评分表上执行SQL查询。在这里进行查询很简单，类似于从MySQL数据库或RDBMS进行查询。
- en: However, if you are not familiar with SQL-based queries, you are suggested to
    look at the SQL query specification to find out how to perform a selection using
    `SELECT` from a particular table, how to perform the ordering using `ORDER`, and
    how to perform a joining operation using the `JOIN` keyword.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您不熟悉基于SQL的查询，建议您查看SQL查询规范，以了解如何使用`SELECT`从特定表中进行选择，如何使用`ORDER`进行排序，以及如何使用`JOIN`关键字进行连接操作。
- en: 'Well, if you know the SQL query, you should get a new Dataset by using a complex
    SQL query as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您了解SQL查询，您应该通过使用以下复杂的SQL查询得到一个新的数据集：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Movie recommendation using Spark MLlib](img/00108.jpeg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行电影推荐](img/00108.jpeg)'
- en: 'Figure 4: Maximum and minimum ratings along with the count of users who have
    rated a movie.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：最高和最低评分以及对电影进行评分的用户数量。
- en: 'To get an insight, we need to know more about the users and their ratings.
    Now let''s find the top most active users and how many times they rated a movie:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地了解，我们需要了解更多关于用户及其评分的信息。现在让我们找出最活跃的用户以及他们对电影进行评分的次数：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Movie recommendation using Spark MLlib](img/00135.jpeg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行电影推荐](img/00135.jpeg)'
- en: 'Figure 5: Number of ratings provided by an individual user.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：单个用户提供的评分数量。
- en: 'Now let''s have a look at a particular user, and find the movies that, say
    user 668, rated higher than 4:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看特定用户，并找出比如说用户668给出了高于4分的电影：
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Movie recommendation using Spark MLlib](img/00146.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行电影推荐](img/00146.jpeg)'
- en: 'Figure 6: The related rating for user 668.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：用户668的相关评分。
- en: '**Step 5: Prepare training and test rating data and see the counts**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：准备训练和测试评分数据并查看计数**'
- en: 'Here is the code illustrated:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码示例：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You should find that there are 84,011 ratings in the training and 21,328 ratings
    in the test Dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该发现在训练集中有84,011个评分，在测试集中有21,328个评分。
- en: '**Step 6: Prepare the data for building the recommendation model using ALS**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：准备数据以构建使用ALS的推荐模型**'
- en: 'The following code illustrates for building the recommendation model using
    APIs:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例用于构建使用API的推荐模型：
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `ratingsRDD` RDD will contain the `userId`, `movieId`, and corresponding
    ratings from the training dataset that we prepared in the previous step. On the
    other hand, the following `testRDD` also contains the same information coming
    from the test Dataset we prepared in the previous step:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`ratingsRDD` RDD将包含来自我们在上一步准备的训练数据集的`userId`、`movieId`和相应的评分。另一方面，以下`testRDD`也包含来自我们在上一步准备的测试数据集的相同信息：'
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Step 7: Build an ALS user product matrix**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：构建ALS用户产品矩阵**'
- en: 'Build an ALS user matrix model based on the `ratingsRDD` by specifying the
    rank, iterations, and lambda:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 基于`ratingsRDD`构建一个ALS用户矩阵模型，指定rank、iterations和lambda：
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that we have randomly selected the value of rank as `20` and have iterated
    the model for learning for 10 times and the lambda as `0.01`. With this setting,
    we got a good prediction accuracy. Readers are suggested to apply the hyper-parameter
    tuning to get to know the most optimum values for these parameters.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，我们已随机选择了rank的值为`20`，并且已对模型进行了10次学习迭代，lambda为`0.01`。通过这个设置，我们得到了良好的预测准确性。建议读者应用超参数调整来了解这些参数的最佳值。
- en: However, readers are suggested to change the value of these two parameters based
    on their dataset. Moreover, as mentioned earlier, they also can use and specify
    other parameters such as `numberblock`, `implicitPrefs`, and `alpha` if the prediction
    performance is not satisfactory. Furthermore, set the number of blocks for both
    user blocks and product blocks to parallelize the computation into `pass -1` for
    an auto-configured number of blocks. The value is `-1`.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，建议读者根据其数据集更改这两个参数的值。此外，如前所述，他们还可以使用和指定其他参数，如`numberblock`、`implicitPrefs`和`alpha`，如果预测性能不理想。此外，将用户块和产品块的块数设置为`pass
    -1`，以将计算并行化为自动配置的块数。该值为`-1`。
- en: '**Step 8: Making predictions**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**第8步：进行预测**'
- en: 'Let''s get the top six movie predictions for user 668:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为用户668获取前六部电影的预测：
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Movie recommendation using Spark MLlib](img/00145.jpeg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行电影推荐](img/00145.jpeg)'
- en: 'Figure 7: Top six movies rated by the user 668.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：用户668评分最高的六部电影。
- en: '**Step 9: Get the predicted ratings to compare with the test ratings**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 第9步：获取预测评分以与测试评分进行比较
- en: 'Here is the code illustrated:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是所示的代码：
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let''s check the top 10 prediction for 10 users:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们检查10个用户的前10个预测：
- en: '[PRE14]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Step 10: Prepare predictions**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 第10步：准备预测
- en: 'Here we will prepare the predictions related to RDDs in two steps. The first
    step includes preparing predictions for comparison from the `predictionsForTestRDD`
    RDD structure. It goes as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将以两个步骤准备与RDD相关的预测。第一步包括从`predictionsForTestRDD` RDD结构中准备比较的预测。步骤如下：
- en: '[PRE15]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The second step includes preparing the test for comparison:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步包括准备测试以进行比较：
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Step 11: Join the test with predictions and see the combined ratings**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 第11步：将测试与预测结合起来，看看综合评分
- en: 'Join `testKeyedByUserProductRDD` and `predictionsKeyedByUserProductRDD` RDDs
    to get the combined test as well as predicted ratings against each user and `movieId`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 将`testKeyedByUserProductRDD`和`predictionsKeyedByUserProductRDD` RDD连接起来，以获取每个用户和`movieId`的组合测试以及预测评分：
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Movie recommendation using Spark MLlib](img/00076.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark MLlib进行电影推荐](img/00076.jpeg)'
- en: 'Figure 8: Combined test as well as predicted ratings against each user and
    movieId.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：组合测试以及对每个用户和电影ID的预测评分。
- en: '**Step 12: Evaluating the model against prediction performance**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**第12步：评估模型对预测性能的表现**'
- en: 'Let''s check the performance of the ALS model by checking the number of true
    positives and false positives:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查真正的正面预测和假阳性的数量来检查ALS模型的性能：
- en: '[PRE18]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now print the number of true positive predictions. We have considered that
    a prediction is a true prediction when the predicted rating is less than the highest
    rating (that is, `5`). Consequently, if the predicted rating is more or equal
    to `5`, consider that prediction as a false positive:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打印真正的正面预测数量。我们认为当预测评分低于最高评分（即`5`）时，预测是真正的预测。因此，如果预测评分大于或等于`5`，则将该预测视为假阳性：
- en: '[PRE19]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You should find the value as follows.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该找到以下值。
- en: 'The number of true positive prediction is 798\. Now it''s time to print the
    statistics for the false positives:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的正面预测数量为798。现在是时候打印假阳性的统计数据了：
- en: '[PRE20]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In this particular example, we have got only 14 false positive predictions,
    which is outstanding. Now let''s check the performance of the prediction in terms
    of mean absolute error calculation between the test and predictions:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的例子中，我们只有14个假阳性预测，这是非常出色的。现在让我们通过测试和预测之间的平均绝对误差计算来检查预测的性能：
- en: '[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Which returns the value as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下值：
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Developing a real-time ML pipeline from streaming
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从流构建实时ML管道
- en: According to the API guidelines provided by Spark at [http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html),
    technically, Spark Streaming receives live input data streams as objects (objects
    could be `Java/Python/R` objects). Later on, the streams are divided into batches,
    which are then processed by the Spark engine to generate the final input stream
    in batches. To make this process even easier, Spark Streaming provides a high-level
    abstraction, which is also called a discretized stream or DStream.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Spark在[http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)提供的API指南，技术上，Spark
    Streaming接收实时输入数据流作为对象（对象可以是`Java/Python/R`对象）。随后，流被分成批次，然后由Spark引擎处理以生成批处理的最终输入流。为了使这个过程更加简单，Spark
    Streaming提供了一个高级抽象，也称为离散流或DStream。
- en: The **DStream** represents a continuous stream of data coming from real-time
    streaming sources such as Twitter, Kafka, Fume, Kinesis, Sensors, or any other
    sources. Discretized streams can be created from these sources, alternatively,
    high-level operations on other DStreams can also be applied for doing that. Internally,
    a DStream is represented as a sequence of RDDs, that means the RDD abstraction
    has been reused for processing the stream of RDDs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**DStream**代表来自实时流数据源（如Twitter、Kafka、Fume、Kinesis、传感器或其他源）的连续数据流。可以从这些源创建离散流，或者也可以对其他DStreams应用高级操作来进行处理。在内部，DStream被表示为一系列RDD，这意味着RDD抽象已被重用以处理RDD流。'
- en: As already discussed in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Large Scale
    Machine Learning Pipelines*, a topic modeling technique automatically infers the
    topics discussed and inherently places them in a collection of documents as hidden
    resources. This is commonly used in the **Natural Language Processing** (**NLP**)
    and text mining tasks. These topics can be used to analyze, summarize, and organize
    those documents. Alternatively, those topics can be used for featurization and
    dimensionality reduction in later stages of a **machine learning** (**ML**) pipeline
    development. The most popular topic modeling algorithms are **Latent Dirichlet
    Allocation** (**LDA**) and **Probabilistic Latent Semantic Analysis** (**pLSA**).
    Previously we discussed how to apply the LDA algorithm for the static dataset
    that is already available. However, if the topic modeling is prepared from the
    real-time streaming data, that would be great and would be more live in knowing
    the trends in social media such as Twitter, LinkedIn, or Facebook.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如已在[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d "第6章。构建可扩展的机器学习管道")中讨论的*构建大规模机器学习管道*，主题建模技术会自动推断讨论的主题，并将它们隐含地放置在文档集合中作为隐藏资源。这在**自然语言处理**（**NLP**）和文本挖掘任务中常用。这些主题可以用于分析、总结和组织这些文档。或者，这些主题可以在**机器学习**（**ML**）管道开发的后期阶段用于特征化和降维。最流行的主题建模算法是**潜在狄利克雷分配**（**LDA**）和**概率潜在语义分析**（**pLSA**）。之前我们讨论了如何应用LDA算法来处理已有的静态数据集。然而，如果主题建模是从实时流数据中准备的，那将会很棒，并且在了解Twitter、LinkedIn或Facebook等社交媒体趋势方面更加实时。
- en: 'However, due to the limited API facility by Facebook or LinkedIn, it would
    be difficult to collect the real-time data from those social media platforms.
    Spark also provides the API for accessing data from Twitter, Kafka, Flume, and
    Kinesis. The workflow of a near real-time ML application development from streaming
    data should follow the workflows as presented in *Figure 9*:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于Facebook或LinkedIn的有限API功能，从这些社交媒体平台收集实时数据将会很困难。Spark还提供了用于访问来自Twitter、Kafka、Flume和Kinesis的数据的API。从流数据开发近实时ML应用程序的工作流程应遵循*图9*中呈现的工作流程：
- en: '![Developing a real-time ML pipeline from streaming](img/00129.jpeg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![从流中开发实时ML管道](img/00129.jpeg)'
- en: 'Figure 9: Real-time predictive ML model development from streaming data using
    Spark.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：使用Spark从流数据开发实时预测ML模型。
- en: 'In this section, we will show you how to develop a real-time ML pipeline that
    handles streaming data. More specifically, we will show a step-by-step topic modeling
    from Twitter streaming data. The topic modeling here has two steps: Twitter data
    collection and topic modeling using LDA.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示如何开发一个处理流数据的实时ML管道。更具体地说，我们将展示从Twitter流数据中逐步进行主题建模。这里的主题建模有两个步骤：Twitter数据收集和使用LDA进行主题建模。
- en: Real-time tweet data collection from Twitter
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Twitter收集实时推文数据
- en: 'Spark provides APIs to access real-time tweets from the Twitter timeline. The
    tweets can be further made by using keywords or hashtags. Alternatively, tweets
    can also be downloaded from someone''s Twitter timeline. However, before accessing
    the tweets data, you will have to create a sample Twitter application on Twitter
    and generate four keys: `consumerKey`, `consumerSecret`, `accessToken`, and `accessTokenSecret`.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了API来访问来自Twitter时间线的实时推文。这些推文可以通过关键词或标签进一步制作。或者，也可以从某人的Twitter时间线下载推文。但是，在访问推文数据之前，您需要在Twitter上创建一个样本Twitter应用程序，并生成四个密钥：`consumerKey`、`consumerSecret`、`accessToken`和`accessTokenSecret`。
- en: Tip
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Note that Twitter (and some other) driver support has been removed during Spark
    upgrade from 1.6.2 to 2.0.0\.  This means streaming data collection support using
    Spark from less used streaming connectors, including Twitter, Akka, MQTT, and
    ZeroMQ has been removed in Spark 2.0.0\. Therefore, it i€™s not possible to develop
    an application for Twitter data collection using Spark 2.0.0\. Consequently, Spark
    1.6.1 will be used for the demonstration for Twitter data collection in this section.
    Readers are suggested to create a Maven project on Eclipse using the provided
    Maven friendly `pom.xml` file.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，从Spark 1.6.2升级到2.0.0时，Twitter（和其他一些）驱动程序支持已被移除。这意味着在Spark 2.0.0中已删除了使用Spark进行流数据收集支持的较少使用的流连接器，包括Twitter、Akka、MQTT和ZeroMQ。因此，不可能使用Spark
    2.0.0开发用于Twitter数据收集的应用程序。因此，在本节中将使用Spark 1.6.1进行Twitter数据收集的演示。建议读者使用提供的Maven友好的`pom.xml`文件在Eclipse上创建一个Maven项目。
- en: After authenticating your Spark ML application to collect data from Twitter,
    you will have to define the `JavaStreamingContext` by specifying the `SparkConf`
    and duration for collecting tweets. After that, tweets data can be downloaded
    as DStream or discrete stream through the `TwitterUtils` API of Spark once you
    revoke the `start()` method using the `JavaStreamingContext` object.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在对Spark ML应用程序进行身份验证以收集来自Twitter的数据之后，您将需要通过指定`SparkConf`和收集推文的持续时间来定义`JavaStreamingContext`。之后，通过使用Spark的`TwitterUtils`
    API，推文数据可以作为DStream或离散流下载，一旦使用`JavaStreamingContext`对象撤销`start()`方法。
- en: Upon starting to receive the tweets, you can save the tweets data on your local
    machine or HDFS or any other filesystem where applicable. However, the streaming
    will be continued until you terminate the streaming using `awaitTermination()`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 开始接收推文后，您可以将推文数据保存在本地计算机、HDFS或其他适用的文件系统上。但是，流将持续进行，直到您使用`awaitTermination()`终止流。
- en: The received tweets, however, can be also be pre-processed or cleaned by using
    the `foreachRDD` design pattern and then can be saved to your desired location.
    Due to page limitation, we have limited our discussion here.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，接收到的推文也可以通过使用`foreachRDD`设计模式进行预处理或清理，然后可以保存到您想要的位置。由于页面限制，我们在这里限制了我们的讨论。
- en: Tip
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Moreover, interested readers should follow the API guidelines for Spark Streaming
    in the following web page of Spark:[http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，感兴趣的读者应该遵循Spark Streaming的API指南，网页地址为：[http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)。
- en: Tweet collection using TwitterUtils API of Spark
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark的TwitterUtils API进行推文收集
- en: In this sub-section, at first, we will show you how to collect real-time tweets
    data from Twitter using the `TwitterUtils` API. Then the same tweets data will
    be used for topic modeling in the next sub-section.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小节中，首先，我们将向您展示如何使用`TwitterUtils` API从Twitter收集实时推文数据。然后相同的推文数据将在下一个小节中用于主题建模。
- en: '**Step 1: Load required packages and APIs**'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：加载所需的包和API**'
- en: 'Here is the code to load the required packages:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载所需包的代码：
- en: '[PRE23]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Step 2: Setting the Logger level**'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：设置日志记录器级别**'
- en: 'For setting the Logger level, we use the following code:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置日志记录器级别，我们使用以下代码：
- en: '[PRE24]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Step 3: Spark streaming environment setting**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：设置Spark流环境**'
- en: 'Here is the codes for Spark streaming illustrated:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Spark流的代码示例：
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Step 4: Setting the authentication for accessing Twitter data**'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：设置访问Twitter数据的身份验证**'
- en: 'Get the authentications values from the sample Twitter application by visiting
    the following URL: [https://apps.twitter.com/](https://apps.twitter.com/):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问以下网址从示例Twitter应用程序获取认证值：[https://apps.twitter.com/](https://apps.twitter.com/)：
- en: '[PRE26]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Note that here we have provided the same values for these four secret keys.
    Replace these values with your own keys accordingly. Well, now we need to set
    the system property using `twitter4j.oauth` for the previous four keys:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里我们为这四个秘密密钥提供了相同的值。请根据自己的密钥替换这些值。现在，我们需要使用`twitter4j.oauth`为前面的四个密钥设置系统属性：
- en: '[PRE27]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Step 5: Enable the check pointing**'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：启用检查点**'
- en: 'The following code shows how to enable the check pointing:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了如何启用检查点：
- en: '[PRE28]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The metadata check pointing is primarily needed for recovery from driver failures,
    whereas data or RDD check pointing is necessary even for basic functioning if
    stateful transformations are used. For more details, please visit the following
    web page of Spark: [http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing](http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据检查点主要用于从驱动程序故障中恢复，而数据或RDD检查点即使在使用有状态转换时也是基本功能所必需的。有关更多详细信息，请访问Spark的以下网页：[http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing](http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing)。
- en: '**Step 6: Start accepting stream of tweets as a discrete stream**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：开始接受推文流作为离散流**'
- en: 'Let''s collect only 100 tweets for simplicity and but can collect as much as
    you want:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们只收集100条推文以简化，但可以收集任意数量的推文：
- en: '[PRE29]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Step 7: Filter tweets and save as regular text files**'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：筛选推文并保存为普通文本文件**'
- en: 'The code for filter tweets is shown here:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 筛选推文的代码显示在这里：
- en: '[PRE30]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here we are pre-processing the tweets using the singleton method, `foreachRDD`,
    which accepts only filtered tweets, that is, if the status count is at least 1\.
    When the number of collected tweets is equal or more than the number of tweets
    to be collected, then we exit the collection. Finally, we save the tweets as texts
    in the output directory.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用单例方法`foreachRDD`对推文进行预处理，该方法仅接受经过筛选的推文，即，如果状态计数至少为1。当收集的推文数量等于或多于要收集的推文数量时，我们退出收集。最后，我们将推文保存为文本文件在输出目录中。
- en: '**Step 8: Controlling the streaming switch**'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：控制流开关**'
- en: 'The code for controlling the streaming switch is shown here:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 控制流开关的代码显示在这里：
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Eventually, we will use these texts of tweets for the topic modeling in the
    next step. If you recall the topic modeling in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*, we saw the corresponding term weight, topic name,
    and term indices. However, we also need to have the actual terms. In the next
    step, we will show the detailed technique of retrieving the terms extensively
    dependent on the vocabulary that needs to be created for that.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将在下一步中使用这些推文的文本进行主题建模。如果您还记得[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "第6章。构建可扩展的机器学习管道")中的主题建模，*构建可扩展的机器学习管道*，我们看到了相应的术语权重、主题名称和术语索引。然而，我们还需要实际的术语。在下一步中，我们将展示详细的检索术语的技术，这取决于需要为此创建的词汇表。
- en: Topic modeling using Spark
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark进行主题建模
- en: In this sub-section, we represented a semi-automated technique of topic modeling
    using Spark. The following steps show the topic modeling from data reading to
    printing the topics along with their term-weights.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小节中，我们使用Spark表示了一种半自动的主题建模技术。以下步骤展示了从数据读取到打印主题及其术语权重的主题建模。
- en: '**Step 1: Load necessary packages and APIs**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：加载必要的包和API**'
- en: 'Here is the code to load the necessary packages:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载必要包的代码：
- en: '[PRE32]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '**Step 2: Configure the Spark environment**'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：配置Spark环境**'
- en: 'Here is the code to configure the Spark:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是配置Spark的代码：
- en: '[PRE33]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**Step 3: Setting the logging level**'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：设置日志级别**'
- en: 'Here is the code to set the logging level:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是设置日志级别的代码：
- en: '[PRE34]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that setting the logging level that was just previously shown is optional.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，设置刚刚显示的日志级别是可选的。
- en: '**Step 4: Create Java RDD and cache them in-memory**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：创建Java RDD并将它们缓存在内存中**'
- en: 'Create Java RDD and cache them for the tweets data from the previous step:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Java RDD并将它们缓存在上一步的推文数据中：
- en: '[PRE35]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '**Step 5: Tokenize the terms**'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：对术语进行标记化**'
- en: 'Load the list of stop-words provided by Spark and tokenize the terms by filtering
    them by applying three constraints: text length at least 4, not a stop word, and
    making them each lower case. Note that we discussed stop-words in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 加载Spark提供的停用词列表，并通过应用三个约束条件对术语进行标记化：文本长度至少为4，不是停用词，并将它们转换为小写。请注意，我们在[第6章](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "第6章。构建可扩展的机器学习管道")中讨论了停用词，*构建可扩展的机器学习管道*：
- en: '[PRE36]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '**Step 6: Prepare the term counts**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：准备术语计数**'
- en: 'Prepare the term counts by filtering them by applying four constraints: text
    length at least 4, not stop words, selecting only characters, and making them
    all lower case:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用四个约束条件对术语计数进行准备：文本长度至少为4，不是停用词，仅选择字符，并将它们全部转换为小写：
- en: '[PRE37]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note that here `isStopWords()` and `isOnlyLetters()` are two user defined methods
    that will be discussed at the end of this step.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里的`isStopWords()`和`isOnlyLetters()`是两个用户定义的方法，将在本步骤末尾讨论。
- en: '**Step 7: Sort the term counts**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：对术语计数进行排序**'
- en: 'Sort the terms counts by applying two transformations, `sortByKey()` and `mapToPair()`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用两个转换`sortByKey()`和`mapToPair()`对术语计数进行排序：
- en: '[PRE38]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '**Step 8: Create the vocabulary**'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8：创建词汇表**'
- en: 'Create a vocabulary RDD by mapping the sorted term counts. Finally, print the
    key value pairs:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 通过映射排序的术语计数创建词汇表RDD。最后，打印键值对：
- en: '[PRE39]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s see a screenshot of the vocabulary terms and their indices shown in
    *Figure 10*:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下*图10*中显示的词汇表术语及其索引的屏幕截图：
- en: '![Topic modeling using Spark](img/00042.jpeg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark进行主题建模](img/00042.jpeg)'
- en: 'Figure 10: Vocabulary terms and their indices.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：词汇表术语及其索引。
- en: '**Step 9: Create the document matrix from the tokenized words/terms**'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9：从标记化的单词/术语中创建文档矩阵**'
- en: 'Create the document matrix as `JavaPairRDD` from the tokenized terms by mapping
    the vocabulary we created in the previous step. After that, cache the RDD in-memory
    for faster processing:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 通过映射我们在上一步中创建的词汇表，将文档矩阵创建为`JavaPairRDD`。之后，将RDD缓存在内存中以加快处理速度：
- en: '[PRE40]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '**Step 10: Train the LDA model**'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤10：训练LDA模型**'
- en: Train the LDA model using the documents matrix from step 9 and describe 10 topic
    terms against four topics for simplicity.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用步骤9中的文档矩阵训练LDA模型，并简要描述四个主题的10个主题术语。
- en: Note that here we have used **Latent Dirichilet Allocation** (**LDA**), which
    is one of the most popular topic modeling algorithms commonly used for text mining.
    We could use more robust topic modeling algorithms such as **Probabilistic Latent
    Sentiment Analysis** (**pLSA**), **Pachinko Allocation Model** (**PAM**), or **Hierarchical
    Dirichilet Process** (**HDP**) algorithms. However, pLSA has the overfitting problem.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里我们使用了**潜在狄利克雷分配**（**LDA**），这是最常用于文本挖掘的主题建模算法之一。我们可以使用更健壮的主题建模算法，如**概率潜在情感分析**（**pLSA**）、**Pachinko分配模型**（**PAM**）或**分层狄利克雷过程**（**HDP**）算法。但是，pLSA存在过拟合问题。
- en: 'On the other hand, both HDP and PAM are more complex topic modeling algorithms
    used for complex text mining such as mining topics from high dimensional text
    data or documents of unstructured text. Moreover, to this date, Spark has implemented
    only one topic modeling algorithm, that is LDA. Therefore, we have to use LDA
    reasonably:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，HDP和PAM是更复杂的主题建模算法，用于复杂的文本挖掘，例如从高维文本数据或非结构化文档中挖掘主题。此外，迄今为止，Spark仅实现了一种主题建模算法，即LDA。因此，我们必须合理使用LDA：
- en: '[PRE41]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that to keep the topic generation simple, we have set the number of the
    topic as 4 and have iterated the LDA 10 times. Another reason is that in the next
    section we want to show how to connect these four topics through their common
    terms. Readers are recommended to change the value based on their requirements.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了使主题生成简单，我们将主题数量设置为4，并迭代LDA 10次。另一个原因是，在下一节中，我们想展示如何通过它们的共同术语连接这四个主题。建议读者根据自己的需求更改值。
- en: '**Step 11: Get the topic terms, index, term weights, and total sum across each
    topic**'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤11：获取主题术语、索引、术语权重和每个主题的总和**'
- en: 'Get these statistics from the vocabulary and topic description described in
    step 10 and step 8:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 从步骤10和步骤8中描述的词汇表和主题描述中获取这些统计信息：
- en: '[PRE42]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'If you look at the preceding code segment carefully, the `vocabKey` indicates
    the corresponding topic term, `vocabIndex` is the index, and `prob` indicates
    the term weight for each term in a topic. The print statements have been used
    to format the outputs. Now let''s see the output that describes four topics for
    simplicity in *Figure 11*:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细查看前面的代码段，`vocabKey`表示相应的主题术语，`vocabIndex`是索引，`prob`表示主题中每个术语的权重。打印语句已用于格式化输出。现在让我们看一下描述四个主题的输出，以简化*图11*：
- en: '![Topic modeling using Spark](img/00098.jpeg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark进行主题建模](img/00098.jpeg)'
- en: 'Figure 11: Describing four topics.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：描述四个主题。
- en: 'As we mentioned in *step 6*, here we will show how we develop the `isStopWord()`
    method. Just use the following code:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*步骤6*中提到的，这里我们将展示如何开发`isStopWord()`方法。只需使用以下代码：
- en: '[PRE43]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'And the `isOnlyLetters()` method goes as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`isOnlyLetters()`方法如下：'
- en: '[PRE44]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In the next section, we will introduce how to parse and handle large-scale graph
    data using GraphX API of Spark to find the connected components from the topics
    data that we got from *Figure 10*.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍如何使用Spark的GraphX API解析和处理大规模图形数据，以从*图10*中得到的主题数据中找到连接的组件。
- en: ML pipeline on graph data and semi-supervised graph-based learning
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图数据上的ML管道和半监督图形学习
- en: Due to the big data deluge, there has been a large amount of unlabeled data,
    and very small amounts of labeled data. As already discussed, labeling and annotating
    this data is computationally expensive and an obstacle in finding real insight
    from the data. Also, the increasing growth of the social network and media producers
    graph data at scale. These data thrive to develop real-time and large-scale supervised
    learning methods that can use information in the input distribution.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大数据的泛滥，存在大量未标记的数据，以及非常少量的标记数据。正如已经讨论的那样，对这些数据进行标记和注释在计算上是昂贵的，并且是发现数据真正见解的障碍。此外，社交网络和媒体生产者图数据的增长也在规模上增长。这些数据努力开发实时和大规模的监督学习方法，可以利用输入分布中的信息。
- en: The idea behind the graph-based semi-supervised learning is to construct a graph
    connecting similar data points or components. That lets the hidden and unobserved
    labels be random variables on the nodes of this graph. In this type of learning,
    similar data points can have similar labels and the information *propagates* from
    labeled data points to other data points. A similar restriction also limits the
    ability to express many important steps in typical processing and analytical pipelines.
    However, the graph-based learning is not optimized for an iterative diffusion
    technique such as PageRank since lots of computational aspects are underlying
    and involved.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的半监督学习的想法是构建连接相似数据点或组件的图。这使得隐藏和未观察到的标签成为该图的节点上的随机变量。在这种类型的学习中，相似的数据点可以具有相似的标签，并且信息会从标记的数据点传播到其他数据点。类似的限制也限制了在典型处理和分析管道中表达许多重要步骤的能力。然而，基于图的学习并不适用于迭代扩散技术，例如PageRank，因为许多计算方面是潜在的并且涉及到。
- en: Moreover, due to API restriction and unavailability, we will not discuss this
    graph-based machine learning in detail with suitable examples.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于API限制和不可用性，我们将不会详细讨论这种基于图的机器学习，并提供合适的示例。
- en: However, in this section, we will provide a graph-based semi-supervised application
    development, which is basically a continuation of the topic modeling that we presented
    in the previous section.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在本节中，我们将提供基于图的半监督应用程序开发，这基本上是我们在上一节中介绍的主题建模的延续。
- en: Introduction to GraphX
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphX简介
- en: GraphX is a comparatively new component in Spark for graphs processing, graph
    analytics, graph visualization, and graph-parallel computation. Actually, the
    original computational aspect of the Spark RDD was extended by introducing a new
    graph abstraction layer as a resilient distributed graph computation that provides
    resilient properties in the graph processing and storage.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX是Spark中用于图处理、图分析、图可视化和图并行计算的相对较新的组件。实际上，通过引入新的图抽象层，作为具有弹性分布式图计算的Spark
    RDD的原始计算方面得到了扩展，该图抽象层在图处理和存储中提供了弹性属性。
- en: To provide the graph related computation, a set of basic operators such as subgraph,
    `jointVertices`, and `aggregateMessages` are exposed by the GraphX. In addition
    to this, it also inherited the optimized variant of the Pregel API in the GraphX
    implementation.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供与图相关的计算，GraphX公开了一组基本运算符，如子图、`jointVertices`和`aggregateMessages`。除此之外，它还继承了GraphX实现中Pregel
    API的优化变体。
- en: Moreover, to simplify graph analytics tasks, GraphX is being enriched and growing
    with a collection of graph algorithms and builders.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了简化图分析任务，GraphX正在丰富并增加一系列图算法和构建器。
- en: In the next section, we will introduce how to parse and handle large-scale graph
    data using the GraphX API of Spark to find the connected components from the topics
    data that we got from *Figure 11*.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍如何使用Spark的GraphX API解析和处理大规模图数据，以从*图11*中获得的主题数据中找到连接的组件。
- en: Getting and parsing graph data using the GraphX API
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用GraphX API获取和解析图数据
- en: In this subsection, we will show you how to parse graph data using the GraphX
    API and then the connected components from the graph will be described in the
    next subsection. Due to API limitation in GraphX, we were unable to provide the
    same implementation in Java, but we did so for Scala implementation.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小节中，我们将向您展示如何使用GraphX API解析图数据，然后在下一个小节中描述图中的连接组件。由于GraphX中的API限制，我们无法在Java中提供相同的实现，但我们在Scala实现中做到了。
- en: 'To run the following source code, go to your Spark distribution and start the
    Spark shell by providing the following commands:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行以下源代码，请转到您的Spark分发并通过提供以下命令启动Spark shell：
- en: '[PRE45]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then the Spark shell will be available with the Spark session. We assume your
    Spark distribution is saved in the `home/spark-2.0.0-bin-hadoop2.7` path. Please
    change the path accordingly in order to run the Spark shell. Also, please save
    the topic terms shown in *Figure 11* to separate text files so that you will be
    able to use those terms for analyzing as graph data before you proceed to follow
    the next steps:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然后Spark shell将可用于Spark会话。我们假设您的Spark分发保存在`home/spark-2.0.0-bin-hadoop2.7`路径中。请根据需要更改路径，以便运行Spark
    shell。另外，请将*图11*中显示的主题术语保存到单独的文本文件中，以便在继续以下步骤之前，您可以使用这些术语来分析作为图数据：
- en: '**Step 1: Loading required packages and APIs**'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：加载所需的包和API**'
- en: 'Here is the code to load the required packages:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载所需包的代码：
- en: '[PRE46]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Step 2: Prepare the Spark environment**'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：准备Spark环境**'
- en: 'Here is the code to prepare the Spark environment:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是准备Spark环境的代码：
- en: '[PRE47]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '**Step 3: Parse the topic''s terms and tokenize them**'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：解析主题术语并对其进行标记**'
- en: 'The following code illustrates to parse he topic''s terms:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码说明了解析主题术语的方法：
- en: '[PRE48]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '**Step 4: Create RDDs of documents**'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：创建文档的RDD**'
- en: 'Create the RDDs of documents as a RDD[(`DocumentID`, (`nodeName`, `wordCount`))]
    notation. For example, RDD[(1L, (Topic_0, 4))]:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 创建文档的RDD，格式为RDD[（`DocumentID`，（`nodeName`，`wordCount`）)]。例如，RDD[(1L，（Topic_0，4）)]：
- en: '[PRE49]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The preceding print method generates the output as shown in Figure 12:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的打印方法生成了如图12所示的输出：
- en: '![Getting and parsing graph data using the GraphX API](img/00126.jpeg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![使用GraphX API获取和解析图数据](img/00126.jpeg)'
- en: 'Figure 12: The nodes.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：节点。
- en: '**Step 5: Make a word document pair**'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：创建一个单词文档对**'
- en: 'Here is the code to make a word document pair:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建单词文档对的代码：
- en: '[PRE50]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '**Step 6: Create the graph relationships between nodes**'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：创建节点之间的图关系**'
- en: 'The following code shows to create a graph relationships between nodes:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了如何创建节点之间的图关系：
- en: '[PRE51]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Note; if you want to make the graph connected, but not undirected, then just
    enable the following line:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果要使图连接，但不是无向的，只需启用以下行：
- en: '[PRE52]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Immediately after the following line:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下行之后立即：
- en: '[PRE53]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Increment the count by 2, that is, `count += 2` to make the changes consistent.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 将计数增加2，即`count += 2`，以使更改保持一致。
- en: '**Step 7: Initialize the graph**'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：初始化图**'
- en: 'The code illustrated here shows how to illustrate the graph:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这里所示的代码显示了如何说明图：
- en: '[PRE54]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Finding the connected components
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查找连接的组件
- en: According to the API documentation at [http://spark.apache.org/docs/latest/graphx-programming-guide.html](http://spark.apache.org/docs/latest/graphx-programming-guide.html),
    each connected components of the graph are labeled by the connected components
    algorithm using the lowest-numbered vertex IDs. For example, in a social network
    analysis, the clusters are approximated by the connected components. To make this
    even easier and faster, the GraphX API contains an implementation of the algorithm
    as the `ConnectedComponents` object.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[http://spark.apache.org/docs/latest/graphx-programming-guide.html](http://spark.apache.org/docs/latest/graphx-programming-guide.html)中的API文档，图的每个连接组件都是通过最低编号的顶点ID使用连接组件算法进行标记的。例如，在社交网络分析中，集群是由连接组件近似的。为了使这更加简单和快速，GraphX
    API包含了算法的实现，即`ConnectedComponents`对象。
- en: 'However, there''s no Java, Python, or R-based implementation for finding the
    connected components. Therefore, it allows us to compute the connected components
    in the topics we have calculated using the LDA algorithm through one or more term,
    as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前没有用于查找连接组件的基于Java、Python或R的实现。因此，它允许我们通过LDA算法计算我们已经计算过的主题中的连接组件，如下所示：
- en: '[PRE55]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This should produce the output as shown in *Figure 13*:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该产生如*图13*所示的输出：
- en: '![Finding the connected components](img/00085.jpeg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![查找连接的组件](img/00085.jpeg)'
- en: 'Figure 13: Connection between the topics using GraphX.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：使用GraphX连接主题之间的关系。
- en: If you look the output in *Figure 13* carefully, we have printed the relationships
    with a triplet. It is to be noted that in addition to the support of the vertices
    and edges, Spark GraphX also has the notion of a triplet. More technically, a
    triplet is an object that broadens the Edge object. From the graph perspective,
    it stores the information about an edge and the related vertices adjacent to it
    in a graph.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细查看*图13*中的输出，我们打印了一个三元组的关系。值得注意的是，除了支持顶点和边之外，Spark GraphX还有三元组的概念。更具体地说，三元组是一个扩展Edge对象的对象。从图的角度来看，它存储了有关图中的边和相关顶点的信息。
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have shown how to develop large-scale machine learning applications
    from real-time Twitter stream data and graph data. We have discussed the social
    network and time-series data analysis. In addition, we also developed an emerging
    recommendation application by using the content-based collaborative filtering
    algorithms of Spark MLlib to make movie recommendations for users. These applications,
    however, can be extended and deployed for other use cases.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了如何从实时Twitter流数据和图数据开发大规模机器学习应用程序。我们讨论了社交网络和时间序列数据分析。此外，我们还使用Spark
    MLlib的基于内容的协同过滤算法开发了一个新兴的推荐应用程序，为用户推荐电影。然而，这些应用程序可以扩展和部署到其他用例中。
- en: It is worth noting that the current implementation of Spark contains a few implemented
    algorithms for the streaming or network data analysis. However, we can hope that,
    for example, GraphX will be improved in the future and extended for not only Scala,
    but for Java, R, and Python too. In the next chapter, we will focus on how to
    interact with external data sources to make the Spark working environment more
    diverse.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，当前的Spark实现包含了一些用于流或网络数据分析的算法。然而，我们希望GraphX在未来得到改进，并且不仅限于Scala，还可以扩展到Java、R和Python。在下一章中，我们将重点介绍如何与外部数据源交互，使Spark工作环境更加多样化。
