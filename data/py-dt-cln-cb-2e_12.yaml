- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Automate Data Cleaning with User-Defined Functions, Classes, and Pipelines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用用户定义的函数、类和管道来自动化数据清理
- en: There are a number of great reasons to write code that is reusable. When we
    step back from the particular data-cleaning problem at hand and consider its relationship
    to very similar problems, we can actually improve our understanding of the key
    issues involved. We are also more likely to address a task systematically when
    we set our sights more on solving it for the long term than on the before-lunch
    solution. This has the additional benefit of helping us to disentangle the substantive
    issues from the mechanics of data manipulation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 编写可重用的代码有很多很好的理由。当我们从当前的数据清理问题中退一步，考虑它与非常相似的问题的关系时，实际上能够帮助我们加深对关键问题的理解。当我们将目光投向长期解决方案而非短期解决方案时，也更有可能以系统化的方式处理任务。这还带来了一个额外的好处，即帮助我们将数据处理的实质问题与操作数据的机制分开。
- en: We will create several modules to accomplish routine data-cleaning tasks in
    this chapter. The functions and classes in these modules are examples of code
    that can be reused across DataFrames, or for one DataFrame over an extended period
    of time. These functions handle many of the tasks we discussed in the first eleven
    chapters, but in a manner that allows us to reuse our code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将创建多个模块来完成常规的数据清理任务。这些模块中的函数和类是可以跨 DataFrame 或针对一个 DataFrame 在较长时间内重用的代码示例。这些函数处理了我们在前十一章中讨论的许多任务，但方式是允许我们重用代码的。
- en: 'Specifically, the recipes in this chapter cover the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的教程具体包括以下内容：
- en: Functions for getting a first look at our data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取数据初步信息的函数
- en: Functions for displaying summary statistics and frequencies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示汇总统计信息和频率的函数
- en: Functions for identifying outliers and unexpected values
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别异常值和意外值的函数
- en: Functions for aggregating or combining data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合或合并数据的函数
- en: Classes that contain the logic for updating Series values
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含更新 Series 值逻辑的类
- en: Classes that handle non-tabular data structures
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理非表格数据结构的类
- en: Functions for checking overall data quality
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查整体数据质量的函数
- en: 'Pre-processing data with pipelines: a simple example'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用管道进行数据预处理：一个简单的例子
- en: 'Pre-processing data with pipelines: a more complicated example'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用管道进行数据预处理：一个更复杂的例子
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need pandas, NumPy, and Matplotlib to complete the recipes in this
    chapter. I used pandas 2.1.4, but the code will run on pandas 1.5.3 or later.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章中的教程，你将需要 pandas、NumPy 和 Matplotlib。我使用的是 pandas 2.1.4，但代码也能在 pandas 1.5.3
    或更高版本上运行。
- en: The code in this chapter can be downloaded from the book’s GitHub repository,
    [https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码可以从本书的 GitHub 仓库下载，[https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition)。
- en: Functions for getting a first look at our data
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据初步信息的函数
- en: The first few steps we take after we import our data into a pandas DataFrame
    are pretty much the same regardless of the characteristics of the data. We almost
    always want to know the number of columns and rows and the column data types,
    and to see the first few rows. We also might want to view the index and check
    whether there is a unique identifier for DataFrame rows. These discrete, easily
    repeatable tasks are good candidates for a collection of functions we can organize
    into a module.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在将数据导入 pandas DataFrame 后，所进行的最初几步操作基本上是相同的，不论数据的特点如何。我们几乎总是想知道列数和行数、列的数据类型，并查看前几行数据。我们也可能想查看索引，并检查是否存在唯一标识符来区分
    DataFrame 的行。这些离散且容易重复的任务是很好的候选项，可以将其组织成一个模块中的函数。
- en: In this recipe, we will create a module with functions that give us a good first
    look at any pandas DataFrame. A module is simply a collection of Python code that
    we can import into another Python program. Modules are easy to reuse because they
    can be referenced by any program with access to the folder where the module is
    saved.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将创建一个包含函数的模块，这些函数可以让我们快速了解任何 pandas DataFrame 的基本情况。模块只是一个 Python 代码的集合，我们可以将其导入到其他
    Python 程序中。模块便于重用，因为任何能够访问存储模块文件夹的程序都可以引用它。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We create two files in this recipe: one with a function we will use to look
    at our data and another to call that function. Let’s call the file with the function
    we will use `basicdesciptives.py` and place it in a subfolder called `helperfunctions`.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们创建两个文件：一个包含我们将用来查看数据的函数，另一个用来调用该函数。让我们将包含我们将使用的函数的文件命名为 `basicdesciptives.py`，并将其放在名为
    `helperfunctions` 的子文件夹中。
- en: We work with the **National Longitudinal Surveys** (**NLS**) data in this recipe.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用**国家纵向调查**（**NLS**）数据。
- en: '**Data note**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据注释**'
- en: The NLS, administered by the United States Bureau of Labor Statistics, is a
    collection of longitudinal surveys of individuals who were in high school in 1997
    when the surveys started. Participants were surveyed each year through 2023\.
    The surveys are available for public use at [nlsinfo.org](https://nlsinfo.org).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: NLS 由美国劳工统计局管理，是一个关于在 1997 年高中毕业的个人的纵向调查集合。参与者每年接受调查直到 2023 年。这些调查可供公众在 [nlsinfo.org](https://nlsinfo.org)
    上使用。
- en: How to do it...
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: We will create a function to take an initial look at a DataFrame.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个函数来初步查看 DataFrame。
- en: Create the `basicdescriptives.py` file with the function we want.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建包含我们想要的函数的 `basicdescriptives.py` 文件。
- en: 'The `getfirstlook` function will return a dictionary with summary information
    on a DataFrame. Save the file in the `helperfunctions` subfolder as `basicdescriptives.py`.
    (You can also just download the code from the GitHub repository.) Also, create
    a function (`displaydict`) to pretty up the display of a dictionary:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`getfirstlook` 函数将返回一个包含 DataFrame 摘要信息的字典。将文件保存在 `helperfunctions` 子文件夹中，文件名为
    `basicdescriptives.py`。（你也可以直接从 GitHub 仓库下载代码。）另外，创建一个函数（`displaydict`）来美化字典的显示：'
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Create a separate file, `firstlook.py`, to call the `getfirstlook` function.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个单独的文件 `firstlook.py`，用来调用 `getfirstlook` 函数。
- en: 'Import the `pandas`, `os`, and `sys` libraries, and load the NLS data:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `pandas`、`os` 和 `sys` 库，并加载 NLS 数据：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Import the `basicdescriptives` module.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `basicdescriptives` 模块。
- en: First, append the `helperfunctions` subfolder to the Python path. We can then
    import `basicdescriptives`. We use the same name as the name of the file to import
    the module. We create an alias, `bd`, to make it easier to access the functions
    in the module later. (We can use `importlib`, commented out here, if we need to
    reload `basicdescriptives` because we have made some changes in the code in that
    module.)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先将 `helperfunctions` 子文件夹添加到 Python 路径中。然后我们可以导入 `basicdescriptives`。我们使用与要导入的模块文件名相同的名称。我们创建一个别名
    `bd`，以便稍后更容易访问模块中的函数。（如果我们需要重新加载 `basicdescriptives`，可以使用 `importlib`，这里被注释掉，因为我们在该模块中进行了一些更改。）
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Take a first look at the NLS data.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先查看 NLS 数据。
- en: 'We can just pass the DataFrame to the `getfirstlook` function in the `basicdescriptives`
    module to get a quick summary of the NLS data. The `displaydict` function gives
    us prettier printing of the dictionary:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 DataFrame 直接传递给 `basicdescriptives` 模块中的 `getfirstlook` 函数，以快速摘要 NLS 数据。`displaydict`
    函数为我们提供了字典的更漂亮打印：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pass values to the `nrows` and `uniqueids` parameters of `getfirstlook`.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将值传递给 `getfirstlook` 的 `nrows` 和 `uniqueids` 参数。
- en: 'The two parameters default to values of 5 and `None` respectively, unless we
    provide values:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个参数默认值分别为 5 和 `None`，除非我们提供值：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Work with some of the returned dictionary keys and values.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一些返回的字典键和值。
- en: 'We can also display selected key values from the dictionary returned from `getfirstlook`.
    Show the number of rows and data types, and check to see whether each row has
    a `uniqueid` instance (`dfinfo[''nrows''] == dfinfo[''uniqueids'']`):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以显示从 `getfirstlook` 返回的字典中选择的关键值。显示行数和数据类型，并检查每行是否具有 `uniqueid` 实例（`dfinfo['nrows']
    == dfinfo['uniqueids']`）：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let’s take a closer look at how the function works and how we call it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看函数的工作原理以及如何调用它。
- en: How it works...
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Almost all of the action in this recipe is in the `getfirstlook` function, which
    we look at in *step 1*. We place the `getfirstlook` function in a separate file
    that we name `basicdescriptives.py`, which we can import as a module with that
    name (minus the extension).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，几乎所有的操作都在 `getfirstlook` 函数中，我们将在*步骤 1*中查看。我们将 `getfirstlook` 函数放在一个单独的文件中，命名为
    `basicdescriptives.py`，我们可以将其作为模块导入，名称为该名称（去掉扩展名）。
- en: We could have typed the function into the file we were using and called it from
    there. By putting it in a module instead, we can call it from any file that has
    access to the folder where the module is saved. When we import the `basicdescriptives`
    module in *step 3*, we load all of the code in `basicdescriptives`, allowing us
    to call all functions in that module.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以将函数键入到我们正在使用的文件中，并从那里调用它。但是，将其放入一个模块中，我们可以从任何具有对模块保存的文件夹的访问权限的文件中调用它。当我们在*步骤
    3*中导入 `basicdescriptives` 模块时，我们加载了 `basicdescriptives` 中的所有代码，从而可以调用该模块中的所有函数。
- en: The `getfirstlook` function returns a dictionary with useful information about
    the DataFrame that is passed to it. We see the first five rows, the number of
    columns and rows, the data types, and the index. By passing a value to the `uniqueid`
    parameter, we also get the number of unique values for the column.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`getfirstlook` 函数返回一个关于传递给它的 DataFrame 的有用信息的字典。我们看到前五行、列数和行数、数据类型和索引。通过向 `uniqueid`
    参数传递一个值，我们还可以得到该列的唯一值数。'
- en: By adding keyword parameters (`nrows` and `uniqueid`) with default values, we
    improve the flexibility of `getfirstlook`, without increasing the amount of effort
    it takes to call the function when we do not need the extra functionality.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加具有默认值的关键字参数（`nrows` 和 `uniqueid`），我们提高了 `getfirstlook` 的灵活性，而不会增加在不需要额外功能时调用函数所需的工作量。
- en: In the first call, in *step 4*, we do not pass values for `nrows` or `uniqueid`,
    sticking with the default values. In *step 5*, we indicate that we only want two
    rows displayed and that we want to examine unique values for `originalid`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次调用中，在*步骤 4*中，我们没有为 `nrows` 或 `uniqueid` 传递值，保持默认值。在*步骤 5*中，我们指示只显示两行，并且要检查
    `originalid` 的唯一值。
- en: There’s more...
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The point of this recipe, and the ones that follow it, is not to provide code
    that you can download and run on your own data, though you are certainly welcome
    to do that. I am mainly trying to demonstrate how you can collect your favorite
    approaches to data cleaning in handy modules, and how this allows for easy code
    reuse. The specific code here is just a serving suggestion, if you will.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例及其后续示例的重点不是提供可以下载并在自己的数据上运行的代码，尽管您当然可以这样做。我主要是想演示如何将您喜欢的数据清理方法收集到方便的模块中，以及如何通过这种方式实现轻松的代码重用。这里的具体代码只是一种供参考的建议。
- en: Whenever we use a combination of positional and keyword parameters, the positional
    parameters must go first.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们使用位置参数和关键字参数的组合时，位置参数必须首先出现。
- en: Functions for displaying summary statistics and frequencies
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于显示摘要统计和频率的函数
- en: During the first few days of working with a DataFrame, we try to get a good
    sense of the distribution of continuous variables and counts for categorical variables.
    We also often do counts by selected groups. Although pandas and NumPy have many
    built-in methods for these purposes—`describe`, `mean`, `valuecounts`, `crosstab`,
    and so on—data analysts often have preferences for how they work with these tools.
    If, for example, an analyst finds that they usually need to see more percentiles
    than those generated by `describe`, they can use their own function instead. We
    will create user-defined functions for displaying summary statistics and frequencies
    in this recipe.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在与 DataFrame 工作的头几天，我们尝试对连续变量的分布和分类变量的计数有一个良好的了解。我们经常按选定的组进行计数。虽然 pandas 和 NumPy
    有许多内置方法用于这些目的——`describe`、`mean`、`valuecounts`、`crosstab` 等等——数据分析师通常对如何使用这些工具有自己的偏好。例如，如果分析师发现他们通常需要看到比`describe`生成的更多的百分位数，他们可以使用自己的函数代替。我们将在这个示例中创建用于显示摘要统计和频率的用户定义函数。
- en: Getting ready
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will be working with the `basicdescriptives` module again in this recipe.
    All of the functions we will define are saved in that module. We will continue
    to work with the NLS data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将再次使用 `basicdescriptives` 模块。我们将定义的所有函数都保存在该模块中。我们将继续使用 NLS 数据。
- en: How to do it...
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We will use functions we create to generate summary statistics and counts:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们创建的函数生成摘要统计和计数：
- en: Create the `gettots` function in the `basicdescriptives` module.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `basicdescriptives` 模块中创建 `gettots` 函数。
- en: 'The function takes a pandas DataFrame and creates a dictionary with selected
    summary statistics. It returns a pandas DataFrame:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受一个 pandas DataFrame，并创建一个包含选定摘要统计的字典。它返回一个 pandas DataFrame：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Import the `pandas`, `os`, and `sys` libraries.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas`、`os` 和 `sys` 库。
- en: 'Do this from a different file, which you can call `taking_measure.py`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个不同的文件中执行此操作，您可以将其命名为 `taking_measure.py`：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Import the `basicdescriptives` module:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`basicdescriptives`模块：
- en: '[PRE15]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Show summary statistics for continuous variables.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示连续变量的汇总统计。
- en: 'Use the `gettots` function from the `basicdescriptives` module that we created
    in *step 1*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们在*步骤 1*中创建的`basicdescriptives`模块中的`gettots`函数：
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Create a function to count missing values by columns and rows.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，通过列和行来计算缺失值的数量。
- en: 'The `getmissings` function will take a DataFrame and a parameter for showing
    percentages or counts. It returns two Series, one with the missing values for
    each column and the other with missing values by row. Save the function in the
    `basicdescriptives` module:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`getmissings`函数将接受一个DataFrame和一个显示百分比或计数的参数。它返回两个Series，一个显示每列的缺失值，另一个显示每行的缺失值。将该函数保存到`basicdescriptives`模块中：'
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Call the `getmissings` function.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`getmissings`函数。
- en: 'Call it first with `byrowperc` (the second parameter) set to `True`. This will
    show the percentage of rows with the associated number of missing values. For
    example, the `missingbyrows` value shows that 73% of rows have 0 missing values
    for `weeksworked20` and `weeksworked21`. Call it again, leaving `byrowperc` at
    its default value of `False`, to get counts instead:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先调用它，将`byrowperc`（第二个参数）设置为`True`。这样可以显示每行缺失值数量的百分比。例如，`missingbyrows`值显示`weeksworked20`和`weeksworked21`的73%的行没有缺失值。然后再次调用它，保持`byrowperc`为默认值`False`，以获取计数：
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Create a function to calculate frequencies for all categorical variables.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，计算所有类别变量的频率。
- en: 'The `makefreqs` function loops through all columns with the category data type
    in the passed DataFrame, running `value_counts` on each one. The frequencies are
    saved to the file indicated by `outfile`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`makefreqs`函数遍历传递的DataFrame中所有类别数据类型的列，并对每一列运行`value_counts`。频率将保存到由`outfile`指定的文件中：'
- en: '[PRE27]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Call the `makefreqs` function.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`makefreqs`函数。
- en: First change the data type of each object column to `category`. This call runs
    `value_counts` on category data columns in the NLS DataFrame and saves the frequencies
    to `nlsfreqs.txt` in the `views` subfolder of the current folder.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将每个对象列的数据类型更改为`category`。此调用会对NLS DataFrame中的类别数据列运行`value_counts`，并将频率保存到当前文件夹中的`views`子文件夹下的`nlsfreqs.txt`文件中。
- en: '[PRE28]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Create a function to get counts by groups.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，根据组来获取计数。
- en: The `getcnts` function counts the number of rows for each combination of column
    values in `cats`, a list of column names. It also counts the number of rows for
    each combination of column values excluding the final column in `cats`. This provides
    a total across all values of the final column. (The next step shows what this
    looks like.)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`getcnts`函数计算`cats`（一个列名列表）中每一组合列值的行数。它还计算排除`cats`中最后一列后的每一组合列值的行数。这将提供最后一列所有值的总数。（接下来的步骤会展示它的效果。）'
- en: '[PRE29]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Pass the `maritalstatus` and `colenroct00` columns to the `getcnts` function.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`maritalstatus`和`colenroct00`列传递给`getcnts`函数。
- en: 'This returns a DataFrame with counts for each column value combination, as
    well as counts for all combinations excluding the last column. This is used to
    calculate percentages within groups. For example, 669 respondents were divorced
    and 560 of those (or 84%) were not enrolled in college in October 2000:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个DataFrame，包含每个列值组合的计数，以及排除最后一列后的所有列值组合的计数。此结果用于计算组内的百分比。例如，有669名受访者已离婚，其中560人（即84%）在2000年10月时没有入学：
- en: '[PRE30]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Use the `rowsel` parameter of `getcnts` to limit the output to specific rows.
    This will show only the not-enrolled-in-college rows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`getcnts`的`rowsel`参数限制输出为特定行。这将仅显示未入学的行：
- en: '[PRE32]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: These steps demonstrate how to create functions and use them to generate summary
    statistics and frequencies.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤展示了如何创建函数并使用它们生成汇总统计和频率。
- en: How it works...
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *step 1*, we created a function, `gettots`, that calculated descriptive statistics
    for all columns in a DataFrame, returning those results in a summary DataFrame.
    Most of the statistics can be generated with the `describe` method, but we add
    a few statistics—the 15^(th) percentile, the 85^(th) percentile, and the interquartile
    range. We call that function twice in *step 4*, the first time for the SAT verbal
    and math scores and the second time for all weeks worked columns.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 1*中，我们创建了一个名为`gettots`的函数，用于计算DataFrame中所有列的描述性统计，并将这些结果返回为一个汇总的DataFrame。大多数统计量可以通过`describe`方法生成，但我们添加了一些额外的统计量——第15百分位数、第85百分位数和四分位距。我们在*步骤
    4*中调用了这个函数两次，第一次是针对SAT语言和数学成绩，第二次是针对所有工作周数的列。
- en: '*Steps 5* and *6* create and call a function that shows the number of missing
    values for each column in the passed DataFrame. The function also counts missing
    values for each row, displaying the frequency of missing values. The frequency
    of missing values by row can also be displayed as a percentage of all rows by
    passing a value of `True` to the `byrowperc` parameter.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 5* 和 *步骤 6* 创建并调用一个函数，显示传递给 DataFrame 的每列的缺失值数量。该函数还计算每行的缺失值数量，并显示缺失值的频率。通过将`True`传递给`byrowperc`参数，还可以将每行的缺失值频率显示为所有行的百分比。'
- en: '*Steps 7* and *8* produce a text file with frequencies for all categorical
    variables in the passed DataFrame. We just loop through all columns with the category
    data type and run `value_counts`. Since often the output is long, we save it to
    a file. It is also good to have frequencies saved somewhere for later reference.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 7* 和 *步骤 8* 会生成一个文本文件，其中包含传递给 DataFrame 的所有分类变量的频率。我们只需循环遍历所有类别数据类型的列，并运行`value_counts`。由于输出通常很长，我们将其保存到文件中。将频率保存在某处以供以后参考也是很有用的。'
- en: The `getcnts` function we create in *step 9* and call in *steps 10* and *11*
    is a tad idiosyncratic. pandas has a very useful `crosstab` function, which I
    use frequently. But I often need a no-fuss way to look at group counts and percentages
    for subgroups within groups. The `getcnts` function does that.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *步骤 9* 中创建并在 *步骤 10* 和 *步骤 11* 中调用的`getcnts`函数有点特殊。pandas有一个非常有用的`crosstab`函数，我经常使用。但我经常需要一种简单的方法来查看组内子组的组计数和百分比。`getcnts`函数可以做到这一点。
- en: There’s more...
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: A function can be very helpful even when it does not do very much. There is
    not much code in the `getmissings` function, but I check for missing values so
    frequently that the small time savings are significant cumulatively. It also reminds
    me to check for missing values by column and by row.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 即使一个函数没有做很多事情，它也可能非常有帮助。`getmissings`函数中的代码并不多，但我经常检查缺失值，所以小小的时间节省在累积起来时是显著的。它还提醒我按列和按行检查缺失值。
- en: See also
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: We explore pandas’ tools for generating summary statistics and frequencies in
    *Chapter 3*, *Taking the Measure of Your Data*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 *第 3 章*，*测量数据*中探索 pandas 用于生成摘要统计信息和频率的工具。
- en: Functions for identifying outliers and unexpected values
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于识别异常值和意外值的函数
- en: If I had to pick one data-cleaning area where I find reusable code most beneficial,
    it would be in the identification of outliers and unexpected values. This is because
    our prior assumptions often lead us to the central tendency of a distribution,
    rather than to the extremes. Quickly—think of a cat. Unless you were thinking
    about a particular cat in your life, an image of a generic feline between 8 and
    10 pounds probably came to mind; not one that is 6 pounds or 22 pounds.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我必须选择一个数据清理领域，我发现可重复使用的代码最有益处的领域，那就是识别异常值和意外值。这是因为我们的先前假设通常导致我们关注分布的中心趋势，而不是极端值。快速想象一只猫——除非你在想你生活中的某只特定猫，否则你脑海中可能会浮现一只体重在
    8 到 10 磅之间的普通猫；而不是一只体重为 6 磅或 22 磅的猫。
- en: We often need to be more deliberate to elevate extreme values to consciousness.
    This is where having a standard set of diagnostic functions to run on our data
    is very helpful. We can run these functions even if nothing in particular triggers
    us to run them. This recipe provides examples of functions that we can use regularly
    to identify outliers and unexpected values.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要更有意识地提升极端值到意识中。在这里，拥有一套标准的诊断函数来运行我们的数据非常有帮助。即使没有特别的触发条件，我们也可以运行这些函数。这个示例提供了我们可以定期使用的函数示例，用于识别异常值和意外值。
- en: Getting ready
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will create two files in this recipe, one with the functions we will use
    to check for outliers and another with the code we will use to call those functions.
    Let’s call the file with the functions we will use `outliers.py`, and place it
    in a subfolder called `helperfunctions`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将创建两个文件，一个包含我们将用来检查异常值的函数，另一个包含我们将用来调用这些函数的代码。让我们把包含我们将使用的函数的文件命名为`outliers.py`，并将其放在一个名为`helperfunctions`的子文件夹中。
- en: You will need the `matplotlib` and `scipy` libraries, in addition to pandas,
    to run the code in this recipe. You can install `matplotlib` and `scipy` by entering
    `pip install matplotlib` and `pip install scipy` in a Terminal client or in Windows
    PowerShell. You will also need the `pprint` utility, which you can install with
    `pip install pprint`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本示例中的代码，你需要`matplotlib`和`scipy`库，除了 pandas。你可以通过在终端客户端或 Windows PowerShell
    中输入`pip install matplotlib`和`pip install scipy`来安装`matplotlib`和`scipy`。你还需要`pprint`实用程序，你可以通过`pip
    install pprint`来安装。
- en: We will work with the NLS and COVID-19 data in this recipe. The COVID-19 data
    has one row per country, with cumulative cases and deaths for that country.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将处理NLS和COVID-19数据。COVID-19数据中每一行代表一个国家，包含该国的累计病例和死亡数。
- en: '**Data note**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据说明**'
- en: Our World in Data provides COVID-19 public use data at [https://ourworldindata.org/covid-cases](https://ourworldindata.org/covid-cases).
    The dataset includes total cases and deaths, tests administered, hospital beds,
    and demographic data such as median age, gross domestic product, and diabetes
    prevalence. The dataset used in this recipe was downloaded on March 3, 2024.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Our World in Data提供了COVID-19公共使用数据，网址：[https://ourworldindata.org/covid-cases](https://ourworldindata.org/covid-cases)。该数据集包括各国的累计病例和死亡数、已进行的测试、医院床位以及一些人口统计数据，如中位年龄、国内生产总值和糖尿病患病率。本示例使用的数据集是在2024年3月3日下载的。
- en: How to do it...
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We create and call functions to check the distribution of variables, list extreme
    values, and visualize a distribution:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建并调用函数来检查变量的分布、列出极端值并可视化分布：
- en: Import the `pandas`, `os`, `sys`, and `pprint` libraries.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`os`、`sys`和`pprint`库。
- en: 'Also, load the NLS and COVID-19 data:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，加载NLS和COVID-19数据：
- en: '[PRE34]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Create a function to show some important properties of a distribution.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来展示分布的一些重要属性。
- en: The `getdistprops` function takes a Series and generates measures of central
    tendency, shape, and spread. The function returns a dictionary with these measures.
    It also handles situations where the Shapiro test for normality does not return
    a value. It will not add keys for `normstat` and `normpvalue` when that happens.
    Save the function in a file named `outliers.py` in the `helperfunctions` subfolder
    of the current directory. (Also load the `pandas`, `matplotlib`, `scipy`, and
    `math` libraries we will need for this and other functions in this module.)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`getdistprops`函数接受一个Series并生成中心趋势、形状和分布的度量。该函数返回一个包含这些度量的字典。它还处理Shapiro正态性检验未返回值的情况。若发生这种情况，将不会为`normstat`和`normpvalue`添加键。将该函数保存在当前目录下`helperfunctions`子文件夹中的一个名为`outliers.py`的文件中。（同时加载我们在此模块中其他函数需要的`pandas`、`matplotlib`、`scipy`和`math`库。）'
- en: '[PRE35]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Pass the total cases per million in population series to the `getdistprops`
    function.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每百万总病例数的Series传递给`getdistprops`函数。
- en: The `skew` and `kurtosis` values suggest that the distribution of `total_cases_pm`
    has a positive skew and shorter tails than a normally distributed variable. The
    Shapiro test of normality (`normpvalue`) confirms this. (Use `pprint` to improve
    the display of the dictionary returned by `getdistprops`.)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`skew`和`kurtosis`值表明`total_cases_pm`的分布有正偏态且尾部比正态分布的变量更短。Shapiro正态性检验（`normpvalue`）证实了这一点。（使用`pprint`改进`getdistprops`返回的字典的显示效果。）'
- en: '[PRE36]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Create a function to list the outliers in a DataFrame.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来列出DataFrame中的异常值。
- en: 'The `getoutliers` function iterates over all columns in `sumvars`. It determines
    outlier thresholds for those columns, setting them at 1.5 times the interquartile
    range (the distance between the first and third quartiles) below the first quartile
    or above the third quartile. It then selects all rows with values above the high
    threshold or below the low threshold. It adds columns that indicate the variable
    examined (`varname`) for outliers and the threshold levels. It also includes columns
    in the `othervars` list in the DataFrame it returns:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`getoutliers`函数遍历`sumvars`中的所有列。它为这些列确定异常值阈值，设置阈值为第一四分位数下方或第三四分位数上方1.5倍四分位间距的值。然后，它选择所有超出高阈值或低阈值的行。它还会添加表示检查的变量（`varname`）的列以及阈值水平的列。它还会包括`othervars`列表中的列，并将其返回为DataFrame：'
- en: '[PRE38]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Call the `getoutlier` function.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`getoutlier`函数。
- en: 'Pass a list of columns to check for outliers (`sumvars`) and another list of
    columns to include in the returned DataFrame (`othervars`). Show the count of
    outliers for each variable and view the outliers for SAT math:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 将检查异常值的列的列表（`sumvars`）和要包含在返回的DataFrame中的列的另一个列表（`othervars`）传递给函数。展示每个变量的异常值计数，并查看SAT数学的异常值：
- en: '[PRE39]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Create a function to generate histograms and boxplots.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个生成直方图和箱型图的函数。
- en: 'The `makeplot` function takes a Series, title, and label for the *x*-axis.
    The default plot is set as a histogram:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`makeplot`函数接受一个Series、标题和*x*-轴的标签。默认绘图类型为直方图：'
- en: '[PRE44]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Call the `makeplot` function to create a histogram:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`makeplot`函数来创建直方图：
- en: '[PRE45]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This generates the following histogram:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下直方图：
- en: '![](img/B18596_12_01.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18596_12_01.png)'
- en: 'Figure 12.1: Frequencies of SAT math values'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：SAT数学值的频率分布
- en: 'Use the `makeplot` function to create a boxplot:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`makeplot`函数创建一个箱线图：
- en: '[PRE46]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This generates the following boxplot:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下箱线图：
- en: '![](img/B18596_12_02.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18596_12_02.png)'
- en: 'Figure 12.2: Show the median, interquartile range, and outlier thresholds with
    a boxplot'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：使用箱线图显示中位数、四分位距和异常值阈值
- en: The preceding steps show how we can develop reusable code to check for outliers
    and unexpected values.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤展示了我们如何开发可重复使用的代码来检查异常值和意外值。
- en: How it works...
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We start by getting the key attributes of a distribution, including the mean,
    median, standard deviation, skew, and kurtosis. We do this by passing a Series
    to the `getdistprop` function in *step 3*, getting back a dictionary with these
    measures.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过将Series传递给*步骤3*中的`getdistprop`函数来获取分布的关键属性，包括均值、中位数、标准差、偏度和峰度。我们得到一个包含这些度量值的字典。
- en: The function in *step 4* selects rows where one of the columns in `sumvars`
    has a value that is an outlier. It also includes the values for the columns in
    `othervars` and the threshold amounts in the DataFrame it returns.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤4*中的函数选择了`sumvars`中某一列具有异常值的行。它还包括了`othervars`列的值和返回的DataFrame中的阈值金额。'
- en: 'We create a function in *step 6* that makes it easier to create a simple histogram
    or boxplot. The functionality of `matplotlib` is great, but it can take a minute
    to remind ourselves of the syntax when we just want to create a simple histogram
    or boxplot. We can avoid that by defining a function with a few routine parameters:
    Series, title, and *x*-label. We call that function in *steps 7* and *8*.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*步骤6*中创建了一个函数，使得创建简单直方图或箱线图变得更加容易。`matplotlib`的功能很强大，但是当我们只想创建一个简单的直方图或箱线图时，可能需要花一点时间来回想语法。我们可以通过定义一个带有几个常规参数的函数来避免这种情况：Series、标题和*x*-label。我们在*步骤7*和*8*中调用该函数。
- en: There’s more...
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We do not want to do too much work with a continuous variable before getting
    a good sense of how its values are distributed; what is the central tendency and
    shape of the distribution? If we run something like the functions in this recipe
    for key continuous variables, we would be off to a good start.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在对连续变量进行过多处理之前，我们需要先了解其数值分布情况；中心趋势和分布形状是什么？如果我们对关键连续变量运行类似本示例中的函数，那么我们就会有一个良好的起点。
- en: The relatively painless portability of Python modules makes this pretty easy
    to do. If we wanted to use the `outliers` module that we used in this example,
    we would just need to save the `outliers.py` file to a folder that our program
    can access, add that folder to the Python path, and import it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Python模块的相对轻松可移植性使得这变得相当容易。如果我们想使用本示例中使用的`outliers`模块，只需将`outliers.py`文件保存到我们的程序可以访问的文件夹中，将该文件夹添加到Python路径中，并导入它。
- en: Usually, when we are inspecting an extreme value, we want to have a better idea
    of the context of other variables that might explain why the value is extreme.
    For example, a height of 178 centimeters is not an outlier for an adult male,
    but it definitely is for a 9-year-old. The DataFrame produced in *steps 4* and
    *5* provides us with both the outlier values and other data that might be relevant.
    Saving the data to an Excel file makes it easy to inspect outlier rows later or
    share that data with others.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们检查极端值时，我们希望更好地了解其他变量的背景，这些变量可能解释为什么该值是极端的。例如，178厘米的身高对于成年男性来说不是异常值，但对于9岁的孩子来说绝对是异常值。*步骤4*和*5*生成的DataFrame为我们提供了异常值以及可能相关的其他数据。将数据保存到Excel文件中使得以后检查异常行或与他人分享数据变得更加容易。
- en: See also
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: We go into a fair bit of detail on detecting outliers and unexpected values
    in *Chapter 4*, *Identifying Outliers in Subsets of Data*. We examine histograms,
    boxplots, and many other visualizations in *Chapter 5*, *Using Visualizations
    for the Identification of Unexpected Values*.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第4章*中详细讨论了如何检测异常值和意外值，*在数据子集中识别异常值*。我们在*第5章*中研究了直方图、箱线图和许多其他可视化方法，*使用可视化方法识别意外值*。
- en: Functions for aggregating or combining data
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于聚合或合并数据的函数
- en: Most data analysis projects require some reshaping of data. We may need to aggregate
    by group or combine data vertically or horizontally. We have to do similar tasks
    each time we prepare our data for this reshaping. We can routinize some of these
    tasks with functions, improving both the reliability of our code and our efficiency
    in getting the work done. We sometimes need to check for mismatches in merge-by
    columns before doing a merge, check for unexpected changes in values in panel
    data from one period to the next before aggregating, or concatenate a number of
    files at once and verify that data has been combined accurately.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据分析项目都需要对数据进行某种形状的调整。我们可能需要按组聚合数据，或者纵向或横向合并数据。在准备数据以进行这些形状调整时，我们每次都会进行类似的任务。我们可以通过函数将其中一些任务标准化，从而提高代码的可靠性和完成工作的效率。有时我们需要在合并之前检查按值合并的列是否匹配，检查面板数据在一个周期到下一个周期之间的值是否发生了意外变化，或者一次性连接多个文件并验证数据是否已正确合并。
- en: These are just a few examples of the kind of data aggregation and combining
    tasks that might lend themselves to a more generalized coding solution. In this
    recipe, we define functions that can help with these tasks.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是数据聚合和组合任务的一些示例，这些任务可能更适合使用更通用的编码解决方案。在这个示例中，我们定义了可以帮助完成这些任务的函数。
- en: Getting ready
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will work with the COVID-19 daily data in this recipe. This data comprises
    new cases and new deaths for each country by day. We will also work with land
    temperature data for several countries in 2023\. The data for each country is
    in a separate file and has one row per weather station in that country for each
    month.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用COVID-19每日数据。该数据包括按天计算的每个国家的新病例和新死亡人数。我们还将使用2023年多个国家的土地温度数据。每个国家的数据在单独的文件中，并且每个月的每个气象站有一行数据。
- en: '**Data note**'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据说明**'
- en: The land temperature DataFrame has the average temperature readings (in °C)
    in 2023 from over 12,000 stations across the world, though a majority of the stations
    are in the United States. The raw data was retrieved from the Global Historical
    Climatology Network integrated database. It is made available for public use by
    the United States National Oceanic and Atmospheric Administration at [https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 土地温度数据框包含来自全球超过12,000个站点的2023年平均温度数据（单位：°C），尽管大多数站点位于美国。原始数据来自全球历史气候学网络集成数据库。美国国家海洋和大气管理局在[https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly)上为公众提供了这些数据。
- en: How to do it...
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We will use functions to aggregate data, combine data vertically, and check
    merge-by values:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用函数来聚合数据，纵向合并数据，并检查按值合并：
- en: 'Import the `pandas`, `os`, and `sys` libraries:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`os`和`sys`库：
- en: '[PRE47]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Create a function (`adjmeans`) to aggregate values by period for a group.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数（`adjmeans`）来按组汇总每个周期的值。
- en: 'Sort the values in the passed DataFrame by group (`byvar`) and then `period`.
    Convert the DataFrame values to a NumPy array. Loop through the values, do a running
    tally of the `var` column, and set the running tally back to 0 when you reach
    a new value for `byvar`. Before aggregating, check for extreme changes in values
    from one period to the next. The `changeexclude` parameter indicates the size
    of a change from one period to the next that should be considered extreme. The
    `excludetype` parameter indicates whether the `changeexclude` value is an absolute
    amount or a percentage of the `var` column’s mean. Save the function in a file
    called `combineagg.py` in the `helperfunctions` subfolder:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 按组（`byvar`）和然后按`period`对传入的数据框的值进行排序。将数据框的值转换为NumPy数组。循环遍历这些值，按`var`列做一个累加，并在遇到`byvar`的新值时将累加值重置为0。在进行聚合之前，检查每个周期之间值的极端变化。`changeexclude`参数表示从一个周期到下一个周期应该视为极端变化的大小。`excludetype`参数表示`changeexclude`值是`var`列均值的绝对值还是百分比。在`helperfunctions`子文件夹中将该函数保存在名为`combineagg.py`的文件中：
- en: '[PRE48]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Import the `combineagg` module:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`combineagg`模块：
- en: '[PRE49]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Load the DataFrames:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据框（DataFrames）：
- en: '[PRE50]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Call the `adjmeans` function to summarize panel data by group and time period.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`adjmeans`函数按组和时间周期汇总面板数据。
- en: 'Indicate that we want a summary of `new_cases` by `location`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 指明我们希望按`location`对`new_cases`进行汇总：
- en: '[PRE51]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Call the `adjmeans` function again, this time excluding values where `new_cases`
    goes up or down by more than 5,000 from one day to the next. Notice some reduction
    in the counts for some countries:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次调用`adjmeans`函数，这次排除`new_cases`从一天到下一天的变化超过5,000的值。注意，一些国家的计数有所减少：
- en: '[PRE53]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Create a function to check values for merge-by columns on one file but not another.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来检查一个文件中合并列的值，但在另一个文件中没有这些值。
- en: 'The `checkmerge` function does an outer join of two DataFrames passed to it,
    using the third and fourth parameters for the merge-by columns for the first and
    second DataFrames respectively. It then does a crosstab that shows the number
    of rows with merge-by values in both DataFrames and those in one DataFrame but
    not the other. It also shows up to 20 rows of data for merge-by values found in
    just one file:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`checkmerge`函数对传入的两个DataFrame进行外连接，使用第三个和第四个参数作为第一个和第二个DataFrame的合并列。然后，它生成一个交叉表，显示在两个DataFrame中都存在的合并列值的行数，以及只出现在一个DataFrame中而另一个DataFrame中不存在的行数。它还会显示最多20行只在一个文件中找到的合并列值的数据：'
- en: '[PRE55]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Call the `checkmerge` function.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`checkmerge`函数。
- en: 'Check a merge between the `countries` land temperatures DataFrame (which has
    one row per country) and the `locations` DataFrame (which has one row for each
    weather station in each country). The crosstab shows that 27,472 merge-by column
    values are in both DataFrames, two are in the `countries` file and not in the
    `locations` file, and one is in the `locations` file but not the `countries` file:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 检查`countries`土地温度DataFrame（每个国家一行）和`locations` DataFrame（每个国家每个气象站一行）之间的合并。交叉表显示，27,472个合并列值同时存在于两个DataFrame中，两个值只在`countries`文件中，而不在`locations`文件中，一个值只在`locations`文件中，而不在`countries`文件中：
- en: '[PRE56]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Create a function that concatenates all CSV files in a folder.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，用于连接文件夹中的所有CSV文件。
- en: 'This function loops through all of the filenames in the specified folder. It
    uses the `endswith` method to check that the filename has a CSV file extension.
    It then loads the DataFrame and prints out the number of rows. Finally, it uses
    `concat` to append the rows of the new DataFrame to the rows already appended.
    If column names on a file are different, it prints those column names:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数遍历指定文件夹中的所有文件名。它使用`endswith`方法检查文件名是否具有CSV文件扩展名。然后，它加载DataFrame并打印出行数。最后，它使用`concat`将新DataFrame的行追加到已追加的行中。如果文件中的列名不同，它会打印出这些列名：
- en: '[PRE58]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Use the `addfiles` function to concatenate all of the `countries` land temperature
    files.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`addfiles`函数连接所有`countries`土地温度文件。
- en: 'It looks like the file for Oman (`ltoman`) is slightly different. It does not
    have the `latabs` column. Notice that the counts for each country in the combined
    DataFrame match the number of rows for each country file:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来阿曼（`ltoman`）的文件稍有不同。它没有`latabs`列。注意，合并后的DataFrame中每个国家的行数与每个国家文件中的行数相匹配：
- en: '[PRE59]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The preceding steps demonstrate how we can systematize some of our messy data
    reshaping work. I am sure you can think of a number of other functions that might
    be helpful.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤演示了我们如何将一些杂乱的数据重塑工作系统化。我相信你可以想到许多其他可能有用的函数。
- en: How it works...
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: You may have noticed that in the `adjmeans` function we define in *step 2*,
    we actually do not append our summary of the `var` column values until we get
    to the next `byvar` column value. This is because there is no way to tell that
    we are on the last row for any `byvar` value until we get to the next `byvar`
    value. That is not a problem because we append the summary to `rowlist` right
    before we reset the value to `0`. This also means that we need to do something
    special to output the totals for the last `byvar` value since no next `byvar`
    value is reached. We do this with a final append after the loop is complete.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，在*步骤2*中定义的`adjmeans`函数中，我们实际上并不会在到达下一个`byvar`列值之前追加`var`列值的汇总。这是因为，直到我们到达下一个`byvar`列值时，无法知道我们是否已经到了某个`byvar`值的最后一行。这不是问题，因为我们会在重置为`0`之前将汇总追加到`rowlist`中。这也意味着，我们需要做一些特别的处理来输出最后一个`byvar`值的总计，因为没有下一个`byvar`值。这是通过在循环结束后进行最终的追加操作来实现的。
- en: In *step 5*, we call the `adjmeans` function we defined in *step 2*. Since we
    do not set a value for the `changeexclude` parameter, the function will include
    all values in the aggregation. This will give us the same results as we would
    get using `groupby` with an aggregation function. When we pass an argument to
    `changeexclude`, however, we determine which rows to exclude from the aggregation.
    In *step 6*, the fifth argument in the call to `adjmeans` indicates that we should
    exclude new case values that are more than 5,000 cases higher or lower than the
    value for the previous day.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤5*中，我们调用了*步骤2*中定义的`adjmeans`函数。由于我们没有为`changeexclude`参数设置值，该函数会将所有值包含在聚合中。这将给我们与使用`groupby`和聚合函数相同的结果。然而，当我们传递一个参数给`changeexclude`时，我们就能确定从聚合中排除哪些行。在*步骤6*中，调用`adjmeans`时的第五个参数表示我们应该排除新案例值与前一天的值相差超过5000个案例的行。
- en: The function in *step 9* works well when the data files to be concatenated have
    the same, or nearly the same, structure. We print an alert when the column names
    are different, as *step 10* shows. The `latabs` column is not in the Oman file.
    This means that in the concatenated file, `latabs` will be missing for all of
    the rows for Oman.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤9*中的函数在要合并的数据文件结构相同或几乎相同时效果很好。当列名不同时，我们会打印出警告，正如*步骤10*所示。`latabs`列在阿曼文件中不存在。这意味着在合并后的文件中，阿曼的所有行将缺少`latabs`这一列。'
- en: There’s more...
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The `adjmeans` function does a fairly straightforward check of each new value
    to be aggregated before including it in the total. But we could imagine much more
    complicated checks. We could even have made a call to another function within
    the `adjmeans` function where we are deciding whether to include the row.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`adjmeans`函数在将每个新值加入总数之前会进行相对简单的检查。但我们也可以想象更加复杂的检查。我们甚至可以在`adjmeans`函数中调用另一个函数，用来决定是否包含某行数据。'
- en: See also
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: We examine combining DataFrames vertically and horizontally in *Chapter 10*,
    *Addressing Data Issues When Combining DataFrames*.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第10章*《合并数据框时处理数据问题》中讨论垂直和水平合并DataFrame。
- en: Classes that contain the logic for updating Series values
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 包含更新Series值逻辑的类
- en: We sometimes work with a particular dataset for an extended period of time,
    occasionally years. The data might be updated regularly, for a new month or year,
    or with additional individuals, but the data structure might be fairly stable.
    If that dataset also has a large number of columns, we might be able to improve
    the reliability and readability of our code by implementing classes.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有时会长时间处理一个特定的数据集，甚至可能是几年。数据可能会定期更新，添加新的月份或年份，或者增加额外的个体，但数据结构可能相对稳定。如果该数据集还包含大量列，我们通过实现类，可能会提高代码的可靠性和可读性。
- en: When we create classes, we define the attributes and methods of objects. When
    I use classes for my data-cleaning work, I tend to conceptualize a class as representing
    my unit of analysis. So, if my unit of analysis is a student, then I have a student
    class. Each instance of a student created by that class might have birth date
    and gender attributes and a course registration method. I might also create a
    subclass for alumni that inherits methods and attributes from the student class.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建类时，会定义对象的属性和方法。在我的数据清理工作中，我倾向于将类概念化为代表我的分析单元。所以，如果我的分析单元是学生，那么我会创建一个学生类。由该类创建的每个学生实例可能具有出生日期和性别属性，以及课程注册方法。我还可以为校友创建一个子类，继承学生类的方法和属性。
- en: Data cleaning for the NLS DataFrame could be implemented nicely with classes.
    The dataset has been relatively stable for 25 years, both in terms of the variables
    and the allowable values for each variable. We explore how to create a respondent
    class for NLS survey responses in this recipe.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: NLS DataFrame的数据清理可以通过类来很好地实现。该数据集在过去25年里相对稳定，无论是变量还是每个变量的允许值。我们在本食谱中探索如何为NLS调查响应创建一个respondent类。
- en: Getting ready
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to create a `helperfunctions` subfolder in your current directory
    to run the code in this recipe. We will save the file (`respondent.py`) for our
    new class in that subfolder.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本食谱中的代码，你需要在当前目录中创建一个名为`helperfunctions`的子文件夹。我们将把新类的文件（`respondent.py`）保存在这个子文件夹中。
- en: How to do it...
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will define a respondent class to create several new Series based on the
    NLS data:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个respondent类，根据NLS数据创建几个新的Series：
- en: Import the `pandas`, `os`, `sys`, and `pprint` libraries.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`os`、`sys`和`pprint`库。
- en: 'We store this code in a different file than we will save the respondent class.
    Let’s call this file `class_cleaning.py`. We will instantiate respondent objects
    from this file:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这段代码存储在一个不同于保存响应者类的文件中。我们将这个文件命名为`class_cleaning.py`。我们将在这个文件中实例化响应者对象：
- en: '[PRE63]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Create a `Respondent` class and save it to `respondent.py` in the `helperfunctions`
    subfolder.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`Respondent`类，并将其保存到`helperfunctions`子文件夹中的`respondent.py`文件中。
- en: When we call our class (instantiate a class object), the `__init__` method runs
    automatically. (There is a double underscore before and after `init`.) The `__init__`
    method has `self` as the first parameter, as any instance method does. The `__init__`
    method of this class also has a `respdict` parameter, which expects a dictionary
    of values from the NLS data. In later steps, we will instantiate a respondent
    object once for each row of data in the NLS DataFrame.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调用我们的类（实例化类对象）时，`__init__`方法会自动运行。（`init`前后都有双下划线。）`__init__`方法的第一个参数是`self`，正如任何实例方法一样。此类的`__init__`方法还有一个`respdict`参数，它期望一个来自NLS数据的字典值。在后续步骤中，我们将为NLS
    DataFrame中的每一行数据实例化一个响应者对象。
- en: The `__init__` method assigns the passed `respdict` value to `self.respdict`
    to create an instance variable that we can reference in other methods. Finally,
    we increment a counter, `respondentcnt`. We will be able to use this later to
    confirm the number of instances of `respondent` that we created. We also import
    the `math` and `datetime` modules because we will need them later. (Notice that
    class names are capitalized by convention.)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__`方法将传递的`respdict`值赋给`self.respdict`，以创建一个实例变量，我们可以在其他方法中引用它。最后，我们递增一个计数器`respondentcnt`。稍后我们可以用它来确认我们创建的`respondent`实例的数量。我们还导入了`math`和`datetime`模块，因为稍后会需要它们。（请注意，类名通常是大写的。）'
- en: '[PRE64]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Add a method for counting the number of children.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个方法，用于计算孩子的数量。
- en: 'This is a very simple method that just adds the number of children living with
    the respondent to the number of children not living with the respondent, to get
    the total number of children. It uses the `childathome` and `childnotathome` key
    values in the `self.respdict` dictionary:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的方法，它将与响应者同住的孩子数和不与响应者同住的孩子数相加，得到孩子的总数。它使用`self.respdict`字典中的`childathome`和`childnotathome`键值：
- en: '[PRE65]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Add a method for calculating average weeks worked across the 25 years of the
    survey.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个方法，用于计算调查中25年期间的平均工作周数。
- en: 'Use dictionary comprehension to create a dictionary (`workdict`) of the weeks-worked
    keys that do not have missing values. Sum the values in `workdict` and divide
    that by the length of `workdict`:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用字典推导式创建一个字典（`workdict`），其中包含没有缺失值的工作周数键。将`workdict`中的值相加并除以`workdict`的长度：
- en: '[PRE66]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Add a method for calculating age as of a given date.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个方法，用于计算某个特定日期的年龄。
- en: This method takes a date string (`bydatestring`) to use for the end date of
    the age calculation. We use the `datetime` module to convert the `date` string
    to a `datetime` object, `bydate`. We subtract the birth year value in `self.respdict`
    from the year of `bydate`, subtracting 1 from that calculation if the birth date
    has not happened yet that year. (We only have birth month and birth year in the
    NLS data, so we choose 15 as a midpoint.)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法采用一个日期字符串（`bydatestring`）作为年龄计算的结束日期。我们使用`datetime`模块将`date`字符串转换为`datetime`对象`bydate`。我们从`self.respdict`中减去出生年份值，并从该计算中减去1，如果出生日期在该年还没有发生。（我们在NLS数据中只有出生月和出生年，所以我们选择15作为中间点。）
- en: '[PRE67]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Add a method to create a flag if the respondent ever enrolled at a 4-year college.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个方法，如果响应者曾经在4年制大学注册过，则创建一个标志。
- en: 'Use dictionary comprehension to check whether any college enrollment values
    are at a 4-year college:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 使用字典推导式检查是否有任何大学注册值为4年制大学：
- en: '[PRE68]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Import the respondent class.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入响应者类。
- en: Now we are ready to instantiate some `Respondent` objects! Let’s do that from
    the `class_cleaning.py` file we started in *step 1*. We start by importing the
    respondent class. (This step assumes that `respondent.py` is in the `helperfunctions`
    subfolder.)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好实例化一些`Respondent`对象了！我们从*步骤1*开始的`class_cleaning.py`文件中进行操作。首先，我们导入响应者类。（此步骤假设`respondent.py`文件位于`helperfunctions`子文件夹中。）
- en: '[PRE69]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Load the NLS data and create a list of dictionaries.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载NLS数据并创建字典列表。
- en: 'Use the `to_dict` method to create the list of dictionaries (`nls97list`).
    Each row from the DataFrame will be a dictionary with column names as keys. Show
    part of the first dictionary (the first row):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`to_dict`方法创建字典列表（`nls97list`）。DataFrame中的每一行将是一个字典，列名作为键。显示第一个字典的一部分（第一行）：
- en: '[PRE70]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Loop through the list, creating a `respondent` instance each time.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历列表，每次创建一个`respondent`实例。
- en: 'We pass each dictionary to the respondent class, `rp.Respondent(respdict)`.
    Once we have created a respondent object (`resp`), we can then use all of the
    instance methods to get the values we need. We create a new dictionary with those
    values returned by instance methods. We then append that dictionary to `analysisdict`:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个字典传递给`rp.Respondent(respdict)`响应者类。一旦我们创建了一个响应者对象（`resp`），我们就可以使用所有实例方法来获取我们需要的值。我们使用这些方法返回的值创建一个新的字典，然后将该字典追加到`analysisdict`中：
- en: '[PRE76]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Pass the dictionary to the pandas `DataFrame` method.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将字典传递给pandas `DataFrame`方法。
- en: 'First, check the number of items in `analysislist` and the number of instances
    created:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，检查`analysislist`中的项数和创建的实例数：
- en: '[PRE77]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: These steps demonstrated how to create a class in Python, how to pass data to
    a class, how to create an instance of a class, and how to call the methods of
    the class to update variable values.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤展示了如何在Python中创建一个类，如何向类传递数据，如何创建类的实例，以及如何调用类的方法来更新变量值。
- en: How it works...
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The key work in this recipe is done in *step 2*. It creates the respondent class
    and sets us up well for the remaining steps. We pass a dictionary with the values
    for each row to the class’s `__init__` method. The `__init__` method assigns that
    dictionary to an instance variable that will be available to all of the class’s
    methods (`self.respdict = respdict`).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方中的关键工作在*第2步*中完成。它创建了响应者类，并为剩余的步骤做好了准备。我们将每行的值传递给该类的`__init__`方法。`__init__`方法将该字典分配给一个实例变量，该变量对所有类的方法都可用（`self.respdict
    = respdict`）。
- en: '*Steps 3* through *6* use that dictionary to calculate the number of children,
    average weeks worked per year, age, and college enrollment. *Steps 4* and *6*
    show how helpful dictionary comprehensions are when we need to test for the same
    value over many keys. The dictionary comprehensions select the relevant keys,
    `weeksworked##`, `colenroct##`, and `colenrfeb##`, and allow us to inspect the
    values of those keys. This is incredibly useful when we have data that is untidy
    in this way, as survey data often is.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '*第3步*到*第6步*使用该字典来计算子女数量、每年平均工作周数、年龄和大学入学情况。*第4步*和*第6步*展示了字典推导式在我们需要对多个键测试相同值时的有用性。字典推导式选择相关的键（`weeksworked##`、`colenroct##`、`colenrfeb##`），并允许我们检查这些键的值。当我们的数据以这种方式杂乱无章时，这非常有用，正如调查数据常常表现出来的那样。'
- en: In *step 8*, we create a list of dictionaries with the `to_dict` method. It
    has the expected number of list items, 8,984, the same as the number of rows in
    the DataFrame. We use `pprint` to show what the dictionary looks like for the
    first list item. The dictionary has keys for the column names and values for the
    column values.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第8步*中，我们使用`to_dict`方法创建一个字典列表。它包含预期数量的列表项，8,984，与DataFrame中的行数相同。我们使用`pprint`来显示字典在第一个列表项中的样子。该字典具有列名作为键，列值作为值。
- en: We iterate over the list in *step 9*, creating a new respondent object and passing
    the list item. We call the methods to get the values we want, except for `originalid`,
    which we can pull directly from the dictionary. We create a dictionary (`newdict`)
    with those values, which we append to a list (`analysislist`).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第9步*中遍历列表，创建一个新的响应者对象并传递列表项。我们调用方法来获取我们想要的值，除了`originalid`，它可以直接从字典中提取。我们用这些值创建一个字典（`newdict`），并将其追加到列表（`analysislist`）中。
- en: In *step 10*, we create a pandas DataFrame from the list (`analysislist`) we
    created in *step 9*. We do this by passing the list to the pandas DataFrame method.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第10步*中，我们从在*第9步*创建的列表（`analysislist`）中创建一个pandas DataFrame。我们通过将列表传递给pandas
    DataFrame方法来完成这一操作。
- en: There’s more...
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We pass dictionaries to the class rather than data rows, which is also a possibility.
    We do this because navigating a NumPy array is more efficient than looping over
    a DataFrame with `itertuples` or `iterrows`. We do not lose much of the functionality
    needed for our class when we work with dictionaries rather than DataFrame rows.
    We are still able to use functions such as `sum` and `mean` and count the number
    of values meeting certain criteria.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递字典给类，而不是数据行，这也是一种选择。我们这么做是因为，相比于通过 `itertuples` 或 `iterrows` 遍历 DataFrame，遍历
    NumPy 数组更高效。当我们使用字典而非 DataFrame 行时，我们并没有失去类所需的太多功能。我们依然可以使用诸如 `sum` 和 `mean` 等函数，并计算符合特定条件的值的数量。
- en: It is hard to avoid having to iterate over data with this conceptualization
    of a respondent class. This respondent class is consistent with our understanding
    of the unit of analysis, the survey respondent. That is also, unsurprisingly,
    how the data comes to us. But iterating over data one row at a time is resource-intensive,
    even with more efficient NumPy arrays.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种响应者类的概念化中，很难避免必须遍历数据。这个响应者类与我们对分析单元——调查响应者——的理解是一致的。这也正是数据呈现给我们的方式。但即使是更高效的
    NumPy 数组，逐行遍历数据也是资源密集型的。
- en: I would argue, however, that you gain more than you lose by constructing a class
    like this one when working with data with many columns and with a structure that
    does not change much over time. The most important advantage is that it matches
    our intuition about the data and focuses our work on understanding the data for
    each respondent. I also think we find that when we construct the class well, we
    do far fewer passes through the data than we otherwise might.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我认为，在处理具有许多列且结构在时间上变化不大的数据时，通过构建像这样的类，你获得的好处超过了失去的。最重要的优势在于它符合我们对数据的直观理解，并将我们的工作集中在理解每个响应者的数据上。我还认为，当我们构建类时，通常会比否则情况减少很多遍历数据的次数。
- en: See also
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: We examine navigating over DataFrame rows and NumPy arrays in *Chapter 9*, *Fixing
    Messy Data When Aggregating*.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *第 9 章*，*聚合时修复杂乱数据* 中，探讨了如何遍历 DataFrame 行和 NumPy 数组。
- en: This was a very quick introduction to working with classes in Python. If you
    would like to learn more about object-oriented programming in Python, I would
    recommend *Python 3 Object-Oriented Programming*, *Third Edition* by Dusty Phillips.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一篇关于在 Python 中使用类的简要介绍。如果你想深入了解 Python 中的面向对象编程，我推荐 Dusty Phillips 编写的 *Python
    3 面向对象编程*，*第三版*。
- en: Classes that handle non-tabular data structures
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理非表格数据结构的类
- en: Data scientists increasingly receive non-tabular data, often in the form of
    JSON or XML files. The flexibility of JSON and XML allows organizations to capture
    complicated relationships between data items in one file. A one-to-many relationship
    stored in two tables in an enterprise data system can be represented well in JSON
    by a parent node for the one side and child nodes for data on the many side.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家越来越多地接收到非表格数据，通常是 JSON 或 XML 文件。JSON 和 XML 的灵活性使得组织能够在一个文件中捕捉数据项之间复杂的关系。在企业数据系统中存储在两个表中的一对多关系，可以通过
    JSON 通过一个父节点来表示一方，多个子节点来表示多方数据，来很好地表示。
- en: When we receive JSON data we often start by trying to normalize it. Indeed,
    we do that in a couple of recipes in this book. We try to recover the one-to-one
    and one-to-many relationships in the data obfuscated by the flexibility of JSON.
    But there is another way to work with such data, one that has many advantages.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们接收 JSON 数据时，我们通常会尝试对其进行规范化。事实上，在本书的一些实例中，我们就是这么做的。我们尝试恢复由于 JSON 灵活性而混淆的数据中的一对一和一对多关系。但也有另一种处理这种数据的方式，它有许多优势。
- en: Instead of normalizing the data, we can create a class that instantiates objects
    at the appropriate unit of analysis, and use the methods of the class to navigate
    the many side of one-to-many relationships. For example, if we get a JSON file
    that has student nodes and then multiple child nodes for each course taken by
    a student, we would usually normalize that data by creating a student file and
    a course file, with student ID as the merge-by column on both files. An alternative,
    which we explore in this recipe, would be to leave the data as it is, create a
    student class, and create methods that do calculations on the child nodes, such
    as calculating total credits taken.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个类，在适当的分析单元上实例化对象，并使用类的方法来导航一对多关系的多个方面，而不是规范化数据。例如，如果我们获取一个包含学生节点的JSON文件，并且每个学生所修课程都有多个子节点，我们通常会通过创建一个学生文件和一个课程文件来规范化数据，学生ID作为两个文件的合并列。在本例中，我们将探讨另一种方法：保持数据原样，创建一个学生类，并创建方法对子节点进行计算，例如计算总学分。
- en: Let’s try that with this recipe using data from the Cleveland Museum of Art
    that has collection items, one or more nodes for media citations for each item,
    and one or more nodes for each creator of the item.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过这个食谱来尝试，使用来自克利夫兰艺术博物馆的数据，其中包含了收藏项目、每个项目的一个或多个媒体引文节点，以及每个项目的一个或多个创作者节点。
- en: Getting ready
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe assumes you have the `requests` and `pprint` libraries. If they
    are not installed, you can install them with `pip`. From Terminal, or PowerShell
    (in Windows), enter `pip install requests` and `pip install pprint`.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例假设你已经安装了`requests`和`pprint`库。如果没有安装，可以通过`pip`进行安装。在终端或PowerShell（Windows系统中）输入`pip
    install requests`和`pip install pprint`。
- en: I show here the structure of the JSON file that is created when using the `collections`
    API of the Cleveland Museum of Art (I have abbreviated the JSON file to save space).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了使用克利夫兰艺术博物馆（Cleveland Museum of Art）`collections` API时创建的JSON文件结构（我已将JSON文件缩短以节省空间）。
- en: '[PRE85]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '**Data note**'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据说明**'
- en: 'The Cleveland Museum of Art provides an API for public access to this data:
    [https://openaccess-api.clevelandart.org/](https://openaccess-api.clevelandart.org/).
    Much more than the citations and creators data used in this recipe is available
    with the API.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 克利夫兰艺术博物馆提供了一个API，允许公众访问这些数据：[https://openaccess-api.clevelandart.org/](https://openaccess-api.clevelandart.org/)。通过API可以访问的内容远远超过本例中使用的引文和创作者数据。
- en: How to do it...
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We create a collection item class that summarizes the data we need on creators
    and media citations:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个收藏项目类，用于汇总创作者和媒体引文的数据：
- en: Import the `pandas`, `json`, `pprint`, and `requests` libraries.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`json`、`pprint`和`requests`库。
- en: 'Let’s first create a file that we will use to instantiate collection item objects
    and call it `class_cleaning_json.py`:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个文件，用来实例化收藏项目对象，命名为`class_cleaning_json.py`：
- en: '[PRE86]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Create a `Collectionitem` class.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`Collectionitem`类。
- en: 'We pass a dictionary for each collection item to the `__init__` method of the
    class, which runs automatically when an instance of the class is created. We assign
    the collection item dictionary to an instance variable. Save the class as `collectionitem.py`
    in the `helperfunctions` folder:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个收藏项目的字典传递给类的`__init__`方法，该方法在类的实例化时会自动运行。我们将收藏项目字典分配给实例变量。将该类保存为`collectionitem.py`文件，并放置在`helperfunctions`文件夹中：
- en: '[PRE87]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Create a method to get the birth year of the first creator for each collection
    item.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个方法来获取每个收藏项目的第一个创作者的出生年份。
- en: 'Remember that collection items can have multiple creators. This means that
    the `creators` key has one or more list items as values, and these items are themselves
    dictionaries. To get the birth year of the first creator, then, we need `[''creators''][0][''birth_year'']`.
    We also need to allow for the birth year key to be missing, so we test for that
    first:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，收藏项目可能有多个创作者。这意味着`creators`键的值可能是一个或多个列表项，这些项本身是字典。要获取第一个创作者的出生年份，我们需要`['creators'][0]['birth_year']`。我们还需要考虑出生年份键可能缺失的情况，因此首先要进行检查：
- en: '[PRE88]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Create a method to get the birth years for all creators.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个方法来获取所有创作者的出生年份。
- en: 'Use list comprehension to loop through all the `creators` items. This will
    return the birth years as a list:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列表推导式循环遍历所有`creators`项。这将返回一个包含出生年份的列表：
- en: '[PRE89]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Create a method to count the number of creators:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个方法来统计创作者的数量：
- en: '[PRE90]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Create a method to count the number of media citations:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个方法来统计媒体引文的数量：
- en: '[PRE91]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Import the `collectionitem` module.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`collectionitem`模块。
- en: 'We do this from the `class_cleaning_json.py` file we created in *step 1*:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从*步骤1*创建的`class_cleaning_json.py`文件中执行此操作：
- en: '[PRE92]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Load the art museum’s collections data.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载艺术博物馆的收藏数据。
- en: 'This returns a list of dictionaries. We just pull a subset of the museum collections
    data with African American artists:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回的是一个字典列表。我们只提取了带有非裔美国艺术家数据的博物馆收藏子集：
- en: '[PRE93]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Loop through the `camcollections` list.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历`camcollections`列表。
- en: Create a collection item instance for each item in `camcollections`. Pass each
    item, which is a dictionary of collections, creators, and citation keys, to the
    class. Call the methods we have just created and assign the values they return
    to a new dictionary (`newdict`). Append that dictionary to a list (`analysislist`).
    (Some of the values can be pulled directly from the dictionary, such as with `title=colldict['title']`,
    since we do not need to change the value in any way.)
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为`camcollections`中的每个项目创建一个集合项实例。将每个项目（即包含集合、创作者和引用键的字典）传递给类。调用我们刚刚创建的方法，并将它们返回的值分配给一个新的字典（`newdict`）。将该字典附加到一个列表（`analysislist`）中。（一些值可以直接从字典中提取，如`title=colldict['title']`，因为我们不需要以任何方式更改其值。）
- en: '[PRE94]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Create an analysis DataFrame with the new list of dictionaries.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的字典列表创建一个分析DataFrame。
- en: 'Confirm that we are getting the correct counts, and print the dictionary for
    the first item:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 确认我们获得了正确的计数，并打印第一个项目的字典：
- en: '[PRE95]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: These steps give a sense of how we can use classes to handle non-tabular data.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤展示了我们如何使用类来处理非表格数据。
- en: How it works...
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: This recipe demonstrated how to work directly with a JSON file, or any file
    with implied one-to-many or many-to-many relationships. We created a class at
    the unit of analysis (a collection item, in this case) and then created methods
    to summarize multiple nodes of data for each collection item.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱展示了如何直接处理JSON文件，或任何包含隐式一对多或多对多关系的文件。我们在分析单元（本例中为一个集合项）中创建了一个类，然后创建了方法来汇总每个集合项的多个数据节点。
- en: The methods we created in *steps 3* to *6* are satisfyingly straightforward.
    When we first look at the structure of the data, displayed in the *Getting ready*
    section of this recipe, it is hard not to feel that it will be really difficult
    to clean. It looks like anything goes. But it turns out to have a fairly reliable
    structure. We can count on one or more child nodes for `creators` and `citations`.
    Each `creators` and `citations` node also has child nodes, which are key and value
    pairs. These keys are not always present, so we need to first check to see whether
    they are present before trying to grab their values. We did this in *step 3*.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*步骤3*到*步骤6*中创建的方法非常简洁。当我们第一次查看数据的结构时，它在本食谱的*准备工作*部分中展示，确实很难不觉得它会非常难以清理。看起来似乎什么都行。但事实证明，它有一个相对可靠的结构。我们可以依赖`creators`和`citations`中的一个或多个子节点。每个`creators`和`citations`节点也有子节点，这些子节点是键值对。这些键不总是存在，所以我们需要先检查它们是否存在，然后再尝试获取它们的值。这就是我们在*步骤3*中所做的。
- en: There’s more...
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: I go into some detail about the advantages of working directly with JSON files
    in *Chapter 2*, *Anticipating Data Cleaning Issues When Working with HTML, JSON,
    and Spark Data*. I think the museum’s collections data is a good example of why
    we might want to stick with the JSON if we can. The structure of the data actually
    makes sense, even if it is in a very different form. There is always a danger
    when we try to normalize it that we will miss some aspects of its structure.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我在*第二章*《在处理HTML、JSON和Spark数据时预见数据清理问题》中详细讨论了直接处理JSON文件的优势。我认为博物馆的收藏数据是一个很好的例子，说明了为什么如果可能的话，我们可能更愿意坚持使用JSON格式。即使数据的形式非常不同，它的结构实际上是有意义的。当我们试图将其规范化时，始终存在一个风险，那就是我们可能会遗漏其结构的某些方面。
- en: Functions for checking overall data quality
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于检查整体数据质量的函数
- en: We can tighten up our data quality checks by being more explicit and upfront
    about what we are evaluating. We likely have some expectations about the distribution
    of variable values, about the range of allowable values, and about the number
    of missing values very early in a data analysis project. This may come from documentation,
    our knowledge of the underlying real-world processes represented by the data,
    or our understanding of statistics. It is a good idea to have a routine for delineating
    those initial assumptions, testing them, and then revising assumptions throughout
    a project. This recipe will demonstrate what that process might look like.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过更明确地说明我们正在评估的内容来加强数据质量检查。我们在数据分析项目的初期，可能已经对变量值的分布、允许值的范围以及缺失值的数量有了一些预期。这些预期可能来自文档、我们对数据所代表的基础现实世界过程的理解，或者我们对统计学的理解。建立一个常规流程，用于明确这些初步假设、测试它们并在项目过程中修订假设是个不错的主意。本节将演示这个过程可能是什么样的。
- en: 'We set up data quality targets for each variable of interest. This includes
    allowable values and thresholds for missing values for categorical variables.
    It also includes ranges of values; missing value, skewness, and kurtosis thresholds;
    and checking for outliers for numeric values. We will check unique identifier
    variables for duplication and for missing values. We start with the assumptions
    in this CSV file about variables on the NLS file:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个感兴趣的变量设定了数据质量目标。这包括类别变量的允许值和缺失值的阈值，也包括数值的取值范围、缺失值、偏度和峰度阈值，并检查异常值。我们将检查唯一标识符变量是否存在重复和缺失值。我们从这个CSV文件中的假设开始，关于NLS文件中的变量：
- en: '![dct](img/B18596_12_03.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![dct](img/B18596_12_03.png)'
- en: 'Figure 12.3: Data checks for selected NLS columns'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：对选定NLS列的数据检查
- en: '*Figure 12.3* shows our initial assumptions. For example, for `maritalstatus`,
    we assume the category values *Divorced|Married|Never-married|Separated|Widowed*,
    and that no more than 20% of values will be missing. For `nightlyhrssleep`, a
    numeric variable, we assume that values will be between 3 and 9, that no more
    than 30% of values will be missing, and that it will have a skew and kurtosis
    close to that of a normal distribution.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.3*展示了我们的初步假设。例如，对于`maritalstatus`，我们假设类别值为*离婚|已婚|从未结婚|分居|寡妇*，且不超过20%的值将缺失。对于`nightlyhrssleep`（一个数值变量），我们假设值将在3到9之间，不超过30%的值将缺失，且其偏度和峰度接近正态分布。'
- en: We also indicate that we want to check for outliers. The final column is a flag
    we can use if we only want to do data checks for a few variables. Here we indicate
    that we want to do checks for `maritalstatus`, `originalid`, `highestgradecompleted`,
    `gpaenglish`, and `nightlyhrssleep`.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还指明了我们想要检查异常值。最后一列是一个标志，如果我们只想对某些变量进行数据检查，可以使用它。在这里，我们指出要对`maritalstatus`、`originalid`、`highestgradecompleted`、`gpaenglish`和`nightlyhrssleep`进行检查。
- en: Getting ready
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will work again with the NLS data in this recipe.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中再次使用NLS数据。
- en: How to do it...
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: We use our predefined data-checking targets to analyze selected variables in
    the NLS data.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用预定义的数据检查目标来分析选定的NLS数据变量。
- en: 'Create the functions we will need for data checking and save them in the `helperfunctions`
    subfolder with the name `runchecks.py`. The following two functions, `checkcats`
    and `checkoutliers`, will be used to test values in a list and outliers respectively.
    We will see how that works in subsequent steps:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建我们需要的函数进行数据检查，并将其保存在`helperfunctions`子文件夹中，命名为`runchecks.py`。以下两个函数，`checkcats`和`checkoutliers`，将分别用于测试列表中的值和异常值。我们将在接下来的步骤中看到它是如何工作的：
- en: '[PRE105]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'We then define a function to run all of our checks, `runchecks`, which will
    take a DataFrame (`df`), our data targets (`dc`), a list of numerical columns
    (`numvars`), a list of categorical columns (`catvars`), and a list of identifier
    columns (`idvars`):'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们定义一个函数来运行所有的检查，`runchecks`，它将接受一个DataFrame（`df`）、我们的数据目标（`dc`）、一个数值列列表（`numvars`）、一个类别列列表（`catvars`）和一个标识符列列表（`idvars`）：
- en: '[PRE106]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Within the `runchecks` function, we loop over the categorical variable columns
    in our data checks. We get the values of all targets for the variable with `dcvals
    = dc.loc[col]`. We create a NumPy array, `compcat`, from the category values.
    We then compare that array to all of the values for that column in the passed
    DataFrame (`df[col].dropna().str.strip().unique()`). If there is a category in
    one array but not the other (`valuediff`) we print that to the console. We also
    calculate the missing-value percentage. If it is beyond the threshold we specified,
    we print a message:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`runchecks`函数中，我们对数据检查中的分类变量列进行循环。我们通过`dcvals = dc.loc[col]`获取该变量的所有目标值。我们从类别值中创建一个NumPy数组`compcat`，然后将该数组与传入DataFrame中该列的所有值进行比较（`df[col].dropna().str.strip().unique()`）。如果一个数组中有而另一个数组中没有某个类别（即`valuediff`），我们会将其打印到控制台。我们还计算缺失值百分比。如果超出了我们指定的阈值，我们会打印一条消息：
- en: '[PRE107]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: Let’s now look at the loop for checking the numeric variables. We create a NumPy
    array from the range value in our data-checking targets, `range = np.fromstring(dcvals.range,
    sep='|')`. The first element of `range` is the lower end of the range. The second
    element is the upper end. We then get the min and max values for the variable
    from the DataFrame and compare those with the range indicated in the target file.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们看一下检查数值变量的循环。我们从数据检查目标中的范围值创建一个NumPy数组，`range = np.fromstring(dcvals.range,
    sep='|')`。`range`的第一个元素是范围的下限，第二个元素是上限。然后，我们从DataFrame中获取变量的最小值和最大值，并将其与目标文件中指示的范围进行比较。
- en: We calculate the missing-values percentage and print if it exceeds the threshold
    we set in the data-checking target file.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算缺失值百分比，并在超过数据检查目标文件中设置的阈值时打印出来。
- en: 'We show outliers if the `showoutliers` flag is set to `Y`. We use the `checkoutliers`
    function we set up earlier, which uses a simple interquartile range calculation
    to determine outliers. Finally, we check the skew and kurtosis to get an indication
    of how far from normally distributed the variable might be:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`showoutliers`标志被设置为`Y`，我们将显示离群值。我们使用之前设置的`checkoutliers`函数，该函数通过简单的四分位距计算来确定离群值。最后，我们检查偏度和峰度，以便了解该变量与正态分布的差距：
- en: '[PRE108]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'We do a couple of straightforward checks for variables identified as id variables
    in the targets file. We look to see if the variable is duplicated and check for
    missing values:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于目标文件中标识为id变量的变量，我们进行一些简单的检查。我们检查该变量是否有重复值，并检查是否有缺失值：
- en: '[PRE109]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Now we are ready to run the data checks. We start by loading the NLS DataFrame
    and the data-checking targets.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们准备开始运行数据检查。我们首先加载NLS DataFrame和数据检查目标。
- en: '[PRE110]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: We import the `runchecks` module we just created.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入刚刚创建的`runchecks`模块。
- en: '[PRE111]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: Let’s mess up some of the id variable values for testing the code. We also fix
    logical missing values for `highestgradecompleted`, setting them to actual missing
    values.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们故意破坏一些id变量值来测试代码。我们还修复了`highestgradecompleted`的逻辑缺失值，将其设置为实际缺失值。
- en: '[PRE112]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'We select just those targets flagged to be included. We then create categorical
    variable, numeric variable, and id variable lists based on the data-checking targets
    file:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只选择那些被标记为包含的目标。然后，我们根据数据检查目标文件创建分类变量、数值变量和id变量列表：
- en: '[PRE117]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: Now, we are ready to run the checks.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备开始运行检查。
- en: '[PRE118]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'This produces the following output:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![datachecksoutput](img/B18596_12_04.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![datachecksoutput](img/B18596_12_04.png)'
- en: 'Figure 12.4: Running the checks'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4：运行检查
- en: We see that `maritalstatus` has more missings (26%) than the 20% threshold we
    set. `highestgradecompleted` and `gpaoverall` have values that exceed the anticipated
    range. The kurtosis for both variables is low. `nightlyhrssleep` has outliers
    substantially below and above the interquartile range. 31 respondents have `nightlyhrssleep`
    of 2 or less. 27 respondents have very high `nightlyhrssleep`, of 12 or more.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现`maritalstatus`的缺失值比我们设置的20%的阈值（26%）多。`highestgradecompleted`和`gpaoverall`的值超出了预期范围。两个变量的峰度较低。`nightlyhrssleep`有显著低于和高于四分位距的离群值。31名受访者的`nightlyhrssleep`为2小时或更少。27名受访者的`nightlyhrssleep`非常高，达到12小时或更多。
- en: These steps show how we can use our prior domain knowledge and understanding
    of statistics to better target our investigation of data quality.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤展示了我们如何利用之前的领域知识和统计学理解，更好地针对数据质量问题进行调查。
- en: How it works...
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We created a CSV file with our data-checking targets. We used that file in our
    checks of the NLS data. We did that by passing both the NLS DataFrame and the
    data-checking targets to `runchecks`. Code in `runchecks` loops through the column
    names from the data-checking file and does checks based on the type of variable.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个包含数据检查目标的CSV文件，并在检查NLS数据时使用了该文件。我们通过将NLS DataFrame和数据检查目标一起传递给`runchecks`来完成此操作。`runchecks`中的代码遍历数据检查文件中的列名，并根据变量类型进行检查。
- en: The targets for each variable are defined by `dcvals = dc.loc[col]`, which grabs
    all of the target values for that row in the target file. We can then refer to
    `dcvals.missingthreshold` to get the missing-values threshold, for example. We
    then compare the percentage of missing values (`df[col].isnull().sum()/df.shape[0]`)
    to the missing threshold, and print a message if the missing-value percentage
    is greater than the threshold. We do the same type of checking for range of values,
    skew, outliers, and so on, depending on the type of variable.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 每个变量的目标由`dcvals = dc.loc[col]`定义，它抓取目标文件中该行的所有目标值。然后我们可以引用`dcvals.missingthreshold`来获取缺失值阈值，例如。接着，我们将缺失值的百分比（`df[col].isnull().sum()/df.shape[0]`）与缺失阈值进行比较，如果缺失值百分比大于阈值，就打印一条消息。我们对值的范围、偏斜、异常值等进行相同类型的检查，具体取决于变量的类型。
- en: We can add new variables to the data-checking targets file without changing
    any of the code in `runchecks`. We can also change target values.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在不更改`runchecks`代码的情况下，向数据检查目标文件添加新变量。我们还可以更改目标值。
- en: There’s more...
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: It is sometimes important to be proactive with our data checks. There is a difference
    between displaying some sample statistics and frequency distributions to get a
    general sense of the data, and marshaling our domain knowledge and understanding
    of statistics for careful examination of data quality. A more intentional approach
    might require us to occasionally step away from our Python development environment
    for a moment to reflect on our expectations for data values and their distribution.
    Setting up initial data quality targets, and revisiting them regularly, can help
    us do that.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们需要主动进行数据检查。展示一些样本统计数据和频率分布以大致了解数据，和动用我们领域知识和统计理解来仔细检查数据质量是不同的。一个更有目的的方法可能要求我们偶尔从Python开发环境中抽离片刻，反思我们对数据值及其分布的预期。设置初始的数据质量目标，并定期回顾这些目标，可以帮助我们做到这一点。
- en: 'Pre-processing data with pipelines: a simple example'
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用管道进行数据预处理：一个简单的示例
- en: When doing predictive analysis, we often need to fold all of our pre-processing
    and feature engineering into a pipeline, including scaling, encoding, and handling
    outliers and missing values. We discussed the reasons why we might need to incorporate
    all of our data preparation into a data pipeline in *Chapter 8*, *Encoding, Transforming,
    and Scaling Features*. The main takeaway from that chapter is that pipelines are
    critical when we are building explanatory models and need to avoid data leakage.
    This can be trickier still when we are using *k*-fold cross-validation for model
    validation, since testing and training DataFrames change during evaluation. Cross-validation
    has become the norm when constructing predictive models.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行预测分析时，我们通常需要将所有的预处理和特征工程折叠进管道中，包括缩放、编码以及处理异常值和缺失值。我们在*第8章*，*编码、转换和缩放特征*中讨论了为什么我们可能需要将所有数据准备工作纳入数据管道。那一章的主要观点是，当我们构建解释性模型并且需要避免数据泄漏时，管道至关重要。尤其是当我们使用*k*-折交叉验证进行模型验证时，这一问题更加复杂，因为在评估过程中，测试和训练的DataFrame会发生变化。交叉验证已成为构建预测模型的标准做法。
- en: '**Note**'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*k*-fold cross-validation trains our model on all but one of the *k* folds,
    or parts, leaving one out for testing. This is repeated *k* times, each time excluding
    a different fold for testing. Performance metrics are then based on the average
    scores across the *k* folds.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-折交叉验证通过对所有的*k*折（或部分）进行训练，留下一个折用于测试。这会重复*k*次，每次排除不同的折进行测试。性能度量是基于*k*折的平均得分。'
- en: Another benefit of pipelines is that they help us ensure reproducible results,
    as they are often intended to take our analysis from raw data to model evaluation.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的另一个好处是，它们有助于确保结果的可重复性，因为它们通常旨在将我们的分析从原始数据带到模型评估。
- en: Although this recipe demonstrates how to use a pipeline all the way through
    model evaluation, we will not go into detail there. A good resource for both model
    evaluation and pipelines with scikit-learn tools is the book *Data Cleaning and
    Exploration with Machine Learning*, also written by me.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个配方演示了如何通过管道来进行模型评估，但我们不会在此详细讨论。关于模型评估和使用scikit-learn工具的管道，一个很好的资源是我写的书《数据清理与机器学习探索》。
- en: We start with a relatively simple example here, a model with two numeric features
    and one numeric target. We work on a much more complicated example in the next
    recipe.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个相对简单的例子开始，这里有一个包含两个数值特征和一个数值目标的模型。在下一个配方中，我们将处理一个更加复杂的例子。
- en: Getting ready
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will work with scikit-learn’s pipeline tool in this recipe, and a few other
    modules for encoding and scaling the data, and imputing values for missing data.
    We will work with land temperature data again in this recipe.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用scikit-learn的管道工具，并结合其他一些模块来对数据进行编码、缩放，以及填补缺失值。我们将再次使用土地温度数据。
- en: How to do it...
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We start by loading the `scikit-learn` modules we will be using in this recipe
    for transforming our data. We will use `StandardScaler` to standardize our features,
    `SimpleImputer` to impute values for missing data, and `make_pipeline` to pull
    all of our pre-processing together. We also use `train_test_split` to create training
    and testing DataFrames. I’ll discuss the other modules as we use them:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载本配方中将要使用的`scikit-learn`模块，用于转换我们的数据。我们将使用`StandardScaler`来标准化特征，使用`SimpleImputer`填补缺失值，并使用`make_pipeline`将所有预处理步骤组合在一起。我们还使用`train_test_split`来创建训练和测试数据框。其他模块将在使用时进行讲解：
- en: '[PRE119]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'We load the land temperature data and create training and testing DataFrames.
    We will try to model temperature as a function of latitude and elevation. Given
    the very different ranges of the `latabs` and `elevation` variables, scaling will
    be important:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载土地温度数据并创建训练和测试数据框。我们将尝试将温度建模为纬度和海拔的函数。考虑到`latabs`和`elevation`变量的范围差异，数据缩放会变得非常重要：
- en: '[PRE120]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: For an introduction to `train_test_split`, see *Chapter 8*, *Encoding, Transforming,
    and Scaling Features*.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`train_test_split`的介绍，请参见*第8章*，*特征编码、转换和缩放*。
- en: 'We set up *k*-fold cross-validation. We indicate that we want five folds and
    for the data to be shuffled:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了*k*-折交叉验证。我们指定希望有五个折叠，并且数据需要被打乱：
- en: '[PRE121]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Now, we are ready to set up our pipeline. The pipeline will do standard scaling,
    impute the mean when values are missing, and then run a linear regression model.
    Both features will be handled in the same way.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备好设置管道了。管道将进行标准缩放，当值缺失时填补均值，然后运行线性回归模型。两个特征将以相同的方式处理。
- en: '[PRE122]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: After constructing the pipeline, and instantiating a *k*-fold cross-validation
    object, we are ready to run the pre-processing, estimate the model, and generate
    evaluation metrics. We pass the pipeline to the `cross_validate` function, as
    well as our training data. We also pass the `Kfold` object we created in *step
    3*. We get a pretty decent *R*-squared value.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建好管道并实例化*k*-折交叉验证对象后，我们准备好执行预处理、估算模型并生成评估指标。我们将管道传递给`cross_validate`函数，并传入我们的训练数据。我们还传递了在*步骤3*中创建的`Kfold`对象。我们得到了一个相当不错的*R*平方值。
- en: '[PRE123]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: How it works...
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We used scikit-learn’s `make_pipeline` to create a pipeline with just three
    steps: apply standard scaling, impute values for missing data based on the mean
    for that variable, and fit a linear regression model. What is so helpful about
    pipelines is that they automatically feed a transformation from one step into
    the next step. This is easy to do, once we get the hang of it, despite the complication
    of *k*-fold cross-validation.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了scikit-learn的`make_pipeline`来创建一个仅包含三步的管道：应用标准缩放、基于该变量的均值填补缺失值，并拟合一个线性回归模型。管道的一个很有用的地方是它们会自动将一个步骤的转换结果传递给下一个步骤。一旦我们掌握了这项操作，尽管有*k*-折交叉验证的复杂性，这个过程仍然很简单。
- en: We can imagine for a moment how messy it would be to write our own code to do
    this when the training and testing DataFrames are changing, as with *k*-fold cross-validation.
    Even something as simple as using the mean for imputation is tricky. We would
    need to take a new mean for the training data each time the training data changed.
    Our pipeline handles all of this for us.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以想象一下，当训练和测试数据框不断变化时（比如*k*-折交叉验证中那样），如果我们自己编写代码来处理这些数据将会有多么复杂。即使是使用均值进行填补这样简单的操作也是棘手的。每次训练数据发生变化时，我们都需要重新计算训练数据的均值。我们的管道会自动处理这些问题。
- en: That was a relatively straightforward example, with just a couple of features
    that we could handle in the same way. We also did not bother to check for outliers
    or scale the target variable. We use a pipeline to handle a much trickier modeling
    project in the next recipe.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 那是一个相对简单的例子，只有几个我们可以用相同方式处理的特征。我们也没有检查离群值或对目标变量进行缩放。在下一个例子中，我们将使用管道处理一个更复杂的建模项目。
- en: 'Pre-processing data with pipelines: a more complicated example'
  id: totrans-419
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用管道进行数据预处理：一个更复杂的例子
- en: If you have ever built a data pipeline, you know that it can be a little messy
    when you are working with several different data types. For example, we might
    need to impute the median for missing values with continuous features and the
    most frequent value for categorical features. We might also need to transform
    our target variable. We explore how to apply different pre-processing to different
    variables in this recipe.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经构建过数据管道，你就会知道，当你处理多个不同的数据类型时，事情可能会有些混乱。例如，我们可能需要对连续特征的缺失值进行中位数插补，对于类别特征则使用最频繁的值。我们可能还需要转换我们的目标变量。在这个例子中，我们将探讨如何对不同变量应用不同的预处理。
- en: Getting ready
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: We will work with a fair number of scikit-learn modules in this recipe. Although
    this can be confusing at first, you quickly become grateful that scikit-learn
    has a tool to do pretty much anything you need. Scikit-learn also allows us to
    add our own transformations to a pipeline if we need to do so. I demonstrate how
    to construct our own transformer in this recipe.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用相当多的 scikit-learn 模块。虽然一开始可能会有些混乱，但你会很快感激 scikit-learn 提供的几乎可以做任何你需要的工具。如果需要，scikit-learn
    还允许我们将自定义的转换器添加到管道中。我将在这个例子中演示如何构建我们自己的转换器。
- en: We will work with wage and employment data from the NLS.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自 NLS 的工资和就业数据。
- en: How to do it...
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: We start by loading the libraries we used in the previous recipe. Then we add
    the `ColumnTransformer` and `TransformedTargetRegressor` classes. We will use
    those classes to transform our features and target respectively.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载在前一个例子中使用的库。然后我们添加 `ColumnTransformer` 和 `TransformedTargetRegressor`
    类。我们将使用这些类分别转换我们的特征和目标变量。
- en: '[PRE125]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: The column transformer is quite flexible. We can even use it with pre-processing
    functions we have defined ourselves. The code block below imports the `OutlierTrans`
    class from the `preprocfunc` module in the `helperfunctions` subfolder.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列转换器非常灵活。我们甚至可以使用它与我们自己定义的预处理函数。下面的代码块从 `helperfunctions` 子文件夹中的 `preprocfunc`
    模块导入 `OutlierTrans` 类。
- en: '[PRE126]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: The `OutlierTrans` class identifies missing values by distance from the interquartile
    range. This is a technique we demonstrated in *Chapter 4*, *Identifying Outliers
    in Subsets of Data*, and have used multiple times in this chapter.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`OutlierTrans` 类通过与四分位距的距离来识别缺失值。这是一种我们在*第 4 章*，“*识别数据子集中的离群值*”中演示过的技术，并且在本章中多次使用。'
- en: To work in a scikit-learn pipeline our class has to have `fit` and `transform`
    methods. We also need to inherit the `BaseEstimator` and `TransformerMixin` classes.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 scikit-learn 管道中使用，我们的类必须具有 `fit` 和 `transform` 方法。我们还需要继承 `BaseEstimator`
    和 `TransformerMixin` 类。
- en: 'In this class, almost all of the action happens in the `transform` method.
    Any value that is more than 1.5 times the interquartile range above the third
    quartile or below the first quartile is assigned missing, though that threshold
    can be changed:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，几乎所有的操作都发生在 `transform` 方法中。任何高于第三四分位数或低于第一四分位数超过 1.5 倍四分位距的值都会被标记为缺失，尽管这个阈值可以更改：
- en: '[PRE127]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: Our `OutlierTrans` class can be used later in our pipeline in the same way we
    use the `StandardScaler` and other transformers. We will do that later.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `OutlierTrans` 类可以像使用 `StandardScaler` 和其他转换器一样，在我们的管道中使用。我们稍后会这样做。
- en: Now we are ready to load the data that needs to be processed. We will work with
    the NLS wage data. Wage income will be our target, and we will use high school
    GPA, mother’s and father’s highest grade completed, parent income, gender, weeks
    worked, and whether the individual completed a bachelor’s degree as features.
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们准备加载需要处理的数据。我们将使用 NLS 工资数据。工资收入将作为我们的目标变量，而我们将使用高中 GPA、母亲和父亲的最高学历、父母收入、性别、工作周数以及个人是否完成本科学位作为特征。
- en: We create lists of features to handle in different ways here. That will be helpful
    later when we instruct our pipeline to carry out different operations on numerical,
    categorical, and binary features.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里创建了处理不同方式的特征列表。稍后在我们指示管道对数值型、类别型和二进制特征进行不同操作时，这将非常有用。
- en: '[PRE128]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: Now we can set up a column transformer. We first create pipelines for handling
    numerical data (`standtrans`), categorical data, and binary data.
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以设置列转换器。我们首先创建用于处理数值数据（`standtrans`）、分类数据和二元数据的管道。
- en: For the numerical data (continuous), we want to assign outlier values to missing.
    Here we pass a value of 2 to the `threshold` parameter of `OutlierTrans`, indicating
    that we want values 2 times the interquartile range above or below that range
    to be set to missing. Recall that it is common to use 1.5, so we are being somewhat
    conservative.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值数据（连续型数据），我们希望将异常值设为缺失值。我们将值2传递给`OutlierTrans`的`threshold`参数，表示我们希望将超出四分位距2倍范围以上或以下的值设置为缺失值。请记住，通常使用1.5，所以我们这里比较保守。
- en: We do one-hot encoding of the `gender` column, essentially creating a dummy
    variable. We drop the last category to avoid the **dummy variable trap**, as discussed
    in *Chapter 8*, *Encoding, Transforming, and Scaling Features*.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对`gender`列进行了独热编码，本质上创建了一个虚拟变量。我们丢弃了最后一个类别，以避免**虚拟变量陷阱**，正如*第 8 章*《编码、转换和缩放特征》中讨论的那样。
- en: We then create a `ColumnTransformer` object, passing to it the three pipelines
    we just created, and indicating which features to use with which pipeline.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个`ColumnTransformer`对象，将刚刚创建的三个管道传递给它，并指明每个管道应该应用于哪些特征。
- en: 'We do not worry about missing values yet for the numeric variables. We will
    handle them later:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值变量，我们暂时不担心缺失值。我们稍后会处理它们：
- en: '[PRE129]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: We can now add the column transformer to a pipeline that also includes the linear
    model that we would like to run. We add KNN imputation to the pipeline to handle
    missing values for the numeric values. We have already handled missing values
    for the categorical variables.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以将列转换器添加到一个包含我们想要运行的线性模型的管道中。我们将 KNN 插补添加到管道中，以处理数值数据的缺失值。对于分类变量的缺失值，我们已经处理过了。
- en: We also need to scale the target, which cannot be done in our pipeline. We use
    scikit-learn’s `TransformedTargetRegressor` for that. We pass the pipeline we
    just created to the target regressor’s `regressor` parameter.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要对目标进行缩放，这不能在我们的管道中完成。我们使用 scikit-learn 的`TransformedTargetRegressor`来完成这项工作。我们将刚才创建的管道传递给目标回归器的`regressor`参数。
- en: '[PRE130]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: Let’s do *k*-fold cross-validation using this pipeline. We can pass our pipeline,
    via the target regressor `ttr`, to the `cross_validate` function.
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用这个管道进行*k*-折交叉验证。我们可以通过目标回归器`ttr`将我们的管道传递给`cross_validate`函数。
- en: '[PRE131]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: These scores are not very good, though that was not quite the point of this
    exercise. The key takeaway here is that we typically want to fold most of the
    pre-processing we will do into a pipeline. This is the best way to avoid data
    leakage. The column transformer is an extremely flexible tool, allowing us to
    apply different transformations to different features.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 这些得分不是很好，尽管这并不是这次练习的重点。这里的关键是，我们通常希望将大部分预处理工作纳入管道中进行处理。这是避免数据泄露的最佳方法。列转换器是一个极其灵活的工具，允许我们对不同的特征应用不同的转换。
- en: How it works...
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We created several different pipelines to pre-process our data before fitting
    our model, one for numeric data, one for categorical data, and one for binary
    data. The column transformer helps us by allowing us to apply different pipelines
    to different columns. We set up the column transformer in *step 5*.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了几个不同的管道来预处理数据，在拟合模型之前，一个用于数值数据，一个用于分类数据，一个用于二元数据。列转换器通过允许我们将不同的管道应用于不同的列来帮助我们。我们在*步骤
    5*中设置了列转换器。
- en: We created another pipeline in *step 6*. That pipeline actually begins with
    the column transformer. Then, the dataset that results from the column transformer’s
    pre-processing is passed to the KNN imputer to handle missing values from the
    numeric columns, and then to the linear regression model.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*步骤 6*中创建了另一个管道。这个管道实际上从列转换器开始。然后，列转换器预处理后的数据集将传递给 KNN 插补器，以处理数值列中的缺失值，接着传递给线性回归模型。
- en: It is good to note that we are able to add transformations to a scikit-learn
    pipeline, even ones we have designed ourselves, because they inherit the `BaseEstimator`
    and `TransformerMixin` classes, as we saw in *step 3*.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们可以向 scikit-learn 管道中添加转换操作，即使是我们自己设计的转换操作，因为它们继承了`BaseEstimator`和`TransformerMixin`类，正如我们在*步骤
    3*中看到的那样。
- en: There’s more...
  id: totrans-454
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There is one additional thing that is very cool, and useful, about pipelines
    that was not demonstrated in this example. If you have ever had to generate predictions
    based on variables that have been scaled or transformed in some way, you likely
    remember how much of a nuisance that can be. Well, pipelines handle that for us,
    generating predictions in the appropriate units.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 有一件关于管道的事非常酷而且实用，而在这个示例中并未展示。如果你曾经需要根据经过缩放或转化的变量生成预测，你可能会记得那是多么麻烦。好消息是，管道帮我们处理了这个问题，生成了适当单位的预测结果。
- en: See also
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: This really just scratches the surface of what can be done with pipelines. For
    a fuller discussion, see the book *Data Cleaning and Exploration with Machine
    Learning*, also written by me.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是管道技术的一小部分内容。如果你想深入了解，请参阅我写的书《*使用机器学习进行数据清理和探索*》。
- en: Summary
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: There was a fair bit packed into this chapter, covering several approaches to
    automating our data-cleaning work. We created functions for showing the structure
    of our data and generating descriptive statistics. We created functions for restructuring
    and aggregating our data. We also developed Python classes for handling data cleaning
    when we have a large number of variables, each requiring very different treatment.
    We also saw how Python classes can make it easier to work directly with a JSON
    file. We examined being more intentional with our data cleaning by checking our
    data against predefined targets. Finally, we explored how to automate our data
    cleaning with pipelines.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含了不少内容，介绍了几种自动化数据清理的方式。我们创建了用于显示数据结构和生成描述性统计的函数。我们还创建了用于重构和聚合数据的函数。此外，我们还开发了用于处理大量变量的数据清理的Python类，这些变量需要非常不同的处理方式。我们还展示了如何利用Python类简化直接操作JSON文件的过程。我们还探讨了通过将数据与预定义目标进行对比，使数据清理更有针对性。最后，我们探讨了如何通过管道自动化数据清理。
- en: Leave a review!
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留下评论！
- en: Enjoyed this book? Help readers like you by leaving an Amazon review. Scan the
    QR code below to get a free eBook of your choice.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢这本书吗？帮助像你一样的读者，通过在亚马逊上留下评论。扫描下方二维码，获得你选择的免费电子书。
- en: '![](img/Review_copy.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Review_copy.png)'
- en: '![](img/New_Packt_Logo1.png)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![](img/New_Packt_Logo1.png)'
- en: '[packt.com](https://www.packt.com)'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '[packt.com](https://www.packt.com)'
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅我们的在线数字图书馆，全面访问超过7,000本书籍和视频，以及行业领先的工具，帮助你规划个人发展并推动职业生涯发展。欲了解更多信息，请访问我们的网站。
- en: Why subscribe?
  id: totrans-466
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要订阅？
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用来自4,000多位行业专家的实用电子书和视频，减少学习时间，增加编程时间
- en: Improve your learning with Skill Plans built especially for you
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用专门为你打造的技能规划提升你的学习
- en: Get a free eBook or video every month
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每月获得一本免费电子书或视频
- en: Fully searchable for easy access to vital information
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全可搜索，便于获取关键信息
- en: Copy and paste, print, and bookmark content
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制、粘贴、打印和书签内容
- en: At [www.packt.com](https://www.packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在[www.packt.com](https://www.packt.com)，你还可以阅读一系列免费的技术文章，注册各种免费电子报，并获得Packt书籍和电子书的独家折扣和优惠。
- en: Other Books You May Enjoy
  id: totrans-473
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你可能喜欢的其他书籍
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这本书，你可能会对Packt出版的以下其他书籍感兴趣：
- en: '[![](img/978-1-80324-659-8.png)](https://www.packtpub.com/product/azure-data-factory-cookbook-second-edition/9781803246598)'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/978-1-80324-659-8.png)](https://www.packtpub.com/product/azure-data-factory-cookbook-second-edition/9781803246598)'
- en: '**Azure Data Factory Cookbook – Second Edition**'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '**Azure 数据工厂实用指南（第二版）**'
- en: Dmitry Foshin
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: Dmitry Foshin
- en: Dimtry Anoshin
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: Dimtry Anoshin
- en: Tonya Chernyshova
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: Tonya Chernyshova
- en: Xenia Ireton
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: Xenia Ireton
- en: 'ISBN: 978-1-80324-659-8'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 'ISBN: 978-1-80324-659-8'
- en: Create an orchestration and transformation job in ADF
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ADF中创建编排和转换作业
- en: Develop, execute, and monitor data flows using Azure Synapse
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Azure Synapse开发、执行和监控数据流
- en: Create big data pipelines using Databricks and Delta tables
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Databricks和Delta表创建大数据管道
- en: Work with big data in Azure Data Lake using Spark Pool
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark池在Azure Data Lake中处理大数据
- en: Migrate on-premises SSIS jobs to ADF
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将本地SSIS作业迁移到ADF
- en: Integrate ADF with commonly used Azure services such as Azure ML, Azure Logic
    Apps, and Azure Functions
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将ADF与常用的Azure服务（如Azure ML、Azure Logic Apps和Azure Functions）集成
- en: Run big data compute jobs within HDInsight and Azure Databricks
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在HDInsight和Azure Databricks中运行大数据计算任务
- en: Copy data from AWS S3 and Google Cloud Storage to Azure Storage using ADF’s
    built-in connectors
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ADF的内置连接器，将数据从AWS S3和Google Cloud Storage复制到Azure Storage
- en: '[![](img/978-1-80461-442-6.png)](https://www.packtpub.com/product/data-engineering-with-aws-second-edition/9781804614426)'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/978-1-80461-442-6.png)](https://www.packtpub.com/product/data-engineering-with-aws-second-edition/9781804614426)'
- en: '**Data Engineering with AWS – Second Edition**'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS数据工程 - 第二版**'
- en: Gareth Eagar
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: Gareth Eagar
- en: 'ISBN: 978-1-80461-442-6'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 'ISBN: 978-1-80461-442-6'
- en: Seamlessly ingest streaming data with Amazon Kinesis Data Firehose
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无缝地摄取流数据，使用Amazon Kinesis Data Firehose
- en: Optimize, denormalize, and join datasets with AWS Glue Studio
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS Glue Studio优化、反规范化并连接数据集
- en: Use Amazon S3 events to trigger a Lambda process to transform a file
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Amazon S3事件触发Lambda进程来转换文件
- en: Load data into a Redshift data warehouse and run queries with ease
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据加载到Redshift数据仓库中，并轻松运行查询
- en: Visualize and explore data using Amazon QuickSight
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Amazon QuickSight可视化和探索数据
- en: Extract sentiment data from a dataset using Amazon Comprehend
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Amazon Comprehend从数据集中提取情感数据
- en: Build transactional data lakes using Apache Iceberg with Amazon Athena
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Iceberg与Amazon Athena构建事务性数据湖
- en: Learn how a data mesh approach can be implemented on AWS
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何在AWS上实现数据网格方法
- en: Packt is searching for authors like you
  id: totrans-502
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Packt正在寻找像你这样的作者
- en: If you’re interested in becoming an author for Packt, please visit [authors.packtpub.com](https://authors.packtpub.com)
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣成为Packt的作者，请访问[authors.packtpub.com](https://authors.packtpub.com)并今天就申请。我们与成千上万的开发者和技术专业人士合作，帮助他们与全球技术社区分享他们的见解。你可以提交一般申请，申请我们正在招聘作者的特定热门话题，或提交你自己的想法。
- en: Share your thoughts
  id: totrans-504
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享你的想法
- en: Now you’ve finished *Python Data Cleaning Cookbook, Second Edition*, we’d love
    to hear your thoughts! If you purchased the book from Amazon, please [click here
    to go straight to the Amazon review page](https://packt.link/r/1803239875) for
    this book and share your feedback or leave a review on the site that you purchased
    it from.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经完成了*Python数据清理食谱，第二版*，我们很想听听你的想法！如果你是在亚马逊购买的这本书，请[点击这里直接前往亚马逊书评页面](https://packt.link/r/1803239875)，分享你的反馈或在购买网站上留下评论。
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 你的评论对我们和技术社区非常重要，将帮助我们确保提供卓越的内容质量。
