- en: '20'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '20'
- en: The Expected Value
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值
- en: In the last chapter, we learned about probability distributions, the objects
    that represent probabilistic models as sequences or functions. After all, there
    is the entire field of calculus to help us deal with functions, so they open up
    a wide array of mathematical tools.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们学习了概率分布，即表示概率模型的对象，作为序列或函数。毕竟，整个微积分领域可以帮助我们处理函数，因此它们为我们提供了广泛的数学工具。
- en: However, we might not need all the information available. Sometimes, simple
    descriptive statistics such as mean, variance, or median suffice. Even in machine
    learning, loss functions are given in terms of them. For instance, the famous
    mean-squared error
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可能并不需要所有可用的信息。有时，简单的描述性统计，如均值、方差或中位数就足够了。即便在机器学习中，损失函数也以这些量为基础。例如，著名的均方误差
- en: '![ n MSE (x,y ) =-1∑ (f(x )− y )2, x,y ∈ ℝn n i i i=1 ](img/file1893.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![ n MSE (x,y ) =-1∑ (f(x )− y )2, x,y ∈ ℝn n i i i=1 ](img/file1893.png)'
- en: is the variance of the prediction error. Deep down, these familiar quantities
    are rooted in probability theory, and we’ll devote this chapter to learning about
    them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 是预测误差的方差。深层次上，这些熟悉的量都根植于概率论中，我们将在本章中专门学习它们。
- en: 20.1 Discrete random variables
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.1 离散随机变量
- en: Let’s play a simple game. I toss a coin, and if it comes up heads, you win $1
    . If it is tails, you lose $2 .
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们玩一个简单的游戏。我投掷一枚硬币，如果是正面，你赢得$1。如果是反面，你输掉$2。
- en: Up until now, we were dealing with questions like the probability of winning.
    Say, for the coin toss, whether you win or lose, we have
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理像是获胜的概率等问题。比如，投掷硬币时，无论你赢还是输，我们有
- en: '![P(heads) = P(tails) = 1\. 2 ](img/file1894.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![P(正面) = P(反面) = 1\. 2 ](img/file1894.png)'
- en: Despite the equal chances of winning and losing, should you play this game?
    Let’s find out.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管赢和输的机会相等，你是否应该玩这个游戏呢？让我们一探究竟。
- en: After n rounds, your earnings can be calculated by the number of heads times
    $1 minus the number of tails times $2 . If we divide total earnings by n, we obtain
    your average winnings per round. That is,
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在n轮之后，你的收益可以通过正面次数乘以$1，减去反面次数乘以$2来计算。如果我们将总收益除以n，就得到每轮的平均奖金。也就是说，
- en: '![ total-winnings- your average winnings = n 1⋅#heads − 2⋅#tails = --------------------
    n = 1⋅ #heads − 2⋅ #tails, n n ](img/file1895.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![ 总奖金- 你的平均奖金 = n 1⋅#正面 − 2⋅#反面 = -------------------- n = 1⋅ #正面 − 2⋅ #反面,
    n n ](img/file1895.png)'
- en: 'where #heads and #tails denote the number of heads and tails respectively.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，#正面和#反面分别表示正面和反面的次数。
- en: Recall the frequentist interpretation of probability from Section [18.2.7](ch030.xhtml#how-to-interpret-probability)?
    According to our intuition, we should have
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得来自第[18.2.7](ch030.xhtml#how-to-interpret-probability)节的频率学派对概率的解释吗？根据我们的直觉，我们应该有
- en: '![ lim #heads- = P(heads) = 1, n→ ∞ n 2 #tails- 1- lni→m∞ n = P(tails) = 2\.
    ](img/file1896.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![ lim #正面- = P(正面) = 1, n→ ∞ n 2 #反面- 1- lni→m∞ n = P(反面) = 2\. ](img/file1896.png)'
- en: This means that if you play long enough, your average winnings per round is
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果你玩得足够长，你每轮的平均奖金是
- en: '![your average winnings = 1⋅P (heads)− 2 ⋅P(tails) = − 1\. 2 ](img/file1897.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![你的平均奖金 = 1⋅P(正面)− 2⋅P(反面) = − 1\. 2 ](img/file1897.png)'
- en: So, as you are losing half a dollar per round on average, you definitely shouldn’t
    play this game.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，既然你每轮平均亏损半美元，那么你肯定不应该玩这个游戏。
- en: Let’s formalize this argument with a random variable. Say, if X describes your
    winnings per round, we have
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过随机变量来正式化这个论点。假设，如果X表示你每轮的奖金，我们有
- en: '![ 1 P(X = 1) = P (X = − 2) =-, 2 ](img/file1898.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 P(X = 1) = P (X = − 2) =-, 2 ](img/file1898.png)'
- en: so the average winnings can be written as
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以平均奖金可以表示为
- en: '![average value of X = 1 ⋅P(X = 1)− 2 ⋅P(X = − 2) 1 = − -. 2 ](img/file1899.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![X的平均值 = 1 ⋅P(X = 1) − 2 ⋅P(X = − 2) 1 = − -. 2 ](img/file1899.png)'
- en: With a bit of a pattern matching, we find that for a general discrete random
    variable X, the formula looks like
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一点模式匹配，我们发现对于一般的离散随机变量X，公式看起来像
- en: '![ ∑ average value of X = (value)⋅P (X = value). value ](img/file1900.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑ X的平均值 = (值)⋅P(X = 值)。值 ](img/file1900.png)'
- en: And from this, the definition of expected value is born.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从中，期望值的定义应运而生。
- en: Definition 92\. (The expected value of discrete random variables)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 定义92.（离散随机变量的期望值）
- en: 'Let (Ω,Σ,P) be a probability space, and X : Ω →{x[1],x[2],…} be a discrete
    random variable. The expected value of X is defined by'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '设(Ω, Σ, P)为一个概率空间，X : Ω →{x[1], x[2], …}为离散随机变量。X的期望值定义为'
- en: '![𝔼 [X ] := ∑ x P (X = x ). k k k ](img/file1901.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![𝔼 [X ] := ∑ x P (X = x ). k k k ](img/file1901.png)'
- en: (Note that if X assumes finitely many values, the sum only contains a finite
    number of terms.)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: （注意，如果X取有限多个值，则和中只包含有限个项。）
- en: In English, the expected value describes the average value of a random variable
    in the long run. The expected value is also called the mean and is often denoted
    by μ. Instead of using random variables, we’ll often use the expected value symbol
    by plugging in distributions, like 𝔼[Bernoulli(p)]. Although this is mathematically
    not precise, 1) it is simpler in certain cases, 2) and the expected value only
    depends on the distribution anyway.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在英语中，期望值描述的是随机变量在长期运行中的平均值。期望值也叫做均值，通常用μ表示。我们常常不使用随机变量，而是通过代入分布来使用期望值符号，比如𝔼[Bernoulli(p)]。虽然从数学上看这不够精确，但1）在某些情况下，它更简单，2）而且期望值反正只依赖于分布。
- en: It’s time for examples.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候看一些例子了。
- en: Example 1\. Expected value of the Bernoulli distribution. (See the definition
    of the Bernoulli distribution in Section [19.2.1](ch031.xhtml#the-bernoulli-distribution).)
    Let X ∼ Bernoulli(p). Its expected value is quite simple to compute, as
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 1. 伯努利分布的期望值。（请参见[19.2.1](ch031.xhtml#the-bernoulli-distribution)节中的伯努利分布定义。）设X
    ∼ Bernoulli(p)。它的期望值很容易计算，计算如下：
- en: '![𝔼[X] = 0⋅P (X = 0 )+ 1⋅P (X = 1) = = 0⋅(1 − p)+ 1 ⋅p = p. ](img/file1902.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![𝔼[X] = 0⋅P (X = 0 )+ 1⋅P (X = 1) = = 0⋅(1 − p)+ 1 ⋅p = p. ](img/file1902.png)'
- en: 'We’ve seen this before: the introductory example with the simple game is the
    transformed Bernoulli distribution 3 ⋅ Bernoulli(1∕2) − 2.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前见过这个：简单游戏的入门例子实际上是变换后的伯努利分布 3 ⋅ 伯努利(1∕2) − 2。
- en: Example 2\. Expected value of the binomial distribution. (See the definition
    of the binomial distribution in Section [19.2.2](ch031.xhtml#the-binomial-distribution).)
    Let X ∼ Binomial(n,p). Then
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2. 二项分布的期望值。（请参见[19.2.2](ch031.xhtml#the-binomial-distribution)节中的二项分布定义。）设X
    ∼ Binomial(n,p)。那么
- en: '![ ∑n 𝔼[X] = kP (X = k ) k=0 ∑n (n ) = k pk(1 − p)n−k k=0 k ∑n = k----n!---
    pk(1 − p)n−k. k=0 k!(n − k)! ](img/file1903.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n 𝔼[X] = kP (X = k ) k=0 ∑n (n ) = k pk(1 − p)n−k k=0 k ∑n = k----n!---
    pk(1 − p)n−k. k=0 k!(n − k)! ](img/file1903.png)'
- en: 'The plan is the following: absorb that k with the fraction ![--n!--- k!(n−k)!](img/file1904.png),
    and adjust the sum such that its terms form the probability mass function for
    Binomial(n − 1,p). As n −k = (n − 1) − (k − 1), we have'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 计划如下：吸收那个k与分数![--n!--- k!(n−k)!](img/file1904.png)，并调整和，使其项形成二项分布(Binomial(n
    − 1,p))的概率质量函数。由于n −k = (n − 1) − (k − 1)，我们得到
- en: '![ ∑n 𝔼[X ] = k ---n!----pk(1− p)n−k k=0 k!(n − k)! ∑n = np ---------(n-−-1)!--------pk−1(1
    − p)(n−1)−(k−1) k=1(k − 1)!((n − 1)− (k − 1))! n−1 = np ∑ ---(n-−-1)!--pk(1− p)(n−1−k)
    k!(n − 1− k)! k=0 n∑−1 = np P (Binomial(n− 1,p) = k) k=0 = np. ](img/file1905.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n 𝔼[X ] = k ---n!----pk(1− p)n−k k=0 k!(n − k)! ∑n = np ---------(n-−-1)!--------pk−1(1
    − p)(n−1)−(k−1) k=1(k − 1)!((n − 1)− (k − 1))! n−1 = np ∑ ---(n-−-1)!--pk(1− p)(n−1−k)
    k!(n − 1− k)! k=0 n∑−1 = np P (Binomial(n− 1,p) = k) k=0 = np. ](img/file1905.png)'
- en: This computation might not look like the simplest, but once you get familiar
    with the trick, it’ll be like second nature for you.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算看起来可能不算最简单，但一旦你熟悉了这个技巧，它将变得像第二天性一样。
- en: Example 3\. Expected value of the geometric distribution. (See the definition
    of the geometric distribution in Section [19.2.3](ch031.xhtml#the-geometric-distribution).)
    Let X ∼ Geo(p). We need to calculate
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 3. 几何分布的期望值。（请参见[19.2.3](ch031.xhtml#the-geometric-distribution)节中的几何分布定义。）设X
    ∼ Geo(p)。我们需要计算
- en: '![ ∑∞ 𝔼[X ] = k (1 − p)k−1p. k=1 ](img/file1906.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑∞ 𝔼[X ] = k (1 − p)k−1p. k=1 ](img/file1906.png)'
- en: Do you remember the geometric series ([19.2](#))? This is almost it, except
    for the k term, which throws a monkey wrench into our gears. To fix that, we’ll
    use another magic trick. Recall that
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你还记得几何级数吗？([19.2](#)) 这几乎就是它，除了那个k项，它让我们的计算变得复杂。为了解决这个问题，我们将使用另一个魔法技巧。回忆一下，
- en: '![ 1 ∑∞ ----- = xk. 1 − x k=0 ](img/file1907.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 ∑∞ ----- = xk. 1 − x k=0 ](img/file1907.png)'
- en: Now, we are going to differentiate the geometric series, thus obtaining
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将对几何级数进行微分，从而得到
- en: '![ d 1 d ∑∞ k dx-1-−-x = dx- x k=0 ∑∞ d k = dx-x k=0 ∑∞ k− 1 = kx , k=1 ](img/file1908.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![ d 1 d ∑∞ k dx-1-−-x = dx- x k=0 ∑∞ d k = dx-x k=0 ∑∞ k− 1 = kx , k=1 ](img/file1908.png)'
- en: where we used the linearity of the derivative and the pleasant analytic properties
    of the geometric series. Mathematicians would scream upon the sight of switching
    the derivative and the infinite sum, but don’t worry, everything here is correct
    as is. (Mathematicians are really afraid of interchanging limits. Mind you, for
    a good reason!)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了导数的线性性质和几何级数的良好解析性质。数学家们看到交换导数和无穷和时会尖叫，但别担心，这里的一切都是正确的。（数学家们真的害怕交换极限。你得知道，他们有理由这么做！）
- en: On the other hand,
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，
- en: '![d---1--- ---1---- dx 1− x = (1 − x)2, ](img/file1909.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![d---1--- ---1---- dx 1− x = (1 − x)2, ](img/file1909.png)'
- en: thus
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: '![ ∞ ∑ k−1 ---1---- kx = (1− x )2\. k=1 ](img/file1910.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![ ∞ ∑ k−1 ---1---- kx = (1− x )2\. k=1 ](img/file1910.png)'
- en: Combining all of these, we finally have
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 综合以上所有内容，我们最终得出
- en: '![ ∑∞ 𝔼[X ] = k (1 − p)k−1p k=1 ∞∑ = p k(1− p)k−1 k=1 1 1 = p-2 = -. p p ](img/file1911.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑∞ 𝔼[X ] = k (1 − p)k−1p k=1 ∞∑ = p k(1− p)k−1 k=1 1 1 = p-2 = -. p p ](img/file1911.png)'
- en: Example 4\. Expected value of the constant random variable. Let c ∈ℝ be an arbitrary
    constant, and let X be the random variable that assumes the value c everywhere.
    As X is a discrete random variable, its expected value is simply
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 示例4\. 常数随机变量的期望值。令c ∈ℝ为任意常数，X为一个在所有情况下都取值为c的随机变量。由于X是离散随机变量，它的期望值就是
- en: '![𝔼[X ] = c⋅P (X = c) = c. ](img/file1912.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![𝔼[X ] = c⋅P (X = c) = c. ](img/file1912.png)'
- en: I know, this example looks silly, but it can be quite useful. When it is clear,
    we abuse the notation by denoting the constant c as the random variable itself.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道，这个例子看起来很傻，但它实际上可以非常有用。当情况清晰时，我们通过将常数c表示为随机变量本身来滥用符号。
- en: 20.1.1 The expected value in poker
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.1.1 扑克中的期望值
- en: One more example before we move on. I was a mediocre no-limit Texas hold’em
    player a while ago, and the first time I heard about the expected value was years
    before I studied probability theory.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前再举一个例子。曾经，我是一个中等水平的无限注德州扑克玩家，而我第一次听说期望值是在学习概率论之前的几年。
- en: According to the rules of Texas hold’em, each player holds two cards on their
    own, while five more shared cards are dealt. The shared cards are available for
    everyone, and the player with the strongest hand wins.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据德州扑克的规则，每个玩家自己持有两张牌，同时发五张公共牌。这些公共牌对每个玩家都可用，持有最强牌的玩家获胜。
- en: Figure [20.1](#) shows how the table looks before the last card (the river)
    is revealed.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图[20.1](#)展示了最后一张牌（河牌）揭示之前桌面的样子。
- en: '![PIC](img/file1913.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1913.png)'
- en: 'Figure 20.1: The poker table before the river card'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.1：河牌之前的扑克桌
- en: There is money in the pot to be won, but to see the river, you have to call
    the opponent’s bet. The question is, should you? Expected value to the rescue.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 底池里有钱可以赢，但要看到河牌，你必须跟注对手的下注。问题是，你应该这么做吗？期望值来帮忙。
- en: Let’s build a probabilistic model. We would win the pot with certain river cards
    but lose with all the others. If X represents our winnings, then
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个概率模型。某些河牌（river card）能让我们赢得底池，而其他所有河牌则让我们失去。若X代表我们的获胜金额，则
- en: '![ #winning cards P(X = pot) = ---------------, #remaining cards P (X = − bet)
    = --#losing-cards-. #remaining cards ](img/file1914.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![ #winning cards P(X = pot) = ---------------, #remaining cards P (X = − bet)
    = --#losing-cards-. #remaining cards ](img/file1914.png)'
- en: Thus, the expected value is
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，期望值是
- en: '![𝔼[X] = pot⋅P (X = pot )− bet⋅P (X = − bet) #winning cards #losing cards =
    pot⋅ ----------------− bet⋅----------------. #remaining cards #remaining cards
    ](img/file1915.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![𝔼[X] = pot⋅P (X = pot )− bet⋅P (X = − bet) #winning cards #losing cards =
    pot⋅ ----------------− bet⋅----------------. #remaining cards #remaining cards
    ](img/file1915.png)'
- en: When is the expected value positive? With some algebra, we obtain that 𝔼[X]/span>0
    if and only if
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值什么时候为正？通过一些代数运算，我们得到𝔼[X]/span>0 当且仅当
- en: '![#winning-cards bet- #losing cards > pot, ](img/file1916.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![#winning-cards bet- #losing cards > pot, ](img/file1916.png)'
- en: which is called positive pot odds. If this is satisfied, making the bet is the
    right call. You might lose a hand with positive pot odds, but in the long term,
    your winnings will be positive.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为正底池赔率。如果满足这一条件，下注是正确的选择。尽管在单手牌中你可能会输，但从长远来看，你的收益将是正的。
- en: Of course, pot odds are extremely hard to determine in practice. For instance,
    you don’t know what others hold, and counting the cards that would win the pot
    for you is not possible unless you have a good read on the opponents. Poker is
    much more than just math. Good players choose their bet specifically to throw
    off their opponents’ pot odds.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，底池赔率在实践中非常难以确定。例如，你不知道别人持有什么牌，除非你对对手有很好的阅读能力，否则无法计算出哪些牌会为你赢得底池。扑克远不止是数学。优秀的玩家会特别选择下注方式，来让对手的底池赔率产生误导。
- en: Now that we understand the idea behind the expected value, let’s move on to
    the general case!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了期望值的基本概念，让我们继续探讨一般情况！
- en: 20.2 Continuous random variables
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.2 连续随机变量
- en: So far, we have only defined the expected value for discrete random variables.
    As 𝔼[X] describes the average value of X in the long run, it should exist for
    continuous random variables as well.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只为离散随机变量定义了期望值。由于𝔼[X]描述的是X在长时间内的平均值，因此它对于连续随机变量也应该是存在的。
- en: 'The interpretation of the expected value was simple: outcome times probability,
    summed over all potential values. However, there is a snag with continuous random
    variables: we don’t have such a mass distribution, as the probabilities of individual
    outcomes are zero: P(X = x) = 0\. Moreover, we can’t sum uncountably many values.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值的解释很简单：结果乘以概率，对所有可能的值求和。然而，连续随机变量有一个问题：我们没有这样的质量分布，因为单个结果的概率为零：P(X = x) =
    0。此外，我们无法对无穷多个值求和。
- en: What can we do?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做什么呢？
- en: Wishful thinking. This is one of the most powerful techniques in mathematics,
    and I am not joking.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一厢情愿的想法。这是数学中最强大的技巧之一，我不是在开玩笑。
- en: Here’s the plan. We’ll pretend that the expected value of a continuous random
    variable is well-defined, and let our imagination run free. Say goodbye to mathematical
    precision, and allow our intuition to unfold. Instead of the probability of a
    given outcome, we can talk about X landing in a small interval. First, we divide
    up the set of real numbers into really small parts. To be more precise, let x[0]/span>x[1]/span>…/span>x[n]
    be a granular partition of the real line. If the partition is refined enough,
    we should have
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 计划是这样的。我们假设连续随机变量的期望值是定义良好的，让我们的想象力自由发挥。告别数学精确性，让我们的直觉展开。与其谈论某一结果的概率，我们可以讨论
    X 落在一个小区间内。首先，我们将实数集划分为非常小的部分。更精确地说，设 x[0]/span>x[1]/span>…/span>x[n] 是实数线的精细划分。如果划分足够精细，我们应该得到：
- en: 𝔼[X] ≈ ∑[k=1]^n x[k] P(x[k−1] ≤ X ≤ x[k]) (20.1)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 𝔼[X] ≈ ∑[k=1]^n x[k] P(x[k−1] ≤ X ≤ x[k]) (20.1)
- en: 'The probabilities in ([20.1](ch032.xhtml#continuous-random-variables)) can
    be expressed in terms of the CDF:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ([20.1](ch032.xhtml#continuous-random-variables)) 中的概率可以用 CDF 表示：
- en: '![ n n ∑ x P(x <X ≤ X ) = ∑ x (F (x ) − F (x )). k k−1 k k X k X k−1 k=1 k=1
    ](img/file1917.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![ n n ∑ x P(x <X ≤ X ) = ∑ x (F (x ) − F (x )). k k−1 k k X k X k−1 k=1 k=1
    ](img/file1917.png)'
- en: 'These increments remind us of the difference quotients. We don’t quite have
    these inside the sum, but with a “fancy multiplication with one,” we can achieve
    this:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些增量让我们想起了差分商。虽然在求和中我们没有完全得到这些，但通过“巧妙的乘以 1”，我们可以实现这一点：
- en: '![∑n ∑n xk(FX (xk)− FX (xk−1)) = xk(xk − xk−1)FX-(xk)−-FX-(xk−1). k=1 k=1 xk
    − xk− 1 ](img/file1918.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![∑n ∑n xk(FX (xk)− FX (xk−1)) = xk(xk − xk−1)FX-(xk)−-FX-(xk−1). k=1 k=1 xk
    − xk− 1 ](img/file1918.png)'
- en: If the x[i]-s are close to each other (and we can select them to be arbitrarily
    close), the difference quotients are close to the derivative of F[X], which is
    the density function f[X]. Thus,
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 x[i] 彼此接近（而且我们可以选择它们任意接近），那么差分商接近于 F[X] 的导数，即密度函数 f[X]。因此，
- en: '![FX (xk)− FX (xk−1) ∑n FX (xk)− FX (xk−1) ∑n -----x-−-x-------- ≈ fX (xk )
    xk(xk − xk−1)-----x-−-x-------- ≈ xk(xk − xk −1)fX(xk). k k− 1 k=1 k k− 1 k=1
    ](img/file1919.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![FX (xk)− FX (xk−1) ∑n FX (xk)− FX (xk−1) ∑n -----x-−-x-------- ≈ fX (xk )
    xk(xk − xk−1)-----x-−-x-------- ≈ xk(xk − xk −1)fX(xk). k k− 1 k=1 k k− 1 k=1
    ](img/file1919.png)'
- en: 'This is a Riemann-sum, defined by ([14.7](#))! Hence, the last sum is close
    to a Riemann-integral:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个黎曼和，定义见于 ([14.7](#))！因此，最后的和接近于黎曼积分：
- en: '![ n ∫ ∑ ∞ xk(xk − xk−1)fX(xk) ≈ −∞ xfX (x)dx. k=1 ](img/file1920.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![ n ∫ ∑ ∞ xk(xk − xk−1)fX(xk) ≈ −∞ xfX (x)dx. k=1 ](img/file1920.png)'
- en: Although we were not exactly precise in our argument, all of the above can be
    made mathematically correct. (But we are not going to do it here, as it is not
    relevant to us.) Thus, we finally obtain the formula of the expected value for
    continuous random variables.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在推理中并没有完全精确，但以上所有内容都可以在数学上得到正确处理。（不过我们这里不做，因为它与我们无关。）因此，我们最终得到了连续随机变量期望值的公式。
- en: Definition 93\. (The expected value of continuous random variables)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 93. （连续随机变量的期望值）
- en: 'Let (Ω,Σ,P) be a probability space, and X : Ω →ℝ be a continuous random variable.
    The expected value of X is defined by'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '设 (Ω,Σ,P) 是一个概率空间，X : Ω → ℝ 是一个连续随机变量。X 的期望值由以下公式定义：'
- en: '![ ∫ ∞ 𝔼 [X ] := xfX(x)dx. −∞ ](img/file1921.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ ∞ 𝔼 [X ] := xfX(x)dx. −∞ ](img/file1921.png)'
- en: As usual, let’s see some examples first.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，让我们先看一些例子。
- en: Example 1\. Expected value of the uniform distribution. (See the definition
    of the uniform distribution in Section [19.3.4](ch031.xhtml#the-uniform-distribution2).)
    Let X ∼ Uniform(a,b). Then
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 1. 均匀分布的期望值。（参见 [19.3.4](ch031.xhtml#the-uniform-distribution2) 节中对均匀分布的定义。）设
    X ∼ Uniform(a,b)。则
- en: '![ ∫ ∞ 1 𝔼[X ] = x-----dx − ∞ b∫− a --1-- b = b − a xdx [ a ]x=b = ---1---x2
    2(b − a) x=a a + b = --2--, ](img/file1922.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ ∞ 1 𝔼[X ] = x-----dx − ∞ b∫− a --1-- b = b − a xdx [ a ]x=b = ---1---x2
    2(b − a) x=a a + b = --2--, ](img/file1922.png)'
- en: which is the midpoint of the interval [a,b], where the Uniform(a,b) lives.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示区间 [a,b] 的中点，其中 Uniform(a,b) 分布存在。
- en: Example 2\. Expected value of the exponential distribution. (See the definition
    of the exponential distribution in Section [19.3.5](ch031.xhtml#the-exponential-distribution).)
    Let X ∼ exp(λ). Then, we need to calculate the integral
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2\. 指数分布的期望值。（参见第[19.3.5](ch031.xhtml#the-exponential-distribution)节中关于指数分布的定义。）设
    X ∼ exp(λ)。然后，我们需要计算积分
- en: '![ ∫ ∞ − λx 𝔼[X ] = xλe dx. 0 ](img/file1923.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ ∞ − λx 𝔼[X ] = xλe dx. 0 ](img/file1923.png)'
- en: 'We can do this via integration by parts (Theorem [95](ch022.xhtml#x1-238002r95)):
    by letting f(x) = x and g^′(x) = λe^(−λx), we have'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过分部积分法来解决这个问题（定理[95](ch022.xhtml#x1-238002r95)）：设 f(x) = x 且 g^′(x) = λe^(−λx)，我们得到
- en: '![ ∫ ∞ 𝔼 [X ] = xλe−λxdx 0 [ ]x=∞ ∫ ∞ = − xe−λx x=0 + e− λxdx ◟-----◝=◜0-----◞
    0 [ ]x=∞ = − 1-e−λx λ x=0 1- = λ. ](img/file1924.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ ∞ 𝔼 [X ] = xλe−λxdx 0 [ ]x=∞ ∫ ∞ = − xe−λx x=0 + e− λxdx ◟-----◝=◜0-----◞
    0 [ ]x=∞ = − 1-e−λx λ x=0 1- = λ. ](img/file1924.png)'
- en: 20.3 Properties of the expected value
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.3 期望值的性质
- en: As usual, the expected value has several useful properties. Most importantly,
    the expected value is linear with respect to the random variable.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，期望值具有几个有用的性质。最重要的是，期望值对随机变量是线性的。
- en: Theorem 129\. (Linearity of the expected value)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 129\. （期望值的线性）
- en: 'Let (Ω,Σ,P) be a probability space, and let X,Y : Ω → ℝ be two random variables.
    Moreover, let a,b ∈ℝ be two scalars. Then'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '设 (Ω,Σ,P) 为一个概率空间，且让 X, Y : Ω → ℝ 为两个随机变量。进一步设 a, b ∈ ℝ 为两个标量。那么'
- en: '![𝔼[aX + bY ] = a𝔼 [X ]+ b𝔼 [Y ] ](img/file1925.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![𝔼[aX + bY ] = a𝔼 [X ]+ b𝔼 [Y ] ](img/file1925.png)'
- en: holds.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 公式成立。
- en: We are not going to prove this theorem here, but know that linearity is an essential
    tool. Do you recall the game that we used to introduce the expected value for
    discrete random variables? I toss a coin, and if it comes up heads, you win $1\.
    Tails, you lose $2\. If you think about it for a minute, this is the
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会证明这个定理，但要知道线性是一个重要的工具。你还记得我们用来引入离散随机变量期望值的游戏吗？我掷硬币，如果正面朝上，你赢得 $1；反面朝上，你输掉
    $2。如果你思考一下，这就是
- en: '![X = 3⋅Bernoulli(1∕2)− 2 ](img/file1926.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![X = 3⋅Bernoulli(1∕2)− 2 ](img/file1926.png)'
- en: distribution, and as such,
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 分布，因此，
- en: '![𝔼[X] = 𝔼[3⋅Bernoulli(1 ∕2)− 2] = 3⋅𝔼 [Bernoulli(1 ∕2)]− 2 = 3⋅ 1− 2 2 1-
    = − 2\. ](img/file1927.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![𝔼[X] = 𝔼[3⋅Bernoulli(1 ∕2)− 2] = 3⋅𝔼 [Bernoulli(1 ∕2)]− 2 = 3⋅ 1− 2 2 1-
    = − 2\. ](img/file1927.png)'
- en: Of course, linearity goes way beyond this simple example. As you’ve gotten used
    to this already, linearity is a crucial property in mathematics. We love linearity.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，线性远不止这个简单的例子。正如你已经习惯了的那样，线性是数学中的一个关键性质。我们喜欢线性。
- en: Remark 20\.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 备注 20\.
- en: Notice that Theorem [129](ch032.xhtml#x1-331002r129) did not say that X and
    Y have to be both discrete or both continuous. Even though we have only defined
    the expected value in such cases, there is a general definition that works for
    all random variables.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，定理[129](ch032.xhtml#x1-331002r129)并未说明 X 和 Y 必须都是离散的或都是连续的。尽管我们只在这种情况下定义了期望值，但存在一个适用于所有随机变量的通用定义。
- en: The snag is, it requires a familiarity with measure theory, falling way outside
    of our scope. Suffice to say, the theorem works as is.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，它需要熟悉测度理论，而这超出了我们的范围。只需知道，这个定理按原样成立。
- en: If the expected value of a sum is the sum of the expected values, does the same
    apply to the product? Not in general, but fortunately, this works for independent
    random variables. (See Definition [84](ch031.xhtml#x1-305002r84) for the definition
    of independent random variables.)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果和的期望值是各自期望值的和，那么乘积是否也适用？一般来说不适用，但幸运的是，这在独立随机变量的情况下成立。（参见定理[84](ch031.xhtml#x1-305002r84)，其中给出了独立随机变量的定义。）
- en: Theorem 130\. (Expected value of the product of independent random variables)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 130\. （独立随机变量乘积的期望值）
- en: 'Let (Ω,Σ,P) be a probability space, and let X,Y : Ω →ℝ be two independent random
    variables.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '设 (Ω,Σ,P) 为一个概率空间，且让 X, Y : Ω → ℝ 为两个独立的随机变量。'
- en: Then
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '![𝔼 [XY ] = 𝔼 [X ]𝔼[Y] ](img/file1928.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![𝔼 [XY ] = 𝔼 [X ]𝔼[Y] ](img/file1928.png)'
- en: holds.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 公式成立。
- en: This property is extremely useful, as we’ll see in the next section, where we’ll
    talk about variance and covariance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个性质非常有用，正如我们将在下一节中看到的，我们将讨论方差和协方差。
- en: 'One more property that’ll help us to calculate the expected value of functions
    of the random variable, such as X² or sinX:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个帮助我们计算随机变量函数期望值的性质，比如 X² 或 sinX：
- en: Theorem 131\. (Law of the unconscious statistician)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 131\. （无意识统计学家的法则）
- en: 'Let (Ω,Σ,P) be a probability space, let X : Ω → ℝ be a random variable, and
    let g : ℝ →ℝ be an arbitrary function.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '设 (Ω,Σ,P) 为一个概率空间，设 X : Ω → ℝ 为一个随机变量，且设 g : ℝ → ℝ 为一个任意函数。'
- en: (a) If X is discrete with possible values x[1],x[2],…, then
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 如果 X 是离散的，且可能的取值为 x[1], x[2], …，那么
- en: '![ ∑ 𝔼[g(X)] = g(xn)P (X = xn). n ](img/file1929.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑ 𝔼[g(X)] = g(xn)P (X = xn). n ](img/file1929.png)'
- en: (b) If X is continuous with the probability density function f[X](x), then
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 如果X是具有概率密度函数f[X](x)的连续型随机变量，则
- en: '![ ∫ ∞ 𝔼[g(X )] = g(x)f (x)dx. −∞ X ](img/file1930.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ ∞ 𝔼[g(X )] = g(x)f (x)dx. −∞ X ](img/file1930.png)'
- en: Thus, calculating 𝔼[X²] for a continuous random variable can be done by simply
    taking
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，计算连续随机变量的𝔼[X²]可以通过简单地取
- en: '![ ∫ ∞ 𝔼[X2] = x2fX (x)dx, −∞ ](img/file1931.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ ∞ 𝔼[X²] = x²fX (x)dx, −∞ ](img/file1931.png)'
- en: which will be used all the time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在整个过程中被频繁使用。
- en: 20.4 Variance
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.4 方差
- en: Plainly speaking, the expected value measures the average value of the random
    variable. However, even though both Uniform(−1,1) and Uniform(−100,100) have zero
    expected value, the latter is much more spread out than the former. Thus, 𝔼[X]
    is not a good descriptor of the random variable X.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，期望值衡量的是随机变量的平均值。然而，即使Uniform(−1,1)和Uniform(−100,100)的期望值都是零，后者的分布却比前者要广泛得多。因此，𝔼[X]并不是描述随机变量X的好方法。
- en: To add one more layer, we measure the average deviation from the expected value.
    This is done via the variance and the standard deviation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了再加一层，我们衡量的是期望值的平均偏差。这是通过方差和标准差来实现的。
- en: Definition 94\. (Variance and standard deviation)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 94\. (方差与标准差)
- en: 'Let (Ω,Σ,P) be a probability space, let X : Ω →ℝ be a random variable, and
    let μ = 𝔼[X] be its expected value. The variance of X is defined by'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '设(Ω, Σ, P)为一个概率空间，设X : Ω → ℝ为一个随机变量，且μ = 𝔼[X]为其期望值。X的方差定义为'
- en: '![ [ 2] Var [X ] := 𝔼 (X − μ ) , ](img/file1932.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![ [ 2] 方差 [X ] := 𝔼 (X − μ ) , ](img/file1932.png)'
- en: while its standard deviation is defined by
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其标准差定义为
- en: '![Std[X] := ∘Var--[X-]. ](img/file1933.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![Std[X] := ∘方差--[X-]. ](img/file1933.png)'
- en: Take note that in the literature, the expected value is often denoted by μ,
    while the standard deviation is denoted by σ. Together, they form two of the most
    important descriptors of a random variable.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在文献中，期望值通常用μ表示，而标准差用σ表示。二者一起构成了描述随机变量的两个最重要的指标。
- en: Figure [20.2](#) shows a visual interpretation of the mean and standard deviation
    in the case of a normal distribution. The mean shows the average value, while
    the standard deviation can be interpreted as the average deviation from the mean.
    (We’ll talk about the normal distribution in detail later, so don’t worry if it
    is not yet familiar to you.)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图[20.2](#)展示了在正态分布情况下，均值和标准差的可视化解释。均值表示平均值，而标准差可以解释为平均偏离均值的程度。（我们稍后会详细讨论正态分布，所以如果现在还不熟悉，也不用担心。）
- en: '![PIC](img/file1934.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1934.png)'
- en: 'Figure 20.2: Mean (μ) and standard deviation ![(σ) ](img/file1935.png) of the
    standard normal distribution'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.2：标准正态分布的均值(μ)和标准差 !(σ) ](img/file1935.png)
- en: The usual method of calculating variance is not taking the expected value of
    (X −μ)², but taking the expected value of X² and subtracting μ² from it. This
    is shown by the following proposition.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 计算方差的常用方法不是取(X − μ)²的期望值，而是取X²的期望值并从中减去μ²。以下命题展示了这一点。
- en: Proposition 5\.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 命题 5\.
- en: 'Let (Ω,Σ,P) be a probability space, and let X : Ω →ℝ be a random variable.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '设(Ω, Σ, P)为一个概率空间，且X : Ω → ℝ为一个随机变量。'
- en: Then
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '![Var[X] = 𝔼[X2 ]− 𝔼[X ]2\. ](img/file1936.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![方差[X] = 𝔼[X² ]− 𝔼[X ]²\. ](img/file1936.png)'
- en: Proof. Let μ = 𝔼[X]. Because of the linearity of the expected value, we have
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。设μ = 𝔼[X]。由于期望值的线性性质，我们有
- en: '![Var[X ] = 𝔼[(X − μ)2] = 𝔼[X2 − 2μX + μ2] = 𝔼[X ]2 − 2μ𝔼 [X ]+ μ2 2 2 2 =
    𝔼[X ] − 2μ + μ = 𝔼[X2 ]− μ2 = 𝔼[X2 ]− 𝔼[X ]2, ](img/file1937.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![方差[X ] = 𝔼[(X − μ)²] = 𝔼[X² − 2μX + μ²] = 𝔼[X ]² − 2μ𝔼 [X ]+ μ² 2 2 2 = 𝔼[X
    ] − 2μ + μ = 𝔼[X² ]− μ² = 𝔼[X² ]− 𝔼[X ]², ](img/file1937.png)'
- en: which is what we had to show.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要证明的。
- en: Is the variance linear as well? No, but there are some important identities
    regarding scalar multiplication and addition.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 方差也具有线性性质吗？不，但关于标量乘法和加法，有一些重要的恒等式。
- en: Theorem 132\. (Variance and the linear operations)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 132\. (方差与线性运算)
- en: 'Let (Ω,Σ,P) be a probability space, and let X : Ω →ℝ be a random variable.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '设(Ω, Σ, P)为一个概率空间，且X : Ω → ℝ为一个随机变量。'
- en: (a) Let a ∈ℝ be an arbitrary constant. Then
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 设a ∈ ℝ为任意常数。那么
- en: '![ 2 Var[aX ] = a Var[X ]. ](img/file1938.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 方差[aX ] = a 方差[X ]. ](img/file1938.png)'
- en: '(b) Let Y : Ω →ℝ be a random variable that is independent from X. Then'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '(b) 设Y : Ω → ℝ为一个与X独立的随机变量。那么'
- en: '![Var[X + Y ] = Var[X ]+ Var[Y]. ](img/file1939.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![方差[X + Y ] = 方差[X ]+ 方差[Y]. ](img/file1939.png)'
- en: Proof. (a) Let μ[X] = 𝔼[X]. Then we have
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。(a) 设μ[X] = 𝔼[X]。则我们有
- en: '![ [ ] [ ] Var[aX ] = 𝔼 (aX − a μX)2 = 𝔼 a2(X − μX )2 [ ] = a2𝔼 (X − μX )2
    = a2Var[X ], ](img/file1940.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![ [ ] [ ] Var[aX ] = 𝔼 (aX − a μX)2 = 𝔼 a2(X − μX )2 [ ] = a2𝔼 (X − μX )2
    = a2Var[X ], ](img/file1940.png)'
- en: which is what we had to show. (b) Let μ[Y] = 𝔼[Y ]. Then, due to the linearity
    of the expected value, we have
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要展示的结果。（b）令μ[Y] = 𝔼[Y]。然后，由于期望值的线性性质，我们有：
- en: '![ [ 2] Var[X + Y ] = 𝔼 (X + Y − (μX + μY )) [ 2] = 𝔼 ((X − μX ) + (Y − μY))
    = 𝔼 [(X − μX)2]+ 2𝔼[(X − μX )(Y − μY)] + 𝔼[(Y − μY )2]. ](img/file1941.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![ [ 2] Var[X + Y ] = 𝔼 (X + Y − (μX + μY )) [ 2] = 𝔼 ((X − μX ) + (Y − μY))
    = 𝔼 [(X − μX)2]+ 2𝔼[(X − μX )(Y − μY)] + 𝔼[(Y − μY )2]. ](img/file1941.png)'
- en: Now, as X and Y are independent, 𝔼[XY ] = 𝔼[X]𝔼[Y ]. Thus, due to the linearity
    of the expected value,
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于X和Y是独立的，𝔼[XY ] = 𝔼[X]𝔼[Y]。因此，由于期望值的线性性质，
- en: '![ [ ] [ ] 𝔼 (X − μX )(Y − μY) = 𝔼 XY − X μY − μX Y + μX μY = 𝔼[XY ] − 𝔼[X
    μ ]− 𝔼 [μ Y ]+ μ μ [ ] [ ] [Y ] X [ ] X Y = 𝔼 X 𝔼 Y − 𝔼 X μY − μX 𝔼 Y + μX μY
    = μXμY − μXμY − μXμY + μXμY = 0\. ](img/file1942.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![ [ ] [ ] 𝔼 (X − μX )(Y − μY) = 𝔼 XY − X μY − μX Y + μX μY = 𝔼[XY ] − 𝔼[X
    μ ]− 𝔼 [μ Y ]+ μ μ [ ] [ ] [Y ] X [ ] X Y = 𝔼 X 𝔼 Y − 𝔼 X μY − μX 𝔼 Y + μX μY
    = μXμY − μXμY − μXμY + μXμY = 0\. ](img/file1942.png)'
- en: Thus, continuing the first calculation,
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，继续进行第一次计算，
- en: '![Var[X + Y ] = 𝔼 [(X − μ )2]+ 2𝔼[(X − μ )(Y − μ )] + 𝔼[(Y − μ )2] X X Y Y
    = 𝔼 [(X − μX)2]+ 𝔼[(Y − μY )2], ](img/file1943.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![Var[X + Y ] = 𝔼 [(X − μ )2]+ 2𝔼[(X − μ )(Y − μ )] + 𝔼[(Y − μ )2] X X Y Y
    = 𝔼 [(X − μX)2]+ 𝔼[(Y − μY )2], ](img/file1943.png)'
- en: which is what we had to show.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要展示的结果。
- en: 20.4.1 Covariance and correlation
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.4.1 协方差和相关性
- en: Expected value and variance measure a random variable in isolation. However,
    in real problems, we need to discover relations between separate measurements.
    Say, X describes the price of a given real estate, while Y measures its size.
    These are certainly related, but one does not determine the other. For instance,
    the location might be a differentiator between the prices.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值和方差度量了一个随机变量的独立性。然而，在实际问题中，我们需要发现不同测量之间的关系。假设，X表示给定房地产的价格，而Y表示其面积。这两者肯定是相关的，但并不是互相决定的。例如，位置可能是价格差异的因素。
- en: The simplest statistical way of measuring similarity is the covariance and correlation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 测量相似性的最简单统计方法是协方差和相关性。
- en: Definition 95\. (Covariance and correlation)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 95.（协方差与相关性）
- en: 'Let (Ω,Σ,P) be a probability space, let X,Y : Ω → ℝ be two random variables,
    and let μ[X] = 𝔼[X],μ[Y] = 𝔼[Y ] be their expected values and σ[X] = Std[X],σ[Y]
    = Std[Y ] their standard deviations.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '设(Ω,Σ,P)为概率空间，设X,Y : Ω → ℝ为两个随机变量，且μ[X] = 𝔼[X]，μ[Y] = 𝔼[Y]为它们的期望值，σ[X] = Std[X]，σ[Y]
    = Std[Y]为它们的标准差。'
- en: (a) The covariance of X and Y is defined by
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: (a) X和Y的协方差由下式定义：
- en: '![ [ ] Cov [X,Y ] := 𝔼 (X − μX )(Y − μY ). ](img/file1944.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![ [ ] Cov [X,Y ] := 𝔼 (X − μX )(Y − μY ). ](img/file1944.png)'
- en: (b) The correlation of X and Y is defined by
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: (b) X和Y的相关性由下式定义：
- en: '![ Cov[X, Y] Corr [X, Y ] := ---------. σXσY ](img/file1945.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![ Cov[X, Y] Corr [X, Y ] := ---------. σXσY ](img/file1945.png)'
- en: Similarly to variance, the definition of covariance can be simplified to provide
    an easier way of calculating its exact value.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于方差，协方差的定义可以简化，从而提供一种更简便的计算其确切值的方法。
- en: Proposition 6\.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 命题 6\.
- en: 'Let (Ω,Σ,P) be a probability space, let X,Y : Ω → ℝ be two random variables,
    and let μ[X] = 𝔼[X],μ[Y] = 𝔼[Y ] be their expected values.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '设(Ω,Σ,P)为概率空间，设X,Y : Ω → ℝ为两个随机变量，且μ[X] = 𝔼[X]，μ[Y] = 𝔼[Y]为它们的期望值。'
- en: Then
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '![Cov[X, Y] = 𝔼[XY ]− μX μY . ](img/file1946.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![Cov[X, Y] = 𝔼[XY ]− μX μY . ](img/file1946.png)'
- en: Proof. This is just a simple calculation. According to the definition, we have
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：这只是一个简单的计算。根据定义，我们有：
- en: '![ [ ] Cov[X, Y] = 𝔼 (X − μX )(Y − μY) [ ] = 𝔼 XY − X μY − μX Y + μX μY = 𝔼[XY
    ] − 𝔼[X μ ]− 𝔼[μ Y ]+ μ μ [ ] [ ]Y X[ ] X Y = 𝔼 XY − 𝔼 X μY − μX 𝔼 Y + μX μY [
    ] = 𝔼 XY − μX μY − μX μY + μX μY = 𝔼[XY ]− μX μY , ](img/file1947.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![ [ ] Cov[X, Y] = 𝔼 (X − μX )(Y − μY) [ ] = 𝔼 XY − X μY − μX Y + μX μY = 𝔼[XY
    ] − 𝔼[X μ ]− 𝔼[μ Y ]+ μ μ [ ] [ ]Y X[ ] X Y = 𝔼 XY − 𝔼 X μY − μX 𝔼 Y + μX μY [
    ] = 𝔼 XY − μX μY − μX μY + μX μY = 𝔼[XY ]− μX μY , ](img/file1947.png)'
- en: which is what we had to show.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要展示的结果。
- en: One of the most important properties of covariance and correlation is that they
    are zero for independent random variables.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差和相关性的一个最重要的性质是，对于独立的随机变量，它们的值为零。
- en: Theorem 133\.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 133\.
- en: 'Let (Ω,Σ,P) be a probability space, and let X,Y : Ω →ℝ be two independent random
    variables.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '设(Ω,Σ,P)为概率空间，且X,Y : Ω → ℝ为两个独立的随机变量。'
- en: Then, Cov[X,Y ] = 0\. (And consequently, Corr[X,Y ] = 0 as well.)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Cov[X,Y ] = 0\. （因此，Corr[X,Y ] = 0 也是如此。）
- en: The proof follows straight from the definition and Theorem [130](ch032.xhtml#x1-331005r130),
    so this is left as an exercise for you.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 证明直接来自定义和定理[130](ch032.xhtml#x1-331005r130)，所以这部分留给你做练习。
- en: 'Take note, as this is extra important: independence implies zero covariance,
    but zero covariance does not imply independence. Here is an example.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这一点非常重要：独立性意味着零协方差，但零协方差并不意味着独立性。这里有一个例子。
- en: Let X be a discrete random variable with the probability mass function
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 设 X 是一个离散随机变量，其概率质量函数为：
- en: '![ 1 P(X = − 1) = P (X = 0) = P(X = 1) = 3, ](img/file1948.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 P(X = − 1) = P (X = 0) = P(X = 1) = 3, ](img/file1948.png)'
- en: and let Y = X².
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 并且设 Y = X²。
- en: The expected value of X is
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: X 的期望值是
- en: '![𝔼 [X ] = (− 1)⋅P (X = − 1)+ 0 ⋅P (X = 0)+ 1 ⋅P (X = 1) = − 1-+ 0 + 1- 3 3
    = 0, ](img/file1949.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![𝔼 [X ] = (− 1)⋅P (X = − 1)+ 0 ⋅P (X = 0)+ 1 ⋅P (X = 1) = − 1-+ 0 + 1- 3 3
    = 0, ](img/file1949.png)'
- en: while the law of the unconscious statistician (Theorem [131](ch032.xhtml#x1-331006r131))
    gives that
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 而无意识统计学家的法则（定理[131](ch032.xhtml#x1-331006r131)）表明：
- en: '![ 2 𝔼[Y ] = 𝔼[X ] = 1 ⋅P(X = − 1) + 0⋅P (X = 0) + 1⋅P (X = 1 ) 1- 1- = 3 +
    0+ 3 2 = -, 3 ](img/file1950.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 𝔼[Y ] = 𝔼[X ] = 1 ⋅P(X = − 1) + 0⋅P (X = 0) + 1⋅P (X = 1 ) 1- 1- = 3 +
    0+ 3 2 = -, 3 ](img/file1950.png)'
- en: and
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![𝔼[XY ] = 𝔼[X3] = 0\. ](img/file1951.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![𝔼[XY ] = 𝔼[X3] = 0\. ](img/file1951.png)'
- en: Thus,
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '![Cov [X,Y ] = 𝔼 [XY ]− 𝔼[X ]𝔼 [Y ] 3 2 = 𝔼 [X ]− 𝔼 [X ]𝔼[X ] 2- = 0 − 0⋅ 3
    = 0\. ](img/file1952.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![Cov [X,Y ] = 𝔼 [XY ]− 𝔼[X ]𝔼 [Y ] 3 2 = 𝔼 [X ]− 𝔼 [X ]𝔼[X ] 2- = 0 − 0⋅ 3
    = 0\. ](img/file1952.png)'
- en: 'However, X and Y are not independent, as Y = X² is a function of X. (I shamelessly
    stole this example from a brilliant Stack Overflow thread, which you should read
    here for more on this question: [https://stats.stackexchange.com/questions/179511/why-zero-correlation-does-not-necessarily-imply-independence](https://stats.stackexchange.com/questions/179511/why-zero-correlation-does-not-necessarily-imply-independence))'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，X 和 Y 不是独立的，因为 Y = X² 是 X 的一个函数。（我毫不羞愧地借用了这个例子，来自一个精彩的 Stack Overflow 讨论，你应该阅读更多关于这个问题的内容：[https://stats.stackexchange.com/questions/179511/why-zero-correlation-does-not-necessarily-imply-independence](https://stats.stackexchange.com/questions/179511/why-zero-correlation-does-not-necessarily-imply-independence)）
- en: Do you recall that we interpreted the concept of probability as the relative
    frequency of occurrences? Now that we have the expected value under our belt,
    we can finally make this precise. Let’s look at the famous law of large numbers!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否还记得我们如何将概率的概念解释为事件发生的相对频率？现在，我们已经掌握了期望值，我们终于可以精确地描述这个概念了。让我们来看看著名的大数法则！
- en: 20.5 The law of large numbers
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.5 大数法则
- en: 'We’ll continue our journey with a quite remarkable and famous result: the law
    of large numbers. You have probably already heard several faulty arguments invoking
    the law of large numbers. For instance, gamblers are often convinced that their
    bad luck will end soon because of said law. This is one of the most frequently
    misused mathematical terms, and we are here to clear that up.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续前进，探索一个相当显著且著名的结果：大数法则。你可能已经听过一些错误的关于大数法则的论点。例如，赌徒们常常相信他们的不幸很快就会结束，因为大数法则。这是最常被误用的数学术语之一，我们在这里将澄清这一点。
- en: We’ll do this in two passes. First, we are going to see an intuitive interpretation,
    then add the technical but important mathematical details. I’ll try to be gentle.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分两步进行。首先，我们将看到一个直观的解释，然后添加技术性但重要的数学细节。我会尽量温和一些。
- en: 20.5.1 Tossing coins…
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.5.1 投掷硬币……
- en: First, let’s toss some coins again. If we toss coins repeatedly, what is the
    relative frequency of heads in the long run?
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们再投几次硬币。如果我们重复投掷硬币，长期来看，正面朝上的相对频率是多少？
- en: 'We should have a pretty good guess already: the average number of heads should
    converge to P(heads) = p as well. Why? Because we saw this when studying the frequentist
    interpretation of probability in Section [18.2.7](ch030.xhtml#how-to-interpret-probability).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该已经有了一个相当不错的猜测：正面朝上的平均次数也应该趋向于 P(正面) = p。为什么？因为我们在第[18.2.7](ch030.xhtml#how-to-interpret-probability)节研究概率的频率解释时看到过这个现象。
- en: Our simulation showed that the relative frequency of heads does indeed converge
    to the true probability. This time, we’ll carry the simulation a bit further.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模拟显示，正面朝上的相对频率确实趋向于真实的概率。这一次，我们将进一步进行模拟。
- en: First, to formulate the problem, let’s introduce the independent random variables
    X[1],X[2],… that are distributed along Bernoulli(p), where X[i] = 0 if the toss
    results in tails, while X[i] = 1 if it is heads. We are interested in the long-term
    behavior of
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了构造问题，我们引入独立随机变量 X[1], X[2], …，它们的分布是伯努利(p)，其中 X[i] = 0 表示投掷结果为反面，而 X[i]
    = 1 表示投掷结果为正面。我们关注的是
- en: '![-- X1-+-⋅⋅⋅+-Xn- Xn = n . ](img/file1953.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![-- X1-+-⋅⋅⋅+-Xn- Xn = n . ](img/file1953.png)'
- en: X[n] is called the sample average. We have already seen that the sample average
    gets closer and closer to p as n grows. Let’s see the simulation one more time,
    before we go any further. (The parameter p is selected to be 1∕2 for the sake
    of the example.)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: X[n] 被称为样本平均值。我们已经看到，随着n的增大，样本平均值越来越接近p。让我们在继续之前再看一次模拟结果。（为了方便示例，参数p选择为1∕2。）
- en: '[PRE0]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And here is the plot.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这是图表。
- en: '[PRE1]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![PIC](img/file1954.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1954.png)'
- en: 'Figure 20.3: Relative frequency of the coin tosses'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.3：掷硬币的相对频率
- en: 'Nothing new so far. However, if you have a sharp eye, you might ask the question:
    is this just an accident? After all, we are studying the average'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止没有什么新内容。然而，如果你眼尖的话，可能会问：这只是偶然吗？毕竟，我们研究的是平均值。
- en: '![-- X1-+-⋅⋅⋅+-Xn- Xn = n , ](img/file1955.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![-- X1-+-⋅⋅⋅+-Xn- Xn = n , ](img/file1955.png)'
- en: which is (almost) a binomially distributed random variable! To be more precise,
    if X[i] ∼ Bernoulli(p), then
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎是一个二项分布的随机变量！更准确地说，如果 X[i] ∼ 伯努利分布(p)，则
- en: '![-- Xn ∼ 1Binomial(n,p). n ](img/file1956.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![-- Xn ∼ 1Binomial(n,p). n ](img/file1956.png)'
- en: (We saw this earlier when discussing the sums of discrete random variables in
    Section [19.2.7](ch031.xhtml#sums-of-discrete-random-variables).)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: （我们在讨论离散随机变量的和时已经看到过这个内容，在第[19.2.7](ch031.xhtml#sums-of-discrete-random-variables)节中。）
- en: At this point, it is far from guaranteed that this distribution will be concentrated
    around a single value. So, let’s do some more simulations. This time, we’ll toss
    a coin a thousand times to see the distribution of the averages. Quite meta, I
    know.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，无法保证这个分布会集中在单一的值附近。因此，让我们做更多的模拟。这次，我们将掷硬币一千次，看看样本平均值的分布情况。确实很有meta的感觉，我知道。
- en: '[PRE2]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can visualize the distributions on histograms.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在直方图上可视化这些分布。
- en: '[PRE3]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![PIC](img/file1957.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1957.png)'
- en: 'Figure 20.4: Sample average distributions of coin tosses'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.4：掷硬币的样本平均值分布
- en: In other words, the probability of X[n] falling far from p becomes smaller and
    smaller. For any small 𝜀, we can formulate the probability of “X[n] falling farther
    from p than 𝜀” as P(|X[n] −p|𝜀).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，X[n]远离p的概率会变得越来越小。对于任何小的𝜖，我们可以将“X[n]远离p超过𝜖”的概率表示为P(|X[n] −p| > 𝜖)。
- en: Thus, mathematically speaking, our guess is
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从数学角度来看，我们的猜测是
- en: '![ -- nl→im∞ P(|Xn − p| >𝜀) = 0\. ](img/file1958.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![ -- nl→im∞ P(|Xn − p| >𝜖) = 0\. ](img/file1958.png)'
- en: Again, is this just an accident, and were we just lucky to study an experiment
    where this is true? Would the same work for random variables other than Bernoulli
    ones? What will the sample averages converge to? (If they converge at all.)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这只是偶然吗？我们是否仅仅幸运地研究了一个符合这个规律的实验？对于除伯努利随机变量外的其他随机变量是否也成立？样本平均值最终会收敛到什么地方？（如果它们确实会收敛的话。）
- en: We’ll find out.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会找到答案的。
- en: 20.5.2 …rolling dice…
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.5.2 …掷骰子…
- en: Let’s play dice. To keep things simple, we are interested in the average value
    of a roll in the long run. To build a proper probabilistic model, let’s introduce
    random variables!
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来玩掷骰子。为了简化问题，我们关注的是长期掷骰子的平均值。为了建立一个合适的概率模型，我们来引入随机变量！
- en: A single roll is uniformly distributed on {1,2,…,6}, and each roll is independent
    from the others. So, let X[1],X[2],… be independent random variables, each distributed
    according to Uniform({1,2,…,6}).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 单次掷骰子是均匀分布在{1,2,…,6}上的，并且每次掷骰子相互独立。因此，设 X[1], X[2], … 为独立随机变量，每个随机变量都按照均匀分布Uniform({1,2,…,6})分布。
- en: How does the sample average X[n] behave? Simulation time. We’ll randomly generate
    1000 rolls, then explore how X[n] behaves.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 样本平均值 X[n] 会怎么变化？是时候进行模拟了。我们将随机生成1000次掷骰子的结果，然后探索 X[n] 的变化情况。
- en: '[PRE4]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Again, to obtain a bit of an insight, we’ll visualize the averages on a plot.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 再次为了获得一些直观的理解，我们将在图表上可视化这些平均值。
- en: '[PRE5]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![PIC](img/file1959.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1959.png)'
- en: 'Figure 20.5: Sample averages of rolling a six-sided dice'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.5：掷六面骰子的样本平均值
- en: 'The first thing to note is that these are suspiciously close to 3.5\. This
    is not a probability, but the expected value:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要注意的是，这些数值异常接近3.5。这不是一个概率，而是期望值：
- en: '![𝔼[X1 ] = 𝔼[X2] = ⋅⋅⋅ = 3.5\. ](img/file1960.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![𝔼[X1 ] = 𝔼[X2] = ⋅⋅⋅ = 3.5\. ](img/file1960.png)'
- en: For Bernoulli(p) distributed random variables, the expected value coincides
    with the probability p. However, this time, X[n] does not have a nice and explicit
    distribution like in the case of coin tosses, where the sample averages were binomially
    distributed. So, let’s roll some more dice to estimate how X[n] is distributed.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 对于伯努利(p)分布的随机变量，期望值与概率p相同。然而，这一次，X[n] 并不像掷硬币时那样有一个明确的分布，样本平均值也不再是二项分布。因此，我们来多掷一些骰子，估计
    X[n] 的分布情况。
- en: '[PRE6]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![PIC](img/file1961.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1961.png)'
- en: 'Figure 20.6: Sample average distributions of dice rolls'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.6：掷骰子时的样本平均分布
- en: It seems like, once more, the distribution of X[n] is concentrated around 𝔼[X[1]].
    Our intuition tells us that this is not an accident; that this phenomenon is true
    for a wide range of random variables.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，一次又一次地，X[n] 的分布集中在 𝔼[X[1]] 附近。我们的直觉告诉我们，这不是偶然；这一现象对于广泛的随机变量都成立。
- en: 'Let me spoil the surprise: this is indeed the case, and we’ll see this now.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我剧透一下：这确实是这样，我们现在就来看。
- en: 20.5.3 …and all the rest
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.5.3 …以及其余部分
- en: 'This time, let X[1],X[2],… be a sequence of independent and identically distributed
    (i.i.d.) random variables. Not coin tosses, not dice rolls, but any distribution.
    We saw that the sample average X[n] seems to converge to the joint expected value
    of the X[i]-s:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，设 X[1],X[2],… 是一列独立同分布（i.i.d.）的随机变量。不是掷硬币，不是掷骰子，而是任何分布。我们看到样本平均值 X[n] 似乎收敛于
    X[i]-s 的联合期望值：
- en: '![′′- ′′ Xn → 𝔼[X1] ](img/file1962.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![′′- ′′ Xn → 𝔼[X1] ](img/file1962.png)'
- en: 'Note the quotation marks: X[n] is not a number but a random variable. Thus,
    we can’t (yet) speak about convergence.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意引号：X[n] 不是一个数字，而是一个随机变量。因此，我们还不能谈论收敛。
- en: In mathematically precise terms, what we saw previously is that for large enough
    n-s, the sample average X[n] is highly unlikely to fall far from the joint expected
    value μ = 𝔼[X[1]]; that is,
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上精确来说，我们之前看到的是，当 n 足够大时，样本平均值 X[n] 很不可能远离联合期望值 μ = 𝔼[X[1]]；也就是说，
- en: lim[n→∞] P(|X[n] − μ| > 𝜀) = 0 (20.2)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: lim[n→∞] P(|X[n] − μ| > 𝜖) = 0 (20.2)
- en: holds for all 𝜀/span>0\.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有 𝜖/span>0\ 都成立。
- en: The limit ([20.2](ch032.xhtml#and-all-the-rest)) seems hard to prove right now
    even in the simple case of coin tossing. There, X[n] ∼![1n](img/file1963.png)
    Binomial(n,p), thus
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 极限（[20.2](ch032.xhtml#and-all-the-rest)）现在似乎很难证明，即使是在简单的掷硬币的情况下。在那里，X[n] ∼![1n](img/file1963.png)
    二项分布(n,p)，因此
- en: '![ ⌊n(p+ 𝜀)⌋ ( ) -- ∑ n k n−k P(|Xn − μ | >𝜀) = 1 − k p (1 − p) , k=⌊n(p− 𝜀)⌋
    ](img/file1964.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![ ⌊n(p+ 𝜖)⌋ ( ) -- ∑ n k n−k P(|Xn − μ | >𝜖) = 1 − k p (1 − p) , k=⌊n(p− 𝜖)⌋
    ](img/file1964.png)'
- en: where the symbol ⌊x⌋ denotes the largest integer that is smaller than x. This
    does not look friendly at all. (I leave the verification as an exercise.)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 其中符号 ⌊x⌋ 表示小于 x 的最大整数。这看起来一点也不友好。（我把验证留作练习。）
- en: Thus, our plan is the following.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的计划如下。
- en: Find a way to estimate P(jX[n]−μj/span>𝜀) in a way that is independent from
    the distribution of the X[i]-s.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找一种估计 P(jX[n]−μj/span>𝜖) 的方法，使其与 X[i]-s 的分布无关。
- en: Use the upper estimate to show lim[n→∞]P(jX[n] −μj/span>𝜀) = 0.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上界估计来证明 lim[n→∞]P(jX[n] −μj/span>𝜖) = 0。
- en: Let’s go.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始吧。
- en: 20.5.4 The weak law of large numbers
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.5.4 大数法则的弱法则
- en: First, the upper estimates. There are two general inequalities that’ll help
    us to deal with P(jX[n] −μj ≥𝜀).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是上界估计。有两个一般不等式可以帮助我们处理 P(jX[n] −μj ≥𝜖)。
- en: Theorem 134\. (Markov’s inequality)
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 134. （马尔可夫不等式）
- en: 'Let (Ω,Σ,P) be a probability space and let X : Ω → [0,∞) be a nonnegative random
    variable. Then'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '设 (Ω,Σ,P) 是一个概率空间，且 X : Ω → [0,∞) 是一个非负随机变量。那么'
- en: '![ 𝔼[X ] P (X ≥ t) ≤ ----- t ](img/file1965.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![ 𝔼[X ] P (X ≥ t) ≤ ----- t ](img/file1965.png)'
- en: holds for any t ∈ (0,∞).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意 t ∈ (0,∞)，此不等式都成立。
- en: Proof. We have to separate the discrete and the continuous cases. The proofs
    are almost identical, so I’ll only do the discrete case here, while the continuous
    is left for you as an exercise to test your understanding.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：我们需要分离离散情况和连续情况。证明几乎完全相同，所以我只会做离散情况，连续情况留给你作为练习来验证你的理解。
- en: 'So, let X : Ω → {x[1],x[2],…} be a discrete random variable (where x[k] ≥ 0
    for all k), and t ∈ (0,∞) be an arbitrary positive real number.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '设 X : Ω → {x[1],x[2],…} 是一个离散型随机变量（其中 x[k] ≥ 0 对所有 k 成立），且 t ∈ (0,∞) 是一个任意的正实数。'
- en: Then
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 那么
- en: 𝔼[X]
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 𝔼[X]
- en: = ∑ [k=1]^∞x [k]P(X = x[k])
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: = ∑ [k=1]^∞x [k]P(X = x[k])
- en: = ∑ [k:x[k]/span>tx[k]P(X = x[k]) + ∑ [k:x[k]≥t]x[k]P(X = x[k]),]
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: = ∑ [k:x[k]/span>tx[k]P(X = x[k]) + ∑ [k:x[k]≥t]x[k]P(X = x[k]),]
- en: 'where the sum ∑ [k:x[k]/span>t only accounts for k-s with x[k]/span>t, and
    similarly, ∑ [k] : x[k] ≥t only accounts for k-s with x[k] ≥t.]'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '其中和 ∑ [k:x[k]/span>t 仅仅计算了 x[k]/span>t 的 k-s，类似地，∑ [k] : x[k] ≥t 仅计算了 x[k]
    ≥t 的 k-s。'
- en: As the x[k]-s are nonnegative by assumption, we can estimate 𝔼[X] from below
    by omitting one of them. Thus,
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 由于假设 x[k]-s 为非负数，因此我们可以通过忽略其中一个来从下方估计 𝔼[X]。因此，
- en: '![ ∑ ∑ 𝔼[X ] = xkP (X = xk )+ xkP (X = xk) k:x < k:x ≥t ∑k k ≥ xkP (X = xk
    ) k:xk≥t ∑ ≥ t P (X = xk ) k:xk≥t = tP(X ≥ t), ](img/file1966.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑ ∑ 𝔼[X ] = xkP (X = xk )+ xkP (X = xk) k:x < k:x ≥t ∑k k ≥ xkP (X = xk
    ) k:xk≥t ∑ ≥ t P (X = xk ) k:xk≥t = tP(X ≥ t), ](img/file1966.png)'
- en: from which Markov’s inequality
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以得到马尔可夫不等式
- en: '![P (X ≥ t) ≤ 𝔼[X-] t ](img/file1967.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![P (X ≥ t) ≤ 𝔼[X-] t ](img/file1967.png)'
- en: follows.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如下。
- en: The law of large numbers is only one step away from Markov’s inequality. This
    last step is so useful that it deserves to be its own theorem. Meet the famous
    inequality of Chebyshev.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 大数法则仅一步之遥，接近于马尔可夫不等式。最后这一步非常有用，值得成为一个独立的定理。让我们来看看著名的切比雪夫不等式。
- en: Theorem 135\. (Chebyshev’s inequality)
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 135\.（切比雪夫不等式）
- en: 'Let (Ω,Σ,P) be a probability space and let X : Ω →ℝ be a random variable with
    finite variance σ² = Var[X]/span>∞and expected value 𝔼[X] = μ.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '设 (Ω, Σ, P) 为一个概率空间，X : Ω →ℝ 是一个具有有限方差 σ² = Var[X] 和期望值 𝔼[X] = μ 的随机变量。'
- en: Then
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '![ σ2 P(|X − μ| ≥ t) ≤ -t2- ](img/file1968.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![ σ2 P(|X − μ| ≥ t) ≤ -t2- ](img/file1968.png)'
- en: holds for all t ∈ (0,∞).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有 t ∈ (0,∞) 成立。
- en: Proof. As |X −μ| is a nonnegative random variable, we can apply Theorem [134](ch032.xhtml#x1-338002r134)
    to obtain
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。由于 |X − μ| 是一个非负随机变量，我们可以应用定理 [134](ch032.xhtml#x1-338002r134) 得到
- en: '![P (|X − μ| ≥ t) = P(|X − μ|2 ≥ t2) 2 ≤ 𝔼[|X--−-μ|-]. t2 ](img/file1969.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![P (|X − μ| ≥ t) = P(|X − μ|2 ≥ t2) 2 ≤ 𝔼[|X--−-μ|-]. t2 ](img/file1969.png)'
- en: However, as 𝔼[|X −μ|²] = Var[X] = σ², we have
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于 𝔼[|X − μ|²] = Var[X] = σ²，我们有
- en: '![ 2 2 P(|X − μ | ≥ t) ≤ 𝔼-[|X-−2μ-|] = σ2 t t ](img/file1970.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 2 P(|X − μ | ≥ t) ≤ 𝔼-[|X-−2μ-|] = σ2 t t ](img/file1970.png)'
- en: which is what we had to show.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要证明的。
- en: And with that, we are ready to precisely formulate and prove the law of large
    numbers. After all this setup, the (weak) law of large numbers is just a small
    step away. Here it is in its full glory.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些内容，我们准备好精确地表述和证明大数法则。经过所有这些准备，（弱）大数法则只是一步之遥。以下是它的完整形式。
- en: Theorem 136\. (The weak law of large numbers)
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 136\.（弱大数法则）
- en: Let X[1],X[2],… be a sequence of independent and identically distributed random
    variables with finite expected value μ = 𝔼[X[1]] and variance σ² = Var[X[1]],
    and let
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 设 X[1], X[2], … 为一列独立同分布的随机变量，其期望值 μ = 𝔼[X[1]] 和方差 σ² = Var[X[1]]，且
- en: '![-- X1 + ⋅⋅⋅+ Xn Xn = ------------- n ](img/file1971.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![-- X1 + ⋅⋅⋅+ Xn Xn = ------------- n ](img/file1971.png)'
- en: be their sample average. Then
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 设它们的样本平均值为。然后
- en: '![lim P (|X- − μ| ≥ 𝜀) = 0 n→ ∞ n ](img/file1972.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![lim P (|X- − μ| ≥ 𝜀) = 0 n→ ∞ n ](img/file1972.png)'
- en: holds for any 𝜀/span>0\.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 对任何 𝜀/span>0\ 成立。
- en: Proof. As the X[i]-s are independent, the variance of the sample average is
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。由于 X[i] 是独立的，样本平均值的方差为
- en: '![ -- [ X1 + ⋅⋅⋅+ Xn ] Var[Xn ] = Var ------------- n = -1-Var[X1 + ⋅⋅⋅+ Xn]
    n2 = -1-(Var[X ]+ ⋅⋅⋅+ Var[X ]) n2 1 n n σ2 σ2 = -n2- = n-. ](img/file1973.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![ -- [ X1 + ⋅⋅⋅+ Xn ] Var[Xn ] = Var ------------- n = -1-Var[X1 + ⋅⋅⋅+ Xn]
    n2 = -1-(Var[X ]+ ⋅⋅⋅+ Var[X ]) n2 1 n n σ2 σ2 = -n2- = n-. ](img/file1973.png)'
- en: Now, by using Chebyshev’s inequality from Theorem [135](ch032.xhtml#x1-338003r135),
    we obtain
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过使用定理 [135](ch032.xhtml#x1-338003r135) 中的切比雪夫不等式，我们可以得到
- en: '![ -- -- Var[Xn-] -σ2- P (|Xn − μ | ≥ 𝜀) ≤ 𝜀2 = n 𝜀2\. ](img/file1974.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![ -- -- Var[Xn-] -σ2- P (|Xn − μ | ≥ 𝜀) ≤ 𝜀2 = n 𝜀2\. ](img/file1974.png)'
- en: Thus,
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '![0 ≤ lim P(|Xn − μ | ≥ 𝜀) n→ ∞ -σ2- ≤ nl→im∞ n𝜀2 = 0, ](img/file1975.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![0 ≤ lim P(|Xn − μ | ≥ 𝜀) n→ ∞ -σ2- ≤ nl→im∞ n𝜀2 = 0, ](img/file1975.png)'
- en: hence
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: '![ -- nli→m∞ P(|Xn − μ | ≥ 𝜀) = 0, ](img/file1976.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![ -- nli→m∞ P(|Xn − μ | ≥ 𝜀) = 0, ](img/file1976.png)'
- en: which is what we needed to show.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要证明的。
- en: Theorem [136](ch032.xhtml#x1-338004r136) is not all that can be said about the
    sample averages. There is a stronger result, showing that the sample averages
    do in fact converge to the mean with probability 1.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 [136](ch032.xhtml#x1-338004r136) 不是关于样本平均值的全部内容。还有更强的结果，表明样本平均值确实以概率 1 收敛到均值。
- en: 20.5.5 The strong law of large numbers
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.5.5 强大数法则
- en: Why is Theorem [136](ch032.xhtml#x1-338004r136) called the “weak” law? Think
    about the statement
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么定理 [136](ch032.xhtml#x1-338004r136) 被称为“弱”大数法则？想想这个声明
- en: lim[n→∞] P(|X[n] − μ| ≥ 𝜀) = 0 (20.3)
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: lim[n→∞] P(|X[n] − μ| ≥ 𝜀) = 0 (20.3)
- en: for a moment. For a given ω ∈ Ω, this doesn’t tell us anything about the convergence
    of a concrete sample average
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 稍等片刻。对于给定的 ω ∈ Ω，这并没有告诉我们具体样本平均值的收敛情况。
- en: '![X- (ω) = X1(ω-)+-⋅⋅⋅+-Xn-(ω-), n n ](img/file1977.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![X- (ω) = X1(ω-)+-⋅⋅⋅+-Xn-(ω-), n n ](img/file1977.png)'
- en: it just tells us that in a probabilistic sense, X[n] is concentrated around
    the joint expected value μ. In a sense, ([20.3](ch032.xhtml#the-strong-law-of-large-numbers))
    is a weaker version of
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是告诉我们，在概率意义上，X[n] 集中在联合期望值 μ 周围。从某种意义上讲，([20.3](ch032.xhtml#the-strong-law-of-large-numbers))
    是
- en: '![P( lim X- = μ ) = 1, n→ ∞ n ](img/file1978.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![P( lim X- = μ ) = 1, n→ ∞ n ](img/file1978.png)'
- en: hence the terminology weak law of large numbers.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是所谓的弱大数法则。
- en: Do we have a stronger result than Theorem [136](ch032.xhtml#x1-338004r136)?
    Yes, we do.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否有比定理 [136](ch032.xhtml#x1-338004r136) 更强的结果？是的，有。
- en: Theorem 137\. (The strong law of large numbers)
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 137\.（强大数法则）
- en: Let X[1],X[2],… be a sequence of independent and identically distributed random
    variables with finite expected value μ = 𝔼[X[1]] and variance σ² = Var[X[1]],
    and let
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 设X[1],X[2],…是独立同分布的随机变量序列，具有有限的期望值μ = 𝔼[X[1]]和方差σ² = Var[X[1]]，并且设
- en: '![-- X + ⋅⋅⋅+ X Xn = -1---------n- n ](img/file1979.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![-- X + ⋅⋅⋅+ X Xn = -1---------n- n ](img/file1979.png)'
- en: be their sample average. Then
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 设它们是样本均值。那么
- en: '![P( lim Xn = μ ) = 1\. n→ ∞ ](img/file1980.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![P( lim Xn = μ ) = 1\. n→ ∞ ](img/file1980.png)'
- en: We are not going to prove this, just know that the sample average will converge
    to the mean with probability one.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会证明这一点，只需要知道样本均值会以概率1收敛到均值。
- en: Remark 21\. (Convergence of random variables)
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 注 21.（随机变量的收敛性）
- en: What we have seen in the weak and strong laws of large numbers are not unique
    to sample averages. Similar phenomena can be observed in other cases, thus, these
    types of convergences have their own exact definitions.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在大数法则的弱法则和强法则中看到的现象并非仅限于样本均值。在其他情况下也可以观察到类似的现象，因此，这些类型的收敛性有其各自的精确定义。
- en: If X[1],X[2],… is a sequence of random variables, we say that
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 如果X[1],X[2],…是一个随机变量序列，我们称
- en: (a) X[n] converges in probability towards X if
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 如果X[n]在概率上收敛到X，则
- en: '![ lim P(|Xn − X | ≥ 𝜀) = 0 n→ ∞ ](img/file1981.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![ lim P(|Xn − X | ≥ 𝜀) = 0 n→ ∞ ](img/file1981.png)'
- en: for all 𝜀/span>0\. Convergence in probability is denoted by X[n]![−P→](img/file1982.png)X.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有𝜀/span>0\. 概率收敛表示为X[n]![−P→](img/file1982.png)X。
- en: (b) X[n] converges almost surely towards X if
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 如果X[n]几乎确定地收敛到X，则
- en: '![P( lim Xn = X ) = 1 n→ ∞ ](img/file1983.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![P( lim Xn = X ) = 1 n→ ∞ ](img/file1983.png)'
- en: holds. Almost sure convergence is denoted by X[n]![−a−.→ s.](img/file1984.png)X.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 成立。几乎确定的收敛表示为X[n]![−a−.→ s.](img/file1984.png)X。
- en: Thus, the weak and strong laws of large numbers state that in certain cases,
    the sample averages converge to the expected value both in probability and almost
    surely.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，大数法则的弱法则和强法则指出，在某些情况下，样本均值在概率上和几乎确定地都收敛到期望值。
- en: 20.6 Information theory
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.6 信息论
- en: If you have already trained a machine learning model in your practice, chances
    are you are already familiar with the mean-squared error
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经在实践中训练过机器学习模型，可能已经熟悉均方误差。
- en: '![ n 1-∑ 2 n MSE (x,y) = n (f(xi)− yi), x, y ∈ ℝ , i=1 ](img/file1985.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![ n 1-∑ 2 n MSE (x,y) = n (f(xi)− yi), x, y ∈ ℝ , i=1 ](img/file1985.png)'
- en: 'where f : ℝ^n →ℝ represents our model, x ∈ℝ^n is the vector of one-dimensional
    observations, and y ∈ℝ^n is the ground truth. After learning all about the expected
    value, this sum should be familiar: if we assume a probabilistic viewpoint and
    let X and Y be the random variables describing the data, then the mean-squared
    error can be written as the expected value'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，f : ℝ^n →ℝ表示我们的模型，x ∈ℝ^n是一个一维观测向量，y ∈ℝ^n是真实值。学完期望值后，这个求和式应该是熟悉的：如果我们假设一个概率视角，并让X和Y是描述数据的随机变量，那么均方误差可以表示为期望值。'
- en: '![ [ ] MSE (x, y) = 𝔼 (f(X )− Y )2\. ](img/file1986.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![ [ ] MSE (x, y) = 𝔼 (f(X )− Y )2\. ](img/file1986.png)'
- en: However, the mean-squared error is not suitable for classification problems.
    For instance, if the task is to classify the object of an image, the output is
    a discrete probability distribution for each sample. In this situation, we could
    use the so-called cross-entropy, defined by
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，均方误差并不适用于分类问题。例如，如果任务是分类一张图像的物体，输出将是每个样本的离散概率分布。在这种情况下，我们可以使用所谓的交叉熵，其定义为
- en: '![ n H [p,q] = − ∑ p logq i i i=1 ](img/file1987.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![ n H [p,q] = − ∑ p logq i i i=1 ](img/file1987.png)'
- en: where p ∈ℝ^n denotes the one-hot encoded vector of the class label for a single
    data sample, and q ∈ℝ^n is the class label prediction, forming a probability distribution.
    (One-hot encoding is the process where we represent a finite set of possible class
    labels, such as {a,b,c} as zero-one vectors, like
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，p ∈ℝ^n表示单个数据样本的类别标签的独热编码向量，q ∈ℝ^n是类别标签预测，形成一个概率分布。（独热编码是将一个有限的类别标签集合，如{a,b,c}，表示为零一向量的过程，例如
- en: '![a ← → (1,0,0), b ← → (0,1,0), c ← → (0,0,1). ](img/file1988.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![a ← → (1,0,0), b ← → (0,1,0), c ← → (0,0,1). ](img/file1988.png)'
- en: We do this because it’s easier to work with vectors and matrices than with strings.)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做是因为处理向量和矩阵比处理字符串更容易。)
- en: 'Not that surprisingly, H[p,q] is also an expected value, but it’s much more
    than that: it quantifies the information content of the distribution q compared
    to the ground truth distribution q.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 并不令人惊讶，H[p,q]也是一个期望值，但它远不止如此：它量化了分布q与真实分布q之间的信息内容。
- en: But what is information in a mathematical sense? Let’s dive in.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 但在数学意义上，什么是信息？让我们深入探讨。
- en: 20.6.1 Guess the number
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.6.1 猜数字
- en: Let’s start with a simple game. I have thought of an integer between 0 and 7,
    and your job is to find out which one by asking yes-no questions.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的游戏开始。我已经想好了一个介于 0 到 7 之间的整数，你的任务是通过是非问题找出这个数字。
- en: '![PIC](img/file1989.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1989.png)'
- en: 'Figure 20.7: Which number am I thinking of?'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.7：我在想哪个数字？
- en: 'One possible strategy is to guess the numbers one by one. In other words, the
    sequence of your questions are:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的策略是逐一猜测数字。换句话说，你的问题顺序是：
- en: Is it 0?
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是 0 吗？
- en: Is it 1?
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是 1 吗？
- en: '![.. . ](img/file1990.png)'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![.. . ](img/file1990.png)'
- en: Is it 7?
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是 7 吗？
- en: Although this strategy works, it is not an effective one. Why? Consider the
    average number of questions. Let the random variable X denote the number I have
    picked. As X is uniformly distributed — that is, P(X = k) = 1∕8 for all k = 0,…,7
    — the probability of asking exactly k questions is
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个策略有效，但并不是最有效的。为什么？考虑一下平均问题数。设随机变量 X 表示我选择的数字。由于 X 是均匀分布的——也就是说，P(X = k)
    = 1/8，对于所有 k = 0,…,7——那么恰好问 k 个问题的概率为
- en: '![P(#questions = k) = P(X = k − 1) = 1 8 ](img/file1991.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![P(#questions = k) = P(X = k − 1) = 1 8 ](img/file1991.png)'
- en: as well. Thus, the number of questions needed is also uniformly distributed
    on {1,…,8}, thus
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 同样。因此，所需的问题数也在 {1,…,8} 上均匀分布，因此
- en: '![ ∑8 𝔼[#questions] = kP (#questions = k ) k=1 8 = 1-∑ k 8 k=1 = 1-8⋅9-= 9-,
    8 2 2 ](img/file1992.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑8 𝔼[#questions] = kP (#questions = k ) k=1 8 = 1-∑ k 8 k=1 = 1-8⋅9-= 9-,
    8 2 2 ](img/file1992.png)'
- en: where we have used that ∑ [k=1]^n = ![n(n+1) 2](img/file1993.png).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了 ∑ [k=1]^n = ![n(n+1) 2](img/file1993.png)。
- en: Can we do better than this? Yes. In the previous sequential strategy, each question
    has a small chance of hitting, and a large chance of eliminating, only one potential
    candidate. It’s easy to see that the best would be to eliminate half the search
    space with each question.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做得比这个更好吗？可以。在之前的顺序策略中，每个问题命中目标的机会较小，大部分情况下只会排除一个潜在的候选项。很容易看出，最好的方法是每个问题都能将搜索空间减半。
- en: Say, the number I thought of is 2\. By asking “is the number larger than 3”?,
    the answer trims out four of the candidates.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，我想到的数字是 2。通过问“这个数字大于 3 吗？”，答案可以排除四个候选项。
- en: '![PIC](img/file1994.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1994.png)'
- en: 'Figure 20.8: The search space after the question is the number larger than
    3?'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.8：问题后搜索空间是比 3 大的数字吗？
- en: 'Each subsequent question cuts the remaining possibilities in half. In the case
    of X = 2, the three questions are the following:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 每个后续的问题都会将剩余的可能性减半。以 X = 2 为例，三个问题如下：
- en: Is X ≥ 4? (no)
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X ≥ 4 吗？（否）
- en: Is X ≥ 2? (yes)
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X ≥ 2 吗？（是）
- en: Is X ≥ 3? (no)
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X ≥ 3 吗？（否）
- en: This is the so-called binary search, illustrated by Figure [20.9](#).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的二分查找，如图 [20.9](#) 所示。
- en: '![PIC](img/file1995.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1995.png)'
- en: 'Figure 20.9: Figuring out the answer with binary search'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.9：通过二分查找得出答案
- en: 'If we write down the answers for our three consecutive questions (X ≥ 4, X
    ≥ 2, X ≥ 3) as a zero-one sequence, we obtain 010\. If this looks familiar, it’s
    not an accident: 010 is 2 in binary. In fact, all of the answers can be coded
    using their binary form:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将三个连续问题的答案（X ≥ 4, X ≥ 2, X ≥ 3）写成一个二进制序列，我们得到 010。如果这看起来很熟悉，那并非偶然：010 是二进制的
    2。事实上，所有答案都可以用其二进制形式进行编码：
- en: '![0 = 0002, 1 = 0012, 2 = 0102, 3 = 0112 4 = 1002, 5 = 1012, 6 = 1102, 7 =
    1112\. ](img/file1996.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![0 = 0002, 1 = 0012, 2 = 0102, 3 = 0112 4 = 1002, 5 = 1012, 6 = 1102, 7 =
    1112\. ](img/file1996.png)'
- en: 'Thus, we can reformulate our three questions:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以重新表述我们的三个问题：
- en: Is 1 the 1st digit of X in binary?
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1 是 X 的第一个二进制位吗？
- en: Is 1 the 2nd digit of X in binary?
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1 是 X 的第二个二进制位吗？
- en: Is 1 the 3rd digit of X in binary?
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1 是 X 的第三个二进制位吗？
- en: 'As the above example shows, guessing the number is equivalent to finding the
    binary representation of the objects to be guessed. Each digit represents exactly
    one bit of information. (In this case, the representation is the actual binary
    form.) Binary codings have an additional perk: we no longer have to sequentially
    go through the questions, we can ask them simultaneously. From now on, instead
    of questions, we’ll talk about binary representations (codings) and their bits.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 如上例所示，猜数字等同于找出待猜数字的二进制表示。每一位代表一位信息。（在这种情况下，表示就是实际的二进制形式。）二进制编码有一个额外的好处：我们不再需要顺序地提问，可以同时问多个问题。从现在起，我们将不再讨论问题，而是讨论二进制表示（编码）及其位。
- en: 'Notice that the number of bits is the same for each outcome of X. Thus, as
    this strategy always requires three bits, their average number is three as well:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个 X 的结果所需的位数是相同的。因此，这种策略始终需要三个位，因此它们的平均数量也是三：
- en: '![ 7 7 𝔼 [#bits] = ∑ 3⋅P (X = k ) = ∑ 3⋅ 1-= 3\. 8 k=0 k=0 ](img/file1997.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![ 7 7 𝔼 [#bits] = ∑ 3⋅P (X = k ) = ∑ 3⋅ 1-= 3\. 8 k=0 k=0 ](img/file1997.png)'
- en: Can we do better than the three questions on average? No. I invite you to come
    up with your arguments, but we’ll see this later.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做得比平均三次提问更好吗？不能。我邀请你提出你的论据，但我们稍后会看到这一点。
- en: Where does the number three in the above come from? In general, if we have 2^k
    possible choices, then log [2]2^k = k questions will be enough to find the answer.
    (As each question cuts the set of possible answers in half.) In other words, we
    have
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的三号数字来自哪里？一般来说，如果我们有 2^k 个可能的选择，那么 log [2]2^k = k 个问题就足够找到答案。（因为每个问题都会将可能答案的范围减半。）换句话说，我们有
- en: '![ ∑7 𝔼[#bits] = P (X = k )log2 23 k=0 7 = ∑ P (X = k )log P(X = k)−1\. 2 k=0
    ](img/file1998.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑7 𝔼[#bits] = P (X = k )log2 23 k=0 7 = ∑ P (X = k )log P(X = k)−1\. 2 k=0
    ](img/file1998.png)'
- en: Thus, the value log [2]P(X = k)^(−1) is the number of bits needed to represent
    k in our coding. In other words,
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，log [2]P(X = k)^(−1) 的值是我们编码中表示 k 所需要的比特数。换句话说，
- en: '![𝔼[#bits] = 𝔼[log2P (X = k)−1]. ](img/file1999.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![𝔼[#bits] = 𝔼[log2P (X = k)−1]. ](img/file1999.png)'
- en: 'Let’s get a bit ahead of ourselves: this is the famous entropy of the random
    variable X, and the quantity log [2]P(X = k)^(−1) is the so-called information
    content of the event X = k.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提前一点：这就是著名的随机变量 X 的熵，而 log [2]P(X = k)^(−1) 是所谓的事件 X = k 的信息量。
- en: However, at this point, these concepts are quite unclear. What does log [2]P(X
    = k)^(−1) have to do with information? Why can’t we represent k better than log
    [2]P(X = k)^(−1) bits? We’ll see the answers soon.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，到此为止，这些概念还很不清楚。log [2]P(X = k)^(−1) 与信息有什么关系？为什么我们不能用比 log [2]P(X = k)^(−1)
    更好的方式来表示 k 的比特数？我们很快就会看到答案。
- en: '20.6.2 Guess the number 2: Electric Boogaloo'
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.6.2 猜数字 2：电气波古鲁
- en: Let’s play the guessing game again but with a twist this time. Now, I have picked
    a number from {0,1,2}, and you have to guess which one. The catch is, I am twice
    as likely to select 0 than the others.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再玩一次猜数字的游戏，但这次有点不同。我现在从 {0,1,2} 中选了一个数字，你得猜是哪一个。难点是，我选择 0 的概率是其他数字的两倍。
- en: In probabilistic terms, if X denotes the number I picked, then P(X = 0) = 1∕2,
    while P(X = 1) = P(X = 2) = 1∕4.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率论中，如果 X 表示我选的数字，那么 P(X = 0) = 1∕2，而 P(X = 1) = P(X = 2) = 1∕4。
- en: 'What is the best strategy? There are two key facts to recall:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳策略是什么？有两个关键事实需要记住：
- en: good questions cut the search space in half,
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好的问题会将搜索空间减少一半，
- en: and asking questions is equivalent to finding a binary encoding of the outcomes.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提问相当于找到这些结果的二进制编码。
- en: However, as we are looking for the encoding that is optimal on average, cutting
    the search space in half with each digit is not meant in a numeric way. Rather,
    in a probabilistic one. Thus, if 0 is indeed twice as likely, representing 0,1,2
    by
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们在寻找平均最优的编码，不能简单地理解为每个数字将搜索空间减少一半。这种理解应当是概率性的。所以，如果 0 的确是更有可能的，那么通过
- en: '![0 ∼ 0, 1 ∼ 01, 2 ∼ 10, ](img/file2000.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![0 ∼ 0, 1 ∼ 01, 2 ∼ 10, ](img/file2000.png)'
- en: the average number of bits is
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 平均比特数是
- en: '![𝔼[#bits] = P (X = 0) ⋅1+ P (X = 1) ⋅2+ P (X = 2) ⋅2 = P (X = 0) log22 + P
    (X = 1)log24 + P (X = 2)log24 2 ∑ −1 = P(X = k)log2P (X = k) k=0 = 3-. 2 ](img/file2001.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![𝔼[#bits] = P (X = 0) ⋅1+ P (X = 1) ⋅2+ P (X = 2) ⋅2 = P (X = 0) log22 + P
    (X = 1)log24 + P (X = 2)log24 2 ∑ −1 = P(X = k)log2P (X = k) k=0 = 3-. 2 ](img/file2001.png)'
- en: Once more, we have arrived at the familiar logarithmic formula. We are one step
    closer to grasping the meaning of the mysterious quantity log [2]P(X = k)^(−1).
    The smaller it is, the more questions we need; equivalently, the more bits we
    need to represent k within our encoding to avoid information loss.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 再一次，我们走到了熟悉的对数公式。我们离理解这个神秘量 log [2]P(X = k)^(−1) 更近一步了。它越小，我们需要的问题就越多；同样地，我们在编码中表示
    k 所需的比特数也越多，以避免信息丢失。
- en: So, what are these mysterious quantities exactly?
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些神秘的量到底是什么呢？
- en: 20.6.3 Information and entropy
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.6.3 信息与熵
- en: It is time to formulate the general problem. Suppose that our random variable
    X assumes a number from the set {1,2,…,N}, each with probability p[k] = P(X =
    k). Upon repeatedly observing X, what is the average information content of our
    observations?
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候对这个问题进行一般化了。假设我们的随机变量 X 从集合 {1,2,…,N} 中取一个数，每个数的概率是 p[k] = P(X = k)。在反复观察
    X 后，我们的观察的平均信息量是多少？
- en: According to what we’ve learned, we are looking for the quantity
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们所学到的，我们正在寻找的量是
- en: '![ ∑N 𝔼 [I (X )] = − p logp , k=1 k k ](img/file2002.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑N 𝔼 [I (X )] = − p logp , k=1 k k ](img/file2002.png)'
- en: 'where I : ℕ →ℝ denotes the information I(k) = −log p[k]. Previously, we have
    seen two special cases where I(k) is the average number of questions needed to
    guess k. (Equivalently, the information is the average number of bits in k using
    the optimal encoding.)'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '其中I : ℕ → ℝ表示信息I(k) = −log p[k]。之前，我们已经看到两个特殊情况，其中I(k)是猜测k所需的平均问题数。（等价地，信息是使用最优编码时k的平均比特数。）'
- en: What is the information in general?
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 那么一般来说，信息是什么？
- en: 'Let’s look for I as an unknown function of the probabilities: I(x) = f(P(X
    = x)). What can f be? There are two key properties of that’ll lead us to the answer.
    First, the more probable an event is, the less information content there is. (Recall
    the previous example, where the most probable outcome required the least amount
    of bits in our binary representation.)'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把I视为概率的未知函数：I(x) = f(P(X = x))。那么f可能是什么？有两个关键性质将引导我们找到答案。首先，事件越可能发生，信息含量就越少。（回想一下之前的例子，最可能的结果需要最少的比特数进行二进制表示。）
- en: 'Second, as a function of the probabilities, the information is additive: f(pq)
    = f(p) + f(q). Why? Suppose that I have picked two numbers, independently from
    each other, and now you have to guess those two. You can do this sequentially,
    applying the optimal strategy to the first one, then the second one.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，作为概率的函数，信息是可加的：f(pq) = f(p) + f(q)。为什么？假设我独立地选择了两个数字，现在你需要猜测这两个数字。你可以顺序进行，首先应用最优策略猜测第一个，然后猜测第二个。
- en: In mathematical terms, f(p) is
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 用数学语言来说，f(p)是
- en: continuous,
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续的，
- en: strictly increasing, that is, f(p)/span>f(q) for any p/span>q,
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 严格递增，即对于任何p > q，有f(p) > f(q)，
- en: and additive, that is, f(pq) = f(p) + f(q) for any p,q.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 且可加性，即对于任何p, q，有f(pq) = f(p) + f(q)。
- en: I’ll spare you the mathematical details, but with a bit of calculus magic, we
    can confidently conclude that the only option is f(p) = −log [a]p, where a/span>1\.
    Seemingly, information depends on the base, but as
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我会省略数学细节，但通过一点微积分魔法，我们可以自信地得出唯一的选项是f(p) = −log [a]p，其中a > 1。表面上看，信息似乎依赖于基数，但实际上
- en: '![log x = logax-, b logab ](img/file2005.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![log x = logax-, b logab ](img/file2005.png)'
- en: the choice of base only influences the information and entropy up to a multiplicative
    scaling factor. Thus, using the natural logarithm is the simplest choice.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 基数的选择仅会影响信息和熵的乘法尺度因子。因此，使用自然对数是最简单的选择。
- en: So, here is the formal definition at last.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这里终于给出了正式定义。
- en: Definition 96\. (Information)
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 96.（信息）
- en: Let X be a discrete random variable with probability mass function {P(X = x[k])}[k].
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 设X为具有概率质量函数{P(X = x[k])}[k]的离散随机变量。
- en: The information of the event X = x[k] is defined by
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 事件X = x[k]的信息定义为
- en: '![I(xk) := − logP (X = xk ) = logP (X = xk )− 1\. ](img/file2006.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![I(xk) := − logP (X = xk ) = logP (X = xk )− 1\. ](img/file2006.png)'
- en: (Note that whenever the base of log is not indicated, we are using the natural
    base e.) To emphasize the dependency of the information on X, we’ll sometimes
    explicitly denote the connection by I[X](x[k]).
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: （注意，当没有指定对数的基数时，我们使用自然对数e。）为了强调信息依赖于X，我们有时会明确表示其关系为I[X](x[k])。
- en: Armed with the notion of information, we are ready to define entropy, the average
    amount of information per observation. This quantity is named after Claude Shannon,
    who essentially founded information theory in his epic paper “A Mathematical Theory
    of Communication.”
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借信息的概念，我们准备好定义熵，即每次观察的平均信息量。这个量以Claude Shannon的名字命名，他在其史诗论文《通信的数学理论》中奠定了信息理论的基础。
- en: Definition 97\. (Shannon entropy)
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 97.（香农熵）
- en: Let X be a discrete random variable with probability mass function {P(X = x[k])}[k].
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 设X为具有概率质量函数{P(X = x[k])}[k]的离散随机变量。
- en: The entropy of X is defined by
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: X的熵定义为
- en: '![H[X ] := 𝔼[I(X )] ∑∞ = − P(X = xk)logP (X = xk). k=1 ](img/file2007.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![H[X ] := 𝔼[I(X )] ∑∞ = − P(X = xk)logP (X = xk). k=1 ](img/file2007.png)'
- en: Even though H[X] is called the Shannon entropy, we’ll just simply refer to it
    as entropy, unless an explicit distinction is needed.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管H[X]被称为香农熵，但我们将简单地称之为熵，除非需要明确区分。
- en: One of the first things we can notice is that H[X] ≥ 0\. This is shown in the
    following proposition.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以注意到的第一件事是H[X] ≥ 0。这一点在下面的命题中有所体现。
- en: Proposition 7\. (The nonnegativity of entropy)
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 命题 7.（熵的非负性）
- en: Let X be an arbitrary discrete random variable. Then H[X] ≥ 0.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 设X为任意离散随机变量。那么H[X] ≥ 0。
- en: Proof. By definition,
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。根据定义，
- en: '![ ∑ H [X ] = P(X = xk)logP (X = xk)−1\. k ](img/file2008.png)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑ H [X ] = P(X = xk)logP (X = xk)−1\. k ](img/file2008.png)'
- en: 'First, suppose that P(X = x[k])≠0 for all k. Then, as 0 P(X = x[k]) ≤ 1, the
    information is nonnegative: log P(X = x[k])^(−1) ≥ 0\. Hence, as all terms in
    the defining sum are nonnegative, H[X] is nonnegative as well.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，假设对于所有的k，P(X = x[k])≠0。那么，由于0 ≤ P(X = x[k]) ≤ 1，信息是非负的：log P(X = x[k])^(−1)
    ≥ 0。因此，定义和中的所有项都是非负的，H[X]也是非负的。
- en: If P(X = x[k]) = 0 for some k, then, as lim[x→0+]xlog x = 0, the expression
    0 ⋅ log 0 is taken to be 0\. Thus, H[X] is still nonnegative.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 如果P(X = x[k]) = 0对于某些k，那么，由于lim[x→0+]xlog x = 0，表达式0 ⋅ log 0被视为0。因此，H[X]仍然是非负的。
- en: Computing the entropy in practice is hard, as we have to evaluate sums that
    involve logarithms. However, there are a few special cases that shed some much
    needed light on the concept of entropy. Let’s look at them!
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上计算熵是困难的，因为我们需要评估包含对数的求和。然而，仍然有几个特殊情况为熵的概念提供了有价值的启示。让我们来看一下！
- en: Example 1\. The discrete uniform distribution. (See the definition of the discrete
    uniform distribution in Section [19.2.4](ch031.xhtml#the-uniform-distribution).)
    Let X ∼ Uniform({1,…,n}). Then
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 1\. 离散均匀分布。（见第[19.2.4](ch031.xhtml#the-uniform-distribution)节对离散均匀分布的定义。）设X
    ∼ Uniform({1,…,n})。则
- en: '![ ∑n H [X ] = − -1log 1- k=1n n ∑n = 1logn k=1 n = log n. ](img/file2009.png)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n H [X ] = − -1log 1- k=1n n ∑n = 1logn k=1 n = log n. ](img/file2009.png)'
- en: 'By now, we have an intuitive understanding of entropy as the average amount
    of information per observation. Take a wild guess: how does the entropy of the
    uniform distribution compare amongst all other distributions concentrated on {1,2,…,n}?
    Is it above or below average? Is it perhaps minimal or maximal?'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经对熵有了直观的理解，即每次观测的平均信息量。猜猜看：在所有集中于{1,2,…,n}的分布中，均匀分布的熵如何与其他分布相比？是高于还是低于平均水平？是最小的还是最大的？
- en: We’ll reveal the answer by the end of this chapter, but take a minute to ponder
    this question before moving on to the next example.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章末尾揭示答案，但在继续下一个示例之前，花一分钟思考这个问题。
- en: Example 2\. The single-point distribution. (See the definition of the single-point
    distribution in Section [19.2.5](ch031.xhtml#the-singlepoint-distribution)) Let
    X ∼δ(a). Then
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2\. 单点分布。（见第[19.2.5](ch031.xhtml#the-singlepoint-distribution)节对单点分布的定义）设X
    ∼δ(a)。则
- en: '![H [X ] = − 1 ⋅log 1 = 0\. ](img/file2010.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![H [X ] = − 1 ⋅log 1 = 0\. ](img/file2010.png)'
- en: In other words, as the event X = a is certain, no information is gained upon
    observing X. Now think back to the previous example. As X ∼δ(k) is concentrated
    on {1,2,…,n} for all k = 1,2,…,n, give the previous question one more thought.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，由于事件X = a是确定的，观察X时没有获得任何信息。现在回想一下之前的例子。由于X ∼δ(k)集中于{1,2,…,n}，对于所有k = 1,2,…,n，再思考一下之前的问题。
- en: Let’s see a partial answer in the next example.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一个示例中看到部分答案。
- en: Example 3\. The Bernoulli distribution. (See the definition of the Bernoulli
    distribution in Section [19.2.1](ch031.xhtml#the-bernoulli-distribution)). Let
    X ∼ Bernoulli(p). Then, it is easy to see that
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 3\. 伯努利分布。（见第[19.2.1](ch031.xhtml#the-bernoulli-distribution)节对伯努利分布的定义）。设X
    ∼ Bernoulli(p)。则，很容易看出
- en: '![H [X ] = − plogp − (1− p)log(1− p). ](img/file2011.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![H [X ] = − plogp − (1− p)log(1− p). ](img/file2011.png)'
- en: Which value of p maximizes the entropy? To find the maxima of H[X], we can turn
    to the derivatives. (Recall how the derivative and second derivative can be used
    for optimization, as claimed by Theorem [87](ch021.xhtml#x1-214004r87).)
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个p值最大化熵？为了找到H[X]的极大值，我们可以求导。（回想一下，导数和二阶导数如何用于优化，正如定理[87](ch021.xhtml#x1-214004r87)所述。）
- en: Thus, let f(p) = H[X] = −plog p − (1 −p)log(1 −p). Then,
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，设f(p) = H[X] = −plog p − (1 −p)log(1 −p)。则，
- en: '![f′(p) = − logp + log(1− p) = log 1-−-p, p f′′(p) = − 1-−--1--. p 1 − p ](img/file2012.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![f′(p) = − logp + log(1− p) = log 1-−-p, p f′′(p) = − 1-−--1--. p 1 − p ](img/file2012.png)'
- en: By solving f^′(p) = 0, we obtain that p = 1∕2, which is the only potential extrema
    of f(p). As f^(′′)(1∕2) = −4/span>0, we see that p = 1∕2 is indeed a local maximum.
    Let’s plot f(p) to obtain a visual confirmation as well.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解f^′(p) = 0，我们得到p = 1∕2，这是f(p)的唯一潜在极值点。由于f^(′′)(1∕2) = −4/span>0，我们可以看到p =
    1∕2确实是一个局部最大值。让我们绘制f(p)图像，以获得直观的确认。
- en: '[PRE8]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![PIC](img/file2013.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file2013.png)'
- en: 'Figure 20.10: The entropy of the Bernoulli distribution'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.10：伯努利分布的熵
- en: For p = 1∕2, that is, where the entropy of Bernoulli(p) is maximal, we have
    a uniform distribution on the two-element set {0,1}. On the other hand, for p
    = 0 or p = 1, where the entropy is minimal, Bernoulli(p) is a single-point distribution.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 对于p = 1∕2，即当伯努利分布的熵达到最大值时，我们得到一个在二元集合{0,1}上的均匀分布。另一方面，当p = 0或p = 1时，熵最小，伯努利分布则为单点分布。
- en: 'As every random variable on {0,1} is Bernoulli-distributed, we seem to have
    a partial answer to our question: the uniform distribution maximizes entropy,
    while single-point ones minimize it.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 {0,1} 上的每个随机变量都是伯努利分布的，我们似乎已经有了部分答案：均匀分布最大化熵，而单点分布最小化熵。
- en: As the following theorem indicates, this is true in general as well.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 如下定理所示，这在一般情况下也是成立的。
- en: Theorem 138\. (The uniform distribution and maximal entropy)
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 138．（均匀分布与最大熵）
- en: 'Let E = {x[1],…,x[n]}be a finite set, and let X : Ω →E be a random variable
    that assumes values in E. Then,'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '设 E = {x[1],…,x[n]} 是一个有限集，且设 X : Ω →E 为一个取值于 E 的随机变量。那么，'
- en: '![H [X] ≤ H [Uniform (E)], ](img/file2014.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![H [X] ≤ H [Uniform (E)], ](img/file2014.png)'
- en: and H[X] = H[Uniform(E)] if and only if X is uniformly distributed on E.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 并且 H[X] = H[Uniform(E)] 当且仅当 X 在 E 上是均匀分布的。
- en: We are not going to show this here, but there are several proofs out there.
    For instance, Bishop’s classic Pattern Recognition and Machine Learning uses the
    Lagrange multiplier method to explicitly find the maximum of the multivariable
    function f(p[1],…,p[n]) = −∑ [k=1]^np[k] log p[k]; feel free to check it out for
    the details.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会展示这个，但有许多证明可以参考。例如，Bishop 的经典著作《模式识别与机器学习》使用拉格朗日乘数法来显式地找到多变量函数 f(p[1],…,p[n])
    = −∑ [k=1]^np[k] log p[k] 的最大值；有兴趣的话可以查看其中的详细内容。
- en: What if we don’t restrict our discrete random variable to a finite set? In that
    case, the Shannon entropy has no upper limit. In the problem set of this chapter,
    you’ll see that the entropy of the geometric distribution is
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不将离散型随机变量限制在有限集上，怎么办？在这种情况下，Shannon 熵没有上限。在本章的问题集中，你将看到几何分布的熵是
- en: '![H [Geo (p)] = − plogp-+-(1−-p)-log(1-−-p). p ](img/file2015.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![H [Geo (p)] = − plogp-+-(1−-p)-log(1-−-p). p ](img/file2015.png)'
- en: It is easy to see that lim[p→0]H[Geo(p)] = ∞. Let’s plot this!
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出 lim[p→0]H[Geo(p)] = ∞。我们来绘制一下图像！
- en: '[PRE9]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![PIC](img/file2016.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file2016.png)'
- en: 'Figure 20.11: The entropy of the geometric distribution'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.11：几何分布的熵
- en: Thus, the Shannon entropy can assume any nonnegative value.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Shannon 熵可以取任何非负值。
- en: 20.6.4 Differential entropy
  id: totrans-468
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.6.4 微分熵
- en: So far, we have only defined the entropy for discrete random variables.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只为离散型随机变量定义了熵。
- en: Does it translate to continuous ones as well? Yes. The formula 𝔼[ − log f[X](X)]
    can be directly applied for continuous random variables, yielding the so-called
    differential entropy. Here is the formal definition.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否也适用于连续型随机变量？是的。公式 𝔼[ − log f[X](X)] 可以直接应用于连续型随机变量，得到所谓的微分熵。这里是其正式定义。
- en: Definition 98\. (Differential entropy)
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 98．（微分熵）
- en: Let X be a continuous random variable. The differential entropy of X is defined
    by the formula
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 设 X 为一个连续型随机变量。X 的微分熵通过以下公式定义：
- en: '![ ∫ ∞ H [X ] := − f (x )log f (x)dx, −∞ X X ](img/file2017.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ ∞ H [X ] := − f (x )log f (x)dx, −∞ X X ](img/file2017.png)'
- en: where f[X] denotes the probability density function of X.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 f[X] 表示 X 的概率密度函数。
- en: 'Now comes the surprise. Can we derive the formula from the Shannon entropy?
    We are going to approach the problem like we did when we defined the expected
    value for continuous random variables in Section [20.2](ch032.xhtml#continuous-random-variables):
    approximate the continuous random variable with a discrete one, then see where
    the Shannon entropy converges.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来了一个惊喜。我们能从 Shannon 熵推导出这个公式吗？我们将像在定义连续型随机变量的期望值时那样处理这个问题：用离散型随机变量来逼近连续型随机变量，然后看看
    Shannon 熵收敛到哪里。
- en: 'Thus, let X : Ω →ℝ be a continuous random variable, and let [a,b] ⊆ℝ be a (large)
    interval, so large that P(X![∈∕](img/file2018.png)[a,b]) is extremely small. We’ll
    subdivide [a,b] into n equal parts by'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，设 X : Ω →ℝ 为连续型随机变量，设 [a,b] ⊆ℝ 为一个（较大的）区间，使得 P(X![∈∕](img/file2018.png)[a,b])
    极小。我们将 [a,b] 分割为 n 等份，方法如下：'
- en: '![ k(b− a) xk = a+ -------, k = 0,1,...,n, n ](img/file2019.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![ k(b− a) xk = a+ -------, k = 0,1,...,n, n ](img/file2019.png)'
- en: and define the approximating random variable X^((n)) by
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 并通过以下方式定义逼近的随机变量 X^((n))：
- en: '![ ( (n) |{ xk if x ∈ (xk −1,xk] for some k = 1,2,...,n, X (ω ) := | ( 0 otherwise.
    ](img/file2020.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![ ( (n) |{ xk if x ∈ (xk −1,xk] for some k = 1,2,...,n, X (ω ) := | ( 0 otherwise.
    ](img/file2020.png)'
- en: This way, the entropy of X^((n)) is given by
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，X^((n)) 的熵由以下公式给出：
- en: '![ (n) ∑n (n) (n) H [X ] = − P (X = xk)logP (X = xk). k=1 ](img/file2021.png)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![ (n) ∑n (n) (n) H [X ] = − P (X = xk)logP (X = xk). k=1 ](img/file2021.png)'
- en: However, due to how we defined X^((n)),
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们定义了 X^((n)),
- en: '![ ∫ (n) xk P(X = xk) = P(xk− 1 <X ≤ Xk ) = x fX (x)dx, k−1 ](img/file2022.png)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ (n) xk P(X = xk) = P(xk− 1 <X ≤ Xk ) = x fX (x)dx, k−1 ](img/file2022.png)'
- en: where f[X] is the density function of X. Now, the mean value theorem for definite
    integrals (Theorem [93](ch022.xhtml#x1-235008r93)) gives that there is a ξ[k]
    ∈ [x[k−1],x[k]] such that
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 其中f[X]是X的密度函数。现在，定积分的均值定理（定理[93](ch022.xhtml#x1-235008r93)）表明，存在ξ[k] ∈ [x[k−1],x[k]]，使得
- en: '![∫ xk f (ξ ) fX (x)dx = (xk − xk− 1)fX (ξk) = -X---k-, xk−1 n ](img/file2023.png)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
  zh: '![∫ xk f (ξ ) fX (x)dx = (xk − xk− 1)fX (ξk) = -X---k-, xk−1 n ](img/file2023.png)'
- en: thus, in conclusion,
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结来说，
- en: '![ (n) fX-(ξk) P (X = xk ) = n . ](img/file2024.png)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
  zh: '![ (n) fX-(ξk) P (X = xk ) = n . ](img/file2024.png)'
- en: (Recall that as the partition x[0]/span>x[1]/span>…/span>x[n] is equidistant,
    x[k] −x[k−1] = 1∕n.)
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: （回想一下，x[0]/span>x[1]/span>…/span>x[n]的划分是等距的，x[k] −x[k−1] = 1∕n。）
- en: Now, using P(X^((n)) = x[k]) = ![fX(ξk) n](img/file2025.png), we obtain
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用P(X^((n)) = x[k]) = ![fX(ξk) n](img/file2025.png)，我们可以得到
- en: '![ ∑n H[X (n)] = − P (X (n) = xk )log P(X (n) = xk) k=1 n = − ∑ fX-(ξk)-log
    fX-(ξk)- n n k=n1 n = − ∑ fX-(ξk)-log f (ξ )+ log n∑ fX(ξk). n X k n k=1 k=1 ](img/file2026.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n H[X (n)] = − P (X (n) = xk )log P(X (n) = xk) k=1 n = − ∑ fX-(ξk)-log
    fX-(ξk)- n n k=n1 n = − ∑ fX-(ξk)-log f (ξ )+ log n∑ fX(ξk). n X k n k=1 k=1 ](img/file2026.png)'
- en: Both of these terms are Riemann-sums, approximating the integral of the functions
    inside. If n is large, and the interval [a,b] is big enough, then
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个项都是黎曼和，近似函数内部的积分。如果n很大，并且区间[a,b]足够大，那么
- en: '![ n ∫ − ∑ fX-(ξk)logf (ξ ) ≈ − ∞ f (x)logf (x)dx = h[X ], n X k −∞ X X k=1
    ](img/file2027.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![ n ∫ − ∑ fX-(ξk)logf (ξ ) ≈ − ∞ f (x)logf (x)dx = h[X ], n X k −∞ X X k=1
    ](img/file2027.png)'
- en: and
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 并且
- en: '![ n ∫ ∞ ∑ fX-(ξk)-≈ f (x)dx = 1, n − ∞ X k=1 ](img/file2028.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![ n ∫ ∞ ∑ fX-(ξk)-≈ f (x)dx = 1, n − ∞ X k=1 ](img/file2028.png)'
- en: implying
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着
- en: '![H [X (n)] ≈ h [X ]+ logn. ](img/file2029.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![H [X (n)] ≈ h [X ]+ logn. ](img/file2029.png)'
- en: This is quite surprising, as one would expect H[X^((n))] to converge towards
    h(X). This is not the case. In fact,
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 这很令人惊讶，因为人们通常会期望H[X^((n))]会收敛到h(X)。但事实并非如此。
- en: '![ lim (H [X (n)]− log n) = h[X ] n→ ∞ ](img/file2030.png)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![ lim (H [X (n)]− log n) = h[X ] n→ ∞ ](img/file2030.png)'
- en: holds.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 保持。
- en: It’s time for the examples.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是示例时间。
- en: Example 1\. The uniform distribution. (See the definition of the uniform distribution
    in Section [19.3.4](ch031.xhtml#the-uniform-distribution2).) Let X ∼ Uniform(a,b).
    Then,
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 示例1. 均匀分布。（见[19.3.4](ch031.xhtml#the-uniform-distribution2)节中的均匀分布定义。）设X∼Uniform(a,b)。那么，
- en: '![ ∫ b -1--- --1-- h[X ] = − b− a log b− a dx a = log(b− a), ](img/file2031.png)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ b -1--- --1-- h[X ] = − b− a log b− a dx a = log(b− a), ](img/file2031.png)'
- en: 'which is similar to the discrete uniform case. However, there is one notable
    difference: h(X) is negative when b−a/span>1\. This is in stark contrast with
    the Shannon entropy, which is always nonnegative.'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 这与离散均匀分布的情况相似。然而，有一个显著的区别：当b−a/span>1时，h(X)是负的。这与香农熵形成鲜明对比，后者始终是非负的。
- en: Example 2\. The normal distribution. (See the definition of the normal distribution
    in Section [19.3.6](ch031.xhtml#the-normal-distribution).) Let X ∼𝒩(μ,σ²). Then,
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 示例2. 正态分布。（见[19.3.6](ch031.xhtml#the-normal-distribution)节中的正态分布定义。）设X∼𝒩(μ,σ²)。那么，
- en: '![ ∫ ∞ 1 (x−μ)2 ( 1 (x−μ)2) h[X ] = − -√---e− 2σ2 log -√----e− 2σ2 dx − ∞ σ
    2π∫ σ 2π ∫ ( √ ---) ∞ --1---− (x−2σμ2)2- ∞ (x−-μ-)2---1---− (x−2μσ)22 = log σ
    2π −∞ σ√ 2πe dx+ − ∞ 2σ2 σ√ 2πe dx ◟--------◝◜--------◞ ◟------------◝◜------------◞
    =1 = 21σ2Var[X ]= 12 1 ( 2 ) = 2- 1+ log(σ 2π ) . ](img/file2032.png)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ ∞ 1 (x−μ)2 ( 1 (x−μ)2) h[X ] = − -√---e− 2σ2 log -√----e− 2σ2 dx − ∞ σ
    2π∫ σ 2π ∫ ( √ ---) ∞ --1---− (x−2σμ2)2- ∞ (x−-μ-)2---1---− (x−2μσ)22 = log σ
    2π −∞ σ√ 2πe dx+ − ∞ 2σ2 σ√ 2πe dx ◟--------◝◜--------◞ ◟------------◝◜------------◞
    =1 = 21σ2Var[X ]= 12 1 ( 2 ) = 2- 1+ log(σ 2π ) . ](img/file2032.png)'
- en: Depending on the value of σ, the value of h[X] can be negative here as well.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 根据σ的值，这里h[X]的值也可能是负的。
- en: Previously, we have seen that for discrete distributions on a given finite set,
    the uniform distribution maximizes entropy, as Theorem [138](ch032.xhtml#x1-343023r138)
    claims.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们已经看到，对于给定有限集合上的离散分布，均匀分布最大化熵，正如定理[138](ch032.xhtml#x1-343023r138)所述。
- en: What is the analogue of Theorem [138](ch032.xhtml#x1-343023r138) for continuous
    distributions? Take a wild guess. If we let X be any continuous distribution,
    then, as we have seen,
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 连续分布的定理[138](ch032.xhtml#x1-343023r138)的类似物是什么？大胆猜一下。如果我们设X为任何连续分布，那么，正如我们已经看到的，
- en: '![h[Uniform (a,b)] = log(b− a), ](img/file2033.png)'
  id: totrans-509
  prefs: []
  type: TYPE_IMG
  zh: '![h[Uniform (a,b)] = log(b− a), ](img/file2033.png)'
- en: which can assume any real number. Similarly to the discrete case, we have to
    make restrictions; this time, we’ll fix the variance. Here is the result.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 其值可以是任意实数。与离散情形类似，我们必须做一些限制；这次，我们将固定方差。结果如下。
- en: Theorem 139\. (Maximizing the differential entropy)
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 定理139.（最大化微分熵）
- en: Let X be a continuous random variable with variance σ². Then
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 设X是一个方差为σ²的连续随机变量。那么
- en: '![h[X ] ≤ h [𝒩 (0,σ2)], ](img/file2034.png)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![h[X ] ≤ h [𝒩 (0,σ2)], ](img/file2034.png)'
- en: and h[X] = h[𝒩(0,σ²)] if and only if X ∼𝒩(μ,σ²).
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 并且h[X] = h[𝒩(0,σ²)] 当且仅当 X ∼𝒩(μ,σ²)。
- en: Again, we are not going to prove this. You can check Bishop’s Pattern Recognition
    and Machine Learning for more details.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不打算证明这个。你可以参考比ISHOP的《模式识别与机器学习》获取更多细节。
- en: 20.7 The Maximum Likelihood Estimation
  id: totrans-516
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.7 最大似然估计
- en: I am an evangelist for simple ideas. Stop me any time you want, but whichever
    field I was in, I’ve always been able to find a small set of mind-numbingly simple
    ideas making the entire shebang work. (Not that you could interrupt me, as this
    is a book. Joke’s on you!)
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 我是简单想法的传播者。随时可以打断我，但无论我在哪个领域，我总是能找到一小套极其简单的想法，使整个工作得以运转。（虽然你不可能打断我，因为这是一本书。笑话是对你的！）
- en: 'Let me give you a concrete example that’s on my mind. What do you think enabled
    the rise of deep learning, including neural networks with billions of parameters?
    Three ideas as simple as ABC:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 让我给你举个我脑海中的具体例子。你认为是什么推动了深度学习的崛起，包括具有数十亿参数的神经网络？有三个像ABC一样简单的想法：
- en: that you can optimize the loss function by going against its gradient (no matter
    the number of parameters),
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过反向梯度优化损失函数（无论参数的数量是多少），
- en: that you can efficiently compute the gradient with a clever application of the
    chain rule and matrix multiplication,
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过巧妙应用链式法则和矩阵乘法来高效地计算梯度，
- en: and that we can perform matrix operations blazingly fast on a GPU.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并且我们可以在GPU上以极快的速度进行矩阵运算。
- en: Sure, there’s a great tower of work built upon these ideas, but these three
    lie at the very foundation of machine learning today. Ultimately, these enable
    you to converse with large language models. To have your car cruise around town
    while you read the newspaper. To predict the exact shape of massive amino-acid
    chains called proteins, responsible for building up every living thing. (Including
    you.)
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在这些想法的基础上建立了一个庞大的工作体系，但这三点构成了今天机器学习的基础。最终，这些使你能够与大型语言模型对话。让你的车在城市中自动巡航，同时你阅读报纸。预测巨大的氨基酸链的准确形状，这些氨基酸链构成了每一个生物体。（包括你。）
- en: Gradient descent, backpropagation, and high-performance linear algebra are on
    the practical side of the metaphorical machine learning coin. If we conjure up
    a parametric model, we can throw some extremely powerful tools at it.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降、反向传播和高效的线性代数属于隐喻中机器学习硬币的实用面。如果我们构建一个参数化模型，可以投入一些极其强大的工具来进行处理。
- en: But where do our models come from?
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们的模型从哪里来？
- en: 'As I’ve said, there is a small set of key ideas that go a long way. We are
    about to meet one: the maximum likelihood estimation.'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所说，存在一小套关键的想法，它们起到了至关重要的作用。我们即将遇到其中之一：最大似然估计。
- en: 20.7.1 Probabilistic modeling 101
  id: totrans-526
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.7.1 概率建模入门
- en: As a self-proclaimed evangelist of simple ideas, I’ll start with a simple example
    to illustrate a simple idea.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个自封为简单思想的传播者，我将从一个简单的例子开始，来说明一个简单的想法。
- en: 'Pick up a coin and toss it a few times, recording each outcome. The question
    is, once more, simple: what’s the probability of heads? We can’t just immediately
    assume p = 1∕2, that is, a fair coin. For instance, one side of our coin could
    be coated with lead, resulting in a bias. To find out, let’s perform some statistics.
    (Rolling up my sleeves, throwing down my gloves.)'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 拿起一枚硬币，投掷几次并记录每次的结果。问题再次简单：正面的概率是多少？我们不能立即假设 p = 1∕2，也就是说，硬币是公平的。例如，硬币的一面可能被涂上铅，从而产生偏差。为了弄清楚这一点，我们来做一些统计。（卷起袖子，丢下手套。）
- en: 'Mathematically speaking, we can model coin tosses with the Bernoulli distribution
    (Section [19.2.1](ch031.xhtml#the-bernoulli-distribution)):'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，我们可以用伯努利分布来建模硬币投掷（第[19.2.1节](ch031.xhtml#the-bernoulli-distribution)）：
- en: '![P(X = 1) = p, P(X = 0) = 1 − p, ](img/file2035.png)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
  zh: '![P(X = 1) = p, P(X = 0) = 1 − p, ](img/file2035.png)'
- en: where
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: X is the random variable representing the outcome of a single toss,
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X 是表示单次投掷结果的随机变量，
- en: X = 1 for heads and X = 0 for tails,
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X = 1 表示正面，X = 0 表示反面，
- en: and p ∈ [0,1] is the probability of heads.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并且 p ∈ [0,1] 是正面的概率。
- en: That’s just the model. We’re here to estimate the parameter p, and this is what
    we have statistics for.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是模型。我们的目标是估计参数 p，这正是我们统计学可以处理的。
- en: Tossing up the coin n times yields the zero-one sequence x[1],x[2],…,x[n], where
    each x[i] is a realization of a Bernoulli-distributed random variable X[i] ∼ Bernoulli(p),
    independent of each other.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 将硬币投掷 n 次得到零一序列 x[1], x[2], …, x[n]，其中每个 x[i] 都是伯努利分布的随机变量 X[i] ∼ 伯努利(p) 的一个实现，且彼此独立。
- en: As we saw previously when discussing the law of large numbers (Theorem [137](ch032.xhtml#x1-339002r137)),
    one natural idea is to compute the sample mean to estimate p, which is coincidentally
    the expected value of X. To move beyond empirical estimates, let’s leverage that,
    this time, we have a probabilistic model.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在讨论大数法则时所看到的（定理 [137](ch032.xhtml#x1-339002r137)），一个自然的想法是通过计算样本均值来估计p，而这恰好是X的期望值。为了超越经验估计，我们可以利用这个机会，因为我们现在有一个概率模型。
- en: 'The key question is this: which parameter p is the most likely to produce our
    sample?'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 关键问题是：哪个参数p最有可能生成我们的样本？
- en: In the language of probability, this question is answered by maximizing the
    likelihood function
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率的语言中，这个问题通过最大化似然函数来回答
- en: '![ n ∏ LLH (p;x1,...,xn) = P(Xi = xi | p), i=1 ](img/file2036.png)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
  zh: '![ n ∏ LLH (p;x1,...,xn) = P(Xi = xi | p), i=1 ](img/file2036.png)'
- en: where P(X[i] = x[i]∣p) represents the probability of observing x[i] given a
    fixed parameter p. The larger the LLH(p;x[1],…,x[n]), the more likely the parameter
    p is. In other words, our estimate of p is going to be
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 其中P(X[i] = x[i]∣p)表示在固定参数p下观察到x[i]的概率。LLH(p;x[1],…,x[n])越大，参数p越有可能。换句话说，我们对p的估计将是：
- en: '![ˆp = argmaxp ∈[0,1]LLH (p;x1,...,xn). ](img/file2037.png)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![ˆp = argmaxp ∈[0,1]LLH (p;x1,...,xn). ](img/file2037.png)'
- en: Let’s find it.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来求解它。
- en: In our concrete case, P(X[i] = x[i]∣p) can be written as
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的具体情况下，P(X[i] = x[i]∣p) 可以写成：
- en: '![ (| {p if xi = 1, P(Xi = xi | p) = | (1 − p if xi = 0\. ](img/file2038.png)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![ (| {p if xi = 1, P(Xi = xi | p) = | (1 − p if xi = 0\. ](img/file2038.png)'
- en: Algebra doesn’t welcome if-else type functions, so with a clever mathematical
    trick, we write P(X[i] = x[i]∣p) as
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 代数不喜欢if-else类型的函数，因此，通过一个巧妙的数学技巧，我们将P(X[i] = x[i]∣p)写成：
- en: '![ x 1−x P(Xi = xi | p) = p i(1− p ) i, ](img/file2039.png)'
  id: totrans-547
  prefs: []
  type: TYPE_IMG
  zh: '![ x 1−x P(Xi = xi | p) = p i(1− p ) i, ](img/file2039.png)'
- en: making the likelihood function be
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 使得似然函数为：
- en: '![ ∏n LLH (p;x1,...,xn) = pxi(1 − p)1−xi. i=1 ](img/file2040.png)'
  id: totrans-549
  prefs: []
  type: TYPE_IMG
  zh: '![ ∏n LLH (p;x1,...,xn) = pxi(1 − p)1−xi. i=1 ](img/file2040.png)'
- en: (We’ll often write LLH(p) to minimize notational complexity.)
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: （我们通常会写LLH(p)来简化符号复杂性。）
- en: 'This is still not easy to optimize, as it is composed of the product of exponential
    functions. So, here’s another mathematical trick: take the logarithm to turn the
    product into a sum.'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然不容易优化，因为它是由指数函数的乘积组成的。所以，这里有另一个数学技巧：取对数，将乘积转换为和。
- en: 'As the logarithm is increasing, it won’t change the optima, so we’re good to
    go:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对数是递增的，它不会改变最优解，所以我们可以继续：
- en: '![ ∏n log LLH (p) = log pxi(1− p )1−xi i=1 ∑n [ ] = log pxi(1 − p)1−xi i=1
    ∑n [ ] = logpxi + log(1− p)1−xi i=1 ∑n ∑n = logp xi + log(1 − p) (1− xi). i=1
    i=1 ](img/file2041.png)'
  id: totrans-553
  prefs: []
  type: TYPE_IMG
  zh: '![ ∏n log LLH (p) = log pxi(1− p )1−xi i=1 ∑n [ ] = log pxi(1 − p)1−xi i=1
    ∑n [ ] = logpxi + log(1− p)1−xi i=1 ∑n ∑n = logp xi + log(1 − p) (1− xi). i=1
    i=1 ](img/file2041.png)'
- en: Trust me, this is much better. According to the second derivative test (Theorem [87](ch021.xhtml#x1-214004r87)),
    we can find the maxima by
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 相信我，这样好多了。根据二阶导数检验（定理 [87](ch021.xhtml#x1-214004r87)），我们可以通过：
- en: solving ![ddp](img/file2042.png) log LLH(p) = 0 to find the critical point p̂
    ,
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解 ![ddp](img/file2042.png) log LLH(p) = 0 来找到临界点p̂，
- en: then showing that p̂ is a maximum because ![-d2- dp2](img/file2045.png) log
    LLH(p)/span>0.
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后证明p̂是最大值，因为 ![-d2- dp2](img/file2045.png) log LLH(p)/span>0。
- en: Let’s get to it.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: As ![ddp](img/file2046.png) log p = ![1p](img/file2047.png) and ![ddp-](img/file2048.png)
    log(1 −p) = −![11−p](img/file2049.png), we have
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ![ddp](img/file2046.png) log p = ![1p](img/file2047.png) 和 ![ddp-](img/file2048.png)
    log(1 −p) = −![11−p](img/file2049.png)，我们有：
- en: '![d 1 ∑n 1 ∑n --logLLH (p;x1,...,xn) = -- xi − ----- (1− xi). dp p i=1 1− p
    i=1 ](img/file2050.png)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![d 1 ∑n 1 ∑n --logLLH (p;x1,...,xn) = -- xi − ----- (1− xi). dp p i=1 1− p
    i=1 ](img/file2050.png)'
- en: Solving ![ddp](img/file2051.png) log LLH(p;x[1],…,x[n]) = 0 yields a single
    solution
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 解 ![ddp](img/file2051.png) log LLH(p;x[1],…,x[n]) = 0 得到一个解
- en: '![ 1 ∑n ˆp = -- xi. n i=1 ](img/file2052.png)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 ∑n ˆp = -- xi. n i=1 ](img/file2052.png)'
- en: (Pick up a pen and paper and calculate the solution yourself.) Regarding the
    second derivative, we have
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: （拿起一支笔和纸，自己计算一下答案。）关于二阶导数，我们有：
- en: '![d2 1 ∑n 1 ∑n --2 logLLH (p) = − -2 xi −-------2 (1− xi), dp p i=1 (1 − p)
    i=1 ](img/file2053.png)'
  id: totrans-563
  prefs: []
  type: TYPE_IMG
  zh: '![d2 1 ∑n 1 ∑n --2 logLLH (p) = − -2 xi −-------2 (1− xi), dp p i=1 (1 − p)
    i=1 ](img/file2053.png)'
- en: which is uniformly negative. Thus, p̂ = ![1n](img/file2055.png) ∑ [i=1]^nx[i]
    is indeed a (local) maximum. Yay!
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 它是均匀负的。因此，p̂ = ![1n](img/file2055.png) ∑ [i=1]^nx[i] 确实是一个（局部）最大值。耶！
- en: 'In this case, the maximum likelihood estimate is identical to the sample mean.
    Trust me, this is one of the rare exceptions. Think of it as validating the sample
    mean: we’ve obtained the same estimate through different trains of thought, so
    it must be good.'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最大似然估计与样本均值相同。相信我，这是一个罕见的例外。把它看作是对样本均值的验证：我们通过不同的思路获得了相同的估计值，所以它一定是正确的。
- en: 20.7.2 Modeling heights
  id: totrans-566
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.7.2 建模高度
- en: Let’s continue with another example. The coin-tossing example demonstrated the
    discrete case. It’s time to move into the continuous domain!
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行另一个例子。抛硬币的例子展示了离散情况。现在是进入连续领域的时候了！
- en: This time, we are measuring the heights of a high school class, and we want
    to build a probabilistic model of it. A natural idea is to assume the heights
    to come from a normal distribution X ∼𝒩(μ,σ²). (Check Section [19.3.6](ch031.xhtml#the-normal-distribution)
    for the normal distribution.)
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们正在测量一个高中班级的身高，并希望建立一个概率模型。一个自然的想法是假设身高来自于正态分布 X ∼𝒩(μ,σ²)。 （查看第 [19.3.6](ch031.xhtml#the-normal-distribution)
    节了解正态分布。）
- en: Our job is to estimate the expected value μ and the variance σ². Let’s go, maximum
    likelihood!
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作是估计期望值 μ 和方差 σ²。来吧，最大似然！
- en: 'To make the problem mathematically precise, we have the measurements x[1],…,x[n],
    coming from independent and identically distributed random variables X[i] ∼𝒩(μ,σ²).
    However, there’s a snag: as our random variables are continuous,'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使问题在数学上精确，我们有测量值 x[1],…,x[n]，它们来自独立同分布的随机变量 X[i] ∼𝒩(μ,σ²)。然而，出现了一个问题：因为我们的随机变量是连续的，
- en: '![ ∏n P (X1 = x1,...,Xn = xn | μ, σ2) = P (Xi = xi | μ, σ2) = 0\. i=1 ](img/file2056.png)'
  id: totrans-571
  prefs: []
  type: TYPE_IMG
  zh: '![ ∏n P (X1 = x1,...,Xn = xn | μ, σ2) = P (Xi = xi | μ, σ2) = 0\. i=1 ](img/file2056.png)'
- en: '(As all terms of the product are zero.) How can we define the likelihood function,
    then? No worries: even though we don’t have a mass function, we have density!
    Thus, the likelihood function defined by'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: （因为所有项的乘积为零。）那么我们该如何定义似然函数呢？别担心：虽然我们没有质量函数，但我们有密度！因此，由以下定义似然函数：
- en: '![ ∏n LLH (μ, σ;x1,...,xn) = fX (xi) i=1 i n 2 = ∏ -√1--e− (xi2−σμ2), i=1 σ
    2π ](img/file2057.png)'
  id: totrans-573
  prefs: []
  type: TYPE_IMG
  zh: '![ ∏n LLH (μ, σ;x1,...,xn) = fX (xi) i=1 i n 2 = ∏ -√1--e− (xi2−σμ2), i=1 σ
    2π ](img/file2057.png)'
- en: where f[X[i]](x) is the probability density function of X[i].
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 f[X[i]](x) 是 X[i] 的概率密度函数。
- en: 'Let’s maximize it. The idea is similar: take the logarithm, find the critical
    points, then use the second derivative test. Here we go:'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来最大化它。思路是类似的：取对数，找到临界点，然后使用二阶导数检验。开始吧：
- en: '![ 1 1 ∑n 2 log LLH (μ,σ) = nlog -√----− σ2- (xi − μ) . σ 2π i=1 ](img/file2058.png)'
  id: totrans-576
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 1 ∑n 2 log LLH (μ,σ) = nlog -√----− σ2- (xi − μ) . σ 2π i=1 ](img/file2058.png)'
- en: Just the usual business from now on. The derivatives are given by
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在起，照常进行。导数如下：
- en: '![ ∑n ∂--log LLH (μ,σ) = -2- (xi − μ), ∂μ σ2 i=1 ∑n ∂--log LLH (μ,σ) = − n-−
    -2- (xi − μ)2\. ∂σ σ σ3 i=1 ](img/file2059.png)'
  id: totrans-578
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n ∂--log LLH (μ,σ) = -2- (xi − μ), ∂μ σ2 i=1 ∑n ∂--log LLH (μ,σ) = − n-−
    -2- (xi − μ)2\. ∂σ σ σ3 i=1 ](img/file2059.png)'
- en: With a bit of number-crunching (that you should attempt to carry out by yourself),
    we get that ∂μ log LLH(μ,σ) = 0 implies
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一点数字运算（你应该尝试自己完成），我们得到 ∂μ log LLH(μ,σ) = 0 这意味着
- en: '![ -1∑n μ = n xi, i=1 ](img/file2061.png)'
  id: totrans-580
  prefs: []
  type: TYPE_IMG
  zh: '![ -1∑n μ = n xi, i=1 ](img/file2061.png)'
- en: and ![-∂ ∂σ](img/file2062.png) log LLH(μ,σ) implies
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 和 ![-∂ ∂σ](img/file2062.png) log LLH(μ,σ) 这意味着
- en: '![ ∑n σ = -1 (xi − μ )2\. n i=1 ](img/file2063.png)'
  id: totrans-582
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n σ = -1 (xi − μ )2\. n i=1 ](img/file2063.png)'
- en: 'We won’t do the second derivative test here, but trust me: it’s a maximum,
    leaving us with the estimates'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不做二阶导数检验，但相信我：这是一个最大值，结果给出我们估计值
- en: '![ n ˆμ = -1∑ x, n i=1 i n ˆσ = -1∑ (x − μˆ)2\. n i i=1 ](img/file2064.png)'
  id: totrans-584
  prefs: []
  type: TYPE_IMG
  zh: '![ n ˆμ = -1∑ x, n i=1 i n ˆσ = -1∑ (x − μˆ)2\. n i i=1 ](img/file2064.png)'
- en: 'Again, the sample mean and variance. Think of it this way: defaulting to the
    sample mean and variance is the simplest thing to do, yet even clever methods
    like the maximum likelihood estimation yield them as parameter estimates.'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提到，样本均值和方差。可以这样理解：默认使用样本均值和方差是最简单的方法，但即便是像最大似然估计这样聪明的方法，也会把它们作为参数估计值。
- en: After working out the above two examples in detail, we are ready to abstract
    away the details and introduce the general problem.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细解答上述两个例子后，我们已经准备好抽象掉细节，引入一般问题。
- en: 20.7.3 The general method
  id: totrans-587
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.7.3 一般方法
- en: We’ve seen how maximum likelihood estimation works. Now, it’s time to construct
    the abstract mathematical framework.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过最大似然估计的工作原理。现在，是时候构建抽象的数学框架了。
- en: Definition 99\. (The likelihood function)
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 定义99.（似然函数）
- en: Let P[𝜃] be a probability distribution parametrized by the parameter 𝜃 ∈ℝ^k,
    and let x[1],…,x[n] ∈ ℝ^d be an independent realization of the probability distribution.
    (That is, the samples are coming from independent and identically distributed
    random variables X[1],…,X[n], distributed according to P[𝜃].)
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 设P[𝜃]是由参数𝜃 ∈ℝ^k参数化的概率分布，且x[1],…,x[n] ∈ ℝ^d是该概率分布的独立实现。（即样本来自独立同分布的随机变量X[1],…,X[n]，其分布由P[𝜃]给出。）
- en: The likelihood function of 𝜃 given the sample x[1],…,x[n] is defined by
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 给定样本x[1],…,x[n]，𝜃的似然函数定义为：
- en: (a)
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![ ∏n LLH (𝜃;x1,...,xn) := P 𝜃(Xi = xi) i=1 ](img/file2065.png)'
  id: totrans-593
  prefs: []
  type: TYPE_IMG
  zh: '![ ∏n LLH (𝜃;x1,...,xn) := P 𝜃(Xi = xi) i=1 ](img/file2065.png)'
- en: if P[𝜃] is discrete, and
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 如果P[𝜃]是离散的，并且
- en: (b)
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![ ∏n LLH (𝜃;x1,...,xn) := f𝜃(xi) i=1 ](img/file2066.png)'
  id: totrans-596
  prefs: []
  type: TYPE_IMG
  zh: '![ ∏n LLH (𝜃;x1,...,xn) := f𝜃(xi) i=1 ](img/file2066.png)'
- en: if P[𝜃] is continuous, where f[𝜃] is the probability density function of P[𝜃].
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 如果P[𝜃]是连续的，其中f[𝜃]是P[𝜃]的概率密度函数。
- en: 'We’ve already seen two examples of the likelihood function: for the Bernoulli-distribution
    Bernoulli(p), given by'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了两个似然函数的示例：对于伯努利分布Bernoulli(p)，给定：
- en: '![ ∏n LLH (p;x1,...,xn) = pxi(1 − p)1−xi, i=1 ](img/file2067.png)'
  id: totrans-599
  prefs: []
  type: TYPE_IMG
  zh: '![ ∏n LLH (p;x1,...,xn) = pxi(1 − p)1−xi, i=1 ](img/file2067.png)'
- en: and for the normal distribution 𝒩(μ,σ), given by
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正态分布𝒩(μ,σ)，给定：
- en: '![ n 2 LLH (μ, σ;x ,...,x ) = ∏ -√1--e− (xi2−σμ2). 1 n i=1 σ 2π ](img/file2068.png)'
  id: totrans-601
  prefs: []
  type: TYPE_IMG
  zh: '![ n 2 LLH (μ, σ;x ,...,x ) = ∏ -√1--e− (xi2−σμ2). 1 n i=1 σ 2π ](img/file2068.png)'
- en: Intuitively, the likelihood function LLH(𝜃;x[1],…,x[n]) expresses the probability
    of our observation x[1],…,x[n] if the parameter 𝜃 is indeed true. The maximum
    likelihood estimate is the parameter ![ˆ𝜃](img/file2069.png) that maximizes this
    probability; that is, under which the observation is the most likely.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上讲，似然函数LLH(𝜃;x[1],…,x[n])表示当参数𝜃为真时，我们观察到的x[1],…,x[n]的概率。最大似然估计是通过最大化此概率得到的参数![ˆ𝜃](img/file2069.png)；即在这个参数下，观察结果最可能发生。
- en: Definition 100\. (The maximum likelihood estimate)
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 定义100. （最大似然估计）
- en: Let P[𝜃] be a probability distribution parametrized by the parameter 𝜃 ∈ℝ^k,
    and let x[1],…,x[n] ∈ ℝ^d be an independent realization of the probability distribution.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 设P[𝜃]是由参数𝜃 ∈ℝ^k参数化的概率分布，且x[1],…,x[n] ∈ ℝ^d是该概率分布的独立实现。
- en: The maximum likelihood estimate of 𝜃 is given by
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 𝜃的最大似然估计由以下公式给出：
- en: '![ˆ𝜃 = argmax 𝜃∈ℝkLLH (𝜃;x1,...,xn ). ](img/file2070.png)'
  id: totrans-606
  prefs: []
  type: TYPE_IMG
  zh: '![ˆ𝜃 = argmax 𝜃∈ℝkLLH (𝜃;x1,...,xn ). ](img/file2070.png)'
- en: In both examples, we used the logarithm to turn the product into a sum. Use
    it once and it’s a trick; use it (at least) twice and it’s a method. Here’s the
    formal definition.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个示例中，我们使用了对数运算将乘积转化为和。用一次是技巧；用（至少）两次就是方法。这是正式定义。
- en: Definition 101\. (The log-likelihood function)
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 定义101. （对数似然函数）
- en: Let P[𝜃] be a probability distribution parametrized by the parameter 𝜃 ∈ℝ^k,
    and let x[1],…,x[n] ∈ ℝ^d be an independent realization of the probability distribution.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 设P[𝜃]是由参数𝜃 ∈ℝ^k参数化的概率分布，且x[1],…,x[n] ∈ ℝ^d是该概率分布的独立实现。
- en: The log-likelihood function of 𝜃 given the sample x[1],…,x[n] is defined by
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 给定样本x[1],…,x[n]，𝜃的对数似然函数定义为：
- en: '![logLLH (𝜃;x1,...,xn), ](img/file2071.png)'
  id: totrans-611
  prefs: []
  type: TYPE_IMG
  zh: '![logLLH (𝜃;x1,...,xn), ](img/file2071.png)'
- en: where LLH(𝜃;x[1],…,x[n]) is the likelihood function.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 其中LLH(𝜃;x[1],…,x[n])是似然函数。
- en: In a classical statistical setting, the maximum likelihood estimation is done
    via
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典统计学中，最大似然估计是通过以下方法进行的：
- en: pulling a parametric probabilistic model out from the mathematician’s hat,
  id: totrans-614
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数学家的帽子里抽出一个参数化的概率模型，
- en: massaging the (log-)likelihood function until we obtain an analytically manageable
    form,
  id: totrans-615
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调整（对数）似然函数直到获得一个解析上可处理的形式，
- en: and solving ∇LLH = 0 (or ∇log LLH = 0) to obtain the parameter estimate.
  id: totrans-616
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过求解∇LLH = 0（或∇log LLH = 0）来获得参数估计。
- en: 'Statistics can be extremely powerful under specific circumstances, but let’s
    face it: the above method has quite a few weaknesses. First, constructing a tractable
    probabilistic model is a challenging task, burdened by the experts’ inherent bias.
    (It’s no accident that I indirectly compared the modeling process to pulling a
    rabbit out of a hat.) Moreover, the more complex the model, the more complex the
    likelihood function is. Which, in turn, increases the complexity of our optimization
    problem.'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学在特定情况下可以非常强大，但我们得面对现实：上述方法有不少缺点。首先，构建一个可处理的概率模型是一个具有挑战性的任务，且容易受到专家固有偏见的影响。（我将建模过程间接地比作从帽子里抽出一只兔子，绝非偶然。）此外，模型越复杂，似然函数也越复杂。这反过来增加了我们优化问题的复杂度。
- en: Why did we spend quite a few pages learning this ancient technique, then?
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 那我们为什么要花这么多页来学习这种古老的技巧呢？
- en: Because its idea is fundamental in machine learning, we’ll arrive at (somewhere
    near the) state of the art by breaking down its barriers one by one. Is modeling
    hard? Let’s construct a function with BILLIONS of parameters that’ll do the job.
    Is optimization computationally intensive? Fear not. We have clusters of GPUs
    at our disposal.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个思想在机器学习中至关重要，我们将通过一步步突破它的障碍，最终达到（或接近）最先进的水平。建模难吗？我们来构造一个拥有数十亿参数的函数来完成这个任务。优化计算密集吗？不用担心，我们有集群的GPU可以使用。
- en: 20.7.4 The German tank problem
  id: totrans-620
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.7.4 德国坦克问题
- en: One more example before finishing up, straight from World War II. Imagine you
    are an Allied intelligence officer tasked to estimate the size of a German armored
    division. (That is, to guess the number of tanks.)
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 再举一个例子，来自二战时期。假设你是一个盟军情报官员，负责估算德军装甲师的规模。（也就是，猜测坦克的数量。）
- en: 'There was no satellite imagery back in the day, so there’s only a little to
    go on, except for a tiny piece of information: the serial numbers of the enemy’s
    destroyed tanks. What can we do with these?'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 那时候没有卫星图像可供参考，因此除了一个小小的信息来源——敌方摧毁坦克的序列号，我们几乎没有其他可以依赖的资料。我们能从这些信息中做些什么呢？
- en: Without detailed knowledge of the manufacturing process, we can assume that
    the tanks are labeled sequentially as they roll out from the factory. We also
    don’t know how the tanks are distributed between the battlefields.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有详细了解制造过程的情况下，我们可以假设这些坦克在出厂时按顺序编号。我们也不知道这些坦克是如何在战场之间分配的。
- en: 'These two pieces of knowledge (or lack of knowledge, to be more precise) translate
    to a simple probabilistic model: encountering an enemy tank is the same as drawing
    from the distribution Uniform(N), where N is the total number of tanks. Thus,
    if x[1],…,x[n] are the serial numbers of destroyed tanks, we can use the maximum
    likelihood method to estimate N.'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 这两块知识（或者更精确地说，缺乏知识）转化为一个简单的概率模型：遇到敌方坦克就像从均匀分布Uniform(N)中抽取样本，其中N是坦克的总数。因此，如果x[1],…,x[n]是摧毁坦克的序列号，我们可以使用最大似然法来估计N。
- en: Let’s do it. The likelihood function for the discrete uniform distribution Uniform(N)
    is given by
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。离散均匀分布Uniform(N)的似然函数由以下公式给出：
- en: '![ ∏n LLH (N ) = P (Xi = xi), i=1 ](img/file2072.png)'
  id: totrans-626
  prefs: []
  type: TYPE_IMG
  zh: '![ ∏n LLH (N ) = P (Xi = xi), i=1 ](img/file2072.png)'
- en: 'where the probability P(X[i] = x[i]) has a quite peculiar form:'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，概率P(X[i] = x[i])有一个相当独特的形式：
- en: '![ ( |{ -1 P (X = x ) = N if xi ∈ {1,...,N }, i i |( 0 otherwise. ](img/file2073.png)'
  id: totrans-628
  prefs: []
  type: TYPE_IMG
  zh: '![ ( |{ -1 P (X = x ) = N if xi ∈ {1,...,N }, i i |( 0 otherwise. ](img/file2073.png)'
- en: Keeping in mind that x[1],…,x[n] ≤N (as no observed serial number can be larger
    than the total number of tanks), we have
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，x[1],…,x[n] ≤N（因为没有观察到的序列号会大于坦克的总数），我们有
- en: '![ ( |{ -1- LLH (N ) = Nn if N <max {x1,...,xn}, |( 0 otherwise. ](img/file2074.png)'
  id: totrans-630
  prefs: []
  type: TYPE_IMG
  zh: '![ ( |{ -1- LLH (N ) = Nn if N <max {x1,...,xn}, |( 0 otherwise. ](img/file2074.png)'
- en: 'Ponder on this a minute: the larger the N, the smaller the LLH(N) is. Thus,
    the maximum likelihood estimate is the smallest possible choice'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想：N越大，LLH(N)就越小。因此，最大似然估计是最小的可能选择。
- en: '![Nˆ= max {x1,...,xn }. ](img/file2075.png)'
  id: totrans-632
  prefs: []
  type: TYPE_IMG
  zh: '![Nˆ= max {x1,...,xn }. ](img/file2075.png)'
- en: In other words, our guess about the number of tanks is the largest serial number
    we’ve encountered.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们对坦克数量的猜测就是我们遇到的最大序列号。
- en: What do you think about this estimate? I won’t lie; I am not a big fan. The
    German tank problem highlights the importance of modeling assumptions in statistics.
    The final estimate ![ˆN](img/file2076.png) is the outcome of our choice of Uniform(N).
    Common wisdom in machine learning is “garbage in, garbage out.” It is true for
    modeling as well.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 你怎么看这个估计？我不骗你，我自己并不特别喜欢。德国坦克问题凸显了建模假设在统计学中的重要性。最终的估计![ˆN](img/file2076.png)是我们选择Uniform(N)的结果。机器学习中的常见智慧是“垃圾进，垃圾出”。建模也是如此。
- en: 20.8 Summary
  id: totrans-635
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.8 小结
- en: In this chapter, we have learned about the concept of the expected value. Mathematically
    speaking, the expected value is defined by
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们了解了期望值的概念。从数学上讲，期望值的定义是
- en: '![ ∑ 𝔼[X ] = xkP (X = xk) k ](img/file2077.png)'
  id: totrans-637
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑ 𝔼[X ] = xkP (X = xk) k ](img/file2077.png)'
- en: for discrete random variables and
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离散随机变量和
- en: '![ ∫ ∞ 𝔼[X ] = xfX(x)dx −∞ ](img/file2078.png)'
  id: totrans-639
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ ∞ 𝔼[X ] = xfX(x)dx −∞ ](img/file2078.png)'
- en: 'for continuous ones. Although these formulas involve possibly infinite sums
    and integrals, the underlying meaning is simple: 𝔼[X] represents the average outcome
    of X, weighted by the underlying probability distribution.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续型随机变量。这些公式虽然可能涉及无限和与积分，但其基本含义很简单：𝔼[X] 代表 X 的平均结果，按其底层概率分布加权。
- en: 'According to the law of large numbers, the expected value also describes a
    long-term average: if the independent and identically distributed random variables
    X[1],X[2],… describe the outcomes of a repeated experiment — say, betting a hand
    in poker — then the sample average converges to the joint expected value, that
    is,'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 根据大数法则，期望值也描述了一个长期的平均值：如果独立同分布的随机变量 X[1], X[2], … 描述了一个重复实验的结果——比如说，在扑克中下注——那么样本平均值将收敛到联合期望值，即，
- en: '![ 1 ∑n lim -- Xi = 𝔼[X1 ] n→ ∞ n i=1 ](img/file2079.png)'
  id: totrans-642
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 ∑n lim -- Xi = 𝔼[X1 ] n→ ∞ n i=1 ](img/file2079.png)'
- en: holds with probability 1\. In a sense, the law of large numbers allows you to
    glimpse into the future and see what happens if you make the same choice. In the
    case of poker, if you only make bets with a positive expected value, you’ll win
    in the long run.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 以概率 1 成立。从某种意义上说，大数法则让你能够瞥见未来，看看如果你做出相同的选择会发生什么。在扑克游戏中，如果你只做期望值为正的下注，长期下来你将会赢。
- en: In machine learning, the LLN also plays an essential role. Check out the mean-squared
    error
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，LLN 同样起着至关重要的作用。查看均方误差
- en: '![ -1∑n 2 n MSE (x,y ) = n (f(xi)− yi) , x,y ∈ ℝ i=1 ](img/file2080.png)'
  id: totrans-645
  prefs: []
  type: TYPE_IMG
  zh: '![ -1∑n 2 n MSE (x,y ) = n (f(xi)− yi) , x,y ∈ ℝ i=1 ](img/file2080.png)'
- en: once more. If the number of samples (n) is in the millions, computing the gradient
    of this sum is not feasible. However, the mean-squared error is the sample average
    of the prediction errors; thus, it’s enough to sample a smaller amount. This is
    the core principle behind stochastic gradients, an idea that makes machine learning
    on a large scale feasible.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调。如果样本数量（n）达到百万级，计算这个总和的梯度就变得不可行。然而，均方误差是预测误差的样本平均值；因此，采样较少的样本就足够了。这就是随机梯度的核心原理，它使得大规模机器学习变得可行。
- en: 'With this chapter, our journey comes to a close. Still, there’s so much to
    learn; I could probably write this book until the end of time. Sadly, we have
    to stop somewhere. Now, instead of giving a summary of all that’s in the book,
    let’s talk about the most important message: learning never ends.'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了我们的旅程。然而，还有很多东西可以学习；我可能会一直写这本书，直到时间的尽头。不幸的是，我们必须在某个地方停下。现在，与其总结书中的所有内容，不如谈谈最重要的信息：学习永无止境。
- en: It’s a spiral that you continue to ascend, meeting familiar landscapes from
    higher and higher vantage points. If you keep going, you’ll know what I’m talking
    about.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个螺旋式上升的过程，你不断从更高的视角遇见熟悉的景象。如果你继续前行，你会明白我在说什么。
- en: If you lead an intellectually challenging life, you’ll also find that knowledge
    is like keeping a dozen leaky cups full of water. If your focus shifts from one,
    it’ll empty faster than you think. In other words, you’ll lose it if you don’t
    use it. This is completely normal. The good news is, if you already have a good
    foundation, refilling the cup can be done quickly. Sometimes, simply glancing
    at a page from a book you read long ago can do the trick.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你过着充满智力挑战的生活，你也会发现知识就像是保持一打漏水的杯子满满的水。如果你的注意力从其中一个杯子移开，它会比你想象的更快地空掉。换句话说，如果你不使用它，你会失去它。这是完全正常的。好消息是，如果你已经打下了坚实的基础，重新填充杯子是很快就能做到的。有时，只是快速浏览一下你很久以前读过的一本书的页面就能解决问题。
- en: This is how I know that it’s not goodbye. If you have found the book useful
    and continue down the rabbit hole that is machine learning, we’ll meet again with
    probability one. You just have to keep going.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我知道这不是告别的原因。如果你觉得这本书有用，并继续深入机器学习的世界，我们将以概率 1 再次相遇。你只需要继续前行。
- en: 20.9 Problems
  id: totrans-651
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.9 问题
- en: 'Problem 1\. Let X,Y : Ω →ℝ be two random variables.'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '问题 1\. 设 X,Y : Ω →ℝ 为两个随机变量。'
- en: (a) Show that if X ≥ 0, then 𝔼[X] ≥ 0.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 证明如果 X ≥ 0，那么 𝔼[X] ≥ 0。
- en: (b) Show that if X ≥Y , then 𝔼[X] ≥𝔼[Y ].
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 证明如果 X ≥ Y，那么 𝔼[X] ≥ 𝔼[Y]。
- en: 'Problem 2\. Let X : Ω →ℝ be a random variable. Show that if Var[X] = 0, then
    X assumes only a single value. (That is, the set X(Ω) = {X(ω) : ω ∈ Ω} has only
    a single element.)'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '问题 2\. 设 X : Ω →ℝ 为一个随机变量。证明如果 Var[X] = 0，那么 X 仅取一个值。（即，X(Ω) = {X(ω) : ω ∈
    Ω} 只有一个元素。）'
- en: Problem 3\. Let X ∼ Geo(p) be a geometrically distributed (Section [19.2.3](ch031.xhtml#the-geometric-distribution))
    discrete random variable. Show that
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 3\. 设 X ∼ Geo(p) 为一个几何分布（见[19.2.3节](ch031.xhtml#the-geometric-distribution)）的离散型随机变量。证明：
- en: '![H [X] = − plogp-+-(1−-p)log(1−-p)-. p ](img/file2081.png)'
  id: totrans-657
  prefs: []
  type: TYPE_IMG
  zh: '![H [X] = − plogp-+-(1−-p)log(1−-p)-. p ](img/file2081.png)'
- en: 'Hint: Use that for any q ∈ (0,1), ∑ [k=1]^∞kq^(k−1) = (1 −q)^(−2).'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：对于任意 q ∈ (0,1)，∑ [k=1]^∞ kq^(k−1) = (1 − q)^(−2)。
- en: Problem 4\. Let X ∼ exp(λ) be an exponentially distributed continuous random
    variable. Show that
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 4：设 X ∼ exp(λ) 为一个指数分布的连续随机变量。证明
- en: '![h [X ] = 1 − logλ. ](img/file2082.png)'
  id: totrans-660
  prefs: []
  type: TYPE_IMG
  zh: '![h [X ] = 1 − logλ. ](img/file2082.png)'
- en: Problem 5\. Find the maximum likelihood estimation for the λ parameter of the
    exponential distribution.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 5：求指数分布的 λ 参数的最大似然估计。
- en: Join our community on Discord
  id: totrans-662
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他用户、机器学习专家以及作者本人一起阅读本书。提出问题，为其他读者提供解决方案，通过“问我任何问题”环节与作者聊天，等等。扫描二维码或访问链接加入社区。[https://packt.link/math](https://packt.link/math)
- en: '![PIC](img/file1.png)'
  id: totrans-664
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
