- en: Ensemble Methods for Multi-Class Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类分类的集成方法
- en: Our modern world is already interconnected with many devices for collecting
    data about human behavior - for example, our cell phones are small spies in our
    pockets tracking number of steps, route, or our eating habits. Even the watches
    that we wear now can track everything from the number of steps we take to our
    heart rate at any given moment in time.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现代世界已经与许多收集有关人类行为数据的设备相互连接-例如，我们的手机是我们口袋里的小间谍，跟踪步数、路线或我们的饮食习惯。甚至我们现在戴的手表也可以追踪从我们走的步数到我们在任何给定时刻的心率的一切。
- en: 'In all these situations, the gadgets try to guess what the user is doing based
    on collected data to provide reports of the user''s activities through the day.
    From a machine learning perspective, the task can be viewed as a classification
    problem: detecting patterns in collected data and assigning the right activity
    category to them (that is, swimming, running, sleeping). But importantly, it is
    still supervised problem - that means to train a model, we need to provide observations
    annotated by actual categories.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，这些小工具试图根据收集的数据猜测用户正在做什么，以提供一天中用户活动的报告。从机器学习的角度来看，这个任务可以被视为一个分类问题：在收集的数据中检测模式，并将正确的活动类别分配给它们（即，游泳、跑步、睡觉）。但重要的是，这仍然是一个监督问题-这意味着为了训练模型，我们需要提供由实际类别注释的观察。
- en: In this section, we are going to focus on ensemble methods for modeling problems
    of multi-class classification - sometimes referred to as multinomial classification
    - using a sensor dataset provided by the UCI dataset library.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点关注集成方法来建模多类分类问题，有时也称为多项分类，使用UCI数据集库提供的传感器数据集。
- en: Note that multi-class classification should not be confused with multi-label
    classification whereby multiple labels can be predicted for a given example. For
    example, a blog post can be tagged with multiple labels as one blog can encompass
    any number of topics; however, in multi-class classification, we are *forced* to
    choose one out of *N* possible topics where *N >* 2 possible labels.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，多类分类不应与多标签分类混淆，多标签分类可以为给定示例预测多个标签。例如，一篇博客文章可以被标记为多个标签，因为一篇博客可以涵盖任意数量的主题；然而，在多类分类中，我们*被迫*选择一个*N*个可能主题中的一个，其中*N
    >* 2个可能标签。
- en: 'The reader is going to learn in this chapter about the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 读者将在本章学习以下主题：
- en: Preparing data for multi-class classification, including handling missing values
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为多类分类准备数据，包括处理缺失值
- en: Multi-class classification using the Spark RF algorithm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark RF算法进行多类分类
- en: Evaluating the quality of Spark classification models using different measures
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的指标评估Spark分类模型的质量
- en: Building H2O tree-based classification models and exploring their quality
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建H2O基于树的分类模型并探索其质量
- en: Data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: 'In this chapter, we are going to use **Physical Activity Monitoring Data Set**
    (**PAMAP2**) published in the Machine Learning Repository by the University of
    Irvine: [https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring](https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用由尔湾大学机器学习库发布的**Physical Activity Monitoring Data Set**（**PAMAP2**）：[https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring](https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring)
- en: 'The full dataset contains **52** input features and **3,850,505** events describing
    18 different physical activities (for example, walking, cycling, running, `watching
    TV`). The data was recorded by a heart rate monitor and three inertial measurement
    units located on the wrist, chest, and dominant side''s ankle. Each event is annotated
    by an activity label describing the ground truth and also a timestamp. The dataset
    contains missing values indicated by the value `NaN`. Furthermore, some columns
    produced by sensors are marked as invalid ("orientation" - see dataset description):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的数据集包含**52**个输入特征和**3,850,505**个事件，描述了18种不同的身体活动（例如，步行、骑车、跑步、看电视）。数据是由心率监测器和三个惯性测量单元记录的，分别位于手腕、胸部和主侧踝部。每个事件都由描述地面真相的活动标签和时间戳进行注释。数据集包含由值`NaN`表示的缺失值。此外，一些传感器生成的列被标记为无效（“方向”-请参阅数据集描述）：
- en: '![](img/00058.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00058.jpeg)'
- en: 'Figure 1: Properties of dataset as published in the Machine Learning Repository
    of the University of Irvine.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：由尔湾大学机器学习库发布的数据集属性。
- en: 'The dataset represents the perfect example for activity recognition: we would
    like to train a robust model which would be able to predict a performed activity
    based on incoming data from physical sensors.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集代表了活动识别的完美示例：我们希望训练一个强大的模型，能够根据来自物理传感器的输入数据来预测执行的活动。
- en: Furthermore, the dataset is spread over multiple files, each file representing
    measurements of a single subject, which is another real-life aspect of data produced
    by multiple data sources so we will need to utilize Spark's ability to read from
    a directory and merge the files to make training/test datasets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据集分布在多个文件中，每个文件代表一个单个主体的测量，这是由多个数据源产生的数据的另一个现实方面，因此我们需要利用Spark从目录中读取并合并文件以创建训练/测试数据集的能力。
- en: 'The following lines show a sample of the data. There are a couple of important
    observations that are worth noting:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行显示了数据的一个样本。有几个重要的观察值值得注意：
- en: Individual values are separated by an empty space character
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个别值由空格字符分隔
- en: The first value in each row represents a timestamp, while the second value holds
    the `activityId`
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每行中的第一个值表示时间戳，而第二个值保存了`activityId`
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `activityId` is represented by a numeric value; hence, we need a translation
    table to transform an ID to a corresponding activity label which the dataset gives
    and we show as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`activityId`由数字值表示；因此，我们需要一个翻译表来将ID转换为相应的活动标签，数据集提供了这个翻译表，我们如下所示：'
- en: '| 1 lying | 2 sitting |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 1 躺着 | 2 坐着 |'
- en: '| 3 standing | 4 walking |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 3 站立 | 4 步行 |'
- en: '| 5 running | 6 cycling |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 5 跑步 | 6 骑车 |'
- en: '| 7 Nordic walking | 9 watching TV |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 7挪威步行| 9看电视|'
- en: '| 10 computer work | 11 car driving |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 10电脑工作| 11开车|'
- en: '| 12 ascending stairs | 13 descending stairs |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 12上楼梯| 13下楼梯|'
- en: '| 16 vacuum cleaning | 17 ironing |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 16吸尘| 17熨烫|'
- en: '| 18 folding laundry | 19 house cleaning |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 18叠衣服| 19打扫房子|'
- en: '| 20 playing soccer | 24 rope jumping |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 20踢足球| 24跳绳|'
- en: '| 0 other (transient activities) |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 0其他（瞬态活动）|  |'
- en: The example lines represent one "other activity" and then two measurements representing
    "car driving".
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 示例行代表一个“其他活动”，然后是两个代表“开车”的测量值。
- en: 'The third column contains heart rate measurements, while the rest of the columns
    represent data from three different inertia measurements units: columns 4-20 are
    from the hand sensor, 21-37 contain data from chest sensor and finally the columns
    38-54 hold ankle sensor measurements. Each sensor measures 17 different values
    including temperature, 3-D acceleration, gyroscope and magnetometer data, and
    orientation. However, the orientation columns are marked as invalid in this dataset.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第三列包含心率测量，而其余列代表来自三种不同惯性测量单位的数据：列4-20来自手部传感器，21-37包含来自胸部传感器的数据，最后列38-54包含踝部传感器的测量数据。每个传感器测量17个不同的值，包括温度、3D加速度计、陀螺仪和磁力计数据以及方向。然而，在这个数据集中，方向列被标记为无效。
- en: The input data pack contains two different folders - protocol, and optional
    measurements which contains data from a few subjects who performed some additional
    activities. In this chapter, we are going to use only data from optional folder.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据包含两个不同的文件夹 - 协议和可选测量，其中包含一些执行了一些额外活动的受试者的数据。在本章中，我们将只使用可选文件夹中的数据。
- en: Modeling goal
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模目标
- en: In this example, we would like to build a model based on information about physical
    activities to classify unseen data and annotate it with the corresponding physical
    activity.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们希望基于有关身体活动的信息构建模型，以对未知数据进行分类并用相应的身体活动进行注释。
- en: Challenges
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: 'For the sensor data, there are numerous way to explore and build models. In
    this chapter, we mainly focus on classification; however, there are several aspects
    which would need deeper exploration, especially the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于传感器数据，有许多探索和构建模型的方法。在本章中，我们主要关注分类；然而，有几个方面需要更深入的探索，特别是以下方面：
- en: Training data represents a time-ordered flow of events, but we are not going
    to reflect the time information but look at the data as one complete piece of
    information
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据代表了一系列事件的时间顺序流，但我们不打算反映时间信息，而是将数据视为一整个完整的信息
- en: The same for test data -a single activity event is a part of an event stream
    captured during performing an activity and it can be easier to categorize it with
    knowledge of actual context
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据也是一样 - 单个活动事件是在执行活动期间捕获的事件流的一部分，如果了解实际上下文，可能更容易对其进行分类
- en: Nevertheless, for now, we ignore the time dimension and apply classification
    to explore possible patterns in the sensor data which would characterize performed
    activities.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前，我们忽略时间维度，并应用分类来探索传感器数据中可能存在的模式，这些模式将表征执行的活动。
- en: Machine learning workflow
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工作流程
- en: 'To build an initial model, our workflow includes several steps:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建初始模型，我们的工作流程包括几个步骤：
- en: Data load and preprocessing, often referenced as **extract-transform-load**
    (**ETL**).
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据加载和预处理，通常称为**提取-转换-加载**（**ETL**）。
- en: Load
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载
- en: Parse
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析
- en: Handle missing values
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: Unify data into a form expected by an algorithm.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据统一成算法所期望的形式。
- en: Model training
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练
- en: Model evaluation
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估
- en: Model deployment
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型部署
- en: Starting Spark shell
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动Spark shell
- en: 'The first step is to prepare the Spark environment to perform analysis. As
    in the previous chapter, we are going to start Spark shell; however, in this case,
    the command line is slightly more complicated:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是准备Spark环境进行分析。与上一章一样，我们将启动Spark shell；但是，在这种情况下，命令行稍微复杂一些：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this case, we require more memory since we are going to load larger data.
    We also need to increase the size of PermGen - a part of JVM memory which stores
    information about loaded classes. This is only necessary if you are using Java
    7.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要更多的内存，因为我们将加载更大的数据。我们还需要增加PermGen的大小 - JVM内存的一部分，它存储有关加载的类的信息。只有在使用Java
    7时才需要这样做。
- en: The memory settings for Spark jobs are an important part of job launching. In
    the simple `local[*]`-based scenario as we are using, there is no difference between
    the Spark driver and executor. However, for a larger job deployed on a standalone
    or YARN Spark cluster, the configuration of driver memory and executor memory
    needs to reflect the size of the data and performed transformations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Spark作业的内存设置是作业启动的重要部分。在我们使用的简单的基于`local[*]`的场景中，Spark驱动程序和执行程序之间没有区别。然而，对于部署在独立或YARN
    Spark集群上的较大作业，驱动程序内存和执行程序内存的配置需要反映数据的大小和执行的转换。
- en: Moreover, as we discussed in the previous chapter, you can mitigate memory pressure
    by using a clever caching strategy and the right cache destination (for example,
    disk, off-heap memory).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如我们在上一章中讨论的，您可以通过使用巧妙的缓存策略和正确的缓存目的地（例如磁盘，离堆内存）来减轻内存压力。
- en: Exploring data
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: The first step involves data load. In the case of multiple files, the SparkContext's
    method `wholeTextFiles` provides the functionality we need. It reads each file
    as a single record and returns it as a key-value pair, where the key contains
    the location of the file and the value holds the file content. We can reference
    input files directly via the wildcard pattern `data/subject*`. This is not only
    useful during loading files from a local filesystem but especially important for
    loading files from HDFS as well.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步涉及数据加载。在多个文件的情况下，SparkContext的`wholeTextFiles`方法提供了我们需要的功能。它将每个文件读取为单个记录，并将其作为键值对返回，其中键包含文件的位置，值包含文件内容。我们可以通过通配符模式`data/subject*`直接引用输入文件。这不仅在从本地文件系统加载文件时很有用，而且在从HDFS加载文件时尤为重要。
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Since the names are not part of the input data, we define a variable that is
    going to hold the column names:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于名称不是输入数据的一部分，我们定义一个变量来保存列名：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We simply defined the first three column names, and then column names for each
    of three position sensors. Furthermore, we also prepared a list of column indexes
    which are useless for modeling, including timestamp and orientation data:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单地定义了前三个列名，然后是每个三个位置传感器的列名。此外，我们还准备了一个在建模中无用的列索引列表，包括时间戳和方向数据：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next step is to process the content of the referenced files and create
    an `RDD` which we use as input for data exploration and modeling. Since we are
    expecting to iterate over the data several times and perform different transformations,
    we are going to cache the data in memory:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是处理引用文件的内容并创建一个`RDD`，我们将其用作数据探索和建模的输入。由于我们希望多次迭代数据并执行不同的转换，我们将在内存中缓存数据：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00059.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00059.jpeg)'
- en: In this case, for each key-value pair we extract its content and split it based
    on line boundaries. Then we transform each line based on the file delimiter, which
    is a space between features. Since the files contains only numeric values and
    the string value `NaN` as a marker for missing values, we can simply transform
    all values into Java's `Double`, leaving `Double.NaN` as a representation for
    a missing value.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，对于每个键值对，我们提取其内容并根据行边界进行拆分。然后我们根据文件分隔符对每行进行转换，该分隔符是特征之间的空格。由于文件只包含数值和字符串值`NaN`作为缺失值的标记，我们可以简单地将所有值转换为Java的`Double`，将`Double.NaN`作为缺失值的表示。
- en: We can see our input file has `977,972` rows. During loading, we also skipped
    the timestamp column and columns which were marked as invalid in the dataset description
    (see the `ignoredColumns` array).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的输入文件有977,972行。在加载过程中，我们还跳过了时间戳列和数据集描述中标记为无效的列（参见`ignoredColumns`数组）。
- en: The RDD's interface follows the design principle of functional programming,
    the same principle which is adopted by the Scala programming language. This shared
    concept brings a uniform API for manipulating data structures; on the other hand,
    it is always good to know when an operation is invoked on a local object (array,
    list, sequence) and when it causes a distribution operation (`RDD`).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: RDD的接口遵循函数式编程的设计原则，这个原则也被Scala编程语言采用。这个共享的概念为操作数据结构提供了统一的API；另一方面，了解何时在本地对象（数组、列表、序列）上调用操作，以及何时导致分布操作（`RDD`）是很重要的。
- en: 'To keep our view of the dataset consistent, we also need to filter column names
    based on the list of ignored columns which was prepared in previous steps:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持数据集的一致视图，我们还需要根据在先前步骤中准备的忽略列的列表来过滤列名：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00060.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00060.jpeg)'
- en: It is always good to get rid of data which is useless for modeling. The motivation
    is to mitigate memory pressure during computation and modeling. For example, good
    targets for data removal are columns which contain random IDs, timestamps, constant
    columns, or columns which are already represented in the dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 始终要摆脱对建模无用的数据。动机是在计算和建模过程中减轻内存压力。例如，可以删除包含随机ID、时间戳、常量列或已在数据集中表示的列等的列。
- en: From an intuitive point of view also, modelling ID terms, for example, doesn't
    make a lot of sense given the nature of the field. Feature selection is a hugely
    important topic and one that we will spend a great deal of time on later in the
    book.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从直觉上讲，例如对建模ID术语进行建模并不太有意义，考虑到该领域的性质。特征选择是一个非常重要的话题，我们将在本书的后面花费大量时间来讨论这个话题。
- en: 'Now let''s look at the distribution of the individual activities in our dataset.
    We are going to use the same trick as in the previous chapter; however, we also
    would like to see actual names of activities instead of pure number-based representation.
    Hence, at first we define mapping describing a relation between an activity number
    and its name:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看数据集中个体活动的分布。我们将使用与上一章相同的技巧；但是，我们也希望看到活动的实际名称，而不仅仅是基于数字的表示。因此，首先我们定义了描述活动编号与其名称之间关系的映射：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we compute the number of individual activities in the data with the help
    of the Spark method `reduceByKey`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用Spark方法`reduceByKey`计算数据中个体活动的数量。
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The command computes the count of individual activities, translates the activity
    number to its label, and sorts the result in descending order based on counts:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令计算个体活动的数量，将活动编号转换为其标签，并根据计数按降序对结果进行排序：
- en: '![](img/00061.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00061.jpeg)'
- en: Or visualized based on activity frequencies as shown in *Figure 2*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 或者根据活动频率进行可视化，如*图2*所示。
- en: '![](img/00062.jpeg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00062.jpeg)'
- en: 'Figure 2: Frequencies of different activities in input data.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：输入数据中不同活动的频率。
- en: It is always good to think about the order of the individual transformations
    which are applied on the data. In the preceding example, we applied the `sortBy`
    transformation after collecting all data locally with help of the Spark `collect`
    action. In this context, it makes perfect sense since we know that the result
    of the `collect` action is reasonably small (we have only 22 activity labels)
    and `sortBy` is applied on the local collection. On the other hand, putting `sortBy`
    before the `collect` action would force invocation of Spark RDD's transformation
    and scheduling sort as Spark distributed task.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 始终要考虑对数据应用的个体转换的顺序。在前面的例子中，我们在使用Spark `collect`操作将所有数据收集到本地后应用了`sortBy`转换。在这种情况下，这是有道理的，因为我们知道`collect`操作的结果是相当小的（我们只有22个活动标签），而`sortBy`是应用在本地集合上的。另一方面，在`collect`操作之前放置`sortBy`会强制调用Spark
    RDD的转换，并安排排序作为Spark分布式任务。
- en: Missing data
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失数据
- en: The data description mentions that sensors used for activity tracking were not
    fully reliable and results contain missing data. We need to explore them in more
    detail to see how this fact can influence our modeling strategy.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据描述提到用于活动跟踪的传感器并不完全可靠，结果包含缺失数据。我们需要更详细地探索它们，看看这个事实如何影响我们的建模策略。
- en: 'The first question is how many missing values are in our dataset. We know from
    the data description that all missing values are marked by the string `NaN` (that
    is, not a number), which is now represented as `Double.NaN` in the `RDD` `rawData`.
    In the next code snippet, we compute the number of missing values per row and
    the total number of missing values in the dataset:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题是我们的数据集中有多少缺失值。我们从数据描述中知道，所有缺失值都由字符串`NaN`标记（即，不是一个数字），现在在`RDD` `rawData`中表示为`Double.NaN`。在下一个代码片段中，我们计算每行的缺失值数量和数据集中的总缺失值数量：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00063.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00063.jpeg)'
- en: Right, now we have overall knowledge about the amount of missing values in our
    data. But we do not know how the missing values are distributed. Are they spread
    uniformly over the whole dataset? Or are there rows/columns which contain more
    missing values? In the following text, we will try to find answers to these questions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经对我们的数据中缺失值的数量有了整体的了解。但我们不知道缺失值是如何分布的。它们是均匀分布在整个数据集上吗？还是有包含更多缺失值的行/列？在接下来的文本中，我们将尝试找到这些问题的答案。
- en: 'A common mistake is to compare a numeric value and `Double.NaN` with comparison
    operators. For example, the `if (v == Double.NaN) { ... }` is wrong, since the
    Java specification says:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的错误是使用比较运算符比较数值和`Double.NaN`。例如，`if (v == Double.NaN) { ... }`是错误的，因为Java规范规定：
- en: '"`NaN` is unordered: (1) The numerical comparison operators `<`,  `<=`, `>`,
    and `>=` return `false` if either or both operands are `NaN`, (2) The equality
    operator `==` returns `false` if either operand is `NaN`."'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '"`NaN`是无序的：（1）如果一个或两个操作数是`NaN`，则数值比较运算符`<`、`<=`、`>`和`>=`返回`false`，（2）等式运算符`==`如果任一操作数是`NaN`，则返回`false`。"'
- en: 'Hence, `Double.NaN == Double.NaN` returns always `false`. The right way to
    compare numeric values with `Double.NaN` is to use the method `isNaN`: `if (v.isNaN)
    { ... }` (or use the corresponding static method `java.lang.Double.isNaN`).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`Double.NaN == Double.NaN`总是返回`false`。用正确的方式比较数值和`Double.NaN`是使用`isNaN`方法：`if
    (v.isNaN) { ... }`（或使用相应的静态方法`java.lang.Double.isNaN`）。
- en: 'At first, considering rows we have already computed numbers of missing values
    per row in the previous step. Sorting them and taking the unique values give us
    an understanding of how rows are affected by missing values:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，考虑到我们已经计算了上一步中每行的缺失值数量。对它们进行排序并取唯一值，让我们了解到行是如何受缺失值影响的：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00064.jpeg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00064.jpeg)'
- en: 'Now we can see that the majority of rows contain a single missing value. However,
    there are lot of rows containing `13` or `14` missing values, or even `40` rows
    containing `27` *NaNs* and 107 rows which contain more than 30 missing values
    (`104` rows with `40` missing values, and `3` rows with `39` missing values).
    Considering that the dataset contains 41 columns, it means there are 107 rows
    which are useless (majority of values are missing), leaving 3,386 rows with at
    least two missing values which need attention, and `885,494` rows with a single
    missing value. We can now look at these rows in more detail. We select all rows
    which contain more missing values than a given threshold, for example, `26`. We
    also collect the index of the rows (it is a zero-based index!):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到大多数行包含一个缺失值。然而，有很多行包含13或14个缺失值，甚至有40行包含27个*NaNs*，以及107行包含超过30个缺失值（104行包含40个缺失值，3行包含39个缺失值）。考虑到数据集包含41列，这意味着有107行是无用的（大部分值都缺失），剩下3386行至少有两个缺失值需要关注，以及885,494行有一个缺失值。我们现在可以更详细地查看这些行。我们选择所有包含超过给定阈值的缺失值的行，例如26。我们还收集行的索引（这是基于零的索引！）：
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now we know exactly which rows are not useful. We have already observed that
    there are 107 bad rows which do not contain any useful information. Furthermore,
    we can see that lines which have `27` missing values have them in the places representing
    hand and ankle IMU sensors.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们确切地知道哪些行是没有用的。我们已经观察到有107行是不好的，它们不包含任何有用的信息。此外，我们可以看到有27个缺失值的行是在代表手和脚踝IMU传感器的位置上。
- en: And finally, most of the lines have assigned `activityId` 10, 19, or 20, which
    represents `computer work`, `house cleaning`, and `playing soccer` activities,
    which are classes with top frequencies in dataset. That can lead us to theory
    that the "bad" lines were produced by explicitly rejecting a measurement device
    by subjects. Furthermore, we can also see the index of each wrong row and verify
    them in the input dataset. For now, we are going to leave the bad rows and focus
    on columns.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，大多数行都分配了`activityId` 10、19或20，分别代表`computer work`、`house cleaning`和`playing
    soccer`活动，这些是数据集中频率最高的类别。这可能导致我们的理论是“坏”行是由受试者明确拒绝测量设备而产生的。此外，我们还可以看到每行错误的索引，并在输入数据集中验证它们。现在，我们将留下坏行，专注于列。
- en: 'We can ask the same question about columns - are there any columns which contain
    a higher amount of missing values? Can we remove such columns? We can start by
    collecting the number of missing values per column:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以问同样的问题关于列 - 是否有任何包含更多缺失值的列？我们可以删除这样的列吗？我们可以开始收集每列的缺失值数量：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00065.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00065.jpeg)'
- en: The result shows that the second column (do not forget that we have already
    removed invalid columns during data load), which represents subjects' heart rate,
    contains lot of missing values. More than 90% of values are marked by `NaN`, which
    was probably caused by a measurement process of the experiment (subjects probably
    do not wear the heart rate monitor during usual daily activities but only when
    practicing sport).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，第二列（不要忘记我们在数据加载过程中已经删除了无效列），代表受试者心率的列，包含了大量的缺失值。超过90%的数值被标记为`NaN`，这可能是由实验的测量过程引起的（受试者可能在日常活动中不佩戴心率监测器，只有在进行运动时才佩戴）。
- en: The rest of the columns contain sporadic missing values.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的列包含零星的缺失值。
- en: Another important observation is that the first column containing `activityId`
    does not include any missing values - that is good news and means that all observations
    were properly annotated and we do not need to drop any of them (for example, without
    a training target, we cannot train a model).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的观察是，包含`activityId`的第一列不包含任何缺失值——这是个好消息，意味着所有的观察都被正确注释，我们不需要删除任何观察（例如，没有训练目标，我们就无法训练模型）。
- en: The RDD's `reduce` method represents action. That means it forces evaluation
    of the `RDD` and the result of the reduce is a single value and not `RDD`. Do
    not confuse it with `reduceByKey` which is an `RDD` operation and returns a new
    `RDD` of key-value pairs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: RDD的`reduce`方法代表动作。这意味着它强制评估`RDD`的结果，并且reduce的结果是一个单一的值而不是`RDD`。不要将其与`reduceByKey`混淆，后者是一个`RDD`操作，返回一个新的键值对`RDD`。
- en: The next step is to decide what to do with missing data. There are many strategies
    to adopt; however we need to preserve the meaning of our data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是决定如何处理缺失数据。有许多策略可供选择；然而，我们需要保留数据的含义。
- en: We can simply drop all rows or columns which contain missing data - a very common
    approach as a matter of fact! It makes good sense for rows which are polluted
    by too many missing values but this is not a good global strategy in this case
    since we observed that missing values are spread over almost all columns and rows.
    Hence, we need a better strategy for handling missing values.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地删除包含缺失数据的所有行或列——事实上这是一个非常常见的方法！对于受到太多缺失值污染的行来说是有意义的，但在这种情况下这并不是一个好的全局策略，因为我们观察到缺失值几乎分布在所有的列和行上。因此，我们需要一个更好的策略来处理缺失值。
- en: A summary of missing values sources and imputation methods is available, for
    example, in the book *Data Analysis Using Regression and Mutlilevel/Hierarchical
    Models* by A. Gelman and J. Hill ([http://www.stat.columbia.edu/~gelman/arm/missing.pdf](http://www.stat.columbia.edu/~gelman/arm/missing.pdf))
    or in the presentation [https://www.amstat.org/sections/srms/webinarfiles/ModernMethodWebinarMay2012.pdf](https://www.amstat.org/sections/srms/webinarfiles/ModernMethodWebinarMay2012.pdf)
    or [https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf](https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf)[.](http://www.stat.columbia.edu/~gelman/arm/missing.pdf)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值来源和插补方法的摘要可以在A. Gelman和J. Hill的书*Data Analysis Using Regression and Mutlilevel/Hierarchical
    Models*（[http://www.stat.columbia.edu/~gelman/arm/missing.pdf](http://www.stat.columbia.edu/~gelman/arm/missing.pdf)）或演示文稿[https://www.amstat.org/sections/srms/webinarfiles/ModernMethodWebinarMay2012.pdf](https://www.amstat.org/sections/srms/webinarfiles/ModernMethodWebinarMay2012.pdf)或[https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf](https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf)中找到。
- en: 'Considering the heart rate column first, we cannot drop it since there is an
    obvious link between higher heart rate and practiced activity. However, we can
    still fill missing values with a reasonable constant. In the context of the heart
    rate, replacing missing values with the mean value of column values - a technique
    sometimes referred to as *mean computation of missing values* - can make good
    sense. We can compute it with the following lines of code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先考虑心率列，我们不能删除它，因为高心率和运动活动之间存在明显的联系。然而，我们仍然可以用一个合理的常数填充缺失值。在心率的情境下，用列值的平均值替换缺失值——有时被称为*平均计算缺失值*的技术是有意义的。我们可以用以下代码来计算它：
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00066.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00066.jpeg)'
- en: We can see that `mean heart rate` is quite a high value, which reflects the
    fact that heart rate measurements are mainly associated with sport activities
    (a reader can verify that). But, for example, considering the activity `watching
    TV`, the value over 90 is slightly higher than the expected value, since the average
    resting rate is between 60 and 100 (based on Wikipedia).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`平均心率`是一个相当高的值，这反映了心率测量主要与运动活动相关（读者可以验证）。但是，例如，考虑到`看电视`这项活动，超过90的数值略高于预期值，因为平均静息心率在60到100之间（根据维基百科）。
- en: So for this case, we can replace missing heart rate values with mean resting
    rate (80) or we can take the computed mean value of heart rate. Later, we will
    impute the computed mean value and compare or combine the results (this is called,
    multiple imputation method). Or we can append a column which marks a line with
    missing value (see, for example, [https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf](https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf)).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，我们可以用平均静息心率（80）替换缺失的心率值，或者我们可以采用计算得到的心率的平均值。之后，我们将填补计算得到的平均值并比较或合并结果（这称为多重插补方法）。或者我们可以附加一个标记有缺失值的列（例如，参见[https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf](https://www.utexas.edu/cola/prc/_files/cs/Missing-Data.pdf)）。
- en: 'The next step is to replace missing values in the rest of the columns. We should
    perform the same analysis that we did for the heart rate column and see if there
    is a pattern in missing data or if they are just missing at random. For example,
    we can explore a dependency between missing value and our prediction target (in
    this case, `activityId`). Hence, we collect a number of missing values per column
    again; however, now we also remember `activityId` with each missing value:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是替换其余列中的缺失值。我们应该执行与心率列相同的分析，并查看缺失数据是否存在模式，或者它们只是随机缺失。例如，我们可以探索缺失值与我们的预测目标（在本例中为`activityId`）之间的依赖关系。因此，我们再次收集每列的缺失值数量；但是，现在我们还记住了每个缺失值的`activityId`：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00067.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00067.jpeg)'
- en: The preceding code is slightly more complicated and deserves explanation. The
    call `(1)` transforms each value in a row into a sequence of `(K, V)` pairs where
    `K` represents the `activityId` stored in the row, and `V` is `1` if a corresponding
    column contains a missing value, else it is `0`. Then the reduce method `(2)`
    recursively transforms row values represented by sequences into the final results,
    where each column has associated a distribution represented by a sequence of `(K,V)` pairs
    where `K` is `activityId` and `V` represents the number of missing values in rows
    with the `activityId`. The method is straightforward but overcomplicated using
    a non-trivial function `inc` `(3)`. Furthermore, this naive solution is highly
    memory-inefficient, since for each column we duplicate information about `activityId`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码稍微复杂，值得解释。调用`(1)`将每行中的每个值转换为`(K, V)`对的序列，其中`K`表示存储在行中的`activityId`，如果相应的列包含缺失值，则`V`为`1`，否则为`0`。然后，reduce方法`(2)`递归地将由序列表示的行值转换为最终结果，其中每列都有一个分布，由`(K,V)`对的序列表示，其中`K`是`activityId`，`V`表示具有`activityId`的行中的缺失值数量。该方法很简单，但使用了一个非平凡的函数`inc`
    `(3)`，过于复杂。此外，这种天真的解决方案在内存效率上非常低，因为对于每一列，我们都重复了关于`activityId`的信息。
- en: 'Hence, we can reiterate the naive solution by slightly changing the result
    representation by not computing distribution per column, but by counting all columns,
    missing value count per `activityId`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过略微改变结果表示来重申天真的解决方案，不是按列计算分布，而是计算所有列，每个`activityId`的缺失值计数：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this case, the result is an array of key-value pairs, where key is activity
    name, and value contains representing distribution of missing value in individual
    columns. Simply by running both samples, we can observe that the first one takes
    much more time than the second one. Also, the first one has higher memory demands
    and is much more complicated.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，结果是一个键值对数组，其中键是活动名称，值包含各列中缺失值的分布。通过运行这两个样本，我们可以观察到第一个样本所需的时间比第二个样本长得多。此外，第一个样本具有更高的内存需求，而且更加复杂。
- en: 'Finally, we can visualize the result as a heatmap where the *x *axis corresponds
    to columns and the *y *axis represents activities as shown in *Figure 3*. Such
    graphical representation gives us a comprehensible overview of how missing values
    are correlated with response column:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将结果可视化为热图，其中*x*轴对应列，*y*轴表示活动，如图3所示。这样的图形表示给我们提供了一个清晰的概述，说明了缺失值如何与响应列相关：
- en: '![](img/00068.jpeg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00068.jpeg)'
- en: 'Figure 3: Heatmap showing number of missing values in each column grouped by
    activity.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：热图显示按活动分组的每列缺失值数量。
- en: The generated heatmap nicely shows the correlation of missing values. We can
    see that missing values are connected to sensors. If a sensor is not available
    or malfunctioning, then all measured values are not available. For example, this
    is visible for ankle sensor and `playing soccer`, other activities. On the other
    hand, the activity `watching TV` does not indicate any missing value pattern connected
    to a sensor.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的热图很好地显示了缺失值的相关性。我们可以看到缺失值与传感器相连。如果传感器不可用或发生故障，那么所有测量值都不可用。例如，这在踝传感器和`踢足球`等其他活动中是可见的。另一方面，活动`看电视`并没有显示与传感器相关的任何缺失值模式。
- en: Moreover, there is no other directly visible connection between missing data
    and activity. Hence, for now, we can decide to fill missing values with `0.0`
    to express that a missing sensor provides default values. However, our goal is
    to be flexible to experiment with different imputation strategies later (for example,
    imputing mean value of observation with the same `activityId`).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，缺失数据与活动之间没有其他直接可见的联系。因此，目前我们可以决定用`0.0`填充缺失值，以表示缺失传感器提供默认值。但是，我们的目标是灵活地尝试不同的插补策略（例如，使用相同`activityId`的观测均值来插补）。 '
- en: Summary of missing value analysis
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失值分析摘要
- en: 'We can now summarize all facts which we learned about missing values:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以总结我们对缺失值学到的所有事实：
- en: There are 107 rows which are useless and need to be filtered out
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有107行是无用的，需要被过滤掉
- en: There are 44 rows with `26` or `27` missing values. These rows seem useless,
    so we are going to filter them out.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有44行有`26`或`27`个缺失值。这些行似乎是无用的，所以我们将它们过滤掉。
- en: 'The heart rate column contains the majority of missing values. Since we expect
    that the column contains important information which can help to distinguish between
    different sport activities, we are not going to ignore the column. However, we
    are going to impute the missing value based on different strategies:'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 心率列包含大部分缺失值。由于我们期望该列包含可以帮助区分不同运动活动的重要信息，我们不打算忽略该列。但是，我们将根据不同的策略填补缺失值：
- en: Mean resting heart rate based on medical research
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于医学研究的平均静息心率
- en: '`mean heart rate` computed from available data'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据可用数据计算的`平均心率`
- en: There is a pattern in the missing values in the rest of the columns - missing
    values are strictly linked to a sensor. We replace all these missing values with
    the value `0.0`.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其余列中的缺失值存在一种模式 - 缺失值严格与传感器相关。我们将用值`0.0`替换所有这些缺失值。
- en: Data unification
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据统一
- en: 'This exploratory analysis gives us an overview about shape of the data and
    the actions which we need to perform to deal with missing values. However, we
    still need to transform the data into a form expected by Spark algorithms. That
    includes:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这种探索性分析给了我们关于数据形状和我们需要执行的操作的概述，以处理缺失值。然而，我们仍然需要将数据转换为Spark算法所期望的形式。这包括：
- en: Handling missing values
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: Handling categorical values
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理分类值
- en: Missing values
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失值
- en: The missing value handling step is easy, since we already performed missing
    value exploration and summarized the required transformations in the previous
    section. The following steps are going to implement them.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值处理步骤很容易，因为我们已经在前一节中执行了缺失值探索，并总结了所需的转换。接下来的步骤将实现它们。
- en: 'First, we define a list of imputed values - for each column, we assign a single
    `Double` value:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个缺失值列表 - 对于每一列，我们分配一个单一的`Double`值：
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And a function which allow us to inject the values into our dataset:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以及一个允许我们将值注入数据集的函数：
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The defined function accepts a Spark `RDD` where each row is represented as
    an array of `Double` numbers, and a parameter which contains values to replace
    the missing value for each column.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 定义的函数接受一个Spark `RDD`，其中每一行都表示为一个`Double`数字数组，以及一个包含每列替换缺失值的值的参数。
- en: 'In the next step, we define a row filter - a method which removes all rows
    which contain more missing values than a given threshold. In this case, we can
    easily reuse the already computed value `nanCountPerRow`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们定义一个行过滤器 - 一个方法，它删除包含的缺失值超过给定阈值的所有行。在这种情况下，我们可以轻松地重用已经计算的值`nanCountPerRow`：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Please notice that we parameterize defined transformations. It is good practice
    to keep code flexible enough to permit further experimentation with parameters.
    On the other hand, it is good to avoid building a complex framework. The rule
    of thumb is to parameterize functionality which we would like to use in different
    contexts or we need to have a freedom in configuring code constants.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们参数化了定义的转换。保持代码足够灵活以允许进一步尝试不同的参数是一个好的做法。另一方面，最好避免构建复杂的框架。经验法则是参数化功能，我们希望在不同上下文中使用，或者我们需要在配置代码常量时具有自由度。
- en: Categorical values
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类值
- en: Spark algorithms can handle different forms of categorical features, but they
    need to be transformed into a form expected by an algorithm. For example, decision
    trees can handle categorical features as they are; on the other hand, linear regression
    or neural networks need to expand categorical values into binary columns.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Spark算法可以处理不同形式的分类特征，但它们需要被转换为算法所期望的形式。例如，决策树可以处理分类特征，而线性回归或神经网络需要将分类值扩展为二进制列。
- en: 'In this example, the good news is that all input features in our dataset are
    continuous. However, the target feature - `activityId` - represents multi-class
    features. The Spark MLlib classification guide ([https://spark.apache.org/docs/latest/mllib-linear-methods.html#classification](https://spark.apache.org/docs/latest/mllib-linear-methods.html#classification))
    says:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，好消息是我们数据集中的所有输入特征都是连续的。然而，目标特征 - `activityId` - 表示多类特征。Spark MLlib分类指南（[https://spark.apache.org/docs/latest/mllib-linear-methods.html#classification](https://spark.apache.org/docs/latest/mllib-linear-methods.html#classification)）说：
- en: '"The training data set is represented by an RDD of LabeledPoint in MLlib, where
    labels are class indices starting from zero."'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: “训练数据集在MLlib中由LabeledPoint的RDD表示，其中标签是从零开始的类索引。”
- en: 'But our dataset contains different numbers of activityIds - see the computed
    variable `activityIdCounts`. Hence, we need to transform them into a form expected
    by MLlib by defining a map from `activityId` to `activityIdx`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们的数据集包含不同数量的activityIds - 参见计算的变量`activityIdCounts`。因此，我们需要通过定义从`activityId`到`activityIdx`的映射，将它们转换为MLlib所期望的形式：
- en: '[PRE19]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Final transformation
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终转换
- en: 'Finally, we can compose all the defined functionality together and prepare
    the data for model building. First, the `rawData` `RDD` is filtered and all bad
    rows are removed with the help of `filterBadRows`, then the result is processed
    by the `imputeNaN` method which injects given values at the location of missing
    values:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将所有定义的功能组合在一起，为模型构建准备数据。首先，`rawData` `RDD`被过滤，所有不良行都被`filterBadRows`移除，然后结果由`imputeNaN`方法处理，该方法在缺失值的位置注入给定的值：
- en: '[PRE20]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'At the end, verify that we invoked the right transformations by at least computing
    the number of rows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过至少计算行数来验证我们调用了正确的转换：
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00069.jpeg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00069.jpeg)'
- en: We can see that we filtered out 151 rows ,which corresponds to our preceding
    observations.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们过滤掉了151行，这对应于我们之前的观察。
- en: Understanding data is the key point of data science. It involves also understanding
    missing data. Never skip this stage since it can lead to biased models giving
    too good results. And, as we continuously point out, not understanding your data
    will lead you to ask poor questions which ultimately results in lackluster answers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 了解数据是数据科学的关键点。这也包括了解缺失数据。永远不要跳过这个阶段，因为它可能导致产生过于良好结果的偏见模型。而且，正如我们不断强调的那样，不了解你的数据将导致你提出不好的问题，最终导致乏味的答案。
- en: Modelling data with Random Forest
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用随机森林对数据建模
- en: Random Forest is an algorithm which can be used for different problems - binomial
    as we showed in the previous chapter, regression, or multiclass classification.
    The beauty of Random Forest is that it combines multiple weak learners represented
    by decision trees into one ensemble.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种可以用于不同问题的算法 - 如我们在上一章中展示的二项式，回归，或者多类分类。随机森林的美妙之处在于它将由决策树表示的多个弱学习器组合成一个整体。
- en: Furthermore, to reduce variance of individual decision trees, the algorithms
    use the concept of bagging (Bootstrap aggregation). Each decision tree is trained
    on a subset of data generated by random selection with replacement.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了减少单个决策树的方差，算法使用了bagging（自举聚合）的概念。每个决策树都是在通过随机选择并替换生成的数据子集上训练的。
- en: Do not confuse bagging with boosting. Boosting incrementally builds an ensemble
    by training each new model to emphasize observations that previous model misclassified.
    Typically, after a weak model is added into the ensemble, the data is reweighted,
    observations that are misclassified gain weight, and vice versa. Furthermore,
    bagging can be invoked in parallel while boosting is a sequential process. Nevertheless,
    the goal of boosting is the same as of bagging - combine predictions of several
    weak models in order to improve generalization and robustness over a single model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 不要混淆装袋和提升。提升通过训练每个新模型来强调先前模型错误分类的观察结果来逐步构建集成。通常，在将弱模型添加到集成后，数据会被重新加权，错误分类的观察结果会增加权重，反之亦然。此外，装袋可以并行调用，而提升是一个顺序过程。然而，提升的目标与装袋的目标相同
    - 结合几个弱模型的预测，以改善单个模型的泛化和鲁棒性。
- en: 'An example of a boosting method is a **Gradient Boosting Machine** (**GBM**)
    which uses the boosting method to combine weak models (decision trees) into an
    ensemble; however, it generalizes the approach by allowing the use of an arbitrary
    loss function: instead of trying to correct the previous weak model misclassified
    observations, the GBM allows you to minimize a specified loss function (for example,
    mean squared error for regression).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 提升方法的一个例子是**梯度提升机**（**GBM**），它使用提升方法将弱模型（决策树）组合成一个集成；然而，它通过允许使用任意损失函数来概括这种方法：而不是试图纠正先前的弱模型错误分类的观察结果，GBM允许您最小化指定的损失函数（例如，回归的均方误差）。
- en: There are different variations of GBM - for example, stochastic GBM which combines
    boosting with bagging. The regular GBM and also stochastic GBM are available in
    H2O's machine learning toolbox. Furthermore, it is important to mention that GBM
    (as well as RandomForest) is an algorithm which builds pretty good models without
    extensive tuning.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: GBM有不同的变体 - 例如，将提升与装袋相结合的随机GBM。常规GBM和随机GBM都可以在H2O的机器学习工具箱中找到。此外，重要的是要提到GBM（以及RandomForest）是一种在不需要广泛调整参数的情况下构建相当不错模型的算法。
- en: 'More information about GBM is available in original paper of J.H. Friedman:
    *Greedy Function Approximation: A Gradient Boosting Machine* [http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf](http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 有关GBM的更多信息可以在J.H. Friedman的原始论文中找到：*贪婪函数逼近：梯度提升机* [http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf](http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf)。
- en: Moreover, RandomForest employs so-called "feature bagging" - while building
    a decision tree, it selects a random subset of feature to make a split decision.
    The motivation is to build a weak learner and enhance generalization - for example,
    if one of the features is a strong predictor for given target variable, it would
    be selected by the majority of trees, resulting in highly similar trees. However,
    by random selection of features, the algorithm can avoid the strong predictor
    and build trees which find a finer-grained structure of data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RandomForest采用所谓的“特征装袋” - 在构建决策树时，它选择一个随机特征子集来做出分裂决策。动机是构建一个弱学习器并增强泛化能力 -
    例如，如果一个特征对于给定的目标变量是一个强预测因子，它将被大多数树选择，导致高度相似的树。然而，通过随机选择特征，算法可以避免强预测因子，并构建能够找到数据更精细结构的树。
- en: RandomForest also helps easily select the most predictive feature since it allows
    for computation of variable importance in different ways. For example, computing
    an overall feature impurity gain over all trees gives a good estimate of how the
    strong feature is.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForest还有助于轻松选择最具预测性的特征，因为它允许以不同的方式计算变量重要性。例如，通过计算所有树的整体特征不纯度增益，可以很好地估计强特征的重要性。
- en: From an implementation point of view, RandomForest can be easily parallelized
    since the *built trees* step is independent. On the other hand, distributing RandomForest
    computation is slightly harder problem, since each tree needs to explore almost
    the full set of data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 从实现的角度来看，RandomForest可以很容易地并行化，因为*构建树*步骤是独立的。另一方面，分布RandomForest计算是一个稍微困难的问题，因为每棵树都需要探索几乎完整的数据集。
- en: The disadvantage of RandomForest is complicated interpretability. The resulting
    ensemble is hard to explore and explain interactions between individual trees.
    However, it is still one of the best models to use if we need to obtain a good
    model without advanced parameters tuning.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForest的缺点是解释性复杂。得到的集成很难探索和解释个别树之间的交互。然而，如果我们需要获得一个不需要高级参数调整的良好模型，它仍然是最好的模型之一。
- en: 'A good source of information about RandomForest is the original paper of Leo
    Breiman and Adele Cutler available, for example, here: [https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForest的一个很好的信息来源是Leo Breiman和Adele Cutler的原始论文，例如可以在这里找到：[https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)。
- en: Building a classification model using Spark RandomForest
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark RandomForest构建分类模型
- en: 'In the previous section, we explored data and unified it into a form without
    missing values. We still need to transform the data into a form expected by Spark
    MLlib. As explained in the previous chapter, it involves the creation of `RDD`
    of `LabeledPoints`. Each `LabeledPoint` is defined by a label and a vector defining
    input features. The label serves as a training target for model builders and it
    references the index of categorical variables (see prepared transformation `activityId2Idx`):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们探索了数据并将其统一成一个没有缺失值的形式。我们仍然需要将数据转换为Spark MLlib所期望的形式。如前一章所述，这涉及到创建`LabeledPoints`的`RDD`。每个`LabeledPoint`由一个标签和定义输入特征的向量组成。标签用作模型构建者的训练目标，并引用分类变量的索引（参见准备好的转换`activityId2Idx`）：
- en: '[PRE22]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The next step is to prepare data for training and model validation. We simply
    split the data into two parts: 80% for training and the remaining 20% for validation:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是为训练和模型验证准备数据。我们简单地将数据分为两部分：80%用于训练，剩下的20%用于验证：
- en: '[PRE23]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And after this step we are ready to invoke the modeling part of the workflow.
    The strategy for building a Spark RandomForest model is the same as GBM we showed
    in the previous chapter by calling the static method `trainClassifier` on object
    `RandomForest`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步之后，我们准备调用工作流程的建模部分。构建Spark RandomForest模型的策略与我们在上一章中展示的GBM相同，通过在对象`RandomForest`上调用静态方法`trainClassifier`来实现：
- en: '[PRE24]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In this example, the parameters are split into two sets:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，参数被分成两组：
- en: Strategy which defines common parameters for building a decision tree
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义构建决策树的常见参数的策略
- en: RandomForest specific parameters
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RandomForest特定参数
- en: 'The strategy parameter list overlaps with the parameter list of decision tree
    algorithms discussed in the previous chapter:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 策略参数列表与上一章讨论的决策树算法的参数列表重叠：
- en: '`input`: References training data represented by `RDD` of `LabeledPoints`.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input`：引用由`LabeledPoints`的`RDD`表示的训练数据。'
- en: '`numClasses`: Number of output classes. In this case we model only classes
    which are included in the input data.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numClasses`：输出类的数量。在这种情况下，我们仅对输入数据中包含的类建模。'
- en: '`categoricalFeaturesInfo`: Map of categorical features and their arity. We
    don''t have categorical features in input data, that is why we pass an empty map.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`categoricalFeaturesInfo`：分类特征及其度量的映射。我们的输入数据中没有分类特征，因此我们传递一个空映射。'
- en: '`impurity`: Impurity measure used for tree node splitting.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`impurity`：用于树节点分裂的不纯度度量。'
- en: '`subsamplingRate`: A fraction of training data used for building a single decision
    tree.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsamplingRate`：用于构建单棵决策树的训练数据的分数。'
- en: '`maxDepth`: Maximum depth of a single tree. Deep trees have tendency to encode
    input data and overfit. On the other hand, overfitting in RandomForest is balanced
    by assembling multiple trees together. Furthermore, larger trees means longer
    training time and higher memory footprint.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxDepth`：单棵树的最大深度。深树倾向于对输入数据进行编码和过拟合。另一方面，在RandomForest中，通过组装多棵树来平衡过拟合。此外，更大的树意味着更长的训练时间和更高的内存占用。'
- en: '`maxBins`: Continuous features are transformed into ordered discretized features
    with at most `maxBins` possible values. The discretization is done before each
    node split.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxBins`：连续特征被转换为具有最多`maxBins`可能值的有序离散特征。离散化是在每个节点分裂之前完成的。'
- en: 'The RandomForest - specific parameters are the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForest特定参数如下：
- en: '`numTrees`: Number of trees in the resulting forest. Increasing the number
    of trees decreases model variance.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numTrees`：结果森林中的树的数量。增加树的数量会减少模型的方差。'
- en: '`featureSubsetStrategy`: Specifies a method which produces a number of how
    many features are selected for training a single tree. For example: "sqrt" is
    normally used for classification, while "onethird" for regression problems. See
    value of `RandomForest.supportedFeatureSubsetStrategies` for available values.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`featureSubsetStrategy`：指定一种方法，用于选择用于训练单棵树的特征数量。例如："sqrt"通常用于分类，而"onethird"用于回归问题。查看`RandomForest.supportedFeatureSubsetStrategies`的值以获取可用值。'
- en: '`seed`: Seed for random generator initialization, since RandomForest depends
    on random selection of features and rows.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed`：用于随机生成器初始化的种子，因为RandomForest依赖于特征和行的随机选择。'
- en: 'The parameters `numTrees` and `maxDepth` are often referenced as stopping criteria.
    Spark also provides additional parameters to stop tree growing and produce fine-grained
    trees:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`numTrees`和`maxDepth`经常被引用为停止标准。Spark还提供了额外的参数来停止树的生长并生成细粒度的树：
- en: '`minInstancesPerNode`: A node is not split anymore, if it would provide left
    or right nodes which would contain smaller number of observations than the value
    specified by this parameter. Default value is 1, but typically for regression
    problems or large trees, the value should be higher.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minInstancesPerNode`：如果节点提供的左节点或右节点包含的观察次数小于此参数指定的值，则不再分裂节点。默认值为1，但通常对于回归问题或大树，该值应该更高。'
- en: '`minInfoGain`: Minimum information gain a split must get. Default value is
    0.0.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minInfoGain`：分裂必须获得的最小信息增益。默认值为0.0。'
- en: Furthermore, Spark RandomForest accepts parameters which influence the performance
    of execution (see Spark documentation).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark RandomForest接受影响执行性能的参数（请参阅Spark文档）。
- en: RandomForest is by definition an algorithm which depends on randomization. However,
    having non-deterministic runs is not the right behavior if you are trying to reproduce
    results or test corner cases. In this case, the seed parameter provides a way
    of fixing execution and providing deterministic results.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForest在定义上是一个依赖于随机化的算法。然而，如果您试图重现结果或测试边缘情况，那么非确定性运行并不是正确的行为。在这种情况下，seed参数提供了一种固定执行并提供确定性结果的方法。
- en: This is a common practice for non-deterministic algorithms; however, it is not
    enough if the algorithm is parallelized and its result depends on thread scheduling.
    In this case, ad-hoc methods need to be adopted (for example, limit parallelization
    by having only one computation thread, limit parallelization by limiting number
    of input partitions, or switching task scheduler to provide a fix schedule).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这是非确定性算法的常见做法；然而，如果算法是并行化的，并且其结果取决于线程调度，那么这还不够。在这种情况下，需要采用临时方法（例如，通过仅使用一个计算线程限制并行化，通过限制输入分区的数量限制并行化，或切换任务调度程序以提供固定的调度）。
- en: Classification model evaluation
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类模型评估
- en: Now, when we have a model, we need to evaluate the quality of the model to decide
    whether the model is good enough for our needs. Keep in mind, that all quality
    metrics connected to a model need to be considered in your specific context and
    evaluated with respect to your target objective (such as sales increase, fraud
    detection, and so on).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们有一个模型时，我们需要评估模型的质量，以决定模型是否足够满足我们的需求。请记住，与模型相关的所有质量指标都需要根据您的特定情况考虑，并与您的目标目标（如销售增长、欺诈检测等）一起评估。
- en: Spark model metrics
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark模型指标
- en: 'At first, use the embedded model metrics which the Spark API provides. We are
    going to use the same approach that we used in the previous chapter. We start
    by defining a method to extract model metrics for a given model and dataset:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用Spark API提供的嵌入模型指标。我们将使用与上一章相同的方法。我们首先定义一个方法，用于提取给定模型和数据集的模型指标：
- en: '[PRE25]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then we can directly compute Spark `MulticlassMetrics`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以直接计算Spark的`MulticlassMetrics`：
- en: '[PRE26]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'And look at first interesting classification model metrics called `Confusion
    matrix`. It is represented by the type `org.apache.spark.mllib.linalg.Matrix`
    allowing you to perform algebraic operations:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然后首先查看有趣的分类模型指标，称为`混淆矩阵`。它由类型`org.apache.spark.mllib.linalg.Matrix`表示，允许您执行代数运算：
- en: '[PRE27]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00070.jpeg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00070.jpeg)'
- en: 'In this case, Spark prints predicted classes in columns. The predicted classes
    are stored in the field `labels` of the object `rfModelMetrics`. However, the
    field contains only translated indexes (see the created variable `activityId2Idx`).
    Nevertheless, we can easily create a function to transform the label index to
    an actual label string:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Spark在列中打印预测的类。预测的类存储在`rfModelMetrics`对象的`labels`字段中。然而，该字段仅包含已翻译的索引（请参见创建的变量`activityId2Idx`）。尽管如此，我们可以轻松地创建一个函数来将标签索引转换为实际的标签字符串：
- en: '[PRE28]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00071.jpeg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00071.jpeg)'
- en: For example, we can see that other activity was mispredicted many times with
    other activities - it was predicted correctly for `36455` cases; however, for
    `1261` cases the model predicted the `other` activity, but actual activity was
    `house cleaning`. On the other hand, the model predicted the activity `folding
    laundry` instead of the `other` activity.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到其他活动与其他活动多次被错误预测 - 它在`36455`个案例中被正确预测；然而，在`1261`个案例中，模型预测了`其他`活动，但实际活动是`家务清洁`。另一方面，模型预测了`叠衣服`活动而不是`其他`活动。
- en: 'You can directly see that we can directly compute overall prediction accuracy
    based on correctly predicted activities located on the diagonal of the `Confusion
    matrix`:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接看到，我们可以基于`混淆矩阵`对角线上正确预测的活动直接计算整体预测准确度：
- en: '[PRE29]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00072.jpeg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00072.jpeg)'
- en: 'However, the overall accuracy can be misleading in cases of classes are not
    evenly distributed (for example, most of the instances are represented by a single
    class). In such cases, overall accuracy can be confusing, since the model just
    predicting a dominant class would provide a high accuracy. Hence, we can look
    at our predictions in more detail and explore accuracy per individual class. However,
    first we look at the distribution of actual labels and predicted labels to see
    `(1)` if there is a dominant class and `(2)` if model preserves input distribution
    of classes and is not skewed towards predicting a single class:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，总体准确度可能会在类别不均匀分布的情况下产生误导（例如，大多数实例由单个类别表示）。在这种情况下，总体准确度可能会令人困惑，因为模型只是预测一个主导类将提供高准确度。因此，我们可以更详细地查看我们的预测，并探索每个单独类别的准确度。然而，首先我们查看实际标签和预测标签的分布，以查看`(1)`是否有主导类，以及`(2)`模型是否保留了类别的输入分布并且没有偏向于预测单一类别：
- en: '[PRE30]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00073.jpeg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00073.jpeg)'
- en: We can easily see that there is no dominant class; however, the classes are
    not uniformly distributed. It is also worth noticing that the model preserves
    distribution of actual classes and there is no trend to prefer a single class.
    This just confirms our observation based on the `Confusion matrix`.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很容易看到没有主导类；然而，这些类并不是均匀分布的。值得注意的是，该模型保留了实际类别的分布，并且没有倾向于偏爱单一类别。这只是确认了我们基于`混淆矩阵`的观察。
- en: 'And finally, we can look at individual classes and compute precision (aka positive
    predictive value), recall (or so-called sensitivity) and `F-1` score. To remind
    definitions from the previous chapter: precision is a fraction of the correct
    predictions for a given class (that is, TP/TP+TF), while recall is defined as
    a fraction of all class instances that were correctly predicted (that is, TP/TP+FN).
    And finally, the `F-1` score combines both of them since it is computed as the
    weighted harmonic mean of precision and recall. We can easily compute them with
    the help of functions we already defined:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以查看各个类别并计算精确度（又称阳性预测值）、召回率（或称灵敏度）和`F-1`分数。为了提醒上一章的定义：精确度是给定类别的正确预测的比例（即TP/TP+TF），而召回率被定义为所有正确预测的类实例的比例（即TP/TP+FN）。最后，`F-1`分数结合了它们两个，因为它是精确度和召回率的加权调和平均数。我们可以使用我们已经定义的函数轻松计算它们：
- en: '[PRE31]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00074.jpeg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00074.jpeg)'
- en: In our case, we deal with a quite good model since most of values are close
    to value 1.0\. It means that the model performs well for each input category -
    generating a low number of false positives (precision) and also false negatives
    (recall).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们处理了一个相当不错的模型，因为大多数值都接近于1.0。这意味着该模型对每个输入类别的表现良好 - 生成了较少的假阳性（精确度）和假阴性（召回）。
- en: The nice feature of the Spark API is that it already provides methods to compute
    all three metrics we computed manually. We can easily call methods `precision`,
    `recall`, `fMeasure` with the index of label to get the same values. However,
    in the Spark case, the `Confusion matrix` is collected for each call and hence
    increases overall computation time.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Spark API的一个很好的特性是它已经提供了计算我们手动计算的所有三个指标的方法。我们可以轻松调用`precision`、`recall`、`fMeasure`方法，并使用标签索引获得相同的值。然而，在Spark的情况下，每次调用都会收集`混淆矩阵`，从而增加整体计算时间。
- en: 'In our case, we use the already computed `Confusion matrix` and get the same
    results directly. Readers can verify that the following code gives us the same
    numbers as stored in `rfPerClassSummary`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们使用已经计算的`混淆矩阵`并直接获得相同的结果。读者可以验证以下代码是否给出了与`rfPerClassSummary`中存储的相同数字：
- en: '[PRE32]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'By having statistics per class, we can compute macro-average metrics simply
    by computing the mean value for each of the computed metrics:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过每个类的统计数据，我们可以通过计算每个计算指标的平均值来简单地计算宏平均指标：
- en: '[PRE33]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00075.jpeg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00075.jpeg)'
- en: The `Macro` statistics give us an overall characteristic for all feature statistics.
    We can see expected values close to 1.0 since our model performs quite well on
    the testing data.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`Macro`统计数据为我们提供了所有特征统计的整体特征。我们可以看到预期值接近1.0，因为我们的模型在测试数据上表现相当不错。'
- en: 'Moreover, the Spark ModelMetrics API provides also weighted precision, recall
    and `F-1` scores, which are mainly useful if we deal with unbalanced classes:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark ModelMetrics API还提供了加权精度、召回率和`F-1`分数，这些主要在处理不平衡的类时非常有用：
- en: '[PRE34]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output is as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00076.jpeg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00076.jpeg)'
- en: 'And at the end, we are going to look at one more way of computing model metrics
    which is useful also in the cases when the classes are not well distributed. The
    method is called one-versus-all and it provides performance of the classifier
    with respect to one class at a time. That means we will compute a `Confusion matrix`
    for each output class - we can consider this approach as treating the classifier
    as a binary classifier predicting a class as positive case and any of other classes
    as negative case:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将看一种计算模型指标的方法，这种方法在类别分布不均匀的情况下也很有用。该方法称为一对所有，它提供了分类器相对于一个类的性能。这意味着我们将为每个输出类别计算一个`混淆矩阵`
    - 我们可以将这种方法视为将分类器视为一个二元分类器，预测一个类作为正例，其他任何类作为负例：
- en: '[PRE35]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This will give us performance of each class with respect to other classes represented
    by a simple binary `Confusion matrix`. We can sum up all matrices and get a `Confusion
    matrix` to compute average accuracy and micro-averaged metrics per class:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供每个类别相对于其他类别的性能，由简单的二进制`混淆矩阵`表示。我们可以总结所有矩阵并得到一个`混淆矩阵`，以计算每个类的平均准确度和微平均指标：
- en: '[PRE36]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00077.jpeg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00077.jpeg)'
- en: 'Having an overall `Confusion matrix`, we can compute average accuracy per class:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 有了整体的`混淆矩阵`，我们可以计算每个类的平均准确度：
- en: '[PRE37]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00078.jpeg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00078.jpeg)'
- en: 'The matrix gives us also `Micro-averaged metrics` (recall, precision, `F-1`).
    However, it is worth mentioning that our `rfOneVsAllCM` matrix is symmetric. This
    means that `Recall`, `Precision` and `F-1` have the same value (since FP and FN
    are the same):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 该矩阵还给出了`微平均指标`（召回率、精度、`F-1`）。然而，值得一提的是我们的`rfOneVsAllCM`矩阵是对称的。这意味着`召回率`、`精度`和`F-1`具有相同的值（因为FP和FN是相同的）：
- en: '[PRE38]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00079.jpeg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00079.jpeg)'
- en: An overview of the Spark ModelMetrics API is provided by the Spark documentation
    [https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html](https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ModelMetrics API的概述由Spark文档提供[https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html](https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html)。
- en: Furthermore, an understanding of model metrics and especially of a role of `Confusion
    matrix` in multiclass classification is crucial but not connected only to the
    Spark API. A great source of information is the Python scikit documentation ([http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html))
    or various R packages (for example, [http://blog.revolutionanalytics.com/2016/03/com_class_eval_metrics_r.html](http://blog.revolutionanalytics.com/2016/03/com_class_eval_metrics_r.html)).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，了解模型指标，特别是多类分类中`混淆矩阵`的作用是至关重要的，但不仅仅与Spark API有关。一个很好的信息来源是Python scikit文档（[http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html)）或各种R包（例如，[http://blog.revolutionanalytics.com/2016/03/com_class_eval_metrics_r.html](http://blog.revolutionanalytics.com/2016/03/com_class_eval_metrics_r.html)）。
- en: Building a classification model using H2O RandomForest
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用H2O RandomForest构建分类模型
- en: H2O provides multiple algorithms for building classification models. In this
    chapter, we will focus on tree ensembles again, but we are going to demonstrate
    their usage in the context of our sensor data problem.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: H2O提供了多种算法来构建分类模型。在本章中，我们将再次专注于树集成，但我们将演示它们在传感器数据问题的背景下的使用。
- en: 'We have already prepared data which we can use directly to build the H2O RandomForest
    model. To transfer it them into H2O format we need to create `H2OContext` and
    then call the corresponding transformation:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好了数据，可以直接用来构建H2O RandomForest模型。要将它们转换为H2O格式，我们需要创建`H2OContext`，然后调用相应的转换：
- en: '[PRE39]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We created two tables referenced by the names `trainHF` and `testHF`. The code
    also updated names of columns by calling the method `setNames` since input `RDD`
    does not carry information about columns. The important step is call of the `update`
    method to save changes into H2O's distributed memory store. This is an important
    pattern exposed by the H2O API - all changes made on an object are done locally;
    to make them visible to other computation nodes, it is necessary to save them
    into the memory store (so-called **distributed key-value store** (**DKV**) )
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了两个表，分别以`trainHF`和`testHF`命名。代码还通过调用`setNames`方法更新了列的名称，因为输入的`RDD`不包含有关列的信息。重要的一步是调用`update`方法将更改保存到H2O的分布式内存存储中。这是H2O
    API暴露的一个重要模式 - 对对象进行的所有更改都是在本地完成的；为了使它们对其他计算节点可见，有必要将它们保存到内存存储中（所谓的**分布式键值存储**（**DKV**））。
- en: Having data stored as H2O tables, we can open the H2O Flow user interface by
    calling `h2oContext.openFlow` and graphically explore the data. For example, the
    distribution of the `activityId` column as a numeric feature is shown in *Figure
    4:*
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据存储为H2O表后，我们可以通过调用`h2oContext.openFlow`打开H2O Flow用户界面，并以图形方式探索数据。例如，数值特征`activityId`列的分布如*图4*所示：
- en: '![](img/00080.jpeg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00080.jpeg)'
- en: 'Figure 4: The view of numeric column activityId which needs transformation
    to categorical type.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：需要转换为分类类型的数值列activityId的视图。
- en: 'We can directly compare the results and verify that we observe right distribution
    by a piece of Spark code:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接比较结果，并通过一段Spark代码验证我们观察到正确的分布：
- en: '[PRE40]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00081.jpeg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00081.jpeg)'
- en: 'The next step is to prepare the input data to run H2O algorithms. First we
    need to verify that column types are in the form expected by the algorithm. The
    H2O Flow UI provides a list of columns with basic attributes (*Figure 5*):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是准备输入数据来运行H2O算法。首先，我们需要验证列类型是否符合算法所期望的形式。H2O Flow UI提供了带有基本属性的列的列表（*图5*）：
- en: '![](img/00082.jpeg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00082.jpeg)'
- en: 'Figure 5: Columns of imported training dataset shown in Flow UI.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在Flow UI中显示的导入训练数据集的列。
- en: 'We can see that the `activityId` column is numeric; however, to perform classification,
    H2O requires columns to be categorical. So we need to transform the column by
    clicking on Convert to enum in UI or programmatically:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以看到`activityId`列是数值的；然而，为了进行分类，H2O要求列必须是分类的。因此，我们需要通过在UI中点击"转换为枚举"或以编程方式进行转换： '
- en: '[PRE41]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Again, we need to update the modified frame in the memory store by calling the
    `update` method. Furthermore, we are transforming a vector to another vector type
    and we do not need the original vector anymore, hence we can call the `remove`
    method on the result of the `replace` call.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们需要通过调用`update`方法更新内存存储中的修改后的帧。此外，我们正在将一个向量转换为另一个向量类型，我们不再需要原始向量，因此我们可以在`replace`调用的结果上调用`remove`方法。
- en: 'After transformation, the `activityId` column is categorical; however, the
    vector domain contains values "0", "1", ..."6" - they are stored in the field
    `trainHF.vec("activityId").domain`. Nevertheless, we can update the vector with
    actual category names. We have already prepared index to name transformation called
    `idx2Activity` - hence we prepare a new domain and update the `activityId` vector
    domain for training and test tables:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后，`activityId`列是分类的；然而，向量域包含值"0"，"1"，..."6" - 它们存储在字段`trainHF.vec("activityId").domain`中。然而，我们可以使用实际的类别名称更新向量。我们已经准备好了索引到名称转换，称为`idx2Activity`
    - 因此我们准备一个新的域，并更新训练和测试表的`activityId`向量域：
- en: '[PRE42]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In this case, we need to update the modified vector in the memory store as well
    - instead of calling the `update` method, the code makes explicit call of the
    method `water.DKV.put` which directly saves the object into the memory store.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们还需要更新内存存储中修改后的向量 - 代码不是调用`update`方法，而是显式调用`water.DKV.put`方法，直接将对象保存到内存存储中。
- en: In the UI, we can again explore the `activityId` column of test dataset and
    compare it with the results computed - *Figure 6:*
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在UI中，我们可以再次探索测试数据集的`activityId`列，并将其与计算结果进行比较- *图6：*
- en: '![](img/00083.jpeg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00083.jpeg)'
- en: 'Figure 6: The column activityId values distribution in test dataset.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：测试数据集中的activityId值分布。
- en: 'At this point, we have prepared the data to perform model building. The configuration
    of H2O RandomForest for a classification problem follows the same pattern we introduced
    in the previous chapter:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经准备好执行模型构建的数据。H2O RandomForest的分类问题配置遵循我们在上一章中介绍的相同模式：
- en: '[PRE43]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'There are several important differences which distinguish the H2O algorithm
    from Spark. The first important difference is that we can directly specify a validation
    dataset as an input parameter (the `_valid` field). This is not necessary since
    we can perform validation after the model is built; however, when the validation
    dataset is specified, we can track the quality of the model in real-time during
    building and stop model building if we consider the model is good enough (see
    *Figure 7* - the "Cancel Job" action stops training but the model is still available
    for further actions). Furthermore, later we can continue model building and append
    more trees if it is demanded. The parameter `_score_each_iteration` controls how
    often scoring should be performed:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: H2O算法与Spark之间有几个重要的区别。第一个重要的区别是我们可以直接指定验证数据集作为输入参数（`_valid`字段）。这并不是必需的，因为我们可以在构建模型后进行验证；然而，当指定验证数据集时，我们可以在构建过程中实时跟踪模型的质量，并在我们认为模型已经足够好时停止模型构建（参见*图7*
    - "取消作业"操作停止训练，但模型仍然可用于进一步操作）。此外，稍后我们可以继续模型构建并添加更多的树，如果需要的话。参数`_score_each_iteration`控制评分应该多频繁进行：
- en: '![](img/00084.jpeg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00084.jpeg)'
- en: 'Figure 7: Model training can be tracked in Flow UI and also stopped by pressing
    "Cancel Job" button.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：在Flow UI中可以跟踪模型训练，并通过按下"取消作业"按钮停止。
- en: Another difference is represented by the parameters `_nbins`, `_nbins_top_level`,
    and `_nbins_cats`. The Spark RandomForest implementation accepts the parameter
    `maxBins` which controls discretization of continuous features. In the H2O case,
    it corresponds to the parameter `_nbins`. However, the H2O machine learning platform
    allows finer-grained tuning of discretization. Since top-level splits are the
    most important and can suffer from loss of information due to discretization,
    H2O permits temporary increase in the number of discrete categories for top-level
    splits via the parameter `_nbins_top_level`. Furthermore, high-value categorical
    features (> 1,024 levels) often degrades performance of computation by forcing
    an algorithm to consider all possible splits into two distinct subsets. Since
    there are 2^N subsets for `N` categorical levels, finding split points for these
    features can be expensive. For such cases, H2O brings the parameter `_nbins_cats`,
    which controls the number of categorical levels - if a feature contains more categorical
    levels than the value stored in the parameter, then the values are re-binned to
    fit into `_nbins_cats` bins.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个区别在于参数`_nbins`、`_nbins_top_level`和`_nbins_cats`。Spark RandomForest实现接受参数`maxBins`来控制连续特征的离散化。在H2O的情况下，它对应于参数`_nbins`。然而，H2O机器学习平台允许对离散化进行更精细的调整。由于顶层分割最重要，并且可能因为离散化而导致信息丢失，H2O允许通过参数`_nbins_top_level`临时增加离散类别的数量。此外，高值分类特征（>
    1,024个级别）通常会通过强制算法考虑所有可能的分割成两个不同子集来降低计算性能。对于这种情况，H2O引入了参数`_nbins_cats`，它控制分类级别的数量
    - 如果一个特征包含的分类级别多于参数中存储的值，则这些值将重新分组以适应`_nbins_cats`个箱子。
- en: 'The last important difference is that we specified an additional stopping criterion
    together with traditional depth and number of trees in ensemble. The criterion
    limits improvement of computed misclassification on validation data - in this
    case, we specified that model building should stop if two consecutive scoring
    measurements on validation data (the field `_stopping_rounds`) do not improve
    by 0.001 (the value of the field `_stopping_tolerance`). This is a perfect criterion
    if we know the expected quality of the model and would like to limit model training
    time. In our case, we can explore the number of trees in the resulting ensemble:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个重要的区别是，我们在集成中指定了额外的停止标准，以及传统的深度和树的数量。该标准限制了在验证数据上计算的误分类的改善 - 在这种情况下，我们指定，如果验证数据上连续两次评分测量（字段`_stopping_rounds`）不提高0.001（字段`_stopping_tolerance`的值），则模型构建应该停止。如果我们知道模型的预期质量并希望限制模型训练时间，这是一个完美的标准。在我们的情况下，我们可以探索生成集成中的树的数量：
- en: '[PRE44]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00085.jpeg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00085.jpeg)'
- en: Even we demanded 50 trees, the resulting model has only `14` trees since model
    training was stopped since the misclassification rate did not improve with respect
    to the given threshold.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们要求50棵树，由于模型训练在给定阈值下未改善误分类率，因此生成的模型只有`14`棵树。
- en: H2O API exposes multiple stopping criteria which can be used by any of the algorithms
    - a user can use AUC value for binomial problems or MSE for regression problems.
    This is one of the most powerful feature which allow you to decrease computation
    time if huge space of hyper-parameters is explored
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: H2O API公开了多个停止标准，可以被任何算法使用 - 用户可以使用AUC值进行二项问题或MSE进行回归问题。这是最强大的功能之一，可以让您在探索大量超参数空间时减少计算时间。
- en: 'The quality of model can be explored in two ways: (1) directly by using the
    Scala API and accessing the model field `_output` which carries all output metrics,
    or (2) using the graphical interface to explore metrics in a more user-friendly
    way. For example, `Confusion matrix` on a specified validation set can be displayed
    as part of the model view directly in the Flow UI. Refer to the following figure:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的质量可以通过两种方式来探索：（1）直接使用Scala API并访问模型字段`_output`，其中包含所有输出指标，或者（2）使用图形界面以更用户友好的方式来探索指标。例如，可以在Flow
    UI中的模型视图中直接显示指定验证集上的`混淆矩阵`。参考下图：
- en: '![](img/00086.jpeg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00086.jpeg)'
- en: 'Figure 8: Confusion matrix for initial RandomForest model composed of 14 trees.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：由14棵树组成的初始RandomForest模型的混淆矩阵。
- en: It directly gives us error rate (0.22%) and misclassification per class and
    we can compare results directly with computed accuracy using Spark model. Furthermore,
    the `Confusion matrix` can be used to compute additional metrics which we explored.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 它直接给出了错误率（0.22%）和每个类别的误分类，我们可以直接与使用Spark模型计算的准确性进行比较。此外，`混淆矩阵`可以用于计算我们探索的其他指标。
- en: 'For example, compute recall, precision, and `F-1` metrics per class. We can
    simply transform H2O''s `Confusion matrix` to Spark `Confusion matrix` and reuse
    all defined methods. But we have to be careful not to confuse actual and predicted
    values in the resulting `Confusion matrix` (the Spark matrix has predicted values
    in columns while the H2O matrix has them in rows):'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，计算每个类别的召回率、精确度和`F-1`指标。我们可以简单地将H2O的`混淆矩阵`转换为Spark的`混淆矩阵`，并重用所有定义的方法。但是我们必须小心不要混淆结果`混淆矩阵`中的实际值和预测值（Spark矩阵的预测值在列中，而H2O矩阵的预测值在行中）：
- en: '[PRE45]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'You can see that computed metrics for the specified validation dataset are
    stored in model output field `_output._validation_metrics`. It contains `Confusion
    matrix` but also additional information about model performance tracked during
    training. Then we simply transformed the H2O representation into Spark matrix.
    Then we can easily compute macro-performance per class:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到指定验证数据集的计算指标存储在模型输出字段`_output._validation_metrics`中。它包含`混淆矩阵`，还包括在训练过程中跟踪的模型性能的其他信息。然后我们简单地将H2O表示转换为Spark矩阵。然后我们可以轻松地计算每个类别的宏性能：
- en: '[PRE46]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00087.jpeg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00087.jpeg)'
- en: You can see that results are slightly better than the Spark results computed
    before, even though H2O used less trees. The explanation needs to explore H2O
    implementation of the RandomForest algorithm - H2O is using an algorithm based
    on generating a regression decision tree per output class - an approach which
    is often referenced as a "one-versus-all" scheme. This algorithm allows more fine-grained
    optimization with respect to individual classes. Hence in this case 14 RandomForest
    trees are internally represented by 14*7 = 98 internal decision trees.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，结果略优于之前计算的Spark结果，尽管H2O使用的树较少。解释需要探索H2O实现的随机森林算法 - H2O使用的算法是基于为每个输出类生成一个回归决策树的方法
    - 这种方法通常被称为“一对所有”方案。该算法允许针对各个类别进行更精细的优化。因此，在这种情况下，14个随机森林树在内部由14*7 = 98个内部决策树表示。
- en: The reader can find more explanation about the benefits of a "one-versus-all"
    scheme for multiclass classification problems in the paper *In Defense of One-Vs-All
    Classification* from Ryan Rifkin and Aldebaro Klautau. The authors show that the
    schema is as accurate as any other approaches; on the other hand, the algorithm
    forces the generation of more decision trees which can negatively influence computation
    time and memory consumption.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以在Ryan Rifkin和Aldebaro Klautau的论文*In Defense of One-Vs-All Classification*中找到更多关于“一对所有”方案在多类分类问题中的好处的解释。作者表明，该方案与其他方法一样准确；另一方面，该算法强制生成更多的决策树，这可能会对计算时间和内存消耗产生负面影响。
- en: 'We can explore more properties about the trained model. One of the important
    RandomForest metrics is variable importance. It is stored under the model''s field
    `_output._varimp`. The object contains raw values which can be scaled by calling
    the `scaled_values` method or obtain relative importance by calling the `summary`
    method. Nevertheless, they can be explored visually in the Flow UI as shown in
    *Figure 9*. The graph shows that the most significant features are measured temperatures
    from all three sensors followed by various movement data. And surprisingly to
    our expectation, the heart rate is not included among the top-level features:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以探索关于训练模型的更多属性。随机森林的一个重要指标是变量重要性。它存储在模型的字段`_output._varimp`下。该对象包含原始值，可以通过调用`scaled_values`方法进行缩放，或者通过调用`summary`方法获得相对重要性。然而，它们可以在Flow
    UI中以图形方式进行探索，如*图9*所示。图表显示，最重要的特征是来自所有三个传感器的测量温度，其次是各种运动数据。令人惊讶的是，与我们的预期相反，心率并未包含在最重要的特征中。
- en: '![](img/00088.jpeg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00088.jpeg)'
- en: 'Figure 9: Variable importance for model "drfModel". The most important features
    include measured temperature.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：模型“drfModel”的变量重要性。最重要的特征包括测量温度。
- en: 'If we are not satisfied with the quality of the model, it can be extended by
    more trees. We can reuse defined parameters and modify them in the following way:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对模型的质量不满意，可以通过增加更多的树来扩展它。我们可以重用定义的参数，并以以下方式修改它们：
- en: Set up the desired numbers of trees in the resulting ensemble (for example,
    20).
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置所需的集成树的数量（例如，20）。
- en: Disable early stopping criterion to avoid stopping model training before achieving
    the demanded number of trees.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禁用早停准则，以避免在达到所需数量的树之前停止模型训练。
- en: Configure a so called *model checkpoint* to point to the previously trained
    model. The model checkpoint is unique feature of the H2O machine learning platform
    available for all published models. It is useful in situations when you need to
    improve a given model by performing more training iterations.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置所谓的*模型检查点*，指向先前训练过的模型。模型检查点是H2O机器学习平台的独特功能，适用于所有已发布的模型。在需要通过执行更多的训练迭代来改进给定模型的情况下，它非常有用。
- en: 'After that, we can simply launch model building again. In this case, the H2O
    platform simply continues model training, reconstructs model state, and builds
    and appends new trees into a new model:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以简单地再次启动模型构建。在这种情况下，H2O平台简单地继续模型训练，重建模型状态，并构建并附加新树到新模型中。
- en: '[PRE47]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00089.jpeg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00089.jpeg)'
- en: 'In this case, only `6` trees were built - to see that, the user can explore
    the model training output in the console and find a line which ends model training
    output and reporting:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，只构建了`6`棵树 - 要查看这一点，用户可以在控制台中探索模型训练输出，并找到一个以模型训练输出和报告结束的行：
- en: '![](img/00090.jpeg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00090.jpeg)'
- en: 'The 6^(th) tree was generated in 2 seconds and it was the last tree appended
    into the existing ensemble creating a new model. We can again explore `Confusion
    matrix` of newly built model and see improvement in overall error rate from 0.23
    to 0.2% (see *Figure 9*):'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 第6棵树在2秒内生成，并且是附加到现有集成中创建新模型的最后一棵树。我们可以再次探索新构建模型的`混淆矩阵`，并看到整体错误率从0.23降至0.2%的改善（见*图9*）：
- en: '![](img/00091.jpeg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00091.jpeg)'
- en: 'Figure 10: Confusion matrix for RandomForest model with 20 trees.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：具有20棵树的随机森林模型的混淆矩阵。
- en: Summary
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter introduced several important concepts including data cleanup and
    handling missing and categorical values, using Spark and H2O to train multi-classification
    models, and various evaluation metrics for classification models. Furthermore,
    the chapter brings the notion of model ensembles demonstrated on RandomForest
    as the ensemble of decision trees.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了几个重要概念，包括数据清理和处理缺失和分类值，使用Spark和H2O训练多分类模型，以及分类模型的各种评估指标。此外，本章介绍了模型集成的概念，以RandomForest作为决策树的集成。
- en: The reader should see the importance of data preparation, which plays a key
    role during every model training and evaluation process. Training and using a
    model without understanding the modeling context can lead to misleading decisions.
    Moreover, every model needs evaluation with respect to the modeling goal (for
    example, minimization of false positives). Hence understanding trade-offs of different
    model metrics of classification models is crucial.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应该看到数据准备的重要性，在每个模型训练和评估过程中都起着关键作用。在不了解建模背景的情况下训练和使用模型可能会导致误导性的决策。此外，每个模型都需要根据建模目标进行评估（例如，最小化假阳性）。因此，了解分类模型的不同模型指标的权衡是至关重要的。
- en: 'In this chapter, we did not cover all possible modelling tricks for classification
    models, but there are a few of them still opened for curious readers:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们没有涵盖所有可能的分类模型建模技巧，但还有一些对好奇的读者来说仍然是开放的。
- en: We used a simple strategy to impute missing values in the heart rate column,
    but there are other possible solutions - for example, mean value imputation, or
    combining imputation with additional binary column which marks rows with the missing
    value. Both strategies can improve the accuracy of the model and we will use them
    later in this book.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个简单的策略来填补心率列中的缺失值，但还有其他可能的解决方案 - 例如，均值插补，或者将插补与额外的二进制列相结合，标记具有缺失值的行。这两种策略都可以提高模型的准确性，我们将在本书的后面部分使用它们。
- en: Furthermore, the Occam's razor principle suggests that it is good idea to prefer
    a simpler model than a complex model providing the same accuracy. Hence, a good
    idea is to define a hyper-space of parameters and use an exploration strategy
    to find the simplest model (for example, fewer trees, less depth) which provides
    the same (or better) accuracy as the models trained in this chapter.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，奥卡姆剃刀原则表明，更倾向于选择一个简单的模型，而不是一个复杂的模型，尽管它们提供相同的准确性是一个好主意。因此，一个好主意是定义一个参数的超空间，并使用探索策略找到最简单的模型（例如，更少的树木，更少的深度），它提供与本章训练的模型相同（或更好）的准确性。
- en: To conclude this chapter, it is important to mention that the tree ensemble
    presented in this chapter is a primitive instance powerful concept of ensembles
    and super-learners which we are going to introduce later in this book.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 总结本章，重要的是要提到，本章介绍的树集成是集成和超学习器强大概念的一个原始实例，我们将在本书的后面部分介绍。
