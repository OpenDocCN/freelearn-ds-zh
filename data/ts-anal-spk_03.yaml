- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Introduction to Apache Spark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark简介
- en: This chapter provides an overview of Apache Spark, explaining its distributed
    computing capabilities and suitability for processing large-scale time series
    data. It explains how Spark addresses the challenges of parallel processing, scalability,
    and fault tolerance. This foundational knowledge is essential as it sets the stage
    for leveraging Spark’s strengths in handling vast temporal datasets, facilitating
    efficient time series analysis. Practical knowledge of Spark’s role enhances practitioners’
    ability to harness its power for complex computations, making it a valuable resource
    for scalable, high-performance time series applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了Apache Spark，解释了它的分布式计算能力以及处理大规模时间序列数据的适用性。它还解释了Spark如何解决并行处理、可扩展性和容错性的问题。这些基础知识非常重要，因为它为利用Spark在处理庞大的时间数据集时的优势奠定了基础，从而促进了高效的时间序列分析。了解Spark的作用，可以增强从业人员利用其强大计算能力的能力，使其成为可扩展、高性能时间序列应用的宝贵资源。
- en: 'We’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将覆盖以下主要内容：
- en: Apache Spark and its architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark及其架构
- en: How Apache Spark works
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark是如何工作的
- en: Installation of Apache Spark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark的安装
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The hands-on focus of this chapter will be to deploy a multi-node Apache Spark
    cluster to get familiar with important components of a deployment. The code for
    this chapter can be found in the `ch3` folder of this book’s GitHub repository
    at this URL: https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch3.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的动手部分将着重于部署一个多节点的Apache Spark集群，以帮助熟悉部署过程中的重要组件。本章的代码可以在本书GitHub仓库的`ch3`文件夹中找到，网址为：https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch3。
- en: The hands-on section of this chapter will go into further detail. This requires
    some skills in building an open source environment. If you do not intend to build
    your own Apache Spark environment and your focus is instead on time series and
    using but not deploying Spark, you can skip the hands-on section of this chapter.
    You can use a managed platform such as Databricks, which comes pre-built with
    Spark, as we will do in future chapters.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的动手实践部分将进一步详细介绍这一过程。此过程需要一些搭建开源环境的技能。如果你不打算搭建自己的Apache Spark环境，而是专注于时间序列并使用Spark（而不是部署它），你可以跳过本章的动手部分。你可以使用像Databricks这样的托管平台，它预先构建了Spark，我们将在未来的章节中使用该平台。
- en: What is Apache Spark?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Apache Spark？
- en: Apache Spark is a distributed computing system that is open source, with a programming
    interface and clusters for parallel data processing at scale and with fault tolerance.
    Started as a project at Berkeley’s AMPLab in 2009, Spark became open source in
    2010 as part of the Apache Software Foundation. The original creators of Spark
    have since founded the Databricks company, which provides a managed version of
    Spark on their multi-cloud platform.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个分布式计算系统，它是开源的，具有编程接口和用于大规模并行数据处理的集群，并且具备容错能力。Spark最初作为伯克利AMPLab的一个项目启动于2009年，并于2010年成为Apache软件基金会的一部分，开源发布。Spark的原始创始人后来成立了Databricks公司，提供基于其多云平台的托管版Spark。
- en: Spark can handle both batch and stream processing, making it a widely usable
    tool for big data processing. Bringing significant performance improvement over
    existing big data systems, Spark uses in-memory computing and optimized query
    execution for very fast analytic queries on data of any size. It is built on the
    concept of **Resilient Distributed Datasets** (**RDDs**) and DataFrames. These
    are collections of data elements distributed across a cluster of computers that
    can be operated on in parallel with fault tolerance. We will expand further on
    these concepts in the rest of this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以处理批处理和流处理，使其成为大数据处理中的一个广泛适用的工具。相较于现有的大数据系统，Spark通过内存计算和优化的查询执行实现了显著的性能提升，能够对任何规模的数据进行非常快速的分析查询。它基于**弹性分布式数据集**（**RDDs**）和数据框架（DataFrames）的概念构建。这些是分布在计算机集群中的数据元素集合，能够并行操作并具备容错能力。在本章的其余部分，我们将进一步扩展这些概念。
- en: Why use Apache Spark?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么使用Apache Spark？
- en: There are numerous benefits to using Spark, which explains its popularity as
    a large-scale data processing solution, as shown in *Figure 3**.1* based on Google
    Trends. We can see here the increasing interest in Apache Spark software in line
    with the big data topic, while the trend for Hadoop software had been increasing,
    then decreased when it was overtaken by Apache Spark software in March 2017.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark有许多优势，这也是其作为大规模数据处理解决方案受欢迎的原因，正如*图3.1*所示，这一趋势基于Google趋势数据。我们可以看到，Apache
    Spark软件在大数据话题上的兴趣不断增加，而Hadoop软件的趋势在2017年3月被Apache Spark软件超越后开始下降。
- en: '![](img/B18568_03_1.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_03_1.jpg)'
- en: 'Figure 3.1: Increasing interest in Apache Spark compared to Hadoop and big
    data'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：与Hadoop和大数据相比，Apache Spark的兴趣不断增加
- en: 'This surge in interest can be explained by some key benefits, as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这一兴趣激增可以通过一些关键优势来解释，具体如下：
- en: '**Speed**: Spark runs up to 100 times faster in memory and up to 10 times faster
    even when running on disk, when compared to non-Spark Hadoop clusters.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：与非Spark Hadoop集群相比，Spark在内存中运行速度可快达100倍，甚至在磁盘上运行时也能快达10倍。'
- en: '**Fault tolerance**: With the use of distributed computing, Spark provides
    a fault-tolerant mechanism with recovery on failure.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：通过使用分布式计算，Spark提供了一个容错机制，能够在故障发生时进行恢复。'
- en: '**Modularity**: Spark includes support for SQL and structured data processing,
    machine learning, graph processing, and stream data processing. With libraries
    for diverse tasks, it can handle a wide range of data processing tasks.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模块化**：Spark支持SQL和结构化数据处理、机器学习、图处理和流数据处理。凭借各种任务的库，Spark能够处理广泛的数据处理任务。'
- en: '**Usability**: With APIs in Python, Java, Scala, and R, as well as Spark Connect,
    Spark is accessible to a wide range of developers and data scientists.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**：Spark提供了Python、Java、Scala和R等API，以及Spark Connect，能够让广泛的开发者和数据科学家使用。'
- en: '**Compatibility**: Spark can run on different platforms – including Databricks,
    Hadoop, Apache Mesos, and Kubernetes, standalone, or in the cloud. It can also
    access diverse data sources, which will be discussed in the *Interfaces and* *integrations*
    section.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**兼容性**：Spark可以在不同平台上运行——包括Databricks、Hadoop、Apache Mesos和Kubernetes，独立运行或在云端。它还可以访问各种数据源，相关内容将在*接口和*
    *集成*部分中讨论。'
- en: The growing popularity of Spark, and the numerous benefits explaining it, came
    over several years of evolution, which we will look at next.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark日益流行以及其背后的诸多优势，是经过多年的演变而来的，接下来我们将进行回顾。
- en: Evolutions
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演变历程
- en: 'Apache Spark has gone through several evolutions over the years, with the following
    major release versions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark多年来经历了几次演进，以下是主要的版本发布：
- en: '**1.x**: These were early versions of Spark, starting with RDDs and some distributed
    data processing capabilities.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1.x**：这些是Spark的早期版本，起初基于RDD和一些分布式数据处理能力。'
- en: '**2.x**: Spark 2.0, in 2016, had significant improvements with the introduction
    of Spark SQL, structured streaming, and the Dataset API, which is more efficient
    than RDDs.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2.x**：Spark 2.0（2016年）引入了Spark SQL、结构化流处理和Dataset API，相比RDD更加高效。'
- en: '**3.x**: From 2020, Spark 3.0 had further improvements, with **Adaptive Query
    Execution** (**AQE**), which dynamically adjusts query plans based on runtime
    statistics, enhanced performance optimizations, and dynamic partition pruning.
    It also included support for newer Python versions as well as additions to the
    **machine learning** **library** (**MLlib**).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3.x**：自2020年起，Spark 3.0进一步改进，增加了**自适应查询执行**（**AQE**），该功能根据运行时统计数据动态调整查询计划，增强了性能优化，并进行了动态分区剪枝。同时，新增对更新版本Python的支持，以及对**机器学习**
    **库**（**MLlib**）的扩展。'
- en: 'As of the time of writing, the latest version is 3.5.3\. To understand the
    direction the project is going in, let’s now zoom in on the highlights of some
    of the most recent versions, which are as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，最新版本为3.5.3。为了了解项目的未来发展方向，接下来我们将聚焦于一些最新版本的亮点，具体如下：
- en: '**PySpark** gains user-friendly support for Python-type hints, the pandas API
    on Spark, and enhanced performance thanks to optimizations.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PySpark**为Python类型提示提供了用户友好的支持，支持在Spark上的pandas API，并通过优化提升了性能。'
- en: Adaptive Query Execution improvements drive more efficient query execution and
    resource utilization.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应查询执行（Adaptive Query Execution）的改进促进了更高效的查询执行和资源利用。
- en: '**Structured Streaming** enhancements give better stability and performance.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化流处理**的改进提升了稳定性和性能。'
- en: Kubernetes supports better integration and resource management capabilities
    for running Spark on Kubernetes. This results in greater efficiency and ease of
    use.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes支持更好的集成和资源管理能力，用于在Kubernetes上运行Spark。这带来了更高的效率和易用性。
- en: API and SQL enhancements bring more efficient data processing and analysis,
    with new functions and improvements to existing ones. The key themes here are
    better usability and performance.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API和SQL的增强带来了更高效的数据处理和分析，新功能和现有功能的改进提升了效率。这里的关键主题是更好的可用性和性能。
- en: As we can see from the preceding, the recent focus is on support for modern
    infrastructure, performance, and usability. As a tool for large-scale data processing
    and analysis, this is turning Spark into an even more widely adopted tool.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从前述内容可以看出，最近的关注点主要集中在对现代基础设施的支持、性能和可用性上。作为一个大规模数据处理和分析的工具，这使得Spark成为一个更加广泛采用的工具。
- en: Distributions of Spark
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark的分发版
- en: With its popularity and wide adoption have come several distributions of Spark.
    These have been developed by different organizations, with Apache Spark, at its
    core, providing different integration capabilities, usability features, and enhancements
    to functionalities. Bundled with other big data tools, these distributions often
    offer improved management interfaces, enhanced security, and different storage
    integrations.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 随着其流行度和广泛的应用，Spark出现了多个分发版。这些分发版由不同的组织开发，Apache Spark作为核心，提供了不同的集成能力、可用性特性和功能增强。与其他大数据工具捆绑在一起的这些分发版，通常提供改进的管理界面、增强的安全性以及不同的存储集成。
- en: 'The following distributions are the most common ones:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是最常见的分发版：
- en: '**Apache Spark** is the original open source version maintained by the Apache
    Software Foundation. It is the basis for the other distributions.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Spark**是由Apache软件基金会维护的原始开源版本，是其他分发版的基础。'
- en: '**Databricks Runtime** is developed by Databricks, the company founded by the
    creators of Spark. It is optimized for cloud environments, with a unified analytics
    platform facilitating collaboration between data engineers, data scientists, and
    business analysts. Databricks provides optimized Spark performance with a C++
    rewritten version called **Photon**, interactive notebooks, integrated workflows
    for data engineering with **Delta Live Tables** (**DLT**), and machine learning
    with MLflow, along with enterprise-grade compliance and security as part of its
    Unity Catalog-based governance capabilities.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Databricks Runtime**是由Databricks公司开发的，这家公司由Spark的创始人创建。它针对云环境进行了优化，提供了一个统一的分析平台，促进了数据工程师、数据科学家和业务分析师之间的协作。Databricks提供了经过优化的Spark性能，采用了C++重写的版本**Photon**，互动笔记本，集成的数据工程工作流（包括**Delta
    Live Tables**（**DLT**）），以及与MLflow的机器学习支持，且作为其基于Unity Catalog的治理功能的一部分，提供企业级的合规性和安全性。'
- en: '**Cloudera Data Platform** (**CDP**) includes Spark as part of its data platform,
    which includes Hadoop and other big data tools.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cloudera Data Platform**（**CDP**）将Spark作为其数据平台的一部分，平台中还包含了Hadoop和其他大数据工具。'
- en: '**Hortonworks Data Platform** (**HDP**), before merging with Cloudera, offered
    its own distribution that included Spark.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hortonworks Data Platform**（**HDP**）在与Cloudera合并之前，提供了其自有的分发版，其中包括Spark。'
- en: '**Microsoft Azure** includes Spark as part of **Azure Databricks**, which is
    a first-party service on Azure, HDInsight, Synapse, and, moving forward, Fabric.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Microsoft Azure**将Spark作为**Azure Databricks**的一部分，后者是Azure上的第一方服务，此外还包括HDInsight、Synapse，以及未来的Fabric。'
- en: '**Amazon Web Services** (**AWS**) offers Databricks in its Marketplace, as
    well as **Elastic MapReduce** (**EMR**) running as a cloud service to run big
    data frameworks such as Apache Spark on AWS.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Web Services**（**AWS**）在其市场中提供Databricks，以及作为云服务运行的**Elastic MapReduce**（**EMR**），可在AWS上运行如Apache
    Spark等大数据框架。'
- en: '**Google Cloud Platform** (**GCP**) hosts Databricks, as well as **Dataproc**,
    which is Google’s managed service for Apache Spark and Hadoop clusters in the
    cloud.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Cloud Platform**（**GCP**）托管了Databricks，以及**Dataproc**，这是Google为Apache
    Spark和Hadoop集群提供的云端托管服务。'
- en: From on-premises to cloud-native solutions to those that integrate with other
    data platforms, each distribution of Apache Spark answers different needs. When
    organizations choose a distribution, factors typically considered are performance
    requirements, ease of management, the existing technology stack, and specific
    capabilities provided by each distribution.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从本地解决方案到云原生解决方案，再到与其他数据平台集成的解决方案，每种Apache Spark的分发版都能满足不同的需求。当组织选择分发版时，通常考虑的因素包括性能要求、管理的简便性、现有的技术栈以及每个分发版所提供的特定功能。
- en: Now that we have gone through what Apache Spark is, its benefits, and its evolutions,
    let’s dive deeper into its architecture and components.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了 Apache Spark 的基本概念、优势和演变之后，让我们深入探讨它的架构和组件。
- en: Apache Spark architecture
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 架构
- en: The primary objective of an architecture with Apache Spark is to process large
    datasets across distributed clusters. Architectures can vary based on the specific
    requirements of the application, whether it is batch processing, stream processing,
    machine learning, querying for reports, or even a combination of these. A typical
    Spark architecture includes several key components that contribute to the data
    processing requirements. An example of such architecture is represented in *Figure
    3**.2*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Apache Spark 架构的主要目标是跨分布式集群处理大规模数据集。架构可以根据应用的具体需求而有所不同，无论是批处理、流处理、机器学习、报告查询，还是这些需求的组合。一个典型的
    Spark 架构包括多个关键组件，这些组件共同满足数据处理需求。此类架构的示例可见于 *图 3.2*。
- en: '![](img/B18568_03_2.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_03_2.jpg)'
- en: 'Figure 3.2: Example of Apache Spark-based architecture (standalone mode)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：基于 Apache Spark 的架构示例（独立模式）
- en: Let’s now drill down into what each of these parts does.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入探讨一下这些部分的具体功能。
- en: Cluster manager
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群管理器
- en: 'Cluster managers are responsible for allocating resources to the clusters,
    which are the operating system environments on which the Spark workloads execute.
    These include the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器负责将资源分配给集群，集群是 Spark 工作负载执行的操作系统环境。包括以下几种：
- en: '**Standalone**: A basic cluster manager is included with Spark, making it easy
    to set up a cluster to get started. This cluster manager node is also known as
    the master node:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立模式**：Spark 附带一个基本的集群管理器，使得搭建集群并开始使用变得容易。这个集群管理器节点也被称为主节点：'
- en: '**Kubernetes**: Spark can be deployed to Kubernetes, which is an open source
    container-based system that automates the deployment, management, and scaling
    of containerized applications.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes**：Spark 可以部署到 Kubernetes 上，Kubernetes 是一个开源的基于容器的系统，能够自动化容器化应用的部署、管理和扩展。'
- en: '**Apache Mesos**: As a cluster manager, Mesos supports Spark, in addition to
    running Hadoop MapReduce.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Mesos**：作为集群管理器，Mesos 支持 Spark，同时也能运行 Hadoop MapReduce。'
- en: '**Hadoop YARN**: Spark can share clusters and datasets with other Hadoop components
    when running with YARN.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop YARN**：在与 YARN 一起运行时，Spark 可以与其他 Hadoop 组件共享集群和数据集。'
- en: '**Proprietary and commercial**: The solutions that incorporate Spark have their
    own cluster managers – usually a variation and improvement of the preceding open
    source versions.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专有和商业**：将 Spark 融入的解决方案通常有自己的集群管理器——通常是对先前开源版本的变种和改进。'
- en: Next, we will look at what is within these Spark clusters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看这些 Spark 集群中的内容。
- en: Spark Core, libraries, and API
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark Core、库和 API
- en: Once we have one or more clusters provided by the cluster manager, Spark Core
    then manages memory and fault recovery, as well as everything related to Spark
    jobs, such as scheduling, distributing, and monitoring. Spark Core abstracts storage
    read and write, using RDDs and, more recently, DataFrames as the data structure.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦集群管理器提供了一个或多个集群，Spark Core 就会管理内存和故障恢复，以及与 Spark 作业相关的所有事务，如调度、分配和监控。Spark
    Core 抽象了存储的读写，使用 RDD 和最近的 DataFrame 作为数据结构。
- en: 'On top of (and working very closely with) Core, several libraries and APIs
    provide additional functionalities specific to the data processing requirements.
    These are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在（并与之紧密合作）Core 的基础上，多个库和 API 提供了针对数据处理需求的附加功能。这些功能包括：
- en: '**Spark SQL** allows querying structured data via SQL'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL** 允许通过 SQL 查询结构化数据。'
- en: '**Spark Structured Streaming** processes data streaming from various sources,
    such as Kafka and Kinesis'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Structured Streaming** 处理来自各种来源的数据流，例如 Kafka 和 Kinesis。'
- en: '**MLlib** provides multiple types of machine learning algorithms for classification,
    regression, and clustering, among others'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLlib** 提供多种机器学习算法，支持分类、回归、聚类等任务。'
- en: '**GraphX** allows the use of graph algorithms for the creation, transformation,
    and querying of graphs'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX** 允许使用图算法来创建、转换和查询图。'
- en: Spark is about data processing, and as such, an important part of the solution
    is the data structure, which we will discuss next.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 涉及数据处理，因此，解决方案中的一个重要部分是数据结构，接下来我们将讨论这个部分。
- en: RDDs, DataFrames, and Datasets
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD、DataFrame 和数据集
- en: We have mentioned RDDs and DataFrames a few times since the start of the chapter
    without going into detail, which we will do now, as well as introducing Datasets.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 自本章开始以来，我们提到了几次RDD和DataFrame，但没有详细说明，现在我们将对此进行详细讲解，并引入Datasets。
- en: 'In short, these are the in-memory data structures representing the data and
    providing us with a programmatic way, more formerly termed an abstraction, to
    manipulate the data. Each of these data structures has its use cases, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这些是内存中的数据结构，表示数据并为我们提供了一种程序化的方式，正式来说，这是一种抽象，来操作数据。每种数据结构都有其适用的场景，如下所示：
- en: An **RDD** is Spark’s fundamental data structure. Immutable and distributed,
    it can store data in memory across a cluster. Fault-tolerant, an RDD can automatically
    recover from failures. Note that in case of insufficient memory on the cluster,
    Spark does store part of the RDD on disk, but as this is managed behind the scenes,
    we will keep referring to RDDs as being in memory.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RDD**是Spark的基本数据结构。它是不可变的和分布式的，可以在集群内存中存储数据。具有容错性，RDD可以自动从故障中恢复。需要注意的是，在集群内存不足的情况下，Spark确实会将部分RDD存储到磁盘上，但由于这一过程是由后台管理的，因此我们仍然将RDD视为存在内存中。'
- en: You are less and less likely to use RDDs, as more operations become possible
    with easier-to-use DataFrames, which we will see next. RDDs are more suitable
    for low-level transformations with direct manipulation of data, useful when you
    need low-level control over computations.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随着越来越多操作变得可以通过更易用的DataFrame实现，你将越来越不可能使用RDD，我们接下来将看到这一点。RDD更适合进行低级转换，直接操作数据，当你需要对计算进行低级控制时，它们非常有用。
- en: A **DataFrame** is built upon an RDD as a distributed collection of data with
    named columns. This is like a table in a relational database. In addition to the
    more user-friendly higher-level API, which makes code more concise and easier
    to understand, DataFrames benefit from performance gain over RDDs thanks to Spark’s
    Catalyst optimizer, which we will discuss later in the chapter.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataFrame**是建立在RDD之上的分布式数据集合，具有命名的列。这类似于关系数据库中的表。除了更易用的高级API，使代码更加简洁易懂外，DataFrame还因为Spark的Catalyst优化器的支持，相较于RDD在性能上有了提升，我们将在本章后面讨论这一点。'
- en: We have already started using DataFrames as part of the hands-on exercises done
    so far. You may have noticed pandas DataFrames in addition to Spark DataFrames
    while doing the exercises. While similar in concept, they are part of different
    libraries and have their underlying implementation differences. Fundamentally,
    pandas DataFrames are on single machines while Spark DataFrames are distributed.
    pandas DataFrames can be converted to pandas-on-Spark DataFrames, with the benefit
    of pandas DataFrame API support in addition to parallelism.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在之前的动手练习中，我们已经开始使用DataFrame。你可能已经注意到在做练习时，除了Spark DataFrame，还有pandas DataFrame。虽然在概念上它们类似，但它们属于不同的库，底层实现有所不同。从根本上讲，pandas
    DataFrame运行在单台机器上，而Spark DataFrame是分布式的。pandas DataFrame可以转换为pandas-on-Spark DataFrame，除了并行化的优势外，还能支持pandas
    DataFrame API。
- en: A **Dataset** provides the type safety of RDDs with the optimizations of DataFrame.
    Type safety means that you can catch data type errors at compilation time, resulting
    in more runtime reliability. This is, however, dependent on the programming language
    supporting data type definition at the time of coding and verification and enforcement
    during compilation. As such, Datasets are only supported in Scala and Java, with
    Python and R, being dynamically typed, using DataFrames.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dataset**结合了RDD的类型安全性和DataFrame的优化。类型安全性意味着你可以在编译时捕捉数据类型错误，从而提高运行时的可靠性。然而，这取决于编程语言是否支持在编码时定义数据类型，并在编译时进行验证和强制执行。因此，Dataset仅在Scala和Java中得到支持，而Python和R由于是动态类型语言，只能使用DataFrame。'
- en: In summary, you will get low-level control with RDDs, optimized higher-level
    abstraction with DataFrames, and type safety with Datasets. Which data structure
    to use depends on the specific requirements of your application.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，RDD提供低级控制，DataFrame提供优化后的高级抽象，而Dataset则提供类型安全。选择使用哪种数据结构取决于你应用的具体需求。
- en: So far, we have considered the internal components. We will next go into the
    external facing parts and how Spark integrates in the backend with storage and
    in the frontend with applications and users.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了内部组件。接下来，我们将探讨外部接口部分，介绍Spark如何在后端与存储系统集成，并在前端与应用和用户交互。
- en: Interfaces and integrations
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接口与集成
- en: 'When considering interfacing and integrating with the environment, there are
    a few ways in which this is fulfilled with Apache Spark. These are the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑与环境的接口和集成时，有几种方法可以通过 Apache Spark 实现。这些方法如下：
- en: '`csv`, `json`, `xml`, `orc`, `avro`, `parquet`, and `protobuf`. Of these, Parquet
    is the most common as it gives good performance with snappy compression. In addition,
    Spark can be extended with packages to support several storage protocols and external
    data sources. Delta is one of these, which we will discuss further in [*Chapter
    4*](B18568_04.xhtml#_idTextAnchor087) and [*Chapter 5*](B18568_05.xhtml#_idTextAnchor103).
    Other formats include Iceberg and Hudi. Note that we are talking here about the
    disk representation of the data, which is loaded into the memory-based data structures
    in RDDs and DataFrames discussed previously.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`csv`、`json`、`xml`、`orc`、`avro`、`parquet` 和 `protobuf`。其中，Parquet 是最常见的格式，因为它在使用
    snappy 压缩时提供了良好的性能。此外，Spark 可以通过扩展包来支持多种存储协议和外部数据源。Delta 就是其中之一，我们将在[*第 4 章*](B18568_04.xhtml#_idTextAnchor087)和[*第
    5 章*](B18568_05.xhtml#_idTextAnchor103)中进一步讨论。其他格式包括 Iceberg 和 Hudi。请注意，我们这里讨论的是数据的磁盘表示形式，这些数据会被加载到之前讨论的基于内存的
    RDD 和 DataFrame 数据结构中。'
- en: We already have some experience with Spark and storage as part of the hands-on
    exercises done so far, where we have been reading CSV files from the local storage
    on the Databricks Community Edition’s Spark clusters.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经通过目前为止的实践演练，积累了一些关于 Spark 和存储的经验，在这些演练中，我们已经从 Databricks Community Edition
    的 Spark 集群读取了本地存储中的 CSV 文件。
- en: '**Applications**: This is the code with the logic for data processing, calling
    the Spark APIs and libraries for tasks such as data transformations, streaming,
    SQL queries, or machine learning. Developers can write in Python, R, Scala, or
    Java. The code is then executed on the Spark clusters.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用程序**：这是包含数据处理逻辑的代码，调用 Spark API 和库来执行数据变换、流处理、SQL 查询或机器学习等任务。开发人员可以使用 Python、R、Scala
    或 Java 编写代码。然后，这些代码会在 Spark 集群上执行。'
- en: Our experience with the application side has started as well, with the hands-on
    code used so far.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在应用程序方面的经验也已经开始，通过到目前为止的实践代码。
- en: '**Platform user interface**: In addition to the web interface for Databricks
    Community Edition, which we have seen in the hands-on exercises, open source Apache
    Spark has a web **user interface** (**UI**) for monitoring the cluster and Spark
    applications. This provides insights into stages of job execution, resource usage,
    and the execution environment. Other data platforms that incorporate Apache Spark
    have their own UIs.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平台用户界面**：除了我们在实践演练中看到的 Databricks Community Edition 的 Web 界面，开源 Apache Spark
    还提供一个 Web **用户界面**（**UI**），用于监控集群和 Spark 应用程序。它提供作业执行的阶段、资源使用情况和执行环境的洞察。其他集成了
    Apache Spark 的数据平台也有自己的 UI。'
- en: '**Application end user interface**: Another type of UI is for end users consuming
    the outcome of the processing by Apache Spark. This can be reporting tools or,
    for example, an application using Apache Spark in the backend for data processing.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用程序终端用户界面**：另一种 UI 是用于终端用户消费 Apache Spark 处理结果的界面。这可以是报告工具，或者例如在后端使用 Apache
    Spark 进行数据处理的应用程序。'
- en: In this section on Apache Spark architecture, we saw how the architecture enables
    data to be ingested from various sources into the Spark system, to be processed
    using Spark’s libraries, and then stored or served to users or downstream applications.
    The chosen architecture is dependent on requirements, such as latency, throughput,
    data size, and the complexity and type of data processing tasks. In the next section,
    we will focus on how Spark performs distributed processing at scale.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节关于 Apache Spark 架构的内容中，我们看到架构如何支持从各种来源将数据引入 Spark 系统，通过 Spark 的库进行处理，然后将结果存储或提供给用户或下游应用程序。所选架构依赖于需求，如延迟、吞吐量、数据大小以及数据处理任务的复杂性和类型。在下一节中，我们将重点讨论
    Spark 如何在大规模上执行分布式处理。
- en: How Apache Spark works
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 的工作原理
- en: So far in this chapter, we have viewed the components and their roles, but not
    so much about their interactions. We will now cover this part, to understand how
    Spark manages distributed data processing across a cluster, starting with transformations
    and actions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我们已经查看了各个组件及其角色，但对它们的交互了解还不多。接下来我们将讨论这一部分，以了解 Spark 如何在集群中管理分布式数据处理，从变换和操作开始。
- en: Transformations and actions
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换和操作
- en: 'Apache Spark does, at a high level, two types of data operations:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 在高层次上执行两种类型的数据操作：
- en: '`filter` and `groupBy`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter` 和 `groupBy`。'
- en: '`count` and `save` types of operations, such as writing to Parquet files or
    using the `saveAsTable` operation. Actions trigger the execution of all transformations
    defined as prior steps in the DAG. This results in Spark computing the result
    of the series of transformations.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count`和`save`类型的操作，如写入Parquet文件或使用`saveAsTable`操作。Action操作触发所有在DAG中定义的变换的执行，这导致Spark计算一系列变换的结果。'
- en: The distinction between transformations and actions is an important consideration
    when writing efficient Spark code. This enables Spark to use its execution engine
    for high-performance processing of jobs, which will be explained next.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 变换和动作之间的区别是编写高效Spark代码时需要考虑的重要问题。这使得Spark能够利用其执行引擎高效地处理作业，接下来将进一步解释。
- en: Jobs, stages, and tasks
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作业、阶段和任务
- en: 'Spark applications are executed as jobs, which are split into stages, and further
    into tasks, as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Spark应用程序作为作业执行，作业被拆分为多个阶段，再进一步拆分为多个任务，具体如下：
- en: '**Job**: Spark submits a job when an action is called on an RDD, DataFrame,
    or Dataset. The job is converted into a physical execution plan with several stages,
    which we will explain next. The purpose of a Spark job is to execute a sequence
    of computational steps as a logical unit of work to achieve a specific goal, such
    as aggregating data or sorting, with the aim of producing an output.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作业**：当在RDD、DataFrame或Dataset上调用Action时，Spark会提交一个作业。作业会转化为一个包含多个阶段的物理执行计划，接下来我们会解释这些阶段。Spark作业的目的是作为逻辑工作单元执行一系列计算步骤，以实现特定目标，比如聚合数据或排序，并最终生成输出。'
- en: '**Stage**: A job can have multiple stages, as defined in its physical execution
    plan. A stage is a group of contiguous tasks that can be completed without moving
    data across the cluster. The data movement between stages is referred to as shuffle.
    The separation of a job into stages is beneficial as shuffling is costly in terms
    of performance impact. A stage is further broken down into tasks, which we will
    look at next.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阶段**：一个作业可以有多个阶段，这些阶段在物理执行计划中定义。阶段是一组连续的任务，可以在不跨集群移动数据的情况下完成。阶段之间的数据移动称为洗牌（shuffle）。将作业拆分为多个阶段是有益的，因为洗牌在性能上开销较大。一个阶段进一步被拆分为任务，接下来我们将讨论任务。'
- en: '**Task**: As the most granular unit of processing, a task is a single operation
    on a Spark in-memory partition of data. Each task processes a different set of
    data and can run in parallel with other tasks. These run on worker nodes, which
    we will look at next.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务**：作为最小的处理单元，任务是在Spark内存中的数据分区上执行的单个操作。每个任务处理不同的数据集，并且可以与其他任务并行运行。这些任务在工作节点上运行，接下来我们将讨论工作节点。'
- en: In summary, jobs, stages, and tasks are related hierarchically. Spark applications
    can have multiple jobs, which are divided into stages based on data shuffling
    boundaries. Stages are further broken down into tasks, which run on different
    partitions in parallel on the cluster. This execution hierarchy allows Spark to
    efficiently distribute the workload across several nodes in a cluster, thus efficiently
    processing data at scale.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，作业、阶段和任务是层级相关的。Spark应用程序可以有多个作业，这些作业基于数据洗牌边界被划分为多个阶段。阶段进一步细分为任务，这些任务在集群的不同分区上并行运行。这样的执行层级使得Spark能够高效地将工作负载分配到集群的多个节点，从而在大规模数据处理时提高效率。
- en: Now that we have seen the units of processing, the next consideration is how
    these units are run on compute resources with driver and worker nodes.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了处理单元，接下来的问题是如何在计算资源上运行这些单元，包括驱动程序和工作节点。
- en: Driver and worker nodes
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 驱动程序和工作节点
- en: Driver and worker nodes are the compute resources created by the cluster manager
    to form part of a Spark cluster. They work together for Spark to process large
    datasets in parallel, using the resources of multiple machines.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序和工作节点是集群管理器创建的计算资源，用于组成一个Spark集群。它们协同工作，利用多台机器的资源并行处理大数据集。
- en: 'Let’s discuss these resources in detail:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论这些资源：
- en: '**Driver nodes**: The driver node is where the main process of a Spark application
    runs. It principally does the following:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**驱动节点**：驱动节点是Spark应用程序的主进程运行的地方，主要负责以下任务：'
- en: '**Resources**: The driver requests resources from the cluster manager for processes
    to run on the worker nodes.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源**：驱动程序请求集群管理器分配资源，以便在工作节点上运行进程。'
- en: '**SparkSession**: This is an object created by the driver and used to programmatically
    access Spark for data processing operations on the cluster.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SparkSession**：这是一个由驱动程序创建的对象，用于以编程方式访问Spark并在集群上进行数据处理操作。'
- en: '**Tasks**: The driver translates code into tasks, schedules the tasks on worker
    nodes, and thereafter manages the tasks’ execution.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务**：驱动节点将代码转化为任务，调度任务到工作节点上的执行器，并管理任务的执行。'
- en: '**Worker nodes**: The worker node is where the data processing happens, via
    what is called the executor process. The executors interact with the storage and
    keep the data in their own memory space, as well as having their own set of CPU
    cores. The tasks are scheduled by the driver nodes to execute on the executors
    with direct communication between drivers and executors. They communicate on task
    status and results.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作节点**：工作节点是数据处理的核心，数据通过所谓的执行器进程在工作节点上处理。执行器与存储交互，并将数据保存在自己的内存空间中，同时拥有自己的一组
    CPU 核心。任务由驱动节点调度到执行器上执行，驱动节点与执行器之间直接通信，传递任务状态和结果。'
- en: '**Driver and worker node interaction**: *Figure 3**.3* summarizes the sequence
    of interactions between driver and worker nodes.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**驱动节点和工作节点的交互**：*图 3.3* 总结了驱动节点和工作节点之间的交互顺序。'
- en: '![](img/B18568_03_3.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_03_3.jpg)'
- en: 'Figure 3.3: Driver and worker nodes in action'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：驱动节点和工作节点的工作示意图
- en: 'The steps are as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: '**Initialization**: When the Spark application is started, the driver converts
    jobs into stages, further broken into tasks.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化**：当 Spark 应用程序启动时，驱动程序将作业转换为阶段，并进一步拆分为任务。'
- en: '**Scheduling**: The driver node schedules tasks on executors on the worker
    nodes, keeping track of status and rescheduling in case of failure.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调度**：驱动节点在工作节点的执行器上调度任务，跟踪任务状态，并在发生故障时重新调度。'
- en: '**Execution**: The tasks assigned by the driver are run by the executor on
    the worker node. In addition, the driver coordinates between executors when data
    needs to be shuffled across executors. This is required for certain operations
    such as joins.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行**：驱动节点分配的任务由工作节点上的执行器运行。此外，当数据需要在执行器之间传递时，驱动节点协调执行器之间的操作。这对于某些操作（如联接）是必需的。'
- en: '**Result**: Finally, the results of processing tasks by the executors are sent
    back to the driver node, which aggregates the results and sends them back to the
    user.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**结果**：最终，执行器处理任务的结果被发送回驱动节点，驱动节点汇总结果并将其发送回用户。'
- en: This cooperative process between the driver and worker nodes is at the core
    of Spark, enabling data processing at scale, in parallel across a cluster, while
    handling fault tolerance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动节点和工作节点之间的这种协作过程是 Spark 的核心，它使得数据处理能够在集群中并行进行，并能够处理容错问题。
- en: Now that we have seen the workings of Spark clusters, let’s zoom in on what
    makes it even more performant and efficient.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Spark 集群的工作原理，让我们深入探讨是什么使它更加高效和优化。
- en: Catalyst optimizer and the Tungsten execution engine
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Catalyst 优化器和 Tungsten 执行引擎
- en: So far we’ve discussed that among the successive improvements brought to Apache
    Spark over the different versions, two notable ones are the Catalyst optimizer
    and the Tungsten execution engine. They play crucial roles in ensuring the Spark
    processes are optimized for fast execution time and efficient use of resources.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了在不同版本中对 Apache Spark 的持续改进，其中两个显著的改进是 Catalyst 优化器和 Tungsten 执行引擎。它们在确保
    Spark 过程优化、快速执行时间和高效资源利用方面发挥着关键作用。
- en: Catalyst optimizer
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Catalyst 优化器
- en: 'Introduced in Spark SQL, the Catalyst optimizer is a query optimization framework
    that significantly improves the performance of queries by using tree transformation
    on the **abstract syntax tree** (**AST**) of queries. It does this through several
    stages, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst 优化器是在 Spark SQL 中引入的一个查询优化框架，通过对查询的 **抽象语法树**（**AST**）进行树形转换，显著提高了查询性能。它通过多个阶段实现优化，具体如下：
- en: '**Analysis**: The query is transformed into a tree of operators called a logical
    plan.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分析**：查询被转化为一个名为逻辑计划的操作符树。'
- en: '**Logical optimization**: The optimizer uses rule-based transformations to
    optimize the logical plan.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**逻辑优化**：优化器使用基于规则的转换来优化逻辑计划。'
- en: '**Physical planning**: The logical plan is converted to physical plans, which
    are based on the choice of algorithm to use for the query operation.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**物理规划**：逻辑计划被转换为物理计划，物理计划是基于选择的算法来进行查询操作的。'
- en: '**Cost model**: The physical plans are then compared based on a cost model
    to find the most efficient one in terms of time and resources.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**成本模型**：然后基于成本模型比较物理计划，以找到在时间和资源上最有效的计划。'
- en: '**Code generation**: As a final stage, the physical plan is converted to executable
    code.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**代码生成**：作为最终阶段，物理计划被转换为可执行代码。'
- en: With these stages, the Catalyst optimizer ensures that the most performant and
    efficient code is run.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些阶段，Catalyst优化器确保运行最具性能和效率的代码。
- en: Tungsten execution engine
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Tungsten执行引擎
- en: 'Another area of focus is the efficient use of CPU and memory by Spark processes.
    The Tungsten execution engine achieves this in the following ways:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关注点是Spark进程对CPU和内存的高效利用。Tungsten执行引擎通过以下方式实现这一目标：
- en: '**Code generation**: Tungsten works in conjunction with the Catalyst optimizer
    to generate optimized, compact code, which reduces runtime overhead while maximizing
    speed.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码生成**：Tungsten与Catalyst优化器协作，生成优化且紧凑的代码，从而减少运行时开销，同时最大化速度。'
- en: '**Cache-awareness**: Reducing cache misses improves the computation speed.
    Tungsten achieves this by making algorithms and data structures cache-aware.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存意识**：减少缓存未命中可以提高计算速度。Tungsten通过使算法和数据结构具备缓存意识来实现这一点。'
- en: '**Memory management**: Tungsten manages memory efficiently, improving the impact
    of the cache while reducing the overhead of garbage collection.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存管理**：Tungsten高效管理内存，提高了缓存的影响力，同时减少了垃圾回收的开销。'
- en: Working together, the Catalyst optimizer and the Tungsten execution engine significantly
    contribute to Spark’s performance by optimizing query plans, generating efficient
    code, and reducing computation overhead. This improves Spark’s efficiency for
    big data processing, at scale and fast.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst优化器与Tungsten执行引擎共同合作，通过优化查询计划、生成高效代码以及减少计算开销，显著提高了Spark的性能。这提升了Spark在大数据处理中的效率，且具备可扩展性和高速性。
- en: Now that we understand how Apache Spark works, we will move on to how to set
    up our own Spark environment.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Apache Spark的工作原理，接下来将介绍如何设置我们自己的Spark环境。
- en: Installing Apache Spark
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Apache Spark
- en: So far, in the previous chapters, we have successfully executed Spark code on
    Databricks Community Edition. This has, however, been on a single-node cluster.
    If we want to make full use of Spark’s parallel processing power, we will need
    multiple nodes. We have the option of using a Databricks-managed **Platform as
    a Service** (**PaaS**) cloud solution, another equivalent cloud PaaS, or we can
    build our own Apache Spark platform. This is what we will do now to deploy the
    environment as per *Figure 3**.2* shown in the section on *Apache* *Spark architecture*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在前面的章节中，我们已成功在Databricks Community Edition上执行了Spark代码。然而，这仅限于单节点集群。如果我们希望充分利用Spark的并行处理能力，就需要多节点集群。我们可以选择使用Databricks管理的**平台即服务**（**PaaS**）云解决方案，或其他等效的云PaaS，或者我们可以构建自己的Apache
    Spark平台。这正是我们现在要做的，按照*图3.2*中展示的*Apache Spark架构*来部署环境。
- en: Note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you do not intend to build your own Apache Spark environment, you can skip
    the practical part of this section and use a managed Spark platform such as Databricks,
    as we will do in future chapters.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不打算构建自己的Apache Spark环境，可以跳过本节的实践部分，改为使用受管Spark平台，如Databricks，我们将在未来的章节中使用。
- en: Using a container for deployment
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用容器进行部署
- en: We can install Apache Spark directly on our local machine, but this will give
    us only one node. By deploying it in containers, such as Docker, we can have multiple
    containers running on the same machine. This effectively provides us with a way
    to have a multi-node cluster. Other advantages of this method include maintaining
    separation with the local execution environment, as well as providing a portable
    and repeatable way to deploy to other machines, including to cloud-based container
    services such as Amazon **Elastic Kubernetes Service** (**EKS**), **Azure Kubernetes
    Service** (**AKS**), or **Google Kubernetes** **Engine** (**GKE**).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接在本地机器上安装Apache Spark，但这将只给我们一个节点。通过将其部署在容器中，如Docker，我们可以在同一台机器上运行多个容器。这有效地为我们提供了一种方法来构建一个多节点集群。这种方法的其他优势包括与本地执行环境的隔离，以及提供一种可移植且可重复的方式，将其部署到其他机器上，包括如Amazon
    **弹性Kubernetes服务**（**EKS**）、**Azure Kubernetes服务**（**AKS**）或**Google Kubernetes引擎**（**GKE**）等基于云的容器服务。
- en: In what follows, we will be using Docker containers, starting by first installing
    Docker, then building and starting the containers with Apache Spark, and finally
    validating our deployment.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将使用Docker容器，首先安装Docker，然后构建并启动包含Apache Spark的容器，最后验证我们的部署。
- en: Alternative to Docker
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Docker替代方案
- en: 'You can use Podman as an open source alternative to Docker. See more information
    here: [https://podman.io/](https://podman.io/).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Podman作为Docker的开源替代方案。请在此查看更多信息：[https://podman.io/](https://podman.io/)。
- en: Docker
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Docker
- en: 'The following instructions guide you on how to install Docker:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下说明指导你如何安装Docker：
- en: 'Refer to the following link to download and install Docker to your local environment,
    based on your OS:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请参考以下链接，根据你的操作系统下载并安装Docker：
- en: '[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)'
- en: 'For macOS users, follow the instructions here:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于macOS用户，请按照此处的说明操作：
- en: '[https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/)'
- en: Once Docker is installed, launch it as shown in *Figure 3**.4*.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦Docker安装完成，按*图3.4*所示启动它。
- en: '![](img/B18568_03_4.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18568_03_4.jpg)'
- en: 'Figure 3.4: Docker Desktop'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：Docker Desktop
- en: 'On macOS, you may see a Docker Desktop warning: “**Another application changed
    your Desktop configurations**”. Depending on your setup, the following command
    may resolve the warning:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在macOS上，你可能会看到Docker Desktop的警告：“**另一个应用程序更改了你的桌面配置**”。根据你的设置，以下命令可能解决此警告：
- en: '[PRE0]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once Docker Desktop is up and running, we can build the containers with Apache
    Spark.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Docker Desktop启动并运行，我们可以使用Apache Spark构建容器。
- en: Network ports
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络端口
- en: 'The following network ports need to be available on your local machine or development
    environment:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下网络端口需要在本地机器或开发环境中可用：
- en: 'Apache Spark: `7077`, `8080`, `8081`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark：`7077`，`8080`，`8081`
- en: 'Jupyter Notebook: `4040`, `4041`, `4042`, `8888`'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter Notebook：`4040`，`4041`，`4042`，`8888`
- en: 'You can check for the current use of these ports by existing applications with
    the following command, run from the command line or terminal:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令检查当前端口是否被现有应用程序占用，在命令行或终端中运行：
- en: '[PRE1]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you see the required ports in the list of ports already in use, you must
    either stop the application using that port or change the docker-compose file
    to use another port.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在已使用端口的列表中看到所需端口，你必须停止使用该端口的应用程序，或者修改`docker-compose`文件以使用其他端口。
- en: As an example, let’s assume that the output of the above `netstat` command reveals
    that port `8080` is already in use on your local machine or development environment,
    and you are not able to stop the existing application using this port.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，假设上述`netstat`命令的输出显示本地机器或开发环境中的端口`8080`已经在使用，并且你无法停止正在使用该端口的现有应用程序。
- en: 'In this case, you will need to change port `8080` (meant for Apache Spark)
    in the `docker-compose.yaml` file to another, unused port. Just search and replace
    `8080` on the left of `:` to, say, `8070` if this port is free, as per the following
    example:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你需要将`docker-compose.yaml`文件中用于Apache Spark的端口`8080`更改为另一个未使用的端口。只需在`:`左侧查找并替换`8080`为例如`8070`，前提是该端口未被占用，如以下示例所示：
- en: 'From:'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自：
- en: '[PRE2]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到：
- en: '[PRE3]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Keep note of the new port and use this instead of the existing one whenever
    you need to type the corresponding URL. In this example, port `8080` is changed
    to `8070`, and the matching URL change for the Airflow web server is as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 记下新端口，并在需要输入相应URL时使用此端口替代现有端口。在此示例中，端口`8080`已更改为`8070`，Airflow Web服务器的匹配URL变更如下：
- en: 'From: [http://localhost:8080/](http://localhost:8080/)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自：[http://localhost:8080/](http://localhost:8080/)
- en: 'To: [http://localhost:8070/](http://localhost:8070/)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到：[http://localhost:8070/](http://localhost:8070/)
- en: Note
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You will need to change the network port in all URLs in the following sections
    that you had to modify as per this section.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要更改以下各节中所有需要修改的URL中的网络端口，以配合本节内容。
- en: Building and deploying Apache Spark
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建并部署Apache Spark
- en: 'The following instructions guide you on how to build and deploy the Docker
    images:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下说明指导你如何构建和部署Docker镜像：
- en: 'We first download the deployment script from the Git repository for this chapter,
    which is at the following URL:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从本章的Git仓库下载部署脚本，网址如下：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch3](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch3)'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch3](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch3)'
- en: 'We will be using the git clone-friendly URL, which is the following:'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将使用适合git克隆的URL，具体如下：
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git)'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git)'
- en: 'To do this, start a terminal or command line and run the following commands:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要做到这一点，启动终端或命令行并运行以下命令：
- en: '[PRE4]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that the preceding is for a macOS or Linux/Unix-based system, and you will
    need to run the equivalent for Windows.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，上述命令适用于 macOS 或基于 Linux/Unix 的系统，您需要运行适用于 Windows 的等效命令。
- en: 'On macOS, you may see the following error when you run this command:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 macOS 上，当您运行此命令时，可能会看到以下错误：
- en: '[PRE5]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this case, you will need to reinstall the command-line tools with the following
    command:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，您需要使用以下命令重新安装命令行工具：
- en: '[PRE6]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can now start the container build and startup. A makefile is provided to
    simplify the process of starting and stopping the containers. The following command
    builds the Docker images for the containers and then starts them:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以开始容器的构建和启动。提供了一个 Makefile 来简化启动和停止容器的过程。以下命令构建容器的 Docker 镜像并启动它们：
- en: '[PRE7]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Windows environment
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Windows 环境
- en: 'If you are using a Windows environment, you can install a Windows version of
    Make, as per the following documentation: [https://gnuwin32.sourceforge.net/packages/make.htm](https://gnuwin32.sourceforge.net/packages/make.htm)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是 Windows 环境，可以根据以下文档安装 Windows 版本的 Make：[https://gnuwin32.sourceforge.net/packages/make.htm](https://gnuwin32.sourceforge.net/packages/make.htm)
- en: 'This will give the following or equivalent output:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下或等效的输出：
- en: '[PRE8]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: make down
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: make down
- en: '[PRE9]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Part 2: From Data to Models'
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：从数据到模型
- en: Building on the foundations, in this part, you will get a holistic view of all
    the stages involved in a time series analysis project, with a focus on the data
    and models. Starting with the ingestion and preparation of time series data, we
    will then do exploratory analysis to understand the nature of the time series.
    The data readiness and analysis will then lead us to the choice of model for analysis,
    development, and testing.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，本部分将提供时间序列分析项目中涉及的所有阶段的整体视图，重点关注数据和模型。从时间序列数据的导入和准备开始，我们将进行探索性分析，以了解时间序列的性质。数据准备和分析将引导我们选择用于分析、开发和测试的模型。
- en: 'This part has the following chapters:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 4*](B18568_04.xhtml#_idTextAnchor087), *End-to-End View of a Time
    Series Analysis Project*'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B18568_04.xhtml#_idTextAnchor087)，*时间序列分析项目的端到端视图*'
- en: '[*Chapter 5*](B18568_05.xhtml#_idTextAnchor103), *Data Preparation*'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B18568_05.xhtml#_idTextAnchor103)，*数据准备*'
- en: '[*Chapter 6*](B18568_06.xhtml#_idTextAnchor116), *Exploratory Data Analysis*'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B18568_06.xhtml#_idTextAnchor116)，*探索性数据分析*'
- en: '[*Chapter 7*](B18568_07.xhtml#_idTextAnchor133), *Building and Testing Models*'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B18568_07.xhtml#_idTextAnchor133)，*构建与测试模型*'
