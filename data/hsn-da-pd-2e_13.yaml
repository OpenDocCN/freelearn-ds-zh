- en: '*Chapter 9*: Getting Started with Machine Learning in Python'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：在Python中入门机器学习'
- en: This chapter will expose us to the vernacular of machine learning and the common
    tasks that machine learning can be used to solve. Afterward, we will learn how
    we can prepare our data for use in machine learning models. We have discussed
    data cleaning already, but only for human consumption—machine learning models
    require different `scikit-learn` to build preprocessing pipelines that streamline
    this procedure, since our models will only be as good as the data they are trained
    on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将让我们了解机器学习的术语及其常见任务。之后，我们将学习如何准备数据以供机器学习模型使用。我们已经讨论过数据清洗，但仅限于供人类使用——机器学习模型需要使用`scikit-learn`构建预处理管道，以简化这一过程，因为我们的模型好坏取决于其训练的数据。
- en: Next, we will walk through how we can use `scikit-learn` to build a model and
    evaluate its performance. Scikit-learn has a very user-friendly API, so once we
    know how to build one model, we can build any number of them. We won't be going
    into any of the mathematics behind the models; there are entire books on this,
    and the goal of this chapter is to serve as an introduction to the topic. By the
    end of this chapter, we will be able to identify what type of problem we are looking
    to solve and some algorithms that can help us, as well as how to implement them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讲解如何使用`scikit-learn`构建模型并评估其表现。Scikit-learn具有非常友好的API，因此一旦我们了解如何构建一个模型，就能构建任意数量的模型。我们不会深入探讨模型背后的数学原理，因为这方面有专门的书籍，
    本章的目标是作为该主题的入门介绍。到本章结束时，我们将能够识别我们希望解决的是什么问题，以及可以帮助我们的算法类型，并且如何实现它们。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Overview of the machine learning landscape
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习领域概览
- en: Performing exploratory data analysis using skills learned in previous chapters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用前几章学习的技能进行探索性数据分析
- en: Preprocessing data for use in a machine learning model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理数据以便用于机器学习模型
- en: Clustering to help understand unlabeled data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用聚类帮助理解无标签数据
- en: Learning when regression is appropriate and how to implement it with scikit-learn
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习回归何时合适以及如何使用scikit-learn实现回归
- en: Understanding classification tasks and learning how to use logistic regression
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分类任务并学习如何使用逻辑回归
- en: Chapter materials
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章材料
- en: In this chapter, we will be working with three datasets. The first two come
    from data on wine quality that was donated to the UCI Machine Learning Data Repository
    ([http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php))
    by P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis, which contains information
    on the chemical properties of various wine samples, along with a rating of the
    quality from a blind tasting by a panel of wine experts. These files can be found
    in the `data/` folder inside this chapter's folder in the GitHub repository ([https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_09](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_09))
    as `winequality-red.csv` and `winequality-white.csv` for red and white wine, respectively.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用三个数据集。前两个数据集来自于UCI机器学习数据集库（[http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)）提供的关于葡萄酒质量的数据，这些数据由P.
    Cortez、A. Cerdeira、F. Almeida、T. Matos和J. Reis捐赠，其中包含了不同葡萄酒样本的化学性质信息，以及来自葡萄酒专家盲测小组对其质量的评分。这些文件可以在GitHub仓库中本章文件夹下的`data/`文件夹中找到，分别为红葡萄酒和白葡萄酒的`winequality-red.csv`和`winequality-white.csv`。
- en: Our third dataset was collected using the Open Exoplanet Catalogue database,
    which can be found at [https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/](https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/).
    This database provides data in `planet_data_collection.ipynb` notebook on GitHub
    contains the code that was used to parse this information into the CSV files we
    will use in this chapter; while we won't be going over this explicitly, I encourage
    you to take a look at it. The data files can be found in the `data/` folder, as
    well. We will use `planets.csv` for this chapter; however, the parsed data for
    the other hierarchies is provided for exercises and further exploration. These
    are `binaries.csv`, `stars.csv`, and `systems.csv`, which contain data on binaries
    (stars or binaries forming a group of two), data on a single star, and data on
    planetary systems, respectively.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第三个数据集是使用开放系外行星目录（Open Exoplanet Catalogue）数据库收集的，您可以在[https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/](https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/)找到该数据库。该数据库提供了在`planet_data_collection.ipynb`笔记本中的数据，GitHub上包含了用于将这些信息解析为CSV文件的代码，我们将在本章中使用这些文件；虽然我们不会详细讨论这部分内容，但我鼓励您去查看它。数据文件也可以在`data/`文件夹中找到。我们将在本章中使用`planets.csv`，不过为进行练习和进一步探索，还提供了其他层级的解析数据。这些文件包括`binaries.csv`（包含双星的数据）、`stars.csv`（包含单颗星的数据）和`systems.csv`（包含行星系统的数据）。
- en: We will be using the `red_wine.ipynb` notebook to predict red wine quality,
    the `wine.ipynb` notebook to classify wines as red or white based on their chemical
    properties, and the `planets_ml.ipynb` notebook to build a regression model to
    predict the year length of planets and perform clustering to find similar planet
    groups. We will use the `preprocessing.ipynb` notebook for the section on preprocessing.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`red_wine.ipynb`笔记本来预测红酒质量，使用`wine.ipynb`笔记本根据红酒的化学特性将其分类为红酒或白酒，使用`planets_ml.ipynb`笔记本构建回归模型来预测行星的年长度，并进行聚类分析以找到相似的行星群体。我们将使用`preprocessing.ipynb`笔记本进行预处理部分的工作。
- en: Back in [*Chapter 1*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015), *Introduction
    to Data Analysis*, when we set up our environment, we installed a package from
    GitHub called `ml_utils`. This package contains utility functions and classes
    that we will use for our three chapters on machine learning. Unlike the last two
    chapters, we won't be discussing how to make this package; however, those interested
    can look through the code at https://github.com/stefmolin/ml-utils/tree/2nd_edition
    and follow the instructions from [*Chapter 7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146),
    *Financial Analysis – Bitcoin and the Stock Market*, to install it in editable
    mode.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[*第1章*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015)，*数据分析导论*，当我们设置环境时，我们安装了一个来自GitHub的名为`ml_utils`的包。这个包包含了我们将在接下来的三章机器学习中使用的工具函数和类。与前两章不同，我们不会讨论如何构建这个包；不过，感兴趣的人可以浏览[https://github.com/stefmolin/ml-utils/tree/2nd_edition](https://github.com/stefmolin/ml-utils/tree/2nd_edition)中的代码，并按照[*第7章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146)，*金融分析——比特币与股市*，中的说明，以可编辑模式安装它。
- en: 'The following are the reference links for the data sources:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数据源的参考链接：
- en: '*Open Exoplanet Catalogue database*, available at [https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure](https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开放系外行星目录（Open Exoplanet Catalogue）数据库*，可在[https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure](https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure)找到。'
- en: '*P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences
    by data mining from physicochemical properties. In Decision Support Systems, Elsevier,
    47(4):547-553, 2009.* Available online at [http://archive.ics.uci.edu/ml/datasets/Wine+Quality](http://archive.ics.uci.edu/ml/datasets/Wine+Quality).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P. Cortez, A. Cerdeira, F. Almeida, T. Matos 和 J. Reis. 通过数据挖掘物理化学特性来建模葡萄酒偏好.
    见《决策支持系统》，Elsevier，47(4):547-553, 2009.* 该文档可在线查看：[http://archive.ics.uci.edu/ml/datasets/Wine+Quality](http://archive.ics.uci.edu/ml/datasets/Wine+Quality)。'
- en: '*Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [*[http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)*].
    Irvine, CA: University of California, School of Information and Computer Science.*'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dua, D. 和 Karra Taniskidou, E. (2017). UCI机器学习库 [*[http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)*]。加利福尼亚州尔湾：加利福尼亚大学信息与计算机科学学院。*'
- en: Overview of the machine learning landscape
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习概览
- en: '**Machine learning** is a subset of **artificial intelligence** (**AI**) whereby
    an algorithm can learn to predict values from input data without explicitly being
    taught rules. These algorithms rely on statistics to make inferences as they learn;
    they then use what they learn to make predictions.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**是**人工智能**（**AI**）的一个子集，其中算法可以从输入数据中学习预测值，而无需明确教授规则。这些算法依赖统计学进行推断，并在学习过程中使用所学内容来做出预测。'
- en: Applying for a loan, using a search engine, sending a robot vacuum to clean
    a specific room with a voice command—machine learning can be found everywhere
    we look. This is because it can be used for many purposes, for example, voice
    recognition by AI assistants such as Alexa, Siri, or Google Assistant, mapping
    floor plans by exploring surroundings, determining who will default on a loan,
    figuring out which search results are relevant, and even painting ([https://www.boredpanda.com/computer-deep-learning-algorithm-painting-masters/](https://www.boredpanda.com/computer-deep-learning-algorithm-painting-masters/)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 申请贷款、使用搜索引擎、通过语音命令让机器人吸尘器清洁特定房间——机器学习在我们周围随处可见。这是因为它可以用于许多目的，例如AI助手（如Alexa、Siri或Google
    Assistant）的语音识别、通过探索周围环境绘制楼层图、判断谁会违约贷款、确定哪些搜索结果相关，甚至进行绘画（[https://www.boredpanda.com/computer-deep-learning-algorithm-painting-masters/](https://www.boredpanda.com/computer-deep-learning-algorithm-painting-masters/))。
- en: Machine learning models can be made to adapt to changes in the input over time
    and are a huge help in making decisions without the need for a human each time.
    Think about applying for a loan or a credit line increase on a credit card; the
    bank or credit card company will rely on a machine learning algorithm to look
    up things from the applicant's credit score and history with them to determine
    whether the applicant should be approved. Most likely, they will only approve
    the applicant at that moment if the model predicts a strong chance he or she can
    be trusted with the loan or new credit limit. In the case where the model can't
    be so sure, they can send it over to a human to make the final decision. This
    reduces the amount of applications employees have to sift through to just the
    borderline cases, while also providing faster answers for those non-borderline
    cases (the process can be nearly instantaneous).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型可以随着时间的推移适应输入的变化，并且在做出决策时，减少每次都需要人工干预的情况。想象一下申请贷款或信用卡额度提升；银行或信用卡公司会依赖机器学习算法，查看申请人的信用评分及历史记录，以确定是否批准申请。很可能，只有当模型预测出申请人有较高的可信度时，银行才会在此时批准申请。如果模型不能做出如此确定的判断，则可以将决策交给人工处理。这减少了员工需要筛选的申请数量，确保只有边缘案例需要人工处理，同时也能为非边缘案例提供更快的响应（处理过程几乎可以瞬时完成）。
- en: One important thing to call out here is that models that are used for tasks
    such as loan approvals, by law, have to be interpretable. There needs to be a
    way to explain to the applicant why they were rejected—sometimes, reasons beyond
    technology can influence and limit what approaches or data we use.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要特别提到的一点是，用于贷款批准等任务的模型，根据法律规定，必须具备可解释性。必须有一种方式向申请人解释他们被拒绝的原因——有时，技术之外的原因可能会影响和限制我们使用的方案或数据。
- en: Types of machine learning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习的类型
- en: 'Machine learning is typically divided into three categories: unsupervised learning,
    supervised learning, and reinforcement learning. We use **unsupervised learning**
    when we don''t have labeled data telling us what our model should say for each
    data point. In many cases, gathering labeled data is costly or just not feasible,
    so unsupervised learning will be used. Note that it is more difficult to optimize
    the performance of these models because we don''t know how well they are performing.
    If we do have access to the labels, we can use **supervised learning**; this makes
    it much easier for us to evaluate our models and look to improve them since we
    can calculate metrics on their performance compared to the true labels.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习通常分为三类：无监督学习、监督学习和强化学习。当我们没有标签数据指示每个数据点应对应的模型输出时，我们使用**无监督学习**。在许多情况下，收集标签数据成本高昂或根本不可行，因此会使用无监督学习。需要注意的是，优化这些模型的性能更为困难，因为我们不知道它们的表现如何。如果我们可以访问标签数据，就可以使用**监督学习**；这使得评估和改进我们的模型变得更加容易，因为我们可以根据模型与真实标签的比较来计算其性能指标。
- en: Tip
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Since unsupervised learning looks to find meaning in the data without a correct
    answer, it can be used to learn more about the data as a part of the analysis
    or before moving on to supervised learning.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无监督学习旨在从数据中找到意义，而不依赖正确答案，它可以在数据分析过程中或在进行有监督学习之前用于更好地理解数据。
- en: '**Reinforcement learning** is concerned with reacting to feedback from the
    environment; this is used for things such as robots and AI in games. It is well
    beyond the scope of this book, but there are resources in the *Further reading*
    section for more information.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**关注的是如何对来自环境的反馈做出反应；这通常用于机器人和游戏中的人工智能等应用。尽管这一部分超出了本书的范围，但在*进一步阅读*部分有相关资源可以获取更多信息。'
- en: Note that not all machine learning approaches fit neatly into the aforementioned
    categories. One example is **deep learning**, which aims to learn data representations
    using methods such as **neural networks**. Deep learning methods are often seen
    as black boxes, which has prevented their use in certain domains where interpretable
    models are required; however, they are used for tasks such as speech recognition
    and image classification. Deep learning is also beyond the scope of this book,
    but it is good to be aware that it is also machine learning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，并非所有的机器学习方法都可以完全归入上述类别。例如，**深度学习**旨在使用**神经网络**等方法学习数据表示。深度学习方法通常被视为“黑箱”，这使得它们在某些需要可解释模型的领域中的应用受限；然而，它们已被广泛应用于语音识别和图像分类等任务中。深度学习超出了本书的讨论范围，但了解它也是机器学习的一部分是很有帮助的。
- en: Important note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Interpretable machine learning is an active area of research. Check out the
    resources in the *Further reading* section for more information.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释机器学习是当前一个活跃的研究领域。有关更多信息，请查看*进一步阅读*部分的资源。
- en: Common tasks
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见任务
- en: The most common machine learning tasks are clustering, classification, and regression.
    In **clustering**, we look to assign data into groups, with the goal being that
    the groups are well-defined, meaning that members of the group are close together
    and groups are separated from other groups. Clustering can be used in an unsupervised
    manner in an attempt to gain a better understanding of the data, or in a supervised
    manner to try to predict which cluster data belongs to (essentially classification).
    Note that clustering can be used for prediction in an unsupervised manner; however,
    we will need to decipher what each cluster means. Labels that are obtained from
    clustering can even be used as the input for a supervised learner to model how
    observations are mapped to each group; this is called **semi-supervised learning**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的机器学习任务有聚类、分类和回归。在**聚类**中，我们希望将数据划分为不同的组，目标是确保组内的数据密切相关且组与组之间相互分离。聚类可以以无监督的方式进行，以帮助更好地理解数据，或者以有监督的方式进行，尝试预测数据属于哪个组（本质上是分类）。需要注意的是，聚类也可以用于无监督的预测；然而，我们仍然需要解读每个簇的含义。从聚类中获得的标签甚至可以作为有监督学习的输入，帮助模型学习如何将观察结果映射到各个组，这种方法被称为**半监督学习**。
- en: '**Classification**, as we discussed in the previous chapter, looks to assign
    a class label to the data, such as *benign* or *malicious*. This may sound like
    assigning it to a cluster, however, we aren''t worried about how similar the values
    that are assigned to *benign* are, just marking them as *benign*. Since we are
    assigning to a class or category, this class of models is used to predict a discrete
    label. **Regression**, on the other hand, is for predicting numeric values, such
    as housing prices or book sales; it models the strength and magnitude of the relationships
    between variables. Both can be performed as unsupervised or supervised learning;
    however, supervised models are more likely to perform better.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类**，如我们在上一章所讨论的，旨在为数据分配一个类标签，例如*良性*或*恶意*。这听起来像是将数据分配到某个簇中，然而，我们并不关心被标为*良性*的数据之间的相似度，只需要将它们标记为*良性*。由于我们是将数据分配到某个类别或类中，因此这类模型用于预测离散标签。而**回归**则是用于预测数值型数据，例如房价或图书销量；它用于建模变量之间的关系强度和大小。两者都可以作为无监督或有监督学习来进行；然而，有监督模型通常表现得更好。'
- en: Machine learning in Python
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python 中的机器学习
- en: 'Now that we know what machine learning is, we need to know how we can build
    our own models. Python offers many packages for building machine learning models;
    some libraries we should be aware of include the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了什么是机器学习，接下来我们需要了解如何构建自己的模型。Python 提供了许多用于构建机器学习模型的包；我们需要关注的一些库包括以下内容：
- en: '`scikit-learn`: Easy to use (and learn), it features a consistent API for machine
    learning in Python ([https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html))'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`：易于使用（也容易学习），它提供了一个一致的Python机器学习API（[https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html)）'
- en: '`statsmodels`: A statistical modeling library that also provides statistical
    tests ([https://www.statsmodels.org/stable/index.html](https://www.statsmodels.org/stable/index.html))'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`statsmodels`：一个统计建模库，提供统计测试功能（[https://www.statsmodels.org/stable/index.html](https://www.statsmodels.org/stable/index.html)）'
- en: '`tensorflow`: A machine learning library developed by Google that features
    faster calculations ([https://www.tensorflow.org/](https://www.tensorflow.org/))'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow`：由Google开发的机器学习库，具有更快的计算速度（[https://www.tensorflow.org/](https://www.tensorflow.org/)）'
- en: '`keras`: A high-level API for running deep learning from libraries such as
    TensorFlow ([https://keras.io/](https://keras.io/))'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keras`：用于运行深度学习的高级API，支持如TensorFlow等库（[https://keras.io/](https://keras.io/)）'
- en: '`pytorch`: A deep learning library developed by Facebook ([https://pytorch.org](https://pytorch.org))'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch`：由Facebook开发的深度学习库（[https://pytorch.org](https://pytorch.org)）'
- en: Tip
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示
- en: Most of these libraries use NumPy and SciPy, a library built on top of NumPy
    for statistics, mathematics, and engineering purposes. SciPy can be used to handle
    linear algebra, interpolation, integration, and clustering algorithms, among other
    things. More information on SciPy can be found at [https://docs.scipy.org/doc/scipy/reference/tutorial/general.html](https://docs.scipy.org/doc/scipy/reference/tutorial/general.html).
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些库大多数使用了NumPy和SciPy，SciPy是基于NumPy构建的一个库，用于统计学、数学和工程目的。SciPy可以用于处理线性代数、插值、积分和聚类算法等内容。更多关于SciPy的信息可以在[https://docs.scipy.org/doc/scipy/reference/tutorial/general.html](https://docs.scipy.org/doc/scipy/reference/tutorial/general.html)找到。
- en: In this book, we will be using `scikit-learn` for its user-friendly API. In
    `scikit-learn`, our base class is an `fit()` method. We use `transform()` method—transforming
    the data into something `predict()` method. The `score()` method. Knowing just
    these four methods, we can easily build any machine learning model offered by
    `scikit-learn`. More information on this design pattern can be found at [https://scikit-learn.org/stable/developers/develop.html](https://scikit-learn.org/stable/developers/develop.html).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用`scikit-learn`，因为它的API用户友好。在`scikit-learn`中，我们的基类是`fit()`方法。我们使用`transform()`方法—将数据转换为可以由`predict()`方法使用的形式。`score()`方法也是常用的。仅了解这四个方法，我们就能轻松构建`scikit-learn`提供的任何机器学习模型。有关该设计模式的更多信息，请参考[https://scikit-learn.org/stable/developers/develop.html](https://scikit-learn.org/stable/developers/develop.html)。
- en: Exploratory data analysis
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: As we have learned throughout this book, our first step should be to engage
    in some **exploratory data analysis** (**EDA**) to get familiar with our data.
    In the interest of brevity, this section will include a subset of the EDA that's
    available in each of the notebooks—be sure to check out the respective notebooks
    for the full version.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中学到的那样，第一步应该进行一些**探索性数据分析**（**EDA**），以便熟悉我们的数据。为了简洁起见，本节将包括每个笔记本中可用的EDA的一个子集—请确保查看相关笔记本，以获取完整版本。
- en: Tip
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: While we will use `pandas` code to perform our EDA, be sure to check out the
    `pandas-profiling` package ([https://github.com/pandas-profiling/pandas-profiling](https://github.com/pandas-profiling/pandas-profiling)),
    which can be used to quickly perform some initial EDA on the data via an interactive
    HTML report.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将使用`pandas`代码来执行EDA，但也请查看`pandas-profiling`包（[https://github.com/pandas-profiling/pandas-profiling](https://github.com/pandas-profiling/pandas-profiling)），它可以通过交互式HTML报告快速执行数据的初步EDA。
- en: 'Let''s start with our imports, which will be the same across the notebooks
    we will use in this chapter:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入库开始，这将在本章中我们使用的所有笔记本中保持一致：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will start our EDA with the wine quality data before moving on to the planets.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在进行行星数据分析之前，首先对红酒质量数据进行探索性数据分析（EDA）。
- en: Red wine quality data
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 红酒质量数据
- en: 'Let''s read in our red wine data and do some EDA using techniques we have learned
    throughout this book:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取红酒数据并利用本书中学到的技巧进行一些EDA：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We have data on 11 different chemical properties of red wine, along with a
    column indicating the quality score from the wine experts that participated in
    the blind taste testing. We can try to predict the quality score by looking at
    the chemical properties:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有关于红酒的11种不同化学属性的数据，以及一个列出参与盲品测试的酒类专家评分的列。我们可以通过观察这些化学属性来预测质量评分：
- en: '![Figure 9.1 – Red wine dataset'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1 – 红酒数据集'
- en: '](img/Figure_9.1_B16834.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.1_B16834.jpg)'
- en: Figure 9.1 – Red wine dataset
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 红葡萄酒数据集
- en: 'Let''s see what the distribution of the `quality` column looks like:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`quality`列的分布情况：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The information on the dataset says that `quality` varies from 0 (terrible)
    to 10 (excellent); however, we only have values in the middle of that range. An
    interesting task for this dataset could be to see if we can predict high-quality
    red wines (a quality score of 7 or higher):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的信息显示`quality`评分从0（非常差）到10（非常好）不等；然而，我们的数据中只有这个范围中间的值。对于这个数据集，一个有趣的任务是看我们是否能预测高质量的红葡萄酒（质量评分为7或更高）：
- en: '![Figure 9.2 – Distribution of red wine quality scores'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.2 – 红葡萄酒质量评分分布](img/Figure_9.2_B16834.jpg)'
- en: '](img/Figure_9.2_B16834.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.2_B16834.jpg)'
- en: Figure 9.2 – Distribution of red wine quality scores
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 红葡萄酒质量评分分布
- en: 'All of our data is numeric, so we don''t have to worry about handling text
    values; we also don''t have any missing values:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据都是数值型数据，所以我们不需要担心处理文本值；并且数据中没有缺失值：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can use `describe()` to get an idea of what scale each of the columns is
    on:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`describe()`来了解每一列数据的尺度：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The result indicates that we will definitely have to do some scaling if our
    model uses distance metrics for anything because our columns aren''t all on the
    same range:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，如果我们的模型使用任何距离度量方法，我们肯定需要对数据进行缩放，因为我们的各列数据范围不一致：
- en: '![Figure 9.3 – Summary statistics for the red wine dataset'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.3 – 红葡萄酒数据集的总结统计](img/Figure_9.3_B16834.jpg)'
- en: '](img/Figure_9.3_B16834.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.3_B16834.jpg)'
- en: Figure 9.3 – Summary statistics for the red wine dataset
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 红葡萄酒数据集的总结统计
- en: 'Lastly, let''s use `pd.cut()` to bin our high-quality red wines (roughly 14%
    of the data) for later:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用`pd.cut()`将高质量的红葡萄酒（大约占数据的14%）分箱，方便后续使用：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Important note
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: 'We are stopping our EDA here for brevity; however, we should make sure to fully
    explore our data and consult domain experts before attempting any modeling. One
    thing to pay particular attention to is correlations between variables and what
    we are trying to predict (high-quality red wine, in this case). Variables with
    strong correlations may be good features to include in a model. However, note
    that correlation does not imply causation. We already learned a few ways to use
    visualizations to look for correlations: the scatter matrix we discussed in [*Chapter
    5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106), *Visualizing Data with Pandas
    and Matplotlib*, and the heatmap and pair plot from [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*. A pair plot is included
    in the `red_wine.ipynb` notebook.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们在此停止了EDA（探索性数据分析）；然而，在尝试任何建模之前，我们应该确保充分探索数据，并咨询领域专家。特别需要注意的一点是变量之间的相关性以及我们试图预测的目标（在此案例中是高质量的红酒）。与目标变量（高质量红酒）有强相关性的变量可能是模型中很好的特征。然而，请注意，相关性并不意味着因果关系。我们已经学到了几种使用可视化来查找相关性的方法：我们在[*第5章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106)《使用Pandas和Matplotlib进行数据可视化》中讨论过的散点矩阵，以及在[*第6章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)《使用Seaborn和定制化技巧绘图》中的热力图和对比图。对比图已包含在`red_wine.ipynb`笔记本中。
- en: White and red wine chemical properties data
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 白葡萄酒和红葡萄酒的化学性质数据
- en: 'Now, let''s look at the red and white wine data together. Since the data comes
    in separate files, we need to read in both and concatenate them into a single
    dataframe. The white wine file is actually semi-colon (`;`) separated, so we must
    provide the `sep` argument to `pd.read_csv()`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将红葡萄酒和白葡萄酒的数据一起查看。由于数据来自不同的文件，我们需要读取两个文件并将其合并为一个数据框。白葡萄酒文件实际上是分号（`;`）分隔的，因此我们必须在`pd.read_csv()`中提供`sep`参数：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can also look at the quality scores of the white wines, just as we did with
    the red ones, and we will find that the white wines tend to be rated higher overall.
    This might bring us to question whether the judges preferred white wine over red
    wine, thus creating a bias in their ratings. As it is, the rating system that
    was used seems to be pretty subjective:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看白葡萄酒的质量评分，就像我们查看红葡萄酒一样，我们会发现白葡萄酒的评分普遍较高。这可能让我们质疑评委是否更偏好白葡萄酒，从而在评分中产生偏差。就目前而言，所使用的评分系统似乎相当主观：
- en: '![Figure 9.4 – Distribution of white wine quality scores'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.4 – 白葡萄酒质量评分分布](img/Figure_9.4_B16834.jpg)'
- en: '](img/Figure_9.4_B16834.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.4_B16834.jpg)'
- en: Figure 9.4 – Distribution of white wine quality scores
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 白葡萄酒质量评分分布
- en: 'Both of these dataframes have the same columns, so we can combine them without
    further work. Here, we use `pd.concat()` to stack the white wine data on top of
    the red wine data after adding a column to identify which wine type each observation
    belongs to:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个数据框具有相同的列，因此我们可以直接合并它们。在这里，我们使用`pd.concat()`将白葡萄酒数据叠加在红葡萄酒数据之上，并在添加一个列以标识每个观测所属的葡萄酒类型后进行操作：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As we did with the red wine dataset, we can run `info()` to check whether we
    need to perform type conversion or whether we are missing any data; thankfully,
    we have no need here either. Our combined wine dataset looks like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们处理红葡萄酒数据集一样，我们可以运行`info()`来检查是否需要执行类型转换或是否有任何缺失数据；幸运的是，这里我们也不需要。我们的组合葡萄酒数据集如下所示：
- en: '![Figure 9.5 – Combined wine dataset'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.5 – 组合葡萄酒数据集'
- en: '](img/Figure_9.5_B16834.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.5_B16834.jpg)'
- en: Figure 9.5 – Combined wine dataset
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 组合葡萄酒数据集
- en: 'Using `value_counts()`, we can see that we have many more white wines than
    red wines in the data:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`value_counts()`，我们可以看到数据中白葡萄酒比红葡萄酒多得多：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Lastly, let''s examine box plots for each chemical property broken out by wine
    type using `seaborn`. This can help us identify **features** (model inputs) that
    will be helpful when building our model to distinguish between red and white wine:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用`seaborn`来查看每种化学性质按葡萄酒类型分组的箱线图。这可以帮助我们识别在建立区分红葡萄酒和白葡萄酒模型时将有帮助的**特征**（模型输入）：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Given the following result, we might look to use fixed acidity, volatile acidity,
    total sulfur dioxide, and sulphates when building a model since they seem to be
    distributed differently for red and white wines:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以下结果，我们可能会考虑在构建模型时使用固定酸度、挥发性酸度、总二氧化硫和硫酸盐，因为它们在红葡萄酒和白葡萄酒中的分布似乎不同：
- en: '![Figure 9.6 – Comparing red and white wine on a chemical level'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.6 – 在化学水平上比较红葡萄酒和白葡萄酒'
- en: '](img/Figure_9.6_B16834.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.6_B16834.jpg)'
- en: Figure 9.6 – Comparing red and white wine on a chemical level
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 在化学水平上比较红葡萄酒和白葡萄酒
- en: Tip
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Comparing the distributions of variables across classes can help inform feature
    selection for our model. If we see that the distribution for a variable is very
    different between classes, that variable may be very useful to include in our
    model. It is essential that we perform an in-depth exploration of our data before
    moving on to modeling. Be sure to use the visualizations we covered in [*Chapter
    5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106), *Visualizing Data with Pandas
    and Matplotlib*, and [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*, as they will prove invaluable
    for this process.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 比较不同类别之间变量的分布可以帮助我们选择模型的特征。如果我们发现某个变量在不同类别之间的分布非常不同，那么这个变量可能非常有用并应该包含在我们的模型中。在进行建模之前，我们必须对数据进行深入探索。一定要使用我们在[*第
    5 章*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106)中介绍的可视化工具，*使用 Pandas 和 Matplotlib
    可视化数据*，以及[*第 6 章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)，*使用 Seaborn
    和自定义技术绘图*，因为它们对这个过程非常有价值。
- en: We will come back to this visualization in [*Chapter 10*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217),
    *Making Better Predictions – Optimizing Models*, when we examine incorrect predictions
    made by our model. Now, let's take a look at the other dataset we will be working
    with.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检查模型犯的错误预测时，我们将在[*第 10 章*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217)中回到这个可视化，*做出更好的预测
    – 优化模型*。现在，让我们看看我们将要处理的另一个数据集。
- en: Planets and exoplanets data
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行星和外行星数据
- en: 'An **exoplanet** is simply a planet that orbits a star outside of our solar
    system, so from here on out we will refer to both collectively as **planets**.
    Let''s read in our planets data now:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**外行星**简单地是指绕着我们太阳系之外的恒星运转的行星，因此从现在开始我们将统称它们为**行星**。现在让我们读取我们的行星数据：'
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Some interesting tasks we can do with this data would be to find clusters of
    similar planets based on their orbits and try to predict the orbit period (how
    long a year is on a planet), in Earth days:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据它们的轨道找到相似行星的聚类，并尝试预测轨道周期（行星一年有多长，以地球日计算）：
- en: '![Figure 9.7 – Planets dataset'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.7 – 行星数据集'
- en: '](img/Figure_9.7_B16834.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.7_B16834.jpg)'
- en: Figure 9.7 – Planets dataset
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 行星数据集
- en: 'We can build a correlation matrix heatmap to help find the best features to
    use:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建相关矩阵热力图来帮助找到最佳特征：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The heatmap shows us that the semi-major axis of a planet''s orbit is highly
    positively correlated to the length of its period, which makes sense since the
    semi-major axis (along with eccentricity) helps define the path that a planet
    travels around its star:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 热力图显示，行星轨道的半长轴与其周期的长度高度正相关，这是有道理的，因为半长轴（以及离心率）有助于定义行星围绕恒星的轨道路径：
- en: '![Figure 9.8 – Correlations between features in the planets dataset'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.8 – 行星数据集特征间的相关性'
- en: '](img/Figure_9.8_B16834.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.8_B16834.jpg)'
- en: Figure 9.8 – Correlations between features in the planets dataset
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – 行星数据集特征间的相关性
- en: 'To predict `period`, we probably want to look at `semimajoraxis`, `mass`, and
    `eccentricity`. The orbit eccentricity quantifies how much the orbit differs from
    a perfect circle:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测`period`，我们可能需要观察`semimajoraxis`、`mass`和`eccentricity`。轨道的离心率量化了轨道与完美圆形的偏离程度：
- en: '![Figure 9.9 – Understanding eccentricity'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.9 – 理解离心率'
- en: '](img/Figure_9.9_B16834.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.9_B16834.jpg)'
- en: Figure 9.9 – Understanding eccentricity
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – 理解离心率
- en: 'Let''s see what shapes the orbits we have are:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们所拥有的轨道形状：
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'It looks like nearly everything is an ellipse, which we would expect since
    these are planets:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来几乎所有的轨道都是椭圆形的，这是我们预期的，因为这些是行星：
- en: '![Figure 9.10 – Distribution of orbit eccentricities'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.10 – 轨道离心率的分布'
- en: '](img/Figure_9.10_B16834.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.10_B16834.jpg)'
- en: Figure 9.10 – Distribution of orbit eccentricities
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 – 轨道离心率的分布
- en: 'An ellipse, being an elongated circle, has two axes: *major* and *minor* for
    the longest and shortest ones, respectively. The semi-major axis is half the major
    axis. When compared to a circle, the axes are analogous to the diameter, crossing
    the entire shape, and the semi-axes are akin to the radius, being half the diameter.
    The following is how this would look in the case where the planet orbited a star
    that was exactly in the center of its elliptical orbit (due to gravity from other
    objects, in reality, the star can be anywhere inside the orbit path):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 椭圆是一个拉长的圆形，具有两个轴：*长轴*和*短轴*，分别是最长和最短的轴。半长轴是长轴的一半。与圆形相比，轴类似于直径，穿过整个形状，而半轴类似于半径，是直径的一半。以下是当行星围绕一颗位于其椭圆轨道中心的恒星运行时的情况（由于其他天体的引力，实际情况是恒星可以位于轨道路径中的任何位置）：
- en: '![Figure 9.11 – Understanding the semi-major axis'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.11 – 理解半长轴'
- en: '](img/Figure_9.11_B16834.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.11_B16834.jpg)'
- en: Figure 9.11 – Understanding the semi-major axis
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 – 理解半长轴
- en: 'Now that we understand what these columns mean, let''s do some more EDA. This
    data isn''t as clean as our wine data was—it''s certainly much easier to measure
    everything when we can reach out and touch it. We only have `eccentricity`, `semimajoraxis`,
    or `mass` data for a fraction of the planets, despite knowing most of the `period`
    values:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了这些列的含义，让我们继续进行更多的探索性数据分析（EDA）。这个数据并不像我们的酒类数据那样干净——当我们能够直接接触到数据时，一切显得更加容易。尽管我们知道大部分的`period`值，我们只拥有一小部分行星的`eccentricity`、`semimajoraxis`或`mass`数据：
- en: '[PRE13]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If we were to drop data where any of these columns was null, we would be left
    with about 30% of it:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们丢弃任何一个列为空的数据，那么剩下的将只有大约 30%：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If we are simply looking for a way to predict the length of the year (when
    we have these values available) to learn more about their relationship, we wouldn''t
    necessarily worry about throwing out the missing data. Imputing it here could
    be far worse for our model. At least everything is properly encoded as a decimal
    (`float64`); however, let''s check whether we need to do some scaling (beneficial
    if our model is sensitive to differences in magnitude):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仅仅是想找到一种预测年份长度的方法（当我们有这些值时），以便更好地了解它们之间的关系，那么我们不一定需要担心丢弃缺失数据。这里的插补可能会对我们的模型造成更大的负面影响。至少所有数据都已正确编码为十进制数（`float64`）；然而，让我们检查是否需要做一些缩放（如果我们的模型对大小差异敏感，缩放将是有益的）：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This shows us that, depending on our model, we will definitely have to do some
    scaling because the values in the `period` column are much larger than the others:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这向我们展示了，根据我们的模型，我们肯定需要做一些缩放，因为`period`列中的值远大于其他列：
- en: '![Figure 9.12 – Summary statistics for the planets dataset'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.12 – 行星数据集的总结统计'
- en: '](img/Figure_9.12_B16834.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.12_B16834.jpg)'
- en: Figure 9.12 – Summary statistics for the planets dataset
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 – 行星数据集的总结统计
- en: 'We could also look at some scatter plots. Note that there is a `list` column
    for the group the planet belongs to, such as `Solar System` or `Controversial`.
    We might want to see if the period (and distance from the star) influences this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看一些散点图。注意，存在一个`list`列，表示行星所属的组别，如`太阳系`或`有争议`。我们可能希望查看周期（以及与恒星的距离）是否对其产生影响：
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The controversial planets appear to be spread throughout and have larger semi-major
    axes and periods. Perhaps they are controversial because they are very far from
    their star:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些有争议的行星似乎分布在各处，并且它们的半长轴和周期较大。或许它们之所以有争议，是因为它们距离恒星非常远：
- en: '![Figure 9.13 – Planet period versus semi-major axis'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.13 – 行星周期与半长轴'
- en: '](img/Figure_9.13_B16834.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.13_B16834.jpg)'
- en: Figure 9.13 – Planet period versus semi-major axis
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 – 行星周期与半长轴
- en: 'Unfortunately, we can see that the scale of `period` is making this pretty
    difficult to read, so we could try a log transformation on the *y*-axis to get
    more separation in the denser section on the lower-left. Let''s just point out
    the planets in our solar system this time:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们可以看到`周期`的尺度使得这张图很难阅读，因此我们可以尝试对*y*-轴进行对数变换，以便在左下角更密集的区域中获得更多的分离。我们这次仅标出太阳系中的行星：
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'There were certainly a lot of planets hiding in that lower-left corner of the
    plot. We can see many planets with years shorter than Mercury''s 88 Earth-day
    year now:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 确实有很多行星藏在图表的左下角。现在，我们可以看到许多行星的年周期比水星的88地球年还要短：
- en: '![Figure 9.14 – Our solar system compared to exoplanets'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.14 – 我们的太阳系与外行星的对比'
- en: '](img/Figure_9.14_B16834.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.14_B16834.jpg)'
- en: Figure 9.14 – Our solar system compared to exoplanets
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – 我们的太阳系与外行星的对比
- en: Now that we have a feel for the data we will be working with, let's learn how
    to prepare it for use in a machine learning model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们对将要使用的数据有了一些了解，让我们学习如何为机器学习模型准备这些数据。
- en: Preprocessing data
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'In this section, we will be working in the `preprocessing.ipynb` notebook before
    we return to the notebooks we used for EDA. We will begin with our imports and
    read in the data:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将在`preprocessing.ipynb`笔记本中工作，然后再返回到用于EDA的笔记本。我们将从导入库和读取数据开始：
- en: '[PRE18]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Machine learning models follow the garbage in, garbage out principle. We have
    to make sure that we **train** our models (have them learn) on the best possible
    version of the data. What this means will depend on the model we choose. For instance,
    models that use a distance metric to calculate how similar observations are will
    easily be confused if our features are on wildly different scales. Unless we are
    working with a **natural language processing** (**NLP**) problem to try and understand
    the meaning of words, our model will have no use for—or worse, be unable to interpret—textual
    values. Missing or invalid data will also cause problems; we will have to decide
    whether to drop them or impute them. All of the adjustments we make to our data
    before giving it to our model to learn from are collectively called **preprocessing**.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型遵循“垃圾进，垃圾出”的原则。我们必须确保在最优版本的数据上**训练**我们的模型（让它学习）。这意味着什么将取决于我们选择的模型。例如，使用距离度量来计算观测值相似度的模型，如果我们的特征尺度差异很大，容易混淆。除非我们正在处理**自然语言处理**（**NLP**）问题，试图理解单词的意义，否则我们的模型对文本值没有用处——甚至无法解释它们。缺失或无效数据也会造成问题；我们必须决定是丢弃它们还是填补它们。我们在将数据提供给模型学习之前所做的所有调整统称为**数据预处理**。
- en: Training and testing sets
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练集与测试集
- en: So far, machine learning sounds pretty great, though—we can build a model that
    will learn how to perform a task for us. Therefore, we should give it all the
    data we have so that it learns well, right? Unfortunately, it's not that simple.
    If we give the model all of our data, we risk **overfitting** it, meaning that
    it won't be able to generalize well to new data points because it was fit to the
    sample rather than the population. On the other hand, if we don't give it enough
    data, it will **underfit** and be unable to capture the underlying information
    in the data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，机器学习听起来相当不错——我们可以构建一个模型来学习为我们执行任务。那么，应该将所有数据都提供给它，让它学得更好，对吗？不幸的是，事情并没有那么简单。如果我们将所有数据提供给模型，就有可能导致**过拟合**，意味着它将无法很好地推广到新的数据点，因为它是针对样本而不是总体进行拟合的。另一方面，如果我们不给模型足够的数据，它将**欠拟合**，无法捕捉数据中的潜在信息。
- en: Tip
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: When a model fits the randomness in the data, it is said to fit the **noise**
    in the data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型适应数据中的随机性时，我们称它为适应数据中的**噪声**。
- en: 'Another thing to consider is that if we use all of our data to train the model,
    how can we evaluate its performance? If we test it on the data we used for training,
    we will be overestimating how good it is because our model will always perform
    better on the training data. For these reasons, it''s important to split our data
    into a **training set** and **testing set**. To do so, we could shuffle our dataframe
    and select the top *x*% of the rows for training and leave the rest for testing:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的问题是，如果我们用所有数据来训练模型，我们该如何评估其性能呢？如果我们在用于训练的数据上进行测试，我们会高估模型的表现，因为模型总是在训练数据上表现得更好。因此，必须将数据分成**训练集**和**测试集**。为了做到这一点，我们可以将数据框进行洗牌，并选择前
    *x*% 的行作为训练集，其余部分作为测试集：
- en: '[PRE19]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This would work, but it''s a lot to write every time. Thankfully, `scikit-learn`
    provides us with the `train_test_split()` function in the `model_selection` module,
    which is a more robust, easier-to-use solution. It requires us to separate our
    input data (`X`) from our output data (`y`) beforehand. Here, we will pick 75%
    of the data to be used for the training set (`X_train`, `y_train`) and 25% for
    the testing set (`X_test`, `y_test`). We will set a seed (`random_state=0`) so
    that the split is reproducible:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是可行的，但每次都要写这么多内容确实有些麻烦。幸运的是，`scikit-learn` 在 `model_selection` 模块中为我们提供了
    `train_test_split()` 函数，这是一个更加稳健、易于使用的解决方案。它要求我们事先将输入数据（`X`）和输出数据（`y`）分开。在这里，我们将选择
    75% 的数据作为训练集（`X_train`，`y_train`），剩下的 25% 用于测试集（`X_test`，`y_test`）。我们将设置一个随机种子（`random_state=0`），以确保数据拆分是可重复的：
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'While there are no specific criteria for what constitutes a good size for the
    test set, a rule of thumb is usually between 10% and 30% of the data. However,
    if we don''t have much data, we will shift toward a 10% testing set to make sure
    that we have enough data to learn from. Conversely, if we have a lot of data,
    we may move toward 30% testing, since, not only do we not want to overfit, but
    we want to give our model a good amount of data to prove its worth. Note that
    there is a big caveat with this rule of thumb: there are diminishing returns on
    the amount of training data we use. If we have a ton of data, we will most likely
    use much less than 70% of it for training because our computational costs may
    rise significantly for possibly minuscule improvements and an increased risk of
    overfitting.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有明确的标准来定义测试集的理想大小，但通常的经验法则是占数据的 10% 到 30%。不过，如果我们的数据量较少，我们会选择 10% 的测试集，以确保有足够的数据用于学习。相反，如果数据量很大，我们可能会选择
    30% 作为测试集，因为我们不仅不希望出现过拟合，还希望给模型提供足够的数据来证明它的价值。需要注意的是，这条经验法则有一个重大警告：我们使用的训练数据越多，回报递减越明显。如果我们有大量数据，可能会用不到
    70% 的数据进行训练，因为计算成本可能会大幅上升，而改进幅度却微乎其微，甚至可能导致过拟合的风险增加。
- en: Important note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When building models that require tuning, we split the data into training, validation,
    and testing sets. We will introduce validation sets in [*Chapter 10*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217),
    *Making Better Predictions – Optimizing Models*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建需要调优的模型时，我们将数据分成训练集、验证集和测试集。我们将在[*第10章*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217)中介绍验证集内容，*使预测更准确
    - 优化模型*。
- en: 'Let''s take a look at the dimensions of our training and testing sets now.
    Since we are using three features (`eccentricity`, `semimajoraxis`, and `mass`),
    `X_train` and `X_test` have three columns. The `y_train` and `y_test` sets will
    be a single column each. The number of observations in the `X` and `y` data for
    training will be equal, as will be the case for the testing set:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看训练集和测试集的维度。由于我们使用了三个特征（`eccentricity`，`semimajoraxis`，和 `mass`），`X_train`
    和 `X_test` 将有三列。`y_train` 和 `y_test` 每个将只有一列。训练数据中的 `X` 和 `y` 观察值数量将相等，测试集也是如此：
- en: '[PRE21]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`X_train` and `X_test` are returned to us as dataframes since that is the format
    we passed them in as. If we are working with data in NumPy directly, we will get
    NumPy arrays or `ndarrays` back instead. We are going to work with this data for
    other examples in the *Preprocessing data* section, so let''s take a look at the
    first five rows of the `X_train` dataframe. Don''t worry about the `NaN` values
    for now; we will discuss different ways of handling them in the *Imputing* section:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_train` 和 `X_test` 被返回为数据框，因为我们传递它们时就是这种格式。如果我们直接处理 NumPy 数据，我们将得到 NumPy
    数组或 `ndarray` 格式的数据。我们将在 *数据预处理* 部分使用这些数据进行其他示例演示，因此让我们先来看一下 `X_train` 数据框的前五行。现在不用担心
    `NaN` 值；我们将在 *填补缺失值* 部分讨论处理这些值的不同方法：'
- en: '[PRE22]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Both `y_train` and `y_test` are series since that is what we passed into the
    `train_test_split()` function. If we had passed in a NumPy array, that is what
    we would have gotten back instead. The rows in `y_train` and `y_test` must line
    up with the rows in `X_train` and `X_test`, respectively. Let''s confirm this
    by looking at the first five rows of `y_train`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`y_train`和`y_test`都是序列，因为这就是我们传递给`train_test_split()`函数的内容。如果我们传入的是NumPy数组，那我们将得到的是返回的结果。`y_train`和`y_test`中的行必须与`X_train`和`X_test`中的行分别对齐。让我们通过查看`y_train`的前五行来确认这一点：'
- en: '[PRE23]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Indeed, everything matches up, as expected. Note that for our wine models, we
    need to use stratified sampling, which can also be done with `train_test_split()`
    by passing the values to stratify on in the `stratify` argument. We will see this
    in the *Classification* section. For now, let's move on to the rest of our preprocessing.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，一切如预期的那样对齐。请注意，对于我们的葡萄酒模型，我们需要使用分层采样，这也可以通过在`train_test_split()`中传递用于分层的值来完成。我们将在*分类*部分看到这一点。现在，让我们继续进行剩余的预处理工作。
- en: Scaling and centering data
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放和居中数据
- en: We've seen that our dataframes had columns with very different scales; if we
    want to use any model that calculates a distance metric (such as k-means, which
    we will discuss in this chapter, or `preprocessing` module for standardizing (scaling
    by calculating Z-scores) and min-max scaling (to normalize data to be in the range
    [0, 1]), among others.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到我们的数据框中有具有非常不同尺度的列；如果我们想使用任何计算距离度量的模型（例如我们将在本章讨论的k-means，或者用于标准化（通过计算Z分数进行缩放）和最小-最大缩放（将数据规范化为[0,
    1]范围）等的`preprocessing`模块）。
- en: Important note
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We should check the requirements of the model we are building to see if the
    data needs to be scaled.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该检查我们所构建的模型的要求，以查看数据是否需要缩放。
- en: 'For standard scaling, we use the `StandardScaler` class. The `fit_transform()`
    method combines `fit()`, which figures out the mean and standard deviation needed
    to center and scale, and `transform()`, which applies the transformation to the
    data. Note that, when instantiating a `StandardScaler` object, we can choose to
    not subtract the mean or not divide by the standard deviation by passing `False`
    to `with_mean` or `with_std`, respectively. Both are `True` by default:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标准化缩放，我们使用`StandardScaler`类。`fit_transform()`方法将`fit()`（它计算出将数据居中和缩放所需的均值和标准差）与`transform()`（它将转换应用于数据）结合起来。请注意，当实例化`StandardScaler`对象时，我们可以选择不减去均值或不除以标准差，通过分别将`False`传递给`with_mean`或`with_std`参数。默认情况下，两者都是`True`：
- en: '[PRE24]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: After this transformation, the data is in `e` tells us where the decimal point
    got moved to. For a `+` sign, we move the decimal point to the right by the number
    of places indicated; we move to the left for a `-` sign. Therefore, `1.00e+00`
    is equivalent to `1`, `2.89e-02` is equivalent to `0.0289`, and `2.89e+02` is
    equivalent to `289`. The transformed planets data is mostly between -3 and 3 because
    everything is now a Z-score.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 经过此转换后，数据中的`e`表示小数点移动的位置。对于`+`符号，我们将小数点向右移动指定的位数；对于`-`符号，我们将小数点向左移动。因此，`1.00e+00`等同于`1`，`2.89e-02`等同于`0.0289`，而`2.89e+02`等同于`289`。转换后的行星数据大多介于-3和3之间，因为现在所有数据都是Z分数。
- en: 'Other scalers can be used with the same syntax. Let''s use the `MinMaxScaler`
    class to transform the planets data into the range [0, 1]:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 其他缩放器可以使用相同的语法。让我们使用`MinMaxScaler`类将行星数据转换为范围[0, 1]：
- en: '[PRE25]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Tip
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Another option is the `RobustScaler` class, which uses the median and IQR for
    robust to outliers scaling. There is an example of this in the notebook. More
    preprocessing classes can be found at [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是`RobustScaler`类，它使用中位数和IQR进行抗异常值缩放。笔记本中有这个的示例。更多预处理类可以在[https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)找到。
- en: Encoding data
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码数据
- en: 'All of the scalers discussed so far address the preprocessing of our numeric
    data, but how can we deal with categorical data? We need to encode the categories
    into integer values. There are a few options here, depending on what the categories
    represent. If our category is binary (such as `0`/`1`, `True`/`False`, or `yes`/`no`),
    then we will `0` is one option and `1` is the other. We can easily do this with
    the `np.where()` function. Let''s encode the wine data''s `kind` field as `1`
    for red and `0` for white:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的所有缩放器都处理了数值数据的预处理，但我们该如何处理类别数据呢？我们需要将类别编码为整数值。这里有几个选择，取决于类别代表什么。如果我们的类别是二进制的（例如`0`/`1`、`True`/`False`或`yes`/`no`），那么`0`是一个选项，`1`是另一个选项。我们可以通过`np.where()`函数轻松做到这一点。让我们将葡萄酒数据的`kind`字段编码为红酒为`1`，白酒为`0`：
- en: '[PRE26]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This is effectively a column that tells us whether or not the wine is red. Remember,
    we concatenated the red wines to the bottom of the white wines when we created
    our `wine` dataframe, so `np.where()` will return zeros for the top rows and ones
    for the bottom rows, just like we saw in the previous result.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一个告诉我们酒是否为红酒的列。记住，在我们创建`wine`数据框时，我们将红酒数据拼接到了白酒数据的底部，因此`np.where()`将对顶部行返回零，对底部行返回一，就像我们在之前的结果中看到的那样。
- en: Tip
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: We can also use the `LabelBinarizer` class from `scikit-learn` to encode the
    `kind` field. Note that if our data is actually continuous, but we want to treat
    it as a binary categorical value, we could use the `Binarizer` class and provide
    a threshold or `pd.cut()`/`pd.qcut()`. There are examples of these in the notebook.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`scikit-learn`的`LabelBinarizer`类来编码`kind`字段。请注意，如果我们的数据实际上是连续的，但我们希望将其视为二进制类别值，我们可以使用`Binarizer`类并提供一个阈值，或者使用`pd.cut()`/`pd.qcut()`。在笔记本中有这些的示例。
- en: 'If our categories are ordered, we may want to use `0`, `1`, and `2`, respectively.
    The advantages of this are that we can use regression techniques to predict the
    quality, or we can use this as a feature in the model to predict something else;
    this model would be able to use the fact that high is better than medium, which
    is better than low quality. We can achieve this with the `LabelEncoder` class.
    Note that the labels will be created according to alphabetical order, so the first
    category alphabetically will be `0`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的类别是有序的，我们可能希望分别使用`0`、`1`和`2`。这样做的好处是我们可以使用回归技术来预测质量，或者可以将其作为模型中的特征来预测其他内容；该模型能够利用“高于中等，中等高于低质量”这一事实。我们可以使用`LabelEncoder`类来实现这一点。请注意，标签将根据字母顺序创建，因此按字母顺序排列的第一个类别将是`0`：
- en: '[PRE27]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Important note
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Scikit-learn provides the `OrdinalEncoder` class, but our data is not in the
    correct format—it expects 2D data (such as a `DataFrame` or `ndarray` object),
    instead of the 1D `Series` object we are working with here. We still need to ensure
    that the categories are in the proper order beforehand.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn提供了`OrdinalEncoder`类，但我们的数据格式不正确——它期望的是二维数据（如`DataFrame`或`ndarray`对象），而我们这里使用的是一维的`Series`对象。我们仍然需要确保类别事先是按照正确的顺序排列的。
- en: However, note that the ordinal encoding may create a potential data issue. In
    our example, if high-quality wines are now `2` and medium-quality wines are `1`,
    the model may interpret that `2 * med = high`. This is implicitly creating an
    association between the levels of quality that we may not agree with.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，序数编码可能会导致潜在的数据问题。在我们的示例中，如果高质量葡萄酒现在是`2`，而中等质量葡萄酒是`1`，模型可能会解释为`2 * med
    = high`。这实际上在不同质量等级之间隐性地创建了关联，这可能是我们不认同的。
- en: Alternatively, a safer approach would be to perform `is_low` and `is_med`, which
    take only `0` or `1`. Using those two, we automatically know whether the wine
    quality was high (when `is_low` = `is_med` = `0`). These are called `1`, that
    row is a member of that group; in our example of wine quality categories, if `is_low`
    is `1`, then that row is a member of the low-quality group. This can be achieved
    with the `pd.get_dummies()` function and the `drop_first` argument, which will
    remove the redundant column.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更安全的方法是执行`is_low`和`is_med`，这两个变量仅取`0`或`1`。通过这两个变量，我们可以自动知道酒的质量是否很高（当`is_low`
    = `is_med` = `0`时）。这些被称为`1`，该行属于该组；在我们关于葡萄酒质量类别的示例中，如果`is_low`为`1`，则该行属于低质量组。这可以通过`pd.get_dummies()`函数和`drop_first`参数来实现，后者会移除冗余列。
- en: 'Let''s use one-hot encoding to encode the `list` column in the planets data,
    since the categories have no inherent order. Before we do any transformations,
    let''s take a look at the lists we have in the data:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用独热编码对行星数据中的 `list` 列进行编码，因为这些类别没有固有的顺序。在进行任何转换之前，先看一下我们数据中的列表：
- en: '[PRE28]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can use the `pd.get_dummies()` function to create dummy variables if we
    want to include the planet list in our models:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望将行星列表包含到我们的模型中，可以使用 `pd.get_dummies()` 函数创建虚拟变量：
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This turns our single series into the following dataframe, where the dummy
    variables were created in the order they appeared in the data:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这会将我们的单一序列转换为以下数据框架，其中虚拟变量按数据中出现的顺序创建：
- en: '![Figure 9.15 – One-hot encoding'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.15 – 独热编码'
- en: '](img/Figure_9.15_B16834.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.15_B16834.jpg)'
- en: Figure 9.15 – One-hot encoding
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 – 独热编码
- en: 'As we discussed previously, one of these columns is redundant because the values
    in the remaining ones can be used to determine the value for the redundant one.
    Some models may be significantly affected by the high correlation between these
    columns (referred to as `drop_first` argument:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的那样，这些列中的一个是多余的，因为其余列的值可以用来推断多余列的值。一些模型可能会受到这些列之间高相关性的显著影响（这就是 `drop_first`
    参数的作用）：
- en: '[PRE30]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Note that the first column from the previous result has been removed, but we
    can still determine that all but the last row were in the `Confirmed Planets`
    list:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到先前结果中的第一列已经被删除，但我们仍然可以确定，除了最后一行外，其他行都在`确认的行星`列表中：
- en: '![Figure 9.16 – Dropping redundant columns after one-hot encoding'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.16 – 在独热编码后删除冗余列'
- en: '](img/Figure_9.16_B16834.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.16_B16834.jpg)'
- en: Figure 9.16 – Dropping redundant columns after one-hot encoding
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 – 在独热编码后删除冗余列
- en: 'Note that we can obtain a similar result by using the `LabelBinarizer` class
    and its `fit_transform()` method on our planets list. This won''t drop a redundant
    feature, so we once again have the first feature belonging to the confirmed planets
    list, which can be seen in bold in the following result:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以通过使用 `LabelBinarizer` 类及其 `fit_transform()` 方法在行星列表上得到类似的结果。这不会删除冗余特征，因此我们再次可以看到第一列属于确认行星列表，在以下结果中以粗体显示：
- en: '[PRE31]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Important note
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Scikit-learn provides the `OneHotEncoder` class, but our data is not in the
    correct format—it expects the data to come in a 2D array, and our series is just
    1D. We will see an example of how to use this in the *Additional transformers*
    section.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 提供了 `OneHotEncoder` 类，但我们的数据格式不正确——它期望数据以 2D 数组的形式出现，而我们的数据是 1D
    的。我们将在*附加转换器*部分看到如何使用这个类的示例。
- en: Imputing
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充
- en: 'We already know that we have some missing values in our planet data, so let''s
    discuss a few of the options `scikit-learn` offers for handling them, which can
    be found in the `impute` module: imputing with a value (using constants or summary
    statistics), imputing based on similar observations, and indicating what is missing.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道行星数据中有一些缺失值，因此让我们讨论 `scikit-learn` 提供的几种处理缺失值的选项，这些选项可以在 `impute` 模块中找到：用某个值填充（使用常数或汇总统计）、根据相似观察值填充，以及指示缺失的部分。
- en: 'Back in the *Exploratory data analysis* section, we ran `dropna()` on the planets
    data we planned to model with. Let''s say we don''t want to get rid of it, and
    we want to try imputing it instead. The last few rows of our data have some missing
    values for `semimajoraxis`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 回到*探索性数据分析*部分，我们对我们计划建模的行星数据使用了 `dropna()`。假设我们不想删除这些数据，而是希望尝试填充缺失值。我们的数据最后几行在
    `semimajoraxis` 列上有缺失值：
- en: '[PRE32]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can use the `SimpleImputer` class to impute with a value, which will be
    the mean by default:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `SimpleImputer` 类来填充这些缺失值，默认情况下将使用均值进行填充：
- en: '[PRE33]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The mean hardly seems like a good strategy here since the planets we know about
    may share something in common, and surely things like what system a planet is
    a part of and its orbit can be good indicators of some of the missing data points.
    We have the option to provide the `strategy` parameter with a method other than
    the mean; currently, it can be `median`, `most_frequent`, or `constant` (specify
    the value with `fill_value`). None of these is really appropriate for us; however,
    `scikit-learn` also provides the `KNNImputer` class for imputing missing values
    based on similar observations. By default, it uses the five nearest neighbors
    and runs k-NN, which we will discuss in [*Chapter 10*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217),
    *Making Better Predictions – Optimizing Models*, using the features that aren''t
    missing:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 平均值似乎不是一个好的策略，因为我们知道的行星可能有一些共同点，显然像行星属于哪个系统及其轨道等特征，可以作为缺失数据点的好指示符。我们可以选择为`strategy`参数提供除均值之外的方法；当前，它可以是`median`（中位数）、`most_frequent`（最频繁值）或`constant`（指定值通过`fill_value`）。这些方法对我们来说都不太合适；然而，`scikit-learn`还提供了`KNNImputer`类，用于基于相似的观察值填充缺失值。默认情况下，它使用五个最近的邻居，并运行k-NN算法，我们将在[*第10章*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217)《做出更好的预测——优化模型》中讨论这一点，使用那些没有缺失的特征。
- en: '[PRE34]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Notice that each of the bottom three rows has a unique value imputed for the
    semi-major axis now. This is because the mass and eccentricity were used to find
    similar planets from which to impute the semi-major axis. While this is certainly
    better than using the `SimpleImputer` class for the planets data, imputing can
    be dangerous.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，底部三行的每一行现在都有一个唯一的值填充给半长轴。这是因为质量和偏心率被用来找到相似的行星，并基于这些行星来填充半长轴。虽然这比使用`SimpleImputer`类处理行星数据要好，但填充数据仍然存在风险。
- en: 'Rather than imputing the data, in some cases, we may be more interested in
    noting where we have missing data and using that as a feature in our model. This
    can be achieved with the `MissingIndicator` class:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能更关心的是标记缺失数据的位置，并将其作为我们模型中的一个特征，而不是对数据进行填充。这可以通过`MissingIndicator`类来实现：
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As we turn our attention to the final set of preprocessors that we will discuss,
    notice that all of them have a `fit_transform()` method, along with `fit()` and
    `transform()` methods. This API design decision makes it very easy to figure out
    how to use new classes and is one of the reasons why `scikit-learn` is so easy
    to learn and use—it's very consistent.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们关注我们将讨论的最后一组预处理器时，注意到它们都有一个`fit_transform()`方法，以及`fit()`和`transform()`方法。这种API设计决策使得我们很容易了解如何使用新类，也是`scikit-learn`如此容易学习和使用的原因之一——它非常一致。
- en: Additional transformers
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外的转换器
- en: 'What if, rather than scaling our data or encoding it, we want to run a mathematical
    operation, such as taking the square root or the logarithm? The `preprocessing`
    module also has some classes for this. While there are a few that perform a specific
    transformation, such as the `QuantileTransformer` class, we will focus our attention
    on the `FunctionTransformer` class, which lets us provide an arbitrary function
    to use:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想进行数学运算，而不是对数据进行缩放或编码，比如取平方根或对数怎么办？`preprocessing`模块也有一些类可以执行此类操作。虽然有一些类执行特定的转换，例如`QuantileTransformer`类，但我们将重点关注`FunctionTransformer`类，它允许我们提供一个任意的函数来使用：
- en: '[PRE36]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, we took the absolute value of every number. Take note of the `validate=True`
    argument; the `FunctionTransformer` class knows that `scikit-learn` models won't
    accept `NaN` values, infinite values, or missing ones, so it will throw an error
    if we get those back. For this reason, we run `dropna()` here as well.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们取了每个数字的绝对值。注意`validate=True`参数；`FunctionTransformer`类知道`scikit-learn`模型不会接受`NaN`值、无穷大值或缺失值，因此如果返回这些值，它会抛出一个错误。为此，我们在这里也运行了`dropna()`。
- en: 'Notice that for scaling, encoding, imputing, and transforming data, everything
    we passed was transformed. If we have features of different data types, we can
    use the `ColumnTransformer` class to map transformations to a column (or group
    of columns) in a single call:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于缩放、编码、填充和转换数据，所有我们传递的数据都被转换了。如果我们有不同数据类型的特征，可以使用`ColumnTransformer`类在一次调用中将转换映射到一个列（或一组列）：
- en: '[PRE37]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'There is also the `make_column_transformer()` function, which will name the
    transformers for us. Let''s make a `ColumnTransformer` object that will treat
    categorical data and numerical data differently:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个`make_column_transformer()`函数，它会为我们命名转换器。我们来创建一个`ColumnTransformer`对象，它将对分类数据和数值数据进行不同的处理：
- en: '[PRE38]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Tip
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: We are passing `sparse=False` upon instantiating our `OneHotEncoder` object
    so that we can see our result. In practice, we don't need to do this since `scikit-learn`
    models know how to handle NumPy sparse matrices.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实例化`OneHotEncoder`对象时传递`sparse=False`，以便我们能看到结果。实际上，我们不需要这样做，因为`scikit-learn`模型知道如何处理NumPy稀疏矩阵。
- en: Building data pipelines
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建数据管道
- en: It sure seems like there are a lot of steps involved in preprocessing our data,
    and they need to be applied in the correct order for both training and testing
    data—quite tedious. Thankfully, `scikit-learn` offers the ability to create pipelines
    to streamline the preprocessing and ensure that the training and testing sets
    are treated the same. This prevents issues, such as calculating the mean using
    all the data in order to standardize it and then splitting it into training and
    testing sets, which will create a model that looks like it will perform better
    than it actually will.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，在预处理数据时似乎涉及了很多步骤，并且它们需要按照正确的顺序应用于训练和测试数据——这非常繁琐。幸运的是，`scikit-learn`提供了创建管道的功能，能够简化预处理过程，并确保训练集和测试集以相同的方式处理。这可以避免例如在标准化时计算所有数据的均值，然后再将数据划分为训练集和测试集的情况，这样会创建一个看起来表现更好的模型，而实际表现可能不如预期。
- en: Important note
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When information from outside the training set (such as using the full dataset
    to calculate the mean for standardization) is used to train the model, it is referred
    to as **data leakage**.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用来自训练集外部的信息（例如，使用完整数据集计算均值进行标准化）来训练模型时，称为**数据泄漏**。
- en: 'We are learning about pipelines before we build our first models because they
    ensure that the models are built properly. Pipelines can contain all the preprocessing
    steps and the model itself. Making a pipeline is as simple as defining the steps
    and naming them:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在构建第一个模型之前学习管道，因为管道确保模型正确构建。管道可以包含所有预处理步骤和模型本身。构建管道就像定义步骤并命名它们一样简单：
- en: '[PRE39]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We aren''t limited to using pipelines with models—they can also be used inside
    other `scikit-learn` objects, for example, `ColumnTransformer` objects. This makes
    it possible for us to first use k-NN imputing on the semi-major axis data (the
    column at index `0`) and then standardize the result. We can then include this
    as part of a pipeline, which gives us tremendous flexibility in how we build our
    models:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅限于在模型中使用管道——它们也可以用于其他`scikit-learn`对象，例如`ColumnTransformer`对象。这使得我们能够首先在半长轴数据（索引为`0`的列）上使用k-NN填充，然后标准化结果。然后，我们可以将其作为管道的一部分，从而在构建模型时提供巨大的灵活性：
- en: '[PRE40]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Just like with the `ColumnTransformer` class, we have a function that can make
    pipelines for us without having to name the steps. Let''s make another pipeline,
    but this time we will use the `make_pipeline()` function:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`ColumnTransformer`类一样，我们有一个函数，可以在不必命名步骤的情况下为我们创建管道。让我们再创建一个管道，这次我们将使用`make_pipeline()`函数：
- en: '[PRE41]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that the steps have been automatically named the lowercase version of the
    class name. As we will see in the next chapter, naming the steps will make it
    easier to optimize model parameters by name. The consistency of the `scikit-learn`
    API will also allow us to use this pipeline to fit our model and make predictions
    using the same object, which we will see in the next section.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，步骤已自动命名为类名的小写版本。正如我们将在下一章看到的那样，命名步骤将使得通过名称优化模型参数变得更容易。`scikit-learn` API的一致性还将使我们能够使用该管道来拟合模型，并使用相同的对象进行预测，这一点我们将在下一节看到。
- en: Clustering
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: We use clustering to divide our data points into groups of similar points. The
    points in each group are more like their fellow group members than those of other
    groups. Clustering is commonly used for tasks such as recommendation systems (think
    of how Netflix recommends what to watch based on what other people who've watched
    similar things are watching) and market segmentation.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用聚类将数据点划分为相似点的组。每个组中的点比其他组中的点更像自己组的成员。聚类通常用于推荐系统（想象一下Netflix是如何根据其他观看过相似内容的人推荐你观看的内容）和市场细分等任务。
- en: 'For example, say we work at an online retailer and want to segment our website
    users for more targeted marketing efforts; we can gather data on time spent on
    the site, page visits, products viewed, products purchased, and much more. Then,
    we can have an unsupervised clustering algorithm find groups of users with similar
    behavior; if we make three groups, we can come up with labels for each group according
    to its behavior:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们在一家在线零售商工作，想要对网站用户进行细分以便进行更有针对性的营销工作；我们可以收集关于在网站上花费的时间、页面访问量、浏览过的产品、购买的产品等数据。然后，我们可以让一个无监督的聚类算法找到具有相似行为的用户群体；如果我们划分为三个群体，可以根据每个群体的行为为其命名：
- en: '![Figure 9.17 – Clustering website users into three groups'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.17 – 将网站用户聚类为三组'
- en: '](img/Figure_9.17_B16834.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.17_B16834.jpg)'
- en: Figure 9.17 – Clustering website users into three groups
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 – 将网站用户聚类为三组
- en: 'Since we can use clustering for unsupervised learning, we will need to interpret
    the groups that are created and then try to derive a meaningful name for each
    group. If our clustering algorithm identified the three clusters in the preceding
    scatter plot, we may be able to make the following behavioral observations:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以使用聚类进行无监督学习，因此我们需要解释创建的群体，然后尝试为每个群体衍生出一个有意义的名称。如果我们的聚类算法在前述的散点图中识别出了这三个群体，我们可能能够做出以下行为观察：
- en: '**Frequent customers (group 0)**: Purchase a lot and look at many products.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频繁购买的客户（第0组）**：购买很多并查看许多产品。'
- en: '**Occasional customers (group 1)**: Have made some purchases, but less than
    the most frequent customers.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偶尔购买的客户（第1组）**：有过一些购买，但少于最常购买的客户。'
- en: '**Browsers (group 2)**: Visit the website, but haven''t bought anything.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**浏览者（第2组）**：访问了网站，但还没有购买任何东西。'
- en: Once these groups have been identified, the marketing team can focus on marketing
    to each of these groups differently; it's clear that the frequent customers will
    do more for the bottom line, but if they are already buying a lot, perhaps the
    marketing budget is better utilized trying to increase the purchases of the occasional
    customers or converting browsers into occasional customers.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦识别出这些群体，营销团队就可以针对每个群体采取不同的营销策略；显然，频繁的客户对盈利有更大的贡献，但如果他们已经购买了很多，也许营销预算更应该用来增加偶尔购买的客户的购买量，或者将浏览者转化为偶尔购买的客户。
- en: Important note
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Deciding on the number of groups to create can clearly influence how the groups
    are later interpreted, meaning that this is not a trivial decision. We should
    at least visualize our data and obtain some domain knowledge on it before attempting
    to guess the number of groups to split it into.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 决定要创建多少个群体显然会影响这些群体后续的解释，这意味着这并不是一个简单的决定。在尝试猜测将数据分成多少个群体之前，我们至少应该可视化我们的数据并获得一些领域知识。
- en: Alternatively, clustering can be used in a supervised fashion if we know the
    group labels for some of the data for training purposes. Say we collected data
    on login activity, like in [*Chapter 8*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172),
    *Rule-Based Anomaly Detection*, but we had some examples of what attacker activity
    looks like; we could gather those data points for all activity and then use a
    clustering algorithm to assign to the valid users group or to the attacker group.
    Since we have the labels, we can tweak our input variables and/or the clustering
    algorithm we use to best align these groups to their true group.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，如果我们知道某些数据的群体标签用于训练目的，聚类也可以以有监督的方式使用。假设我们收集了关于登录活动的数据，就像在[*第8章*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172)中提到的，*基于规则的异常检测*，但我们有一些关于攻击者活动的样本；我们可以为所有活动收集这些数据点，然后使用聚类算法将其分配到有效用户群体或攻击者群体。由于我们有标签，我们可以调整输入变量和/或使用的聚类算法，以最佳方式将这些群体与其真实群体对齐。
- en: k-means
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means
- en: The clustering algorithms offered by `scikit-learn` can be found in the `cluster`
    module's documentation at [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster).
    We will take a look at **k-means**, which iteratively assigns points to the nearest
    group using the distance from the **centroid** of the group (center point), making
    *k* groups. Since this model uses distance calculations, it is imperative that
    we understand the effect scale will have on our results beforehand; we can then
    decide which columns, if any, to scale.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 提供的聚类算法可以在 `cluster` 模块的文档中找到，地址是 [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster)。我们将查看
    **k-means**，它通过距离**质心**（簇的中心点）最近的组来迭代地分配点，形成 *k* 个组。由于该模型使用距离计算，因此我们必须事先了解数据的尺度对结果的影响；然后可以决定是否需要对某些列进行缩放。'
- en: Important note
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: There are many ways to measure the distance between points in space. Often,
    Euclidean distance, or straight-line distance, is the default; however, another
    common one is Manhattan distance, which can be thought of as city-block distance.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 测量空间中点之间距离的方法有很多种。通常情况下，欧几里得距离或直线距离是默认的度量方法；然而，另一种常见的度量是曼哈顿距离，可以被看作是城市街区距离。
- en: When we plotted out the period versus the semi-major axis for all the planets
    using a log scale for the period, we saw a nice separation of the planets along
    an arc. We are going to use k-means to find groups of planets with similar orbits
    along that arc.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将所有行星的周期与半长轴绘制在一起，并使用周期的对数刻度时，我们看到了行星沿弧线的良好分离。我们将使用 k-means 来找出在这条弧线上具有相似轨道的行星组。
- en: Grouping planets by orbit characteristics
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根据轨道特征分组行星
- en: 'As we discussed in the *Preprocessing data* section, we can build a pipeline
    to scale and then model our data. Here, our model will be a `KMeans` object that
    makes eight clusters (for the number of planets in our solar system—sorry, Pluto).
    Since the k-means algorithm randomly picks its starting centroids, it''s possible
    to get different cluster results unless we specify the seed. Therefore, we also
    provide `random_state=0` for reproducibility:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 *数据预处理* 部分讨论的那样，我们可以构建一个管道来缩放数据并建模。在这里，我们的模型将是一个 `KMeans` 对象，它将行星分为八个簇（对应我们太阳系中的行星数量——抱歉，冥王星）。由于
    k-means 算法随机选择其初始质心，除非我们指定种子，否则可能会得到不同的聚类结果。因此，我们还提供了 `random_state=0` 以确保可复现性：
- en: '[PRE42]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Once we have our pipeline, we fit it on all the data since we aren''t trying
    to predict anything (in this case)—we just want to find similar planets:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了我们的管道，就可以将其拟合到所有数据上，因为我们不打算进行任何预测（在这种情况下）——我们只是想找出相似的行星：
- en: '[PRE43]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Once the model is fit to our data, we can use the `predict()` method to get
    the cluster labels for each point (on the same data that we used previously).
    Let''s take a look at the clusters that k-means identified:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将模型拟合到数据上，就可以使用 `predict()` 方法为每个点获取簇标签（使用的是我们之前使用过的数据）。让我们来看看 k-means 所识别的簇：
- en: '[PRE44]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Mercury and Venus landed in the same cluster, as did Earth and Mars. Jupiter,
    Saturn, and Uranus each belong to separate clusters, while Neptune and Pluto share
    a cluster:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 水星和金星落入了同一个簇，地球和火星也属于同一簇。木星、土星和天王星分别属于不同的簇，而海王星和冥王星共享一个簇：
- en: '![Figure 9.18 – Eight clusters of planets identified by k-means'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.18 – 通过 k-means 算法识别的八个行星簇'
- en: '](img/Figure_9.18_B16834.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.18_B16834.jpg)'
- en: Figure 9.18 – Eight clusters of planets identified by k-means
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 – 通过 k-means 算法识别的八个行星簇
- en: We picked eight clusters arbitrarily here, since this is the number of planets
    in our solar system. Ideally, we would have some domain knowledge about the true
    groupings or need to pick a specific number. For example, say we want to fit wedding
    guests at five tables so that they all get along, then our *k* is 5; if we can
    run three marketing campaigns on user groups, we have a *k* of 3\. If we have
    no intuition as to the number of groups there will be in the data, a rule of thumb
    is to try the square root of our observations, but this can yield an unmanageable
    amount of clusters. Therefore, if it doesn't take too long to create many k-means
    models on our data, we can use the elbow point method.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里随意选择了八个聚类，因为这是我们太阳系中的行星数量。理想情况下，我们应该对真实的分组情况有一些领域知识，或者需要选择一个特定的数量。例如，假设我们要将婚礼宾客分配到五张桌子上，以确保他们相处融洽，那么我们的
    *k* 就是 5；如果我们可以在用户群体上运行三个营销活动，那么我们的 *k* 就是 3。如果我们无法直观地判断数据中将有多少个组，一条经验法则是尝试观察数的平方根，但这可能会导致不可管理的聚类数量。因此，如果在我们的数据上创建许多
    k-means 模型不会太耗时，我们可以使用肘部点方法。
- en: The elbow point method for determining k
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于确定 k 的肘部点方法
- en: The **elbow point method** involves creating multiple models with many values
    of *k* and plotting each model's **inertia** (**within-cluster sum of squares**)
    versus the number of clusters. We want to minimize the sum of squared distances
    from points to their cluster's center while not creating too many clusters.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '**肘部点方法**涉及使用多个不同 *k* 值创建多个模型，并绘制每个模型的**惯性**（**聚类内平方和**）与聚类数量的关系图。我们希望最小化点到其聚类中心的平方距离总和，同时避免创建过多的聚类。'
- en: 'The `ml_utils.elbow_point` module contains our `elbow_point()` function, which
    has been reproduced here:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`ml_utils.elbow_point` 模块包含我们的 `elbow_point()` 函数，已在此处复现：'
- en: '[PRE45]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let''s use the elbow point method to find an appropriate value for *k*:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用肘部点方法来找到一个合适的 *k* 值：
- en: '[PRE46]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The point at which we see diminishing returns is an appropriate *k*, which
    may be around two or three here:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到收益递减的点是一个合适的*k*值，在这里可能是二或三：
- en: '![Figure 9.19 – Interpreting an elbow point plot'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.19 – 解释肘部点图'
- en: '](img/Figure_9.19_B16834.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.19_B16834.jpg)'
- en: Figure 9.19 – Interpreting an elbow point plot
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 – 解释肘部点图
- en: 'If we create just two clusters, we divide the planets into a group with most
    of the planets (orange) and a second group with only a few in the upper-right
    (blue), which are likely to be outliers:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只创建两个聚类，我们将行星分为一组包含大多数行星（橙色），另一组位于右上方只有少数几个（蓝色），这些可能是离群点：
- en: '![Figure 9.20 – Two clusters of planets identified by k-means'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.20 – 通过 k-means 方法识别的两个行星聚类'
- en: '](img/Figure_9.20_B16834.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.20_B16834.jpg)'
- en: Figure 9.20 – Two clusters of planets identified by k-means
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20 – 通过 k-means 方法识别的两个行星聚类
- en: Note that while this may have been an appropriate amount of clusters, it doesn't
    tell us as much as the previous attempt. If we wanted to know about planets that
    are similar to each of the planets in our solar system, we would want to use a
    larger *k*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然这可能是一个合适的聚类数量，但它没有提供像之前的尝试那么多的信息。如果我们想了解与我们太阳系中每个行星相似的行星，我们需要使用更大的*k*。
- en: Interpreting centroids and visualizing the cluster space
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释质心并可视化聚类空间
- en: Since we standardized our data before clustering, we can look at the `cluster_centers_`
    attribute of the model. The centroid of the blue cluster is located at (18.9,
    20.9), which is in the (semi-major axis, period) format; remember, these are Z-scores,
    so these are quite far from the rest of the data. The orange cluster, on the other
    hand, is centered at (-0.035, -0.038).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在聚类之前我们已经标准化了数据，因此可以查看模型的`cluster_centers_`属性。蓝色聚类的中心位于(18.9, 20.9)，以（半长轴，周期）格式表示；请记住，这些是
    Z 分数，因此这些值与其余数据相差较远。另一方面，橙色聚类的中心位于(-0.035, -0.038)。
- en: 'Let''s build a visualization that shows us the location of the centroids with
    the scaled input data and the cluster distance space (where points are represented
    as the distance to their cluster''s centroid). First, we will set up our layout
    for a smaller plot inside of a larger one:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个可视化，展示标定输入数据的质心位置以及聚类距离空间（其中点表示到其聚类中心的距离）。首先，我们将在一个更大的图表中设置一个较小的子图布局：
- en: '[PRE47]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, we grab the scaled version of the input data and the distances between
    those data points and the centroid of the cluster they belong to. We can use the
    `transform()` and `fit_transform()` (`fit()` followed by `transform()`) methods
    to convert the input data into cluster distance space. We get NumPy `ndarrays`
    back, where each value in the outer array represents the coordinates of a point:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们获取输入数据的缩放版本，以及这些数据点与它们所属聚类的质心之间的距离。我们可以使用`transform()`和`fit_transform()`（即先调用`fit()`再调用`transform()`）方法将输入数据转换到聚类距离空间。返回的是NumPy的`ndarrays`，其中外部数组中的每个值表示一个点的坐标：
- en: '[PRE48]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Since we know that each array in the outer array will have the semi-major axis
    as the first entry and the period as the second, we use `[:,0]` to select all
    the semi-major axis values and `[:,1]` to select all the period values. These
    will be the *x* and *y* for our scatter plot. Note that we actually don''t need
    to call `predict()` to get the cluster labels for the data because we want the
    labels for the data we trained the model on; this means that we can use the `labels_`
    attribute of the `KMeans` object:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道外部数组中的每个数组将以半长轴作为第一个元素，周期作为第二个元素，因此我们使用`[:,0]`来选择所有半长轴值，使用`[:,1]`来选择所有周期值。这些将作为我们散点图的
    *x* 和 *y*。请注意，我们实际上不需要调用`predict()`来获取数据的聚类标签，因为我们想要的是我们用来训练模型的数据的标签；这意味着我们可以使用`KMeans`对象的`labels_`属性：
- en: '[PRE49]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Lastly, we annotate the location of the centroids on the outer plot, which
    shows the scaled data:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在外部图中标注了质心的位置，图中显示了缩放后的数据：
- en: '[PRE50]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In the resulting plot, we can easily see that the three blue points are quite
    different from the rest and that they are the only members of the second cluster:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在结果图中，我们可以清楚地看到，三个蓝色点与其他点有明显不同，它们是第二个聚类的唯一成员：
- en: '![Figure 9.21 – Visualizing the planets in the cluster distance space'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.21 – 可视化聚类距离空间中的行星'
- en: '](img/Figure_9.21_B16834.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.21_B16834.jpg)'
- en: Figure 9.21 – Visualizing the planets in the cluster distance space
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.21 – 可视化聚类距离空间中的行星
- en: 'So far, we have been using `transform()` or combination methods, such as `fit_predict()`
    or `fit_transform()`, but not all models will support these methods. We will see
    a slightly different workflow in the *Regression* and *Classification* sections.
    In general, most `scikit-learn` objects will support the following, based on what
    they are used for:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用`transform()`或组合方法（如`fit_predict()`或`fit_transform()`），但并不是所有模型都支持这些方法。在*回归*和*分类*部分，我们将看到一个略有不同的工作流程。通常，大多数`scikit-learn`对象将根据它们的用途支持以下方法：
- en: '![Figure 9.22 – General reference for the scikit-learn model API'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.22 – scikit-learn模型API的通用参考'
- en: '](img/Figure_9.22_B16834.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.22_B16834.jpg)'
- en: Figure 9.22 – General reference for the scikit-learn model API
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22 – scikit-learn模型API的通用参考
- en: 'Now that we have built a few models, we are ready for the next step: quantifying
    their performance. The `metrics` module in `scikit-learn` contains various metrics
    for evaluating model performance across clustering, regression, and classification
    tasks; the API lists the functions at [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).
    Let''s discuss how to evaluate an unsupervised clustering model next.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了一些模型，准备进行下一步：量化它们的表现。`scikit-learn`中的`metrics`模块包含了用于评估聚类、回归和分类任务的各种度量指标；可以在[https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)查看API中列出的函数。接下来我们讨论如何评估一个无监督聚类模型。
- en: Evaluating clustering results
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估聚类结果
- en: The most important criterion for evaluating our clustering results is that they
    are useful for what we set out to do; we used the elbow point method to pick an
    appropriate value for *k*, but that wasn't as useful to us as the original model
    with eight clusters. That being said, when looking to quantify the performance,
    we need to pick metrics that match the type of learning we performed.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 评估我们聚类结果的最重要标准是它们对我们所要做的事情有用；我们使用肘部法则选择了一个合适的*k*值，但这并不像原始模型中的八个聚类那样对我们有用。也就是说，在量化表现时，我们需要选择与我们所做学习类型相匹配的度量指标。
- en: When we know the true clusters for our data, we can check that our clustering
    model places the points together in a cluster as they are in the true cluster.
    The cluster label given by our model can be different than the true one—all that
    matters is that the points in the same true cluster are also together in the predicted
    clusters. One such metric is the **Fowlkes-Mallows Index**, which we will see
    in the end-of-chapter exercises.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们知道数据的真实聚类时，我们可以检查聚类模型是否将数据点正确地归类到真实聚类中。我们模型所给出的聚类标签可能与真实标签不同——唯一重要的是，真实聚类中的点也应该被归为同一聚类。一个这样的指标是**Fowlkes-Mallows指数**，我们将在章节末的练习中看到它。
- en: With the planets data, we performed unsupervised clustering because we don't
    have labels for each data point, and therefore, we can't measure how well we did
    against those. This means that we have to use metrics that evaluate aspects of
    the clusters themselves, such as how far apart they are and how close the points
    in a cluster are together. We can compare multiple metrics to get a more well-rounded
    evaluation of the performance.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在行星数据集上，我们进行了无监督聚类，因为我们没有每个数据点的标签，因此无法评估模型与这些标签的匹配情况。这意味着我们必须使用评估聚类本身方面的指标，比如它们之间的距离和聚类内点的紧密度。我们可以比较多个指标，获得对性能的更全面评估。
- en: 'One such method is called the **silhouette coefficient**, which helps quantify
    cluster separation. It is calculated by subtracting the mean of the distances
    between every two points in a cluster (*a*) from the mean of distances between
    points in a given cluster and the closest different cluster (*b*) and dividing
    by the maximum of the two:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 一种这样的方法是称为**轮廓系数**，它有助于量化聚类分离度。其计算方法是将聚类中每两点之间距离的均值（*a*）与给定聚类与最接近的不同聚类之间的点距离的均值（*b*）相减，再除以两者中的最大值：
- en: '![](img/Formula_09_001.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_001.jpg)'
- en: 'This metric returns values in the range [-1, 1], where -1 is the worst (clusters
    are wrongly assigned) and 1 is the best; values near 0 indicate overlapping clusters.
    The higher this number is, the better defined (more separated) the clusters are:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标返回的值范围为[-1, 1]，其中-1表示最差（聚类分配错误），1表示最佳；值接近0则表示聚类重叠。这个数字越高，聚类越明确（分离度越大）：
- en: '[PRE51]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Another score we could use to evaluate our clustering result is the ratio of
    **within-cluster distances** (distances between points in a cluster) to the **between-cluster
    distances** (distances between points in different clusters), called the **Davies-Bouldin
    score**. Values closer to zero indicate better partitions between clusters:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们可以用来评估聚类结果的评分是**聚类内距离**（同一聚类内点之间的距离）与**聚类间距离**（不同聚类间点之间的距离）之比，这被称为**Davies-Bouldin评分**。值越接近零，表示聚类间的分离越好：
- en: '[PRE52]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'One last metric for unsupervised clustering that we will discuss here is the
    **Calinski and Harabasz score**, or **Variance Ratio Criterion**, which is the
    ratio of dispersion within a cluster to dispersion between clusters. Higher values
    indicate better defined (more separated) clusters:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里讨论的最后一个无监督聚类指标是**Calinski和Harabasz评分**，或称**方差比准则**，它是聚类内的离散度与聚类间离散度的比值。较高的值表示聚类更为明确（更为分离）：
- en: '[PRE53]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: For a complete list of clustering evaluation metrics offered by `scikit-learn`
    (including supervised clustering) and when to use them, check out the *Clustering
    performance evaluation* section of their guide at [https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看`scikit-learn`提供的完整聚类评估指标列表（包括监督聚类）以及何时使用它们，请查阅他们指南中的*聚类性能评估*部分，网址：[https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation)。
- en: Regression
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归
- en: With the planets dataset, we want to predict the length of the year, which is
    a numeric value, so we will turn to regression. As mentioned at the beginning
    of this chapter, regression is a technique for modeling the strength and magnitude
    of the relationship between independent variables (our `X` data)—often called
    `y` data) that we want to predict.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在行星数据集中，我们希望预测年份的长度，这是一个数值型的变量，因此我们将使用回归。正如本章开头所提到的，回归是一种用于建模自变量（我们的`X`数据）与因变量（通常称为`y`数据）之间关系的强度和大小的技术，我们希望通过这种技术来进行预测。
- en: Linear regression
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'Scikit-learn provides many algorithms that can handle regression tasks, ranging
    from decision trees to linear regression, spread across modules according to the
    various algorithm classes. However, typically, the best starting point is a linear
    regression, which can be found in the `linear_model` module. In **simple linear
    regression**, we fit our data to a line of the following form:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn提供了许多可以处理回归任务的算法，从决策树到线性回归，根据各种算法类别分布在不同模块中。然而，通常最好的起点是线性回归，它可以在`linear_model`模块中找到。在**简单线性回归**中，我们将数据拟合到以下形式的直线上：
- en: '![](img/Formula_09_002.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_002.jpg)'
- en: Here, epsilon (*ε*) is the error term and betas (*β*) are coefficients.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，epsilon (*ε*)是误差项，betas (*β*)是系数。
- en: Important note
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The coefficients we get from our model are those that minimize the **cost function**,
    or error between the observed values (*y*) and those predicted with the model
    (*ŷ*, pronounced *y-hat*). Our model gives us estimates of these coefficients,
    and we write them as ![](img/Formula_09_004.png) (pronounced *beta-hat*).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从模型中得到的系数是那些最小化**代价函数**的系数，即最小化观察值(*y*)与模型预测值(*ŷ*，发音为*y-hat*)之间的误差。我们的模型给出了这些系数的估计值，我们将其表示为
    ![](img/Formula_09_004.png)（发音为*beta-hat*）。
- en: 'However, if we want to model additional relationships, we need to use **multiple
    linear regression**, which contains multiple regressors:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们想要建模其他关系，我们需要使用**多元线性回归**，它包含多个回归变量：
- en: '![](img/Formula_09_005.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_005.jpg)'
- en: Linear regression in `scikit-learn` uses **ordinary least squares** (**OLS**),
    which yields the coefficients that minimize the sum of squared errors (measured
    as the distance between *y* and *ŷ*). The coefficients can be found using the
    closed-form solution, or estimated with optimization methods, such as **gradient
    descent**, which uses the negative gradient (direction of steepest ascent calculated
    with partial derivatives) to determine which coefficients to try next (see the
    link in the *Further reading* section for more information). We will use gradient
    descent in [*Chapter 11*](B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237), *Machine
    Learning Anomaly Detection*.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`中的线性回归使用**最小二乘法**（**OLS**），该方法得到的系数最小化平方误差之和（即*y*与*ŷ*之间的距离）。这些系数可以通过闭式解法找到，或者通过优化方法估计，例如**梯度下降法**，后者使用负梯度（通过偏导数计算的最陡上升方向）来确定下一个要尝试的系数（有关更多信息，请参见*进一步阅读*部分中的链接）。我们将在[*第11章*](B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237)中使用梯度下降法，主题为*机器学习中的异常检测*。'
- en: Important note
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Linear regression makes some assumptions about the data, which we must keep
    in mind when choosing to use this technique. It assumes that the residuals are
    normally distributed and homoskedastic and that there is no multicollinearity
    (high correlations between the regressors).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归对数据做出了一些假设，我们在选择使用这种技术时必须牢记这些假设。它假设残差服从正态分布且同方差，并且没有多重共线性（回归变量之间的高度相关性）。
- en: Now that we have a little background on how linear regression works, let's build
    a model to predict the orbit period of a planet.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对线性回归的工作原理有了些了解，让我们建立一个模型来预测行星的轨道周期。
- en: Predicting the length of a year on a planet
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测行星的年长度
- en: 'Before we can build our model, we must isolate the columns that are used to
    predict (`semimajoraxis`, `mass`, and `eccentricity`) from the column that will
    be predicted (`period`):'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们建立模型之前，我们必须从用于预测的列（`semimajoraxis`、`mass`和`eccentricity`）中分离出要预测的列（`period`）：
- en: '[PRE54]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This is a supervised task. We want to be able to predict the length of a year
    on a planet using its semi-major axis, mass, and eccentricity of orbit, and we
    have the period lengths for most of the planets in the data. Let''s create a 75/25
    split of training to testing data so that we can assess how well this model predicts
    year length:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个监督任务。我们希望能够通过行星的半长轴、质量和轨道偏心率来预测一个行星的年长度，并且我们已有大多数行星的周期数据。让我们将数据按75/25的比例分为训练集和测试集，以便评估该模型对年长度的预测效果：
- en: '[PRE55]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Once we have separated the data into the training and testing sets, we can
    create and fit the model:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将数据分成训练集和测试集，我们就可以创建并拟合模型：
- en: '[PRE56]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: This fitted model can be used to examine the estimated coefficients and also
    to predict the value of the dependent variable for a given set of independent
    variables. We will cover both of these use cases in the next two sections.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 这个拟合模型可以用来检查估计的系数，也可以用来预测给定一组自变量时因变量的值。我们将在接下来的两节中讨论这两种使用情况。
- en: Interpreting the linear regression equation
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释线性回归方程
- en: The equation derived from a linear regression model gives coefficients to quantify
    the relationships between the variables. Care must be exercised when attempting
    to interpret these coefficients if we are dealing with more than a single regressor.
    In the case of multicollinearity, we can't interpret them because we are unable
    to hold all other regressors constant to examine the effect of a single one.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 从线性回归模型中得出的方程给出了系数，用以量化变量之间的关系。在尝试解释这些系数时，必须小心，如果我们处理的是多个回归变量的情况。对于多重共线性，我们无法解释这些系数，因为我们无法保持所有其他回归变量不变，以便检查单个回归变量的影响。
- en: 'Thankfully, the regressors we are using for the planets data aren''t correlated,
    as we saw from the correlation matrix heatmap we made in the *Exploratory data
    analysis* section (*Figure 9.8*). So, let''s get the intercept and coefficients
    from the fitted linear model object:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们用于行星数据的回归变量是没有相关性的，正如我们在*探索性数据分析*部分（*图 9.8*）中通过相关矩阵热图所看到的那样。那么，让我们从拟合的线性模型对象中获取截距和系数：
- en: '[PRE57]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This yields the following equation for our linear regression model of planet
    year length:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了我们行星年长度的线性回归模型方程：
- en: '![](img/Formula_09_006.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_006.jpg)'
- en: 'In order to interpret this more completely, we need to understand the units
    everything is in:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更完整地解释这一点，我们需要了解所有单位是什么：
- en: '`period` (length of year): Earth days'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`period`（年长度）：地球日'
- en: '`semimajoraxis`: **Astronomical units** (**AUs**)'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semimajoraxis`：**天文单位**（**AUs**）'
- en: '`mass`: Jupiter masses (planet mass divided by Jupiter''s mass)'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mass`：木星质量（行星质量除以木星质量）'
- en: '`eccentricity`: N/A'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eccentricity`：无'
- en: Tip
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示
- en: An astronomical unit is the average distance between the Earth and the Sun,
    which is equivalent to 149,597,870,700 meters.
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个天文单位是地球与太阳之间的平均距离，约为149,597,870,700米。
- en: 'The intercept in this particular model doesn''t have any meaning: if the planet
    had a semi-major axis of zero, no mass, and a perfect circle eccentricity, its
    year would be -623 Earth days long. A planet must have a non-negative, non-zero
    period, semi-major axis, and mass, so this clearly makes no sense. We can, however,
    interpret the other coefficients. The equation says that, holding mass and eccentricity
    constant, adding one additional AU to the semi-major axis distance increases the
    year length by 1,880 Earth days. Holding the semi-major axis and eccentricity
    constant, each additional Jupiter mass decreases the year length by 90.2 Earth
    days.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定模型中的截距没有任何意义：如果行星的半长轴为零，质量为零，且偏心率为完美圆形，则它的一年将是-623个地球日。一个行星必须具有非负、非零的周期、半长轴和质量，因此这显然没有意义。然而，我们可以解释其他系数。方程告诉我们，在保持质量和偏心率不变的情况下，增加一个天文单位（AU）到半长轴距离将使一年长度增加1,880个地球日。在保持半长轴和偏心率不变的情况下，每增加一个木星质量将使一年长度减少90.2个地球日。
- en: Going from a perfectly circular orbit (`eccentricity=0`) to a nearly parabolic
    escape orbit (`eccentricity=1`) will decrease the year length by 3,201 Earth days;
    note that these are approximate for this term because, with a parabolic escape
    orbit, the planet will never return, and consequently, this equation wouldn't
    make sense. In fact, if we tried to use this equation for eccentricities greater
    than or equal to 1, we would be extrapolating because we have no such values in
    the training data. This is a clear example of when extrapolation doesn't work.
    The equation tells us that the larger the eccentricity, the shorter the year,
    but once we get to eccentricities of one and beyond, the planets never come back
    (they have reached escape orbits), so the year is infinite.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个完美的圆形轨道（`eccentricity=0`）到一个几乎是抛物线逃逸轨道（`eccentricity=1`），将使一年长度减少3,201个地球日；注意，这些数值对于这个项来说是近似值，因为对于抛物线逃逸轨道，行星将永远不会回来，因此这个方程就没有意义了。实际上，如果我们试图在偏心率大于或等于1的情况下使用这个方程，我们将进行外推，因为训练数据中没有这样的值。这是外推不起作用的一个明确例子。方程告诉我们，偏心率越大，一年就越短，但一旦偏心率达到1或更大，行星就永远不再回来（它们已经达到逃逸轨道），因此一年是无限的。
- en: All of the eccentricity values in the training data are in the range [0, 1),
    so we are interpolating (predicting period values using data in the ranges we
    trained on). This means that as long as the eccentricity of the planet we want
    to predict is also in the range [0, 1), we can use this model to make the prediction.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据中的所有偏心率值都在[0, 1)范围内，因此我们正在进行插值（使用训练数据中的范围预测周期值）。这意味着，只要我们想要预测的行星的偏心率也在[0,
    1)范围内，我们就可以使用这个模型进行预测。
- en: Making predictions
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 做出预测
- en: 'Now that we have an idea of the effect each of our regressors has, let''s use
    our model to make predictions of year length for the planets in the test set:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们大致了解了每个回归器的效果，让我们使用模型对测试集中的行星进行年限预测：
- en: '[PRE58]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Let''s visualize how well we did by plotting the actual and predicted values:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过绘制实际值和预测值来可视化我们的表现：
- en: '[PRE59]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The predicted values seem pretty close to the actual values and follow a similar
    pattern:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值似乎与实际值非常接近，并遵循类似的模式：
- en: '![Figure 9.23 – Predictions versus actual values'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.23 – 预测值与实际值'
- en: '](img/Figure_9.23_B16834.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.23_B16834.jpg)'
- en: Figure 9.23 – Predictions versus actual values
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23 – 预测值与实际值
- en: Tip
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Try running this regression with just the `semimajoraxis` regressor. Some reshaping
    of the data will be necessary, but this will show how much better this performs
    as we add in `eccentricity` and `mass`. In practice, we often have to build many
    versions of our model to find one we are happy with.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试仅使用`semimajoraxis`回归器来运行此回归。数据可能需要进行一些重塑，但这将展示随着我们添加`eccentricity`和`mass`，性能的改善。在实践中，我们通常需要构建多个模型版本，直到找到一个我们满意的。
- en: 'We can check their correlation to see how well our model tracks the true relationship:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查它们的相关性，看看我们的模型与真实关系的契合度：
- en: '[PRE60]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Our predictions are very strongly positively correlated with the actual values
    (0.97 correlation coefficient). Note that the correlation coefficient will tell
    us whether our model moves with the actual data; however, it will not tell us
    whether we are off magnitude-wise. For that, we will use the metrics discussed
    in the following section.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测值与实际值的相关性非常强（相关系数为0.97）。需要注意的是，相关系数可以告诉我们模型与实际数据是否同步变化；但它无法告诉我们在数量级上是否存在偏差。为此，我们将使用下一节讨论的度量标准。
- en: Evaluating regression results
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估回归结果
- en: When looking to evaluate a regression model, we are interested in how much of
    the variance in the data our model is able to capture, as well as how accurate
    the predictions are. We can use a combination of metrics and visuals to assess
    the model for each of these aspects.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估回归模型时，我们关注的是模型能够捕捉到数据方差的程度，以及预测的准确性。我们可以结合度量标准和可视化方法来评估模型在这两个方面的表现。
- en: Analyzing residuals
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析残差
- en: Whenever we work with linear regression, we should visualize our **residuals**,
    or the discrepancies between the actual values and the model's predictions; as
    we learned in [*Chapter 7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146), *Financial
    Analysis – Bitcoin and the Stock Market*, they should be centered around zero
    and homoskedastic (similar variance throughout). We can use a kernel density estimate
    to assess whether the residuals are centered around zero and a scatter plot to
    see if they are homoskedastic.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们使用线性回归时，都应当可视化我们的**残差**，即实际值与模型预测值之间的差异；正如我们在[*第7章*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146)《金融分析——比特币与股市》中学到的那样，残差应围绕零居中且具有同方差性（即整个数据的方差相似）。我们可以使用核密度估计来评估残差是否围绕零居中，并使用散点图来检查它们是否具有同方差性。
- en: 'Let''s look at the utility function in `ml_utils.regression`, which will create
    these subplots for checking the residuals:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下`ml_utils.regression`中的工具函数，它将创建这些子图来检查残差：
- en: '[PRE61]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now, let''s look at the residuals for this linear regression:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看这次线性回归的残差：
- en: '[PRE62]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'It looks like our predictions don''t have a pattern (left subplot), which is
    good; however, they aren''t quite centered around zero and the distribution has
    negative skew (right subplot). These negative residuals occur when the predicted
    year was longer than the actual year:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的预测值没有明显的模式（左侧子图），这是好事；然而，它们并没有完全围绕零对称，且分布有负偏（右侧子图）。这些负残差出现在预测年份长于实际年份时：
- en: '![Figure 9.24 – Examining the residuals'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.24 – 检查残差'
- en: '](img/Figure_9.24_B16834.jpg)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.24_B16834.jpg)'
- en: Figure 9.24 – Examining the residuals
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.24 – 检查残差
- en: Tip
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: If we find patterns in the residuals, our data isn't linear and the chances
    are that visualizing the residuals could help us plan our next move. This may
    mean employing strategies such as polynomial regression or log transformations
    of the data.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在残差中发现模式，说明我们的数据不是线性的，这时可视化残差可能会帮助我们规划下一步的操作。这可能意味着需要采用多项式回归或对数据进行对数变换等策略。
- en: Metrics
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 度量标准
- en: 'In addition to examining the residuals, we should calculate metrics to evaluate
    our regression model. Perhaps the most common is **R****2** (pronounced *R-squared*),
    or the **coefficient of determination**, which quantifies the proportion of variance
    in the dependent variable that we can predict from our independent variables.
    It is calculated by subtracting the ratio of the sum of squared residuals to the
    total sum of squares from 1:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 除了检查残差外，我们还应该计算一些指标来评估回归模型。或许最常见的指标是**R²**（读作*R方*），也叫**决定系数**，它量化了我们能从自变量中预测的因变量方差的比例。它通过从1中减去残差平方和与总平方和的比值来计算：
- en: '![](img/Formula_09_007.jpg)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_007.jpg)'
- en: Tip
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Sigma (*Σ*) represents the sum. The average of the *y* values is denoted as
    *ȳ* (pronounced *y-bar*). The predictions are denoted with *ŷ* (pronounced *y-hat*).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 西格玛（*Σ*）表示总和。*y*值的平均值表示为*ȳ*（读作*y-bar*）。预测值表示为*ŷ*（读作*y-hat*）。
- en: 'This value will be in the range [0, 1], where higher values are better. Objects
    of the `LinearRegression` class in `scikit-learn` use R2 as their scoring method.
    Therefore, we can simply use the `score()` method to calculate it for us:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 该值将在[0, 1]范围内，值越高越好。`scikit-learn`中的`LinearRegression`类对象使用R2作为评分方法。因此，我们可以简单地使用`score()`方法来计算它：
- en: '[PRE63]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We can also get R2 from the `metrics` module:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过`metrics`模块获取R2值：
- en: '[PRE64]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: This model has a very good R2; however, keep in mind that there are many factors
    that affect the period, such as the stars and other planets, which exert a gravitational
    force on the planet in question. Despite this abstraction, our simplification
    does pretty well because the orbital period of a planet is determined in large
    part by the distance that must be traveled, which we account for by using the
    semi-major axis data.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型具有非常好的R2值；然而，记住有许多因素会影响周期，比如恒星和其他行星，它们会对相关行星施加引力。尽管有这样的抽象简化，我们的简化方法表现得相当不错，因为行星的轨道周期在很大程度上由必须行驶的距离决定，我们通过使用半长轴数据来考虑这一点。
- en: 'There is a problem with R2, though; we can keep adding regressors, which would
    make our model more and more complex while at the same time increasing R2\. We
    need a metric that penalizes model complexity. For that, we have **adjusted R**2,
    which will only increase if the added regressor improves the model more than what
    would be expected by chance:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，R2存在一个问题；我们可以不断增加回归变量，这会使我们的模型变得越来越复杂，同时也会提高R2值。我们需要一个可以惩罚模型复杂度的指标。为此，我们有**调整后的R2**，它只有在新增的回归变量比随机预期的更好时，才会增加：
- en: '![](img/Formula_09_008.jpg)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_008.jpg)'
- en: 'Unfortunately, `scikit-learn` doesn''t offer this metric; however, it is very
    easy to implement ourselves. The `ml_utils.regression` module contains a function
    for calculating the adjusted R2 for us. Let''s take a look at it:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，`scikit-learn`没有提供这个指标；然而，我们可以很容易地自己实现。`ml_utils.regression`模块包含一个计算调整后的R2值的函数。让我们来看一下：
- en: '[PRE65]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Adjusted R2 will always be lower than R2\. By using the `adjusted_r2()` function,
    we can see that our adjusted R2 is slightly lower than the R2 value:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的R2值总是低于R2值。通过使用`adjusted_r2()`函数，我们可以看到调整后的R2值略低于R2值：
- en: '[PRE66]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Unfortunately, R2 (and adjusted R2) values don''t tell us anything about our
    prediction error or even whether we specified our model correctly. Think back
    to when we discussed Anscombe''s quartet in [*Chapter 1*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015),
    *Introduction to Data Analysis*. These four different datasets have the same summary
    statistics. They also have the same R2 when fit with a linear regression line
    (0.67), despite some of them not indicating a linear relationship:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，R2（以及调整后的R2）值并不能告诉我们关于预测误差的信息，甚至无法告诉我们是否正确地指定了模型。回想一下我们在[*第1章*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015)中讨论过的安斯科姆四重奏，*数据分析导论*。这四个不同的数据集具有相同的汇总统计值。尽管其中一些数据集并未呈现线性关系，它们在使用线性回归线拟合时R2值也相同（0.67）：
- en: '![Figure 9.25 – R2 can be misleading'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.25 – R2可能会误导'
- en: '](img/Figure_9.25_B16834.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.25_B16834.jpg)'
- en: Figure 9.25 – R2 can be misleading
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.25 – R2可能会误导
- en: 'Another metric offered by `scikit-learn` is the **explained variance score**,
    which tells us the percentage of the variance that is explained by our model.
    We want this as close to 1 as possible:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个由`scikit-learn`提供的指标是**解释方差得分**，它告诉我们模型所解释的方差百分比。我们希望这个值尽可能接近1：
- en: '![](img/Formula_09_009.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_009.jpg)'
- en: 'We can see that our model explains 92% of the variance:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，模型解释了92%的方差：
- en: '[PRE67]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: We aren't limited to looking at variance when evaluating our regression models;
    we can also look at the magnitude of the errors themselves. The remaining metrics
    we will discuss in this section all yield errors in the same unit of measurement
    we are using for prediction (Earth days here), so we can understand the meaning
    of the size of the error.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估回归模型时，我们不仅限于观察方差；我们还可以查看误差本身的大小。本节接下来讨论的所有其他度量标准都以我们用于预测的相同单位（这里是地球日）来表示误差，因此我们可以理解误差大小的含义。
- en: '**Mean absolute error** (**MAE**) tells us the average error our model made
    in either direction. Values range from 0 to ∞ (infinity), with smaller values
    being better:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）告诉我们模型在两个方向上所犯的平均误差。其值范围从0到∞（无穷大），值越小越好：'
- en: '![](img/Formula_09_010.jpg)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_010.jpg)'
- en: 'By using the `scikit-learn` function, we can see that our MAE was 1,369 Earth
    days:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`scikit-learn`函数，我们可以看到我们的平均绝对误差（MAE）为1,369地球日：
- en: '[PRE68]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '**Root mean squared error** (**RMSE**) allows for further penalization of poor
    predictions:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方根误差**（**RMSE**）允许对差的预测进行进一步惩罚：'
- en: '![](img/Formula_09_011.jpg)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_011.jpg)'
- en: 'Scikit-learn provides a function for the **mean squared error** (**MSE**),
    which is the portion of the preceding equation inside the square root; therefore,
    we simply have to take the square root of the result. We would use this metric
    when large errors are undesirable:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn提供了**均方误差**（**MSE**）函数，它是前述方程中平方根部分的计算结果；因此，我们只需对结果取平方根即可。我们会在不希望出现大误差的情况下使用此度量：
- en: '[PRE69]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'An alternative to all these mean-based measures is the **median absolute error**,
    which is the median of the residuals. This can be used in cases where we have
    a few outliers in our residuals, and we want a more accurate description of the
    bulk of the errors. Note that this is smaller than the MAE for our data:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基于均值的度量方法的替代是**中位数绝对误差**，即残差的中位数。当我们的残差中有少量异常值时，可以使用此方法，旨在更准确地描述大部分误差。请注意，对于我们的数据，这个值比MAE要小：
- en: '[PRE70]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: There is also a `mean_squared_log_error()` function, which can only be used
    for non-negative values. Some of the predictions are negative, which prevents
    us from using this. Negative predictions happen when the semi-major axis is very
    small (less than 1) since that is the only portion of the regression equation
    with a positive coefficient. If the semi-major axis isn't large enough to balance
    out the rest of our equation, the prediction will be negative and, thus, automatically
    incorrect. For a complete list of regression metrics offered by `scikit-learn`,
    check out [https://scikit-learn.org/stable/modules/classes.html#regression-metrics](https://scikit-learn.org/stable/modules/classes.html#regression-metrics).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个`mean_squared_log_error()`函数，它只能用于非负值。当某些预测值为负时，这会使我们无法使用此函数。负预测值的出现是因为半长轴非常小（小于1），这是回归方程中唯一带有正系数的部分。如果半长轴不足以平衡方程的其余部分，预测值将为负，从而自动成为错误的预测。有关`scikit-learn`提供的回归度量标准的完整列表，请查看[https://scikit-learn.org/stable/modules/classes.html#regression-metrics](https://scikit-learn.org/stable/modules/classes.html#regression-metrics)。
- en: Classification
  id: totrans-427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: The goal of classification is to determine how to label data using a set of
    discrete labels. This probably sounds similar to supervised clustering; however,
    in this case, we don't care how close members of the groups are spatially. Instead,
    we concern ourselves with classifying them with the correct class label. Remember,
    in [*Chapter 8*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172), *Rule-Based
    Anomaly Detection*, when we classified the IP addresses as valid user or attacker?
    We didn't care how well-defined clusters of IP addresses were—we just wanted to
    find the attackers.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 分类的目标是通过一组离散标签来确定如何标记数据。这听起来可能与有监督聚类相似；然而，在这种情况下，我们不关心组内成员在空间上有多接近。相反，我们关注的是如何为它们分配正确的类别标签。记住，在[*第8章*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172)，*基于规则的异常检测*中，当我们将IP地址分类为有效用户或攻击者时，我们并不关心IP地址的聚类是否定义得很好——我们只关心找到攻击者。
- en: Just as with regression, `scikit-learn` provides many algorithms for classification
    tasks. These are spread across modules, but will usually say *Classifier* at the
    end for classification tasks, as opposed to *Regressor* for regression tasks.
    Some common methods are logistic regression, **support vector machines** (**SVMs**),
    k-NN, decision trees, and random forests; here, we will discuss logistic regression.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 就像回归问题一样，`scikit-learn` 提供了许多用于分类任务的算法。这些算法分布在不同的模块中，但通常会在分类任务的名称后加上 *Classifier*，而回归任务则加上
    *Regressor*。一些常见的方法有逻辑回归、**支持向量机**（**SVM**）、k-NN、决策树和随机森林；这里我们将讨论逻辑回归。
- en: Logistic regression
  id: totrans-430
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'Logistic regression is a way to use linear regression to solve classification
    tasks. However, it uses the logistic sigmoid function to return probabilities
    in the range [0, 1] that can be mapped to class labels:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种使用线性回归来解决分类任务的方法。它使用逻辑 sigmoid 函数返回在 [0, 1] 范围内的概率，这些概率可以映射到类别标签：
- en: '![Figure 9.26 – The logistic sigmoid function'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.26 – 逻辑 sigmoid 函数'
- en: '](img/Figure_9.26_B16834.jpg)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.26_B16834.jpg)'
- en: Figure 9.26 – The logistic sigmoid function
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.26 – 逻辑 sigmoid 函数
- en: Let's use logistic regression to classify red wines as high or low quality and
    to classify wines as red or white based on their chemical properties. We can treat
    logistic regression as we did the linear regression in the previous section, using
    the `linear_model` module in `scikit-learn`. Just like the linear regression problem,
    we will be using a supervised method, so we have to split our data into testing
    and training sets.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用逻辑回归将红酒分为高质量或低质量，并根据其化学性质将酒分为红酒或白酒。我们可以像处理前一节中的线性回归一样使用逻辑回归，使用 `scikit-learn`
    的 `linear_model` 模块。就像线性回归问题一样，我们将使用监督学习方法，因此需要将数据划分为测试集和训练集。
- en: Tip
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: While the examples discussed in this section are both binary classification
    problems (two classes), `scikit-learn` provides support for multiclass problems
    as well. The process of building multiclass models will be nearly identical to
    the binary case but may require passing an additional parameter to let the model
    know that there are more than two classes. You will have a chance to build a multiclass
    classification model in the exercises at the end of this chapter.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本节讨论的示例都是二分类问题（两类），`scikit-learn` 同样支持多分类问题。构建多分类模型的过程与二分类几乎相同，但可能需要传递一个额外的参数，以便模型知道有多个类别。你将在本章末的练习中有机会构建一个多分类模型。
- en: Predicting red wine quality
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测红酒质量
- en: 'We made the `high_quality` column back at the beginning of this chapter, but
    remember that there was a large imbalance in the number of red wines that were
    high quality. So, when we split our data, we will stratify by that column for
    a stratified random sample to make sure that both the training and testing sets
    preserve the ratio of high-quality to low-quality wines in the data (roughly 14%
    are high quality):'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章开始时创建了 `high_quality` 列，但请记住，红酒中的高质量比例严重失衡。因此，在划分数据时，我们将按该列进行分层抽样，确保训练集和测试集保持数据中高质量和低质量酒的比例（大约
    14% 是高质量）：
- en: '[PRE71]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Let''s make a pipeline that will first standardize all of our data and then
    build a logistic regression. We will provide the seed (`random_state=0`) for reproducibility
    and `class_weight=''balanced''` to have `scikit-learn` compute the weights of
    the classes, since we have an imbalance:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个管道，首先标准化所有数据，然后构建一个逻辑回归模型。我们将提供种子（`random_state=0`）以确保结果可复现，并将 `class_weight='balanced'`
    传递给 `scikit-learn`，让它计算类别的权重，因为我们有类别不平衡问题：
- en: '[PRE72]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The class weights determine how much the model will be penalized for wrong predictions
    for each class. By selecting balanced weights, wrong predictions on smaller classes
    will carry more weight, where the weight will be inversely proportional to the
    frequency of the class in the data. These weights are used for regularization,
    which we will discuss more in [*Chapter 10*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217),
    *Making Better Predictions – Optimizing Models*.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 类别权重决定了模型对每个类别错误预测的惩罚程度。通过选择平衡的权重，对较小类别的错误预测将承担更大的权重，且权重与该类别在数据中的频率成反比。这些权重用于正则化，我们将在
    [*第 10 章*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217) 中进一步讨论，*优化模型以做出更好的预测*。
- en: 'Once we have our pipeline, we can fit it to the data with the `fit()` method:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了管道，就可以通过 `fit()` 方法将其拟合到数据上：
- en: '[PRE73]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Lastly, we can use our model fit on the training data to predict the red wine
    quality for the test data:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用我们在训练数据上拟合的模型来预测测试数据中的红酒质量：
- en: '[PRE74]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Tip
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Scikit-learn makes it easy to switch between models because we can count on
    them to have the same methods, such as `score()`, `fit()`, and `predict()`. In
    some cases, we also can use `predict_proba()` for probabilities or `decision_function()`
    to evaluate a point with the equation derived by the model instead of `predict()`.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn使得切换模型变得容易，因为我们可以依赖它们拥有相同的方法，例如`score()`、`fit()`和`predict()`。在某些情况下，我们还可以使用`predict_proba()`来获得概率，或者使用`decision_function()`来评估通过模型推导出的方程点，而不是使用`predict()`。
- en: Before we move on to evaluating the performance of this model, let's build another
    classification model using the full wine dataset.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续评估该模型的性能之前，让我们使用完整的葡萄酒数据集构建另一个分类模型。
- en: Determining wine type by chemical properties
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过化学属性确定葡萄酒类型
- en: 'We want to know whether it is possible to tell red and white wine apart based
    solely on their chemical properties. To test this, we will build a second logistic
    regression model, which will predict whether a wine is red or white. First, let''s
    split our data into testing and training sets:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想知道是否仅通过化学属性就能区分红酒和白酒。为了测试这一点，我们将构建第二个逻辑回归模型，它将预测葡萄酒是红酒还是白酒。首先，让我们将数据分为测试集和训练集：
- en: '[PRE75]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'We will once again use logistic regression in a pipeline:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次在管道中使用逻辑回归：
- en: '[PRE76]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Finally, we will save our predictions of which kind of wine each observation
    in the test set was:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将保存对测试集中每个观察值的葡萄酒类型预测结果：
- en: '[PRE77]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Now that we have predictions for both of our logistic regression models using
    their respective testing sets, we are ready to evaluate their performance.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了两个逻辑回归模型的预测结果，使用它们各自的测试集，我们准备好评估它们的性能了。
- en: Evaluating classification results
  id: totrans-459
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估分类结果
- en: We evaluate the performance of classification models by looking at how well
    each class in the data was predicted by the model. The **positive class** is the
    class of interest to us; all other classes are considered **negative classes**.
    In our red wine classification, the positive class is high quality, while the
    negative class is low quality. Despite our problem only being a binary classification
    problem, the metrics that are discussed in this section extend to multiclass classification
    problems.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过查看每个类别在数据中被模型预测得多好来评估分类模型的性能。**正类**是我们感兴趣的类别，所有其他类别被视为**负类**。在我们的红酒分类中，正类是高质量，负类是低质量。尽管我们的问题只是二分类问题，但本节中讨论的指标也适用于多分类问题。
- en: Confusion matrix
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'As we discussed in [*Chapter 8*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172),
    *Rule-Based Anomaly Detection*, a classification problem can be evaluated by comparing
    the predicted labels to the actual labels using a **confusion matrix**:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第8章*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172)中讨论的，*基于规则的异常检测*，可以通过将预测标签与实际标签进行比较来使用**混淆矩阵**评估分类问题：
- en: '![Figure 9.27 – Evaluating classification results with a confusion matrix'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.27 – 使用混淆矩阵评估分类结果'
- en: '](img/Figure_9.27_B16834.jpg)'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.27_B16834.jpg)'
- en: Figure 9.27 – Evaluating classification results with a confusion matrix
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.27 – 使用混淆矩阵评估分类结果
- en: 'Each prediction can be one of four outcomes, based on how it matches up to
    the actual value:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 每个预测结果可以有四种可能的结果，基于它与实际值的匹配情况：
- en: '**True Positive (TP)**: Correctly predicted to be the positive class'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正类（TP）**：正确预测为正类'
- en: '**False Positive (FP)**: Incorrectly predicted to be the positive class'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性（FP）**：错误地预测为正类'
- en: '**True Negative (TN)**: Correctly predicted to not be the positive class'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真负类（TN）**：正确预测为非正类'
- en: '**False Negative (FN)**: Incorrectly predicted to not be the positive class'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性（FN）**：错误地预测为非正类'
- en: Important note
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: False positives are also referred to as **type I errors**, while false negatives
    are **type II errors**. Given a certain classifier, an effort to reduce one will
    cause an increase in the other.
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假阳性也被称为**I型错误**，而假阴性则是**II型错误**。对于某一特定分类器，减少一种错误会导致另一种错误的增加。
- en: 'Scikit-learn provides the `confusion_matrix()` function, which we can pair
    with the `heatmap()` function from `seaborn` to visualize our confusion matrix.
    In the `ml_utils.classification` module, the `confusion_matrix_visual()` function
    handles this for us:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn提供了`confusion_matrix()`函数，我们可以将其与`seaborn`中的`heatmap()`函数结合，用来可视化我们的混淆矩阵。在`ml_utils.classification`模块中，`confusion_matrix_visual()`函数为我们处理了这个操作：
- en: '[PRE78]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Let''s call our confusion matrix visualization function to see how we did for
    each of our classification models. First, we will look at how well the model identified
    high-quality red wines:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调用混淆矩阵可视化函数，看看我们在每个分类模型中的表现如何。首先，我们将看看模型如何识别高质量的红葡萄酒：
- en: '[PRE79]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Using the confusion matrix, we can see that the model had trouble finding the
    high-quality red wines consistently (bottom row):'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 使用混淆矩阵，我们可以看到模型在持续准确地找到高质量红葡萄酒时遇到了困难（底行）：
- en: '![Figure 9.28 – Results for the red wine quality model'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.28 – 红葡萄酒质量模型的结果'
- en: '](img/Figure_9.28_B16834.jpg)'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.28_B16834.jpg)'
- en: Figure 9.28 – Results for the red wine quality model
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.28 – 红葡萄酒质量模型的结果
- en: 'Now, let''s look at how well the `white_or_red` model predicted the wine type:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看`white_or_red`模型预测葡萄酒类型的效果：
- en: '[PRE80]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'It looks like this model had a much easier time, with very few incorrect predictions:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这个模型的表现要容易得多，几乎没有错误预测：
- en: '`'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '`'
- en: '![Figure 9.29 – Results for the white or red wine model'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.29 – 白葡萄酒或红葡萄酒模型的结果'
- en: '](img/Figure_9.29_B16834.jpg)'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.29_B16834.jpg)'
- en: Figure 9.29 – Results for the white or red wine model
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.29 – 白葡萄酒或红葡萄酒模型的结果
- en: Now that we understand the composition of the confusion matrix, we can use it
    to calculate additional performance metrics.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了混淆矩阵的组成，可以利用它来计算其他的性能指标。
- en: Classification metrics
  id: totrans-489
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类指标
- en: Using the values in the confusion matrix, we can calculate metrics to help evaluate
    the performance of a classifier. The best metrics will depend on the goal for
    which we are building the model and whether our classes are balanced. The formulas
    in this section are derived from the data we get from the confusion matrix, where
    *TP* is the number of true positives, *TN* is the number of true negatives, and
    so on.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 使用混淆矩阵中的值，我们可以计算一些指标来帮助评估分类器的性能。最佳指标取决于我们构建模型的目标以及类别是否平衡。本节中的公式是从我们从混淆矩阵中得到的数据推导出来的，其中*TP*表示真正例数，*TN*表示真反例数，依此类推。
- en: Accuracy and error rate
  id: totrans-491
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准确率和错误率
- en: 'When our classes are roughly equal in size, we can use **accuracy**, which
    will give us the percentage of correctly classified values:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的类别大致相同时，可以使用**准确率**，它会给出正确分类值的百分比：
- en: '![](img/Formula_09_012.jpg)'
  id: totrans-493
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_012.jpg)'
- en: 'The `accuracy_score()` function in `sklearn.metrics` will calculate the accuracy
    as per the formula; however, the `score()` method of our model will also give
    us the accuracy (this isn''t always the case, as we will see with grid search
    in [*Chapter 10*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217), *Making Better
    Predictions – Optimizing Models*):'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.metrics`中的`accuracy_score()`函数会根据公式计算准确率；然而，我们模型的`score()`方法也会给出准确率（但并非总是如此，正如我们将在[*第10章*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217)中看到的，*做出更好的预测
    – 优化模型*）：'
- en: '[PRE81]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Since accuracy is the percentage we correctly classified (our **success rate**),
    it follows that our **error rate** (the percentage we got wrong) can be calculated
    as follows:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 由于准确率是我们正确分类的百分比（即我们的**成功率**），因此我们的**错误率**（即错误分类的百分比）可以通过以下方式计算：
- en: '![](img/Formula_09_013.jpg)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_013.jpg)'
- en: 'Our accuracy score tells us that we got 77.5% of the red wines correctly classified
    according to their quality. Conversely, the `zero_one_loss()` function gives us
    the percentage of values that were misclassified, which is 22.5% for the red wine
    quality model:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的准确率分数告诉我们，红葡萄酒中有77.5%被正确分类到相应的质量等级。相反，`zero_one_loss()`函数给出了误分类的百分比，对于红葡萄酒质量模型来说是22.5%：
- en: '[PRE82]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Note that while both of these are easy to compute and understand, they require
    a threshold. By default, this is 50%, but we can use any probability we wish as
    a cutoff when predicting the class using the `predict_proba()` method in `scikit-learn`.
    In addition, accuracy and error rate can be misleading in cases of class imbalance.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然这两个指标都容易计算和理解，但它们需要一个阈值。默认情况下，阈值是50%，但我们可以在使用`scikit-learn`中的`predict_proba()`方法预测类别时，使用任何我们希望的概率作为截止值。此外，在类别不平衡的情况下，准确率和错误率可能会产生误导。
- en: Precision and recall
  id: totrans-501
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精确度和召回率
- en: 'When we have a **class imbalance**, accuracy can become an unreliable metric
    for measuring our performance. For instance, if we had a 99/1 split between two
    classes, A and B, where the rare event, B, is our positive class, we could build
    a model that was 99% accurate by just saying everything belonged to class A. This
    problem stems from the fact that true negatives will be very large, and being
    in the numerator (in addition to the denominator), they will make the results
    look better than they are. Clearly, we shouldn''t bother building a model if it
    doesn''t do anything to identify class B; thus, we need different metrics that
    will discourage this behavior. For this, we use precision and recall instead of
    accuracy. **Precision** is the ratio of true positives to everything flagged positive:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们存在**类别不平衡**时，准确率可能成为衡量性能的一个不可靠指标。例如，如果我们有一个 99/1 的两类分布，其中稀有事件 B 是我们的正类，我们可以通过将所有数据都分类为
    A 来构建一个 99% 准确率的模型。这一问题源于真正负类数量非常大，并且它们会出现在分子（以及分母）中，使得结果看起来比实际情况更好。显然，如果模型根本没有识别
    B 类的功能，我们就不应该费心构建该模型；因此，我们需要不同的指标来避免这种行为。为此，我们使用精度和召回率而不是准确率。**精度**是指真阳性与所有被标记为阳性的结果的比率：
- en: '![](img/Formula_09_014.jpg)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_014.jpg)'
- en: '**Recall** gives us the **true positive rate** (**TPR**), which is the ratio
    of true positives to everything that was actually positive:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '**召回率**给出了**真正阳性率**（**TPR**），即真阳性与所有实际为阳性的样本的比率：'
- en: '![](img/Formula_09_015.jpg)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_015.jpg)'
- en: In the case of the 99/1 split between classes A and B, the model that classifies
    everything as A would have a recall of 0% for the positive class, B (precision
    would be undefined—0/0). Precision and recall provide a better way of evaluating
    model performance in the face of a class imbalance. They will correctly tell us
    that the model has little value for our use case.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在 A 类和 B 类 99/1 分布的情况下，如果模型将所有数据都分类为 A，则对正类 B 的召回率为 0%（精度将无法定义——0/0）。精度和召回率在类别不平衡的情况下提供了更好的评估模型性能的方法。它们能正确告诉我们模型对我们使用场景的价值很低。
- en: Scikit-learn provides the `classification_report()` function, which will calculate
    precision and recall for us. In addition to calculating these metrics per class
    label, it also calculates the **macro** average (unweighted average between classes)
    and the **weighted** average (average between classes weighted by the number of
    observations in each class). The **support** column indicates the count of observations
    that belong to each class using the labeled data.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 提供了 `classification_report()` 函数，可以为我们计算精度和召回率。除了为每个类别标签计算这些指标外，它还计算了**宏观**平均值（各类别之间的无权平均）和**加权**平均值（根据每个类别的观察数加权后的平均值）。**支持**列表示使用标记数据属于每个类别的观察数量。
- en: 'The classification report indicates that our model does well at finding the
    low-quality red wines, but not so great with the high-quality red wines:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 分类报告表明，我们的模型在找到低质量红酒方面表现良好，但在高质量红酒的表现上则不尽如人意：
- en: '[PRE83]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Given that the quality scores are very subjective and not necessarily related
    to the chemical properties, it is no surprise that this simple model doesn''t
    perform too well. On the other hand, chemical properties are different between
    red and white wines, so this information is more useful for the `white_or_red`
    model. As we can imagine, based on the confusion matrix for the `white_or_red`
    model, the metrics are good:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于质量评分是非常主观的，并不一定与化学性质相关，因此这个简单模型的表现不佳也就不足为奇。另一方面，红酒和白酒之间的化学性质不同，因此这些信息对于 `white_or_red`
    模型来说更加有用。正如我们可以想象的，基于 `white_or_red` 模型的混淆矩阵，评估指标表现良好：
- en: '[PRE84]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Just like accuracy, both precision and recall are easy to compute and understand,
    but require thresholds. In addition, precision and recall each only consider half
    of the confusion matrix:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 就像准确率一样，精度和召回率也容易计算和理解，但需要设定阈值。此外，精度和召回率各自只考虑了混淆矩阵的一半：
- en: '![Figure 9.30 – Confusion matrix coverage for precision and recall'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.30 – 精度和召回率的混淆矩阵覆盖'
- en: '](img/Figure_9.30_B16834.jpg)'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.30_B16834.jpg)'
- en: Figure 9.30 – Confusion matrix coverage for precision and recall
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.30 – 精度和召回率的混淆矩阵覆盖
- en: There is typically a trade-off between maximizing recall and maximizing precision,
    and we have to decide which is more important to us. This preference can be quantified
    using the F score.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在最大化召回率和最大化精度之间存在权衡，我们必须决定哪个对我们更为重要。这种偏好可以通过 F 分数来量化。
- en: F score
  id: totrans-517
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F 分数
- en: 'The classification report also includes the **F**1 **score**, which helps us
    balance precision and recall using the **harmonic mean** of the two:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 分类报告还包括**F**1 **得分**，它帮助我们通过**调和均值**平衡精度和召回率：
- en: '![](img/Formula_09_016.jpg)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_016.jpg)'
- en: Important note
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The harmonic mean is the reciprocal of the arithmetic mean, and is used with
    rates to get a more accurate average (compared to the arithmetic mean of the rates).
    Both precision and recall are proportions in the range [0, 1], which we can treat
    as rates.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 调和均值是算术均值的倒数，通常用于处理比率，能够得到比算术均值更准确的平均值（相比于算术均值的比率）。精度和召回率都是在[0, 1]范围内的比例，我们可以将其视为比率。
- en: 'The **F**β **score**, pronounced *F-beta*, is the more general formulation
    for the F score. By varying β, we can put more weight on precision (β between
    0 and 1) or on recall (β greater than 1), where β is how many more times recall
    is valued over precision:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '**F**β **得分**，发音为*F-beta*，是F得分的更一般化公式。通过调整β值，我们可以更重视精度（β介于0和1之间）或召回率（β大于1），其中β表示召回率相对于精度的重要性：'
- en: '![](img/Formula_09_017.jpg)'
  id: totrans-523
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_017.jpg)'
- en: 'Some commonly used values for β are as follows:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常用的β值如下：
- en: '**F**0.5 **score**: Precision twice as important as recall'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F**0.5 **得分**：精度是召回率的两倍重要'
- en: '**F**1 **score**: Harmonic mean (equal importance)'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F**1 **得分**：调和均值（同等重要）'
- en: '**F**2 **score**: Recall twice as important as precision'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F**2 **得分**：召回率是精度的两倍重要'
- en: The F score is also easy to compute and relies on thresholds. However, it doesn't
    consider true negatives and is hard to optimize due to the trade-offs between
    precision and recall. Note that when working with large class imbalances, we are
    typically more concerned with predicting the positive class correctly, meaning
    that we may be less interested in true negatives, so using a metric that ignores
    them isn't necessarily an issue.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: F得分也很容易计算，且依赖于阈值。然而，它并不考虑真正的负类，且由于精度和召回率之间的权衡关系，优化起来较为困难。请注意，在处理大类不平衡时，我们通常更关心正确预测正类样本，这意味着我们可能对真正负类不那么感兴趣，因此使用忽略它们的指标通常不是问题。
- en: Tip
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Functions for precision, recall, F1 score, and Fβ score can be found in the
    `sklearn.metrics` module.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 精度、召回率、F1得分和Fβ得分的函数可以在`sklearn.metrics`模块中找到。
- en: Sensitivity and specificity
  id: totrans-531
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 敏感度和特异度
- en: 'Along the lines of the precision and recall trade-off, we have another pair
    of metrics that can be used to illustrate the delicate balance we strive to achieve
    with classification problems: sensitivity and specificity.'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在精度与召回率的权衡关系中，我们还有另一对可以用来说明我们在分类问题中力求实现的微妙平衡的指标：敏感度和特异度。
- en: '**Sensitivity** is the true positive rate, or recall, which we saw previously.
    **Specificity**, however, is the **true negative rate**, or the proportion of
    true negatives to everything that should have been classified as negative:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '**敏感度**是真正的正类率，或者叫召回率，我们之前提到过。而**特异度**是**真正的负类率**，即将所有应被分类为负类的样本中，真正负类所占的比例：'
- en: '![](img/Formula_09_021.jpg)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_021.jpg)'
- en: 'Note that, together, specificity and sensitivity consider the full confusion
    matrix:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，敏感度和特异度一起考虑了完整的混淆矩阵：
- en: '![Figure 9.31 – Confusion matrix coverage for sensitivity and specificity'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.31 – 敏感度和特异度的混淆矩阵覆盖率'
- en: '](img/Figure_9.31_B16834.jpg)'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.31_B16834.jpg)'
- en: Figure 9.31 – Confusion matrix coverage for sensitivity and specificity
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.31 – 敏感度和特异度的混淆矩阵覆盖率
- en: We would like to maximize both sensitivity and specificity; however, we could
    easily maximize specificity by decreasing the number of times we classify something
    as the positive class, which would decrease sensitivity. Scikit-learn doesn't
    offer specificity as a metric—preferring precision and recall—however, we can
    easily make our own by writing a function or using the `make_scorer()` function
    from `scikit-learn`. We are discussing them here because they form the basis of
    the sensitivity-specificity plot, or ROC curve, which is the topic of the following
    section.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望同时最大化敏感度和特异度；然而，我们可以通过减少将样本分类为正类的次数来轻松最大化特异度，这样会导致敏感度降低。Scikit-learn没有提供特异度作为评估指标—它更偏向精度和召回率—然而，我们可以通过编写函数或使用`scikit-learn`中的`make_scorer()`函数轻松自定义这个指标。我们在这里讨论它们，是因为它们构成了敏感度-特异度图或ROC曲线的基础，ROC曲线是接下来章节的主题。
- en: ROC curve
  id: totrans-540
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ROC曲线
- en: In addition to using metrics to evaluate classification problems, we can turn
    to visualizations. By plotting the true positive rate (*sensitivity*) versus the
    false positive rate (*1 - specificity*), we get the `predict_proba()` method in
    `scikit-learn`. Say that we find the threshold to be 60%—we would require `predict_proba()`
    to return a value greater than or equal to 0.6 to predict the positive class (`predict()`
    uses 0.5 as the cutoff).
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用指标评估分类问题外，我们还可以借助可视化。通过绘制真正率（*敏感度*）与假正率（*1 - 特异性*）的关系，我们得到了`scikit-learn`中的`predict_proba()`方法。假设我们将阈值设为
    60%——我们需要`predict_proba()`返回大于或等于 0.6 的值来预测正类（`predict()`方法使用 0.5 作为切割点）。
- en: The `roc_curve()` function from `scikit-learn` calculates the false and true
    positive rates at thresholds from 0 to 100% using the probabilities of an observation
    belonging to a given class, as determined by the model. We can then plot this,
    with the goal being to maximize the **area under the curve** (**AUC**), which
    is in the range [0, 1]; values below 0.5 are worse than guessing and good scores
    are above 0.8\. Note that when referring to the area under a ROC curve, the AUC
    may also be written as **AUROC**. The AUROC summarizes the model's performance
    across thresholds.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`中的`roc_curve()`函数通过模型确定的观察值属于给定类别的概率，在从 0 到 100% 的阈值下计算假正率和真正率。然后我们可以绘制这个曲线，目标是最大化**曲线下的面积**（**AUC**），其范围在
    [0, 1] 之间；低于 0.5 的值比猜测还差，而良好的分数则大于 0.8。请注意，当提到 ROC 曲线下的面积时，AUC 也可以写作**AUROC**。AUROC
    总结了模型在各个阈值下的表现。'
- en: 'The following are examples of good ROC curves. The dashed line would be random
    guessing (no predictive value) and is used as a baseline; anything below that
    is considered worse than guessing. We want to be toward the top-left corner:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些良好的 ROC 曲线示例。虚线代表随机猜测（没有预测价值），用作基准；低于虚线的表示比猜测更差。我们希望处于左上角：
- en: '![Figure 9.32 – Comparing ROC curves'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.32 – 比较 ROC 曲线'
- en: '](img/Figure_9.32_B16834.jpg)'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.32_B16834.jpg)'
- en: Figure 9.32 – Comparing ROC curves
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.32 – 比较 ROC 曲线
- en: 'The `ml_utils.classification` module contains a function for plotting our ROC
    curve. Let''s take a look at it:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '`ml_utils.classification`模块包含一个绘制 ROC 曲线的函数。让我们来看看它：'
- en: '[PRE85]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: As we can imagine, our `white_or_red` model will have a very good ROC curve.
    Let's see what that looks like by calling the `plot_roc()` function. Since we
    need to pass the probabilities of each entry belonging to the positive class,
    we need to use the `predict_proba()` method instead of `predict()`. This gives
    us the probabilities that each observation belongs to each class.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所想，我们的`white_or_red`模型会有一个非常好的 ROC 曲线。让我们通过调用`plot_roc()`函数来看看它的效果。因为我们需要传递每个条目属于正类的概率，所以我们需要使用`predict_proba()`方法而不是`predict()`。这样我们就能得到每个观察值属于各类的概率。
- en: 'Here, for every row in `w_X_test`, we have a NumPy array of `[P(white), P(red)]`.
    Therefore, we use slicing to select the probabilities that the wine is red for
    the ROC curve (`[:,1]`):'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，对于`w_X_test`中的每一行，我们得到一个 NumPy 数组`[P(white), P(red)]`。因此，我们使用切片来选择红酒的概率用于
    ROC 曲线（`[:,1]`）：
- en: '[PRE86]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Just as we expected, the ROC curve for the `white_or_red` model is very good,
    with an AUC of nearly 1:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预期的那样，`white_or_red`模型的 ROC 曲线非常好，AUC 接近 1：
- en: '![Figure 9.33 – ROC curve for the white or red wine model'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.33 – 白酒或红酒模型的 ROC 曲线'
- en: '](img/Figure_9.33_B16834.jpg)'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.33_B16834.jpg)'
- en: Figure 9.33 – ROC curve for the white or red wine model
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.33 – 白酒或红酒模型的 ROC 曲线
- en: 'Given the other metrics we have looked at, we don''t expect the red wine quality
    prediction model to have a great ROC curve. Let''s call our function to see what
    the ROC curve for the red wine quality model looks like:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们观察过的其他指标，我们不期望红酒质量预测模型有很好的 ROC 曲线。让我们调用函数来看看红酒质量模型的 ROC 曲线长什么样：
- en: '[PRE87]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'This ROC curve isn''t as good as the previous one, as expected:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 ROC 曲线不如前一个好，正如预期的那样：
- en: '![Figure 9.34 – ROC curve for the red wine quality model'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.34 – 红酒质量模型的 ROC 曲线'
- en: '](img/Figure_9.34_B16834.jpg)'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.34_B16834.jpg)'
- en: Figure 9.34 – ROC curve for the red wine quality model
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.34 – 红酒质量模型的 ROC 曲线
- en: Our AUROC is 0.85; however, note that the AUROC provides optimistic estimates
    under class imbalance (since it considers true negatives). For this reason, we
    should also look at the precision-recall curve.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 AUROC 是 0.85；然而，请注意，AUROC 在类别不平衡的情况下提供乐观的估计（因为它考虑了真正负类）。因此，我们还应该查看精准率-召回率曲线。
- en: Precision-recall curve
  id: totrans-563
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精准率-召回率曲线
- en: When faced with a class imbalance, we use **precision-recall curves** instead
    of ROC curves. This curve shows precision versus recall at various probability
    thresholds. The baseline is a horizontal line at the percentage of the data that
    belongs to the positive class. We want our curve above this line, with an **area
    under the precision-recall curve** (**AUPR**) greater than that percentage (the
    higher the better).
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 当面临类别不平衡时，我们使用**精确度-召回率曲线**而不是ROC曲线。该曲线显示了在不同概率阈值下的精确度与召回率的关系。基线是数据中属于正类的百分比的水平线。我们希望我们的曲线位于这条线之上，且**精确度-召回率曲线下的面积**（**AUPR**）大于该百分比（值越高越好）。
- en: 'The `ml_utils.classification` module contains the `plot_pr_curve()` function
    for drawing precision-recall curves and providing the AUPR:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '`ml_utils.classification`模块包含`plot_pr_curve()`函数，用于绘制精确度-召回率曲线并提供AUPR：'
- en: '[PRE88]'
  id: totrans-566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Since the implementation of the AUC calculation in `scikit-learn` uses interpolation,
    it may give an optimistic result, so our function also calculates **average precision**
    (**AP**), which summarizes the precision-recall curve as the weighted mean of
    the precision scores (*P*n) achieved at various thresholds. The weights are derived
    from the change in recall (*R*n) between one threshold and the next. Values are
    between 0 and 1, with higher values being better:'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`scikit-learn`中AUC计算的实现使用了插值方法，可能会给出过于乐观的结果，因此我们的函数还计算了**平均精确度**（**AP**），它将精确度-召回率曲线总结为在各个阈值下实现的精确度得分（*P*n）的加权平均值。加权值来源于一个阈值和下一个阈值之间召回率（*R*n）的变化。值的范围在0到1之间，值越高越好：
- en: '![](img/Formula_09_022.jpg)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_022.jpg)'
- en: 'Let''s take a look at the precision-recall curve for the red wine quality model:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看红酒质量模型的精确度-召回率曲线：
- en: '[PRE89]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'This still shows that our model is better than the baseline of random guessing;
    however, the performance reading we get here seems more in line with the lackluster
    performance we saw in the classification report. We can also see that the model
    loses lots of precision when going from a recall of 0.2 to 0.4\. Here, the trade-off
    between precision and recall is evident, and we will likely choose to optimize
    one:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然表明我们的模型优于随机猜测的基线；然而，我们在这里获得的性能读数似乎更符合我们在分类报告中看到的平庸表现。我们还可以看到，从召回率0.2到0.4的过程中，模型的精确度大幅下降。在这里，精确度和召回率之间的权衡是显而易见的，我们很可能会选择优化其中一个：
- en: '![Figure 9.35 – Precision-recall curve for the red wine quality model'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.35 – 红酒质量模型的精确度-召回率曲线'
- en: '](img/Figure_9.35_B16834.jpg)'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.35_B16834.jpg)'
- en: Figure 9.35 – Precision-recall curve for the red wine quality model
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.35 – 红酒质量模型的精确度-召回率曲线
- en: Since we have a class imbalance between the high-quality and low-quality red
    wines (less than 14% are high quality), we must make a choice as to whether we
    optimize precision or recall. Our choice would depend on who we work for in the
    wine industry. If we are renowned for producing high-quality wine, and we are
    choosing which wines to provide to critics for reviews, we want to make sure we
    pick the best ones and would rather miss out on good ones (false negatives) than
    tarnish our name with low-quality ones that the model classifies as high quality
    (false positives). However, if we are trying to make the best profit from selling
    the wines, we wouldn't want to sell such a high-quality wine for the same price
    as a low-quality wine (false negative), so we would rather overprice some low-quality
    wines (false positives).
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在高质量和低质量红酒之间存在类别不平衡（高质量红酒少于14%），我们必须选择优化精确度还是召回率。我们的选择将取决于我们在葡萄酒行业中所服务的对象。如果我们以生产高质量红酒而闻名，并且我们要为评论家提供酒样进行评审，我们希望确保挑选出最好的酒，并宁愿错过一些好酒（假阴性），也不愿让低质量的酒被模型误分类为高质量酒（假阳性）影响我们的声誉。然而，如果我们的目标是从销售酒品中获得最佳利润，我们就不希望以低质量酒的价格出售高质量酒（假阴性），因此我们宁愿为一些低质量酒定高价（假阳性）。
- en: Note that we could easily have classified everything as low quality to never
    disappoint or as high quality to maximize our profit selling them; however, this
    isn't too practical. It's clear that we need to strike an acceptable balance between
    false positives and false negatives. To do so, we need to quantify this trade-off
    between the two extremes in terms of what matters to us more. Then, we can use
    the precision-recall curve to find a threshold that meets our precision and recall
    targets. In [*Chapter 11*](B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237), *Machine
    Learning Anomaly Detection*, we will work through an example of this.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们本可以轻松地将所有内容分类为低质量，以免让人失望，或者将其分类为高质量，以最大化销售利润；然而，这并不太实际。显然，我们需要在假阳性和假阴性之间找到一个可接受的平衡。为此，我们需要量化这两极之间的权衡，以确定哪些对我们更重要。然后，我们可以使用精度-召回曲线找到一个符合我们精度和召回率目标的阈值。在[*第11章*](B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237)，*机器学习异常检测*中，我们将通过一个示例来讲解这一点。
- en: 'Let''s now take a look at the precision-recall curve for our white or red wine
    classifier:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下白酒或红酒分类器的精度-召回曲线：
- en: '[PRE90]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Note that this curve is in the upper right-hand corner. With this model, we
    can achieve high precision and high recall:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这条曲线位于右上角。使用这个模型，我们可以实现高精度和高召回率：
- en: '![Figure 9.36 – Precision-recall curve for the white or red wine model'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.36 – 白酒或红酒模型的精度-召回曲线'
- en: '](img/Figure_9.36_B16834.jpg)'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.36_B16834.jpg)'
- en: Figure 9.36 – Precision-recall curve for the white or red wine model
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.36 – 白酒或红酒模型的精度-召回曲线
- en: As we saw with the red wine quality model, AUPR works very well with class imbalance.
    However, it can't be compared across datasets, is expensive to compute, and is
    hard to optimize. Note that this was just a subset of the metrics we can use to
    evaluate classification problems. All the classification metrics offered by `scikit-learn`
    can be found at [https://scikit-learn.org/stable/modules/classes.html#classification-metrics](https://scikit-learn.org/stable/modules/classes.html#classification-metrics).
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在红酒质量模型中看到的，AUPR在类别不平衡时表现非常好。然而，它无法跨数据集进行比较，计算成本高，且难以优化。请注意，这只是我们可以用来评估分类问题的指标的一个子集。所有`scikit-learn`提供的分类指标可以在[https://scikit-learn.org/stable/modules/classes.html#classification-metrics](https://scikit-learn.org/stable/modules/classes.html#classification-metrics)找到。
- en: Summary
  id: totrans-584
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter served as an introduction to machine learning in Python. We discussed
    the terminology that's commonly used to describe learning types and tasks. Then,
    we practiced EDA using the skills we learned throughout this book to get a feel
    for the wine and planet datasets. This gave us some ideas about what kinds of
    models we would want to build. A thorough exploration of the data is essential
    before attempting to build a model.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 本章作为Python机器学习的入门介绍。我们讨论了常用的学习类型和任务术语。接着，我们使用本书中学到的技能进行了EDA，了解了酒类和行星数据集。这为我们构建模型提供了一些思路。在尝试构建模型之前，彻底探索数据是至关重要的。
- en: Next, we learned how to prepare our data for use in machine learning models
    and the importance of splitting the data into training and testing sets before
    modeling. In order to prepare our data efficiently, we used pipelines in `scikit-learn`
    to package up everything from our preprocessing through our model.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了如何为机器学习模型准备数据，以及在建模之前将数据拆分为训练集和测试集的重要性。为了高效准备数据，我们在`scikit-learn`中使用了管道，将从预处理到模型的所有步骤进行打包。
- en: We used unsupervised k-means to cluster the planets using their semi-major axis
    and period; we also discussed how to use the elbow point method to find a good
    value for *k*. Then, we moved on to supervised learning and made a linear regression
    model to predict the period of a planet using its semi-major axis, eccentricity
    of orbit, and mass. We learned how to interpret the model coefficients and how
    to evaluate the model's predictions. Finally, we turned to classification to identify
    high-quality red wines (which had a class imbalance) and distinguish between red
    and white wine by their chemical properties. Using precision, recall, F1 score,
    confusion matrices, ROC curves, and precision-recall curves, we discussed how
    to evaluate classification models.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用无监督的k-means算法，基于行星的半长轴和周期对行星进行聚类；我们还讨论了如何使用肘部法则来找到合适的*k*值。接着，我们转向有监督学习，构建了一个线性回归模型，利用行星的半长轴、轨道偏心率和质量来预测其周期。我们学习了如何解释模型系数以及如何评估模型的预测结果。最后，我们进入了分类问题，识别高质量的红酒（存在类别不平衡），并根据它们的化学特性区分红酒和白酒。通过使用精确度、召回率、F1得分、混淆矩阵、ROC曲线和精确度-召回率曲线，我们讨论了如何评估分类模型。
- en: It's important to remember that machine learning models make assumptions about
    the underlying data, and while this wasn't a chapter on the mathematics of machine
    learning, we should make sure that we understand that there are consequences for
    violating these assumptions. In practice, when looking to build models, it's crucial
    that we have a solid understanding of statistics and domain-level expertise. We
    saw that there is a multitude of metrics for evaluating our models. Each metric
    has its strengths and weaknesses, and, depending on the problem, some are better
    than others; we must take care to choose the appropriate metrics for the task
    at hand.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，机器学习模型对底层数据做出假设，尽管本章没有深入讨论机器学习的数学原理，但我们应当理解，违反这些假设会带来一定的后果。在实际操作中，构建模型时，我们需要对统计学和领域知识有充分的理解。我们看到，评估模型的指标有很多，每个指标都有其优缺点，具体哪种指标更好取决于问题的类型；我们必须小心选择适合当前任务的评估指标。
- en: In the next chapter, we will learn how to tune our models to improve their performance,
    so make sure to complete the exercises to practice this chapter's material before
    moving on.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何调优我们的模型以提升其性能，因此，在继续之前，确保完成这些练习以巩固本章内容。
- en: Exercises
  id: totrans-590
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Practice building and evaluating machine learning models in `scikit-learn`
    with the following exercises:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 练习使用`scikit-learn`构建和评估机器学习模型，完成以下练习：
- en: 'Build a clustering model to distinguish between red and white wine by their
    chemical properties:'
  id: totrans-592
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个聚类模型，通过化学特性区分红酒和白酒：
- en: a) Combine the red and white wine datasets (`data/winequality-red.csv` and `data/winequality-white.csv`,
    respectively) and add a column for the kind of wine (red or white).
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 合并红酒和白酒数据集（分别是`data/winequality-red.csv`和`data/winequality-white.csv`），并添加一个表示酒的种类（红酒或白酒）的列。
- en: b) Perform some initial EDA.
  id: totrans-594
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 执行一些初步的EDA（探索性数据分析）。
- en: c) Build and fit a pipeline that scales the data and then uses k-means clustering
    to make two clusters. Be sure not to use the `quality` column.
  id: totrans-595
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 构建并拟合一个管道（pipeline），对数据进行标准化处理，然后使用k-means聚类算法将数据分成两个聚类。注意不要使用`quality`列。
- en: d) Use the Fowlkes-Mallows Index (the `fowlkes_mallows_score()` function is
    in `sklearn.metrics`) to evaluate how well k-means is able to make the distinction
    between red and white wine.
  id: totrans-596
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 使用Fowlkes-Mallows指数（`fowlkes_mallows_score()`函数在`sklearn.metrics`中）评估k-means算法在区分红酒和白酒时的效果。
- en: e) Find the center of each cluster.
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 找出每个聚类的中心。
- en: 'Predict star temperature:'
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测恒星温度：
- en: a) Using the `data/stars.csv` file, perform some initial EDA and then build
    a linear regression model of all the numeric columns to predict the temperature
    of the star.
  id: totrans-599
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 使用`data/stars.csv`文件，执行一些初步的EDA，然后构建一个线性回归模型，对所有数值列进行分析，以预测恒星的温度。
- en: b) Train the model on 75% of the initial data.
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 使用初始数据的75%来训练模型。
- en: c) Calculate the R2 and RMSE of the model.
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 计算模型的R2和RMSE（均方根误差）。
- en: d) Find the coefficients for each regressor and the intercept of the linear
    regression equation.
  id: totrans-602
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 找出每个回归器的系数和线性回归方程的截距。
- en: e) Visualize the residuals using the `plot_residuals()` function from the `ml_utils.regression`
    module.
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 使用`ml_utils.regression`模块中的`plot_residuals()`函数可视化残差。
- en: 'Classify planets that have shorter years than Earth:'
  id: totrans-604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类那些年比地球更短的行星：
- en: a) Using the `data/planets.csv` file, build a logistic regression model with
    the `eccentricity`, `semimajoraxis`, and `mass` columns as regressors. You will
    need to make a new column to use for the *y* (year shorter than Earth).
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 使用`data/planets.csv`文件，构建一个逻辑回归模型，将`eccentricity`、`semimajoraxis`和`mass`列作为回归变量。你需要创建一个新的列作为*y*（比地球年份短）。
- en: b) Find the accuracy score.
  id: totrans-606
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 找出准确率分数。
- en: c) Use the `classification_report()` function from `scikit-learn` to see the
    precision, recall, and F1 score for each class.
  id: totrans-607
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 使用`scikit-learn`中的`classification_report()`函数查看每个类别的精度、召回率和F1分数。
- en: d) With the `plot_roc()` function from the `ml_utils.classification` module,
    plot the ROC curve.
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 使用`ml_utils.classification`模块中的`plot_roc()`函数绘制ROC曲线。
- en: e) Create a confusion matrix using the `confusion_matrix_visual()` function
    from the `ml_utils.classification` module.
  id: totrans-609
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 使用`ml_utils.classification`模块中的`confusion_matrix_visual()`函数创建混淆矩阵。
- en: 'Multiclass classification of white wine quality:'
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 白葡萄酒质量的多类别分类：
- en: a) Using the `data/winequality-white.csv` file, perform some initial EDA on
    the white wine data. Be sure to look at how many wines had a given quality score.
  id: totrans-611
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 使用`data/winequality-white.csv`文件，对白葡萄酒数据进行初步的EDA。一定要查看每种质量评分的葡萄酒数量。
- en: b) Build a pipeline to standardize the data and fit a multiclass logistic regression
    model. Pass `multi_class='multinomial'` and `max_iter=1000` to the `LogisticRegression`
    constructor.
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 构建一个管道以标准化数据并拟合一个多类别的逻辑回归模型。将`multi_class='multinomial'`和`max_iter=1000`传递给`LogisticRegression`构造函数。
- en: c) Look at the classification report for your model.
  id: totrans-613
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 查看你模型的分类报告。
- en: d) Create a confusion matrix using the `confusion_matrix_visual()` function
    from the `ml_utils.classification` module. This will work as is for multiclass
    classification problems.
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 使用`ml_utils.classification`模块中的`confusion_matrix_visual()`函数创建混淆矩阵。这对于多类别分类问题可以直接使用。
- en: e) Extend the `plot_roc()` function to work for multiple class labels. To do
    so, you will need to create a ROC curve for each class label (which are quality
    scores here), where a true positive is correctly predicting that quality score
    and a false positive is predicting any other quality score. Note that `ml_utils`
    has a function for this, but try to build your own implementation.
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 扩展`plot_roc()`函数以支持多个类别标签。为此，你需要为每个类别标签（这里是质量分数）创建一个ROC曲线，其中真正例是正确预测该质量分数，假正例是预测任何其他质量分数。请注意，`ml_utils`已有此功能，但你可以尝试自己实现。
- en: f) Extend the `plot_pr_curve()` function to work for multiple class labels by
    following a similar method to part *e)*. However, give each class its own subplot.
    Note that `ml_utils` has a function for this, but try to build your own implementation.
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f) 扩展`plot_pr_curve()`函数以支持多个类别标签，方法类似于*e)*部分。但为每个类别创建单独的子图。请注意，`ml_utils`已有此功能，但你可以尝试自己实现。
- en: 'We have seen how easy the `scikit-learn` API is to navigate, making it a cinch
    to change which algorithm we are using for our model. Rebuild the red wine quality
    model that we created in this chapter using an SVM instead of logistic regression.
    We haven''t discussed this model, but you should still be able to use it in `scikit-learn`.
    Check out the link in the *Further reading* section to learn more about the algorithm.
    Some guidance for this exercise is as follows:'
  id: totrans-617
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经看到`scikit-learn` API的易用性，使得切换我们模型中使用的算法变得轻松。使用支持向量机（SVM）重建本章中创建的红酒质量模型，而不是使用逻辑回归。我们虽然没有讨论这个模型，但你仍然可以在`scikit-learn`中使用它。可以参考*进一步阅读*部分的链接，了解更多关于该算法的信息。以下是本练习的一些指导：
- en: a) You will need to use the `SVC` (support vector classifier) class from `scikit-learn`,
    which can be found at [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 你需要使用`scikit-learn`中的`SVC`（支持向量分类器）类，详情请见[https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)。
- en: b) Use `C=5` as an argument to the `SVC` constructor.
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 在`SVC`构造函数中使用`C=5`作为参数。
- en: c) Pass `probability=True` to the `SVC` constructor to be able to use the `predict_proba()`
    method.
  id: totrans-620
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 在`SVC`构造函数中传递`probability=True`，以便使用`predict_proba()`方法。
- en: d) Build a pipeline first using the `StandardScaler` class and then the `SVC`
    class.
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 首先使用`StandardScaler`类构建管道，然后使用`SVC`类。
- en: e) Be sure to look at the classification report, precision-recall curve, and
    confusion matrix for the model.
  id: totrans-622
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 确保查看模型的分类报告、精度-召回率曲线和混淆矩阵。
- en: Further reading
  id: totrans-623
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Check out the following resources for more information on the topics that were
    covered in this chapter:'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下资源，获取有关本章所涵盖主题的更多信息：
- en: '*A Beginner''s Guide to Deep Reinforcement Learning*: [https://pathmind.com/wiki/deep-reinforcement-learning](https://pathmind.com/wiki/deep-reinforcement-learning)'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度强化学习入门指南*: [https://pathmind.com/wiki/deep-reinforcement-learning](https://pathmind.com/wiki/deep-reinforcement-learning)'
- en: '*An Introduction to Gradient Descent and Linear Regression*: [https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/)'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*梯度下降和线性回归简介*: [https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/)'
- en: '*Assumptions of Multiple Linear Regression*: [https://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/](https://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/)'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多元线性回归的假设*: [https://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/](https://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/)'
- en: '*Clustering*: [https://scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html)'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚类*: [https://scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html)'
- en: '*Generalized Linear Models*: [https://scikit-learn.org/stable/modules/linear_model.html](https://scikit-learn.org/stable/modules/linear_model.html)'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*广义线性模型*: [https://scikit-learn.org/stable/modules/linear_model.html](https://scikit-learn.org/stable/modules/linear_model.html)'
- en: '*Guide to Interpretable Machine Learning – Techniques to dispel the black box
    myth of deep learning*: [https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf](https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf)'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释的机器学习指南 – 破解深度学习黑箱迷思的技巧*: [https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf](https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf)'
- en: '*In Depth: k-Means*: [https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深入讲解: k-Means*: [https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)'
- en: '*Interpretable Machine Learning – A Guide for Making Black Box Models Explainable*:
    [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释的机器学习 – 使黑箱模型可解释的指南*: [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)'
- en: '*Interpretable Machine Learning – Extracting human understandable insights
    from any Machine Learning model*: [https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b](https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b)'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释的机器学习 – 从任何机器学习模型中提取人类可理解的洞察*: [https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b](https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b)'
- en: '*MAE and RMSE – Which Metric is Better?*: [https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MAE和RMSE – 哪个指标更好?*: [https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)'
- en: '*Model evaluation: quantifying the quality of predictions*: [https://scikit-learn.org/stable/modules/model_evaluation.html](https://scikit-learn.org/stable/modules/model_evaluation.html)'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型评估：量化预测质量*: [https://scikit-learn.org/stable/modules/model_evaluation.html](https://scikit-learn.org/stable/modules/model_evaluation.html)'
- en: '*Preprocessing data*: [https://scikit-learn.org/stable/modules/preprocessing.html](https://scikit-learn.org/stable/modules/preprocessing.html)'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据预处理*: [https://scikit-learn.org/stable/modules/preprocessing.html](https://scikit-learn.org/stable/modules/preprocessing.html)'
- en: '*Scikit-learn Glossary of Common Terms and API Elements*: [https://scikit-learn.org/stable/glossary.html#glossary](https://scikit-learn.org/stable/glossary.html#glossary)'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Scikit-learn 常见术语和API元素词汇表*: [https://scikit-learn.org/stable/glossary.html#glossary](https://scikit-learn.org/stable/glossary.html#glossary)'
- en: '*Scikit-learn User Guide*: [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Scikit-learn 用户指南*: [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)'
- en: '*Seeing Theory* [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)*:
    Regression Analysis*: [https://seeing-theory.brown.edu/index.html#secondPage/chapter6](https://seeing-theory.brown.edu/index.html#secondPage/chapter6)'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Seeing Theory* [*第6章*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)*: 回归分析*:
    [https://seeing-theory.brown.edu/index.html#secondPage/chapter6](https://seeing-theory.brown.edu/index.html#secondPage/chapter6)'
- en: '*Simple Beginner''s Guide to Reinforcement Learning & its implementation*:
    [https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/](https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/)'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*简单的强化学习入门指南及其实现*：[https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/](https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/)'
- en: '*Support Vector Machine – Introduction to Machine Learning Algorithms*: [https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*支持向量机——机器学习算法简介*：[https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)'
- en: '*The 5 Clustering Algorithms Data Scientists Need to Know*: [https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据科学家需要了解的5种聚类算法*：[https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)'
