- en: Chapter 4. Spark Programming with R
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 章 Spark 编程与 R
- en: R is a popular statistical computing programming language used by many and freely
    available under the **General Public License** (**GNU**). R originated from the
    programming language S, created by John Chambers. R was developed by Ross Ihaka
    and Robert Gentleman. Many data scientists use R for their computing needs. R
    has inherent support for many statistical functions and many scalar data types,
    and has composite data structures for vectors, matrices, data frames, and more,
    for statistical computation. R is highly extensible and for that, external packages
    can be created. Once an external package is created, it has to be installed and
    loaded for any program to use it. A collection of such packages under a directory
    forms an R library. In other words, R comes with a set of base packages and additional
    packages that can be installed on top of it to form the required library for the
    desired computing needs. In addition to functions, datasets can also be packaged
    in R packages.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: R 是一种流行的统计计算编程语言，被许多人使用，并根据**通用公共许可证**（**GNU**）免费提供。R 源自 John Chambers 创建的编程语言
    S。R 由 Ross Ihaka 和 Robert Gentleman 开发。许多数据科学家使用 R 来满足他们的计算需求。R 内置支持许多统计函数和许多标量数据类型，并具有向量、矩阵、数据框等复合数据结构，用于统计计算。R
    高度可扩展，因此可以创建外部包。一旦创建了外部包，就必须安装并加载它，以便任何程序都可以使用它。目录下的一组此类包构成一个 R 库。换句话说，R 附带了一组基本包和附加包，可以安装在它上面，以形成满足所需计算需求的库。除了函数之外，数据集也可以打包在
    R 包中。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: The need for SparkR
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 SparkR 的需求
- en: Essentials of R
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 基础
- en: Dataframes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框
- en: Aggregations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合
- en: Multi-datasource joins with SparkR
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SparkR 进行多数据源连接
- en: The need for SparkR
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对 SparkR 的需求
- en: A plain base R installation cannot interact with Spark. The **SparkR** package
    exposes all the required objects and functions for R to talk to the Spark ecosystem.
    Compared to Scala, Java, and Python, the Spark programming in R is different and
    the SparkR package mainly exposes R API for DataFrame-based Spark SQL programming.
    At the moment, R cannot be used to manipulate the RDDs of Spark directly. So for
    all practical purposes, the R API for Spark has access to only Spark SQL abstractions.
    The Spark **MLlib** can also be programmed using R because Spark MLlib uses DataFrames.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**SparkR** 包使得基础 R 安装能够与 Spark 交互，它提供了 R 与 Spark 生态系统对话所需的所有对象和函数。与 Scala、Java
    和 Python 相比，R 中的 Spark 编程有所不同，SparkR 包主要提供基于 DataFrame 的 Spark SQL 编程的 R API。目前，R
    无法直接操作 Spark 的 RDD。因此，实际上，R 的 Spark API 只能访问 Spark SQL 抽象。由于 Spark **MLlib** 使用
    DataFrames，因此也可以使用 R 进行编程。'
- en: How is SparkR going to help the data scientists to do better data processing?
    The base R installation mandates that all the data to be stored (or accessible)
    on the computer where R is installed. The data processing occurs on the single
    computer on which the R installation is available. Moreover, if the data size
    is more than the main memory available on the computer, R will not be able to
    do the required processing. With the SparkR package, there is access to a whole
    new world of a cluster of nodes for data storage and for doing data processing.
    With the help of SparkR package, R can be used to access the Spark DataFrames
    as well as R DataFrames.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 如何帮助数据科学家进行更好的数据处理？基础 R 安装要求所有数据都必须存储（或可访问）在安装了 R 的计算机上。数据处理发生在安装了 R
    的单台计算机上。此外，如果数据大小超过了计算机上的主内存，R 将无法执行所需的加工。通过 SparkR 包，可以访问一个全新的集群节点世界，用于数据存储和数据处理。借助
    SparkR 包，R 可以访问 Spark DataFrames 和 R DataFrames。
- en: It is very important to know the distinction between the two types of data frames,
    R Dataframes and Spark Dataframes. An R DataFrame is completely local and a data
    structure of the R language. A Spark DataFrame is a parallel collection of structured
    data managed by the Spark infrastructure.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 了解 R Dataframes 和 Spark Dataframes 这两种数据框之间的区别非常重要。R DataFrame 是完全本地的，是 R 语言的数据结构。Spark
    DataFrame 是由 Spark 基础设施管理的结构化数据的并行集合。
- en: An R DataFrame can be converted to a Spark DataFrame and a Spark DataFrame can
    be converted to an R DataFrame.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: R DataFrame 可以转换为 Spark DataFrame，Spark DataFrame 也可以转换为 R DataFrame。
- en: When a Spark DataFrame is converted to an R DataFrame, it should fit in the
    available memory of the computer. This conversion is a great feature and there
    is a need to do so. By converting an R DataFrame to a Spark DataFrame, the data
    can be distributed and processed in parallel. By converting a Spark DataFrame
    to an R DataFrame, a lot of computations, charting, and plotting that is done
    by other R functions can be done. In a nutshell, the SparkR package brings the
    power of distributed and parallel computing capabilities to R.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当Spark DataFrame转换为R DataFrame时，它应该能适配到计算机的可用内存中。这种转换是一个很好的特性，也是有必要的。通过将R DataFrame转换为Spark
    DataFrame，数据可以分布并并行处理。通过将Spark DataFrame转换为R DataFrame，可以使用其他R函数进行大量计算、制图和绘图。简而言之，SparkR包为R带来了分布式和并行计算的能力。
- en: Often, when performing data processing with R, because of the sheer size of
    the data and the need to fit it into the main memory of the computer, the data
    processing is done in multiple batches and the results are consolidated to compute
    the final results. This kind of multi-batch processing can be completely avoided
    if Spark with R is used to process the data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在使用R进行数据处理时，由于数据量巨大且需要将其适配到计算机的主内存中，数据处理会分多个批次进行，并将结果汇总以计算最终结果。如果使用Spark与R来处理数据，这种多批次处理完全可以避免。
- en: Often, reporting, charting, and plotting are done on the aggregated and summarized
    raw data. The raw data size can be huge and need not fit into one computer. In
    such cases, Spark with R can be used to process the entire raw data and finally,
    the aggregated and summarized data can be used to produce the reports, charts,
    or plots.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，报告、制图和绘图都是基于汇总和概述的原始数据进行的。原始数据量可能非常庞大，无需适配到一台计算机上。在这种情况下，可以使用Spark与R来处理整个原始数据，最终，汇总和概述的数据可用于生成报告、图表或绘图。
- en: Because of the inability to process huge amounts of data and for doing data
    analysis with R, many times, ETL tools are made to use for doing the pre-processing
    or transformations on the raw data, and only in the final stage is the data analysis
    done using R. Because of Spark's ability to process data at scale, Spark with
    R can replace the entire ETL pipeline and do the desired data analysis with R.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于R无法处理大量数据以及进行数据分析，很多时候，ETL工具被用来进行原始数据的预处理或转换，只有在最后阶段才使用R进行数据分析。由于Spark能够大规模处理数据，Spark与R可以取代整个ETL流程，并用R进行所需的数据分析。
- en: Many R users use the **dplyr **R package for manipulating datasets in R. This
    package provides fast data manipulation capabilities with R DataFrames. Just like
    manipulating local R DataFrames, it can access data from some of the RDBMS tables
    too. Apart from these primitive data manipulation capabilities, it lacks many
    of the data processing features available in Spark. So Spark with R is a good
    alternative to packages such as dplyr.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 许多R用户使用**dplyr** R包来操作R中的数据集。该包提供了快速的数据操作功能，支持R DataFrames。就像操作本地R DataFrames一样，它也可以访问某些RDBMS表中的数据。除了这些基本的数据操作功能外，它缺乏Spark中提供的许多数据处理特性。因此，Spark与R是诸如dplyr等包的良好替代品。
- en: The SparkR package is yet another R package, but that is not stopping anybody
    from using any of the R packages that are already being used. At the same time,
    it supplements the data processing capability of R manifold by making use of the
    huge data processing capabilities of Spark.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR包是另一个R包，但这并不妨碍任何人继续使用已有的任何R包。同时，它通过利用Spark强大的数据处理能力，极大地增强了R的数据处理功能。
- en: Basics of the R language
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R语言基础
- en: This is not in any way a guide to R programming. But, it is important to touch
    upon the basics of R as a language very briefly for the benefit of those who are
    not familiar with R to appreciate what is being covered in this chapter. A very
    basic introduction to the language features is covered here.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这并非R编程的指南。但是，为了帮助不熟悉R的人理解本章所涵盖的内容，简要介绍R语言的基础知识是很重要的。这里涵盖了语言特性的非常基本的介绍。
- en: R comes with a few built-in data types to hold numerical values, character values,
    and boolean values. There are composite data structures available and the most
    important ones are, namely, vectors, lists, matrices, and data frames. A vector
    consists of ordered collection of values of a given type. A list is an ordered
    collection of elements that can be of different types. For example, a list can
    hold two vectors, of which one is a vector containing numerical values and the
    the other is a vector containing boolean values. A matrix is a two-dimensional
    data structure holding numerical values in rows and columns. A data frame is a
    two-dimensional data structure containing rows and columns, where columns can
    have different data types but a single column cannot hold different data types.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: R自带了几种内置数据类型来存储数值、字符和布尔值。还有复合数据结构，其中最重要的是向量、列表、矩阵和数据框。向量是由给定类型的有序值集合组成。列表是有序的元素集合，这些元素可以是不同类型。例如，一个列表可以包含两个向量，其中一个向量包含数值，另一个向量包含布尔值。矩阵是二维数据结构，按行和列存储数值。数据框是二维数据结构，包含行和列，其中列可以有不同的数据类型，但单个列不能包含不同的数据类型。
- en: 'Code samples of using a variable (a special case of vector), a numeric vector,
    a character vector, a list, a matrix, a data frame, and assigning column names
    to a data frame are as follows. The variable names are given as self-descriptive
    as possible for the reader to understand without the help of additional explanation.
    The following code snippet run on a regular R REPL gives an idea of the data structures
    of R:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例展示了使用变量（向量的特殊情况）、数值向量、字符向量、列表、矩阵、数据框以及为数据框分配列名的方法。变量名尽可能自描述，以便读者无需额外解释即可理解。以下代码片段在常规R
    REPL上运行，展示了R的数据结构：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The main topic of discussion here is going to be revolving around data frames.
    Some of the functions that are commonly used with data frames are demonstrated
    here. All these commands are to be executed on the regular R REPL as a continuation
    of the session that executed the preceding code snippet:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论的主要话题将围绕数据框展开。以下展示了与数据框常用的一些函数。所有这些命令都应在常规R REPL中执行，作为执行前述代码片段的会话的延续：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: DataFrames in R and Spark
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R与Spark中的数据框
- en: 'When working with Spark using R, it is very easy to get confused with the DataFrame
    data structure. As mentioned earlier, it is there in R and in Spark SQL. The following
    code snippet deals with converting an R DataFrame to a Spark DataFrame and vice
    versa. This is going to be a very common operation when programming Spark with
    R. The following code snippet is to be executed in the R REPL of Spark. From now
    on, all the references to the R REPL are with respect to the R REPL of Spark:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用R操作Spark时，很容易对DataFrame数据结构感到困惑。如前所述，R和Spark SQL中都存在DataFrame。下面的代码片段涉及将R数据框转换为Spark数据框以及反向转换。当使用R编程Spark时，这将是一种非常常见的操作。以下代码片段应在Spark的R
    REPL中执行。从现在开始，所有对R REPL的引用都是指Spark的R REPL：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There is no complete compatibility and interoperability between an R DataFrame
    and a Spark DataFrame in terms of the supported functions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在支持的函数方面，R数据框与Spark数据框之间没有完全的兼容性和互操作性。
- en: Tip
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: As a good practice, it is better to name the R DataFrame and Spark DataFrame
    with agreed  conventions in R programs in order to have a distinction between
    the two different types. Not all the functions that are supported on R DataFrames
    are not supported on Spark DataFrames and vice versa. Always refer to the right
    version of the R API for Spark.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了区分两种不同类型的数据框，在R程序中最好按照约定俗成的规则为R数据框和Spark数据框命名。并非所有R数据框支持的函数都适用于Spark数据框，反之亦然。务必参考正确的R
    API版本以使用Spark。
- en: 'Those who use a lot of charting and plotting have to be extra careful while
    dealing with R DataFrames in conjunction with Spark DataFrames. The charting and
    plotting of R works with only R DataFrames. If there is a need to produce charts
    or plots with the data processed by Spark and available in Spark DataFrame, it
    has to be converted to an R DataFrame to proceed with the charting and plotting.
    The following code snippet will give an idea this. We will use the faithful dataset
    again for elucidation purposes in the R REPL of Spark:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于经常使用图表和绘图的人来说，在处理R DataFrames与Spark DataFrames结合时必须格外小心。R的图表和绘图功能仅适用于R DataFrames。如果需要使用Spark处理的数据并将其呈现在Spark
    DataFrame中的图表或绘图中，则必须将其转换为R DataFrame才能继续进行图表和绘图。以下代码片段将对此有所启示。我们将在Spark的R REPL中再次使用faithful数据集进行说明：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The figure here is used jut to demonstrate that the Spark DataFrame cannot
    be used to do charting and R DataFrame has to be used for the same:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此处数字仅用于演示，说明Spark DataFrame不能用于制图，而必须使用R DataFrame进行相同的操作：
- en: '![DataFrames in R and Spark](img/image_04_002.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![R与Spark中的DataFrames](img/image_04_002.jpg)'
- en: Figure 1
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: The charting and plotting library, when used with Spark DataFrame, gave an error
    because of the incompatibility of the data types.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当图表和绘图库与Spark DataFrame一起使用时，由于数据类型不兼容，出现了错误。
- en: Tip
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The most important aspect to have in mind is that an R DataFrame is an in-memory
    resident data structure, while a Spark DataFrame is a parallel collection of datasets
    distributed across a cluster of nodes. So, all the functions that use R DataFrames
    need not work with Spark DataFrames and vice versa.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的最重要的一点是，R DataFrame是一种内存驻留数据结构，而Spark DataFrame是一种跨集群节点分布的并行数据集集合。因此，所有使用R
    DataFrames的功能不一定适用于Spark DataFrames，反之亦然。
- en: 'Let''s revisit the bigger picture again, as given in *Figure 2*, to set the
    context and see what is being discussed here before getting into and taking up
    the use cases. In the previous chapter, the same subject was introduced by using
    the programming languages Scala and Python. In this chapter, the same set of use
    cases used in the Spark SQL programming will be implemented using R:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回顾一下大局，如*图2*所示，以便设定背景并了解正在讨论的内容，然后再深入探讨并处理这些用例。在前一章中，同一主题是通过使用Scala和Python编程语言引入的。在本章中，将使用R实现Spark
    SQL编程中使用的同一组用例：
- en: '![DataFrames in R and Spark](img/image_04_004.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![R与Spark中的DataFrames](img/image_04_004.jpg)'
- en: Figure 2
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: The use cases that are going to be discussed here will be demonstrating the
    ability to mix SQL queries with Spark programs in R. Multiple data sources will
    be chosen, data will be read from those sources using DataFrame, and uniform data
    access will be demonstrated.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此处将要讨论的用例将展示在R中混合使用SQL查询和Spark程序的能力。将选择多个数据源，使用DataFrame从这些源读取数据，并演示统一的数据访问。
- en: Spark DataFrame programming with R
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark DataFrame编程与R
- en: 'The use cases selected for elucidating the Spark SQL way of programming with
    DataFrame are given as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于阐明使用DataFrame进行Spark SQL编程的用例：
- en: The transaction records are comma-separated values.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交易记录是以逗号分隔的值。
- en: Filter out only the good transaction records from the list. The account number
    should start with `SB` and the transaction amount should be greater than zero.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从列表中筛选出仅包含良好交易记录的记录。账户号码应以`SB`开头，且交易金额应大于零。
- en: Find all the high value transaction records with a transaction amount greater
    than 1000.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出所有交易金额大于1000的高价值交易记录。
- en: Find all the transaction records where the account number is bad.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出所有账户号码不良的交易记录。
- en: Find all the transaction records where the transaction amount is less than or
    equal to zero.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出所有交易金额小于或等于零的交易记录。
- en: Find a combined list of all the bad transaction records.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出所有不良交易记录的合并列表。
- en: Find the total of all the transaction amounts.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出所有交易金额的总和。
- en: Find the maximum of all the transaction amounts.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出所有交易金额的最大值。
- en: Find the minimum of all the transaction amounts.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出所有交易金额的最小值。
- en: Find all the good account numbers.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出所有良好账户号码。
- en: This is exactly the same set of use cases that were used in the previous chapter,
    but here, the programming model is totally different. Here, the programming is
    done in R. Using this set of use cases, two types of programming model are demonstrated
    here. One is using the SQL queries and other is using DataFrame APIs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是上一章中使用的一组用例，但在这里，编程模型完全不同。此处，编程采用R语言。通过这组用例，展示了两种编程模型：一种是使用SQL查询，另一种是使用DataFrame
    API。
- en: Tip
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The data files needed for running the following code snippets are available
    from the same directory where the R code is kept.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下代码片段所需的数据文件可从保存R代码的同一目录中获取。
- en: In the following code snippets, data is read from files located in the filesystem.
    Since all these code snippets are executed from the R REPL of Spark, all the data
    files are to be kept in the `$SPARK_HOME` directory.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，数据从文件系统中的文件读取。由于所有这些代码片段都在Spark的R REPL中执行，因此所有数据文件都应保存在`$SPARK_HOME`目录中。
- en: Programming with SQL
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SQL编程
- en: 'At the R REPL prompt, try the following statements:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在R REPL提示符下，尝试以下语句：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The retail banking transaction records come with account number, transaction
    amount are processed using SparkSQL to get the desired results of the use cases.
    Here is the summary of what the preceding script did:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 零售银行交易记录包含账号、交易金额，通过SparkSQL处理以获得用例所需的预期结果。以下是前述脚本所做工作的概述：
- en: Unlike other programming languages supported with Spark, R doesn't have an RDD
    programming capability. So, instead of going with the construction of RDD from
    collections, the data is read from the JSON file containing the transaction records.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他支持Spark的编程语言不同，R不具备RDD编程能力。因此，不采用从集合构建RDD的方式，而是从包含交易记录的JSON文件中读取数据。
- en: A Spark DataFrame is created from the JSON file.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从JSON文件创建了一个Spark DataFrame。
- en: A table is registered with the DataFrame with a name. This registered name of
    the table can be used in SQL statements.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过给DataFrame注册一个名称，该名称可用于SQL语句中。
- en: Then, all the other activities are issuing SQL statements using the SQL function
    from the SparkR package.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，所有其他活动都是通过SparkR包中的SQL函数发出SQL语句。
- en: The result of all these SQL statements is stored as Spark DataFrames, and showDF
    function is used to extract the values to the calling R program.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有这些SQL语句的结果都存储为Spark DataFrames，并使用`showDF`函数将值提取到调用的R程序中。
- en: The aggregate value calculations are also done through the SQL statements.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过SQL语句也进行了聚合值的计算。
- en: The DataFrame contents are displayed in table format using the the `showDF`
    function of SparkR.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SparkR的`showDF`函数，DataFrame内容以表格形式显示。
- en: A detailed view of the structure of the DataFrame is displayed using the print
    function. This is akin to the describe command of the database tables.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用print函数显示DataFrame结构的详细视图。这类似于数据库表的describe命令。
- en: In the preceding R code, the style of programming is different as compared to
    the Scala code. That is because it is an R program. Using the SparkR library,
    the Spark features are being used. But the functions and other abstractions are
    not in a really different style.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述的R代码中，编程风格与Scala代码相比有所不同，这是因为它是R程序。通过使用SparkR库，正在使用Spark特性。但函数和其他抽象并没有采用截然不同的风格。
- en: Note
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Throughout this chapter, there will be instances where DataFrames are used.
    It is very easy to get confused by which is the R DataFrame and which is the Spark
    DataFrame. Hence, care is taken to specifically mention by qualifying the DataFrame,
    such as R DataFrame and Spark DataFrame.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，将多次涉及DataFrames的使用。很容易混淆哪个是R DataFrame，哪个是Spark DataFrame。因此，特别注意通过限定DataFrame来明确指出，例如R
    DataFrame和Spark DataFrame。
- en: Programming with R DataFrame API
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用R DataFrame API编程
- en: In this section, the code snippets will be run in the same R REPL. Like the
    preceding code snippets, initially, some DataFrame-specific basic commands are
    given. These are used regularly to see the contents and for doing some sanity
    tests on the DataFrame and its contents. These are commands that are typically
    used in the exploratory stage of the data analysis quite often to get more insight
    into the structure and contents of the underlying data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，代码片段将在同一R REPL中运行。与前述代码片段类似，最初会给出一些DataFrame特定的基本命令。这些命令常用于查看内容并对DataFrame及其内容进行一些基本测试。这些命令在数据分析的探索阶段经常使用，以深入了解底层数据的结构和内容。
- en: 'At the R REPL prompt, try the following statements:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在R REPL提示符下，尝试以下语句：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the summary of what the preceding script did from a DataFrame API perspective:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从DataFrame API角度对前面脚本所做操作的概述：
- en: The DataFrame containing the superset of data used in the preceding section
    is used here.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前一节中使用的包含所有数据的DataFrame在此处被使用。
- en: Filtering of the records is demonstrated next. Here, the most important aspect
    to notice is that the filter predicate is to be given exactly like the predicates
    in the SQL statements. Filters can not be chained.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来演示记录的筛选。这里需注意的最重要一点是，筛选条件必须与SQL语句中的谓词完全一致。无法链式应用过滤器。
- en: The aggregation methods are calculated next.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来计算聚合方法。
- en: The final statements in this set are doing the selection, filtering, choosing
    distinct records, and ordering.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本组中的最终语句执行选择、筛选、选择唯一记录和排序。
- en: Finally, the transaction records are persisted in Parquet format, read from
    the Parquet store, and created a Spark DataFrame. More details on the persistence
    formats have been covered in the previous chapter and the concepts remain the
    same. Only the DataFrame API syntax is different.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，交易记录以Parquet格式持久化，从Parquet存储中读取，并创建了一个Spark DataFrame。关于持久化格式的更多细节已在上一章中涵盖，概念保持不变。仅DataFrame
    API语法有所不同。
- en: In this code snippet, the Parquet format data is stored in the current directory,
    from where the corresponding REPL is invoked. When it is run as a Spark program,
    the directory again will be the current directory from where the Spark submit
    is invoked.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此代码片段中，Parquet格式的数据存储在当前目录，从该目录调用相应的REPL。当作为Spark程序运行时，目录再次成为从该处调用Spark提交的当前目录。
- en: The last few statements are about the persisting of the DataFrame contents into
    the media. If this is compared with the persistence mechanisms in the previous
    chapter with Scala and Python, here also it is done in similar ways.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后几条语句涉及将DataFrame内容持久化到介质中。若与前一章中Scala和Python的持久化机制相比较，此处也以类似方式实现。
- en: Understanding aggregations in Spark R
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Spark R中的聚合
- en: In SQL, aggregation of data is very flexible. The same thing is true in Spark
    SQL too. Instead of running SQL statements on a single data source located in
    a single machine, here, Spark SQL can do the same on distributed data sources.
    In the chapter where RDD-based programming is covered, a MapReduce use case was
    discussed to do data aggregation and the same is being used here to demonstrate
    the aggregation capabilities of Spark SQL. In this section also, the use cases
    are approached in the SQL query way as well as in the DataFrame API way.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL中，数据聚合非常灵活。Spark SQL中亦是如此。与在单机上的单一数据源运行SQL语句不同，Spark SQL能够在分布式数据源上执行相同操作。在涵盖RDD编程的章节中，讨论了一个用于数据聚合的MapReduce用例，这里同样使用该用例来展示Spark
    SQL的聚合能力。本节中，用例既通过SQL查询方式处理，也通过DataFrame API方式处理。
- en: 'The use cases selected for elucidating the MapReduce kind of data processing
    are given here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此处给出了用于阐明MapReduce类型数据处理的选定用例：
- en: The retail banking transaction records come with account number and transaction
    amount in comma-separated strings
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零售银行业务交易记录以逗号分隔的字符串形式包含账户号和交易金额
- en: Find an account level summary of all the transactions to get the account balance
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有交易的账户级别汇总以获取账户余额
- en: 'At the R REPL prompt, try the following statements:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在R REPL提示符下，尝试以下语句：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the R DataFrame API, there are some syntax differences as compared to its
    Scala or Python counterparts, mainly because this is a purely API-based programming
    model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在R DataFrame API中，与Scala或Python版本相比，存在一些语法差异，主要是因为这是一种纯粹基于API的编程模型。
- en: Understanding multi-datasource joins with SparkR
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解SparkR中的多数据源连接
- en: In the previous chapter, the joining of multiple DataFrames based on the key
    has been discussed. In this section, the same use case is implemented using R
    API of Spark SQL. The use cases selected for elucidating the join of multiple
    datasets using a key are given in the following section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，基于键合并多个DataFrame的内容已进行讨论。本节中，同一用例通过Spark SQL的R API实现。以下部分给出了用于阐明基于键合并多个数据集的选定用例。
- en: The first dataset contains a retail banking master records summary with the
    account number, first name, and last name. The second dataset contains the retail
    banking account balance with account number and balance amount. The key on both
    of the datasets is the account number. Join the two datasets and create one dataset
    containing the account number, first name, last name, and balance amount. From
    this report, pick up the top three accounts in terms of the balance amount.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数据集包含零售银行业务主记录摘要，包括账号、名字和姓氏。第二个数据集包含零售银行账户余额，包括账号和余额金额。两个数据集的关键字段均为账号。将两个数据集连接，创建一个包含账号、名字、姓氏和余额金额的数据集。从此报告中，挑选出余额金额排名前三的账户。
- en: The Spark DataFrames are created from persisted JSON files. Instead of the JSON
    files, it can be any supported data files. Then they are read from the disk to
    form the DataFrames and they are joined together.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrames由持久化的JSON文件创建。除JSON文件外，还可使用任何支持的数据文件。随后，这些文件从磁盘读取以形成DataFrames，并进行连接。
- en: 'At the R REPL prompt, try the following statements:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在R REPL提示符下，尝试以下语句：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Continuing from the same R REPL session, the following lines of code get the
    same result through the DataFrame API:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一R REPL会话中，以下代码行通过DataFrame API获得相同结果：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The join type selected in the preceding section of the code is inner join. Instead
    of that, any other type of join can be used, either through the SQL query way
    or through the DataFrame API way. One word of caution before using the join using
    DataFrame API is that the column names of both the Spark DataFrames have to be
    different to avoid ambiguity in the resultant Spark DataFrame. In this particular
    use case, it can be seen that the DataFrame API is becoming a bit difficult to
    deal with, while the SQL query way is looking very straightforward.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码段中选择的连接类型为内连接。实际上，可通过SQL查询方式或DataFrame API方式使用其他任何类型的连接。在使用DataFrame API进行连接前，需注意两个Spark
    DataFrame的列名必须不同，以避免结果DataFrame中的歧义。在此特定用例中，可以看出DataFrame API处理起来略显复杂，而SQL查询方式则显得非常直接。
- en: In the preceding sections, the R API for Spark SQL has been covered. In general,
    if possible, it is better to write the code using the SQL query way as much as
    possible. The DataFrame API is getting better, but it is not as flexible as in
    the other languages, such as Scala or Python.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述章节中，已涵盖Spark SQL的R API。通常，若可能，应尽可能使用SQL查询方式编写代码。DataFrame API正在改进，但与其他语言（如Scala或Python）相比，其灵活性仍显不足。
- en: Unlike the other chapters in this book, this is a self-contained one to introduce
    Spark to R programmers. All the use cases that are discussed in this chapter are
    run in the R REPL of Spark. But in real-world applications, this method is not
    ideal. The R commands have to be organized in script files and to be submitted
    to a Spark cluster to run. The easiest way is to use the already existing `$SPARK_HOME/bin/spark-submit
    <path to the R script file>` script, where the fully-qualified R filename is given
    with respect to the current directory from where the command is being invoked.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书其他章节不同，本章是专为R程序员介绍Spark的独立章节。本章讨论的所有用例均在Spark的R REPL中运行。但在实际应用中，这种方法并不理想。R命令需组织在脚本文件中，并提交至Spark集群运行。最简便的方法是使用现有的`$SPARK_HOME/bin/spark-submit
    <path to the R script file>`脚本，其中R文件名需相对于命令调用时的当前目录给出完整路径。
- en: References
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'For more information refer to: [https://spark.apache.org/docs/latest/api/R/index.html](https://spark.apache.org/docs/latest/api/R/index.html)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考：[https://spark.apache.org/docs/latest/api/R/index.html](https://spark.apache.org/docs/latest/api/R/index.html)
- en: Summary
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: A whirlwind tour of the R language was covered in this chapter, followed by
    a special mention about the need to have a distinction of understanding the difference
    between an R DataFrame and a Spark DataFrame. Then, basic Spark programming with
    R was covered using the same use cases of the previous chapters. R API for Spark
    was covered, and the use cases have been implemented using the SQL query way and
    DataFrame API way. This chapter helps data scientists understand the power of
    Spark and use it in their R applications, using the SparkR package that comes
    with Spark. This opens up the door of big data processing, using Spark with R
    to process structured data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了对R语言的快速概览，随后特别提到了需要明确理解R DataFrame与Spark DataFrame之间的区别。接着，使用与前几章相同的用例介绍了基本的Spark编程与R。涵盖了Spark的R
    API，并通过SQL查询方式和DataFrame API方式实现了用例。本章帮助数据科学家理解Spark的强大功能，并将其应用于他们的R应用程序中，使用随Spark附带的SparkR包。这为使用Spark与R处理结构化数据的大数据处理打开了大门。
- en: The subject of Spark-based data processing in various languages has been discussed,
    and it is time to focus on some data analysis with charting and plotting. Python
    comes with a lot of charting and plotting libraries that produce publication quality
    pictures. The next chapter will discuss charting and plotting with the data processed
    by Spark.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于Spark的数据处理在多种语言中的主题已经讨论过，现在是时候专注于一些数据分析以及图表和绘图了。Python自带了许多能够生成出版质量图片的图表和绘图库。下一章将讨论使用Spark处理的数据进行图表和绘图。
