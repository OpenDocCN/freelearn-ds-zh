- en: A Brief Tour of Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习概览
- en: This chapter will take you on a whirlwind tour of machine learning, focusing
    on using the `pandas` library as a tool to preprocess the data used by machine
    learning programs. It will also introduce you to the `scikit-learn` library, which
    is the most popular machine learning toolkit in Python.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将带您快速了解机器学习，重点介绍如何使用`pandas`库作为预处理机器学习程序数据的工具。它还将向您介绍`scikit-learn`库，这是Python中最受欢迎的机器学习工具包。
- en: 'In this chapter, we will illustrate machine learning techniques by applying
    them to a well-known problem about classifying which passengers survived the Titanic
    disaster at the turn of the last century. The various topics addressed in this
    chapter include the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过将机器学习技术应用于一个著名的问题来说明机器学习，即对哪些乘客在世纪之交的Titanic灾难中幸存进行分类。本章将讨论的主题包括：
- en: The role of pandas in machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas在机器学习中的作用
- en: Installing `scikit-learn`
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装`scikit-learn`
- en: Introduction to machine learning concepts
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习概念介绍
- en: Applying machine learning—Kaggle Titanic competition
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用机器学习——Kaggle Titanic竞赛
- en: Data analysis and preprocessing using pandas
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用pandas进行数据分析和预处理
- en: A naïve approach to the Titanic problem
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对Titanic问题的朴素方法
- en: The `scikit-learn` ML classifier interface
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`的ML分类器接口'
- en: Supervised learning algorithms
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习算法
- en: Unsupervised learning algorithms
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习算法
- en: The role of pandas in machine learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas在机器学习中的作用
- en: The library we will be considering for machine learning is called `scikit-learn`.
    The `scikit-learn` Python library is an extensive library of machine learning
    algorithms that can be used to create adaptive programs that learn from data inputs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑的机器学习库是`scikit-learn`。`scikit-learn` Python库是一个广泛的机器学习算法库，可用于创建能够从数据输入中学习的自适应程序。
- en: However, before this data can be used by `scikit-learn`, it must undergo some
    preprocessing. This is where pandas comes in. pandas can be used to preprocess
    and filter data before passing it to the algorithm implemented in `scikit-learn`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这些数据可以被`scikit-learn`使用之前，它必须经过一些预处理。这正是pandas的作用。pandas可以在将数据传递给`scikit-learn`实现的算法之前进行预处理和筛选。
- en: In the coming sections, we will see how `scikit-learn` can be used for machine
    learning. So, as the first step, we will learn how to install it on our machines.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将看到如何使用`scikit-learn`进行机器学习。所以，第一步，我们将学习如何在我们的机器上安装它。
- en: Installation of scikit-learn
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`scikit-learn`的安装'
- en: 'As was mentioned in Chapter 2, *Installation of Python and pandas from Third-Party
    Vendors*, the easiest way to install pandas and its accompanying libraries is
    to use a third-party distribution such as Anaconda and be done with it. Installing
    `scikit-learn` should be no different. I will briefly highlight the steps for
    installation on various platforms and third-party distributions, starting with
    Anaconda. The `scikit-learn` library requires the following libraries:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如第2章所述，*从第三方供应商安装Python和pandas*，安装pandas及其附带库的最简单方法是使用像Anaconda这样的第三方分发包，安装完成即可。安装`scikit-learn`应该没有什么不同。我将简要介绍在不同平台和第三方分发包上安装的步骤，首先从Anaconda开始。`scikit-learn`库需要以下库：
- en: Python 2.6.x or higher
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 2.6.x 或更高版本
- en: NumPy 1.6.1 or higher
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 1.6.1 或更高版本
- en: SciPy 0.9 or higher
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SciPy 0.9 或更高版本
- en: Assuming that you have already installed pandas as described in [Chapter 2](34a6977b-f807-4ee6-9da8-034d9216bb49.xhtml),
    *Installation of pandas and Supporting Software*, these dependencies should already
    be in place. The various options to install `scikit-learn` on different platforms
    are discussed in the following sections.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经按照[第2章](34a6977b-f807-4ee6-9da8-034d9216bb49.xhtml)中描述的内容安装了pandas，*安装pandas和支持软件*，那么这些依赖项应该已经就绪。在接下来的部分中，将讨论在不同平台上安装`scikit-learn`的各种选项。
- en: Installing via Anaconda
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Anaconda安装
- en: 'You can install `scikit-learn` on Anaconda by running the `conda` Python package
    manager:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行`conda` Python包管理器在Anaconda上安装`scikit-learn`：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Installing on Unix (Linux/macOS)
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Unix（Linux/macOS）上的安装
- en: 'For Unix, it is best to install from the source (C compiler is required). Assuming
    that pandas and NumPy are already installed and the required dependent libraries
    are already in place, you can install `scikit-learn` via Git by running the following
    commands:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Unix系统，最好从源代码安装（需要C编译器）。假设pandas和NumPy已经安装并且所需的依赖库已就绪，您可以通过Git运行以下命令来安装`scikit-learn`：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The pandas library can also be installed on Unix by using `pip` from `PyPi`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用`pip`从`PyPi`在Unix上安装pandas库：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Installing on Windows
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Windows上安装
- en: 'To install on Windows, you can open a console and run the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Windows上安装，您可以打开控制台并运行以下命令：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For more in-depth information on installation, you can take a look at the official
    `scikit-learn` documentation at [http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于安装的详细信息，您可以查看官方的`scikit-learn`文档，网址是[http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html)。
- en: You can also take a look at the README file for the `scikit-learn` Git repository
    at [https://github.com/scikit-learn/scikit-learn/blob/master/README.rst](https://github.com/scikit-learn/scikit-learn/blob/master/README.rst).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以查看`scikit-learn` Git 仓库的README文件，网址是[https://github.com/scikit-learn/scikit-learn/blob/master/README.rst](https://github.com/scikit-learn/scikit-learn/blob/master/README.rst)。
- en: Introduction to machine learning
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: Machine learning is the art of creating software programs that learn from data.
    More formally, it can be defined as the practice of building adaptive programs
    that use tunable parameters to improve predictive performance. It is a sub-field
    of artificial intelligence.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是创建从数据中学习的软件程序的艺术。更正式地说，它可以定义为构建自适应程序的实践，这些程序通过可调参数来提高预测性能。它是人工智能的一个子领域。
- en: We can separate machine learning programs based on the type of problems they
    are trying to solve. These problems are appropriately called learning problems.
    The two categories of these problems, broadly speaking, are referred to as supervised
    and unsupervised learning problems. Furthermore, there are some hybrid problems
    that have aspects that involve both categories—supervised and unsupervised.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据机器学习程序尝试解决的问题类型来将其分类。这些问题被称为学习问题。广义上讲，这些问题分为两大类——监督学习问题和无监督学习问题。此外，还有一些混合问题，它们涉及监督学习和无监督学习的多个方面。
- en: The input to a learning problem consists of a dataset of *n* rows. Each row
    represents a sample and may involve one or more fields referred to as attributes
    or features. A dataset can be canonically described as consisting of *n* samples,
    each consisting of *n* features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 学习问题的输入由一个包含*n*行的数据集组成。每一行代表一个样本，可能涉及一个或多个称为属性或特征的字段。数据集可以被标准化描述为包含*n*个样本，每个样本由*n*个特征组成。
- en: Supervised versus unsupervised learning
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习与无监督学习
- en: 'For supervised learning problems, the input to a learning problem is a dataset
    consisting of *labeled* data. By this, we mean that we have outputs whose values
    are known. The learning program is fed with input samples and their corresponding
    outputs and its goal is to decipher the relationship between them. Such input
    is known as labeled data. Supervised learning problems include the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监督学习问题，学习问题的输入是一个由*已标记*数据组成的数据集。这里的“已标记”意味着我们知道输出的值。学习程序会接收输入样本及其对应的输出，目标是解码它们之间的关系。这种输入称为已标记数据。监督学习问题包括以下几种：
- en: '**Classification**: The learned attribute is categorical (nominal) or discrete'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：学习的属性是类别型（名义型）或离散型'
- en: '**Regression**: The learned attribute is numeric/continuous'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：学习的属性是数值型/连续型'
- en: 'In unsupervised learning or data mining, the learning program is fed with inputs
    but does without the corresponding outputs. This input data is referred to as
    unlabeled data. The goal of machine learning in such cases is to learn or decipher
    the hidden label. Such problems include the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习或数据挖掘中，学习程序会接受输入，但没有相应的输出。这些输入数据被称为未标记数据。在这种情况下，机器学习的目标是学习或解码隐藏的标签。这类问题包括以下几种：
- en: Clustering
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Dimensionality reduction
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维
- en: Illustration using document classification
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以文档分类为例说明
- en: A common usage of machine learning techniques is in the area of document classification.
    The two main categories of machine learning can be applied to this problem—supervised
    and unsupervised learning.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习技术的常见应用之一是在文档分类领域。机器学习的两大主要类别可以应用于这个问题——监督学习和无监督学习。
- en: Supervised learning
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: Each document in the input collection is assigned to a category; that is, a
    label. The learning program/algorithm uses the input collection of documents to
    learn how to make predictions for another set of documents with no labels. This
    method is known as **classification**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输入集合中的每个文档都被分配到一个类别，也就是一个标签。学习程序/算法使用输入集合中的文档来学习如何为另一组没有标签的文档做出预测。这种方法被称为**分类**。
- en: Unsupervised learning
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: The documents in the input collection are not assigned to categories; hence,
    they are unlabeled. The learning program takes this as input and tries to *cluster*
    or discover groups of related or similar documents. This method is known as **clustering**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 输入集合中的文档没有被分配到类别中，因此它们是未标注的。学习程序将这些作为输入，尝试*聚类*或发现相关或相似文档的组。这种方法被称为**聚类**。
- en: How machine learning systems learn
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习系统如何学习
- en: 'Machine learning systems utilize what is known as a classifier to learn from
    data. A *classifier* is an interface that takes a matrix of what is known as *feature
    values* and produces an output vector, also known as the class. These feature
    values may be discrete or continuously valued. There are three core components
    of classifiers:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统利用一种被称为分类器的工具从数据中学习。*分类器*是一个接口，它接受一个被称为*特征值*的矩阵，并生成一个输出向量，也就是类别。这些特征值可以是离散的，也可以是连续的。分类器的三个核心组成部分如下：
- en: '**Representation**: What type of classifier is it?'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表示**：它是什么类型的分类器？'
- en: '**Evaluation**: How good is the classifier?'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：分类器的表现如何？'
- en: '**Optimization**: How can you search among the alternatives?'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化**：你如何在备选方案中进行搜索？'
- en: Application of machine learning – Kaggle Titanic competition
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习应用 – Kaggle 泰坦尼克号比赛
- en: To illustrate how we can use pandas to assist us at the start of our machine
    learning journey, we will apply it to a classic problem, which is hosted on the
    Kaggle website ([http://www.kaggle.com](http://www.kaggle.com)). **Kaggle** is
    a competition platform for machine learning problems. The idea behind Kaggle is
    to enable companies that are interested in solving predictive analytics problems
    with their data to post their data on Kaggle and invite data scientists to come
    up with proposed solutions to their problems. A competition can be ongoing over
    a period of time, and the rankings of the competitors are posted on a leaderboard.
    At the close of the competition, the top-ranked competitors receive cash prizes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何使用pandas帮助我们启动机器学习旅程，我们将应用它于一个经典问题，该问题托管在Kaggle网站上（[http://www.kaggle.com](http://www.kaggle.com)）。**Kaggle**是一个机器学习问题的竞赛平台。Kaggle的理念是，允许有兴趣通过数据解决预测分析问题的公司将其数据发布到Kaggle，并邀请数据科学家提出解决方案。竞赛可以在一段时间内进行，竞争者的排名会公布在排行榜上。在竞赛结束时，排名靠前的竞争者将获得现金奖励。
- en: 'The classic problem that we will study to illustrate the use of pandas for
    machine learning with `scikit-learn` is the *Titanic: Machine Learning from Disaster*
    problem hosted on Kaggle as their classic introductory machine learning problem.
    The dataset involved in the problem is a raw dataset. Hence, pandas is very useful
    in the preprocessing and cleansing of the data before it is submitted as input
    to the machine learning algorithm implemented in `scikit-learn`.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究的经典问题是为了说明如何使用pandas进行机器学习，并结合`scikit-learn`，这是Kaggle上托管的经典入门机器学习问题——*泰坦尼克号：灾难中的机器学习*问题。该问题涉及的数据集是一个原始数据集。因此，pandas在数据预处理和清洗方面非常有用，帮助在将数据输入到`scikit-learn`实现的机器学习算法之前对其进行整理。
- en: 'The Titanic: Machine Learning from Disaster problem'
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 泰坦尼克号：灾难中的机器学习问题
- en: The dataset for the Titanic consists of the passenger manifest for the doomed
    trip, along with various features and an indicator variable telling whether the
    passenger survived the sinking of the ship or not. The essence of the problem
    is to be able to predict, given a passenger and his/her associated features, whether
    this passenger survived the sinking of the Titanic or not. The features are as
    follows.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 泰坦尼克号的数据集包括这次命运多舛旅行的乘客名单，以及各种特征和一个指示变量，告诉我们乘客是否在船沉没时幸存。问题的核心是，给定一位乘客及其相关特征，能否预测该乘客是否幸存于泰坦尼克号的沉船事件中。特征如下所示。
- en: 'The data consists of two datasets: one training dataset and one test dataset.
    The training dataset consists of 891 passenger cases, and the test dataset consists
    of 491 passenger cases.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包括两个数据集：一个训练数据集和一个测试数据集。训练数据集包含891个乘客案例，测试数据集包含491个乘客案例。
- en: The training dataset also consists of 11 variables, of which 10 are features
    and 1 dependent/indicator variable, `Survived`, which indicated whether the passenger
    survived the disaster or not.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集也包含11个变量，其中10个是特征，1个是因变量/指示变量`Survived`，它指示乘客是否在灾难中幸存。
- en: 'The feature variables are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 特征变量如下：
- en: PassengerID
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 乘客ID
- en: Cabin
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 舱位
- en: Sex
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性别
- en: Pclass (passenger class)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 船舱等级（Pclass）
- en: Fare
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 票价
- en: Parch (number of parents and children)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 父母和子女数量（Parch）
- en: Age
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄
- en: Sibsp (number of siblings)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 兄弟姐妹数量（Sibsp）
- en: Embarked
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上船港口
- en: 'We can make use of pandas to help us to preprocess data in the following ways:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用pandas来帮助我们通过以下方式进行数据预处理：
- en: Data cleaning and the categorization of some variables
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据清理和某些变量的分类
- en: The exclusion of unnecessary features that obviously have no bearing on the
    survivability of the passenger; for example, name
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除明显与乘客生存性无关的无关特征；例如，姓名
- en: Handling missing data
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: 'There are various algorithms that we can use to tackle this problem. They are
    as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用多种算法来解决这个问题。它们如下：
- en: Decision trees
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Neural networks
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Random forests
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Support vector machines
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: The problem of overfitting
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合问题
- en: '**Overfitting** is a well-known problem in machine learning, whereby the program
    memorizes the specific data that it is fed as input, leading to perfect results
    on the training data and abysmal results on the test data.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合**是机器学习中的一个著名问题，程序会记住输入的数据，导致在训练数据上表现完美，但在测试数据上却表现糟糕。'
- en: To prevent overfitting, the tenfold cross-validation technique can be used to
    introduce variability in the data during the training phase.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止过拟合，可以使用十折交叉验证技术，在训练阶段引入数据的变异性。
- en: Data analysis and preprocessing using pandas
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pandas进行数据分析和预处理
- en: In this section, we will utilize pandas to do some analysis and preprocessing
    of the data before submitting it as input to `scikit-learn`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将利用pandas对数据进行一些分析和预处理，然后将其作为输入提交给`scikit-learn`。
- en: Examining the data
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查数据
- en: To start our preprocessing of the data, let's read in the training dataset and
    examine what it looks like.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始我们的数据预处理，让我们读取训练数据集并检查它的样子。
- en: 'Here, we read the training dataset into a pandas DataFrame and display the
    first rows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将训练数据集读取到一个pandas DataFrame中并显示前几行：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/f0f654ad-9bf5-41ce-a31f-ef012a4e523f.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0f654ad-9bf5-41ce-a31f-ef012a4e523f.png)'
- en: 'Hence, we can see the various features: PassengerId, Survived, PClass, Name,
    Sex, Age, Sibsp, Parch, Ticket, Fare, Cabin, and Embarked. One question that springs
    to mind immediately is this: which of the features are likely to influence whether
    a passenger survived or not?'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到各个特征：PassengerId、Survived、PClass、Name、Sex、Age、Sibsp、Parch、Ticket、Fare、Cabin
    和 Embarked。一个立刻浮现在脑海中的问题是：哪些特征可能会影响乘客是否幸存？
- en: It should seem obvious that PassengerID, Ticket Code, and Name should not be
    influencers on survivability since they're *identifier* variables. We will skip
    these in our analysis.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来显而易见的是，PassengerID、票号和姓名不应当影响生存性，因为它们是*标识符*变量。我们将在分析中跳过这些变量。
- en: Handling missing values
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: One issue that we have to deal with in datasets for machine learning is how
    to handle missing values in the training set.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的数据集处理过程中，我们必须解决的一个问题是如何处理训练集中的缺失值。
- en: Let's visually identify where we have missing values in our feature set.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直观地识别特征集中缺失值的位置。
- en: 'For that, we can make use of an equivalent of the `missmap` function in R,
    written by Tom Augspurger. The next screenshot shows how much data is missing
    for the various features in an intuitively appealing manner:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以使用Tom Augspurger编写的R语言`missmap`函数的等效版本。下一个截图直观地展示了各个特征缺失数据的情况：
- en: '![](img/0f9de889-cbba-4253-b9bc-50fad33d7ff9.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f9de889-cbba-4253-b9bc-50fad33d7ff9.png)'
- en: 'For more information and the code used to generate this data, see the following:
    [http://tomaugspurger.github.io/blog/2014/02/22/Visualizing%20Missing%20Data/](http://tomaugspurger.github.io/blog/2014/02/22/Visualizing%20Missing%20Data/).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息及生成此数据的代码，请参阅以下链接：[http://tomaugspurger.github.io/blog/2014/02/22/Visualizing%20Missing%20Data/](http://tomaugspurger.github.io/blog/2014/02/22/Visualizing%20Missing%20Data/)。
- en: 'We can also calculate how much data is missing for each of the features:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以计算每个特征缺失的数据量：
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Hence, we can see that most of the `Cabin` data is missing (77%), while around
    20% of the `Age` data is missing. We then decide to drop the `Cabin` data from
    our learning feature set as the data is too sparse to be of much use.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到大部分`Cabin`数据缺失（77%），而约20%的`Age`数据缺失。我们决定从学习特征集中删除`Cabin`数据，因为该数据过于稀疏，无法提供有用的信息。
- en: 'Let''s do a further breakdown of the various features that we would like to
    examine. In the case of categorical/discrete features, we use bar plots; for continuous
    valued features, we use histograms. The code to generate the charts is as shown:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步细分我们希望检查的各个特征。对于分类/离散特征，我们使用条形图；对于连续值特征，我们使用直方图。生成图表的代码如下所示：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Take a look at the following output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下输出：
- en: '![](img/f90ae22d-a5f1-448a-85d8-0979f1254429.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f90ae22d-a5f1-448a-85d8-0979f1254429.png)'
- en: 'From the data and illustration in the preceding screenshot, we can observe
    the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的数据和截图中，我们可以观察到以下几点：
- en: About twice as many passengers perished than survived (62% versus 38%).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 死亡的乘客数量大约是幸存者的两倍（62%对38%）。
- en: There were about twice as many male passengers as female passengers (65% versus
    35%).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 男性乘客大约是女性乘客的两倍（65%对35%）。
- en: There were about 20% more passengers in the third class versus the first and
    second together (55% versus 45%).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三舱的乘客比第一和第二舱的总和多约20%（55%对45%）。
- en: Most passengers were solo; that is, had no children, parents, siblings, or spouse
    on board.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数乘客是单独旅行的；即，船上没有孩子、父母、兄弟姐妹或配偶。
- en: These observations might lead us to dig deeper and investigate whether there
    is some correlation between the chances of survival and gender and fare class,
    particularly if we take into account the fact that the Titanic had a women-and-children-first
    policy and the fact that the Titanic was carrying fewer lifeboats (20) than it
    was designed to (32).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观察结果可能促使我们深入研究是否存在生还机会与性别和票价等级之间的某种相关性，特别是如果我们考虑到泰坦尼克号实行“先妇女儿童后”政策以及泰坦尼克号所搭载的救生艇（20艘）比设计容量（32艘）要少这一事实。
- en: 'In light of this, let''s further examine the relationships between survival
    and some of these features. We''ll start with gender:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，让我们进一步考察生还与这些特征之间的关系。我们从性别开始：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Take a look at the following table:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下表格：
- en: '| **Gender** | **Survived** | **Perished** | **Survival Rate** |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| **性别** | **幸存** | **遇难** | **生还率** |'
- en: '| Men | 109 | 468 | 18.89 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 109 | 468 | 18.89 |'
- en: '| Women | 233 | 81 | 74.2 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 女性 | 233 | 81 | 74.2 |'
- en: 'We''ll now illustrate this data in a bar chart:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将以条形图的形式展示这些数据：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code produces the following bar graph diagram:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下条形图：
- en: '![](img/23a1457e-6572-4ee0-9e3f-276d3a8b03ad.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23a1457e-6572-4ee0-9e3f-276d3a8b03ad.png)'
- en: From the preceding diagram, we can see that the majority of the women survived
    (74%), while most of the men perished (only 19% survived).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中可以看到，大多数女性幸存（74%），而大多数男性遇难（仅19%幸存）。
- en: This leads us to the conclusion that the gender of the passenger may be a contributing
    factor to whether a passenger survived or not.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们得出结论：乘客的性别可能是决定乘客是否生还的一个因素。
- en: 'Next, let''s look at passenger class. First, we generate the survived and perished
    data for each of the three passenger classes, as well as survival rates:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下乘客舱位。首先，我们生成每个舱位的幸存和遇难数据，并计算生还率：
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we show them in a table:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这些数据展示在表格中：
- en: '| **Class** | **Survived** | **Perished** | **Survival Rate** |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **舱位** | **幸存** | **遇难** | **生还率** |'
- en: '| First Class | 136 | 80 | 62.96 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 第一舱 | 136 | 80 | 62.96 |'
- en: '| Second Class | 87 | 97 | 47.28 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 第二舱 | 87 | 97 | 47.28 |'
- en: '| Third Class | 119 | 372 | 24.24 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 第三舱 | 119 | 372 | 24.24 |'
- en: 'We can then plot the data by using `matplotlib` in a similar manner to that
    for the survivor count by gender described earlier:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`matplotlib`以类似于之前描述的按性别统计幸存者数量的方式来绘制数据：
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This produces the following bar plot diagram:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下条形图：
- en: '![](img/a2771a28-dae4-414b-bfe0-e48e2e22cb91.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2771a28-dae4-414b-bfe0-e48e2e22cb91.png)'
- en: It seems clear from the preceding data and diagram that the higher the passenger
    fare class, the greater the passenger's chances of survival.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的数据和图表中可以清晰地看出，乘客的票价等级越高，乘客的生还几率越大。
- en: 'Given that both gender and fare class seem to influence the chances of a passenger''s
    survival, let''s see what happens when we combine these two features and plot
    a combination of both. For this, we will use the `crosstab` function in pandas:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于性别和票价等级似乎会影响乘客的生还几率，让我们看看当我们将这两个特征结合起来并绘制它们的组合时会发生什么。为此，我们将使用pandas中的`crosstab`函数：
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s now display this data using `matplotlib`. First, let''s do some re-labeling
    for display purposes:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`matplotlib`来显示这些数据。首先，为了展示方便，我们做一些重新标记：
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we plot the passenger data by using the `plot` function of a pandas `DataFrame`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用pandas `DataFrame`的`plot`函数绘制乘客数据：
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/2f1447b0-5947-4214-af44-5a4c924e9e20.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f1447b0-5947-4214-af44-5a4c924e9e20.png)'
- en: A naive approach to the Titanic problem
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一种对泰坦尼克号问题的简单方法
- en: 'Our first attempt at classifying the Titanic data is to use a naive, yet very
    intuitive, approach. This approach involves the following steps:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对泰坦尼克号数据的首次分类尝试是使用一种简单且非常直观的方法。该方法包括以下步骤：
- en: Select a set of features, *S*, that influence whether a person survived or not.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一组特征，*S*，来影响一个人是否生还。
- en: For each possible combination of features, use the training data to indicate
    whether the majority of cases survived or not. This can be evaluated in what is
    known as a survival matrix.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每种可能的特征组合，使用训练数据来指示大多数情况下是否生还。这可以通过所谓的生还矩阵来评估。
- en: For each test example that we wish to predict survival, look up the combination
    of features that corresponds to the values of its features and assign its predicted
    value to the survival value in the survival table. This approach is a naive K-nearest
    neighbor approach.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们希望预测生还的每个测试样本，查找与其特征值对应的特征组合，并将其预测值分配给生还表中的生还值。该方法是一种简单的K近邻方法。
- en: 'Based on what we have seen earlier in our analysis, three features seem to
    have the most influence on the survival rate:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们之前在分析中看到的，三个特征似乎对生还率有最大的影响：
- en: Passenger class
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 乘客舱位
- en: Gender
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性别
- en: Passenger fare (bucketed)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 乘客票价（按类别划分）
- en: We include passenger fare as it is related to passenger class.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们包括了乘客票价，因为它与乘客舱位有关。
- en: 'The survival table looks something similar to the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 生还表大致如下所示：
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To see how we use this table, let''s take a look at a snippet of our test data:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看我们如何使用这个表格，让我们来看一下我们测试数据的一部分：
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For passenger `892`, we see that he is male, his ticket price was 7.8292, and
    he traveled in the third class. Hence, the key for the survival table lookup for
    this passenger is *{Sex='male', Pclass=3, PriceBucket=0 (since 7.8292 falls in
    bucket 0)}*. If we look up the survival value corresponding to this key in our
    survival table (row 17), we see that the value is `0`, that is, perished; this
    is the value that we will predict.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于乘客`892`，我们看到他是男性，票价为7.8292，他乘坐的是第三舱。因此，这个乘客生还表查找的关键字是*{Sex='male', Pclass=3,
    PriceBucket=0（因为7.8292属于0号桶）}*。如果我们在生还表中查找这个关键字对应的生还值（第17行），我们看到该值为`0`，即未能生还；这是我们将要预测的值。
- en: Similarly, for passenger `893`, we have *key={Sex='female', Pclass=3, PriceBucket=0}*.
    This corresponds to row 16, and hence, we will predict `1`, that is, survived,
    and her predicted survival is `1`, that is, survived.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于乘客`893`，我们有*key={Sex='female', Pclass=3, PriceBucket=0}*。这对应于第16行，因此我们会预测`1`，即生还，而她的预测生还值是`1`，即生还。
- en: 'Hence, our results look like the following command:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的结果类似于以下命令：
- en: '[PRE16]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The source of this information is at [http://www.markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/](http://www.markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 该信息的来源在 [http://www.markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/](http://www.markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/)。
- en: Using the survival table approach outlined earlier, we can achieve an accuracy
    of 0.77990 on Kaggle ([http://www.kaggle.com](http://www.kaggle.com)). The survival
    table approach, while intuitive, is a very basic approach that represents only
    the tip of the iceberg of possibilities in machine learning.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前概述的生还表方法，我们可以在Kaggle上实现0.77990的准确度（[http://www.kaggle.com](http://www.kaggle.com)）。尽管生还表方法直观且简单，但它只是机器学习中可能性冰山一角的非常基础的方法。
- en: In the following sections, we will take a whirlwind tour of various machine
    learning algorithms that will help you, the reader, to get a feel for what is
    available in the machine learning universe.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将快速浏览几种机器学习算法，帮助读者了解机器学习领域中的各种可用工具。
- en: The scikit-learn ML/classifier interface
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: scikit-learn 的 ML/分类器接口
- en: We'll be diving into the basic principles of machine learning and demonstrate
    the use of these principles via the `scikit-learn` basic API.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨机器学习的基本原理，并通过 `scikit-learn` 的基础 API 演示这些原理的应用。
- en: 'The `scikit-learn` library has an estimator interface. We illustrate it by
    using a linear regression model. For example, consider the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 库具有估计器接口。我们通过使用线性回归模型来说明这一点。例如，考虑以下内容：'
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The estimator interface is instantiated to create a model, which is a linear
    regression model in this case:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器接口被实例化以创建一个模型，在这种情况下是线性回归模型：
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we specify `normalize=True`, indicating that the *x*-values will be normalized
    before regression. **Hyperparameters** (estimator parameters) are passed on as
    arguments in the model creation. This is an example of creating a model with tunable
    parameters.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定 `normalize=True`，表示在回归之前将对 *x* 值进行归一化处理。**超参数**（估计器参数）作为模型创建时的参数传入。这是创建具有可调参数模型的示例。
- en: 'The estimated parameters are obtained from the data when the data is fitted
    with an estimator. Let''s first create some sample training data that is normally
    distributed about `y = x/2`. We first generate our `x` and `y` values:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 估计参数是通过拟合估计器时从数据中获得的。让我们首先创建一些样本训练数据，这些数据围绕 `y = x/2` 正态分布。我们首先生成 `x` 和 `y`
    的值：
- en: '[PRE19]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`sklearn` takes a 2D array of `num_samples × num_features` as input, so we
    convert our `x` data into a 2D array:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn` 接受一个 `num_samples × num_features` 的二维数组作为输入，因此我们将我们的 `x` 数据转换为二维数组：'
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In this case, we have 500 samples and 1 feature, `x`. We now train/fit the
    model and display the slope (coefficient) and the intercept of the regression
    line, which is the prediction:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们有 500 个样本和 1 个特征，`x`。现在我们训练/拟合模型，并显示回归线的斜率（系数）和截距，即预测值：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This can be visualized as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下方式进行可视化：
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/533274a2-3d1e-4c27-8033-cfaf94cf0390.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/533274a2-3d1e-4c27-8033-cfaf94cf0390.png)'
- en: 'To summarize the basic use of the estimator interface, follow these steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 总结估计器接口的基本使用方法，请遵循以下步骤：
- en: 'Define your model: `LinearRegression`, `SupportVectorMachine`, `DecisionTrees`,
    and so on. You can specify the required hyperparameters in this step; for example,
    `normalize=True`, as specified earlier.'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义你的模型：`LinearRegression`、`SupportVectorMachine`、`DecisionTrees` 等。你可以在此步骤中指定所需的超参数；例如，如前所述，`normalize=True`。
- en: Once the model has been defined, you can train your model on your data by calling
    the `fit(..)` method on the model defined in the previous step.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型被定义，你可以通过调用在前一步定义的模型上的 `fit(..)` 方法，在数据上训练模型。
- en: Once we have fit the model, we can call the `predict(..)` method on test data
    to make predictions or estimations.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们拟合了模型，就可以在测试数据上调用 `predict(..)` 方法进行预测或估算。
- en: In the case of a supervised learning problem, the `predict(X)` method is given
    unlabeled observations, `X`, and returns predicted labels, `y`.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于监督学习问题，`predict(X)` 方法接受未标记的观测值 `X`，并返回预测标签 `y`。
- en: 'For extra information, please see the following: [http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参见以下链接：[http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html)
- en: Supervised learning algorithms
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习算法
- en: We will take a brief tour of some well-known supervised learning algorithms
    and see how we can apply them to the Titanic survival prediction problem described
    earlier.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要介绍一些著名的监督学习算法，并看看如何将它们应用于前面提到的泰坦尼克号生存预测问题。
- en: Constructing a model using Patsy for scikit-learn
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Patsy 为 scikit-learn 构建模型
- en: Before we start our tour of the machine learning algorithms, we need to know
    a little bit about the `Patsy` library. We will make use of `Patsy` to design
    features that will be used in conjunction with `scikit-learn`. `Patsy` is a package
    for creating what is known as design matrices. These design matrices are transformations
    of the features in our input data. The transformations are specified by expressions
    known as formulas, which correspond to a specification of what features we wish
    the machine learning program to utilize in learning.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始介绍机器学习算法之前，我们需要了解一些关于`Patsy`库的知识。我们将利用`Patsy`来设计与`scikit-learn`配合使用的特征。`Patsy`是一个用于创建设计矩阵的包。设计矩阵是我们输入数据特征的变换。这些变换由公式表示，公式对应于我们希望机器学习程序在学习过程中使用的特征规范。
- en: 'A simple example of this is as follows: suppose that we wish to linearly regress
    a variable, y, against some other variables—`x`, `a`, and `b`—and the interaction
    between `a` and `b`; then, we can specify the model as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的例子如下：假设我们希望对一个变量y进行线性回归，回归变量包括其他变量`x`、`a`和`b`，以及`a`与`b`的交互作用；那么，我们可以按如下方式指定模型：
- en: '[PRE23]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the preceding line of code, the formula is specified by the following expression:
    `y ~ x + a + b + a:b`.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一行代码中，公式通过以下表达式指定：`y ~ x + a + b + a:b`。
- en: For further information, look at [http://patsy.readthedocs.org/en/latest/overview.html](http://patsy.readthedocs.org/en/latest/overview.html).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请查看[http://patsy.readthedocs.org/en/latest/overview.html](http://patsy.readthedocs.org/en/latest/overview.html)。
- en: General boilerplate code explanation
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用模板代码解释
- en: In this section, we will introduce boilerplate code for the implementation of
    the following algorithms by using `Patsy` and `scikit-learn`. The reason for doing
    this is that most of the code for the following algorithms is repeatable.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将介绍使用`Patsy`和`scikit-learn`实现以下算法的模板代码。之所以这样做，是因为以下算法的大部分代码是可重复的。
- en: 'In the following sections, the workings of the algorithms will be described
    together with the code specific to each algorithm:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，将描述算法的工作原理，并提供每种算法的特定代码：
- en: 'First, let''s make sure that we''re in the correct folder by using the following
    command line. Assuming that the working directory is located at `~/devel/Titanic`,
    we have the following:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们通过以下命令确保我们位于正确的文件夹中。假设工作目录位于`~/devel/Titanic`，我们有以下内容：
- en: '[PRE24]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, we import the required packages and read in our training and test datasets:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们导入所需的包并读取训练集和测试集数据：
- en: '[PRE25]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we specify the formulas we would like to submit to `Patsy`:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们指定希望提交给`Patsy`的公式：
- en: '[PRE26]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We will define a function that helps us to handle missing values. The following
    function finds the cells within the DataFrame that have null values, obtains the
    set of similar passengers, and sets the null value to the mean value of that feature
    for the set of similar passengers. Similar passengers are defined as those having
    the same gender and passenger class as the passenger with the null feature value:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个函数来帮助处理缺失值。以下函数会查找数据框中包含空值的单元格，获取相似乘客的集合，并将空值设置为该集合中相似乘客特征的均值。相似乘客定义为性别和乘客舱位与缺失特征值的乘客相同的乘客：
- en: '[PRE27]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here, we create filled versions of our training and test DataFrames.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了填充版本的训练和测试数据框。
- en: 'Our test DataFrame is what the fitted `scikit-learn` model will generate predictions
    on to produce output that will be submitted to Kaggle for evaluation:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试数据框是经过拟合的`scikit-learn`模型将在其上生成预测，并将输出提交给Kaggle进行评估：
- en: '[PRE28]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here is the actual implementation of the call to `scikit-learn` to learn from
    the training data by fitting a model and then generate predictions on the test
    dataset. Note that even though this is boilerplate code, for the purpose of illustration,
    an actual call is made to a specific algorithm—in this case, `DecisionTreeClassifier`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是实际调用`scikit-learn`进行训练并生成预测的实现。请注意，尽管这段代码是模板代码，但为了说明目的，实际调用了一个特定算法——在此例中为`DecisionTreeClassifier`。
- en: 'The output data is written to files with descriptive names, for example, `csv/dt_PClass_Sex_Age_Sibsp_Parch_1.csv`
    and `csv/dt_PClass_Sex_Fare_1.csv`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 输出数据会写入带有描述性名称的文件，例如`csv/dt_PClass_Sex_Age_Sibsp_Parch_1.csv`和`csv/dt_PClass_Sex_Fare_1.csv`：
- en: '[PRE29]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding code follows a standard recipe, and the summary is as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码遵循了一个标准的流程，摘要如下：
- en: Read in the training and test datasets.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取训练集和测试集数据。
- en: Fill in any missing values for the features we wish to consider in both datasets.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充我们希望在两个数据集中考虑的特征的任何缺失值。
- en: Define formulas for the various feature combinations we wish to generate machine
    learning models for in `Patsy`.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Patsy`中定义我们希望为其生成机器学习模型的各种特征组合的公式。
- en: 'For each formula, perform the following set of steps:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个公式，执行以下步骤：
- en: 1\. Call `Patsy` to create design matrices for our training feature set and
    training label set (designated by `X_train` and `y_train`).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 调用`Patsy`为我们的训练特征集和训练标签集（分别由`X_train`和`y_train`指定）创建设计矩阵。
- en: 2\. Instantiate the appropriate `scikit-learn` classifier. In this case, we
    use `DecisionTreeClassifier`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 实例化适当的`scikit-learn`分类器。在本例中，我们使用`DecisionTreeClassifier`。
- en: 3\. Fit the model by calling the `fit(..)` method.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 通过调用`fit(..)`方法来拟合模型。
- en: 4\. Make a call to `Patsy` to create a design matrix (`X_test`) for our predicted
    output via a call to `patsy.dmatrix(..)`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 调用`Patsy`创建我们的预测输出的设计矩阵（`X_test`），通过调用`patsy.dmatrix(..)`。
- en: 5\. Predict on the `X_test` design matrix, and save the results in the variable
    predicted.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 在`X_test`设计矩阵上进行预测，并将结果保存在变量predicted中。
- en: 6\. Write our predictions to an output file, which will be submitted to Kaggle.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 将我们的预测结果写入输出文件，提交给 Kaggle。
- en: 'We will consider the following supervised learning algorithms:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑以下监督学习算法：
- en: Logistic regression
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Support vector machine
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Random forest
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Decision trees
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Logistic regression
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: In logistic regression, we attempt to predict the outcome of a categorical,
    that is, discrete-valued dependent variable based on one or more input predictor
    variables.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，我们尝试根据一个或多个输入预测变量预测一个类别的结果，即离散值的因变量。
- en: 'Logistic regression can be thought of as the equivalent of applying linear
    regression but to discrete or categorical variables. However, in the case of binary
    logistic regression (which applies to the Titanic problem), the function to which
    we''re trying to fit is not a linear one as we''re trying to predict an outcome
    that can take only two values—0 and 1\. Using a linear function for our regression
    doesn''t make sense as the output cannot take values between 0 and 1\. Ideally,
    what we need to model for the regression of a binary valued output is some sort
    of step function for values 0 and 1\. However, such a function is not well-defined
    and not differentiable, so an approximation with nicer properties was defined:
    the logistic function. The logistic function takes values between 0 and 1 but
    is skewed toward the extreme values of 0 and 1 and can be used as a good approximation
    for the regression of categorical variables. The formal definition of the logistic
    regression function is as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归可以看作是将线性回归应用于离散或类别变量的等价方法。然而，在二元逻辑回归（应用于 Titanic 问题）的情况下，我们试图拟合的函数并不是线性的，因为我们要预测的结果只有两个可能的取值——0
    和 1\. 使用线性函数进行回归没有意义，因为输出值不能介于 0 和 1 之间。理想情况下，我们需要为二值输出的回归建模一个阶跃函数，用于 0 和 1 之间的值。然而，这样的函数没有明确的定义，且不可微分，因此定义了一个具有更好性质的近似函数：逻辑函数。逻辑函数的值介于
    0 和 1 之间，但它偏向于 0 和 1 的极值，可以作为类别变量回归的良好近似。逻辑回归函数的正式定义如下：
- en: '*          f(x) = 1/((1+e^(-ax))*'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*          f(x) = 1/((1+e^(-ax))*'
- en: 'The following diagram is a good illustration as to why the logistic function
    is suitable for binary logistic regression:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示很好地说明了为何逻辑函数适用于二元逻辑回归：
- en: '![](img/a6c93ebc-7c4d-4a2c-991a-8dc9f77c1a32.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6c93ebc-7c4d-4a2c-991a-8dc9f77c1a32.png)'
- en: We can see that as we increase the value of our parameter, *a*, we get closer
    to taking on the 0 to 1 values and to the step function we wish to model. A simple
    application of the preceding function would be to set the output value to 0, if
    *f(x) <0.5*, and 1 if not.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，随着我们增加参数*a*的值，结果值逐渐接近 0 到 1 之间的值，并且逐步逼近我们希望建模的阶跃函数。前面函数的简单应用是，如果*f(x)
    < 0.5*，则将输出值设为 0，否则设为 1。
- en: The code for plotting the function is included in `plot_logistic.py`.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制该函数的代码包含在`plot_logistic.py`中。
- en: A more detailed examination of logistic regression may be found at [http://en.wikipedia.org/wiki/Logit](http://en.wikipedia.org/wiki/Logit)
    and [http://logisticregressionanalysis.com/86-what-is-logistic-regression](http://logisticregressionanalysis.com/86-what-is-logistic-regression).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 对逻辑回归的更详细检查可以参考[http://en.wikipedia.org/wiki/Logit](http://en.wikipedia.org/wiki/Logit)
    和 [http://logisticregressionanalysis.com/86-what-is-logistic-regression](http://logisticregressionanalysis.com/86-what-is-logistic-regression)。
- en: In applying logistic regression to the Titanic problem, we wish to predict a
    binary outcome, that is, whether a passenger survived or not.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在将逻辑回归应用于 Titanic 问题时，我们希望预测一个二元结果，即乘客是否幸存。
- en: We adapted the boilerplate code to use the `sklearn.linear_model.LogisticRegression`
    class of `scikit-learn`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调整了模板代码，使用 `scikit-learn` 的 `sklearn.linear_model.LogisticRegression` 类。
- en: 'Upon submitting our data to Kaggle, the following results were obtained:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 提交数据到 Kaggle 后，得到以下结果：
- en: '| **Formula** | **Kaggle Score** |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| **公式** | **Kaggle 分数** |'
- en: '| C(Pclass) + C(Sex) + Fare | 0.76077 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) + Fare | 0.76077 |'
- en: '| C(Pclass) + C(Sex) | 0.76555 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) | 0.76555 |'
- en: '| C(Sex) | 0.76555 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| C(Sex) | 0.76555 |'
- en: '| C(Pclass) + C(Sex) + Age + SibSp + Parch | 0.74641 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) + Age + SibSp + Parch | 0.74641 |'
- en: '| C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked) | 0.75598 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked) | 0.75598 |'
- en: The code implementing logistic regression can be found in the `run_logistic_regression_titanic.py`
    file.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 实现逻辑回归的代码可以在 `run_logistic_regression_titanic.py` 文件中找到。
- en: Support vector machine
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: A **Support Vector Machine** (**SVM**) is a powerful supervised learning algorithm
    used for classification and regression. It is a discriminative classifier—it draws
    a boundary between clusters or classifications of data, so new points can be classified
    based on the cluster that they fall into.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）是一种强大的监督学习算法，用于分类和回归。它是一种判别分类器——它在数据的不同聚类或分类之间画出边界，因此新点可以根据它们所属的聚类来分类。'
- en: SVMs do not just find a boundary line; they also try to determine margins for
    the boundary on either side. The SVM algorithm tries to find the boundary with
    the largest possible margin around it.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 不仅仅是找到一条边界线；它还试图为边界的两侧确定边际。SVM 算法试图找到边界的最大可能边际。
- en: 'Support vectors are points that define the largest margin around the boundary—remove
    these points, and possibly, a larger margin can be found. Hence the name, support,
    as they support the margin around the boundary line. The support vectors matter.
    This is illustrated in the following diagram:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量是定义边界周围最大边际的点——如果移除这些点，可能会找到一个更大的边际。因此，称其为“支持”，因为它们支撑着边界线周围的边际。支持向量非常重要。如下图所示：
- en: '![](img/303f105c-1f90-43c1-8d09-accdbeec3d20.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/303f105c-1f90-43c1-8d09-accdbeec3d20.png)'
- en: For more information on this, refer to [http://winfwiki.wi-fom.de/images/c/cf/Support_vector_2.png](http://winfwiki.wi-fom.de/images/c/cf/Support_vector_2.png).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此更多信息，请参见 [http://winfwiki.wi-fom.de/images/c/cf/Support_vector_2.png](http://winfwiki.wi-fom.de/images/c/cf/Support_vector_2.png)。
- en: 'To use the SVM algorithm for classification, we specify one of the following
    three kernels: linear, poly, and **rbf** (also known as **radial basis functions**).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 SVM 算法进行分类，我们需要指定以下三种核函数之一：线性（linear）、多项式（poly）和 **rbf**（也称为 **径向基函数**）。
- en: 'Then, we import the **Support Vector Classifier** (**SVC**):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们导入 **支持向量分类器**（**SVC**）：
- en: '[PRE30]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We then instantiate an SVM classifier, fit the model, and predict the following:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们实例化一个 SVM 分类器，拟合模型，并预测以下内容：
- en: '[PRE31]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Upon submitting our data to Kaggle, the following results were obtained:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 提交数据到 Kaggle 后，得到以下结果：
- en: '| **Formula** | **Kernel Type** | **Kaggle Score** |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| **公式** | **核函数类型** | **Kaggle 分数** |'
- en: '| C(Pclass) + C(Sex) + Fare | poly | 0.71292 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) + Fare | poly | 0.71292 |'
- en: '| C(Pclass) + C(Sex) | poly | 0.76555 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) | poly | 0.76555 |'
- en: '| C(Sex) | poly | 0.76555 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| C(Sex) | poly | 0.76555 |'
- en: '| C(Pclass) + C(Sex) + Age + SibSp + Parch | poly | 0.75598 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) + Age + SibSp + Parch | poly | 0.75598 |'
- en: '| C(Pclass) + C(Sex) + Age + Parch + C(Embarked) | poly | 0.77512 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) + Age + Parch + C(Embarked) | poly | 0.77512 |'
- en: '| C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(embarked) | poly | 0.79426 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(embarked) | poly | 0.79426 |'
- en: '| C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked) | rbf | 0.7512 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked) | rbf | 0.7512 |'
- en: 'The code can be seen in its entirety in the following file: `run_svm_titanic.py`.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的完整内容可以在以下文件中查看：`run_svm_titanic.py`。
- en: Here, we see that the SVM with a kernel type of poly (polynomial) and the combination
    of the `Pclass`, `Sex`, `Age`, `Sibsp`, and `Parch` features produces the best
    results when submitted to Kaggle. Surprisingly, it seems as if the embarkation
    point (**Embarked**) and whether the passenger traveled alone or with family members
    (**Sibsp + Parch**) do have a material effect on a passenger's chances of survival.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到使用多项式（poly）核类型的SVM，以及`Pclass`、`Sex`、`Age`、`Sibsp`和`Parch`特征的组合，在提交到Kaggle时产生了最佳结果。令人惊讶的是，**登船地点**（**Embarked**）以及乘客是否单独旅行或与家人同行（**Sibsp
    + Parch**）似乎对乘客的生存机会有显著影响。
- en: The latter effect was probably due to the women-and-children-first policy on
    the Titanic.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 后者的影响可能是由于泰坦尼克号的“妇女与儿童优先”政策。
- en: Decision trees
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: The basic idea behind decision trees is to use the training dataset to create
    a tree of decisions to make a prediction.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的基本思想是利用训练数据集创建一个决策树来进行预测。
- en: It recursively splits the training dataset into subsets based on the value of
    a single feature. Each split corresponds to a node in the decision tree. The splitting
    process is continued until every subset is pure; that is, all elements belong
    to a single class. This always works except in cases where there are duplicate
    training examples that fall into different classes. In this case, the majority
    class wins.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 它基于单一特征的值递归地将训练数据集划分为多个子集。每次划分对应决策树中的一个节点。划分过程会一直持续，直到每个子集都纯净；即，所有元素都属于同一类。除非存在属于不同类的重复训练样本，否则该方法总是有效。在这种情况下，最大类将胜出。
- en: The end result is a ruleset for making predictions on the test dataset.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是一个用于对测试数据集进行预测的规则集。
- en: Decision trees encode a sequence of binary choices in a process that mimics
    how a human might classify things, but decide which question is most useful at
    each step by using the information criteria.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通过模拟人类如何分类事物的过程来编码一系列二元选择，但通过使用信息准则来决定每一步中哪个问题最有用。
- en: 'An example of this would be if you wished to determine whether animal `x` is
    a mammal, fish, or reptile; in this case, we would ask the following questions:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是，如果你想确定动物`x`是哺乳动物、鱼类还是爬行动物；在这种情况下，我们会提出以下问题：
- en: '[PRE32]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This generates a decision tree that looks similar to the following:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成一个类似于以下内容的决策树：
- en: '![](img/5e758343-82d7-41e7-89ab-fb37b10cd099.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e758343-82d7-41e7-89ab-fb37b10cd099.png)'
- en: Refer to the following link for more information: [https://labs.opendns.com/wp-content/uploads/2013/09/animals.gif](https://labs.opendns.com/wp-content/uploads/2013/09/animals.gif).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参见以下链接：[https://labs.opendns.com/wp-content/uploads/2013/09/animals.gif](https://labs.opendns.com/wp-content/uploads/2013/09/animals.gif)。
- en: The binary splitting of questions at each node is the essence of a decision
    tree algorithm. A major drawback of decision trees is that they can *overfit*
    the data. They are so flexible that, given a large depth, they can memorize the
    inputs, and this results in poor results when they are used to classify unseen
    data.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点处的二元问题划分是决策树算法的精髓。决策树的一个主要缺点是它们可能会*过拟合*数据。它们的灵活性非常高，当树的深度很大时，它们能够记住输入，从而导致在对未见过的数据进行分类时效果不佳。
- en: The way to fix this is to use multiple decision trees, and this is known as
    using an ensemble estimator. An example of an ensemble estimator is the random
    forest algorithm, which we will address next.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是使用多个决策树，这称为使用集成估计器。集成估计器的一个例子是随机森林算法，接下来我们将讨论这个算法。
- en: 'To use a decision tree in `scikit-learn`, we import the `tree` module:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 要在`scikit-learn`中使用决策树，我们需要导入`tree`模块：
- en: '[PRE33]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We then instantiate an SVM classifier, fit the model, and predict the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们实例化一个SVM分类器，拟合模型，并进行以下预测：
- en: '[PRE34]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Upon submitting our data to Kaggle, the following results are obtained:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 提交我们的数据到Kaggle后，得到以下结果：
- en: '| **Formula** | **Kaggle Score** |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| **公式** | **Kaggle得分** |'
- en: '| C(Pclass) + C(Sex) + Fare | 0.77033 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) + Fare | 0.77033 |'
- en: '| C(Pclass) + C(Sex) | 0.76555 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) | 0.76555 |'
- en: '| C(Sex) | 0.76555 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| C(Sex) | 0.76555 |'
- en: '| C(Pclass) + C(Sex) + Age + SibSp + Parch | 0.76555 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) + Age + SibSp + Parch | 0.76555 |'
- en: '| C(Pclass) + C(Sex) + Age + Parch +C(Embarked) | 0.78947 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) + Age + Parch + C(Embarked) | 0.78947 |'
- en: '| C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked) | 0.79426 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked) | 0.79426 |'
- en: Random forest
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: Random forest is an example of a non-parametric model, as are decision trees.
    Random forests are based on decision trees. The decision boundary is learned from
    the data itself. It doesn't have to be a line or a polynomial or radial basis
    function. The random forest model builds upon the decision tree concept by producing
    a large number of, or a forest of, decision trees. It takes a random sample of
    the data and identifies a set of features to grow each decision tree. The error
    rate of the model is compared across sets of decision trees to find the set of
    features that produce the strongest classification model.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一个非参数模型的例子，决策树也是如此。随机森林基于决策树，决策边界是从数据本身学习的。它不必是直线、多项式或径向基函数。随机森林模型在决策树的基础上发展，通过生成大量决策树，或一片决策树森林。它从数据中随机抽取样本，并识别一组特征来生长每棵决策树。模型的错误率在多个决策树集合中进行比较，以找到产生最强分类模型的特征集合。
- en: 'To use a random forest in `scikit-learn`, we import the `RandomForestClassifier`
    module:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 要在`scikit-learn`中使用随机森林，我们需要导入`RandomForestClassifier`模块：
- en: '[PRE35]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We then instantiate a random forest classifier, fit the model, and predict
    the following:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们实例化一个随机森林分类器，拟合模型并进行预测：
- en: '[PRE36]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Upon submitting our data to Kaggle (using the formula: *C(Pclass) + C(Sex)
    + Age + Sibsp + Parch + C(Embarked)*), the following results are obtained:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 提交数据到Kaggle（使用公式：*C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked)*）后，得到以下结果：
- en: '| **Formula** | **Kaggle Score** |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| **公式** | **Kaggle 分数** |'
- en: '| 10 | 0.74163 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.74163 |'
- en: '| 100 | 0.76077 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 0.76077 |'
- en: '| 1000 | 0.76077 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 0.76077 |'
- en: '| 10000 | 0.77990 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 10000 | 0.77990 |'
- en: '| 100000 | 0.77990 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 100000 | 0.77990 |'
- en: Unsupervised learning algorithms
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习算法
- en: 'There are two tasks that we are mostly concerned with in unsupervised learning:
    dimensionality reduction and clustering.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，我们主要关注的两个任务是：降维和聚类。
- en: Dimensionality reduction
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: Dimensionality reduction is used to help to visualize higher-dimensional data
    systematically. This is useful because the human brain can visualize only three
    spatial dimensions (and possibly, a temporal one), but most datasets involve much
    higher dimensions.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 降维用于帮助系统地可视化高维数据。这非常有用，因为人类大脑只能可视化三维空间（可能还包括时间维度），但大多数数据集涉及更高的维度。
- en: The typical technique used in dimensionality reduction is **Principal Component
    Analysis** (**PCA**). PCA involves using linear algebra techniques to project
    higher-dimensional data onto a lower-dimensional space. This inevitably involves
    the loss of information, but often, by projecting along the correct set and number
    of dimensions, the information loss can be minimized. A common dimensionality
    reduction technique is to find the combination of variables that explain the most
    variance (proxy for information) in our data and project along those dimensions.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 降维中常用的技术是**主成分分析**（**PCA**）。PCA通过使用线性代数技术将高维数据投影到低维空间。这不可避免地会导致信息的丧失，但通常，通过沿着正确的维度和数量投影，可以最小化信息损失。一种常见的降维技术是找到解释数据中最大方差（信息的代理）的变量组合，并沿这些维度进行投影。
- en: In the case of unsupervised learning problems, we do not have the set of labels
    (`Y`), and so, we only call `fit()` on the input data, `X`, itself, and for PCA,
    we call `transform()` instead of `predict()` as we're trying to transform the
    data into a new representation.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习问题中，我们没有标签集（`Y`），因此我们只对输入数据`X`本身调用`fit()`，对于 PCA，我们调用`transform()`而不是`predict()`，因为我们正在尝试将数据转换为新的表示形式。
- en: One of the datasets that we will be using to demonstrate USL is the iris dataset,
    possibly the most famous dataset in all of machine learning.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于演示无监督学习的一个数据集是鸢尾花数据集，可能是所有机器学习中最著名的数据集。
- en: The `scikit-learn` library provides a set of pre-packaged datasets, which are
    available via the `sklearn.datasets` modules. The iris dataset is one of them.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`库提供了一组预打包的数据集，可以通过`sklearn.datasets`模块访问。其中，鸢尾花数据集就是其中之一。'
- en: 'The iris dataset consists of 150 samples of data from three different species
    of iris flowers—versicolor, setosa, and virginica—with 50 samples of each type.
    The dataset consists of four features/dimensions:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集由150个样本组成，来自三种不同的鸢尾花——变色鸢尾、剑藜鸢尾和维吉尼亚鸢尾——每种类型有50个样本。该数据集包含四个特征/维度：
- en: Petal length
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣长度
- en: Petal width
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度
- en: Sepal length
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花萼长度
- en: Sepal width
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花萼宽度
- en: 'The length and width values are in centimeters. It can be loaded as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 长度和宽度的值以厘米为单位。它可以按如下方式加载：
- en: '[PRE37]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In our examination of unsupervised learning, we will be focusing on how to visualize
    and cluster this data.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对无监督学习的探讨中，我们将专注于如何可视化和聚类这些数据。
- en: 'Before discussing unsupervised learning, let''s examine the iris data a bit.
    The `load_iris()` command returns what is known as a bunch object, which is essentially
    a dictionary with keys in addition to the key containing the data. Hence, we have
    the following:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论无监督学习之前，让我们先来看看鸢尾花数据。`load_iris()`命令返回一个所谓的“bunch”对象，实际上它是一个字典，除了包含数据的键外，还包括其他键。因此，我们得到如下内容：
- en: '[PRE38]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Further, the data itself looks similar to the following:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据本身看起来类似于以下内容：
- en: '[PRE39]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This corresponds to 150 samples of four features. These four features are shown
    as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这对应于150个样本的四个特征。以下是这四个特征：
- en: '[PRE40]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can also take a peek at the actual data:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看实际数据：
- en: '[PRE41]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Our target names (what we''re trying to predict) look similar to the following:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标名称（我们试图预测的内容）看起来如下所示：
- en: '[PRE42]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'As noted earlier, the iris feature set corresponds to five-dimensional data
    and we cannot visualize this on a color plot. One thing that we can do is pick
    two features and plot them against each other while using color to differentiate
    between the species features. We do this next for all possible combinations of
    features, selecting two at a time for a set of six different possibilities. These
    combinations are as follows:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，鸢尾花特征集对应的是五维数据，我们无法在彩色图上可视化它。我们可以做的一件事是选择两个特征，并将它们相互绘制，同时使用颜色来区分不同物种的特征。接下来，我们将为所有可能的特征组合进行此操作，每次选择两个特征，共有六种不同的组合。它们如下所示：
- en: Sepal width versus sepal length
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片宽度与萼片长度
- en: Sepal width versus petal width
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片宽度与花瓣宽度
- en: Sepal width versus petal length
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片宽度与花瓣长度
- en: Sepal length versus petal width
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片长度与花瓣宽度
- en: Sepal length versus petal length
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片长度与花瓣长度
- en: Petal width versus petal length
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度与花瓣长度
- en: '![](img/df7ca771-5f28-4cb2-91c3-c4558134174a.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df7ca771-5f28-4cb2-91c3-c4558134174a.png)'
- en: 'The code for this can be found in the following file: `display_iris_dimensions.py`.
    From the preceding plots, we can observe that the setosa points tend to be clustered
    by themselves, while there is a bit of overlap between the virginica and the versicolor
    points. This may lead us to conclude that the latter two species are more closely
    related to one another than to the setosa species.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码可以在以下文件中找到：`display_iris_dimensions.py`。从前面的图表中，我们可以观察到setosa物种的点倾向于聚集在一起，而virginica和versicolor物种的点之间有一些重叠。这可能使我们得出结论，后两者的物种关系比与setosa物种的关系更紧密。
- en: These are, however, two-dimensional slices of data. What if we wanted a somewhat
    more holistic view of the data, with some representation of all four sepal and
    petal dimensions? What if there were some hitherto undiscovered connection between
    the dimensions that our two-dimensional plot wasn't showing? Is there a means
    of visualizing this? Enter dimensionality reduction. We will use dimensionality
    reduction to extract two combinations of sepal and petal dimensions to help to
    visualize it.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这只是数据的二维切片。如果我们想要对数据有一个更全面的视图，展示所有四个萼片和花瓣的维度呢？如果在这四个维度之间存在某些我们二维图表未能展示的尚未发现的联系呢？有没有办法可视化这个呢？引入降维技术。我们将使用降维技术提取出萼片和花瓣维度的两个组合来帮助我们进行可视化。
- en: 'We can apply dimensionality reduction to do this as follows:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式应用降维技术来实现这一点：
- en: '[PRE43]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Hence, we see that the reduced dataset is now in two dimensions. Let''s display
    the data visually in two dimensions as follows:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们看到降维后的数据现在是二维的。我们将如下所示地在二维空间中可视化这些数据：
- en: '[PRE44]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](img/2b93c4f6-41e2-4b88-b741-428741ef005d.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b93c4f6-41e2-4b88-b741-428741ef005d.png)'
- en: 'We can examine the makeup of the two PCA-reduced dimensions as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式检查这两个主成分分析（PCA）降维后的数据维度：
- en: '[PRE45]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Hence, we can see that the two reduced dimensions are a linear combination of
    all four sepal and petal dimensions.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到这两个降维后的维度是所有四个萼片和花瓣维度的线性组合。
- en: The source of this information is at [https://github.com/jakevdp/sklearn_pycon2014](https://github.com/jakevdp/sklearn_pycon2014).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 该信息的来源请访问[https://github.com/jakevdp/sklearn_pycon2014](https://github.com/jakevdp/sklearn_pycon2014)。
- en: K-means clustering
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K均值聚类
- en: The idea behind clustering is to group together similar points in a dataset
    based on a given criterion, hence finding clusters in the data.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的思想是根据给定的标准将相似的点聚集在一起，从而在数据中找到聚类。
- en: The K-means algorithm aims to partition a set of data points into *K* clusters
    such that each data point belongs to the cluster with the nearest mean point or
    centroid.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 算法旨在将一组数据点划分为 *K* 个簇，使得每个数据点都属于与其最近的均值点或质心的簇。
- en: 'To illustrate K-means clustering, we can apply it to the set of reduced iris
    data that we obtained via PCA, but in this case, we do not pass the actual labels
    to the `fit(..)` method as we do for supervised learning:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 K-means 聚类，我们可以将其应用于通过 PCA 获得的降维后的鸢尾花数据集，但在这种情况下，我们不会像监督学习那样将实际标签传递给 `fit(..)`
    方法：
- en: '[PRE46]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We now display the clustered data as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们显示聚类后的数据如下：
- en: '[PRE47]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![](img/356cef4d-149b-4986-9f9d-adf0907aa1d9.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![](img/356cef4d-149b-4986-9f9d-adf0907aa1d9.png)'
- en: Note that our K-means algorithm clusters do not exactly correspond to the dimensions
    obtained via PCA. The source code is available at [https://github.com/jakevdp/sklearn_pycon2014](https://github.com/jakevdp/sklearn_pycon2014).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的 K-means 算法聚类与通过 PCA 获得的维度并不完全对应。源代码可在[https://github.com/jakevdp/sklearn_pycon2014](https://github.com/jakevdp/sklearn_pycon2014)中找到。
- en: More information on K-means clustering in `scikit-learn` and, in general can
    be found at [http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html](http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html)
    and [http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering).
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 K-means 聚类的更多信息，您可以在 `scikit-learn` 官方文档和以下链接中找到：[http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html](http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html)
    和 [http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering)。
- en: XGBoost case study
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 案例研究
- en: '**XGBoost** is an ensemble algorithm popular for its outstanding performance.
    An ensemble algorithm involves multiple models instead of just one. Ensemble algorithms
    are of two types:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '**XGBoost** 是一种因其出色性能而广受欢迎的集成算法。集成算法涉及多个模型，而不仅仅是一个。集成算法分为两种类型：'
- en: '**Bagging**: Here, a result from the algorithm is the average of results from
    individual models.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bagging**：在这里，算法的结果是来自各个模型结果的平均值。'
- en: '**Boosting**: Here, we start from a base learner model. Each successive model
    gets created with better-trained parameters. The learning of new parameters happens
    through optimization algorithms such as gradient descent.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Boosting**：在这里，我们从一个基础学习模型开始。每个后续模型都基于更好训练的参数创建。新参数的学习通过优化算法（如梯度下降）进行。'
- en: Next, we will look at the application of XGBoost on a dataset to predict the
    testing time of a newly manufactured car.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过一个数据集来应用 XGBoost，以预测新制造的汽车的测试时间。
- en: 'It is a step-by-step guide and you can just follow along:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个逐步指南，你可以按照它进行操作：
- en: 'Import the required packages:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: '[PRE48]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Change the working directory:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改工作目录：
- en: '[PRE49]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Read the train and test data:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取训练和测试数据：
- en: '[PRE50]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Prepare for removing columns with zero variance:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备删除具有零方差的列：
- en: '[PRE51]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Input the function for removing columns with zero variance:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入删除零方差列的函数：
- en: '[PRE52]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Get a list of zero variance columns in the train and test datasets:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取训练和测试数据集中零方差列的列表：
- en: '[PRE53]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Remove zero variance columns in the train data from test data:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从测试数据中移除训练数据中的零方差列：
- en: '[PRE54]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Find `Unique, Total Count and NAs` and write it to a CSV file:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找 `Unique, Total Count 和 NAs` 并写入 CSV 文件：
- en: '[PRE55]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Find a list of categorical variables:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找分类变量的列表：
- en: '[PRE56]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Create dummy variables from the categorical variables:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从分类变量中创建虚拟变量：
- en: '[PRE57]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Delete the categorical variables from `train` and `test`:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除训练和测试数据中的分类变量：
- en: '[PRE58]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Plot a scree plot to get the number of components that will explain the 90%
    variance in the data:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制碎石图，以确定能够解释数据中 90% 方差的主成分数量：
- en: '[PRE59]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Perform PCA on the train and test data:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练和测试数据执行 PCA：
- en: '[PRE60]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Separate the `x` and `y` variables to be passed to `xgboost`:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `x` 和 `y` 变量从训练数据中分开，准备传递给 `xgboost`：
- en: '[PRE61]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Define the `xgboost` model:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `xgboost` 模型：
- en: '[PRE62]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Predict from the `xgboost` model:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `xgboost` 模型进行预测：
- en: '[PRE63]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Calculate the root mean square error:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算均方根误差：
- en: '[PRE64]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Entropy
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵
- en: Entropy is a measure of the homogeneity (or heterogeneity) of data. The more
    homogeneous the data, the more entropy it has. Please keep in mind that, to make
    a better classification decision, heterogeneous data is better.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 熵是数据同质性（或异质性）的度量。数据越同质，它的熵就越大。请记住，为了做出更好的分类决策，异质性数据更为重要。
- en: For example, consider a dataset where 1,000 people were surveyed about whether
    they smoke or not. In the first case, let's say that 500 people said yes and 500
    said no. In the second case, let's assume that 800 people said yes and 200 said
    no. In which case would the entropy be more?
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个数据集，其中对1,000人进行了关于是否吸烟的调查。在第一个情况下，假设500人回答“是”，500人回答“否”。在第二个情况下，假设800人回答“是”，200人回答“否”。在哪种情况下，熵会更大？
- en: Yes, you guessed right. It is the first one because it is more homogeneous or,
    in other words, the decisions are equally distributed. If a person had to guess
    whether a survey participant answered yes or no, without knowing the actual answer,
    then the chances of them getting the right answer are less in the first case.
    Hence, we say that the data is messier in terms of classification information
    and hence has more entropy.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，你猜对了。是第一个情况，因为它更均匀，换句话说，决策是均匀分布的。如果一个人必须猜测调查参与者是否回答“是”或“否”，而不知道实际答案，那么在第一个情况下，他们猜对的几率更低。因此，我们说该数据在分类信息方面更混乱，因此具有更高的熵。
- en: The goal of any classification problem, especially decision tree (and hence
    random forest and XGBoost), is to decrease this entropy and gain information.
    Next, let's see how we can quantify this seemingly qualitative term.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 任何分类问题的目标，特别是决策树（因此也包括随机森林和XGBoost），都是减少这个熵并获取信息。接下来，让我们看看如何量化这个看似定性的术语。
- en: 'Equation for calculating entropy for the overall dataset is mathematically
    defined as follows:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 计算整个数据集熵的公式在数学上定义如下：
- en: '![](img/cca91546-a2b0-4241-a59a-3c5f7af90e04.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cca91546-a2b0-4241-a59a-3c5f7af90e04.png)'
- en: Here, *p[i]* is the proportion of the dataset with the *i*^(th) class.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*p[i]* 是数据集中具有*i*^(th)类别的比例。
- en: For example, in the first case we suggested earlier, *p[yes]* would be 500/1,000
    and *p[no]* would be 500/1,000.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们之前提到的第一个案例中，*p[yes]* 将是 500/1,000，*p[no]* 将是 500/1,000。
- en: 'The following diagram shows the variation of entropy (the *y* variable) as
    the *p[i]* (the *x* variable) changes from 0 to 1\. Please notice that the *p[i]s*
    have been multiplied by 100 for plotting purposes:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了熵（*y*变量）如何随着*p[i]*（*x*变量）从0变化到1而变化。请注意，*p[i]* 已乘以100以便于绘图：
- en: '![](img/a39a2df8-a350-4bf7-8c0b-19503b03a95b.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a39a2df8-a350-4bf7-8c0b-19503b03a95b.png)'
- en: Plot of entropy versus fraction / proportion (0 to 1)
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 熵与比例/分数（0到1）的关系图
- en: 'Observe the following about the graph:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 观察以下图表：
- en: It's almost symmetric, about *p=0.5*.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它几乎是对称的，关于*p=0.5*。
- en: It maximizes at *p=0.5*, which makes sense as the messiness is maximal when
    the two classes are equally distributed.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在*p=0.5*时最大，这也是合理的，因为当两个类别均匀分布时，混乱度最大。
- en: 'The code used to generate this plot is as follows:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成此图的代码如下：
- en: '[PRE65]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Next, let''s see how we can write a function using pandas to calculate the
    entropy of a dataset on its own and of a column from a dataset. For this purpose,
    we can first create dummy data with two columns: `purchase` (the *y* variable)
    and `se_status` (the predictor variables).'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用pandas编写一个函数，来计算数据集自身的熵以及数据集中的一列的熵。为此，我们可以首先创建一个包含两列的虚拟数据：`purchase`（*y*变量）和`se_status`（预测变量）。
- en: 'Define the unique values of the categorical variables:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 定义类别变量的唯一值：
- en: '[PRE66]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Next, we write a function to calculate the initial entropy of a dataset given
    the dataset and the name of the *y* variable:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写一个函数来计算给定数据集和*y*变量名称的数据集的初始熵：
- en: '[PRE67]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Once we have the initial entropy, the next goal is to find the entropy assuming
    that one of the predictor variables was used for classification. To calculate
    entropy for such a case, we follow these steps:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了初始熵，下一步的目标是找到假设使用某个预测变量进行分类的情况下的熵。计算这种情况下的熵，我们需要遵循以下步骤：
- en: Subset the data based on categories in the particular predictor column—one dataset
    for one category.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于特定预测列中的类别对子集数据进行划分——每个类别对应一个数据集。
- en: Calculate the entropy for each of these datasets so that you have one entropy
    value for each category of the variable.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个数据集的熵，这样你就可以为变量的每个类别得到一个熵值。
- en: Take a weighted average of these entropies. Weights are given by the proportion
    of that category in that dataset.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对这些熵值进行加权平均。权重由该类别在数据集中的比例给出。
- en: 'Mathematically, it can be represented by the following:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，它可以表示为以下公式：
- en: '![](img/7b052203-f238-4dd5-a011-69f464bbc0b5.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b052203-f238-4dd5-a011-69f464bbc0b5.png)'
- en: Equation for calculating entropy for a column
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 计算列熵的公式
- en: 'Here, *f[j]*represents the proportion of the *i^([th])* category in the dataset,
    and *p[ij]* represents the proportion of the *j[th]* category of the *y* variable
    in the dataset for the *i*^(th) category of the predictor column. Let''s see how
    a function can be written to calculate this entropy for a given dataset, the *y*
    variable, and a predictor variable:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*f[j]* 代表数据集中第 *i*^(th)* 类别的比例，*p[ij]* 代表数据集中第 *i*^(th)* 类别的预测变量列中 *y* 变量的第
    *j*^(th)* 类别的比例。接下来，我们看看如何编写一个函数来计算给定数据集、*y* 变量和预测变量的熵：
- en: '[PRE68]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Information gain is defined as the reduction in entropy when we move from making
    a classification decision based on only the `y` variable distribution to making
    this decision based on a column. This can be calculated as follows:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益被定义为当我们从仅基于 `y` 变量分布做分类决策到基于某一列做决策时，熵的减少。它可以按如下方式计算：
- en: '[PRE69]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: For the dataset that I have in this instance, I got an information gain of around
    0.08\. While making a decision tree, this information gain is calculated for every
    column. The column with the highest information gain is selected as the next branching
    node in the tree.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我这个例子中的数据集，我得到了大约 0.08 的信息增益。在构建决策树时，会对每一列计算信息增益，信息增益最大的列将被选作树中下一个分支节点。
- en: Summary
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we embarked on a whirlwind tour of machine learning, examining
    the role of pandas in feature extraction, selection, and engineering as well as
    learning about key concepts in machine learning such as supervised versus unsupervised
    learning. We also had a brief introduction to a few key algorithms in both methods
    of machine learning, and we used the `scikit-learn` package to utilize these algorithms
    to learn and make predictions on data. This chapter was not intended to be a comprehensive
    treatment of machine learning, but rather to illustrate how pandas can be used
    to assist users in the machine learning space.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们快速浏览了机器学习的基本内容，探讨了 pandas 在特征提取、选择和工程中的作用，同时了解了机器学习中的一些关键概念，如有监督学习与无监督学习。我们还简单介绍了两种机器学习方法中的一些关键算法，并使用
    `scikit-learn` 包来利用这些算法学习并对数据做出预测。本章并不打算全面讲解机器学习，而是旨在展示 pandas 如何在机器学习领域中为用户提供帮助。
