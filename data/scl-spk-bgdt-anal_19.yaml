- en: PySpark and SparkR
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark和SparkR
- en: 'In this chapter, we will discuss two other popular APIs: PySpark and SparkR
    for writing Spark code in Python and R programming languages respectively. The
    first part of this chapter will cover some technical aspects while working with
    Spark using PySpark. Then we will move to SparkR and see how to use it with ease.
    The following topics will be discussed throughout this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论另外两种流行的API：PySpark和SparkR，分别用于在Python和R编程语言中编写Spark代码。本章的第一部分将涉及使用PySpark时的一些技术细节。接着我们将讨论SparkR，看看如何轻松地使用它。本章将讨论以下主题：
- en: Introduction to PySpark
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark简介
- en: Installation and getting started with PySpark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装并开始使用PySpark
- en: Interacting with DataFrame APIs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与DataFrame API的交互
- en: UDFs with PySpark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PySpark的UDFs
- en: Data analytics using PySpark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PySpark进行数据分析
- en: Introduction to SparkR
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkR简介
- en: Why SparkR?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择SparkR？
- en: Installation and getting started with SparkR
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装并开始使用SparkR
- en: Data processing and manipulation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据处理与操作
- en: Working with RDD and DataFrame using SparkR
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SparkR处理RDD和DataFrame
- en: Data visualization using SparkR
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SparkR进行数据可视化
- en: Introduction to PySpark
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark简介
- en: Python is one of the most popular and general purpose programming languages
    with a number of exciting features for data processing and machine learning tasks.
    To use Spark from Python, PySpark was initially developed as a lightweight frontend
    of Python to Apache Spark and using Spark's distributed computation engine. In
    this chapter, we will discuss a few technical aspects of using Spark from Python
    IDE such as PyCharm.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Python是最受欢迎的通用编程语言之一，具有多种用于数据处理和机器学习任务的激动人心的特性。为了在Python中使用Spark，最初开发了PySpark作为Python到Apache
    Spark的轻量级前端，并利用Spark的分布式计算引擎。在本章中，我们将讨论在Python IDE中（如PyCharm）使用Spark的一些技术细节。
- en: Many data scientists use Python because it has a rich variety of numerical libraries
    with a statistical, machine learning, or optimization focus. However, processing
    large-scale datasets in Python is usually tedious as the runtime is single-threaded.
    As a result, data that fits in the main memory can only be processed. Considering
    this limitation and for getting the full flavor of Spark in Python, PySpark was
    initially developed as a lightweight frontend of Python to Apache Spark and using
    Spark's distributed computation engine. This way, Spark provides APIs in non-JVM
    languages like Python.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据科学家使用Python，因为它拥有丰富的数值库，专注于统计、机器学习或优化。然而，在Python中处理大规模数据集通常很繁琐，因为其运行时是单线程的。因此，只能处理适合主内存的数据。考虑到这一限制，为了在Python中充分体验Spark，PySpark最初作为Python到Apache
    Spark的轻量级前端开发，并使用Spark的分布式计算引擎。这样，Spark就能提供在非JVM语言（如Python）中的API。
- en: The purpose of this PySpark section is to provide basic distributed algorithms
    using PySpark. Note that PySpark is an interactive shell for basic testing and
    debugging and is not supposed to be used for a production environment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本节PySpark的目的是提供使用PySpark的基本分布式算法。请注意，PySpark是一个用于基本测试和调试的交互式命令行工具，不适用于生产环境。
- en: Installation and configuration
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装与配置
- en: There are many ways of installing and configuring PySpark on Python IDEs such
    as PyCharm, Spider, and so on. Alternatively, you can use PySpark if you have
    already installed Spark and configured the `SPARK_HOME`. Thirdly, you can also
    use PySpark from the Python shell. Below we will see how to configure PySpark
    for running standalone jobs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python IDE（如PyCharm、Spider等）中安装和配置PySpark有多种方式。或者，如果你已经安装了Spark并配置了`SPARK_HOME`，也可以使用PySpark。第三，你还可以在Python
    shell中使用PySpark。接下来我们将看到如何为运行独立作业配置PySpark。
- en: By setting SPARK_HOME
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过设置SPARK_HOME
- en: 'At first, download and place the Spark distribution at your preferred place,
    say `/home/asif/Spark`. Now let''s set the `SPARK_HOME` as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，下载并将Spark发行版放置在你偏好的位置，比如`/home/asif/Spark`。现在，让我们按如下方式设置`SPARK_HOME`：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now let''s set `PYTHONPATH` as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们按如下方式设置`PYTHONPATH`：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we need to add the following two paths to the environmental path:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将以下两个路径添加到环境变量路径中：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, let''s refresh the current terminal so that the newly modified `PATH`
    variable is used:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们刷新当前的终端，以便使用新修改的`PATH`变量：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'PySpark depends on the `py4j` Python package. It helps the Python interpreter
    to dynamically access the Spark object from the JVM. This package can be installed
    on Ubuntu as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark依赖于`py4j` Python包。它帮助Python解释器动态访问来自JVM的Spark对象。可以通过以下方式在Ubuntu上安装该包：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Alternatively, the default `py4j`, which is already included in Spark (`$SPARK_HOME/python/lib`),
    can be used too.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，也可以使用默认的 `py4j`，它已经包含在 Spark 中（`$SPARK_HOME/python/lib`）。
- en: Using Python shell
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python Shell
- en: 'Like Scala interactive shell, an interactive shell is also available for Python.
    You can execute Python code from Spark root folder as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Scala 交互式 Shell 类似，Python 也提供了一个交互式 Shell。你可以从 Spark 根目录文件夹执行 Python 代码，方法如下：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If the command went fine, you should observer the following screen on Terminal
    (Ubuntu):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令执行成功，你应该在终端（Ubuntu）上看到以下屏幕：
- en: '![](img/00375.jpeg)**Figure 1**: Getting started with PySpark shell'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00375.jpeg)**图 1**：开始使用 PySpark Shell'
- en: Now you can enjoy Spark using the Python interactive shell. This shell might
    be sufficient for experimentations and developments. However, for production level,
    you should use a standalone application.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以通过 Python 交互式 Shell 使用 Spark。这种 Shell 对于实验和开发可能已经足够。但对于生产环境，你应该使用独立应用程序。
- en: 'PySpark should be available in the system path by now. After writing the Python
    code, one can simply run the code using the Python command, then it runs in local
    Spark instance with default configurations:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在，PySpark 应该已经在系统路径中。编写 Python 代码后，只需使用 Python 命令运行代码，它就会在本地 Spark 实例中使用默认配置运行：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that the current distribution of Spark is only Python 2.7+ compatible.
    Hence, we will have been strict on this.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，目前的 Spark 发行版仅兼容 Python 2.7+。因此，我们在这一点上将严格遵守。
- en: 'Furthermore, it is better to use the `spark-submit` script if you want to pass
    the configuration values at runtime. The command is pretty similar to the Scala
    one:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你想在运行时传递配置值，最好使用 `spark-submit` 脚本。该命令与 Scala 的类似：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The configuration values can be passed at runtime, or alternatively, they can
    be changed in the `conf/spark-defaults.conf` file. After configuring the Spark
    config file, the changes also get reflected while running PySpark applications
    using a simple Python command.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 配置值可以在运行时传递，或者可以更改 `conf/spark-defaults.conf` 文件中的配置。配置 Spark 配置文件后，修改内容将在使用简单
    Python 命令运行 PySpark 应用时反映出来。
- en: However, unfortunately, at the time of this writing, there's no pip install
    advantage for using PySpark. But it is expected to be available in the Spark 2.2.0
    release (for more, refer to [https://issues.apache.org/jira/browse/SPARK-1267](https://issues.apache.org/jira/browse/SPARK-1267)).
    The reason why there is no pip install for PySpark can be found in the JIRA ticket
    at [https://issues.apache.org/jira/browse/SPARK-1267](https://issues.apache.org/jira/browse/SPARK-1267).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不幸的是，在撰写本文时，使用 PySpark 并没有 pip 安装的优势。但预计在 Spark 2.2.0 版本中会提供此功能（更多信息，请参见
    [https://issues.apache.org/jira/browse/SPARK-1267](https://issues.apache.org/jira/browse/SPARK-1267)）。没有
    pip 安装 PySpark 的原因可以在 JIRA 工单 [https://issues.apache.org/jira/browse/SPARK-1267](https://issues.apache.org/jira/browse/SPARK-1267)
    中找到。
- en: By setting PySpark on Python IDEs
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过在 Python IDE 中设置 PySpark
- en: We can also configure and run PySpark from Python IDEs such as PyCharm. In this
    section, we will show how to do it. If you're a student, you can get the free
    licensed copy of PyCharm once you register using your university/college/institute
    email address at [https://www.jetbrains.com/student/](https://www.jetbrains.com/student/).
    Moreover, there's also a community (that is, free) edition of PyCharm, so you
    don't need to be a student in order to use it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过 Python IDE（如 PyCharm）来配置和运行 PySpark。在本节中，我们将展示如何操作。如果你是学生，可以在 [https://www.jetbrains.com/student/](https://www.jetbrains.com/student/)
    使用你的大学/学院/机构电子邮件地址注册后免费获得 PyCharm 的授权版。此外，还有 PyCharm 的社区版（即免费的版本），因此你不必是学生也可以使用它。
- en: 'Recently PySpark has been published with Spark 2.2.0 PyPI (see [https://pypi.python.org/pypi/pyspark](https://pypi.python.org/pypi/pyspark)/.
    This has been a long time coming (previous releases included pip installable artifacts
    that for a variety of reasons couldn''t be published to PyPI). So if you (or your
    friends) want to be able to work with PySpark locally on your laptop you''ve got
    an easier path getting started, just execute the following command:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，PySpark 已经通过 Spark 2.2.0 发布到了 PyPI（请参见 [https://pypi.python.org/pypi/pyspark](https://pypi.python.org/pypi/pyspark)）。这一变化是长期以来的期待（之前的版本发布了可以
    pip 安装的组件，但由于种种原因无法发布到 PyPI）。所以，如果你（或你的朋友）希望能够在本地笔记本电脑上使用 PySpark，现有的安装路径更加简便，只需执行以下命令：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'However, if you are using Windos 7, 8 or 10, you should install pyspark manually.
    For exmple using PyCharm, you can do it as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你使用的是 Windows 7、8 或 10，你需要手动安装 pyspark。例如，使用 PyCharm，你可以按照以下步骤操作：
- en: '![](img/00281.jpeg)**Figure 2:** Installing PySpark on Pycharm IDE on Windows
    10'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00281.jpeg)**图 2：** 在 Windows 10 上的 Pycharm IDE 安装 PySpark'
- en: 'At first, you should create a Python script with Project interpreter as Python
    2.7+. Then you can import pyspark along with other required models as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您应创建一个 Python 脚本，并将项目解释器设置为 Python 2.7+。然后，您可以按如下方式导入 pyspark 和其他所需的模块：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that if you''re a Windows user, Python also needs to have the Hadoop runtime;
    you should put the `winutils.exe` file in the `SPARK_HOME/bin` folder. Then create
    a environmental variable as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是 Windows 用户，Python 还需要安装 Hadoop 运行时；您应将 `winutils.exe` 文件放入 `SPARK_HOME/bin`
    文件夹中。然后，创建一个环境变量，方式如下：
- en: 'Select your python file | Run | Edit configuration | Create an environmental
    variable whose key is `HADOOP_HOME` and the value is the `PYTHON_PATH` for example
    for my case it''s `C:\Users\admin-karim\Downloads\spark-2.1.0-bin-hadoop2.7`.
    Finally, press OK then you''re done:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 选择您的 Python 文件 | 运行 | 编辑配置 | 创建一个环境变量，键为 `HADOOP_HOME`，值为 `PYTHON_PATH`，例如在我的情况下，它是
    `C:\Users\admin-karim\Downloads\spark-2.1.0-bin-hadoop2.7`。最后，按 OK 完成设置：
- en: '![](img/00110.jpeg)**Figure 3:** Setting Hadoop runtime env on Pycharm IDE
    on Windows 10'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00110.jpeg)**图 3：** 在 Windows 10 上的 Pycharm IDE 设置 Hadoop 运行时环境'
- en: 'That''s all you need. Now if you start writing Spark code, you should at first
    place the imports in the `try` block as follows (just for example):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是您需要的内容。现在，如果您开始编写 Spark 代码，首先应将导入语句放入 `try` 块中，如下所示（仅为示例）：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the `catch` block can be placed as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`catch` 块可以按如下方式放置：'
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Refer to the following figure that shows importing and placing Spark packages
    in the PySpark shell:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅以下图示，展示了在 PySpark shell 中导入并放置 Spark 包：
- en: '![](img/00256.jpeg)**Figure 4**: Importing and placing Spark packages in PySpark
    shell'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00256.jpeg)**图 4**：在 PySpark shell 中导入并放置 Spark 包'
- en: 'If these blocks execute successfully, you should observe the following message
    on the console:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些代码块成功执行，您应在控制台看到以下信息：
- en: '![](img/00005.jpeg)**Figure 5**: PySpark package has been imported successfully'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00005.jpeg)**图 5**：PySpark 包已成功导入'
- en: Getting started with PySpark
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 PySpark
- en: 'Before going deeper, at first, we need to see how to create the Spark session.
    It can be done as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入之前，首先，我们需要了解如何创建 Spark 会话。可以通过以下方式完成：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now under this code block, you should place your codes, for example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在这个代码块下，您应放置您的代码，例如：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code demonstrates how to compute principal components on a RowMatrix
    and use them to project the vectors into a low-dimensional space. For a clearer
    picture, refer to the following code that shows how to use the PCA algorithm on
    PySpark:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码演示了如何在 RowMatrix 上计算主成分，并使用它们将向量投影到低维空间。为了更清楚地理解，请参阅以下代码，展示了如何在 PySpark
    上使用 PCA 算法：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/00188.jpeg)**Figure 6**: PCA result after successful execution of the
    Python script'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00188.jpeg)**图 6**：Python 脚本成功执行后的 PCA 结果'
- en: Working with DataFrames and RDDs
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DataFrame 和 RDD
- en: 'SparkDataFrame is a distributed collection of rows under named columns. Less
    technically, it can be considered as a table in a relational database with column
    headers. Furthermore, PySpark DataFrame is similar to Python pandas. However,
    it also shares some mutual characteristics with RDD:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: SparkDataFrame 是一个分布式的按列命名的行集合。更简单地说，它可以看作是一个关系数据库中的表，具有列标题。此外，PySpark DataFrame
    类似于 Python 的 pandas。但它也与 RDD 共享一些共同特性：
- en: '**Immutable**: Just like an RDD, once a DataFrame is created, it can''t be
    changed. We can transform a DataFrame to an RDD and vice versa after applying
    transformations.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可变：** 就像 RDD 一样，一旦创建了 DataFrame，它就不能被更改。我们可以在应用转换后将 DataFrame 转换为 RDD，反之亦然。'
- en: '**Lazy Evaluations:** Its nature is a lazy evaluation. In other words, a task
    is not executed until an action is performed.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟评估：** 它的特点是延迟评估。换句话说，任务不会执行，直到执行某个操作时才会触发。'
- en: '**Distributed:** Both the RDD and DataFrame are distributed in nature.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式：** RDD 和 DataFrame 都具有分布式特性。'
- en: Just like Java/Scala's DataFrames, PySpark DataFrames are designed for processing
    a large collection of structured data; you can even handle petabytes of data.
    The tabular structure helps us understand the schema of a DataFrame, which also
    helps optimize execution plans on SQL queries. Additionally, it has a wide range
    of data formats and sources.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Java/Scala 的 DataFrame 一样，PySpark DataFrame 设计用于处理大量结构化数据；您甚至可以处理 PB 级别的数据。表格结构帮助我们理解
    DataFrame 的模式，这也有助于优化 SQL 查询的执行计划。此外，它支持广泛的数据格式和数据源。
- en: You can create RDDs, datasets, and DataFrames in a number of ways using PySpark.
    In the following subsections, we will show some examples of doing that.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过多种方式使用PySpark创建RDD、数据集和DataFrame。在接下来的子部分中，我们将展示一些示例。
- en: Reading a dataset in Libsvm format
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取LIBSVM格式的数据集
- en: 'Let''s see how to read data in LIBSVM format using the read API and the `load()`
    method by specifying the format of the data (that is, `libsvm`) as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用读取API和`load()`方法通过指定数据的格式（即`libsvm`）来读取LIBSVM格式的数据，方法如下：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding MNIST dataset can be downloaded from [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2).
    This will essentially return a DataFrame and the content can be seen by calling
    the `show()` method as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上述MNIST数据集可以从[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2)下载。这将返回一个DataFrame，并且可以通过调用`show()`方法查看内容，方法如下：
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/00068.gif)**Figure 7**: A snap of the handwritten dataset in LIBSVM
    format'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00068.gif)**图 7**：LIBSVM格式的手写数据集快照'
- en: 'You can also specify other options such as how many features of the raw dataset
    you want to give to your DataFrame as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以指定其他选项，例如想要为DataFrame提供多少原始数据集特征，方法如下：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now if you want to create an RDD from the same dataset, you can use the MLUtils
    API from `pyspark.mllib.util` as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您想从相同的数据集中创建一个RDD，可以使用`pyspark.mllib.util`中的MLUtils API，方法如下：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now you can save the RDD in your preferred location as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以按照如下方式将RDD保存在您喜欢的位置：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Reading a CSV file
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取CSV文件
- en: 'Let''s start with loading, parsing, and viewing simple flight data. At first,
    download the NYC flights dataset as a CSV from [https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv](https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv).
    Now let''s load and parse the dataset using `read.csv()` API of PySpark:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载、解析和查看简单的航班数据开始。首先，下载纽约市航班数据集的CSV文件，链接地址为：[https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv](https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv)。现在，让我们使用PySpark的`read.csv()`
    API来加载和解析数据集：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This is pretty similar to reading the libsvm format. Now you can see the resulting
    DataFrame''s structure as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这与读取libsvm格式非常相似。现在您可以看到生成的DataFrame的结构，如下所示：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/00338.gif)**Figure 8**: Schema of the NYC flight dataset'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00338.gif)**图 8**：纽约市航班数据集的架构'
- en: 'Now let''s see a snap of the dataset using the `show()` method as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`show()`方法查看数据集的快照，方法如下：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now let''s view the sample of the data as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看数据的样本，方法如下：
- en: '![](img/00370.gif)**Figure 9**: Sample of the NYC flight dataset'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00370.gif)**图 9**：纽约市航班数据集的样本'
- en: Reading and manipulating raw text files
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取并操作原始文本文件
- en: 'You can read a raw text data file using the `textFile()` method. Suppose you
    have the logs of some purchase:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`textFile()`方法读取原始文本数据文件。假设您有一些购买记录的日志：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now reading and creating RDD is pretty straightforward using the `textFile()`
    method as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用`textFile()`方法读取和创建RDD变得非常简单，方法如下：
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As you can see, the structure is not that readable. So we can think of giving
    a better structure by converting the texts as DataFrame. At first, we need to
    collect the header information as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，结构并不容易阅读。所以我们可以考虑通过将文本转换为DataFrame来提供更好的结构。首先，我们需要收集头信息，方法如下：
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now filter out the header and make sure the rest looks correct as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，过滤掉头信息，确保其余部分看起来正确，方法如下：
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We still have the RDD but with a bit better structure of the data. However,
    converting it into DataFrame will provide a better view of the transactional data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然有RDD，但是数据的结构略有改进。不过，将其转换为DataFrame将提供更好的事务数据视图。
- en: 'The following code creates a DataFrame by specifying the `header.split` is
    providing the names of the columns:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码通过指定`header.split`来创建一个DataFrame，这个操作提供了列的名称：
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/00208.gif)**Figure 10**: Sample of the transactional data'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00208.gif)**图 10**：事务数据样本'
- en: 'Now you could save this DataFrame as a view and make a SQL query. Let''s do
    a query with this DataFrame now:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以将这个DataFrame保存为视图，并进行SQL查询。现在让我们使用这个DataFrame做一个查询：
- en: '[PRE28]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/00051.gif)**Figure 11**: Query result on the transactional data using
    Spark SQL'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00051.gif)**图 11**：使用Spark SQL对事务数据的查询结果'
- en: Writing UDF on PySpark
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PySpark中编写UDF
- en: Like Scala and Java, you can also work with **User Defined Functions** (aka.
    **UDF**) on PySpark. Let's see an example in the following. Suppose we want to
    see the grade distribution based on the score for some students who have taken
    courses at a university.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 像Scala和Java一样，你也可以在PySpark中使用**用户定义函数**（即**UDF**）。让我们通过以下示例来看一下。假设我们想根据一些已经在大学上过课程的学生的分数来查看成绩分布。
- en: 'We can store them in two separate arrays as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将它们存储在两个独立的数组中，如下所示：
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now let''s declare an empty array for storing the data about courses and students
    so that later on both can be appended to this array as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们声明一个空数组来存储关于课程和学生的数据，以便稍后将它们添加到该数组中，如下所示：
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Note that for the preceding code to work, please import the following at the
    beginning of the file:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，要使前面的代码正常工作，请在文件开头导入以下内容：
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now let's create a DataFrame from these two objects toward converting corresponding
    grades against each one's score. For this, we need to define an explicit schema.
    Let's suppose that in your planned DataFrame, there would be three columns named
    `Student`, `Course`, and `Score`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从这两个对象创建一个DataFrame，用于根据每个分数转换相应的成绩。为此，我们需要定义一个显式的模式。假设在你计划的DataFrame中，会有三列，分别命名为`Student`、`Course`和`Score`。
- en: 'At first, let''s import necessary modules:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的模块：
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now the schema can be defined as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以按如下方式定义模式：
- en: '[PRE33]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now let''s create an RDD from the Raw Data as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从原始数据中创建一个RDD，如下所示：
- en: '[PRE34]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now let''s convert the RDD into the DataFrame as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将RDD转换为DataFrame，如下所示：
- en: '[PRE35]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![](img/00311.gif)**Figure 12**: Sample of the randomly generated score for
    students in subjects'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00311.gif)**图12**：学生在科目中的随机生成分数示例'
- en: 'Well, now we have three columns. However, we need to convert the score into
    grades. Say you have the following grading schema:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们有了三列。然而，我们需要将分数转换为等级。假设你有如下的评分标准：
- en: '*90~100=> A*'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*90~100 => A*'
- en: '*80~89 => B*'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*80~89 => B*'
- en: '*60~79 => C*'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*60~79 => C*'
- en: '*0~59 => D*'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*0~59 => D*'
- en: 'For this, we can create our own UDF such that this will convert the numeric
    score to grade. It can be done in several ways. Following is an example of doing
    so:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以创建自己的UDF， 将数字分数转换为成绩。这可以通过多种方式完成。以下是一个实现示例：
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now we can have our own UDF as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建自己的UDF，如下所示：
- en: '[PRE37]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The second argument in the `udf()` method is the return type of the method
    (that is, `scoreToCategory`). Now you can call this UDF to convert the score into
    grade in a pretty straightforward way. Let''s see an example of it:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`udf()`方法中的第二个参数是方法的返回类型（即`scoreToCategory`）。现在你可以非常简单地调用这个UDF来将分数转换为成绩。让我们来看一下它的示例：'
- en: '[PRE38]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The preceding line will take score as input for all entries and convert the
    score to a grade. Additionally, a new DataFrame with a column named `Grade` will
    be added.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的这一行将接受所有条目的分数作为输入，并将分数转换为等级。此外，还将添加一个名为`Grade`的新DataFrame列。
- en: 'The output is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![](img/00354.gif)**Figure 13**: Assigned grades'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00354.gif)**图13**：分配的成绩'
- en: 'Now we can use the UDF with the SQL statement as well. However, for that, we
    need to register this UDF as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们也可以使用SQL语句来使用UDF。然而，为此我们需要按照以下方式注册这个UDF：
- en: '[PRE39]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The preceding line will register the UDF as a temporary function in the database
    by default. Now we need to create a team view to allow executing SQL queries:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 前面这一行默认会将UDF注册为数据库中的临时函数。现在我们需要创建一个团队视图以便执行SQL查询：
- en: '[PRE40]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now let''s execute an SQL query on the view `score` as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在视图`score`上执行SQL查询，如下所示：
- en: '[PRE41]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![](img/00153.gif)**Figure 14**: Query on the students score and corresponding
    grades'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00153.gif)**图14**：查询学生分数和对应的成绩'
- en: 'The complete source code for this example is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例的完整源代码如下：
- en: '[PRE42]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: A more detailed discussion on using UDF can be found at [https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html.](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用UDF的更详细讨论，请访问[https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html.](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html)
- en: Now let's do some analytics tasks on PySpark. In the next section, we will show
    an example using the k-means algorithm for a clustering task using PySpark.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在PySpark上进行一些分析任务。在接下来的部分，我们将展示一个使用PySpark进行聚类任务的k-means算法示例。
- en: Let's do some analytics with k-means clustering
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们使用k-means聚类进行一些分析
- en: Anomalous data refers to data that is unusual from normal distributions. Thus,
    detecting anomalies is an important task for network security, anomalous packets
    or requests can be flagged as errors or potential attacks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 异常数据是指与正常分布不同的数据。因此，检测异常是网络安全中的一个重要任务，异常数据包或请求可以标记为错误或潜在攻击。
- en: 'In this example, we will use the KDD-99 dataset (can be downloaded here: [http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)
    ). A number of columns will be filtered out based on certain criteria of the data
    points. This will help us understand the example. Secondly, for the unsupervised
    task; we will have to remove the labeled data. Let''s load and parse the dataset
    as simple texts. Then let''s see how many rows there are in the dataset:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用KDD-99数据集（可以在这里下载：[http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)）。根据数据点的某些标准，许多列将被过滤掉。这将帮助我们理解示例。其次，对于无监督任务，我们将不得不移除标注的数据。让我们将数据集作为简单文本加载和解析。然后，看看数据集有多少行：
- en: '[PRE43]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This essentially returns an RDD. Let''s see how many rows in the dataset are
    using the `count()` method as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这本质上返回一个RDD。让我们使用`count()`方法查看数据集中有多少行，如下所示：
- en: '[PRE44]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'So, the dataset is pretty big with lots of features. Since we have parsed the
    dataset as simple texts, we should not expect to see the better structure of the
    dataset. Thus, let''s work toward converting the RDD into DataFrame as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据集非常庞大，包含许多特征。由于我们已经将数据集解析为简单文本，所以不应期望看到数据集的更好结构。因此，让我们朝着将RDD转换为DataFrame的方向进行：
- en: '[PRE45]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then let''s see some selected columns in the DataFrame as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们查看DataFrame中的一些选定列，如下所示：
- en: '[PRE46]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00080.gif)**Figure 15**: Sample of the KKD cup 99 dataset'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00080.gif)**图15**：KKD杯99数据集样本'
- en: 'Thus, this dataset is already labeled. This means that the types of malicious
    cyber behavior have been assigned to a row where the label is the last column
    (that is, `_42`). The first five rows off the DataFrame are labeled normal. This
    means that these data points are normal. Now this is the time that we need to
    determine the counts of the labels for the entire dataset for each type of labels:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个数据集已经标注过了。这意味着恶意网络行为的类型已经被分配到一行，其中标签位于最后一列（即`_42`）。数据框架的前五行被标注为正常。这意味着这些数据点是正常的。现在，我们需要为整个数据集中的每种标签类型确定标签的计数：
- en: '[PRE47]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00134.gif)**Figure 16**: Available labels (attack types) in the KDD
    cup dataset'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00134.gif)**图16**：KDD杯数据集中的可用标签（攻击类型）'
- en: We can see that there are 23 distinct labels (behavior for data objects). The
    most data points belong to Smurf. This is an abnormal behavior also known as DoS
    packet floods. The Neptune is the second highest abnormal behavior. The *normal*
    events are the third most occurring types of events in the dataset. However, in
    a real network dataset, you will not see any such labels.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到有23个不同的标签（数据对象的行为）。大多数数据点属于Smurf。这是一种异常行为，也称为DoS数据包洪水。Neptune是第二大异常行为。*正常*事件是数据集中第三大出现类型。然而，在一个真实的网络数据集中，你不会看到任何这样的标签。
- en: Also, the normal traffic will be much higher than any anomalous traffic. As
    a result, identifying the anomalous attack or anomaly from the large-scale unlabeled
    data would be tedious. For simplicity, let's ignore the last column (that is,
    labels) and think that this dataset is unlabeled too. In that case, the only way
    to conceptualize the anomaly detection is using unsupervised learning algorithms
    such as k-means for clustering.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正常流量将远高于任何异常流量。因此，从大规模无标签数据中识别异常攻击或异常将是繁琐的。为了简化起见，我们忽略最后一列（即标签），假设这个数据集也是没有标签的。在这种情况下，唯一的异常检测概念化方式是使用无监督学习算法，如k-means聚类。
- en: 'Now let''s work toward clustering the data points for this. One important thing
    about K-means is that it only accepts numeric values for modeling. However, our
    dataset also contains some categorical features. Now we can assign the categorical
    features binary values of 1 or 0 based on whether they are *TCP* or not. This
    can be done as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们朝着对数据点进行聚类的方向努力。关于K-means的一个重要事项是，它只接受数值型数据进行建模。然而，我们的数据集还包含一些类别特征。现在，我们可以根据这些特征是否是*TCP*，为类别特征分配1或0的二进制值。这样可以通过以下方式完成：
- en: '[PRE48]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Thus, our dataset is almost ready. Now we can prepare our training and test
    set to training the k-means model with ease:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的数据集几乎准备好了。现在我们可以准备训练集和测试集，轻松地训练 k-means 模型：
- en: '[PRE49]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output is as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE50]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: However, some standardization is also required since we converted some categorical
    features to numeric features. Standardization can improve the convergence rate
    during the optimization process and can also prevent features with very large
    variances exerting an influence during model training.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们将一些分类特征转换为数值特征，因此还需要进行标准化。标准化可以提高优化过程中的收敛速度，并且可以防止具有非常大方差的特征在模型训练过程中产生影响。
- en: 'Now we will use StandardScaler, which is a feature transformer. It helps us
    standardize features by scaling them to unit variance. It then sets the mean to
    zero using column summary statistics in the training set samples:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用 StandardScaler，它是一个特征转换器。它通过将特征缩放到单位方差来帮助我们标准化特征。然后，它通过使用训练集样本中的列汇总统计量将均值设置为零：
- en: '[PRE51]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now let''s compute the summary statistics by fitting the preceding transformer
    as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过拟合前面的转换器来计算汇总统计数据，如下所示：
- en: '[PRE52]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now the problem is the data that we have for training the k-means does not
    have a normal distribution. Thus, we need to normalize each feature in the training
    set to have the unit standard deviation. To make this happen, we need to further
    transform the preceding standardizer model as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，我们用于训练 k-means 的数据没有正态分布。因此，我们需要将训练集中的每个特征进行标准化，使其具有单位标准差。为实现这一点，我们需要进一步转换前面的标准化器模型，如下所示：
- en: '[PRE53]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Well done! Now the training set is finally ready to train the k-means model.
    As we discussed in the clustering chapter, the trickiest thing in the clustering
    algorithm is finding the optimal number of clusters by setting the value of K
    so that the data objects get clustered automatically.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！现在训练集终于准备好训练 k-means 模型了。正如我们在聚类章节中讨论的，聚类算法中最棘手的事情是通过设置 K 值来找到最优的簇数，使得数据对象能够自动聚类。
- en: 'One Naive approach considered a brute force is setting `K=2` and observing
    the results and trying until you get an optimal one. However, a much better approach
    is the Elbow approach, where we can keep increasing the value of `K` and compute
    the **Within Set Sum of Squared Errors** (**WSSSE**) as the clustering cost. In
    short, we will be looking for the optimal `K` values that also minimize the WSSSE.
    Whenever a sharp decrease is observed, we will get to know the optimal value for
    `K`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一种天真的方法是将 `K=2`，观察结果并进行尝试，直到得到最优值。然而，更好的方法是肘部法则，我们可以不断增加 `K` 的值，并计算**簇内平方误差和**（**WSSSE**）作为聚类成本。简而言之，我们将寻找能够最小化
    WSSSE 的最佳 `K` 值。每当观察到急剧下降时，我们就能确定 `K` 的最优值：
- en: '[PRE54]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output is as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE55]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'In this case, 30 is the best value for k. Let''s check the cluster assignments
    for each data point when we have 30 clusters. The next test would be to run for
    `k` values of 30, 35, and 40\. Three values of k are not the most you would test
    in a single run, but only used for this example:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，30 是 k 的最佳值。让我们查看当我们有 30 个簇时每个数据点的聚类分配。下一个测试将会是运行 `k` 值为 30、35 和 40 的情况。三个
    k 值并不是单次运行中测试的最大值，而是用于这个示例：
- en: '[PRE56]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output is as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00100.jpeg)**Figure 17**: Final cluster centers for each attack type
    (abridged)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.jpeg)**图 17**：每种攻击类型的最终聚类中心（简化版）'
- en: 'Now let''s compute and print the total cost for the overall clustering as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算并打印出整体聚类的总成本，如下所示：
- en: '[PRE57]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The output is as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE58]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Finally, the WSSSE of our k-means model can be computed and printed as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以计算并打印出我们的 k-means 模型的 WSSSE，如下所示：
- en: '[PRE59]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output is as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE60]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Your results might be slightly different. This is due to the random placement
    of the centroids when we first begin the clustering algorithm. Performing this
    many times allows you to see how points in your data change their value of k or
    stay the same. The full source code for this solution is given in the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你的结果可能会有所不同。这是因为当我们首次开始聚类算法时，质心的位置是随机的。多次执行这个过程可以让你观察数据点在变化它们的 k 值时是如何变化的，或者保持不变。该解决方案的完整源代码如下：
- en: '[PRE61]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: A more comprehensive discussion on this topic can be found at [https://github.com/jadianes/kdd-cup-99-spark](https://github.com/jadianes/kdd-cup-99-spark).
    Also, interested readers can refer to the main and latest documentation on PySpark
    APIs at [http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此主题的更全面讨论可以参见 [https://github.com/jadianes/kdd-cup-99-spark](https://github.com/jadianes/kdd-cup-99-spark)。此外，感兴趣的读者还可以参考
    PySpark API 的最新官方文档：[http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/)。
- en: Well, now it's time to move to SparkR, another Spark API to work with population
    statistical programming language called R.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在是时候了解 SparkR 了，它是另一个与人口统计编程语言 R 配合使用的 Spark API。
- en: Introduction to SparkR
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkR 介绍
- en: R is one of the most popular statistical programming languages with a number
    of exciting features that support statistical computing, data processing, and
    machine learning tasks. However, processing large-scale datasets in R is usually
    tedious as the runtime is single-threaded. As a result, only datasets that fit
    in someone's machine memory can be processed. Considering this limitation and
    for getting the full flavor of Spark in R, SparkR was initially developed at the
    AMPLab as a lightweight frontend of R to Apache Spark and using Spark's distributed
    computation engine.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: R 是最流行的统计编程语言之一，具有许多令人兴奋的功能，支持统计计算、数据处理和机器学习任务。然而，在 R 中处理大规模数据集通常是繁琐的，因为其运行时是单线程的。因此，只有适合机器内存的数据集才能被处理。考虑到这一限制，并为了充分发挥
    Spark 在 R 中的优势，SparkR 最初在 AMPLab 开发，作为 R 到 Apache Spark 的轻量级前端，并使用 Spark 的分布式计算引擎。
- en: This way it enables the R programmer to use Spark from RStudio for large-scale
    data analysis from the R shell. In Spark 2.1.0, SparkR provides a distributed
    data frame implementation that supports operations such as selection, filtering,
    and aggregation. This is somewhat similar to R data frames like `dplyr` but can
    be scaled up for large-scale datasets.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，它使得 R 程序员能够通过 RStudio 从 R shell 使用 Spark 进行大规模数据分析。在 Spark 2.1.0 中，SparkR
    提供了一个支持选择、过滤和聚合等操作的分布式数据框架实现。这与 R 中的数据框架（如 `dplyr`）有些相似，但可以扩展到大规模数据集。
- en: Why SparkR?
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择 SparkR？
- en: 'You can write Spark codes using SparkR too that supports distributed machine
    learning using MLlib. In summary, SparkR inherits many benefits from being tightly
    integrated with Spark including the following:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用 SparkR 编写 Spark 代码，支持使用 MLlib 进行分布式机器学习。总之，SparkR 继承了与 Spark 紧密集成的诸多优势，包括以下几点：
- en: '**Supports various data sources API**: SparkR can be used to read in data from
    a variety of sources including Hive tables, JSON files, RDBMS, and Parquet files.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持各种数据源 API：** SparkR 可用于从多种数据源读取数据，包括 Hive 表、JSON 文件、关系数据库（RDBMS）和 Parquet
    文件。'
- en: '**DataFrame optimizations**: SparkR DataFrames also inherit all of the optimizations
    made to the computation engine in terms of code generation, memory management,
    and so on. From the following graph, it can be observed that the optimization
    engine of Spark enables SparkR competent with Scala and Python:'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataFrame 优化：** SparkR DataFrame 还继承了计算引擎中在代码生成、内存管理等方面所做的所有优化。从下图可以看出，Spark
    的优化引擎使得 SparkR 能与 Scala 和 Python 相媲美：'
- en: '![](img/00108.jpeg)**Figure 18:** SparkR DataFrame versus Scala/Python DataFrame'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00108.jpeg)**图 18：** SparkR DataFrame 与 Scala/Python DataFrame 对比'
- en: '**Scalability:** Operations executed on SparkR DataFrames get automatically
    distributed across all the cores and machines available on the Spark cluster.
    Thus, SparkR DataFrames can be used on terabytes of data and run on clusters with
    thousands of machines.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性：** 在 SparkR DataFrame 上执行的操作会自动分布到 Spark 集群中所有可用的核心和机器上。因此，SparkR DataFrame
    可用于处理数 TB 的数据，并且可以在具有成千上万台机器的集群上运行。'
- en: Installing and getting started
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和开始使用
- en: The best way of using SparkR is from RStudio. Your R program can be connected
    to a Spark cluster from RStudio using R shell, Rescript, or other R IDEs.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SparkR 的最佳方式是通过 RStudio。你的 R 程序可以通过 RStudio 使用 R shell、Rescript 或其他 R IDE
    连接到 Spark 集群。
- en: '**Option 1.** Set `SPARK_HOME` in the environment (you can check [https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html)),
    load the SparkR package, and call `sparkR.session` as follows. It will check for
    the Spark installation, and, if not found, it will be downloaded and cached automatically:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**选项 1.** 在环境中设置`SPARK_HOME`（您可以查看[https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html)），加载SparkR包，并调用`sparkR.session`如下所示。它将检查Spark安装情况，如果未找到，将自动下载和缓存：'
- en: '[PRE62]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '**Option 2.** You can also manually configure SparkR on RStudio. For doing
    so, create an R script and execute the following lines of R code on RStudio:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**选项 2.** 您还可以在RStudio上手动配置SparkR。为此，请在RStudio上创建一个R脚本并执行以下R代码：'
- en: '[PRE63]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now load the SparkR library as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如下加载SparkR库：
- en: '[PRE64]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, like Scala/Java/PySpark, the entry point to your SparkR program is the
    SparkR session that can be created by calling `sparkR.session` as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，像Scala/Java/PySpark一样，您的SparkR程序的入口点是通过调用`sparkR.session`创建的SparkR会话，如下所示：
- en: '[PRE65]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Furthermore, if you want, you could also specify certain Spark driver properties.
    Normally, these application properties and runtime environment cannot be set programmatically,
    as the driver JVM process would have been started; in this case, SparkR takes
    care of this for you. To set them, pass them as you would pass other configuration
    properties in the `sparkConfig` argument to `sparkR.session()` as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果需要，还可以指定某些Spark驱动程序属性。通常，这些应用程序属性和运行时环境无法以编程方式设置，因为驱动程序JVM进程已启动；在这种情况下，SparkR会为您处理。要设置它们，请将它们传递给`sparkR.session()`中的`sparkConfig`参数，如下所示：
- en: '[PRE66]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'In addition, the following Spark driver properties can be set in `sparkConfig`
    with `sparkR.session` from RStudio:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还可以在RStudio上使用`sparkR.session`设置以下Spark驱动程序属性：
- en: '![](img/00146.gif)**Figure 19**: Spark driver properties can be set in `sparkConfig`
    with `sparkR.session` from RStudio'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00146.gif)**图 19**：可以使用`sparkR.session`从RStudio设置Spark驱动程序属性的`sparkConfig`'
- en: Getting started
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: 'Let''s start with loading, parsing, and viewing simple flight data. At first,
    download the NY flights dataset as a CSV from [https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv](https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv).
    Now let''s load and parse the dataset using `read.csv()` API of R:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载、解析和查看简单的航班数据开始。首先，从[https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv](https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv)下载NY航班数据集作为CSV文件。现在让我们使用R的`read.csv()`
    API加载和解析数据集：
- en: '[PRE67]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Now let''s view the structure of the dataset using `View()` method of R as
    follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用R的`View()`方法查看数据集的结构如下：
- en: '[PRE68]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '![](img/00217.jpeg)**Figure 20**: A snap of the NYC flight dataset'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00217.jpeg)**图 20**：NYC航班数据集的快照'
- en: 'Now let''s create the Spark DataFrame from the R DataFrame as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从R DataFrame创建Spark DataFrame如下：
- en: '[PRE69]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let''s see the structure by exploring the schema of the DataFrame:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过探索DataFrame的模式来查看其结构：
- en: '[PRE70]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00197.gif)**Figure 21**: The schema of the NYC flight dataset'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00197.gif)**图 21**：NYC航班数据集的模式'
- en: 'Now let''s see the first 10 rows of the DataFrame:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看DataFrame的前10行：
- en: '[PRE71]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output is as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00222.gif)**Figure 22**: The first 10 rows of the NYC flight dataset'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00222.gif)**图 22**：NYC航班数据集的前10行'
- en: So, you can see the same structure. However, this is not scalable since we loaded
    the CSV file using standard R API. To make it faster and scalable, like in Scala,
    we can use external data source APIs.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以看到相同的结构。但是，这不具有可扩展性，因为我们使用标准的R API加载了CSV文件。为了使其更快速和可扩展，就像在Scala中一样，我们可以使用外部数据源API。
- en: Using external data source APIs
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用外部数据源API
- en: 'As mentioned earlier, we can create DataFrame using external data source APIs
    as well. For the following example, we used `com.databricks.spark.csv` API as
    follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，我们也可以使用外部数据源API创建DataFrame。对于以下示例，我们使用`com.databricks.spark.csv` API如下：
- en: '[PRE72]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Let''s see the structure by exploring the schema of the DataFrame:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过探索DataFrame的模式来查看其结构：
- en: '[PRE73]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The output is as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00132.gif)**Figure 23**: The same schema of the NYC flight dataset
    using external data source API'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00132.gif)**图 23**：使用外部数据源API的NYC航班数据集相同的模式'
- en: 'Now let''s see the first 10 rows of the DataFrame:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看DataFrame的前10行：
- en: '[PRE74]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The output is as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00003.jpeg)**Figure 24**: Same sample data from NYC flight dataset
    using external data source API'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00003.jpeg)**图 24**：使用外部数据源 API 的纽约市航班数据集中的相同样本数据'
- en: So, you can see the same structure. Well done! Now it's time to explore something
    more, such as data manipulation using SparkR.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如此，你可以看到相同的结构。做得好！现在是时候探索更多内容了，比如使用 SparkR 进行数据处理。
- en: Data manipulation
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理
- en: 'Show the column names in the SparkDataFrame as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 显示 SparkDataFrame 中的列名，如下所示：
- en: '[PRE75]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Show the number of rows in the SparkDataFrame as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 显示 SparkDataFrame 中的行数，如下所示：
- en: '[PRE76]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Filter flights data whose destination is only Miami and show the first six
    entries as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤出目的地仅为迈阿密的航班数据，并显示前六条记录，如下所示：
- en: '[PRE77]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The output is as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00285.jpeg)**Figure 25**: Flights with destination Miami only'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00285.jpeg)**图 25**：仅目的地为迈阿密的航班'
- en: 'Select specific columns. For example, let''s select all the flights that are
    going to Iowa that are delayed. Also, include the origin airport names:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 选择特定的列。例如，让我们选择所有前往爱荷华州的延误航班，并包括起点机场名称：
- en: '[PRE78]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The output is as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00269.gif)**Figure 26**: All the flights that are going to Iowa that
    are delayed'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00269.gif)**图 26**：所有前往爱荷华州的延误航班'
- en: 'We can even use it to chain data frame operations. To show an example, at first,
    group the flights by date and then find the average daily delay. Then, finally,
    write the result into a SparkDataFrame as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以用它来链接数据框操作。举个例子，首先按日期分组航班，然后找出每日的平均延误时间。最后，将结果写入 SparkDataFrame，如下所示：
- en: '[PRE79]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now print the computed DataFrame:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打印计算后的 DataFrame：
- en: '[PRE80]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The output is as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00300.gif)**Figure 27**: Group the flights by date and then find the
    average daily delay'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00300.gif)**图 27**：按日期分组航班，然后计算每日平均延误时间'
- en: 'Let''s see another example that aggregates average arrival delay for the entire
    destination airport:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个聚合所有目标机场的平均到达延误时间的例子：
- en: '[PRE81]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Even more complex aggregation can be performed. For example, the following
    code aggregates the average, maximum, and minimum delay per each destination airport.
    It also shows the number of flights that land in those airports:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至可以执行更复杂的聚合操作。例如，以下代码聚合了每个目标机场的平均、最大和最小延误时间。它还显示了在这些机场降落的航班数量：
- en: '[PRE82]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The output is as follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00290.gif)**Figure 28**: Maximum and minimum delay per each destination
    airport'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00290.gif)**图 28**：每个目标机场的最大和最小延误'
- en: Querying SparkR DataFrame
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询 SparkR DataFrame
- en: 'Similar to Scala, we can perform a SQL query on the DataFrame once it is saved
    as `TempView` using the `createOrReplaceTempView()` method. Let''s see an example
    of that. At first, let''s save the fight DataFrame (that is, `flightDF`) as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Scala，我们可以对已保存为 `TempView` 的 DataFrame 执行 SQL 查询，方法是使用 `createOrReplaceTempView()`。让我们看一个例子。首先，我们将航班
    DataFrame（即 `flightDF`）保存如下：
- en: '[PRE83]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now let''s select destination and destinations of all the flights with their
    associated carrier information as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们选择所有航班的目的地及其关联的航空公司信息，如下所示：
- en: '[PRE84]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The output is as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00293.jpeg)**Figure 29**: All the flights with their associated carrier
    information'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00293.jpeg)**图 29**：所有航班及其关联的航空公司信息'
- en: 'Now let''s make the SQL a bit more complex, such as finding the destination''s
    airport of all the flights that are at least 120 minutes delayed as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们让 SQL 查询变得更加复杂，比如查找所有至少延误 120 分钟的航班的目标机场，如下所示：
- en: '[PRE85]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The preceding code segment queries and shows the name of the airports of all
    the flights that are delayed by at least 2 hours:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段查询并显示了所有至少延误 2 小时的航班的机场名称：
- en: '![](img/00302.jpeg)**Figure 30**: Destination airports of all the flights that
    are delayed by at least 2 hours'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00302.jpeg)**图 30**：所有至少延误 2 小时的航班的目的地机场'
- en: 'Now let''s do a more complex query. Let''s find the origins of all the flights
    to Iowa that are delayed by at least 2 hours. Finally, sort them by arrival delay
    and limit the count up to 20 as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进行一个更复杂的查询。让我们找出所有前往爱荷华州且至少延误 2 小时的航班的起点。最后，按到达延误时间对它们进行排序，并将结果限制为最多 20
    条，如下所示：
- en: '[PRE86]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The preceding code segment queries and shows the name of the airports of all
    the flights that are delayed by at least 2 hours to Iowa:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段查询并显示了所有至少延误 2 小时前往爱荷华州的航班的机场名称：
- en: '![](img/00308.jpeg)**Figure 31**: Origins of all the flights that are delayed
    by at least 2 hours where the destination is Iowa'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00308.jpeg)**图 31**：所有前往爱荷华州且至少延误 2 小时的航班的起点'
- en: Visualizing your data on RStudio
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 RStudio 中可视化数据
- en: 'In the previous section, we have seen how to load, parse, manipulate, and query
    the DataFrame. Now it would be great if we could show the data for better visibility.
    For example, what could be done for the airline carriers? I mean, is it possible
    to find the most frequent carriers from the plot? Let''s give `ggplot2` a try.
    At first, load the library for the same:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们已经了解了如何加载、解析、操作和查询 DataFrame。现在，如果我们能够展示数据以便更好地观察效果，那就太好了。例如，如何处理航空公司承运人数据？我的意思是，我们能否从图表中找到最常见的承运人？让我们尝试一下
    `ggplot2`。首先，加载该库：
- en: '[PRE87]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Now we already have the SparkDataFrame. What if we directly try to use our SparkSQL
    DataFrame class in `ggplot2`?
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了 SparkDataFrame。如果我们直接尝试在 `ggplot2` 中使用我们的 SparkSQL DataFrame 类，会怎样呢？
- en: '[PRE88]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Obviously, it doesn''t work that way because the `ggplot2` function doesn''t
    know how to deal with those types of distributed data frames (the Spark ones).
    Instead, we need to collect the data locally and convert it back to a traditional
    R data frame as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这样不行，因为 `ggplot2` 函数不知道如何处理这些分布式数据框（即 Spark 数据）。相反，我们需要将数据收集到本地，并将其转换回传统的
    R 数据框，如下所示：
- en: '[PRE89]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Now let''s have a look at what we got using the `str()` method as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用 `str()` 方法查看我们得到的结果，如下所示：
- en: '[PRE90]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The output is as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE91]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'This is good because when we collect results from a SparkSQL DataFrame, we
    get a regular R `data.frame`. It is also very convenient since we can manipulate
    it as needed. And now we are ready to create the `ggplot2` object as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，因为当我们从 SparkSQL DataFrame 中收集结果时，我们会得到一个常规的 R `data.frame`。这也非常方便，因为我们可以根据需要进行操作。现在我们准备好创建
    `ggplot2` 对象，如下所示：
- en: '[PRE92]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Finally, let''s give the plot a proper representation as a bar diagram as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将图表适当地展示为条形图，如下所示：
- en: '[PRE93]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The output is as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/00314.jpeg)**Figure 32**: Most frequent carriers are UA, B6, EV, and
    DL'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00314.jpeg)**图 32**：最常见的承运人是 UA、B6、EV 和 DL'
- en: 'From the graph, it is clear that the most frequent carriers are UA, B6, EV,
    and DL. This gets clearer from the following line of code in R:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中可以看出，最常见的承运人是 UA、B6、EV 和 DL。从 R 代码的以下一行中可以看得更清楚：
- en: '[PRE94]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The output is as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/00317.gif)**Figure 33:** Most most frequent carriers are UA, B6, EV,
    and DL'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00317.gif)**图 33：** 最常见的承运人是 UA、B6、EV 和 DL'
- en: 'The full source code of the preceding analysis is given in the following to
    understand the flow of the code:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述分析的完整源代码，帮助理解代码的流程：
- en: '[PRE95]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Summary
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we showed some examples of how to write your Spark code in
    Python and R. These are the most popular programming languages in the data scientist
    community.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了如何在 Python 和 R 中编写 Spark 代码的一些示例。这些是数据科学家社区中最流行的编程语言。
- en: We covered the motivation of using PySpark and SparkR for big data analytics
    with almost similar ease with Java and Scala. We discussed how to install these
    APIs on their popular IDEs such as PyCharm for PySpark and RStudio for SparkR.
    We also showed how to work with DataFrames and RDDs from these IDEs. Furthermore,
    we discussed how to execute Spark SQL queries from PySpark and SparkR. Then we
    also discussed how to perform some analytics with visualization of the dataset.
    Finally, we saw how to use UDFs with PySpark with examples.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了使用 PySpark 和 SparkR 进行大数据分析的动机，这两者与 Java 和 Scala 的使用几乎同样简便。我们讲解了如何在流行的集成开发环境（IDE）上安装这些
    API，例如 PySpark 在 PyCharm 中安装，SparkR 在 RStudio 中安装。我们还展示了如何在这些 IDE 中使用 DataFrame
    和 RDD。进一步，我们讨论了如何从 PySpark 和 SparkR 执行 Spark SQL 查询。然后，我们也讨论了如何通过可视化数据集来执行一些分析。最后，我们展示了如何使用
    PySpark 中的 UDF，并通过示例进行讲解。
- en: 'Thus, we have discussed several aspects for two Spark''s APIs; PySpark and
    SparkR. There are much more to explore. Interested readers should refer to their
    websites for more information:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经讨论了 Spark 两个 API 的若干方面：PySpark 和 SparkR。还有更多内容待探索。感兴趣的读者可以参考它们的官方网站获取更多信息：
- en: 'PySpark: [http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PySpark: [http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/)'
- en: "SparkR: [https://spark.apache.org/docs/latest/sparkr.html\uFEFF](https://spark.apache.org/docs/latest/sparkr.html)"
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: "SparkR: [https://spark.apache.org/docs/latest/sparkr.html\uFEFF](https://spark.apache.org/docs/latest/sparkr.html)"
