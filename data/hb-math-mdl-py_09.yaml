- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Exploring Optimization Techniques
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索优化技术
- en: This chapter primarily aims to address the question, “Why is optimization necessary
    while solving problems?” Mathematical optimization, or mathematical programming,
    is a powerful decision-making tool that has been talked about in depth in the
    chapters of Part I. What is important is to recall the simple fact that optimization
    yields the best result to a problem by reducing errors that are, essentially,
    the gaps between predicted and real data. Optimization comes at a cost; almost
    all optimization problems are described in terms of costs such as money, time,
    and resources. This cost function is the error function. If a business problem
    has clear goals and constraints, such as in the airline and logistics industries,
    mathematical optimization is applied for efficient decision-making.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要旨在回答“为什么在解决问题时优化是必要的？”这个问题。数学优化，或称数学规划，是一种强大的决策工具，在第一部分的各章中已进行了深入讨论。重要的是要回忆一个简单的事实：优化通过减少误差（本质上是预测数据与实际数据之间的差距）来为问题提供最佳结果。优化是有代价的；几乎所有的优化问题都通过成本来描述，例如金钱、时间和资源。这个成本函数就是误差函数。如果一个商业问题具有明确的目标和约束条件，例如航空和物流行业，那么数学优化就可以用于高效的决策制定。
- en: In **machine learning** (**ML**) problems, the cost is often called the **loss
    function**. ML models make predictions about trends or classify data wherein training
    a model is a process of optimization, as each iteration in this process aims to
    improve the accuracy of the model and lower the margin of error. Selecting the
    optimum value of hyperparameters is key to ensuring an accurately and efficiently
    performing model. Hyperparameters are the elements of an ML model (for example,
    learning rate, number of clusters, etc.) that are tuned to fit a specific dataset
    to the model. In short, they are parameters whose values control the learning
    process. Optimization is an iterative process, meaning that the ML model becomes
    more accurate with each iteration in most cases and becomes better at predicting
    an outcome or classifying data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在**机器学习**（**ML**）问题中，成本通常称为**损失函数**。机器学习模型对趋势进行预测或对数据进行分类，而训练一个模型的过程本质上就是优化过程，因为该过程的每次迭代都旨在提高模型的准确性并降低误差范围。选择超参数的最佳值是确保模型准确高效运行的关键。超参数是机器学习模型中的元素（例如，学习率、簇的数量等），它们经过调优以使特定数据集适应模型。简而言之，它们是控制学习过程的参数值。优化是一个迭代过程，意味着在大多数情况下，机器学习模型在每次迭代后变得更加准确，并且在预测结果或分类数据方面变得更好。
- en: The right blend of ML and mathematical optimization can prove to be useful for
    certain business problems. For example, the output of an ML model can determine
    the scope of an optimization model, especially in routing problems where one uses
    both predictive maintenance with ML as well as clustering, the results of which
    are fed into a mathematical model to create optimal routes. Similarly, an ML model
    may learn from a mathematical model. Initial values of decision variables obtained
    from a mathematical model can be used in an ML model that not only predicts optimal
    values of the decision variables, but also helps accelerate the performance of
    an optimization algorithm.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与数学优化的正确结合对于某些商业问题是有用的。例如，机器学习模型的输出可以决定优化模型的范围，特别是在路径规划问题中，其中使用机器学习进行预测性维护和聚类，其结果被输入到数学模型中，以创建最佳路径。同样，机器学习模型也可以从数学模型中学习。通过数学模型获得的决策变量的初始值可以用于机器学习模型，不仅预测决策变量的最佳值，还可以帮助加速优化算法的性能。
- en: ML optimization is performed using algorithms that exploit a range of techniques
    to refine an ML model. The optimization process searches for the most effective
    configuration or set of hyperparameters for the model to suit the specific use
    case (dataset) or business problem.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ML优化是通过使用一系列技术的算法来优化机器学习模型。优化过程寻找最有效的配置或超参数集，以使模型适应特定的使用案例（数据集）或商业问题。
- en: To summarize, ML is data-driven and optimization is algorithm-driven. Every
    ML model operates on the principle of minimizing the cost function; hence, optimization
    is a superset at its core.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，机器学习（ML）是数据驱动的，而优化是算法驱动的。每个机器学习模型都遵循最小化成本函数的原则；因此，优化在其核心上是一个超集。
- en: 'This chapter covers the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含以下主题：
- en: Optimizing machine learning models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化机器学习模型
- en: Optimization in operations research
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运筹学中的优化
- en: Evolutionary optimization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演化优化
- en: The next section explores approaches and techniques used in optimizing ML models
    to arrive at the best set of hyperparameters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将探讨用于优化机器学习模型的各种方法和技术，以便找到最佳的超参数集。
- en: Optimizing machine learning models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化机器学习模型
- en: The concept of optimization is integral to an ML model. ML helps make clusters,
    detect anomalies, predict the future from historical data, and so forth. However,
    when it comes to minimizing costs in a business, finding optimal placement of
    business facilities, et cetera, what we need is a mathematical optimization model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 优化的概念是机器学习模型的核心。机器学习有助于进行聚类、检测异常、从历史数据预测未来等。然而，当涉及到最小化业务成本、寻找业务设施的最佳位置等问题时，我们所需要的是一个数学优化模型。
- en: 'We will talk about optimization in machine learning in this section. Optimization
    ensures that the structure and configuration of the ML model are as effective
    as possible to achieve the goal it has been built for. Optimization techniques
    automate the testing of different model configurations. The best configuration
    (set of hyperparameters) has the lowest margin of error, thereby yielding the
    most accurate model for a given dataset. Getting the hyperparameter optimization
    right for an ML model can be tedious, as both under-optimized (underfit) as well
    as over-optimized (overfit) models fail. Overfitting is when a model is trained
    too closely to training data, resulting in inaccurate yields with new data. Underfitting
    is when a model is poorly trained, making it ineffective with training data as
    well as new data. Hyperparameters can be sought manually, which is an exhaustive
    method using trial and error. Underfit, optimal, and overfit models are illustrated
    in *Figure 9**.1* as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论机器学习中的优化问题。优化确保了机器学习模型的结构和配置尽可能有效，以实现其建立的目标。优化技术可以自动化测试不同的模型配置。最佳配置（超参数集）具有最小的误差范围，从而为给定的数据集提供最准确的模型。对于机器学习模型，正确的超参数优化可能是一个繁琐的过程，因为无论是未优化（欠拟合）还是过度优化（过拟合）的模型都会失败。过拟合是指模型过于贴合训练数据，导致在新数据上的预测不准确。欠拟合是指模型训练不足，使其在训练数据和新数据上都表现不佳。超参数可以通过手动搜索获得，这是一种通过反复试验进行的繁琐方法。欠拟合、最佳拟合和过拟合的模型在*图
    9.1*中进行了说明，如下所示：
- en: '![Figure 9.1: Under-optimized (L) and over-optimized (R) model fits](img/Figure_09_01_B18943.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1：未优化（L）和过度优化（R）模型拟合](img/Figure_09_01_B18943.jpg)'
- en: 'Figure 9.1: Under-optimized (L) and over-optimized (R) model fits'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：未优化（L）和过度优化（R）模型拟合
- en: The main techniques of optimization include **random search**, **grid search**
    of hyperparameters, and **Bayesian optimization**, all of which are discussed
    in the following subsections.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 优化的主要技术包括**随机搜索**、**网格搜索**超参数和**贝叶斯优化**，这些都将在接下来的小节中讨论。
- en: Random search
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机搜索
- en: The process of random sampling of the search space and identifying the most
    effective configuration of a hyperparameter set is random search. A random search
    technique discovers new combinations of hyperparameters for an optimized ML model.
    The number of iterations in the search process has to be set, which limits these
    new combinations, without which the process is a lot more time-consuming. It is
    an efficient process as it replaces an exhaustive search with randomness. A search
    space can be thought of as a volume in space, each dimension of which represents
    a hyperparameter, and each point or vector in the volume represents a model configuration.
    An optimization procedure involves defining the search space.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随机抽样搜索空间并确定最有效的超参数配置集的过程叫做随机搜索。随机搜索技术可以发现新的超参数组合，从而优化机器学习模型。搜索过程中的迭代次数需要设置，这限制了这些新组合的数量，否则过程会变得更加耗时。它是一个高效的过程，因为它用随机性替代了穷举搜索。搜索空间可以被看作是一个空间中的体积，每个维度代表一个超参数，体积中的每个点或向量代表一个模型配置。优化过程包括定义搜索空间。
- en: 'The search space is a dictionary in the Python code, and the `scikit-learn`
    library provides functions to tune model hyperparameters. An example code of a
    random search for a classification model is provided here:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间是 Python 代码中的一个字典，`scikit-learn` 库提供了调节模型超参数的函数。这里提供一个分类模型的随机搜索代码示例：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The dataset used is a set of 60 patterns obtained by bouncing sonar signals
    off a metal cylinder under various conditions. Each pattern is a set of numbers
    lying in the range between 0.0 and 1.0, with each number representing the energy
    within a frequency band integrated over a period of time. The label associated
    with each record is either *R* if the object is a rock, or *M* if the object is
    a metal cylinder or mine. Data can be found in the GitHub repository at [https://github.com/ranja-sarkar/dataset](https://github.com/ranja-sarkar/dataset).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据集是一组由在各种条件下将声纳信号反射到金属圆柱体上获得的 60 个模式。每个模式是一组介于 0.0 和 1.0 之间的数字，每个数字代表在一段时间内集成的频率带内的能量。每个记录关联的标签要么是*R*（表示物体为岩石），要么是*M*（表示物体为金属圆柱体或地雷）。数据可以在
    GitHub 仓库中找到：[https://github.com/ranja-sarkar/dataset](https://github.com/ranja-sarkar/dataset)。
- en: An example code of random search for a linear regression model has also been
    provided. The insurance dataset with two variables, namely the number of claims
    and total payment (in Swedish Krona) for all claims in geographical Swedish zones,
    can be found in the GitHub repository at [https://github.com/ranja-sarkar/dataset](https://github.com/ranja-sarkar/dataset).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 还提供了一个用于线性回归模型的随机搜索示例代码。保险数据集包含两个变量，即索赔数量和所有索赔的总支付金额（以瑞典克朗计），可以在 GitHub 仓库中找到：[https://github.com/ranja-sarkar/dataset](https://github.com/ranja-sarkar/dataset)。
- en: 'The difference between regression and classification tasks is in choosing the
    performance scoring protocol for the models. The hyperparameter optimization methods
    in the `scikit-learn` Python library assume good performance scores are negative
    values close to zero (for regression), with zero representing a perfect regression
    model:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 回归任务和分类任务的区别在于选择模型的性能评分协议。`scikit-learn` Python 库中的超参数优化方法假设好的性能评分为接近零的负值（对于回归任务），其中零表示完美的回归模型：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The runtime of the code depends on the size of the search space and the system
    processor speed. The `result` class in the code provides the outcome, the most
    important value being the best score for the best performance of the model and
    the hyperparameters that achieved this score. Once the best set of hyperparameters
    becomes known, one can define a new model, set the hyperparameters to the known
    values, and fit the model on available data. This model can then be used for predictions
    on new data. The number of random configurations in the parameter space look like
    *Figure 9**.2*, which shows that random search works best for low-dimensional
    data:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的运行时间取决于搜索空间的大小和系统处理器的速度。代码中的`result`类提供了结果，其中最重要的值是最佳得分，用于表示模型和超参数的最佳表现。知道最佳的超参数集后，可以定义一个新模型，将超参数设置为已知值，并在可用数据上拟合该模型。然后，这个模型可以用于对新数据进行预测。参数空间中随机配置的数量类似于*图
    9.2*，该图表明随机搜索在低维数据中表现最佳：
- en: '![Figure 9.2: Random search](img/Figure_09_02_B18943.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2：随机搜索](img/Figure_09_02_B18943.jpg)'
- en: 'Figure 9.2: Random search'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：随机搜索
- en: In the next subsection, we elaborate on grid search for optimization of classification
    and regression models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将详细阐述网格搜索在分类和回归模型优化中的应用。
- en: Grid search
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格搜索
- en: The process of assessing the effectiveness of known hyperparameter values of
    an ML model is grid search. Each hyperparameter is represented as a dimension
    on a grid across the search space and each point in the grid is searched and evaluated.
    Grid search is great for checking intuitive guesses and hyperparameter combinations
    that are known to perform well in general. As mentioned earlier, an optimization
    procedure involves defining a search space (a dictionary in Python), which can
    be thought of as a volume where each dimension represents a hyperparameter and
    each point (vector) represents a model configuration. A discrete grid has to be
    defined here. In other words, the grid search space takes discrete values (that
    can be on a log scale) instead of a log-uniform distribution used in a random
    search space.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 评估机器学习模型已知超参数值有效性的过程是网格搜索。每个超参数在搜索空间的网格上表示为一个维度，网格中的每个点都要进行搜索和评估。网格搜索非常适合检查直观的猜测和已知在一般情况下表现良好的超参数组合。如前所述，优化过程涉及定义搜索空间（在
    Python 中为字典），可以将其视为一个体积，其中每个维度表示一个超参数，每个点（向量）表示一个模型配置。这里必须定义一个离散网格。换句话说，网格搜索空间取离散值（可以是对数尺度），而不是随机搜索空间中使用的对数均匀分布。
- en: 'A sample code of grid search for a classification model using the same dataset
    (`sonar.csv`) explored for a random search algorithm is given here:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下给出一个使用相同数据集（`sonar.csv`）进行分类模型网格搜索的示例代码，该数据集也用于随机搜索算法：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'A sample code of a grid search for a linear regression model using the same
    dataset (`auto-insurance.csv`) explored for a random search algorithm is provided
    as follows. The best hyperparameters obtained using the random search and grid
    search algorithms for this dataset can be compared to get an estimate of which
    algorithm works better for the dataset:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下提供了一个使用相同数据集（`auto-insurance.csv`）进行线性回归模型网格搜索的示例代码，该数据集也用于随机搜索算法。可以比较使用随机搜索和网格搜索算法为该数据集获得的最佳超参数，以估算哪种算法对该数据集表现更好：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The scores obtained for the datasets with random search and grid search in
    classification and regression models are nearly identical. The selection of optimization
    technique for a given dataset depends on the use case. Though random search might
    in some cases result in better performance, it needs more time, while grid search
    is appropriate for quick searches of hyperparameters that perform well in general.
    The values of hyperparameters are placed like a matrix as shown in *Figure 9**.3*,
    similar to a grid:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机搜索和网格搜索获得的分类和回归模型数据集的分数几乎相同。针对给定数据集选择优化技术取决于具体的应用场景。虽然在某些情况下，随机搜索可能会导致更好的性能，但它需要更多的时间，而网格搜索则适用于快速搜索那些通常表现良好的超参数。超参数的值像矩阵一样排列，如*图
    9**.3*所示，类似于一个网格：
- en: '![Figure 9.3: Grid search](img/Figure_09_03_B18943.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3：网格搜索](img/Figure_09_03_B18943.jpg)'
- en: 'Figure 9.3: Grid search'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：网格搜索
- en: Another method, known as Bayesian optimization, whose search procedure is different
    from the preceding two, is discussed in the following subsection.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法，称为贝叶斯优化，其搜索过程不同于前述两种方法，将在以下小节中讨论。
- en: Bayesian optimization
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: 'A directed and iterative approach to global optimization using probability
    is Bayesian optimization. This is a Gaussian process that converges fast for continuous
    hyperparameters that is, in a continuous search space (*Figure 9**.4*). In Bayesian
    optimization, a probabilistic model of the function is built, and maps hyperparameters
    to the objectives evaluated on a validation dataset. This process evaluates a
    hyperparameter configuration based on the current model, then updates it until
    an optimal point is reached and it attempts to find the global optimum in a minimum
    number of steps. In most cases, it is more efficient and effective than optimization
    by way of random search. The optimization landscape (multiple local minima) with
    one global minimum is illustrated as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用概率进行全局优化的有向迭代方法是贝叶斯优化。这是一个高斯过程，对于连续超参数的优化收敛速度较快，即在连续搜索空间中（*图 9**.4*）。在贝叶斯优化中，会构建一个函数的概率模型，并将超参数映射到在验证数据集上评估的目标。该过程基于当前模型评估一个超参数配置，然后更新模型，直到达到最优点，并尽量在最少的步骤中找到全局最优解。在大多数情况下，它比随机搜索更高效、有效。优化景观（多个局部极小值）与一个全局极小值的示例如下：
- en: '![Figure 9.4: Optimization landscape (response surface)](img/Figure_09_04_B18943.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4：优化景观（响应面）](img/Figure_09_04_B18943.jpg)'
- en: 'Figure 9.4: Optimization landscape (response surface)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：优化景观（响应面）
- en: 'Bayesian optimization incorporates prior belief (marginal probability) about
    the objective function and updates the prior with samples drawn from the function
    to obtain a posterior belief (conditional probability) that better approximates
    the function, which is illustrated in *Figure 9**.5*. This process repeats itself
    until the extremum of the objective function is located or resources are exhausted:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化结合了关于目标函数的先验信念（边际概率），并通过从该函数中抽取样本来更新先验信念，从而获得一个后验信念（条件概率），该后验信念能更好地逼近该函数，如*图
    9**.5*所示。该过程会不断重复，直到找到目标函数的极值点或资源耗尽：
- en: '![Figure 9.5: Bayesian statistics](img/Figure_09_05_B18943.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5：贝叶斯统计](img/Figure_09_05_B18943.jpg)'
- en: 'Figure 9.5: Bayesian statistics'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：贝叶斯统计
- en: 'Bayesian search is typically beneficial when there is a large amount of data,
    the learning is slow, and tuning time has to be minimized. The `scikit-optimize`
    library provides functions for Bayesian optimization of ML models. A sample code
    for hyperparameter tuning by the Bayesian method in a classification problem is
    provided as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯搜索通常在数据量大、学习速度慢且需要最小化调优时间时非常有益。`scikit-optimize`库提供了用于机器学习模型贝叶斯优化的函数。以下是一个在分类问题中使用贝叶斯方法进行超参数调优的示例代码：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The model used for approximating the objective function is called the **surrogate
    model**, and the posterior probability is a surrogate objective function that
    can be used to estimate the cost of candidate samples. The posterior is used to
    select the next sample from the search space and the technique that does this
    is called the **acquisition function**. Bayesian optimization is best when the
    function evaluation is expensive or the form of the objective function is complex
    (nonlinear, non-convex, highly multi-dimensional, or highly noisy) – for example,
    in deep neural networks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 用于逼近目标函数的模型称为**代理模型**，而后验概率则是一个代理目标函数，可用于估算候选样本的成本。后验概率用于从搜索空间中选择下一个样本，执行此操作的技术称为**获取函数**。贝叶斯优化在函数评估成本较高或目标函数形式复杂（非线性、非凸、高维或噪声很大）时效果最佳——例如，在深度神经网络中。
- en: The process of optimization lowers errors or loss from predictions in an ML
    model and improves the model’s accuracy. The very premise of ML relies on a form
    of function optimization so inputs can be almost accurately mapped to expected
    outputs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程通过减少机器学习模型预测中的错误或损失，从而提高模型的准确性。机器学习的基本前提依赖于某种形式的函数优化，以便将输入几乎精确地映射到预期的输出。
- en: In the next section, we will learn about mathematical optimization in operations
    research.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将学习运筹学中的数学优化。
- en: Optimization in operations research
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运筹学中的优化
- en: The term **operations research** was coined during World War I, when the British
    military brought together a group of scientists to allocate insufficient resources
    (food, medicines, etc.) in the most effective way possible to different military
    operations. Therefore, the term implies optimization, which is maximizing or minimizing
    an objective function subject to constraints, often in complex problems and in
    high dimensions. Operations problems typically include planning work shifts or
    creating a schedule for large organizations, designing facilities for customers
    at a large store, choosing investments for available funds, supply chain management,
    and inventory management, all of which can be posed or formulated as mathematical
    problems with a collection of variables and their relationships.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**运筹学**这一术语源于第一次世界大战，当时英国军方将一群科学家聚集在一起，旨在最有效地分配不足的资源（如食物、药品等）到不同的军事行动中。因此，这一术语暗示了优化，即在约束条件下最大化或最小化目标函数，通常用于复杂问题和高维度问题。运筹问题通常包括规划工作班次或为大组织创建时间表、为大商店设计客户设施、选择可用资金的投资、供应链管理和库存管理，这些都可以通过一系列变量及其关系来表述或形式化为数学问题。'
- en: In operations research, a business problem is mapped to a lower-level generic
    problem that is concise enough to be described in mathematical notations. These
    generic problems can in turn be expressed using higher-level languages; for example,
    resources and activities are used to describe a scheduling problem. The higher-level
    language is problem-specific, hence the generic problems can be described using
    modeling paradigms. A modeling paradigm is a set of rules and practices that allows
    for the representation of higher-level problems using lower-level data structures
    such as matrices. These data structures or matrices are passed to the last step
    of abstraction, which is algorithms. The most prominent modeling paradigms are
    linear programming, integer programming, and mixed-integer programming, all of
    which use linear equality constraints. There is a family of algorithms to solve
    these linear programming problems. Search algorithms, such as branch and bound,
    solve integer programming problems, while the simplex algorithm is used in a linear
    programming modeling paradigm.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在运筹学中，商业问题被映射到一个更低层次的通用问题，该问题足够简洁，可以用数学符号来描述。这些通用问题反过来可以使用更高层次的语言来表达；例如，资源和活动用于描述调度问题。更高层次的语言是问题特定的，因此通用问题可以使用建模范式来描述。建模范式是一组规则和实践，允许使用更低层次的数据结构（如矩阵）来表示更高层次的问题。这些数据结构或矩阵会传递到最后一步的抽象层次，即算法。最突出的建模范式包括线性规划、整数规划和混合整数规划，所有这些方法都使用线性等式约束。有一系列算法用于解决这些线性规划问题。搜索算法（如分支限界法）用于解决整数规划问题，而单纯形算法则用于线性规划建模范式。
- en: 'An example of how to solve a knapsack problem by optimization is illustrated
    with the following data (*Figures 9.6a* *and 9.6b*):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下数据演示了如何通过优化来解决背包问题（*图 9.6a* *和 9.6b*）：
- en: '![Figure 9.6a: Knapsack problem](img/Figure_09_06_B18943.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6a：背包问题](img/Figure_09_06_B18943.jpg)'
- en: 'Figure 9.6a: Knapsack problem'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6a：背包问题
- en: 'Let’s say the constraint is the ability to only carry a maximum of 2.9 kg in
    the camping sack, while the total weight of all items is 3.09 kg. The item’s value
    assists in choosing the optimum number of items. As the number of items increases,
    the problem becomes bigger, and solving it by trying all possible combinations
    of items takes a significant amount of time:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 假设约束条件是背包只能携带最大 2.9 公斤的物品，而所有物品的总重量为 3.09 公斤。物品的价值有助于选择最优的物品数量。随着物品数量的增加，问题变得更加复杂，通过尝试所有可能的物品组合来求解，所需时间会显著增加：
- en: '![Figure 9.6b: Knapsack problem with another variable](img/Figure_09_07_B18943.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6b：具有另一个变量的背包问题](img/Figure_09_07_B18943.jpg)'
- en: 'Figure 9.6b: Knapsack problem with another variable'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6b：具有另一个变量的背包问题
- en: 'The objective function is value, which must be maximized. The best of items
    has to be chosen to meet the constraint of 2.9 kg by total weight. A solver (pulp,
    in this case) is used to solve this linear programming problem, as shown in the
    following code. The decision variables (to be determined) are given by ![](img/Formula_09_001.png).
    The variable is 1 if the item is chosen and 0 if the item is not chosen:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数是价值，必须最大化。必须选择最佳物品，以满足总重量不超过 2.9 公斤的约束条件。使用求解器（在此情况下为 pulp）来解决此线性规划问题，代码如下所示。决策变量（待确定）由
    ![](img/Formula_09_001.png) 给出。若选择该物品，变量为 1；若未选择该物品，变量为 0：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This code when executed results in the following output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码时，将产生以下输出：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Going by the result (optimal solution), a hand sanitizer must not be carried
    in the sack. This is a simple integer programming problem as decision variables
    are restricted to being integers. In a very similar manner, other practical business
    problems such as production planning are solved by mathematical optimization wherein
    the right resources are chosen to maximize profit and so on. When operations research
    is combined with ML predictions, data science is effectively transformed into
    decision science, allowing organizations to make actionable decisions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果（最优解），背包中不应该携带手部消毒液。这是一个简单的整数规划问题，因为决策变量被限制为整数。以类似的方式，其他实际的商业问题，如生产规划，都是通过数学优化来解决的，在这些问题中，选择合适的资源来最大化利润等。当运筹学与机器学习预测相结合时，数据科学实际上转变为决策科学，帮助组织做出可行的决策。
- en: In the next section, we will learn about **evolutionary optimization**, which
    is motivated by optimization processes observed in nature such as the migration
    of species, bird swarms, and ant colonies.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习关于**进化优化**的内容，这受到自然界中观察到的优化过程的启发，例如物种迁移、鸟群和蚂蚁群。
- en: Evolutionary optimization
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进化优化
- en: Evolutionary optimization makes use of algorithms that mimic the selection process
    within the natural world. Examples of this are genetic algorithms that optimize
    via natural selection. Each iteration of a hyperparameter value is like a mutation
    in genetics that is assessed and altered. The process continues using recombined
    choices until the most effective configuration is reached. Hence, each generation
    improves with every iteration as it is optimized. Genetic algorithms are often
    used to train neural networks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 进化优化利用模仿自然界内选择过程的算法。例如通过自然选择优化的遗传算法。每次超参数值的迭代就像是遗传学中的突变，进行评估和改变。这个过程通过重新组合的选择一直持续，直到达到最有效的配置。因此，每一代在优化中都会随着每次迭代而改进。遗传算法经常用于训练神经网络。
- en: 'An evolutionary algorithm typically consists of three steps: initialization,
    selection, and termination. Fitter generations survive and proliferate, like in
    natural selection. In general, an initial population of a wide range of solutions
    is randomly created within the constraints of the problem. The population contains
    an arbitrary number of possible solutions to the problem, or the solutions are
    roughly centered around what is believed to be an ideal solution. These solutions
    are then evaluated in the next step according to a fitness (or objective) function.
    A good fitness function is one that represents the data and calculates a numerical
    value for the viability of a solution to a specific problem. Once the fitness
    of all solutions is calculated, the top-scoring solutions are selected. There
    may be multiple fitness functions that result in more than one optimal solution,
    which is when a decider is used to narrow down a single problem-specific solution
    that is based on some key metrics. *Figure 9**.7* depicts the steps of these algorithms
    as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算法通常包括三个步骤：初始化、选择和终止。更适应的生成会像自然选择那样生存和繁殖。一般来说，在问题的约束条件下随机创建一个广泛范围的解初始种群。种群包含了解决问题的任意数量的可能解，或者这些解大致集中在被认为是理想解的周围。然后根据适应度（或目标）函数对这些解进行评估。一个好的适应度函数能够代表数据并计算一个解对特定问题可行性的数值。一旦计算出所有解的适应度，就会选择得分最高的解。可能存在多个适应度函数导致多个最优解，这时会使用决策者根据一些关键指标缩小到一个单一问题特定解。*图
    9**.7* 描述了这些算法的步骤如下：
- en: '![Figure 9.7: Steps of evolutionary algorithms](img/Figure_09_08_B18943.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7: 进化算法的步骤](img/Figure_09_08_B18943.jpg)'
- en: 'Figure 9.7: Steps of evolutionary algorithms'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.7: 进化算法的步骤'
- en: The top solutions make the next generation in the algorithm. These solutions
    typically have a mixture of the characteristics of solutions from the previous
    generation. New genetic material is introduced into this new generation, which,
    mathematically speaking, means introducing new probability distribution. This
    step is mutation, without which optimal results are difficult to achieve. The
    last step is termination, when the algorithm reaches either some threshold of
    performance or some maximum number of iterations (runtime). A final solution is
    then selected and returned.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最优解会成为算法的下一代。这些解通常具有上一代解的特征混合。新的遗传材料被引入到这个新的代中，从数学角度来说，这意味着引入新的概率分布。这一步骤是突变，没有它很难达到最优结果。最后一步是终止，当算法达到性能阈值或最大迭代次数（运行时间）时。然后选择并返回最终解。
- en: An evolutionary algorithm is a heuristic-based approach to solving problems
    that would take too long to exhaustively process using deterministic methods.
    It is a stochastic search technique typically applied to combinatorial problems
    or in tandem with other methods to find an optimal solution quickly.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算法是一种基于启发式方法的问题解决方法，使用确定性方法耗时过长。它是一种随机搜索技术，通常应用于组合问题或与其他方法一起快速找到最优解。
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about optimization techniques, especially the ones
    used in machine learning that aim to find the most effective hyperparameter configuration
    for an ML model fitted to a dataset. An optimized ML model has minimum errors,
    thereby improving the accuracy of predictions. There would be no learning or development
    of models without optimization.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们了解了优化技术，尤其是那些用于机器学习的技术，旨在找到适合数据集的机器学习模型的最有效超参数配置。优化后的机器学习模型具有最小的错误，从而提高预测的准确性。没有优化，就没有模型的学习或发展。
- en: We touched upon optimization algorithms that are used in operations research,
    as well as evolutionary algorithms that find usage in the optimization of deep
    learning models and network modeling of more complex problems.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了在运筹学中使用的优化算法，以及在深度学习模型优化和更复杂问题的网络建模中应用的进化算法。
- en: In the final chapter of the book, we will learn about how standard techniques
    are selected to optimize ML models. Multiple optimal solutions may exist for a
    given problem and there may be multiple optimization techniques to arrive at them.
    Hence, it is essential to choose the technique carefully while building the model
    addressing the pertinent business question.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一章，我们将学习如何选择标准技术来优化机器学习模型。对于给定的问题，可能存在多个最优解，并且可能有多种优化技术可以达到这些解。因此，在构建模型并解决相关业务问题时，选择合适的技术至关重要。
