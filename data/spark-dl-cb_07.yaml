- en: Natural Language Processing with TF-IDF
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TF-IDF进行自然语言处理
- en: 'In this chapter, the following recipes will be covered:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，将涵盖以下内容：
- en: Downloading the therapy bot session text dataset
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载治疗机器人会话文本数据集
- en: Analyzing the therapy bot session dataset
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析治疗机器人会话数据集
- en: Visualizing word counts in the dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化数据集中的词频
- en: Calculating sentiment analysis of text
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算文本的情感分析
- en: Removing stop words from the text
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本中删除停用词
- en: Training the TF-IDF model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练TF-IDF模型
- en: Evaluating TF-IDF model performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估TF-IDF模型性能
- en: Comparing model performance to a baseline score
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型性能与基准分数进行比较
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '**Natural language processing** (**NLP**) is all over the news lately, and
    if you ask five different people, you will get ten different definitions. Recently
    NLP has been used to help identify bots or trolls on the internet trying to spread
    fake news or, even worse, tactics such as cyberbullying. In fact, recently there
    was a case in Spain where a student at a school was getting cyberbullied through
    social media accounts and it was having such a serious effect on the health of
    the student that the teachers started to get involved. The school reached out
    to researchers who were able to help identify several potential sources for the
    trolls using NLP methods such as TF-IDF. Ultimately, the list of potential students
    was presented to the school and when confronted the actual suspect admitted to
    being the perpetrator. The story was published in a paper titled *Supervised Machine
    Learning for the Detection of Troll Profiles in Twitter Social Network: Application
    to a Real Case of Cyberbullying* by Patxi Galan-Garcıa, Jose Gaviria de la Puerta,
    Carlos Laorden Gomez, Igor Santos, and Pablo Garcıa Bringas.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）最近成为新闻的焦点，如果你问五个不同的人，你会得到十个不同的定义。最近，NLP已被用于帮助识别互联网上试图传播假新闻或更糟的是欺凌行为的机器人或喷子。事实上，最近在西班牙发生了一起案件，一所学校的学生通过社交媒体账户遭到网络欺凌，这对学生的健康产生了严重影响，老师们开始介入。学校联系了研究人员，他们能够帮助识别使用TF-IDF等NLP方法的潜在喷子。最终，潜在的学生名单被提交给学校，当面对时，实际嫌疑人承认了自己的行为。这个故事发表在一篇名为《Twitter社交网络中喷子档案检测的监督机器学习：网络欺凌的真实案例应用》的论文中，作者是Patxi
    Galan-Garcıa、Jose Gaviria de la Puerta、Carlos Laorden Gomez、Igor Santos和Pablo
    Garcıa Bringas。
- en: 'This paper highlights the ability to utilize several varying methods to analyze
    text and develop human-like language processing. It is this methodology that incorporates
    NLP into machine learning, deep learning, and artificial intelligence. Having
    machines able to ingest text data and potentially make decisions from that same
    text data is the core of natural language processing. There are many algorithms
    that are used for NLP, such as the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点介绍了利用多种不同方法分析文本和开发类似人类语言处理的能力。正是这种方法将自然语言处理（NLP）融入到机器学习、深度学习和人工智能中。让机器能够摄取文本数据并可能从同样的文本数据中做出决策是自然语言处理的核心。有许多用于NLP的算法，例如以下内容：
- en: TF-IDF
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Word2Vec
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec
- en: N-grams
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N-gram
- en: Latent Dirichlet allocation (LDA)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（LDA）
- en: Long short-term memory (LSTM)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）
- en: This chapter will focus on a dataset that contains conversations between an
    individual and a chatbot from an online therapy website. The purpose of the chatbot
    is to recognize conversations that need to be flagged for immediate attention
    to an individual rather than continued discussion with the chatbot. Ultimately,
    we will focus on using a TF-IDF algorithm to perform text analysis on the dataset
    to determine whether the chat conversation warrants a classification that needs
    to be escalated to an individual or not. **TF-IDF** stands for **Term Frequency-Inverse
    Document Frequency**. This is a technique commonly used in algorithms to identify
    the importance of a word in a document. Additionally, TF-IDF is easy to compute
    especially when dealing with high word counts in documents and has the ability
    to measure the uniqueness of a word. This comes in handy when dealing with a chatbot
    data. The main goal is to quickly identify a unique word that would trigger escalation
    to an individual to provide immediate support.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将专注于一个包含个人与在线治疗网站聊天机器人之间对话的数据集。聊天机器人的目的是识别需要立即引起个人关注而不是继续与聊天机器人讨论的对话。最终，我们将专注于使用TF-IDF算法对数据集进行文本分析，以确定聊天对话是否需要被升级到个人的分类。TF-IDF代表词项频率-逆文档频率。这是一种常用的算法技术，用于识别文档中单词的重要性。此外，TF-IDF在处理文档中的高词频时易于计算，并且能够衡量单词的独特性。在处理聊天机器人数据时，这非常有用。主要目标是快速识别一个唯一的单词，触发升级到个人以提供即时支持。
- en: Downloading the therapy bot session text dataset
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载治疗机器人会话文本数据集
- en: This section will focus on downloading and setting up the dataset that will
    be used for NLP in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将重点介绍下载和设置本章中用于NLP的数据集。
- en: Getting ready
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The dataset that we will use in this chapter is based on interactions between
    a therapy bot and visitors to an online therapy website. It contains 100 interactions
    and each interaction is tagged as either `escalate` or `do_not_escalate`. If the
    discussion warrants a more serious conversation, the bot will tag the discussion
    as `escalate` to an individual. Otherwise, the bot will continue the discussion
    with the user.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用基于治疗机器人与在线治疗网站访客之间的互动的数据集。它包含100个互动，每个互动都被标记为“升级”或“不升级”。如果讨论需要更严肃的对话，机器人将会将讨论标记为“升级”给个人。否则，机器人将继续与用户讨论。
- en: How it works...
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section walks through the steps for downloading the chatbot data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍下载聊天机器人数据的步骤。
- en: Access the dataset from the following GitHub repository: [https://github.com/asherif844/ApacheSparkDeepLearningCookbook/tree/master/CH07/data](https://github.com/asherif844/ApacheSparkDeepLearningCookbook/tree/master/CH07/data)
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下 GitHub 存储库访问数据集：[https://github.com/asherif844/ApacheSparkDeepLearningCookbook/tree/master/CH07/data](https://github.com/asherif844/ApacheSparkDeepLearningCookbook/tree/master/CH07/data)
- en: 'Once you arrive at the repository, right-click on the file seen in the following
    screenshot:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您到达存储库，右键单击以下截图中看到的文件：
- en: '![](img/00198.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00198.jpeg)'
- en: Download `TherapyBotSession.csv` and save to the same local directory as the
    Jupyter notebook `SparkSession`.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 `TherapyBotSession.csv` 并保存到与 Jupyter 笔记本 `SparkSession` 相同的本地目录中。
- en: 'Access the dataset through the Jupyter notebook using the following script
    to build the `SparkSession` called `spark`, as well as to assign the dataset to
    a dataframe in Spark, called `df`:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下脚本在 Jupyter 笔记本中访问数据集，构建名为 `spark` 的 `SparkSession`，并将数据集分配给 Spark 中的数据框
    `df`：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: This section explains how the chatbot data makes its way into our Jupyter notebook.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了聊天机器人数据如何进入我们的 Jupyter 笔记本。
- en: 'The contents of the dataset can be seen by clicking on TherapyBotSession.csv
    on the repository as seen in the following screenshot:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集的内容可以通过点击存储库中的 TherapyBotSession.csv 查看，如下截图所示：
- en: '![](img/00199.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00199.jpeg)'
- en: 'Once the dataset is downloaded, it can be uploaded and converted into a dataframe,
    `df`. The dataframe can be viewed by executing `df.show()`, as seen in the following
    screenshot:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据集被下载，它可以被上传并转换为一个名为 `df` 的数据框。可以通过执行 `df.show()` 来查看数据框，如下截图所示：
- en: '![](img/00200.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00200.jpeg)'
- en: 'There are 3 main fields that are of particular interest to us from the dataframe:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有 3 个主要字段对我们来说特别感兴趣：
- en: '`id`: the unique id of each transaction between a visitor to the website and
    the chatbot.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`id`：网站访问者和聊天机器人之间每笔交易的唯一标识。'
- en: '`label`: since this is a supervised modeling approach where we know the outcome
    that we are trying to predict, each transaction has been classified as either
    `escalate` or `do_not_escalate`. This field will be used during the modeling process
    to train the text to identify words that would classify falling under one of these
    two scenarios.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`label`：由于这是一种监督建模方法，我们知道我们要预测的结果，每个交易都被分类为 `escalate` 或 `do_not_escalate`。在建模过程中，将使用该字段来训练文本以识别属于这两种情况之一的单词。'
- en: '`chat`: lastly we have the `chat` text from the visitor on the website that
    our model will classify.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`chat`：最后我们有来自网站访问者的 `chat` 文本，我们的模型将对其进行分类。'
- en: There's more...
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The dataframe, `df`, has some additional columns, `_c3`, `_c4`, `_c5`, and
    `_c6` that will not be used in the model and therefore, can be excluded from the
    dataset using the following script:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框 `df` 还有一些额外的列 `_c3`、`_c4`、`_c5` 和 `_c6`，这些列将不会在模型中使用，因此可以使用以下脚本从数据集中排除。
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output of the script can be seen in the following screenshot:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的输出可以在以下截图中看到：
- en: '![](img/00201.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00201.jpeg)'
- en: Analyzing the therapy bot session dataset
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析治疗机器人会话数据
- en: It is always important to first analyze any dataset before applying models on
    that same dataset
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用模型之前，始终先分析任何数据集是很重要的
- en: Getting ready
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This section will require importing `functions` from `pyspark.sql` to be performed
    on our dataframe.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分将需要从 `pyspark.sql` 导入 `functions` 来在我们的数据框上执行。
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How to do it...
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: The following section walks through the steps to profile the text data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将介绍对文本数据进行分析的步骤。
- en: 'Execute the following script to group the `label` column and to generate a
    count distribution:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本来对 `label` 列进行分组并生成计数分布：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Add a new column, `word_count`, to the dataframe, `df`, using the following
    script:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本向数据框 `df` 添加一个新列 `word_count`：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Aggregate the average word count, `avg_word_count`, by `label` using the following
    script:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本按 `label` 聚合平均单词计数 `avg_word_count`：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The following section explains the feedback obtained from analyzing the text
    data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分解释了分析文本数据所获得的反馈。
- en: 'It is useful to collect data across multiple rows and group the results by
    a dimension. In this case, the dimension is `label`. A `df.groupby()` function
    is used to measure the count of 100 therapy transactions online distributed by `label`.
    We can see that there is a `65`:`35` distribution of `do_not_escalate` to `escalate`
    as seen in the following screenshot:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集跨多行的数据并按维度对结果进行分组是很有用的。在这种情况下，维度是 `label`。使用 `df.groupby()` 函数来测量按 `label`
    分布的 100 笔在线治疗交易的计数。我们可以看到 `do_not_escalate` 到 `escalate` 的分布是 `65`：`35`，如下截图所示：
- en: '![](img/00202.jpeg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00202.jpeg)'
- en: 'A new column, `word_count`, is created to calculate how many words are used
    in each of the 100 transactions between the chatbot and the online visitor. The
    newly created column, `word_count`, can be seen in the following screenshot:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新列 `word_count`，用于计算聊天机器人和在线访问者之间的 100 笔交易中每笔交易使用了多少单词。新创建的列 `word_count`
    可以在以下截图中看到：
- en: '![](img/00203.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00203.jpeg)'
- en: 'Since the `word_count` is now added to the dataframe, it can be aggregated
    to calculate the average word count by `label`. Once this is performed, we can
    see that `escalate` conversations on average are more than twice as long as `do_not_escalate`
    conversations, as seen in the following screenshot:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于现在在数据框中添加了 `word_count`，可以对其进行聚合以计算按 `label` 的平均单词计数。一旦执行了这个操作，我们可以看到 `escalate`
    对话的平均长度是 `do_not_escalate` 对话的两倍多，如下截图所示：
- en: '![](img/00204.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00204.jpeg)'
- en: Visualizing word counts in the dataset
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化数据集中的单词计数
- en: A picture is worth a thousand words and this section will set out to prove that.
    Unfortunately, Spark does not have any inherent plotting capabilities as of version
    2.2\. In order to plot values in a dataframe, we must convert to `pandas`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一张图片胜过千言万语，本节将证明这一点。不幸的是，截至版本 2.2，Spark 没有任何内在的绘图能力。为了在数据框中绘制值，我们必须转换为 `pandas`。
- en: Getting ready
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This section will require importing `matplotlib` for plotting:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将需要导入`matplotlib`进行绘图：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How to do it...
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: This section walks through the steps to convert the Spark dataframe into a visualization
    that can be seen in the Jupyter notebook.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍将Spark数据框转换为可以在Jupyter笔记本中查看的可视化的步骤。
- en: 'Convert Spark dataframe to a `pandas` dataframe using the following script:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将Spark数据框转换为`pandas`数据框：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Plot the dataframe using the following script:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本绘制数据框：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How it works...
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: This section explains how the Spark dataframe is converted to `pandas` and then
    plotted.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了如何将Spark数据框转换为`pandas`，然后绘制。
- en: A subset of the Spark dataframe is collected and converted to `pandas` using
    the `toPandas()` method in Spark.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Spark中收集数据框的子集，并使用Spark中的`toPandas()`方法转换为`pandas`。
- en: 'That subset of data is then plotted using matplotlib setting the y-values to
    be `word_count` and the x-values to be the `id` as seen in the following screenshot:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用matplotlib绘制数据的子集，将y值设置为`word_count`，将x值设置为`id`，如下面的屏幕截图所示：
- en: '![](img/00205.jpeg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00205.jpeg)'
- en: See also
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: There are other plotting capabilities in Python other than `matplotlib` such
    as `bokeh`, `plotly`, and `seaborn`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Python中除了`matplotlib`之外还有其他绘图功能，例如`bokeh`、`plotly`和`seaborn`。
- en: 'To learn more about `bokeh`, visit the following website:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关`bokeh`的更多信息，请访问以下网站：
- en: '[https://bokeh.pydata.org/en/latest/](https://bokeh.pydata.org/en/latest/)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bokeh.pydata.org/en/latest/](https://bokeh.pydata.org/en/latest/)'
- en: 'To learn more about `plotly`, visit the following website:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关`plotly`的更多信息，请访问以下网站：
- en: '[https://plot.ly/](https://plot.ly/)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://plot.ly/](https://plot.ly/)'
- en: 'To learn more about `seaborn`, visit the following website:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关`seaborn`的更多信息，请访问以下网站：
- en: '[https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
- en: Calculating sentiment analysis of text
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算文本的情感分析
- en: Sentiment analysis is the ability to derive tone and feeling behind a word or
    series of words. This section will utilize techniques in python to calculate a
    sentiment analysis score from the 100 transactions in our dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是从单词或一系列单词中推导出语气和感觉的能力。本节将利用Python技术从数据集中的100个交易中计算情感分析分数。
- en: Getting ready
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This section will require using functions and data types within PySpark. Additionally,
    we well importing the `TextBlob` library for sentiment analysis. In order to use
    SQL and data type functions within PySpark, the following must be imported:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将需要在PySpark中使用函数和数据类型。此外，我们还将导入`TextBlob`库进行情感分析。为了在PySpark中使用SQL和数据类型函数，必须导入以下内容：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Additionally, in order to use `TextBlob`, the following library must be imported:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了使用`TextBlob`，必须导入以下库：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How to do it...
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: The following section walks through the steps to apply sentiment score to the
    dataset.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将介绍将情感分数应用于数据集的步骤。
- en: 'Create a sentiment score function, `sentiment_score`, using the following script:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本创建情感分数函数`sentiment_score`：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Apply `sentiment_score` to each conversation response in the dataframe using
    the following script:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本在数据框中的每个对话响应上应用`sentiment_score`：
- en: 'Create a `lambda` function, called `sentiment_score_udf`, that maps `sentiment_score`
    into a user-defined function within Spark, `udf`, to each transaction and specifies
    the output type of `FloatType()` as seen in the following script:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`sentiment_score_udf`的`lambda`函数，将`sentiment_score`映射到Spark中的用户定义函数`udf`，并指定`FloatType()`的输出类型，如下脚本所示：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Apply the function, `sentiment_score_udf`, to each `chat` column in the dataframe
    as seen in the following script:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据框中的每个`chat`列上应用函数`sentiment_score_udf`，如下脚本所示：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Calculate the average sentiment score, `avg_sentiment_score`, by `label` using
    the following script:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本计算按`label`分组的平均情感分数`avg_sentiment_score`：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works...
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: This section explains how a Python function is converted into a user-defined
    function, `udf`, within Spark to apply a sentiment analysis score to each column
    in the dataframe.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了如何将Python函数转换为Spark中的用户定义函数`udf`，以将情感分析分数应用于数据框中的每一列。
- en: '`Textblob` is a sentiment analysis library in Python. It can calculate the
    sentiment score from a method called `sentiment.polarity` that is scored from
    -1 (very negative) to +1 (very positive) with 0 being neutral. Additionally, `Textblob`
    can measure subjectivity from 0 (very objective) to 1 (very subjective); although,
    we will not be measuring subjectivity in this chapter.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Textblob`是Python中的情感分析库。它可以从名为`sentiment.polarity`的方法中计算情感分数，该方法的得分范围为-1（非常负面）到+1（非常正面），0为中性。此外，`Textblob`还可以从0（非常客观）到1（非常主观）测量主观性；尽管在本章中我们不会测量主观性。'
- en: 'There are a couple of steps to applying a Python function to Spark dataframe:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Python函数应用于Spark数据框有几个步骤：
- en: '`Textblob` is imported and a function called `sentiment_score` is applied to
    the `chat` column to generate the sentiment polarity of each bot conversation
    in a new column, also called `sentiment_score`.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`Textblob`并将名为`sentiment_score`的函数应用于`chat`列，以生成每个机器人对话的情感极性，并在新列中生成情感分数，也称为`sentiment_score`。
- en: A Python function cannot be directly applied to a Spark dataframe without first
    going through a user-defined function transformation, `udf`, within Spark.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Python函数不能直接应用于Spark数据框，而必须先经过用户定义函数转换`udf`，然后在Spark中应用。
- en: Additionally, the output of the function must also be explicitly stated, whether
    it be an integer or float data type. In our situation, we explicitly state that
    the output of the function will be using the `FloatType() from pyspark.sql.types`.
    Finally, the sentiment is applied across each row using a `lambda` function within
    the `udf` sentiment score function, called `sentiment_score_udf`.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，函数的输出也必须明确说明，无论是整数还是浮点数据类型。在我们的情况下，我们明确说明函数的输出将使用`FloatType() from pyspark.sql.types`。最后，使用`udf`情感分数函数内的`lambda`函数在每行上应用情感。
- en: 'The updated dataframe with the newly created field,`sentiment score`, can be
    seen by executing `df.show()`, as shown in the following screenshot:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行`df.show()`，可以看到具有新创建字段`情感分数`的更新后的数据框，如下截屏所示：
- en: '![](img/00206.jpeg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00206.jpeg)'
- en: 'Now that the `sentiment_score` is calculated for each response from the chat
    conversation, we can denote a value range of -1 (very negative polarity) to +1
    (very positive polarity) for each row. Just as we did with counts and average
    word count, we can compare whether `escalate` conversations are more positive
    or negative in sentiment than `do_not_escalate` conversations on average. We can
    calculate an average sentiment score, `avg_sentiment_score`, by `label` as seen
    in the following screenshot:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，对于聊天对话中的每个响应计算了`sentiment_score`之后，我们可以为每行指定-1（非常负面的极性）到+1（非常正面的极性）的值范围。就像我们对计数和平均词数所做的那样，我们可以比较`升级`对话在情感上是否比`不升级`对话更积极或更消极。我们可以通过`label`计算平均情感分数`avg_sentiment_score`，如下截屏所示：
- en: '![](img/00207.jpeg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00207.jpeg)'
- en: Initially, it would make sense to assume that `escalate` conversations would
    be more negative from a polarity score than `do_not_escalate`. We actually find
    that `escalate` is slightly more positive in polarity than `do_not_escalate`;
    however, both are pretty neutral as they are close to 0.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初，假设`升级`对话的极性得分会比`不升级`更负面是有道理的。实际上，我们发现`升级`在极性上比`不升级`稍微更积极；但是，两者都相当中性，因为它们接近0。
- en: See also
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'To learn more about the `TextBlob` library, visit the following website:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关`TextBlob`库的更多信息，请访问以下网站：
- en: '[http://textblob.readthedocs.io/en/dev/](http://textblob.readthedocs.io/en/dev/)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://textblob.readthedocs.io/en/dev/](http://textblob.readthedocs.io/en/dev/)'
- en: Removing stop words from the text
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文本中删除停用词
- en: A stop word is a very common word used in the English language and is often
    removed from common NLP techniques because they can be distracting. Common stop
    word would be words such as *the* or *and*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是英语中非常常见的单词，通常会从常见的NLP技术中删除，因为它们可能会分散注意力。常见的停用词可能是诸如*the*或*and*之类的单词。
- en: Getting ready
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This section requires importing the following libraries:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 本节需要导入以下库：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: How to do it...
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: This section walks through the steps to remove stop words.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了删除停用词的步骤。
- en: 'Execute the following script to extract each word in `chat` into a string within
    an array:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本，将`chat`中的每个单词提取为数组中的字符串：
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Assign a list of common words to a variable, `stop_words`, that will be considered
    stop words using the following script:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将一组常见单词分配给变量`stop_words`，这些单词将被视为停用词：
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Execute the following script to import the `StopWordsRemover` function from
    PySpark and configure the input and output columns, `words` and `word without
    stop`:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本，从PySpark导入`StopWordsRemover`函数，并配置输入和输出列`words`和`word without stop`：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Execute the following script to import Pipeline and define the `stages` for
    the stop word transformation process that will be applied to the dataframe:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本以导入Pipeline并为将应用于数据框的停用词转换过程定义`stages`：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, apply the stop word removal transformation, `pipelineFitRemoveStopWords`,
    to the dataframe, `df`, using the following script:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用以下脚本将停用词移除转换`pipelineFitRemoveStopWords`应用于数据框`df`：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How it works...
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: This section explains how to remove stop words from the text.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了如何从文本中删除停用词。
- en: Just as we did by applying some analysis when profiling and exploring the `chat` data,
    we can also tweak the text of the `chat` conversation and break up each word into
    a separate array. This will be used to isolate stop words and remove them.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像我们在对`chat`数据进行分析时一样，我们也可以调整`chat`对话的文本，并将每个单词分解为单独的数组。这将用于隔离停用词并将其删除。
- en: 'The new column with each word extracted as a string is called `words` and can
    be seen in the following screenshot:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个单词提取为字符串的新列称为`words`，可以在以下截屏中看到：
- en: '![](img/00208.jpeg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00208.jpeg)'
- en: There are many ways to assign a group of words to a stop word list. Some of
    these words can be automatically downloaded and updated using a proper Python
    library called `nltk`, which stands for natural language toolkit. For our purposes,
    we will utilize a common list of 124 stop words to generate our own list. Additional
    words can be easily added or removed from the list manually.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有许多方法可以将一组单词分配给停用词列表。其中一些单词可以使用适当的Python库`nltk`（自然语言工具包）自动下载和更新。对于我们的目的，我们将利用一个常见的124个停用词列表来生成我们自己的列表。可以轻松地手动添加或从列表中删除其他单词。
- en: Stop words do not add any value to the text and will be removed from the newly
    created column by specifying `outputCol="words without stop"`. Additionally, the
    column that will serve as the source for the transformation is set by specifying `inputCol
    = "words"`.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停用词不会为文本增添任何价值，并且将通过指定`outputCol="words without stop"`从新创建的列中删除。此外，通过指定`inputCol
    = "words"`来设置将用作转换源的列。
- en: We create a pipeline, `stopWordRemovalPipeline`, to define the sequence of steps
    or `stages` that will transform the data. In this situation, the only stage that
    will be used to transform the data is the feature, `stopwordsRemover`.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个管道，`stopWordRemovalPipeline`，来定义将转换数据的步骤或`阶段`的顺序。在这种情况下，唯一用于转换数据的阶段是特征`stopwordsRemover`。
- en: 'Each stage in a pipeline can have a transforming role and an estimator role.
    The estimator role, `pipeline.fit(df)`, is called on to produce a transformer
    function called `pipelineFitRemoveStopWords`. Finally, the `transform(df)` function
    is called on the dataframe to produce an updated dataframe with a new column called `words
    without stop`. We can compare both columns side by side to examine the differences
    as seen in the following screenshot:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道中的每个阶段都可以具有转换角色和估计角色。估计角色`pipeline.fit(df)`用于生成名为`pipelineFitRemoveStopWords`的转换器函数。最后，在数据框上调用`transform(df)`函数，以生成具有名为`words
    without stop`的新列的更新后的数据框。我们可以将两列并排比较以查看差异，如下截屏所示：
- en: '![](img/00209.jpeg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00209.jpeg)'
- en: The new column, `words without stop`, contains none of the strings that are
    considered stop words from the original column, `words`.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新列`words without stop`不包含原始列`words`中被视为停用词的任何字符串。
- en: See also
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'To learn more about stop words from `nltk`, visit the following website:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关`nltk`的停用词的更多信息，请访问以下网站：
- en: '[https://www.nltk.org/data.html](https://www.nltk.org/data.html)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.nltk.org/data.html](https://www.nltk.org/data.html)'
- en: 'To learn more about Spark machine learning pipelines, visit the following website:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于Spark机器学习管道的信息，请访问以下网站：
- en: '[https://spark.apache.org/docs/2.2.0/ml-pipeline.html](https://spark.apache.org/docs/2.2.0/ml-pipeline.html)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/2.2.0/ml-pipeline.html](https://spark.apache.org/docs/2.2.0/ml-pipeline.html)'
- en: 'To learn more about the `StopWordsRemover` feature in PySpark, visit the following
    website:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解PySpark中`StopWordsRemover`功能的更多信息，请访问以下网站：
- en: '[https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover)'
- en: Training the TF-IDF model
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练TF-IDF模型
- en: We are now ready to train our TF-IDF NLP model and see if we can classify these
    transactions as either `escalate` or `do_not_escalate`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备训练我们的TF-IDF NLP模型，并查看是否可以将这些交易分类为`升级`或`不升级`。
- en: Getting ready
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This section will require importing from `spark.ml.feature` and `spark.ml.classification`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将需要从`spark.ml.feature`和`spark.ml.classification`导入。
- en: How to do it...
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: The following section walks through the steps to train the TF-IDF model.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将逐步介绍训练TF-IDF模型的步骤。
- en: 'Create a new user-defined function, `udf`, to define numerical values for the `label`
    column using the following script:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的用户定义函数`udf`，使用以下脚本为`label`列定义数值：
- en: '[PRE21]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Execute the following script to set the TF and IDF columns for the vectorization
    of the words:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本以设置单词向量化的TF和IDF列：
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Set up a pipeline, `pipelineTFIDF`, to set the sequence of stages for `TF_` and `IDF_` using
    the following script:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本设置管道`pipelineTFIDF`，以设置`TF_`和`IDF_`的阶段顺序：
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Fit and transform the IDF estimator onto the dataframe, `df`, using the following
    script:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将IDF估计器拟合到数据框`df`上：
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Split the dataframe into a 75:25 split for model evaluation purposes using
    the following script:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将数据框拆分为75:25的比例，用于模型评估目的：
- en: '[PRE25]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Import and configure a classification model, `LogisticRegression`, using the
    following script:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本导入和配置分类模型`LogisticRegression`：
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Fit the logistic regression model, `logreg`, onto the training dataframe, `trainingDF.` A
    new dataframe, `predictionDF`, is created based on the `transform()` method from
    the logistic regression model, as seen in the following script:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将逻辑回归模型`logreg`拟合到训练数据框`trainingDF`上。基于逻辑回归模型的`transform()`方法，创建一个新的数据框`predictionDF`，如下脚本所示：
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: How it works...
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The following section explains to effectively train a TF-IDF NLP model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分解释了如何有效地训练TF-IDF NLP模型。
- en: 'It is ideal to have labels in a numerical format rather than a categorical
    form as the model is able to interpret numerical values while classifying outputs
    between 0 and 1. Therefore, all labels under the `label` column are converted
    to a numerical `label` of 0.0 or 1.0, as seen in the following screenshot:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最好将标签以数值格式而不是分类形式呈现，因为模型能够在将输出分类为0和1之间时解释数值。因此，`label`列下的所有标签都转换为0.0或1.0的数值`label`，如下截图所示：
- en: '![](img/00210.jpeg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00210.jpeg)'
- en: TF-IDF models require a two-step approach by importing both `HashingTF` and
    `IDF` from `pyspark.ml.feature` to handle separate tasks. The first task merely
    involves importing both `HashingTF` and `IDF` and assigning values for the input
    and subsequent output columns. The `numfeatures` parameter is set to 100,000 to
    ensure that it is larger than the distinct number of words in the dataframe. If
    `numfeatures` were to be than the distinct word count, the model would be inaccurate.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TF-IDF模型需要通过从`pyspark.ml.feature`导入`HashingTF`和`IDF`来进行两步处理，以处理不同的任务。第一个任务仅涉及导入`HashingTF`和`IDF`并为输入和随后的输出列分配值。`numfeatures`参数设置为100,000，以确保它大于数据框中单词的不同数量。如果`numfeatures`小于不同的单词计数，模型将不准确。
- en: As stated earlier, each step of the pipeline contains a transformation process
    and an estimator process. The pipeline, `pipelineTFIDF`, is configured to order
    the sequence of steps where `IDF` will follow `HashingTF`.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，管道的每个步骤都包含一个转换过程和一个估计器过程。管道`pipelineTFIDF`被配置为按顺序排列步骤，其中`IDF`将跟随`HashingTF`。
- en: '`HashingTF` is used to transform the `words without stop` into vectors within
    a new column called `rawFeatures`. Subsequently, `rawFeatures` will then be consumed
    by `IDF` to estimate the size and fit the dataframe to produce a new column called `features`,
    as seen in the following screenshot:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`HashingTF`用于将`words without stop`转换为新列`rawFeatures`中的向量。随后，`rawFeatures`将被`IDF`消耗，以估算大小并适应数据框以生成名为`features`的新列，如下截图所示：'
- en: '![](img/00211.jpeg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00211.jpeg)'
- en: For training purposes, our dataframe will be conservatively split into a `75`:`25`
    ratio with a random seed set at `1234`.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了培训目的，我们的数据框将以`75`:`25`的比例保守地拆分，随机种子设置为`1234`。
- en: Since our main goal is to classify each conversation as either `escalate` for
    escalation or `do_not_escalate` for continued bot chat, we can use a traditional
    classification algorithm such as a logistic regression model from the PySpark
    library. The logistic regression model is configured with a regularization parameter,
    `regParam`, of 0.025\. We use the parameter to slightly improve the model by minimizing
    overfitting at the expense of a little bias.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的主要目标是将每个对话分类为`升级`以进行升级或`不升级`以进行继续的机器人聊天，因此我们可以使用PySpark库中的传统分类算法，如逻辑回归模型。逻辑回归模型配置了正则化参数`regParam`为0.025。我们使用该参数略微改进模型，以最小化过度拟合，代价是略微偏差。
- en: 'The logistic regression model is trained and fitted on `trainingDF`, and then
    a new dataframe, `predictionDF`, is created with the newly transformed field, `prediction`,
    as seen in the following screenshot:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逻辑回归模型在`trainingDF`上进行训练和拟合，然后创建一个新的数据框`predictionDF`，其中包含新转换的字段`prediction`，如下截图所示：
- en: '![](img/00212.jpeg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00212.jpeg)'
- en: There's more...
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: While we did use the user-defined function, `udf`, to manually create a numerical
    label column, we also could have used a built-in feature from PySpark called `StringIndexer`
    to assign numerical values to categorical labels. To see `StringIndexer` in action,
    visit [Chapter 5](part0211.html#6976M0-3be7262ff9a54db3b2ea862fdce1797b), *Predicting
    Fire Department Calls with Spark ML*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们确实使用了用户定义的函数`udf`来手动创建一个数值标签列，但我们也可以使用PySpark的内置功能`StringIndexer`来为分类标签分配数值。要查看`StringIndexer`的操作，请访问[第5章](part0211.html#6976M0-3be7262ff9a54db3b2ea862fdce1797b)，*使用Spark
    ML预测消防部门呼叫*。
- en: See also
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'To learn more about the TF-IDF model within PySpark, visit the following website:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关PySpark中TF-IDF模型的更多信息，请访问以下网站：
- en: '[https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf](https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf](https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf)'
- en: Evaluating TF-IDF model performance
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估TF-IDF模型性能
- en: At this point, we are ready to evaluate our model's performance
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已准备好评估我们模型的性能
- en: Getting ready
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This section will require importing the following libraries:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将需要导入以下库：
- en: '`metrics` from `sklearn`'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自`sklearn`的`metrics`
- en: '`BinaryClassificationEvaluator` from `pyspark.ml.evaluation`'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pyspark.ml.evaluation`中的`BinaryClassificationEvaluator`'
- en: How to do it...
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: This section walks through the steps to evaluate the TF-IDF NLP model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了评估TF-IDF NLP模型的步骤。
- en: 'Create a confusion matrix using the following script:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本创建混淆矩阵：
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Evaluate the model using `metrics` from sklearn with the following script:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本从`sklearn`评估模型的`metrics`：
- en: '[PRE29]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Calculate the ROC score using the following script:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本计算ROC分数：
- en: '[PRE30]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: How it works...
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section explains how we use the evaluation calculations to determine the
    accuracy of our model.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了我们如何使用评估计算来确定模型的准确性。
- en: 'A confusion matrix is helpful to quickly summarize the accuracy numbers between
    actual results and predicted results. Since we had a 75:25 split, we should see
    25 predictions from our training dataset. We can build a build a confusion matric
    using the following script: `predictionDF.crosstab(''label'', ''prediction'').show()`.
    The output of the script can be seen in the following screenshot:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混淆矩阵有助于快速总结实际结果和预测结果之间的准确性数字。由于我们有75:25的分割，我们应该从训练数据集中看到25个预测。我们可以使用以下脚本构建混淆矩阵：`predictionDF.crosstab('label',
    'prediction').show()`。脚本的输出可以在以下截图中看到：
- en: '![](img/00213.jpeg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00213.jpeg)'
- en: We are now at the stage of evaluating the accuracy of the model by comparing
    the `prediction` values against the actual `label` values. `sklearn.metrics` intakes
    two parameters, the `actual` values tied to the `label` column, as well as the
    `predicted` values derived from the logistic regression model.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在处于通过比较`prediction`值和实际`label`值来评估模型准确度的阶段。`sklearn.metrics`接受两个参数，与`label`列相关联的`actual`值，以及从逻辑回归模型派生的`predicted`值。
- en: Please note that once again we are converting the column values from Spark dataframes
    to pandas dataframes using the `toPandas()` method.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们再次将Spark数据框的列值转换为pandas数据框，使用`toPandas()`方法。
- en: 'Two variables are created, `actual` and `predicted`, and an accuracy score
    of 91.7% is calculated using the `metrics.accuracy_score()` function, as seen
    in the following screenshot:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了两个变量`actual`和`predicted`，并使用`metrics.accuracy_score()`函数计算了91.7%的准确度分数，如下截图所示：
- en: '![](img/00214.jpeg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00214.jpeg)'
- en: 'The ROC (Receiver Operating Characteristic) is often associated with a curve
    measuring the true positive rate against the false positive rate. The greater
    the area under the curve, the better. The ROC score associated with the curve
    is another indicator that can be used to measure the performance of the model.
    We can calculate the `ROC` using the `BinaryClassificationEvaluator` as seen in
    the following screenshot:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ROC（接收器操作特性）通常与测量真正率相对于假正率的曲线相关联。曲线下面积越大，越好。与曲线相关的ROC分数是另一个指标，可用于衡量模型的性能。我们可以使用`BinaryClassificationEvaluator`计算`ROC`，如下截图所示：
- en: '![](img/00215.jpeg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00215.jpeg)'
- en: See also
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'To learn more about the `BinaryClassificationEvaluator` from PySpark, visit
    the following website:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关PySpark中的`BinaryClassificationEvaluator`的更多信息，请访问以下网站：
- en: '[https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html](https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html](https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html)'
- en: Comparing model performance to a baseline score
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型性能与基线分数进行比较
- en: While it is great that we have a high accuracy score from our model of 91.7
    percent, it is also important to compare this to a baseline score. We dig deeper
    into this concept in this section.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的模型具有91.7%的高准确度分数，这很好，但将其与基线分数进行比较也很重要。我们在本节中深入探讨了这个概念。
- en: How to do it...
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: This section walks through the steps to calculate the baseline accuracy.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了计算基线准确度的步骤。
- en: 'Execute the following script to retrieve the mean value from the `describe()`
    method:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本以从`describe()`方法中检索平均值：
- en: '[PRE31]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Subtract `1- mean value score` to calculate baseline accuracy.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减去`1-平均值分数`以计算基线准确度。
- en: How it works...
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section explains the concept behind the baseline accuracy and how we can
    use it to understand the effectiveness of our model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了基线准确度背后的概念，以及我们如何使用它来理解模型的有效性。
- en: What if every `chat` conversation was flagged for `do_not_escalate` or vice
    versa. Would we have a baseline accuracy higher than 91.7 percent? The easiest
    way to figure this out is to run the `describe()` method on the `label` column
    from `predictionDF` using the following script: `predictionDF.describe('label').show()`
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果每个`chat`对话都被标记为`do_not_escalate`或反之亦然，我们是否会有高于91.7％的基准准确率？找出这一点最简单的方法是使用以下脚本在`predictionDF`的`label`列上运行`describe()`方法：`predictionDF.describe('label').show()`
- en: 'The output of the script can be seen in the following screenshot:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以在以下截图中看到脚本的输出：
- en: '![](img/00216.jpeg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00216.jpeg)'
- en: The mean of `label` is at 0.2083 or ~21%, which means that a `label` of 1 occurs
    only 21% of the time. Therefore, if we labeled each conversation as `do_not_escalate`,
    we would be correct ~79% of the time, which is less than our model accuracy of
    91.7%.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`label`的平均值为0.2083或约21％，这意味着`label`为1的情况仅发生了21％的时间。因此，如果我们将每个对话标记为`do_not_escalate`，我们将有大约79％的准确率，这低于我们的模型准确率91.7％。'
- en: Therefore, we can say that our model performs better than a blind baseline performance
    model.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们可以说我们的模型表现比盲目基准性能模型更好。
- en: See also
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'To learn more about the `describe()` method in a PySpark dataframe, visit the
    following website:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解PySpark数据框中`describe()`方法的更多信息，请访问以下网站：
- en: '[http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe](http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe](http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe)'
