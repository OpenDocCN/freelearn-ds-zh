- en: Advanced Machine Learning Best Practices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级机器学习最佳实践
- en: '"Hyperparameter optimization or model selection is the problem of choosing
    a set of hyperparameters [when defined as?] for a learning algorithm, usually
    with the goal of optimizing a measure of the algorithm''s performance on an independent
    dataset."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “超参数优化或模型选择是选择学习算法的一组超参数[何时定义为？]的问题，通常目标是优化算法在独立数据集上的性能度量。”
- en: '- Machine learning model tuning quote'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 机器学习模型调整报价'
- en: In this chapter, we will provide some theoretical and practical aspects of some
    advanced topics of machine learning (ML) with Spark. We will see how to tune machine
    learning models for better and optimized performance using grid search, cross-validation
    and hyperparameter tuning. In the later section, we will cover how to develop
    a scalable recommendation system using the ALS, which is an example of a model-based
    recommendation algorithm. Finally, a topic modeling application as a text clustering
    technique will be demonstrated.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将提供一些关于使用Spark进行机器学习（ML）的一些高级主题的理论和实践方面。我们将看到如何使用网格搜索、交叉验证和超参数调整来调整机器学习模型，以获得更好和优化的性能。在后面的部分，我们将介绍如何使用ALS开发可扩展的推荐系统，这是一个基于模型的推荐算法的示例。最后，将演示一种文本聚类技术作为主题建模应用。
- en: 'In a nutshell, we will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章中我们将涵盖以下主题：
- en: Machine learning best practice
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习最佳实践
- en: Hyperparameter tuning of ML models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML模型的超参数调整
- en: Topic modeling using latent dirichlet allocation (LDA)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用潜在狄利克雷分配（LDA）进行主题建模
- en: A recommendation system using collaborative filtering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用协同过滤的推荐系统
- en: Machine learning best practices
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习最佳实践
- en: 'Sometimes, it is recommended to consider the error rate rather than only the
    accuracy. For example, suppose an ML system with 99% accuracy and 50% errors is
    worse than the one with 90% accuracy but 25% errors. So far, we have discussed
    the following machine learning topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，建议考虑错误率而不仅仅是准确性。例如，假设一个ML系统的准确率为99%，错误率为50%，比一个准确率为90%，错误率为25%的系统更差。到目前为止，我们已经讨论了以下机器学习主题：
- en: '**Regression**: This is for predicting values that are linearly separable'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：用于预测线性可分离的值'
- en: '**Anomaly detection**: This is for finding unusual data points often done using
    a clustering algorithm'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测**：用于发现异常数据点，通常使用聚类算法进行'
- en: '**Clustering**: This is for discovering the hidden structure in the dataset
    for clustering homogeneous data points'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：用于发现数据集中同质数据点的隐藏结构'
- en: '**Binary classification**: This is for predicting two categories'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元分类**：用于预测两个类别'
- en: '**Multi-class classification**: This is for predicting three or more categories'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类分类**：用于预测三个或更多类别'
- en: Well, we have also seen that there are some good algorithms for these tasks.
    However, choosing the right algorithm for your problem type is a tricky task for
    achieving higher and outstanding accuracy in your ML algorithms. For this, we
    need to adopt some good practices through the stages, that is, from data collection,
    feature engineering, model building, evaluating, tuning, and deployment. Considering
    these, in this section, we will provide some practical recommendation while developing
    your ML application using Spark.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们也看到了一些适合这些任务的好算法。然而，选择适合您问题类型的正确算法是实现ML算法更高和更出色准确性的棘手任务。为此，我们需要通过从数据收集、特征工程、模型构建、评估、调整和部署的阶段采用一些良好的实践。考虑到这些，在本节中，我们将在使用Spark开发ML应用程序时提供一些建议。
- en: Beware of overfitting and underfitting
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意过拟合和欠拟合
- en: A straight line cutting across a curving scatter plot would be a good example
    of under-fitting, as we can see in the diagram here. However, if the line fits
    the data too well, there evolves an opposite problem called **overfitting**. When
    we say a model overfits a dataset, we mean it may have a low error rate for the
    training data, but it does not generalize well to the overall population in the
    data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一条直线穿过一个弯曲的散点图将是欠拟合的一个很好的例子，正如我们在这里的图表中所看到的。然而，如果线条过于贴合数据，就会出现一个相反的问题，称为**过拟合**。当我们说一个模型过拟合了数据集，我们的意思是它可能在训练数据上有低错误率，但在整体数据中不能很好地泛化。
- en: '![](img/00148.jpeg)**Figure 1**: Overfitting-underfitting trade-off (source:
    The book, "Deep Learning" by Adam Gibson, Josh Patterson)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00148.jpeg)**图1**：过拟合-欠拟合权衡（来源：亚当吉布森，乔什帕特森的书《深度学习》）'
- en: 'More technically, if you evaluate your model on the training data instead of
    test or validated data, you probably won''t be able to articulate whether your
    model is overfitting or not. The common symptoms are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，如果您在训练数据上评估模型而不是测试或验证数据，您可能无法确定您的模型是否过拟合。常见的症状如下：
- en: Predictive accuracy of the data used for training can be over accurate (that
    is, sometimes even 100%).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练的数据的预测准确性可能过于准确（即有时甚至达到100%）。
- en: The model might show better performance compared to the random prediction for
    new data.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与随机预测相比，模型可能在新数据上表现更好。
- en: We like to fit a dataset to a distribution because if the dataset is reasonably
    close to the distribution, we can make assumptions based on the theoretical distribution
    of how we operate with the data. Consequently, the normal distribution in the
    data allows us to assume that sampling distributions of statistics are normally
    distributed under specified conditions. The normal distribution is defined by
    its mean and standard deviation and has generally the same shape across all variations.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们喜欢将数据集拟合到分布中，因为如果数据集与分布相当接近，我们可以基于理论分布对我们如何处理数据进行假设。因此，数据中的正态分布使我们能够假设在指定条件下统计的抽样分布是正态分布的。正态分布由其均值和标准差定义，并且在所有变化中通常具有相同的形状。
- en: '![](img/00012.jpeg)**Figure 2**: Normal distribution in the data helps overcoming
    both the over-fitting and underfitting (source: The book, "Deep Learning" by Adam
    Gibson, Josh Patterson)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '**图2**：数据中的正态分布有助于克服过度拟合和拟合不足（来源：Adam Gibson、Josh Patterson的《深度学习》一书）'
- en: 'Sometimes, the ML model itself becomes underfit for a particular tuning or
    data point, which means the model become too simplistic. Our recommendation (like
    that of others, we believe) is as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，ML模型本身对特定调整或数据点拟合不足，这意味着模型变得过于简单。我们的建议（我们相信其他人也是如此）如下：
- en: Splitting the dataset into two sets to detect overfitting situations--the first
    one is for training and model selection called the training set, and the second
    one is the test set for evaluating the model started in place of the ML workflow
    section.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分为两组以检测过度拟合情况——第一组用于训练和模型选择的训练集，第二组是用于评估模型的测试集，开始替代ML工作流程部分。
- en: Alternatively, you also could avoid the overfitting by consuming simpler models
    (for example, linear classifiers in preference to Gaussian kernel SVM) or by swelling
    the regularization parameters of your ML model (if available).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，您还可以通过使用更简单的模型（例如，线性分类器而不是高斯核SVM）或增加ML模型的正则化参数（如果可用）来避免过度拟合。
- en: Tune the model with a correct data value of parameters to avoid both the overfitting
    as well as underfitting.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整模型的正确数据值参数，以避免过度拟合和拟合不足。
- en: 'Thus, solving underfitting is the priority, but most of the machines learning
    practitioners suggest spending more time and effort attempting not to overfit
    the line to the data. Also, many machine learning practitioners, on the other
    hand, have recommended splitting the large-scale dataset into three sets: a training
    set (50%), validation set (25%), and test set (25%). They also suggested to build
    the model using the training set and to calculate the prediction errors using
    the validation set. The test set was recommended to be used to assess the generalization
    error of the final model. If the amount of labeled data available on the other
    hand during the supervised learning is smaller, it is not recommended to split
    the datasets. In that case, use cross validation. More specifically, divide the
    dataset into 10 parts of (roughly) equal size; after that, for each of these 10
    parts, train the classifier iteratively and use the 10th part to test the model.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，解决拟合不足是首要任务，但大多数机器学习从业者建议花更多时间和精力尝试不要过度拟合数据。另一方面，许多机器学习从业者建议将大规模数据集分为三组：训练集（50%）、验证集（25%）和测试集（25%）。他们还建议使用训练集构建模型，并使用验证集计算预测误差。测试集被推荐用于评估最终模型的泛化误差。然而，在监督学习期间，如果可用的标记数据量较小，则不建议拆分数据集。在这种情况下，使用交叉验证。更具体地说，将数据集分为大致相等的10个部分；然后，对这10个部分中的每一个，迭代训练分类器，并使用第10个部分来测试模型。
- en: Stay tuned with Spark MLlib and Spark ML
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 请继续关注Spark MLlib和Spark ML
- en: The first step of the pipeline designing is to create the building blocks (as
    a directed or undirected graph consisting of nodes and edges) and make a linking
    between those blocks. Nevertheless, as a data scientist, you should be focused
    on scaling and optimizing nodes (primitives) too so that you are able to scale
    up your application for handling large-scale datasets in the later stage to make
    your ML pipeline performing consistently. The pipeline process will also help
    you to make your model adaptive for new datasets. However, some of these primitives
    might be explicitly defined to particular domains and data types (for example,
    text, image, and video, audio, and spatiotemporal).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 管道设计的第一步是创建构件块（作为由节点和边组成的有向或无向图），并在这些块之间建立联系。然而，作为一名数据科学家，您还应该专注于扩展和优化节点（原语），以便在后期处理大规模数据集时能够扩展应用程序，使您的ML管道能够持续执行。管道过程还将帮助您使模型适应新数据集。然而，其中一些原语可能会明确定义为特定领域和数据类型（例如文本、图像和视频、音频和时空）。
- en: Beyond these types of data, the primitives should also be working for general
    purpose domain statistics or mathematics. The casting of your ML model in terms
    of these primitives will make your workflow more transparent, interpretable, accessible,
    and explainable.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些类型的数据之外，原语还应该适用于通用领域统计或数学。将您的ML模型转换为这些原语将使您的工作流程更加透明、可解释、可访问和可解释。
- en: A recent example would be the ML-matrix, which is a distributed matrix library
    that can be used on top of Spark. Refer to the JIRA issue at [https://issues.apache.org/jira/browse/SPARK-3434](https://issues.apache.org/jira/browse/SPARK-3434).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一个例子是ML-matrix，它是一个可以在Spark之上使用的分布式矩阵库。请参阅[JIRA问题](https://issues.apache.org/jira/browse/SPARK-3434)。
- en: '![](img/00109.jpeg)**Figure 3:** Stay tune and interoperate ML and MLlib'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3**：保持关注并相互操作ML和MLlib'
- en: As we already stated in the previous section, as a developer, you can seamlessly
    combine the implementation techniques in Spark MLlib along with the algorithms
    developed in Spark ML, Spark SQL, GraphX, and Spark Streaming as hybrid or interoperable
    ML applications on top of RDD, DataFrame, and datasets as shown in *Figure 3*.
    Therefore, the recommendation here is to stay in tune or synchronized with the
    latest technologies around you for the betterment of your ML application.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中已经提到的，作为开发人员，您可以无缝地将Spark MLlib中的实现技术与Spark ML、Spark SQL、GraphX和Spark
    Streaming中开发的算法结合起来，作为RDD、DataFrame和数据集的混合或可互操作的ML应用程序，如*图3*所示。因此，这里的建议是与您周围的最新技术保持同步，以改善您的ML应用程序。
- en: Choosing the right algorithm for your application
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为您的应用程序选择正确的算法
- en: '"What machine learning algorithm should I use?" is a very frequently asked
    question for the naive machine learning practitioners but the answer is always
    *it depends*. More elaborately:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: “我应该使用什么机器学习算法？”是一个非常常见的问题，但答案总是“这取决于”。更详细地说：
- en: It depends on the volume, quality, complexity, and the nature of the data you
    have to be tested/used
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于你要测试/使用的数据的数量、质量、复杂性和性质
- en: It depends on external environments and parameters like your computing system's
    configuration or underlying infrastructures
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于外部环境和参数，比如你的计算系统配置或基础设施
- en: It depends on what you want to do with the answer
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于你想要用答案做什么
- en: It depends on how the mathematical and statistical formulation of the algorithm
    was translated into machine instructions for the computer
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于算法的数学和统计公式如何被转化为计算机的机器指令
- en: It depends on how much time do you have
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于你有多少时间
- en: 'The reality is, even the most experienced data scientists or data engineers
    can''t give a straight recommendation about which ML algorithm will perform the
    best before trying them all together. Most of the statements of agreement/disagreement
    begins with "It depends...hmm..." Habitually, you might wonder if there are cheat
    sheets of machine learning algorithms, and if so, how should you use that cheat
    sheet? Several data scientists have said that the only sure way to find the very
    best algorithm is to try all of them; therefore, there is no shortcut dude! Let''s
    make it clearer; suppose you do have a set of data and you want to do some clustering.
    Technically, this could be either a classification or regression problem that
    you want o apply on your dataset if your data is labeled. However, if you have
    a unlabeled dataset, it is the clustering technique that you will be using. Now,
    the concerns that evolve in your mind are as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，即使是最有经验的数据科学家或数据工程师在尝试所有算法之前也无法直接推荐哪种机器学习算法会表现最好。大多数同意/不同意的陈述都以“这取决于...嗯...”开始。习惯上，你可能会想知道是否有机器学习算法的备忘单，如果有的话，你应该如何使用？一些数据科学家表示，找到最佳算法的唯一方法是尝试所有算法；因此，没有捷径！让我们更清楚地说明一下；假设你有一组数据，你想做一些聚类。从技术上讲，如果你的数据有标签，这可能是一个分类或回归问题。然而，如果你有一个无标签的数据集，你将使用聚类技术。现在，你脑海中出现的问题如下：
- en: Which factors should I consider before choosing an appropriate algorithm? Or
    should I just choose an algorithm randomly?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择适当的算法之前，我应该考虑哪些因素？还是应该随机选择一个算法？
- en: How do I choose any data pre-processing algorithm or tools that can be applied
    to my data?
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何选择适用于我的数据的任何数据预处理算法或工具？
- en: What sort of feature engineering techniques should I be using to extract the
    useful features?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该使用什么样的特征工程技术来提取有用的特征？
- en: What factors can improve the performance of my ML model?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么因素可以提高我的机器学习模型的性能？
- en: How can I adopt my ML application for new data types?
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何适应新的数据类型？
- en: Can I scale up my ML application for large-scale datasets? And so on.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能否扩展我的机器学习应用以处理大规模数据集？等等。
- en: In this section, we will try to answer these questions with our little machine
    learning knowledge.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试用我们有限的机器学习知识来回答这些问题。
- en: Considerations when choosing an algorithm
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择算法时的考虑因素
- en: 'The recommendation or suggestions we provide here are for the novice data scientist
    who is just learning machine learning. These will use useful to expert the data
    scientists too, who is trying to choose an optimal algorithm to start with Spark
    ML APIs. Don''t worry, we will guide you to the direction! We also recommend going
    with the following algorithmic properties when choosing an algorithm:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里提供的建议或建议是给那些刚开始学习机器学习的新手数据科学家。这些对于试图选择一个最佳算法来开始使用Spark ML API的专家数据科学家也会有用。不用担心，我们会指导你的方向！我们还建议在选择算法时考虑以下算法属性：
- en: '**Accuracy**: Whether getting the best score is the goal or an approximate
    solution (*good enough*) in terms of precision, recall, f1 score or AUC and so
    on, while trading off overfitting.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：是否达到最佳分数是目标，还是在精确度、召回率、f1分数或AUC等方面进行权衡，得到一个近似解（足够好），同时避免过拟合。'
- en: '**Training time**: The amount of time available to train the model (including
    the model building, evaluation, and tanning time).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练时间**：训练模型的可用时间（包括模型构建、评估和训练时间）。'
- en: '**Linearity**: An aspect of model complexity in terms of how the problem is
    modeled. Since most of the non-linear models are often more complex to understand
    and tune.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性度**：模型复杂性的一个方面，涉及问题建模的方式。由于大多数非线性模型通常更复杂，难以理解和调整。'
- en: '**Number of parameters**'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数数量**'
- en: '**Number of features**: The problem of having more attributes than instances,
    the *p>>n* problem. This often requires specialized handling or specialized techniques
    using dimensionality reduction or better feature engineering approach.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征数量**：拥有的属性比实例多的问题，即*p>>n*问题。这通常需要专门处理或使用降维或更好的特征工程方法。'
- en: Accuracy
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确性
- en: 'Getting the most accurate results from your ML application isn''t always indispensable.
    Depending on what you want to use it for, sometimes an approximation is adequate
    enough. If the situation is something like this, you may be able to reduce the
    processing time drastically by incorporating the better-estimated methods. When
    you become familiar of the workflow with the Spark machine learning APIs, you
    will enjoy the advantage of having more approximation methods, because these approximation
    methods will tend to avoid the overfitting problem of your ML model automatically.
    Now, suppose you have two binary classification algorithms that perform as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从你的机器学习应用中获得最准确的结果并非总是必不可少的。根据你想要使用它的情况，有时近似解就足够了。如果情况是这样的，你可以通过采用更好的估计方法大大减少处理时间。当你熟悉了Spark机器学习API的工作流程后，你将享受到更多的近似方法的优势，因为这些近似方法将自动避免你的机器学习模型的过拟合问题。现在，假设你有两个二元分类算法的表现如下：
- en: '| **Classifier** | **Precision** | **Recall** |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| **分类器** | **精确度** | **召回率** |'
- en: '| X | 96% | 89% |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| X | 96% | 89% |'
- en: '| Y | 99% | 84% |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Y | 99% | 84% |'
- en: 'Here, none of the classifiers is obviously superior, so it doesn''t immediately
    guide you toward picking the optimal one. F1-score which is the harmonic mean
    of precision and recall helps you. Let''s calculate it and place it in the table:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，没有一个分类器显然优于其他分类器，因此它不会立即指导您选择最佳的分类器。F1分数是精确度和召回率的调和平均值，它可以帮助您。让我们计算一下，并将其放在表中：
- en: '| **Classifier** | **Precision** | **Recall** | **F1 score** |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| **分类器** | **精度** | **召回率** | **F1分数** |'
- en: '| X | 96% | 89% | 92.36% |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| X | 96% | 89% | 92.36% |'
- en: '| Y | 99% | 84% | 90.885% |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Y | 99% | 84% | 90.885% |'
- en: Therefore, having an F1-score helps make a decision for selecting from a large
    number of classifiers. It gives a clear preference ranking among all of them,
    and therefore a clear direction for progress--that is classifier **X**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，具有F1分数有助于从大量分类器中进行选择。它为所有分类器提供了清晰的偏好排序，因此也为进展提供了明确的方向--即分类器**X**。
- en: Training time
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练时间
- en: Training time is often closely related to the model training and the accuracy.
    In addition, often you will discover that some of the algorithms are elusive to
    the number of data points compared to others. However, when your time is inadequate
    but the training set is large with a lot of features, you can choose the simplest
    one. In this case, you might have to compromise with the accuracy. But it will
    fulfill your minimum requirements at least.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间通常与模型训练和准确性密切相关。此外，通常您会发现，与其他算法相比，有些算法对数据点的数量更加难以捉摸。然而，当您的时间不足但训练集又很大且具有许多特征时，您可以选择最简单的算法。在这种情况下，您可能需要牺牲准确性。但至少它将满足您的最低要求。
- en: Linearity
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性
- en: There are many machine learning algorithms developed recently that make use
    of linearity (also available in the Spark MLlib and Spark ML). For example, linear
    classification algorithms undertake that classes can be separated by plotting
    a differentiating straight line or using higher-dimensional equivalents. Linear
    regression algorithms, on the other hand, assume that data trends simply follow
    a straight line. This assumption is not naive for some machine learning problems;
    however, there might be some other cases where the accuracy will be down. Despite
    their hazards, linear algorithms are very popular with data engineers and data
    scientists as the first line of the outbreak. Moreover, these algorithms also
    tend to be simple and fast, to train your models during the whole process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最近开发了许多利用线性的机器学习算法（也可在Spark MLlib和Spark ML中使用）。例如，线性分类算法假设类别可以通过绘制不同的直线或使用高维等价物来分离。另一方面，线性回归算法假设数据趋势简单地遵循一条直线。对于一些机器学习问题，这种假设并不天真；然而，在某些其他情况下，准确性可能会下降。尽管存在危险，线性算法在数据工程师和数据科学家中非常受欢迎，作为爆发的第一线。此外，这些算法还倾向于简单和快速，以在整个过程中训练您的模型。
- en: Inspect your data when choosing an algorithm
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在选择算法时检查您的数据
- en: 'You will find many machine learning datasets available at the UC Irvine Machine
    Learning Repository. The following data properties should also be prioritized:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在UC Irvine机器学习库中找到许多机器学习数据集。以下数据属性也应该优先考虑：
- en: Number of parameters
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数数量
- en: Number of features
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征数量
- en: Size of the training dataset
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据集的大小
- en: Number of parameters
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数数量
- en: Parameters or data properties are the handholds for a data scientist when setting
    up an algorithm. They are numbers that affect the algorithm's performance, such
    as error tolerance or a number of iterations, or options between variants of how
    the algorithm acts. The training time and accuracy of the algorithm can sometimes
    be quite sensitive making it difficult get the right settings. Typically, algorithms
    with a large number of parameters require more trial and error to find an optimal
    combination.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数或数据属性是数据科学家在设置算法时的抓手。它们是影响算法性能的数字，如误差容限或迭代次数，或者是算法行为变体之间的选项。算法的训练时间和准确性有时可能非常敏感，这使得难以找到正确的设置。通常，具有大量参数的算法需要更多的试错来找到最佳组合。
- en: 'Despite the fact that this is a great way to span the parameter space, the
    model building or train time increases exponentially with the increased number
    of parameters. This is a dilemma as well as a time-performance trade-off. The
    positive sides are:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是跨越参数空间的一个很好的方法，但随着参数数量的增加，模型构建或训练时间呈指数增长。这既是一个困境，也是一个时间性能的权衡。积极的方面是：
- en: Having many parameters characteristically indicates the greater flexibility
    of the ML algorithms
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有许多参数特征性地表明了ML算法的更大灵活性
- en: Your ML application achieves much better accuracy
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的ML应用程序实现了更高的准确性
- en: How large is your training set?
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的训练集有多大？
- en: If your training set is smaller, high bias with low variance classifiers, such
    as Naive Bayes have an advantage over low bias with high variance classifiers
    (also can be used for regression) such as the **k-nearest neighbors algorithm**
    (**kNN**).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的训练集较小，具有低方差的高偏差分类器，如朴素贝叶斯，比具有高方差的低偏差分类器（也可用于回归）如**k最近邻算法**（**kNN**）更有优势。
- en: '**Bias, Variance, and the kNN model:** In reality, *increasing k* will *decrease
    the variance,* but *increase the bias*. On the other hand, *decreasing k* will
    *increase variance* and *decrease bias*. As *k* increases, this variability is
    reduced. But if we increase *k* too much, then we no longer follow the true boundary
    line and we observe high bias. This is the nature of the Bias-Variance Trade-off.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差、方差和kNN模型：**实际上，*增加k*会*减少方差*，但会*增加偏差*。另一方面，*减少k*会*增加方差*，*减少偏差*。随着*k*的增加，这种变异性减少。但如果我们增加*k*太多，那么我们就不再遵循真实的边界线，我们会观察到高偏差。这就是偏差-方差权衡的本质。'
- en: 'We have seen the over and underfitting issue already. Now, you can assume that
    dealing with bias and variance is like dealing with over- and underfitting. Bias
    is reduced and variance is increased in relation to model complexity. As more
    and more parameters are added to a model, the complexity of the model rises and
    variance becomes our primary concern while bias steadily falls. In other words,
    bias has a negative first-order derivative in response to model complexity, while
    variance has a positive slope. Refer to the following figure for a better understanding:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了过拟合和欠拟合的问题。现在，可以假设处理偏差和方差就像处理过拟合和欠拟合一样。随着模型复杂性的增加，偏差减小，方差增加。随着模型中添加更多参数，模型的复杂性增加，方差成为我们关注的主要问题，而偏差稳步下降。换句话说，偏差对模型复杂性的响应具有负的一阶导数，而方差具有正的斜率。请参考以下图表以更好地理解：
- en: '![](img/00077.jpeg)**Figure 4:** Bias and variance contributing to total error'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00077.jpeg)**图4：** 偏差和方差对总误差的影响'
- en: Therefore, the latter will overfit. But low bias with high variance classifiers,
    on the other hand, starts to win out as your training set grows linearly or exponentially,
    since they have lower asymptotic errors. High bias classifiers aren't powerful
    enough to provide accurate models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，后者会过拟合。但是低偏差高方差的分类器，在训练集线性或指数增长时，开始获胜，因为它们具有更低的渐近误差。高偏差分类器不足以提供准确的模型。
- en: Number of features
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征数
- en: For certain types of experimental dataset, the number of extracted features
    can be very large compared to the number of data points itself. This is often
    the case with genomics, biomedical, or textual data. A large number of features
    can swamp down some learning algorithms, making training time ridiculously higher.
    **Support Vector Machines** (**SVMs**) are particularly well suited in this case
    for its high accuracy, nice theoretical guarantees regarding overfitting, and
    with an appropriate kernel.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些类型的实验数据集，提取的特征数量可能与数据点本身的数量相比非常大。这在基因组学、生物医学或文本数据中经常发生。大量的特征可能会淹没一些学习算法，使训练时间变得非常长。**支持向量机**（**SVM**）特别适用于这种情况，因为它具有高准确性，对过拟合有良好的理论保证，并且具有适当的核函数。
- en: '**The SVM and the Kernel**: The task is to find a set of weight and bias such
    that the margin can maximize the function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机和核函数：** 任务是找到一组权重和偏差，使间隔最大化函数：'
- en: y = w*¥(x) +b,
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: y = w*¥(x) +b,
- en: 'Where *w* is the weight, *¥* is the feature vector, and *b* is the bias. Now
    if *y> 0*, then we classify datum to class *1*, else to class *0*, whereas, the
    feature vector *¥(x)* makes the data linearly separable. However, using the kernel
    makes the calculation process faster and easier, especially when the feature vector
    *¥* consisting of very high dimensional data. Let''s see a concrete example. Suppose
    we have the following value of *x* and *y*: *x = (x1, x2, x3)* and *y = (y1, y2,
    y3)*, then for the function *f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1,
    x3x2, x3x3)*, the kernel is *K(x, y ) = (<x, y>)²*. Following the above, if *x*
    *= (1, 2, 3)* and *y = (4, 5, 6)*, then we have the following values:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*w*是权重，*¥*是特征向量，*b*是偏差。现在如果*y> 0*，那么我们将数据分类到类*1*，否则到类*0*，而特征向量*¥(x)*使数据线性可分。然而，使用核函数可以使计算过程更快、更容易，特别是当特征向量*¥*包含非常高维的数据时。让我们看一个具体的例子。假设我们有以下值*x*和*y*：*x
    = (x1, x2, x3)*和*y = (y1, y2, y3)*，那么对于函数*f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2,
    x2x3, x3x1, x3x2, x3x3)*，核函数是*K(x, y ) = (<x, y>)²*。根据上述，如果*x* *= (1, 2, 3)*和*y
    = (4, 5, 6)*，那么我们有以下值：
- en: f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)
- en: f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)
- en: <f(x), f(y)> = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <f(x), f(y)> = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024
- en: This is a simple linear algebra that maps a 3-dimensional space to a 9 dimensional.
    On the other hand, the kernel is a similarity measure used for SVMs. Therefore,
    choosing an appropriate kernel value based on the prior knowledge of invariances
    is suggested. The choice of the kernel and kernel and regularization parameters
    can be automated by optimizing a cross-validation based model selection.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的线性代数，将一个3维空间映射到一个9维空间。另一方面，核函数是用于支持向量机的相似性度量。因此，建议根据对不变性的先验知识选择适当的核值。核和正则化参数的选择可以通过优化基于交叉验证的模型选择来自动化。
- en: Nevertheless, an automated choice of kernels and kernel parameters is a tricky
    issue, as it is very easy to overfit the model selection criterion. This might
    result in a worse model than you started with. Now, if we use the kernel function
    *K(x, y), this gives the same value but with much simpler calculation -i.e. (4
    + 10 + 18) ^2 = 32^2 = 1024*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自动选择核和核参数是一个棘手的问题，因为很容易过度拟合模型选择标准。这可能导致比开始时更糟糕的模型。现在，如果我们使用核函数*K(x, y)*，这将给出相同的值，但计算更简单
    - 即(4 + 10 + 18) ^2 = 32^2 = 1024。
- en: Hyperparameter tuning of ML models
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的超参数调整
- en: 'Tuning an algorithm is simply a process that one goes through in order to enable
    the algorithm to perform optimally in terms of runtime and memory usage. In Bayesian
    statistics, a hyperparameter is a parameter of a prior distribution. In terms
    of machine learning, the term hyperparameter refers to those parameters that cannot
    be directly learned from the regular training process. Hyperparameters are usually
    fixed before the actual training process begins. This is done by setting different
    values for those hyperparameters, training different models, and deciding which
    ones work best by testing them. Here are some typical examples of such parameters:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 调整算法只是一个过程，通过这个过程，使算法在运行时间和内存使用方面表现最佳。在贝叶斯统计中，超参数是先验分布的参数。在机器学习方面，超参数指的是那些不能直接从常规训练过程中学习到的参数。超参数通常在实际训练过程开始之前固定。这是通过为这些超参数设置不同的值，训练不同的模型，并通过测试来决定哪些模型效果最好来完成的。以下是一些典型的超参数示例：
- en: Number of leaves, bins, or depth of a tree
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 叶子节点数、箱数或树的深度
- en: Number of iterations
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数
- en: Number of latent factors in a matrix factorization
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵分解中的潜在因子数量
- en: Learning rate
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: Number of hidden layers in a deep neural network
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络中的隐藏层数量
- en: Number of clusters in a k-means clustering and so on.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k均值聚类中的簇数量等。
- en: In this section, we will discuss how to perform hyperparameter tuning using
    the cross-validation technique and grid searching.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何使用交叉验证技术和网格搜索进行超参数调整。
- en: Hyperparameter tuning
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整
- en: 'Hyperparameter tuning is a technique for choosing the right combination of
    hyperparameters based on the performance of presented data. It is one of the fundamental
    requirements to obtain meaningful and accurate results from machine learning algorithms
    in practice. The following figure shows the model tuning process, consideration,
    and workflow:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整是一种根据呈现数据的性能选择正确的超参数组合的技术。这是从实践中获得机器学习算法的有意义和准确结果的基本要求之一。下图显示了模型调整过程、考虑因素和工作流程：
- en: '![](img/00343.jpeg)**Figure 5**: The model tuning process, consideration, and
    workflow'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00343.jpeg)**图5**：模型调整过程、考虑因素和工作流程'
- en: 'For example, suppose we have two hyperparameters to tune for a pipeline presented
    in *Figure 17* from [Chapter 11](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c),
    *Learning Machine Learning - Spark MLlib and Spark ML*, a Spark ML pipeline model
    using a logistic regression estimator (dash lines only happen during pipeline
    fitting). We can see that we have put three candidate values for each. Therefore,
    there would be nine combinations in total. However, only four are shown in the
    diagram, namely, Tokenizer, HashingTF, Transformer, and Logistic Regression (LR).
    Now, we want to find the one that will eventually lead to the model with the best
    evaluation result. The fitted model consists of the Tokenizer, the HashingTF feature
    extractor, and the fitted logistic regression model:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有两个要为管道调整的超参数，该管道在[第11章](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c)中的*图17*中呈现，使用逻辑回归估计器的Spark
    ML管道模型（虚线只会在管道拟合期间出现）。我们可以看到我们为每个参数放置了三个候选值。因此，总共会有九种组合。但是，在图中只显示了四种，即Tokenizer、HashingTF、Transformer和Logistic
    Regression（LR）。现在，我们要找到最终会导致具有最佳评估结果的模型。拟合的模型包括Tokenizer、HashingTF特征提取器和拟合的逻辑回归模型：
- en: If you recall *Figure 17* from [Chapter 11](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c),
    *Learning Machine Learning - Spark MLlib and Spark ML*, the dashed line, however,
    happens only during the pipeline fitting. As mentioned earlier, the fitted pipeline
    model is a Transformer. The Transformer can be used for prediction, model validation,
    and model inspection. In addition, we also argued that one ill-fated distinguishing
    characteristic of the ML algorithms is that typically they have many hyperparameters
    that need to be tuned for better performance. For example, the degree of regularizations
    in these hyperparameters is distinctive from the model parameters optimized by
    the Spark MLlib.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您回忆起[第11章](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c)中的*图17*，*学习机器学习
    - Spark MLlib和Spark ML*，虚线只会在管道拟合期间出现。正如前面提到的，拟合的管道模型是一个Transformer。Transformer可用于预测、模型验证和模型检查。此外，我们还认为ML算法的一个不幸的特点是，它们通常有许多需要调整以获得更好性能的超参数。例如，这些超参数中的正则化程度与Spark
    MLlib优化的模型参数有所不同。
- en: As a consequence, it is really hard to guess or measure the best combination
    of hyperparameters without expert knowledge of the data and the algorithm to use.
    Since the complex dataset is based on the ML problem type, the size of the pipeline
    and the number of hyperparameters may grow exponentially (or linearly); the hyperparameter
    tuning becomes cumbersome even for an ML expert, not to mention that the result
    of the tuning parameters may become unreliable.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果没有对数据和要使用的算法的专业知识，很难猜测或衡量最佳超参数组合。由于复杂数据集基于ML问题类型，管道的大小和超参数的数量可能会呈指数级增长（或线性增长）；即使对于ML专家来说，超参数调整也会变得繁琐，更不用说调整参数的结果可能会变得不可靠。
- en: 'According to Spark API documentation, a unique and uniform API is used for
    specifying Spark ML estimators and Transformers. A `ParamMap` is a set of (parameter,
    value) pairs with a Param as a named parameter with self-contained documentation
    provided by Spark. Technically, there are two ways for passing the parameters
    to an algorithm as specified in the following options:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Spark API文档，用于指定Spark ML估计器和Transformer的是一个独特且统一的API。`ParamMap`是一组(参数，值)对，其中Param是由Spark提供的具有自包含文档的命名参数。从技术上讲，有两种方法可以将参数传递给算法，如下所示：
- en: '**Setting parameters**: If an LR is an instance of Logistic Regression (that
    is, Estimator), you can call the `setMaxIter()` method as follows: `LR.setMaxIter(5)`.
    It essentially fits the model pointing the regression instance as follows: `LR.fit()`.
    In this particular example, there would be at most five iterations.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置参数**：如果LR是逻辑回归的实例（即估计器），则可以调用`setMaxIter()`方法，如下所示：`LR.setMaxIter(5)`。它基本上将模型拟合到回归实例，如下所示：`LR.fit()`。在这个特定的例子中，最多会有五次迭代。'
- en: '**The second option**: This one involves passing a `ParamMaps` to `fit()` or
    `transform()` (refer *Figure 5* for details). In this circumstance, any parameters
    will be overridden by the `ParamMaps` previously specified via setter methods
    in the ML application-specific codes or algorithms.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二个选项**：这涉及将`ParamMaps`传递给`fit()`或`transform()`（有关详细信息，请参见*图5*）。在这种情况下，任何参数都将被先前通过ML应用程序特定代码或算法中的setter方法指定的`ParamMaps`覆盖。'
- en: Grid search parameter tuning
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网格搜索参数调整
- en: 'Suppose you selected your hyperparameters after necessary feature engineering.
    In this regard, a full grid search of the space of hyperparameters and features
    is computationally too intensive. Therefore, you need to perform a fold of the
    K-fold cross-validation instead of a full-grid search:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您在必要的特征工程之后选择了您的超参数。在这方面，对超参数和特征空间进行完整的网格搜索计算量太大。因此，您需要执行K折交叉验证的折叠，而不是进行完整的网格搜索：
- en: Tune the required hyperparameters using cross validation on the training set
    of the fold, using all the available features
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在折叠的训练集上使用交叉验证来调整所需的超参数，使用所有可用的特征
- en: Select the required features using those hyperparameters
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这些超参数选择所需的特征
- en: Repeat the computation for each fold in K
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对K中的每个折叠重复计算
- en: The final model is constructed on all the data using the N most prevalent features
    that were selected from each fold of CV
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终模型是使用从每个CV折叠中选择的N个最常见特征构建的所有数据
- en: The interesting thing is that the hyperparameters would also be tuned again
    using all the data in a cross-validation loop. Would there be a large downside
    from this method as compared to a full-grid search? In essence, I am doing a line
    search in each dimension of free parameters (finding the best value in one dimension,
    holding that constant, then finding the best in the next dimension), rather than
    every single combination of parameter settings. The most important downside for
    searching along single parameters instead of optimizing them all together, is
    that you ignore interactions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，超参数也将在交叉验证循环中再次进行调整。与完整的网格搜索相比，这种方法会有很大的不利因素吗？实质上，我在每个自由参数的维度上进行线性搜索（找到一个维度中的最佳值，将其保持恒定，然后找到下一个维度中的最佳值），而不是每个参数设置的所有组合。沿着单个参数搜索而不是一起优化它们的最重要的不利因素是，您忽略了相互作用。
- en: It is quite common that, for instance, more than one parameter influences model
    complexity. In that case, you need to look at their interaction in order to successfully
    optimize the hyperparameters. Depending on how large your dataset is and how many
    models you compare, optimization strategies that return the maximum observed performance
    may run into trouble (this is true for both grid search and your strategy).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，很常见的是，不止一个参数影响模型复杂性。在这种情况下，您需要查看它们的相互作用，以成功地优化超参数。根据您的数据集有多大以及您比较了多少个模型，返回最大观察性能的优化策略可能会遇到麻烦（这对网格搜索和您的策略都是如此）。
- en: 'The reason is that searching through a large number of performance estimates
    for the maximum skims the variance of the performance estimate: you may just end
    up with a model and training/test split combination that accidentally happens
    to look good. Even worse, you may get several perfect-looking combinations, and
    the optimization then cannot know which model to choose and thus becomes unstable.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是在大量性能估计中寻找最大值会削弱性能估计的方差：您可能最终只得到一个模型和训练/测试分割组合，碰巧看起来不错。更糟糕的是，您可能会得到几个看起来完美的组合，然后优化无法知道选择哪个模型，因此变得不稳定。
- en: Cross-validation
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Cross-validation (also called the **rotation estimation** (**RE**)) is a model
    validation technique for assessing the quality of the statistical analysis and
    results. The target is to make the model generalize toward an independent test
    set. One of the perfect uses of the cross-validation technique is making a prediction
    from a machine learning model. It will help if you want to estimate how a predictive
    model will perform accurately in practice when you deploy it as an ML application.
    During the cross-validation process, a model is usually trained with a dataset
    of a known type. Conversely, it is tested using a dataset of unknown type.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证（也称为**旋转估计**（**RE**））是一种模型验证技术，用于评估统计分析和结果的质量。目标是使模型向独立测试集泛化。交叉验证技术的一个完美用途是从机器学习模型中进行预测。如果您想要估计在实践中部署为ML应用时预测模型的准确性，这将有所帮助。在交叉验证过程中，模型通常是使用已知类型的数据集进行训练的。相反，它是使用未知类型的数据集进行测试的。
- en: 'In this regard, cross-validation helps to describe a dataset to test the model
    in the training phase using the validation set. There are two types of cross-validation
    that can be typed as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，交叉验证有助于描述数据集，以便在训练阶段使用验证集测试模型。有两种类型的交叉验证，可以如下分类：
- en: '**Exhaustive cross-validation**: This includes leave-p-out cross-validation
    and leave-one-out cross-validation.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**穷举交叉验证**：这包括留p-out交叉验证和留一出交叉验证。'
- en: '**Non-exhaustive cross-validation**: This includes K-fold cross-validation
    and repeated random sub-sampling cross-validation.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非穷尽交叉验证**：这包括K折交叉验证和重复随机子采样交叉验证。'
- en: 'In most of the cases, the researcher/data scientist/data engineer uses 10-fold
    cross-validation instead of testing on a validation set. This is the most widely
    used cross-validation technique across the use cases and problem type as explained
    by the following figure:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，研究人员/数据科学家/数据工程师使用10折交叉验证，而不是在验证集上进行测试。这是最广泛使用的交叉验证技术，如下图所示：
- en: '![](img/00372.gif)**Figure 6:** Cross-validation basically splits your complete
    available training data into a number of folds. This parameter can be specified.
    Then the whole pipeline is run once for every fold and one machine learning model
    is trained for each fold. Finally, the different machine learning models obtained
    are joined by a voting scheme for classifiers or by averaging for regression'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00372.gif)**图6：**交叉验证基本上将您的完整可用训练数据分成多个折叠。可以指定此参数。然后，整个流程对每个折叠运行一次，并为每个折叠训练一个机器学习模型。最后，通过分类器的投票方案或回归的平均值将获得的不同机器学习模型结合起来'
- en: 'Moreover, to reduce the variability, multiple iterations of cross-validation
    are performed using different partitions; finally, the validation results are
    averaged over the rounds. The following figure shows an example of hyperparameter
    tuning using the logistic regression:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了减少变异性，使用不同分区进行多次交叉验证迭代；最后，将验证结果在各轮上进行平均。下图显示了使用逻辑回归进行超参数调整的示例：
- en: '![](img/00046.jpeg)**Figure 7:** An example of hyperparameter tuning using
    the logistic regression'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00046.jpeg)**图7：**使用逻辑回归进行超参数调整的示例'
- en: 'Using cross-validation instead of conventional validation has two main advantages
    outlined as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉验证而不是传统验证有以下两个主要优点：
- en: Firstly, if there is not enough data available to partition across the separate
    training and test sets, there's the chance of losing significant modeling or testing
    capability.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，如果没有足够的数据可用于在单独的训练和测试集之间进行分区，就有可能失去重要的建模或测试能力。
- en: Secondly, the K-fold cross-validation estimator has a lower variance than a
    single hold-out set estimator. This low variance limits the variability and is
    again very important if the amount of available data is limited.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，K折交叉验证估计器的方差低于单个留出集估计器。这种低方差限制了变异性，如果可用数据量有限，这也是非常重要的。
- en: 'In these circumstances, a fair way to properly estimate the model prediction
    and related performance are to use cross-validation as a powerful general technique
    for model selection and validation. If we need to perform manual features and
    a parameter selection for the model tuning, after that, we can perform a model
    evaluation with a 10-fold cross-validation on the entire dataset. What would be
    the best strategy? We would suggest you go for the strategy that provides an optimistic
    score as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，一个公平的方法来正确估计模型预测和相关性能是使用交叉验证作为模型选择和验证的强大通用技术。如果我们需要对模型调整进行手动特征和参数选择，然后，我们可以在整个数据集上进行10折交叉验证的模型评估。什么是最佳策略？我们建议您选择提供乐观分数的策略如下：
- en: Divide the dataset into training, say 80%, and testing 20% or whatever you chose
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分为训练集（80%）和测试集（20%）或您选择的其他比例
- en: Use the K-fold cross-validation on the training set to tune your model
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练集上使用K折交叉验证来调整您的模型
- en: Repeat the CV until you find your model optimized and therefore tuned.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复CV，直到找到优化并调整您的模型。
- en: Now, use your model to predict on the testing set to get an estimate of out
    of model errors.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用您的模型在测试集上进行预测，以获得模型外误差的估计。
- en: Credit risk analysis – An example of hyperparameter tuning
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信用风险分析-超参数调整的一个例子
- en: In this section, we will show a practical example of machine learning hyperparameter
    tuning in terms of grid searching and cross-validation technique. More specifically,
    at first, we will develop a credit risk pipeline that is commonly used in financial
    institutions such as banks and credit unions. Later on, we will look at how to
    improve the prediction accuracy by hyperparameter tuning. Before diving into the
    example, let's take a quick overview of what credit risk analysis is and why it
    is important?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示一个实际的机器学习超参数调整的示例，涉及网格搜索和交叉验证技术。更具体地说，首先，我们将开发一个信用风险管道，这在金融机构如银行和信用合作社中常用。随后，我们将看看如何通过超参数调整来提高预测准确性。在深入示例之前，让我们快速概述一下信用风险分析是什么，以及为什么它很重要？
- en: What is credit risk analysis? Why is it important?
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是信用风险分析？为什么它很重要？
- en: 'When an applicant applies for loans and a bank receives that application, based
    on the applicant''s profile, the bank has to make a decision whether to approve
    the loan application or not. In this regard, there are two types of risk associated
    with the bank''s decision on the loan application:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当申请人申请贷款并且银行收到该申请时，根据申请人的资料，银行必须决定是否批准贷款申请。在这方面，银行对贷款申请的决定存在两种风险：
- en: '**The applicant is a good credit risk**: That means the client or applicant
    is more likely to repay the loan. Then, if the loan is not approved, the bank
    can potentially suffer the loss of business.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**申请人是一个良好的信用风险**：这意味着客户或申请人更有可能偿还贷款。然后，如果贷款未获批准，银行可能会遭受业务损失。'
- en: '**The applicant is a bad credit risk**: That means that the client or applicant
    is most likely not to repay the loan. In that case, approving the loan to the
    client will result in financial loss to the bank.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**申请人是一个不良的信用风险**：这意味着客户或申请人很可能不会偿还贷款。在这种情况下，向客户批准贷款将导致银行的财务损失。'
- en: The institution says that the second one is riskier than that of the first one,
    as the bank has a higher chance of not getting reimbursed the borrowed amount.
    Therefore, most banks or credit unions evaluate the risks associated with lending
    money to a client, applicant, or customer. In business analytics, minimizing the
    risk tends to maximize the profit to the bank itself.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 该机构表示第二个风险比第一个更高，因为银行有更高的机会无法收回借款金额。因此，大多数银行或信用合作社评估向客户、申请人或顾客放贷所涉及的风险。在业务分析中，最小化风险往往会最大化银行自身的利润。
- en: In other words, maximizing the profit and minimizing the loss from a financial
    perspective is important. Often, the bank makes a decision about approving a loan
    application based on different factors and parameters of an applicant, such as
    demographic and socio-economic conditions regarding their loan application.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，从财务角度来看，最大化利润和最小化损失是重要的。通常，银行根据申请人的不同因素和参数，如贷款申请的人口统计和社会经济状况，来决定是否批准贷款申请。
- en: The dataset exploration
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集探索
- en: The German credit dataset was downloaded from the UCI Machine Learning Repository
    at [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/).
    Although a detailed description of the dataset is available in the link, we provide
    some brief insights here in **Table 3**. The data contains credit-related data
    on 21 variables and the classification of whether an applicant is considered a
    good or a bad credit risk for 1000 loan applicants (that is, binary classification
    problem).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 德国信用数据集是从UCI机器学习库[https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)下载的。尽管链接中提供了数据集的详细描述，但我们在**表3**中提供了一些简要的见解。数据包含21个变量的与信用相关的数据，以及对于1000个贷款申请人是否被认为是良好还是不良的信用风险的分类（即二元分类问题）。
- en: 'The following table shows details about each variable that was considered before
    making the dataset available online:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了在将数据集提供在线之前考虑的每个变量的详细信息：
- en: '| **Entry** | **Variable** | **Explanation** |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **条目** | **变量** | **解释** |'
- en: '| 1 | creditability | Capable of repaying: has value either 1.0 or 0.0 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 1 | creditability | 有能力偿还：值为1.0或0.0 |'
- en: '| 2 | balance | Current balance |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 2 | balance | 当前余额 |'
- en: '| 3 | duration | Duration of the loan being applied for |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 3 | duration | 申请贷款的期限 |'
- en: '| 4 | history | Is there any bad loan history? |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 4 | history | 是否有不良贷款历史？ |'
- en: '| 5 | purpose | Purpose of the loan |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 5 | purpose | 贷款目的 |'
- en: '| 6 | amount | Amount being applied for |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 6 | amount | 申请金额 |'
- en: '| 7 | savings | Monthly saving |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 7 | savings | 每月储蓄 |'
- en: '| 8 | employment | Employment status |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 8 | employment | 就业状况 |'
- en: '| 9 | instPercent | Interest percent |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 9 | instPercent | 利息百分比 |'
- en: '| 10 | sexMarried | Sex and marriage status |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 10 | sexMarried | 性别和婚姻状况 |'
- en: '| 11 | guarantors | Are there any guarantors? |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 11 | guarantors | 是否有担保人？ |'
- en: '| 12 | residenceDuration | Duration of residence at the current address |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 12 | residenceDuration | 目前地址的居住时间 |'
- en: '| 13 | assets | Net assets |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 13 | assets | 净资产 |'
- en: '| 14 | age | Age of the applicant |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 14 | age | 申请人年龄 |'
- en: '| 15 | concCredit | Concurrent credit |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 15 | concCredit | 并发信用 |'
- en: '| 16 | apartment | Residential status |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 16 | apartment | 住房状况 |'
- en: '| 17 | credits | Current credits |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 17 | credits | 当前信用 |'
- en: '| 18 | occupation | Occupation |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: 18 | occupation | 职业 |
- en: '| 19 | dependents | Number of dependents |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 19 | dependents | 受抚养人数 |'
- en: '| 20 | hasPhone | If the applicant uses a phone |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 20 | hasPhone | 申请人是否使用电话 |'
- en: '| 21 | foreign | If the applicant is a foreigner |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 21 | foreign | 申请人是否是外国人 |'
- en: Note that, although *Table 3* describes the variables with an associated header,
    there is no associated header in the dataset. In *Table 3*, we have shown the
    variable, position, and associated significance of each variable.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管*表3*描述了具有相关标题的变量，但数据集中没有相关标题。在*表3*中，我们展示了每个变量的变量、位置和相关重要性。
- en: Step-by-step example with Spark ML
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark ML的逐步示例
- en: 'Here, we will provide a step-by-step example of credit risk prediction using
    the Random Forest classifier. The steps include from data ingestion, some statistical
    analysis, training set preparation, and finally model evaluation:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将提供使用随机森林分类器进行信用风险预测的逐步示例。步骤包括数据摄入、一些统计分析、训练集准备，最后是模型评估：
- en: '**Step 1.** Load and parse the dataset into RDD:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1.** 加载并解析数据集为RDD：'
- en: '[PRE0]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For the preceding line, the `parseRDD()` method is used to split the entry
    with `,` and then converted all of them as `Double` value (that is, numeric).
    This method goes as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前一行，`parseRDD()`方法用于使用`,`拆分条目，然后将它们全部转换为`Double`值（即数值）。该方法如下：
- en: '[PRE1]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'On the other hand, the `parseCredit()` method is used to parse the dataset
    based on the `Credit` case class:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`parseCredit()`方法用于基于`Credit` case类解析数据集：
- en: '[PRE2]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `Credit` case class goes as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`Credit` case类如下所示：'
- en: '[PRE3]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 2\. Prepare the DataFrame for the ML pipeline** - Get the DataFrame
    for the ML pipeline'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2.准备ML管道的DataFrame** - 获取ML管道的DataFrame'
- en: '[PRE4]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Save them as a temporary view for making the query easier:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 将它们保存为临时视图，以便更容易进行查询：
- en: '[PRE5]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s take a snap of the DataFrame:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下DataFrame的快照：
- en: '[PRE6]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding `show()` method prints the credit DataFrame:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的`show()`方法打印了信用DataFrame：
- en: '![](img/00124.gif)**Figure 8:** A snap of the credit dataset'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00124.gif)**图8：**信用数据集的快照'
- en: '**Step 3\. Observing related statistics** - First, let''s see some aggregate
    values:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3.观察相关统计数据** - 首先，让我们看一些聚合值：'
- en: '[PRE7]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s see the statistics about the balance:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下余额的统计信息：
- en: '[PRE8]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let''s see the creditability per average balance:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下平均余额的信用性：
- en: '[PRE9]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output of the three lines:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 三行的输出：
- en: '![](img/00030.gif)**Figure 9:** Some statistics of the dataset'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00030.gif)**图9：**数据集的一些统计信息'
- en: '**Step 4\. Feature vectors and labels creation** - As you can see, the credibility
    column is the response column, and, for the result, we need to create the feature
    vector without considering this column. Now, let''s create the feature column
    as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4.特征向量和标签的创建** - 如您所见，可信度列是响应列，为了得到结果，我们需要创建不考虑此列的特征向量。现在，让我们创建特征列如下：'
- en: '[PRE10]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s assemble all the features of these selected columns using `VectorAssembler()`
    API as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`VectorAssembler()` API组装这些选定列的所有特征：
- en: '[PRE11]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now let''s see what the feature vectors look like:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下特征向量的样子：
- en: '[PRE12]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding line shows the features created by the VectorAssembler transformer:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 前一行显示了由VectorAssembler转换器创建的特征：
- en: '![](img/00360.gif)**Figure 10:** Generating features for ML models using VectorAssembler'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00360.gif)**图10：**使用VectorAssembler为ML模型生成特征'
- en: 'Now, let''s create a new column as a label from the old response column creditability
    using `StringIndexer` as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`StringIndexer`从旧的响应列creditability创建一个新的标签列，如下所示：
- en: '[PRE13]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding line shows the features and labels created by the `VectorAssembler`
    transformer:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 前一行显示了`VectorAssembler`转换器创建的特征和标签：
- en: '![](img/00274.gif)**Figure 11:** Corresponding labels and feature for ML models
    using VectorAssembler'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00274.gif)**图11：** 使用VectorAssembler的ML模型的相应标签和特征'
- en: '**Step 5.** Prepare the training and test set:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5.** 准备训练集和测试集：'
- en: '[PRE14]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Step 6\. Train the random forest model** - At first, instantiate the model:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6. 训练随机森林模型** - 首先，实例化模型：'
- en: '[PRE15]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For an explanation of the preceding parameters, refer to the random forest
    algorithm section in this chapter. Now, let''s train the model using the training
    set:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 有关上述参数的解释，请参阅本章中的随机森林算法部分。现在，让我们使用训练集训练模型：
- en: '[PRE16]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Step 7.** Compute the raw prediction for the test set:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7.** 计算测试集的原始预测：'
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s see the top 20 rows of this DataFrame:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个DataFrame的前20行：
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding line shows the DataFrame containing the label, raw prediction,
    probablity, and actual prediciton:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 前一行显示了包含标签、原始预测、概率和实际预测的DataFrame：
- en: '![](img/00040.gif)**Figure 12:** The DataFrame containing raw and actual prediction
    for test set'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00040.gif)**图12：** 包含测试集的原始和实际预测的DataFrame'
- en: Now after seeing the prediction from the last column, a bank can make a decision
    about the applications for which the application should be accepted.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在看到最后一列的预测之后，银行可以对申请做出决定，决定是否接受申请。
- en: '**Step 8\. Model evaluation before tuning** - Instantiate the binary evaluator:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8. 模型调优前的模型评估** - 实例化二元评估器：'
- en: '[PRE19]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Calculate the accuracy of the prediction for the test set as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 计算测试集的预测准确率如下：
- en: '[PRE20]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The accuracy before pipeline fitting: `0.751921784149243`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 管道拟合前的准确率：`0.751921784149243`
- en: 'This time, the accuracy is 75%, which is not that good. Let''s compute other
    important performance metrics for the binary classifier like **area under receiver
    operating characteristic** (**AUROC**) and **area under precision recall curve**
    (**AUPRC**):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，准确率是75%，并不是很好。让我们计算二元分类器的其他重要性能指标，比如**接收器操作特征下面积**（**AUROC**）和**精确度召回曲线下面积**（**AUPRC**）：
- en: '[PRE21]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `printlnMetric()` method goes as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`printlnMetric()` 方法如下：'
- en: '[PRE22]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, let''s compute a few more performance metrics using the `RegressionMetrics
    ()` API for the random forest model we used during training:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用训练过程中使用的随机森林模型的`RegressionMetrics()` API计算一些额外的性能指标：
- en: '[PRE23]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, let''s see how our model is:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的模型如何：
- en: '[PRE24]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We get the following output:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE25]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Not that bad! However, not satisfactory either, right? Let's tune the model
    using grid search and cross-validation techniques.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 不算太糟！但也不尽如人意，对吧？让我们使用网格搜索和交叉验证技术调优模型。
- en: '**Step 9\. Model tuning using grid search and cross-validation** - First, let''s
    use the `ParamGridBuilder` API to construct a grid of parameters to search over
    the param grid consisting of 20 to 70 trees with `maxBins` between 25 and 30,
    `maxDepth` between 5 and 10, and impurity as entropy and gini:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9. 使用网格搜索和交叉验证进行模型调优** - 首先，让我们使用`ParamGridBuilder` API构建一个参数网格，搜索20到70棵树，`maxBins`在25到30之间，`maxDepth`在5到10之间，以及熵和基尼作为不纯度：'
- en: '[PRE26]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s train the cross-validator model using the training set as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用训练集训练交叉验证模型：
- en: '[PRE27]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Compute the raw prediction for the test set as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下方式计算测试集的原始预测：
- en: '[PRE28]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Step 10\. Evaluation of the model after tuning** - Let''s see the accuracy:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤10. 调优后模型的评估** - 让我们看看准确率：'
- en: '[PRE29]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We get the following output:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE30]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, it''s above 83%. Good improvement indeed! Let''s see the two other metrics
    computing AUROC and AUPRC:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，准确率超过83%。确实有很大的改进！让我们看看计算AUROC和AUPRC的另外两个指标：
- en: '[PRE31]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We get the following output:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE32]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now based on the `RegressionMetrics` API, compute the other metrics:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在基于`RegressionMetrics` API，计算其他指标：
- en: '[PRE33]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We get the following output:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE34]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Step 11\. Finding the best cross-validated model** - Finally, let''s find
    the best cross-validated model information:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤11. 寻找最佳的交叉验证模型** - 最后，让我们找到最佳的交叉验证模型信息：'
- en: '[PRE35]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We get the following output:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE36]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: A recommendation system with Spark
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark的推荐系统
- en: A recommender system tries to predict potential items a user might be interested
    in based on a history from other users. Model-based collaborative filtering is
    commonly used in many companies such as Netflix. It is to be noted that Netflix
    is an American entertainment company founded by Reed Hastings and Marc Randolph
    on August 29, 1997, in Scotts Valley, California. It specializes in and provides
    streaming media and video-on-demand online and DVD by mail. In 2013, Netflix expanded
    into film and television production, as well as online distribution. As of 2017,
    the company has its headquarters in Los Gatos, California (source Wikipedia).
    Netflix is a recommender system for a real-time movie recommendation. In this
    section, we will see a complete example of how it works toward recommending movies
    for new users.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统试图根据其他用户的历史来预测用户可能感兴趣的潜在项目。基于模型的协同过滤在许多公司中被广泛使用，如Netflix。需要注意的是，Netflix是一家美国娱乐公司，由里德·黑斯廷斯和马克·兰道夫于1997年8月29日在加利福尼亚州斯科茨谷成立。它专门提供流媒体和在线点播以及DVD邮寄服务。2013年，Netflix扩展到了电影和电视制作，以及在线发行。截至2017年，该公司总部位于加利福尼亚州洛斯加托斯（来源：维基百科）。Netflix是一个实时电影推荐系统。在本节中，我们将看到一个完整的示例，说明它是如何为新用户推荐电影的。
- en: Model-based recommendation with Spark
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行基于模型的推荐
- en: 'The implementation in Spark MLlib supports model-based collaborative filtering.
    In the model based collaborative filtering technique, users and products are described
    by a small set of factors, also called the **latent factors** (**LFs**). From
    the following figure, you can get some idea of a different recommender system.
    *Figure 13* justifies why are going to use model-based collaborative filtering
    for the movie recommendation example:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib中的实现支持基于模型的协同过滤。在基于模型的协同过滤技术中，用户和产品由一小组因子描述，也称为**潜在因子**（**LFs**）。从下图中，您可以对不同的推荐系统有一些了解。*图13*
    说明了为什么我们将在电影推荐示例中使用基于模型的协同过滤：
- en: '![](img/00284.gif)**Figure 13**: A comparative view of a different recommendation
    system'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：不同推荐系统的比较视图
- en: 'The LFs are then used for predicting the missing entries. Spark API provides
    the implementation of the alternating least squares (also known as the ALS widely)
    algorithm, which is used to learn these latent factors by considering six parameters,
    including:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用LFs来预测缺失的条目。Spark API提供了交替最小二乘（也称为ALS广泛）算法的实现，该算法通过考虑六个参数来学习这些潜在因素，包括：
- en: '*numBlocks*: This is the number of blocks used to parallelize computation (set
    to -1 to auto-configure).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*numBlocks*：这是用于并行计算的块数（设置为-1以自动配置）。'
- en: '*rank*: This is the number of latent factors in the model.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*rank*：这是模型中潜在因素的数量。'
- en: '*iterations*: This is the number of iterations of ALS to run. ALS typically
    converges to a reasonable solution in 20 iterations or less.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*iterations*：这是运行ALS的迭代次数。ALS通常在20次迭代或更少的情况下收敛到一个合理的解决方案。'
- en: '*lambda*: This specifies the regularization parameter in ALS.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*lambda*：这指定ALS中的正则化参数。'
- en: '*implicitPrefs*: This specifies whether to use the *explicit feedback* ALS
    variant or one adapted for *implicit feedback* data.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*implicitPrefs*：这指定是否使用*显式反馈*ALS变体或适用于*隐式反馈*数据的变体。'
- en: '*alpha*: This is a parameter applicable to the implicit feedback variant of
    ALS that governs the *baseline* confidence in preference observations.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*alpha*：这是适用于ALS隐式反馈变体的参数，它控制对偏好观察的*基线*置信度。'
- en: 'Note that to construct an ALS instance with default parameters; you can set
    the value based on your requirements. The default values are as follows: `numBlocks:
    -1`, `rank: 10`, `iterations: 10`, `lambda: 0.01`, `implicitPrefs: false`, and
    `alpha: 1.0`.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，要使用默认参数构建ALS实例，您可以根据自己的需求设置值。默认值如下：`numBlocks: -1`，`rank: 10`，`iterations:
    10`，`lambda: 0.01`，`implicitPrefs: false`，和`alpha: 1.0`。'
- en: Data exploration
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: The movie and the corresponding rating dataset were downloaded from the MovieLens
    Website ([https://movielens.org](https://movielens.org)). According to the data
    description on the MovieLens Website, all the ratings are described in the `ratings.csv`
    file. Each row of this file followed by the header represents one rating for one
    movie by one user.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 电影和相应的评分数据集是从MovieLens网站（[https://movielens.org](https://movielens.org)）下载的。根据MovieLens网站上的数据描述，所有评分都在`ratings.csv`文件中描述。该文件的每一行在标题之后表示一个用户对一部电影的评分。
- en: 'The CSV dataset has the following columns: **userId**, **movieId**, **rating**,
    and **timestamp**, as shown in *Figure 14*. The rows are ordered first by the
    **userId**, and within the user, by **movieId**. Ratings are made on a five-star
    scale, with half-star increments (0.5 stars up to 5.0 stars). The timestamps represent
    the seconds since midnight Coordinated Universal Time (UTC) on January 1, 1970,
    where we have 105,339 ratings from the 668 users on 10,325 movies:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: CSV数据集有以下列：**userId**，**movieId**，**rating**和**timestamp**，如*图14*所示。行首先按**userId**排序，然后按**movieId**排序。评分是在五星级评分上进行的，可以增加半星（0.5星至5.0星）。时间戳表示自1970年1月1日协调世界时（UTC）午夜以来的秒数，我们有来自668个用户对10325部电影的105339个评分：
- en: '![](img/00244.gif)**Figure 14:** A snap of the ratings dataset'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：评分数据集的快照
- en: 'On the other hand, the movie information is contained in the `movies.csv` file.
    Each row apart from the header information represents one movie containing the
    columns: movieId, title, and genres (see *Figure 14*). Movie titles are either
    created or inserted manually or imported from the website of the movie database
    at [https://www.themoviedb.org/](https://www.themoviedb.org/). The release year,
    however, is shown in the bracket. Since movie titles are inserted manually, some
    errors or inconsistencies may exist in these titles. Readers are, therefore, recommended
    to check the IMDb database ([https://www.ibdb.com/](https://www.ibdb.com/)) to
    make sure if there are no inconsistencies or incorrect titles with their corresponding
    release year.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，电影信息包含在`movies.csv`文件中。除了标题信息之外，每一行代表一个包含列：movieId，title和genres的电影（见*图14*）。电影标题可以手动创建或插入，也可以从电影数据库网站[https://www.themoviedb.org/](https://www.themoviedb.org/)导入。然而，发行年份显示在括号中。由于电影标题是手动插入的，因此这些标题可能存在一些错误或不一致。因此，建议读者检查IMDb数据库（[https://www.ibdb.com/](https://www.ibdb.com/)）以确保没有不一致或不正确的标题与其对应的发行年份。
- en: 'Genres are a separated list, and are selected from the following genre categories:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 类型是一个分开的列表，可以从以下类型类别中选择：
- en: Action, Adventure, Animation, Children's, Comedy, Crime
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作，冒险，动画，儿童，喜剧，犯罪
- en: Documentary, Drama, Fantasy, Film-Noir, Horror, Musical
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纪录片，戏剧，奇幻，黑色电影，恐怖，音乐
- en: Mystery, Romance, Sci-Fi, Thriller, Western, War
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神秘，浪漫，科幻，惊悚，西部，战争
- en: '![](img/00356.gif)**Figure 15**: The title and genres for the top 20 movies'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：前20部电影的标题和类型
- en: Movie recommendation using ALS
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ALS进行电影推荐
- en: In this subsection, we will show you how to recommend the movie for other users
    through a step-by-step example from data collection to movie recommendation.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将通过从数据收集到电影推荐的逐步示例向您展示如何为其他用户推荐电影。
- en: '**Step 1\. Load, parse and explore the movie and rating Dataset** - Here is
    the code illustrated:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1. 加载、解析和探索电影和评分数据集** - 以下是示例代码：'
- en: '[PRE37]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This code segment should return you the DataFrame of the ratings. On the other
    hand, the following code segment shows you the DataFrame of movies:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码应该返回您的评分数据框。另一方面，以下代码段显示了电影的数据框：
- en: '[PRE38]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '**Step 2\. Register both DataFrames as temp tables to make querying easier**
    - To register both Datasets, we can use the following code:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2. 注册两个数据框为临时表，以便更轻松地查询** - 要注册两个数据集，我们可以使用以下代码：'
- en: '[PRE39]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This will help to make the in-memory querying faster by creating a temporary
    view as a table in min-memory. The lifetime of the temporary table using the `createOrReplaceTempView
    ()` method is tied to the `[[SparkSession]]` that was used to create this DataFrame.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这将通过在内存中创建一个临时视图作为表来加快内存中的查询速度。使用`createOrReplaceTempView()`方法创建的临时表的生命周期与用于创建此DataFrame的`[[SparkSession]]`相关联。
- en: '**Step 3\. Explore and query for related statistics** - Let''s check the ratings-related
    statistics. Just use the following code lines:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3. 探索和查询相关统计数据** - 让我们检查与评分相关的统计数据。只需使用以下代码行：'
- en: '[PRE40]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: You should find 105,339 ratings from 668 users on 10,325 movies. Now, let's
    get the maximum and minimum ratings along with the count of users who have rated
    a movie. However, you need to perform a SQL query on the ratings table we just
    created in-memory in the previous step. Making a query here is simple, and it
    is similar to making a query from a MySQL database or RDBMS. However, if you are
    not familiar with SQL-based queries, you are recommended to look at the SQL query
    specification to find out how to perform a selection using `SELECT` from a particular
    table, how to perform the ordering using `ORDER`, and how to perform a joining
    operation using the `JOIN` keyword.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该找到来自668个用户对10325部电影进行了105339次评分。现在，让我们获取最大和最小评分，以及对电影进行评分的用户数量。然而，你需要在我们在上一步中在内存中创建的评分表上执行SQL查询。在这里进行查询很简单，类似于从MySQL数据库或RDBMS进行查询。然而，如果你不熟悉基于SQL的查询，建议查看SQL查询规范，了解如何使用`SELECT`从特定表中进行选择，如何使用`ORDER`进行排序，以及如何使用`JOIN`关键字进行连接操作。
- en: 'Well, if you know the SQL query, you should get a new dataset by using a complex
    SQL query as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，如果你知道SQL查询，你应该通过使用以下复杂的SQL查询来获得一个新的数据集：
- en: '[PRE41]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We get the following output:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/00073.gif)**Figure 16:** max, min ratings along with the count of users
    who have rated a movie'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：最大、最小评分以及对电影进行评分的用户数量
- en: 'To get an insight, we need to know more about the users and their ratings.
    Now, let''s find the top most active users and how many times they rated a movie:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地了解，我们需要了解更多关于用户和他们的评分。现在，让我们找出最活跃的用户以及他们对电影进行评分的次数：
- en: '[PRE42]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/00262.jpeg)**Figure 17:** top 10 most active users and how many times
    they rated a movie'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：前10名最活跃用户以及他们对电影进行评分的次数
- en: 'Let''s take a look at a particular user, and find the movies that, say user
    668, rated higher than 4:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个特定的用户，并找出，比如说用户668，对哪些电影进行了高于4的评分：
- en: '[PRE43]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![](img/00035.gif)**Figure 18:** movies that user 668 rated higher than 4'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：用户668对评分高于4的电影
- en: '**Step 4\. Prepare training and test rating data and see the counts** - The
    following code splits ratings RDD into training data RDD (75%) and test data RDD
    (25%). Seed here is optional but required for the reproducibility purpose:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4. 准备训练和测试评分数据并查看计数** - 以下代码将评分RDD分割为训练数据RDD（75%）和测试数据RDD（25%）。这里的种子是可选的，但是出于可重现性的目的是必需的：'
- en: '[PRE44]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: You should find that there are 78,792 ratings in the training and 26,547 ratings
    in the test
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该发现训练中有78792个评分，测试中有26547个评分
- en: DataFrame.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame。
- en: '**Step 5\. Prepare the data for building the recommendation model using ALS**
    - The ALS algorithm takes the RDD of `Rating` for the training purpose. The following
    code illustrates for building the recommendation model using APIs:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5. 准备数据以构建使用ALS的推荐模型** - ALS算法使用训练目的的`Rating`的RDD。以下代码说明了使用API构建推荐模型的过程：'
- en: '[PRE45]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The `ratingsRDD` is an RDD of ratings that contains the `userId`, `movieId`,
    and corresponding ratings from the training dataset that we prepared in the previous
    step. On the other hand, a test RDD is also required for evaluating the model.
    The following `testRDD` also contains the same information coming from the test
    DataFrame we prepared in the previous step:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`ratingsRDD`是一个包含来自我们在上一步准备的训练数据集的`userId`、`movieId`和相应评分的评分的RDD。另一方面，还需要一个测试RDD来评估模型。以下`testRDD`也包含了来自我们在上一步准备的测试DataFrame的相同信息：'
- en: '[PRE46]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Step 6\. Build an ALS user product matrix** - Build an ALS user matrix model
    based on the `ratingsRDD` by specifying the maximal iteration, a number of blocks,
    alpha, rank, lambda, seed, and `implicitPrefs`. Essentially, this technique predicts
    missing ratings for specific users for specific movies based on ratings for those
    movies from other users who did similar ratings for other movies:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6. 构建ALS用户产品矩阵** - 基于`ratingsRDD`构建ALS用户矩阵模型，指定最大迭代次数、块数、alpha、rank、lambda、种子和`implicitPrefs`。基本上，这种技术根据其他用户对其他电影的评分来预测特定用户对特定电影的缺失评分。'
- en: '[PRE47]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Finally, we iterated the model for learning 15 times. With this setting, we
    got good prediction accuracy. Readers are suggested to apply the hyperparameter
    tuning to get to know the optimum values for these parameters. Furthermore, set
    the number of blocks for both user blocks and product blocks to parallelize the
    computation into a pass -1 for an auto-configured number of blocks. The value
    is -1.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对模型进行了15次学习迭代。通过这个设置，我们得到了良好的预测准确性。建议读者对超参数进行调整，以了解这些参数的最佳值。此外，设置用户块和产品块的块数以将计算并行化为一次传递-1以进行自动配置的块数。该值为-1。
- en: '**Step 7\. Making predictions** - Let''s get the top six movie predictions
    for user 668\. The following source code can be used to make the predictions:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7. 进行预测** - 让我们为用户668获取前六部电影的预测。以下源代码可用于进行预测：'
- en: '[PRE48]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The preceding code segment produces the following output containing the rating
    prediction with `UserID`, `MovieID`, and corresponding `Rating` for that movie:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码段产生了包含`UserID`、`MovieID`和相应`Rating`的评分预测的输出：
- en: '![](img/00376.gif)**Figure 19**: top six movie predictions for user 668'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：用户668的前六部电影预测
- en: '**Step 8\. Evaluating the model** - In order to verify the quality of the models,
    **Root Mean Squared Error** (**RMSE**) is used to measure the differences between
    values predicted by a model and the values actually observed. By default, the
    smaller the calculated error, the better the model. In order to test the quality
    of the model, the test data is used (which was split above in step 4). According
    to many machine learning practitioners, the RMSE is a good measure of accuracy,
    but only to compare forecasting errors of different models for a particular variable
    and not between variables, as it is scale-dependent. The following line of code
    calculates the RMSE value for the model that was trained using the training set:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '**第8步。评估模型** - 为了验证模型的质量，使用**均方根误差**（**RMSE**）来衡量模型预测值与实际观察值之间的差异。默认情况下，计算出的误差越小，模型越好。为了测试模型的质量，使用测试数据（在第4步中拆分）进行测试。根据许多机器学习从业者的说法，RMSE是衡量准确性的一个很好的指标，但只能用于比较特定变量的不同模型的预测误差，而不能用于变量之间的比较，因为它依赖于比例。以下代码行计算了使用训练集训练的模型的RMSE值。'
- en: '[PRE49]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'It is to be noted that the `computeRmse()` is a UDF, that goes as follows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是`computeRmse()`是一个UDF，其步骤如下：
- en: '[PRE50]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The preceding method computes the RMSE to evaluate the model. Less the RMSE,
    the better the model and its prediction capability.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方法计算了RMSE以评估模型。RMSE越小，模型及其预测能力就越好。
- en: 'For the earlier setting, we got the following output:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 对于先前的设置，我们得到了以下输出：
- en: '[PRE51]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The performance of the preceding model could be increased further we believe.
    Interested readers should refer to this URL for more on tuning the ML-based ALS
    models [https://spark.apache.org/docs/preview/ml-collaborative-filtering.html](https://spark.apache.org/docs/preview/ml-collaborative-filtering.html).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信前面模型的性能可以进一步提高。感兴趣的读者应该参考此网址，了解有关调整基于ML的ALS模型的更多信息[https://spark.apache.org/docs/preview/ml-collaborative-filtering.html](https://spark.apache.org/docs/preview/ml-collaborative-filtering.html)。
- en: The topic modeling technique is widely used in the task of mining text from
    a large collection of documents. These topics can then be used to summarize and
    organize documents that include the topic terms and their relative weights. In
    the next section, we will show an example of topic modeling using the **latent
    dirichlet allocation** (**LDA**) algorithm.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模技术广泛用于从大量文档中挖掘文本的任务。然后可以使用这些主题来总结和组织包括主题术语及其相对权重的文档。在下一节中，我们将展示使用**潜在狄利克雷分配**（**LDA**）算法进行主题建模的示例。
- en: Topic modelling - A best practice for text clustering
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模-文本聚类的最佳实践
- en: The topic modeling technique is widely used in the task of mining text from
    a large collection of documents. These topics can then be used to summarize and
    organize documents that include the topic terms and their relative weights. The
    dataset that will be used for this example is just in plain text, however, in
    an unstructured format. Now the challenging part is finding useful patterns about
    the data using LDA called topic modeling.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模技术广泛用于从大量文档中挖掘文本的任务。然后可以使用这些主题来总结和组织包括主题术语及其相对权重的文档。将用于此示例的数据集只是以纯文本的形式存在，但是以非结构化格式存在。现在具有挑战性的部分是使用称为主题建模的LDA找到有关数据的有用模式。
- en: How does LDA work?
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDA是如何工作的？
- en: LDA is a topic model which infers topics from a collection of text documents.
    LDA can be thought of as a clustering algorithm where topics correspond to cluster
    centers, and documents correspond to examples (rows) in a dataset. Topics and
    documents both exist in a feature space, where feature vectors are vectors of
    word counts (bag of words). Instead of estimating a clustering using a traditional
    distance, LDA uses a function based on a statistical model of how text documents
    are generated.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一种主题模型，它从一系列文本文档中推断主题。LDA可以被视为一种聚类算法，其中主题对应于簇中心，文档对应于数据集中的示例（行）。主题和文档都存在于特征空间中，其中特征向量是词频的向量（词袋）。LDA不是使用传统距离来估计聚类，而是使用基于文本文档生成的统计模型的函数。
- en: 'LDA supports different inference algorithms via `setOptimizer` function. `EMLDAOptimizer`
    learns clustering using expectation-maximization on the likelihood function and
    yields comprehensive results, while `OnlineLDAOptimizer` uses iterative mini-batch
    sampling for online variational inference and is generally memory friendly. LDA
    takes in a collection of documents as vectors of word counts and the following
    parameters (set using the builder pattern):'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: LDA通过`setOptimizer`函数支持不同的推断算法。`EMLDAOptimizer`使用期望最大化来学习聚类，并产生全面的结果，而`OnlineLDAOptimizer`使用迭代小批量抽样进行在线变分推断，并且通常对内存友好。LDA接受一系列文档作为词频向量以及以下参数（使用构建器模式设置）：
- en: '`k`: Number of topics (that is, cluster centers).'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k`：主题数（即，簇中心）。'
- en: '`optimizer`: Optimizer to use for learning the LDA model, either `EMLDAOptimizer`
    or `OnlineLDAOptimizer`.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`：用于学习LDA模型的优化器，可以是`EMLDAOptimizer`或`OnlineLDAOptimizer`。'
- en: '`docConcentration`: Dirichlet parameter for prior over documents'' distributions
    over topics. Larger values encourage smoother inferred distributions.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docConcentration`：文档分布在主题上的Dirichlet先验参数。较大的值鼓励更平滑的推断分布。'
- en: '`topicConcentration`: Dirichlet parameter for prior over topics'' distributions
    over terms (words). Larger values encourage smoother inferred distributions.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topicConcentration`：主题分布在术语（词）上的Dirichlet先验参数。较大的值鼓励更平滑的推断分布。'
- en: '`maxIterations`: Limit on the number of iterations.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxIterations`：迭代次数上限。'
- en: '`checkpointInterval`: If using checkpointing (set in the Spark configuration),
    this parameter specifies the frequency with which checkpoints will be created.
    If `maxIterations` is large, using checkpointing can help reduce shuffle file
    sizes on disk and help with failure recovery.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`checkpointInterval`：如果使用检查点（在Spark配置中设置），此参数指定将创建检查点的频率。如果`maxIterations`很大，使用检查点可以帮助减少磁盘上的洗牌文件大小，并有助于故障恢复。'
- en: Particularly, we would like to discuss the topics people talk about most from
    the large collection of texts. Since the release of Spark 1.3, MLlib supports
    the LDA, which is one of the most successfully used topic modeling techniques
    in the area of text mining and **Natural Language Processing** (**NLP**). Moreover,
    LDA is also the first MLlib algorithm to adopt Spark GraphX.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们想讨论人们在大量文本中谈论的主题。自Spark 1.3发布以来，MLlib支持LDA，这是文本挖掘和**自然语言处理**（**NLP**）领域中最成功使用的主题建模技术之一。此外，LDA也是第一个采用Spark
    GraphX的MLlib算法。
- en: To get more information about how the theory behind the LDA works, please refer
    to David M. Blei, Andrew Y. Ng and Michael I. Jordan, Latent, Dirichlet Allocation,
    *Journal of Machine Learning Research 3* (2003) 993-1022.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解LDA背后的理论如何工作的更多信息，请参考David M. Blei，Andrew Y. Ng和Michael I. Jordan，Latent，Dirichlet
    Allocation，*Journal of Machine Learning Research 3*（2003）993-1022。
- en: 'The following figure shows the topic distribution from randomly generated tweet
    text:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了从随机生成的推文文本中的主题分布：
- en: '![](img/00165.jpeg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00165.jpeg)'
- en: '**Figure 20**: The topic distribution and how it looks like'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '**图20**：主题分布及其外观'
- en: In this section, we will look at an example of topic modeling using the LDA
    algorithm of Spark MLlib with unstructured raw tweets datasets. Note that here
    we have used LDA, which is one of the most popular topic modeling algorithms commonly
    used for text mining. We could use more robust topic modeling algorithms such
    as **Probabilistic Latent Sentiment Analysis** (**pLSA**), **pachinko allocation
    model** (**PAM**), or **hierarchical dirichlet process** (**HDP**) algorithms.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看一个使用Spark MLlib的LDA算法对非结构化原始推文数据集进行主题建模的示例。请注意，这里我们使用了LDA，这是最常用于文本挖掘的主题建模算法之一。我们可以使用更健壮的主题建模算法，如**概率潜在情感分析**（**pLSA**）、**赌博分配模型**（**PAM**）或**分层狄利克雷过程**（**HDP**）算法。
- en: However, pLSA has the overfitting problem. On the other hand, both HDP and PAM
    are more complex topic modeling algorithms used for complex text mining such as
    mining topics from high dimensional text data or documents of unstructured text.
    Moreover, to this date, Spark has implemented only one topic modeling algorithm,
    that is LDA. Therefore, we have to use LDA reasonably.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，pLSA存在过拟合问题。另一方面，HDP和PAM是更复杂的主题建模算法，用于复杂的文本挖掘，如从高维文本数据或非结构化文档中挖掘主题。此外，迄今为止，Spark只实现了一个主题建模算法，即LDA。因此，我们必须合理使用LDA。
- en: Topic modeling with Spark MLlib
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark MLlib进行主题建模
- en: 'In this subsection, we represent a semi-automated technique of topic modeling
    using Spark. Using other options as defaults, we train LDA on the dataset downloaded
    from the GitHub URL at [https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test](https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test).
    The following steps show the topic modeling from data reading to printing the
    topics, along with their term-weights. Here''s the short workflow of the topic
    modeling pipeline:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小节中，我们使用Spark表示了一种半自动的主题建模技术。使用其他选项作为默认值，我们在从GitHub URL下载的数据集上训练LDA，网址为[https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test](https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test)。以下步骤展示了从数据读取到打印主题及其词权重的主题建模过程。以下是主题建模流程的简要工作流程：
- en: '[PRE52]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The actual computation on topic modeling is done in the `LDAforTM` class. The
    `Params` is a case class, which is used for loading the parameters to train the
    LDA model. Finally, we train the LDA model using the parameters setting via the
    `Params` class. Now, we will explain each step broadly with step-by-step source
    code:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模的实际计算是在`LDAforTM`类中完成的。`Params`是一个案例类，用于加载参数以训练LDA模型。最后，我们使用`Params`类设置的参数来训练LDA模型。现在，我们将逐步解释每个步骤的源代码：
- en: '**Step 1\. Creating a Spark session** - Let''s create a Spark session by defining
    number of computing core, SQL warehouse, and application name as follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1\. 创建一个Spark会话** - 让我们通过定义计算核心数量、SQL仓库和应用程序名称来创建一个Spark会话，如下所示：'
- en: '[PRE53]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**Step 2\. Creating vocabulary, tokens count to train LDA after text pre-processing**
    - At first, load the documents, and prepare them for LDA as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2\. 创建词汇表，标记计数以在文本预处理后训练LDA** - 首先，加载文档，并准备好LDA，如下所示：'
- en: '[PRE54]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The pre-process method is used to process the raw texts. At first, let''s read
    the whole texts using the `wholeTextFiles()` method as follows:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理方法用于处理原始文本。首先，让我们使用`wholeTextFiles()`方法读取整个文本，如下所示：
- en: '[PRE55]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'In the preceding code, paths are the path of the text files. Then, we need
    to prepare a morphological RDD from the raw text based on the lemma texts as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，paths是文本文件的路径。然后，我们需要根据词形文本准备一个形态学RDD，如下所示：
- en: '[PRE56]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Here the `getLemmaText()` method from the `helperForLDA` class supplies the
    lemma texts after filtering the special characters such as `("""[! @ # $ % ^ &
    * ( ) _ + - − , " '' ; : . ` ? --]` as regular expressions using the `filterSpaecialChatacters()`
    method.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，`helperForLDA`类中的`getLemmaText()`方法在使用`filterSpaecialChatacters()`方法过滤特殊字符（例如`("""[!
    @ # $ % ^ & * ( ) _ + - − , " '' ; : . ` ? --]`）后提供了词形文本。'
- en: 'It is to be noted that the `Morphology()` class computes the base form of English
    words, by removing just inflections (not derivational morphology). That is, it
    only does noun plurals, pronoun case, and verb endings, and not things like comparative
    adjectives or derived nominals. This comes from the Stanford NLP group. To use
    this, you should have the following import in the main class file: `edu.stanford.nlp.process.Morphology`.
    In the `pom.xml` file, you will have to include the following entries as dependencies:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，`Morphology()`类计算英语单词的基本形式，只删除屈折（不是派生形态）。也就是说，它只处理名词复数、代词格和动词词尾，而不处理比较级形容词或派生名词等。这来自于斯坦福NLP组。要使用这个，你应该在主类文件中包含以下导入：`edu.stanford.nlp.process.Morphology`。在`pom.xml`文件中，你将需要包含以下条目作为依赖项：
- en: '[PRE57]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The method goes as follows:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 方法如下：
- en: '[PRE58]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The `filterSpecialCharacters()` goes as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`filterSpecialCharacters()`如下所示：'
- en: '`def filterSpecialCharacters(document: String) = document.replaceAll("""[!
    @ # $ % ^ & * ( ) _ + - − , " '' ; : . ` ? --]""", " ")`. Once we have the RDD
    with special characters removed in hand, we can create a DataFrame for building
    the text analytics pipeline:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '`def filterSpecialCharacters(document: String) = document.replaceAll("""[!
    @ # $ % ^ & * ( ) _ + - − , " '' ; : . ` ? --]""", " ")`。一旦我们手头有去除特殊字符的RDD，我们就可以创建一个用于构建文本分析管道的DataFrame：'
- en: '[PRE59]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'So, the DataFrame consist of only of the documents tag. A snapshot of the DataFrame
    is as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DataFrame仅包含文档标签。DataFrame的快照如下：
- en: '![](img/00332.gif)**Figure 21**: Raw texts'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00332.gif)**图21**：原始文本'
- en: 'Now if you examine the preceding DataFrame carefully, you will see that we
    still need to tokenize the items. Moreover, there are stop words in a DataFrame
    such as this, so we need to remove them as well. At first, let''s tokenize them
    using the `RegexTokenizer` API as follows:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您仔细检查前面的DataFrame，您会发现我们仍然需要对项目进行标记。此外，在这样的DataFrame中还有停用词，因此我们也需要将它们删除。首先，让我们使用`RegexTokenizer`
    API对它们进行标记如下：
- en: '[PRE60]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now, let''s remove all the stop words as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们按如下方式删除所有停用词：
- en: '[PRE61]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Furthermore, we also need to apply count victories to find only the important
    features from the tokens. This will help make the pipeline chained at the pipeline
    stage. Let''s do it as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要应用计数胜利以仅从标记中找到重要特征。这将有助于使管道在管道阶段链接。让我们按如下方式做：
- en: '[PRE62]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now, create the pipeline by chaining the transformers (`tokenizer`, `stopWordsRemover`,
    and `countVectorizer`) as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过链接转换器（`tokenizer`、`stopWordsRemover`和`countVectorizer`）创建管道如下：
- en: '[PRE63]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Let''s fit and transform the pipeline towards the vocabulary and number of
    tokens:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拟合和转换管道以适应词汇和标记数：
- en: '[PRE64]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, return the vocabulary and token count pairs as follows:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，返回词汇和标记计数对如下：
- en: '[PRE65]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now, let''s see the statistics of the training data:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看训练数据的统计信息：
- en: '[PRE66]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We get the following output:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE67]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '****Step 4\. Instantiate the LDA model before training****'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4. 在训练之前实例化LDA模型**'
- en: '[PRE68]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '**Step 5: Set the NLP optimizer**'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤5：设置NLP优化器
- en: 'For better and optimized results from the LDA model, we need to set the optimizer
    for the LDA model. Here we use the `EMLDAOPtimizer` optimizer. You can also use
    the `OnlineLDAOptimizer()` optimizer. However, you need to add (1.0/actualCorpusSize)
    to `MiniBatchFraction` to be more robust on tiny datasets. The whole operation
    goes as follows. First, instantiate the `EMLDAOptimizer` as follows:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从LDA模型获得更好和优化的结果，我们需要为LDA模型设置优化器。这里我们使用`EMLDAOPtimizer`优化器。您还可以使用`OnlineLDAOptimizer()`优化器。但是，您需要将(1.0/actualCorpusSize)添加到`MiniBatchFraction`中，以使其在小型数据集上更加稳健。整个操作如下。首先，实例化`EMLDAOptimizer`如下：
- en: '[PRE69]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now set the optimizer using the `setOptimizer()` method from the LDA API as
    follows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用LDA API的`setOptimizer()`方法设置优化器如下：
- en: '[PRE70]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The `Params` case class is used to define the parameters to training the LDA
    model. This goes as follows:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '`Params` case类用于定义训练LDA模型的参数。具体如下：'
- en: '[PRE71]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'For a better result you can set these parameters in a naive way. Alternatively,
    you should go with the cross-validation for even better performance. Now if you
    want to checkpoint the current parameters, use the following line of codes:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好的结果，您可以以一种天真的方式设置这些参数。或者，您应该进行交叉验证以获得更好的性能。现在，如果您想要对当前参数进行检查点，请使用以下代码行：
- en: '[PRE72]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '**Step 6.** Training the LDA model:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6.** 训练LDA模型：'
- en: '[PRE73]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: For the texts we have, the LDA model took 6.309715286 sec to train. Note that
    these timing codes are optional. Here we provide them for reference purposes,
    only to get an idea of the training time.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们拥有的文本，LDA模型花费了6.309715286秒进行训练。请注意，这些时间代码是可选的。我们提供它们仅供参考，只是为了了解训练时间。
- en: '**Step 7\. Measuring the likelihood of the data** - Now, of to get some more
    statistics about the data such as maximum likelihood or log-likelihood, we can
    use the following code:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7.** 测量数据的可能性 - 现在，为了获得有关数据的更多统计信息，如最大似然或对数似然，我们可以使用以下代码：'
- en: '[PRE74]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The preceding code calculates the average log likelihood if the LDA model is
    an instance of the distributed version of the LDA model. We get the following
    output:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码计算了平均对数似然性，如果LDA模型是分布式版本的LDA模型的实例。我们得到以下输出：
- en: '[PRE75]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The likelihood is used after data are available to describe a function of a
    parameter (or parameter vector) for a given outcome. This helps especially for
    estimating a parameter from a set of statistics. For more information on the likelihood
    measurement, interested readers should refer to [https://en.wikipedia.org/wiki/Likelihood_function](https://en.wikipedia.org/wiki/Likelihood_function).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 似然性在数据可用后用于描述给定结果的参数（或参数向量）的函数。这对于从一组统计数据中估计参数特别有帮助。有关似然性测量的更多信息，感兴趣的读者应参考[https://en.wikipedia.org/wiki/Likelihood_function](https://en.wikipedia.org/wiki/Likelihood_function)。
- en: '**Step 8\. Prepare the topics of interests** - Prepare the top five topics
    with each topic having 10 terms. Include the terms and their corresponding weights.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8. 准备感兴趣的主题** - 准备前五个主题，每个主题有10个术语。包括术语及其相应的权重。'
- en: '[PRE76]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '**Step 9\. Topic modeling** - Print the top ten topics, showing the top-weighted
    terms for each topic. Also, include the total weight in each topic as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9. 主题建模** - 打印前十个主题，显示每个主题的权重最高的术语。还包括每个主题的总权重如下：'
- en: '[PRE77]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Now, let''s see the output of our LDA model toward topics modeling:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的LDA模型对主题建模的输出：
- en: '[PRE78]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'From the preceding output, we can see that the topic of the input documents
    is topic 5 having the most weight of `0.31363611105890865`. This topic discusses
    the terms love, long, shore, shower, ring, bring, bear and so on. Now, for a better
    understanding of the flow, here''s the complete source code:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以看到输入文档的主题是主题5，其权重最高为`0.31363611105890865`。该主题讨论了爱、长、海岸、淋浴、戒指、带来、承担等术语。现在，为了更好地理解流程，这是完整的源代码：
- en: '[PRE79]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Scalability of LDA
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDA的可扩展性
- en: 'The previous example shows how to perform topic modeling using the LDA algorithm
    as a standalone application. The parallelization of LDA is not straightforward,
    and there have been many research papers proposing different strategies. The key
    obstacle in this regard is that all methods involve a large amount of communication.
    According to the blog on the Databricks website ([https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html](https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html)),
    here are the statistics of the dataset and related training and test sets that
    were used during the experimentation:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例展示了如何使用LDA算法进行主题建模作为独立应用程序。LDA的并行化并不直接，已经有许多研究论文提出了不同的策略。在这方面的关键障碍是所有方法都涉及大量的通信。根据Databricks网站上的博客([https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html](https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html))，以下是在实验过程中使用的数据集和相关训练和测试集的统计数据：
- en: 'Training set size: 4.6 million documents'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集大小：460万个文档
- en: 'Vocabulary size: 1.1 million terms'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇量：110万个术语
- en: 'Training set size: 1.1 billion tokens (~239 words/document)'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集大小：110亿个标记（~每个文档239个词）
- en: 100 topics
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100个主题
- en: 16-worker EC2 cluster, for example, M4.large or M3.medium depending upon budget
    and requirements
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 16个worker的EC2集群，例如M4.large或M3.medium，具体取决于预算和要求
- en: For the preceding setting, the timing result was 176 secs/iteration on average
    over 10 iterations. From these statistics, it is clear that LDA is quite scalable
    for a very large number of the corpus as well.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前述设置，平均每次迭代的时间结果为176秒/迭代，共进行了10次迭代。从这些统计数据可以清楚地看出，对于非常大量的语料库，LDA是相当可扩展的。
- en: Summary
  id: totrans-425
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we provided theoretical and practical aspects of some advanced
    topics of machine learning with Spark. We also provided some recommendations about
    the best practice in machine learning. Following that, we have seen how to tune
    machine learning models for better and optimized performance using grid search,
    cross-validation, and hyperparameter tuning. In the later section, we have seen
    how to develop a scalable recommendation system using the ALS, which is an example
    of a model-based recommendation system using a model-based collaborative filtering
    approach. Finally, we have seen how to develop a topic modeling application as
    a text clustering technique.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们提供了有关Spark机器学习一些高级主题的理论和实践方面。我们还提供了一些关于机器学习最佳实践的建议。在此之后，我们已经看到如何使用网格搜索、交叉验证和超参数调整来调整机器学习模型，以获得更好和优化的性能。在后面的部分，我们看到了如何使用ALS开发可扩展的推荐系统，这是使用基于模型的协同过滤方法的基于模型的推荐系统的一个示例。最后，我们看到了如何开发主题建模应用作为文本聚类技术。
- en: For additional aspects and topics on machine learning best practice, interested
    readers can refer to the book titled *Large Scale Machine Learning with Spark*
    at [https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark.](https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark)
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习最佳实践的其他方面和主题，感兴趣的读者可以参考名为*Large Scale Machine Learning with Spark*的书籍[https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark.](https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark)
- en: In the next chapter, we will enter into more advanced use of Spark. Although
    we have discussed and provided a comparative analysis on binary and multiclass
    classification, we will get to know more about other multinomial classification
    algorithms with Spark such as Naive Bayes, decision trees, and the One-vs-Rest
    classifier.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将进入更高级的Spark使用。虽然我们已经讨论并提供了关于二元和多类分类的比较分析，但我们将更多地了解Spark中的其他多项式分类算法，如朴素贝叶斯、决策树和一对多分类器。
