- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Machine Learning and AI with Streamlit
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Streamlit进行机器学习与人工智能
- en: A very common situation data scientists find themselves in is at the end of
    the model creation process, not knowing exactly how to convince non-data scientists
    that their model is worthwhile. They might have performance metrics from their
    model or some static visualizations but have no easy way to allow others to interact
    with their model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家常见的一个情况是，在模型创建过程的最后，无法确定如何说服非数据科学家相信他们的模型是有价值的。他们可能有模型的性能指标或一些静态可视化，但没有一个简单的方式让其他人与他们的模型进行互动。
- en: Before Streamlit, there were a couple of other options, the most popular being
    creating a full-fledged app in Flask or Django or even turning a model into an
    **Application Programming Interface** (**API**) and pointing developers toward
    it. These are great options but tend to be time-consuming and suboptimal for valuable
    use cases such as prototyping an app.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在Streamlit之前，有几个其他选项，其中最流行的是在Flask或Django中创建一个完整的应用程序，或者甚至将模型转化为**应用程序编程接口**（**API**），并引导开发人员使用它。这些都是不错的选择，但往往需要耗费时间，并且对于像应用原型开发这样的宝贵用例来说并不理想。
- en: The incentives for teams are a little misaligned here. Data scientists want
    to create the best models for their teams, but if they need to take a day or two
    (or, if they have experience, a few hours) of work to turn their model into a
    Flask or Django app, it doesn’t make much sense to build this out until they think
    they are nearly complete with the modeling process. It would be ideal for data
    scientists to also involve stakeholders early and often, so they can build things
    that people actually want!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 团队的激励机制在这里有些不一致。数据科学家希望为他们的团队创建最好的模型，但如果他们需要花一天或两天的时间（或者，如果有经验的话，几个小时）将模型转化为Flask或Django应用，直到他们认为模型过程几乎完成时才去做这个，似乎没有太大意义。理想情况下，数据科学家应该早期并且经常地与利益相关者进行沟通，这样他们才能构建人们真正需要的东西！
- en: The benefit of Streamlit is that it helps us turn this arduous process into
    a frictionless app creation experience. In this chapter, we’ll go over how to
    create **Machine Learning** (**ML**) prototypes in Streamlit, how to add user
    interaction to your ML apps, and also how to understand the ML results. And we’ll
    do all this with the most popular ML libraries, including PyTorch, Hugging Face,
    OpenAI, and scikit-learn.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Streamlit的好处在于，它帮助我们将这个繁琐的过程转化为一个无缝的应用程序创建体验。在本章中，我们将介绍如何在Streamlit中创建**机器学习**（**ML**）原型，如何为你的机器学习应用添加用户互动，以及如何理解机器学习结果。我们将使用包括PyTorch、Hugging
    Face、OpenAI和scikit-learn在内的最流行的机器学习库来完成这一切。
- en: 'Specifically, the following topics are covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The standard ML workflow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准机器学习工作流程
- en: Predicting penguin species
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测企鹅物种
- en: Utilizing a pre-trained ML model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用预训练的机器学习模型
- en: Training models inside Streamlit apps
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Streamlit应用中训练模型
- en: Understanding ML results
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解机器学习结果
- en: Integrating external ML libraries – a Hugging Face example
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成外部机器学习库——Hugging Face示例
- en: Integrating external AI libraries – an OpenAI example
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成外部人工智能库——OpenAI示例
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, we will need an OpenAI account. To create one, head over to
    ([https://platform.openai.com/](https://platform.openai.com/)) and follow the
    instructions on the page.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将需要一个OpenAI账户。要创建账户，请前往([https://platform.openai.com/](https://platform.openai.com/))并按照页面上的说明操作。
- en: The standard ML workflow
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准机器学习工作流程
- en: 'The first step to creating an app that uses ML is creating the ML model itself.
    There are dozens of popular workflows for creating your own ML models. It’s likely
    you might have your own already! There are two parts of this process to consider:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个使用机器学习（ML）的应用程序的第一步是创建机器学习模型本身。创建自己的机器学习模型有许多流行的工作流程。你可能已经有自己的方法了！这个过程有两个部分需要考虑：
- en: The generation of the ML model
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型的生成
- en: The use of the ML model in production
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中使用机器学习模型
- en: If the plan is to train a model once and then use this model in our Streamlit
    app, the best method is to create this model outside of Streamlit first (for example,
    in a Jupyter notebook or in a standard Python file), and then use this model within
    the app.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计划是一次训练一个模型，然后在Streamlit应用中使用该模型，最好的方法是先在Streamlit外部创建这个模型（例如，在Jupyter笔记本或标准Python文件中），然后在应用中使用该模型。
- en: If the plan is to use the user input to train the model inside our app, then
    we can no longer create the model outside of Streamlit and instead will need to
    run the model training within the Streamlit app.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计划是使用用户输入来训练我们应用中的模型，那么我们就不能再在Streamlit外部创建模型，而需要在Streamlit应用内进行模型训练。
- en: We will start by building our ML models outside of Streamlit and move on to
    training our models inside Streamlit apps.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先在Streamlit之外构建我们的机器学习模型，然后再将模型训练过程移入Streamlit应用中。
- en: Predicting penguin species
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测企鹅物种
- en: The dataset that we will primarily use in this chapter is the same Palmer Penguins
    dataset that we used earlier in *Chapter 1*, *An Introduction to Streamlit*. As
    is typical, we will create a new folder that will house our new Streamlit app
    and accompanying code.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们主要使用的数据集是我们在*第一章*《Streamlit入门》中使用的Palmer Penguins数据集。按照惯例，我们将创建一个新文件夹来存放我们的新Streamlit应用和相关代码。
- en: 'The following code creates this new folder within our `streamlit_apps` folder
    and copies the data from our `penguin_app` folder. If you haven’t downloaded the
    Palmer Penguins dataset yet, please follow the instructions in the *The setup:
    Palmer Penguins* section in *Chapter 2*, *Uploading, Downloading, and Manipulating
    Data*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '以下代码会在我们的`streamlit_apps`文件夹内创建一个新文件夹，并将数据从我们的`penguin_app`文件夹复制过来。如果你还没有下载Palmer
    Penguins数据集，请按照*第二章*《上传、下载与数据处理*中的*The setup: Palmer Penguins*部分的说明操作：'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you may have noticed in the preceding code, there are two Python files here,
    one to create the ML model (`penguins_ml.py`) and the second to create the Streamlit
    app (`penguins_streamlit.py`). We will start with the `penguins_ml.py` file, and
    once we have a model that we are happy with, we will then move on to the `penguins_streamlit.py`
    file.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的代码中看到的，这里有两个Python文件，一个用于创建机器学习模型（`penguins_ml.py`），另一个用于创建Streamlit应用（`penguins_streamlit.py`）。我们将从`penguins_ml.py`文件开始，一旦我们有了一个满意的模型，我们将继续处理`penguins_streamlit.py`文件。
- en: You can also opt to create the model in a Jupyter notebook, which is less reproducible
    by design (as cells can be run out of order) but is still incredibly popular.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以选择在Jupyter Notebook中创建模型，尽管它的可复现性较差（因为单元格可以乱序运行），但它仍然非常流行。
- en: 'Let’s get re-familiarized with the `penguins.csv` dataset. The following code
    will read the dataset and print out the first five rows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新熟悉一下`penguins.csv`数据集。以下代码将读取数据集并打印出前五行：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output of the preceding code, when we run our Python file `penguins_ml.py`
    in the terminal, will look something like the following screenshot:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在终端中运行我们的Python文件`penguins_ml.py`时，前面的代码输出将类似于以下截图：
- en: '![Figure 4.1 – First five penguins ](img/B18444_04_01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 前五只企鹅](img/B18444_04_01.png)'
- en: 'Figure 4.1: First five penguins'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：前五只企鹅
- en: For this app, we are going to attempt to create an app that will help researchers
    in the wild know what species of penguin they are looking at. It will predict
    the species of the penguin given some measurements of the bill, flippers, and
    body mass, and knowledge about the sex and location of the penguin.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个应用程序，我们将尝试创建一个帮助野外研究人员识别企鹅物种的应用。它将根据企鹅的喙、鳍和体重的测量数据，以及企鹅的性别和位置，预测企鹅的物种。
- en: 'This next section is not an attempt to make the best ML model possible, but
    just to create something as a quick prototype for our Streamlit app that we can
    iterate on. In that light, we are going to drop our few rows with null values,
    and not use the `year`variable in our features as it does not fit with our use
    case. We will need to define our features and output variables, do one-hot-encoding
    (or as *pandas* calls it, creating dummy variables for our text columns) on our
    features, and factorize our output variable (turn it from a string into a number).
    The following code should get our dataset in a better state to run through a classification
    algorithm:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节并不是要尝试制作最好的机器学习模型，而只是为了创建一个快速原型供我们的Streamlit应用迭代。在这种情况下，我们将丢弃一些带有空值的行，并且不在特征中使用`year`变量，因为它与我们的用例不匹配。我们需要定义我们的特征和输出变量，对特征进行独热编码（或称*Pandas*中的虚拟变量处理），并对输出变量进行因子化（将其从字符串转换为数字）。以下代码将把我们的数据集转换成适合分类算法的状态：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, when we run our Python file `penguins_ml.py` again, we see that the output
    and feature variables are separated, as shown in the following screenshot:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们再次运行Python文件`penguins_ml.py`时，我们会看到输出和特征变量已被分开，如下图所示：
- en: '![Figure 4.2 – Output variables ](img/B18444_04_02.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 输出变量](img/B18444_04_02.png)'
- en: 'Figure 4.2: Output variables'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：输出变量
- en: Now, we want to create a classification model using a subset (in this case,
    80%) of our data, and get the accuracy of said model. The following code runs
    through those steps using a random forest model, but you can use other classification
    algorithms if you would like. Again, the point here is to get a quick prototype
    to show to the penguin researchers for feedback!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想使用数据的一个子集（在本例中为 80%）来创建一个分类模型，并获取该模型的准确度。以下代码通过使用随机森林模型来执行这些步骤，但如果你愿意，也可以使用其他分类算法。再次强调，这里主要是为了快速构建一个原型，向企鹅研究人员展示以便获取反馈！
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We now have a pretty good model for predicting the species of penguins! Our
    last step in the model-generating process is to save the two parts of this model
    that we need the most – the model itself and the `uniques` variable, which maps
    the factorized output variable to the species name that we recognize. To the previous
    code, we will add a few lines that will save these objects as pickle files (files
    that turn a Python object into something we can save directly and import easily
    from another Python file such as our Streamlit app). More specifically, the `open()`
    function creates two pickle files, the `pickle.dump()` function writes our Python
    files to said files, and the `close()` function closes the files. The `wb` in
    the `open()` function stands for **write bytes**, which tells Python that we want
    to write, not read, to this file:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个相当不错的模型来预测企鹅的物种！在模型生成过程中的最后一步是保存我们最需要的两个部分——模型本身和 `uniques` 变量，它将因子化的输出变量映射到我们识别的物种名称。在之前的代码基础上，我们将添加几行代码，用于将这些对象保存为
    pickle 文件（这是一种将 Python 对象转化为可直接保存并可以轻松从另一个 Python 文件（如我们的 Streamlit 应用）中导入的格式）。更具体地说，`open()`
    函数创建了两个 pickle 文件，`pickle.dump()` 函数将我们的 Python 文件写入这些文件，而 `close()` 函数则关闭文件。`open()`
    函数中的 `wb` 表示**写入字节**，它告诉 Python 我们要写入而不是读取该文件：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We now have two more files in our `penguin_ml` folder: A file called `random_forest_penguin.pickle`,
    which contains our model, and `output_penguin_.pickle`, which has the mapping
    between penguin species and the output of our model. This is it for the `penguins_ml.py`
    function! We can move on to creating our Streamlit app, which uses the machine
    model we just created.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在 `penguin_ml` 文件夹中有了两个文件：一个名为 `random_forest_penguin.pickle` 的文件，其中包含我们的模型，另一个是
    `output_penguin_.pickle`，它包含企鹅物种与模型输出之间的映射。这就是 `penguins_ml.py` 函数的内容！接下来我们可以开始创建我们的
    Streamlit 应用，使用我们刚刚创建的机器学习模型。
- en: Utilizing a pre-trained ML model in Streamlit
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Streamlit 中使用预训练的机器学习模型
- en: 'Now that we have our model, we want to load it (along with our mapping function
    as well) into Streamlit. In our file, `penguins_streamlit.py`, that we created
    before, we will again use the `pickle` library to load our files using the following
    code. We use the same functions as before, but instead of `wb`, we use the `rb`
    parameter, which stands for **read bytes**. To make sure these are the same Python
    objects that we used before, we will use the `st.write()` function that we are
    so familiar with already to check:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了模型，我们想将它（以及我们的映射函数）加载到 Streamlit 中。在我们之前创建的文件 `penguins_streamlit.py`
    中，我们将再次使用 `pickle` 库通过以下代码加载我们的文件。我们使用与之前相同的函数，但这次我们使用 `rb` 参数，而不是 `wb`，`rb` 表示**读取字节**。为了确保这些是我们之前使用的相同的
    Python 对象，我们将使用我们非常熟悉的 `st.write()` 函数来进行检查：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As with our previous Streamlit apps, we run the following code in the terminal
    to run our app:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前的 Streamlit 应用一样，我们在终端运行以下代码来启动我们的应用：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We now have our random forest classifier, along with the penguin mapping! Our
    next step is to add Streamlit functions to get the user input. In our app, we
    used the island, bill length, bill depth, flipper length, body mass, and sex to
    predict the penguin species, so we will need to get each of these from our user.
    For island and sex, we know which options were in our dataset already and want
    to avoid having to parse through user text, so we will use `st.selectbox()`. For
    the other data, we just need to make sure that the user has input a positive number,
    so we will use the `st.number_input()` function and make the minimum value `0`.
    The following code takes these inputs in and prints them out on our Streamlit
    app:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了随机森林分类器，并且完成了企鹅的映射！接下来的步骤是添加 Streamlit 函数以获取用户输入。在我们的应用程序中，我们使用了岛屿、嘴长、嘴深、鳍肢长度、体重和性别来预测企鹅的物种，所以我们需要从用户那里获取这些信息。对于岛屿和性别，我们知道这些选项已经在我们的数据集中，并且希望避免解析用户输入的文本，因此我们将使用
    `st.selectbox()`。对于其他数据，我们只需要确保用户输入的是一个正数，所以我们将使用 `st.number_input()` 函数并设置最小值为
    `0`。以下代码会接收这些输入并在我们的 Streamlit 应用程序中显示它们：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding code should make the following app. Try it out and see if it works
    by changing the values and seeing if the output changes as well.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应当生成如下的应用程序。尝试一下，看看通过更改值，输出是否也发生变化。
- en: 'Streamlit is designed so that, by default, each time a value is changed, the
    entire app reruns. The following screenshot shows the app live, with some values
    that I’ve changed. We can either change numeric values with the **+** and **-**
    buttons on the right-hand side or we can just enter the values manually:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Streamlit 设计时默认情况下，每次更改值时，整个应用程序都会重新运行。以下截图显示了应用程序的实时效果，并展示了我更改过的一些值。我们可以通过右侧的
    **+** 和 **-** 按钮来更改数值，或者直接手动输入值：
- en: '![](img/B18444_04_03.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18444_04_03.png)'
- en: 'Figure 4.3: Model inputs'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：模型输入
- en: 'Now that we have all of our inputs and model ready, the next step is to format
    the data into the same format as our preprocessed data. For example, our model
    does not have one variable called `sex` but instead has two variables called `sex_female`
    and `sex_male`. Once our data is in the right shape, we can call the `predict`
    function and map the prediction to our original species list to see how our model
    functions. The following code does exactly this, and also adds some basic titles
    and instructions to the app to make it more usable. This app is rather long, so
    I will break it up into multiple sections for readability. We will start by adding
    instructions and a title to our app:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了所有输入和模型，下一步是将数据格式化为与我们预处理数据相同的格式。例如，我们的模型没有一个名为 `sex` 的变量，而是有两个名为
    `sex_female` 和 `sex_male` 的变量。一旦数据的格式正确，我们就可以调用 `predict` 函数，并将预测结果映射到我们的原始物种列表中，以查看我们的模型如何工作。以下代码正是完成这一任务，同时还向应用程序添加了一些基本标题和说明，使其更易于使用。这个应用程序比较长，因此我会将其分成多个部分，以便阅读。我们首先为应用程序添加说明和标题：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We now have an app with our title and instructions for the user. The next step
    is to get the user inputs as we did before. We also need to put our `sex` and
    `island` variables into the correct format, as discussed before:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经有了一个带有标题和用户说明的应用程序。下一步是像之前一样获取用户输入。我们还需要将 `sex` 和 `island` 变量转换为正确的格式，如之前所述：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'All of our data is in the correct format! The last step here is to use the
    `predict()` function on our model and our new data, which this final section takes
    care of:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的所有数据格式正确！最后一步是使用 `predict()` 函数处理我们的模型和新数据，这一部分代码完成了这一任务：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now our app should look like the following screenshot.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的应用程序应该像下面的截图一样。
- en: I have added some example values to the inputs, but you should play around with
    changing the data to see if you can make the species change!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我已为输入添加了一些示例值，但你应该尝试更改数据，看看能否使物种发生变化！
- en: '![](img/B18444_04_04.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18444_04_04.png)'
- en: 'Figure 4.4: Full Streamlit app for prediction'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：用于预测的完整 Streamlit 应用
- en: We now have a full Streamlit app that utilizes our pre-trained ML model, takes
    user input, and outputs the prediction. Next, we will discuss how to train models
    directly within Streamlit apps!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有一个完整的 Streamlit 应用程序，利用我们预训练的机器学习模型，获取用户输入并输出预测结果。接下来，我们将讨论如何直接在 Streamlit
    应用程序中训练模型！
- en: Training models inside Streamlit apps
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Streamlit 应用程序中训练模型
- en: Often, we may want to have the user input change how our model is trained. We
    may want to accept data from the user or ask the user what features they would
    like to use, or even allow the user to pick the type of ML algorithm that they
    would like to use. All of these options are feasible in Streamlit, and in this
    section, we will cover the basics of using user input to affect the training process.
    As we discussed in the section above, if a model is going to be trained only once,
    it is probably best to train the model outside of Streamlit and import the model
    into Streamlit. But what if, in our example, the penguin researchers have the
    data stored locally, or do not know how to retrain the model but have the data
    in the correct format already? In cases like these, we can add the `st.file_uploader()`
    option and include a method for these users to input their own data, and get a
    custom model deployed for them without having to write any code. The following
    code will add a user option to accept data and will use the preprocessing/training
    code that we originally had in `penguins_ml.py` to make a unique model for this
    user. It is important to note here that this will only work if the user has data
    in the exact same format and style that we used, which may be unlikely. One other
    potential add-on here is to show the user what format the data needs to be in
    for this app to correctly train a model as expected!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 经常情况下，我们可能希望用户输入的数据能够影响模型的训练。我们可能希望接受用户的数据，或者询问用户他们希望使用哪些特征，甚至允许用户选择他们希望使用的机器学习算法类型。所有这些选项在
    Streamlit 中都是可行的，在本节中，我们将介绍如何使用用户输入来影响训练过程的基础知识。如上节所讨论的那样，如果模型只需要训练一次，最好在 Streamlit
    外部训练模型，并将模型导入到 Streamlit 中。但如果在我们的示例中，企鹅研究人员将数据保存在本地，或者不知道如何重新训练模型，但已有正确格式的数据呢？在这种情况下，我们可以添加`st.file_uploader()`选项，并为这些用户提供一种方法，让他们输入自己的数据，并部署一个定制的模型，而无需编写任何代码。以下代码将添加一个用户选项来接受数据，并使用我们最初在`penguins_ml.py`中的预处理/训练代码为该用户创建一个独特的模型。需要注意的是，这只有在用户的数据格式与我们使用的完全相同时才会有效，这可能性较小。这里的另一个潜在补充是，向用户展示数据需要具备的格式，以便该应用能够正确地训练模型，按预期输出！
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This first section imports the libraries that we need, adds the title – as
    we have used before – and adds the `file_uploader()` function. What happens, however,
    when the user has yet to upload a file? We can set the default to load our random
    forest model if there is no penguin file, as shown in the next section of code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分首先导入了我们需要的库，添加了标题——如我们之前所用——并添加了`file_uploader()`函数。然而，当用户还没有上传文件时会发生什么呢？如果没有企鹅文件，我们可以将默认设置为加载我们的随机森林模型，如下一个代码块所示：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The next problem we need to solve is how to take in the user’s data, clean
    it, and train a model based on it. Luckily, we can reuse the model training code
    that we have already created and put it within our `else` statement in the next
    code block:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要解决的问题是如何接收用户的数据，清理数据，并基于这些数据训练一个模型。幸运的是，我们可以重用已经创建的模型训练代码，并将其放入下一个代码块中的`else`语句中：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We have now created our model within the app and need to get the inputs from
    the user for our prediction. This time, however, we can make an improvement on
    what we have done before. As of now, each time a user changes an input in our
    app, the entire Streamlit app will rerun. We can use the `st.form()` and `st.submit_form_button()`
    functions to wrap the rest of our user inputs in and allow the user to change
    all of the inputs and submit the entire form at once, instead of multiple times:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经在应用中创建了模型，并且需要从用户处获取输入来进行预测。然而，这次我们可以在之前的基础上进行改进。到目前为止，每当用户在应用中更改输入时，整个
    Streamlit 应用都会重新运行。我们可以使用`st.form()`和`st.submit_form_button()`函数将其余的用户输入包裹起来，并允许用户一次性更改所有输入并提交整个表单，而不是多次提交：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now that we have the inputs with our new form, we need to create our prediction
    and write the prediction to the user, as shown in the next block:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了新的表单输入，需要创建预测并将预测结果写入用户界面，如下一个代码块所示：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And there we go! We now have a Streamlit app that allows the user to input
    their own data, trains a model based on their data, and outputs the results, as
    shown in the next screenshot:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们现在拥有一个 Streamlit 应用，允许用户输入自己的数据，基于这些数据训练模型，并输出结果，如下图所示：
- en: '![](img/B18444_04_05.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18444_04_05.png)'
- en: 'Figure 4.5: Penguin classifier app'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：企鹅分类器应用
- en: There are potential improvements that can be made here, such as using caching
    functions (explored in *Chapter 2*, *Uploading, Downloading, and Manipulating
    Data*), as one example. Apps like these, where users bring their own data, are
    significantly harder to build, especially without a universal data format. It
    is more common as of the time of writing to see Streamlit apps that show off impressive
    ML models and use cases rather than apps that build them directly in-app (especially
    with more computationally expensive model training). As we mentioned before, Streamlit
    developers often will provide references to the required data format before asking
    for user input in the form of a dataset. However, this option of allowing users
    to bring their own data is available and practical, especially to allow for quick
    iterations on model building.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里可以做一些潜在的改进，比如使用缓存函数（在*第2章*，*上传、下载和处理数据*中有介绍）作为一个例子。像这种让用户提供自己数据的应用，通常要比直接在应用内构建的应用更难开发，尤其是没有统一的数据格式。根据写作时的情况，更常见的是看到展示令人印象深刻的ML模型和用例的Streamlit应用，而不是直接在应用内构建这些模型的应用（尤其是当模型训练的计算开销较大时）。如前所述，Streamlit开发者通常会在要求用户输入数据集之前提供所需数据格式的参考。然而，允许用户提供自己数据的选项仍然可用且实用，特别是为了快速迭代模型构建。
- en: Understanding ML results
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解ML结果
- en: So far, our app might be useful, but often, just showing a result is not good
    enough for a data app. We should show some explanation of the results. In order
    to do this, we can include a section in the output of the app that we have already
    made that helps users in understanding the model better.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的应用可能是有用的，但仅仅显示一个结果通常不足以满足数据应用的需求。我们应该展示一些对结果的解释。为了做到这一点，我们可以在我们已经创建的应用输出中包含一个部分，帮助用户更好地理解模型。
- en: To start, random forest models already have a built-in feature importance method
    derived from the set of individual decision trees that make up the random forest.
    We can edit our `penguins_ml.py` file to graph this importance, and then call
    that image from within our Streamlit app. We could also graph this directly from
    within our Streamlit app, but it is more efficient to make this graph once in
    `penguins_ml.py` instead of every time our Streamlit app reloads (which is every
    time a user changes a user input!). The following code edits our `penguins_ml.py`
    file and adds the feature importance graph, saving it to our folder. We also call
    the `tight_layout()` feature, which helps format our graph better and makes sure
    we avoid any labels getting cut off. This set of code is long, and the top half
    of the file remains unchanged, so only the section on library importing and data
    cleaning has been omitted. One other note about this section is that we’re going
    to try out using other graphing libraries such as Seaborn and Matplotlib, just
    to get a bit of diversity in the graphing libraries used.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，随机森林模型已经内建了一个特征重要性方法，这个方法是从构成随机森林的每个单独决策树中得出的。我们可以编辑我们的`penguins_ml.py`文件来绘制这个特征重要性图，并从Streamlit应用中调用该图像。我们也可以直接在Streamlit应用中绘制这个图，但是一次性在`penguins_ml.py`中生成这个图比每次Streamlit应用重新加载时（即每次用户更改输入时）都生成图像更高效。以下代码编辑了我们的`penguins_ml.py`文件，添加了特征重要性图，并将其保存到我们的文件夹中。我们还调用了`tight_layout()`功能，帮助我们更好地格式化图表，确保不会有标签被截断。这段代码较长，文件的上半部分没有更改，因此只省略了库导入和数据清理部分。还有一点需要说明的是，我们将尝试使用其他图表库，如Seaborn和Matplotlib，只是为了在使用的图形库上增加一些多样性。
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now when we rerun `penguins_ml.py`, we should see a file called `feature_importance.png`,
    which we can call from our Streamlit app. Let’s do that now! We can use the `st.image()`
    function to load an image from our `.png` and print it to our penguin app. The
    following code adds our image to the Streamlit app and also improves our explanations
    around the prediction we made. Because of the length of this code block, we will
    just show the new code from the point where we start to predict using the user’s
    data:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们重新运行`penguins_ml.py`时，我们应该能看到一个名为`feature_importance.png`的文件，我们可以从我们的Streamlit应用中调用这个文件。现在就让我们来做吧！我们可以使用`st.image()`函数从`.png`文件中加载图像，并将其打印到我们的企鹅应用中。以下代码将把我们的图像添加到Streamlit应用中，并改进我们对预测结果的解释。由于代码块较长，我们只展示从开始使用用户数据进行预测的代码部分：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, the bottom of your Streamlit app should look like the following screenshot
    (note that your string might be slightly different based on your inputs):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您的Streamlit应用底部应该看起来像下面的截图（注意您的字符串可能会根据输入有所不同）：
- en: '![Figure 4.6 – Feature importance screenshot ](img/B18444_04_06.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6 – 特征重要性截图](img/B18444_04_06.png)'
- en: 'Figure 4.6: Feature importance screenshot'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：特征重要性截图
- en: 'As we can see, bill length, bill depth, and flipper length are the most important
    variables according to our random forest model. A final option for explaining
    how our model works is to plot the distributions of each of these variables by
    species, and also plot some vertical lines representing the user input. Ideally,
    the user can begin to understand the underlying data holistically and therefore,
    will understand the predictions that come from the model as well. To do this,
    we will need to actually import the data into our Streamlit app, which we have
    not done previously. The following code imports the penguin data that we used
    to build the model, and plots three histograms (for *bill length*, *bill depth*,
    and *flipper length*) along with the user input as a vertical line, starting from
    the model explanation section:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，鸟喙长度、鸟喙深度和鳍长是根据我们的随机森林模型得出的最重要变量。解释我们模型工作原理的另一个最终选项是绘制每个变量按物种分布的图表，并绘制一些表示用户输入的垂直线。理想情况下，用户可以开始从整体上理解底层数据，因此也能理解模型所做的预测。为了实现这一点，我们需要将数据实际导入到Streamlit应用中，这是我们之前没有做的。以下代码导入了我们用来构建模型的企鹅数据，并绘制了三个直方图（*鸟喙长度*、*鸟喙深度*和*鳍长*），同时将用户输入作为一条垂直线展示，从模型解释部分开始：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now that we have set up our app for displaying histograms, we can use the `displot()`
    function in the Seaborn visualization library to create our three histograms for
    our most important variables:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好应用程序来显示直方图，我们可以使用Seaborn可视化库中的`displot()`函数来创建我们最重要变量的三个直方图：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code should create the app shown in the following figure, which
    is our app in its final form. For viewing ease, we will just show the first histogram:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应当生成如下图所示的应用程序，这是我们应用程序的最终形式。为了方便查看，我们只展示第一个直方图：
- en: '![](img/B18444_04_07.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18444_04_07.png)'
- en: 'Figure 4.7: Bill length by species'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：按物种分类的鸟喙长度
- en: As always, the complete and final code can be found at [https://github.com/tylerjrichards/Streamlit-for-Data-Science](https://github.com/tylerjrichards/Streamlit-for-Data-Science).
    That completes this section. We have now created a fully formed Streamlit app
    that takes a pre-built model and user input and outputs both the result of the
    prediction and an explanation of the output as well. Now, let’s explore how to
    integrate your other favorite ML libraries into Streamlit!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常一样，完整的最终代码可以在[https://github.com/tylerjrichards/Streamlit-for-Data-Science](https://github.com/tylerjrichards/Streamlit-for-Data-Science)找到。这部分到此为止。我们现在已经创建了一个完全构建的Streamlit应用程序，它可以接受一个预先构建的模型和用户输入，并输出预测结果以及对输出的解释。接下来，让我们探讨如何将你最喜爱的其他机器学习库集成到Streamlit中！
- en: Integrating external ML libraries – a Hugging Face example
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成外部机器学习库——以Hugging Face为例
- en: Over the last few years, there has been a massive increase in the number of
    ML models created by startups and institutions. There is one that, in my opinion,
    has stood out above the rest for prioritizing the open sourcing and sharing of
    their models and methods, and that is Hugging Face. Hugging Face makes it incredibly
    easy to use ML models that some of the best researchers in the field have created
    for your own use cases, and in this bit, we’ll quickly show off how to integrate
    Hugging Face into Streamlit.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，由初创公司和机构创建的机器学习模型数量大幅增加。在我看来，有一个公司因其优先考虑开源和分享模型及方法而脱颖而出，那就是Hugging Face。Hugging
    Face使得使用一些领域内最优秀的研究人员创建的机器学习模型变得异常简单，你可以将这些模型应用到自己的用例中，在这一部分中，我们将快速展示如何将Hugging
    Face集成到Streamlit中。
- en: 'As part of the original setup for this book, we have already downloaded the
    two libraries that we need: PyTorch (the most popular deep learning Python framework)
    and transformers (a Hugging Face’s library that makes it easy to use their pre-trained
    models). So, for our app, let’s try one of the most basic tasks in natural language
    processing: Getting the sentiment of a bit of text! Hugging Face makes this incredibly
    easy with its pipeline function, which lets us ask for a model by name. This next
    code snippet gets a text input from the user and then retrieves the sentiment
    analysis model from Hugging Face:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的初始设置中，我们已经下载了所需的两个库：PyTorch（最流行的深度学习Python框架）和transformers（Hugging Face的库，简化了使用其预训练模型的过程）。因此，对于我们的应用，让我们尝试进行自然语言处理中的一项基础任务：获取一段文本的情感！Hugging
    Face通过其pipeline函数使这变得异常简单，该函数让我们按名称请求模型。接下来的代码片段会从用户那里获取文本输入，然后从Hugging Face检索情感分析模型：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When we run this, we should see the following.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个时，应该会看到如下结果。
- en: '![](img/B18444_04_08.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18444_04_08.png)'
- en: 'Figure 4.8: Hugging Face Demo'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：Hugging Face演示
- en: I put a random sentence in the app, but go ahead and play around with it! Try
    to give the model a bit of text that the confidence is low in (I tried “streamlit
    is a pizza pie” and sufficiently confused the model). To learn more about the
    models that are used here, Hugging Face has extensive documentation ([https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我在应用中放入了一个随机句子，但你可以随意尝试！试着给模型一些信心较低的文本（我试过“streamlit is a pizza pie”，并成功地让模型困惑）。想了解更多这里使用的模型，Hugging
    Face提供了丰富的文档（[https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)）。
- en: 'As you play around with the app, you notice that the app often takes a long
    time to load. This is because each time the app is run, the transformers library
    fetches the model from Hugging Face, and then uses it in the app. We already learned
    how to cache data, but Streamlit has a similar caching function called `st.cache_resource`,
    which lets us cache objects like ML models and database connections. Let’s use
    it here to speed up our app:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当你玩这个应用时，你会发现应用经常加载较慢。这是因为每次运行应用时，transformers库会从Hugging Face获取模型，并在应用中使用它。我们已经学习了如何缓存数据，但Streamlit有一个类似的缓存功能，叫做`st.cache_resource`，它可以让我们缓存像ML模型和数据库连接这样的对象。我们可以在这里使用它来加速应用：
- en: '[PRE21]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, our app should run much faster for multiple uses. This app is not perfect
    but shows us how easy it is to integrate some of the best-in-class libraries into
    Streamlit. Later in this book, we’ll go over how to deploy Streamlit apps directly
    on Hugging Face for free, but I would encourage you to explore the Hugging Face
    website ([https://huggingface.co/](https://huggingface.co/)) and see all that
    they have to offer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的应用在多次使用时应该会运行得更快。这个应用并不完美，但它向我们展示了将一些顶尖库集成到Streamlit中的简便性。在本书的后续章节中，我们将讨论如何将Streamlit应用直接免费部署在Hugging
    Face上，但我鼓励你浏览Hugging Face网站（[https://huggingface.co/](https://huggingface.co/)），看看他们提供的所有资源。
- en: Integrating external AI libraries – an OpenAI example
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成外部AI库——OpenAI示例
- en: 2023 has surely been the year of generative AI, with ChatGPT taking the world
    and developer community by storm. The availability of generative models behind
    services like ChatGPT has also exploded, with each of the largest technology companies
    coming out with their own versions ([https://ai.meta.com/llama/](https://ai.meta.com/llama/)
    from Meta and [https://bard.google.com/](https://bard.google.com/) from Google,
    for example). The most popular series of these generative models is OpenAI’s **GPT**
    (**Generative Pre-trained Transformer**). This section will show you how to use
    the OpenAI API to add generative AI to your Streamlit apps!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年无疑是生成式AI的年份，ChatGPT以其强大的影响力席卷了全球和开发者社区。像ChatGPT这样的服务背后生成模型的可用性也急剧增加，各大科技公司纷纷推出了自己的版本（例如Meta的[https://ai.meta.com/llama/](https://ai.meta.com/llama/)和Google的[https://bard.google.com/](https://bard.google.com/)）。其中最受欢迎的生成模型系列是OpenAI的**GPT**（**生成预训练变换器**）。本节将向你展示如何使用OpenAI
    API将生成式AI添加到你的Streamlit应用中！
- en: Authenticating with OpenAI
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用OpenAI进行身份验证
- en: Our first step is to make an OpenAI account and get an API key. To do this,
    head over to [https://platform.openai.com](https://platform.openai.com) and create
    an account. Once you have created an account, go to the **API keys** section ([https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys))
    and press the button **Create new secret key**. Once you create the key, make
    sure to save it somewhere safe because OpenAI will not show you your key again!
    I saved mine in my password manager to ensure I wouldn’t lose it ([https://1password.com/](https://1password.com/)),
    but you can save yours wherever you want.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是创建一个 OpenAI 账户并获取 API 密钥。为此，请访问[https://platform.openai.com](https://platform.openai.com)并创建一个账户。创建账户后，进入**API
    密钥**部分（[https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)），点击**创建新的秘密密钥**按钮。创建密钥后，务必将其保存在安全的地方，因为
    OpenAI 不会再向你显示密钥！我将它保存在我的密码管理器中，以确保不会丢失（[https://1password.com/](https://1password.com/)），但你可以将它保存在任何你想要的地方。
- en: OpenAI API cost
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI API 费用
- en: The OpenAI API is not free, but the one we will use (GPT-3.5 turbo) currently
    costs $.0015/1k tokens (~750 words) for input and $.002 /1k tokens for output
    (see [https://openai.com/pricing](https://openai.com/pricing) for updated info).
    You can also set a hard limit on the maximum you want to spend on this API at
    [https://platform.openai.com/account/billing/limits](https://platform.openai.com/account/billing/limits).
    If you set a hard limit, OpenAI will not allow you to spend above it. I certainly
    recommend setting a limit. Set one for this example section of 1 USD; we should
    stay well within that! Once you start to create generative AI apps of your own
    that you share publicly, this feature will become even more useful (often, developers
    either ask the user to enter their own API key or charge them for access to the
    Streamlit app with libraries like [https://github.com/tylerjrichards/st-paywall](https://github.com/tylerjrichards/st-paywall)
    to get around paying too much).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI API 不是免费的，但我们将使用的这一款（GPT-3.5 turbo）目前的费用为每千个 tokens（大约750个单词）输入$0.0015，输出每千个
    tokens$0.002（有关最新信息，请参见[https://openai.com/pricing](https://openai.com/pricing)）。你也可以在[https://platform.openai.com/account/billing/limits](https://platform.openai.com/account/billing/limits)设置一个硬性限制，限制你在该
    API 上的最大消费。如果设置了硬性限制，OpenAI 将不允许你超过此限制。我强烈建议设置限制。对这个示例部分设置 1 美元的限制；我们应该完全在这个范围内！一旦你开始创建并公开分享你自己的生成
    AI 应用，这个功能将变得更加有用（通常，开发者要么要求用户输入自己的 API 密钥，要么通过像[https://github.com/tylerjrichards/st-paywall](https://github.com/tylerjrichards/st-paywall)这样的库收取访问
    Streamlit 应用的费用，以避免支付过多）。
- en: Streamlit and OpenAI
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Streamlit 和 OpenAI
- en: For this example, we’re going to recreate the sentiment analysis from our Hugging
    Face example but using GPT-3.5 turbo. As you play around with models like these,
    you will find that they are generally very intelligent, and can be used for almost
    any task you can think of without any extra training on top of them. Let me prove
    it to you!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将重新创建我们 Hugging Face 示例中的情感分析，但使用的是 GPT-3.5 turbo。随着你尝试这些模型，你会发现它们通常非常智能，可以用来完成几乎所有你能想到的任务，而无需额外的训练。我来证明给你看！
- en: Now that we have our API, we add it to a Secrets file (we’ll cover Secrets in
    more detail in the *Streamlit Secrets* section in *Chapter 5*, *Deploying Streamlit
    with Streamlit Community Cloud*). Create a folder called `.streamlit` and create
    a `secrets.toml` file inside it, and then put your API key in there assigned to
    a variable called `OPENAI_API_KEY` so that it becomes `OPENAI_API_KEY="sk-xxxxxxxxxxxx"`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 API，将其添加到一个 Secrets 文件中（我们将在*第 5 章*的*Streamlit Secrets*部分更详细地介绍 Secrets，*部署
    Streamlit 到 Streamlit Community Cloud*）。创建一个名为`.streamlit`的文件夹，并在其中创建一个`secrets.toml`文件，然后将你的
    API 密钥放入其中，并将其分配给名为`OPENAI_API_KEY`的变量，使其变成`OPENAI_API_KEY="sk-xxxxxxxxxxxx"`。
- en: 'Let’s open our existing Streamlit app and put a title at the bottom of it,
    button we can have the user click to analyze the text, and our authentication
    key:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开现有的 Streamlit 应用，并在底部添加一个标题、一个按钮，让用户点击分析文本，以及我们的认证密钥：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The OpenAI Python library (which we installed with our initial `requirements.txt`
    file) provides an easy way to interact with the OpenAI API all in Python, which
    is a wonderfully useful resource. The endpoint we want to hit here is called the
    chat completion endpoint ([https://platform.openai.com/docs/api-reference/chat/create](https://platform.openai.com/docs/api-reference/chat/create)),
    which takes in a system message (which is a way for us to instruct the OpenAPI
    model on how to respond, which in our case is a helpful sentiment analysis assistant)
    and a few other parameters about what underlying model we want to call. There
    are more up-to-date and expensive models than the one we will use, but I’ve found
    GPT 3.5 to be excellent and very fast.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Python 库（我们通过初始的 `requirements.txt` 文件安装的）提供了一种方便的方式，以 Python 与 OpenAI
    API 进行交互，这真是一个非常有用的资源。我们要调用的端点叫做聊天完成端点 ([https://platform.openai.com/docs/api-reference/chat/create](https://platform.openai.com/docs/api-reference/chat/create))，它接受一个系统消息（这是我们告诉
    OpenAI 模型如何响应的方式，在我们的案例中是一个有帮助的情感分析助手）以及关于我们要调用的底层模型的其他参数。虽然有比我们将要使用的模型更现代和昂贵的版本，但我发现
    GPT 3.5 非常出色且速度很快。
- en: 'We can call the API and write the response back to our app like this:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像这样调用 API，并将响应写回到我们的应用程序：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s test it out! We can use the same text input as we did in the Hugging
    Face example to compare the two:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测试一下！我们可以使用与 Hugging Face 示例中相同的文本输入来对比这两个分析器：
- en: '![A screenshot of a computer  Description automatically generated](img/B18444_04_09.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a computer  Description automatically generated](img/B18444_04_09.png)'
- en: 'Figure 4.9: A comparison of the Hugging Face and OpenAI sentiment analyzers'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：Hugging Face 和 OpenAI 情感分析器的比较
- en: It looks like both versions think that this sentiment is positive with fairly
    high confidence. This is remarkable! The Hugging Face model is specifically trained
    for sentiment analysis, but OpenAI’s is not at all. For this trivial example,
    they both seem to work. What about if we try out giving each just a single word,
    like Streamlit?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来两个版本都认为这个情感是积极的，且置信度相当高。这很了不起！Hugging Face 模型是专门为情感分析训练的，但 OpenAI 的模型并不是。对于这个简单的例子，它们似乎都能正常工作。如果我们尝试只给每个模型一个单词，比如
    “Streamlit”，会怎么样呢？
- en: '![A screenshot of a computer  Description automatically generated](img/B18444_04_10.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a computer  Description automatically generated](img/B18444_04_10.png)'
- en: 'Figure 4.10: Testing the sentiment for “Streamlit”'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10：测试“Streamlit”的情感
- en: In this case, the two methods disagree. OpenAI thinks this is neutral with medium
    confidence, and Hugging Face thinks the sentiment is positive with very high confidence.
    I think OpenAI is probably right here, which is endlessly fascinating. There is
    clearly a large number of use cases for a model like this.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，两种方法得出的结论不同。OpenAI 认为这是中性情感，置信度适中，而 Hugging Face 认为情感是积极的，并且置信度非常高。我认为
    OpenAI 在这里可能是对的，这真是令人着迷。显然，这种模型有着广泛的应用场景。
- en: 'Through Streamlit widgets, we can let the user change any part of the API call.
    We just add the correct widget type and the user’s input to the OpenAI function,
    and then we’re good to go! Let’s try one more thing. What if we let the user change
    the system message we started with? To do this, we’ll need to add a new text input.
    We will use a Streamlit input widget called `st.text_area`, which works the same
    as our familiar `st.text_input` but allows for a multi-line input for longer sections
    of text:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Streamlit 小部件，我们可以让用户更改 API 调用的任何部分。我们只需添加正确的小部件类型并将用户的输入传递给 OpenAI 函数，然后就可以了！再试一次吧。如果我们让用户更改我们最初的系统消息会怎样？为此，我们需要添加一个新的文本输入。我们将使用一个叫做
    `st.text_area` 的 Streamlit 输入小部件，它与我们熟悉的 `st.text_input` 相同，但允许多行输入，以便处理更长的文本段落：
- en: '[PRE24]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The user can now change the system message, but our default message is the
    same. I went ahead and changed the system message here to something ridiculous.
    I asked the model to be a terrible sentiment analysis assistant, always messing
    up the sentiment that was input:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 用户现在可以更改系统消息，但我们的默认消息保持不变。我已经将系统消息改成了一些荒谬的内容。我要求模型成为一个糟糕的情感分析助手，总是把输入的情感分析弄错：
- en: '![A screenshot of a computer  Description automatically generated](img/B18444_04_11.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a computer  Description automatically generated](img/B18444_04_11.png)'
- en: 'Figure 4.11: Changing the system message for the OpenAI text analyzer'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11：更改 OpenAI 文本分析器的系统消息
- en: As you can see, the model did what I asked and screwed up the sentiment analysis
    for **streamlit is awesome**, saying that the sentiment was negative.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，模型按照我要求的做法，错误地进行了 **streamlit is awesome** 的情感分析，结果显示情感是负面的。
- en: 'A quick warning: When you allow user input into a large language model, users
    may try and inject undesirable prompts into your applications. Here is one example
    using the same app, where I ask the model to ignore all the other instructions
    and instead write a pirate themed story:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 快速警告：当你允许用户输入到大型语言模型中时，用户可能会尝试将不良的提示注入到你的应用程序中。这里有一个使用同一个应用程序的例子，我要求模型忽略所有其他指令，而改写一个海盗主题的故事：
- en: '![A screenshot of a computer  Description automatically generated](img/B18444_04_12.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图 自动生成的描述](img/B18444_04_12.png)'
- en: 'Figure 4.12: OpenAI and pirates'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12：OpenAI与海盗
- en: This story continued for many more lines, but you can see how the more input
    I give the user control over, the more likely it is that they can use my application
    in ways I did not intend. There are many novel ways to get around this, including
    running the prompt through another API call, this time asking the model if it
    thinks the prompt is disingenuous, or preventing common injections like “ignore
    the previous prompt.”
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事还继续了很多行，但你可以看到，越是给予用户更多控制输入的权力，就越有可能让他们以我没有预料到的方式使用我的应用程序。对此有许多创新的解决方法，包括将提示传递给另一个API调用，这次询问模型它是否认为提示不真诚，或是防止一些常见的注入，如“忽略之前的提示”。
- en: There are also open-source libraries like Rebuff ([https://github.com/protectai/rebuff](https://github.com/protectai/rebuff)),
    which are extremely useful as well! I hesitate to give any specific advice here,
    as the field of generative AI moves extremely quickly, but the general principles
    of caution and intentional user input should be very useful.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 也有像Rebuff这样的开源库（[https://github.com/protectai/rebuff](https://github.com/protectai/rebuff)），它们也非常有用！由于生成性AI领域发展极其迅速，我不太敢给出具体建议，但谨慎的原则和有意的用户输入应该是非常有帮助的。
- en: If you’re interested in more generative AI Streamlit apps, the Streamlit team
    has made a landing page that has all the most recent information and examples
    at [https://streamlit.io/generative-ai](https://streamlit.io/generative-ai).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对更多生成性AI的Streamlit应用感兴趣，Streamlit团队已经制作了一个网页，汇集了所有最新的信息和示例，网址是[https://streamlit.io/generative-ai](https://streamlit.io/generative-ai)。
- en: Summary
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we learned about some ML basics: How to take a pre-built ML
    model and use it within Streamlit, how to create our own models from within Streamlit,
    how to use user input to understand and iterate on ML models, and even how to
    use models from Hugging Face and OpenAI. Hopefully, by the end of this chapter,
    you’ll feel comfortable with each of these. Next, we will dive into the world
    of deploying Streamlit apps using Streamlit Community Cloud!'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了一些机器学习的基础知识：如何在Streamlit中使用预构建的机器学习模型，如何从Streamlit内部创建自己的模型，如何利用用户输入理解并迭代机器学习模型，甚至如何使用Hugging
    Face和OpenAI的模型。希望到本章结束时，你能对这些内容感到自如。接下来，我们将深入探讨如何使用Streamlit社区云部署Streamlit应用！
- en: Learn more on Discord
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Discord上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 加入本书的Discord社区——在这里你可以分享反馈、向作者提问，并了解新版本的发布——请扫描下方的二维码：
- en: '[https://packt.link/sl](https://packt.link/sl)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/sl](https://packt.link/sl)'
- en: '![](img/QR_Code13440134443835796.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code13440134443835796.png)'
