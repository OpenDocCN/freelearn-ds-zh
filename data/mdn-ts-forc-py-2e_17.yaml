- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Attention and Transformers for Time Series
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列中的注意力与变换器
- en: In the previous chapter, we rolled up our sleeves and implemented a few **deep
    learning** (**DL**) systems for time series forecasting. We used the common building
    blocks we discussed in *Chapter 12*, *Building Blocks of Deep Learning for Time
    Series*, put them together in an encoder-decoder architecture, and trained them
    to produce the forecast we desired.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们卷起袖子，实施了一些用于时间序列预测的**深度学习**（**DL**）系统。我们使用了在*第12章*中讨论的常见构建块，*深度学习时间序列的构建块*，将它们组合成一个编码器-解码器架构，并训练它们产生我们期望的预测。
- en: Now, let’s talk about another key concept in DL that has taken the field by
    storm over the past few years—**attention**. Attention has a long-standing history,
    which has culminated in it being one of the most sought-after tools in the DL
    toolkit. This chapter takes you on a journey to understand attention and transformer
    models from the ground up from a theoretical perspective and solidify that understanding
    with practical examples.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈深度学习（DL）中的另一个关键概念，它在过去几年迅速席卷了这个领域——**注意力**。注意力有着悠久的历史，并最终成为了DL工具箱中最受欢迎的工具之一。本章将带领你从理论的角度理解注意力和变换器模型，并通过实际示例巩固这一理解。
- en: 'In this chapter, we will be covering these main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: What is attention?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是注意力？
- en: Generalized attention model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广义注意力模型
- en: Forecasting with sequence-to-sequence models and attention
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用序列到序列模型和注意力进行预测
- en: Transformers—Attention is all you need
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器——注意力就是你所需要的一切
- en: Forecasting with Transformers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用变换器进行预测
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need to set up the **Anaconda** environment by following the instructions
    in the *Preface* of the book to get a working environment with all the libraries
    and datasets required for the code in this book. Any additional libraries will
    be installed while running the notebooks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要通过遵循本书*前言*中的说明来设置**Anaconda**环境，以便获得一个包含本书中所需所有库和数据集的工作环境。任何额外的库将在运行笔记本时自动安装。
- en: 'You need to run the following notebooks for this chapter:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章你需要运行以下笔记本：
- en: '`02-Preprocessing_London_Smart_Meter_Dataset.ipynb` in `Chapter02`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02-Preprocessing_London_Smart_Meter_Dataset.ipynb` 在 `Chapter02` 中'
- en: '`01-Setting_up_Experiment_Harness.ipynb` in `Chapter04`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01-Setting_up_Experiment_Harness.ipynb` 在 `Chapter04` 中'
- en: '`01-Feature_Engineering.ipynb` in `Chapter06`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`01-Feature_Engineering.ipynb` 在 `Chapter06` 中'
- en: '`02-One-Step_RNN.ipynb` and `03-Seq2Seq_RNN.ipynb` in `Chapter13` (for benchmarking)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`02-One-Step_RNN.ipynb` 和 `03-Seq2Seq_RNN.ipynb` 在 `Chapter13` 中（用于基准测试）'
- en: '`00-Single_Step_Backtesting_Baselines.ipynb` and `01-Forecasting_with_ML.ipynb`
    in `Chapter08`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`00-Single_Step_Backtesting_Baselines.ipynb` 和 `01-Forecasting_with_ML.ipynb`
    在 `Chapter08` 中'
- en: The associated code for the chapter can be found at [https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter14](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter14).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章相关的代码可以在[https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter14](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter14)找到。
- en: What is attention?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是注意力？
- en: The idea of attention was inspired by human cognitive function. At any moment,
    the optic nerves in our eyes, the olfactory nerves in our noses, and the auditory
    nerves in our ears send a massive amount of sensory input to the brain. This is
    way too much information, definitely more than the brain can handle. But our brains
    have developed a mechanism that helps us to pay *attention* to only the stimuli
    that matter—such as a sound or a smell that doesn’t belong. Years of evolution
    have *trained* our brains to pick out anomalous sounds or smells because that
    was key for us surviving in the wild, where predators roamed free. In cognitive
    science, attention is defined as the cognitive process that allows an individual
    to selectively focus on specific information while ignoring other irrelevant stimuli.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力的概念灵感来自于人类的认知功能。我们眼中的视神经、鼻中的嗅神经以及耳中的听神经，时刻向大脑传送大量的感官信息。这些信息量过于庞大，显然超过了大脑的处理能力。但我们的大脑已经发展出一种机制，帮助我们只对那些重要的刺激物*注意*——比如一个不属于的声音或气味。经过多年的进化，我们的大脑已经被*训练*去挑出异常的声音或气味，因为这对于我们在野外生存至关重要，那时捕食者自由漫游。在认知科学中，注意力被定义为一种认知过程，允许个体选择性地专注于特定信息，同时忽略其他无关的刺激。
- en: Apart from this kind of instinctive attention, we are also able to control our
    attention by what we call *focusing* on something. You are doing it right now
    by choosing to ignore all the other stimuli that you are getting and focusing
    your attention on the contents of this book. While you are reading, your mobile
    phone pings you, the screen lights up, and your brain decides to focus its attention
    on the mobile screen, even though the book is still open in front of you. This
    feature of the human cognitive function has been the inspiration behind the attention
    mechanism in DL. Giving learning machines the ability to acquire this kind of
    attention has led to big breakthroughs in all fields of AI today.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这种本能的注意力外，我们还可以通过所谓的*集中注意力*来控制我们的注意力。你现在正在做的就是通过选择忽视所有其他的刺激，专注于本书的内容。当你阅读时，手机响了，屏幕亮了，尽管书本仍然摆在你面前，你的大脑决定将注意力集中到手机屏幕上。这种人类认知功能的特性正是深度学习中注意力机制的灵感来源。赋予学习机器这种注意力的能力，已经带来了今天AI各个领域的巨大突破。
- en: The idea was first applied to DL in Seq2Seq models, which we learned about in
    *Chapter 13*, *Common Modeling Patterns for Time Series*. In that chapter, we
    saw how the handshake between the encoder and decoder was done. For the **recurrent
    neural network** (**RNN**) family of models, we use the hidden states from the
    encoder at the end of the sequence as the initial hidden states in the decoder.
    Let’s call this handshake the **context**. The assumption here is that all the
    information required for the decoding task is encoded in the context and this
    is done in a timestep-by-timestep manner. So, for long context windows, the information
    from the first timestep has to be retained through multiple writes and re-writes
    until it’s used in the last timestep. This creates a kind of information bottleneck
    (*Figure 14.1*), where the model may struggle to retain important information
    through this limited context. There may be information in previous hidden states
    that can be useful for the decoding task. In 2015, Bahdanau et al. (Reference
    *1*) proposed the first known attention model in the context of DL. They proposed
    to learn attention weights, ![](img/B22389_14_001.png), for each hidden state
    corresponding to the input sequence and combine them into a single context vector
    while decoding.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个理念最早应用于深度学习中的Seq2Seq模型，我们在*第13章*《时间序列的常见建模模式》中学习了这一点。在那一章中，我们看到了编码器和解码器之间的“握手”是如何进行的。对于**递归神经网络**（**RNN**）系列模型，我们使用序列末尾的编码器隐藏状态作为解码器的初始隐藏状态。我们把这种“握手”称为**上下文**。这里的假设是，解码任务所需的所有信息都被编码在上下文中，并且这是按时间步进行的。因此，对于较长的上下文窗口，第一时间步的信息必须通过多次写入和重写才能在最后一个时间步使用。这就形成了一种信息瓶颈（*图14.1*），模型可能会在这个有限的上下文中难以保持重要信息。先前的隐藏状态中可能包含对解码任务有用的信息。2015年，Bahdanau等人（参考文献*1*）提出了深度学习领域中第一个已知的注意力模型。他们提出为每个对应于输入序列的隐藏状态学习注意力权重，![](img/B22389_14_001.png)，并在解码时将这些权重合并成一个单一的上下文向量。
- en: 'These attention weights are re-calculated for each decoding step using the
    similarity between the hidden states during decoding and all the hidden states
    in the input sequence (*Figure 14.2*):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个解码步骤中，这些注意力权重会根据解码过程中隐藏状态与输入序列中所有隐藏状态之间的相似度重新计算（*图14.2*）：
- en: '![Figure 14.1 – Traditional (top) versus attention model (bottom) in Seq2Seq
    models ](img/B22389_14_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图14.1 – 传统模型（上）与注意力模型（下）在Seq2Seq模型中的对比](img/B22389_14_01.png)'
- en: 'Figure 14.1: Traditional (top) versus attention model (bottom) in Seq2Seq models'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：传统模型（上）与注意力模型（下）在Seq2Seq模型中的对比
- en: 'To make things clearer, let’s adopt a formal way of describing the mechanism.
    Let ![](img/B22389_14_002.png) be the hidden states generated during the encoding
    process and ![](img/B22389_14_003.png) be the hidden states generated during decoding.
    The context vector will be *c*[j]:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地说明这一机制，我们采用一种正式的描述方式。设![](img/B22389_14_002.png)为编码过程中生成的隐藏状态，![](img/B22389_14_003.png)为解码过程中生成的隐藏状态。上下文向量将是*c*[j]：
- en: '![Figure 14.2 – Decoding using attention ](img/B22389_14_02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图14.2 – 使用注意力机制解码](img/B22389_14_02.png)'
- en: 'Figure 14.2: Decoding using attention'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2：使用注意力机制进行解码
- en: 'So, now we have the hidden states from the encoding stage (*H*), and we need
    to have a way to use this information in each step of decoding. The key here is
    that in each step of the decoding process, information from different hidden states
    might be relevant. This is exactly what attention weights do. So, for decoding
    step *j*, we use *s*[j][-1] and calculate attention weights (we’ll look at how
    attention weights are learned in detail soon), *a*[i], *j*, using the similarity
    between *s*[j][-1] and each hidden state in *H*. Now, we calculate the context
    vector, which combines the information in *H* in the right way:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了来自编码阶段的隐藏状态（*H*），我们需要一种方法在每个解码步骤中使用这些信息。关键在于，在每个解码步骤中，来自不同隐藏状态的信息可能是相关的。这正是注意力权重的作用。因此，在解码步骤
    *j* 中，我们使用 *s*[j][-1] 并计算注意力权重（我们很快会详细看一下如何学习注意力权重），*a*[i]，*j*，使用 *s*[j][-1] 与
    *H* 中每个隐藏状态的相似度。现在，我们计算上下文向量，它以正确的方式结合 *H* 中的信息：
- en: '![](img/B22389_14_004.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_14_004.png)'
- en: There are two main ways we can use this context vector, which we will look at
    in more detail later in the chapter. This breaks the information bottleneck that
    was present in the traditional Seq2Seq model and allows the models to access a
    larger pool of information and decide which information is relevant at each step
    of the decoding process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个上下文向量的两种主要方式，我们将在本章稍后详细讨论。这打破了传统 Seq2Seq 模型中的信息瓶颈，使得模型可以访问更大的信息池，并决定在每个解码步骤中哪些信息是相关的。
- en: Now, let’s see how these attention weights, ![](img/B22389_04_009.png), are
    calculated.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下这些注意力权重，![](img/B22389_04_009.png)，是如何计算的。
- en: The generalized attention model
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用的注意力模型
- en: Over the course of years, researchers have come up with different ways of calculating
    attention weights and using attention in DL models. Sneha Chaudhari et al. (Reference
    *8*) published a survey paper on attention models that proposes a generalized
    attention model that tries to incorporate all the variations in a single framework.
    Let’s structure our discussion around this generalized framework.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，研究人员提出了不同的计算注意力权重和在深度学习模型中使用注意力的方法。Sneha Chaudhari 等人（参考文献*8*）发布了一篇关于注意力模型的调查论文，提出了一种通用的注意力模型，旨在将所有变体融合到一个框架中。让我们围绕这个通用框架组织我们的讨论。
- en: 'We can think of an attention model as learning an attention distribution (![](img/B22389_04_009.png))
    for a set of keys, *K*, using a set of queries, *q*. In the example we discussed
    in the last section, the query would be *S*[j][-1]—the hidden state from the last
    timestep during decoding—and the keys would be *H*—all the hidden states generated
    using the input sequence. In some cases, the generated attention distribution
    is applied to another set of inputs called values, *V*. In many cases, *K* and
    *V* are the same, but to maintain the general form of the framework, we consider
    these separately. Using this terminology, we can define an attention model as
    a function of *q*, *K*, and *V*:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将注意力模型看作是使用一组查询 *q* 来为一组键 *K* 学习一个注意力分布（![](img/B22389_04_009.png)）。在我们上一节讨论的示例中，查询将是
    *S*[j][-1]——解码过程中最后一个时间步的隐藏状态——而键将是 *H*——使用输入序列生成的所有隐藏状态。在某些情况下，生成的注意力分布被应用到另一个称为值
    *V* 的输入集合。在许多情况下，*K* 和 *V* 是相同的，但为了保持框架的通用形式，我们将它们分开考虑。使用这个术语，我们可以将注意力模型定义为 *q*、*K*
    和 *V* 的一个函数：
- en: '![](img/B22389_14_007.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_14_007.png)'
- en: Here, *a* is an **alignment function** that calculates a similarity or a notion
    of similarity between the query (*q*) and keys (*k*[i]), and *v*[i] is the corresponding
    value for index ![](img/B22389_14_008.png). In the example we discussed in the
    previous section, this alignment function calculates how relevant an encoder hidden
    state is to a decoder hidden state, and *p* is a **distribution function** that
    converts this score into attention weights that sum up to 1.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*a* 是一个**对齐函数**，它计算查询（*q*）和键（*k*[i]）之间的相似性或相似度的概念，而 *v*[i] 是与索引 ![](img/B22389_14_008.png)
    对应的值。在我们在前面一节中讨论的示例中，这个对齐函数计算的是编码器隐藏状态与解码器隐藏状态的相关性，*p* 是一个**分布函数**，它将此分数转换为总和为1的注意力权重。
- en: '**Reference check:**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献检查：**'
- en: The research papers by Sneha Choudhari et al. are cited in the *References*
    section as reference *8*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Sneha Choudhari 等人的研究论文在*参考文献*部分被引用为参考文献*8*。
- en: Now that we have the generalized attention model, let’s also see how we can
    implement this in PyTorch. The full implementation can be found in the `Attention`
    class in `src/dl/attention.py`, but we will cover the key parts of it here.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了通用的注意力模型，让我们看看如何在PyTorch中实现它。完整实现可以在`src/dl/attention.py`中的`Attention`类中找到，但我们在这里将重点介绍其中的关键部分。
- en: 'The only information we require beforehand to initialize such a module is the
    hidden dimension of the queries and keys (encoder and decoder). So, the class
    definition and the `__init__` function of the class look like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化该模块之前，我们所需的唯一信息是查询和键（编码器和解码器）的隐藏维度。因此，类定义和`__init__`函数如下所示：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we need to define a `forward` function, which takes in two inputs:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要定义一个`forward`函数，该函数接受两个输入：
- en: '`query`: The query vector of size (*batch size*, *decoder dimension*), which
    we are going to use to find the attention weights with which to combine the keys.
    This is the *q* in *A*(*q*, *K*, *V*).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query`：大小为（*batch size*，*decoder dimension*）的查询向量，我们将用它来计算注意力权重，并用这些权重来组合键值。这是*A*(*q*，*K*，*V*)中的*q*。'
- en: '`key`: The key vector of size (*batch size*, *sequence length*, *encoder dimension*),
    which is the sequence of hidden states across which we will be calculating the
    attention weights. This is the *K* in *A*(*q*, *K*, *V*).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`key`：大小为（*batch size*，*sequence length*，*encoder dimension*）的键向量，这是我们将计算注意力权重的隐状态序列。这是*A*(*q*，*K*，*V*)中的*K*。'
- en: 'We are assuming keys and values are the same because, in most cases, they are.
    So, from the generalized attention model, we know that there are a few steps we
    need to perform:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设键和值是相同的，因为在大多数情况下，它们确实是。所以，根据通用的注意力模型，我们知道我们需要执行几个步骤：
- en: Calculate an alignment score—*a*(*k*[i], *q*)—for each query and key combination.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算一个对齐分数—*a*(*k*[i]，*q*)—对于每个查询和键的组合。
- en: Convert the scores to weights by applying a function—*p*(*a*(*k*[i], *q*)).
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过应用一个函数将分数转换为权重—*p*(*a*(*k*[i]，*q*))。
- en: Use the learned weights to combine the values—![](img/B22389_14_009.png).
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用学习到的权重来组合数值—![](img/B22389_14_009.png)。
- en: 'So, let’s see those steps in code in the `forward` method:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们看看在`forward`方法中的这些步骤：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The three lines of code in the `forward` method correspond to the three steps
    we discussed earlier. The first step, which is calculating the scores, is the
    key step that has led to many different types of attention, and therefore we have
    generalized that into a `_get_scores` abstract method that must be implemented
    by any class inheriting the `Attention` class. For the second line, we have used
    the `softmax` function for converting the scores to weights, and in the last line,
    we are doing an element-wise multiplication (`*`) between weights and values and
    summing across the sequence length to get the weighted value.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`方法中的三行代码对应我们之前讨论的三个步骤。第一步是计算分数，这是关键步骤，促成了许多不同类型的注意力机制，因此我们将其抽象为一个`_get_scores`方法，任何继承`Attention`类的类都必须实现此方法。在第二行，我们使用了`softmax`函数将分数转换为权重，最后一行则是对权重和数值进行逐元素相乘（`*`），并沿序列长度求和，得到加权后的值。'
- en: Now, let’s turn our attention toward alignment functions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向对齐函数。
- en: Alignment functions
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对齐函数
- en: There are many variations of the alignment function that have come up over the
    years. Let’s review a few popular ones that are used today.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，已经出现了许多不同版本的对齐函数。让我们回顾一下今天常用的几种。
- en: Dot product
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 点积
- en: 'This is probably the simplest alignment function of all. Luong et al. proposed
    this form of attention in 2015\. From linear algebra, we know that the dot product
    of two vectors tells us what amount of one vector goes in the direction of the
    other. It measures some kind of similarity between the two vectors, and this similarity
    considers both the magnitude of the vectors and the angle between them in the
    vector space. Therefore, when we take the dot product of our query and key vectors,
    we get a notion of similarity between them. One thing to note here is that the
    hidden dimensions of the query and the key should be the same for dot product
    attention to be applied. Formally, the similarity function can be defined as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是所有对齐函数中最简单的一种。Luong等人于2015年提出了这种形式的注意力机制。从线性代数中我们知道，两个向量的点积可以告诉我们一个向量在另一个向量方向上的投影量。它衡量了两个向量之间的相似性，这种相似性同时考虑了向量的大小和它们在向量空间中的夹角。因此，当我们计算查询和键向量的点积时，我们得到了一种它们之间相似性的度量。需要注意的一点是，查询和键的隐藏维度应该相同，才能应用点积注意力。正式地，相似度函数可以定义如下：
- en: '![](img/B22389_14_010.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_14_010.png)'
- en: We need to calculate this score for each of the elements, *K*[i], in the key,
    *K*, and instead of running a loop over each element in *K*, we can use a clever
    matrix multiplication trick to calculate the scores for all the keys in *K* in
    one shot. Let’s see how we can define the `_get_scores` function for dot product
    attention.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为每个键中的元素*K*[i]计算这个分数，*K*中的每个元素可以通过使用一个巧妙的矩阵乘法技巧，一次性计算出所有键的分数，而无需遍历*K*中的每个元素。让我们看看如何为点积注意力定义`_get_scores`函数。
- en: 'We know from the previous section that the query and values (which are the
    same as keys in our case) are of (*batch size*, *decoder dimension*) and (*batch
    size*, *sequence length*, *encoder dimension*) dimensions respectively, and will
    be called `q` and `v` in the `_get_scores` function. In this particular case,
    the decoder dimension and the encoder dimension are the same, so the scores can
    be calculated as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一节我们知道，查询和值（在我们的情况下与键相同）分别是（*batch size*，*decoder dimension*）和（*batch size*，*sequence
    length*，*encoder dimension*）维度，在`_get_scores`函数中它们将分别称为`q`和`v`。在这个特殊情况下，解码器维度和编码器维度是相同的，因此分数可以通过如下方式计算：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, `@` is shorthand for `torch.matmul`, which does matrix multiplication.
    The entire implementation is named `DotProductAttention` and can be found in `src/dl/attention.py`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`@`是`torch.matmul`的简写，用于进行矩阵乘法。整个实现的名称为`DotProductAttention`，可以在`src/dl/attention.py`中找到。
- en: Scaled dot product attention
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缩放点积注意力
- en: In 2017, Vaswani et al. proposed this type of attention in the seminal paper,
    *Attention Is All You Need*. We will delve into that paper later in this chapter,
    but now, let’s understand one key modification they suggested to the dot product
    attention. The modification is motivated by the concern that when the input is
    large, the *softmax function* we use to convert scores to weights may have very
    small gradients, which makes efficient learning difficult.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，Vaswani等人提出了这种注意力机制，在开创性的论文《*Attention Is All You Need*》中进行了介绍。我们将在本章稍后深入探讨这篇论文，但现在，让我们先理解他们对点积注意力提出的一个关键修改。这个修改是出于这样一个考虑：当输入非常大时，我们用来将分数转换为权重的*softmax函数*可能会有非常小的梯度，这使得高效学习变得困难。
- en: 'This is because the *softmax* function is not scale-invariant. The exponential
    function in the *softmax* function is the reason for this behavior. So, the higher
    we scale the inputs to the function, the more the largest input dominates the
    output, and this throttles the gradient flow in the network. If we assume *q*
    and *v* are *d*[k] dimensional vectors with 0 mean and a variance of 1, then their
    dot product would have a mean of zero and a variance of *d*[k]. Therefore, if
    we scale the output of the dot product by ![](img/B22389_14_011.png), then we
    bring the variance of the dot product back to 1\. So, by controlling for the scale
    of the inputs to the *softmax* function, we manage a healthy gradient flow through
    the network. The *Further reading* section has a link to a blog post that goes
    into this in more depth. Therefore, the scaled dot product alignment function
    can be defined as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为*softmax*函数不是尺度不变的。*softmax*函数中的指数函数是导致这种行为的原因。因此，当我们扩大输入的尺度时，最大的输入会更加主导输出，从而限制了网络中的梯度流动。如果我们假设*q*和*v*是具有零均值和方差为1的*d*[k]维向量，那么它们的点积将具有零均值和*d*[k]的方差。因此，如果我们通过![](img/B22389_14_011.png)来缩放点积的输出，那么我们就将点积的方差重新调整为1。因此，通过控制*softmax*函数输入的尺度，我们能够在网络中维持健康的梯度流动。*进一步阅读*部分包含了一篇博客文章链接，详细讲解了这一内容。因此，缩放点积对齐函数可以定义如下：
- en: '![](img/B22389_14_012.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_14_012.png)'
- en: 'Consequently, the only change we will have to make to the `PyTorch` implementation
    is one additional line:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要对`PyTorch`实现做的唯一更改就是再增加一行：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This has been implemented as a parameter in `DotProductAttention` in `src/dl/attention.py`.
    If you pass `scaled=True` while initializing the class, it will perform scaled
    dot product attention. We need to keep in mind that, similar to dot product attention,
    the scaled variant also requires the query and values to have the same dimensions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经在`src/dl/attention.py`中的`DotProductAttention`参数中实现。如果在初始化类时传递`scaled=True`，它将执行缩放点积注意力。我们需要记住，与点积注意力类似，缩放变种也要求查询和数值具有相同的维度。
- en: General attention
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通用注意力
- en: 'In 2015, Luong et al. (Reference *2*) proposed a slight variation of dot product
    attention by introducing a learnable *W* matrix into the calculation. They called
    it general attention. We can think of it as an attention mechanism that allows
    the query to be projected into a learned plane of the same dimension as the values/keys
    using the *W* matrix before computing the similarity score using a dot product.
    The alignment function can be written as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，Luong等人（参考文献*2*）通过在计算中引入可学习的*W*矩阵，提出了点积注意力的轻微变种。他们称之为通用注意力。我们可以将其视为一种注意力机制，它允许查询在计算相似度分数时，先通过*W*矩阵投影到一个与值/键相同维度的学习平面上，再使用点积来计算相似度分数。对齐函数可以写成如下：
- en: '![](img/B22389_14_013.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_14_013.png)'
- en: 'The corresponding `PyTorch` implementation can be found under the name `GeneralAttention`
    in `src/dl/attention.py`. The key line calculating the attention scores can be
    written as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的`PyTorch`实现可以在`src/dl/attention.py`中找到，命名为`GeneralAttention`。计算注意力分数的关键代码如下：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, `self.W` is a tensor of size (*encoder hidden dimension x decoder hidden
    dimension*). General attention can be used in cases where the query and key/value
    dimensions are different.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`self.W`是一个大小为（*编码器隐藏维度 x 解码器隐藏维度*）的张量。通用注意力可以用于查询和键/值维度不同的情况。
- en: Additive/concat attention
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加性/拼接注意力
- en: 'In 2015, Bahdanau et al. proposed additive attention, which was one of the
    first attempts at introducing attention to DL systems. Instead of using a defined
    similarity function such as the dot product, Bahdanau et al. proposed that the
    similarity function can be learned, giving the network more flexibility in deciding
    what it deems to be similar. They suggested that we can concatenate the query
    and the key into a single tensor and use a learnable matrix, *W*, to calculate
    the attention scores. This alignment function can be written as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，Bahdanau等人提出了加性注意力，这是首次将注意力引入深度学习系统的尝试之一。与使用定义好的相似度函数（如点积）不同，Bahdanau等人提出相似度函数可以通过学习来获得，使得网络在判断什么是相似时更加灵活。他们建议我们可以将查询和键拼接成一个张量，并使用可学习矩阵*W*来计算注意力分数。这个对齐函数可以写成如下：
- en: '![](img/B22389_14_014.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_14_014.png)'
- en: 'Here, *v*[t], *W*[q], and *W*[k] are learnable matrices. In cases where the
    query and key have different hidden dimensions, we can use *W*[q] and *W*[k] to
    project them into a single dimension and then perform a similarity calculation
    on them. If the query and key have the same hidden dimension, this is also equivalent
    to the variant of attention used in Luong et al., which they call *concat* attention,
    represented as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*v*[t]、*W*[q] 和 *W*[k] 是可学习的矩阵。如果查询和键具有不同的隐藏维度，我们可以使用 *W*[q] 和 *W*[k] 将它们投影到同一个维度，然后对它们进行相似度计算。如果查询和键具有相同的隐藏维度，这也等同于
    Luong 等人使用的注意力变体，他们称之为 *concat* 注意力，表示如下：
- en: '![](img/B22389_14_015.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_14_015.png)'
- en: It is simple linear algebra to see that both are the same and for engineering
    simplicity. The *Further reading* section has a link to a Stack Overflow answer
    that explains the equivalence.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单的线性代数可以看出，这两者是相同的，并且为了工程简便性，*进一步阅读* 部分有一个指向 Stack Overflow 解答的链接，解释了两者的等价性。
- en: We have included both implementations in `src/dl/attention.py` under `ConcatAttention`
    and `AdditiveAttention`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `src/dl/attention.py` 中的 `ConcatAttention` 和 `AdditiveAttention` 下包含了这两种实现。
- en: 'For `AdditiveAttention`, the key lines calculating the score are as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `AdditiveAttention`，计算得分的关键行如下：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first line repeats the query vector to the sequence length. This is just
    a linear algebra trick to calculate the score for all the encoder hidden states
    in a single operation rather than looping through them. *Line 2* projects both
    query and value into the same dimension using `self.W_q` and `self.W_v`, and *line
    3* applies the `tanh` activation function and uses matrix multiplication with
    `self.v` to produce the final scores. `self.W_q`, `self.W_v`, and `self.v` are
    learnable matrices, defined as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行将查询向量重复到序列长度。这只是一个线性代数技巧，用于一次性计算所有编码器隐藏状态的得分，而不是通过它们进行循环。*第 2 行* 使用 `self.W_q`
    和 `self.W_v` 将查询和数值投影到相同的维度，*第 3 行* 应用 `tanh` 激活函数并与 `self.v` 进行矩阵乘法以产生最终得分。`self.W_q`、`self.W_v`
    和 `self.v` 是可学习的矩阵，定义如下：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The only difference in `ConcatAttention` is that instead of two separate weights—`self.W_q`
    and `self.W_v`—we just have a single weight—`self.W`—defined as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConcatAttention` 中唯一的不同之处是，我们没有两个独立的权重——`self.W_q` 和 `self.W_v`——而是只有一个权重——`self.W`，定义如下：'
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And instead of adding the projections (*line 2*), we use the following line:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 并且我们不再添加投影（*第 2 行*），而是使用以下一行：
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Therefore, we can think of `AdditiveAttention` and `ConcatAttention` doing the
    same operation, but `AdditiveAttention` is adapted to handle different encoder
    and decoder dimensions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以认为 `AdditiveAttention` 和 `ConcatAttention` 执行相同的操作，但 `AdditiveAttention`
    被调整以处理不同的编码器和解码器维度。
- en: '**Reference check:**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查：**'
- en: The research papers by Luong et al., Badahnau et al., and Vaswani et al. are
    cited in the *References* section as references *2*, *1*, and *5* respectively.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Luong 等人、Badahnau 等人和 Vaswani 等人的研究论文在 *参考文献* 部分分别作为参考文献 *2*、*1* 和 *5* 被引用。
- en: Now that we have learned about a few popular alignment functions, let’s turn
    our attention toward the distribution function of the attention model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些流行的对齐函数，让我们将注意力转向注意力模型的分布函数。
- en: The distribution function
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布函数
- en: The primary goal of the distribution function is to convert the learned scores
    from the alignment function into a set of weights that add up to 1\. The *softmax*
    function is the most popular choice as a distribution function. It converts the
    score into a set of weights that sum up to one. This also gives us the freedom
    to interpret the learned weights as probabilities—the probability that the corresponding
    element is the most relevant.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 分布函数的主要目标是将对齐函数中学习到的得分转换为一组加起来为 1 的权重。*softmax* 函数是最常用的分布函数，它将得分转换为一组加起来为 1
    的权重。这也让我们能够将学习到的权重解释为概率——即相应元素是最相关的概率。
- en: Although *softmax* is the most popular choice, it is not without its drawbacks.
    The *softmax* weights are typically *dense*. What that means is that a probability
    mass (some weight) will be assigned to every element in the sequence over which
    we calculated the attention. The weights can be low, but still not 0\. There are
    situations where sparsity in the distribution function is desirable. Maybe we
    want to make sure we don’t give any weights some implausible options. Maybe we
    want to make the attention mechanism more interpretable.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然*softmax*是最常用的选择，但它并非没有缺点。*softmax*的权重通常是*密集的*。这意味着概率质量（某些权重）会分配给我们计算注意力时序列中的每个元素。这些权重可能很低，但仍然不为零。有时我们希望在分布函数中引入稀疏性。也许我们希望确保不为某些不太可能的选项分配任何权重。也许我们希望使注意力机制更加可解释。
- en: There are alternate distribution functions such as `sparsemax` (Martins et al.
    2016, Reference *3*) and `entmax` (Peters et al. 2019, Reference *4*) that are
    capable of assigning probability mass to a select few relevant elements and assigning
    zero to the rest of them. When we know that the output is only dependent on a
    few timesteps in the encoder, we can use such distribution functions to encode
    that knowledge into the model. Space activation functions like `Sparsemax` have
    the advantage of interpretability, as they provide a clearer distinction between
    the elements that are important (non-zero probabilities) and those that are not
    (zero probabilities).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他分布函数，如`sparsemax`（Martins 等人 2016年，参考文献*3*）和`entmax`（Peters 等人 2019年，参考文献*4*），它们能够将概率质量分配给一些相关元素，并将其余元素的概率设置为零。当我们知道输出仅依赖于编码器中的某些时间步时，我们可以使用这样的分布函数将这些知识编码到模型中。像`Sparsemax`这样的空间激活函数具有可解释性的优势，因为它们提供了一个更清晰的区分，表明哪些元素是重要的（非零概率），哪些是无关的（零概率）。
- en: '**Reference check:**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考检查：**'
- en: The research papers by Martins et al. and Peters et al. are cited in the *References*
    section as references *3* and *4* respectively.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Martins 等人和 Peters 等人的研究论文分别在*参考文献*部分被引用为参考文献*3*和*4*。
- en: Now that we have learned about a few attention mechanisms, it’s time to put
    them into practice.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些注意力机制，是时候将它们付诸实践了。
- en: Forecasting with sequence-to-sequence models and attention
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用序列到序列模型和注意力机制进行预测
- en: Let’s pick up the thread from *Chapter 13*, *Common Modeling Patterns for Time
    Series*, where we used Seq2Seq models to forecast a sample household (if you have
    not read the previous chapter, I strongly suggest you do it now) and modify the
    `Seq2SeqModel` class to also include an attention mechanism.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从*第13章*，*时间序列的常见建模模式*中接着讲，那里我们使用Seq2Seq模型预测了一个示例家庭的情况（如果你还没有阅读前一章，我强烈建议你现在阅读）并修改了`Seq2SeqModel`类以包括注意力机制。
- en: '**Notebook alert:**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒：**'
- en: To follow along with the complete code, use the notebook named `01-Seq2Seq_RNN_with_Attention.ipynb`
    in the `Chapter14` folder and the code in the `src` folder.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 若要跟进完整的代码，请使用`Chapter14`文件夹中的`01-Seq2Seq_RNN_with_Attention.ipynb`笔记本以及`src`文件夹中的代码。
- en: We are still going to inherit the `BaseModel` class we have defined in `src/dl/models.py`,
    and the overall structure is going to be very similar to the `Seq2SeqModel` class.
    The key difference will be that in our new model, with attention, we do not accept
    a fully connected layer as the decoder. It is not because it is not possible,
    but for convenience and brevity of the implementation. In fact, implementing a
    Seq2Seq model with a fully connected decoder is something you can do on your own
    to really internalize the concept.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然会继承在`src/dl/models.py`中定义的`BaseModel`类，并且整体结构将与`Seq2SeqModel`类非常相似。关键的区别在于，在我们的新模型中，使用注意力机制时，我们不接受全连接层作为解码器。并不是因为它不可行，而是出于实现的便利性和简洁性。事实上，实现一个带有全连接解码器的Seq2Seq模型是你可以自己做的事情，这样可以真正内化这个概念。
- en: Similar to the `Seq2SeqConfig` class, we define a very similar `Seq2SeqwAttnConfig`
    class that has the exact same set of parameters, but with some additional validation
    checks. One of the validation checks is disallowing a fully connected decoder.
    Another validation check would be making sure the decoder input size allows for
    the attention mechanism as well. We will see those requirements in detail shortly.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`Seq2SeqConfig`类，我们定义了一个非常相似的`Seq2SeqwAttnConfig`类，它具有完全相同的一组参数，但增加了一些额外的验证检查。一个验证检查是禁止使用全连接的解码器。另一个验证检查是确保解码器输入的大小允许使用注意力机制。我们稍后会详细看到这些要求。
- en: 'In addition to `Seq2SeqwAttnConfig`, we also define a `Seq2SeqwAttnModel` class
    to enable attention-enabled decoding. The only additional parameter here is `attention_type`,
    which is a string parameter that takes the following values:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `Seq2SeqwAttnConfig`，我们还定义了一个 `Seq2SeqwAttnModel` 类来启用支持注意力的解码。这里唯一的额外参数是
    `attention_type`，这是一个字符串类型的参数，可以取以下值：
- en: '`dot`: Dot product attention'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dot`：点积注意力'
- en: '`scaled dot`: Scaled dot product attention'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaled dot`：缩放点积注意力'
- en: '`general`: General attention'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`general`：通用注意力'
- en: '`additive`: Additive attention'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additive`：加法注意力'
- en: '`concat`: Concat attention'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`concat`：拼接注意力'
- en: The entire code is available in `src/dl/models.py`. We will be covering only
    the `forward` function in detail in the book because that is the only place where
    there is a key difference. The rest of the class is about defining the right attention
    model based on input parameters and so on.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 整个代码可以在 `src/dl/models.py` 中找到。我们将在书中详细讲解 `forward` 函数，因为这是唯一一个有关键区别的地方。类的其余部分涉及根据输入参数等定义正确的注意力模型。
- en: The encoder part is exactly the same as `SeqSeqModel`, which we saw in the last
    chapter. The only difference is in the decoding where we will be using attention.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器部分与我们在上一章看到的 `SeqSeqModel` 完全相同。唯一的区别是在解码部分，我们将使用注意力。
- en: Now, let’s talk about how we are going to use the attention output in decoding.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一下我们将如何在解码中使用注意力输出。
- en: As I mentioned before, there are two schools of thought on how to use attention
    while decoding. Using the same terminology we have been using for attention, let’s
    see the difference between them.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前提到的，在解码时使用注意力有两种思路。我们使用的注意力术语来看，让我们看看它们之间的区别。
- en: Luong et al. use the decoder hidden state at step *j*, *s*[j], to calculate
    the similarity between itself and all the encoder hidden states, *H*, to calculate
    the context vector, *c*[j]. This context vector, *c*[j], is then concatenated
    with the decoder hidden state, *s*[j], and this combined tensor is used as the
    input to the linear layer that generates the output.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Luong 等人使用解码器在步骤 *j* 的隐藏状态 *s*[j]，计算它与所有编码器隐藏状态 *H* 之间的相似度，从而计算上下文向量 *c*[j]。然后，将该上下文向量
    *c*[j] 与解码器隐藏状态 *s*[j] 拼接在一起，这个组合后的张量被用作输入到生成输出的线性层。
- en: Bahdanau et al. use attention in another way. They use the decoder hidden state
    from the previous timestep, *s*[j-][1], and calculate the similarity with all
    the encoder hidden states, *H*, to calculate the context vector, *c*[j]. And now,
    this context vector, *c*[j], is concatenated with the input to decoding step *j*,
    *x*[j]. It is this concatenated input that is used in the decoding step that uses
    an RNN.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Bahdanau 等人以另一种方式使用注意力。他们使用前一时间步解码器的隐藏状态 *s*[j-1]，并计算它与所有编码器隐藏状态 *H* 之间的相似度，从而计算上下文向量
    *c*[j]。然后，这个上下文向量 *c*[j] 会与解码步骤 *j* 的输入 *x*[j] 拼接在一起。正是这个拼接后的输入被用在使用 RNN 的解码步骤中。
- en: 'We can see the differences visually in *Figure 14.3*. The *Further reading*
    section also has another brilliant animation of attention in *Attn: Illustrated
    Attention*. This can also help you understand the mechanism well:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以在 *图 14.3* 中直观地看到它们之间的区别。*进一步阅读* 部分还提供了关于注意力的另一精彩动画——*Attn: Illustrated
    Attention*。这也能帮助你更好地理解机制：'
- en: '![Figure 14.3 – Attention-based decoding: Bahdanau versus Luong ](img/B22389_14_03.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.3 – 基于注意力的解码：Bahdanau 与 Luong](img/B22389_14_03.png)'
- en: 'Figure 14.3: Attention-based decoding: Bahdanau versus Luong'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3：基于注意力的解码：Bahdanau 与 Luong
- en: 'In our implementation, we have chosen the Bahdanau way of decoding, where we
    use the concatenated context vector and input as the input for decoding. Because
    of that, there is a condition the decoder must satisfy: the `input_size` parameter
    of the decoder should be equal to the sum of the `input_size` parameter of the
    encoder and the `hidden_size` parameter of the encoder. This validation is built
    into `Seq2SeqwAttnConfig`.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们选择了 Bahdanau 解码方式，在这种方式下，我们将拼接的上下文向量和输入作为解码的输入。因此，解码器必须满足一个条件：解码器的
    `input_size` 参数应当等于编码器的 `input_size` 参数与编码器的 `hidden_size` 参数之和。这个验证被内建在 `Seq2SeqwAttnConfig`
    中。
- en: 'The following code block has all the code necessary for decoding with attention
    and has line numbers so that we can go line by line and explain what we are doing:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含了所有必要的注意力解码代码，并且带有行号，这样我们可以逐行解释我们在做什么：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Lines 1* and *2* are the same as in the `Seq2SeqMode`l class where we set
    up the variable to store the prediction and extract the starting input to be passed
    to the decoder, and *line 3* starts the loop for decoding step by step.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*第1行*和*第2行*与`Seq2SeqModel`类中的设置相同，我们在其中设置变量来存储预测，并提取传递给解码器的起始输入，*第3行*开始逐步进行解码循环。'
- en: 'Now, in each step, we need to use the hidden state from the previous timestep
    to calculate the context vector. If you remember the output shapes of an RNN (*Chapter
    12*, *Building Blocks of Deep Learning for Time Series*), we know that it is (*number
    of layers*, *batch size*, *hidden size*). But we need our query hidden state to
    be of the dimension (*batch size*, *hidden size*). Luong et al. suggested using
    the hidden states from the top layer of a stacked RNN model as the query, and
    we are doing just that here:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在每一步中，我们需要使用前一时间步的隐藏状态来计算上下文向量。如果你记得RNN的输出形状（*第12章*，*构建时间序列深度学习的基础*），我们知道它是（*层数*，*批量大小*，*隐藏大小*）。但我们需要的查询隐藏状态的维度是（*批量大小*，*隐藏大小*）。Luong等人建议使用堆叠RNN模型顶部层的隐藏状态作为查询，这正是我们在这里所做的：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If the RNN is bi-directional, we would need to slightly alter the retrieval
    because now, the last two rows of the tensor would be the output from the last
    layer (one forward and one backward). There are many ways to combine them into
    a single tensor—we can concatenate them, we can sum them, or we can even mix them
    using a linear layer. Here, we just concatenate them:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果RNN是双向的，我们需要稍微调整检索过程，因为现在，张量的最后两行将是来自最后一层的输出（一前一后）。有很多方式可以将它们合并为一个单一张量——我们可以将它们连接起来，或者对它们求和，甚至可以通过线性层将它们混合。这里，我们只是将它们连接起来：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we have the hidden state, we use it as the query in the attention layer
    (*line 5*). In *line 8*, we concatenate the context with the input. Lines *9*
    to *16* do the rest of the decoding in a similar way to `Seq2SeqModel`.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了隐藏状态，我们将其作为查询传入注意力层（*第5行*）。在*第8行*，我们将上下文与输入连接起来。*第9*到*第16行*以类似于`Seq2SeqModel`的方式完成剩余的解码。
- en: 'The notebook trains a multi-step Seq2Seq model (the best-performing variant
    with teacher forcing) with all the different types of attention we covered in
    the chapter using the same setup we developed in the last chapter. The results
    are summarized in the following table:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本训练了一个多步骤的Seq2Seq模型（最佳表现的变体使用教师强制）以及本章中介绍的所有不同类型的注意力机制，使用我们在上一章中开发的相同设置。结果总结如下表所示：
- en: '![Figure 14.4 – Summary table for Seq2Seq models with attention ](img/B22389_14_04.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.4 – 带注意力机制的Seq2Seq模型汇总表](img/B22389_14_04.png)'
- en: 'Figure 14.4: Summary table for Seq2Seq models with attention'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4：带注意力机制的Seq2Seq模型汇总表
- en: We can see that it is showing considerable improvements in **MAE**, **MSE**,
    and **MASE** by including attention, and out of all the variants of attention,
    the simple dot product attention performed the best, closely followed by additive
    attention. At this point, some of you might have a question in your mind—*Why
    didn’t the scaled dot product work better than the dot product attention?* Scaling
    was supposed to make the dot product work better, wasn’t it?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，通过引入注意力机制，**MAE**、**MSE**和**MASE**都有了显著的改善，在所有注意力变体中，简单的点积注意力表现最好，其次是加性注意力。此时，可能有些人心中会有一个问题——*为什么缩放点积没有比点积注意力表现得更好？*
    缩放应该能使点积效果更好，不是吗？
- en: There is a lesson to be learned here (which applies to all **machine learning**
    (**ML**)). No matter how much better a particular technique is in theory, you
    can always find examples in which it performs worse. Here, we saw just one household,
    and it is not surprising that we saw that scaled dot product attention didn’t
    work better than the normal dot product attention. But if we had evaluated at
    scale and realized that this is a pattern across multiple datasets, then it would
    be concerning.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个教训（适用于所有**机器学习**（**ML**））。无论某种技术在理论上有多好，你总能找到一些例子证明它表现更差。在这里，我们只看到了一个家庭，并不奇怪我们看到缩放点积注意力没有比普通点积注意力更好。但如果我们在大规模评估中发现这是跨多个数据集的模式，那么就值得关注了。
- en: So, we have seen that attention does make the models better. There was a lot
    of research done on using attention in various forms to enhance the performance
    of **neural network** (**NN**) models. Most of that research was carried out in
    **natural language processing** (**NLP**), specifically in language translation
    and language modeling. Soon, researchers stumbled upon a surprising result that
    changed the course of DL progress drastically—the Transformer model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们已经看到，注意力机制确实使得模型变得更好。大量的研究都在探讨如何利用不同形式的注意力来增强**神经网络**（**NN**）模型的性能。大部分这类研究都集中在**自然语言处理**（**NLP**）领域，特别是在语言翻译和语言建模方面。不久后，研究人员偶然发现了一个惊人的结果，这一发现极大地改变了深度学习（DL）发展的轨迹——Transformer模型。
- en: Transformers—Attention is all you need
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer——注意力就是你所需要的一切
- en: While the introduction of attention was a shot in the arm for RNNs and Seq2Seq
    models, they still had one problem. The RNNs were recurrent, and that meant they
    needed to process each word in a sentence in a sequential manner.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然引入注意力机制对RNN和Seq2Seq模型来说是一针强心剂，但它们仍然存在一个问题。RNN是递归的，这意味着它们需要按顺序处理句子中的每个单词。
- en: For popular Seq2Seq model applications such as language translation, it meant
    processing long sequences of words became really time-consuming. In short, it
    was difficult to scale them to a large corpus of data. In 2017, Vaswani et al.
    (Reference *5*) authored a seminal paper titled *Attention Is All You Need*. Just
    as the title of the paper implies, they explored an architecture that used attention
    (scaled dot product attention) and threw away recurrent networks altogether. To
    the surprise of NLP researchers around the world, these models (which were dubbed
    Transformers) outperformed the then state-of-the-art Seq2Seq models in language
    translation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于流行的Seq2Seq模型应用，如语言翻译，这意味着处理长序列的单词变得非常耗时。简而言之，很难将它们扩展到大规模的数据语料库。在2017年，Vaswani等人（参考文献*5*）发表了一篇具有里程碑意义的论文，题为*Attention
    Is All You Need*。正如论文标题所暗示的，他们探讨了一种使用注意力（缩放点积注意力）的架构，并完全抛弃了递归网络。令全球NLP研究人员惊讶的是，这些被称为Transformer的模型在语言翻译任务中超过了当时最先进的Seq2Seq模型。
- en: This spurred a flurry of research activity around this new class of models,
    and pretty soon, in 2018, Devlin et al. (Reference *6*) from Google developed
    a bi-directional version of Transformers and trained the now famous language model
    **BERT** (which stands for **Bidirectional Encoder Representations from Transformers**),
    and broke many state-of-the-art results in multiple tasks. This is considered
    to be the moment when Transformers as a model class really *arrived*. Fast-forward
    to 2022—Transformer models are ubiquitous. They are used in almost all tasks in
    NLP, and in many other sequence-based tasks such as time series forecasting and
    **reinforcement learning** (**RL**). They have also been successfully used in
    **computer vision** (**CV**) tasks as well.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这激发了围绕这一新型模型类别的研究热潮，没过多久，在2018年，Google的Devlin等人（参考文献*6*）开发了一种双向Transformer，并训练了现在著名的语言模型**BERT**（即**Bidirectional
    Encoder Representations from Transformers**），并在多个任务中突破了许多最新的技术成果。这被认为是Transformer作为模型类别真正*登上舞台*的时刻。快进到2022年——Transformer模型已经无处不在。它们几乎被应用于NLP中的所有任务，并且在许多其他基于序列的任务中也有所应用，比如时间序列预测和**强化学习**（**RL**）。它们还成功应用于**计算机视觉**（**CV**）任务中。
- en: There have been numerous modifications and adaptations to the vanilla Transformer
    model to make it more suitable for time series forecasting. But let’s discuss
    the vanilla Transformer architecture that Vaswani et al. proposed in 2017 first.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对原始Transformer模型进行了许多修改和适应，使其更适合时间序列预测。但让我们先讨论Vaswani等人2017年提出的原始Transformer架构。
- en: Attention is all you need
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力就是你所需要的一切
- en: The model Vaswani et al. proposed (hereby referred to as the vanilla Transformer)
    is also an encoder-decoder model, but both the encoder and decoder are non-recurrent.
    They are entirely composed of attention mechanisms and feed-forward networks.
    Since the Transformer model was developed first for text sequences, let’s use
    the same example to understand and then adapt to the time series context.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Vaswani等人提出的模型（以下简称“原始Transformer”）也是一个编码器-解码器模型，但编码器和解码器都是非递归的。它们完全由注意力机制和前馈网络组成。由于Transformer模型最初是为文本序列开发的，我们就用相同的例子来理解，然后再适应到时间序列的上下文。
- en: There are a few key components of the model that need to be understood to put
    the whole thing together. Let’s take them one by one.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将整个模型组合起来，需要理解模型中的几个关键组件。我们逐一来看。
- en: Self-attention
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自注意力
- en: We saw how scaled dot product attention works earlier in this chapter (in the
    *Alignment functions* section), but there, we were calculating attention between
    the encoder and decoder hidden states. Self-attention is when we have an input
    sequence, and we calculate the attention scores between that input sequence itself.
    Intuitively, we can think of this operation as enhancing the contextual information
    and enabling the downstream components to use this enhanced information for further
    processing.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前在本章中看到了缩放点积注意力是如何工作的（在*对齐函数*部分），但在那里，我们计算的是编码器和解码器隐藏状态之间的注意力。当我们有一个输入序列并计算该输入序列本身之间的注意力分数时，这就是自注意力。直观地说，我们可以将这个操作视为增强上下文信息，并使下游组件能够利用这些增强的信息进行进一步处理。
- en: We saw the `PyTorch` implementation for encoder-decoder attention earlier, but
    that implementation was more aligned toward the step-by-step decoding of an RNN.
    Computing the attention scores for each query-key pair in one shot is something
    very simple to achieve using standard matrix multiplication and is essential to
    computing efficiency.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看过`PyTorch`中编码器-解码器注意力的实现，但那个实现更偏向于逐步解码RNN。通过标准矩阵乘法一次性计算每个查询-键对的注意力分数是非常简单且对计算效率至关重要的事情。
- en: '**Notebook alert:**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提示：**'
- en: To follow along with the complete code, use the notebook named `02-Self-Attention_and_Multi-Headed_Attention.ipynb`
    in the `Chapter14` folder.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随完整的代码，请使用位于`Chapter14`文件夹中的笔记本`02-Self-Attention_and_Multi-Headed_Attention.ipynb`。
- en: In NLP, it is standard practice to represent each word as a learnable vector
    called an embedding. This is because text or strings have no place in a mathematical
    model. For our example’s sake, let’s assume we use an embedding vector of size
    512 for each word, and let’s assume that the attention mechanism has an internal
    dimension of 64\. Let’s elucidate the attention mechanism using a sentence with
    10 words.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，将每个单词表示为称为嵌入的可学习向量是标准做法。这是因为文本或字符串在数学模型中没有位置。为了我们的示例，假设我们为每个单词使用大小为512的嵌入向量，并假设注意机制具有64的内部维度。让我们通过一个包含10个单词的句子来阐明注意机制。
- en: 'After embedding, the sentence would be a tensor with dimensions `(10, 512)`.
    We need to have three learnable weight matrices, *W*[q], *W*[k], and *W*[v], to
    project the input embedding into the attention dimension `(64)`. See *Figure 14.5*:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入后，句子将成为一个维度为`(10, 512)`的张量。我们需要三个可学习的权重矩阵*W*[q]、*W*[k]和*W*[v]来将输入嵌入投影到注意力维度`(64)`。参见*图
    14.5*：
- en: '![Figure 14.5 – Self-attention layer: input sentence and the learnable weights
    ](img/B22389_14_05.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.5 – 自注意力层：输入句子和可学习权重](img/B22389_14_05.png)'
- en: 'Figure 14.5: Self-attention layer: input sentence and the learnable weights'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5：自注意力层：输入句子和可学习权重
- en: 'The first operation projects the sentence tensor into a query, key, and value
    with dimensions equal to (*sequence length*, *attention dim*). This is done by
    using a matrix multiplication between the sentence tensor and learnable matrices.
    See *Figure 14.6*:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步操作将句子张量投影到查询、键和值，其维度等于（*序列长度*，*注意力维度*）。这是通过使用句子张量和可学习矩阵之间的矩阵乘法来实现的。参见*图 14.6*：
- en: '![Figure 14.6 – Self-attention layer: query, key, and value projection ](img/B22389_14_06.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.6 – 自注意力层：查询、键和值投影](img/B22389_14_06.png)'
- en: 'Figure 14.6: Self-attention layer: query, key, and value projection'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6：自注意力层：查询、键和值投影
- en: 'Now that we have the query, key, and value, we can calculate the attention
    weights of every query-key pair using matrix multiplication between the query
    and the transpose of the keys. The matrix multiplication is nothing but the dot
    product of each query with each of the values and gives us a square matrix of
    (*sequence length*, *sequence length*). See *Figure 14.7*:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了查询、键和值，我们可以使用查询与键的转置之间的矩阵乘法来计算每个查询-键对的注意力权重。矩阵乘法实际上就是每个查询与每个值的点积，给出了一个大小为（*序列长度*，*序列长度*）的方阵。参见*图
    14.7*：
- en: '![](img/B22389_14_07.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.7](img/B22389_14_07.png)'
- en: 'Figure 14.7: Self-attention layer: attention scores between Q and K'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7：自注意力层：查询和键之间的注意力分数
- en: Converting the attention scores to attention weights is just about scaling and
    applying the *softmax* function, as we discussed in the *Scaled dot product attention*
    section.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 将注意力分数转换为注意力权重只是简单地进行缩放并应用*softmax*函数，正如我们在*缩放点积注意力*部分讨论过的那样。
- en: 'Now that we have the attention weights, we can use them to combine the value.
    The element-wise multiplication and then summing across the weights can be done
    efficiently using another matrix multiplication. See *Figure 14.8*:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了注意力权重，可以利用它们来结合值。通过元素级的乘法然后在权重上求和，可以通过另一种矩阵乘法高效完成。见*图 14.8*：
- en: '![Figure 14.8 – Self-attention layer: combining V using the learned attention
    weights ](img/B22389_14_08.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.8 – 自注意力层：使用学习到的注意力权重结合V](img/B22389_14_08.png)'
- en: 'Figure 14.8: Self-attention layer: combining V using the learned attention
    weights'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8：自注意力层：使用学习到的注意力权重结合V
- en: Now, we have seen how attention is applied to all the query-key pairs in monolithic
    matrix operations rather than doing the same operation for each query in a sequential
    way. But *Attention Is All You Need* proposed something even better.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经看到注意力如何应用于所有查询-键对的整体矩阵运算，而不是以顺序方式对每个查询进行相同的操作。但*Attention Is All You
    Need*提出了一个更好的方法。
- en: Multi-headed attention
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多头注意力
- en: Since Transformers intended to take away the entire recurrent architecture,
    they needed to beef up the attention mechanism because that was the workhorse
    of the model. So, instead of using a single attention head, the authors of the
    paper proposed multiple attention heads acting together in different subspaces.
    We know that attention helps the model focus on a few elements of the many. **Multi-headed
    attention** (**MHA**) does the same thing but focuses on different aspects or
    different sets of elements, thereby increasing the capacity of the model. If we
    want to draw an analogy to the human mind, we consider many aspects of a situation
    before we make a decision.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Transformers旨在摒弃整个递归架构，它们需要增强注意力机制，因为那是模型的主力。因此，论文的作者提出了多个注意力头共同作用于不同子空间。我们知道，注意力帮助模型专注于众多元素中的少数几个。**多头注意力**（**MHA**）做了同样的事情，但它关注的是不同的方面或不同的元素集，从而增加了模型的容量。如果我们想用人类思维来做个类比，我们在做决策前会考虑情况的多个方面。
- en: For instance, if we decide to step out of the house, we will pay attention to
    the weather, we will pay attention to the time so that whatever we want to accomplish
    is still possible, we will pay attention to how punctual that friend you made
    a plan with has been in the past, and leave our house accordingly. You can think
    of each of these things as one head of attention. Therefore, MHA enables Transformers
    to *attend* to multiple aspects at the same time.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，如果我们决定走出家门，我们会关注天气，关注时间，以确保无论我们想完成什么，都是可能的，我们会关注和你约好见面的朋友过去的守时情况，然后根据这些去决定何时离开家。你可以把这些看作是注意力的每一个头。因此，MHA使得Transformers能够同时*关注*多个方面。
- en: Normally, if there are eight heads, we would assume that we would have to do
    the computation that we saw in the last section eight times. But thankfully, that
    is not the case. There are clever ways of accomplishing this MHA using the same
    kind of matrix multiplication, but now with larger matrices. Let’s see how that
    is done.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，如果有八个头，我们会认为我们必须做上节中看到的计算八次。但幸运的是，事实并非如此。通过使用相同类型的矩阵乘法，但现在使用更大的矩阵，有巧妙的方法完成这个MHA。让我们来看一下是如何做到的。
- en: We will continue the same example and see a case where we have eight attention
    heads. There is one condition that needs to be satisfied to do this efficient
    calculation of MHA—the attention dimension should be divisible by the number of
    heads we are using.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用相同的例子，看看在我们有八个注意力头的情况下会发生什么。有一个条件需要满足，以便高效计算MHA——注意力维度应该能够被我们使用的头数整除。
- en: 'The initial steps are exactly the same. We take in the input sentence tensor
    and project it into the query, key, and value. Now, we split the query, key, and
    value into separate query, key, and value subspaces for each head by doing some
    basic tensor re-arrangement. See *Figure 14.9*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的步骤完全相同。我们将输入句子的张量传入，并将其投影到查询、键和值。现在，我们通过进行一些基本的张量重排，将查询、键和值分割成每个头的独立查询、键和值子空间。见*图
    14.9*：
- en: '![Figure 14.9 – Multi-headed attention: reshaping Q, K, and V into subspaces
    for each head ](img/B22389_14_09.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.9 – 多头注意力：将Q、K和V重塑为每个头的子空间](img/B22389_14_09.png)'
- en: 'Figure 14.9: Multi-headed attention: reshaping Q, K, and V into subspaces for
    each head'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9：多头注意力：将Q、K和V重塑为每个头的子空间
- en: 'Now, we calculate the attention scores for each head in a single operation
    and combine them with the value to get the attention output for each head. See
    *Figure 14.10*:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对每个头计算注意力得分并将其与值结合，以获取每个头的注意力输出。请参见*图 14.10*：
- en: '![](img/B22389_14_10.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_14_10.png)'
- en: 'Figure 14.10: Multi-headed attention: calculating attention weights and combining
    the value'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10：多头注意力：计算注意力权重并结合值
- en: 'We have the attention output of each head in the `attn_output` variable. Now,
    all we need to do is reshape the array so that we stack the outputs from all the
    attention heads in a single dimension. See *Figure 14.11*:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了每个头的注意力输出，保存在`attn_output`变量中。现在，我们只需要重塑数组，将所有注意力头的输出堆叠在一个维度上。请参见*图 14.11*：
- en: '![Figure 14.11 – Multi-headed attention: reshaping and stacking all the attention
    head outputs ](img/B22389_14_11.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.11 – 多头注意力：重塑并堆叠所有注意力头输出](img/B22389_14_11.png)'
- en: 'Figure 14.11: Multi-headed attention: reshaping and stacking all the attention
    head outputs'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.11：多头注意力：重塑并堆叠所有注意力头输出
- en: In this way, we can do MHA in a fast and efficient manner. Now, let’s look at
    another key innovation that makes Transformers work.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以快速高效地执行多头注意力（MHA）。现在，让我们来看一下另一项关键创新，它使得Transformers能够工作。
- en: Positional encoding
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置信息编码
- en: Transformers successfully prevented recurrence and unlocked a performance bottleneck
    of sequential operations. This also means that the Transformer model is agnostic
    of the order of the sequence. In mathematical terms, if RNNs were considering
    the sequence as a sequence, Transformers consider it as a set of values. For the
    Transformer, each position is independent of the other, and hence one key aspect
    we would seek from a model that processes sequences is missing. The original authors
    did propose a way to make sure we do not lose this information—**positional encoding**.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer成功地避免了递归，突破了顺序操作的性能瓶颈。这也意味着Transformer模型对序列的顺序不敏感。从数学角度来看，如果RNNs考虑将序列视为一个序列，Transformers则将其视为一组值。对于Transformer来说，每个位置彼此独立，因此我们期望从处理序列的模型中获得的一个关键特征是缺失的。原始作者确实提出了一种方法，确保我们不会丢失这些信息——**位置信息编码**。
- en: There have been many variants of positional encoding that have come up in subsequent
    years of research, but the most common one is still the variant that is used in
    the vanilla Transformer.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续几年的研究中，出现了许多位置信息编码的变种，但最常见的仍然是原始Transformer中使用的变种。
- en: 'The solution proposed by Vaswani et al. was to add a particular vector, which
    encodes the position mathematically using sine and cosine functions, to each of
    the input tokens before processing them through the self-attention layer. If the
    input *X*, is a *d*[model]-dimensional embedding for *n* tokens in a sequence,
    positional embeddings, *P*, is a matrix of the same size (*n* x *d*[model]). The
    element on the *pos*^(th) row and 2*i*^(th) or (2*i* + 1)^(th) column is defined
    as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Vaswani等人提出的解决方案是，在处理输入标记通过自注意力层之前，向每个输入标记添加一个特殊的向量，该向量通过正弦和余弦函数对位置进行数学编码。如果输入*X*是一个*n*个标记的*d*[model]维嵌入，位置信息编码*P*是一个相同大小的矩阵（*n*
    x *d*[model]）。矩阵中*pos*^(行)和2*i*^(列)或(2*i* + 1)^(列)的元素定义如下：
- en: '![](img/B22389_14_016.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_14_016.png)'
- en: '![](img/B22389_14_017.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22389_14_017.png)'
- en: Although this looks a little complicated and counterintuitive, let’s break this
    down to understand it better.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这看起来有点复杂且违背直觉，但让我们分解一下，便于更好地理解。
- en: From 20,000 feet, we know that these positional encodings capture the positional
    information, and we add them to the input embeddings. But why do we add them to
    the input embeddings? Let’s make this clearer. Let’s assume the embedding dimension
    is just 2 (this is for ease of visualization and grasping the concept better),
    and we have a word, *A*, represented using this token. For our experiment, let’s
    assume that we have the same word, *A*, repeated several times in our sequence.
    What happens if we add the positional encoding to it?
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 从20,000英尺的高度来看，我们知道这些位置信息编码捕捉了位置信息，并将其添加到输入嵌入中。但为什么我们要将它们添加到输入嵌入中呢？让我们来澄清一下。假设嵌入维度只有2（这是为了便于可视化和更好地理解概念），并且我们有一个单词，*A*，用这个标记表示。为了方便实验，假设在我们的序列中多次重复相同的单词，*A*。如果我们将位置信息编码添加到它上面会发生什么呢？
- en: 'We know that the sine or cosine functions vary between 0 and 1\. So, each of
    these encodings we add to the word embedding just perturbs the word embedding
    within a unit circle. As *pos* increases, we can see the position-encoded word
    embedding trace a unit circle around the original embedding (*Figure 14.12*):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道正弦或余弦函数的值在0和1之间变化。因此，我们添加到单词嵌入中的每个编码只是扰动了单词嵌入在单位圆内的位置。随着*pos*的增加，我们可以看到位置编码的单词嵌入在原始嵌入周围描绘一个单位圆（见*图
    14.12*）：
- en: '![Figure 14.12 – Position encoding: intuition ](img/B22389_14_12.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.12 – 位置编码：直观理解](img/B22389_14_12.png)'
- en: 'Figure 14.12: Position encoding: intuition'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.12：位置编码：直观理解
- en: In *Figure 14.12*, we have assumed a random embedding for a word, *A* (represented
    by the cross marker), and added position embedding assuming *A* is in different
    positions. These position-encoded vectors are represented by star markers with
    the corresponding positions mentioned in numbers next to them. We can see how
    each position is a slightly perturbed point of the original vector, and it happens
    in a cyclical manner in a clockwise direction. We can see position 0 right at
    the top with 1, 2, 3, and so on in the clockwise direction. By having this representation,
    the model can figure out the word in different locations, and still retain the
    overall position in the semantic space.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 14.12*中，我们假设了一个单词*A*（由交叉标记表示）的随机嵌入，并且假设*A*处于不同的位置，添加了位置嵌入。这些位置编码向量由星号标记表示，并在旁边用数字标注了对应的位置。我们可以看到，每个位置是原始向量的一个稍微扰动的点，并且这种扰动是以顺时针方向周期性进行的。我们可以看到位置0位于最上方，接下来是1、2、3，依此类推，按顺时针方向排列。通过这种表示，模型能够识别单词在不同位置的含义，并且仍然保持语义空间中的整体位置。
- en: Now that we know why we are adding the positional encodings to the input embeddings
    and have seen why it works, let’s get into the details and see how the terms inside
    the sine and cosine are calculated. *pos* represents the position of the token
    in the sequence. If the maximum length of the sequence is 128, *pos* varies from
    0 to 127\. *i* represents the position along the embedding dimension, and because
    of the way the formula has been defined, for each value of *i*, we have two values—a
    sine and a cosine. Therefore, *i* will be half the number of dimensions, *d*[model],
    and will go from 0 to *d*[model]/2.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道为什么要将位置编码添加到输入嵌入中，并且了解了它为何有效，让我们深入了解细节，看看正弦和余弦函数中的各个项是如何计算的。*pos*表示标记在序列中的位置。如果序列的最大长度是128，*pos*的值从0到127变化。*i*表示嵌入维度中的位置，由于公式的定义方式，对于每个*i*值，我们有两个值——一个正弦和一个余弦。因此，*i*将是维度数量的一半，*d*[model]，并且从0到*d*[model]/2变化。
- en: 'With all this information, we know that the term inside the sine and cosine
    functions approaches 0 as we go toward the end of the embedding dimension. It
    also increases from 0 as we move along the sequence dimension. For each pair (2*i*
    and 2*i*+1) of positions in the embedding dimension, we have a complementary sine
    and cosine wave, as *Figure 14.13* shows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我们知道正弦和余弦函数内部的项在我们接近嵌入维度的末端时趋向于0。它还从0开始随着序列维度的推进而增加。对于嵌入维度中每一对（2*i* 和
    2*i*+1）的位置，我们都有一个互补的正弦和余弦波，如*图 14.13*所示：
- en: '![Figure 14.13 – Positional encoding: sine and cosine terms ](img/B22389_14_13.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.13 – 位置编码：正弦和余弦项](img/B22389_14_13.png)'
- en: 'Figure 14.13: Positional encoding: sine and cosine terms'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.13：位置编码：正弦和余弦项
- en: 'We can see that embedding dimensions **40** and **41** are sine and cosine
    waves of the same frequency, and embedding dimensions **40** and **42** are sine
    waves with a slight increase in frequency. By using the combination of sine and
    cosine waves of varying frequencies, the positional encoding can encode rich positional
    information as a vector. If we plot a heatmap (refer to the color images file:[https://packt.link/gbp/9781835883181](https://packt.link/gbp/9781835883181))
    of the whole positional encoding vector (*Figure 14.14*), we can see how the values
    change and encode the positional information:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，嵌入维度**40**和**41**是具有相同频率的正弦和余弦波，而嵌入维度**40**和**42**是正弦波，频率略有增加。通过使用频率不同的正弦和余弦波组合，位置编码可以将丰富的位置信息编码为一个向量。如果我们绘制整个位置编码向量的热图（参考颜色图像文件：[https://packt.link/gbp/9781835883181](https://packt.link/gbp/9781835883181)），我们可以看到值的变化及其如何编码位置信息：
- en: '![Figure 14.14 – Positional encoding: heatmap of the entire vector ](img/B22389_14_14.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.14 – 位置编码：整个向量的热图](img/B22389_14_14.png)'
- en: 'Figure 14.14: Positional encoding: heatmap of the entire vector'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.14：位置编码：整个向量的热图
- en: Another interesting observation is that the positional encoding quickly shrinks
    to 0/1 as we move forward in the embedding dimension because the term inside the
    sine or cosine functions (angle in radians) quickly becomes zero on account of
    the large denominator. The zoomed plot shows the color differences more clearly.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的观察是，随着我们在嵌入维度中前进，位置编码会迅速收敛到0/1，因为正弦或余弦函数中的项（弧度角度）会由于分母过大而迅速变为零。放大的图表清晰地显示了颜色差异。
- en: Now, for the last component in the Transformer model.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下Transformer模型中的最后一个组件。
- en: Position-wise feed-forward layer
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置-wise前馈层
- en: We have already covered what feed-forward networks are in *Chapter 12*, *Building
    Blocks of Deep Learning for Time Series*. The only thing to be noted here is that
    the position-wise feed-forward layer is when we apply the same feed-forward layer
    in each position, independently. If we have 12 positions (or words), we will have
    a single feed-forward network to process each of these positions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在*第12章*《时间序列深度学习的构建块》中讨论过前馈网络。这里唯一需要注意的是，位置-wise前馈层是指我们在每个位置上独立地应用相同的前馈层。如果我们有12个位置（或单词），那么我们将有一个前馈网络来处理每个位置。
- en: 'Vaswani et al. defined this as a two-layer feed-forward network where the transformations
    were defined so that the input dimensions are expanded to four times the input
    dimension, with a ReLU activation function applied at that stage, and then transformed
    back to the input dimension again. The exact operation can be written as a mathematical
    formula:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Vaswani等人将其定义为一个两层的前馈网络，其中转换操作被定义为将输入维度扩展到四倍的输入维度，应用ReLU激活函数后，再将其转换回原输入维度。具体操作可以写成如下数学公式：
- en: '*FFN*(*x*) = *max*(0, *W*[1]*x* + *b*[1]) *W*[2] + *b*[2]'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*FFN*(*x*) = *max*(0, *W*[1]*x* + *b*[1]) *W*[2] + *b*[2]'
- en: Here, *W*[1] is a matrix of dimensions (*input size*, *4*input size*), *W*[2]
    is a matrix of dimensions (*4*input size, input size*), *b*[1] and *b*[2] are
    the corresponding biases, and *max*(0, *x*) is the standard ReLU operator.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W*[1]是一个维度为（*输入大小*，*4*输入大小）的矩阵，*W*[2]是一个维度为（*4*输入大小，输入大小）的矩阵，*b*[1]和*b*[2]是相应的偏置，*max*(0,
    *x*)是标准的ReLU操作符。
- en: There have been studies where researchers have tried replacing ReLU with other
    activation functions, more specifically **Gated Linear Units** (**GLUs**), which
    have shown promise. Noam Shazeer from Google has a paper on the topic, and if
    you want to know more about these new activation functions, I recommend checking
    out his paper in the *Further reading* section.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些研究尝试将ReLU替换为其他激活函数，特别是**门控线性单元**（**GLUs**），这在实验中显示出了潜力。来自谷歌的Noam Shazeer在此方面有一篇论文，如果你想了解更多关于这些新激活函数的信息，我建议查阅他在*进一步阅读*部分的论文。
- en: Now that we know all the necessary components of a Transformer model, let’s
    see how they are put together.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Transformer模型的所有必要组件，接下来看看它们是如何组合在一起的。
- en: Encoder
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器
- en: 'The vanilla Transformer model is an encoder-decoder model. There are N blocks
    of encoders, and each block contains an MHA layer and a position-wise feed-forward
    layer with residual connections in between (*Figure 14.15*):'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的Transformer模型是一个编码器-解码器模型。模型包含N个编码器块，每个编码器块内含有一个MHA层，并且在其间有一个带残差连接的位置-wise前馈层（*图14.15*）：
- en: '![Figure 14.15 – Transformer model from Attention is All you Need by Vaswani
    et al. ](img/B22389_14_15.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图14.15 – Transformer模型，来自Vaswani等人《Attention is All You Need》](img/B22389_14_15.png)'
- en: 'Figure 14.15: Transformer model from Attention Is All You Need by Vaswani et
    al.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.15：Vaswani等人《Attention Is All You Need》中的Transformer模型
- en: For now, let’s focus on the left side of *Figure 14.15*, which is the encoder.
    The encoder takes in the input embeddings, with the positional encoding vector
    added to it, as the input. The three-pronged arrow that goes into MHA denotes
    the query, key, and value split. The output from the MHA goes into a block named
    *Add and Norm*. Let’s quickly see what that does.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们关注一下*图14.15*的左侧部分，即编码器。编码器接收输入嵌入，并将位置编码向量加到输入中作为输入。进入MHA的三叉箭头表示查询（query）、键（key）和值（value）分割。MHA的输出进入一个名为*Add
    and Norm*的块。让我们快速了解一下这个操作。
- en: There are two key operations that happen here—**residual connections** and **layer
    normalization**.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个关键操作——**残差连接**和**层归一化**。
- en: Residual connections
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 残差连接
- en: Residual connections (or skip connections) are a family of techniques that were
    introduced to DL to make learning deep networks easier. The primary benefit of
    the technique is that it makes the gradient flow through the network better and
    thereby encourages learning in all parts of the network. They incorporate a pass-through
    memory highway in the network. We have already seen one instance where a skip
    connection (although not an apparent one) resolved gradient flow issues—**long
    short-term memory networks** (**LSTMs**). The cell state in the LSTM serves as
    this highway to let gradients flow through the network without getting into vanishing
    gradient issues.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接（或跳跃连接）是一系列引入深度学习的技术，旨在使深度网络的学习变得更加容易。该技术的主要优势在于它改善了网络中的梯度流动，从而促进了网络各部分的学习。它们在网络中引入了一个通过的记忆通道。我们已经看到一个实例，跳跃连接（尽管不是显而易见的）解决了梯度流动问题——**长短时记忆网络**（**LSTM**）。LSTM中的细胞状态作为这个通道，让梯度能够顺利通过网络，避免了梯度消失问题。
- en: 'But nowadays, when we say residual connections, we typically think of *ResNets*,
    which made a splash in the history of DL through a **convolutional neural network**
    (**CNN**) architecture that won major image classification challenges, including
    ImageNet in 2015\. They introduced residual connections to train much deeper architectures
    than those prevalent at the time. The concept is deceptively simple. Let’s visualize
    it:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 但如今，当我们提到残差连接时，我们通常想到的是 *ResNets*，它通过一种**卷积神经网络**（**CNN**）架构，在深度学习历史上掀起了波澜，赢得了多个重要的图像分类挑战赛，包括2015年的ImageNet竞赛。他们引入了残差连接，以训练比当时流行的架构更深的网络。这个概念看似简单，我们来直观地理解它：
- en: '![Figure 14.16 – Residual networks ](img/Image3805.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.16 – 残差网络](img/Image3805.jpg)'
- en: 'Figure 14.16: Residual networks'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.16：残差网络
- en: 'Let’s assume a DL model with two layers with functions, *M*[1] and *M*[2].
    In a regular neural network, the input, *x*, passes through the two layers to
    give us the output, *y*. These two individual functions can be considered as a
    single function that converts *x* to *y*: *y* = *F*(*x*).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含两层函数的深度学习模型，*M*[1]和*M*[2]。在常规的神经网络中，输入 *x* 会通过这两层，从而得到输出 *y*。这两个单独的函数可以看作一个将
    *x* 转换为 *y* 的单一函数：*y* = *F*(*x*)。
- en: In residual networks, we change this paradigm into saying that each individual
    function (or layer) only learns the difference between the input to the function
    and the expected output. That is where the name residual connections came from.
    So, if *h*[1] is the desired output and *x* is the input, then *M*[1](*x*) = *h*[1]
    - *x*. Rewriting that, we get *h*[1] = *M*[1](*x*) + *x*. And this is what is
    most commonly used as residual connections.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在残差网络中，我们将这种范式改变为每个独立的函数（或层）仅学习输入与期望输出之间的差异。这就是残差连接名称的由来。因此，如果 *h*[1] 是期望输出，*x*
    是输入，那么 *M*[1](*x*) = *h*[1] - *x*。重写这一公式，我们得到 *h*[1] = *M*[1](*x*) + *x*。这就是最常用的残差连接。
- en: Among many benefits such as better gradient flows, residual connections also
    make the loss surface smooth (Li et al. 2018, Reference *7*) and more amenable
    to gradient-based optimization. For more details and intuition around residual
    networks, I urge you to check out the blog linked in the *Further reading* section.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接的诸多好处之一是它改善了梯度流动，此外它还使损失面更加平滑（Li等，2018年，参考文献 *7*），更适合基于梯度的优化。关于残差网络的更多细节和直觉，我建议你查看
    *Further reading* 部分中的博客链接。
- en: So, the *Add* in the *Add and Norm* block in the Transformer is actually the
    residual connection.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，Transformer 中的 *Add and Norm* 块中的 *Add* 实际上是残差连接。
- en: Layer normalization
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层归一化
- en: Normalization in **deep neural networks** (**DNNs**) has been an active field
    of research. Among many benefits, normalization leads to faster training, higher
    learning rates, and even a bit of regularization. Batch normalization is the most
    common normalization technique in use, typically in CV, which makes the input
    have approximately zero mean and unit variance by subtracting the input mean in
    the current batch and dividing it by the standard deviation.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度神经网络**（**DNNs**）中的归一化一直是一个活跃的研究领域。在众多好处中，归一化能够加速训练、提高学习速率，甚至起到一定的正则化作用。批归一化是最常见的归一化技术，通常应用于计算机视觉（CV）中，它通过在当前批次中减去输入均值并除以标准差，使得输入数据的均值接近零，方差接近单位。'
- en: 'But in NLP, researchers prefer layer normalization, where the normalization
    is happening in each feature. The difference can be seen in *Figure 14.17*:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 但在自然语言处理（NLP）中，研究人员更倾向于使用层归一化，其中归一化发生在每个特征上。可以在 *图 14.17* 中看到两者的区别：
- en: '![Figure 14.17 – Batch normalization versus layer normalization ](img/B22389_14_17.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.17 – 批量归一化与层归一化](img/B22389_14_17.png)'
- en: 'Figure 14.17: Batch normalization versus layer normalization'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.17：批量归一化与层归一化
- en: This preference for layer normalization emerged empirically, but there has been
    research about the reason for this preference. NLP data usually has a higher variance
    as opposed to CV data, and this variance causes some problems for batch normalization.
    Layer normalization, on the other hand, is immune to this because it doesn’t rely
    on batch-level variance.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化的偏好是通过经验得出的，但已经有研究探讨了这种偏好的原因。与计算机视觉（CV）数据相比，自然语言处理（NLP）数据通常具有更高的方差，而这种方差会导致批量归一化出现一些问题。另一方面，层归一化对此免疫，因为它不依赖于批量级别的方差。
- en: Either way, Vaswani et al. decided to use layer normalization in their *Add
    and Norm* block.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，Vaswani 等人决定在他们的*加法与归一化*（Add and Norm）块中使用层归一化。
- en: Now, we know the *Add and Norm* block is nothing but a residual connection that
    is then passed through a layer normalization. So, we can see that the position-encoded
    inputs are first used in the MHA layer, and the output from the MHA is added with
    the position-encoded inputs again and passed through a layer normalization. Now,
    this output is passed through the position-wise feed-forward network and another
    *Add and Norm* layer, and this becomes one block of the encoder. An important
    point to keep in mind is that the architecture of all the elements in the encoder
    is designed in such a way that the dimension of the input at each position is
    preserved throughout. In other words, if the embedding vector is of dimension
    100, the output from the encoder will also have a dimension of 100\. This is a
    convenient way to make it possible to have residual connections and stack as many
    layers on top of each other as possible. Now, there are multiple such encoder
    blocks stacked on top of each other to form the encoder of the Transformer.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道*加法与归一化*块实际上就是一个残差连接，之后通过层归一化。因此，我们可以看到，位置编码的输入首先在多头自注意力（MHA）层中使用，MHA的输出再次与位置编码的输入相加，并通过层归一化。接下来，这个输出通过位置-wise前馈网络和另一个*加法与归一化*层，这就形成了编码器的一个块。一个重要的点是，编码器中所有元素的架构设计使得每个位置的输入维度在整个过程中得以保持。换句话说，如果嵌入向量的维度为100，那么编码器的输出也将具有100的维度。这是一种便捷的方式，使得能够使用残差连接并尽可能堆叠多个层。现在，有多个这样的编码器块堆叠在一起，形成Transformer的编码器。
- en: Decoder
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器
- en: The decoder block is also very similar to the encoder block, but with one key
    addition. Instead of a single self-attention layer, the decoder block has a self-attention
    layer, which operates on the decoder input, and an encoder-decoder attention layer.
    The encoder-decoder attention layer takes the query from the decoder at each stage
    and the key and values from the top encoder block.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器块与编码器块非常相似，但有一个关键的区别。解码器块不仅包含单一的自注意力层，还包括一个自注意力层，该层作用于解码器输入，并且还有一个编码器-解码器注意力层。编码器-解码器注意力层在每个阶段从解码器获取查询（query），并从上层编码器块获取键（key）和值（value）。
- en: There is something peculiar to the self-attention that is applied in the decoder
    block. Let’s see what that is.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器块中应用的自注意力有一些特别之处。让我们来看看到底是什么。
- en: Masked self-attention
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 掩蔽自注意力
- en: We talked about how the Transformer can process sequences in parallel and be
    computationally efficient. But the decoding paradigm poses another challenge.
    Suppose we have an input sequence, *X* = {*x*[1], *x*[2], …, *x*[n]}, and the
    task is to predict the next step. So, in the decoder, if we give the sequence,
    *X*, because of the parallel-processing architecture, each sequence is processed
    at once using self-attention. And we know self-attention is agnostic to sequence
    order. If left unrestricted, the model will cheat by using information from the
    future timesteps to predict the current timestep. This is where masked attention
    becomes important.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们谈到了Transformer如何并行处理序列并且在计算上具有高效性。但解码的范式提出了另一个挑战。假设我们有一个输入序列，*X* = {*x*[1],
    *x*[2], …, *x*[n]}，任务是预测下一个步骤。所以，在解码器中，如果我们给定序列*X*，由于并行处理架构，每个序列都会通过自注意力一次性处理。而且我们知道自注意力与序列顺序无关。如果不加限制，模型将通过使用未来时间步的信息来预测当前时间步。这就是掩蔽注意力变得重要的地方。
- en: We saw earlier in the *Self-attention* section how to calculate a square matrix
    (if the query and key have the same length) of attention weights, and it is with
    these weights that we combine the information from the value vector. This self-attention
    has no concept of temporality, and all the tokens will attend to all other tokens
    irrespective of their position.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *自注意力* 部分中已经看过如何计算一个方阵（如果查询和键有相同的长度）的注意力权重，正是使用这些权重我们将信息从值向量中进行组合。这种自注意力没有时间性概念，所有的令牌都会关注所有其他令牌，而不管它们的位置。
- en: 'Let’s see *Figure 14.18* to solidify our understanding:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 *图 14.18* 来巩固我们的理解：
- en: '![Figure 14.18 – Masked self-attention ](img/Image3820.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.18 – 掩码自注意力](img/Image3820.jpg)'
- en: 'Figure 14.18: Masked self-attention'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.18：掩码自注意力
- en: We have the sequence, *X* = {*x*[1], *x*[2], …, *x*[5]}, and we are still trying
    to predict one step ahead. So, the expected output from the decoder would be ![](img/B22389_14_018.png).
    When we use self-attention, the attention weights that will be learned will be
    a square matrix of 5 X 5 dimension. But if we look at the upper triangle of the
    square matrix (the part that is shaded in *Figure 14.18*), those combinations
    of tokens violate the temporal sanctity.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有序列，*X* = {*x*[1], *x*[2], …, *x*[5]}，我们仍然尝试预测一步之遥。所以，解码器的期望输出是 ![](img/B22389_14_018.png)。当我们使用自注意力时，学习到的注意力权重将是一个
    5 X 5 的方阵。但是如果我们看方阵的上三角部分（*图 14.18* 中阴影部分），这些令牌组合会违反时间序列的独立性。
- en: We can take care of this simply by adding a pre-generated mask that has zeros
    in all the white cells and *-inf* in all the shaded cells to the generated attention
    energies (the stage before applying *softmax*). This makes sure the attention
    weights for the shaded region will be zero, and this in turn ensures that no future
    information is used while calculating the weighted sum of the value vector.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过简单地添加一个预生成的掩码来解决这个问题，掩码中所有白色单元格为零，所有阴影单元格为 *-inf*，然后将其添加到生成的注意力能量中（即应用
    *softmax* 之前的阶段）。这样可以确保阴影区域的注意力权重为零，从而确保在计算值向量的加权和时不使用未来的信息。
- en: Now, to wrap everything up, the output from the decoder is passed to a standard
    task-specific head to generate the output we desire.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了总结所有内容，解码器的输出会传递给一个标准的任务特定头部，以生成我们期望的输出。
- en: We discussed the Transformer in the context of NLP, but it is a very small leap
    to adapt it to time series data.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在讨论 Transformer 时是在 NLP 的背景下进行的，但将其适配到时间序列数据上是一个非常小的飞跃。
- en: Transformers in time series
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列中的 Transformers
- en: Time series have a lot of similarities with NLP because of the fact that both
    deal with information in sequences and in both cases the order of elements matters.
    In time series, the elements are typically time-ordered data points, while in
    NLP, the elements are tokens (such as words or characters) that form sentences
    or documents. This can be further evidenced by the phenomenon that most of the
    popular techniques that are used in NLP are promptly adapted to a time series
    context. Transformers are no exception to that.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列与 NLP 有很多相似之处，因为两者都涉及到序列中的信息，而且在这两种情况下，元素的顺序都很重要。在时间序列中，元素通常是按时间排序的数据点，而在
    NLP 中，元素是构成句子或文档的令牌（如单词或字符）。这一点可以通过这样一个现象得到进一步验证：大多数流行的 NLP 技术很快就被适配到时间序列的上下文中，Transformers
    也不例外。
- en: Instead of looking at tokens at each position, we have real numbers in each
    position. And instead of talking about input embeddings, we can talk in terms
    of input features. The vector of features at each timestep can be considered the
    equivalent of an embedding vector in NLP. And instead of making causal decoding
    an optional step (in NLP, that really depends on the task at hand), we have a
    strict requirement for causal decoding. There, it is trivial to adapt Transformers
    to time series, although in practice, there are many challenges because in time
    series we typically encounter sequences that are much longer than the ones in
    NLP, and this creates a problem because the complexity of self-attention is scaled
    quadratically with respect to the input sequence length. There have been many
    alternate proposals for self-attention that make it feasible to use it for long
    sequences as well, and we will be covering a few of them in *Chapter 16*, *Specialized
    Deep Learning Architectures for Forecasting*.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再查看每个位置的标记，而是每个位置都有实数。而且，我们不再谈论输入嵌入，而是可以谈论输入特征。每个时间步的特征向量可以视为 NLP 中嵌入向量的等效物。并且，我们对因果解码有严格要求，而在
    NLP 中，因果解码通常是一个可选步骤（这实际上取决于任务）。因此，将 Transformer 适应时间序列是微不足道的，尽管实际上存在许多挑战，因为在时间序列中，我们通常遇到比
    NLP 中更长的序列，这会带来问题，因为自注意力的复杂度随着输入序列长度的增加呈二次方增长。已经有许多替代性的自注意力提案使得在长序列中使用自注意力成为可能，我们将在*第
    16 章*《用于预测的专门化深度学习架构》中介绍其中的一些。
- en: Now, let’s try to put everything we have learned about Transformers into practice.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试将我们学到的关于 Transformer 的知识付诸实践。
- en: Forecasting with Transformers
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Transformer 进行预测
- en: For some continuity, we will use the same household example we were forecasting
    with RNNs and RNNs with attention.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持一致性，我们将使用之前用 RNN 和带注意力的 RNN 进行预测的相同家庭示例。
- en: '**Notebook alert**:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提醒**：'
- en: To follow along with the complete code, use the notebook named `03-Transformers.ipynb`
    in the `Chapter14` folder and the code in the `src` folder.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随完整代码，请使用 `Chapter14` 文件夹中的 `03-Transformers.ipynb` 笔记本，并使用 `src` 文件夹中的代码。
- en: Although we learned about the vanilla Transformer as a model with an encoder-decoder
    architecture, it was really designed for language translation tasks. In language
    translation, the source sequence and target sequence are quite different, and
    therefore the encoder-decoder architecture made sense. But soon after, researchers
    figured out that using the decoder part of the Transformer alone does well. It
    is called a decoder-only Transformer in literature. The naming is a bit confusing
    because if you think about it, the decoder is different from the encoder in two
    ways—masked self-attention and encoder-decoder attention. So, in a decoder-only
    Transformer, how do we make up for the encoder-decoder attention? The short answer
    is that we don’t. The architecture of the decoder-only Transformer resembles the
    encoder block more, but we call it decoder-only because we use masked self-attention
    to make our model respect the temporal sanctity of our sequences.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们学习了 vanilla Transformer 作为一个具有编码器-解码器架构的模型，但它最初是为语言翻译任务设计的。在语言翻译中，源序列和目标序列是完全不同的，因此编码器-解码器架构显得有意义。但很快，研究人员发现，仅使用
    Transformer 的解码器部分也能取得良好的效果。文献中称之为解码器仅 Transformer。这个命名有点令人困惑，因为如果你思考一下，解码器和编码器有两个不同之处——掩码自注意力和编码器-解码器注意力。那么，在解码器仅
    Transformer 中，我们如何弥补编码器-解码器注意力呢？简短的回答是我们不需要。解码器仅 Transformer 的架构更像是编码器块，但我们称其为解码器仅
    Transformer，因为我们使用掩码自注意力来确保模型遵守序列的时间顺序。
- en: 'We are also going to implement a decoder-only Transformer. The first thing
    we need to do is to define a config class, `TransformerConfig`, with the following
    parameters:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将实现一个解码器仅 Transformer。我们需要做的第一件事是定义一个配置类 `TransformerConfig`，并包含以下参数：
- en: '`input_size`: This parameter defines the number of features the Transformer
    is expecting.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_size`：此参数定义了 Transformer 所期望的特征数量。'
- en: '`d_model`: This parameter defines the hidden dimension of the Transformer or
    the dimension over which all the attention calculation and subsequent operations
    happen.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model`：此参数定义了 Transformer 的隐藏维度，或所有注意力计算和后续操作发生的维度。'
- en: '`n_heads`: This parameter defines how many heads we have in the MHA mechanism.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_heads`：此参数定义了 MHA 机制中有多少个头。'
- en: '`n_layers`: This parameter defines how many blocks of encoders we are going
    to stack on top of each other.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_layers`：此参数定义了我们要堆叠在一起的编码器块数量。'
- en: '`ff_multiplier`: This parameter defines the scale of expansion within the position-wise
    feed-forward layers.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ff_multiplier`：此参数定义了位置前馈层内扩展的尺度。'
- en: '`activation`: This parameter lets us define which activation we need to use
    in the position-wise feed-forward layers. It can be either `relu` or `gelu`.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation`：此参数允许我们定义在位置前馈层中使用的激活函数，可以是`relu`或`gelu`。'
- en: '`multi_step_horizon`: This parameter lets us define how many timesteps into
    the future we should be forecasting.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multi_step_horizon`：此参数让我们定义预测未来多少个时间步。'
- en: '`dropout`: This parameter lets us define the magnitude of dropout regularization
    to be applied in the Transformer model.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout`：此参数允许我们定义在Transformer模型中应用的dropout正则化的大小。'
- en: '`learning_rate`: This defines the learning rate of the optimization procedure.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：定义优化过程的学习率。'
- en: '`optimizer_params`, `lr_scheduler`, `lr_scheduler_params`: These are parameters
    that let us tweak the optimization procedure. Let’s not worry about these for
    now because all of them have been set to intelligent defaults.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer_params`、`lr_scheduler`、`lr_scheduler_params`：这些参数允许我们调整优化过程。暂时不需要担心这些，因为它们都已设置为智能默认值。'
- en: Now, we are going to inherit the `BaseModel` class we defined in `src/dl/models.py`
    and define a `TransformerModel` class.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继承我们在`src/dl/models.py`中定义的`BaseModel`类，并定义一个`TransformerModel`类。
- en: The first method we need to implement is `_build_network`. The entire model
    can be found in `src/dl/models.py`, but we will be covering the important aspects
    here as well.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要实现的第一个方法是`_build_network`。整个模型可以在`src/dl/models.py`中找到，但我们也将在这里介绍一些重要的部分。
- en: 'The first module we need to define is a linear projection layer that takes
    in the `input_size` parameter and projects it into `d_model`:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要定义的第一个模块是一个线性投影层，它接受`input_size`参数并将其投影到`d_model`：
- en: '[PRE12]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is an additional step we have introduced to adapt the Transformers to the
    time series forecasting paradigm. In the vanilla Transformer, this is not needed
    because each word is represented by an embedding vector that typically has dimensions
    such as 200 or 500\. But while doing time series forecasting, we might have to
    do the forecasting with just one feature (which is the history), and this seriously
    restricts our ability to provide capacity to the model because, without the projection
    layer, `d_model` can only be equal to `input_size`. Therefore, we have introduced
    a linear projection layer that decouples the number of features available and
    `d_model`.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们为使Transformer适应时间序列预测范式所添加的额外步骤。在传统的Transformer中，这一步并不需要，因为每个词都由一个通常维度为200或500的嵌入向量表示。但是在进行时间序列预测时，我们可能需要仅使用一个特征（即历史数据）进行预测，这大大限制了我们为模型提供能力的方式，因为没有投影层时，`d_model`只能等于`input_size`。因此，我们引入了一个线性投影层，解耦了可用特征的数量和`d_model`。
- en: 'Now, we need to have a module that adds positional encoding. We have packaged
    the same code we saw earlier into a `PyTorch` module and added it to `src/dl/models.py`.
    We just use that module and define our positional encoding operator, like so:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要有一个模块来添加位置编码。我们已将之前看到的代码打包成一个`PyTorch`模块，并将其添加到`src/dl/models.py`中。我们只需使用该模块并定义我们的位置信息操作符，如下所示：
- en: '[PRE13]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We said earlier that we are going to use a decoder-only approach to building
    the model, and for that, we are using the `TransformerEncoderLayer` and `TransformerEncoder`
    modules defined in `PyTorch`. Just keep in mind that when using these layers,
    we will be using masked self-attention, and that makes it a decoder-only Transformer.
    The code is presented here:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前说过，我们将使用仅解码器的方法来构建模型，为此，我们使用了`TransformerEncoderLayer`和`TransformerEncoder`模块，这些模块在`PyTorch`中已定义。只需要记住，当使用这些层时，我们将使用掩蔽自注意力，这使得它成为一个仅解码器的Transformer。代码如下：
- en: '[PRE14]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The last module we need to define is a linear layer that converts the output
    from the Transformer into the number of timesteps we are forecasting:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要定义的最后一个模块是一个线性层，它将Transformer的输出转换为我们预测的时间步数：
- en: '[PRE15]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: That concludes the definition of the model. Now, let’s define a forward pass
    in the `forward` method.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是模型定义的全部内容。接下来，让我们在`forward`方法中定义前向传播。
- en: 'The first step is to generate a mask we need to apply masked self-attention:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是生成我们需要应用掩蔽自注意力的掩码：
- en: '[PRE16]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We define the mask to have the same length as the input sequence. `_generate_square_subsequent_mask`
    is a method we have defined that generates a mask. Assuming the sequence length
    is 5, we can look at the two steps in preparing the mask:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了掩码，使其与输入序列的长度相同。`_generate_square_subsequent_mask`是我们定义的方法，用于生成掩码。假设序列长度为5，我们可以查看准备掩码的两个步骤：
- en: '[PRE17]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`torch.ones(sz,sz)` creates a square matrix filled with ones, and `torch.triu(torch.ones(sz,sz))`
    makes a matrix with a top triangle (including the diagonal) filled with ones and
    the rest filed with zeros. By using an equality operator with one condition and
    transposing it, we get a mask that has `True` in all the bottom triangles, including
    the diagonal, and `False` everywhere else. The output of the previous statement
    will be this:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.ones(sz,sz)`会创建一个全是1的方阵，而`torch.triu(torch.ones(sz,sz))`会生成一个上三角矩阵（包括对角线），其余部分填充为0。通过使用带有一个条件的等式运算符并进行转置，我们可以得到一个掩码，该掩码在所有下三角区域（包括对角线）中为`True`，其他地方为`False`。前面语句的输出将是这样的：'
- en: '[PRE18]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can see that this matrix has `False` at all the places where we need to
    mask attention. Now, all we need to do is to fill all `True` instances with `0`
    and all `False` instances with `-inf`:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这个矩阵在所有需要掩蔽注意力的位置上是`False`。现在，我们只需要将所有`True`实例填充为`0`，将所有`False`实例填充为`-inf`：
- en: '[PRE19]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: These two lines of code are packaged into the `_generate_square_subsequent_mask`
    method, which we can use while training the model.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这两行代码被封装到`_generate_square_subsequent_mask`方法中，我们可以在训练模型时使用它。
- en: 'Now that we have created the mask for masked self-attention, let’s start processing
    the input, `x`:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为掩蔽自注意力创建了掩码，接下来我们开始处理输入`x`：
- en: '[PRE20]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In these four lines of code, we project the input to `d_model` dimensions, add
    positional encoding, pass it through the Transformer model, and lastly, use the
    linear layer to convert the output to the predictions we want.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在这四行代码中，我们将输入投影到`d_model`维度，添加位置编码，通过Transformer模型处理，最后使用线性层将输出转换为我们想要的预测结果。
- en: Now we have `y_hat`, which is the prediction from the model. All we need to
    think of now is how to train this output to be the desired output.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了`y_hat`，它是模型的预测结果。现在我们需要思考的是如何训练这个输出，使其成为期望的输出。
- en: We know that the Transformer model processes all the tokens in one shot, and
    if we have *N* elements in the sequence, we will have *N* predictions as well
    (each prediction corresponding to the next timestep). And if each prediction is
    for the next H timesteps, the shape of `y_hat` would be (*B*, *N*, *H*), where
    *B* is the batch size. There are a few ways we can use this output to compare
    with the target. The most simple and naïve way is to just take the prediction
    from the last position (which will have *H* timesteps) and compare it with `y`
    (which also has *H* timesteps).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道Transformer模型一次性处理所有的tokens，如果序列中有*N*个元素，那么也会有*N*个预测值（每个预测值对应下一个时间步）。如果每个预测值对应接下来的H个时间步，那么`y_hat`的形状将是(*B*,
    *N*, *H*)，其中*B*是批量大小。我们可以通过几种方式使用这个输出与目标进行比较。最简单且最朴素的方法是直接使用最后一个位置的预测（它将有*H*个时间步）并将其与`y`（它也有*H*个时间步）进行比较。
- en: But this is not the most efficient way of using all the information we have,
    is it? We are discarding *N-1* predictions and not giving any signal to the model
    on all those *N-1* predictions. So, while training, it makes sense to use all
    these *N-1* predictions also so that the model has a much richer signal feeding
    back while learning.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不是利用我们所有信息的最有效方式，对吧？我们丢弃了*N-1*个预测值，并且没有给模型提供关于这些*N-1*个预测的任何信号。因此，在训练时，使用这些*N-1*个预测是有意义的，这样模型在学习时可以得到更加丰富的反馈信号。
- en: 'We can do that by using the original input sequence, `x`, but offsetting it
    by one. When *H=1*, we can think of this as a simple task where each position’s
    prediction is compared with the target for the next position (one step ahead).
    We can easily accomplish this by concatenating `x[:,1:,:]` (the input sequence
    offset by 1) with `y` (the original target) and treating this as the target. But
    when *H* > 1, this becomes slightly complicated, but we can still do it by using
    a helpful function from `PyTorch` called `unfold`:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用原始输入序列`x`，但是将其偏移一个位置来实现。当*H=1*时，我们可以将其视为一个简单的任务，其中每个位置的预测值与下一个位置（即前进一步）的目标进行比较。我们可以通过将`x[:,1:,:]`（输入序列偏移1）与`y`（原始目标）连接，并将其视为目标来轻松完成。但当*H*
    > 1时，这变得稍微复杂，但我们仍然可以通过使用`PyTorch`中的一个有用函数`unfold`来做到这一点：
- en: '[PRE21]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We first concatenate the input sequence (offset by one) with `y` and then use
    `unfold` to create sliding windows of *size* = *H*. This gives us a target of
    the same shape, (*B*, *N*, *H*).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将输入序列（偏移一个位置）与`y`连接起来，然后使用`unfold`创建*大小* = *H*的滑动窗口。这样我们就得到了一个形状相同的目标（*B*，*N*，*H*）。
- en: 'But during inference (when we are predicting using a trained model), we do
    not need the output of all the other positions, and hence we discard them, as
    shown here:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在推理过程中（当我们使用训练好的模型进行预测时），我们不需要所有其他位置的输出，因此我们会将它们丢弃，如下所示：
- en: '[PRE22]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Our `BaseModel` class that we defined also lets us define a slightly different
    prediction step by using the `predict` method. You can look over the complete
    model in `src/dl/models.py` once again to solidify your understanding now.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的`BaseModel`类还允许我们通过使用`predict`方法来定义一个稍有不同的预测步骤。你可以再次查看`src/dl/models.py`中的完整模型，以巩固你的理解。
- en: 'Now that we have defined the model, we can use the same framework we have been
    using to train `TransformerModel`. The full code is available in the notebook,
    but we will just look at a summary table with the results:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了模型，可以使用我们一直在使用的相同框架来训练`TransformerModel`。完整的代码可以在笔记本中找到，但我们将只查看一个总结表格，展示结果：
- en: '![Figure 14.19 – Metrics for Transformer model on MAC000193 household ](img/B22389_14_19.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![图14.19 – Transformer模型在MAC000193家庭上的度量](img/B22389_14_19.png)'
- en: 'Figure 14.19: Metrics for Transformer model on MAC000193 household'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.19：Transformer模型在MAC000193家庭上的度量
- en: We can see that the model is not doing as well as its RNN cousins. There could
    be many reasons for this, but the most probable one is that Transformers are really
    data-hungry. Transformers have far fewer inductive biases and therefore only shine
    where there is lots of data available to learn from. When forecasting just one
    household alone, our model has access to far less data and may not work very well.
    This is true, to an extent, for all the DL models we have seen so far. In *Chapter
    10*, *Global Forecasting Models*, we talked about how we can train a single model
    for multiple households together, but that discussion was limited to classical
    ML models. DL is also perfectly capable of global forecasting models and that
    is exactly what we will be talking about in the next chapter—*Chapter 15*, *Strategies
    for Global Deep Learning Forecasting Models*.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型的表现不如其RNN同行。造成这种情况的原因可能有很多，但最可能的原因是Transformers非常依赖数据。Transformers的归纳偏差要少得多，因此只有在有大量数据可供学习时才能发挥其优势。当仅对一个家庭进行预测时，我们的模型可以访问的数据非常有限，可能效果不好。到目前为止，这对于我们看到的所有深度学习模型都在一定程度上是成立的。在*第10章*，*全球预测模型*中，我们讨论了如何训练一个可以同时处理多个家庭的模型，但那个讨论仅限于经典的机器学习模型。深度学习同样完全能够应对全球预测模型，这正是我们在下一章——*第15章*，*全球深度学习预测模型的策略*中要讨论的内容。
- en: For now, congratulations on getting through another concept-heavy and information-packed
    chapter. The concept of attention, which has taken the field by storm, should
    be clearer in your mind now than when you started the chapter. I urge you to take
    a second stab at the chapter, read through the *Further reading* section, and
    do some of your own research if it’s not clear because the future chapters assume
    you understand this.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，恭喜你完成了又一章充满概念和信息的章节。注意力机制这一席卷领域的概念，现在应该比开始时更清晰了。我建议你再花点时间重新阅读这一章，通读*进一步阅读*部分，如果有不清楚的地方，可以做一些自己的研究，因为未来的章节假设你理解这一内容。
- en: Summary
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have been storming through the world of DL in the last few chapters. We started
    off with the basic premise of DL, what it is, and why it became so popular. Then,
    we saw a few common building blocks that are typically used in time series forecasting
    and got our hands dirty learning how we can put what we have learned into practice
    using PyTorch. Although we talked about RNNs, LSTMs, GRUs, and so on, we purposefully
    left out attention and Transformers because they deserved a separate chapter.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们快速穿越了深度学习的世界。我们从深度学习的基本前提开始，了解了它是什么，为什么它变得如此流行。接着，我们看到了时间序列预测中常用的一些基本构件，并亲自实践了如何使用PyTorch将所学知识付诸实践。虽然我们讨论了RNN、LSTM、GRU等，但我们有意将注意力机制和Transformers留给了独立的章节。
- en: We started the chapter by learning about the generalized attention model, helping
    you put a framework around all the different schemes of attention out there, and
    then went into detail on a few common attention schemes, such as scaled dot product,
    additive, and general attention. Right after incorporating attention into the
    Seq2Seq models we were playing with in *Chapter 12*, *Building Blocks of Deep
    Learning for Time Series*, we started with the Transformer.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们学习了广义注意力模型，帮助你将所有不同的注意力方案框架化，然后详细讨论了几种常见的注意力方案，如缩放点积、加性和一般性注意力。在将注意力机制融入我们在
    *第12章*，*时间序列深度学习构建模块* 中使用的 Seq2Seq 模型后，我们开始研究 Transformer。
- en: We examined all the building blocks and architecture decisions involved in the
    original Transformer from the point of view of NLP, and after understanding the
    architecture, we adapted it to a time-series setting.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从自然语言处理的角度审视了原始 Transformer 模型中所有的构建模块和架构决策，并在理解了架构之后，将其适配到时间序列设置中。
- en: And finally, we capped it off by training a Transformer model for forecasting
    on a sample household. And now, by finishing this chapter, we have all the basic
    ingredients to really start using DL for time series forecasting.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过训练一个 Transformer 模型来对一个样本家庭进行预测，从而为本章画上了圆满的句号。现在，通过完成这一章，我们已经掌握了所有基本的要素，可以真正开始使用深度学习进行时间序列预测。
- en: In the next chapter, we are going to elevate what we have been doing and move
    on to the global forecasting model paradigm.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将提升我们一直在做的工作，并转向全球预测模型范式。
- en: References
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Following is the list of the references used in this chapter:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章中使用的参考文献列表：
- en: Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio (2015). *Neural Machine Translation
    by Jointly Learning to Align and Translate*. In *3rd International Conference
    on Learning Representations*. [https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dzmitry Bahdanau, KyungHyun Cho, 和 Yoshua Bengio (2015). *通过联合学习对齐与翻译的神经机器翻译*。收录于
    *第三届国际学习表征会议*。[https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)
- en: Thang Luong, Hieu Pham, and Christopher D. Manning (2015). *Effective Approaches
    to Attention-based Neural Machine Translation*. In *Proceedings of the 2015 Conference
    on Empirical Methods in Natural Language Processing*. [https://aclanthology.org/D15-1166/](https://aclanthology.org/D15-1166/)
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Thang Luong, Hieu Pham, 和 Christopher D. Manning (2015). *基于注意力的神经机器翻译的有效方法*。收录于
    *2015年自然语言处理经验方法会议*。[https://aclanthology.org/D15-1166/](https://aclanthology.org/D15-1166/)
- en: 'André F. T. Martins, Ramón Fernandez Astudillo (2016). *From Softmax to Sparsemax:
    A Sparse Model of Attention and Multi-Label Classification*. In *Proceedings of
    the 33rd International Conference on Machine Learning*. [http://proceedings.mlr.press/v48/martins16.html](http://proceedings.mlr.press/v48/martins16.html)'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: André F. T. Martins, Ramón Fernandez Astudillo (2016). *从 Softmax 到 Sparsemax：一种稀疏的注意力模型及多标签分类*。收录于
    *第33届国际机器学习会议论文集*。[http://proceedings.mlr.press/v48/martins16.html](http://proceedings.mlr.press/v48/martins16.html)
- en: Ben Peters, Vlad Niculae, André F. T. Martins (2019). *Sparse Sequence-to-Sequence
    Models*. In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*. [https://aclanthology.org/P19-1146/](https://aclanthology.org/P19-1146/)
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ben Peters, Vlad Niculae, André F. T. Martins (2019). *稀疏序列到序列模型*。收录于 *第57届计算语言学协会年会论文集*。[https://aclanthology.org/P19-1146/](https://aclanthology.org/P19-1146/)
- en: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
    N. Gomez, Lukasz Kaiser, and Illia Polosukhin (2017). *Attention is All you Need*.
    In *Advances in Neural Information Processing Systems*. [https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
    N. Gomez, Lukasz Kaiser, 和 Illia Polosukhin (2017). *注意力即你所需的一切*。收录于 *神经信息处理系统进展*。[https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
- en: 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova (2019). *BERT:
    Pre-training of Deep Bidirectional Transformers for Language Understanding*. In
    *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*. [https://aclanthology.org/N19-1423/](https://aclanthology.org/N19-1423/)'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova（2019）。*BERT：深度双向
    Transformer 的预训练用于语言理解*。发表于 *2019 年北美计算语言学协会年会论文集：人类语言技术，第1卷（长篇和短篇论文）*。[https://aclanthology.org/N19-1423/](https://aclanthology.org/N19-1423/)
- en: Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein (2018).
    *Visualizing the Loss Landscape of Neural Nets*. In *Advances in Neural Information
    Processing Systems*. [https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf)
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, 和 Tom Goldstein（2018）。*可视化神经网络的损失景观*。发表于
    *神经信息处理系统进展*。[https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf)
- en: Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and Rohan Ramanath (2021). *An
    Attentive Survey of Attention Models*. *ACM Trans. Intell. Syst. Technol. 12,
    5, Article 53 (October 2021)*. [https://doi.org/10.1145/3465055](https://doi.org/10.1145/3465055)
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sneha Chaudhari, Varun Mithal, Gungor Polatkan 和 Rohan Ramanath（2021）。*注意力模型的细致调查*。*ACM
    智能系统技术期刊，12卷，第5期，第53号文章（2021年10月）*。[https://doi.org/10.1145/3465055](https://doi.org/10.1145/3465055)
- en: Further reading
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Here are a few resources for further reading:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些进一步阅读的资源：
- en: '*The Illustrated Transformer* by *Jay Alammar*: [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图解 Transformer* 作者：*Jay Alammar*：[https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)'
- en: '*Transformer Networks: A mathematical explanation why scaling the dot products
    leads to more stable gradients*: [https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500](https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Transformer 网络：一个数学解释，为什么缩放点积会导致更稳定的梯度*：[https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500](https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500)'
- en: '*Why is Bahdanau’s attention sometimes called concat attention?*: [https://stats.stackexchange.com/a/524729](https://stats.stackexchange.com/a/524729)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为什么 Bahdanau 的注意力有时被称为拼接注意力？*：[https://stats.stackexchange.com/a/524729](https://stats.stackexchange.com/a/524729)'
- en: '*Noam Shazeer* (2020). *GLU Variants Improve Transformer.* arXiv preprint:
    *Arxiv-2002.05202*. [https://arxiv.org/abs/2002.05202](https://arxiv.org/abs/2002.05202)'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Noam Shazeer*（2020）。*GLU 变种改进 Transformer*。arXiv 预印本：*Arxiv-2002.05202*。[https://arxiv.org/abs/2002.05202](https://arxiv.org/abs/2002.05202)'
- en: '*What is Residual Connection?* by *Wanshun Wong*: [https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55](https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55)'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*什么是残差连接？* 作者：*Wanshun Wong*：[https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55](https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55)'
- en: '*Attn: Illustrated Attention* by *Raimi Karim*: [https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Attn: 图解注意力* 作者：*Raimi Karim*：[https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)'
- en: Join our community on Discord
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with authors and other readers:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者讨论：
- en: '[https://packt.link/mts](https://packt.link/mts)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mts](https://packt.link/mts)'
- en: '![](img/QR_Code15080603222089750.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code15080603222089750.png)'
