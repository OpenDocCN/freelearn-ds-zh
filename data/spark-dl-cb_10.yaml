- en: Face Recognition Using Deep Convolutional Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度卷积网络进行人脸识别
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下配方：
- en: Downloading and loading the MIT-CBCL dataset into the memory
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载并将MIT-CBCL数据集加载到内存中
- en: Plotting and visualizing images from the directory
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从目录中绘制和可视化图像
- en: Preprocessing images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像预处理
- en: Model building, training, and analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型构建、训练和分析
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In today's world, the need to maintain the security of information is becoming
    increasingly important, as well as increasingly difficult. There are various methods
    by which this security can be enforced (passwords, fingerprint IDs, PIN numbers,
    and so on). However, when it comes to ease of use, accuracy, and low intrusiveness,
    face recognition algorithms have been doing very well. With the availability of
    high-speed computing and the evolution of deep convolutional networks, it has
    been made possible to further increase the robustness of these algorithms. They
    have gotten so advanced that they are now being used as the primary security feature
    in many electronic devices (for example, iPhoneX) and even banking applications.
    The goal of this chapter is to develop a robust, pose-invariant face recognition
    algorithm for use in security systems. For the purposes of this chapter, we will
    be using the openly available `MIT-CBCL` dataset of face images of 10 different
    subjects.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今世界，维护信息安全的需求变得越来越重要，同时也变得越来越困难。有各种方法可以强制执行此安全性（密码、指纹ID、PIN码等）。然而，就易用性、准确性和低侵入性而言，人脸识别算法一直表现得非常出色。随着高速计算的可用性和深度卷积网络的发展，进一步增加了这些算法的稳健性成为可能。它们已经变得如此先进，以至于现在它们被用作许多电子设备（例如iPhoneX）甚至银行应用程序中的主要安全功能。本章的目标是开发一个稳健的、姿势不变的人脸识别算法，用于安全系统。为了本章的目的，我们将使用公开可用的`MIT-CBCL`数据集，其中包含10个不同主题的人脸图像。
- en: Downloading and loading the MIT-CBCL dataset into the memory
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载并将MIT-CBCL数据集加载到内存中
- en: In this recipe, we will understand how to download the MIT-CBCL dataset and
    load it into the memory.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将了解如何下载MIT-CBCL数据集并将其加载到内存中。
- en: With a predicted worth of $15 billion by 2025, the biometrics industry is poised
    to grow like never before. Some of the examples of physiological characteristics
    used for biometric authentication include fingerprints, DNA, face, retina or ear
    features, and voice. While technologies such as DNA authentication and fingerprints
    are quite advanced, face recognition brings its own advantages to the table.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到2025年，生物识别行业的预测价值将达到150亿美元，这意味着它将前所未有地增长。用于生物识别认证的一些生理特征的例子包括指纹、DNA、面部、视网膜或耳朵特征和声音。虽然DNA认证和指纹等技术相当先进，但人脸识别也带来了自己的优势。
- en: Ease of use and robustness due to recent developments in deep learning models
    are some of the driving factors behind face recognition algorithms gaining so
    much popularity.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习模型的最新发展，易用性和稳健性是人脸识别算法如此受欢迎的驱动因素之一。
- en: Getting ready
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The following key points need to be considered for this recipe:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，需要考虑以下关键点：
- en: The `MIT-CBCL` dataset is composed of 3,240 images (324 images per subject).
    In our model, we will make arrangements to augment the data in order to increase
    model robustness. We will employ techniques such as shifting the subject, rotation,
    zooming, and shearing of the subject to obtain this augmented data.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MIT-CBCL`数据集由3,240张图像组成（每个主题324张图像）。在我们的模型中，我们将安排增加数据以增加模型的稳健性。我们将采用诸如移动主题、旋转、缩放和剪切主题等技术来获得这些增强的数据。'
- en: We will use 20% of the dataset to test our model (648 images) by randomly selecting
    these images from the dataset. Similarly, we randomly select 80% of the images
    in the dataset and use this as our training dataset (2,592 images).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用数据集的20%（648张图像）来测试我们的模型，通过从数据集中随机选择这些图像。同样，我们随机选择数据集中80%的图像，并将其用作我们的训练数据集（2,592张图像）。
- en: The biggest challenge is cropping the images to the exact same size so that
    they can be fed into the neural network.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大的挑战是裁剪图像到完全相同的大小，以便它们可以被馈送到神经网络中。
- en: It is a known fact that it is much easier to design a network when all the input
    images are of the same size. However, since some of the subjects in these images
    have a side profile or rotated/tilted profiles, we have to adapt our network to
    take input images of different sizes.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 众所周知，当所有输入图像的大小相同时，设计网络要容易得多。然而，由于这些图像中的一些主题具有侧面轮廓或旋转/倾斜轮廓，我们必须使我们的网络适应接受不同大小的输入图像。
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作方法...
- en: The steps are as follows.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下。
- en: 'Download the `MIT-CBCL` dataset by visiting the FACE RECOGNITION HOMEPAGE,
    which contains a number of databases for face recognition experiments. The link,
    as well as a screenshot of the homepage, is provided as follows:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过访问人脸识别主页下载`MIT-CBCL`数据集，该主页包含用于人脸识别实验的多个数据库。链接以及主页的屏幕截图如下所示：
- en: '[http://www.face-rec.org/databases/](http://www.face-rec.org/databases/):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.face-rec.org/databases/](http://www.face-rec.org/databases/)'
- en: '![](img/00289.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00289.jpeg)'
- en: 'Navigate down to the link that is named MIT-CBCL Face Recognition Database
    and click on it, as shown in the following screenshot:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向下导航到名为MIT-CBCL人脸识别数据库的链接，并单击它，如下面的屏幕截图所示：
- en: '![](img/00290.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00290.jpeg)'
- en: Once you have clicked on it, it will take you to a license page on which you
    are required to accept the license agreement and proceed to the download page.
    Once on the download page, click on `download now`. This downloads a zip file
    of about 116 MB. Go ahead and extract the contents into the working directory.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你点击它，它会带你到一个许可页面，在这个页面上你需要接受许可协议并转到下载页面。一旦在下载页面上，点击`立即下载`。这将下载一个大约116MB的zip文件。继续提取内容到工作目录中。
- en: How it works...
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The functionality is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 功能如下：
- en: The license agreement requires the appropriate citation for the use of the database
    in any projects. This database was developed by the research team from the Massachusetts
    Institute of Technology.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 许可协议要求在任何项目中使用数据库时进行适当引用。该数据库是由麻省理工学院的研究团队开发的。
- en: Credit is hereby given to the Massachusetts Institute of Technology and to the
    center for biological and computational learning for providing the database of
    facial images. The license also requires the mentioning of the paper titled *Component-based
    Face Recognition with 3D Morphable Models, First IEEE Workshop on Face Processing
    in Video,* Washington, D.C., 2004, B. Weyrauch, J. Huang, B. Heisele, and V. Blanz.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特此感谢麻省理工学院和生物计算学习中心提供面部图像数据库。许可证还要求提及题为*Component-based Face Recognition with
    3D Morphable Models, First IEEE Workshop on Face Processing in Video,* Washington,
    D.C., 2004, B. Weyrauch, J. Huang, B. Heisele, and V. Blanz的论文。
- en: 'The following screenshot describes the license agreement as well as the link
    to download the dataset:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下截图描述了许可协议以及下载数据集的链接：
- en: '![](img/00291.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00291.jpeg)'
- en: Face Recognition Database Homepage
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 面部识别数据库主页
- en: Once the dataset is downloaded and extracted, you will see a folder titled MIT-CBCL-facerec-database.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集下载并提取后，您将看到一个名为MIT-CBCL-facerec-database的文件夹。
- en: 'For the purposes of this chapter, we will only be using the images in the **`training-synthetic`**
    folder, which contains all 3,240 images, as shown in the following screenshot:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为本章的目的，我们将仅使用**`training-synthetic`**文件夹中的图像，该文件夹包含所有3,240张图像，如下截图所示：
- en: '![](img/00292.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00292.jpeg)'
- en: There's more...
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'For this chapter, you will require the following libraries to be imported by
    Python:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您将需要Python导入以下库：
- en: '`os`'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`os`'
- en: '`matplotlib`'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`'
- en: '`numpy`'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`keras`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keras`'
- en: '`TensorFlow`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorFlow`'
- en: The following section of the chapter will deal with importing the necessary
    libraries and preprocessing the images before building the neural network model
    and loading them into it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '本章的以下部分将涉及导入必要的库和预处理图像，然后构建神经网络模型并将其加载到其中。 '
- en: See also
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'For complete information on the packages used in this chapter, visit the following
    links:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本章中使用的软件包的完整信息，请访问以下链接：
- en: '[https://matplotlib.org/](https://matplotlib.org/)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://matplotlib.org/](https://matplotlib.org/)'
- en: '[https://docs.python.org/2/library/os.html](https://docs.python.org/2/library/os.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/2/library/os.html](https://docs.python.org/2/library/os.html)'
- en: '[https://www.tensorflow.org/get_started/](https://www.tensorflow.org/get_started/)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/get_started/](https://www.tensorflow.org/get_started/)'
- en: '[https://keras.io/layers/about-keras-layers/](https://keras.io/layers/about-keras-layers/)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/layers/about-keras-layers/](https://keras.io/layers/about-keras-layers/)'
- en: '[https://docs.scipy.org/doc/numpy-1.9.1/reference/](https://docs.scipy.org/doc/numpy-1.9.1/reference/)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.scipy.org/doc/numpy-1.9.1/reference/](https://docs.scipy.org/doc/numpy-1.9.1/reference/)'
- en: Plotting and visualizing images from the directory
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制和可视化目录中的图像
- en: This section will describe how to read and visualize the downloaded images before
    they are preprocessed and fed into the neural network for training. This is an
    important step in this chapter because the images need to be visualized to get
    a better understanding of the image sizes so they can be accurately cropped to
    omit the background and preserve only the necessary facial features.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述如何在对图像进行预处理并输入到神经网络进行训练之前，如何读取和可视化下载的图像。这是本章中的重要步骤，因为需要可视化图像以更好地了解图像尺寸，以便可以准确裁剪以省略背景并仅保留必要的面部特征。
- en: Getting ready
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Before beginning, complete the initial setup of importing the necessary libraries
    and functions as well as setting the path of the working directory.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，完成导入必要库和函数以及设置工作目录路径的初始设置。
- en: How to do it...
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps are as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Download the necessary libraries using the following lines of code. The output
    must result in a line that says `Using TensorFlow backend`, as shown in the screenshot
    that follows:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码行下载必要的库。输出必须产生一行，显示`Using TensorFlow backend`，如下截图所示：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The importing of the libraries is as shown:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库如下所示：
- en: '![](img/00293.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00293.jpeg)'
- en: 'Print and set the current working directory as shown in the following screenshot.
    In our case, the desktop was set as the working directory:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下截图中所示的方式打印并设置当前工作目录。在我们的案例中，桌面被设置为工作目录：
- en: '![](img/00294.jpeg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00294.jpeg)'
- en: 'Read all the images directly from the folder by using the commands illustrated
    in the following screenshot:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用以下截图中说明的命令直接从文件夹中读取所有图像：
- en: '![](img/00295.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00295.jpeg)'
- en: 'Print a few random images from the dataset using the `plt.imshow (images[])`
    command, as shown in the following screenshots, to get a better idea of the face
    profiles in the images. This will also give an idea of the size of the image,
    which will be required at a later stage:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plt.imshow(images[])`命令从数据集中打印一些随机图像，如下截图所示，以更好地了解图像中的面部轮廓。这也将给出图像的大小的概念，这将在后期需要：
- en: '![](img/00296.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00296.jpeg)'
- en: Shown here are the images of different test subjects from the first image.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里显示了来自第一张图像的不同测试对象的图像。
- en: '![](img/00297.jpeg)![](img/00298.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00297.jpeg)![](img/00298.jpeg)'
- en: '![](img/00299.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00299.jpeg)'
- en: How it works...
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The functionality is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 功能如下：
- en: The `mypath` variable sets the path to read all the files from. The `training-synthetic`
    folder is specified in this step, as only the files in this folder are going to
    be used for this chapter.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`mypath`变量设置要从中读取所有文件的路径。在此步骤中指定了`training-synthetic`文件夹，因为本章仅使用该文件夹中的文件。'
- en: The `onlyfiles` variable is used in order to count all the files under the folder
    whose path is provided in the previous step by looping through all the files contained
    in the folder. This will be required in the next step for reading and storing
    the images.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`onlyfiles`变量用于通过循环遍历文件夹中包含的所有文件来计算文件夹中的所有文件的数量。这将在下一步中用于读取和存储图像。'
- en: The `images` variable is used to create an empty array of size 3,240 in order
    to store the images, which are all 200 x 200-pixels.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`images`变量用于创建一个大小为3,240的空数组，以存储所有尺寸为200 x 200像素的图像。'
- en: Next, by looping through all the files using the `onlyfiles` variable as an
    argument in the for loop, each image contained in the folder is read and stored
    into the previously defined `images` array using the `matplotlib.image` function.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过在for循环中使用`onlyfiles`变量作为参数来循环遍历所有文件，将文件夹中包含的每个图像读取并存储到先前定义的`images`数组中，使用`matplotlib.image`函数。
- en: Finally, on printing randomly chosen images by specifying different indices
    of the images you will notice that each image is a 200 x 200-pixel array and each
    subject may either be facing forward or rotated between zero and fifteen degrees
    on either side.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过指定不同索引的图像来打印随机选择的图像，您将注意到每个图像都是一个200 x 200像素的数组，每个主题可能是面向前方，也可能在两侧之间旋转零至十五度。
- en: There's more...
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The following points are of note:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几点值得注意：
- en: An interesting feature of this database is that the fourth digit of each filename
    describes which subject is in the respective image.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该数据库的一个有趣特点是，每个文件名的第四个数字描述了相应图像中的主题是谁。
- en: The names of the images are unique in the sense that the fourth digit represents
    the individual in the respective image. Two examples of image names are `0001_-4_0_0_60_45_1.pgm` and
    `0006_-24_0_0_0_75_15_1.pgm`. One can easily understand that the fourth digits
    represent the second and seventh individual respectively.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像的名称在某种意义上是唯一的，第四个数字代表了相应图像中的个体。图像名称的两个示例是`0001_-4_0_0_60_45_1.pgm`和`0006_-24_0_0_0_75_15_1.pgm`。可以很容易地理解，第四个数字分别代表了第二个和第七个个体。
- en: We will need to store this information for later use while making predictions.
    This will help the neural network during training by knowing what subject's facial
    features it is learning.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要存储这些信息以备将来在进行预测时使用。这将有助于神经网络在训练过程中了解它正在学习哪个主题的面部特征。
- en: 'The filenames of each image can be read into an array, and each of the ten
    subjects can be segregated by using the following lines of code:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过以下代码将每个图像的文件名读入数组，并使用以下代码将十个主题中的每一个分隔开：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding code will initialize an empty one-dimensional `numpy` array of
    size 3,240 (the number of images in the `training-synthetic` folder) and store
    the relevant subjects in different arrays by looping through the whole set of
    files.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述代码将初始化一个大小为3,240的空的一维`numpy`数组（`training-synthetic`文件夹中的图像数量），并通过循环遍历整个文件集，将相关主题存储在不同的数组中。
- en: The `if` statements are basically checking what the fourth digit is under each
    filename and storing that digit in the initialized `numpy` array.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`if`语句基本上是在检查每个文件名下的第四个数字，并将该数字存储在初始化的`numpy`数组中。'
- en: 'The output in the iPython notebook for the same is shown in the following screenshot:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在iPython笔记本中的输出如下截图所示：
- en: '![](img/00300.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00300.jpeg)'
- en: See also
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'The following blog describes a method of cropping images in Python and can
    be used for image preprocessing which will be required in the following section:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下博客描述了Python中裁剪图像的方法，并可用于图像预处理，这将在下一节中需要：
- en: '[https://www.blog.pythonlibrary.org/2017/10/03/how-to-crop-a-photo-with-python/](https://www.blog.pythonlibrary.org/2017/10/03/how-to-crop-a-photo-with-python/)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.blog.pythonlibrary.org/2017/10/03/how-to-crop-a-photo-with-python/](https://www.blog.pythonlibrary.org/2017/10/03/how-to-crop-a-photo-with-python/)'
- en: 'More information about the Adam Optimizer and its use cases can be found by
    visiting the following links:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Adam Optimizer及其用例的更多信息，请访问以下链接：
- en: '[https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)'
- en: '[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)'
- en: '[https://www.coursera.org/lecture/deep-neural-network/adam-optimization-algorithm-w9VCZ](https://www.coursera.org/lecture/deep-neural-network/adam-optimization-algorithm-w9VCZ)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.coursera.org/lecture/deep-neural-network/adam-optimization-algorithm-w9VCZ](https://www.coursera.org/lecture/deep-neural-network/adam-optimization-algorithm-w9VCZ)'
- en: Preprocessing images
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像预处理
- en: In the previous section, you may have noticed how all the images are not a front
    view of the face profiles, and that there are also slightly rotated side profiles.
    You may also have noticed some unnecessary background areas in each image that
    needs to be omitted. This section will describe how to preprocess and handle the
    images so that they are ready to be fed into the network for training.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，您可能已经注意到所有图像都不是脸部正面视图，还有些略微旋转的侧面轮廓。您可能还注意到每个图像中都有一些不必要的背景区域需要去除。本节将描述如何预处理和处理图像，使其准备好被馈送到网络进行训练。
- en: Getting ready
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Consider the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下内容：
- en: A lot of algorithms are devised to crop the significant part of an image; for
    example, SIFT, LBP, Haar-cascade filter, and so on.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多算法被设计用来裁剪图像的重要部分；例如SIFT、LBP、Haar-cascade滤波器等。
- en: We will, however, tackle this problem with a very simplistic naïve code to
    crop the facial portion from the image. This is one of the novelties of this algorithm.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，我们将用一个非常简单的天真代码来解决这个问题，从图像中裁剪出面部部分。这是该算法的一个新颖之处。
- en: We have found that the pixel intensity of the unnecessary background part is
    28.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们发现不必要的背景部分的像素强度为28。
- en: Remember that each image is a three-channel matrix of 200 x 200-pixels. This
    means that every image contains three matrices or Tensors of red, green, and blue
    pixels with an intensity ranging from 0 to 255.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请记住，每个图像都是一个三通道的200 x 200像素矩阵。这意味着每个图像包含三个矩阵或张量，红色、绿色和蓝色像素的强度范围从0到255。
- en: Therefore, we will discard any row or column of the images that contain only
    28s as the pixel intensities.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，我们将丢弃图像中仅包含像素强度为28的行或列。
- en: We will also make sure that all the images have the same pixel size after the
    cropping action to achieve the highest parallelizability of the convolutional
    neural network.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将确保所有图像在裁剪操作后具有相同的像素大小，以实现卷积神经网络的最高并行性。
- en: How to do it...
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤如下：
- en: 'The steps are as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Define the `crop()` function to crop images to obtain only the significant
    part, as shown in the following lines of code:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`crop()`函数以裁剪图像，仅获取重要部分，如下代码所示：
- en: '[PRE2]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Use the following lines of code to loop through every image in the folder and
    crop it using the preceding defined function:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码循环遍历文件夹中的每个图像并使用前面定义的函数进行裁剪：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, randomly choose an image and print it to check that it has been cropped
    from a 200 x 200 sized image to a different size. We have chosen image 23 in our
    case. This can be done using the following lines of code:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，随机选择一幅图像并打印它，以检查它是否已从200 x 200大小的图像裁剪到不同的大小。在我们的案例中，我们选择了图像23。可以使用以下代码完成：
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, split the data into a test and train set using `80%` of the images in
    the folder as the training set and the remaining `20% `as the test set. This can
    be done with the following commands:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用文件夹中`80%`的图像作为训练集，剩余的`20%`作为测试集，将数据分割为测试集和训练集。可以使用以下命令完成：
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once the data has finished splitting, segregate the training and test images
    using the following commands:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据完成拆分，使用以下命令将训练和测试图像分开：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, reshape all the cropped images into sizes of 128 x 150, since this is
    the size that is to be fed into the neural network. This can be done using the
    following commands:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将所有裁剪后的图像重塑为128 x 150的大小，因为这是要馈送到神经网络中的大小。可以使用以下命令完成：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once the data is done reshaping, convert it into `float32` type, which will
    make it easier to handle in the next step when it is normalized. Converting from
    int to float32 can be done using the following commands:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据完成重塑，将其转换为`float32`类型，这将使其在下一步中更容易处理。可以使用以下命令从int转换为float32：
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After reshaping and converting the data into the float32 type, it has to be
    normalized in order to adjust all the values to a similar scale. This is an important
    step in preventing data redundancy. Perform normalization using the following
    commands:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在重塑和将数据转换为float32类型后，必须对其进行归一化，以调整所有值到相似的范围。这是防止数据冗余的重要步骤。使用以下命令执行归一化：
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The final step is to convert the reshaped, normalized images into vectors,
    as this is the only form of input the neural network understands. Convert the
    images into vectors using the following commands:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是将重塑、归一化的图像转换为向量，因为这是神经网络理解的唯一输入形式。使用以下命令将图像转换为向量：
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How it works...
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理如下：
- en: 'The functionality is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 功能如下：
- en: 'The `crop()` function executes the following tasks:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`crop()`函数执行以下任务：'
- en: Multiplies all pixels with an intensity of 28 with a numpy array of 1s and stores
    in variable `a`.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有像素强度为28的像素乘以一个numpy数组1，并存储在变量`a`中。
- en: Checks for all instances where an entire column consists of only pixel intensities
    of 28 and stores in variable `b`.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查所有实例，其中整列仅由像素强度为28的像素组成，并存储在变量`b`中。
- en: Deletes all columns (or *Y* axes) where pixel intensities are 28 for the entire
    column.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除所有列（或*Y*轴）中像素强度为28的整列。
- en: Plots the resulting image.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制生成的图像。
- en: Transposes the image in order to perform the preceding set of operations on
    all the rows (or *X* axes) in a similar manner.
  id: totrans-132
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转置图像，以便对所有行（或*X*轴）执行类似的操作。
- en: Multiplies all pixels with an intensity of 28 with a `numpy` array of 1s and
    stores in variable `d`.
  id: totrans-133
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有像素强度为28的像素乘以一个`numpy`数组1，并存储在变量`d`中。
- en: Checks for all instances where an entire column consists of only pixel intensities
    of 28 and stores in variable `e`.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查所有实例，其中整列仅由像素强度为28的像素组成，并存储在变量`e`中。
- en: Deletes all columns (from the transposed image) where pixel intensities are
    28 for the entire column.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除所有列（从转置图像中）中像素强度为28的整列。
- en: Transposes the image to get back the original image.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转置图像以恢复原始图像。
- en: Prints the shape of the image.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印图像的形状。
- en: Wherever a pixel intensity of less than 29 is found, replaces those pixel intensities
    with zeros, which will result in the cropping of all those pixels by making them
    white.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在发现像素强度小于29的地方，将这些像素强度替换为零，这将导致通过使它们变白来裁剪所有这些像素。
- en: Plots the resulting image.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制生成的图像。
- en: Reshapes the resulting image to a size of 150 x 128 pixels.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的图像重塑为150 x 128像素的大小。
- en: 'The output for the `crop()` function, as seen on the Jupyter notebook during
    execution, is shown in the following screenshot:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`crop()`函数的输出，如在Jupyter笔记本执行期间所见，如下截图所示：'
- en: '![](img/00301.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00301.jpeg)'
- en: 'Next, the defined `crop()` function is applied to all the files contained in
    the `training-synthetic` folder by looping through every file. This will result
    in an output as shown in the following screenshots:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过循环遍历`training-synthetic`文件夹中包含的所有文件，将定义的`crop()`函数应用于所有文件。这将导致如下截图所示的输出：
- en: '![](img/00302.jpeg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00302.jpeg)'
- en: 'The output continues as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/00303.jpeg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00303.jpeg)'
- en: Notice that only the relevant facial features are preserved and the resulting
    shapes of all the cropped images are less than 200 x 200, which was the initial
    size.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，仅保留了相关的面部特征，并且所有裁剪后的图像的形状都小于200 x 200，这是初始大小。
- en: On printing the image and shape of any random image, you will notice that every
    image is now resized to a 150 x 128-pixel array, and you will see the following
    output:![](img/00304.jpeg)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印任意图像的图像和形状，您会注意到每个图像现在都被调整为一个150 x 128像素的数组，并且您将看到以下输出：![](img/00304.jpeg)
- en: Splitting the images into test and train sets as well as segregating them into
    variables named `x_train`, `y1_train`, `x_test`, and `y1_test` will result in
    the output shown in the following screenshot:![](img/00305.jpeg)
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像分割为测试集和训练集，并将它们分隔为名为`x_train`、`y1_train`、`x_test`和`y1_test`的变量，将导致以下截图中看到的输出：![](img/00305.jpeg)
- en: 'Segregating the data is done as follows:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据的分离如下进行：
- en: '![](img/00306.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00306.jpeg)'
- en: Reshaping the training and test images and converting the data type to float32
    will result in the output seen in the following screenshot:![](img/00307.jpeg)
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练和测试图像进行重塑并将数据类型转换为float32将导致以下截图中看到的输出：![](img/00307.jpeg)
- en: There's more...
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Consider the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下内容：
- en: Once the images are done preprocessing they still need to be normalized and
    converted into vectors (in this case tensors) before being fed into the network.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦图像完成预处理，它们仍然需要被规范化并转换为向量（在本例中是张量），然后才能被输入到网络中。
- en: Normalization, in the simplest case, means adjusting values measured on different
    scales to a notionally common scale, often prior to averaging. It is always a
    good idea to normalize data in order to prevent gradients from exploding or vanishing
    as seen in the vanishing and exploding gradient problems during gradient descent.
    Normalization also ensures there is no data redundancy.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最简单的情况下，规范化意味着调整在不同尺度上测量的值到一个概念上的共同尺度，通常是在平均之前。规范化数据总是一个好主意，以防止梯度在梯度下降过程中爆炸或消失，如梯度消失和爆炸问题所示。规范化还确保没有数据冗余。
- en: Normalization of the data is done by dividing each pixel in each image by `255`
    since the pixel values range between 0 and `255`. This will result in the output
    shown in the following screenshot:![](img/00308.jpeg)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将每个图像中的每个像素除以`255`来对数据进行规范化，因为像素值的范围在0和`255`之间。这将导致以下截图中看到的输出：![](img/00308.jpeg)
- en: 'Next, the images are converted to input vectors with ten different classes
    using the `to_categorical()` function from `numpy_utils`, as shown in the following
    screenshot:'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，使用`numpy_utils`中的`to_categorical()`函数将图像转换为具有十个不同类的输入向量，如下截图所示：
- en: '![](img/00309.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00309.jpeg)'
- en: See also
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Additional resources are as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是其他资源：
- en: 'For more information on data normalization, check the following link:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关数据规范化的更多信息，请查看以下链接：
- en: '[https://www.quora.com/What-is-normalization-in-machine-learning](https://www.quora.com/What-is-normalization-in-machine-learning)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.quora.com/What-is-normalization-in-machine-learning](https://www.quora.com/What-is-normalization-in-machine-learning)'
- en: 'For information on overfitting and why data is split into test and training
    sets, visit the following link:'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关过拟合以及为什么数据被分成测试集和训练集的信息，请访问以下链接：
- en: '[https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)'
- en: 'For more information on encoding variables and their importance, visit the
    following link:'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关编码变量及其重要性的更多信息，请访问以下链接：
- en: '[http://pbpython.com/categorical-encoding.html](http://pbpython.com/categorical-encoding.html)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://pbpython.com/categorical-encoding.html](http://pbpython.com/categorical-encoding.html)'
- en: Model building, training, and analysis
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型构建、训练和分析
- en: We will use a standard sequential model from the `keras` library to build the
    CNN. The network will consist of three convolutional layers, two maxpooling layers,
    and four fully connected layers. The input layer and the subsequent hidden layers
    have 16 neurons, while the maxpooling layers contain a pool size of (2,2). The
    four fully connected layers consist of two dense layers and one flattened layer
    and one dropout layer. Dropout 0.25 was used to reduce the overfitting problem.
    Another novelty of this algorithm is the use of data augmentation to fight the
    overfitting phenomenon. Data augmentation is carried by rotating, shifting, shearing,
    and zooming the images to different extents to fit the model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`keras`库中的标准顺序模型来构建CNN。该网络将包括三个卷积层，两个最大池化层和四个全连接层。输入层和随后的隐藏层有16个神经元，而最大池化层包含(2,2)的池大小。四个全连接层包括两个密集层和一个扁平层和一个dropout层。使用0.25的dropout来减少过拟合问题。该算法的另一个新颖之处是使用数据增强来对抗过拟合现象。数据增强通过旋转、移位、剪切和缩放图像到不同程度来适应模型。
- en: The `relu` function is used as the activation function in both the input and
    hidden layers, while the `softmax` classifier is used in the output layer to classify
    the test images based on the predicted output.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入和隐藏层中，使用`relu`函数作为激活函数，而在输出层中使用`softmax`分类器来根据预测的输出对测试图像进行分类。
- en: Getting ready
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The network which will be constructed can be visualized as shown in the following
    diagram:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 将构建的网络可视化如下图所示：
- en: '![](img/00310.jpeg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00310.jpeg)'
- en: How to do it...
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps are as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Define the model using the `Sequential()` function in the Keras framework using
    the following commands:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在Keras框架中使用`Sequential()`函数定义模型：
- en: '[PRE11]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Print the summary of the model to get a better understanding of how the model
    is built and to ensure that it is built as per the preceding specifications. This
    can be done by using the `model.summary()` command.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印模型的摘要以更好地了解模型的构建方式，并确保它是根据前述规格构建的。这可以通过使用`model.summary()`命令来完成。
- en: 'Next, compile the model using the following command:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令编译模型：
- en: '[PRE12]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In order to prevent overfitting and improve model accuracy further, implement
    some form of data augmentation. In this step, the images will be sheared, shifted
    on a horizontal as well as the vertical axis, zoomed in, and rotated. The ability
    of the model to learn and identify these anomalies will dictate how robust the
    model is. Augment the data using the following commands:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了防止过拟合并进一步提高模型的准确性，实现某种形式的数据增强。在这一步中，图像将被剪切、水平和垂直轴上移动、放大和旋转。模型学习和识别这些异常的能力将决定模型的鲁棒性。使用以下命令增强数据：
- en: '[PRE13]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, fit and evaluate the model after data augmentation using the following
    commands:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用以下命令进行数据增强后拟合和评估模型：
- en: '[PRE14]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works...
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The functionality is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 功能如下：
- en: 'By using the sequential function, a nine-layer convolutional neural network
    is defined with each layer performing the following functions:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用sequential函数，定义了一个九层卷积神经网络，每一层执行以下功能：
- en: The first layer is a convolutional layer with 16 neurons and performs convolution
    on the input tensor/matrix. The size of the feature map is defined to be a 3 x
    3 matrix. The input shape needs to be specified for the first layer since the
    neural network needs to know what type of input to expect. Since all the images
    have been cropped to a size of 128 x 150 pixels, this will be the input shape
    defined for the first layer of the network as well. The activation function used
    in this layer is a **rectified linear unit** (**relu**).
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一层是一个具有16个神经元的卷积层，并对输入张量/矩阵进行卷积。特征图的大小被定义为一个3 x 3的矩阵。由于神经网络需要知道期望的输入类型，因此需要为第一层指定输入形状。由于所有图像都被裁剪为128
    x 150像素的大小，这也将是网络第一层定义的输入形状。在这一层中使用的激活函数是**修正线性单元**（**relu**）。
- en: The second layer of the network (first hidden layer) is another convolution
    layer with 16 neurons as well. Again, a `relu` will be used as the activation
    function for this layer.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络的第二层（第一个隐藏层）是另一个具有16个神经元的卷积层。同样，这一层的激活函数将使用`relu`。
- en: The third layer of the network (second hidden layer) is a max pooling layer
    with a pool size of 2 x 2\. The function of this layer is to extract all the valid
    features learned by performing convolution in the first two layers and reducing
    the size of the matrix with all the learned features. Convolution is nothing but
    a matrix multiplication between the feature map and the input matrix (in our case,
    an image). The resulting values, which form the convolution process, are stored
    by the network in a matrix. The maximum values from these stored values will define
    a certain feature in the input image. These maximum values are what will be preserved
    by the max pooling layer, which will omit the non-relevant features.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络的第三层（第二个隐藏层）是一个具有2 x 2池大小的最大池化层。这一层的功能是提取通过前两层卷积学习到的所有有效特征，并减小包含所有学习到的特征的矩阵的大小。卷积无非是特征图和输入矩阵（在我们的情况下是图像）之间的矩阵乘法。网络将存储卷积过程中产生的结果值。这些存储的值中的最大值将定义输入图像中的某个特征。这些最大值将由最大池化层保留，该层将省略不相关的特征。
- en: The fourth layer of the network (third hidden layer) is another convolutional
    layer with a feature map of 3 x 3 again. The activation function used in this
    layer will again be a `relu` function.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络的第四层（第三个隐藏层）是另一个3 x 3的特征图的卷积层。在这一层中使用的激活函数将再次是`relu`函数。
- en: The fifth layer of the network (fourth hidden layer) is a max pooling layer
    with a pool size of 2 x 2.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络的第五层（第四个隐藏层）是一个具有2 x 2池大小的最大池化层。
- en: The sixth layer of the network (fifth hidden layer) is a flatten layer that
    will convert the matrix containing all the learned features (stored in the form
    of numbers) into a single row instead of a multi-dimensional matrix.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络的第六层（第五个隐藏层）是一个扁平化层，它将包含所有学习到的特征（以数字形式存储）的矩阵转换为单行，而不是多维矩阵。
- en: The seventh layer in the network (sixth hidden layer) is a dense layer with
    512 neurons and a `relu` activation. Each neuron will basically process a certain
    weight and bias, which is nothing but a representation of all the learned features
    from a particular image. This is done in order to easily classify the image by
    using a `softmax` classifier on the dense layer.
  id: totrans-194
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络中的第七层（第六个隐藏层）是一个具有512个神经元和`relu`激活的密集层。每个神经元基本上会处理特定的权重和偏差，这无非是对特定图像中所有学习到的特征的表示。这是为了通过在密集层上使用`softmax`分类器轻松对图像进行分类。
- en: The eighth layer in the network (seventh hidden layer) is a dropout layer with
    a dropout probability of 0.25 or 25%. This layer will randomly `dropout` 25% of
    the neurons during the training process and help prevent overfitting by encouraging
    the network to learn a given feature using many alternative paths.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络中的第八层（第七个隐藏层）是一个具有0.25或25%的丢弃概率的丢弃层。这一层将在训练过程中随机丢弃25%的神经元，并通过鼓励网络使用许多替代路径来防止过拟合。
- en: The final layer in the network is a dense layer with just 10 neurons and the
    `softmax` classifier. This is the eighth hidden layer and will also serve as the
    output layer of the network.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络中的最后一层是一个只有10个神经元和`softmax`分类器的密集层。这是第八个隐藏层，也将作为网络的输出层。
- en: 'The output after defining the model must look like the one in the following
    screenshot:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义模型后的输出必须如下截图所示：
- en: '![](img/00311.jpeg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00311.jpeg)'
- en: On printing the `model.summary()` function, you must see an output like the
    one in the following screenshot:![](img/00312.jpeg)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在打印`model.summary()`函数时，必须看到如下截图中的输出：![](img/00312.jpeg)
- en: 'The model is compiled using categorical crossentropy, which is a function to
    measure and compute the loss from the network while transferring information from
    one layer to the subsequent layers. The model will make use of the `Adam()` optimizer
    function from the Keras framework, which will basically dictate how the network
    optimizes the weights and biases while learning the features. The output of the
    `model.compile()` function must look like the following screenshot:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型使用分类交叉熵进行编译，这是一个函数，用于在将信息从一个层传输到后续层时测量和计算网络的损失。模型将使用Keras框架中的`Adam()`优化器函数，它基本上会指导网络在学习特征时如何优化权重和偏差。`model.compile()`函数的输出必须如下截图所示：
- en: '![](img/00313.jpeg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00313.jpeg)'
- en: 'Since the neural network is quite dense and the number of total images is only
    3,240, we devise a method to prevent overfitting. This is done by generating more
    images from the training set by performing data augmentation. In this step, the
    images are generated through the `ImageDataGenerator()` function. This function
    takes the training and test sets and augments images by:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于神经网络非常密集，总图像数量仅为3,240，因此我们设计了一种方法来防止过拟合。这是通过执行数据增强从训练集生成更多图像来完成的。在这一步中，图像是通过`ImageDataGenerator()`函数生成的。该函数通过以下方式对训练和测试集进行图像增强：
- en: Rotating them
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旋转它们
- en: Shearing them
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剪切它们
- en: Shifting the width, which is basically widening the images
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动宽度，基本上是扩大图像
- en: Shifting the images on a horizontal axis
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在水平轴上移动图像
- en: Shifting the images on a vertical axis
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在垂直轴上移动图像
- en: 'The output of the preceding function must look like the following screenshot:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 前述函数的输出必须如下截图所示：
- en: '![](img/00314.jpeg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00314.jpeg)'
- en: 'Finally, the model is fitted to the data and evaluated after training over
    5 epochs. The output we obtained is shown in the following screenshot:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，模型在训练5个时期后适应数据并进行评估。我们获得的输出如下截图所示：
- en: '![](img/00315.jpeg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00315.jpeg)'
- en: As you can see, we obtained an accuracy of 98.46%, which resulted in an error
    rate of 1.54%. This is pretty good, but convolutional networks have advanced so
    much that we can improve this error rate by tuning a few hyperparameters or using
    a deeper network.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，我们获得了98.46%的准确性，导致错误率为1.54%。这相当不错，但是卷积网络已经进步了很多，我们可以通过调整一些超参数或使用更深的网络来改进这个错误率。
- en: There's more...
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Using a deeper CNN with 12 layers (one extra convolution and one extra max
    pooling layer) resulted in an improvement of accuracy to 99.07%, as shown in the
    following screenshot:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 使用12层更深的CNN（一个额外的卷积和一个额外的最大池化层）将准确性提高到99.07%，如下截图所示：
- en: '![](img/00316.jpeg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00316.jpeg)'
- en: 'Using data normalization after every two layers during model building, we were
    further able to improve the accuracy to 99.85%, as shown in the following screenshot:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建过程中每两层之后使用数据归一化后，我们进一步将准确性提高到99.85%，如下截图所示：
- en: '![](img/00317.jpeg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00317.jpeg)'
- en: 'You may obtain different results, but feel free to run the training step a
    few times. The following are some of the steps you can take to experiment with
    the network in the future to understand it better:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会得到不同的结果，但可以随意运行几次训练步骤。以下是您可以采取的一些步骤，以便在将来实验网络以更好地了解它：
- en: Try to tune hyperparameters better and implement a higher dropout percentage
    and see how the network responds.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试更好地调整超参数，并实施更高的丢失百分比，看看网络的响应如何。
- en: The accuracy greatly reduced when we tried using different activation functions
    or a smaller (less dense) network.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们尝试使用不同的激活函数或更小（不太密集）的网络时，准确性大大降低。
- en: Also, change the size of the feature maps and max pooling layer and see how
    this influences training time and model accuracy.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，更改特征图和最大池化层的大小，并查看这如何影响训练时间和模型准确性。
- en: Try including more neurons in a less dense CNN and tune it to improve accuracy.
    This may also result in a faster network that trains in less time.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试在不太密集的CNN中包含更多的神经元并进行调整以提高准确性。这也可能导致更快的网络，训练时间更短。
- en: Use more training data. Explore other online repositories and find larger databases
    to train the network. Convolutional neural networks usually perform better when
    the size of the training data is increased.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更多的训练数据。探索其他在线存储库，找到更大的数据库来训练网络。当训练数据的大小增加时，卷积神经网络通常表现更好。
- en: See also
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'The following published papers are good resources to obtain a better understanding
    of convolutional neural networks. They may be used as further reading in order
    to gain more understanding of various applications of convolutional neural networks:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下已发表的论文是了解卷积神经网络的更好资源。它们可以作为进一步阅读，以更多地了解卷积神经网络的各种应用：
- en: '[http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)'
- en: '[https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882)'
- en: '[https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf)'
- en: '[http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/pdfs/Simard.pdf](http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/pdfs/Simard.pdf)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/pdfs/Simard.pdf](http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/pdfs/Simard.pdf)'
- en: '[https://dl.acm.org/citation.cfm?id=2807412](https://dl.acm.org/citation.cfm?id=2807412)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://dl.acm.org/citation.cfm?id=2807412](https://dl.acm.org/citation.cfm?id=2807412)'
- en: '[https://ieeexplore.ieee.org/abstract/document/6165309/](https://ieeexplore.ieee.org/abstract/document/6165309/)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://ieeexplore.ieee.org/abstract/document/6165309/](https://ieeexplore.ieee.org/abstract/document/6165309/)'
- en: '[http://openaccess.thecvf.com/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://openaccess.thecvf.com/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)'
- en: '[http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/download/3098/3425](http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/download/3098/3425)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/download/3098/3425](http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/download/3098/3425)'
- en: '[https://ieeexplore.ieee.org/abstract/document/6288864/](https://ieeexplore.ieee.org/abstract/document/6288864/)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://ieeexplore.ieee.org/abstract/document/6288864/](https://ieeexplore.ieee.org/abstract/document/6288864/)'
