- en: Analyzing Textual Data
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 分析文本数据
- en: In the age of information, data is produced at incredible speeds and volumes.
    The data produced is not only structured or tabular types, it can also be in a
    variety of unstructured types such as textual data, image or graphic data, speech
    data, and video. Text is a very common and rich type of data. Articles, blogs,
    tutorials, social media posts, and website content all produce unstructured textual
    data. Thousands of emails, messages, comments, and tweets are sent by people every
    minute. Such a large amount of text data needs to be mined. Text analytics offers
    lots of opportunities for business people; for instance, Amazon can interpret
    customer feedback on a particular product, news analysts can analyze news trends
    and the latest issues on Twitter, and Netflix can also interpret reviews of each
    movie and web series. Business analysts can interpret customer activities, reviews,
    feedback, and sentiments to drive their business effectively using NLP and text
    analysis.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息化时代，数据以惊人的速度和体量产生。产生的数据不仅限于结构化或表格类型，也可以是多种非结构化类型，如文本数据、图像或图形数据、语音数据和视频数据。文本是非常常见且丰富的数据类型。文章、博客、教程、社交媒体帖子和网站内容都会生成非结构化的文本数据。每分钟都会有成千上万的电子邮件、消息、评论和推文发送出去。如此大量的文本数据需要被挖掘。文本分析为商业人士提供了大量机会；例如，亚马逊可以解读关于特定产品的客户反馈，新闻分析师可以分析Twitter上的新闻趋势和最新问题，Netflix也可以解读每部电影和网络剧集的评论。商业分析师可以通过NLP和文本分析解读客户活动、评论、反馈和情感，从而有效推动业务发展。
- en: In this chapter, we will start with basic text analytics operations such as
    tokenization, removing stopwords, stemming, lemmatization, PoS tagging, and entity
    recognition. After this, we will see how to visualize your text analysis using
    WordCloud. We will see how to find out the opinions of customers about a product
    based on reviews, using sentiment analysis. Here, we will perform sentiment analysis
    using text classification and assess model performance using accuracy, precision,
    recall, and f1-score. Finally, we will focus on text similarity between two sentences
    using Jaccard and cosine similarity.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从基本的文本分析操作开始，如分词、移除停用词、词干提取、词形还原、词性标注和实体识别。之后，我们将学习如何使用WordCloud可视化文本分析结果。我们将看到如何基于评论，通过情感分析找出客户对某个产品的看法。在这里，我们将使用文本分类进行情感分析，并通过准确率、精确度、召回率和F1分数评估模型的性能。最后，我们将重点关注通过Jaccard和余弦相似度衡量两句话之间的文本相似性。
- en: 'The topics of this chapter are listed as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主题如下所示：
- en: Installing NLTK and SpaCy
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装NLTK和SpaCy
- en: Text normalization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本规范化
- en: Tokenization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词
- en: Removing stopwords
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除停用词
- en: Stemming and lemmatization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取与词形还原
- en: POS tagging
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注
- en: Recognizing entities
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体识别
- en: Dependency parsing
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依存句法分析
- en: Creating a word cloud
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建词云
- en: Bag of words
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词袋模型
- en: TF-IDF
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Sentiment analysis using text classification
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文本分类进行情感分析
- en: Text similarity
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本相似度
- en: Technical requirements
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术要求如下：
- en: 'You can find the code and the datasets at the following GitHub link: [https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter12\.](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter12)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过以下GitHub链接找到代码和数据集：[https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter12](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter12)
- en: All the code blocks are available in the `ch12.ipynb` file.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有代码块都可以在`ch12.ipynb`文件中找到。
- en: This chapter uses only one TSV file (`amazon_alexa.tsv`) for practice purposes.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章仅使用一个TSV文件（`amazon_alexa.tsv`）进行练习。
- en: In this chapter, we will use the NLTK, SpaCy, WordCloud, matplotlib, seaborn,
    and scikit-learn Python libraries.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用NLTK、SpaCy、WordCloud、matplotlib、seaborn和scikit-learn Python库。
- en: Installing NLTK and SpaCy
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装NLTK和SpaCy
- en: 'NLTK is one of the popular and essential Python packages for natural language
    processing. It offers all the basic, as well as advanced, NLP operations. It comprises
    common algorithms such as tokenization, stemming, lemmatization, part-of-speech,
    and named entity recognition. The main features of the NLTK library are that it''s
    open-source, easy to learn, easy to use, has a prominent community, and has well-organized
    documentation. The NLTK library can be installed using the `pip install` command
    running on the command line as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 是一个流行且重要的 Python 自然语言处理库。它提供所有基础和高级的 NLP 操作。它包括常见的算法，如分词、词干提取、词形还原、词性标注和命名实体识别。NLTK
    库的主要特点是开源、易于学习、易于使用、拥有活跃的社区以及完善的文档。NLTK 库可以通过在命令行中运行以下`pip install`命令来安装：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'NLTK is not a pre-installed library in Anaconda. We can directly install `nltk`
    in the Jupyter Notebook. We can use an exclamation point (!) before the command
    in the cell:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 不是 Anaconda 中预安装的库。我们可以直接在 Jupyter Notebook 中安装`nltk`。我们可以在单元格中的命令前加上感叹号
    (!)：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'SpaCy is another essential and powerful Python package for NLP. It offers a
    common NLP algorithm as well as advanced functionalities. It is designed for production
    purposes and develops applications for a large volume of data. The SpaCy library
    can be installed using the `pip install` command running on the command line as
    follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy 是另一个重要且强大的 Python NLP 库。它提供了常见的 NLP 算法和高级功能。它旨在用于生产目的，并为大规模数据开发应用程序。可以使用以下命令在命令行中通过`pip
    install`安装 SpaCy 库：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After installing spaCy, we need to install a `spacy` English-language model.
    We can install it using the following command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完 spaCy 后，我们需要安装一个`spacy`英语语言模型。我们可以使用以下命令安装：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Spacy and its English model are not pre-installed in Anaconda. We can directly
    install `spacy` using the following code. We can use the exclamation point (!)
    before the command in the cell:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy 及其英语模型没有预安装在 Anaconda 中。我们可以使用以下代码直接安装`spacy`。我们可以在单元格中的命令前加上感叹号 (!)：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using the preceding syntax, we can install `spacy` and its English model in
    Jupyter Notebooks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述语法，我们可以在 Jupyter Notebook 中安装`spacy`及其英语模型。
- en: Text normalization
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本标准化
- en: Text normalization converts text into standard or canonical form. It ensures
    consistency and helps in processing and analysis. There is no single approach
    to the normalization process. The first step in normalization is the lower case
    all the text. It is the simplest, most applicable, and effective method for text
    pre-processing. Another approach could be handling wrongly spelled words, acronyms,
    short forms, and the use of out-of-vocabulary words; for example, "super," "superb,"
    and "superrrr" can be converted into "super". Text normalization handles the noise
    and disturbance in test data and prepares noise-free data. We also apply stemming
    and lemmatization to normalize the words present in the text.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 文本标准化将文本转换为标准或规范形式。它确保一致性，并有助于处理和分析。标准化过程没有单一的方法。标准化的第一步是将所有文本转换为小写。这是最简单、最适用且最有效的文本预处理方法。另一种方法可能是处理拼写错误的单词、缩写词、简写和使用超出词汇表的单词；例如，“super”，“superb”和“superrrr”可以转换为“super”。文本标准化处理测试数据中的噪声和干扰，准备无噪声数据。我们还会应用词干提取和词形还原来标准化文本中存在的单词。
- en: 'Let''s perform a basic normalization operation by converting the text into
    lowercase:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将文本转换为小写来执行基本的标准化操作：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This results in the following output:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code block, we have converted the given input paragraph into
    lowercase by using the `lower()` method.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们通过使用`lower()`方法将给定的输入段落转换为小写。
- en: In NLP, text normalization deals with the randomness and converts text into
    a standard form that improves the overall performance of NLP solutions. It also
    reduces the size of the document term matrix by converting the words into their
    root word. In the upcoming sections, we will focus on basic text preprocessing
    operations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 中，文本标准化处理随机性，并将文本转换为标准形式，从而提高 NLP 解决方案的整体性能。它还通过将单词转换为根词来减少文档词项矩阵的大小。在接下来的部分中，我们将重点介绍基本的文本预处理操作。
- en: Tokenization
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词
- en: 'Tokenization is the initial step in text analysis. Tokenization is defined
    as breaking down text paragraphs into smaller parts or tokens such as sentences
    or words and ignoring punctuation marks. Tokenization can be of two types: sentence
    tokenization and word tokenization. A sentence tokenizer splits a paragraph into
    sentences and word tokenization splits a text into words or tokens.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是文本分析中的初步步骤。分词被定义为将文本段落分解成更小的部分或词元，如句子或单词，并忽略标点符号。分词可以分为两种类型：句子分词和单词分词。句子分词器将段落拆分为句子，而单词分词器将文本拆分为单词或词元。
- en: 'Let''s tokenize a paragraph using NLTK and spaCy:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 NLTK 和 spaCy 对段落进行分词：
- en: 'Before tokenization, import NLTK and download the required files:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分词之前，导入 NLTK 并下载所需的文件：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we will tokenize paragraphs into sentences using the `sent_tokenize()`
    method of NLTK:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用 NLTK 的 `sent_tokenize()` 方法将段落分割成句子：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding example, we have taken a paragraph and passed it as a parameter
    to the `sent_tokenize()` method. The output of this method will be a list of sentences.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们将段落作为参数传递给 `sent_tokenize()` 方法。此方法的输出将是一个句子列表。
- en: 'Let''s tokenize the paragraph into sentences using spaCy:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 spaCy 将段落分割成句子：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This results in the following output:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding example, first, we have imported the English language model
    and instantiated it. After this, we created the NLP pipe using `sentencizer` and
    added it to the pipeline. Finally, we created the NLP object and iterated through
    the `sents` attribute of the NLP object to create a list of tokenized sentences.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们首先导入了英语语言模型并实例化它。之后，我们使用 `sentencizer` 创建了 NLP 管道，并将其添加到管道中。最后，我们创建了
    NLP 对象，并通过迭代 NLP 对象的 `sents` 属性来生成一个分词后的句子列表。
- en: 'Let''s tokenize paragraphs into words using the `word_tokenize()` function
    of NLTK:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 NLTK 的 `word_tokenize()` 函数将段落分词：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in the following output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding example, we have taken a paragraph and passed it as a parameter
    to the `word_tokenize()` method. The output of this method will be a list of words.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们将段落作为参数传递给 `word_tokenize()` 方法。此方法的输出将是一个单词列表。
- en: 'Let''s tokenize the paragraph into words using spaCy:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 spaCy 将段落分词为单词：
- en: '[PRE14]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This results in the following output:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding example, first, we imported the English language model and
    instantiated it. After this, we created a text paragraph. Finally, we created
    the NLP object using text paragraphs and iterated it to create a list of tokenized
    words.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们首先导入了英语语言模型并实例化它。之后，我们创建了一个文本段落。最后，我们使用文本段落创建了一个NLP对象，并通过迭代它来生成一个分词后的单词列表。
- en: 'Let''s create the frequency distribution of tokenized words:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建分词后的单词的频率分布：
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This results in the following output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE17]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s create a frequency distribution plot using matplotlib:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 matplotlib 创建一个频率分布图：
- en: '[PRE18]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This results in the following output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/97dbcf7d-d87a-44b7-9c08-6ca9aa665183.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97dbcf7d-d87a-44b7-9c08-6ca9aa665183.png)'
- en: In the preceding example, we have generated the frequency distribution of tokens
    using the `FreqDist` class. After sentence and word tokenization, we will learn
    how to remove stopwords from the given text.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们使用 `FreqDist` 类生成了词元的频率分布。在句子和单词分词后，我们将学习如何从给定文本中移除停用词。
- en: Removing stopwords
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除停用词
- en: Stopwords are counted as noise in text analysis. Any text paragraph has to have
    verbs, articles, and propositions. These are all considered stop words. Stop words
    are necessary for human conversation but they don't make many contributions in
    text analysis. Removing stopwords from text is called noise elimination.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词被视为文本分析中的噪声。任何文本段落中都必须包含动词、冠词和介词。这些都被视为停用词。停用词对于人类对话是必要的，但在文本分析中贡献不大。从文本中移除停用词称为噪声消除。
- en: 'Let''s see how to remove stopwords using NLTK:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用 NLTK 移除停用词：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This results in the following output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding example, first, we imported the stopwords and loaded the English
    word list. After this, we iterated the tokenized word list that we generated in
    the previous section using a `for` loop and filtered the tokenized words from
    the stop word list using the `if` condition. We saved the filtered words in the
    `fltered_word_list` list object.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，首先，我们导入了停用词并加载了英文单词列表。之后，我们使用`for`循环迭代在上一节中生成的分词单词列表，并使用`if`条件从停用词列表中过滤分词单词。我们将过滤后的单词保存在`fltered_word_list`列表对象中。
- en: 'Let''s see how to remove stopwords using spaCy:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用spaCy去除停用词：
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This results in the following output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE22]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the preceding example, first, we imported the stopwords and loaded the English
    word list into the stopwords variable. After this, we iterated the NLP object
    using a `for` loop and filtered each word with the property `"is_stop"` from the
    stop word list using the `if` condition. We appended the filtered words in the
    `fltered_token_list` list object. In this section, we have looked at removing
    stopwords. Now, it's time to learn about stemming and lemmatization to find the
    root word.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，首先，我们导入了停用词并将英文单词列表加载到停用词变量中。之后，我们使用`for`循环迭代NLP对象，并通过`if`条件从停用词列表中过滤每个单词，使用属性`"is_stop"`。我们将过滤后的单词附加到`fltered_token_list`列表对象中。在这一部分，我们了解了如何去除停用词。现在，轮到我们学习词干提取和词形还原，找到根词了。
- en: Stemming and lemmatization
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词干提取和词形还原
- en: 'Stemming is another step in text analysis for normalization at the language
    level. The stemming process replaces a word with its root word. It chops off the
    prefixes and suffixes. For example, the word connect is the root word for connecting,
    connected, and connection. All the mentioned words have a common root: **connect**.
    Such differences between word spellings make it difficult to analyze text data.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取（Stemming）是文本分析中语言层面的另一步规范化。词干提取过程将单词替换为其词根。它会去除单词的前缀和后缀。例如，单词“connect”是“connecting”、“connected”和“connection”的词根。所有这些单词都有一个共同的词根：**connect**。这种单词拼写的差异使得文本数据分析变得困难。
- en: Lemmatization is another type of lexicon normalization, which converts a word
    into its root word. It is closely related to stemming. The main difference is
    that lemmatization considers the context of the word while normalization is performed,
    but stemmer doesn't consider the contextual knowledge of the word. Lemmatization
    is more sophisticated than a stemmer. For example, the word "geese" lemmatizes
    as "goose." Lemmatization reduces words to their valid lemma using a dictionary.
    Lemmatization considers the part of speech near the words for normalization; that
    is why it is difficult to implement and slower, while stemmers are easier to implement
    and faster but with less accuracy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原（Lemmatization）是另一种词汇规范化方法，它将一个单词转换为其词根。它与词干提取（stemming）密切相关。主要区别在于，词形还原在执行规范化时考虑了单词的上下文，而词干提取则没有考虑单词的上下文知识。词形还原比词干提取更为复杂。例如，单词“geese”经过词形还原后变成“goose”。词形还原通过使用词典将单词简化为有效的词元。词形还原会根据单词的词性进行规范化；这就是为什么它难以实现且速度较慢，而词干提取则容易实现且速度较快，但准确度较低的原因。
- en: 'Let''s see how to get stemmed and lemmatized using NLTK:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用NLTK进行词干提取和词形还原：
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This results in the following output:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE24]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding example, first, we imported `WordNetLemmatizer` for lemmatization
    and instantiated its object. Similarly, we imported `PorterStemmer` to stem an
    instantiate of its object. After this, we got the lemma using the `lemmatize()`
    function and the stemmed word using the `stem()` function.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，首先，我们导入了`WordNetLemmatizer`进行词形还原并实例化了它的对象。类似地，我们导入了`PorterStemmer`进行词干提取并实例化了它的对象。之后，我们通过`lemmatize()`函数获得了词形，还通过`stem()`函数得到了词干。
- en: 'Let''s see how to get lemmatized words using spaCy:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用spaCy进行词形还原：
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This results in the following output:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the preceding example, first, we imported the English language model and
    instantiated it. After this, we created the NLP object and iterated it using a
    `for` loop. In the loop, we got the text value and its lemma value using the `text`
    and `lemma_` properties. In this section, we have looked at stemming and lemmatization.
    Now, we will learn PoS tagging in the given document.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，首先，我们导入了英文语言模型并实例化了它。之后，我们创建了NLP对象并使用`for`循环对其进行迭代。在循环中，我们通过`text`和`lemma_`属性获取了文本值及其词形还原值。在这一部分，我们学习了词干提取和词形还原。现在，我们将学习给定文档中的词性标注（PoS
    tagging）。
- en: POS tagging
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词性标注（POS tagging）
- en: PoS stands for part of speech. The main objective of POS tagging is to discover
    the syntactic type of words, such as nouns, pronouns, adjectives, verbs, adverbs,
    and prepositions. PoS tagging finds the relationship among words within a sentence.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: PoS代表词性。PoS标签的主要目标是发现单词的句法类型，如名词、代词、形容词、动词、副词和介词。PoS标签能够找到句子中单词之间的关系。
- en: 'Let''s see how to get POS tags for words using NLTK:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用NLTK获取单词的PoS标签：
- en: '[PRE27]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This results in the following output:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE28]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the preceding example, first, we imported `word_tokenize` and `pos_tag`.
    After this, we took a text paragraph and passed it as a parameter to the `word_tokenize()`
    method. The output of this method will be a list of words. After this, generate
    PoS tags for each token using the `pos_tag()` function.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，首先，我们导入了`word_tokenize`和`pos_tag`。接着，我们获取了一段文本并将其作为参数传递给`word_tokenize()`方法。此方法的输出将是一个单词列表。接下来，使用`pos_tag()`函数为每个标记生成PoS标签。
- en: 'Let''s see how to get POS tags for words using spaCy:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用spaCy获取单词的PoS标签：
- en: '[PRE29]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This results in the following output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE30]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding example, first, we imported the English language model and
    instantiated it. After this, we created the NLP object and iterated it using a
    `for` loop. In the loop, we got the text value and its lemma value using the `text`
    and `pos_` properties. In this section, we have looked at PoS tags. Now, it's
    time to jump to recognizing named entities in the text.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，首先，我们导入了英语语言模型并实例化了它。接着，我们创建了NLP对象，并使用`for`循环对其进行迭代。在循环中，我们通过`text`和`pos_`属性获取文本值及其词干值。在这一部分中，我们已经研究了PoS标签。现在，是时候跳转到识别文本中的命名实体了。
- en: Recognizing entities
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别实体
- en: Entity recognition means extracting or detecting entities in the given text.
    It is also known as **Named Entity Recognition** (**NER**). An entity can be defined
    as an object, such as a location, people, an organization, or a date. Entity recognition
    is one of the advanced topics of NLP. It is used to extract important information
    from text.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 实体识别是指从给定文本中提取或检测实体。它也被称为**命名实体识别**（**NER**）。实体可以定义为一个对象，如地点、人物、组织或日期。实体识别是NLP的高级主题之一，它用于从文本中提取重要信息。
- en: 'Let''s see how to get entities from text using spaCy:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用spaCy从文本中提取实体：
- en: '[PRE31]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This results in the following output:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE32]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the preceding example, first, we imported spaCy and loaded the English language
    model. After this, we created the NLP object and iterated it using a `for` loop.
    In the loop, we got the text value and its entity type value using the `text`
    and `label_` properties. Let''s visualize the entities in the text using a spaCy
    display class:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，首先，我们导入了spaCy并加载了英语语言模型。接着，我们创建了NLP对象，并使用`for`循环对其进行迭代。在循环中，我们通过`text`和`label_`属性获取文本值及其实体类型值。现在，让我们使用spaCy的`display`类来可视化文本中的实体：
- en: '[PRE33]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This results in the following output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/225347f8-1378-43ca-88aa-1f1d9a183188.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/225347f8-1378-43ca-88aa-1f1d9a183188.png)'
- en: In the preceding example, we imported the display class and called its `render()`
    method with a NLP text object, `style` as `ent`, and `jupyter` as `True`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们导入了`display`类，并使用一个NLP文本对象调用了它的`render()`方法，`style`设置为`ent`，`jupyter`设置为`True`。
- en: Dependency parsing
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依存解析
- en: 'Dependency parsing finds the relationship among words – how words are related
    to each other. It helps computers to understand sentences for analysis; for example,
    "Taj Mahal is one of the most beautiful monuments." We can''t understand this
    sentence just by analyzing words. We need to dig down and understand the word
    order, sentence structure, and parts of speech:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 依存解析用于发现单词之间的关系——即单词是如何彼此关联的。它帮助计算机理解句子以便进行分析；例如，“泰姬陵是最美丽的纪念碑之一。”我们不能仅通过分析单词来理解这个句子。我们需要深入分析并理解单词顺序、句子结构以及词性：
- en: '[PRE34]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This results in the following output:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/7c45d0f8-b69f-49ed-97ca-ad30f7862e69.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c45d0f8-b69f-49ed-97ca-ad30f7862e69.png)'
- en: In the preceding example, we have imported the display class and called its
    `render()` method with a NLP text object, `style` as '`dep`', `jupyter` as `True`,
    and `options` as a dictionary with a distance key and a value of 150\. Now, we
    will see how to visualize text data using a word cloud, based on the word's frequency
    in the text.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们导入了`display`类，并使用一个NLP文本对象调用了它的`render()`方法，`style`设置为`'dep'`，`jupyter`设置为`True`，并将`options`设置为一个包含`distance`键和值150的字典。接下来，我们将看看如何根据文本中单词的频率，使用词云可视化文本数据。
- en: Creating a word cloud
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建词云
- en: 'As a data analyst, you need to identify the most frequent words and represent
    them in graphical form to the top management. A word cloud is used to represent
    a word-frequency plot. It represents the frequency by the size of the word, that
    is, the more frequent word looks larger in size and less frequent words looks
    smaller in size. It is also known as a tag cloud. We can create a word cloud using
    the `wordcloud` library in Python. We can install it using the following commands:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据分析师，你需要识别最常见的单词，并以图形形式向高层管理层呈现它们。词云用于表示单词频率图。它通过单词的大小来表示频率，也就是说，频率越高的单词看起来越大，频率较低的单词看起来越小。它也被称为标签云。我们可以使用Python中的`wordcloud`库来创建词云。我们可以使用以下命令安装它：
- en: '[PRE35]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Or, alternatively, this one:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，也可以使用这个：
- en: '[PRE36]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let''s learn how to create a word cloud:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何创建词云：
- en: 'Import libraries and load a stopwords list:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库并加载停用词列表：
- en: '[PRE37]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In the preceding example, we imported `WordCloud`, `STOPWORDS`, and `matplotlib.pyplot`
    classes. We also created the stopword set and defined the paragraph text.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们导入了`WordCloud`、`STOPWORDS`和`matplotlib.pyplot`类。我们还创建了停用词集合并定义了段落文本。
- en: 'Create and generate a word cloud:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建并生成词云：
- en: '[PRE38]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: After this, the `WordCloud` object with the parameters `width`, `height`, `background_color`,
    `stopwords`, and `min_font_size` are created and generated the cloud on the paragraph
    text string.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，创建并生成一个`WordCloud`对象，设置`width`、`height`、`background_color`、`stopwords`和`min_font_size`等参数，并在段落文本字符串上生成词云。
- en: 'Visualize the word cloud:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化词云：
- en: '[PRE39]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This results in the following output:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/b0aa3f4f-1b10-49fd-b0ec-1843c67bacb9.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0aa3f4f-1b10-49fd-b0ec-1843c67bacb9.png)'
- en: In the preceding example, we visualized the word cloud using `matplotlib.pyplot`.
    Let's learn how to convert text documents into a numeric vector using Bag of Words.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们使用`matplotlib.pyplot`可视化了词云。让我们学习如何使用词袋模型将文本文档转换为数值向量。
- en: Bag of Words
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词袋模型
- en: '**Bag of Words** (**BoW**) is one of the most basic, simplest, and popular
    feature engineering techniques for converting text into a numeric vector. It works
    in two steps: collecting vocabulary words and counting their presence or frequency
    in the text. It does not consider the document structure and contextual information.
    Let''s take the following three documents and understand BoW:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**词袋模型**（**BoW**）是最基本、最简单和最流行的特征工程技术之一，用于将文本转换为数值向量。它分为两个步骤：收集词汇表中的单词，并计算它们在文本中的出现频率或存在。它不考虑文档结构和上下文信息。让我们以以下三个文档为例，理解BoW：'
- en: 'Document 1: I like pizza.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 文档1：我喜欢披萨。
- en: 'Document 2: I do not like burgers.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 文档2：我不喜欢汉堡。
- en: 'Document 3: Pizza and burgers both are junk food.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 文档3：披萨和汉堡都属于垃圾食品。
- en: Now, we will create the **Document Term Matrix** (**DTM**). This matrix consists
    of the document at rows, words at the column, and the frequency at cell values.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建**文档词矩阵**（**DTM**）。该矩阵由文档（行）、单词（列）以及单词频率（单元格值）组成。
- en: '|  | I | like | pizza | do | not | burgers | and | both | are | junk | food
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | I | like | pizza | do | not | burgers | and | both | are | junk | food
    |'
- en: '| Doc-1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Doc-1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
- en: '| Doc-2 | 1 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Doc-2 | 1 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 |'
- en: '| Doc-3 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Doc-3 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 |'
- en: In the preceding example, we generated the DTM using a single keyword known
    as a unigram. We can also use a combination of continuous two keywords, known
    as the bigram model, and three keywords, known as the trigram model. The generalized
    form is known as the n-gram model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们使用了单一的关键词生成了DTM，这称为单元法（unigram）。我们还可以使用连续的两个关键词组合，这被称为二元模型（bigram），或者三个关键词的组合，称为三元模型（trigram）。这种通用形式被称为n元模型（n-gram）。
- en: In Python, scikit-learn offers `CountVectorizer` for generating the BoW DTM.
    We'll see in the *Sentiment analysis using text classification* section how to
    generate it using scikit-learn.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，scikit-learn提供了`CountVectorizer`来生成BoW的文档词矩阵（DTM）。我们将在*使用文本分类进行情感分析*章节中看到如何使用scikit-learn生成它。
- en: TF-IDF
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF
- en: '**TF-IDF** stands for **Term Frequency-Inverse Document Frequency.** It has
    two segments: **Term Frequency** (**TF**) and **Inverse Document Frequency** (**IDF**).
    TF only counts the occurrence of words in each document. It is equivalent to BoW.
    TF does not consider the context of words and is biased toward longer documents.
    **IDF** computes values that correspond to the amount of information kept by a
    word.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**TF-IDF** 代表 **词频-逆文档频率**。它有两个部分：**词频** (**TF**) 和 **逆文档频率** (**IDF**)。TF
    仅计算每个文档中单词的出现次数，等同于 BoW（词袋模型）。TF 不考虑单词的上下文，且偏向较长的文档。**IDF** 计算值，表示某个词保留的信息量。'
- en: '![](img/d912905e-f510-41bd-a460-b50456eaa376.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d912905e-f510-41bd-a460-b50456eaa376.png)'
- en: 'TF-IDF is the dot product of both segments – TF and IDF. TF-IDF normalizes
    the document weights. A higher value of TF-IDF for a word represents a higher
    occurrence in that document. Let''s take the following three documents:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 是 TF 和 IDF 两个部分的点积。TF-IDF 会对文档权重进行归一化处理。TF-IDF 值越高，表示某个词在该文档中的出现频率越高。让我们看看以下三个文档：
- en: 'Document 1: I like pizza.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 文档 1：我喜欢比萨。
- en: 'Document 2: I do not like burgers.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 文档 2：我不喜欢汉堡。
- en: 'Document 3: Pizza and burgers both are junk food.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 文档 3：比萨和汉堡都是垃圾食品。
- en: 'Now, we will create the DTM. This matrix consists of the document name in the
    row headers, the words in the column headers, and the TF-IDF values in the cells:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建 DTM（文档-词项矩阵）。该矩阵的行头是文档名称，列头是词汇，单元格中则是 TF-IDF 值：
- en: '|  | I | like | pizza | do | not | burgers | and | both | are | junk | food
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | I | like | pizza | do | not | burgers | and | both | are | junk | food
    |'
- en: '| Doc-1 | 0.58 | 0.58 | 0.58 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Doc-1 | 0.58 | 0.58 | 0.58 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
- en: '| Doc-2 | 0.58 | 0.58 | 0 | 1.58 | 1.58 | 0.58 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Doc-2 | 0.58 | 0.58 | 0 | 1.58 | 1.58 | 0.58 | 0 | 0 | 0 | 0 | 0 |'
- en: '| Doc-3 | 0 | 0 | 0.58 | 0 | 0 | 0.58 | 1.58 | 1.58 | 1.58 | 1.58 | 1.58 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Doc-3 | 0 | 0 | 0.58 | 0 | 0 | 0.58 | 1.58 | 1.58 | 1.58 | 1.58 | 1.58 |'
- en: In Python, scikit-learn offers `TfidfVectorizer` for generating the TF-IDF DTM.
    Let's see in the upcoming section how to generate it using scikit-learn.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，scikit-learn 提供了 `TfidfVectorizer` 用于生成 TF-IDF DTM。我们将在接下来的章节中看到如何使用
    scikit-learn 来生成它。
- en: Sentiment analysis using text classification
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用文本分类进行情感分析
- en: A business or data analyst needs to understand customer feedback and reviews
    about a specific product. What did customers like or dislike? And how are sales
    going? As a business analyst, you need to analyze these things with reasonable
    accuracy and quantify customer reviews, feedback, opinions, and tweets to understand
    the target audience. Sentiment analysis extracts the core information from the
    text and provides people's perception of products, services, brands, and political
    and social topics. Sentiment analysis is used to understand customers' and people's
    mindset. It is not only used in marketing, we can also use it in politics, public
    administration, policy-making, information security, and research. It helps us
    to understand the polarity of people's feedback. Sentiment analysis also covers
    words, tone, and writing style.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一个业务或数据分析师需要了解客户对某一特定产品的反馈和评价。客户喜欢或不喜欢什么？销售情况如何？作为一名业务分析师，你需要以合理的准确性分析这些内容，并量化客户的评论、反馈、意见和推文，从而了解目标受众。情感分析提取文本中的核心信息，并提供人们对产品、服务、品牌以及政治和社会话题的看法。情感分析用于了解客户和人们的心态。它不仅应用于营销领域，我们还可以在政治、公共管理、政策制定、信息安全和研究中使用它。它帮助我们理解人们反馈的极性。情感分析还涵盖了词汇、语气和写作风格。
- en: 'Text classification can be one of the approaches used for sentiment analysis.
    It is a supervised method used to detect a class of web content, news articles,
    blogs, tweets, and sentiments. The classification has a huge number of applications,
    from marketing, finance, e-commerce, and security. First, we preprocess the text,
    then we find the features of the preprocessed text, and then we feed features
    and the labels to the machine learning algorithm to do the classification. The
    following diagram explains the full idea of sentiment analysis using text classification:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类可以作为情感分析的一种方法。它是一种监督学习方法，用于检测网页内容、新闻文章、博客、推文和情感类别。文本分类在营销、金融、电子商务和安全等领域有着广泛的应用。首先，我们对文本进行预处理，然后找出预处理文本的特征，再将这些特征和标签输入机器学习算法进行分类。以下图解展示了使用文本分类进行情感分析的完整过程：
- en: '![](img/b96ae002-40f6-434b-87d2-747be7d03ea9.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b96ae002-40f6-434b-87d2-747be7d03ea9.png)'
- en: Let's classify the sentiments for Amazon Alexa product reviews. We can get data
    from the Kaggle website ([https://www.kaggle.com/sid321axn/amazon-alexa-reviews](https://www.kaggle.com/sid321axn/amazon-alexa-reviews)).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对 Amazon Alexa 产品评论进行情感分类。我们可以从 Kaggle 网站获取数据（[https://www.kaggle.com/sid321axn/amazon-alexa-reviews](https://www.kaggle.com/sid321axn/amazon-alexa-reviews)）。
- en: The Alexa product reviews data is a tab-separated values file (TSV file). This
    data has five columns or attributes – **rating**, **date**, **variation**, **verified_reviews**,
    and **feedback**.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Alexa 产品评论数据是一个制表符分隔的值文件（TSV 文件）。该数据有五列或属性——**评分**，**日期**，**变体**，**已验证评论**，和
    **反馈**。
- en: The `rating` column indicates the user ratings for Alexa products. The date
    column is the date on which the review was given by the user. The `variation`
    column represents the product model name. `verified_reviews` has the actual user
    review about the product.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`评分` 列表示用户对 Alexa 产品的评分。日期列是用户给出评论的日期。`变体` 列表示产品型号。`已验证评论` 包含用户对产品的实际评论。'
- en: The rating denotes the rating given by each user to the product. The date is
    the date of the review, and variation describes the model name. `verified_reviews`
    contains the text review written by the user, and the feedback column represents
    the sentiment score, where 1 denotes positive and 0 denotes negative sentiment.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 评分表示每个用户对产品给出的评分。日期表示评论的日期，变体描述了型号名称。`已验证评论` 包含用户撰写的文本评论，反馈列表示情感分数，其中 1 表示正面情感，0
    表示负面情感。
- en: Classification using BoW
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BoW 进行分类
- en: 'In this subsection, we will perform sentiment analysis and text classification
    based on BoW. Here, a bag of words is generated using the `scikit-learn` library.
    Let''s see how we perform sentiment analysis using BoW features in the following
    steps:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将基于 BoW 执行情感分析和文本分类。这里，我们使用 `scikit-learn` 库生成词袋。接下来，让我们看看如何在以下步骤中使用
    BoW 特征执行情感分析：
- en: 'Load the dataset:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: 'The first step to build a machine learning model is to load the dataset. Let''s
    first read the data using the pandas `read_csv()` function:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 构建机器学习模型的第一步是加载数据集。让我们首先使用 pandas 的 `read_csv()` 函数读取数据：
- en: '[PRE40]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This results in the following output:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![](img/702b332e-734c-4789-be1e-23ba6ffcc4d9.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/702b332e-734c-4789-be1e-23ba6ffcc4d9.png)'
- en: 'In the preceding output dataframe, we have seen that the Alexa review dataset
    has five columns: **rating**, **date**, **variation**, **verified_reviews**, and
    **feedback**.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的输出数据框中，我们看到 Alexa 评论数据集有五个列：**评分**，**日期**，**变体**，**已验证评论**，和 **反馈**。
- en: Explore the dataset.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索数据集。
- en: 'Let''s plot the **feedback** column count to see how many positive and negative
    reviews the dataset has:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制 **反馈** 列的计数，以查看数据集中的正面和负面评论数量：
- en: '[PRE41]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This results in the following output:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![](img/f17a8862-f0c7-4b8f-ae9e-ffe9c85ee309.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f17a8862-f0c7-4b8f-ae9e-ffe9c85ee309.png)'
- en: In the preceding code, we drew the bar chart for the feedback column using the
    seaborn `countplot()` function. This function counts and plots the values of the
    **feedback** column. In this plot, we can observe that 2,900 reviews are positive
    and 250 reviews are negative feedback.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用 seaborn 的 `countplot()` 函数绘制了反馈列的条形图。该函数会计算并绘制 **反馈** 列的值。在此图中，我们可以看到
    2,900 条评论为正面评论，250 条评论为负面评论。
- en: 'Generating features using `CountVectorizer`:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `CountVectorizer` 生成特征：
- en: 'Let''s generate a BoW matrix for the customer reviews using scikit-learn''s
    `CountVectorizer`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 scikit-learn 的 `CountVectorizer` 为客户评论生成一个 BoW 矩阵：
- en: '[PRE42]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In the preceding code, we created a `RegexTokenizer` object with an input regular
    expression that removes the special characters and symbols. After this, the `CountVectorizer`
    object was created and performed the fit and transform operation on verified reviews.
    Here, `CountVectorizer` takes parameters such as `lowercase` for converting keywords
    into lowercase, `stop_words` for specifying a language-specific stopwords list,
    `ngram_range` for specifying the unigram, bigram, or trigram, and `tokenizer`
    is used to pass the `tokenizer` object. The `RegexTokenizer` object is passed
    to the `tokenizer` parameter. Finally, we called the `fit_transform()` function
    that converts text reviews into a DTM as per specified parameters.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们创建了一个`RegexTokenizer`对象，并使用输入的正则表达式来去除特殊字符和符号。之后，创建了`CountVectorizer`对象，并对已验证的评论执行了fit和transform操作。这里，`CountVectorizer`接受一些参数，如`lowercase`用于将关键字转换为小写，`stop_words`用于指定特定语言的停用词列表，`ngram_range`用于指定单词一元组、二元组或三元组，而`tokenizer`则用来传递`tokenizer`对象。`RegexTokenizer`对象被传递给`tokenizer`参数。最后，我们调用了`fit_transform()`函数，该函数根据指定的参数将文本评论转换为文档-词项矩阵（DTM）。
- en: 'Split train and test set:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分割训练集和测试集：
- en: 'Let''s split the feature set and target column into `feature_train`, `feature_test`,
    `target_train`, and `target_test` using `train_test_split()`. `train_test_split()`
    takes dependent, independent dataframes, `test_size` and `random_state`. Here,
    `test_size` will decide the ratio of the train-test split (that is, `test_size
    0.3` means 30% for the testing set and the remaining 70% will be the training
    set), and `random_state` is used as a seed value for reproducing the same data
    split each time. If `random_state` is `None`, then it will randomly split the
    records each time, which will give different performance measures:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`train_test_split()`将特征集和目标列分割为`feature_train`、`feature_test`、`target_train`和`target_test`。`train_test_split()`接受依赖和独立数据帧、`test_size`和`random_state`作为参数。这里，`test_size`决定了训练集与测试集的比例（即，`test_size
    0.3`表示30%的数据作为测试集，剩下的70%作为训练集），而`random_state`则用作种子值，以便每次能重复相同的数据分割。如果`random_state`为`None`，则会每次随机分割记录，这会导致不同的性能度量：
- en: '[PRE43]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In the preceding code, we are partitioning the feature set and target column
    into `feature_train`, `feature_test`, `target_train`, and `target_test` using
    the `train_test_split()` method.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`train_test_split()`方法将特征集和目标列分割为`feature_train`、`feature_test`、`target_train`和`target_test`。
- en: 'Classification Model Building using Logistic Regression:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用逻辑回归构建分类模型：
- en: 'In this section, we will build the logistic regression model to classify the
    review sentiments using BoW (or `CountVectorizer`). Let''s create the logistic
    regression model:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将构建逻辑回归模型，通过BoW（或`CountVectorizer`）对评论情感进行分类。让我们创建逻辑回归模型：
- en: '[PRE44]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In the preceding code, we imported `LogisticRegression` and created the `LogisticRegression`
    object. After creating the model object, we performed the `fit()` operation on
    the training data and `predict()` to forecast the sentiment for the test dataset.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们导入了`LogisticRegression`并创建了`LogisticRegression`对象。创建模型对象后，我们对训练数据执行了`fit()`操作，并使用`predict()`对测试数据集进行情感预测。
- en: 'Evaluate the Classification Model:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估分类模型：
- en: 'Let''s evaluate the classification model using the `metrics` class and its
    methods – `accuracy_score`, `precision_score`, and `recall_score`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`metrics`类及其方法——`accuracy_score`、`precision_score`和`recall_score`来评估分类模型：
- en: '[PRE45]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This results in the following output:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE46]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: In the preceding code, we have evaluated the model performance using accuracy,
    precision, recall, and f1-score using the `scikit-learn metrics` function. All
    the measures are greater than 94%, so we can say that our model is performing
    well and classifying both the sentiment levels with a good amount of precision
    and recall.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`scikit-learn metrics`函数评估了模型的性能，指标包括准确率、精确度、召回率和F1值。所有这些度量都大于94%，所以我们可以说我们的模型表现良好，能够准确分类情感级别，并且精确度和召回率都很高。
- en: Classification using TF-IDF
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TF-IDF进行分类
- en: 'In this subsection, we will perform sentiment analysis and text classification
    based on TF-IDF. Here, TF-IDF is generated using the `scikit-learn` library. Let''s
    see how we perform sentiment analysis using TF-IDF features using the following
    steps:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们将基于TF-IDF执行情感分析和文本分类。这里，TF-IDF是通过`scikit-learn`库生成的。让我们看看如何使用TF-IDF特征执行情感分析，具体步骤如下：
- en: 'Load the dataset:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: The first step for building a machine learning model is to load the dataset.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 构建机器学习模型的第一步是加载数据集。
- en: 'Let''s first read the data using the pandas `read_csv()` function:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用pandas的`read_csv()`函数读取数据：
- en: '[PRE47]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This results in the following output:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/1812f01f-3a8c-499f-850d-1379a72097e0.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1812f01f-3a8c-499f-850d-1379a72097e0.png)'
- en: 'In the preceding output dataframe, we have seen that the Alexa review dataset
    has five columns: **rating**, **date**, **variation**, **verified_reviews**, and
    **feedback**.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出数据帧中，我们看到Alexa评论数据集包含五列：**rating**、**date**、**variation**、**verified_reviews**和**feedback**。
- en: 'Feature generation using `TfidfVectorizer`:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`TfidfVectorizer`生成特征：
- en: 'Let''s generate a TF-IDF matrix for the customer reviews using scikit-learn''s
    `TfidfVectorizer`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用scikit-learn的`TfidfVectorizer`生成客户评论的TF-IDF矩阵：
- en: '[PRE48]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the preceding code, we created a `RegexTokenizer` object with an input regular
    expression that removes the special characters and symbols. After this, the `TfidfVectorizer`
    object was created and performed the fit and transform operation on verified reviews.
    Here, `TfidfVectorizer` takes parameters such as `lowercase` for converting keywords
    into lowercase, `stop_words` for a specified language-specific stopwords list,
    `ngram_range` for specifying the unigram, bigram, or trigram, and `tokenizer`
    is used to pass the `tokenizer` object. The `RegexTokenizer` object is passed
    to the `tokenizer` parameter. Finally, we called the `fit_transform()` function
    that converts text reviews into a DTM as per specified parameters.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们创建了一个`RegexTokenizer`对象，并使用一个输入的正则表达式来去除特殊字符和符号。之后，我们创建了`TfidfVectorizer`对象，并对已验证的评论进行了拟合和转换操作。在这里，`TfidfVectorizer`接受一些参数，比如`lowercase`，用于将关键词转换为小写；`stop_words`，指定语言特定的停用词列表；`ngram_range`，指定单词组（unigram）、双词组（bigram）或三词组（trigram）；`tokenizer`则用于传递`tokenizer`对象。`RegexTokenizer`对象被传递给`tokenizer`参数。最后，我们调用了`fit_transform()`函数，根据指定的参数将文本评论转换为文档-词项矩阵（DTM）。
- en: 'Split the training and testing datasets:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分割训练集和测试集：
- en: 'Let''s split the feature set and target column into `feature_train`, `feature_test`,
    `target_train`, and `target_test` using `train_test_split()`. `train_test_split()`
    takes dependent, independent dataframes, `test_size` and `random_state`. Let''s
    split the dataset into a training and testing set:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`train_test_split()`将特征集和目标列分割成`feature_train`、`feature_test`、`target_train`和`target_test`。`train_test_split()`接受依赖变量和独立数据帧、`test_size`和`random_state`作为参数。让我们将数据集分割为训练集和测试集：
- en: '[PRE49]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In the preceding code, we partition the feature set and target column into `feature_train`,
    `feature_test`, `target_train`, and `target_test` using the `train_test_split()`
    method.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`train_test_split()`方法将特征集和目标列分割成`feature_train`、`feature_test`、`target_train`和`target_test`。
- en: 'Classification model building using logistic regression:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用逻辑回归构建分类模型：
- en: 'In this section, we will build the logistic regression model to classify the
    review sentiments using TF-IDF. Let''s create the logistic regression model:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建逻辑回归模型，以使用TF-IDF对评论情感进行分类。让我们创建逻辑回归模型：
- en: '[PRE50]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In the preceding code, we imported `LogisticRegression` and created the `LogisticRegression`
    object. After creating the model object, we performed a `fit()` operation on the
    training data and `predict()` to forecast the sentiment for the test dataset.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们导入了`LogisticRegression`并创建了`LogisticRegression`对象。在创建模型对象之后，我们对训练数据进行了`fit()`操作，并使用`predict()`对测试数据集进行情感预测。
- en: 'Evaluate the classification model:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估分类模型：
- en: 'Let''s evaluate the classification model using the `metrics` class and its
    methods – `accuracy_score`, `precision_score`, and `recall_score`:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`metrics`类及其方法——`accuracy_score`、`precision_score`和`recall_score`来评估分类模型：
- en: '[PRE51]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This results in the following output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE52]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: In the preceding code, we evaluated the model performance using accuracy, precision,
    recall, and f1-score using the scikit-learn `metrics` function. All the measures
    are greater than 94%, so we can say that our model is performing well and classifying
    both sentiment levels with a good amount of precision and recall. In this section,
    we have looked at sentiment analysis using text classification. Text classification
    is performed using BoW and TF-IDF features. In the next section, we will learn
    how to find similarities between two pieces of text, such as sentences or paragraphs.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用scikit-learn的`metrics`函数，通过准确率、精确率、召回率和F1分数评估了模型的性能。所有指标都大于94%，因此我们可以说我们的模型表现良好，并且能够以较好的精度和召回率对两种情感类别进行分类。在本节中，我们了解了如何通过文本分类进行情感分析。文本分类使用了BoW和TF-IDF特征。在下一节中，我们将学习如何找出两段文本之间的相似性，比如句子或段落。
- en: Text similarity
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本相似度
- en: Text similarity is the process of determining the two closest texts. Text similarity
    is very helpful in finding similar documents, questions, and queries. For example,
    a search engine such as Google uses similarity to find document relevance, and
    Q&A systems such as StackOverflow or a consumer service system use similar questions.
    There are two common metrics used for text similarity, namely Jaccard and cosine
    similarity.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 文本相似度是确定两个最接近文本的过程。文本相似度对于查找相似文档、问题和查询非常有帮助。例如，像 Google 这样的搜索引擎使用相似度来查找文档的相关性，而像
    StackOverflow 这样的问答系统或客户服务系统则使用类似问题。文本相似度通常使用两种度量标准，即 Jaccard 相似度和余弦相似度。
- en: 'We can also use the similarity method available in spaCy. The `nlp` object''s
    `similarity` method returns a score between two sentences. Let''s look at the
    following example:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 spaCy 中可用的相似度方法。`nlp` 对象的 `similarity` 方法返回两个句子之间的分数。我们来看以下示例：
- en: '[PRE53]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This results in the following output:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE54]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'In the preceding code block, we have found the similarity between two sentences
    using spaCy''s `similarity()` function. Spacy''s similarity function does not
    give better results with small models (such as the `en_core_web_sm` and `en` models);
    that''s why you will get a warning: **UserWarning: [W007]****.** To remove this
    warning, use larger models such as `en_core_web_lg`.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '在前面的代码块中，我们使用 spaCy 的 `similarity()` 函数计算了两个句子之间的相似度。spaCy 的相似度函数在使用较小模型（如
    `en_core_web_sm` 和 `en` 模型）时效果不佳；因此你会收到一个警告：**UserWarning: [W007]**。要去除此警告，请使用更大的模型，如
    `en_core_web_lg`。'
- en: Jaccard similarity
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Jaccard 相似度
- en: 'Jaccard similarity calculates the similarity between two sets by the ratio
    of common words (intersection) to totally unique words (union) in both sets. It
    takes a list of unique words in each sentence or document. It is useful where
    the repetition of words does not matter. Jaccard similarity ranges from 0-100%;
    the higher the percentage, the more similar the two populations:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard 相似度通过两个集合中共同单词（交集）与完全独特单词（并集）的比例来计算相似度。它会列出每个句子或文档中的唯一单词列表。这个方法适用于单词重复不重要的场景。Jaccard
    相似度的范围是 0-100%；百分比越高，两个集合的相似度越大：
- en: '![](img/ac41da11-47fd-4a9e-9a21-f0a2c3abbea0.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac41da11-47fd-4a9e-9a21-f0a2c3abbea0.png)'
- en: 'Let''s look at a Jaccard similarity example:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个 Jaccard 相似度的示例：
- en: '[PRE55]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This results in the following output:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE56]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: In the preceding example, we have created a function, `jaccard_similarity()`,
    which takes two arguments, `sent1` and `sent2`. It will find the ratio between
    the intersection of keywords and the union of keywords between two sentences.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，我们创建了一个函数 `jaccard_similarity()`，该函数接受两个参数 `sent1` 和 `sent2`。它将计算两个句子中关键词的交集与并集之间的比例。
- en: Cosine similarity
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: Cosine similarity computes the cosine of the angle between two multidimensional
    projected vectors. It indicates how two documents are related to each other. Two
    vectors can be made of the bag of words or TF-IDF or any equivalent vector of
    the document. It is useful where the duplication of words matters. Cosine similarity
    can measure text similarity irrespective of the size of documents.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度计算两个多维投影向量之间的夹角余弦值。它表示两个文档之间的关系。两个向量可以由词袋模型、TF-IDF 或任何等效的文档向量组成。它适用于单词重复重要的情况。余弦相似度可以测量文本相似度，而不考虑文档的大小。
- en: '![](img/d115797f-04b8-49eb-99bb-9b7800a97ef5.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d115797f-04b8-49eb-99bb-9b7800a97ef5.png)'
- en: 'Let''s look at a cosine similarity example:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个余弦相似度的示例：
- en: '[PRE57]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This results in the following output:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE58]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: In the preceding example, first, we import `TfidfVectorizer` and generate the
    TF-IDF vector for given documents. After this, we apply the `cosine_similarity()`
    metric on the document list and get similarity metrics.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，首先我们导入 `TfidfVectorizer` 并生成给定文档的 TF-IDF 向量。之后，我们应用 `cosine_similarity()`
    度量标准对文档列表进行相似度计算。
- en: Summary
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored text analysis using NLTK and spaCy. The main focus
    was on text preprocessing, sentiment analysis, and text similarity. The chapter
    started with text preprocessing tasks such as text normalization, tokenization,
    removing stopwords, stemming, and lemmatization. We also focused on how to create
    a word cloud, recognize entities in a given text, and find dependencies among
    tokens. In later sections, we focused on BoW, TFIDF, sentiment analysis, and text
    classification.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了使用NLTK和spaCy进行文本分析。主要内容包括文本预处理、情感分析和文本相似度。章节从文本预处理任务入手，如文本标准化、分词、去除停用词、词干提取和词形还原。我们还重点讲解了如何创建词云、识别给定文本中的实体并找到词元之间的依赖关系。在后续部分，我们将重点讨论BoW、TFIDF、情感分析和文本分类。
- en: The next chapter, Chapter 13, *Analyzing Image Data*, focuses on image processing,
    basic image processing operations, and face detection using OpenCV. The chapter
    starts with image color models, and image operations such as drawing on an image,
    resizing an image, and flipping and blurring an image. In later sections, the
    focus will be on face detection in a given input image.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章，第13章，*图像数据分析*，主要讲解图像处理、基本图像处理操作以及使用OpenCV进行人脸检测。该章节从图像颜色模型开始，并介绍了图像操作，如在图像上绘制、调整图像大小、翻转和模糊图像。在后续部分，将重点讨论如何在人脸检测输入图像中进行分析。
