- en: 'Chapter 8: Unsupervised Machine Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章：无监督机器学习
- en: In the previous two chapters, you were introduced to the supervised learning
    class of machine learning algorithms, their real-world applications, and how to
    implement them at scale using Spark MLlib. In this chapter, you will be introduced
    to the unsupervised learning category of machine learning, where you will learn
    about parametric and non-parametric unsupervised algorithms. A few real-world
    applications of **clustering** and **association** algorithms will be presented
    to help you understand the applications of **unsupervised learning** to solve
    real-life problems. You will gain basic knowledge and understanding of clustering
    and association problems when using unsupervised machine learning. We will also
    look at the implementation details of a few clustering algorithms in Spark ML,
    such as **K-means clustering**, **hierarchical clustering**, **latent Dirichlet
    allocation**, and an association algorithm called **alternating least squares**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，你已经了解了监督学习类的机器学习算法、它们的实际应用，以及如何使用 Spark MLlib 在大规模环境中实现它们。在本章中，你将接触到无监督学习类别的机器学习，学习有关参数化和非参数化的无监督算法。为了帮助你理解
    **无监督学习** 在解决实际问题中的应用，我们将介绍一些 **聚类** 和 **关联** 算法的实际应用。你将获得使用无监督机器学习进行聚类和关联问题的基本知识和理解。我们还将讨论在
    Spark ML 中实现一些聚类算法的细节，如 **K-means 聚类**、**层次聚类**、**潜在狄利克雷分配**，以及一种叫做 **交替最小二乘法**
    的关联算法。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主要内容：
- en: Introduction to unsupervised learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习简介
- en: Clustering using machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习进行聚类
- en: Building association using machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习建立关联
- en: Real-world applications of unsupervised learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习的实际应用
- en: By the end of this chapter, you should have gained sufficient knowledge and
    practical understanding of the clustering and association types of unsupervised
    machine learning algorithms, their practical applications, and skills to implement
    these types of algorithms at scale using Spark MLlib.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该已经获得了足够的知识和实践经验，了解聚类和关联类型的无监督机器学习算法、它们的实际应用，并能够使用 Spark MLlib 在大规模环境中实现这些类型的算法。
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will be using Databricks Community Edition to run our code
    ([https://community.cloud.databricks.com](https://community.cloud.databricks.com)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 Databricks 社区版来运行我们的代码（[https://community.cloud.databricks.com](https://community.cloud.databricks.com)）。
- en: Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册说明可以在[https://databricks.com/try-databricks](https://databricks.com/try-databricks)找到。
- en: The code for this chapter can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter08](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter08).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的代码可以从[https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter08](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter08)下载。
- en: The datasets for this chapter can be found at [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的datasets可以在[https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data)找到。
- en: Introduction to unsupervised machine learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督机器学习简介
- en: Unsupervised learning is a machine learning technique where no guidance is available
    to the learning algorithm in the form of known label values in the training data.
    Unsupervised learning is useful in categorizing unknown data points into groups
    based on patterns, similarities, or differences that are inherent within the data,
    without any prior knowledge of the data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是一种机器学习技术，在这种技术中，学习算法在训练数据中没有已知标签值的指导。无监督学习在根据数据中固有的模式、相似性或差异将未知数据点分组时非常有用，而无需任何先验数据知识。
- en: In supervised learning, a model is trained on known data, and then inferences
    are drawn from the model using new, unseen data. On the other hand, in unsupervised
    learning, the model training process in itself is the end goal, where patterns
    hidden within the training data are discovered during the model training process.
    Unsupervised learning is harder compared to supervised learning since it is difficult
    to ascertain if the results of an unsupervised learning algorithm are meaningful
    without any external evaluation, especially without access to any correctly labeled
    data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，模型是在已知数据上进行训练的，然后使用新数据（未见过的数据）从模型中推断结论。另一方面，在无监督学习中，模型训练过程本身就是最终目标，在模型训练过程中会发现隐藏在训练数据中的模式。与监督学习相比，无监督学习更难，因为在没有任何外部评估的情况下，很难确定无监督学习算法的结果是否有意义，特别是当无法访问任何正确标记的数据时。
- en: One of the advantages of unsupervised learning is that it helps interpret very
    large datasets where labeling existing data would not be practical. Unsupervised
    learning is also useful for tasks such as predicting the number of classes within
    a dataset, or grouping and clustering data before applying a supervised learning
    algorithm. It is also very useful in solving classification problems as unsupervised
    learning can work well with unlabelled data, and there is no need for any manual
    intervention either. Unsupervised learning can be classified into two major learning
    techniques, known as clustering and association. These will be presented in the
    following sections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的一个优势是，它有助于解释非常大的数据集，在这些数据集上标注现有数据并不现实。无监督学习还可用于预测数据集中类别的数量，或在应用监督学习算法之前对数据进行分组和聚类。它在解决分类问题时也非常有用，因为无监督学习可以很好地处理未标记的数据，且无需任何人工干预。无监督学习可以分为两种主要的学习技术，分别是聚类和关联。接下来的部分将介绍这两种方法。
- en: Clustering using machine learning
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用机器学习进行聚类
- en: 'In machine learning, clustering deals with identifying patterns or structures
    within uncategorized data without needing any external guidance. Clustering algorithms
    parse given data to identify clusters or groups with matching patterns that exist
    in the dataset. The result of clustering algorithms are clusters of data that
    can be defined as a collection of objects that are similar in a certain way. The
    following diagram illustrates how clustering works:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，聚类是指在不需要任何外部指导的情况下识别未分类数据中的模式或结构。聚类算法会解析给定的数据，以识别数据集中具有匹配模式的簇或数据组。聚类算法的结果是数据簇，这些簇可以定义为在某种方式上相似的对象集合。下图展示了聚类是如何工作的：
- en: '![Figure 8.1 – Clustering'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.1 – 聚类'
- en: '](img/B16736_08_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_08_01.jpg)'
- en: Figure 8.1 – Clustering
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 聚类
- en: In the previous diagram, an uncategorized dataset is being passed through a
    clustering algorithm, resulting in the data being categorized into smaller clusters
    or groups of data, based on a data point's proximity to another data point in
    a two-dimensional Euclidian space.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，一个未分类的数据集通过聚类算法进行处理，结果是根据数据点与另一个数据点在二维欧几里得空间中的接近程度，将数据分类成较小的簇或数据组。
- en: Thus, the clustering algorithm groups data based on the Euclidean distance between
    the data on a two-dimensional plane. Clustering algorithms consider the Euclidean
    distance between data points in the training dataset in that, within a cluster,
    the distance between the data points should be small, while outside the cluster,
    the distance between the data points should be large. A few types of clustering
    techniques that are available in Spark MLlib will be presented in the following
    sections.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，聚类算法基于二维平面上数据之间的欧几里得距离进行数据分组。聚类算法会考虑训练数据集中数据点之间的欧几里得距离，在同一簇内，数据点之间的距离应当较小，而簇外，数据点之间的距离应当较大。接下来的部分将介绍
    Spark MLlib 中几种可用的聚类技术。
- en: K-means clustering
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means 聚类
- en: '**K-means** is the most popular clustering algorithm and one of the simplest
    of the unsupervised learning algorithms as well. The K-means clustering algorithm
    works iteratively on the provided dataset to categorize it into *k* groups. The
    larger the value of *k*, the smaller the size of the clusters, and vice versa.
    Thus, with K-means, the user can control the number of clusters that are identified
    within the given dataset.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-means** 是最流行的聚类算法，也是最简单的无监督学习算法之一。K-means 聚类算法在给定数据集上以迭代的方式工作，将其分为 *k*
    组。*k* 的值越大，聚类的大小越小，反之亦然。因此，使用 K-means，用户可以控制在给定数据集内识别的聚类数量。'
- en: In K-means clustering, each cluster is defined by creating a center for each
    cluster. These centroids are placed as far away as possible from each other. Then,
    K-means associates each data point with the given dataset to its nearest centroid,
    thus forming the first group of clusters. K-means then iteratively recalculates
    the centroids' position within the dataset so that it's as close to the center
    of the identified clusters. This process stops when the centroids don't need to
    be moved anymore.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 K-means 聚类中，每个簇都是通过为每个簇创建一个中心来定义的。这些质心尽可能远离彼此。然后，K-means 将每个数据点与给定数据集中的最近质心进行关联，从而形成第一个簇。K-means
    随后会迭代地重新计算质心在数据集中的位置，使其尽可能接近已识别簇的中心。当质心不再需要移动时，过程停止。
- en: 'The following code block illustrates how to implement K-means clustering using
    Spark MLlib:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块演示了如何使用 Spark MLlib 实现 K-means 聚类：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the preceding code snippet, we did the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们进行了以下操作：
- en: First, we used `import` to import the appropriate MLlib packages related to
    clustering and clustering evaluation.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用 `import` 导入了与聚类和聚类评估相关的适当 MLlib 包。
- en: Then, we imported the already existing feature vector that we derived during
    the feature engineering process into a Spark DataFrame and stored it in the data
    lake in Delta format.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将之前在特征工程过程中获得的现有特征向量导入到 Spark DataFrame 中，并以 Delta 格式将其存储在数据湖中。
- en: Next, a new `KMeans` object was initialized by us passing in the number of desired
    clusters and the column name for the feature vector.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们初始化了一个新的 `KMeans` 对象，通过传入所需的聚类数和特征向量的列名来进行设置。
- en: The `fit()` method was called on the training DataFrame to kick off the learning
    process. A model object was generated as a result.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在训练 DataFrame 上调用了 `fit()` 方法，启动了学习过程。结果生成了一个模型对象。
- en: Predictions on the original training dataset were generated by calling the `transform()`
    method on the model object.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用模型对象的 `transform()` 方法，生成了原始训练数据集上的预测结果。
- en: Next, we invoked Spark MLlib's `ClusteringEvaluator()` helper function, which
    is useful for evaluating clustering algorithms, and applied it to the predictions
    DataFrame we generated in the previous step. This resulted in a value referred
    to as `silhouette`, which is a measure of consistency within clusters and is calculated
    based on the Euclidean distance measure between data points. A `silhouette` value
    closer to `1` means that the points within a cluster are close together and that
    points outside the cluster are far apart. The closer the `silhouette` value is
    to `1`, the more performant the learned model is.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们调用了 Spark MLlib 的 `ClusteringEvaluator()` 辅助函数，该函数用于评估聚类算法，并将其应用于我们在前一步生成的预测
    DataFrame。这会得到一个被称为 `silhouette` 的值，它是衡量聚类一致性的指标，计算方式基于数据点之间的欧氏距离度量。`silhouette`
    值越接近 `1`，表示簇内的数据点越聚集，簇外的数据点则相距较远。`silhouette` 值越接近 `1`，表示学习到的模型性能越好。
- en: Finally, we printed the centroids of each of the categorized clusters.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印了每个分类簇的质心。
- en: This way, using just a few lines of code, uncategorized data can easily be clustered
    using Spark's implementation of the K-means clustering algorithm.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，只需几行代码，就可以通过 Spark 实现的 K-means 聚类算法轻松地将未分类数据进行聚类。
- en: Hierarchical clustering using bisecting K-means
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用二分 K-means 的层次聚类
- en: '**Hierarchical clustering** is a type of clustering technique where all the
    data points start within a single cluster. They are then recursively split into
    smaller clusters by moving them down a hierarchy. Spark ML implements this kind
    of divisive hierarchical clustering via the bisecting K-means algorithm. The following
    example illustrates how to implement bisecting K-means clustering using Spark
    MLlib:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**层次聚类**是一种聚类技术，其中所有数据点从一个单一的聚类开始。然后它们通过递归地向下分割到更小的聚类中，从而构成一个层次结构。Spark ML通过二分K均值算法实现这种分裂式的层次聚类。以下示例说明了如何使用Spark
    MLlib实现二分K均值聚类：'
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the previous code snippet, we did the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，我们执行了以下操作：
- en: First, we initialized a new `BisectingKMeans` object by passing in the number
    of desired clusters and the column name for the feature column.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们通过传入所需聚类的数量和特征列的列名来初始化一个新的`BisectingKMeans`对象。
- en: The `fit()` method was called on the training DataFrame to start the learning
    process. A model object was generated as a result.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在训练数据集上调用了`fit()`方法以开始学习过程。作为结果，生成了一个模型对象。
- en: Next, predictions on the original training dataset were generated by calling
    the `transform()` method on the model object.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过在模型对象上调用`transform()`方法，对原始训练数据集生成了预测结果。
- en: After, we invoked Spark MLlib's `ClusteringEvaluator()` helper function, which
    is useful for evaluating clustering algorithms, and applied it to the predictions
    DataFrame we generated in the previous step. This results in the `silhouette`
    value, which is a measure of consistency within clusters and is calculated based
    on the Euclidean distance measure between data points.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们调用了Spark MLlib的`ClusteringEvaluator()`辅助函数，该函数对于评估聚类算法非常有用，并将其应用于我们在前一步生成的预测数据框。这将得到`silhouette`值，它是衡量聚类内一致性的指标，基于数据点之间的欧几里得距离计算。
- en: Finally, we printed the centroids of each of the clusters.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印了每个聚类的中心点。
- en: Now that we have learned a clustering technique, let's find out about a learning
    technique in the next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了一种聚类技术，让我们在下一节中了解另一种学习技术。
- en: Topic modeling using latent Dirichlet allocation
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用潜在狄利克雷分配进行主题建模
- en: '**Topic modeling** is a learning technique where you categorize documents.
    Topic modeling is not the same as topic classification since topic classification
    is a supervised learning technique where the learning model tries to classify
    unseen documents based on some previously labeled data. On the other hand, topic
    modeling categorizes documents containing text or natural language in the same
    way as clustering groups categorize numeric data without any external guidance.
    Thus, topic modeling is an unsupervised learning problem.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题建模**是一种学习技术，通过它你可以对文档进行分类。主题建模与主题分类不同，因为主题分类是一种有监督学习技术，其中学习模型尝试根据一些先前标注的数据对未见过的文档进行分类。而主题建模则像聚类算法对数值数据进行分组一样，在没有任何外部指导的情况下对包含文本或自然语言的文档进行分类。因此，主题建模是一个无监督学习问题。'
- en: '**Latent Dirichlet allocation** (**LDA**) is a popular topic modeling technique.
    The goal of LDA is to associate a given document with a particular topic based
    on the keywords found within the document. Here, the topics are unknown and hidden
    within the documents, thus the latent part of LDA. LDA works by assuming each
    word within a document belongs to a different topic and assigns a probability
    score to each word. Once the probability of each word belonging to a particular
    topic is estimated, LDA tries to pick all the words belonging to a topic by setting
    a threshold and choosing every word that meets or exceeds that threshold value.
    LDA also considers each document to be just a bag of words, without placing any
    importance on the grammatical role played by the individual words. Also, stop
    words in a language such as articles, conjunctions, and interjections need to
    be removed before LDA is applied as these words do not carry any topic information.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）是一种流行的主题建模技术。LDA的目标是根据文档中发现的关键词将给定文档与特定主题关联起来。在这里，主题是未知的，隐藏在文档中，这就是LDA中的潜在部分。LDA的工作方式是，假设文档中的每个单词都属于一个不同的主题，并为每个单词分配一个概率值。一旦估算出每个单词属于特定主题的概率，LDA就会通过设置阈值并选择所有符合或超过该阈值的单词来挑选属于某个主题的所有单词。LDA还认为每个文档只是一个词袋，并不重视单个单词在语法中的角色。此外，像文章、连词和感叹词等停用词需要在应用LDA之前被去除，因为这些词并不携带任何主题信息。'
- en: 'The following code example illustrates how LDA can be implemented using Spark
    MLlib:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例说明了如何使用Spark MLlib实现LDA：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the previous code snippet, we did the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们做了以下操作：
- en: First, we imported the appropriate MLlib packages related to LDA.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入了与LDA相关的适当MLlib包。
- en: Next, we imported the already existing feature vector that had been derived
    during the feature engineering process into a Spark DataFrame and stored it in
    the data lake in Delta format.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将特征工程过程中生成的现有特征向量导入到Spark DataFrame中，并以Delta格式将其存储在数据湖中。
- en: After, we initialized a new `LDA` object by passing in the number of clusters
    and the maximum number of iterations.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们通过传入聚类的数量和最大迭代次数来初始化一个新的`LDA`对象。
- en: Next, the `fit()` method was called on the training DataFrame to start the learning
    process. A model object was generated as a result.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在训练DataFrame上调用了`fit()`方法，开始了学习过程。最终生成了一个模型对象。
- en: The topics that were modeled by the LDA algorithm can be shown by using the
    `describeTopics()` method on the model object.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用LDA算法建模的主题可以通过在模型对象上使用`describeTopics()`方法来显示。
- en: As we have seen, by using Apache Spark's implementation of the LDA algorithm,
    topic modeling can be implemented at scale.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，通过使用Apache Spark实现的LDA算法，主题建模可以在大规模上实现。
- en: Gaussian mixture model
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: One of the disadvantages of K-means clustering is that it will associate every
    data point with exactly one cluster. This way, it is not possible to get the probability
    of a data point belonging to a particular cluster. The **Gaussian mixture model**
    (**GSM**) attempts to solve this hard clustering problem of K-means clustering.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类的一个缺点是它会将每个数据点与一个特定的聚类关联起来。这样，就无法获得数据点属于某一特定聚类的概率。**高斯混合模型**（**GSM**）尝试解决K-means聚类的硬聚类问题。
- en: GSM is a probabilistic model for representing a subset of a sample within an
    overall sample of data points. A GSM represents a mixture of several Gaussian
    distributions of data points, where a data point is drawn from one of the *K*
    Gaussian distributions and has a probability score of it belonging to one of those
    distributions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: GSM是一个概率模型，用于表示数据点总体样本中的一个子集。GSM表示多个高斯分布的数据点的混合，其中数据点从其中一个*K*高斯分布中抽取，并具有属于这些分布之一的概率得分。
- en: 'The following code example describes the implementation details of a GSM using
    Spark ML:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例描述了使用Spark ML实现GSM的实现细节：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding code block, we initialized a new `GaussianMixture` object after
    importing the appropriate libraries from the `pyspark.ml.clustering` package.
    Then, we passed in some hyperparameters, including the number of clusters and
    the name of the column containing the feature vector. Then, we trained the model
    using the `fit()` method and displayed the results of the trained model using
    the model's `gaussianDF` attribute.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们在从`pyspark.ml.clustering`包中导入适当的库后，初始化了一个新的`GaussianMixture`对象。然后，我们传入了一些超参数，包括聚类的数量和包含特征向量的列的名称。接着，我们使用`fit()`方法训练了模型，并通过模型的`gaussianDF`属性显示了训练后的结果。
- en: So far, you have seen different kinds of clustering and topic modeling techniques
    and their implementations when using Spark MLlib. In the following section, you
    will learn about another type of unsupervised learning algorithm called **association
    rules**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到了不同类型的聚类和主题建模技术及其在使用Spark MLlib时的实现。在接下来的部分中，您将了解另一种叫做**关联规则**的无监督学习算法。
- en: Building association rules using machine learning
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用机器学习构建关联规则
- en: '`if-then-else` statements that help show the probability of relationships between
    entities. The association rules technique is widely used in recommender systems,
    market basket analysis, and affinity analysis problems.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`if-then-else`语句有助于展示实体之间关系的概率。关联规则技术广泛应用于推荐系统、市场篮子分析和亲和力分析等问题。'
- en: Collaborative filtering using alternating least squares
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用交替最小二乘法进行协同过滤
- en: In machine learning, **collaborative filtering** is more commonly used for **recommender
    systems**. A recommender system is a technique that's used to filter information
    by considering user preference. Based on user preference and taking into consideration
    their past behavior, recommender systems can make predictions on items that the
    user might like. Collaborative filtering performs information filtering by making
    use of historical user behavior data and their preferences to build a user-item
    association matrix. Spark ML uses the **alternating least squares** algorithm
    to implement the collaborative filtering technique.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，**协同过滤**更常用于 **推荐系统**。推荐系统是一种通过考虑用户偏好来过滤信息的技术。根据用户的偏好并考虑他们的过去行为，推荐系统可以预测用户可能喜欢的项。协同过滤通过利用历史用户行为数据和他们的偏好，构建用户-项关联矩阵来执行信息过滤。Spark
    ML 使用 **交替最小二乘法** 算法实现协同过滤技术。
- en: 'The following code example demonstrates Spark MLlib''s implementation of the
    alternating least squares algorithm:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例演示了 Spark MLlib 中交替最小二乘算法的实现：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the previous code block, we did the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们做了以下操作：
- en: First, we generated the ratings dataset as a Spark DataFrame using the feature
    dataset stored in the Delta Lake. A few of the columns that the ALS algorithm
    required, such as `user_id`, `item_id`, and `ratings` were not in the required
    integer format. Thus, we used the `CAST` Spark SQL method to convert them into
    the required data format.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用存储在 Delta Lake 中的特征数据集生成了一个 Spark DataFrame 作为评分数据集。ALS 算法所需的几个列，如 `user_id`、`item_id`
    和 `ratings`，并未采用所需的整数格式。因此，我们使用了 `CAST` Spark SQL 方法将其转换为所需的数据格式。
- en: Next, we initialized an ALS object with the desired parameters and split our
    training dataset into two random parts using the `randomSplit()` method.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用所需的参数初始化了一个 ALS 对象，并通过 `randomSplit()` 方法将训练数据集随机分成了两部分。
- en: After, we started the learning process by calling the `fit()` method on the
    training dataset.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过在训练数据集上调用 `fit()` 方法启动了学习过程。
- en: Then, we evaluated the accuracy metric's `RMSE` using the evaluator provided
    by Spark MLlib.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用 Spark MLlib 提供的评估器评估了准确度指标的 `RMSE`。
- en: Finally, we gathered the predictions for the top 5 item recommendations for
    each user and the top 5 user recommendations per item using the built-in `recommendForAllUsers()`
    and `recommendForAllItems()` methods, respectively.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过内置的 `recommendForAllUsers()` 和 `recommendForAllItems()` 方法分别收集了每个用户的前
    5 个推荐项和每个项的前 5 个用户推荐。
- en: This way, you can leverage alternating least squares to build recommender systems
    for use cases such as movie recommendations for a **video on demand** platform,
    product recommendations, or **market basket analysis** for an e-tailer application.
    Spark MLlib helps you implement this scale with only a few lines of code.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，你可以利用交替最小二乘法构建推荐系统，用于诸如 **视频点播** 平台的电影推荐、产品推荐或电子商务应用中的 **市场篮分析** 等用例。Spark
    MLlib 使你能够只用几行代码就实现这一规模。
- en: In addition to clustering and association rules, Spark MLlib also allows you
    to implement **dimensionality reduction** algorithms such as **singular value
    decomposition** (**SVD**) and **principal component analysis** (**PCA**). Dimensionality
    reduction is the process of reducing the number of random variables under consideration.
    Though an unsupervised learning method, dimensionality reduction is useful for
    feature extraction and selection. A detailed discussion of this topic is beyond
    the scope of this book, and Spark MLlib only has the dimensionality reduction
    algorithm's implementation available for the RDD API. More details on dimensionality
    reduction can be found in Apache Spark's public documentation at [https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html](https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 除了聚类和关联规则外，Spark MLlib 还允许你实现 **降维** 算法，如 **奇异值分解**（**SVD**）和 **主成分分析**（**PCA**）。降维是减少考虑的随机变量数量的过程。尽管它是一种无监督学习方法，但降维在特征提取和选择中非常有用。关于这一主题的详细讨论超出了本书的范围，Spark
    MLlib 仅为 RDD API 提供了降维算法的实现。有关降维的更多详细信息，可以在 Apache Spark 的公共文档中找到，网址为 [https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html](https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html)。
- en: In the next section, we will delve into a few more real-life applications of
    unsupervised learning algorithms that are in use today by various businesses.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入探讨一些当前各大企业正在使用的无监督学习算法的实际应用。
- en: Real-world applications of unsupervised learning
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习的实际应用
- en: Unsupervised learning algorithms are being used today to solve some real-world
    business challenges. We will take a look at a few such challenges in this section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法如今正被用来解决一些现实世界中的商业挑战。在本节中，我们将探讨几个这样的挑战。
- en: Clustering applications
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类应用
- en: This section presents some of the real-world business applications of clustering
    algorithms.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一些聚类算法在实际商业中的应用。
- en: Customer segmentation
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户细分
- en: Retail marketing teams, as well as business-to-customer organizations, are always
    trying to optimize their marketing spends. Marketing teams in particular are concerned
    with one specific metric called **cost per acquisition** (**CPA**). CPA is indicative
    of the amount that an organization needs to spend to acquire a single customer,
    and an optimal CPA means a better return on marketing investments. The best way
    to optimize CPA is via customer segmentation as this improves the effectiveness
    of marketing campaigns. Traditional customer segmentation takes standard customer
    features such as demographic, geographic, and social information into consideration,
    along with historical transactional data, to define standard customer segments.
    This traditional way of customer segmentation is time-consuming and involves a
    lot of manual work and is prone to errors. However, machine learning algorithms
    can be leveraged to find hidden patterns and associations among data sources.
    Also, in recent years. the number of customer touchpoints has increased, and it
    is not practical and intuitive to identify patterns among all those customer touchpoints
    to identify patterns manually. However, machine learning algorithms can easily
    parse through millions of records and surface insights that can be leveraged promptly
    by marketing teams to meet their customers where they want, when they want. Thus,
    by leveraging clustering algorithms, marketers can improve the efficacy of their
    marketing campaigns via refined customer segmentation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 零售营销团队以及面向消费者的企业组织，始终致力于优化其营销支出。尤其是营销团队，他们关注一个特定的指标——**每次获取成本**（**CPA**）。CPA表示组织需要花费多少才能获得一个客户，最优的CPA意味着更好的营销投资回报。优化CPA的最佳方式是通过客户细分，因为这能提高营销活动的效果。传统的客户细分会考虑标准的客户特征，如人口统计、地理位置和社会信息，以及历史交易数据，从而定义标准的客户群体。然而，传统的客户细分方法费时费力，且容易出错。与此同时，可以利用机器学习算法发现数据源之间隐藏的模式和关联。近年来，客户接触点的数量大幅增加，手动识别所有这些接触点之间的模式变得不实际且不直观。然而，机器学习算法能够轻松地分析数百万条记录，提取出营销团队可以迅速利用的见解，从而满足客户的需求，在客户需要的时候、他们想要的地方。因此，通过利用聚类算法，营销人员可以通过更加精准的客户细分，提高营销活动的有效性。
- en: Retail assortment optimization
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零售产品组合优化
- en: Retailers with brick-and-mortar stores have limited store space. Thus, they
    need to ensure that their store space is utilized optimally by placing only those
    products that are highly likely to sell. A classic example of assortment optimization
    is that of a hardware retailer, stocking up on lawnmowers during the deep winter
    season in the midwestern parts of the United States, when it is highly likely
    to snow through the season. In this example, the store space is being sub-optimally
    utilized by having lawnmowers in there, which have a very small chance of selling
    during the snow season. A better choice would have been space heaters, snow shovels,
    or other winter season equipment. To overcome this problem, retailers usually
    employ analysts, who take historical transactional data, seasonality, and current
    trends into consideration to make recommendations on the optimal assortment of
    products that are appropriate for the season and location of the store. However,
    what if we increase the scale of this problem to a much larger retailer, with
    thousands of warehouses and tens of thousands of retail outlets? At such a scale,
    manually planning optimal assortments of products becomes impractical and very
    time-consuming, reducing the time to value drastically. Assortment optimization
    can be treated as a clustering problem, and clustering algorithms can be applied
    to help plan how these clusters will be sorted. Here, several more data points
    must be taken into consideration, including historical consumer buying patterns,
    seasonality, trends on social media, search patterns on search engines, and more.
    This not only helps with better assortment optimization but also in increased
    revenues, a decrease in product waste, and faster time to market for businesses.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有实体店面的零售商面临有限的店铺空间。因此，他们需要确保通过放置那些最有可能销售的产品来实现店铺空间的最佳利用。一个经典的商品组合优化案例是美国中西部地区的一个五金零售商，在寒冷的冬季进货草坪割草机，当时雪季几乎在整个冬季都会持续。在这个例子中，店铺空间没有得到最佳利用，因为草坪割草机在雪季几乎没有销售机会。更好的选择应该是空间加热器、雪铲或其他冬季用品。为了解决这个问题，零售商通常会雇佣分析师，结合历史交易数据、季节性变化和当前趋势，提出适合季节和商店位置的商品组合推荐。然而，如果我们把这个问题规模扩大到一个拥有成千上万仓库和数万个零售店的大型零售商呢？在这种规模下，手动规划商品的最佳组合变得不切实际且非常耗时，极大地降低了实现价值的速度。商品组合优化可以看作一个聚类问题，聚类算法可以应用于帮助规划如何对这些群体进行排序。在这里，必须考虑更多的数据点，包括历史消费者购买模式、季节性变化、社交媒体上的趋势、搜索引擎的搜索模式等。这不仅有助于更好的商品组合优化，还能提高收入，减少产品浪费，并加快企业的市场响应速度。
- en: Customer churn analysis
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户流失分析
- en: It is becoming increasingly difficult for businesses to acquire customers because
    of ever-changing customer preferences and fierce competition in the marketplace.
    Thus,  businesses need to retain existing customers. **Customer churn rate** is
    one of the prominent metrics that business executives want to minimize. Machine
    learning classification algorithms can be used to predict if a particular customer
    will churn. However, having an understanding of the factors that affect churn
    would be useful, so that they can change or improve their operations to increase
    customer satisfaction. Clustering algorithms can be used not only to identify
    which group of customers are likely to churn, but also to further the analysis
    by identifying a set of factors that are affecting churn. Businesses can then
    act on these churn factors to either bring new products into the mix or improve
    churn to improve customer satisfaction and, in turn, decrease customer churn.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于客户偏好的不断变化和市场竞争的激烈，企业越来越难以获取新客户。因此，企业需要留住现有客户。**客户流失率**是企业高层希望最小化的一个重要指标。机器学习分类算法可以用于预测某个客户是否会流失。然而，了解影响流失的因素会很有帮助，这样企业可以调整或改进运营以提高客户满意度。聚类算法不仅可以用来识别哪些客户群体可能会流失，还可以通过识别影响流失的一组因素来进一步分析。企业可以根据这些流失因素采取行动，引入新产品或改进产品以提高客户满意度，进而减少客户流失。
- en: Insurance fraud detection
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保险欺诈检测
- en: Insurance companies traditionally use manual inspection, along with rules engines,
    to flag insurance claims as fraudulent. However, as the size of the data increases,
    the traditional methods might miss a sizeable portion of the claims since manual
    inspection is time-consuming and error-prone, and fraudsters are constantly innovating
    and devising new ways of committing fraud. Machine learning clustering algorithms
    can be used to group new claims with existing fraud clusters, and classification
    algorithms can be used to classify whether these claims are fraudulent. This way,
    by leveraging machine learning and clustering algorithms, insurance companies
    can constantly detect and prevent insurance fraud.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 保险公司传统上使用人工检查以及规则引擎来标记保险索赔是否为欺诈。然而，随着数据量的增加，传统方法可能会错过相当大一部分的索赔，因为人工检查既费时又容易出错，且欺诈者不断创新并策划新的欺诈方式。机器学习的聚类算法可以用来将新的索赔与现有的欺诈群体进行分组，而分类算法可以用来判断这些索赔是否为欺诈。通过利用机器学习和聚类算法，保险公司可以不断地检测和防止保险欺诈。
- en: Association rules and collaborative filtering applications
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关联规则和协同过滤应用
- en: Association rules and collaborative filtering are techniques that are used for
    building recommender systems. This section will explore some practical use cases
    of recommendation systems for practical business applications.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则和协同过滤是用于构建推荐系统的技术。本节将探讨推荐系统的一些实际应用案例，以便于实际商业应用。
- en: Recommendation systems
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推荐系统
- en: Recommendation systems are employed by e-retailers to perform market basket
    analysis, where the system makes product recommendations to users based on their
    preferences, as well as items already in their cart. Recommendation systems can
    also be used for location- or proximity-based recommendations, such as displaying
    ads or coupons when a customer is near a particular store. Recommendation systems
    are also used in marketing, where marketers can get recommendations regarding
    users who are likely to buy an item, which helps with the effectiveness of marketing
    campaigns.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统被电子零售商用来进行市场购物篮分析，在该分析中，系统根据用户的偏好以及购物车中已存在的商品向用户推荐产品。推荐系统还可以用于基于位置或临近度的推荐，例如，当客户靠近特定商店时，系统可以显示广告或优惠券。推荐系统还广泛应用于营销领域，营销人员可以通过推荐系统获取有关可能购买某商品的用户的信息，从而提高营销活动的效果。
- en: Recommendation systems are also heavily employed by online music and video service
    providers for user content personalization. Here, recommendation systems are used
    to make new music or video recommendations to users based on their preferences
    and historical usage patterns.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统在在线音乐和视频服务提供商中也得到了广泛应用，用于用户内容个性化。在这里，推荐系统根据用户的偏好和历史使用模式，向用户推荐新的音乐或视频。
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter introduced you to unsupervised learning algorithms, as well as
    how to categorize unlabeled data and identify associations between data entities.
    Two main areas of unsupervised learning algorithms, namely clustering and association
    rules, were presented. You were introduced to the most popular clustering and
    collaborative filtering algorithms. You were also presented with working code
    examples of clustering algorithms such as K-means, bisecting K-means, LDA, and
    GSM using code in Spark MLlib. You also saw code examples for building a recommendation
    engine using the alternative least-squares algorithm in Spark MLlib. Finally,
    a few real-world business applications of unsupervised learning algorithms were
    presented. We looked at several concepts, techniques, and code examples surrounding
    unsupervised learning algorithms so that you can train your models at scale using
    Spark MLlib.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了无监督学习算法，以及如何对未标记的数据进行分类并识别数据实体之间的关联。介绍了无监督学习算法的两个主要领域，即聚类和关联规则。你了解了最流行的聚类算法和协同过滤算法，并通过Spark
    MLlib中的代码示例展示了K-means、二分K-means、LDA和GSM等聚类算法的工作原理。你还看到了使用Spark MLlib中替代最小二乘法算法构建推荐引擎的代码示例。最后，展示了一些无监督学习算法在现实商业中的应用。我们探讨了关于无监督学习算法的若干概念、技术和代码示例，以便你能够使用Spark
    MLlib在大规模上训练模型。
- en: So far, in this and the previous chapter, you have only explored the data wrangling,
    feature engineering, and model training parts of the machine learning process.
    In the next chapter, you will be introduced to machine learning life cycle management,
    where you will explore concepts such as model performance tuning, tracking machine
    learning experiments, storing machine learning models in a central repository,
    and operationalizing ML models before putting them in production applications.
    Finally, an open end-to-end ML life cycle management tool called MLflow will also
    be introduced and explored.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章和前一章中，你只探讨了机器学习过程中的数据清洗、特征工程和模型训练部分。在下一章中，你将接触到机器学习生命周期管理，在这里你将探索一些概念，如模型性能调优、跟踪机器学习实验、将机器学习模型存储在中央仓库中，以及在将它们投入生产应用之前进行机器学习模型的运营化。最后，还将介绍并探索一个开源的端到端机器学习生命周期管理工具——MLflow。
