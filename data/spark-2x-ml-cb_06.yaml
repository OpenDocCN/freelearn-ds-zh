- en: Practical Machine Learning with Regression and Classification in Spark 2.0 -
    Part II
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用回归和分类的实用机器学习 - 第二部分
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用逻辑回归探索ML管道和数据框
- en: Linear regression with SGD optimization in Spark 2.0
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用SGD优化的线性回归
- en: Logistic regression with SGD optimization in Spark 2.0
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用SGD优化的逻辑回归
- en: Ridge regression with SGD optimization in Spark 2.0
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这一章中，Spark 2.0中回归和分类的后半部分，我们重点介绍了基于RDD的回归，这是目前许多现有Spark ML实现中的实践。由于现有的代码库，预期中级到高级的从业者能够使用这些技术。
- en: Lasso regression with SGD optimization in Spark 2.0
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用SGD优化的Lasso回归
- en: Logistic regression with L-BFGS optimization in Spark 2.0
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用L-BFGS优化的逻辑回归
- en: Support Vector Machine (SVM) with Spark 2.0
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下配方：
- en: Naïve Bayes machine learning with Spark 2.0 MLlib
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark 2.0 MLlib的朴素贝叶斯机器学习
- en: Exploring ML pipelines and DataFrames using logistic regression in Spark 2.0
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, the second half of regression and classification in Spark 2.0,
    we highlight RDD-based regression, which is currently in practice in a lot of
    existing Spark ML implementations. Any intermediate to advanced practitioner is
    expected to be able to work with these techniques due to the existing code base.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark 2.0的支持向量机（SVM）
- en: In this chapter, you will learn how to implement a small application using various
    regressions (linear, logistic, ridge, and lasso) with **Stochastic Gradient Descent**
    (**SGD**) and L-BFGS optimization with linear yet powerful classifiers such as
    **Support Vector Machines** (**SVM**) and **Naive Bayes** **classifiers** using
    the Apache Spark API. We augment each recipe with sample fit measurement when
    appropriate (for example, MSE, RMSE, ROC, and binary and multi-class metrics)
    to demonstrate the power and completeness of Spark MLlib. We introduce RDD-based
    linear, logistic, ridge, and lasso regression, and then discuss SVM and Naïve
    Bayes to demonstrate more sophisticated classifiers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何使用各种回归（线性、逻辑、岭和套索）以及**随机梯度下降**（**SGD**）和L-BFGS优化来实现一个小型应用程序，使用线性但强大的分类器，如**支持向量机**（**SVM**）和**朴素贝叶斯**分类器，使用Apache
    Spark API。我们将每个配方与适当的样本拟合度量相结合（例如，MSE、RMSE、ROC和二进制和多类度量），以展示Spark MLlib的强大和完整性。我们介绍了基于RDD的线性、逻辑、岭和套索回归，然后讨论了SVM和朴素贝叶斯，以展示更复杂的分类器。
- en: 'The following diagram depicts the regression and classification coverage in
    this chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了本章中的回归和分类覆盖范围：
- en: '![](img/00129.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00129.jpeg)'
- en: There have been reports of issues with regression using SGD in the field, but
    the issues are most likely due to poor tuning of SGD or a failure to understand
    the pros and cons of this technique in large parametric systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有报道称在实际应用中使用SGD进行回归存在问题，但这些问题很可能是由于SGD的调优不足或者未能理解大型参数系统中这种技术的利弊。
- en: In this chapter and going forward, we start moving toward more complete (plug
    and play) regression and classification systems that can be leveraged while building
    ML applications. While each recipe is a program by itself, a more complicated
    system can be assembled using Spark's ML pipeline to create an end-to-end ML system
    (for example, classifying cancer clusters via Naive Bayes then performing parameter
    selection on each segment using lasso). You will see a good example of ML pipelines
    in the last recipe in this chapter. While the two regression and classification
    chapters give you a good sample of what is available in Spark 2.0 classification,
    we reserve the more complicated methods for later chapters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和以后，我们开始朝着更完整（即插即用）的回归和分类系统迈进，这些系统可以在构建机器学习应用程序时加以利用。虽然每个配方都是一个独立的程序，但可以使用Spark的ML管道来组装一个更复杂的系统，以创建一个端到端的机器学习系统（例如，通过朴素贝叶斯对癌症簇进行分类，然后使用套索对每个部分进行参数选择）。您将在本章的最后一个配方中看到ML管道的一个很好的例子。虽然两个回归和分类章节为您提供了Spark
    2.0分类中可用内容的良好示例，但我们将更复杂的方法保留到以后的章节中。
- en: It is preferable to use the latest methods in data science, but it is important
    to master the fundamentals first, starting with GLM, LRM, ridge, lasso, and SVM
    - make sure you understand when to use each model before graduating to more complex
    models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最好使用数据科学中的最新方法，但在毕业到更复杂的模型之前，掌握基础知识是很重要的，从GLM、LRM、岭回归、套索和SVM开始 - 确保您了解何时使用每个模型。
- en: Linear regression with SGD optimization in Spark 2.0
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用SGD优化的线性回归
- en: In this recipe, we use Spark RDD-based regression API to demonstrate how to
    use an iterative optimization technique to minimize the cost function and arrive
    at a solution for a linear regression.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们使用Spark基于RDD的回归API来演示如何使用迭代优化技术来最小化成本函数，并得出线性回归的解决方案。
- en: We examine how Spark uses an iterative method to converge on a solution to the
    regression problem using a well-known method called **Gradient Descent**. Spark
    provides a more practical implementation known as SGD, which is used to compute
    the intercept (in this case set to 0) and the weights for the parameters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究Spark如何使用迭代方法来收敛到回归问题的解决方案，使用一种称为**梯度下降**的众所周知的方法。Spark提供了一种更实用的实现，称为SGD，用于计算截距（在本例中设置为0）和参数的权重。
- en: How to do it...
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在这个配方中，我们使用Spark基于RDD的回归API来演示如何使用迭代优化技术来最小化成本函数，并得出线性回归的解决方案。
- en: 'We use a housing dataset from the UCI machine library depository. You can download
    the entire dataset from the following URL:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用了UCI机器库存储中的一个房屋数据集。您可以从以下网址下载整个数据集：
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用SGD优化的岭回归
- en: The dataset comprises 14 columns with the first 13 columns being the independent
    variables (features) that try to explain the median price (last column) of an
    owner-occupied house in Boston, USA.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包括14列，前13列是独立变量（特征），试图解释美国波士顿自住房的中位价格（最后一列）。
- en: 'We have chosen and cleaned the first eight columns as features. We use the
    first 200 rows to train and predict the median price:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择并清理了前八列作为特征。我们使用前200行来训练和预测中位价格：
- en: '![](img/00130.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00130.jpeg)'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中开始一个新项目。确保必要的JAR文件已包含。
- en: 'Set up the package location where the program will reside:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages for Spark session to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的Spark会话包以访问集群和`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Initialize a SparkSession specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化一个SparkSession，指定配置，从而为Spark集群提供入口点：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Set the output level to ERROR to reduce Spark''s output:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为ERROR以减少Spark的输出：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We ingest and parallelize the dataset (first 200 rows only):'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们摄取并并行化数据集（仅前200行）：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We take the parallelized RDD (that is, the data variable) and split the columns
    using a `map()` function. We then proceed to walk through the columns and store
    them in a structure required by Spark (LabeledPoint). LabeledPoint is a data structure
    with the first column being the dependent variable (that is, label) followed by
    a DenseVector (that is, `Vectors.Dense`). We must present the data in this format
    for Spark''s `LinearRegressionWithSGD()` algorithm:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获取并并行化RDD（即数据变量），并使用`map()`函数拆分列。然后我们继续遍历列，并将它们存储在Spark所需的结构中（LabeledPoint）。LabeledPoint是一个数据结构，第一列是因变量（即标签），后面是一个DenseVector（即`Vectors.Dense`）。我们必须以这种格式呈现数据，以供Spark的`LinearRegressionWithSGD()`算法使用：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We now examine the regression data via the output to get ourselves familiar
    with the LabeledPoint data structure:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们通过输出来检查回归数据，以熟悉LabeledPoint数据结构：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We set model parameters, which are the number of iterations and SGD steps.
    Since this is a gradient descent approach, one must experiment with various values
    to find the optimal values that result in a good fit and avoid wasting resources.
    We usually use values ranging from 100 to 20000 (rare cases) for iterations and
    values ranging from .01 to .00001 for SGD steps:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置模型参数，即迭代次数和SGD步长。由于这是梯度下降方法，必须尝试各种值，找到能够得到良好拟合并避免浪费资源的最佳值。我们通常使用迭代次数从100到20000（极少情况下），SGD步长从.01到.00001的值范围：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We make a call to build the model:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们调用构建模型：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this step, we use the dataset to predict values using the model that was
    built in the previous step. We then place the predicted and labelled values in
    the `predictedLabelValue` data structure. To clarify, the previous step was to
    build a model (that is, decide on the fit for the data), while this step uses
    the model to predict:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们使用数据集使用在前一步中构建的模型来预测值。然后将预测值和标记值放入`predictedLabelValue`数据结构中。澄清一下，前一步是构建模型（即决定数据的拟合），而这一步使用模型进行预测：
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this step, we examine the intercept (by default, no intercept selected)
    and the weights for the eight columns (columns 0 to 7):'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们检查截距（默认情况下未选择截距）和八列的权重（列0到7）：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To get a feel for the predicted value, we randomly select twenty values without
    replacement using the `takesample()` function. In this case, we show the values
    for only seven of the twenty values:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了感受预测值，我们使用`takesample()`函数随机选择了二十个值，但不进行替换。在这种情况下，我们仅展示了其中七个值的数值：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We use Root Mean Squared Error (one of the many) to quantify the fit. The fit
    can be improved drastically (more data, stepsSGD, the number of iterations, and
    most importantly, experimentation with feature engineering), but we leave that
    to a statistics book to explore. Here is the formula for RMSD:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用均方根误差（其中之一）来量化拟合。拟合可以得到显著改善（更多数据、步骤SGD、迭代次数，最重要的是特征工程的实验），但我们将其留给统计书籍来探讨。以下是RMSD的公式：
- en: '![](img/00131.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00131.jpeg)'
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: How it works...
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We used selected columns from a housing data (independent variables) file to
    predict housing prices (dependent variable). We used the RDD-based regression
    method with an SGD optimizer to iterate toward a solution. We then proceeded to
    output the intercept and each parameter's weight. In the last step, we predicted
    with sample data and showed the output. The last step was to output the MSE and
    RMSE values for the model. Please note that this was for demonstration purposes
    only and you should use the evaluation metrics demonstrated in [Chapter 4](part0200.html#5UNGG0-4d291c9fed174a6992fd24938c2f9c77),
    *Common Recipes for Implementing a Robust Machine Learning System*, for model
    evaluation and the final selection process.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了来自房屋数据（自变量）文件的选定列来预测房屋价格（因变量）。我们使用了基于RDD的回归方法，采用SGD优化器进行迭代求解。然后我们输出了截距和每个参数的权重。在最后一步，我们使用样本数据进行了预测并显示了输出。最后一步是输出模型的MSE和RMSE值。请注意，这仅用于演示目的，您应该使用[第4章](part0200.html#5UNGG0-4d291c9fed174a6992fd24938c2f9c77)中演示的评估指标，*实施强大机器学习系统的常见方法*，进行模型评估和最终选择过程。
- en: 'The Signatures for this method constructor are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法构造函数的签名如下：
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Defaults for parameters:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的默认值：
- en: '`stepSize= 1.0`'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stepSize= 1.0`'
- en: '`numIterations= 100`'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numIterations= 100`'
- en: '``miniBatchFraction= 1.0``'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '``miniBatchFraction= 1.0``'
- en: '`miniBatchFraction` is an important parameter that can have a significant impact
    on performance. This is what is referred to as batch gradient versus gradient
    in academic literature.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`miniBatchFraction`是一个重要的参数，它对性能有重大影响。这在学术文献中被称为批量梯度与梯度。'
- en: There's more...
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We can also change the default intercept behavior using the constructor to
    create a new regression model and then using `setIntercept(true)` accordingly:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用构造函数更改默认的拦截行为，创建一个新的回归模型，然后相应地使用`setIntercept(true)`：
- en: 'The sample code is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 示例代码如下：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If the weight for the model is computed as NaN, you must change the model parameters
    (SGD steps or number of iterations) until you get convergence. An example is model
    weights that are not computed correctly (there is a convergence problem with SGD
    in general) due to poor parameter selection. The first move should be to use a
    more fine-grain step parameter for SGD:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果模型的权重计算为NaN，则必须更改模型参数（SGD步数或迭代次数），直到收敛。例如，模型权重未正确计算（通常由于参数选择不当导致SGD存在收敛问题）。第一步应该是使用更精细的SGD步长参数：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: See also
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: We cover both Gradient Descent and SGD in detail in [Chapter 9](part0437.html#D0O5Q0-4d291c9fed174a6992fd24938c2f9c77),
    *Optimization - Going Down the Hill with Gradient Descent*. In this chapter, the
    reader should abstract the SGD as an optimization technique that minimizes the
    loss function for fitting a line to a series of points. There are parameters that
    will affect the behavior of the SGD and we encourage the reader to change these
    parameters to either extreme to observe poor performance and non-convergence (that
    is, the result will appear as NaN).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第9章](part0437.html#D0O5Q0-4d291c9fed174a6992fd24938c2f9c77)中详细介绍了梯度下降和SGD，*优化
    - 用梯度下降下山*。在本章中，读者应该将SGD抽象为一种优化技术，用于最小化拟合一系列点的损失函数。有一些参数会影响SGD的行为，我们鼓励读者将这些参数改变到极端值，观察性能不佳和不收敛（即结果将显示为NaN）。
- en: 'The documentation for the `LinearRegressionWithSGD()` constructor is available
    at the following URL:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`LinearRegressionWithSGD()`构造函数的文档位于以下URL：'
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package)'
- en: Logistic regression with SGD optimization in Spark 2.0
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0中带有SGD优化的逻辑回归
- en: In this recipe, we use admission data from the UCI Machine Library Repository
    to build and then train a model to predict student admissions based on a given
    set of features (GRE, GPA, and Rank) used during the admission process using the
    RDD-based `LogisticRegressionWithSGD()` Apache Spark API set.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用UCI机器库存储库中的入学数据来构建并训练一个模型，以预测基于给定一组特征（GRE、GPA和等级）的学生入学情况，使用基于RDD的`LogisticRegressionWithSGD()`
    Apache Spark API集。
- en: This recipe demonstrates both optimization (SGD) and regularization (penalizing
    the model for complexity or over-fitting). We emphasize that they are two different
    things and often cause confusion to beginners. In the upcoming chapter, we demonstrate
    both concepts in more detail since understanding both is fundamental to a successful
    study of ML.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例演示了优化（SGD）和正则化（惩罚模型的复杂性或过拟合）。我们强调它们是两个不同的概念，常常让初学者感到困惑。在接下来的章节中，我们将更详细地演示这两个概念，因为理解它们对于成功学习机器学习是至关重要的。
- en: How to do it...
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We use the admission dataset from the UCLA **Institute for Digital Research**
    **and** **Education** (**IDRE**). You can download the entire dataset from the
    following URLs:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用了来自UCLA **数字研究和教育**（**IDRE**）的入学数据集。您可以从以下URL下载整个数据集：
- en: For home page, you can refer to [http://www.ats.ucla.edu/stat/](http://www.ats.ucla.edu/stat/)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于主页，您可以参考[http://www.ats.ucla.edu/stat/](http://www.ats.ucla.edu/stat/)
- en: For data file, you can refer to [https://stats.idre.ucla.edu/stat/data/binary.csv](https://stats.idre.ucla.edu/stat/data/binary.csv)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于数据文件，您可以参考[https://stats.idre.ucla.edu/stat/data/binary.csv](https://stats.idre.ucla.edu/stat/data/binary.csv)
- en: The dataset comprises four columns, with the first column being the dependent
    variable (label - whether the student was admitted or not) and the next three
    columns being the explanatory variables, that is, the features that will explain
    the admission of a student.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包括四列，第一列是因变量（标签 - 学生是否被录取），接下来的三列是解释变量，即将解释学生入学情况的特征。
- en: 'We have chosen and cleaned the first three columns as features. We use the
    first 200 rows to train and predict the median price:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择并清理了前三列作为特征。我们使用前200行来训练和预测中位数价格：
- en: Admission - 0, 1 indicating whether the student was admitted or not
  id: totrans-89
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入学 - 0，1表示学生是否被录取
- en: GRE - The score from Graduate Record Examination
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRE - 研究生录取考试的分数
- en: GPA - Grade Point Average score
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPA - 平均成绩
- en: RANK - The ranking
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RANK - 排名
- en: 'Here''s sample data from the first 10 rows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前10行的样本数据：
- en: '![](img/00132.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00132.jpeg)'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Import the necessary packages for the Spark session to gain access to the cluster,
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以便Spark会话可以访问集群，并使用`Log4j.Logger`来减少Spark产生的输出量：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Initialize a `SparkSession` specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`SparkSession`，使用构建器模式指定配置，从而为Spark集群提供入口点：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Set the output level to `ERROR` to reduce Spark''s output:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出级别设置为`ERROR`以减少Spark的输出：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Load the data file and turn it into RDDs:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据文件并将其转换为RDD：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Ingest the data by splitting it and then converting to double while building
    a LabeledPoint (a data structure required by Spark) dataset. Column 1 (position
    0) is the dependent variable in the regression, while columns 2 through 4 (GRE,
    GPA, Rank) are features:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过拆分数据并将其转换为双精度，同时构建一个LabeledPoint（Spark所需的数据结构）数据集来摄取数据。第1列（位置0）是回归中的因变量，而第2到第4列（GRE、GPA、Rank）是特征：
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We examine the dataset after loading, which is always recommended, and also
    demonstrate the label point internals, which is a single value (for example, label
    or dependent variable), followed by a DenseVector of features we are trying to
    use to explain the dependent variable:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载数据集后检查数据集，这是一直建议的，还演示了标签点内部，这是一个单一值（例如标签或因变量），后面是我们试图用来解释因变量的DenseVector特征：
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We set up the model parameter for `LogisticRegressionWithSGD()`.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为`LogisticRegressionWithSGD()`设置了模型参数。
- en: 'These parameters ultimately control the fit, and hence, some level of experimentation
    is required to achieve a good fit. We saw the first two parameters in the previous
    recipe. The third parameter''s value will affect the selection of the weights.
    You must experiment and use model selection techniques to decide the final value.
    In the *There''s more...* section of this recipe, we show the weight selection
    of the features (that is, which weights are set to 0) based on two extreme values:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数最终控制拟合，因此需要一定程度的实验才能获得良好的拟合。我们在上一个示例中看到了前两个参数。第三个参数的值将影响权重的选择。您必须进行实验并使用模型选择技术来决定最终值。在本示例的*还有更多...*部分，我们基于两个极端值展示了特征的权重选择（即哪些权重设置为0）：
- en: '[PRE26]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create and train the logistic regression model with a call to `LogisticRegressionWithSGD()`
    using the LabeledPoint and the preceding parameters:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用LabeledPoint和前述参数创建和训练逻辑回归模型，调用`LogisticRegressionWithSGD()`：
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Predict the values using our model and the dataset (similar to all Spark regression
    methods):'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们的模型和数据集预测值（类似于所有Spark回归方法）：
- en: '[PRE28]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We print our model intercept and weights. If you compare values with linear
    or ridge regression, you will see the selection effect. The effect will be more
    dramatic at extreme values or when choosing a dataset with more collinearity.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印出模型的截距和权重。如果将值与线性或岭回归进行比较，您将看到选择效果。在极端值或选择具有更多共线性的数据集时，效果将更加显著。
- en: 'In this example, lasso eliminated three parameters by setting the weights to
    0.0 using a regularization parameter (for example, 4.13):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，通过设置权重为0.0，套索消除了三个参数，使用了正则化参数（例如4.13）：
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The principles of model parameter selection from statistics still applies whether
    we use Spark MLlib or not. For example, a parameter weight of -8.625514722380274E-6
    might be too small to include in the model. We need to look at `t-statistic` and
    `p value` for each parameter and decide on the final model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计学的模型参数选择原则仍然适用于我们是否使用Spark MLlib。例如，参数权重为-8.625514722380274E-6可能太小而无法包含在模型中。我们需要查看每个参数的`t-statistic`和`p
    value`，并决定最终的模型。
- en: 'We randomly choose 20 predicted values and visually inspect the predicted result
    (only the first five values are shown here):'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们随机选择20个预测值，并直观地检查预测结果（这里只显示了前五个值）：
- en: '[PRE30]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We calculate the RMSE and show the result:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算RMSE并显示结果：
- en: '[PRE31]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE32]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How it works...
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We used the admission data and tried to use the logistic regression with some
    of the features in order to predict whether a student with a given feature set
    (vector) will be admitted or not (the label). We fitted the regression, set SGD
    parameters (you should experiment), and ran the API. We then output intercept
    and model weights for the regression coefficients. Using the model, we predicted
    and output some predicted values for visual inspection. The last step was to output
    MSE and RMSE values for the model. Please note that this was for demonstration
    purposes only and you should use evaluation metrics demonstrated in the previous
    chapter for model evaluation and the final selection process. Looking at SME and
    RMSE, we probably need a different model, parameter settings, parameters, or more
    data points to do a better job.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用入学数据，并尝试使用逻辑回归来预测具有给定特征集（向量）的学生是否被录取（标签）。我们拟合了回归，设置了SGD参数（您应该进行实验），并运行了API。然后，我们输出回归系数的截距和模型权重。使用模型，我们预测并输出一些预测值以进行视觉检查。最后一步是输出模型的MSE和RMSE值。请注意，这仅用于演示目的，您应该使用上一章中演示的评估指标进行模型评估和最终选择过程。通过查看SME和RMSE，我们可能需要不同的模型、参数设置、参数或更多数据点来做得更好。
- en: 'The Signatures for this method constructor are as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法构造函数的签名如下：
- en: '[PRE34]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Defaults for Parameters:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的默认值：
- en: '`stepSize`= 1.0'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stepSize`= 1.0'
- en: '`numIterations`= 100'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numIterations`= 100'
- en: '`regParm`= 0.01'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`regParm`= 0.01'
- en: '`miniBatchFraction`= 1.0'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`miniBatchFraction`= 1.0'
- en: There's more...
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: While non-logistic regression attempts to discover a linear or non-linear relationship
    that relates the explanatory factors (features) to a numeric variable on the left-hand
    side of the equation, logistic regression attempts to classify a feature set to
    a set of discrete classes (for example, pass/fail, good/bad, or multi-class).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然非逻辑回归试图发现将解释因素（特征）与方程左侧的数值变量相关联的线性或非线性关系，逻辑回归试图将特征集分类到一组离散类别（例如通过/不通过，好/坏或多类）。
- en: The best way to understand logistic regression is to think of the domain on
    the left-side as a discrete set of outcomes (that is, the classification class)
    that is used to label a new prediction. Using a discrete label (for example, 0
    or 1) we are able to predict whether a set of features belong to a specific class
    (for example, the presence or absence of disease).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 理解逻辑回归的最佳方法是将左侧的领域视为一组离散的结果（即分类类），用于标记新的预测。使用离散标签（例如0或1），我们能够预测一组特征是否属于特定类别（例如疾病的存在或不存在）。
- en: '![](img/00133.jpeg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00133.jpeg)'
- en: In short, the main difference between regular regression and logistic regression
    is the type of variable that can be used on the left-hand side. In regular regression,
    the predicted outcome (that is, the label) will be a numeric value, while in logistic
    regression, prediction is a selection from a discrete class of possible outcomes
    (that is, labels).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，常规回归和逻辑回归之间的主要区别是可以在左侧使用的变量类型。 在常规回归中，预测的结果（即标签）将是一个数值，而在逻辑回归中，预测是从可能结果的离散类别中进行选择（即标签）。
- en: In the interest of time and space, we do not cover breaking a dataset to train
    and test in every recipe since we have demonstrated this in the previous recipes.
    We also do not use any caching, but emphasize that a real-life application must
    cache the data due to the way lazy instantiation, staging, and optimization is
    used in Spark. See [Chapter 4](part0200.html#5UNGG0-4d291c9fed174a6992fd24938c2f9c77),
    *Common Recipes for Implementing a Robust Machine Learning System*, to refer to
    recipes regarding caching and training/test data split during ML development.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 出于时间和空间的考虑，我们不在每个示例中涵盖将数据集分割为训练和测试的内容，因为我们在之前的示例中已经演示了这一点。 我们也不使用任何缓存，但强调现实生活中的应用必须缓存数据，因为Spark中使用了惰性实例化、分阶段和优化的方式。
    请参阅[第4章](part0200.html#5UNGG0-4d291c9fed174a6992fd24938c2f9c77)，*实施强大的机器学习系统的常见示例*，以参考有关缓存和训练/测试数据拆分的示例。
- en: If the weight for the model is computed as NaN, you must change the model parameters
    (that is, SGD steps or number of iterations) until you get convergence.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型的权重计算为NaN，则必须更改模型参数（即SGD步骤或迭代次数），直到收敛。
- en: See also
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Here''s the documentation for the constructor:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是构造函数的文档：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithSGD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithSGD)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithSGD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithSGD)'
- en: Ridge regression with SGD optimization in Spark 2.0
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0中的SGD优化岭回归
- en: In this recipe, we use admission data from the UCI Machine Library Repository
    to build and then train a model to predict student admission using the RDD-based
    `LogisticRegressionWithSGD()` Apache Spark API set. We use a given set of features
    (GRE, GPA, and Rank) used during the admission to predict model weights using
    ridge regression. We demonstrate the input feature standardization in a different
    recipe, but it should be noted that parameter standardization has an important
    effect on the results, especially in a ridge regression setting.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用UCI机器库存储库中的入学数据来构建并训练一个模型，以使用基于RDD的`LogisticRegressionWithSGD()`
    Apache Spark API集来预测学生入学。 我们使用一组给定的特征（GRE，GPA和Rank）在入学期间用于预测模型权重，使用岭回归。 我们在另一个示例中演示了输入特征标准化，但应该注意的是，参数标准化对结果有重要影响，特别是在岭回归设置中。
- en: Spark's ridge regression API (`LogisticRegressionWithSGD`) is meant to deal
    with multicollinearity (the explanatory variable or features are correlated and
    the assumption of intendent and randomly distributed feature variables are somewhat
    flawed). Ridge is about shrinking (penalizing via L2 regularization or a quadratic
    function) some of the parameters, therefore reducing their effect and in turn
    reducing complexity. It is critical to remember that `LogisticRegressionWithSGD()`
    does not have a lasso effect in which some of the parameters are actually reduced
    to zero (that is, eliminated). Ridge regression only shrinks the parameter and
    does not set them to zero (a small effect will still remain after shrinkage).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的岭回归API（`LogisticRegressionWithSGD`）旨在处理多重共线性（解释变量或特征相关，并且独立和随机分布的特征变量的假设有些错误）。
    岭回归是关于收缩（通过L2正则化或二次函数进行惩罚）一些参数，从而减少它们的影响，进而降低复杂性。 需要记住的是，`LogisticRegressionWithSGD()`没有套索效应，其中一些参数实际上被减少到零（即被消除）。
    岭回归只是收缩参数，而不是将它们设为零（收缩后仍将保留一些小效果）。
- en: How to do it...
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We use a housing dataset from UCI Machine Library Repository. You can download
    the entire dataset from the following URL:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用UCI机器库存储库中的房屋数据集。 您可以从以下URL下载整个数据集：
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)'
- en: The dataset comprises 14 columns, with the first 13 columns being the independent
    variables (that is, features) that try to explain the median price (the last column)
    of an owner-occupied house in Boston, USA.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包括14列，前13列是独立变量（即特征），试图解释美国波士顿自住房的中位价格（最后一列）。
- en: We have chosen and cleaned the first eight columns as features. We use the first
    200 rows to train and predict the median price.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择并清理了前八列作为特征。 我们使用前200行来训练和预测中位价格。
- en: '| 1 | CRIM | Per capita crime rate by town |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 1 | CRIM | 按城镇人均犯罪率 |'
- en: '| 2 | ZN | Proportion of residential land zoned for lots over 25,000 sq. ft.
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 2 | ZN | 用于超过25,000平方英尺的地块的住宅用地比例 |'
- en: '| 3 | INDUS | Proportion of non-retail business acres per town |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 3 | INDUS | 每个镇的非零售业务面积比例 |'
- en: '| 4 | CHAS | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 4 | CHAS | 查尔斯河虚拟变量（如果地块与河流相接则为1；否则为0） |'
- en: '| 5 | NOX | Nitric oxides concentration (parts per 10 million) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 5 | NOX | 一氧化氮浓度（每1000万份之） |'
- en: '| 6 | RM | Average number of rooms per dwelling |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 6 | RM | 每个住宅的平均房间数 |'
- en: '| 7 | AGE | Proportion of owner-occupied units built prior to 1940 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 7 | AGE | 1940年之前建造的自住单位比例 |'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动新项目。 确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE35]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Import the necessary packages for SparkSession to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包以使SparkSession能够访问集群和`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE36]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Initialize a SparkSession specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化SparkSession，从而为Spark集群提供入口点：
- en: '[PRE37]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To effectively demonstrate the shrinkage of the model parameter (it will shrink
    to a small value, but never get eliminated) in ridge regression, we use the same
    housing data file, and clean and use the first eight columns to predict the value
    of last column (median housing price):'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了有效地展示岭回归中模型参数的收缩（它会收缩到一个小值，但永远不会被消除），我们使用相同的房屋数据文件，并清理和使用前八列来预测最后一列的值（房屋价格中位数）：
- en: '[PRE38]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Ingest the data by splitting it and then converting to double while building
    a LabeledPoint (a data structure required by Spark) dataset:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过拆分数据并将其转换为double，同时构建一个LabeledPoint（Spark所需的数据结构）数据集来摄取数据：
- en: '[PRE39]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Examine the dataset after loading, which is always recommended, and also demonstrate
    the LabeledPoint internals, which is a single value (label/ dependent variable),
    followed by a DenseVector of the features we are trying to use to explain the
    dependent variable:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集后检查数据集，这是一直建议的，还演示了LabeledPoint的内部结构，它是一个单一值（标签/因变量），后跟我们试图用来解释因变量的特征的DenseVector：
- en: '[PRE40]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We set up the model parameter for `RidgeRegressionWithSGD()`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为`RidgeRegressionWithSGD()`设置了模型参数。
- en: In the *There's more...* section of this recipe, we show the shrinkage effect
    based on two extreme values.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程的*还有更多...*部分，我们基于两个极端值展示了收缩效应。
- en: '[PRE41]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Create and train the ridge regression model with a call to `RidgeRegressionWithSGD()`
    and LabeledPoint using the preceding parameters:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上述参数调用`RidgeRegressionWithSGD()`和LabeledPoint创建和训练岭回归模型：
- en: '[PRE42]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Predict values using our model and the dataset (similar to all Spark regression
    methods):'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们的模型和数据集预测数值（类似于所有Spark回归方法）：
- en: '[PRE43]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Print the model intercept and weights. If you compare values with linear regression,
    you will see the shrinkage effect. The effect will be more dramatic at extreme
    values or when choosing a dataset with more collinearity:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印模型截距和权重。如果将值与线性回归进行比较，您将看到收缩效应。在选择具有更多共线性的数据集或极端值时，效果将更加明显：
- en: '[PRE44]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Randomly choose 20 predicted values and visually inspect the predicted result
    (only the first five values are shown):'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择20个预测数值并直观地检查预测结果（仅显示前五个值）：
- en: '[PRE45]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Calculate the RMSE and show the result:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算RMSE并显示结果：
- en: '[PRE46]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE47]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: How it works...
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: To be able to compare with other regression methods and see the shrinkage effect,
    we used the housing data again and trained a model using `RidgeRegressionWithSGD.train`*.*
    After fitting the model, we output intercept and parameter weights for the model
    that we just trained. We then proceeded to predict the values using the `*.predict()*`
    API. We printed the predicted values and visually inspected the first 20 numbers
    before outputting the MSE and RMSE.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够与其他回归方法进行比较并观察收缩效应，我们再次使用房屋数据并使用`RidgeRegressionWithSGD.train`训练模型。在拟合模型后，我们输出了刚刚训练的模型的截距和参数权重。然后我们使用`*.predict()*`API预测数值。在输出MSE和RMSE之前，我们打印了预测值并直观地检查了前20个数字。
- en: 'The Signature for this method constructor is as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法构造函数的签名如下：
- en: '[PRE48]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: These parameters ultimately control the fit, and hence, some level of experimentation
    is required to achieve a good fit. We saw the first two parameters in the previous
    recipe. The third parameter will affect the shrinkage of the weights based on
    the value selected. You must experiment and use model selection techniques to
    decide the final value.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数最终控制了拟合，因此需要一定程度的实验来实现良好的拟合。我们在上一个教程中看到了前两个参数。第三个参数将根据所选的值影响权重的收缩。您必须进行实验并使用模型选择技术来决定最终值。
- en: 'Defaults for Parameters:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的默认值：
- en: '`stepSize`= 1.0'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stepSize` = 1.0'
- en: '`numIterations`= 100'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numIterations` = 100'
- en: '`regParm`= 0.01'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`regParm` = 0.01'
- en: '`miniBatchFraction`= 1.0'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`miniBatchFraction` = 1.0'
- en: We cover optimization and L1 (absolute value) versus L2 (quadratic) in detail
    in [Chapter 9](part0437.html#D0O5Q0-4d291c9fed174a6992fd24938c2f9c77), *Optimization
    - Going Down the Hill with Gradient Descent*, but for the purposes of this recipe
    the reader should understand that ridge regression uses L2 to penalize (that is,
    shrink some of the parameters) while the upcoming recipe, *Lasso regression with
    SGD optimization in Spark 2.0*, uses L1 to penalize (that is, eliminate some of
    the parameters based on the threshold used). We encourage the user to compare
    the weights of this recipe with the linear and lasso regression recipes to see
    the effect first-hand. We use the same housing dataset to demonstrate the effect.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第9章](part0437.html#D0O5Q0-4d291c9fed174a6992fd24938c2f9c77)中详细介绍了优化和L1（绝对值）与L2（平方），“优化
    - 使用梯度下降下山”，但是对于本教程的目的，读者应该了解岭回归使用L2进行惩罚（即收缩一些参数），而即将到来的教程“Spark 2.0中使用SGD优化的套索回归”使用L1进行惩罚（即根据所使用的阈值消除一些参数）。我们鼓励用户将本教程的权重与线性回归和套索回归教程进行比较，以亲自看到效果。我们使用相同的房屋数据集来展示效果。
- en: 'The following diagram shows ridge regression with a regularization function:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了带有正则化函数的岭回归：
- en: '![](img/00134.jpeg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00134.jpeg)'
- en: In short, it is a remedy to deal with feature dependency by adding a small bias
    factor (ridge regression) that reduces the variables using a regularization penalty.
    Ridge regression shrinks the explanatory variable but never sets it to 0, unlike
    lasso regression, which will eliminate variables.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这是通过添加一个小的偏差因子（岭回归）来处理特征依赖的一种补救措施，它使用正则化惩罚来减少变量。岭回归会收缩解释变量，但永远不会将其设置为0，不像套索回归会消除变量。
- en: The scope of this recipe is limited to a demonstration of the API call for ridge
    regression in Spark. The math and a deep explanation for ridge regression is a
    topic of multiple chapters in statistical books. For better understanding, we
    strongly recommend that the reader familiarizes himself/herself with the concept
    while taking into account the topics of L1, L2, ... L4 regularization along with
    the relationship between ridge and linear PCA.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的范围仅限于演示Spark中岭回归的API调用。岭回归的数学和深入解释是统计书籍中多章的主题。为了更好地理解，我们强烈建议读者在考虑L1、L2、...
    L4正则化以及岭回归和线性PCA之间的关系的同时，熟悉这个概念。
- en: There's more...
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The amount of shrinkage for parameters varies by parameter selection, but the
    presence of collinearity is required for the weights to shrink. You can demonstrate
    this to yourself by using truly random (IID explanatory variables generated by
    a random generator) versus features that are highly dependent on one another (for
    example, waist line and weight).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的收缩量因参数选择而异，但权重收缩需要共线性的存在。您可以通过使用真正随机（由随机生成器生成的IID解释变量）与高度相互依赖的特征（例如，腰围和体重）来自行证明这一点。
- en: 'Here are two examples of extreme regularization values and their effect on
    model weights and shrinkage:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是极端正则化值的两个示例以及它们对模型权重和收缩的影响：
- en: '[PRE49]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: If the weight for the model is computed as NaN, you must change the model parameters
    (SGD steps or number of iterations) until you get convergence.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型的权重计算为NaN，则必须更改模型参数（SGD步骤或迭代次数），直到收敛。
- en: 'Here is an example of model weights that are not computed correctly due to
    poor parameter selection. The first move should be to use a more fine-grained
    step parameter for SGD:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个由于参数选择不当而未正确计算模型权重的示例。第一步应该是使用更精细的SGD步骤参数：
- en: '[PRE50]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: See also
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Here''s the documentation for the constructor:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是构造函数的文档：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.RidgeRegressionWithSGD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.RidgeRegressionWithSGD)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.RidgeRegressionWithSGD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.RidgeRegressionWithSGD)'
- en: Lasso regression with SGD optimization in Spark 2.0
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0中使用SGD优化的套索回归
- en: In this recipe, we will use the housing dataset from the previous recipes to
    demonstrate shrinkage with Spark's RDD-based lasso regression `LassoWithSGD()`,
    which can select a subset of parameters by setting the other weights to zero (hence
    eliminating some parameters based on the threshold) while reducing the effect
    of others (regularization). We emphasize again that ridge regression reduces the
    parameter weight, but never sets it to zero.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将使用先前示例中的住房数据集，以演示Spark的基于RDD的套索回归`LassoWithSGD()`，它可以通过将其他权重设置为零（因此根据阈值消除一些参数）来选择一部分参数，同时减少其他参数的影响（正则化）。我们再次强调，岭回归减少了参数权重，但从不将其设置为零。
- en: '`LassoWithSGD()`, which is Spark''s RDD-based lasso (Least Absolute Shrinkage
    and Selection Operator) API, a regression method that performs both variable selection
    and regularization at the same time in order to eliminate non-contributing explanatory
    variables (that is, features), therefore enhancing the prediction''s accuracy.
    Lasso, which is based on **Ordinary Least Squares** (**OLS**), can be easily extended
    to other methods, such as **General Liner Methods** (**GLM**).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`LassoWithSGD()`是Spark的基于RDD的套索（最小绝对收缩和选择算子）API，这是一种回归方法，同时执行变量选择和正则化，以消除不贡献的解释变量（即特征），从而提高预测的准确性。基于**普通最小二乘法**（**OLS**）的套索可以轻松扩展到其他方法，例如**广义线性方法**（**GLM**）。'
- en: How to do it...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE51]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Import the necessary packages for SparkSession to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入SparkSession所需的必要包，以便访问集群和`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE52]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Initialize a SparkSession specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化SparkSession，从而为Spark集群提供入口点：
- en: '[PRE53]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'To effectively demonstrate the shrinkage of the model parameter (it will shrink
    to a small value but never get eliminated) in ridge regression, we use the same
    housing data file and clean and use the first eight columns to predict the value
    of the last column (median housing price):'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了有效地演示岭回归模型参数的收缩（它将收缩到一个小值，但永远不会被消除），我们使用相同的住房数据文件，并清理并使用前八列来预测最后一列的值（中位数住房价格）：
- en: '[PRE54]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We ingest the data by splitting it and then converting it to double while building
    a LabeledPoint (a data structure required by Spark) dataset:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过拆分数据并将其转换为双精度来摄取数据，同时构建一个LabeledPoint（Spark所需的数据结构）数据集：
- en: '[PRE55]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We examine the dataset after loading, which is always recommended, and also
    demonstrate the Label point internals, which is a single value (that is, the label/dependent
    variable), followed by a DenseVector of the features we are trying to use to explain
    the dependent variable:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集后，我们检查数据集，这是一直建议的，并演示标签点的内部，这是一个值（即标签/因变量），后跟我们试图用来解释因变量的特征的DenseVector：
- en: '[PRE56]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We set up the model parameter for `lassoWithSGD()`. These parameters ultimately
    control the fit, so some level of experimentation is required to achieve a good
    fit. We saw the first two parameters in the previous recipe. The third parameter''s
    value will affect the selection of the weights. You must experiment and use model
    selection techniques to decide the final value. In the *There''s more...* section
    of this recipe, we show the weight selection of the features (that is, which weights
    are set to 0) based on two extreme values:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了`lassoWithSGD()`的模型参数。这些参数最终控制拟合，因此需要一定程度的实验来获得良好的拟合。我们在前面的配方中看到了前两个参数。第三个参数的值将影响权重的选择。您必须进行实验并使用模型选择技术来决定最终值。在本配方的*更多内容*部分，我们展示了基于两个极端值的特征的权重选择（即哪些权重设置为0）：
- en: '[PRE57]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We create and train the ridge regression model with a call to `RidgeRegressionWithSGD()`
    and our LabeledPoint using the preceding parameters:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过调用`RidgeRegressionWithSGD()`和我们的LabeledPoint来创建和训练岭回归模型，使用了前述参数：
- en: '[PRE58]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We predict values using our model and the dataset (similar to all Spark regression
    methods):'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用我们的模型和数据集来预测值（与所有Spark回归方法类似）：
- en: '[PRE59]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We print our model intercept and weights. If you compare values with linear
    or ridge regression, you will see the selection effect. The effect will be more
    dramatic at extreme values or when choosing a dataset with more collinearity.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印我们的模型截距和权重。如果将值与线性或岭回归进行比较，您将看到选择效果。在极端值或选择具有更多共线性的数据集时，效果将更加显著。
- en: 'In this example, lasso eliminated three parameters by setting the weights to
    0.0 using a regularization parameter (for example, 4.13):'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，套索通过设置权重为0.0使用正则化参数（例如4.13）消除了三个参数。
- en: '[PRE60]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We randomly choose 20 predicted values and visually inspect the predicted result
    (only the first five values shown here):'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们随机选择了20个预测值，并直观地检查了预测结果（这里只显示了前五个值）：
- en: '[PRE61]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We calculate the RMSE and show the result:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算RMSE并展示结果：
- en: '[PRE62]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output is as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE63]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: How it works...
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Again, we used the housing data so we can compare this method with ridge regression
    and show how lasso not only shrinks the parameters, like ridge regression, but
    it goes all the way and sets the parameters that are not significantly contributing
    to zero.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们使用了房屋数据，这样我们可以将这种方法与岭回归进行比较，并展示套索不仅像岭回归一样收缩参数，而且它会一直进行下去，并将那些没有显著贡献的参数设置为零。
- en: 'The Signatures for this method constructor are as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法构造函数的签名如下：
- en: '[PRE64]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Defaults for Parameters:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的默认值：
- en: '`stepSize`= 1.0'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stepSize`= 1.0'
- en: '`numIterations`= 100'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numIterations`= 100'
- en: '`regParm`= 0.01'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`regParm`= 0.01'
- en: '`miniBatchFraction`= 1.0'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`miniBatchFraction`= 1.0'
- en: As a reminder, ridge regression reduces the parameter's weight but does not
    eliminate them. When dealing with a huge number of parameters without a deep learning
    system in data mining/machine learning, lasso is usually preferred to reduce the
    number of inputs in the early stages of a ML pipeline, at least in the exploration
    phase.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，岭回归减少了参数的权重，但并不会消除它们。在数据挖掘/机器学习中处理大量参数时，如果没有深度学习系统，通常会优先选择套索，以减少ML管道早期阶段的输入数量，至少在探索阶段。
- en: Lasso plays a substantial role in advanced data mining and machine learning
    due to its ability to select a subset of the weights (that is, parameters) based
    on the threshold. In short, lasso regression decides which parameters to include
    or exclude (that is, set weight to 0) based on the threshold.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 由于套索能够根据阈值选择一部分权重（即参数），因此在高级数据挖掘和机器学习中扮演着重要角色。简而言之，套索回归根据阈值决定包括或排除哪些参数（即将权重设置为0）。
- en: While ridge regression can substantially reduce a parameter's contribution to
    the overall result, it will never reduce the weight to zero. Lasso regression
    differs from ridge regression by being able to reduce the weight of a feature's
    contribution to zero (hence selecting a subset of features that contribute the
    most).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然岭回归可以大大减少参数对整体结果的贡献，但它永远不会将权重减少到零。套索回归通过能够将特征的权重减少到零（因此选择了贡献最大的特征子集）而不同于岭回归。
- en: '![](img/00135.jpeg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00135.jpeg)'
- en: There's more...
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: The parameter selection (that is, setting some weights to zero) varies by regularization
    parameter value.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 参数选择（即将一些权重设置为零）随正则化参数值的变化而变化。
- en: 'Here''s two examples of extreme regularization values and their effect on model
    weights and shrinkage:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两个极端正则化值的例子，以及它们对模型权重和收缩的影响：
- en: '[PRE65]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'In this case, we eliminated one of the parameters using lasso:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用套索消除了一个参数：
- en: '[PRE66]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'In this case, we eliminated four of the parameters using lasso:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用套索消除了四个参数：
- en: '[PRE68]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: If the weight for the model is computed as NaN, you must change the model parameters
    (that is, SGD steps or number of iterations) until you get convergence.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型的权重计算为NaN，则必须更改模型参数（即SGD步骤或迭代次数），直到收敛。
- en: 'Here is an example of model weights that are not computed correctly (that is,
    convergence problem with SGD in general) due to poor parameter selection. The
    first move should be to use a more fine-grained step parameter for SGD:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个模型权重计算不正确的例子（即SGD中的收敛问题），这是由于参数选择不当。第一步应该是使用更精细的SGD步骤参数：
- en: '[PRE69]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: See also
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Here is the documentation for the constructor:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这是构造函数的文档：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.LassoWithSGD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.LassoWithSGD)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.LassoWithSGD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.LassoWithSGD)'
- en: Logistic regression with L-BFGS optimization in Spark 2.0
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0中使用L-BFGS优化的逻辑回归
- en: In this recipe, we will use the UCI admission dataset again so we can demonstrate
    Spark's RDD-based logistic regression solution, `LogisticRegressionWithLBFGS()`,
    for an extremely large number of parameters that are present in certain types
    of ML problem.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将再次使用UCI录取数据集，以便演示Spark基于RDD的逻辑回归解决方案，`LogisticRegressionWithLBFGS()`，用于某些类型的ML问题中存在的大量参数。
- en: We recommend L-BFGS for very large variable space since the Hessian matrix of
    second derivatives can be approximated using updates. If you have an ML problem
    with millions or billions of parameters, we recommend deep learning techniques.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常大的变量空间，我们建议使用L-BFGS，因为可以使用更新来近似二阶导数的Hessian矩阵。如果您的ML问题涉及数百万或数十亿个参数，我们建议使用深度学习技术。
- en: How to do it...
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We use the admission dataset from UCLA IDRE. You can download the entire dataset
    from the following URLs:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用UCLA IDRE的录取数据集。您可以从以下网址下载整个数据集：
- en: For home page go through the [http://www.ats.ucla.edu/stat/](http://www.ats.ucla.edu/stat/)
    link.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要查看主页，请访问[http://www.ats.ucla.edu/stat/](http://www.ats.ucla.edu/stat/)链接。
- en: For the data file go through the [https://stats.idre.ucla.edu/stat/data/binary.csv](https://stats.idre.ucla.edu/stat/data/binary.csv)
    link.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要查看数据文件，请访问[https://stats.idre.ucla.edu/stat/data/binary.csv](https://stats.idre.ucla.edu/stat/data/binary.csv)链接。
- en: The dataset comprises four columns, with the first column being the dependent
    variable (that is, the label - whether the student was admitted or not) and the
    next three columns being the explanatory variables (features that will explain
    the admission of a student).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包括四列，第一列是因变量（即标签-学生是否被录取），接下来的三列是自变量（解释学生录取的特征）。
- en: We have chosen and cleaned the first eight columns as features. We use the first
    200 rows to train and predict the median price.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择并清理了前八列作为特征。我们使用前200行来训练和预测中位数价格。
- en: '![](img/00136.jpeg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00136.jpeg)'
- en: 'Here is some sample data from the first three rows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前三行的一些示例数据：
- en: '![](img/00137.jpeg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00137.jpeg)'
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中开始一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE70]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Import the necessary packages for SparkSession to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入SparkSession所需的包，以便访问集群和`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE71]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Initialize a SparkSession specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器模式初始化SparkSession，从而为Spark集群提供入口点：
- en: '[PRE72]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Load the data file and turn it into RDDs:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据文件并将其转换为RDD：
- en: '[PRE73]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Ingest the data by splitting it and then converting to double while building
    a LabeledPoint (that is, data structure required by Spark) dataset. Column 1 (that
    is, position 0) is the dependent variable in the regression, while columns 2 through
    4 (that is, GRE, GPA, Rank) are the features:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过拆分数据并转换为双精度来摄取数据，同时构建一个LabeledPoint（即Spark所需的数据结构）数据集。列1（即位置0）是回归中的因变量，而列2到4（即GRE、GPA、Rank）是特征：
- en: '[PRE74]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Examine the dataset after loading, which is always recommended, and also demonstrate
    the Label point internals, which is a single value (that is, the label/dependent
    variable), followed by a DenseVector of the features we are trying to use to explain
    the dependent variable:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载后检查数据集，这是一直建议的，还演示标签点内部，这是一个单一值（即标签/因变量），后面是我们试图用来解释因变量的特征的DenseVector：
- en: '[PRE75]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Create a LBFGS regression object with new operator and set the intercept to
    false so we can compare the result equally with the `logisticregressionWithSGD()`
    recipe:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新运算符创建一个LBFGS回归对象，并将截距设置为false，以便我们可以与`logisticregressionWithSGD()`示例进行公平比较：
- en: '[PRE76]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Use the `run()` method to create the trained model using the dataset (that
    is, structured as LabeledPoint):'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`run()`方法使用数据集创建训练好的模型（即结构化为LabeledPoint）：
- en: '[PRE77]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The model is trained. Use the `predict()` method to have it predict and classify
    the groups accordingly. In the next lines, simply use a dense vector to define
    two students'' data (the GRE, GPA, and Rank features) and have it predict whether
    the student will be admitted or not (0 means denied admission and 1 means the
    student will be admitted):'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型已经训练好。使用`predict()`方法来预测并分类相应的组。在接下来的行中，只需使用一个密集向量来定义两个学生的数据（GRE、GPA和Rank特征），并让它预测学生是否被录取（0表示被拒绝，1表示学生将被录取）：
- en: '[PRE78]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The output will be as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE79]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'To show a slightly complicated process, define a SEQ data structure for five
    students and attempt to use `map()` and `predict()` in the next step to predict
    in bulk. It should be obvious that any data file can be read at this point and
    converted so we can predict in larger chunks:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了展示一个稍微复杂的过程，为五名学生定义一个SEQ数据结构，并尝试在下一步使用`map()`和`predict()`来批量预测。很明显，此时可以读取任何数据文件并转换，以便我们可以预测更大的数据块：
- en: '[PRE80]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now use `map()` and `predict()` to run through the SEQ data structure and produce
    predictions in bulk with the trained model:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用`map()`和`predict()`来运行SEQ数据结构，并使用训练好的模型批量产生预测：
- en: '[PRE81]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Look at the output and the resulting predictions for the students. The presence
    of 0 or 1 indicates denial or acceptance of the student based on the model:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看学生的输出和预测结果。0或1的存在表示基于模型的学生被拒绝或接受：
- en: '[PRE82]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: How it works...
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We used the UCI admission data with `LogisticRegressionWithLBFGS()` to predict
    whether a student will be admitted or not. The intercept was set to false and
    the `.run()` and `.predict()` API is used to predict with the fitted model. The
    point here was that L-BFGS is suitable for an extremely large number of parameters,
    particularly when there is a lot of sparsity. Regardless of what optimization
    technique was used, we emphasized again that ridge regression reduces the parameter
    weight, but never sets it to zero.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用UCI入学数据和`LogisticRegressionWithLBFGS()`来预测学生是否被录取。截距被设置为false，使用`.run()`和`.predict()`
    API来预测拟合模型。这里的重点是L-BFGS适用于大量参数，特别是在存在大量稀疏性时。无论使用了什么优化技术，我们再次强调岭回归可以减少参数权重，但永远不会将其设置为零。
- en: 'The Signature for this method constructor is as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法构造函数的签名如下：
- en: '[PRE83]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: L-BFGS optimization in Spark, `L-BFGS()`, is based on Newton's optimization
    algorithm (uses curvature in addition to 2^(nd) derivative of the curve at point),
    which can be thought of as a maximizing likelihood function that seeks a stationary
    point on a differentiable function. The convergence for this algorithm should
    be given special attention (that is, require optimal or gradient of zero).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的L-BFGS优化，`L-BFGS()`，基于牛顿优化算法（在点处使用曲率和曲线的2^(nd)导数），可以被认为是寻找可微函数上的稳定点的最大似然函数。这种算法的收敛应该特别注意（也就是说，需要最优或梯度为零）。
- en: Please note that this recipe is for demonstration purposes only and you should
    use the evaluation metrics demonstrated in [Chapter 4](part0200.html#5UNGG0-4d291c9fed174a6992fd24938c2f9c77),
    *Common Recipes for Implementing a Robust Machine Learning System*,for model evaluation
    and the final selection process.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本示例仅用于演示目的，您应该使用[第4章](part0200.html#5UNGG0-4d291c9fed174a6992fd24938c2f9c77)中演示的评估指标进行模型评估和最终选择过程。
- en: There's more...
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The `LogisticRegressionWithLBFGS()` object has a method called `setNumClasses()`
    that allows it to deal with multinomials (that is, more than two groups). By default,
    it is set to two, which is a binary logistic regression.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`LogisticRegressionWithLBFGS()`对象有一个名为`setNumClasses()`的方法，允许它处理多项式（也就是说，超过两个组）。默认情况下，它设置为二，这是二元逻辑回归。'
- en: L-BFGS is a limited memory adaptation of the original BFGS (Broyden-Fletcher-Goldfarb-Shanno)
    method. L-BFGS is well suited for regression models that deal with a large number
    of variables. It is a form of BFGS approximation with limited memory in which
    it tries to estimate the Hessian matrix while searching through the large search
    space.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: L-BFGS是原始BFGS（Broyden-Fletcher-Goldfarb-Shanno）方法的有限内存适应。L-BFGS非常适合处理大量变量的回归模型。它是一种带有有限内存的BFGS近似，它试图在搜索大搜索空间时估计Hessian矩阵。
- en: We encourage the reader to step back and look at the problem as regression plus
    an optimization technique (regression with SGD versus regression with L-BFGS).
    In this recipe, we used logistic regression, which itself is a form of linear
    regression except with discrete labels, plus an optimization algorithm (that is,
    we choose L-BFGS rather than SGD) to solve the problem.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励读者退后一步，将问题视为回归加上优化技术（使用SGD的回归与使用L-BFGS的回归）。在这个示例中，我们使用了逻辑回归，它本身是线性回归的一种形式，只是标签是离散的，再加上一个优化算法（也就是说，我们选择了L-BFGS而不是SGD）来解决问题。
- en: In order to appreciate the details of L-BFGS, one must understand the Hessian
    matrix and its role, along with the concomitant difficulties with large numbers
    of parameters (Hessian or Jacobian techniques), especially when using a sparse
    matrix configuration in optimization.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了欣赏L-BFGS的细节，必须了解Hessian矩阵及其作用，以及在优化中使用稀疏矩阵配置时出现的大量参数（Hessian或Jacobian技术）的困难。
- en: See also
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Here is the documentation for the constructor:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这是构造函数的文档：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS)'
- en: Support Vector Machine (SVM) with Spark 2.0
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0中的支持向量机（SVM）
- en: In this recipe, we use Spark's RDD-based SVM API `SVMWithSGD` with SGD to classify
    the population into two binary classes, and then use count and `BinaryClassificationMetrics`
    to look at model performance.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用Spark的基于RDD的SVM API `SVMWithSGD`与SGD将人口分类为两个二进制类，并使用计数和`BinaryClassificationMetrics`来查看模型性能。
- en: In the interest of time and space, we use the sample `LIBSVM` format already
    supplied with Spark, but provide links to additional data files offered by National
    Taiwan University so the reader can experiment on their own. **Support Vector
    Machine** (**SVM**) as a concept is fundamentally very simple, unless you want
    to get into the details of its implementation in Spark or any other package.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省时间和空间，我们使用了已经提供给Spark的样本`LIBSVM`格式，但提供了由台湾大学提供的额外数据文件的链接，以便读者可以自行进行实验。**支持向量机**（**SVM**）作为一个概念基本上非常简单，除非你想深入了解它在Spark或任何其他软件包中的实现细节。
- en: While the mathematics behind SVM is beyond the scope of this book, readers are
    encouraged to read the following tutorials and the original SVM paper for a deeper
    understanding.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SVM背后的数学超出了本书的范围，但鼓励读者阅读以下教程和原始SVM论文，以便更深入地理解。
- en: 'The original papers are by *Vapnik* and *Chervonenkis* (1974, 1979 - in Russian)
    and there''s also *Vapnik''s* 1982 translation of his 1979 book:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文件是由*Vapnik*和*Chervonenkis*（1974年，1979年-俄语）编写的，还有*Vapnik*的1982年翻译他的1979年著作：
- en: '[https://www.amazon.com/Statistical-Learning-Theory-Vladimir-Vapnik/dp/0471030031](https://www.amazon.com/Statistical-Learning-Theory-Vladimir-Vapnik/dp/0471030031)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.amazon.com/Statistical-Learning-Theory-Vladimir-Vapnik/dp/0471030031](https://www.amazon.com/Statistical-Learning-Theory-Vladimir-Vapnik/dp/0471030031)'
- en: 'For a more modern write up, we recommend the following three books from our
    library:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更现代的写作，我们建议从我们的图书馆中选择以下三本书：
- en: '*The Nature of Statistical Learning Theory* by V. Vapnik:'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V. Vapnik的《统计学习理论的本质》*：'
- en: '[https://www.amazon.com/Statistical-Learning-Information-Science-Statistics/dp/0387987800](https://www.amazon.com/Statistical-Learning-Information-Science-Statistics/dp/0387987800)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.amazon.com/Statistical-Learning-Information-Science-Statistics/dp/0387987800](https://www.amazon.com/Statistical-Learning-Information-Science-Statistics/dp/0387987800)'
- en: '*Learning with Kernels: Support Vector Machines, Regularization, Optimization,
    and Beyond* by B. Scholkopf and A. Smola:'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*B. Scholkopf和A. Smola的《使用核方法学习：支持向量机、正则化、优化和更多》*：'
- en: '[https://mitpress.mit.edu/books/learning-kernels](https://mitpress.mit.edu/books/learning-kernels)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://mitpress.mit.edu/books/learning-kernels](https://mitpress.mit.edu/books/learning-kernels)'
- en: '*Machine Learning: A Probabilistic Perspective* by K. Murphy:'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*K. Murphy的《机器学习：概率视角》*：'
- en: '[https://mitpress.mit.edu/books/machine-learning-0](https://mitpress.mit.edu/books/machine-learning-0)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://mitpress.mit.edu/books/machine-learning-0](https://mitpress.mit.edu/books/machine-learning-0)'
- en: How to do it...
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE84]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Import the necessary packages for SparkSession to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的SparkSession包以访问集群和`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE85]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Initialize a SparkSession specifying configurations with the builder pattern
    thus making an entry point available for the Spark cluster:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建模式初始化SparkSession，从而为Spark集群提供入口点：
- en: '[PRE86]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Spark provides the MLUtils package, which enables us to read any file that
    is properly formatted as `libsvm`. We use `LoadLibSVMFile()` to load one of the
    short sample files (100 rows) that is included with Spark for easy experimentation.
    The `sample_libsvm_data` file can be found in the `.../data/mlib/` directory of
    Spark home. We simply copy the file into our own directory on a Windows machine
    as is:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark提供了MLUtils包，使我们能够读取任何格式正确的`libsvm`文件。我们使用`LoadLibSVMFile()`来加载Spark附带的一个短样本文件（100行），以便进行简单的实验。`sample_libsvm_data`文件可以在Spark主目录的`.../data/mlib/`目录中找到。我们只需将文件复制到我们自己的Windows机器上的目录中：
- en: '[PRE87]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Print and examine the content of the sample file via output. A short version
    of the output is included for reference:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印并检查样本文件的内容输出。输出中包含了简短版本的内容以供参考：
- en: '[PRE88]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Check to make sure all the data is loaded and there are no duplicates of the
    file:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查确保所有数据都已加载，并且文件没有重复：
- en: '[PRE89]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'In this step, split the data into two sets (80/20) and get ready to train the
    model accordingly. The `allDataSVM` variable will have two randomly selected sections
    based on the split ratios. The sections can be referenced by an index, 0 and 1
    to refer to the training and test datasets respectively. You can also use a second
    parameter in `randomSplit()` to define the initial seed for random split:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，将数据分成两组（80/20），并准备相应地训练模型。`allDataSVM`变量将根据拆分比例随机选择两个部分。这些部分可以通过索引0和1来引用，分别指代训练和测试数据集。您还可以在`randomSplit()`中使用第二个参数来定义随机拆分的初始种子：
- en: '[PRE90]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Set the number of iterations to 100\. The next two parameters are SGD steps
    and the regularization parameter - we use the defaults here, but you must experiment
    to make sure the algorithms converge:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将迭代次数设置为100。接下来的两个参数是SGD步骤和正则化参数-我们在这里使用默认值，但您必须进行实验以确保算法收敛：
- en: '[PRE91]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'After training the model in the previous step, now use the `map()` and `predict()`
    functions to predict the outcome for the test data (that is, index 1 of the split
    data):'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一步中训练模型之后，现在使用`map()`和`predict()`函数来预测测试数据的结果（即拆分数据的索引1）：
- en: '[PRE92]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Visually examine the predictions via output (shortened for convenience). The
    next steps attempt to quantify how well we did with our predictions:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过输出直观地检查预测（为方便起见进行了缩短）。接下来的步骤尝试量化我们的预测效果：
- en: '[PRE93]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'First, use a quick count/ratio method to get a feel for the accuracy. Since
    we did not set the seed, the numbers will vary from run to run (but remain stable):'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用快速计数/比率方法来感受准确度。由于我们没有设置种子，数字将因运行而异（但保持稳定）：
- en: '[PRE94]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Now use a more formal way to quantify the ROC (that is, the area under the curve).
    This is one of the most basic standard measures of accuracy. Readers can find
    many tutorials on this subject. We use a combination of standard and proprietary
    methods (that is, hand-coded) to quantify the measurement.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用更正式的方法来量化ROC（即曲线下面积）。这是最基本的准确度标准之一。读者可以在这个主题上找到许多教程。我们使用标准和专有方法（即手工编码）的组合来量化测量。
- en: 'Spark comes with a binary classification quantification measurement out of
    the box. Use this to collect the measurement:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark自带了一个二元分类量化测量。使用这个来收集测量：
- en: '[PRE95]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Access the `areaUnderROC()` method to obtain the ROC measurement:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问`areaUnderROC()`方法以获取ROC测量：
- en: '[PRE96]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: How it works...
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We used the sample data provided with Spark, which is in `LIBSVM` format, to
    run the SVM classification recipe. After reading the file, we used `SVMWithSGD.train`
    to train the model and then proceeded to predict the data into two sets of labeled
    output, 0 and 1\. We used the `BinaryClassificationMetrics` metric to measure
    the performance. We focused on a popular metric, the area under the ROC curve,
    using `metrics.areaUnderROC()` to measure performance.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了Spark提供的样本数据，格式为`LIBSVM`，来运行SVM分类配方。在读取文件后，我们使用`SVMWithSGD.train`来训练模型，然后继续将数据预测为两组标记输出，0和1。我们使用`BinaryClassificationMetrics`指标来衡量性能。我们专注于一个流行的指标，即ROC曲线下面积，使用`metrics.areaUnderROC()`来衡量性能。
- en: 'The Signature for this method constructor is as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法构造函数的签名如下：
- en: '[PRE97]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Defaults for Parameters:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的默认值：
- en: '`stepSize`= 1.0'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stepSize`= 1.0'
- en: '`numIterations`= 100'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numIterations`= 100'
- en: '`regParm`= 0.01'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`regParm`= 0.01'
- en: '`miniBatchFraction`= 1.0'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`miniBatchFraction`= 1.0'
- en: It is suggested that the readers should experiment with various parameters in
    order to get the best settings.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 建议读者尝试各种参数以获得最佳设置。
- en: What makes SVM great is the fact that it is OK for some of the points to fall
    on the wrong side, but the model penalizes the models to pick the best fit.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: SVM之所以伟大，是因为一些点落在错误的一侧是可以接受的，但模型会惩罚模型选择最佳拟合。
- en: 'The SVM implementation in Spark uses SGD optimization to classify the labels
    for the feature sets. When we use SVM in Spark, we need to prepare the data into
    a format called `libsvm`. The user can use the following links to understand the
    format and also obtain ready-to-use datasets in `libsvm` format from National
    Taiwan University:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的SVM实现使用SGD优化来对特征集的标签进行分类。当我们在Spark中使用SVM时，我们需要将数据准备成一种称为`libsvm`的格式。用户可以使用以下链接了解格式，并从国立台湾大学获取`libsvm`格式的现成数据集：
- en: '[https://www.csie.ntu.edu.tw/~cjlin/libsvm/](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.csie.ntu.edu.tw/~cjlin/libsvm/](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)'
- en: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/)'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/)'
- en: 'In short, the `libsvm` format is as follows:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，`libsvm`格式如下：
- en: '[PRE98]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: You can simply create pipelines with Python or Scala programs to transform text
    files into `libsvm` format as needed.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Python或Scala程序创建管道，将文本文件转换为所需的`libsvm`格式。
- en: 'Spark has a good number of sample files for various algorithms in the `/data/mlib`
    directory. We encourage the readers to use these files while getting familiar
    with the Spark MLlib algorithms:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在`/data/mlib`目录中有大量各种算法的示例文件。我们鼓励读者在熟悉Spark MLlib算法时使用这些文件：
- en: '[PRE99]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '**Receiver Operating Characteristic** (**ROC**) is a graphical plot that illustrates
    the diagnostic ability of a binary classifier system, as its discrimination threshold
    is varied.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收器操作特性**（**ROC**）是一个图形绘图，说明了二元分类器系统的诊断能力，随着其判别阈值的变化。'
- en: 'A tutorial for ROC can be found at the following link:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ROC的教程可以在以下链接找到：
- en: '[https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)'
- en: There's more...
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You can either use publically available data sources in `libsvm` format or
    a Spark API call, `SVMDataGenerator()`, which generates sample data for SVM (that
    is, Gaussian distribution):'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`libsvm`格式的公开可用数据源，也可以使用Spark API调用`SVMDataGenerator()`，该调用会生成SVM的样本数据（即，高斯分布）：
- en: '[PRE100]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'The idea behind SVM can be summarized as follows: rather than using a linear
    discriminant (for example, selecting a line among many lines) and an objective
    function (for example, least square minimization) to separate and label the left-hand
    variable, use the largest separating margin (as shown in the following graph)
    first and then draw the solid line in between the largest margin. Another way
    to think about it is how you can use two lines (the dashed lines in the following
    graph) to separate the classes the most (that is, the best and most discriminate
    separators). In short, the wider we can separate the classes, the better the discrimination,
    and hence, more accuracy in labeling the classes.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: SVM背后的想法可以总结如下：不是使用线性判别（例如，在许多线中选择一条线）和目标函数（例如，最小二乘法）来分隔和标记左侧变量，而是首先使用最大分隔边界（如下图所示），然后在最大边界之间绘制实线。另一种思考方式是如何使用两条线（下图中的虚线）来最大程度地分隔类别（即，最好和最具有歧视性的分隔器）。简而言之，我们能够分隔类别的越宽，歧视性就越好，因此，在标记类别时更准确。
- en: 'Perform the following steps to find out more about SVM:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以了解有关SVM的更多信息：
- en: Pick the widest margin that can best separate the two groups.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最能够分隔两组的最宽边界。
- en: Second, draw a line that divides the widest margin. This will serve as a linear
    discriminant.
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，绘制一条分隔最宽边界的线。这将作为线性判别。
- en: 'The objective function: maximize the two separating lines.'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标函数：最大化两条分隔线。
- en: '![](img/00138.gif)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00138.gif)'
- en: See also
  id: totrans-401
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Here is the documentation for the constructor:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 这是构造函数的文档：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.SVMWithSGD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.SVMWithSGD)'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.SVMWithSGD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.SVMWithSGD)'
- en: Naive Bayes machine learning with Spark 2.0 MLlib
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark 2.0 MLlib的朴素贝叶斯机器学习
- en: In this recipe, we use the famous Iris dataset and use Apache Spark API `NaiveBayes()`
    to classify/predict which of the three classes of flower a given set of observations
    belongs to. This is an example of a multi-class classifier and requires multi-class
    metrics for measurements of fit. The previous recipe used a binary classification
    and metric to measure the fit.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用著名的鸢尾花数据集，并使用Apache Spark API `NaiveBayes()`来对给定的一组观测属于三类花中的哪一类进行分类/预测。这是一个多类分类器的示例，并需要多类度量来衡量拟合度。之前的示例使用了二元分类和度量来衡量拟合度。
- en: How to do it...
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: For the Naive Bayes exercise, we use a famous dataset called `iris.data`, which
    can be obtained from UCI. The dataset was originally introduced in the 1930s by
    R. Fisher. The set is a multivariate dataset with flower attribute measurements
    classified into three groups.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于朴素贝叶斯练习，我们使用一个名为`iris.data`的著名数据集，可以从UCI获得。该数据集最初是由R. Fisher在1930年代引入的。该集是一个多元数据集，具有被分类为三组的花属性测量。
- en: In short, by measuring four columns, we attempt to classify a species into one
    of the three classes of Iris flower (that is, Iris Setosa, Iris Versicolor, Iris
    Virginica).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，通过测量四列，我们试图将一种物种分类为三类鸢尾花中的一类（即，鸢尾花Setosa，鸢尾花Versicolor，鸢尾花Virginica）。
- en: 'We can download the data from here:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这里下载数据：
- en: '[https://archive.ics.uci.edu/ml/datasets/Iris/](https://archive.ics.uci.edu/ml/datasets/Iris/)'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/datasets/Iris/](https://archive.ics.uci.edu/ml/datasets/Iris/)'
- en: 'The column definition is as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 列定义如下：
- en: Sepal length in cm
  id: totrans-412
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以厘米为单位的萼片长度
- en: Sepal width in cm
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以厘米为单位的萼片宽度
- en: Petal length in cm
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以厘米为单位的花瓣长度
- en: Petal width in cm
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以厘米为单位的花瓣宽度
- en: 'Class:'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类：
- en: -- Iris Setosa => Replace it with 0
  id: totrans-417
  prefs:
  - PREF_UL
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: -- 鸢尾花山鸢尾 => 用0替换
- en: -- Iris Versicolour => Replace it with 1
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -- 鸢尾花变色鸢尾 => 用1替换
- en: -- Iris Virginica => Replace it with 2
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -- 鸢尾花维吉尼亚 => 用2替换
- en: 'The steps/actions we need to perform on the data are as follows:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对数据执行的步骤/操作如下：
- en: Download and then replace column five (that is, the label or classification
    classes) with a numerical value, thus producing the iris.data.prepared data file.
    The Naïve Bayes call requires numerical labels and not text, which is very common
    with most tools.
  id: totrans-421
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载并用数字值替换第五列（即标签或分类类别），从而生成iris.data.prepared数据文件。朴素贝叶斯调用需要数字标签而不是文本，这在大多数工具中都很常见。
- en: Remove the extra lines at the end of the file.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除文件末尾的额外行。
- en: Remove duplicates within the program by using the `distinct()` call.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`distinct()`调用在程序内去除重复项。
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中开始一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE101]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Import the necessary packages for SparkSession to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入SparkSession所需的包，以访问集群和`Log4j.Logger`以减少Spark产生的输出量：
- en: '[PRE102]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Initialize a SparkSession specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个SparkSession，使用构建器模式指定配置，从而为Spark集群提供一个入口点：
- en: '[PRE103]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Load the `iris.data` file and turn the data file into RDDs:'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`iris.data`文件并将数据文件转换为RDDs：
- en: '[PRE104]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Parse the data using `map()` and then build a LabeledPoint data structure.
    In this case, the last column is the Label and the first four columns are the
    features. Again, we replace the text in the last column (that is, the class of
    Iris) with numeric values (that is, 0, 1, 2) accordingly:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`map()`解析数据，然后构建一个LabeledPoint数据结构。在这种情况下，最后一列是标签，前四列是特征。同样，我们将最后一列的文本（即鸢尾花的类别）替换为相应的数值（即0、1、2）：
- en: '[PRE105]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Then make sure that the file does not contain any redundant rows. In this case,
    it has three redundant rows. We will use the distinct dataset going forward:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后确保文件不包含任何冗余行。在这种情况下，有三行冗余。我们将使用不同的数据集继续：
- en: '[PRE106]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'We inspect the data by examining the output:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过检查输出来检查数据：
- en: '[PRE107]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Split the data into training and test sets using a 30% and 70% ratio. The 13L
    in this case is simply a seeding number (L stands for long data type) to make
    sure the result does not change from run to run when using a `randomSplit()` method:'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用30%和70%的比例将数据分割为训练集和测试集。在这种情况下，13L只是一个种子数（L代表长数据类型），以确保在使用`randomSplit()`方法时结果不会因运行而改变：
- en: '[PRE108]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Print the count for each set:'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印每个集合的计数：
- en: '[PRE109]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Build the model using `train()` and the training dataset:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train()`和训练数据集构建模型：
- en: '[PRE110]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Use the training dataset plus the `map()` and `predict()` methods to classify
    the flowers based on their features:'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练数据集加上`map()`和`predict()`方法根据它们的特征对花进行分类：
- en: '[PRE111]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Examine the predictions via the output:'
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过输出检查预测：
- en: '[PRE112]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Use `MulticlassMetrics()` to create metrics for the multi-class classifier.
    As a reminder, this is different from the previous recipe, in which we used `BinaryClassificationMetrics()`:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`MulticlassMetrics()`创建多类分类器的度量。提醒一下，这与之前的方法不同，之前我们使用的是`BinaryClassificationMetrics()`：
- en: '[PRE113]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Use the commonly used confusion matrix to evaluate the model:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用常用的混淆矩阵来评估模型：
- en: '[PRE114]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'We examine other properties to evaluate the model:'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们检查其他属性来评估模型：
- en: '[PRE115]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: How it works...
  id: totrans-455
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We used the IRIS dataset for this recipe, but we prepared the data ahead of
    time and then selected the distinct number of rows by using the `NaiveBayesDataSet.distinct()`
    API. We then proceeded to train the model using the `NaiveBayes.train()` API.
    In the last step, we predicted using `.predict()` and then evaluated the model
    performance via `MulticlassMetrics()` by outputting the confusion matrix, precision,
    and F-Measure metrics.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了IRIS数据集进行这个方法，但是我们提前准备了数据，然后使用`NaiveBayesDataSet.distinct()` API选择了不同数量的行。然后我们使用`NaiveBayes.train()`
    API训练模型。在最后一步，我们使用`.predict()`进行预测，然后通过`MulticlassMetrics()`评估模型性能，输出混淆矩阵、精度和F-度量。
- en: The idea here was to classify the observations based on a selected feature set
    (that is, feature engineering) into classes that correspond to the left-hand label.
    The difference here was that we are applying joint probability given conditional
    probability to the classification. This concept is known as **Bayes' theorem**,
    which was originally proposed by Thomas Bayes in the 18th century. There is a
    strong assumption of independence that must hold true for the underlying features
    to make Bayes' classifier work properly.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是根据选择的特征集（即特征工程）对观察结果进行分类，使其对应于左侧的标签。这里的不同之处在于，我们将联合概率应用于分类的条件概率。这个概念被称为**贝叶斯定理**，最初由18世纪的托马斯·贝叶斯提出。必须满足独立性的强烈假设，以使贝叶斯分类器正常工作。
- en: 'At a high level, the way we achieved this method of classification was to simply
    apply Bayes'' rule to our dataset. As a refresher from basic statistics, Bayes''
    rule can be written as follows:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，我们实现这种分类方法的方式是简单地将贝叶斯定理应用于我们的数据集。作为基本统计学的复习，贝叶斯定理可以写成如下形式：
- en: '![](img/00139.gif)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00139.gif)'
- en: The formula states that the probability of A given B is true is equal to probability
    of B given A is true times probability of A being true divided by probability
    of B being true. It is a complicated sentence, but if we step back and think about
    it, it will make sense.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式说明了给定B为真时A为真的概率等于给定A为真时B为真的概率乘以A为真的概率除以B为真的概率。这是一个复杂的句子，但如果我们退后一步思考，它就会有意义。
- en: The Bayes' classifier is a simple yet powerful one that allows the user to take
    the entire probability feature space into consideration. To appreciate its simplicity,
    one must remember that probability and frequency are two sides of the same coin.
    The Bayes' classifier belongs to the incremental learner class in which it updates
    itself upon encountering a new sample. This allows the model to update itself
    on-the-fly as the new observation arrives rather than only operating in batch
    mode.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯分类器是一个简单而强大的分类器，允许用户考虑整个概率特征空间。要欣赏其简单性，必须记住概率和频率是同一枚硬币的两面。贝叶斯分类器属于增量学习器类，遇到新样本时会更新自身。这使得模型能够在新观察到达时即时更新自身，而不仅仅在批处理模式下运行。
- en: There's more...
  id: totrans-462
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We evaluated a model with different metrics. Since this is a multi-class classifier,
    we have to use `MulticlassMetrics()` to examine model accuracy.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用不同的指标评估了一个模型。由于这是一个多类分类器，我们必须使用`MulticlassMetrics()`来检查模型的准确性。
- en: 'For more information, see the following link:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参见以下链接：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)'
- en: See also
  id: totrans-466
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Here is the documentation for the constructor:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 这是构造函数的文档：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.NaiveBayes](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.NaiveBayes)'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.NaiveBayes](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.classification.NaiveBayes)'
- en: Exploring ML pipelines and DataFrames using logistic regression in Spark 2.0
  id: totrans-469
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark 2.0中使用逻辑回归探索ML管道和数据框
- en: We have gone out of our way to present the code in detail and as simply as possible
    so you can get started without the additional syntactic sugar that Scala uses.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经尽力以尽可能简单的方式详细呈现代码，以便您可以开始，而无需使用Scala的额外语法糖。
- en: Getting ready
  id: totrans-471
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we combine the ML pipelines and logistic regression to demonstrate
    how you can combine various steps in a single pipeline that operates on DataFrames
    as they get transformed and travel through the pipe. We skip some of the steps,
    such as splitting the data and model evaluation, and reserve them for later chapters
    to make the program shorter, but provide a full treatment of pipeline, DataFrame,
    estimators, and transformers in a single recipe.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将ML管道和逻辑回归结合起来，以演示如何将各种步骤组合成单个管道，该管道在数据框上操作，使其在转换和传输过程中。我们跳过了一些步骤，比如数据拆分和模型评估，并将它们保留到后面的章节中，以使程序更短，但提供了管道、数据框、估计器和转换器的全面处理。
- en: This recipe explores the details of the pipeline and DataFrames as they travel
    through the pipeline and get operated on.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程探讨了管道和数据框在管道中传输并进行操作的细节。
- en: How to do it...
  id: totrans-474
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ或您选择的IDE中启动一个新项目。确保包含必要的JAR文件。
- en: 'Set up the package location where the program will reside:'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置程序所在的包位置：
- en: '[PRE116]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Import the `LogisticRegression` package required to build and train the model.
    There are others forms of `LogisticRegression` in Spark MLlib, but for now we
    just concentrate on the basic logistic regression method:'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`LogisticRegression`包，以构建和训练模型所需。在Spark MLlib中有其他形式的`LogisticRegression`，但现在我们只集中在基本的逻辑回归方法上：
- en: '[PRE117]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Import SparkSession so we can gain access to the cluster, and Spark SQL, and
    hence the DataFrame and Dataset abstractions as needed via SparkSession:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入SparkSession，以便我们可以通过SparkSession获得对集群、Spark SQL以及DataFrame和Dataset抽象的访问：
- en: '[PRE118]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Import the Vector Package from `ml.linlang`. This will allow us to import and
    use vectors, both dense and sparse, from the Spark ecosystem:'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`ml.linlang`导入Vector包。这将允许我们从Spark生态系统中导入和使用向量，包括密集和稀疏向量：
- en: '[PRE119]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Import the necessary packages from `log4j` so we can set the output level to
    ERROR and make the output less verbose for the program:'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`log4j`导入必要的包，这样我们就可以将输出级别设置为ERROR，并使程序的输出更简洁：
- en: '[PRE120]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Use the imported SparkSession to set various parameters needed to successfully
    initiate and gain a handle to the Spark cluster. The style for instantiating and
    getting access to Spark has changed in Spark 2.0\. See the *There's more...* section
    in this recipe for more detail.
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用导入的SparkSession设置各种参数，以成功初始化并获得对Spark集群的控制。在Spark 2.0中，实例化和访问Spark的方式已经发生了变化。有关更多详细信息，请参阅本教程中的*There's
    more...*部分。
- en: 'Set the parameters as follows:'
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置参数如下：
- en: '[PRE121]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Set the type of Spark cluster that you need and define the additional parameters
    needed to get access to Spark.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置您需要的Spark集群类型，并定义获取对Spark的访问所需的其他参数。
- en: Here, set it to a local cluster and let it grab as many threads/cores as are
    available. You can use a number rather than `*` to tell Spark exactly how many
    cores/threads to
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，将其设置为本地集群，并让它抓取尽可能多的线程/核心。您可以使用数字而不是`*`来告诉Spark确切有多少核心/线程
- en: '[PRE122]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'You have the option of specifying the exact number of cores you want to allocate
    by using a number rather than `*`:'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以选择指定要分配的确切核心数，而不是使用`*`：
- en: '[PRE123]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: This will allocate two cores. This might come handy on smaller laptops with
    limited resources.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 这将分配两个核心。这在资源有限的较小笔记本电脑上可能会很方便。
- en: 'Set the application name so it is easy to trace if more than one app is running
    on the cluster:'
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置应用程序名称，以便在集群上运行多个应用程序时易于跟踪：
- en: '[PRE124]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'Configure the working directory relative to Spark home:'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相对于Spark主目录配置工作目录：
- en: '[PRE125]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'We now progress toward building the data structure needed to house the first
    20 rows of the downloaded student admission data (see the previous recipe):'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们继续构建所需的数据结构，以容纳下载的学生入学数据的前20行（请参阅前一个示例）：
- en: '[PRE126]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'The best way to understand a given row is to look at it as two parts:'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解给定行的最佳方法是将其视为两部分：
- en: The label - 0.0, meaning the student was not admitted.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签 - 0.0，表示学生未被录取。
- en: The feature vector - `Vectors.dense(380.0, 3.61, 3.0)`, which shows the student's
    GRE, GPA, and RANK. We will cover the details of a dense vector in upcoming chapters.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征向量 - `Vectors.dense(380.0, 3.61, 3.0)`，显示了学生的GRE、GPA和RANK。我们将在接下来的章节中介绍密集向量的细节。
- en: 'SEQ is a Scala collection with special properties. Sequences can be thought
    of as iterable data structures with defined orders. For more information on SEQ
    see the following URL:'
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SEQ是具有特殊属性的Scala集合。序列可以被视为具有定义顺序的可迭代数据结构。有关SEQ的更多信息，请参阅以下URL：
- en: '[http://www.scala-lang.org/api/current/index.html#scala.collection.Seq](http://www.scala-lang.org/api/current/index.html#scala.collection.Seq)'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.scala-lang.org/api/current/index.html#scala.collection.Seq](http://www.scala-lang.org/api/current/index.html#scala.collection.Seq)'
- en: 'The next step takes the SEQ structure and converts it into a DataFrame. We
    highly recommend that you use DataFrame and Dataset rather than the low-level
    RDDs for any new programming, in order to stay aligned with the new Spark programming
    paradigm:'
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步将SEQ结构转换为DataFrame。我们强烈建议您在任何新编程中使用DataFrame和Dataset，而不是低级别的RDD，以便与新的Spark编程范式保持一致：
- en: '[PRE127]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '`label` and `feature` will be the column headings in the DataFrame.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '`label`和`feature`将成为DataFrame中的列标题。'
- en: 'An Estimator is an API abstraction that accepts its data as a DataFrame and
    produces an actual model by calling the `Fit` function. Here, we create an Estimator
    from the `LogisticRegression` class in Spark MLlib and then set the maximum iteration
    to 80; it is 100 by default. We set the regularization parameter to 0.01 and tell
    the model we want to also fit an intercept:'
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Estimator是一个接受DataFrame作为其数据的API抽象，并通过调用`Fit`函数生成实际模型。在这里，我们从Spark MLlib中的`LogisticRegression`类创建一个Estimator，然后将最大迭代次数设置为80；默认值为100。我们将正则化参数设置为0.01，并告诉模型我们也想拟合一个截距：
- en: '[PRE128]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'To get a better feeling for what the program does, see the following output
    and examine the parameters:'
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更好地了解程序的功能，请查看以下输出并检查参数：
- en: '[PRE129]'
  id: totrans-512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Output is as follows:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE130]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'Here is how to interpret and understand the `Admission_lr_Model` parameters
    listed in the previous step:'
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是如何解释和理解前一步中列出的`Admission_lr_Model`参数：
- en: '`elasticNetParam`: The ElasticNet mixing parameter, in the range [0, 1]. For
    alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default:
    0.0).'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`elasticNetParam`: ElasticNet混合参数，范围为[0, 1]。对于alpha = 0，惩罚是L2惩罚。对于alpha = 1，它是L1惩罚（默认值：0.0）。'
- en: '`featuresCol`: Features column name (default: features).'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`featuresCol`: 特征列名称（默认值：features）。'
- en: '`fitIntercept`: Whether to fit an intercept term (default: true, current: true).'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fitIntercept`: 是否拟合截距项（默认值：true，当前值：true）。'
- en: '`labelCol`: Label column name (default: label).'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labelCol`: 标签列名称（默认值：label）。'
- en: '`maxIter`: Maximum number of iterations (>= 0) (default: 100, current: 80).'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxIter`: 最大迭代次数（>= 0）（默认值：100，当前值：80）。'
- en: '`predictionCol`: Prediction column name (default: prediction).'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predictionCol`: 预测列名称（默认值：prediction）。'
- en: '`probabilityCol`: Column name for predicted class conditional probabilities.
    Note that not all models output well-calibrated probability estimates! These probabilities
    should be treated as confidences, not precise probabilities (default: probability).'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`probabilityCol`: 预测类条件概率的列名。请注意，并非所有模型都输出经过良好校准的概率估计！这些概率应被视为置信度，而不是精确概率（默认值：probability）。'
- en: '`rawPredictionCol`: Raw prediction, otherwise known as confidence, column name
    (default: rawPrediction).'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rawPredictionCol`: 原始预测，也称为置信度，列名（默认值：rawPrediction）。'
- en: '`regParam`: Regularization parameter (>= 0) (default: 0.0, current: 0.01).'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`regParam`: 正则化参数（>= 0）（默认值：0.0，当前值：0.01）。'
- en: '`standardization`: Whether to standardize the training features before fitting
    the model (default: true).'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`standardization`: 在拟合模型之前是否对训练特征进行标准化（默认值：true）。'
- en: '`threshold`: Threshold in binary classification prediction, in range [0, 1]
    (default: 0.5).'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold`: 二元分类预测中的阈值，范围为[0, 1]（默认值：0.5）。'
- en: '`thresholds`: Thresholds in multi-class classification to adjust the probability
    of predicting each class. The array must have a length equal to the number of
    classes, with values >= 0\. The class with the largest value p/t is predicted,
    where p is the original probability of that class and t is the class'' threshold
    (undefined).'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thresholds`: 多类分类中的阈值，用于调整预测每个类的概率。数组的长度必须等于类的数量，其值>= 0。预测具有最大值p/t的类，其中p是该类的原始概率，t是类的阈值（未定义）。'
- en: '`tol`: The convergence tolerance for iterative algorithms (default: 1.0E-6).'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tol`: 迭代算法的收敛容限（默认值：1.0E-6）。'
- en: 'Now make the call to fit the prepared DataFrame and produce our logistic regression
    model:'
  id: totrans-529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在调用fit准备好的DataFrame并生成我们的逻辑回归模型：
- en: '[PRE131]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'Now explore the model summary to understand what we get after fitting. We need
    to understand the components so we know what to extract for next steps:'
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在探索模型摘要，以了解拟合后我们得到了什么。我们需要了解组件，以便知道要提取什么内容进行下一步操作：
- en: '[PRE132]'
  id: totrans-532
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'Here is the output:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE133]'
  id: totrans-534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'Now build the actual and final model from our training DataFrame. Take the
    Estimator we created and have it run the model by executing the transform function.
    We will now have a new DataFrame with all the parts populated (for example, predictions).
    Print the schema for our DataFrame to get a feel for the newly populated DataFrame:'
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在从我们的训练DataFrame构建实际和最终模型。取我们创建的Estimator，并让它通过执行transform函数来运行模型。现在我们将有一个新的DataFrame，其中所有部分都被填充（例如，预测）。打印我们的DataFrame的模式，以了解新填充的DataFrame的情况：
- en: '[PRE134]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: This is the actual transformation step.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 这是实际的转换步骤。
- en: 'Print the schema to understand the newly populated DataFrame:'
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印模式以了解新填充的DataFrame：
- en: '[PRE135]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'The output is as follows:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE136]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: The first two columns are our label and features vector as we set in our API
    call when converting to DataFrames. The `rawPredictions` column is referred to
    as confidence. The probability column will contain our probability pair. The last
    column, prediction, will be the outcome predicted by our model. This shows us
    the structure for the fitted model and what information is available for each
    parameter.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 前两列是我们的标签和特征向量，就像我们在API调用中设置的那样，当转换为DataFrame时。`rawPredictions`列被称为置信度。概率列将包含我们的概率对。最后一列，预测，将是我们模型预测的结果。这向我们展示了拟合模型的结构以及每个参数可用的信息。
- en: 'We now proceed with extracting the model parameter for the regression. To make
    the code clear and simple, we extract each parameter''s property separately into
    a collection:'
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在继续提取回归模型的参数。为了使代码清晰简单，我们将每个参数的属性分别提取到一个集合中：
- en: '[PRE137]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'For information only purposes, we print the number of the original training
    set:'
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅供信息目的，我们打印原始训练集的数量：
- en: '[PRE138]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'The output is as follows:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE139]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'We now proceed to extract the model predictions (outcome, confidence, and probability)
    for each row:'
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在继续提取每一行的模型预测（结果、置信度和概率）：
- en: '[PRE140]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'The output is as follows:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE141]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: Looking at the output from the previous step, we can examine how the model performed
    and what it predicted versus the actual via the output. In the subsequent chapters,
    we will use models to predict outcomes.
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过查看上一步的输出，我们可以检查模型的表现以及它的预测与实际情况的对比。在接下来的章节中，我们将使用模型来预测结果。
- en: 'Here''s some examples from a couple of rows:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些来自几行的示例：
- en: '**Row 10**: Model predicted correctly'
  id: totrans-555
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第10行**：模型预测正确'
- en: '**Row 13**: Model predicted incorrectly'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第13行**：模型预测错误'
- en: 'In the last step, we stop the cluster and signal resource de-allocation:'
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最后一步，我们停止集群并发出资源释放的信号：
- en: '[PRE142]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: How it works...
  id: totrans-559
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We started by defining a `Seq` data structure to house a series of vectors,
    each being a label and a feature vector. We then proceeded to convert the data
    structure to a DataFrame and ran it through `Estimator.fit()`to produce a model
    that fits the data. We examined the model's parameters and DataFrame schemas to
    understand the resulting model. We then proceeded to combine `.select()` and `.predict()`
    to decompose the DataFrame before looping to display the predictions and result.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了一个`Seq`数据结构来容纳一系列向量，每个向量都是一个标签和一个特征向量。然后我们将数据结构转换为DataFrame，并通过`Estimator.fit()`运行它以产生适合数据的模型。我们检查了模型的参数和DataFrame模式，以了解产生的模型。然后我们继续组合`.select()`和`.predict()`来分解DataFrame，然后循环显示预测和结果。
- en: While we don't have to use pipelines (a workflow concept in Spark borrowed from
    scikit-learn, [http://scikit-learn.org/stable/index.html](http://scikit-learn.org/stable/index.html))
    to run a regression, we decided to expose you to the power of Spark ML pipelines
    and logistic regression algorithms in an all-in-one recipe.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不必使用流水线（Spark中从scikit-learn借鉴的工作流概念，[http://scikit-learn.org/stable/index.html](http://scikit-learn.org/stable/index.html)）来运行回归，但我们决定向您展示Spark
    ML流水线和逻辑回归算法的强大功能。
- en: In our experience, all the production ML code uses a form of pipeline to combine
    multiple steps (for example, wrangle data, cluster, and regress). The upcoming
    chapters show you how to use these algorithms without a pipeline to reduce coding
    during the development process.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，所有生产ML代码都使用一种流水线形式来组合多个步骤（例如，整理数据、聚类和回归）。接下来的章节将向您展示如何在开发过程中使用这些算法而不使用流水线来减少编码。
- en: There's more...
  id: totrans-563
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Since we have just seen how to code pipeline concepts in Scala and Spark, let's
    revisit and define some of the concepts at a high level for a solid conceptual
    understanding.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们刚刚看到如何在Scala和Spark中编写流水线概念的代码，让我们重新审视并以高层次定义一些概念，以便对其有一个坚实的概念理解。
- en: PipeLine
  id: totrans-565
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道
- en: Spark makes it easy to combine steps in the machine learning pipelines (MLlib)
    by standardizing APIs that can be combined into a workflow (that is, referred
    to as pipeline in Spark). While a regression can be invoked without these pipelines,
    the reality of a working system (that is, end-to-end) requires us to take a multi-step
    pipeline approach.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: Spark通过标准化API使机器学习流水线（MLlib）中的步骤组合变得容易，这些API可以组合成工作流程（即在Spark中称为流水线）。虽然可以在不使用这些流水线的情况下调用回归，但一个工作系统（即端到端）的现实需要我们采取多步流水线方法。
- en: 'The pipeline concept comes from another popular library called **scikit-learn**:'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线的概念来自另一个流行的库**scikit-learn**：
- en: '**Transformer**: A Transformer is a method that can transform one DataFrame
    into another DataFrame.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换器**：转换器是一种可以将一个DataFrame转换为另一个DataFrame的方法。'
- en: '**Estimator**: An Estimator operates on a DataFrame to produce a Transformer.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**估计器**：估计器在DataFrame上操作，以产生一个转换器。'
- en: Vectors
  id: totrans-570
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量
- en: A base class of vectors supports both dense and sparse vectors. The fundamental
    difference is the efficiency of presentation for data structures with sparsity.
    The dense vector is the choice here, since the training data is all meaningful
    per row and very little sparsity is present. In the cases where we deal with sparse
    vectors, matrices, and so on, the sparse tuple will contain the index and corresponding
    values at the same time.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 向量的基类支持密集向量和稀疏向量。根本区别在于对于稀疏数据结构的表示效率。这里选择了密集向量，因为训练数据每行都是有意义的，几乎没有稀疏性。在处理稀疏向量、矩阵等情况时，稀疏元组将同时包含索引和相应的值。
- en: See also
  id: totrans-572
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'While using the Spark documentation and Scala references is optional and perhaps
    too early for this chapter, they are included for completeness:'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用Spark文档和Scala参考是可选的，也许对于本章来说还为时过早，但它们被包含在内是为了完整性：
- en: SEQ documentation in Scala is available at [http://www.scala-lang.org/api/current/index.html#scala.collection.Seq](http://www.scala-lang.org/api/current/index.html#scala.collection.Seq)
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala中的SEQ文档可在[http://www.scala-lang.org/api/current/index.html#scala.collection.Seq](http://www.scala-lang.org/api/current/index.html#scala.collection.Seq)找到
- en: Spark DataFrame documentation is available at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark DataFrame文档可在[http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)找到
- en: Spark vectors documentation is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.Vectors$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.Vectors%24)
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark向量文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.Vectors$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.Vectors%24)找到
- en: 'Spark pipeline documentation is available at the following URLs:'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark管道文档可在以下网址找到：
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline)'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline)'
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel)'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel)'
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage)'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage)'
- en: You should also familiarize yourself with the basic linear algebra package in
    Spark, you can do this by referring to [http://spark.apache.org/docs/latest/mllib-statistics.html](http://spark.apache.org/docs/latest/mllib-statistics.html)
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你也应该熟悉Spark中的基本线性代数包，你可以参考[http://spark.apache.org/docs/latest/mllib-statistics.html](http://spark.apache.org/docs/latest/mllib-statistics.html)
- en: Familiarity with basic data types, especially vectors, is highly recommended,
    for that, you can refer to [http://spark.apache.org/docs/latest/mllib-data-types.html](http://spark.apache.org/docs/latest/mllib-data-types.html)
    link
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉基本数据类型，特别是向量，是非常推荐的，你可以参考[http://spark.apache.org/docs/latest/mllib-data-types.html](http://spark.apache.org/docs/latest/mllib-data-types.html)链接
