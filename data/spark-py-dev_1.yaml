- en: Chapter 1. Setting Up a Spark Virtual Environment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。设置Spark虚拟环境
- en: 'In this chapter, we will build an isolated virtual environment for development
    purposes. The environment will be powered by Spark and the PyData libraries provided
    by the Python Anaconda distribution. These libraries include Pandas, Scikit-Learn,
    Blaze, Matplotlib, Seaborn, and Bokeh. We will perform the following activities:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将为开发目的构建一个隔离的虚拟环境。该环境将由Spark和Python Anaconda发行版提供的PyData库驱动。这些库包括Pandas、Scikit-Learn、Blaze、Matplotlib、Seaborn和Bokeh。我们将执行以下活动：
- en: Setting up the development environment using the Anaconda Python distribution.
    This will include enabling the IPython Notebook environment powered by PySpark
    for our data exploration tasks.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Anaconda Python发行版设置开发环境。这将包括启用由PySpark驱动的IPython Notebook环境，用于我们的数据探索任务。
- en: Installing and enabling Spark, and the PyData libraries such as Pandas, Scikit-
    Learn, Blaze, Matplotlib, and Bokeh.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和启用Spark，以及Pandas、Scikit-Learn、Blaze、Matplotlib和Bokeh等PyData库。
- en: Building a `word count` example app to ensure that everything is working fine.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个“单词计数”示例应用程序，以确保一切正常运行。
- en: The last decade has seen the rise and dominance of data-driven behemoths such
    as Amazon, Google, Twitter, LinkedIn, and Facebook. These corporations, by seeding,
    sharing, or disclosing their infrastructure concepts, software practices, and
    data processing frameworks, have fostered a vibrant open source software community.
    This has transformed the enterprise technology, systems, and software architecture.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年见证了像亚马逊、谷歌、Twitter、LinkedIn和Facebook这样的数据驱动巨头的崛起和主导地位。这些公司通过播种、分享或披露他们的基础设施概念、软件实践和数据处理框架，培育了一个充满活力的开源软件社区。这已经改变了企业技术、系统和软件架构。
- en: This includes new infrastructure and DevOps (short for development and operations),
    concepts leveraging virtualization, cloud technology, and software-defined networks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括利用虚拟化、云技术和软件定义网络的新基础设施和DevOps（开发和运维）概念。
- en: To process petabytes of data, Hadoop was developed and open sourced, taking
    its inspiration from the **Google File System** (**GFS**) and the adjoining distributed
    computing framework, MapReduce. Overcoming the complexities of scaling while keeping
    costs under control has also led to a proliferation of new data stores. Examples
    of recent database technology include Cassandra, a columnar database; MongoDB,
    a document database; and Neo4J, a graph database.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理千兆字节的数据，Hadoop被开发并开源，它从**Google文件系统**（**GFS**）和相邻的分布式计算框架MapReduce中汲取了灵感。克服了扩展的复杂性，同时控制成本也导致了新数据存储的大量出现。最近的数据库技术示例包括列数据库Cassandra、文档数据库MongoDB和图数据库Neo4J。
- en: Hadoop, thanks to its ability to process huge datasets, has fostered a vast
    ecosystem to query data more iteratively and interactively with Pig, Hive, Impala,
    and Tez. Hadoop is cumbersome as it operates only in batch mode using MapReduce.
    Spark is creating a revolution in the analytics and data processing realm by targeting
    the shortcomings of disk input-output and bandwidth-intensive MapReduce jobs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其处理大型数据集的能力，Hadoop已经培育出一个庞大的生态系统，可以使用Pig、Hive、Impala和Tez更迭地和交互地查询数据。Hadoop在使用MapReduce时只能以批处理模式运行，因此它很繁琐。Spark通过针对磁盘输入输出和带宽密集型MapReduce作业的缺点，正在为分析和数据处理领域带来革命。
- en: Spark is written in Scala, and therefore integrates natively with the **Java
    Virtual Machine** (**JVM**) powered ecosystem. Spark had early on provided Python
    API and bindings by enabling PySpark. The Spark architecture and ecosystem is
    inherently polyglot, with an obvious strong presence of Java-led systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是用Scala编写的，因此与由**Java虚拟机**（**JVM**）驱动的生态系统本地集成。Spark早期提供了Python API和绑定，通过启用PySpark。Spark架构和生态系统本质上是多语言的，显然有着Java主导系统的强大存在。
- en: This book will focus on PySpark and the PyData ecosystem. Python is one of the
    preferred languages in the academic and scientific community for data-intensive
    processing. Python has developed a rich ecosystem of libraries and tools in data
    manipulation with Pandas and Blaze, in Machine Learning with Scikit-Learn, and
    in data visualization with Matplotlib, Seaborn, and Bokeh. Hence, the aim of this
    book is to build an end-to-end architecture for data-intensive applications powered
    by Spark and Python. In order to put these concepts in to practice, we will analyze
    social networks such as Twitter, GitHub, and Meetup. We will focus on the activities
    and social interactions of Spark and the Open Source Software community by tapping
    into GitHub, Twitter, and Meetup.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将专注于PySpark和PyData生态系统。Python是学术和科学界进行数据密集处理的首选语言之一。Python在数据处理方面发展了丰富的库和工具生态系统，包括Pandas和Blaze的数据操作、Scikit-Learn的机器学习以及Matplotlib、Seaborn和Bokeh的数据可视化。因此，本书的目标是构建一个由Spark和Python驱动的数据密集型应用程序的端到端架构。为了将这些概念付诸实践，我们将分析Twitter、GitHub和Meetup等社交网络。我们将关注Spark和开源软件社区的活动和社交互动，通过GitHub、Twitter和Meetup进行调查。
- en: Building data-intensive applications requires highly scalable infrastructure,
    polyglot storage, seamless data integration, multiparadigm analytics processing,
    and efficient visualization. The following paragraph describes the data-intensive
    app architecture blueprint that we will adopt throughout the book. It is the backbone
    of the book. We will discover Spark in the context of the broader PyData ecosystem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 构建数据密集型应用程序需要高度可扩展的基础架构、多语言存储、无缝数据集成、多范式分析处理和高效的可视化。下一段描述了我们将在整本书中采用的数据密集型应用程序架构蓝图。这是本书的骨干。我们将在更广泛的PyData生态系统的背景下发现Spark。
- en: Tip
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Downloading the example code**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载示例代码**'
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从您在[http://www.packtpub.com](http://www.packtpub.com)购买的所有Packt图书的帐户中下载示例代码文件。如果您在其他地方购买了这本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便文件直接通过电子邮件发送给您。
- en: Understanding the architecture of data-intensive applications
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据密集型应用程序的架构
- en: 'In order to understand the architecture of data-intensive applications, the
    following conceptual framework is used. The is architecture is designed on the
    following five layers:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解数据密集型应用程序的架构，使用以下概念框架。架构设计在以下五个层次上：
- en: Infrastructure layer
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施层
- en: Persistence layer
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久层
- en: Integration layer
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成层
- en: Analytics layer
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析层
- en: Engagement layer
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参与层
- en: 'The following screenshot depicts the five layers of the **Data Intensive App
    Framework**:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图描述了**数据密集型应用程序框架**的五个层次：
- en: '![Understanding the architecture of data-intensive applications](img/B03968_01_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![理解数据密集型应用程序的架构](img/B03968_01_01.jpg)'
- en: From the bottom up, let's go through the layers and their main purpose.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从下到上，让我们逐层介绍它们的主要目的。
- en: Infrastructure layer
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础设施层
- en: The infrastructure layer is primarily concerned with virtualization, scalability,
    and continuous integration. In practical terms, and in terms of virtualization,
    we will go through building our own development environment in a VirtualBox and
    virtual machine powered by Spark and the Anaconda distribution of Python. If we
    wish to scale from there, we can create a similar environment in the cloud. The
    practice of creating a segregated development environment and moving into test
    and production deployment can be automated and can be part of a continuous integration
    cycle powered by DevOps tools such as **Vagrant**, **Chef**, **Puppet**, and **Docker**.
    Docker is a very popular open source project that eases the installation and deployment
    of new environments. The book will be limited to building the virtual machine
    using VirtualBox. From a data-intensive app architecture point of view, we are
    describing the essential steps of the infrastructure layer by mentioning scalability
    and continuous integration beyond just virtualization.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施层主要涉及虚拟化，可扩展性和持续集成。在实际操作和虚拟化方面，我们将通过在VirtualBox中构建我们自己的开发环境，并使用Spark和Python的Anaconda发行版来驱动虚拟机。如果我们希望从那里扩展，我们可以在云中创建类似的环境。创建一个分隔的开发环境并移动到测试和生产部署的实践可以自动化，并且可以成为由DevOps工具（如**Vagrant**，**Chef**，**Puppet**和**Docker**）驱动的持续集成周期的一部分。
    Docker是一个非常受欢迎的开源项目，它简化了新环境的安装和部署。本书将仅限于使用VirtualBox构建虚拟机。从数据密集型应用程序架构的角度来看，我们通过提及可扩展性和持续集成来描述基础设施层的基本步骤。
- en: Persistence layer
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久层
- en: The persistence layer manages the various repositories in accordance with data
    needs and shapes. It ensures the set up and management of the polyglot data stores.
    It includes relational database management systems such as **MySQL** and **PostgreSQL**;
    key-value data stores such as **Hadoop**, **Riak**, and **Redis**; columnar databases
    such as **HBase** and **Cassandra**; document databases such as **MongoDB** and
    **Couchbase**; and graph databases such as **Neo4j**. The persistence layer manages
    various filesystems such as Hadoop's HDFS. It interacts with various storage systems
    from native hard drives to Amazon S3\. It manages various file storage formats
    such as `csv`, `json`, and `parquet`, which is a column-oriented format.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 持久层根据数据需求和形状管理各种存储库。它确保设置和管理多语言数据存储。它包括关系数据库管理系统，如**MySQL**和**PostgreSQL**；键值数据存储，如**Hadoop**，**Riak**和**Redis**；列数据库，如**HBase**和**Cassandra**；文档数据库，如**MongoDB**和**Couchbase**；以及图数据库，如**Neo4j**。持久层管理Hadoop的HDFS等各种文件系统。它与从本地硬盘到Amazon
    S3的各种存储系统进行交互。它管理各种文件存储格式，如`csv`，`json`和`parquet`，这是一种面向列的格式。
- en: Integration layer
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成层
- en: 'The integration layer focuses on data acquisition, transformation, quality,
    persistence, consumption, and governance. It is essentially driven by the following
    five Cs: *connect*, *collect*, *correct*, *compose*, and *consume*.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 集成层专注于数据获取、转换、质量、持久性、消费和治理。它基本上由以下五个C驱动：*连接*，*收集*，*校正*，*组合*和*消费*。
- en: 'The five steps describe the lifecycle of data. They are focused on how to acquire
    the dataset of interest, explore it, iteratively refine and enrich the collected
    information, and get it ready for consumption. So, the steps perform the following
    operations:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这五个步骤描述了数据的生命周期。它们关注如何获取感兴趣的数据集，探索它，迭代地完善和丰富收集的信息，并准备好供使用。因此，这些步骤执行以下操作：
- en: '**Connect**: Targets the best way to acquire data from the various data sources,
    APIs offered by these sources, the input format, input schemas if they exist,
    the rate of data collection, and limitations from providers'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接**：针对从各种数据源获取数据的最佳方式，这些数据源提供的API，输入格式，如果存在的话，输入模式，数据收集速率和提供者的限制'
- en: '**Correct**: Focuses on transforming data for further processing and also ensures
    that the quality and consistency of the data received are maintained'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**校正**：专注于转换数据以进行进一步处理，并确保所接收的数据的质量和一致性得到维护'
- en: '**Collect**: Looks at which data to store where and in what format, to ease
    data composition and consumption at later stages'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收集**：查看存储哪些数据以及以何种格式，以便在后期阶段轻松进行数据组合和使用'
- en: '**Compose**: Concentrates its attention on how to mash up the various data
    sets collected, and enrich the information in order to build a compelling data-driven
    product'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组合**：集中关注如何混合收集的各种数据集，并丰富信息以构建引人注目的数据驱动产品'
- en: '**Consume**: Takes care of data provisioning and rendering and how the right
    data reaches the right individual at the right time'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费：负责数据供应和呈现，以及确保正确的数据在正确的时间到达正确的个人
- en: '**Control**: This sixth *additional* step will sooner or later be required
    as the data, the organization, and the participants grow and it is about ensuring
    data governance'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制：随着数据、组织和参与者的增长，迟早会需要这第六个额外步骤，它关乎确保数据治理
- en: 'The following diagram depicts the iterative process of data acquisition and
    refinement for consumption:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了数据获取和精炼的迭代过程，以供使用：
- en: '![Integration layer](img/B03968_01_02.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![集成层](img/B03968_01_02.jpg)'
- en: Analytics layer
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析层
- en: The analytics layer is where Spark processes data with the various models, algorithms,
    and machine learning pipelines in order to derive insights. For our purpose, in
    this book, the analytics layer is powered by Spark. We will delve deeper in subsequent
    chapters into the merits of Spark. In a nutshell, what makes it so powerful is
    that it allows multiple paradigms of analytics processing in a single unified
    platform. It allows batch, streaming, and interactive analytics. Batch processing
    on large datasets with longer latency periods allows us to extract patterns and
    insights that can feed into real-time events in streaming mode. Interactive and
    iterative analytics are more suited for data exploration. Spark offers bindings
    and APIs in Python and R. With its **SparkSQL** module and the Spark Dataframe,
    it offers a very familiar analytics interface.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 分析层是Spark处理数据的地方，使用各种模型、算法和机器学习管道来获取洞察力。在本书中，分析层由Spark提供支持。我们将在随后的章节中更深入地探讨Spark的优点。简而言之，它之所以如此强大，是因为它允许在单一统一平台上进行多种分析处理范式。它允许批处理、流处理和交互式分析。在具有较长延迟周期的大型数据集上进行批处理允许我们提取模式和洞察力，这些可以用于流处理模式中的实时事件。交互式和迭代式分析更适合数据探索。Spark提供了Python和R的绑定和API。通过其SparkSQL模块和Spark
    Dataframe，它提供了一个非常熟悉的分析接口。
- en: Engagement layer
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参与层
- en: The engagement layer interacts with the end user and provides dashboards, interactive
    visualizations, and alerts. We will focus here on the tools provided by the PyData
    ecosystem such as Matplotlib, Seaborn, and Bokeh.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 参与层与最终用户进行交互，并提供仪表板、交互式可视化和警报。我们将重点关注PyData生态系统提供的工具，如Matplotlib、Seaborn和Bokeh。
- en: Understanding Spark
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Spark
- en: Hadoop scales horizontally as the data grows. Hadoop runs on commodity hardware,
    so it is cost-effective. Intensive data applications are enabled by scalable,
    distributed processing frameworks that allow organizations to analyze petabytes
    of data on large commodity clusters. Hadoop is the first open source implementation
    of map-reduce. Hadoop relies on a distributed framework for storage called **HDFS**
    (**Hadoop Distributed File System**). Hadoop runs map-reduce tasks in batch jobs.
    Hadoop requires persisting the data to disk at each map, shuffle, and reduce process
    step. The overhead and the latency of such batch jobs adversely impact the performance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop随着数据增长而水平扩展。Hadoop在廉价硬件上运行，因此具有成本效益。可扩展的分布式处理框架使得机构能够在大型廉价集群上分析PB级数据。Hadoop是map-reduce的第一个开源实现。Hadoop依赖于称为HDFS（Hadoop分布式文件系统）的分布式存储框架。Hadoop在批处理作业中运行map-reduce任务。Hadoop需要在每个map、shuffle和reduce过程步骤中将数据持久化到磁盘上。这种批处理作业的开销和延迟对性能产生不利影响。
- en: Spark is a fast, distributed general analytics computing engine for large-scale
    data processing. The major breakthrough from Hadoop is that Spark allows data
    sharing between processing steps through in-memory processing of data pipelines.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个快速的、分布式的大规模数据处理通用分析计算引擎。与Hadoop的主要突破之处在于，Spark允许数据在处理步骤之间通过内存处理进行共享。
- en: 'Spark is unique in that it allows four different styles of data analysis and
    processing. Spark can be used in:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Spark独特之处在于它允许四种不同的数据分析和处理样式。Spark可用于：
- en: '**Batch**: This mode is used for manipulating large datasets, typically performing
    large map-reduce jobs'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理：此模式用于操作大型数据集，通常执行大型map-reduce作业
- en: '**Streaming**: This mode is used to process incoming information in near real
    time'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流处理：此模式用于近实时处理传入信息
- en: '**Iterative**: This mode is for machine learning algorithms such as a gradient
    descent where the data is accessed repetitively in order to reach convergence'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代式：这种模式适用于机器学习算法，例如梯度下降，其中数据被重复访问以达到收敛
- en: '**Interactive**: This mode is used for data exploration as large chunks of
    data are in memory and due to the very quick response time of Spark'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交互式：此模式用于数据探索，因为大量数据在内存中，并且由于Spark的非常快的响应时间
- en: 'The following figure highlights the preceding four processing styles:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表突出了前面四种处理样式：
- en: '![Understanding Spark](img/B03968_01_03.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![理解Spark](img/B03968_01_03.jpg)'
- en: 'Spark operates in three modes: one single mode, standalone on a single machine
    and two distributed modes on a cluster of machines—on Yarn, the Hadoop distributed
    resource manager, or on Mesos, the open source cluster manager developed at Berkeley
    concurrently with Spark:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有三种模式：单一模式，在单台机器上独立运行；两种分布式模式，在机器集群上运行——在Hadoop分布式资源管理器Yarn上，或者在与Spark同时开发的开源集群管理器Mesos上：
- en: '![Understanding Spark](img/B03968_01_04.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![理解Spark](img/B03968_01_04.jpg)'
- en: Spark offers a polyglot interface in Scala, Java, Python, and R.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了Scala、Java、Python和R的多语言接口。
- en: Spark libraries
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark库
- en: 'Spark comes with batteries included, with some powerful libraries:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Spark自带一些强大的库：
- en: '**SparkSQL**: This provides the SQL-like ability to interrogate structured
    data and interactively explore large datasets'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkSQL：提供类似SQL的能力来查询结构化数据并交互式地探索大型数据集
- en: '**SparkMLLIB**: This provides major algorithms and a pipeline framework for
    machine learning'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkMLLIB：为机器学习提供主要算法和管道框架
- en: '**Spark Streaming**: This is for near real-time analysis of data using micro
    batches and sliding widows on incoming streams of data'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming**：用于对数据进行近实时分析，使用微批处理和滑动窗口处理传入的数据流'
- en: '**Spark GraphX**: This is for graph processing and computation on complex connected
    entities and relationships'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark GraphX**：用于图处理和复杂连接实体和关系的计算'
- en: PySpark in action
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PySpark的实际应用
- en: Spark is written in Scala. The whole Spark ecosystem naturally leverages the
    JVM environment and capitalizes on HDFS natively. Hadoop HDFS is one of the many
    data stores supported by Spark. Spark is agnostic and from the beginning interacted
    with multiple data sources, types, and formats.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是用Scala编写的。整个Spark生态系统自然地利用了JVM环境，并充分利用了HDFS。 Hadoop HDFS是Spark支持的许多数据存储之一。Spark是不可知的，并且从一开始就与多个数据源、类型和格式进行交互。
- en: PySpark is not a transcribed version of Spark on a Java-enabled dialect of Python
    such as Jython. PySpark provides integrated API bindings around Spark and enables
    full usage of the Python ecosystem within all the nodes of the cluster with the
    pickle Python serialization and, more importantly, supplies access to the rich
    ecosystem of Python's machine learning libraries such as Scikit-Learn or data
    processing such as Pandas.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark不是Spark在支持Java的Python方言（如Jython）上的抄写版本。PySpark提供了围绕Spark的集成API绑定，并允许在集群的所有节点中完全使用Python生态系统，使用pickle
    Python序列化，并更重要的是，提供对Python的丰富生态系统的访问，如Scikit-Learn等机器学习库或数据处理库，如Pandas。
- en: When we initialize a Spark program, the first thing a Spark program must do
    is to create a `SparkContext` object. It tells Spark how to access the cluster.
    The Python program creates a `PySparkContext`. Py4J is the gateway that binds
    the Python program to the Spark JVM `SparkContext`. The JVM `SparkContextserializes`
    the application codes and the closures and sends them to the cluster for execution.
    The cluster manager allocates resources and schedules, and ships the closures
    to the Spark workers in the cluster who activate Python virtual machines as required.
    In each machine, the Spark Worker is managed by an executor that controls computation,
    storage, and cache.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们初始化一个Spark程序时，Spark程序必须做的第一件事是创建一个`SparkContext`对象。它告诉Spark如何访问集群。Python程序创建一个`PySparkContext`。Py4J是将Python程序绑定到Spark
    JVM `SparkContext`的网关。JVM `SparkContext`序列化应用程序代码和闭包，并将它们发送到集群进行执行。集群管理器分配资源并安排，并将闭包发送到集群中的Spark工作程序，根据需要激活Python虚拟机。在每台机器上，Spark
    Worker由控制计算、存储和缓存的执行者管理。
- en: 'Here''s an example of how the Spark driver manages both the PySpark context
    and the Spark context with its local filesystems and its interactions with the
    Spark worker through the cluster manager:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Spark驱动程序如何管理PySpark上下文和Spark上下文以及其与集群管理器通过本地文件系统和与Spark工作程序的交互的示例：
- en: '![PySpark in action](img/B03968_01_05.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark in action](img/B03968_01_05.jpg)'
- en: The Resilient Distributed Dataset
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 弹性分布式数据集
- en: Spark applications consist of a driver program that runs the user's main function,
    creates distributed datasets on the cluster, and executes various parallel operations
    (transformations and actions) on those datasets.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Spark应用程序由驱动程序组成，驱动程序运行用户的主要函数，在集群上创建分布式数据集，并对这些数据集执行各种并行操作（转换和操作）。
- en: Spark applications are run as an independent set of processes, coordinated by
    a `SparkContext` in a driver program.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Spark应用程序作为一组独立的进程运行，由驱动程序中的`SparkContext`协调。
- en: The `SparkContext` will be allocated system resources (machines, memory, CPU)
    from the **Cluster manager**.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkContext`将从**集群管理器**分配系统资源（机器、内存、CPU）。'
- en: The `SparkContext` manages executors who manage workers in the cluster. The
    driver program has Spark jobs that need to run. The jobs are split into tasks
    submitted to the executor for completion. The executor takes care of computation,
    storage, and caching in each machine.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkContext`管理执行者，执行者管理集群中的工作节点。驱动程序具有需要运行的Spark作业。作业被拆分为任务，提交给执行者完成。执行者负责在每台机器上进行计算、存储和缓存。'
- en: The key building block in Spark is the **RDD** (**Resilient Distributed Dataset**).
    A dataset is a collection of elements. Distributed means the dataset can be on
    any node in the cluster. Resilient means that the dataset could get lost or partially
    lost without major harm to the computation in progress as Spark will re-compute
    from the data lineage in memory, also known as the **DAG** (short for **Directed
    Acyclic Graph**) of operations. Basically, Spark will snapshot in memory a state
    of the RDD in the cache. If one of the computing machines crashes during operation,
    Spark rebuilds the RDDs from the cached RDD and the DAG of operations. RDDs recover
    from node failure.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的关键构建块是**RDD**（**弹性分布式数据集**）。数据集是元素的集合。分布式意味着数据集可以在集群中的任何节点上。弹性意味着数据集可能会丢失或部分丢失，而不会对正在进行的计算造成重大伤害，因为Spark将从内存中的数据血统重新计算，也称为操作的**DAG**（**有向无环图**）。基本上，Spark将在缓存中快照RDD的状态。如果在操作过程中其中一台计算机崩溃，Spark将从缓存的RDD和操作的DAG重新构建RDD。RDD可以从节点故障中恢复。
- en: 'There are two types of operation on RDDs:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: RDD上有两种操作类型：
- en: '**Transformations**: A transformation takes an existing RDD and leads to a
    pointer of a new transformed RDD. An RDD is immutable. Once created, it cannot
    be changed. Each transformation creates a new RDD. Transformations are lazily
    evaluated. Transformations are executed only when an action occurs. In the case
    of failure, the data lineage of transformations rebuilds the RDD.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换**：转换获取现有的RDD，并导致新转换的RDD的指针。RDD是不可变的。一旦创建，就无法更改。每个转换都会创建一个新的RDD。转换是惰性评估的。转换只有在发生操作时才会执行。在失败的情况下，转换的数据血统会重建RDD。'
- en: '**Actions**: An action on an RDD triggers a Spark job and yields a value. An
    action operation causes Spark to execute the (lazy) transformation operations
    that are required to compute the RDD returned by the action. The action results
    in a DAG of operations. The DAG is compiled into stages where each stage is executed
    as a series of tasks. A task is a fundamental unit of work.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作**：对RDD的操作会触发一个Spark作业并产生一个值。操作操作会导致Spark执行（懒惰的）转换操作，这些操作是计算由操作返回的RDD所需的。操作会导致一系列操作的DAG。DAG被编译成阶段，每个阶段都作为一系列任务执行。任务是工作的基本单位。'
- en: 'Here''s some useful information on RDDs:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于RDD的一些有用信息：
- en: 'RDDs are created from a data source such as an HDFS file or a DB query. There
    are three ways to create an RDD:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD是从数据源（如HDFS文件或数据库查询）创建的。有三种方法可以创建RDD：
- en: Reading from a datastore
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据存储中读取
- en: Transforming an existing RDD
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换现有的RDD
- en: Using an in-memory collection
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内存集合
- en: RDDs are transformed with functions such as `map` or `filter`, which yield new
    RDDs.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD可以通过`map`或`filter`等函数进行转换，产生新的RDD。
- en: An action such as first, take, collect, or count on an RDD will deliver the
    results into the Spark driver. The Spark driver is the client through which the
    user interacts with the Spark cluster.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对RDD进行的操作，比如first、take、collect或count，会将结果传递到Spark驱动程序。Spark驱动程序是用户与Spark集群交互的客户端。
- en: 'The following diagram illustrates the RDD transformation and action:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示了RDD的转换和操作：
- en: '![The Resilient Distributed Dataset](img/B03968_01_06.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![弹性分布式数据集](img/B03968_01_06.jpg)'
- en: Understanding Anaconda
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解Anaconda
- en: 'Anaconda is a widely used free Python distribution maintained by **Continuum**
    ([https://www.continuum.io/](https://www.continuum.io/)). We will use the prevailing
    software stack provided by Anaconda to generate our apps. In this book, we will
    use PySpark and the PyData ecosystem. The PyData ecosystem is promoted, supported,
    and maintained by **Continuum** and powered by the **Anaconda** Python distribution.
    The Anaconda Python distribution essentially saves time and aggravation in the
    installation of the Python environment; we will use it in conjunction with Spark.
    Anaconda has its own package management that supplements the traditional `pip`
    `install` and `easy-install`. Anaconda comes with batteries included, namely some
    of the most important packages such as Pandas, Scikit-Learn, Blaze, Matplotlib,
    and Bokeh. An upgrade to any of the installed library is a simple command at the
    console:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Anaconda是一个广泛使用的免费Python发行版，由**Continuum**维护（[https://www.continuum.io/](https://www.continuum.io/)）。我们将使用Anaconda提供的主流软件堆栈来生成我们的应用程序。在本书中，我们将使用PySpark和PyData生态系统。PyData生态系统由**Continuum**推广、支持和维护，并由**Anaconda**
    Python发行版提供支持。Anaconda Python发行版在安装Python环境方面节省了时间和烦恼；我们将与Spark一起使用它。Anaconda有自己的软件包管理，补充了传统的`pip`
    `install`和`easy-install`。Anaconda自带了一些最重要的软件包，比如Pandas、Scikit-Learn、Blaze、Matplotlib和Bokeh。对已安装库的升级只需在控制台上输入一个简单的命令：
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A list of installed libraries in our environment can be obtained with command:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令获取我们环境中安装的库的列表：
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The key components of the stack are as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 堆栈的关键组件如下：
- en: '**Anaconda**: This is a free Python distribution with almost 200 Python packages
    for science, math, engineering, and data analysis.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Anaconda**：这是一个免费的Python发行版，几乎包含了200个用于科学、数学、工程和数据分析的Python软件包。'
- en: '**Conda**: This is a package manager that takes care of all the dependencies
    of installing a complex software stack. This is not restricted to Python and manages
    the install process for R and other languages.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Conda**：这是一个软件包管理器，负责安装复杂软件堆栈的所有依赖项。它不仅限于Python，还管理R和其他语言的安装过程。'
- en: '**Numba**: This provides the power to speed up code in Python with high-performance
    functions and just-in-time compilation.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Numba**：它提供了在Python中加速代码的能力，具有高性能函数和即时编译。'
- en: '**Blaze**: This enables large scale data analytics by offering a uniform and
    adaptable interface to access a variety of data providers, which include streaming
    Python, Pandas, SQLAlchemy, and Spark.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Blaze**：它通过提供统一和可适应的接口来访问各种数据提供程序（包括流式Python、Pandas、SQLAlchemy和Spark），实现了大规模数据分析。'
- en: '**Bokeh**: This provides interactive data visualizations for large and streaming
    datasets.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bokeh**：它为大型和流式数据集提供交互式数据可视化。'
- en: '**Wakari**: This allows us to share and deploy IPython Notebooks and other
    apps on a hosted environment.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Wakari**：这允许我们在托管环境中共享和部署IPython Notebooks和其他应用程序。'
- en: 'The following figure shows the components of the Anaconda stack:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了Anaconda堆栈的组件：
- en: '![Understanding Anaconda](img/B03968_01_07.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![了解Anaconda](img/B03968_01_07.jpg)'
- en: Setting up the Spark powered environment
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立由Spark驱动的环境
- en: 'In this section, we will learn to set up Spark:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何设置Spark：
- en: Create a segregated development environment in a virtual machine running on
    Ubuntu 14.04, so it does not interfere with any existing system.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行Ubuntu 14.04的虚拟机中创建一个独立的开发环境，以便不会干扰任何现有系统。
- en: Install Spark 1.3.0 with its dependencies, namely.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Spark 1.3.0及其依赖项，即。
- en: Install the Anaconda Python 2.7 environment with all the required libraries
    such as Pandas, Scikit-Learn, Blaze, and Bokeh, and enable PySpark, so it can
    be accessed through IPython Notebooks.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Anaconda Python 2.7环境以及所有必需的库，比如Pandas、Scikit-Learn、Blaze和Bokeh，并启用PySpark，以便可以通过IPython
    Notebooks访问。
- en: Set up the backend or data stores of our environment. We will use MySQL as the
    relational database, MongoDB as the document store, and Cassandra as the columnar
    database.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置我们环境的后端或数据存储。我们将使用MySQL作为关系数据库，MongoDB作为文档存储，Cassandra作为列式数据库。
- en: Each storage backend serves a specific purpose depending on the nature of the
    data to be handled. The MySQL RDBMs is used for standard tabular processed information
    that can be easily queried using SQL. As we will be processing a lot of JSON-type
    data from various APIs, the easiest way to store them is in a document. For real-time
    and time-series-related information, Cassandra is best suited as a columnar database.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 每个存储后端根据要处理的数据的性质提供特定的用途。MySQL RDBMs用于可以使用SQL轻松查询的标准表格处理信息。由于我们将从各种API处理大量JSON类型数据，因此将它们存储在文档中是最简单的方式。对于实时和时间序列相关信息，Cassandra最适合作为列式数据库。
- en: 'The following diagram gives a view of the environment we will build and use
    throughout the book:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了我们将在整本书中构建和使用的环境：
- en: '![Setting up the Spark powered environment](img/B03968_01_08.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![设置Spark动力环境](img/B03968_01_08.jpg)'
- en: Setting up an Oracle VirtualBox with Ubuntu
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Ubuntu上设置Oracle VirtualBox
- en: Setting up a clean new VirtualBox environment on Ubuntu 14.04 is the safest
    way to create a development environment that does not conflict with existing libraries
    and can be later replicated in the cloud using a similar list of commands.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在Ubuntu 14.04上设置一个干净的新VirtualBox环境是创建一个开发环境的最安全方式，它不会与现有的库发生冲突，并且可以在云中使用类似的命令列表进行复制。
- en: In order to set up an environment with Anaconda and Spark, we will create a
    VirtualBox virtual machine running Ubuntu 14.04.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立一个带有Anaconda和Spark的环境，我们将创建一个运行Ubuntu 14.04的VirtualBox虚拟机。
- en: 'Let''s go through the steps of using VirtualBox with Ubuntu:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看在Ubuntu上使用VirtualBox的步骤：
- en: Oracle VirtualBox VM is free and can be downloaded from [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads).
    The installation is pretty straightforward.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Oracle VirtualBox VM是免费的，可以从[https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads)下载。安装非常简单。
- en: After installing VirtualBox, let's open the Oracle VM VirtualBox Manager and
    click the **New** button.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装VirtualBox后，让我们打开Oracle VM VirtualBox Manager并单击**新建**按钮。
- en: We'll give the new VM a name, and select Type **Linux** and Version **Ubuntu
    (64 bit)**.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将为新的虚拟机命名，并选择类型**Linux**和版本**Ubuntu（64位）**。
- en: 'You need to download the ISO from the Ubuntu website and allocate sufficient
    RAM (4 GB recommended) and disk space (20 GB recommended). We will use the Ubuntu
    14.04.1 LTS release, which is found here: [http://www.ubuntu.com/download/desktop](http://www.ubuntu.com/download/desktop).'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要从Ubuntu网站下载ISO并分配足够的RAM（建议4GB）和磁盘空间（建议20GB）。我们将使用Ubuntu 14.04.1 LTS版本，可以在这里找到：[http://www.ubuntu.com/download/desktop](http://www.ubuntu.com/download/desktop)。
- en: Once the installation completed, it is advisable to install the VirtualBox Guest
    Additions by going to (from the VirtualBox menu, with the new VM running) **Devices**
    | **Insert Guest Additions CD image**. Failing to provide the guest additions
    in a Windows host gives a very limited user interface with reduced window sizes.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，建议通过转到（从新的虚拟机运行的VirtualBox菜单）**设备** | **插入增强功能光盘映像**来安装VirtualBox增强功能。在Windows主机中未提供增强功能会导致用户界面非常有限，窗口大小减小。
- en: Once the additional installation completes, reboot the VM, and it will be ready
    to use. It is helpful to enable the shared clipboard by selecting the VM and clicking
    **Settings**, then go to **General** | **Advanced** | **Shared Clipboard** and
    click on **Bidirectional**.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，重新启动虚拟机，它将准备好使用。通过选择虚拟机并单击**设置**，然后转到**常规** | **高级** | **共享剪贴板**并单击**双向**，可以启用共享剪贴板。
- en: Installing Anaconda with Python 2.7
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装带有Python 2.7的Anaconda
- en: 'PySpark currently runs only on Python 2.7\. (There are requests from the community
    to upgrade to Python 3.3.) To install Anaconda, follow these steps:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark目前仅在Python 2.7上运行。（社区要求升级到Python 3.3。）要安装Anaconda，请按照以下步骤进行：
- en: Download the Anaconda Installer for Linux 64-bit Python 2.7 from [http://continuum.io/downloads#all](http://continuum.io/downloads#all).
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[http://continuum.io/downloads#all](http://continuum.io/downloads#all)下载Linux
    64位Python 2.7的Anaconda安装程序。
- en: 'After downloading the Anaconda installer, open a terminal and navigate to the
    directory or folder where the installer has been saved. From here, run the following
    command, replacing the `2.x.x` in the command with the version number of the downloaded
    installer file:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载Anaconda安装程序后，打开终端并导航到安装程序保存的目录或文件夹。然后运行以下命令，将命令中的`2.x.x`替换为下载安装程序文件的版本号：
- en: '[PRE2]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After accepting the license terms, you will be asked to specify the install
    location (which `defaults to ~/anaconda`).
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受许可条款后，您将被要求指定安装位置（默认为~/anaconda）。
- en: 'After the self-extraction is finished, you should add the anaconda binary directory
    to your PATH environment variable:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自解压完成后，您应该将anaconda二进制目录添加到您的PATH环境变量中：
- en: '[PRE3]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Installing Java 8
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Java 8
- en: Spark runs on the JVM and requires the Java **SDK** (short for **Software Development
    Kit**) and not the **JRE** (short for **Java Runtime Environment**), as we will
    build apps with Spark. The recommended version is Java Version 7 or higher. Java
    8 is the most suitable, as it includes many of the functional programming techniques
    available with Scala and Python.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在JVM上运行，并且需要Java **SDK**（软件开发工具包）而不是**JRE**（Java运行环境），因为我们将使用Spark构建应用程序。推荐的版本是Java版本7或更高版本。Java
    8是最合适的，因为它包括许多Scala和Python可用的函数式编程技术。
- en: 'To install Java 8, follow these steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Java 8，请按照以下步骤进行：
- en: 'Install Oracle Java 8 using the following commands:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装Oracle Java 8：
- en: '[PRE4]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Set the `JAVA_HOME` environment variable and ensure that the Java program is
    on your PATH.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`JAVA_HOME`环境变量，并确保Java程序在您的PATH上。
- en: 'Check that `JAVA_HOME` is properly installed:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查`JAVA_HOME`是否正确安装：
- en: '[PRE5]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Installing Spark
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Spark
- en: Head over to the Spark download page at [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 转到Spark下载页面[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)。
- en: The Spark download page offers the possibility to download earlier versions
    of Spark and different package and download types. We will select the latest release,
    pre-built for Hadoop 2.6 and later. The easiest way to install Spark is to use
    a Spark package prebuilt for Hadoop 2.6 and later, rather than build it from source.
    Move the file to the directory `~/spark` under the root directory.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Spark下载页面提供了下载早期版本的Spark和不同的软件包和下载类型的可能性。我们将选择最新版本，为Hadoop 2.6及更高版本预构建。安装Spark的最简单方法是使用为Hadoop
    2.6及更高版本预构建的Spark软件包，而不是从源代码构建。将文件移动到根目录下的`~/spark`目录中。
- en: 'Download the latest release of Spark—Spark 1.5.2, released on November 9, 2015:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 下载最新版本的Spark—2015年11月9日发布的Spark 1.5.2：
- en: Select Spark release **1.5.2 (Nov 09 2015),**
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择Spark版本**1.5.2（2015年11月9日发布）**，
- en: Chose the package type **Prebuilt for Hadoop 2.6 and later**,
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择软件包类型**为Hadoop 2.6及更高版本预构建**，
- en: Chose the download type **Direct Download**,
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择下载类型**直接下载**，
- en: 'Download Spark: **spark-1.5.2-bin-hadoop2.6.tgz**,'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载Spark：**spark-1.5.2-bin-hadoop2.6.tgz**，
- en: Verify this release using the 1.3.0 signatures and checksums,
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用1.3.0签名和校验和验证此版本，
- en: 'This can also be accomplished by running:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以通过运行以下命令来完成：
- en: '[PRE6]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we''ll extract the files and clean up:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将提取文件并清理：
- en: '[PRE7]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we can run the Spark Python interpreter with:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以运行Spark Python解释器：
- en: '[PRE8]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should see something like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似于这样的东西：
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The interpreter will have already provided us with a Spark context object,
    `sc`, which we can see by running:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 解释器将已经为我们提供了一个Spark上下文对象`sc`，我们可以通过运行来查看：
- en: '[PRE10]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Enabling IPython Notebook
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用IPython笔记本
- en: We will work with IPython Notebook for a friendlier user experience than the
    console.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用IPython Notebook以获得比控制台更友好的用户体验。
- en: 'You can launch IPython Notebook by using the following command:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令启动IPython Notebook：
- en: '[PRE11]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Launch PySpark with `IPYNB` in the directory `examples/AN_Spark` where Jupyter
    or IPython Notebooks are stored:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储Jupyter或IPython笔记本的`examples/AN_Spark`目录中使用`IPYNB`启动PySpark：
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Building our first app with PySpark
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PySpark构建我们的第一个应用程序
- en: We are ready to check now that everything is working fine. The obligatory word
    count will be put to the test in processing a word count on the first chapter
    of this book.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备检查一切是否正常工作。在处理本书第一章的单词计数时，将对其进行测试。
- en: 'The code we will be running is listed here:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要运行的代码在这里列出：
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this program, we are first reading the file from the directory `/home/an/Documents/A00_Documents/Spark4Py
    20150315` into `file_in`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在此程序中，我们首先从目录`/home/an/Documents/A00_Documents/Spark4Py 20150315`中读取文件到`file_in`中。
- en: We are then introspecting the file by counting the number of lines and the number
    of characters per line.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过计算每行的行数和每行的字符数来审查文件。
- en: We are splitting the input file in to words and getting them in lower case.
    For our word count purpose, we are choosing words longer than three characters
    in order to avoid shorter and much more frequent words such as *the*, *and*, *for*
    to skew the count in their favor. Generally, they are considered stop words and
    should be filtered out in any language processing task.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输入文件拆分为单词并将它们转换为小写。为了避免较短和更频繁的单词（如*the*、*and*、*for*）对计数产生偏向，我们选择长度超过三个字符的单词进行单词计数。通常，它们被认为是停用词，并且应该在任何语言处理任务中被过滤掉。
- en: At this stage, we are getting ready for the MapReduce steps. To each word, we
    map a value of `1` and reduce it by summing all the unique words.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们准备进行MapReduce步骤。对于每个单词，我们将映射一个值`1`并通过对所有唯一单词求和来减少它。
- en: Here are illustrations of the code in the IPython Notebook. The first 10 cells
    are preprocessing the word count on the dataset, which is retrieved from the local
    file directory.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是IPython Notebook中代码的示例。前10个单元格是对数据集上的单词计数进行预处理，该数据集是从本地文件目录中检索的。
- en: '![Building our first app with PySpark](img/B03968_01_09.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![使用PySpark构建我们的第一个应用程序](img/B03968_01_09.jpg)'
- en: 'Swap the word count tuples in the format `(count, word)` in order to sort by
    `count`, which is now the primary key of the tuple:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 交换元组中的单词计数格式`(count, word)`，以便按`count`排序，这现在是元组的主键：
- en: '[PRE14]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In order to display our result, we are creating the tuple `(count, word)` and
    displaying the top 20 most frequently used words in descending order:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了显示我们的结果，我们创建元组`(count, word)`并按降序显示前20个最常用的单词：
- en: '![Building our first app with PySpark](img/B03968_01_10.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![使用PySpark构建我们的第一个应用程序](img/B03968_01_10.jpg)'
- en: 'Let''s create a histogram function:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个直方图函数：
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, we visualize the most frequent words by plotting them in a bar chart.
    We have to first swap the tuple from the original `(count, word)` to `(word, count)`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过在条形图中绘制它们来可视化最常用的单词。我们首先将元组从原始的`(count, word)`交换为`(word, count)`：
- en: '![Building our first app with PySpark](img/B03968_01_11.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![使用PySpark构建我们的第一个应用程序](img/B03968_01_11.jpg)'
- en: 'So here you have it: the most frequent words used in the first chapter are
    **Spark**, followed by **Data** and **Anaconda**.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您现在拥有的是：第一章中最常用的单词是**Spark**，其次是**Data**和**Anaconda**。
- en: Virtualizing the environment with Vagrant
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Vagrant虚拟化环境
- en: In order to create a portable Python and Spark environment that can be easily
    shared and cloned, the development environment can be built with a `vagrantfile`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个可以轻松共享和克隆的便携式Python和Spark环境，可以使用`vagrantfile`构建开发环境。
- en: 'We will point to the **Massive Open Online Courses** (**MOOCs**) delivered
    by *Berkeley University and Databricks*:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将指向由*伯克利大学和Databricks*提供的**大规模在线开放课程**（**MOOCs**）：
- en: '*Introduction to Big Data with Apache Spark, Professor Anthony D. Joseph* can
    be found at [https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x](https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Anthony D. Joseph教授的Apache Spark大数据介绍*可在[https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x](https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x)找到'
- en: '*Scalable Machine Learning, Professor* *Ameet Talwalkar* can be found at [https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x](https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可扩展机器学习，教授* *Ameet Talwalkar* 可以在[https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x](https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x)找到'
- en: 'The course labs were executed on IPython Notebooks powered by PySpark. They
    can be found in the following GitHub repository: [https://github.com/spark-mooc/mooc-setup/](https://github.com/spark-mooc/mooc-setup/).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 课程实验室是在由PySpark提供动力的IPython笔记本上执行的。它们可以在以下GitHub存储库中找到：[https://github.com/spark-mooc/mooc-setup/](https://github.com/spark-mooc/mooc-setup/)。
- en: 'Once you have set up Vagrant on your machine, follow these instructions to
    get started: [https://docs.vagrantup.com/v2/getting-started/index.html](https://docs.vagrantup.com/v2/getting-started/index.html).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在您的机器上设置了Vagrant，请按照以下说明开始：[https://docs.vagrantup.com/v2/getting-started/index.html](https://docs.vagrantup.com/v2/getting-started/index.html)。
- en: 'Clone the `spark-mooc/mooc-setup/ github` repository in your work directory
    and launch the command `$ vagrant up`, within the cloned directory:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的工作目录中克隆`spark-mooc/mooc-setup/ github`存储库，并在克隆的目录中启动命令`$ vagrant up`：
- en: Be aware that the version of Spark may be outdated as the `vagrantfile` may
    not be up-to-date.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于`vagrantfile`可能不是最新的，Spark的版本可能已过时。
- en: 'You will see an output similar to this:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到类似于这样的输出：
- en: '[PRE16]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will launch the IPython Notebooks powered by PySpark on `localhost:8001`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在`localhost:8001`上启动由PySpark提供动力的IPython笔记本：
- en: '![Virtualizing the environment with Vagrant](img/B03968_01_12.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![使用Vagrant虚拟化环境](img/B03968_01_12.jpg)'
- en: Moving to the cloud
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转移到云端
- en: As we are dealing with distributed systems, an environment on a virtual machine
    running on a single laptop is limited for exploration and learning. We can move
    to the cloud in order to experience the power and scalability of the Spark distributed
    framework.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在处理分布式系统，运行在单个笔记本电脑上的虚拟机上的环境对于探索和学习是有限的。我们可以转移到云端，以体验Spark分布式框架的强大和可扩展性。
- en: Deploying apps in Amazon Web Services
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Amazon Web Services中部署应用程序
- en: Once we are ready to scale our apps, we can migrate our development environment
    to **Amazon** **Web Services** (**AWS**).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好扩展我们的应用程序，我们可以将开发环境迁移到**Amazon** **Web Services** (**AWS**)。
- en: 'How to run Spark on EC2 is clearly described in the following page: [https://spark.apache.org/docs/latest/ec2-scripts.html](https://spark.apache.org/docs/latest/ec2-scripts.html).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在EC2上运行Spark在以下页面中清楚地描述：[https://spark.apache.org/docs/latest/ec2-scripts.html](https://spark.apache.org/docs/latest/ec2-scripts.html)。
- en: 'We emphasize five key steps in setting up the AWS Spark environment:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调在设置AWS Spark环境时的五个关键步骤：
- en: Create an AWS EC2 key pair via the AWS console [http://aws.amazon.com/console/](http://aws.amazon.com/console/).
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过AWS控制台创建AWS EC2密钥对[http://aws.amazon.com/console/](http://aws.amazon.com/console/)。
- en: 'Export your key pair to your environment:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的密钥对导出到您的环境中：
- en: '[PRE17]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Launch your cluster:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动您的集群：
- en: '[PRE18]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'SSH into a cluster to run Spark jobs:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SSH进入集群运行Spark作业：
- en: '[PRE19]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Destroy your cluster after usage:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用后销毁您的集群：
- en: '[PRE20]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Virtualizing the environment with Docker
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Docker虚拟化环境
- en: In order to create a portable Python and Spark environment that can be easily
    shared and cloned, the development environment can be built in Docker containers.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个可以轻松共享和克隆的便携式Python和Spark环境，开发环境可以在Docker容器中构建。
- en: 'We wish capitalize on Docker''s two main functions:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望利用Docker的两个主要功能：
- en: Creating isolated containers that can be easily deployed on different operating
    systems or in the cloud.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建可以轻松部署在不同操作系统或云中的隔离容器。
- en: Allowing easy sharing of the development environment image with all its dependencies
    using The DockerHub. The DockerHub is similar to GitHub. It allows easy cloning
    and version control. The snapshot image of the configured environment can be the
    baseline for further enhancements.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DockerHub允许轻松共享开发环境镜像及其所有依赖项。DockerHub类似于GitHub。它允许轻松克隆和版本控制。配置环境的快照图像可以作为进一步增强的基线。
- en: The following diagram illustrates a Docker-enabled environment with Spark, Anaconda,
    and the database server and their respective data volumes.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了一个具有Spark、Anaconda和数据库服务器及其各自数据卷的Docker启用环境。
- en: '![Virtualizing the environment with Docker](img/B03968_01_13.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![使用Docker虚拟化环境](img/B03968_01_13.jpg)'
- en: Docker offers the ability to clone and deploy an environment from the Dockerfile.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Docker提供了从Dockerfile克隆和部署环境的能力。
- en: 'You can find an example Dockerfile with a PySpark and Anaconda setup at the
    following address: [https://hub.docker.com/r/thisgokeboysef/pyspark-docker/~/dockerfile/](https://hub.docker.com/r/thisgokeboysef/pyspark-docker/~/dockerfile/).'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下地址找到一个带有PySpark和Anaconda设置的示例Dockerfile：[https://hub.docker.com/r/thisgokeboysef/pyspark-docker/~/dockerfile/](https://hub.docker.com/r/thisgokeboysef/pyspark-docker/~/dockerfile/)。
- en: 'Install Docker as per the instructions provided at the following links:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下链接提供的说明安装Docker：
- en: '[http://docs.docker.com/mac/started/](http://docs.docker.com/mac/started/)
    if you are on Mac OS X'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://docs.docker.com/mac/started/](http://docs.docker.com/mac/started/)
    如果您使用的是Mac OS X'
- en: '[http://docs.docker.com/linux/started/](http://docs.docker.com/linux/started/)
    if you are on Linux'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://docs.docker.com/linux/started/](http://docs.docker.com/linux/started/)
    如果你在Linux上'
- en: '[http://docs.docker.com/windows/started/](http://docs.docker.com/windows/started/)
    if you are on Windows'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://docs.docker.com/windows/started/](http://docs.docker.com/windows/started/)
    如果您使用的是Windows'
- en: 'Install the docker container with the Dockerfile provided earlier with the
    following command:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令安装提供的Dockerfile的docker容器：
- en: '[PRE21]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Other great sources of information on how to *dockerize* your environment can
    be seen at Lab41\. The GitHub repository contains the necessary code:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何*dockerize*您的环境的其他重要信息源可以在Lab41中找到。GitHub存储库包含必要的代码：
- en: '[https://github.com/Lab41/ipython-spark-docker](https://github.com/Lab41/ipython-spark-docker)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/Lab41/ipython-spark-docker](https://github.com/Lab41/ipython-spark-docker)'
- en: 'The supporting blog post is rich in information on thought processes involved
    in building the docker environment: [http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/](http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的博客文章中包含了构建docker环境所涉及的思维过程丰富的信息：[http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/](http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/)。
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We set the context of building data-intensive apps by describing the overall
    architecture structured around the infrastructure, persistence, integration, analytics,
    and engagement layers. We also discussed Spark and Anaconda with their respective
    building blocks. We set up an environment in a VirtualBox with Anaconda and Spark
    and demonstrated a word count app using the text content of the first chapter
    as input.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过描述围绕基础设施、持久性、集成、分析和参与层的整体架构来设定构建数据密集型应用的背景。我们还讨论了Spark和Anaconda以及它们各自的构建模块。我们在VirtualBox中使用Anaconda和Spark设置了一个环境，并演示了使用第一章的文本内容作为输入的词频统计应用程序。
- en: In the next chapter, we will delve more deeply into the architecture blueprint
    for data-intensive apps and tap into the Twitter, GitHub, and Meetup APIs to get
    a feel of the data we will be mining with Spark.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地探讨数据密集型应用的架构蓝图，并利用Twitter、GitHub和Meetup的API来感受我们将使用Spark进行挖掘的数据。
