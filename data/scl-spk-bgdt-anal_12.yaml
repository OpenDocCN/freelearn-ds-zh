- en: Advanced Machine Learning Best Practices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级机器学习最佳实践
- en: '"Hyperparameter optimization or model selection is the problem of choosing
    a set of hyperparameters [when defined as?] for a learning algorithm, usually
    with the goal of optimizing a measure of the algorithm''s performance on an independent
    dataset."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “超参数优化或模型选择是选择一组超参数的问题[当其定义为？]用于学习算法，通常目标是优化该算法在独立数据集上的表现度量。”
- en: '- Machine learning model tuning quote'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 机器学习模型调优引用'
- en: In this chapter, we will provide some theoretical and practical aspects of some
    advanced topics of machine learning (ML) with Spark. We will see how to tune machine
    learning models for better and optimized performance using grid search, cross-validation
    and hyperparameter tuning. In the later section, we will cover how to develop
    a scalable recommendation system using the ALS, which is an example of a model-based
    recommendation algorithm. Finally, a topic modeling application as a text clustering
    technique will be demonstrated.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一些关于使用Spark的机器学习（ML）高级主题的理论和实践方面的内容。我们将看到如何通过网格搜索、交叉验证和超参数调优来调整机器学习模型，以实现更好和优化的性能。在后续部分，我们将展示如何使用ALS开发一个可扩展的推荐系统，这是一种基于模型的推荐算法的例子。最后，我们将展示一个作为文本聚类技术的主题建模应用。
- en: 'In a nutshell, we will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们将在本章中涵盖以下主题：
- en: Machine learning best practice
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习最佳实践
- en: Hyperparameter tuning of ML models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型的超参数调优
- en: Topic modeling using latent dirichlet allocation (LDA)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用潜在狄利克雷分配（LDA）进行主题建模
- en: A recommendation system using collaborative filtering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用协同过滤的推荐系统
- en: Machine learning best practices
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习最佳实践
- en: 'Sometimes, it is recommended to consider the error rate rather than only the
    accuracy. For example, suppose an ML system with 99% accuracy and 50% errors is
    worse than the one with 90% accuracy but 25% errors. So far, we have discussed
    the following machine learning topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，建议考虑误差率，而不仅仅是准确率。例如，假设一个机器学习系统的准确率为99%，但误差率为50%，这比一个准确率为90%但误差率为25%的系统更差。到目前为止，我们已经讨论了以下机器学习主题：
- en: '**Regression**: This is for predicting values that are linearly separable'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：这是用于预测线性可分的值'
- en: '**Anomaly detection**: This is for finding unusual data points often done using
    a clustering algorithm'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测**：这是用于寻找不寻常的数据点，通常通过使用聚类算法来实现'
- en: '**Clustering**: This is for discovering the hidden structure in the dataset
    for clustering homogeneous data points'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：这是用于发现数据集中隐藏的结构，以便对同质数据点进行聚类'
- en: '**Binary classification**: This is for predicting two categories'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二分类**：这是用于预测两类类别的任务'
- en: '**Multi-class classification**: This is for predicting three or more categories'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类分类**：这是用于预测三个或更多类别的任务'
- en: Well, we have also seen that there are some good algorithms for these tasks.
    However, choosing the right algorithm for your problem type is a tricky task for
    achieving higher and outstanding accuracy in your ML algorithms. For this, we
    need to adopt some good practices through the stages, that is, from data collection,
    feature engineering, model building, evaluating, tuning, and deployment. Considering
    these, in this section, we will provide some practical recommendation while developing
    your ML application using Spark.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也看到了一些适合这些任务的优秀算法。然而，选择正确的算法来解决你的问题类型，对于实现更高和卓越的准确性来说是一个具有挑战性的任务。为此，我们需要在各个阶段采取一些好的实践，也就是说，从数据收集、特征工程、模型构建、评估、调优到部署。考虑到这些，在本节中，我们将提供一些实践性的建议，帮助你在使用Spark开发机器学习应用时更为高效。
- en: Beware of overfitting and underfitting
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小心过拟合和欠拟合
- en: A straight line cutting across a curving scatter plot would be a good example
    of under-fitting, as we can see in the diagram here. However, if the line fits
    the data too well, there evolves an opposite problem called **overfitting**. When
    we say a model overfits a dataset, we mean it may have a low error rate for the
    training data, but it does not generalize well to the overall population in the
    data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一条穿过弯曲散点图的直线可以作为欠拟合的一个典型例子，如图所示。然而，如果这条直线过于贴合数据，就会出现一个相反的问题，称为**过拟合**。当我们说一个模型对数据集发生过拟合时，我们的意思是它可能在训练数据上的误差率很低，但却无法很好地对数据中的总体分布进行泛化。
- en: '![](img/00148.jpeg)**Figure 1**: Overfitting-underfitting trade-off (source:
    The book, "Deep Learning" by Adam Gibson, Josh Patterson)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00148.jpeg)**图 1**：过拟合与欠拟合的权衡（来源：《深度学习》一书，作者：Adam Gibson, Josh Patterson）'
- en: 'More technically, if you evaluate your model on the training data instead of
    test or validated data, you probably won''t be able to articulate whether your
    model is overfitting or not. The common symptoms are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 更技术性地说，如果你在训练数据上评估你的模型，而不是在测试数据或验证数据上进行评估，你可能无法明确指出模型是否发生了过拟合。常见的症状如下：
- en: Predictive accuracy of the data used for training can be over accurate (that
    is, sometimes even 100%).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练的数据的预测准确度可能过于准确（也就是说，有时甚至达到100%）。
- en: The model might show better performance compared to the random prediction for
    new data.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相较于随机预测，模型可能在新数据上表现出更好的性能。
- en: We like to fit a dataset to a distribution because if the dataset is reasonably
    close to the distribution, we can make assumptions based on the theoretical distribution
    of how we operate with the data. Consequently, the normal distribution in the
    data allows us to assume that sampling distributions of statistics are normally
    distributed under specified conditions. The normal distribution is defined by
    its mean and standard deviation and has generally the same shape across all variations.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们喜欢将数据集拟合到某个分布，因为如果数据集与该分布较为接近，我们可以基于该理论分布作出假设，以指导我们如何处理数据。因此，数据中的正态分布使我们可以假设，在指定条件下，统计量的抽样分布是正态分布。正态分布由其均值和标准差定义，通常在所有变体中形状相同。
- en: '![](img/00012.jpeg)**Figure 2**: Normal distribution in the data helps overcoming
    both the over-fitting and underfitting (source: The book, "Deep Learning" by Adam
    Gibson, Josh Patterson)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00012.jpeg)**图2**：数据中的正态分布有助于克服过拟合和欠拟合（来源：《深度学习》一书，作者：Adam Gibson,
    Josh Patterson）'
- en: 'Sometimes, the ML model itself becomes underfit for a particular tuning or
    data point, which means the model become too simplistic. Our recommendation (like
    that of others, we believe) is as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，机器学习模型本身会在某些特定的调参或数据点上发生欠拟合，这意味着模型变得过于简化。我们的建议（如同其他人的观点一样）如下：
- en: Splitting the dataset into two sets to detect overfitting situations--the first
    one is for training and model selection called the training set, and the second
    one is the test set for evaluating the model started in place of the ML workflow
    section.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集拆分为两部分以检测过拟合情况——第一部分用于训练和模型选择，称为训练集；第二部分是用于评估模型的测试集，取代机器学习工作流部分中的评估步骤。
- en: Alternatively, you also could avoid the overfitting by consuming simpler models
    (for example, linear classifiers in preference to Gaussian kernel SVM) or by swelling
    the regularization parameters of your ML model (if available).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另外，你也可以通过使用更简单的模型（例如，优先选择线性分类器而非高斯核SVM）或通过增加机器学习模型的正则化参数（如果可用）来避免过拟合。
- en: Tune the model with a correct data value of parameters to avoid both the overfitting
    as well as underfitting.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整模型的参数值，以避免过拟合和欠拟合。
- en: 'Thus, solving underfitting is the priority, but most of the machines learning
    practitioners suggest spending more time and effort attempting not to overfit
    the line to the data. Also, many machine learning practitioners, on the other
    hand, have recommended splitting the large-scale dataset into three sets: a training
    set (50%), validation set (25%), and test set (25%). They also suggested to build
    the model using the training set and to calculate the prediction errors using
    the validation set. The test set was recommended to be used to assess the generalization
    error of the final model. If the amount of labeled data available on the other
    hand during the supervised learning is smaller, it is not recommended to split
    the datasets. In that case, use cross validation. More specifically, divide the
    dataset into 10 parts of (roughly) equal size; after that, for each of these 10
    parts, train the classifier iteratively and use the 10th part to test the model.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，解决欠拟合是首要任务，但大多数机器学习从业者建议投入更多的时间和精力，避免将模型过拟合到数据上。另一方面，许多机器学习从业者推荐将大规模数据集分成三个部分：训练集（50%）、验证集（25%）和测试集（25%）。他们还建议使用训练集构建模型，并使用验证集计算预测误差。测试集则建议用于评估最终模型的泛化误差。如果在监督学习过程中可用的标记数据较少，则不推荐拆分数据集。在这种情况下，使用交叉验证。更具体地说，将数据集分成10个（大致）相等的部分；然后，对于这10个部分中的每一个，迭代训练分类器，并使用第10部分来测试模型。
- en: Stay tuned with Spark MLlib and Spark ML
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 请关注Spark MLlib和Spark ML
- en: The first step of the pipeline designing is to create the building blocks (as
    a directed or undirected graph consisting of nodes and edges) and make a linking
    between those blocks. Nevertheless, as a data scientist, you should be focused
    on scaling and optimizing nodes (primitives) too so that you are able to scale
    up your application for handling large-scale datasets in the later stage to make
    your ML pipeline performing consistently. The pipeline process will also help
    you to make your model adaptive for new datasets. However, some of these primitives
    might be explicitly defined to particular domains and data types (for example,
    text, image, and video, audio, and spatiotemporal).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 管道设计的第一步是创建构建模块（作为由节点和边组成的有向或无向图），并在这些模块之间建立链接。然而，作为数据科学家，你也应该专注于对节点（基本元素）进行扩展和优化，以便能够在后续阶段扩展你的应用程序，以处理大规模数据集，使得你的
    ML 管道能够稳定地执行。管道过程还将帮助你使模型适应新数据集。不过，这些基本元素中的一些可能会显式地定义为特定领域和数据类型（例如文本、图像、视频、音频以及时空数据）。
- en: Beyond these types of data, the primitives should also be working for general
    purpose domain statistics or mathematics. The casting of your ML model in terms
    of these primitives will make your workflow more transparent, interpretable, accessible,
    and explainable.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些类型的数据外，基本元素还应该适用于通用领域的统计或数学。将你的 ML 模型以这些基本元素的形式表示，将使你的工作流程更加透明、可解释、可访问且易于解释。
- en: A recent example would be the ML-matrix, which is a distributed matrix library
    that can be used on top of Spark. Refer to the JIRA issue at [https://issues.apache.org/jira/browse/SPARK-3434](https://issues.apache.org/jira/browse/SPARK-3434).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一个例子是 ML-matrix，这是一个分布式矩阵库，可以在 Spark 上使用。请参考 JIRA 问题：[https://issues.apache.org/jira/browse/SPARK-3434](https://issues.apache.org/jira/browse/SPARK-3434)。
- en: '![](img/00109.jpeg)**Figure 3:** Stay tune and interoperate ML and MLlib'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00109.jpeg)**图 3：**保持同步并互操作 ML 和 MLlib'
- en: As we already stated in the previous section, as a developer, you can seamlessly
    combine the implementation techniques in Spark MLlib along with the algorithms
    developed in Spark ML, Spark SQL, GraphX, and Spark Streaming as hybrid or interoperable
    ML applications on top of RDD, DataFrame, and datasets as shown in *Figure 3*.
    Therefore, the recommendation here is to stay in tune or synchronized with the
    latest technologies around you for the betterment of your ML application.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中所述，作为开发者，你可以将 Spark MLlib 中的实现技术与在 Spark ML、Spark SQL、GraphX 和 Spark
    Streaming 中开发的算法无缝结合，作为基于 RDD、DataFrame 和数据集的混合或互操作的 ML 应用程序，正如*图 3*所示。因此，这里的建议是保持同步或与周围最新的技术保持一致，以便改进你的
    ML 应用程序。
- en: Choosing the right algorithm for your application
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为你的应用程序选择正确的算法
- en: '"What machine learning algorithm should I use?" is a very frequently asked
    question for the naive machine learning practitioners but the answer is always
    *it depends*. More elaborately:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: “我应该使用什么机器学习算法？”是许多初学者常常问的问题，但答案总是*取决于情况*。更详细地说：
- en: It depends on the volume, quality, complexity, and the nature of the data you
    have to be tested/used
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于你必须测试/使用的数据的量、质量、复杂性和性质
- en: It depends on external environments and parameters like your computing system's
    configuration or underlying infrastructures
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于外部环境和参数，例如计算系统的配置或底层基础设施
- en: It depends on what you want to do with the answer
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于你想用答案做什么
- en: It depends on how the mathematical and statistical formulation of the algorithm
    was translated into machine instructions for the computer
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于算法的数学和统计公式是如何转化为计算机的机器指令的
- en: It depends on how much time do you have
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于你有多少时间
- en: 'The reality is, even the most experienced data scientists or data engineers
    can''t give a straight recommendation about which ML algorithm will perform the
    best before trying them all together. Most of the statements of agreement/disagreement
    begins with "It depends...hmm..." Habitually, you might wonder if there are cheat
    sheets of machine learning algorithms, and if so, how should you use that cheat
    sheet? Several data scientists have said that the only sure way to find the very
    best algorithm is to try all of them; therefore, there is no shortcut dude! Let''s
    make it clearer; suppose you do have a set of data and you want to do some clustering.
    Technically, this could be either a classification or regression problem that
    you want o apply on your dataset if your data is labeled. However, if you have
    a unlabeled dataset, it is the clustering technique that you will be using. Now,
    the concerns that evolve in your mind are as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现实情况是，即使是最有经验的数据科学家或数据工程师，在尝试所有算法之前也无法直接推荐哪个机器学习算法会表现最佳。大多数同意或不同意的陈述都会以“这取决于...嗯...”开始。习惯性地，你可能会想知道是否有机器学习算法的备忘单，如果有，我该如何使用那个备忘单？一些数据科学家表示，找到最好的算法唯一的确切方法是尝试所有的算法；所以，没有捷径，伙计！让我们再说得清楚一点；假设你确实有一组数据，并且想做一些聚类。技术上讲，如果你的数据是有标签的，这可以是一个分类或回归问题。但如果你有一个无标签数据集，你将使用聚类技术。那么，你脑海中出现的疑问如下：
- en: Which factors should I consider before choosing an appropriate algorithm? Or
    should I just choose an algorithm randomly?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择适当的算法之前，我应该考虑哪些因素？或者我应该随机选择一个算法？
- en: How do I choose any data pre-processing algorithm or tools that can be applied
    to my data?
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我该如何选择任何适用于我的数据的数据预处理算法或工具？
- en: What sort of feature engineering techniques should I be using to extract the
    useful features?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该使用什么样的特征工程技术来提取有用的特征？
- en: What factors can improve the performance of my ML model?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有哪些因素可以提高我的机器学习模型的性能？
- en: How can I adopt my ML application for new data types?
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我该如何调整我的机器学习应用以适应新的数据类型？
- en: Can I scale up my ML application for large-scale datasets? And so on.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能否为大规模数据集扩展我的机器学习应用？等等。
- en: In this section, we will try to answer these questions with our little machine
    learning knowledge.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试用我们有限的机器学习知识回答这些问题。
- en: Considerations when choosing an algorithm
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择算法时的考虑因素
- en: 'The recommendation or suggestions we provide here are for the novice data scientist
    who is just learning machine learning. These will use useful to expert the data
    scientists too, who is trying to choose an optimal algorithm to start with Spark
    ML APIs. Don''t worry, we will guide you to the direction! We also recommend going
    with the following algorithmic properties when choosing an algorithm:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里提供的建议或推荐是针对刚刚学习机器学习的初学者数据科学家。这些建议对于尝试选择一个最优算法来开始使用 Spark ML API 的专家数据科学家也很有用。别担心，我们会引导你朝着正确的方向前进！我们还建议在选择算法时考虑以下算法特性：
- en: '**Accuracy**: Whether getting the best score is the goal or an approximate
    solution (*good enough*) in terms of precision, recall, f1 score or AUC and so
    on, while trading off overfitting.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：无论是为了获得最佳得分，还是为了在精确度、召回率、F1得分或AUC等指标上获得一个近似解（*足够好*），同时权衡过拟合问题。'
- en: '**Training time**: The amount of time available to train the model (including
    the model building, evaluation, and tanning time).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练时间**：用于训练模型的时间量（包括模型构建、评估和训练时间）。'
- en: '**Linearity**: An aspect of model complexity in terms of how the problem is
    modeled. Since most of the non-linear models are often more complex to understand
    and tune.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性**：模型复杂性的一个方面，指的是问题是如何建模的。由于大多数非线性模型通常更复杂，难以理解和调优。'
- en: '**Number of parameters**'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数数量**'
- en: '**Number of features**: The problem of having more attributes than instances,
    the *p>>n* problem. This often requires specialized handling or specialized techniques
    using dimensionality reduction or better feature engineering approach.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征数量**：当特征数量超过实例数时出现的问题，*p>>n*问题。这通常需要使用降维或更好的特征工程方法来进行专门的处理或采用专门的技术。'
- en: Accuracy
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确性
- en: 'Getting the most accurate results from your ML application isn''t always indispensable.
    Depending on what you want to use it for, sometimes an approximation is adequate
    enough. If the situation is something like this, you may be able to reduce the
    processing time drastically by incorporating the better-estimated methods. When
    you become familiar of the workflow with the Spark machine learning APIs, you
    will enjoy the advantage of having more approximation methods, because these approximation
    methods will tend to avoid the overfitting problem of your ML model automatically.
    Now, suppose you have two binary classification algorithms that perform as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '从你的机器学习应用程序中获得最准确的结果并不总是必不可少的。根据你的使用需求，有时近似结果就足够了。如果情况是这样，你可以通过采用更好估计的方法大幅减少处理时间。当你熟悉Spark机器学习API的工作流后，你将享受拥有更多近似方法的优势，因为这些近似方法通常能自动避免机器学习模型的过拟合问题。现在，假设你有两个二分类算法，表现如下：  '
- en: '| **Classifier** | **Precision** | **Recall** |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| **分类器** | **精确度** | **召回率** |  '
- en: '| X | 96% | 89% |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| X | 96% | 89% |  '
- en: '| Y | 99% | 84% |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Y | 99% | 84% |  '
- en: 'Here, none of the classifiers is obviously superior, so it doesn''t immediately
    guide you toward picking the optimal one. F1-score which is the harmonic mean
    of precision and recall helps you. Let''s calculate it and place it in the table:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，没有哪个分类器明显优于其他，因此它并不能立即指导你选择最优的一个。F1分数，作为精确度和召回率的调和平均值，将帮助你。让我们计算它并将其放入表格：  '
- en: '| **Classifier** | **Precision** | **Recall** | **F1 score** |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| **分类器** | **精确度** | **召回率** | **F1分数** |  '
- en: '| X | 96% | 89% | 92.36% |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| X | 96% | 89% | 92.36% |  '
- en: '| Y | 99% | 84% | 90.885% |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Y | 99% | 84% | 90.885% |  '
- en: Therefore, having an F1-score helps make a decision for selecting from a large
    number of classifiers. It gives a clear preference ranking among all of them,
    and therefore a clear direction for progress--that is classifier **X**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，拥有F1分数有助于在大量分类器中做出选择。它为所有分类器提供了一个清晰的优先级排序，从而为你的进步提供了明确的方向——那就是分类器**X**。
- en: Training time
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '训练时间  '
- en: Training time is often closely related to the model training and the accuracy.
    In addition, often you will discover that some of the algorithms are elusive to
    the number of data points compared to others. However, when your time is inadequate
    but the training set is large with a lot of features, you can choose the simplest
    one. In this case, you might have to compromise with the accuracy. But it will
    fulfill your minimum requirements at least.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '训练时间通常与模型训练和准确性密切相关。此外，你常常会发现某些算法在数据点数量上比其他算法表现得更为模糊。然而，当你的时间有限，但训练集非常庞大且特征较多时，你可以选择最简单的算法。在这种情况下，你可能需要在准确性上做出妥协。但至少，它将满足你的最低要求。  '
- en: Linearity
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '线性  '
- en: There are many machine learning algorithms developed recently that make use
    of linearity (also available in the Spark MLlib and Spark ML). For example, linear
    classification algorithms undertake that classes can be separated by plotting
    a differentiating straight line or using higher-dimensional equivalents. Linear
    regression algorithms, on the other hand, assume that data trends simply follow
    a straight line. This assumption is not naive for some machine learning problems;
    however, there might be some other cases where the accuracy will be down. Despite
    their hazards, linear algorithms are very popular with data engineers and data
    scientists as the first line of the outbreak. Moreover, these algorithms also
    tend to be simple and fast, to train your models during the whole process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，许多机器学习算法利用了线性特性（在Spark MLlib和Spark ML中也可以使用）。例如，线性分类算法假设类可以通过绘制一个分隔的直线或使用更高维度的等价物来分离。而线性回归算法则假设数据趋势简单地遵循一条直线。对于某些机器学习问题，这一假设并不天真；然而，也可能有一些其他情况会导致准确性下降。尽管存在一些风险，线性算法仍然非常受数据工程师和数据科学家的欢迎，因为它们是应对突发问题的首选。更重要的是，这些算法通常简单且快速，能够在整个过程中训练你的模型。  '
- en: Inspect your data when choosing an algorithm
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '选择算法时请检查你的数据  '
- en: 'You will find many machine learning datasets available at the UC Irvine Machine
    Learning Repository. The following data properties should also be prioritized:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '你可以在UC Irvine机器学习库找到许多机器学习数据集。以下数据属性也应优先考虑：  '
- en: Number of parameters
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '参数数量  '
- en: Number of features
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '特征数量  '
- en: Size of the training dataset
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '训练数据集的大小  '
- en: Number of parameters
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '参数数量  '
- en: Parameters or data properties are the handholds for a data scientist when setting
    up an algorithm. They are numbers that affect the algorithm's performance, such
    as error tolerance or a number of iterations, or options between variants of how
    the algorithm acts. The training time and accuracy of the algorithm can sometimes
    be quite sensitive making it difficult get the right settings. Typically, algorithms
    with a large number of parameters require more trial and error to find an optimal
    combination.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数或数据属性是数据科学家在设置算法时的抓手。它们是影响算法性能的数字，比如误差容忍度、迭代次数，或者算法行为变体之间的选择。算法的训练时间和准确性有时会非常敏感，这使得找到正确的设置变得困难。通常，具有大量参数的算法需要更多的试验和错误来找到最优的组合。
- en: 'Despite the fact that this is a great way to span the parameter space, the
    model building or train time increases exponentially with the increased number
    of parameters. This is a dilemma as well as a time-performance trade-off. The
    positive sides are:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是跨越参数空间的一个好方法，但随着参数数量的增加，模型构建或训练时间呈指数增长。这既是一个困境，也是时间与性能之间的权衡。其积极方面是：
- en: Having many parameters characteristically indicates the greater flexibility
    of the ML algorithms
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有许多参数通常表示机器学习算法具有更大的灵活性
- en: Your ML application achieves much better accuracy
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的机器学习应用实现了更好的准确度
- en: How large is your training set?
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的训练集有多大？
- en: If your training set is smaller, high bias with low variance classifiers, such
    as Naive Bayes have an advantage over low bias with high variance classifiers
    (also can be used for regression) such as the **k-nearest neighbors algorithm**
    (**kNN**).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的训练集较小，低偏差且低方差的分类器，如朴素贝叶斯，在低偏差且高方差的分类器（如**k-最近邻算法**（**kNN**））上具有优势（也可以用于回归）。
- en: '**Bias, Variance, and the kNN model:** In reality, *increasing k* will *decrease
    the variance,* but *increase the bias*. On the other hand, *decreasing k* will
    *increase variance* and *decrease bias*. As *k* increases, this variability is
    reduced. But if we increase *k* too much, then we no longer follow the true boundary
    line and we observe high bias. This is the nature of the Bias-Variance Trade-off.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差、方差与kNN模型：** 实际上，*增加k*会*减少方差*，但*增加偏差*。另一方面，*减少k*会*增加方差*并*减少偏差*。随着*k*的增加，这种变异性被减小。但如果我们过度增加*k*，那么我们就不再遵循真实的边界线，观察到的是较高的偏差。这就是偏差-方差权衡的本质。'
- en: 'We have seen the over and underfitting issue already. Now, you can assume that
    dealing with bias and variance is like dealing with over- and underfitting. Bias
    is reduced and variance is increased in relation to model complexity. As more
    and more parameters are added to a model, the complexity of the model rises and
    variance becomes our primary concern while bias steadily falls. In other words,
    bias has a negative first-order derivative in response to model complexity, while
    variance has a positive slope. Refer to the following figure for a better understanding:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到过拟合和欠拟合的问题了。现在，你可以假设，处理偏差和方差就像处理过拟合和欠拟合一样。随着模型复杂度的增加，偏差减少，而方差增加。随着模型中参数的增多，模型的复杂度上升，方差成为我们主要关注的问题，而偏差则稳步下降。换句话说，偏差对模型复杂度的导数为负，而方差则有正斜率。请参考下图以便更好地理解：
- en: '![](img/00077.jpeg)**Figure 4:** Bias and variance contributing to total error'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00077.jpeg)**图 4：** 偏差和方差对总误差的贡献'
- en: Therefore, the latter will overfit. But low bias with high variance classifiers,
    on the other hand, starts to win out as your training set grows linearly or exponentially,
    since they have lower asymptotic errors. High bias classifiers aren't powerful
    enough to provide accurate models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，后者将会过拟合。然而，低偏差且高方差的分类器则在训练集线性或指数增长时开始占据优势，因为它们具有较低的渐近误差。高偏差的分类器则不足以提供准确的模型。
- en: Number of features
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征数量
- en: For certain types of experimental dataset, the number of extracted features
    can be very large compared to the number of data points itself. This is often
    the case with genomics, biomedical, or textual data. A large number of features
    can swamp down some learning algorithms, making training time ridiculously higher.
    **Support Vector Machines** (**SVMs**) are particularly well suited in this case
    for its high accuracy, nice theoretical guarantees regarding overfitting, and
    with an appropriate kernel.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些类型的实验数据集，提取的特征数可能会比数据点本身的数量大得多。通常在基因组学、生物医学或文本数据中会出现这种情况。大量的特征可能会拖慢一些学习算法的速度，使得训练时间极度增加。**支持向量机**（**SVM**）在这种情况下特别适用，因为它具有较高的准确性，关于过拟合的良好理论保证，并且使用合适的核函数。
- en: '**The SVM and the Kernel**: The task is to find a set of weight and bias such
    that the margin can maximize the function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机与核函数**：任务是找到一组权重和偏置，使得可以最大化边距的函数：'
- en: y = w*¥(x) +b,
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: y = w*¥(x) + b,
- en: 'Where *w* is the weight, *¥* is the feature vector, and *b* is the bias. Now
    if *y> 0*, then we classify datum to class *1*, else to class *0*, whereas, the
    feature vector *¥(x)* makes the data linearly separable. However, using the kernel
    makes the calculation process faster and easier, especially when the feature vector
    *¥* consisting of very high dimensional data. Let''s see a concrete example. Suppose
    we have the following value of *x* and *y*: *x = (x1, x2, x3)* and *y = (y1, y2,
    y3)*, then for the function *f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1,
    x3x2, x3x3)*, the kernel is *K(x, y ) = (<x, y>)²*. Following the above, if *x*
    *= (1, 2, 3)* and *y = (4, 5, 6)*, then we have the following values:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *w* 是权重，*¥* 是特征向量，*b* 是偏置。现在如果 *y > 0*，那么我们将数据分类为 *1* 类，反之为 *0* 类，而特征向量 *¥(x)*
    使得数据线性可分。然而，使用核函数使得计算过程更快、更简便，尤其当特征向量 *¥* 包含高维数据时。我们来看一个具体的例子。假设我们有以下 *x* 和 *y*
    的值：*x = (x1, x2, x3)* 和 *y = (y1, y2, y3)*，那么对于函数 *f(x) = (x1x1, x1x2, x1x3, x2x1,
    x2x2, x2x3, x3x1, x3x2, x3x3)*，核函数为 *K(x, y) = (<x, y>)²*。按照上面的方式，如果 *x = (1,
    2, 3)* 和 *y = (4, 5, 6)*，那么我们得到以下值：
- en: f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)
- en: f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)
- en: <f(x), f(y)> = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <f(x), f(y)> = 16 + 40 + 72 + 40 + 100 + 180 + 72 + 180 + 324 = 1024
- en: This is a simple linear algebra that maps a 3-dimensional space to a 9 dimensional.
    On the other hand, the kernel is a similarity measure used for SVMs. Therefore,
    choosing an appropriate kernel value based on the prior knowledge of invariances
    is suggested. The choice of the kernel and kernel and regularization parameters
    can be automated by optimizing a cross-validation based model selection.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种简单的线性代数操作，将三维空间映射到九维空间。另一方面，核函数是用于支持向量机的相似性度量。因此，建议根据对不变性的先验知识选择合适的核函数值。核函数、核函数参数和正则化参数的选择可以通过优化基于交叉验证的模型选择来自动化。
- en: Nevertheless, an automated choice of kernels and kernel parameters is a tricky
    issue, as it is very easy to overfit the model selection criterion. This might
    result in a worse model than you started with. Now, if we use the kernel function
    *K(x, y), this gives the same value but with much simpler calculation -i.e. (4
    + 10 + 18) ^2 = 32^2 = 1024*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自动选择核函数及其参数是一个棘手的问题，因为很容易导致模型选择标准的过拟合。这可能导致一个比最初更差的模型。现在，如果我们使用核函数 *K(x,
    y)*，它给出的结果与传统计算相同，但计算过程要简单得多——即 (4 + 10 + 18) ^2 = 32^2 = 1024。
- en: Hyperparameter tuning of ML models
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的超参数调优
- en: 'Tuning an algorithm is simply a process that one goes through in order to enable
    the algorithm to perform optimally in terms of runtime and memory usage. In Bayesian
    statistics, a hyperparameter is a parameter of a prior distribution. In terms
    of machine learning, the term hyperparameter refers to those parameters that cannot
    be directly learned from the regular training process. Hyperparameters are usually
    fixed before the actual training process begins. This is done by setting different
    values for those hyperparameters, training different models, and deciding which
    ones work best by testing them. Here are some typical examples of such parameters:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 调整算法参数是一个过程，通过这个过程可以让算法在运行时间和内存使用上表现得最优。在贝叶斯统计中，超参数是先验分布的一个参数。在机器学习中，超参数指的是那些不能通过常规训练过程直接学习到的参数。超参数通常在实际训练过程开始之前就已经确定。这是通过为这些超参数设置不同的值，训练不同的模型，然后通过测试它们来决定哪些效果最好。以下是一些典型的超参数示例：
- en: Number of leaves, bins, or depth of a tree
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树的叶节点数、箱数或深度
- en: Number of iterations
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数
- en: Number of latent factors in a matrix factorization
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵分解中的潜在因子数
- en: Learning rate
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: Number of hidden layers in a deep neural network
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络中的隐藏层数
- en: Number of clusters in a k-means clustering and so on.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means聚类中的聚类数量，等等。
- en: In this section, we will discuss how to perform hyperparameter tuning using
    the cross-validation technique and grid searching.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论如何使用交叉验证技术和网格搜索进行超参数调优。
- en: Hyperparameter tuning
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调优
- en: 'Hyperparameter tuning is a technique for choosing the right combination of
    hyperparameters based on the performance of presented data. It is one of the fundamental
    requirements to obtain meaningful and accurate results from machine learning algorithms
    in practice. The following figure shows the model tuning process, consideration,
    and workflow:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优是一种选择合适超参数组合的技术，基于呈现数据的性能。它是从实际机器学习算法中获得有意义且准确结果的基本要求之一。下图展示了模型调优过程、考虑因素和工作流程：
- en: '![](img/00343.jpeg)**Figure 5**: The model tuning process, consideration, and
    workflow'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00343.jpeg)**图5**：模型调优过程、考虑因素和工作流程'
- en: 'For example, suppose we have two hyperparameters to tune for a pipeline presented
    in *Figure 17* from [Chapter 11](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c),
    *Learning Machine Learning - Spark MLlib and Spark ML*, a Spark ML pipeline model
    using a logistic regression estimator (dash lines only happen during pipeline
    fitting). We can see that we have put three candidate values for each. Therefore,
    there would be nine combinations in total. However, only four are shown in the
    diagram, namely, Tokenizer, HashingTF, Transformer, and Logistic Regression (LR).
    Now, we want to find the one that will eventually lead to the model with the best
    evaluation result. The fitted model consists of the Tokenizer, the HashingTF feature
    extractor, and the fitted logistic regression model:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有两个超参数需要调节，来自[第11章](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c)的*图17*，*学习机器学习
    - Spark MLlib 和 Spark ML*，一个使用逻辑回归估计器的Spark ML管道模型（虚线仅出现在管道拟合过程中）。我们可以看到，我们为每个超参数提供了三个候选值。因此，总共有九种组合。然而，图中只显示了四个，即Tokenizer、HashingTF、Transformer和逻辑回归（LR）。现在，我们希望找到最终能得到最佳评估结果的模型。拟合后的模型包括Tokenizer、HashingTF特征提取器和拟合后的逻辑回归模型：
- en: If you recall *Figure 17* from [Chapter 11](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c),
    *Learning Machine Learning - Spark MLlib and Spark ML*, the dashed line, however,
    happens only during the pipeline fitting. As mentioned earlier, the fitted pipeline
    model is a Transformer. The Transformer can be used for prediction, model validation,
    and model inspection. In addition, we also argued that one ill-fated distinguishing
    characteristic of the ML algorithms is that typically they have many hyperparameters
    that need to be tuned for better performance. For example, the degree of regularizations
    in these hyperparameters is distinctive from the model parameters optimized by
    the Spark MLlib.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回想一下来自[第11章](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c)的*图17*，*学习机器学习
    - Spark MLlib 和 Spark ML*，那么虚线仅出现在管道拟合过程中。如前所述，拟合后的管道模型是一个Transformer。Transformer可以用于预测、模型验证和模型检查。此外，我们还提到，机器学习算法的一个不幸的特点是，通常它们有许多超参数需要调节以提高性能。例如，这些超参数中的正则化程度，与由Spark
    MLlib优化的模型参数是不同的。
- en: As a consequence, it is really hard to guess or measure the best combination
    of hyperparameters without expert knowledge of the data and the algorithm to use.
    Since the complex dataset is based on the ML problem type, the size of the pipeline
    and the number of hyperparameters may grow exponentially (or linearly); the hyperparameter
    tuning becomes cumbersome even for an ML expert, not to mention that the result
    of the tuning parameters may become unreliable.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，没有专家知识的情况下，很难猜测或测量出最佳的超参数组合，因为需要使用的数据和算法类型都很复杂。由于复杂的数据集是基于机器学习问题类型的，管道的规模和超参数的数量可能会呈指数级（或线性）增长；即使对于机器学习专家来说，超参数调优也变得繁琐，更不用说调优结果可能变得不可靠了。
- en: 'According to Spark API documentation, a unique and uniform API is used for
    specifying Spark ML estimators and Transformers. A `ParamMap` is a set of (parameter,
    value) pairs with a Param as a named parameter with self-contained documentation
    provided by Spark. Technically, there are two ways for passing the parameters
    to an algorithm as specified in the following options:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Spark API文档，Spark ML的估算器和转换器都使用一个唯一且统一的API来指定。`ParamMap`是由一组（参数，值）对组成的，每个Param是一个具有自包含文档的命名参数，由Spark提供。技术上，有两种方式可以将参数传递给算法，如下所述：
- en: '**Setting parameters**: If an LR is an instance of Logistic Regression (that
    is, Estimator), you can call the `setMaxIter()` method as follows: `LR.setMaxIter(5)`.
    It essentially fits the model pointing the regression instance as follows: `LR.fit()`.
    In this particular example, there would be at most five iterations.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置参数**：如果LR是逻辑回归（即Estimator）的一个实例，你可以调用`setMaxIter()`方法，如下所示：`LR.setMaxIter(5)`。这基本上是指向回归实例的模型拟合，如下所示：`LR.fit()`。在这个特定示例中，最多会进行五次迭代。'
- en: '**The second option**: This one involves passing a `ParamMaps` to `fit()` or
    `transform()` (refer *Figure 5* for details). In this circumstance, any parameters
    will be overridden by the `ParamMaps` previously specified via setter methods
    in the ML application-specific codes or algorithms.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二个选项**：这个选项涉及将`ParamMaps`传递给`fit()`或`transform()`（有关详细信息，请参见*图5*）。在这种情况下，任何参数都会被之前通过setter方法在ML应用程序特定代码或算法中指定的`ParamMaps`覆盖。'
- en: Grid search parameter tuning
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网格搜索参数调优
- en: 'Suppose you selected your hyperparameters after necessary feature engineering.
    In this regard, a full grid search of the space of hyperparameters and features
    is computationally too intensive. Therefore, you need to perform a fold of the
    K-fold cross-validation instead of a full-grid search:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在进行必要的特征工程后，你选择了你的超参数。在这种情况下，全面的网格搜索超参数和特征的空间计算开销过大。因此，你需要在K折交叉验证的折叠中执行一次，而不是完全的网格搜索：
- en: Tune the required hyperparameters using cross validation on the training set
    of the fold, using all the available features
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证对训练集的折叠进行超参数调优，使用所有可用的特征
- en: Select the required features using those hyperparameters
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这些超参数选择所需的特征
- en: Repeat the computation for each fold in K
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对K的每个折叠重复计算
- en: The final model is constructed on all the data using the N most prevalent features
    that were selected from each fold of CV
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终模型使用所有数据构建，使用从每个CV折叠中选择的N个最常见的特征
- en: The interesting thing is that the hyperparameters would also be tuned again
    using all the data in a cross-validation loop. Would there be a large downside
    from this method as compared to a full-grid search? In essence, I am doing a line
    search in each dimension of free parameters (finding the best value in one dimension,
    holding that constant, then finding the best in the next dimension), rather than
    every single combination of parameter settings. The most important downside for
    searching along single parameters instead of optimizing them all together, is
    that you ignore interactions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，超参数也会在交叉验证循环中使用所有数据再次调优。与完全网格搜索相比，这种方法会带来较大的缺点吗？本质上，我在每个自由参数的维度上进行线性搜索（先在一个维度上找到最佳值，保持常数，然后在下一个维度上找到最佳值），而不是每一种参数设置的所有组合。沿单个参数进行搜索，而不是一起优化所有参数的最大缺点，就是你忽略了它们之间的交互作用。
- en: It is quite common that, for instance, more than one parameter influences model
    complexity. In that case, you need to look at their interaction in order to successfully
    optimize the hyperparameters. Depending on how large your dataset is and how many
    models you compare, optimization strategies that return the maximum observed performance
    may run into trouble (this is true for both grid search and your strategy).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，多个参数通常会影响模型的复杂度。在这种情况下，你需要查看它们之间的交互作用，以便成功地优化超参数。根据数据集的大小以及比较的模型数量，返回最大观察性能的优化策略可能会遇到问题（无论是网格搜索还是你的策略，都存在此问题）。
- en: 'The reason is that searching through a large number of performance estimates
    for the maximum skims the variance of the performance estimate: you may just end
    up with a model and training/test split combination that accidentally happens
    to look good. Even worse, you may get several perfect-looking combinations, and
    the optimization then cannot know which model to choose and thus becomes unstable.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其原因是，搜索大量性能估计以找到最大值会削减性能估计的方差：你可能会最终得到一个看似不错的模型和训练/测试拆分组合。更糟的是，你可能会得到多个看似完美的组合，然后优化过程就无法判断选择哪个模型，从而变得不稳定。
- en: Cross-validation
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Cross-validation (also called the **rotation estimation** (**RE**)) is a model
    validation technique for assessing the quality of the statistical analysis and
    results. The target is to make the model generalize toward an independent test
    set. One of the perfect uses of the cross-validation technique is making a prediction
    from a machine learning model. It will help if you want to estimate how a predictive
    model will perform accurately in practice when you deploy it as an ML application.
    During the cross-validation process, a model is usually trained with a dataset
    of a known type. Conversely, it is tested using a dataset of unknown type.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证（也称为**旋转估计**（**RE**））是一种用于评估统计分析和结果质量的模型验证技术。其目标是使模型能够泛化到独立的测试集上。交叉验证技术的一个完美应用是从机器学习模型进行预测。它能帮助你估计当你将预测模型部署为机器学习应用时，模型在实际中的表现如何。交叉验证过程中，通常使用已知类型的数据集来训练模型，反之，则使用未知类型的数据集进行测试。
- en: 'In this regard, cross-validation helps to describe a dataset to test the model
    in the training phase using the validation set. There are two types of cross-validation
    that can be typed as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，交叉验证有助于描述数据集，以便在训练阶段使用验证集来测试模型。有两种交叉验证类型，如下所述：
- en: '**Exhaustive cross-validation**: This includes leave-p-out cross-validation
    and leave-one-out cross-validation.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**穷尽交叉验证**：包括留P交叉验证和留一交叉验证。'
- en: '**Non-exhaustive cross-validation**: This includes K-fold cross-validation
    and repeated random sub-sampling cross-validation.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非穷尽交叉验证**：包括K折交叉验证和重复随机子抽样交叉验证。'
- en: 'In most of the cases, the researcher/data scientist/data engineer uses 10-fold
    cross-validation instead of testing on a validation set. This is the most widely
    used cross-validation technique across the use cases and problem type as explained
    by the following figure:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，研究人员/数据科学家/数据工程师使用10折交叉验证，而不是在验证集上进行测试。这是跨用例和问题类型中最广泛使用的交叉验证技术，正如下图所示：
- en: '![](img/00372.gif)**Figure 6:** Cross-validation basically splits your complete
    available training data into a number of folds. This parameter can be specified.
    Then the whole pipeline is run once for every fold and one machine learning model
    is trained for each fold. Finally, the different machine learning models obtained
    are joined by a voting scheme for classifiers or by averaging for regression'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00372.gif)**图6：**交叉验证基本上将你的完整可用训练数据分成若干折。这个参数是可以指定的。然后，整个管道会对每一折运行一次，并且每一折都会训练一个机器学习模型。最后，通过投票方案对分类器进行联合，或者通过平均对回归进行联合。'
- en: 'Moreover, to reduce the variability, multiple iterations of cross-validation
    are performed using different partitions; finally, the validation results are
    averaged over the rounds. The following figure shows an example of hyperparameter
    tuning using the logistic regression:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了减少变异性，会使用不同的分区进行多次交叉验证迭代；最后，验证结果会在各轮中取平均值。下图展示了使用逻辑回归进行超参数调优的示例：
- en: '![](img/00046.jpeg)**Figure 7:** An example of hyperparameter tuning using
    the logistic regression'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00046.jpeg)**图7：**使用逻辑回归进行超参数调优的示例'
- en: 'Using cross-validation instead of conventional validation has two main advantages
    outlined as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉验证代替常规验证有两个主要优点，概述如下：
- en: Firstly, if there is not enough data available to partition across the separate
    training and test sets, there's the chance of losing significant modeling or testing
    capability.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，如果可用的数据不足以在单独的训练集和测试集之间进行划分，就可能会失去重要的建模或测试能力。
- en: Secondly, the K-fold cross-validation estimator has a lower variance than a
    single hold-out set estimator. This low variance limits the variability and is
    again very important if the amount of available data is limited.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，K折交叉验证估计器的方差低于单一的保留集估计器。较低的方差限制了变动，这在数据量有限时尤为重要。
- en: 'In these circumstances, a fair way to properly estimate the model prediction
    and related performance are to use cross-validation as a powerful general technique
    for model selection and validation. If we need to perform manual features and
    a parameter selection for the model tuning, after that, we can perform a model
    evaluation with a 10-fold cross-validation on the entire dataset. What would be
    the best strategy? We would suggest you go for the strategy that provides an optimistic
    score as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，合理的估算模型预测和相关性能的公平方式是使用交叉验证作为一种强大的模型选择和验证技术。如果我们需要手动选择特征和参数调优，那么在此之后，我们可以对整个数据集进行10折交叉验证来评估模型。什么策略最合适？我们建议你选择一种提供乐观评分的策略，具体如下：
- en: Divide the dataset into training, say 80%, and testing 20% or whatever you chose
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分为训练集，例如80%，和测试集20%或其他你选择的比例
- en: Use the K-fold cross-validation on the training set to tune your model
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K折交叉验证在训练集上调优模型
- en: Repeat the CV until you find your model optimized and therefore tuned.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复交叉验证，直到找到优化并调优的模型。
- en: Now, use your model to predict on the testing set to get an estimate of out
    of model errors.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用你的模型对测试集进行预测，以估算模型误差。
- en: Credit risk analysis – An example of hyperparameter tuning
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信用风险分析 – 超参数调优的示例
- en: In this section, we will show a practical example of machine learning hyperparameter
    tuning in terms of grid searching and cross-validation technique. More specifically,
    at first, we will develop a credit risk pipeline that is commonly used in financial
    institutions such as banks and credit unions. Later on, we will look at how to
    improve the prediction accuracy by hyperparameter tuning. Before diving into the
    example, let's take a quick overview of what credit risk analysis is and why it
    is important?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示机器学习超参数调优的实际示例，涉及网格搜索和交叉验证技术。更具体地说，首先，我们将开发一个信用风险管道，该管道在银行和信用合作社等金融机构中常见。随后，我们将探讨如何通过超参数调优提高预测准确性。在深入示例之前，让我们快速概述一下什么是信用风险分析，以及它为何重要？
- en: What is credit risk analysis? Why is it important?
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是信用风险分析？它为何重要？
- en: 'When an applicant applies for loans and a bank receives that application, based
    on the applicant''s profile, the bank has to make a decision whether to approve
    the loan application or not. In this regard, there are two types of risk associated
    with the bank''s decision on the loan application:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当申请人申请贷款并且银行收到该申请时，基于申请人的资料，银行需要决定是否批准该贷款申请。在这方面，银行在贷款申请决策时面临两种类型的风险：
- en: '**The applicant is a good credit risk**: That means the client or applicant
    is more likely to repay the loan. Then, if the loan is not approved, the bank
    can potentially suffer the loss of business.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**申请人属于信用风险较低**：这意味着客户或申请人更有可能偿还贷款。如果贷款未被批准，银行可能会因此失去业务。'
- en: '**The applicant is a bad credit risk**: That means that the client or applicant
    is most likely not to repay the loan. In that case, approving the loan to the
    client will result in financial loss to the bank.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**申请人属于信用风险较高**：这意味着客户或申请人最有可能无法偿还贷款。在这种情况下，批准贷款将导致银行遭受财务损失。'
- en: The institution says that the second one is riskier than that of the first one,
    as the bank has a higher chance of not getting reimbursed the borrowed amount.
    Therefore, most banks or credit unions evaluate the risks associated with lending
    money to a client, applicant, or customer. In business analytics, minimizing the
    risk tends to maximize the profit to the bank itself.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 该机构表示，第二种情况比第一种情况更具风险，因为银行更有可能无法收回借款。因此，大多数银行或信用合作社会评估向客户、申请人或顾客借款所涉及的风险。在商业分析中，最小化风险往往能最大化银行自身的利润。
- en: In other words, maximizing the profit and minimizing the loss from a financial
    perspective is important. Often, the bank makes a decision about approving a loan
    application based on different factors and parameters of an applicant, such as
    demographic and socio-economic conditions regarding their loan application.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，从财务角度来看，最大化利润并最小化损失非常重要。银行通常会根据申请人的不同因素和参数（例如有关贷款申请的各类人口和社会经济条件）来决定是否批准贷款申请。
- en: The dataset exploration
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集探索
- en: The German credit dataset was downloaded from the UCI Machine Learning Repository
    at [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/).
    Although a detailed description of the dataset is available in the link, we provide
    some brief insights here in **Table 3**. The data contains credit-related data
    on 21 variables and the classification of whether an applicant is considered a
    good or a bad credit risk for 1000 loan applicants (that is, binary classification
    problem).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 德国信用数据集从 UCI 机器学习库下载，网址为 [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)。虽然在该链接中可以找到数据集的详细描述，但我们在
    **表 3** 中提供了一些简要的见解。数据包含了关于 21 个变量的信用相关数据，以及对于 1000 名贷款申请人是否被认为是好的或坏的信用风险的分类（即二分类问题）。
- en: 'The following table shows details about each variable that was considered before
    making the dataset available online:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了在将数据集公开之前，所考虑的每个变量的详细信息：
- en: '| **Entry** | **Variable** | **Explanation** |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **条目** | **变量** | **说明** |'
- en: '| 1 | creditability | Capable of repaying: has value either 1.0 or 0.0 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 1 | creditability | 还款能力：值为 1.0 或 0.0 |'
- en: '| 2 | balance | Current balance |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 2 | balance | 当前余额 |'
- en: '| 3 | duration | Duration of the loan being applied for |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 3 | duration | 贷款申请期限 |'
- en: '| 4 | history | Is there any bad loan history? |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 4 | history | 是否有不良贷款历史？ |'
- en: '| 5 | purpose | Purpose of the loan |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 5 | purpose | 贷款目的 |'
- en: '| 6 | amount | Amount being applied for |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 6 | amount | 申请金额 |'
- en: '| 7 | savings | Monthly saving |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 7 | savings | 每月储蓄 |'
- en: '| 8 | employment | Employment status |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 8 | employment | 就业状态 |'
- en: '| 9 | instPercent | Interest percent |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 9 | instPercent | 利率百分比 |'
- en: '| 10 | sexMarried | Sex and marriage status |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 10 | sexMarried | 性别及婚姻状态 |'
- en: '| 11 | guarantors | Are there any guarantors? |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 11 | guarantors | 是否有担保人？ |'
- en: '| 12 | residenceDuration | Duration of residence at the current address |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 12 | residenceDuration | 当前地址的居住时长 |'
- en: '| 13 | assets | Net assets |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 13 | assets | 净资产 |'
- en: '| 14 | age | Age of the applicant |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 14 | age | 申请人年龄 |'
- en: '| 15 | concCredit | Concurrent credit |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 15 | concCredit | 并行信用 |'
- en: '| 16 | apartment | Residential status |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 16 | apartment | 住宅状态 |'
- en: '| 17 | credits | Current credits |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 17 | credits | 当前信用 |'
- en: '| 18 | occupation | Occupation |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 18 | occupation | 职业 |'
- en: '| 19 | dependents | Number of dependents |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 19 | dependents | 赡养人数 |'
- en: '| 20 | hasPhone | If the applicant uses a phone |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 20 | hasPhone | 申请人是否使用电话 |'
- en: '| 21 | foreign | If the applicant is a foreigner |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 21 | foreign | 申请人是否为外国人 |'
- en: Note that, although *Table 3* describes the variables with an associated header,
    there is no associated header in the dataset. In *Table 3*, we have shown the
    variable, position, and associated significance of each variable.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管 *表 3* 描述了与变量相关的标题，但数据集本身没有相关标题。在 *表 3* 中，我们展示了每个变量的名称、位置以及相关的重要性。
- en: Step-by-step example with Spark ML
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤示例：使用 Spark ML
- en: 'Here, we will provide a step-by-step example of credit risk prediction using
    the Random Forest classifier. The steps include from data ingestion, some statistical
    analysis, training set preparation, and finally model evaluation:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将提供一个使用随机森林分类器进行信用风险预测的逐步示例。步骤包括从数据导入、一些统计分析、训练集准备，到最终的模型评估：
- en: '**Step 1.** Load and parse the dataset into RDD:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1.** 加载并解析数据集到 RDD：'
- en: '[PRE0]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For the preceding line, the `parseRDD()` method is used to split the entry
    with `,` and then converted all of them as `Double` value (that is, numeric).
    This method goes as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上一行，`parseRDD()` 方法用于用 `,` 分割条目，然后将它们转换为 `Double` 类型（即数值）。该方法如下所示：
- en: '[PRE1]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'On the other hand, the `parseCredit()` method is used to parse the dataset
    based on the `Credit` case class:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`parseCredit()` 方法用于基于 `Credit` 案例类解析数据集：
- en: '[PRE2]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `Credit` case class goes as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`Credit` 案例类如下所示：'
- en: '[PRE3]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 2\. Prepare the DataFrame for the ML pipeline** - Get the DataFrame
    for the ML pipeline'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2.** 准备 ML 管道的数据框架 - 获取 ML 管道的数据框架'
- en: '[PRE4]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Save them as a temporary view for making the query easier:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 将它们保存为临时视图，以便于查询：
- en: '[PRE5]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s take a snap of the DataFrame:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这个数据框的快照：
- en: '[PRE6]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding `show()` method prints the credit DataFrame:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 上述 `show()` 方法打印了信贷数据框：
- en: '![](img/00124.gif)**Figure 8:** A snap of the credit dataset'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00124.gif)**图 8：** 信贷数据集的快照'
- en: '**Step 3\. Observing related statistics** - First, let''s see some aggregate
    values:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3. 观察相关统计信息** - 首先，让我们看看一些聚合值：'
- en: '[PRE7]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s see the statistics about the balance:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看余额的统计数据：
- en: '[PRE8]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let''s see the creditability per average balance:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看平均余额的信贷能力：
- en: '[PRE9]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output of the three lines:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 三行代码的输出：
- en: '![](img/00030.gif)**Figure 9:** Some statistics of the dataset'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00030.gif)**图 9：** 数据集的一些统计信息'
- en: '**Step 4\. Feature vectors and labels creation** - As you can see, the credibility
    column is the response column, and, for the result, we need to create the feature
    vector without considering this column. Now, let''s create the feature column
    as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4. 特征向量和标签的创建** - 如你所见，credibility 列是响应列，结果时，我们需要创建特征向量，而不考虑该列。现在，让我们按照以下方式创建特征列：'
- en: '[PRE10]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s assemble all the features of these selected columns using `VectorAssembler()`
    API as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `VectorAssembler()` API 来组合这些选定列的所有特征，如下所示：
- en: '[PRE11]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now let''s see what the feature vectors look like:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看特征向量是什么样子的：
- en: '[PRE12]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding line shows the features created by the VectorAssembler transformer:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 上述行显示了由 VectorAssembler 转换器创建的特征：
- en: '![](img/00360.gif)**Figure 10:** Generating features for ML models using VectorAssembler'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00360.gif)**图 10：** 使用 VectorAssembler 为 ML 模型生成特征'
- en: 'Now, let''s create a new column as a label from the old response column creditability
    using `StringIndexer` as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 `StringIndexer` 从旧的响应列“creditability”中创建一个新列作为标签，如下所示：
- en: '[PRE13]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding line shows the features and labels created by the `VectorAssembler`
    transformer:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 上述行显示了由 `VectorAssembler` 转换器创建的特征和标签：
- en: '![](img/00274.gif)**Figure 11:** Corresponding labels and feature for ML models
    using VectorAssembler'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00274.gif)**图 11：** 使用 VectorAssembler 的 ML 模型的对应标签和特征'
- en: '**Step 5.** Prepare the training and test set:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5.** 准备训练集和测试集：'
- en: '[PRE14]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Step 6\. Train the random forest model** - At first, instantiate the model:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 6. 训练随机森林模型** - 首先，实例化模型：'
- en: '[PRE15]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For an explanation of the preceding parameters, refer to the random forest
    algorithm section in this chapter. Now, let''s train the model using the training
    set:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 有关前面参数的解释，请参考本章中的随机森林算法部分。现在，让我们使用训练集训练模型：
- en: '[PRE16]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Step 7.** Compute the raw prediction for the test set:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 7.** 计算测试集的原始预测：'
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s see the top 20 rows of this DataFrame:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看这个数据框的前 20 行：
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding line shows the DataFrame containing the label, raw prediction,
    probablity, and actual prediciton:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 上述行显示了包含标签、原始预测、概率和实际预测的 DataFrame：
- en: '![](img/00040.gif)**Figure 12:** The DataFrame containing raw and actual prediction
    for test set'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00040.gif)**图 12：** 包含测试集原始和实际预测的 DataFrame'
- en: Now after seeing the prediction from the last column, a bank can make a decision
    about the applications for which the application should be accepted.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在看到最后一列的预测后，银行可以决定接受哪些申请。
- en: '**Step 8\. Model evaluation before tuning** - Instantiate the binary evaluator:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 8. 调整前的模型评估** - 实例化二元评估器：'
- en: '[PRE19]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Calculate the accuracy of the prediction for the test set as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 计算测试集预测准确性如下：
- en: '[PRE20]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The accuracy before pipeline fitting: `0.751921784149243`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 管道拟合之前的准确率：`0.751921784149243`
- en: 'This time, the accuracy is 75%, which is not that good. Let''s compute other
    important performance metrics for the binary classifier like **area under receiver
    operating characteristic** (**AUROC**) and **area under precision recall curve**
    (**AUPRC**):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，准确率为 75%，虽然不算很好，但让我们计算二元分类器的其他重要性能指标，如 **接收者操作特征曲线下的面积**（**AUROC**）和 **精度召回曲线下的面积**（**AUPRC**）：
- en: '[PRE21]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `printlnMetric()` method goes as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`printlnMetric()` 方法如下：'
- en: '[PRE22]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, let''s compute a few more performance metrics using the `RegressionMetrics
    ()` API for the random forest model we used during training:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用 `RegressionMetrics()` API 计算一些我们在训练过程中使用的随机森林模型的其他性能指标：
- en: '[PRE23]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, let''s see how our model is:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的模型如何：
- en: '[PRE24]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We get the following output:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE25]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Not that bad! However, not satisfactory either, right? Let's tune the model
    using grid search and cross-validation techniques.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 还不错！不过，也不算满意，对吧？让我们使用网格搜索和交叉验证技术来调整模型。
- en: '**Step 9\. Model tuning using grid search and cross-validation** - First, let''s
    use the `ParamGridBuilder` API to construct a grid of parameters to search over
    the param grid consisting of 20 to 70 trees with `maxBins` between 25 and 30,
    `maxDepth` between 5 and 10, and impurity as entropy and gini:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 9. 使用网格搜索和交叉验证进行模型调优** - 首先，让我们使用 `ParamGridBuilder` API 构建一个参数网格，在 20
    到 70 棵树之间搜索，`maxBins` 在 25 到 30 之间，`maxDepth` 在 5 到 10 之间，且不纯度使用熵和基尼系数：'
- en: '[PRE26]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s train the cross-validator model using the training set as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用训练集来训练交叉验证模型，如下所示：
- en: '[PRE27]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Compute the raw prediction for the test set as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 计算测试集的原始预测，如下所示：
- en: '[PRE28]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Step 10\. Evaluation of the model after tuning** - Let''s see the accuracy:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 10. 调优后的模型评估** - 让我们看一下准确率：'
- en: '[PRE29]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We get the following output:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE30]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, it''s above 83%. Good improvement indeed! Let''s see the two other metrics
    computing AUROC and AUPRC:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，准确率超过了 83%。确实有了很大的改进！让我们看看另外两个指标，计算 AUROC 和 AUPRC：
- en: '[PRE31]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We get the following output:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE32]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now based on the `RegressionMetrics` API, compute the other metrics:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据 `RegressionMetrics` API，计算其他指标：
- en: '[PRE33]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We get the following output:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE34]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Step 11\. Finding the best cross-validated model** - Finally, let''s find
    the best cross-validated model information:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 11. 查找最佳的交叉验证模型** - 最后，让我们找到最佳的交叉验证模型信息：'
- en: '[PRE35]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We get the following output:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE36]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: A recommendation system with Spark
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个基于 Spark 的推荐系统
- en: A recommender system tries to predict potential items a user might be interested
    in based on a history from other users. Model-based collaborative filtering is
    commonly used in many companies such as Netflix. It is to be noted that Netflix
    is an American entertainment company founded by Reed Hastings and Marc Randolph
    on August 29, 1997, in Scotts Valley, California. It specializes in and provides
    streaming media and video-on-demand online and DVD by mail. In 2013, Netflix expanded
    into film and television production, as well as online distribution. As of 2017,
    the company has its headquarters in Los Gatos, California (source Wikipedia).
    Netflix is a recommender system for a real-time movie recommendation. In this
    section, we will see a complete example of how it works toward recommending movies
    for new users.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统试图根据其他用户的历史预测用户可能感兴趣的潜在项目。基于模型的协同过滤在许多公司中被广泛使用，例如 Netflix。值得注意的是，Netflix
    是一家美国娱乐公司，由 Reed Hastings 和 Marc Randolph 于 1997 年 8 月 29 日在加利福尼亚州的 Scotts Valley
    创立。它专注于并提供在线流媒体和视频点播服务，以及通过邮寄方式提供 DVD。2013 年，Netflix 扩展到电影和电视制作以及在线分发。到 2017 年，该公司将总部设在加利福尼亚州的
    Los Gatos（来源：维基百科）。Netflix 是一个实时电影推荐的推荐系统。在本节中，我们将看到一个完整的例子，了解它如何为新用户推荐电影。
- en: Model-based recommendation with Spark
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于 Spark 的模型推荐
- en: 'The implementation in Spark MLlib supports model-based collaborative filtering.
    In the model based collaborative filtering technique, users and products are described
    by a small set of factors, also called the **latent factors** (**LFs**). From
    the following figure, you can get some idea of a different recommender system.
    *Figure 13* justifies why are going to use model-based collaborative filtering
    for the movie recommendation example:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib 中的实现支持基于模型的协同过滤。在基于模型的协同过滤技术中，用户和产品通过一小组因子来描述，这些因子也称为 **潜在因子** (**LFs**)。从下图中，你可以对不同的推荐系统有一些了解。*图
    13* 说明了为什么我们要使用基于模型的协同过滤进行电影推荐示例：
- en: '![](img/00284.gif)**Figure 13**: A comparative view of a different recommendation
    system'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00284.gif)**图 13**：不同推荐系统的对比视图'
- en: 'The LFs are then used for predicting the missing entries. Spark API provides
    the implementation of the alternating least squares (also known as the ALS widely)
    algorithm, which is used to learn these latent factors by considering six parameters,
    including:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用 LF 来预测缺失的条目。Spark API 提供了交替最小二乘法（也称为 ALS）算法的实现，该算法用于通过考虑六个参数来学习这些潜在因子，包括：
- en: '*numBlocks*: This is the number of blocks used to parallelize computation (set
    to -1 to auto-configure).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*numBlocks*：这是用于并行计算的块数（设置为 -1 以自动配置）。'
- en: '*rank*: This is the number of latent factors in the model.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*rank*：这是模型中潜在因子的数量。'
- en: '*iterations*: This is the number of iterations of ALS to run. ALS typically
    converges to a reasonable solution in 20 iterations or less.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*iterations*：这是 ALS 运行的迭代次数。ALS 通常在 20 次迭代或更少的次数内收敛到一个合理的解。'
- en: '*lambda*: This specifies the regularization parameter in ALS.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*lambda*：这指定了 ALS 中的正则化参数。'
- en: '*implicitPrefs*: This specifies whether to use the *explicit feedback* ALS
    variant or one adapted for *implicit feedback* data.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*implicitPrefs*：该参数指定是否使用 *显式反馈* ALS 变体，或者使用适应于 *隐式反馈* 数据的变体。'
- en: '*alpha*: This is a parameter applicable to the implicit feedback variant of
    ALS that governs the *baseline* confidence in preference observations.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*alpha*：这是一个适用于 ALS 隐式反馈变体的参数，控制在偏好观测中的 *基线* 信心。'
- en: 'Note that to construct an ALS instance with default parameters; you can set
    the value based on your requirements. The default values are as follows: `numBlocks:
    -1`, `rank: 10`, `iterations: 10`, `lambda: 0.01`, `implicitPrefs: false`, and
    `alpha: 1.0`.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，要构建具有默认参数的 ALS 实例；您可以根据需求设置该值。默认值如下：`numBlocks: -1`、`rank: 10`、`iterations:
    10`、`lambda: 0.01`、`implicitPrefs: false` 和 `alpha: 1.0`。'
- en: Data exploration
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: The movie and the corresponding rating dataset were downloaded from the MovieLens
    Website ([https://movielens.org](https://movielens.org)). According to the data
    description on the MovieLens Website, all the ratings are described in the `ratings.csv`
    file. Each row of this file followed by the header represents one rating for one
    movie by one user.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 电影及其相应的评分数据集是从 MovieLens 网站下载的 ([https://movielens.org](https://movielens.org))。根据
    MovieLens 网站上的数据描述，所有评分都记录在 `ratings.csv` 文件中。该文件中的每一行（紧跟在标题之后）代表一位用户对一部电影的评分。
- en: 'The CSV dataset has the following columns: **userId**, **movieId**, **rating**,
    and **timestamp**, as shown in *Figure 14*. The rows are ordered first by the
    **userId**, and within the user, by **movieId**. Ratings are made on a five-star
    scale, with half-star increments (0.5 stars up to 5.0 stars). The timestamps represent
    the seconds since midnight Coordinated Universal Time (UTC) on January 1, 1970,
    where we have 105,339 ratings from the 668 users on 10,325 movies:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 数据集包含以下列：**userId**、**movieId**、**rating** 和 **timestamp**，如 *图14* 所示。行按
    **userId** 排序，用户内再按 **movieId** 排序。评分是基于五颗星的评分标准，每次增量为半颗星（从 0.5 星到 5.0 星）。时间戳表示自
    1970 年 1 月 1 日午夜协调世界时（UTC）以来的秒数，我们有来自 668 个用户对 10,325 部电影的 105,339 条评分：
- en: '![](img/00244.gif)**Figure 14:** A snap of the ratings dataset'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00244.gif)**图14：** 评分数据集快照'
- en: 'On the other hand, the movie information is contained in the `movies.csv` file.
    Each row apart from the header information represents one movie containing the
    columns: movieId, title, and genres (see *Figure 14*). Movie titles are either
    created or inserted manually or imported from the website of the movie database
    at [https://www.themoviedb.org/](https://www.themoviedb.org/). The release year,
    however, is shown in the bracket. Since movie titles are inserted manually, some
    errors or inconsistencies may exist in these titles. Readers are, therefore, recommended
    to check the IMDb database ([https://www.ibdb.com/](https://www.ibdb.com/)) to
    make sure if there are no inconsistencies or incorrect titles with their corresponding
    release year.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，电影信息包含在 `movies.csv` 文件中。除了标题信息外，该文件的每一行代表一部电影，包含以下列：movieId、title 和 genres（见
    *图14*）。电影标题可以手动创建或插入，或者从电影数据库网站 [https://www.themoviedb.org/](https://www.themoviedb.org/)
    导入。然而，发行年份会显示在括号中。由于电影标题是手动插入的，因此可能存在一些错误或不一致。建议读者检查 IMDb 数据库 ([https://www.ibdb.com/](https://www.ibdb.com/))，确保没有与相应发行年份不一致或错误的标题。
- en: 'Genres are a separated list, and are selected from the following genre categories:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 类型是一个分隔列表，可以从以下类型类别中选择：
- en: Action, Adventure, Animation, Children's, Comedy, Crime
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作片、冒险片、动画片、儿童片、喜剧片、犯罪片
- en: Documentary, Drama, Fantasy, Film-Noir, Horror, Musical
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纪录片、剧情片、幻想片、黑色电影、恐怖片、音乐剧
- en: Mystery, Romance, Sci-Fi, Thriller, Western, War
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 悬疑片、浪漫片、科幻片、惊悚片、西部片、战争片
- en: '![](img/00356.gif)**Figure 15**: The title and genres for the top 20 movies'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00356.gif)**图15**：前 20 部电影的标题和类型'
- en: Movie recommendation using ALS
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ALS 进行电影推荐
- en: In this subsection, we will show you how to recommend the movie for other users
    through a step-by-step example from data collection to movie recommendation.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将通过从数据收集到电影推荐的逐步示例，展示如何为其他用户推荐电影。
- en: '**Step 1\. Load, parse and explore the movie and rating Dataset** - Here is
    the code illustrated:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1\. 加载、解析和探索电影和评分数据集** - 以下是代码示例：'
- en: '[PRE37]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This code segment should return you the DataFrame of the ratings. On the other
    hand, the following code segment shows you the DataFrame of movies:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码段应返回评分的 DataFrame。另一方面，以下代码段显示了电影的 DataFrame：
- en: '[PRE38]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '**Step 2\. Register both DataFrames as temp tables to make querying easier**
    - To register both Datasets, we can use the following code:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2\. 将两个DataFrame注册为临时表以便于查询** - 为了注册这两个数据集，我们可以使用以下代码：'
- en: '[PRE39]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This will help to make the in-memory querying faster by creating a temporary
    view as a table in min-memory. The lifetime of the temporary table using the `createOrReplaceTempView
    ()` method is tied to the `[[SparkSession]]` that was used to create this DataFrame.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这将通过在内存中创建一个临时视图作为表来加速内存查询。使用`createOrReplaceTempView()`方法创建的临时表的生命周期与用于创建该DataFrame的`[[SparkSession]]`相绑定。
- en: '**Step 3\. Explore and query for related statistics** - Let''s check the ratings-related
    statistics. Just use the following code lines:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3\. 探索和查询相关统计信息** - 让我们检查与评分相关的统计信息。只需要使用以下代码行：'
- en: '[PRE40]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: You should find 105,339 ratings from 668 users on 10,325 movies. Now, let's
    get the maximum and minimum ratings along with the count of users who have rated
    a movie. However, you need to perform a SQL query on the ratings table we just
    created in-memory in the previous step. Making a query here is simple, and it
    is similar to making a query from a MySQL database or RDBMS. However, if you are
    not familiar with SQL-based queries, you are recommended to look at the SQL query
    specification to find out how to perform a selection using `SELECT` from a particular
    table, how to perform the ordering using `ORDER`, and how to perform a joining
    operation using the `JOIN` keyword.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会发现，用户668对10,325部电影进行了105,339次评分。现在，让我们获取最大和最小评分，并统计评分电影的用户数。为此，你需要在前一步创建的内存中的评分表上执行SQL查询。这里进行查询很简单，类似于从MySQL数据库或关系型数据库管理系统（RDBMS）执行查询。如果你不熟悉基于SQL的查询，建议查阅SQL查询规范，了解如何使用`SELECT`从特定表中选择数据，如何使用`ORDER`进行排序，以及如何使用`JOIN`关键字进行联接操作。
- en: 'Well, if you know the SQL query, you should get a new dataset by using a complex
    SQL query as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，如果你知道SQL查询，你应该能通过以下复杂的SQL查询获得一个新的数据集：
- en: '[PRE41]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We get the following output:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出：
- en: '![](img/00073.gif)**Figure 16:** max, min ratings along with the count of users
    who have rated a movie'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00073.gif)**图 16：** 最大、最小评分及评分电影的用户数'
- en: 'To get an insight, we need to know more about the users and their ratings.
    Now, let''s find the top most active users and how many times they rated a movie:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更多的洞察，我们需要了解更多关于用户及其评分的信息。现在，让我们找出最活跃的用户以及他们评分电影的次数：
- en: '[PRE42]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/00262.jpeg)**Figure 17:** top 10 most active users and how many times
    they rated a movie'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00262.jpeg)**图 17：** 最活跃的10个用户以及他们评分电影的次数'
- en: 'Let''s take a look at a particular user, and find the movies that, say user
    668, rated higher than 4:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看某个特定用户，找到例如用户668评分高于4的电影：
- en: '[PRE43]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![](img/00035.gif)**Figure 18:** movies that user 668 rated higher than 4'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00035.gif)**图 18：** 用户668评分高于4的电影'
- en: '**Step 4\. Prepare training and test rating data and see the counts** - The
    following code splits ratings RDD into training data RDD (75%) and test data RDD
    (25%). Seed here is optional but required for the reproducibility purpose:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4\. 准备训练和测试评分数据并查看数量** - 以下代码将评分 RDD 划分为训练数据 RDD（75%）和测试数据 RDD（25%）。这里的种子值是可选的，但为了可重复性目的，推荐使用：'
- en: '[PRE44]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: You should find that there are 78,792 ratings in the training and 26,547 ratings
    in the test
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会发现训练集中有78,792条评分，测试集中有26,547条评分
- en: DataFrame.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame。
- en: '**Step 5\. Prepare the data for building the recommendation model using ALS**
    - The ALS algorithm takes the RDD of `Rating` for the training purpose. The following
    code illustrates for building the recommendation model using APIs:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5\. 为构建使用ALS的推荐模型准备数据** - ALS算法使用`Rating`类型的 RDD 作为训练数据。以下代码演示了如何使用API构建推荐模型：'
- en: '[PRE45]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The `ratingsRDD` is an RDD of ratings that contains the `userId`, `movieId`,
    and corresponding ratings from the training dataset that we prepared in the previous
    step. On the other hand, a test RDD is also required for evaluating the model.
    The following `testRDD` also contains the same information coming from the test
    DataFrame we prepared in the previous step:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`ratingsRDD` 是一个包含来自训练数据集的 `userId`、`movieId` 和相应评分的评分 RDD。另一方面，还需要一个测试 RDD
    来评估模型。以下 `testRDD` 也包含来自前一步准备的测试 DataFrame 的相同信息：'
- en: '[PRE46]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Step 6\. Build an ALS user product matrix** - Build an ALS user matrix model
    based on the `ratingsRDD` by specifying the maximal iteration, a number of blocks,
    alpha, rank, lambda, seed, and `implicitPrefs`. Essentially, this technique predicts
    missing ratings for specific users for specific movies based on ratings for those
    movies from other users who did similar ratings for other movies:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 6\. 构建 ALS 用户产品矩阵** - 基于`ratingsRDD`构建 ALS 用户矩阵模型，通过指定最大迭代次数、块数、alpha、rank、lambda、种子和`implicitPrefs`。本质上，这种技术通过其他用户对其他电影的评分来预测特定用户对特定电影的缺失评分：'
- en: '[PRE47]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Finally, we iterated the model for learning 15 times. With this setting, we
    got good prediction accuracy. Readers are suggested to apply the hyperparameter
    tuning to get to know the optimum values for these parameters. Furthermore, set
    the number of blocks for both user blocks and product blocks to parallelize the
    computation into a pass -1 for an auto-configured number of blocks. The value
    is -1.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将模型迭代学习了 15 次。通过这个设置，我们获得了良好的预测准确度。建议读者进行超参数调优，以便了解这些参数的最佳值。此外，将用户块和产品块的数量设置为
    -1，以自动配置块数并并行化计算。该值为 -1。
- en: '**Step 7\. Making predictions** - Let''s get the top six movie predictions
    for user 668\. The following source code can be used to make the predictions:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 7\. 进行预测** - 让我们为用户 668 获取前六个电影预测。可以使用以下源代码来进行预测：'
- en: '[PRE48]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The preceding code segment produces the following output containing the rating
    prediction with `UserID`, `MovieID`, and corresponding `Rating` for that movie:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码段产生了以下输出，包含了带有`UserID`、`MovieID`及相应`Rating`的电影评分预测：
- en: '![](img/00376.gif)**Figure 19**: top six movie predictions for user 668'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00376.gif)**图 19**：用户 668 的前六个电影预测'
- en: '**Step 8\. Evaluating the model** - In order to verify the quality of the models,
    **Root Mean Squared Error** (**RMSE**) is used to measure the differences between
    values predicted by a model and the values actually observed. By default, the
    smaller the calculated error, the better the model. In order to test the quality
    of the model, the test data is used (which was split above in step 4). According
    to many machine learning practitioners, the RMSE is a good measure of accuracy,
    but only to compare forecasting errors of different models for a particular variable
    and not between variables, as it is scale-dependent. The following line of code
    calculates the RMSE value for the model that was trained using the training set:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 8\. 评估模型** - 为了验证模型的质量，使用**均方根误差**（**RMSE**）来衡量模型预测值与实际观测值之间的差异。默认情况下，计算出的误差越小，模型越好。为了测试模型的质量，使用测试数据（如第
    4 步中所拆分的数据）。根据许多机器学习从业者的说法，RMSE 是衡量准确度的一个好方法，但仅限于比较同一变量不同模型的预测误差，而不能用于不同变量之间的比较，因为它是依赖于尺度的。以下代码行计算了使用训练集训练的模型的
    RMSE 值：'
- en: '[PRE49]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'It is to be noted that the `computeRmse()` is a UDF, that goes as follows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，`computeRmse()`是一个 UDF，具体如下：
- en: '[PRE50]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The preceding method computes the RMSE to evaluate the model. Less the RMSE,
    the better the model and its prediction capability.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法计算 RMSE 用以评估模型。RMSE 越小，模型及其预测能力越好。
- en: 'For the earlier setting, we got the following output:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 对于之前的设置，我们得到了以下输出：
- en: '[PRE51]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The performance of the preceding model could be increased further we believe.
    Interested readers should refer to this URL for more on tuning the ML-based ALS
    models [https://spark.apache.org/docs/preview/ml-collaborative-filtering.html](https://spark.apache.org/docs/preview/ml-collaborative-filtering.html).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，前述模型的性能还可以进一步提高。感兴趣的读者可以访问此网址，了解有关调优基于 ML 的 ALS 模型的更多信息：[https://spark.apache.org/docs/preview/ml-collaborative-filtering.html](https://spark.apache.org/docs/preview/ml-collaborative-filtering.html)。
- en: The topic modeling technique is widely used in the task of mining text from
    a large collection of documents. These topics can then be used to summarize and
    organize documents that include the topic terms and their relative weights. In
    the next section, we will show an example of topic modeling using the **latent
    dirichlet allocation** (**LDA**) algorithm.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模技术广泛应用于从大量文档中挖掘文本的任务。这些主题可以用来总结和组织包含主题词及其相对权重的文档。在下一部分，我们将展示使用**潜在狄利克雷分配**（**LDA**）算法进行主题建模的示例。
- en: Topic modelling - A best practice for text clustering
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模 - 文本聚类的最佳实践
- en: The topic modeling technique is widely used in the task of mining text from
    a large collection of documents. These topics can then be used to summarize and
    organize documents that include the topic terms and their relative weights. The
    dataset that will be used for this example is just in plain text, however, in
    an unstructured format. Now the challenging part is finding useful patterns about
    the data using LDA called topic modeling.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模技术广泛应用于从大量文档集合中挖掘文本的任务。这些主题随后可以用来总结和组织包含主题词及其相对权重的文档。这个示例所使用的数据集只是纯文本格式，然而，它是一个非结构化格式。现在，具有挑战性的部分是通过
    LDA 进行主题建模，从数据中找到有用的模式。
- en: How does LDA work?
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDA 是如何工作的？
- en: LDA is a topic model which infers topics from a collection of text documents.
    LDA can be thought of as a clustering algorithm where topics correspond to cluster
    centers, and documents correspond to examples (rows) in a dataset. Topics and
    documents both exist in a feature space, where feature vectors are vectors of
    word counts (bag of words). Instead of estimating a clustering using a traditional
    distance, LDA uses a function based on a statistical model of how text documents
    are generated.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 是一种从文本集合中推断主题的主题模型。LDA 可以看作是一种聚类算法，其中主题对应于聚类中心，文档对应于数据集中的实例（行）。主题和文档都存在于一个特征空间中，其中特征向量是词频向量（词袋模型）。与传统距离估计聚类不同，LDA
    使用基于文本文档生成统计模型的函数。
- en: 'LDA supports different inference algorithms via `setOptimizer` function. `EMLDAOptimizer`
    learns clustering using expectation-maximization on the likelihood function and
    yields comprehensive results, while `OnlineLDAOptimizer` uses iterative mini-batch
    sampling for online variational inference and is generally memory friendly. LDA
    takes in a collection of documents as vectors of word counts and the following
    parameters (set using the builder pattern):'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 通过 `setOptimizer` 函数支持不同的推断算法。`EMLDAOptimizer` 使用期望最大化对似然函数进行学习，并提供全面的结果，而
    `OnlineLDAOptimizer` 使用迭代的 mini-batch 采样进行在线变分推断，通常更节省内存。LDA 输入一组文档，作为词频向量，并使用以下参数（通过构建器模式设置）：
- en: '`k`: Number of topics (that is, cluster centers).'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k`：主题数量（即聚类中心）。'
- en: '`optimizer`: Optimizer to use for learning the LDA model, either `EMLDAOptimizer`
    or `OnlineLDAOptimizer`.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`：用于学习 LDA 模型的优化器，可以是`EMLDAOptimizer`或`OnlineLDAOptimizer`。'
- en: '`docConcentration`: Dirichlet parameter for prior over documents'' distributions
    over topics. Larger values encourage smoother inferred distributions.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docConcentration`：Dirichlet 参数，用于定义文档在主题分布上的先验。较大的值有助于生成更平滑的推断分布。'
- en: '`topicConcentration`: Dirichlet parameter for prior over topics'' distributions
    over terms (words). Larger values encourage smoother inferred distributions.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topicConcentration`：Dirichlet 参数，用于定义主题在词汇（单词）上的分布的先验。较大的值有助于生成更平滑的推断分布。'
- en: '`maxIterations`: Limit on the number of iterations.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxIterations`：迭代次数的限制。'
- en: '`checkpointInterval`: If using checkpointing (set in the Spark configuration),
    this parameter specifies the frequency with which checkpoints will be created.
    If `maxIterations` is large, using checkpointing can help reduce shuffle file
    sizes on disk and help with failure recovery.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`checkpointInterval`：如果使用检查点（在 Spark 配置中设置），该参数指定创建检查点的频率。如果`maxIterations`值较大，使用检查点可以帮助减少磁盘上
    shuffle 文件的大小，并有助于故障恢复。'
- en: Particularly, we would like to discuss the topics people talk about most from
    the large collection of texts. Since the release of Spark 1.3, MLlib supports
    the LDA, which is one of the most successfully used topic modeling techniques
    in the area of text mining and **Natural Language Processing** (**NLP**). Moreover,
    LDA is also the first MLlib algorithm to adopt Spark GraphX.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们希望讨论从大量文本集合中，人们最常谈论的主题。自 Spark 1.3 版本发布以来，MLlib 支持 LDA，这是一种在文本挖掘和**自然语言处理**（**NLP**）领域广泛使用的主题建模技术。此外，LDA
    也是第一个采用 Spark GraphX 的 MLlib 算法。
- en: To get more information about how the theory behind the LDA works, please refer
    to David M. Blei, Andrew Y. Ng and Michael I. Jordan, Latent, Dirichlet Allocation,
    *Journal of Machine Learning Research 3* (2003) 993-1022.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关 LDA 背后理论的更多信息，请参阅 David M. Blei、Andrew Y. Ng 和 Michael I. Jordan 的《潜在狄利克雷分配》，*机器学习研究期刊
    3*（2003）993-1022。
- en: 'The following figure shows the topic distribution from randomly generated tweet
    text:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了从随机生成的推文文本中得到的主题分布：
- en: '![](img/00165.jpeg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00165.jpeg)'
- en: '**Figure 20**: The topic distribution and how it looks like'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 20**：主题分布及其外观'
- en: In this section, we will look at an example of topic modeling using the LDA
    algorithm of Spark MLlib with unstructured raw tweets datasets. Note that here
    we have used LDA, which is one of the most popular topic modeling algorithms commonly
    used for text mining. We could use more robust topic modeling algorithms such
    as **Probabilistic Latent Sentiment Analysis** (**pLSA**), **pachinko allocation
    model** (**PAM**), or **hierarchical dirichlet process** (**HDP**) algorithms.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过使用Spark MLlib的LDA算法处理非结构化的原始推文数据集来演示主题建模的一个示例。请注意，这里我们使用的是LDA，这是最常用的文本挖掘主题建模算法之一。我们还可以使用更强大的主题建模算法，如**概率潜在情感分析**（**pLSA**）、**八股分配模型**（**PAM**）或**层次狄利克雷过程**（**HDP**）算法。
- en: However, pLSA has the overfitting problem. On the other hand, both HDP and PAM
    are more complex topic modeling algorithms used for complex text mining such as
    mining topics from high dimensional text data or documents of unstructured text.
    Moreover, to this date, Spark has implemented only one topic modeling algorithm,
    that is LDA. Therefore, we have to use LDA reasonably.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，pLSA存在过拟合问题。另一方面，HDP和PAM是更复杂的主题建模算法，通常用于复杂的文本挖掘任务，例如从高维文本数据或非结构化文本文档中挖掘主题。此外，至今为止，Spark只实现了一种主题建模算法，即LDA。因此，我们必须合理使用LDA。
- en: Topic modeling with Spark MLlib
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark MLlib进行主题建模
- en: 'In this subsection, we represent a semi-automated technique of topic modeling
    using Spark. Using other options as defaults, we train LDA on the dataset downloaded
    from the GitHub URL at [https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test](https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test).
    The following steps show the topic modeling from data reading to printing the
    topics, along with their term-weights. Here''s the short workflow of the topic
    modeling pipeline:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们展示了一种使用Spark进行半自动化主题建模的技术。使用默认的其他选项，我们在从GitHub URL下载的数据集上训练LDA：[https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test](https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test)。以下步骤展示了从数据读取到打印主题的主题建模过程，同时显示每个主题的术语权重。下面是主题建模管道的简短工作流程：
- en: '[PRE52]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The actual computation on topic modeling is done in the `LDAforTM` class. The
    `Params` is a case class, which is used for loading the parameters to train the
    LDA model. Finally, we train the LDA model using the parameters setting via the
    `Params` class. Now, we will explain each step broadly with step-by-step source
    code:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模的实际计算是在`LDAforTM`类中完成的。`Params`是一个案例类，用于加载训练LDA模型的参数。最后，我们通过`Params`类设置的参数来训练LDA模型。现在，我们将逐步解释每个步骤及其源代码：
- en: '**Step 1\. Creating a Spark session** - Let''s create a Spark session by defining
    number of computing core, SQL warehouse, and application name as follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1\. 创建Spark会话** - 让我们通过定义计算核心数、SQL仓库和应用程序名称来创建Spark会话，如下所示：'
- en: '[PRE53]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**Step 2\. Creating vocabulary, tokens count to train LDA after text pre-processing**
    - At first, load the documents, and prepare them for LDA as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2\. 创建词汇表，令牌计数以便在文本预处理后训练LDA** - 首先，加载文档，并为LDA做准备，如下所示：'
- en: '[PRE54]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The pre-process method is used to process the raw texts. At first, let''s read
    the whole texts using the `wholeTextFiles()` method as follows:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理方法用于处理原始文本。首先，使用`wholeTextFiles()`方法读取整个文本，如下所示：
- en: '[PRE55]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'In the preceding code, paths are the path of the text files. Then, we need
    to prepare a morphological RDD from the raw text based on the lemma texts as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，paths是文本文件的路径。然后，我们需要根据词干文本从原始文本准备一个形态学RDD，如下所示：
- en: '[PRE56]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Here the `getLemmaText()` method from the `helperForLDA` class supplies the
    lemma texts after filtering the special characters such as ``("""[! @ # $ % ^
    & * ( ) _ + - − , " '' ; : . ` ? --]`` as regular expressions using the `filterSpaecialChatacters()`
    method.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '这里，`helperForLDA`类中的`getLemmaText()`方法提供了在过滤掉特殊字符（如``("""[! @ # $ % ^ & * (
    ) _ + - − , " '' ; : . ` ? --]``）之后的词干文本，使用`filterSpaecialChatacters()`方法作为正则表达式进行过滤。'
- en: 'It is to be noted that the `Morphology()` class computes the base form of English
    words, by removing just inflections (not derivational morphology). That is, it
    only does noun plurals, pronoun case, and verb endings, and not things like comparative
    adjectives or derived nominals. This comes from the Stanford NLP group. To use
    this, you should have the following import in the main class file: `edu.stanford.nlp.process.Morphology`.
    In the `pom.xml` file, you will have to include the following entries as dependencies:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，`Morphology()`类计算英语单词的基础形式，方法是去除词尾变化（而非派生形态）。也就是说，它仅处理名词的复数形式、代词的格、动词的时态和数等，而不涉及比较级形容词或派生名词等内容。这一方法来自斯坦福NLP小组。要使用它，你需要在主类文件中添加以下导入：`edu.stanford.nlp.process.Morphology`。在`pom.xml`文件中，你需要将以下条目作为依赖项包含：
- en: '[PRE57]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The method goes as follows:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 方法实现如下：
- en: '[PRE58]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The `filterSpecialCharacters()` goes as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`filterSpecialCharacters()`的实现如下：'
- en: '``def filterSpecialCharacters(document: String) = document.replaceAll("""[!
    @ # $ % ^ & * ( ) _ + - − , " '' ; : . ` ? --]""", " ")``. Once we have the RDD
    with special characters removed in hand, we can create a DataFrame for building
    the text analytics pipeline:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '``def filterSpecialCharacters(document: String) = document.replaceAll("""[!
    @ # $ % ^ & * ( ) _ + - − , " '' ; : . ` ? --]""", " ")``。一旦我们手头有了去除特殊字符的RDD，我们就可以创建一个DataFrame，用于构建文本分析管道：'
- en: '[PRE59]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'So, the DataFrame consist of only of the documents tag. A snapshot of the DataFrame
    is as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DataFrame仅包含文档标签。DataFrame的快照如下：
- en: '![](img/00332.gif)**Figure 21**: Raw texts'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00332.gif)**图 21**：原始文本'
- en: 'Now if you examine the preceding DataFrame carefully, you will see that we
    still need to tokenize the items. Moreover, there are stop words in a DataFrame
    such as this, so we need to remove them as well. At first, let''s tokenize them
    using the `RegexTokenizer` API as follows:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细检查前面的DataFrame，你会发现我们仍然需要对项目进行分词。此外，在像这样的DataFrame中存在停用词，因此我们也需要将它们移除。首先，让我们使用`RegexTokenizer`
    API按以下方式对其进行分词：
- en: '[PRE60]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now, let''s remove all the stop words as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们按以下方式移除所有停用词：
- en: '[PRE61]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Furthermore, we also need to apply count victories to find only the important
    features from the tokens. This will help make the pipeline chained at the pipeline
    stage. Let''s do it as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要应用计数胜利来从词元中仅提取重要的特征。这将有助于在管道阶段将管道链式连接。我们按以下方式操作：
- en: '[PRE62]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now, create the pipeline by chaining the transformers (`tokenizer`, `stopWordsRemover`,
    and `countVectorizer`) as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过以下方式链式连接变换器（`tokenizer`、`stopWordsRemover`和`countVectorizer`）创建管道：
- en: '[PRE63]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Let''s fit and transform the pipeline towards the vocabulary and number of
    tokens:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拟合并转换管道，以适应词汇表和词元数量：
- en: '[PRE64]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, return the vocabulary and token count pairs as follows:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，按以下方式返回词汇表和词元计数对：
- en: '[PRE65]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now, let''s see the statistics of the training data:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看训练数据的统计信息：
- en: '[PRE66]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We get the following output:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE67]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '****Step 4\. Instantiate the LDA model before training****'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '****步骤 4\. 在训练之前实例化LDA模型****'
- en: '[PRE68]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '**Step 5: Set the NLP optimizer**'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5: 设置NLP优化器**'
- en: 'For better and optimized results from the LDA model, we need to set the optimizer
    for the LDA model. Here we use the `EMLDAOPtimizer` optimizer. You can also use
    the `OnlineLDAOptimizer()` optimizer. However, you need to add (1.0/actualCorpusSize)
    to `MiniBatchFraction` to be more robust on tiny datasets. The whole operation
    goes as follows. First, instantiate the `EMLDAOptimizer` as follows:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从LDA模型中获得更好且经过优化的结果，我们需要为LDA模型设置优化器。在这里，我们使用`EMLDAOPtimizer`优化器。你也可以使用`OnlineLDAOptimizer()`优化器。不过，你需要在`MiniBatchFraction`中加入(1.0/actualCorpusSize)，以便在小数据集上更加健壮。整个操作如下。首先，按以下方式实例化`EMLDAOptimizer`：
- en: '[PRE69]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now set the optimizer using the `setOptimizer()` method from the LDA API as
    follows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 现在通过以下方式使用LDA API中的`setOptimizer()`方法设置优化器：
- en: '[PRE70]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The `Params` case class is used to define the parameters to training the LDA
    model. This goes as follows:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '`Params`案例类用于定义训练LDA模型的参数。其结构如下：'
- en: '[PRE71]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'For a better result you can set these parameters in a naive way. Alternatively,
    you should go with the cross-validation for even better performance. Now if you
    want to checkpoint the current parameters, use the following line of codes:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好的结果，你可以以简单的方式设置这些参数。或者，你可以选择交叉验证以获得更好的性能。如果你想保存当前的参数，请使用以下代码行：
- en: '[PRE72]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '**Step 6.** Training the LDA model:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 6.** 训练LDA模型：'
- en: '[PRE73]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: For the texts we have, the LDA model took 6.309715286 sec to train. Note that
    these timing codes are optional. Here we provide them for reference purposes,
    only to get an idea of the training time.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们所拥有的文本，LDA模型训练时间为6.309715286秒。请注意，这些时间代码是可选的。我们提供这些代码仅供参考，以了解训练时间。
- en: '**Step 7\. Measuring the likelihood of the data** - Now, of to get some more
    statistics about the data such as maximum likelihood or log-likelihood, we can
    use the following code:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7. 测量数据的似然性** - 现在，为了获得更多关于数据的统计信息，如最大似然或对数似然，我们可以使用以下代码：'
- en: '[PRE74]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The preceding code calculates the average log likelihood if the LDA model is
    an instance of the distributed version of the LDA model. We get the following
    output:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码计算了LDA模型作为分布式版本的实例时的平均对数似然。我们得到了以下输出：
- en: '[PRE75]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The likelihood is used after data are available to describe a function of a
    parameter (or parameter vector) for a given outcome. This helps especially for
    estimating a parameter from a set of statistics. For more information on the likelihood
    measurement, interested readers should refer to [https://en.wikipedia.org/wiki/Likelihood_function](https://en.wikipedia.org/wiki/Likelihood_function).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 似然函数在数据可用后用于描述给定结果的参数（或参数向量）的函数。这在从一组统计数据估计参数时特别有用。有关似然度量的更多信息，感兴趣的读者可以参考[https://en.wikipedia.org/wiki/Likelihood_function](https://en.wikipedia.org/wiki/Likelihood_function)。
- en: '**Step 8\. Prepare the topics of interests** - Prepare the top five topics
    with each topic having 10 terms. Include the terms and their corresponding weights.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤8. 准备感兴趣的主题** - 准备前五个主题，每个主题包含10个词条。包括这些词条及其对应的权重。'
- en: '[PRE76]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '**Step 9\. Topic modeling** - Print the top ten topics, showing the top-weighted
    terms for each topic. Also, include the total weight in each topic as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤9. 主题建模** - 打印前十个主题，展示每个主题的权重最高的词条。同时，列出每个主题的总权重，如下所示：'
- en: '[PRE77]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Now, let''s see the output of our LDA model toward topics modeling:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看LDA模型在主题建模方面的输出：
- en: '[PRE78]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'From the preceding output, we can see that the topic of the input documents
    is topic 5 having the most weight of `0.31363611105890865`. This topic discusses
    the terms love, long, shore, shower, ring, bring, bear and so on. Now, for a better
    understanding of the flow, here''s the complete source code:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 从前述输出中，我们可以看到输入文档的主题为主题5，权重最大为`0.31363611105890865`。该主题讨论了诸如love、long、shore、shower、ring、bring、bear等词汇。现在，为了更好地理解流程，以下是完整的源代码：
- en: '[PRE79]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Scalability of LDA
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDA的可扩展性
- en: 'The previous example shows how to perform topic modeling using the LDA algorithm
    as a standalone application. The parallelization of LDA is not straightforward,
    and there have been many research papers proposing different strategies. The key
    obstacle in this regard is that all methods involve a large amount of communication.
    According to the blog on the Databricks website ([https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html](https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html)),
    here are the statistics of the dataset and related training and test sets that
    were used during the experimentation:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例展示了如何使用LDA算法作为独立应用程序进行主题建模。LDA的并行化并不简单，已有许多研究论文提出了不同的策略。关键的障碍在于所有方法都涉及大量的通信。根据Databricks网站上的博客([https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html](https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html))，以下是实验过程中使用的数据集及相关训练集和测试集的统计信息：
- en: 'Training set size: 4.6 million documents'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集大小：460万篇文档
- en: 'Vocabulary size: 1.1 million terms'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇量：110万个词条
- en: 'Training set size: 1.1 billion tokens (~239 words/document)'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集大小：110亿个标记（约239个词/文档）
- en: 100 topics
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100个主题
- en: 16-worker EC2 cluster, for example, M4.large or M3.medium depending upon budget
    and requirements
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 16工作节点EC2集群，例如，M4.large或M3.medium，具体取决于预算和需求
- en: For the preceding setting, the timing result was 176 secs/iteration on average
    over 10 iterations. From these statistics, it is clear that LDA is quite scalable
    for a very large number of the corpus as well.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前述设置，10次迭代的平均时间结果为176秒/迭代。从这些统计数据可以看出，LDA对于非常大规模的语料库也具有很好的可扩展性。
- en: Summary
  id: totrans-425
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we provided theoretical and practical aspects of some advanced
    topics of machine learning with Spark. We also provided some recommendations about
    the best practice in machine learning. Following that, we have seen how to tune
    machine learning models for better and optimized performance using grid search,
    cross-validation, and hyperparameter tuning. In the later section, we have seen
    how to develop a scalable recommendation system using the ALS, which is an example
    of a model-based recommendation system using a model-based collaborative filtering
    approach. Finally, we have seen how to develop a topic modeling application as
    a text clustering technique.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们提供了有关 Spark 中一些高级机器学习主题的理论和实践方面的内容。我们还提供了关于机器学习最佳实践的一些建议。接下来，我们展示了如何使用网格搜索、交叉验证和超参数调优来调优机器学习模型，以获得更好和更优化的性能。在后续部分，我们展示了如何使用
    ALS 开发一个可扩展的推荐系统，这是一个基于模型的协同过滤方法的模型推荐系统示例。最后，我们展示了如何开发一个文本聚类技术的主题建模应用。
- en: For additional aspects and topics on machine learning best practice, interested
    readers can refer to the book titled *Large Scale Machine Learning with Spark*
    at [https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark.](https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark)
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣了解更多机器学习最佳实践的读者，可以参考名为*《Spark 大规模机器学习》*的书籍，网址为[https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark](https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark)。
- en: In the next chapter, we will enter into more advanced use of Spark. Although
    we have discussed and provided a comparative analysis on binary and multiclass
    classification, we will get to know more about other multinomial classification
    algorithms with Spark such as Naive Bayes, decision trees, and the One-vs-Rest
    classifier.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将进入 Spark 的更高级应用。尽管我们已经讨论并提供了二分类和多分类的对比分析，但我们将进一步了解其他 Spark 中的多项式分类算法，如朴素贝叶斯、决策树和一对多分类器（One-vs-Rest）。
