- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Data Grouping, Aggregation, Filtering, and Applying Functions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分组、聚合、过滤和应用函数
- en: '**Data grouping** and **aggregation** are fundamental techniques in data cleaning
    and preprocessing, serving several critical purposes. Firstly, they enable the
    summarization of large datasets, transforming extensive raw data into concise,
    meaningful summaries that facilitate analysis and insight derivation. Additionally,
    aggregation helps manage missing or noisy data by smoothing out inconsistencies
    and filling gaps with combined data points. These techniques also contribute to
    reducing data volume, enhancing processing efficiency, and creating valuable features
    for further analysis or machine learning models.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据分组**和**聚合**是数据清理和预处理中的基础技术，具有多个关键用途。首先，它们能够对大规模数据集进行总结，将庞大的原始数据转化为简洁、有意义的汇总，方便分析和洞察的提取。此外，聚合有助于处理缺失或噪声数据，通过平滑不一致性并填补数据空白。这些技术还帮助减少数据量，提高处理效率，并为进一步的分析或机器学习模型创建有价值的特征。'
- en: The main components of data grouping and aggregation include group keys, which
    define how data is segmented; aggregation functions, which perform operations
    such as summing, averaging, counting, and more; and output columns, which display
    the group keys and aggregated values.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分组和聚合的主要组成部分包括分组键，它定义了数据的分段方式；聚合函数，它执行诸如求和、平均、计数等操作；以及输出列，它显示分组键和聚合后的值。
- en: 'In this chapter, we’ll cover the following main points:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Grouping data using one or multiple keys
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个或多个键进行数据分组
- en: Applying aggregate functions on grouped data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对分组数据应用聚合函数
- en: Applying functions on grouped data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对分组数据应用函数
- en: Data filtering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据过滤
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find the code for the chapter in the following GitHub repository: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter06](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter06).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下GitHub仓库中找到本章的代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter06](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter06)。
- en: Grouping data using one or multiple keys
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用一个或多个键进行数据分组
- en: 'In pandas, grouping data is a fundamental operation that involves splitting
    data into groups based on one or more keys and then performing operations within
    each group. Grouping is often used in data analysis to gain insights and perform
    aggregate calculations on subsets of data. Let’s dive deeper into grouping data
    and provide examples to illustrate their usage. The code for this section can
    be found here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/2.groupby_full_example.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/2.groupby_full_example.py).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas中，数据分组是一项基础操作，它涉及根据一个或多个键将数据拆分为多个组，然后在每个组内执行操作。分组常用于数据分析，以便对数据子集进行汇总计算并获得洞察。让我们更深入地探讨数据分组，并通过示例来说明它们的使用。本节的代码可以在这里找到：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/2.groupby_full_example.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/2.groupby_full_example.py)。
- en: Grouping data using one key
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用一个键进行数据分组
- en: Grouping data with pandas using one key is a common operation for data analysis.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个键进行数据分组是数据分析中的常见操作。
- en: 'To group data using one key, we use the `groupby()` method of a DataFrame and
    specify the column that we want to use as the key for grouping:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用一个键对数据进行分组，我们可以使用DataFrame的`groupby()`方法，并指定我们希望作为分组键的列：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After grouping, you typically want to perform some aggregation. Common aggregation
    functions include the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在分组之后，你通常需要执行一些聚合操作。常见的聚合函数包括：
- en: '`grouped.sum()`: This calculates the sum of all numeric columns'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grouped.sum()`：这会计算所有数值列的总和'
- en: '`grouped.mean()`: This calculates the average (arithmetic mean)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grouped.mean()`：这会计算平均值（算术平均）'
- en: '`grouped.count()`: This counts the number of non-null values'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grouped.count()`：这会统计非空值的数量'
- en: '`grouped.agg([''sum'', ''mean'', ''count''])`: This applies multiple aggregation
    functions at once: `sum`, `mean`, and `count`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grouped.agg([''sum'', ''mean'', ''count''])`：这会同时应用多个聚合函数：`sum`、`mean` 和 `count`'
- en: 'Let’s present a common use case on which to apply our learnings. Let’s pretend
    we are working for an electronics retail company and we need to analyze the sales
    data for the different products. A sample of the data is presented here:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示一个常见的应用案例，来应用我们的学习成果。假设我们为一家电子零售公司工作，需要分析不同产品的销售数据。以下是数据的一个样本：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In data analysis, certain columns are typically candidates for grouping due
    to their *categorical* nature. These columns often represent categories, classifications,
    or time-related segments that make sense to aggregate data around:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析中，某些列由于其*类别*性质，通常是进行分组的候选列。这些列通常表示类别、分类或时间相关的分段，适合围绕这些进行数据聚合：
- en: '**Category columns**: Columns that represent distinct groups or types within
    the data. Examples include product categories, user types, or service types. These
    columns help in understanding the performance or behavior of each group.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别列**：表示数据中不同组别或类型的列。例如，产品类别、用户类型或服务类型。这些列有助于理解每个组的表现或行为。'
- en: '**Geographical columns**: Columns that denote geographical divisions, such
    as country, region, city, or store location. These are useful for regional performance
    analysis.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**地理列**：表示地理区域的列，例如国家、地区、城市或商店位置。这些对于区域表现分析很有用。'
- en: '**Temporal columns**: Columns representing time-related information, such as
    year, quarter, month, week, or day. Grouping by these columns helps in trend analysis
    over time.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间列**：表示与时间相关的信息的列，例如年份、季度、月份、周或天。按这些列进行分组有助于进行趋势分析。'
- en: '**Demographic columns**: Columns that describe demographic attributes, such
    as age group, gender, or income level. These are useful for segmenting data based
    on population characteristics.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人口统计列**：描述人口统计属性的列，例如年龄段、性别或收入水平。这些列对于根据人口特征进行数据细分非常有用。'
- en: '**Transaction-related columns**: Columns related to the nature of transactions,
    such as transaction type, payment method, or order status. These help in understanding
    different aspects of transactional data.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交易相关列**：与交易性质相关的列，例如交易类型、支付方式或订单状态。这些列有助于理解交易数据的不同方面。'
- en: 'Given the data we have in our example, the candidate columns for grouping are
    `Category`, `Subcategory`, and `Region`. `Date` could also be a candidate if we
    had multiple records per day and we wanted to calculate the number of sales per
    day. In our case, our manager asked us to report on the total number of sales
    (sales volume) for each category. Let’s see how to calculate this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们示例中的数据，适合进行分组的列包括`类别`、`子类别`和`地区`。如果我们每天有多个记录，并且想计算每日销售量，那么`日期`也可以作为一个候选列。在我们的例子中，经理要求我们报告每个类别的总销售量（销售额）。让我们看看如何计算这个：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this code example, we group the data by the `Category` column, sum the `Sales`
    column for each category, and reset the index. The resulting DataFrame is as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中，我们按`类别`列对数据进行分组，对每个类别的`销售`列求和，并重置索引。结果的DataFrame如下所示：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have seen how to group data by a single key, let’s add more complexity
    by grouping the data by `Category` and `Region`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何按单个键进行分组，让我们通过按`类别`和`地区`分组来增加一些复杂性。
- en: Grouping data using multiple keys
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多个键对数据进行分组
- en: Grouping by multiple keys allows for a more granular and detailed examination
    of the data. This approach helps uncover insights that may be hidden when only
    using a single key, offering a deeper understanding of relationships and patterns
    within the dataset. In our example, grouping by both `Region` and `Category` allows
    the company to see not only the overall sales performance but also how different
    categories perform in each region. This helps in identifying which products are
    popular in specific regions and tailoring marketing strategies accordingly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 按多个键进行分组可以更细致、详细地检查数据。这种方法有助于发现仅使用单一键时可能隐藏的见解，从而更深入地理解数据集中的关系和模式。在我们的示例中，按`地区`和`类别`进行分组，不仅可以看到整体的销售表现，还能看到不同类别在每个地区的表现。这有助于识别哪些产品在特定地区受欢迎，从而根据地区特征调整营销策略。
- en: 'To group data using multiple keys, we pass a list of column names to the `groupby()`
    method. Pandas will create groups based on *unique combinations* of values from
    these columns:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用多个键对数据进行分组，我们将列名列表传递给`groupby()`方法。Pandas将根据这些列的*唯一组合*来创建组：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this piece of code, we group the data by both the `Category` and `Region`
    columns, and then we perform the aggregation by summing the `Sales` column for
    each group. Finally, we reset the index. Let’s see the output from this operation:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们按`Category`和`Region`列对数据进行分组，然后通过对每个组的`Sales`列求和来执行聚合。最后，我们重置索引。让我们看看这次操作的输出：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With just a line of code, we have managed to summarize and present all the sales
    for each `Category` and `Region` value, making our manager very happy. Now, let’s
    have a look at some best practices when working with groupby statements.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 只需一行代码，我们就能汇总并展示每个`Category`和`Region`值的所有销售数据，使我们的经理非常满意。现在，让我们看看在使用groupby语句时的一些最佳实践。
- en: Best practices for grouping
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分组的最佳实践
- en: 'When grouping data in pandas, there are several things to consider to ensure
    accurate results:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas中进行数据分组时，需要考虑几件事，以确保结果准确：
- en: '**Missing data**: Be aware of missing data in the columns used for grouping.
    Pandas will *exclude* rows with missing data from the grouped result, which can
    affect the final calculations.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失数据**：要注意用于分组的列中是否存在缺失数据。Pandas会*排除*包含缺失数据的行，这可能会影响最终的计算结果。'
- en: '`MultiIndex`: When grouping by multiple columns, pandas returns a hierarchical
    index (`MultiIndex`). Be familiar when working with `MultiIndex` and consider
    resetting the index if needed as we have been doing for simplicity.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MultiIndex`：当按多个列分组时，pandas会返回一个层次索引（`MultiIndex`）。在使用`MultiIndex`时要熟悉，并考虑在需要时重置索引，就像我们为了简化所做的那样。'
- en: '**Order of operations**: The order in which you perform groupings and aggregations
    *can affect the results*. Be mindful of the sequence in which you apply grouping
    and aggregation functions.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运算顺序**：执行分组和聚合的顺序*可能会影响结果*。请注意应用分组和聚合函数的顺序。'
- en: '**Grouping large datasets**: For large datasets, grouping can be memory intensive.
    Consider using techniques such as chunking or parallel processing to manage memory
    usage and computation time.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分组大数据集**：对于大型数据集，分组可能会占用大量内存。考虑使用分块处理或并行处理等技术来管理内存使用和计算时间。'
- en: Our managing team saw the efficiency of the groupby operations we performed,
    and they asked us for a more detailed summary of sales! With multiple keys in
    place, we can further enhance our analysis by applying multiple aggregation functions
    to the `Sales` column. This will give us a more detailed summary of the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的管理团队看到了我们执行的groupby操作的效率，他们要求我们提供更详细的销售总结！通过设置多个键，我们可以通过对`Sales`列应用多个聚合函数，进一步增强我们的分析。这将为我们提供更详细的数据总结。
- en: Applying aggregate functions on grouped data
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对分组数据应用聚合函数
- en: In pandas, after grouping data using the `groupby()` method, you can apply aggregate
    functions to perform calculations on the grouped data. **Aggregate functions**
    are used to summarize or compute statistics for each group, resulting in a new
    DataFrame or Series. Let’s dive deeper into applying aggregate functions on grouped
    data and provide examples to illustrate their usage.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas中，使用`groupby()`方法对数据进行分组后，可以应用聚合函数对分组数据执行计算。**聚合函数**用于总结或计算每个组的统计信息，结果是一个新的DataFrame或Series。让我们更深入地探讨如何在分组数据上应用聚合函数，并提供一些示例来说明其用法。
- en: Basic aggregate functions
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本聚合函数
- en: 'We have touched base on the basic aggregation function in the first section
    as you cannot perform groupby without an aggregation function. In this section,
    we will expand a bit more on what each function does and when should we use each
    one, starting by presenting all the available functions in the following table:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第一部分中已经介绍了基本的聚合函数，因为没有聚合函数就无法执行groupby。在本节中，我们将进一步探讨每个函数的作用，以及何时使用每个函数，首先展示以下表格中的所有可用函数：
- en: '| **Aggregation** **function** | **Description** | **When** **to use** | **Code
    example** |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **聚合** **函数** | **描述** | **使用时机** | **代码示例** |'
- en: '| `sum` | Adds up all values in a group | When you need the total value for
    each group.**Example**: Total sales per category. | `df.groupby(''Category'')[''Sales''].sum()`
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `sum` | 对组中的所有值求和 | 当你需要每个组的总值时。**示例**：按类别计算总销售额。 | `df.groupby(''Category'')[''Sales''].sum()`
    |'
- en: '| `mean` | Calculates the average of values in a group | When you need the
    average value for each group.**Example**: Average sales per region. | `df.groupby(''Category'')[''Sales''].mean()`
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `mean` | 计算组中值的平均数 | 当你需要每个组的平均值时。**示例**：按区域计算平均销售额。 | `df.groupby(''Category'')[''Sales''].mean()`
    |'
- en: '| `count` | Counts the number of non-null values in a group | When you need
    to know the number of occurrences in each group.**Example**: Number of sales transactions
    per sub-category. | `df.groupby(''Category'')[''Sales''].count()` |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `count` | 计算组中非空值的数量 | 当你需要知道每个组中出现次数时。**示例**：每个子类别的销售交易次数。 | `df.groupby(''Category'')[''Sales''].count()`
    |'
- en: '| `min` | Finds the minimum value in a group | When you need the smallest value
    in each group.**Example**: Minimum sales value per region. | `df.groupby(''Category'')[''Sales''].min()`
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `min` | 查找组中的最小值 | 当你需要每个组中的最小值时。**示例**：每个地区的最小销售值。 | `df.groupby(''Category'')[''Sales''].min()`
    |'
- en: '| **Aggregation** **function** | **Description** | **When** **to use** | **Code
    example** |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| **聚合** **函数** | **描述** | **何时** **使用** | **代码示例** |'
- en: '| `max` | Finds the maximum value in a group | When you need the largest value
    in each group.**Example**: Maximum sales value per category. | `df.groupby(''Category'')[''Sales''].max()`
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `max` | 查找组中的最大值 | 当你需要每个组中的最大值时。**示例**：每个类别的最大销售值。 | `df.groupby(''Category'')[''Sales''].max()`
    |'
- en: '| `median` | Finds the median value in a group | When you need the middle value
    in a sorted list of numbers. **Example**: Median sales value per category. | `df.groupby(''Category'')[''Sales''].median()`
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `median` | 查找组中的中位数值 | 当你需要一个排序数字列表中的中间值时。**示例**：每个类别的中位销售值。 | `df.groupby(''Category'')[''Sales''].median()`
    |'
- en: '| `std`(Standard Deviation) | Measures the spread of values in a group | When
    you need to understand the variation in values.**Example**: Standard deviation
    of sales per region. | `df.groupby(''Category'')[''Sales''].std()` |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `std`（标准差） | 衡量组中数值的分布 | 当你需要了解数值的变化时。**示例**：每个地区的销售标准差。 | `df.groupby(''Category'')[''Sales''].std()`
    |'
- en: Table 6.1 – Summary table of the basic aggregation functions
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1 – 基本聚合函数的汇总表
- en: 'You can call each of these functions one by one or all together, for example:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以逐个调用这些函数，也可以将它们一起调用，例如：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This calculates the number of sales per category, as we’ve learned, and it’s
    sufficient if this is the only aggregate information you want to extract from
    the dataset. However, if you find yourself being asked to produce multiple sale
    aggregates for the different product categories, a more efficient way is to perform
    all the aggregates at once:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这计算了每个类别的销售数量，正如我们所学的，如果这是你从数据集中提取的唯一聚合信息，那么这已经足够了。然而，如果你被要求为不同的产品类别生成多个销售聚合，一个更高效的方法是一次性执行所有的聚合：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this code, we apply multiple aggregation functions (`sum` and `mean`) to
    the `Sales` column. The result is as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们对 `Sales` 列应用了多个聚合函数（`sum` 和 `mean`）。结果如下：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We can add as many aggregations as we want in the group by clause.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在分组子句中添加任意数量的聚合。
- en: We’ve been really efficient with calculating all the different metrics our managing
    team asked for, and as a result, they are now keen to understand the sales metrics
    and the number of unique sub-category sales per region and category. Let’s do
    that next.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在计算管理团队要求的各种指标时非常高效，结果是他们现在热衷于理解每个地区和类别的销售指标以及唯一子类别的销售数量。接下来我们来做这个。
- en: Advanced aggregation with multiple columns
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多个列的高级聚合
- en: 'To understand the sales metrics and the number of unique sub-category sales
    per region and category, we can group additional columns and apply multiple aggregations
    to both the `Sales` and `Subcategory` columns:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解每个地区和类别的销售指标以及每个子类别的唯一销售数量，我们可以对额外的列进行分组，并对 `Sales` 和 `Subcategory` 列应用多个聚合：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this code, we group the DataFrame by `Category` and `Region` and we perform
    a couple of aggregations:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们通过 `Category` 和 `Region` 对 DataFrame 进行分组，并执行了几个聚合操作：
- en: '`''Sales'': [''sum'', ''mean'', ''count'']` calculates the total sales, average
    sales, and number of transactions (count of rows) for each group'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''Sales'': [''sum'', ''mean'', ''count'']` 计算每个组的总销售额、平均销售额和交易次数（行数）。'
- en: '`''Sub-Category'': ''nunique''` calculates the number of unique sub-categories
    within each group of `Category` and `Region`'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''Sub-Category'': ''nunique''` 计算每个 `Category` 和 `Region` 组内唯一子类别的数量。'
- en: 'The summarized results are presented here:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的是汇总结果：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, you may be wondering, what have we learned by making these calculations?
    Let me answer that! Total sales, average sales, and transaction count were calculated
    to understand the financial performance across different category-region combinations.
    Additionally, the `Sub-Category` unique count revealed crucial aspects of our
    product distribution strategy. This analysis serves multiple purposes: it provides
    insights into the diversity of products within each category-region segment, for
    example, in the context of our data, knowing the number of unique products (sub-categories)
    sold in each region under different categories provides insights into the market
    segmentation and product assortment strategies. It also aids in assessing market
    penetration by highlighting regions with a broader product offering and supports
    strategic decisions in product portfolio management, including expansion and inventory
    strategies tailored to regional preferences.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能会想，我们通过这些计算学到了什么？让我来回答这个问题！我们计算了总销售额、平均销售额和交易次数，以了解不同类别-地区组合的财务表现。此外，`Sub-Category`
    的唯一计数揭示了我们产品分销策略的关键方面。此分析有多个目的：它为每个类别-地区细分内产品的多样性提供了洞察。例如，在我们的数据背景下，了解在不同类别下，每个地区销售的独特产品（子类别）数量，有助于了解市场细分和产品组合策略。它还帮助评估市场渗透率，通过突出显示提供更多产品的地区，支持产品组合管理的战略决策，包括扩展和针对区域偏好的库存策略。
- en: Standard aggregation functions, such as sum, mean, and count, provide fundamental
    statistics. However, custom functions allow you to calculate metrics that are
    specific to your business needs or analysis goals. For example, calculating the
    range or coefficient of variation in sales data can reveal insights into the distribution
    and variability of sales within different groups. As you can imagine, we were
    asked to implement these custom metrics, which we’ll do next.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的聚合函数，如求和、平均值和计数，提供了基本统计信息。然而，自定义函数使你能够计算那些特定于你业务需求或分析目标的指标。例如，计算销售数据的范围或变异系数，可以揭示不同组内销售的分布和变异性。如你所见，我们被要求实现这些自定义指标，接下来我们将进行此操作。
- en: Applying custom aggregate functions
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用自定义聚合函数
- en: Custom functions are valuable when the aggregation requires complex calculations
    that go beyond simple statistics. You can use them when you need to calculate
    metrics that are unique to your analysis objectives or business context. For example,
    in sales analysis, you might want to compute profit margins, customer lifetime
    value, or churn rates, which are not typically available through standard aggregation
    functions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当聚合需要复杂的计算，超出简单统计时，自定义函数非常有价值。你可以在需要计算那些独特于你分析目标或业务背景的指标时使用它们。例如，在销售分析中，你可能希望计算利润率、客户生命周期价值或流失率，这些通常不是通过标准聚合函数能够获得的。
- en: 'Let’s go back to our example and build the metrics we were asked about: For
    each region, we want to calculate the sales range and the variability of sales.
    Let’s have a look at the following code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到示例中，构建我们被要求计算的指标：对于每个地区，我们要计算销售范围和销售变异性。让我们看看下面的代码：
- en: 'We create a function that calculates the range (difference between max and
    min) of sales:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个计算销售范围（最大值与最小值的差）的函数：
- en: '[PRE11]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we create a function that computes the coefficient of variation of sales,
    which measures the relative variability in relation to the mean:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个计算销售变异系数的函数，它衡量相对于均值的相对变异性：
- en: '[PRE12]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `df` DataFrame is then grouped by `Region`:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df` 数据框随后按 `Region` 分组：'
- en: '[PRE13]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`Sales: [''sum'', ''mean'', ''count'', range_sales, coefficient_of_variation]`
    calculates the total sales, average sales, transaction count, sales range, and
    coefficient of variation using the custom functions. `''Sub-Category'':''nunique''`
    counts the number of unique sub-categories within each group. Then, we reset the
    index to flatten the `df` DataFrame and make it easier to work with.'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Sales: [''sum'', ''mean'', ''count'', range_sales, coefficient_of_variation]`
    使用自定义函数计算总销售额、平均销售额、交易次数、销售范围和变异系数。`''Sub-Category'':''nunique''` 计算每个组内独特子类别的数量。然后，我们重置索引以扁平化
    `df` 数据框，使其更易于处理。'
- en: 'Finally, we rename the aggregated columns for clarity and better readability
    of the output:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们重命名聚合后的列，以便输出更加清晰和易于阅读：
- en: '[PRE14]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s print the final DataFrame:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印最终的数据框：
- en: '[PRE15]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The final DataFrame is presented here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的数据框在这里呈现：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Let’s spend some time understanding the sales variability for each region. The
    range of sales within each region can reveal the **spread** or **difference**
    between the highest and lowest sales figures. For instance, a wide range may indicate
    significant variability in consumer demand or sales performance across different
    regions. The coefficient of variation helps to standardize the variability of
    sales relative to their average. A higher coefficient suggests greater relative
    variability, which may prompt further investigation into factors influencing sales
    fluctuations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间了解一下各个区域的销售波动性。每个区域内销售额的范围可以揭示**差异**或**区别**，即最高和最低销售额之间的差距。例如，较大的范围可能表明不同区域间消费者需求或销售表现的显著差异。变异系数有助于将销售波动性相对于其平均值进行标准化。较高的变异系数表明更大的相对波动性，这可能促使进一步调查影响销售波动的因素。
- en: Note
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: I hope it’s clear to you that you can build any function that you want as a
    custom aggregate function as long as it computes a *single* aggregation result
    from an input series of values. The function should also return a single scalar
    value, which is the result of the aggregation for that group.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能清楚地理解，只要一个函数能够从输入的值序列中计算出*单一*的聚合结果，你就可以将它作为自定义聚合函数来构建。该函数还应返回一个单一的标量值，这是该组聚合的结果。
- en: Now, let’s have a look at some best practices when working with aggregate functions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下在使用聚合函数时的一些最佳实践。
- en: Best practices for aggregate functions
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合函数的最佳实践
- en: 'When working with aggregate functions in pandas, there are several things to
    consider to ensure accurate results:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Pandas 中的聚合函数时，需要考虑一些事项，以确保结果的准确性：
- en: Write efficient custom functions that minimize computational overhead, especially
    when working with large datasets. Avoid unnecessary loops or operations that can
    slow down processing time.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写高效的自定义函数，尽量减少计算开销，特别是在处理大型数据集时。避免不必要的循环或操作，这些操作可能会减慢处理时间。
- en: Clearly document the logic and purpose of your custom aggregation functions.
    This helps in maintaining and sharing code within your team or organization, ensuring
    transparency and reproducibility of analysis.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清楚地记录自定义聚合函数的逻辑和目的。这有助于在团队或组织内部维护和共享代码，确保分析的透明性和可重复性。
- en: Validate the accuracy of custom aggregation functions by comparing results with
    known benchmarks or manual calculations. This step is crucial to ensure that your
    custom metrics are reliable and correctly implemented.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将结果与已知基准或手动计算进行比较，验证自定义聚合函数的准确性。此步骤对于确保自定义指标的可靠性和正确实现至关重要。
- en: In pandas, when using the `.agg()` method with `groupby`, the aggregation function
    you define should *ideally* return a single scalar value for each column it operates
    on within each group. However, there are scenarios where you might want to return
    multiple values or perform more complex operations. While the pandas `.agg()`
    method expects scalar values, you can achieve more complex aggregations by using
    custom functions that return tuples or lists. However, this requires careful handling
    and often isn’t straightforward within pandas’ native aggregation framework. For
    more complex scenarios where you need to return multiple values or perform intricate
    calculations, we can use `apply()` instead of `agg()`, which is more flexible,
    as we will see in the next section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pandas 中，使用 `.agg()` 方法与 `groupby` 时，你定义的聚合函数*理想情况下*应该为每个操作的列返回单一的标量值。然而，在某些情况下，你可能希望返回多个值或执行更复杂的操作。虽然
    Pandas 的 `.agg()` 方法期望返回标量值，但你可以通过使用返回元组或列表的自定义函数来实现更复杂的聚合。然而，这需要谨慎处理，并且在 Pandas
    的原生聚合框架中通常并不简单。对于需要返回多个值或执行复杂计算的更复杂场景，我们可以使用 `apply()` 替代 `agg()`，它更灵活，正如我们将在下一节中看到的。
- en: Using the apply function on grouped data
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在分组数据上使用 apply 函数
- en: 'The `apply()` function in Pandas is a powerful method used to apply a custom
    function along an axis of a DataFrame or Series. It is highly versatile and can
    be used in various scenarios to manipulate data, compute complex aggregations,
    or transform data based on custom logic. The `apply()` function can be used to
    do the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 中的 `apply()` 函数是一个强大的方法，用于沿着 DataFrame 或 Series 的轴应用自定义函数。它非常灵活，可以用于各种场景，以根据自定义逻辑操作数据、计算复杂的聚合或转换数据。`apply()`
    函数可以用于以下操作：
- en: Apply functions row-wise or column-wise
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按行或按列应用函数
- en: Apply functions to groups of data when used in conjunction with `groupby()`
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当与 `groupby()` 配合使用时，将函数应用于数据组
- en: In the next section, we will focus on using the `apply` function on groups of
    data by first grouping on the column we want and then performing the `apply` operation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将重点讨论如何在数据分组后使用`apply`函数，首先按我们想要的列进行分组，然后执行`apply`操作。
- en: Note
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Using the `apply` function without `groupby` allows you to apply a function
    across either rows or columns of a DataFrame directly. This is useful when you
    need to perform row-wise or column-wise operations that don’t require grouping
    the data. Apply the same learnings and just skip the group by clause.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不带`groupby`的`apply`函数，可以直接对DataFrame的行或列应用函数。这在你需要执行不需要分组数据的行或列级别的操作时非常有用。应用相同的学习，只需跳过`groupby`子句。
- en: When using the `apply` function in pandas, `axis=0` (default) applies the function
    to each column, while `axis=1` applies it to each row. Let’s go a little bit deeper
    on this.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用pandas的`apply`函数时，`axis=0`（默认）将函数应用于每一列，而`axis=1`则将其应用于每一行。我们来深入了解一下这一点。
- en: '`axis=0` applies the function along the *rows*. In other words, it processes
    each *column* independently. This is typically used when you want to aggregate
    data column-wise (e.g., summing up values in each column), as shown in the following
    figure:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`axis=0`将函数应用于*行*。换句话说，它独立处理每一*列*。当你想按列汇总数据（例如，对每列的值求和）时，通常会使用此方法，如下图所示：'
- en: '![Figure 6.1 – Apply() with axis=0](img/B19801_06_1.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – Apply()与axis=0](img/B19801_06_1.jpg)'
- en: Figure 6.1 – Apply() with axis=0
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – Apply()与axis=0
- en: 'If we go back to our use case, the managing team wants to understand more about
    the actual quantity sold for the products per category apart from the sum of dollars
    in sales we achieved. Our example gets more and more complex; so, it’s a good
    idea to implement that with `apply()`. Let’s see this with a code example that
    can also be found here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/3.apply_axis0.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/3.apply_axis0.py).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回到我们的用例，管理团队希望了解每个类别中产品的实际销售数量，而不仅仅是销售总额。我们的示例变得越来越复杂，因此，用`apply()`实现这个功能是个好主意。我们来看一个代码示例，代码也可以在这里找到：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/3.apply_axis0.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/3.apply_axis0.py)。
- en: 'Let’s extend our DataFrame to add the `Quantity` column:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们扩展我们的DataFrame，添加`Quantity`列：
- en: '[PRE17]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We then convert the `Date` column to datetime format:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将`Date`列转换为日期时间格式：
- en: '[PRE18]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, let’s define a custom function to compute multiple statistics for `Sales`
    and `Quantity`:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个自定义函数来计算`Sales`和`Quantity`的多个统计量：
- en: '[PRE19]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The custom function (`compute_statistics`) now computes multiple statistics
    (sum, mean, std, CV) for both the `Sales` and `Quantity` columns within each group
    defined by `Category`. For each category group (series), it calculates the following:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个自定义函数（`compute_statistics`）现在计算了在每个由`Category`定义的组内，`Sales`和`Quantity`列的多个统计量（总和、均值、标准差、变异系数）。对于每个类别组（系列），它计算以下内容：
- en: '`Sum_Sales`: The sum of sales'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Sum_Sales`：销售总和'
- en: '`Mean_Sales`: The mean of sales'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Mean_Sales`：销售的均值'
- en: '`Std_Sales`: The standard deviation of sales'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Std_Sales`：销售的标准差'
- en: '`CV_Sales`: The `Sum_Quantity`: Sum of quantities'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CV_Sales`：`Sum_Quantity`：数量的总和'
- en: '`Mean_Quantity`: Mean of quantities'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Mean_Quantity`：数量的均值'
- en: '`Std_Quantity`: Standard deviation of quantities'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Std_Quantity`：数量的标准差'
- en: '`CV_Quantity`: The CV of quantities'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CV_Quantity`：数量的变异系数'
- en: In the end, it returns a pandas Series with these computed statistics, indexed
    appropriately.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终，它返回一个包含这些计算统计量的pandas Series，并适当地进行索引。
- en: 'Next, we’ll perform a groupby operation on `Category` and apply our custom
    function to compute statistics of `Sales` and `Quantity`:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在`Category`上执行`groupby`操作，并应用我们自定义的函数来计算`Sales`和`Quantity`的统计量：
- en: '[PRE20]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We use `apply()` in conjunction with `groupby(''Category'')` to apply the `compute_statistics`
    function to each group of sales data defined by the `Category` column. The function
    operates on the entire group (series), allowing the computation of statistics
    for both the `Sales` and `Quantity` columns simultaneously. Finally, `reset_index()`
    is used to flatten the resulting DataFrame, providing a structured output with
    category-wise statistics for both columns. Let’s have a look at the final DataFrame:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将`apply()`与`groupby('Category')`结合使用，将`compute_statistics`函数应用于由`Category`列定义的每组销售数据。该函数作用于整个组（系列），允许同时计算`Sales`和`Quantity`列的统计数据。最后，使用`reset_index()`将结果DataFrame展平，提供按类别划分的两个列的统计数据结构化输出。我们来看一下最终的DataFrame：
- en: '![](img/B19801_06_2.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19801_06_2.jpg)'
- en: By grouping the data by `Category`, we can analyze sales and quantity metrics
    at the category level, which helps us understand how different types of products
    (electronics, furniture, clothing) perform in terms of sales and quantity. As
    we can see from the presented results, `Furniture` is the key income generator
    as it has the highest `Sum_Sales` and `Mean_Sales`, indicating a category with
    popular or high-value products. Categories with lower `CV_Sales` and `CV_Quantity`
    values, such as `Clothing`, are more consistent in sales and quantity, suggesting
    stable demand or predictable sales patterns whereas categories with higher variability
    (`Std_Sales` and `Std_Quantity`) may indicate products with fluctuating sales
    or seasonal demand.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过按`Category`对数据进行分组，我们可以在类别层面分析销售和数量指标，这有助于我们理解不同类型的产品（电子产品、家具、服装）在销售和数量方面的表现。正如我们从呈现的结果中看到的，`Furniture`（家具）是主要的收入来源，因为它具有最高的`Sum_Sales`和`Mean_Sales`，这表明该类别包含受欢迎或高价值的产品。具有较低`CV_Sales`和`CV_Quantity`值的类别，如`Clothing`（服装），在销售和数量上更为稳定，表明需求稳定或销售模式可预测，而具有较高变动性的类别（`Std_Sales`和`Std_Quantity`）可能表示销售波动或季节性需求。
- en: 'This is great in terms of data analysis, but now, we get asked to come up with
    some strategic decisions related to product assortment, pricing strategies, and
    marketing initiatives. Let’s be a little bit more creative at this point:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这在数据分析方面非常有用，但现在，我们需要做出一些与产品组合、定价策略和市场营销措施相关的战略决策。在这一点上，让我们更加富有创意：
- en: Categories with high `Sum_Sales` values and stable metrics (`CV_Sales`, `CV_Quantity`)
    are prime candidates for expanding product lines or investing in marketing efforts
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高`Sum_Sales`值且指标稳定（`CV_Sales`，`CV_Quantity`）的类别是扩展产品线或投资市场营销的最佳候选者
- en: Categories with high variability (`Std_Sales`, `Std_Quantity`) may require dynamic
    pricing strategies or seasonal promotions to optimize sales
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高变动性的类别（`Std_Sales`，`Std_Quantity`）可能需要动态定价策略或季节性促销来优化销售
- en: We can use the `Mean_Sales` and `Mean_Quantity` values to identify categories
    with potential for growth
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用`Mean_Sales`和`Mean_Quantity`的值来识别具有增长潜力的类别
- en: When using the `apply()` function in pandas without specifying the axis parameter,
    the default behavior is `axis=0`. This means that the function will be applied
    to each column (i.e., it will process each column independently). This is what
    we have applied in the example code provided earlier. Depending on your specific
    use case, adjust `apply()` to operate row-wise (`axis=1`) or column-wise (`axis=0`)
    as needed. Next, let’s focus on `axis=1`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用pandas中的`apply()`函数时，如果没有指定axis参数，默认行为是`axis=0`。这意味着该函数将应用于每一列（即，它将独立处理每一列）。这就是我们在之前示例代码中所采用的方法。根据你的具体使用情况，可以调整`apply()`来按行（`axis=1`）或按列（`axis=0`）操作。接下来，让我们关注`axis=1`。
- en: '`axis=1` applies the function along the columns, so it processes each row independently.
    This is typically used when you want to perform row-wise operations (e.g., calculating
    a custom metric for each row).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`axis=1`沿列应用函数，因此它独立处理每一行。这通常用于你想要执行按行操作时（例如，为每一行计算自定义指标）。'
- en: '![Figure 6.2 – Apply() with axis=1](img/B19801_06_3.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – Apply() 使用 axis=1](img/B19801_06_3.jpg)'
- en: Figure 6.2 – Apply() with axis=1
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – Apply() 使用 axis=1
- en: 'Applying functions row-wise allows for row-level transformations and calculations.
    Let’s see a code example with `axis=1`. Let’s start by defining a function to
    be applied across columns (`axis=1`). The code can be found here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/4.apply_axis1.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/4.apply_axis1.py):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 按行应用函数允许进行行级转换和计算。让我们通过 `axis=1` 来查看一个代码示例。我们先定义一个要跨列（`axis=1`）应用的函数。代码可以在这里找到：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/4.apply_axis1.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/4.apply_axis1.py)：
- en: 'The `row_summary` function takes a single row of a DataFrame as input and returns
    a summary of that row’s data. The input for the function is key to understanding
    that is a single row of the DataFrame, passed as a pandas Series:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`row_summary` 函数将 DataFrame 的单行作为输入，并返回该行数据的汇总。该函数的输入是关键，理解这一点是至关重要的，它是作为 pandas
    Series 传入的 DataFrame 单行：'
- en: '[PRE21]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `total_sales_quantity` variable will store the sum of `Sales` and `Quantity`
    for the row. The `sales_quantity_ratio` variable will store the ratio of `Sales`
    to `Quantity` for the row, or `np.nan` if the quantity is zero, providing insight
    into the sales efficiency.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`total_sales_quantity` 变量将存储该行的 `Sales` 和 `Quantity` 的总和。`sales_quantity_ratio`
    变量将存储该行的 `Sales` 与 `Quantity` 的比率，如果数量为零，则为 `np.nan`，以提供销售效率的洞察。'
- en: 'We apply the function row-wise (`axis=1`):'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按行应用函数（`axis=1`）：
- en: '[PRE22]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This will produce a new `df_row_summary` DataFrame where each row corresponds
    to the calculated values for `total_sales_quantity` and `sales_quantity_ratio`
    for the original rows in `df`.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成一个新的 `df_row_summary` DataFrame，其中每一行对应于原始 `df` 中每行的 `total_sales_quantity`
    和 `sales_quantity_ratio` 计算值。
- en: 'Finally, we group by `Category` to calculate metrics per category:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们按 `Category` 分组，以计算每个类别的指标：
- en: '[PRE23]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s see the final result:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看最终结果：
- en: '[PRE24]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `total_sales_quantity` metric provides a simple yet effective measure of
    overall sales performance per transaction, helping to understand the combined
    impact of the number of items sold (`Quantity`) and their sales value (`Sales`).
    By analyzing `total_sales_quantity`, we can identify transactions with high combined
    sales and quantity, which might indicate popular product categories or successful
    sales strategies. Conversely, it also helps recognize low-performing transactions,
    thereby guiding inventory management and promotional adjustments to improve sales
    efficiency and product performance. This dual insight aids in strategic decision-making
    to optimize sales and inventory management.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`total_sales_quantity` 指标提供了一个简单但有效的衡量标准，帮助我们了解每笔交易的总体销售表现，理解销售数量（`Quantity`）和销售价值（`Sales`）的综合影响。通过分析
    `total_sales_quantity`，我们可以识别出销售和数量都较高的交易，这可能表示受欢迎的产品类别或成功的销售策略。相反，它也有助于识别表现不佳的交易，从而指导库存管理和促销调整，以提高销售效率和产品表现。这种双重洞察有助于战略决策，以优化销售和库存管理。'
- en: The `sales_quantity_ratio` metric provides valuable insights into the efficiency
    of sales per unit quantity, revealing how effectively products convert quantity
    into revenue. This metric is crucial for assessing the value derived from each
    unit sold. With this, we can identify products that generate high revenue per
    unit, indicating high-value items that may warrant prioritized marketing efforts.
    Conversely, it helps uncover products with a low revenue per unit, signaling potential
    areas for price adjustments, targeted promotions, or re-evaluation within the
    product portfolio to optimize profitability and sales performance.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`sales_quantity_ratio` 指标提供了每单位数量的销售效率的宝贵洞察，揭示了产品如何有效地将数量转化为收入。这个指标对于评估每单位销售所产生的价值至关重要。通过它，我们可以识别每单位产生高收入的产品，表明这些可能是值得优先考虑营销的高价值商品。相反，它有助于发现每单位收入较低的产品，提示可能需要调整价格、进行有针对性的促销，或重新评估产品组合，以优化盈利能力和销售表现。'
- en: Note
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Whenever feasible, prefer using vectorized operations (built-in pandas methods
    or NumPy functions) over `apply` for performance reasons. Vectorized operations
    are generally faster because they leverage optimized C code under the hood.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，出于性能考虑，优先使用矢量化操作（内置的 pandas 方法或 NumPy 函数）而不是 `apply`。矢量化操作通常更快，因为它们利用了优化的
    C 代码。
- en: The concepts and techniques we have explored so far directly lead to the importance
    of filtering in data cleaning. Once we have applied transformations or aggregated
    data, filtering allows us to focus on specific subsets of data that are relevant
    to our analysis or meet certain conditions. For example, after calculating sales
    performance metrics, such as `Total_Sales_Quantity` and `Sales_Quantity_Ratio`,
    across different product categories, filtering can help identify categories or
    products that require further investigation, such as those with unusually high
    or low performance metrics.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止我们探讨的概念和技巧直接体现了数据清理中筛选的重要性。一旦我们应用了转换或聚合数据，筛选就能帮助我们聚焦于对分析有意义的特定数据子集，或满足特定条件的子集。例如，在计算了不同产品类别的销售表现指标（如`Total_Sales_Quantity`和`Sales_Quantity_Ratio`）之后，筛选可以帮助我们识别需要进一步调查的类别或产品，比如那些具有异常高或低表现指标的产品。
- en: Data filtering
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据筛选
- en: '**Data filtering** is a fundamental operation in data manipulation that involves
    selecting a subset of data based on specified conditions or criteria. It is used
    to extract relevant information from a larger dataset, exclude unwanted data points,
    or focus on specific segments that are of interest for analysis or reporting.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据筛选**是数据处理中的一项基本操作，涉及根据指定条件或标准选择数据子集。它用于从较大的数据集中提取相关信息、排除不需要的数据点，或集中关注分析或报告所需的特定部分。'
- en: 'In the following example, we filter the DataFrame to only include rows where
    the `Quantity` column is greater than `10`. This operation selects products that
    have sold more than 10 units, focusing our analysis on potentially high-performing
    products. [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/5.simple_filtering.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/5.simple_filtering.py):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们筛选了 DataFrame，仅保留`Quantity`列大于`10`的行。这个操作选择了销量超过10个单位的产品，重点分析潜在的高绩效产品。[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/5.simple_filtering.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/5.simple_filtering.py)：
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s have a look at the resulting DataFrame:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下筛选后的 DataFrame：
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Moving beyond simple filters allows us to identify electronic products that
    satisfy more complex conditions, as we will see in the next section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 超越简单筛选可以帮助我们识别符合更复杂条件的电子产品，正如我们将在下一节看到的那样。
- en: Multiple criteria for filtering
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多重筛选条件
- en: Filtering may involve complex conditions, such as combining logical `AND` and
    `OR` operations, or using nested conditions. Let’s say that the management team
    wants us to identify high-value electronics products (`sales > 1000`) with relatively
    low sales quantities (`quantity <` `30`).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 筛选可能涉及复杂的条件，比如结合逻辑`AND`和`OR`操作，或使用嵌套条件。假设管理团队要求我们识别高价值的电子产品（`sales > 1000`），且销售数量相对较低（`quantity
    <` `30`）。
- en: 'Let’s see how we can do this with multiple filtering criteria (the code can
    be found here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/6.advanced_filtering.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/6.advanced_filtering.py)):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用多个筛选条件来完成这个操作（代码可以在这里找到：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/6.advanced_filtering.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/6.advanced_filtering.py)）：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In this example, we define a filter condition that filters rows where the sales
    are greater than `1000` and the quantity is less than `30`. Let’s have a look
    at the resulting DataFrame:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们定义了一个筛选条件，筛选出销售额大于`1000`且数量小于`30`的行。让我们来看一下筛选后的 DataFrame：
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Filtering is a straightforward operation to implement, but let’s explore some
    best practices to optimize its effectiveness.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 筛选是一个直接的操作，但让我们探索一些最佳实践，以优化其效果。
- en: Best practices for filtering
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 筛选的最佳实践
- en: 'Let’s explore the best practices that can enhance the effectiveness of filtering
    operations:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一些最佳实践，以提升筛选操作的效果：
- en: Clearly define the filtering criteria based on the analysis goals. Use conditions
    that are specific and relevant to the insights you want to derive.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据分析目标清晰定义筛选标准。使用那些具体且与您想要得出的见解相关的条件。
- en: Utilize built-in filter functions provided by data manipulation libraries, such
    as pandas in Python or SQL queries in databases. These functions are optimized
    for performance and ease of use.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用数据操作库（如Python中的pandas或数据库中的SQL查询）提供的内置过滤函数。这些函数在性能和易用性上进行了优化。
- en: Ensure that the filtering criteria do not exclude important data points that
    might be valuable for analysis. Validate the results to confirm they align with
    the expected outcomes.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保过滤条件不会排除那些可能对分析有价值的重要数据点。验证结果以确认它们与预期的结果一致。
- en: Document the filtering criteria and steps applied to maintain transparency and
    facilitate reproducibility of the analysis.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录过滤条件和应用步骤，以保持透明度并促进分析的可重复性。
- en: As datasets grow, filtering becomes essential to manage and extract insights
    efficiently. Operations that involve processing large volumes of data can become
    prohibitively slow without effective filtering strategies. Filtering helps in
    optimizing resource utilization, such as memory and processing power, by reducing
    the amount of data that needs to be stored and processed at any given time.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据集的增长，过滤变得至关重要，用于高效管理和提取洞察。没有有效的过滤策略，处理大量数据的操作可能会变得极其缓慢。通过减少每次需要存储和处理的数据量，过滤有助于优化资源利用，如内存和处理能力。
- en: Performance considerations as data grows
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随着数据增长，性能考虑因素
- en: 'Let’s have a look at things to keep in mind as data grows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下随着数据增长需要注意的事项：
- en: Filtering operations optimize query execution by reducing the number of rows
    or columns that need to be processed, leading to faster response times for data
    queries and analyses.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤操作通过减少需要处理的行或列数来优化查询执行，从而加快数据查询和分析的响应速度。
- en: Large datasets consume significant memory and storage resources. Filtering reduces
    the amount of data held in memory or stored on disk, improving efficiency, and
    reducing operational costs associated with data storage.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型数据集消耗大量内存和存储资源。过滤减少了存储在内存中或硬盘上的数据量，提高了效率，并降低了与数据存储相关的运营成本。
- en: Let’s now summarize the learnings from this chapter.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们总结一下本章的学习内容。
- en: Summary
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we’ve explored some powerful techniques, such as grouping,
    aggregation, and applying custom functions. These methods are essential for summarizing
    and transforming data, enabling deeper insights into datasets. We’ve learned how
    to efficiently group data by categorical variables, such as `Category` and `Region`,
    and apply aggregate functions, such as sum, mean, and custom metrics to derive
    meaningful summaries.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了一些强大的技术，例如分组、聚合和应用自定义函数。这些方法对于总结和转化数据至关重要，有助于深入洞察数据集。我们学习了如何根据类别变量（如`Category`和`Region`）高效地分组数据，并应用聚合函数（如求和、平均值和自定义指标）来得出有意义的总结。
- en: Additionally, we deep-dived into the versatility of `apply` functions, which
    allow for row-wise or column-wise custom computations. Best practices, such as
    optimizing function efficiency, handling missing values, and understanding performance
    implications, were emphasized to ensure effective data processing. Finally, we
    discussed the strategic application of filters to refine datasets based on specific
    criteria, enhancing data analysis precision.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们深入探讨了`apply`函数的多功能性，它允许进行行或列的自定义计算。强调了优化函数效率、处理缺失值和理解性能影响等最佳实践，以确保有效的数据处理。最后，我们讨论了过滤器的战略性应用，基于特定标准精炼数据集，提升数据分析精度。
- en: In the next chapter, we will discuss designing and optimizing data write operations
    to efficiently store the transformed and cleaned data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论设计和优化数据写入操作，以高效地存储转化和清洗后的数据。
