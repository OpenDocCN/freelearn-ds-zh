- en: Chapter 1.  The Big Data Science Ecosystem
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。大数据科学生态系统
- en: As a data scientist, you'll no doubt be very familiar with handling files and
    processing perhaps even large amounts of data. However, as I'm sure you will agree,
    doing anything more than a simple analysis over a single type of data requires
    a method of organizing and cataloguing data so that it can be managed effectively.
    Indeed, this is the cornerstone of a great data scientist. As the data volume
    and complexity increases, a consistent and robust approach can be the difference
    between generalized success and over-fitted failure!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名数据科学家，您无疑对处理文件和处理大量数据非常熟悉。然而，正如您所同意的，除了简单分析单一类型的数据之外，需要一种组织和编目数据的方法，以便有效地管理数据。事实上，这是一名优秀数据科学家的基石。随着数据量和复杂性的增加，一种一致而坚固的方法可以决定泛化的成功和过度拟合的失败之间的差异！
- en: This chapter is an introduction to an approach and ecosystem for achieving success
    with data at scale. It focuses on the data science tools and technologies. It
    introduces the environment, and how to configure it appropriately, but also explains
    some of the nonfunctional considerations relevant to the overall data architecture.
    While there is little actual data science at this stage, it provides the essential
    platform to pave the way for success in the rest of the book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是介绍一种在大规模数据上取得成功的方法和生态系统。它侧重于数据科学工具和技术。它介绍了环境以及如何适当配置，但也解释了一些与整体数据架构相关的非功能性考虑。虽然在这个阶段几乎没有实际的数据科学，但它为本书的其余部分的成功铺平了道路。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Data management responsibilities
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管理责任
- en: Data architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据架构
- en: Companion tools
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伴侣工具
- en: Introducing the Big Data ecosystem
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍大数据生态系统
- en: Data management is of particular importance, especially when the data is in
    flux; either constantly changing or being routinely produced and updated. What
    is needed in these cases is a way of storing, structuring, and auditing data that
    allows for the continuous processing and refinement of models and results.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管理尤为重要，特别是当数据处于不断变化或定期产生和更新的状态时。在这些情况下需要的是一种存储、结构化和审计数据的方式，允许对模型和结果进行持续处理和改进。
- en: Here, we describe how to best hold and organize your data to integrate with
    Apache Spark and related tools within the context of a data architecture that
    is broad enough to fit the everyday requirement.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们描述了如何最好地持有和组织您的数据，以便与Apache Spark和相关工具在数据架构的背景下进行整合。
- en: Data management
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管理
- en: Even if, in the medium term, you only intend to play around with a bit of data
    at home; then without proper data management, more often than not, efforts will
    escalate to the point where it is easy to lose track of where you are and mistakes
    will happen. Taking the time to think about the organization of your data, and
    in particular, its ingestion, is crucial. There's nothing worse than waiting for
    a long running analytic to complete, collating the results and producing a report,
    only to discover you used the wrong version of data, or data is incomplete, has
    missing fields, or even worse you deleted your results!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在中期，您只打算在家里玩一点数据；然而，如果没有适当的数据管理，往往会导致努力升级到您很容易迷失方向并且会发生错误的程度。花时间考虑数据的组织，特别是其摄入，是至关重要的。没有比等待长时间运行的分析完成，整理结果并生成报告，然后发现您使用了错误版本的数据，或者数据不完整，缺少字段，甚至更糟糕的是您删除了结果更糟糕的事情了！
- en: The bad news is that, despite its importance, data management is an area that
    is consistently overlooked in both commercial and non-commercial ventures, with
    precious few off-the-shelf solutions available. The good news is that it is much
    easier to do great data science using the fundamental building blocks that this
    chapter describes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 坏消息是，尽管它很重要，但数据管理是一个在商业和非商业企业中一直被忽视的领域，几乎没有现成的解决方案。好消息是，使用本章描述的基本构建模块进行出色的数据科学要容易得多。
- en: Data management responsibilities
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管理责任
- en: 'When we think about data, it is easy to overlook the true extent of the scope
    of the areas we need to consider. Indeed, most data "newbies" think about the
    scope in this way:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑数据时，很容易忽视我们需要考虑的范围的真正程度。事实上，大多数数据“新手”以这种方式考虑范围：
- en: Obtain data
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据
- en: Place the data somewhere (anywhere)
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据放在某个地方（任何地方）
- en: Use the data
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据
- en: Throw the data away
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扔掉数据
- en: 'In reality, there are a large number of other considerations, it is our combined
    responsibility to determine which ones apply to a given work piece. The following
    data management building blocks assist in answering or tracking some important
    questions about the data:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，还有许多其他考虑因素，我们有责任确定哪些适用于特定的工作。以下数据管理构建模块有助于回答或跟踪有关数据的一些重要问题：
- en: File integrity
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件完整性
- en: Is the data file complete?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据文件是否完整？
- en: How do you know?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你怎么知道的？
- en: Was it part of a set?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是否属于一组？
- en: Is the data file correct?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据文件是否正确？
- en: Was it tampered with in transit?
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在传输过程中是否被篡改？
- en: Data integrity
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据完整性
- en: Is the data as expected?
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否符合预期？
- en: Are all of the fields present?
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有字段都存在吗？
- en: Is there sufficient metadata?
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有足够的元数据？
- en: Is the data quality sufficient?
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量是否足够？
- en: Has there been any data drift?
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有任何数据漂移？
- en: Scheduling
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度
- en: Is the data routinely transmitted?
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否定期传输？
- en: How often does the data arrive?
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据多久到达一次？
- en: Was the data received on time?
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否按时接收？
- en: Can you prove when the data was received?
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能证明数据是何时接收的吗？
- en: Does it require acknowledgement?
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要确认吗？
- en: Schema management
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式管理
- en: Is the data structured or unstructured?
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是结构化的还是非结构化的？
- en: How should the data be interpreted?
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据应该如何解释？
- en: Can the schema be inferred?
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否可以推断出模式？
- en: Has the data changed over time?
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否随时间改变？
- en: Can the schema be evolved from the previous version?
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式是否可以从上一个版本演变？
- en: Version Management
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本管理
- en: What is the version of the data?
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的版本是多少？
- en: Is the version correct?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本是否正确？
- en: How do you handle different versions of the data?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何处理不同版本的数据？
- en: How do you know which version you're using?
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您如何知道自己使用的是哪个版本？
- en: Security
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全
- en: Is the data sensitive?
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否敏感？
- en: Does it contain personally identifiable information (PII)?
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含个人可识别信息（PII）吗？
- en: Does it contain personal health information (PHI)?
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含个人健康信息（PHI）吗？
- en: Does it contain payment card information (PCI)?
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含支付卡信息（PCI）吗？
- en: How should I protect the data?
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该如何保护数据？
- en: Who is entitled to read/write the data?
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁有权读取/写入数据？
- en: Does it require anonymization/sanitization/obfuscation/encryption?
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否需要匿名化/清理/混淆/加密？
- en: Disposal
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处置
- en: How do we dispose of the data?
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何处理数据？
- en: When do we dispose of the data?
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们何时处置数据？
- en: If, after all that, you are still not convinced, before you go ahead and write
    that bash script using the `gawk` and `crontab` commands, keep reading and you
    will soon see that there is a far quicker, flexible, and safer method that allow
    you to start small and incrementally create commercial grade ingestion pipelines!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果经过所有这些之后，你仍然不确定，在你继续编写使用`gawk`和`crontab`命令的bash脚本之前，继续阅读，你很快就会发现有一种更快、更灵活、更安全的方法，可以让你从小处着手，逐步创建商业级的摄取管道！
- en: The right tool for the job
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合适的工具来完成工作
- en: Apache Spark is the emerging de facto standard for scalable data processing.
    At the time of writing this book, it is the most active **Apache Software Foundation**
    (**ASF**) project and has a rich variety of companion tools available. There are
    new projects appearing every day, many of which overlap in functionality. So it
    takes time to learn what they do and decide whether they are appropriate to use.
    Unfortunately, there's no quick way around this. Usually, specific trade-offs
    must be made on a case-by-case basis; there is rarely a one-size-fits-all solution.
    Therefore, the reader is encouraged to explore the available tools and choose
    wisely!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是可扩展数据处理的新兴事实标准。在撰写本书时，它是最活跃的**Apache软件基金会**（**ASF**）项目，并且有丰富多样的伴随工具可用。每天都会出现新项目，其中许多项目在功能上有重叠。因此，需要时间来了解它们的功能并决定是否适合使用。不幸的是，这方面没有快速的方法。通常，必须根据具体情况做出特定的权衡；很少有一刀切的解决方案。因此，鼓励读者探索可用的工具并明智地选择！
- en: Various technologies are introduced throughout this book, and the hope is that
    they will provide the reader with a taster of some of the more useful and practical
    ones to a level where they may start utilizing them in their own projects. And
    further, we hope to show that if the code is written carefully, technologies may
    be interchanged through clever use of **Application Program Interface** (**APIs**)
    (or high order functions in Spark Scala) even when a decision is proved to be
    incorrect.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本书介绍了各种技术，希望能为读者提供一些更有用和实用的技术的入门，以至于他们可以开始在自己的项目中利用它们。此外，我们希望展示，如果代码编写得当，即使决定被证明是错误的，也可以通过巧妙地使用**应用程序接口**（**API**）（或Spark
    Scala中的高阶函数）来交换技术。
- en: Overall architecture
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总体架构
- en: 'Let''s start with a high-level introduction to data architectures: what they
    do, why they''re useful, when they should be used, and how Apache Spark fits in.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据架构的高层介绍开始：它们的作用是什么，为什么它们有用，何时应该使用它们，以及Apache Spark如何适应其中。
- en: '![Overall architecture](img/image_01_001.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![总体架构](img/image_01_001.jpg)'
- en: 'At their most general, modern data architectures have four basic characteristics:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在最一般的情况下，现代数据架构具有四个基本特征：
- en: Data Ingestion
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取
- en: Data Lake
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖
- en: Data Science
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学
- en: Data Access
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据访问
- en: Let's introduce each of these now, so that we can go into more detail in the
    later chapters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们介绍每一个，这样我们可以在后面的章节中更详细地讨论。
- en: Data Ingestion
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据摄取
- en: Traditionally, data is ingested under strict rules and formatted according to
    a predetermined schema. This process is known as **Extract, Transform, Load**
    (**ETL**), and is still a very common practice supported by a large array of commercial
    tools as well as some open source products.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，数据是根据严格的规则摄取，并根据预定的模式进行格式化。这个过程被称为**提取、转换、加载**（**ETL**），仍然是一个非常常见的做法，得到了大量商业工具以及一些开源产品的支持。
- en: '![Data Ingestion](img/image_01_002.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![数据摄取](img/image_01_002.jpg)'
- en: The ETL approach favors performing up-front checks, which ensure data quality
    and schema conformance, in order to simplify follow-on online analytical processing.
    It is particularly suited to handling data with a specific set of characteristics,
    namely, those that relate to a classical entity-relationship model. However, it
    is not suitable for all scenarios.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ETL方法倾向于进行前期检查，以确保数据质量和模式一致，以简化后续的在线分析处理。它特别适用于处理具有特定特征集的数据，即与经典实体关系模型相关的数据。然而，并不适用于所有情况。
- en: 'During the big data revolution, there was a metaphorical explosion of demand
    for structured, semi-structured, and unstructured data, leading to the creation
    of systems that were required to handle data with a different set of characteristics.
    These came to be defined by the phrase, *4 Vs: Volume, Variety, Velocity, and
    Veracity* [http://www.ibmbigdatahub.com/infographic/four-vs-big-data](http://www.ibmbigdatahub.com/infographic/four-vs-big-data).
    While traditional ETL methods floundered under this new burden-because they simply
    required too much time to process the vast quantities of data, or were too rigid
    in the face of change, a different approach emerged. Enter the **schema-on-read**
    paradigm. Here, data is ingested in its original form (or at least very close
    to) and the details of normalization, validation, and so on are done at the time
    of analytical processing.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据革命期间，对结构化、半结构化和非结构化数据的需求出现了象征性的爆炸，导致需要处理具有不同特征集的系统的创建。这些被定义为“4V：容量、多样性、速度和准确性”[http://www.ibmbigdatahub.com/infographic/four-vs-big-data](http://www.ibmbigdatahub.com/infographic/four-vs-big-data)。传统的ETL方法在这种新负担下陷入困境，因为它们处理大量数据需要太长时间，或者在面对变化时过于僵化，于是出现了一种不同的方法。进入“读时模式”范式。在这里，数据以其原始形式（或至少非常接近）被摄入，规范化、验证等细节在分析处理时进行。
- en: 'This is typically referred to as **Extract Load Transform** (**ELT**), a reference
    to the traditional approach:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被称为“提取加载转换”（ELT），是对传统方法的参考：
- en: '![Data Ingestion](img/image_01_003.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![数据摄入](img/image_01_003.jpg)'
- en: This approach values the delivery of data in a timely fashion, delaying the
    detailed processing until it is absolutely required. In this way, a data scientist
    can gain access to the data immediately, searching for insight using a range of
    techniques not available with a traditional approach.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法重视及时交付数据，延迟详细处理直到绝对需要。这样，数据科学家可以立即访问数据，使用一系列传统方法不可用的技术寻找洞见。
- en: Although we only provide a high-level overview here, this approach is so important
    that throughout the book we will explore further by implementing various schema-on-read
    algorithms. We will assume the ELT method for data ingestion, that is to say we
    encourage the loading of data at the user's convenience. This may be every *n*
    minute, overnight or during times of low usage. The data can then be checked for
    integrity, quality, and so forth by running batch processing jobs offline, again
    at the user's discretion.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在这里只提供了一个高层概述，但这种方法非常重要，因此在整本书中，我们将通过实施各种读时模式算法来进一步探讨。我们将假定数据摄入采用ELT方法，也就是说我们鼓励用户根据自己的方便加载数据。这可以是每隔*n*分钟、夜间或在低使用率时进行。然后可以通过运行离线批处理作业再次由用户自行决定检查数据的完整性、质量等等。
- en: Data Lake
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据湖
- en: 'A data lake is a convenient, ubiquitous store of data. It is useful because
    it provides a number of key benefits, primarily:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖是一个方便、无处不在的数据存储。它很有用，因为它提供了许多关键的好处，主要包括：
- en: Reliable storage
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可靠的存储
- en: Scalable data processing capability
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展的数据处理能力
- en: Let's take a brief look at each of these.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地看一下每一个。
- en: Reliable storage
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可靠的存储
- en: There is a good choice of underlying storage implementations for a data lake,
    these include **Hadoop Distributed File System** (**HDFS**), **MapR-FS**, and
    **Amazon AWS S3**.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖有许多可靠的底层存储实现，包括Hadoop分布式文件系统（HDFS）、MapR-FS和Amazon AWS S3。
- en: Throughout the book, HDFS will be the assumed storage implementation. Also,
    in this book the authors use a distributed Spark setup, deployed on **Yet Another
    Resource Negotiator** (**YARN**) running inside a Hortonworks HDP environment.
    Therefore, HDFS is the technology used, unless otherwise stated. If you are not
    familiar with any of these technologies, they are discussed further on in this
    chapter.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，HDFS将被假定为存储实现。此外，在本书中，作者使用部署在Hortonworks HDP环境中运行的YARN的分布式Spark设置。因此，除非另有说明，否则HDFS是使用的技术。如果您对这些技术不熟悉，它们将在本章后面进一步讨论。
- en: In any case, it's worth knowing that Spark references HDFS locations natively,
    accesses local file locations via the prefix `file://` and references S3 locations
    via the prefix `s3a://`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，值得知道的是，Spark可以本地引用HDFS位置，通过前缀`file://`访问本地文件位置，并通过前缀`s3a://`引用S3位置。
- en: Scalable data processing capability
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可扩展的数据处理能力
- en: 'Clearly, Apache Spark will be our data processing platform of choice. In addition,
    as you may recall, Spark allows the user to execute code in their preferred environment,
    be that local, standalone, YARN or Mesos, by configuring the appropriate cluster
    manager; in `masterURL`. Incidentally, this can be done in any one of the three
    locations:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，Apache Spark将是我们首选的数据处理平台。此外，正如您可能记得的那样，Spark允许用户在其首选环境中执行代码，无论是本地、独立、YARN还是Mesos，都可以通过配置适当的集群管理器来实现；在`masterURL`中。顺便说一句，这可以在以下三个位置之一完成：
- en: Using the `--master` option when issuing the `spark-submit` command
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在发出`spark-submit`命令时使用`--master`选项
- en: Adding the `spark.master` property in the `conf/spark-defaults.conf` file
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`conf/spark-defaults.conf`文件中添加`spark.master`属性
- en: Invoking the `setMaster` method on the `SparkConf` object
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`SparkConf`对象上调用`setMaster`方法
- en: If you're not familiar with HDFS, or if you do not have access to a cluster,
    then you can run a local Spark instance using the local filesystem, which is useful
    for testing. However, beware that there are often bad behaviors that only appear
    when executing on a cluster. So, if you're serious about Spark, it's worth investing
    in a distributed cluster manager why not try Spark standalone cluster mode, or
    Amazon AWS EMR? For example, Amazon offers a number of affordable paths to cloud
    computing, you can explore the idea of spot instances at [https://aws.amazon.com/ec2/spot/](https://aws.amazon.com/ec2/spot/).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉HDFS，或者没有访问集群的权限，那么可以使用本地文件系统运行本地Spark实例，这对于测试很有用。但是要注意，有时只有在集群上执行时才会出现不良行为。因此，如果您对Spark很认真，值得投资于分布式集群管理器，为什么不尝试Spark独立集群模式，或者亚马逊AWS
    EMR？例如，亚马逊提供了许多负担得起的云计算路径，您可以探索[https://aws.amazon.com/ec2/spot/](https://aws.amazon.com/ec2/spot/)上的抢购实例的想法。
- en: Data science platform
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据科学平台
- en: A data science platform provides services and APIs that enable effective data
    science to take place, including explorative data analysis, machine learning model
    creation and refinement, image and audio processing, natural language processing,
    and text sentiment analysis.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学平台提供了服务和API，使得有效的数据科学得以进行，包括探索性数据分析、机器学习模型的创建和完善、图像和音频处理、自然语言处理和文本情感分析。
- en: This is the area where Spark really excels and forms the primary focus of the
    remainder of this book, exploiting a robust set of native machine learning libraries,
    unsurpassed parallel graph processing capabilities and a strong community. Spark
    provides truly scalable opportunities for data science.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Spark真正擅长的领域，也是本书剩下部分的主要重点，利用强大的本地机器学习库、无与伦比的并行图处理能力和强大的社区。Spark为数据科学提供了真正可扩展的机会。
- en: The remaining chapters will provide insight into each of these areas, including
    [Chapter 6](ch06.xhtml "Chapter 6. Scraping Link-Based External Data"), *Scraping
    Link-Based External Data*, [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"),
    *Building Communities*, and [Chapter 8](ch08.xhtml "Chapter 8. Building a Recommendation
    System"), *Building a Recommendation System*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的章节将深入探讨这些领域，包括[第6章](ch06.xhtml "第6章。抓取基于链接的外部数据")，“抓取基于链接的外部数据”，[第7章](ch07.xhtml
    "第7章。构建社区")，“构建社区”，和[第8章](ch08.xhtml "第8章。构建推荐系统")，“构建推荐系统”。
- en: Data Access
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据访问
- en: Data in a data lake is most frequently accessed by data engineers and scientists
    using the Hadoop ecosystem tools, such as Apache Spark, Pig, Hive, Impala, or
    Drill. However, there are times when other users, or even other systems, need
    access to the data and the normal tools are either too technical or do not meet
    the demanding expectations of the user in terms of real-world latency.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖中的数据最常由数据工程师和科学家使用Hadoop生态系统工具访问，比如Apache Spark、Pig、Hive、Impala或Drill。然而，有时其他用户，甚至其他系统，需要访问数据，而常规工具要么太技术化，要么无法满足用户对实时延迟的苛刻期望。
- en: In these circumstances, the data often needs to be copied into data marts or
    index stores so that it may be exposed to more traditional methods, such as a
    report or dashboard. This process, which typically involves creating indexes and
    restructuring data for low-latency access, is known as data egress.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，数据通常需要被复制到数据仓库或索引存储中，以便可以暴露给更传统的方法，比如报告或仪表盘。这个过程通常涉及创建索引和重组数据以实现低延迟访问，被称为数据出口。
- en: Fortunately, Apache Spark has a wide variety of adapters and connectors into
    traditional databases, BI tools, and visualization and reporting software. Many
    of these will be introduced throughout the book.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Apache Spark有各种适配器和连接器，可以连接传统数据库、BI工具以及可视化和报告软件。本书将介绍其中许多。
- en: Data technologies
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据技术
- en: When Hadoop first started, the word Hadoop referred to the combination of HDFS
    and the MapReduce processing paradigm, as that was the outline of the original
    paper [http://research.google.com/archive/mapreduce.html](http://research.google.com/archive/mapreduce.html).
    Since that time, a plethora of technologies have emerged to complement Hadoop,
    and with the development of Apache YARN we now see other processing paradigms
    emerge such as Spark.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当Hadoop刚开始时，Hadoop这个词指的是HDFS和MapReduce处理范式的组合，因为这是原始论文的概要[http://research.google.com/archive/mapreduce.html](http://research.google.com/archive/mapreduce.html)。自那时起，出现了大量的技术来补充Hadoop，随着Apache
    YARN的发展，我们现在看到其他处理范式的出现，比如Spark。
- en: 'Hadoop is now often used as a colloquialism for the entire big data software
    stack and so it would be prudent at this point to define the scope of that stack
    for this book. The typical data architecture with a selection of technologies
    we will visit throughout the book is detailed as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Hadoop通常被用作整个大数据软件堆栈的俗语，因此在这一点上，为本书定义该堆栈的范围是明智的。本书将在整本书中访问的一系列技术的典型数据架构如下所述：
- en: '![Data technologies](img/B05261_01_04-1.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![数据技术](img/B05261_01_04-1.jpg)'
- en: The relationship between these technologies is a dense topic as there are complex
    interdependencies, for example, Spark depends on GeoMesa, which depends on Accumulo,
    which depends on Zookeeper and HDFS! Therefore, in order to manage these relationships,
    there are platforms available, such as Cloudera or Hortonworks HDP [http://hortonworks.com/products/sandbox/](http://hortonworks.com/products/sandbox/).
    These provide consolidated user interfaces and centralized configuration. The
    choice of platform is that of the reader, however, it is not recommended to install
    a few of the technologies initially and then move to a managed platform as the
    version problems encountered will be very complex. Therefore, it is usually easier
    to start with a clean machine and make a decision upfront as to which direction
    to take.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术之间的关系是一个复杂的话题，因为它们之间存在复杂的相互依赖关系，例如，Spark依赖于GeoMesa，而GeoMesa又依赖于Accumulo，Accumulo又依赖于Zookeeper和HDFS！因此，为了管理这些关系，有一些可用的平台，比如Cloudera或Hortonworks
    HDP [http://hortonworks.com/products/sandbox/](http://hortonworks.com/products/sandbox/)。这些平台提供了集中的用户界面和集中的配置。平台的选择取决于读者，然而，不建议最初安装一些技术，然后转移到受管理的平台，因为遇到的版本问题会非常复杂。因此，通常更容易从一个干净的机器开始，并在前期做出决定。
- en: All of the software we use in this book is platform-agnostic and therefore fits
    into the general architecture described earlier. It can be installed independently
    and it is relatively straightforward to use with single or multiple server environment
    without the use of a managed product.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中使用的所有软件都是与平台无关的，因此适用于前面描述的一般架构。它可以独立安装，并且在单个或多个服务器环境中使用相对简单，而不需要使用受管理的产品。
- en: The role of Apache Spark
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Spark的作用
- en: 'In many ways, Apache Spark is the glue that holds these components together.
    It increasingly represents the hub of the software stack. It integrates with a
    wide variety of components but none of them are hard-wired. Indeed, even the underlying
    storage mechanism can be swapped out. Combining this feature with the ability
    to leverage different processing frameworks means the original Hadoop technologies
    effectively become components, rather than an imposing framework. The logical
    diagram of our architecture appears as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，Apache Spark是将这些组件联系在一起的粘合剂。它越来越多地代表了软件堆栈的中心。它与各种组件集成，但没有一个是硬连接的。事实上，甚至底层存储机制都可以被替换。将这个特性与利用不同处理框架的能力相结合，意味着最初的Hadoop技术有效地成为组件，而不是一个庞大的框架。我们的架构的逻辑图如下所示：
- en: '![The role of Apache Spark](img/image_01_005.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark的作用](img/image_01_005.jpg)'
- en: As Spark has gained momentum and wide-scale industry acceptance, many of the
    original Hadoop implementations for various components have been refactored for
    Spark. Thus, to add further complexity to the picture, there are often several
    possible ways to programmatically leverage any particular component; not least
    the imperative and declarative versions depending upon whether an API has been
    ported from the original Hadoop Java implementation. We have attempted to remain
    as true as possible to the Spark ethos throughout the remaining chapters.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Spark的发展和广泛的行业认可，许多最初的Hadoop实现已经被重构为Spark。因此，为了给这个画面增加更多的复杂性，通常有几种可能的方法来以编程方式利用任何特定的组件；尤其是根据API是否从最初的Hadoop
    Java实现中移植出来的命令式和声明式版本。在接下来的章节中，我们尽量保持对Spark精神的忠实。
- en: Companion tools
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伴随工具
- en: Now that we have established a technology stack to use, let's describe each
    of the components and explain why they are useful in a Spark environment. This
    part of the book is designed as a reference rather than a straight read. If you're
    familiar with most of the technologies, then you can refresh your knowledge and
    continue to the next section, [Chapter 2](ch02.xhtml "Chapter 2. Data Acquisition"),
    *Data Acquisition*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了一个要使用的技术堆栈，让我们描述每个组件，并解释它们在Spark环境中的用处。本书的这一部分旨在作为参考而不是直接阅读。如果您熟悉大多数技术，那么您可以刷新您的知识并继续阅读下一节，[第2章](ch02.xhtml
    "第2章。数据采集")，*数据采集*。
- en: Apache HDFS
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache HDFS
- en: The **Hadoop Distributed File System** (**HDFS**) is a distributed filesystem
    with built-in redundancy. It is optimized to work on three or more nodes by default
    (although one will work fine and the limit can be increased), which provides the
    ability to store data in replicated blocks. So not only is a file split into a
    number of blocks but three copies of those blocks exist at any one time. This
    cleverly provides data redundancy (if one is lost two others still exist) but
    also provides *data locality*. When a distributed job is run against HDFS, not
    only will the system attempt to gather all of the blocks required for the data
    input to that job, it will also attempt to only use the blocks which are physically
    close to the server running that job; so it has the ability to reduce network
    bandwidth using only the blocks on its local storage, or those on nodes close
    to itself. This is achieved in practice by allocating HDFS physical disks to nodes,
    and nodes to racks; blocks are written in a node-local, rack-local, and cluster-local
    method. All instructions to HDFS are passed through a central server called **NameNode**,
    so this provides a possible central point of failure; there are various methods
    for providing NameNode redundancy.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hadoop分布式文件系统**（**HDFS**）是一个带有内置冗余的分布式文件系统。它默认优化为在三个或更多节点上工作（尽管一个节点也可以正常工作，且限制可以增加），这提供了存储数据的能力。因此，文件不仅被分割成多个块，而且这些块的三个副本在任何时候都存在。这巧妙地提供了数据冗余（如果一个丢失了，其他两个仍然存在），同时也提供了*数据局部性*。当对HDFS运行分布式作业时，系统不仅会尝试收集作业输入所需的所有块，还会尝试仅使用与运行该作业的服务器物理接近的块；因此，它有能力减少网络带宽，只使用其本地存储上的块，或者那些接近自身的节点上的块。实际上，这是通过将HDFS物理磁盘分配给节点，并将节点分配给机架来实现的；块是以节点本地、机架本地和集群本地的方式写入的。所有对HDFS的指令都通过一个名为**NameNode**的中央服务器传递，因此这提供了一个可能的单点故障；有各种方法可以提供NameNode的冗余。'
- en: Furthermore, in a multi-tenanted HDFS scenario, where many processes are accessing
    the same file at the same time, load balancing can also be achieved through the
    use of multiple blocks; for example, if a file takes up one block, this block
    is replicated three times and, therefore, potentially can be read from three different
    physical locations concurrently. Although this may not seem like a big win, on
    clusters of hundreds or thousands of nodes the network IO is often the single
    most limiting factor to a running job–the authors have certainly experienced times
    on multi-thousand node clusters where jobs have had to wait hours to complete
    purely because the network bandwidth has been maxed out due to the large number
    of other threads calling for data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在多租户HDFS场景中，许多进程同时访问同一文件时，通过使用多个块也可以实现负载平衡；例如，如果一个文件占用一个块，这个块被复制三次，因此可能可以同时从三个不同的物理位置读取。尽管这可能看起来不是一个很大的优势，在数百或数千个节点的集群上，网络IO通常是运行作业的最大限制因素--作者在多千节点集群上确实经历过作业不得不等待数小时才能完成的情况，纯粹是因为网络带宽由于大量其他线程调用数据而达到最大值。
- en: If you are running a laptop, require data to be stored locally, or wish to use
    the hardware you already have, then HDFS is a good option.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在运行笔记本电脑，需要将数据存储在本地，或者希望使用您已经拥有的硬件，那么HDFS是一个不错的选择。
- en: Advantages
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优势
- en: 'The following are the advantages of using HDFS:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用HDFS的优势如下：
- en: '**Redundancy**: Configurable replication of blocks provides tolerance for node
    and disk failure'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冗余**：块的可配置复制提供了对节点和磁盘故障的容忍'
- en: '**Load balancing**: Block replication means the same data can be accessed from
    different physical locations'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载平衡**：块复制意味着相同的数据可以从不同的物理位置访问'
- en: '**Data locality**: Analytics try to access the closest relevant physical block,
    reducing network IO.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据本地性**：分析尝试访问最接近的相关物理块，减少网络IO。'
- en: '**Data balance**: An algorithm is available to re-balance the data blocks as
    they become too clustered or fragmented.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据平衡**：有一个算法可以在数据块变得过于集中或碎片化时重新平衡数据块。'
- en: '**Flexible storage**: If more space is needed, further disks and nodes can
    be added; although this is not a hot process, the cluster will require outage
    to add these resources'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活的存储**：如果需要更多空间，可以添加更多磁盘和节点；尽管这不是一个热过程，但集群将需要停机来添加这些资源'
- en: '**Additional costs**: No third-party costs are involved'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**额外成本**：没有第三方成本涉及'
- en: '**Data encryption**: Implicit encryption (when turned on)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据加密**：隐式加密（打开时）'
- en: Disadvantages
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: 'The following are the disadvantages:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是缺点：
- en: The NameNode provides for a central point of failure; to mitigate this, there
    are secondary and high availability options available
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NameNode提供了一个中心故障点；为了减轻这一点，有辅助和高可用性选项可用
- en: A cluster requires basic administration and potentially some hardware effort
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群需要基本的管理和可能一些硬件工作
- en: Installation
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: 'To use HDFS, we should decide whether to run Hadoop in a local, pseudo-distributed
    or fully-distributed manner; for a single server, pseudo-distributed is useful
    as analytics should translate directly from this machine to any Hadoop cluster.
    In any case, we should install Hadoop with at least the following components:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用HDFS，我们应该决定是以本地、伪分布式还是完全分布式的方式运行Hadoop；对于单个服务器，伪分布式对于分析是有用的，因为分析应该可以直接从这台机器转移到任何Hadoop集群。无论如何，我们应该安装Hadoop，至少包括以下组件：
- en: NameNode
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NameNode
- en: Secondary NameNode (or High Availability NameNode)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辅助NameNode（或高可用性NameNode）
- en: DataNode
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataNode
- en: Hadoop can be installed via [http://hadoop.apache.org/releases.html](http://hadoop.apache.org/releases.html).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop可以通过[http://hadoop.apache.org/releases.html](http://hadoop.apache.org/releases.html)进行安装。
- en: 'Spark needs to know the location of the Hadoop configuration, specifically
    the following files: `hdfs-site.xml`, `core-site.xml`. This is then set in the
    configuration parameter `HADOOP_CONF_DIR` in your Spark configuration.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Spark需要知道Hadoop配置的位置，特别是以下文件：`hdfs-site.xml`，`core-site.xml`。然后在Spark配置中设置配置参数`HADOOP_CONF_DIR`。
- en: HDFS will then be available natively, so the file `hdfs://user/local/dir/text.txt`
    can be addressed in Spark simply using `/user/local/dir/text.txt`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后HDFS将以本地方式可用，因此在Spark中可以简单地使用`/user/local/dir/text.txt`来访问文件`hdfs://user/local/dir/text.txt`。
- en: Amazon S3
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊S3
- en: 'S3 abstracts away all of the issues related to parallelism, storage restrictions,
    and security allowing very large parallel read/write operations along with a great
    **Service Level Agreement** (**SLA**) for a very small cost. This is perfect if
    you need to get up and running quickly, can''t store data locally, or don''t know
    what your future storage requirements might be. It should be recognized that `s3n`
    and `S3a` utilize an object storage model, not file storage, and therefore there
    are some compromises:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: S3将所有与并行性、存储限制和安全性相关的问题都抽象化，允许非常大规模的并行读/写操作，并提供了极低的成本和极好的服务级别协议（SLA）。如果您需要快速启动、无法在本地存储数据，或者不知道未来的存储需求是什么，这是完美的选择。需要注意的是，`s3n`和`S3a`采用对象存储模型，而不是文件存储，因此存在一些妥协：
- en: Eventual consistency is where changes made by one application (creation, updates,
    and deletions) will not be visible until some undefined time, although most AWS
    regions now support read-after-write consistency.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终一致性是指一个应用程序所做的更改（创建、更新和删除）在一段时间内不可见，尽管大多数AWS区域现在支持写后读一致性。
- en: '`s3n` and `s3a` utilize nonatomic rename and delete operations; therefore,
    renaming or deleting large directories takes time proportional to the number of
    entries. However, target files can remain visible to other processes during this
    time, and indeed, until the eventual consistency has been resolved.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s3n`和`s3a`利用了非原子重命名和删除操作；因此，重命名或删除大型目录需要与条目数量成比例的时间。然而，在此期间，目标文件可能对其他进程可见，直到最终一致性得到解决。'
- en: S3 can be accessed through command-line tools (`s3cmd`) via a webpage and via
    APIs for most popular languages; it has native integration with Hadoop and Spark
    through a basic configuration.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: S3可以通过命令行工具（`s3cmd`）通过网页和大多数流行语言的API访问；它通过基本配置与Hadoop和Spark进行本地集成。
- en: Advantages
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优势
- en: 'The following are the advantages:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是优势：
- en: Infinite storage capacity
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无限的存储容量
- en: No hardware considerations
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无硬件考虑
- en: Encryption available (user stored keys)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用加密（用户存储的密钥）
- en: 99.9% availability
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 99.9%的可用性
- en: Redundancy
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冗余
- en: Disadvantages
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: 'The following are the disadvantages:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是缺点：
- en: Cost to store and transfer data
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储和传输数据的成本
- en: No data locality
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有数据局部性
- en: Eventual consistency
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终一致性
- en: Relatively high latency
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对较高的延迟
- en: Installation
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: 'You can create an AWS account: [https://aws.amazon.com/free/](https://aws.amazon.com/free/).
    Through this account, you will have access to S3 and will simply need to create
    some credentials.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以创建一个AWS账户：[https://aws.amazon.com/free/](https://aws.amazon.com/free/)。通过这个账户，您将可以访问S3，并且只需要创建一些凭据。
- en: 'The current S3 standard is `s3a`; to use it through Spark requires some changes
    to the Spark configuration:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的S3标准是`s3a`；要通过Spark使用它需要对Spark配置进行一些更改：
- en: '[PRE0]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If using HDP, you may also need:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用HDP，您可能还需要：
- en: '[PRE1]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'All S3 files will then be accessible within Spark using the prefix `s3a://`
    to the S3 object reference:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，所有S3文件都可以在Spark中使用前缀`s3a://`来访问S3对象引用：
- en: '[PRE2]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also use the AWS credentials inline assuming that we have set `spark.hadoop.fs.s3a.impl`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以内联使用AWS凭据，假设我们已经设置了`spark.hadoop.fs.s3a.impl`：
- en: '[PRE3]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: However, this method will not accept the forward-slash character `/` in either
    of the keys. This is usually solved by obtaining another key from AWS (keep generating
    a new one until there are no forward-slashes present).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法不会接受键中的斜杠字符`/`。这通常可以通过从AWS获取另一个键来解决（直到没有斜杠出现为止）。
- en: We can also browse the objects through the web interface located under the S3
    tab in your AWS account.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过AWS账户中的S3选项卡下的Web界面浏览对象。
- en: Apache Kafka
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Kafka
- en: Apache Kafka is a distributed, message broker written in Scala and available
    under the Apache Software Foundation license. The project aims to provide a unified,
    high-throughput, low-latency platform for handling real-time data feeds. The result
    is essentially a massively scalable publish-subscribe message queue, making it
    highly valuable for enterprise infrastructures to process streaming data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka是一个分布式的、用Scala编写的消息代理，可在Apache软件基金会许可下使用。该项目旨在提供一个统一的、高吞吐量、低延迟的平台，用于处理实时数据源。其结果本质上是一个大规模可扩展的发布-订阅消息队列，对于企业基础设施处理流式数据非常有价值。
- en: Advantages
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优势
- en: 'The following are the advantages:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是优势：
- en: Publish-subscribe messaging
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布-订阅消息
- en: Fault-tolerant
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容错
- en: Guaranteed delivery
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保证交付
- en: Replay messages on failure
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障时重播消息
- en: Highly-scalable, shared-nothing architecture
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可扩展的共享无架构
- en: Supports back pressure
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持背压
- en: Low latency
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低延迟
- en: Good Spark-streaming integration
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的Spark-streaming集成
- en: Simple for clients to implement
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端实现简单
- en: Disadvantages
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: 'The following are the disadvantages:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是缺点：
- en: At least once semantics - cannot provide exactly-once messaging due to lack
    of a transaction manager (as yet)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少一次语义-由于缺乏事务管理器，无法提供精确一次性消息传递
- en: Requires Zookeeper for operation
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要Zookeeper进行操作
- en: Installation
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: As Kafka is a pub-sub tool, its purpose is to manage messages (publishers) and
    direct them to the relevant endpoints (subscribers). This is done using a broker,
    which is installed when implementing Kafka. Kafka is available through the Hortonworks
    HDP platform, or can be installed independently from this link [http://kafka.apache.org/downloads.html](http://kafka.apache.org/downloads.html).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Kafka是一个发布-订阅工具，其目的是管理消息（发布者）并将其定向到相关的端点（订阅者）。这是通过经过Kafka实现时安装的代理来完成的。Kafka可以通过Hortonworks
    HDP平台获得，也可以独立安装，链接如下[http://kafka.apache.org/downloads.html](http://kafka.apache.org/downloads.html)。
- en: Kafka uses Zookeeper to manage leadership election (as Kafka can be distributed
    thus allowing for redundancy), the quick start guide found in the preceding link
    can be used to set up a single node Zookeeper instance, and also provide a client
    and consumer to publish and subscribe to topics, which provide the mechanism for
    message handling.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka使用Zookeeper来管理领导选举（因为Kafka可以分布式，从而实现冗余），在前面的链接中找到的快速入门指南可以用于设置单节点Zookeeper实例，并提供客户端和消费者来发布和订阅主题，这提供了消息处理的机制。
- en: Apache Parquet
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Parquet
- en: Since the inception of Hadoop, the idea of columnar-based formats (as opposed
    to row based) has been gaining increasing support. Parquet has been developed
    to take advantage of compressed, efficient columnar data representation and is
    designed with complex nested data structures in mind; taking the lead from algorithms
    discussed in the Apache Dremel paper [http://research.google.com/pubs/pub36632.html](http://research.google.com/pubs/pub36632.html).
    Parquet allows compression schemes to be specified on a per-column level, and
    is future-proofed for adding more encodings as they are implemented. It has also
    been designed to provide compatibility throughout the Hadoop ecosystem and, like
    Avro, stores the data schema with the data itself.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 自Hadoop诞生以来，基于列的格式（而不是基于行）的想法得到了越来越多的支持。Parquet已经开发出来，以利用压缩、高效的列式数据表示，并且设计时考虑了复杂的嵌套数据结构；它借鉴了Apache
    Dremel论文中讨论的算法。Parquet允许在每一列上指定压缩方案，并且为添加更多编码做好了未来的准备。它还被设计为在整个Hadoop生态系统中提供兼容性，并且像Avro一样，将数据模式与数据本身一起存储。
- en: Advantages
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优势
- en: 'The following are the advantages:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是优势：
- en: Columnar storage
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列式存储
- en: Highly storage efficient
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度存储效率
- en: Per column compression
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每列压缩
- en: Supports predicate pushdown
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持谓词下推
- en: Supports column pruning
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持列剪枝
- en: Compatible with other formats, for example, Avro
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他格式兼容，例如Avro
- en: Read efficient, designed for partial data retrieval
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效读取，设计用于部分数据检索
- en: Disadvantages
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: 'The following are the disadvantages:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是缺点：
- en: Not good for random access
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适合随机访问
- en: Potentially computationally intensive for writes
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写入可能需要大量计算
- en: Installation
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: 'Parquet is natively available in Spark and can be accessed directly as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet在Spark中是原生可用的，并且可以直接访问如下：
- en: '[PRE4]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Apache Avro
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Avro
- en: Apache Avro is a data serialization framework originally developed for Hadoop.
    It uses JSON for defining data types and protocols (although there is an alternative
    IDL), and serializes data in a compact binary format. Avro provides both a serialization
    format for persistent data, and a wire format for communication between Hadoop
    nodes, and from client programs to the Hadoop services. Another useful feature
    is its ability to store the data schema along with the data itself, so any Avro
    file can always be read without the need for referencing external sources. Further,
    Avro supports schema evolution and therefore backwards compatibility between Avro
    files written with older schema versions being read with a newer schema version.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Avro最初是为Hadoop开发的数据序列化框架。它使用JSON定义数据类型和协议（尽管还有另一种IDL），并以紧凑的二进制格式序列化数据。Avro既提供了持久数据的序列化格式，又提供了Hadoop节点之间通信的传输格式，以及客户端程序与Hadoop服务之间的通信格式。另一个有用的功能是它能够将数据模式与数据本身一起存储，因此任何Avro文件都可以在不需要引用外部源的情况下读取。此外，Avro支持模式演变，因此旧模式版本编写的Avro文件可以与新模式版本兼容。
- en: Advantages
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优点
- en: 'The following are the advantages:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是优点：
- en: Schema evolution
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式演变
- en: Disk space savings
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节省磁盘空间
- en: Supports schemas in JSON and IDL
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持JSON和IDL中的模式
- en: Supports many languages
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持多种语言
- en: Supports compression
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持压缩
- en: Disadvantages
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: 'The following are the disadvantages:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是缺点：
- en: Requires schema to read and write data
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要模式才能读写数据
- en: Serialization computationally heavy
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列化计算量大
- en: Installation
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: 'As we are using Scala, Spark, and Maven environments in this book, Avro can
    be imported as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书中使用Scala、Spark和Maven环境，因此可以导入Avro如下：
- en: '[PRE5]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It is then a matter of creating a schema and producing the Scala code to write
    data to Avro using the schema. This is explained in detail in [Chapter 3](ch03.xhtml
    "Chapter 3. Input Formats and Schema"), *Input Formats and Schema*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然后就是创建模式并生成Scala代码，使用模式将数据写入Avro。这在[第3章](ch03.xhtml "第3章 输入格式和模式") *输入格式和模式*中有详细说明。
- en: Apache NiFi
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache NiFi
- en: Apache NiFi originated from the United States **National Security Agency** (**NSA**)
    where it was released to open source in 2014 as part of their Technology Transfer
    Program. NiFi enables the production of scalable directed graphs of data routing
    and transformation, within a simple user interface. It also supports data provenance,
    a wide range of prebuilt processors and the ability to build new processors quickly
    and efficiently. It has prioritization, tunable delivery tolerances, and back-pressure
    features included, which allow the user to tune processors and pipelines for specific
    requirements, even allowing flow modification at runtime. All of this adds up
    to an incredibly flexible tool for building everything from one-off file download
    data flows through to enterprise grade ETL pipelines. It is generally quicker
    to build a pipeline and download files with NiFi than even writing a quick bash
    script, adding in the feature-rich processors used for this and it makes for a
    compelling proposition.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Apache NiFi起源于美国国家安全局（NSA），并于2014年作为其技术转移计划的一部分发布为开源项目。NiFi能够在简单的用户界面中生成可扩展的数据路由和转换有向图。它还支持数据溯源，各种预构建的处理器以及快速高效地构建新处理器的能力。它包括优先级设置、可调的交付容忍度和反压功能，允许用户根据特定要求调整处理器和管道，甚至允许在运行时修改流程。所有这些都使其成为一个非常灵活的工具，可以构建从一次性文件下载数据流到企业级ETL管道的所有内容。通常使用NiFi构建管道和下载文件比编写快速的bash脚本甚至更快，加上用于此目的的功能丰富的处理器，这使其成为一个引人注目的选择。
- en: Advantages
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优点
- en: 'The following are the advantages:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是优点：
- en: Wide range of processors
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广泛的处理器范围
- en: Hub and spoke architecture
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集线器和辐射结构
- en: '**Graphical User Interface** (**GUI**)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图形用户界面（GUI）
- en: Scalable
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展
- en: Simplifies parallel processing
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化并行处理
- en: Simplifies thread handling
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化线程处理
- en: Allows runtime modifications
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许运行时修改
- en: Redundancy through clusters
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过集群实现冗余
- en: Disadvantages
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: 'The following are the disadvantages:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是缺点：
- en: No cross-cutting error handler
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有横切错误处理程序
- en: Expression language is only partially implemented
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表达语言只有部分实现
- en: Flowfile version management lacking
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流文件版本管理不足
- en: Installation
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: Apache NiFi can be installed with Hortonworks and is known as Hortonworks Dataflow.
    It is also available as a standalone install from Apache, [https://nifi.apache.org/](https://nifi.apache.org/).
    There is an introduction to NiFi in [Chapter 2](ch02.xhtml "Chapter 2. Data Acquisition"),
    *Data Acquisition*.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 'Apache NiFi可以与Hortonworks一起安装，称为Hortonworks Dataflow。它也可以作为Apache的独立安装程序使用，[https://nifi.apache.org/](https://nifi.apache.org/)。在[第2章](ch02.xhtml
    "第2章 数据采集") *数据采集*中有关NiFi的介绍。 '
- en: Apache YARN
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache YARN
- en: 'YARN is the principle component of Hadoop 2.0, which essentially allows Hadoop
    to plug in processing paradigms rather than being limited to just the original
    MapReduce. YARN consists of three main components: the resource manager, node
    manager, and application manager. It is out of the scope of this book to dive
    into YARN; the main thing to understand is that if we are running a Hadoop cluster,
    then our Spark jobs can be executed using YARN in client mode, as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: YARN是Hadoop 2.0的主要组件，它基本上允许Hadoop插入处理范式，而不仅仅限于原始的MapReduce。YARN由三个主要组件组成：资源管理器、节点管理器和应用程序管理器。本书不涉及深入研究YARN；主要要理解的是，如果我们运行Hadoop集群，那么我们的Spark作业可以在客户端模式下使用YARN执行，如下所示：
- en: '[PRE6]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Advantages
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优点
- en: 'The following are the advantages:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是优点：
- en: Supports Spark
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持Spark
- en: Supports prioritized scheduling
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持优先级调度
- en: Supports data locality
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持数据本地性
- en: Job history archive
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业历史存档
- en: Works out of the box with HDP
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与HDP一起开箱即用
- en: Disadvantages
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: 'The following are the disadvantages:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是缺点：
- en: No CPU resource control
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有CPU资源控制
- en: No support for data lineage
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持数据谱系
- en: Installation
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: 'YARN is installed as part of Hadoop; this could either be Hortonworks HDP,
    Apache Hadoop, or one of the other vendors. In any case, we should install Hadoop
    with at least the following components:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: YARN作为Hadoop的一部分安装；这可以是Hortonworks HDP，Apache Hadoop，或其他供应商之一。无论如何，我们应该至少安装带有以下组件的Hadoop：
- en: ResourceManager
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源管理器
- en: NodeManager (1 or more)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NodeManager（1个或更多）
- en: To ensure that Spark can use YARN, it simply needs to know the location of `yarn-site.xml`,
    which is set using the `YARN_CONF_DIR` parameter in your Spark configuration.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保Spark可以使用YARN，它只需要知道`yarn-site.xml`的位置，这是通过在Spark配置中使用`YARN_CONF_DIR`参数设置的。
- en: Apache Lucene
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Lucene
- en: Lucene is an indexing and search library tool originally built with Java, but
    now ported to several other languages, including Python. Lucene has spawned a
    number of subprojects in its time, including Mahout, Nutch, and Tika. These have
    now become top-level Apache projects in their own right while Solr has more recently
    joined as a subproject. Lucene has a comprehensive capability, but is particularly
    known for its use in Q&A search engines and information-retrieval systems.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Lucene是一个最初用Java构建的索引和搜索库工具，但现在已经移植到其他几种语言，包括Python。 Lucene在其时间内产生了许多子项目，包括Mahout，Nutch和Tika。这些现在已成为自己的顶级Apache项目，而Solr最近作为子项目加入。Lucene具有全面的功能，但尤其以在问答搜索引擎和信息检索系统中的使用而闻名。
- en: Advantages
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优点
- en: 'The following are the advantages:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是优点：
- en: Highly efficient full-text searches
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效的全文搜索
- en: Scalable
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展的
- en: Multilanguage support
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多语言支持
- en: Excellent out-of-the-box functionality
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出色的开箱即用功能
- en: Disadvantages
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: The disadvantage is databases are generally better for relational operations.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点是数据库通常更适合关系操作。
- en: Installation
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: Lucene can be downloaded from [https://lucene.apache.org/](https://lucene.apache.org/)
    if you wish to learn more and interact with the library directly.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望了解更多并直接与库交互，可以从[https://lucene.apache.org/](https://lucene.apache.org/)下载Lucene。
- en: 'When utilizing Lucene, we only really need to include `lucene-core-<version>.jar`
    in our project. For example, when using Maven:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Lucene时，我们只需要在项目中包含`lucene-core-<version>.jar`。例如，使用Maven时：
- en: '[PRE7]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Kibana
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kibana
- en: Kibana is an analytics and visualization platform that also provides charting
    and streaming data summarization. It uses Elasticsearch for its data source (which
    in turn uses Lucene) and can therefore leverage very powerful search and indexing
    capabilities at scale. Kibana can be used to visualize data in many different
    ways, including bar charts, histograms, and maps. We have mentioned Kibana briefly
    towards the end of this chapter and it will be used extensively throughout this
    book.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana是一个分析和可视化平台，还提供图表和流数据汇总。它使用Elasticsearch作为其数据源（反过来使用Lucene），因此可以利用规模上非常强大的搜索和索引功能。Kibana可以以许多不同的方式可视化数据，包括条形图，直方图和地图。我们在本章末尾简要提到了Kibana，并且在本书中将广泛使用它。
- en: Advantages
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优点
- en: 'The following are the advantages:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是优点：
- en: Visualize data at scale
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在规模上可视化数据
- en: Intuitive interface to quickly develop dashboards
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直观的界面，快速开发仪表板
- en: Disadvantages
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: 'The following are the disadvantages:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是缺点：
- en: Only integrates with Elasticsearch
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只与Elasticsearch集成
- en: Kibana releases are tied to specific Elasticsearch versions
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kibana发布与特定的Elasticsearch版本绑定
- en: Installation
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: Kibana can easily be installed as a standalone piece since it has its own web
    server. It can be downloaded from [https://www.elastic.co/downloads/kibana](https://www.elastic.co/downloads/kibana).
    As Kibana requires Elasticsearch, this will also need to be installed; see preceding
    link for more information. The Kibana configuration is handled in `config/kibana.yml`,
    if you have installed a standalone version of Elasticsearch, then no changes are
    required, it will work out of the box!
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana可以作为独立的部分轻松安装，因为它有自己的Web服务器。它可以从[https://www.elastic.co/downloads/kibana](https://www.elastic.co/downloads/kibana)下载。由于Kibana需要Elasticsearch，因此还需要安装Elasticsearch；有关更多信息，请参见前面的链接。Kibana配置在`config/kibana.yml`中处理，如果安装了独立版本的Elasticsearch，则不需要进行任何更改，它将立即运行！
- en: Elasticsearch
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: Elasticsearch is a web-based search engine based on Lucene (see previously).
    It provides a distributed, multitenant-capable full-text search engine with schema-free
    JSON documents. It is built in Java but can be utilized from any language due
    to its HTTP web interface. This makes it particularly useful for transactions
    and/or data-intensive instructions that are to be displayed via web pages.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch是一个基于Lucene（见前文）的基于Web的搜索引擎。它提供了一个分布式的，多租户的，无模式的JSON文档全文搜索引擎。它是用Java构建的，但由于其HTTP
    Web界面，可以从任何语言中利用。这使得它特别适用于要通过网页显示的交易和/或数据密集型指令。
- en: Advantages
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优点
- en: 'The advantages are as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 优点如下：
- en: Distributed
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式
- en: Schema free
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无模式
- en: HTTP interface
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP接口
- en: Disadvantages
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: The disadvantages are as follows
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点如下
- en: Unable to perform distributed transactions
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法执行分布式事务
- en: Lack of frontend tooling
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏前端工具
- en: Installation
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: 'Elasticsearch can be installed from [https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch).
    To provide access to the Rest API, we can import the Maven dependency:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch可以从[https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch)下载。为了提供对Rest
    API的访问，我们可以导入Maven依赖项：
- en: '[PRE8]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'There is also a great tool to help with administering Elasticsearch content.
    Search for the Chrome extension, Sense, at [https://chrome.google.com/webstore/category/extensions](https://chrome.google.com/webstore/category/extensions).
    With a further explanation found at: [https://www.elastic.co/blog/found-sense-a-cool-json-aware-interface-to-elasticsearch](https://www.elastic.co/blog/found-sense-a-cool-json-aware-interface-to-elasticsearch).
    Alternatively, it is available for Kibana at [https://www.elastic.co/guide/en/sense/current/installing.html](https://www.elastic.co/guide/en/sense/current/installing.html).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个很好的工具可以帮助管理Elasticsearch内容。在[https://chrome.google.com/webstore/category/extensions](https://chrome.google.com/webstore/category/extensions)搜索Chrome扩展名Sense。也可以在[https://www.elastic.co/guide/en/sense/current/installing.html](https://www.elastic.co/guide/en/sense/current/installing.html)找到更多解释。或者，它也适用于Kibana，网址为[https://www.elastic.co/guide/en/sense/current/installing.html](https://www.elastic.co/guide/en/sense/current/installing.html)。
- en: Accumulo
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Accumulo
- en: Accumulo is a no-sql database based on Google's Bigtable design and was originally
    developed by the American National Security Agency, subsequently being released
    to the Apache community in 2011\. Accumulo offers us the usual big data advantages
    such as bulk loading and parallel reading but also has some additional capabilities;
    iterators, for efficient server and client side pre-computation, data aggregation
    and, most importantly, cell level security. The security aspect of Accumulo makes
    it very useful for Enterprise usage as it enables flexible security in a multitenant
    environment. Accumulo is powered by Apache Zookeeper, in the same way as Kafka,
    and also leverages Apache Thrift, [https://thrift.apache.org/](https://thrift.apache.org/),
    which enables a cross language **Remote Procedural Call** (**RPC**) capability.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Accumulo是基于Google的Bigtable设计的NoSQL数据库，最初由美国国家安全局开发，随后于2011年发布给Apache社区。Accumulo为我们提供了通常的大数据优势，如批量加载和并行读取，但还具有一些额外的功能；迭代器，用于高效的服务器和客户端预计算，数据聚合，最重要的是单元级安全。Accumulo的安全方面使其在企业使用中非常有用，因为它在多租户环境中实现了灵活的安全性。Accumulo由Apache
    Zookeeper提供支持，与Kafka一样，并且利用了Apache Thrift，[https://thrift.apache.org/](https://thrift.apache.org/)，它实现了跨语言的远程过程调用（RPC）功能。
- en: Advantages
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优点
- en: 'The advantages are as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 优点如下：
- en: Pure implementation of Google Bigtable
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Bigtable的纯实现
- en: Cell level security
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元级安全
- en: Scalable
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展的
- en: Redundancy
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冗余
- en: Provides iterators for server-side computation
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为服务器端计算提供迭代器
- en: Disadvantages
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: 'The disadvantages are as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点如下：
- en: Zookeeper not universally popular with DevOps
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zookeeper在DevOps中并不普遍受欢迎
- en: Not always the most efficient choice for bulk relational operations
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并不总是批量关系操作的最有效选择
- en: Installation
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: Accumulo can be installed as part of the Hortonworks HDP release, or may be
    installed as a standalone instance from [https://accumulo.apache.org/](https://accumulo.apache.org/).
    The instance should then be configured using the installation documentation, at
    the time of writing [https://accumulo.apache.org/1.7/accumulo_user_manual#_installation](https://accumulo.apache.org/1.7/accumulo_user_manual#_installation).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: Accumulo可以作为Hortonworks HDP发布的一部分安装，也可以作为独立实例从[https://accumulo.apache.org/](https://accumulo.apache.org/)安装。然后应根据安装文档进行配置，在撰写本文时为[https://accumulo.apache.org/1.7/accumulo_user_manual#_installation](https://accumulo.apache.org/1.7/accumulo_user_manual#_installation)。
- en: In [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"), *Building Communities*,
    we demonstrate the use of Accumulo with Spark, along with some of the more advanced
    features such as `Iterators` and `InputFormats`. We also show how to work with
    data between Elasticsearch and Accumulo.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.xhtml "Chapter 7. Building Communities")中，*构建社区*，我们演示了Accumulo与Spark的使用，以及一些更高级的功能，如`迭代器`和`输入格式`。我们还展示了如何在Elasticsearch和Accumulo之间处理数据。
- en: Summary
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the idea of data architecture and explained how
    to group responsibilities into capabilities that help manage data throughout its
    lifecycle. We explained that all data handling requires a level of due diligence,
    whether this is enforced by corporate rules or otherwise, and without this, analytics
    and their results can quickly become invalid.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了数据架构的概念，并解释了如何将责任分组为能力，以帮助管理数据的整个生命周期。我们解释了所有数据处理都需要一定程度的尽职调查，无论是由公司规定还是其他方式，没有这一点，分析及其结果很快就会变得无效。
- en: Having scoped our data architecture, we have walked through the individual components
    and their respective advantages/disadvantages, explaining that our choices are
    based upon collective experience. Indeed, there are always options when it comes
    to choosing components and their individual features should always be carefully
    considered before any commitment.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了我们的数据架构范围后，我们已经详细介绍了各个组件及其各自的优缺点，并解释了我们的选择是基于集体经验的。事实上，在选择组件时总是有选择的，他们各自的特性在做出任何承诺之前都应该仔细考虑。
- en: In the next chapter, we will dive deeper into how to source and capture data.
    We will advise on how to bring data onto the platform and discuss aspects related
    to processing and handling data through a pipeline.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨如何获取和捕获数据。我们将建议如何将数据带入平台，并讨论与数据处理和处理相关的方面。
