- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Unsupervised Machine Learning Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督机器学习算法
- en: This chapter is about unsupervised machine learning algorithms. We aim, by the
    end of this chapter, to be able to understand how unsupervised learning, with
    its basic algorithms and methodologies, can be effectively applied to solve real-world
    problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讲解的是无监督机器学习算法。我们的目标是在本章结束时，能够理解无监督学习及其基本算法和方法如何有效应用于解决现实世界中的问题。
- en: 'We will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Introducing unsupervised learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍无监督学习
- en: Understanding clustering algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解聚类算法
- en: Dimensionality reduction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维
- en: Association rules mining
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关联规则挖掘
- en: Introducing unsupervised learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍无监督学习
- en: If the data is not generated randomly, it tends to exhibit certain patterns
    or relationships among its elements within a multi-dimensional space. Unsupervised
    learning involves the process of detecting and utilizing these patterns within
    a dataset to structure and comprehend it more effectively. Unsupervised learning
    algorithms uncover these patterns and use them as a foundation for imparting a
    certain structure to the dataset. The identification of these patterns contributes
    to a deeper understanding and representation of the data. Extracting patterns
    from raw data leads to a better understanding of the raw data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据不是随机生成的，它往往在多维空间中表现出某些元素之间的模式或关系。无监督学习涉及在数据集中检测并利用这些模式，以便更有效地对其进行结构化和理解。无监督学习算法揭示这些模式，并利用它们作为赋予数据集特定结构的基础。识别这些模式有助于更深入地理解和呈现数据。从原始数据中提取模式有助于更好地理解原始数据。
- en: 'This concept is shown in *Figure 6.1*:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该概念如*图6.1*所示：
- en: '![Shape, arrow  Description automatically generated](img/B18046_06_01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![形状，箭头 说明自动生成](img/B18046_06_01.png)'
- en: 'Figure 6.1: Using unsupervised machine learning to extract patterns from unlabeled
    raw data'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：使用无监督机器学习从未标记的原始数据中提取模式
- en: In the upcoming discussion, we will navigate through the CRISP-DM lifecycle,
    a popular model for the machine learning process. Within this context, we’ll pinpoint
    where unsupervised learning fits in. To illustrate, think of unsupervised learning
    like a detective piecing together clues to form patterns or groups, without having
    any predefined knowledge of what the end result might be. Just as a detective’s
    insights can be crucial in solving a case, unsupervised learning plays a pivotal
    role in the machine learning lifecycle.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的讨论中，我们将穿越CRISP-DM生命周期，这是一种流行的机器学习过程模型。在这个背景下，我们将确定无监督学习适合的位置。举例来说，可以将无监督学习比作侦探在没有预设知识的情况下，通过拼凑线索来形成模式或群组。就像侦探的洞察力对破案至关重要一样，无监督学习在机器学习生命周期中也扮演着关键角色。
- en: Unsupervised learning in the data-mining lifecycle
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习在数据挖掘生命周期中的作用
- en: Let us first look into the different phases of a typical machine learning process.
    To understand the different phases of the machine learning lifecycle, we will
    study the example of using machine learning for a data mining process. Data mining
    is the process of discovering meaningful correlations, patterns, and trends in
    a given dataset. To discuss the different phases of data mining using machine
    learning, this book utilizes the **Cross-Industry Standard Process for Data Mining**
    (**CRISP-DM**). CRISP-DM was conceived and brought to life by a group of data
    miners from different organizations, including notable names like Chrysler and
    IBM. More details can be found at [https://www.ibm.com/docs/en/spss-modeler/saas?topic=dm-crisp-help-overview](https://www.ibm.com/docs/en/spss-modeler/saas?topic=dm-crisp-help-overview).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们了解一下典型机器学习过程的不同阶段。为了理解机器学习生命周期的不同阶段，我们将通过使用机器学习进行数据挖掘的例子来学习。数据挖掘是从给定数据集中发现有意义的相关性、模式和趋势的过程。为了讨论使用机器学习进行数据挖掘的不同阶段，本书采用了**跨行业数据挖掘标准流程**（**CRISP-DM**）。CRISP-DM由来自不同组织的数据挖掘专家们设计并实现，包括克莱斯勒和IBM等知名公司。更多详情请参考[https://www.ibm.com/docs/en/spss-modeler/saas?topic=dm-crisp-help-overview](https://www.ibm.com/docs/en/spss-modeler/saas?topic=dm-crisp-help-overview)。
- en: 'The CRISP-DM lifecycle consists of six distinct phases, which are shown in
    the following figure:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CRISP-DM 生命周期包含六个不同的阶段，见下图：
- en: '![Diagram  Description automatically generated](img/B18046_06_02.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图示 说明自动生成](img/B18046_06_02.png)'
- en: 'Figure 6.2: Different phases of the CRISP-DM lifecycle'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：CRISP-DM 生命周期的不同阶段
- en: Let’s break down and explore each phase, one by one.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一解析并探索每个阶段。
- en: 'Phase 1: Business understanding'
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶段 1：业务理解
- en: This phase is about gathering the requirements and involves trying to fully
    understand the problem in depth from a business point of view. Defining the scope
    of the problem and properly rephrasing it according to machine learning is an
    important part of this phase. This phase involves identifying the goals, defining
    the scope of the project, and understanding the requirements of the stakeholders.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本阶段的重点是收集需求，并尝试从业务角度深入理解问题。定义问题的范围并根据机器学习适当重新表述它是这一阶段的重要部分。本阶段包括确定目标、定义项目的范围，并理解利益相关者的需求。
- en: It is important to note that Phase 1 of the CRISP-DM lifecycle is about business
    understanding. It focuses on what needs to be done, not on how it will be done.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，CRISP-DM 生命周期的第一阶段是关于业务理解。它关注的是需要做什么，而不是如何做。
- en: 'Phase 2: Data understanding'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶段 2：数据理解
- en: This phase is about understanding the data that is available for data mining.
    In this phase, we will find out whether we have all information needed to solve
    the problem defined in Phase 1 in the given datasets. We can use tools like data
    visualization, dashboards, and summary reports to understand the patterns in the
    data. As explained later in this chapter, unsupervised machine learning algorithms
    can also be used to discover the patterns in the data and to understand them by
    analyzing their structure in detail.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本阶段的重点是理解可用于数据挖掘的数据。在此阶段，我们将了解给定的数据集中是否拥有解决第一阶段定义的问题所需的所有信息。我们可以使用数据可视化、仪表板和汇总报告等工具来理解数据中的模式。如本章后面所述，无监督机器学习算法也可以用于发现数据中的模式，并通过详细分析其结构来理解这些模式。
- en: 'Phase 3: Data preparation'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶段 3：数据准备
- en: This is about preparing the data for the ML model that we will later train in
    Phase 4\. Depending on the use case and requirements, data preparation may include
    removing outliers, normalization, taking out null values, and reducing the dimensionality
    of the data. This is discussed in more detail in later chapters. After processing
    and preparing the data, it is usually split in a 70-30 ratio. The larger chunk,
    called the training data, is used to educate the model on various patterns, while
    the smaller chunk, referred to as the testing data, is saved for evaluating the
    model’s performance on unseen data during Phase 5\. An optional set of data can
    also be kept aside for validating and fine-tuning the model to prevent it from
    overfitting.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及为我们将在阶段 4 中训练的机器学习模型准备数据。根据使用案例和需求，数据准备可能包括去除异常值、归一化、去除空值以及减少数据的维度。后续章节会更详细地讨论这些内容。数据处理和准备好后，通常会以
    70-30 的比例进行分割。较大的一部分，称为训练数据，用于训练模型识别各种模式，而较小的一部分，称为测试数据，则保存以评估模型在阶段 5 中对未见数据的表现。还可以选择保留一部分数据用于验证和微调模型，以防止过拟合。
- en: 'Phase 4: Modeling'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶段 4：建模
- en: This is the phase where we formulate the patterns in the data by training the
    model. For model training, we will use the training data partition prepared in
    Phase 3\. Model training involves feeding our prepared data into the machine learning
    algorithm. Through iterative learning, the algorithm identifies and learns the
    inherent patterns within the data. The objective is to formulate patterns representing
    the relationships and dependencies among different variables in the dataset. We
    will discuss in later chapters how the complexity and nature of these mathematical
    formulations depend heavily on our chosen algorithm – for instance, a linear regression
    model will generate a linear equation, while a decision tree model will construct
    a tree-like model of decisions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们通过训练模型来形成数据模式的阶段。对于模型训练，我们将使用在阶段 3 中准备的训练数据分区。模型训练包括将我们准备好的数据输入机器学习算法。通过迭代学习，算法识别并学习数据中固有的模式。目标是形成表示数据集中不同变量之间关系和依赖性的模式。我们将在后续章节中讨论这些数学公式的复杂性和性质如何在很大程度上依赖于我们选择的算法——例如，线性回归模型将生成一个线性方程，而决策树模型将构建一个类似树的决策模型。
- en: In addition to model training, model tuning is another component of this phase
    of the CRISP-DM lifecycle. This process includes optimizing the parameters of
    the learning algorithm to enhance its performance, thus making predictions more
    accurate. It involves fine-tuning the model using an optional validation set,
    which assists in adjusting the model’s complexity to find the right balance between
    learning from the data and generalizing to unseen data. A validation set, in machine
    learning terms, is a subset of your dataset that is used for the fine adjustment
    of a predictive model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型训练，模型调优也是 CRISP-DM 生命周期这一阶段的一个组成部分。该过程包括优化学习算法的参数，以提高其性能，从而使预测更为准确。它涉及使用可选的验证集对模型进行微调，帮助调整模型的复杂性，找到从数据中学习与对未见数据进行概括之间的平衡。在机器学习中，验证集是数据集的一部分，用于对预测模型进行精细调整。
- en: It assists in modulating the model’s complexity, aiming to find an optimal balance
    between learning from known data and generalizing to unseen data. This balance
    is important in preventing overfitting, which is a scenario where the model learns
    the training data too well but performs poorly on new, unseen data. Hence, model
    tuning not only refines the model’s predictive power but also ensures its robustness
    and reliability.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 它有助于调整模型的复杂性，旨在找到已知数据学习与对未见数据进行概括之间的最佳平衡。这种平衡对于防止过拟合至关重要，过拟合是指模型过于“记住”训练数据，从而在新数据上表现不佳。因此，模型调优不仅可以提升模型的预测能力，还可以确保其稳健性和可靠性。
- en: 'Phase 5: Evaluation'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶段 5：评估
- en: This stage involves evaluating the recently trained model by using the test
    data derived from Phase 3\. We measure the model’s performance against the established
    baseline, which is set during Phase 1\. Setting a baseline in machine learning
    serves as a reference point, which can be determined using various methods. It
    could be established through basic rule-based systems, simple statistical models,
    random chance, or even based on the performance of human experts. The purpose
    of this baseline is to offer a minimal performance threshold that our machine
    learning models should surpass. The baseline acts as a benchmark for comparison,
    giving us a reference point for our expectations. If the model’s evaluation aligns
    with the expectations originally defined in Phase 1, we proceed further. If not,
    we must revisit and iterate through all the previous phases, starting again with
    Phase 1.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这一阶段涉及通过使用来自阶段 3 的测试数据来评估最近训练的模型。我们将模型的性能与阶段 1 中设定的基准进行对比。基准在机器学习中的作用是提供一个参考点，可以通过多种方法来确定。它可以通过基本的基于规则的系统、简单的统计模型、随机机会，甚至是基于人类专家的表现来建立。基准的目的是提供一个最小的性能门槛，我们的机器学习模型应该超越这个门槛。基准充当比较的标准，给我们提供一个期望的参考点。如果模型的评估结果与阶段
    1 中最初定义的期望一致，我们就可以继续。如果不一致，我们必须重新审视并迭代所有之前的阶段，从阶段 1 开始。
- en: 'Phase 6: Deployment'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶段 6：部署
- en: Once the evaluation phase, Phase 5, concludes, we examine whether the performance
    of the trained model meets or surpasses the established expectations. It’s vital
    to remember that a successful evaluation doesn’t automatically imply readiness
    for deployment. The model has performed well on our test data, but that is not
    the only criterion for determining whether the model is ready to solve real-world
    problems, as defined in Phase 1\. We must consider factors such as how the model
    will perform with new data it has never seen before, how it will integrate with
    existing systems, and how it will handle unforeseen edge cases. Therefore, it’s
    only when these extensive evaluations have been met satisfactorily that we can
    confidently proceed to deploy the model into a production environment, where it
    begins to provide a usable solution to our predefined problem.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦评估阶段（阶段 5）结束，我们就会检查训练好的模型的表现是否达到或超过了既定的期望。需要记住的是，成功的评估并不自动意味着可以部署。模型在我们的测试数据上表现良好，但这并不是判断模型是否准备好解决实际问题的唯一标准，如阶段
    1 所定义的那样。我们必须考虑诸如模型在从未见过的新数据上的表现、如何与现有系统集成以及如何处理未预见的边缘情况等因素。因此，只有当这些广泛的评估得到了令人满意的验证时，我们才能自信地将模型部署到生产环境中，在那里它开始为我们预先定义的问题提供可用的解决方案。
- en: Phase 2 (Data understanding) and Phase 3 (Data preparation) of the CRISP-DM
    lifecycle are all about understanding the data and preparing it for training the
    model. These phases involve data processing. Some organizations employ specialists
    for this data engineering phase.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: CRISP-DM 生命周期的第 2 阶段（数据理解）和第 3 阶段（数据准备）都是关于理解数据并为训练模型做准备。这些阶段涉及数据处理。一些组织为此数据工程阶段配备了专门的人员。
- en: It is obvious that the process of suggesting a solution to a problem is fully
    data-driven. A combination of supervised and unsupervised machine learning is
    used to formulate a workable solution. This chapter focuses on the unsupervised
    learning part of the solution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，提出问题解决方案的过程是完全数据驱动的。结合有监督和无监督机器学习来制定可行的解决方案。本章专注于解决方案中的无监督学习部分。
- en: 'Data engineering comprises Phase 2 and Phase 3 and is the most time-consuming
    part of machine learning. It can take as much as 70% of the time and resources
    of a typical **Machine Learning** (**ML**) project (*Data Management in Machine
    Learning: Challenges, Techniques, and Systems*, Cody et al, SIGMOD ‘17: Proceedings
    of the 2017 ACM International Conference on Management of Data, May 2017). Unsupervised
    learning algorithms can play an important role in data engineering.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '数据工程包括第 2 阶段和第 3 阶段，是机器学习中最耗时的部分。它可能占据典型**机器学习**（**ML**）项目的 70% 的时间和资源（*Data
    Management in Machine Learning: Challenges, Techniques, and Systems*, Cody 等人，SIGMOD
    ‘17：2017 年 ACM 国际数据管理会议论文集，2017 年 5 月）。无监督学习算法在数据工程中可以发挥重要作用。'
- en: The following sections provide more details regarding unsupervised algorithms.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分提供了关于无监督算法的更多详细信息。
- en: Current research trends in unsupervised learning
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当前无监督学习的研究趋势
- en: The field of machine learning research has undergone a considerable transformation.
    In earlier times, the focus was primarily centered on supervised learning techniques.
    These methods are immediately useful for inference tasks, offering clear advantages
    such as time savings, cost reductions, and discernible improvements in prediction
    accuracy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习研究领域经历了显著的变化。早期的重点主要集中在有监督学习技术上。这些方法对推理任务立即有效，提供了明显的优势，如节省时间、降低成本和提高预测准确性。
- en: Conversely, the intrinsic capabilities of unsupervised machine learning algorithms
    have only gained attention more recently. Unlike their supervised counterparts,
    unsupervised techniques function without direct instructions or preconceived assumptions.
    They are adept at exploring broader “dimensions” or facets in data, thus enabling
    a more comprehensive examination of a dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，无监督机器学习算法的内在能力最近才开始引起关注。与有监督的对应方法不同，无监督技术在没有直接指令或先入为主的假设下运作。它们擅长探索数据中更广泛的“维度”或方面，从而使数据集的审查更加全面。
- en: To clarify, in machine learning terminology, “features” are the individual measurable
    properties or characteristics of the phenomena being observed. For example, in
    a dataset concerning customer information, features could be aspects like the
    customer’s age, purchase history, or browsing behavior. “Labels,” on the other
    hand, represent the outcomes we want the model to predict based on these features.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清，在机器学习术语中，“特征”是被观察现象的个体可度量的属性或特征。例如，在一个涉及客户信息的数据集中，特征可能包括客户的年龄、购买历史或浏览行为。“标签”则代表我们希望模型根据这些特征预测的结果。
- en: While supervised learning focuses primarily on establishing relationships between
    these features and a specific label, unsupervised learning does not restrict itself
    to a pre-determined label. Instead, it can delve deeper, unearthing intricate
    patterns among various features that might be overlooked when using supervised
    methods. This makes unsupervised learning potentially more expansive and versatile
    in its applications.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有监督学习主要集中于建立这些特征与特定标签之间的关系，但无监督学习并不局限于预先确定的标签。相反，它可以更深入地挖掘，发现各种特征之间的复杂模式，这些模式在使用有监督方法时可能会被忽视。这使得无监督学习在应用中具有更广阔和灵活的潜力。
- en: This inherent flexibility of unsupervised learning, however, brings with it
    a challenge. Since the exploration space is larger, it can often result in **increased
    computational** requirements, leading to greater costs and longer processing times.
    Furthermore, managing the scale or “scope” of unsupervised learning tasks can
    be more complex due to their exploratory nature. Yet, the ability to unearth hidden
    patterns or correlations within the data makes unsupervised learning a powerful
    tool for data-driven insights.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无监督学习固有的灵活性带来了一个挑战。由于探索空间更大，它往往会导致**更高的计算**需求，从而带来更高的成本和更长的处理时间。此外，由于其探索性特点，管理无监督学习任务的规模或“范围”可能更为复杂。然而，能够挖掘数据中隐藏的模式或关联，使得无监督学习成为数据驱动洞察的强大工具。
- en: Today, research trends are moving toward the integration of supervised and unsupervised
    learning methods. This combined strategy aims to exploit the advantages of both
    methods.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，研究趋势正朝着监督学习和无监督学习方法的整合发展。这种结合策略旨在利用两种方法的优势。
- en: Now let us look into some practical examples.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一些实际的例子。
- en: Practical examples
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际示例
- en: Currently, unsupervised learning is used to get a better sense of the data and
    provide it with more structure—for example, it is used in marketing segmentation,
    data categorization, fraud detection, and market basket analysis (which is discussed
    later in this chapter). Let us look at the example of the use of unsupervised
    learning for marketing segmentation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，无监督学习用于更好地理解数据并为其提供更多结构——例如，它被用于市场细分、数据分类、欺诈检测和市场篮分析（本章稍后讨论）。让我们看一下无监督学习在市场细分中的应用示例。
- en: Marketing segmentation using unsupervised learning
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用无监督学习进行市场细分
- en: Unsupervised learning serves as a powerful tool for marketing segmentation.
    Marketing segmentation refers to the process of dividing a target market into
    distinct groups based on shared characteristics, enabling companies to tailor
    their marketing strategies and messages to effectively reach and engage specific
    customer segments. The characteristics used for grouping the target market could
    include demographics, behaviors, or geographic similarities. By leveraging algorithms
    and statistical techniques, it enables businesses to extract meaningful insights
    from their customer data, identify hidden patterns, and group customers into distinct
    segments based on similarities in their behavior, preferences, or characteristics.
    This data-driven approach empowers marketers to develop tailored strategies, improve
    customer targeting, and enhance overall marketing effectiveness.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习作为市场细分的强大工具。市场细分是指根据共享特征将目标市场划分为不同组的过程，使公司能够量身定制其营销策略和信息，有效地接触并吸引特定的客户群体。用于分组目标市场的特征可能包括人口统计、行为或地理相似性。通过利用算法和统计技术，它使企业能够从客户数据中提取有意义的见解，识别隐藏的模式，并根据客户行为、偏好或特征的相似性将其分为不同的群体。这种数据驱动的方法使营销人员能够制定量身定制的策略、提高客户定位能力，并增强整体营销效果。
- en: Understanding clustering algorithms
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解聚类算法
- en: One of the simplest and most powerful techniques used in unsupervised learning
    is based on grouping similar patterns together through clustering algorithms.
    It is used to understand a particular aspect of the data that is related to the
    problem we are trying to solve. Clustering algorithms look for natural grouping
    in data items. As the group is not based on any target or assumptions, it is classified
    as an unsupervised learning technique.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习中最简单且最强大的技术之一是通过聚类算法将相似的模式组合在一起。它用于理解与我们要解决的问题相关的特定数据方面。聚类算法寻找数据项中的自然分组。由于该分组不基于任何目标或假设，因此它被归类为无监督学习技术。
- en: Consider a vast library full of books as an example. Each book represents a
    data point – containing a multitude of attributes like genre, author, publication
    year, and so forth. Now, imagine a librarian (the clustering algorithm) who is
    tasked with organizing these books. With no pre-existing categories or instructions,
    the librarian starts sorting the books based on their attributes – all the mysteries
    together, the classics together, books by the same author together, and so on.
    This is what we mean by “natural groups” in data items, where items that share
    similar characteristics are grouped together.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个充满书籍的巨大图书馆为例。每本书代表一个数据点——包含诸如类型、作者、出版年份等多种属性。现在，想象一个图书管理员（聚类算法），他被要求对这些书籍进行分类。在没有预设类别或说明的情况下，图书管理员开始根据书籍的属性对其进行分类——所有的侦探小说放在一起，经典文学放在一起，同一作者的书籍放在一起，等等。这就是我们所说的“自然组”，即那些共享相似特征的数据项被聚集在一起。
- en: Groupings created by various clustering algorithms are based on finding the
    similarities between various data points in the problem space. Note that, in the
    context of machine learning, a data point is a set of measurements or observations
    that exist in a multi-dimensional space. In simpler terms, it’s a single piece
    of information that helps the machine learn about the task it is trying to accomplish.
    The best way to determine the similarities between data points will vary from
    problem to problem and will depend on the nature of the problem we are dealing
    with. Let’s look at the various methods that can be used to calculate the similarities
    between various data points.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 各种聚类算法所创建的分组是基于在问题空间中找到各个数据点之间的相似性。请注意，在机器学习的背景下，数据点是存在于多维空间中的一组测量值或观察结果。简单来说，它是帮助机器了解其所要完成任务的单一信息。确定数据点之间相似性的最佳方法因问题而异，且取决于我们所处理问题的性质。让我们看看可以用来计算数据点之间相似性的各种方法。
- en: Quantifying similarities
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 量化相似性
- en: Unsupervised learning techniques, such as clustering algorithms, work effectively
    by determining similarities between various data points within a given problem
    space. The effectiveness of these algorithms largely depends on our ability to
    correctly measure these similarities, and in machine learning terminology, these
    are often referred to as “distance measures.” But what exactly is a distance measure?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习技术，如聚类算法，通过在给定问题空间内确定各种数据点之间的相似性，能够有效工作。这些算法的有效性在很大程度上取决于我们是否能够正确地衡量这些相似性，在机器学习术语中，这些常常被称为“距离度量”。那么，究竟什么是距离度量呢？
- en: In essence, a distance measure is a mathematical formula or method that calculates
    the “distance” or similarity between two data points. It’s crucial to understand
    that, in this context, the term “distance” doesn’t refer to physical distance,
    but rather to the similarity or dissimilarity between data points based on their
    features or characteristics.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，距离度量是一个数学公式或方法，用于计算两个数据点之间的“距离”或相似度。在这个背景下，重要的是要理解，“距离”并不指物理距离，而是基于数据点的特征或属性来衡量相似性或差异性。
- en: 'In clustering, we can talk about two main types of distances: intercluster
    and intracluster. The intercluster distance refers to the distance between different
    clusters, or groups of data points. In contrast, intracluster distance refers
    to the distance within the same cluster, or, in other words, the distance between
    data points within the same group. The objective of a good clustering algorithm
    is to maximize intercluster distance (making sure each cluster is distinct from
    the others) while minimizing intracluster distance (ensuring data points within
    the same cluster are as similar as possible). The following are three of the most
    popular methods that are used to quantify similarities:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，我们可以讨论两种主要的距离类型：簇间距离和簇内距离。簇间距离指的是不同簇（或数据点组）之间的距离。与此相对，簇内距离指的是同一簇内（或者说，同一组内）数据点之间的距离。一个好的聚类算法的目标是最大化簇间距离（确保每个簇彼此独立），同时最小化簇内距离（确保同一簇内的数据点尽可能相似）。以下是三种最常用的量化相似性的方法：
- en: Euclidean distance measure
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧几里得距离度量
- en: Manhattan distance measure
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曼哈顿距离度量
- en: Cosine distance measure
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦距离度量
- en: Let’s look at these distance measures in more detail.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解这些距离度量。
- en: Euclidean distance
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: The distance between different points can quantify the similarity between two
    data points and is extensively used in unsupervised machine learning techniques,
    such as clustering. Euclidean distance is the most common and simplest distance
    measure used. The term “distance,” in this context, quantifies how similar or
    different two data points are in a multi-dimensional space, which is crucial in
    understanding the grouping of data points. One of the simplest and most widely
    used measures of this distance is the Euclidean distance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 不同点之间的距离可以量化两个数据点之间的相似性，并广泛应用于无监督机器学习技术，如聚类。欧几里得距离是最常用且最简单的距离度量。这里所说的“距离”量化的是两个数据点在多维空间中相似或不同的程度，这对于理解数据点的分组至关重要。最简单且最广泛使用的距离度量之一就是欧几里得距离。
- en: The Euclidean distance can be thought of as the straight-line distance between
    two points in a three-dimensional space, similar to how we might measure distance
    in the real world. For example, consider two cities on a map; the Euclidean distance
    would be the “as-the-crow-flies” distance between these two cities, a straight
    line from city A to city B, ignoring any potential obstacles such as mountains
    or rivers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离可以被认为是三维空间中两个点之间的直线距离，类似于我们在现实世界中测量距离的方式。例如，考虑地图上的两座城市；欧几里得距离就是这两座城市之间的“鸟飞直线”距离，即从城市A到城市B的直线距离，忽略了任何可能的障碍物，如山脉或河流。
- en: 'In a similar manner, in the multi-dimensional space of our data, the Euclidean
    distance calculates the shortest possible “straight line” distance between two
    data points. By doing so, it provides a quantitative measure of how close or far
    apart the data points are, based on their features or attributes. For example,
    let’s consider two points, `A(1,1)` and `B(4,4)`, in a two-dimensional space,
    as shown in the following plot:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在我们数据的多维空间中，欧几里得距离计算的是两个数据点之间最短的“直线”距离。通过这种方式，它提供了一个量化的度量，表示数据点之间的相似性或远近，基于它们的特征或属性。例如，假设我们有两个点，`A(1,1)`和`B(4,4)`，它们位于二维空间中，如下图所示：
- en: '![Chart  Description automatically generated](img/B18046_06_03.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图表 说明自动生成](img/B18046_06_03.png)'
- en: 'Figure 6.3: Calculating the Euclidean distance between two given points'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3：计算两个给定点之间的欧几里得距离
- en: 'To calculate the distance between `A` and `B`—that is `d(A,B)`, we can use
    the following Pythagorean formula:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算`A`和`B`之间的距离——即`d(A,B)`，我们可以使用以下勾股定理公式：
- en: '![](img/B18046_06_001.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_06_001.png)'
- en: 'Note that this calculation is for a two-dimensional problem space. For an *n*-dimensional
    problem space, we can calculate the distance between two points, **A** and **B**,
    as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个计算适用于二维问题空间。对于*n*维问题空间，我们可以按照以下方式计算两个点**A**和**B**之间的距离：
- en: '![](img/B18046_06_002.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_06_002.png)'
- en: Manhattan distance
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 曼哈顿距离
- en: In many situations, measuring the shortest distance between two points using
    the Euclidean distance measure will not truly represent the similarity or closeness
    between two points—for example, if two data points represent locations on a map,
    then the actual distance from point A to point B using ground transportation,
    such as a car or taxi, will be more than the distance calculated by the Euclidean
    distance. Let’s think of a bustling city grid, where you can’t cut straight through
    buildings to get from one point to another (like in the case of Euclidean distance),
    but rather, you must navigate through the grid of streets. Manhattan distance
    mirrors this real-world navigation – it calculates the total distance traveled
    along these grid lines from point A to point B.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，使用欧几里得距离度量两点之间的最短距离并不能真正代表两点之间的相似性或接近性——例如，如果两个数据点代表地图上的位置，那么通过陆地交通工具（如汽车或出租车）从A点到B点的实际距离将比通过欧几里得距离计算的距离更远。想象一下一个繁忙的城市网格，在这里你不能像欧几里得距离那样穿越建筑物从一个点到另一个点，而是必须在街道网格中导航。曼哈顿距离就像是这种现实世界中的导航——它计算的是沿着这些网格线从点A到点B所走的总距离。
- en: 'For situations such as these, we use Manhattan distance, which estimates the
    distance between two points, traveled when moving along grid-like city streets
    from a starting point to a destination. In contrast to straight-line distance
    measures like the Euclidean distance, the Manhattan distance provides a more accurate
    reflection of the practical distance between two locations in such contexts. The
    comparison between the Manhattan and Euclidean distance measures is shown in the
    following plot:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类似的情况，我们使用曼哈顿距离，它估算了在城市街道上沿网格状街道从起点到目的地时两点之间的距离。与欧几里得距离等直线距离度量不同，曼哈顿距离在这种情况下能更准确地反映两个位置之间的实际距离。曼哈顿距离和欧几里得距离度量之间的比较如下面的图所示：
- en: '![Chart, line chart  Description automatically generated](img/B18046_06_04.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 说明自动生成](img/B18046_06_04.png)'
- en: 'Figure 6.4: Calculating the Manhattan distance between two points'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：计算两个点之间的曼哈顿距离
- en: Note that, in the figure, the Manhattan distance between these points is represented
    as a zigzag path that moves strictly along the grid lines of this plot. In contrast,
    the Euclidean distance is shown as a direct, straight line from point A to point
    B. It is obvious that the Manhattan distance will always be equal to or larger
    than the corresponding Euclidean distance calculated.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在图中，这些点之间的曼哈顿距离表示为严格沿着该图网格线的折线路径。相比之下，欧几里得距离则显示为从A点到B点的直线。显然，曼哈顿距离总是等于或大于对应的欧几里得距离。
- en: Cosine distance
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 余弦距离
- en: While Euclidean and Manhattan distance measures serve us well in simpler, lower-dimensional
    spaces, their effectiveness diminishes as we venture into more complex, “high-dimensional”
    settings. A “high-dimensional” space refers to a dataset that contains a large
    number of features or variables. As the number of dimensions (features) increases,
    the calculation of distance becomes less meaningful and more computationally intensive
    with Euclidean and Manhattan distances.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管欧几里得和曼哈顿距离度量在简单的低维空间中表现良好，但随着我们进入更加复杂的“高维”环境，它们的效果会减少。“高维”空间指的是包含大量特征或变量的数据集。随着维度（特征）的增加，使用欧几里得和曼哈顿距离计算的距离变得越来越没有意义，且计算负担更加繁重。
- en: To tackle this issue, we use the “cosine distance” measure in high-dimensional
    contexts. This measure works by assessing the cosine of the angle formed by two
    data points connected to an origin point. It’s not the physical distance between
    the points that matters here, but the angle they create.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们在高维情况下使用“余弦距离”度量。该度量通过评估由两个数据点与原点连接所形成的角度的余弦来工作。在这里，重要的不是两点之间的物理距离，而是它们形成的角度。
- en: 'If the data points are close in the multi-dimensional space, they’ll form a
    smaller angle, regardless of the number of dimensions involved. Conversely, if
    the data points are far apart, the resulting angle will be larger. Hence, cosine
    distance provides a more nuanced measure of similarity in high-dimensional data,
    helping us make better sense of complex data patterns:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据点在多维空间中距离较近，它们将形成一个较小的角度，无论涉及多少维度。相反，如果数据点之间相距较远，产生的角度会更大。因此，余弦距离在高维数据中提供了一个更细致的相似度度量，帮助我们更好地理解复杂的数据模式：
- en: '![Chart  Description automatically generated](img/B18046_06_05.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图表 说明自动生成](img/B18046_06_05.png)'
- en: 'Figure 6.5: Calculating the cosine distance'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：计算余弦距离
- en: Textual data can almost be considered a highly dimensional space. It stems from
    the unique nature of text data, where each unique word can be considered a distinct
    dimension or feature. As the cosine distance measure works very well with h-dimensional
    spaces, it is a good choice when dealing with textual data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据几乎可以被视为一个高维空间。这源于文本数据的独特性质，每个独特的单词可以被视为一个不同的维度或特征。由于余弦距离度量在高维空间中表现得非常好，因此它在处理文本数据时是一个不错的选择。
- en: Note that, in the preceding figure, the cosine of the angle between `A(2,5)`
    and `B(4.4)` is the cosine distance represented by ![](img/B18046_06_003.png)
    in *Figure 6.5*. The reference between these points is the origin—that is, `X(0,0)`.
    But in reality, any point in the problem space can act as the reference data point,
    and it does not have to be the origin.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的图中，`A(2,5)`和`B(4.4)`之间的角度余弦就是由*图 6.5*中的 ![](img/B18046_06_003.png) 表示的余弦距离。这些点之间的参考点是原点——即`X(0,0)`。但实际上，问题空间中的任何点都可以作为参考数据点，不一定非得是原点。
- en: Let us now look into one of the most popular unsupervised machine learning techniques
    – that is, the k-means clustering algorithm.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看最流行的无监督机器学习技术之一——即 k-means 聚类算法。
- en: k-means clustering algorithm
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means 聚类算法
- en: The k-means clustering algorithm gets its name from the procedure of creating
    “k” clusters and using means or averages to ascertain the “closeness” between
    data points. The term “means” refers to the method of calculating the centroid
    or the “center point” of each cluster, which is essentially the average of all
    the data points within the cluster. In other words, the algorithm calculates the
    mean value for each feature within the cluster, which results in a new data point
    – the centroid. This centroid then acts as the reference point for measuring the
    “closeness” of other data points.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类算法得名于创建“k”个聚类并使用均值或平均值来确定数据点之间的“接近度”的过程。“均值”一词指的是计算每个聚类的质心或“中心点”的方法，质心本质上是聚类内所有数据点的平均值。换句话说，算法会计算聚类内每个特征的均值，从而得出一个新的数据点——质心。然后，这个质心作为衡量其他数据点“接近度”的参考点。
- en: The popularity of k-means stems from its scalability and speed. The algorithm
    is computationally efficient because it uses a straightforward iterative process
    where the centroids of clusters are repeatedly adjusted until they become representative
    of the cluster members. This simplicity makes the algorithm particularly fast
    and scalable for large datasets.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 的流行源于其可扩展性和速度。该算法计算高效，因为它使用一个简单的迭代过程，其中聚类的质心会反复调整，直到它们成为聚类成员的代表。正是这种简单性使得该算法在处理大规模数据集时尤其快速且可扩展。
- en: However, a notable limitation of the k-means algorithm is its inability to determine
    the optimal number of clusters, “k,” independently. The ideal “k” depends on the
    natural groupings within a given dataset. The design philosophy behind this constraint
    is to keep the algorithm straightforward and fast, hence assuming an external
    mechanism to calculate “k.” Depending on the context of the problem, “k” could
    be directly determined. For instance, if the task involves segregating a class
    of data science students into two clusters, one focusing on data science skills
    and the other on programming skills, “k” would naturally be two. However, for
    problems where the value of “k” is not readily apparent, an iterative process
    involving trial and error, or a heuristic-based method, might be required to estimate
    the most suitable number of clusters for a dataset.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，k-means 算法的一个显著限制是它无法独立地确定最佳的聚类数“k”。理想的“k”依赖于给定数据集中的自然分组。这个限制背后的设计理念是保持算法的简洁和快速，因此假设有一个外部机制来计算“k”。根据问题的上下文，“k”可以直接确定。例如，如果任务是将一组数据科学学生分成两个聚类，一个专注于数据科学技能，另一个专注于编程技能，那么“k”自然为二。然而，对于那些“k”的值不容易显现的问题，可能需要通过试错的迭代过程，或基于启发式的方法，来估算最适合的数据集聚类数。
- en: The logic of k-means clustering
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-means 聚类的逻辑
- en: In this part, we’ll dive into the workings of the k-means clustering algorithm.
    We’ll break down how it operates, step by step, to give you a clear understanding
    of its mechanisms and uses. This section describes the logic of the k-means clustering
    algorithm.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将深入探讨 k-means 聚类算法的工作原理。我们将逐步拆解它的操作过程，以帮助你清晰地理解其机制和应用。本节描述了 k-means
    聚类算法的逻辑。
- en: Initialization
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化
- en: In order to group them, the k-means algorithm uses a distance measure to find
    the similarity or closeness between data points. Before using the k-means algorithm,
    the most appropriate distance measure needs to be selected. By default, the Euclidean
    distance measure will be used. However, depending on the nature and requirement
    of your data, you might find another distance measure, such as Manhattan or cosine,
    more suitable. Also, if the dataset has outliers, then a mechanism needs to be
    devised to determine the criteria that are to be identified and remove the outliers
    of the dataset.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行分组，k-means 算法使用距离度量来找出数据点之间的相似性或接近度。在使用 k-means 算法之前，需要选择最合适的距离度量。默认情况下，会使用欧几里得距离度量。然而，根据数据的性质和需求，你可能会发现其他距离度量，如曼哈顿距离或余弦相似度，更为合适。此外，如果数据集存在离群值，则需要设计一个机制来确定哪些标准需要被识别并从数据集中去除离群值。
- en: Various statistical methods are available for outlier detection, such as the
    Z-score method or the **Interquartile Range** (**IQR**) method. Now let’s look
    at the different steps involved in the k-means algorithm.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种统计方法可用于异常值检测，如 Z-score 方法或 **四分位距** (**IQR**) 方法。现在让我们来看一下 k-means 算法中涉及的不同步骤。
- en: The steps of the k-means algorithm
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-means 算法的步骤
- en: 'The steps involved in the k-means clustering algorithm are as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类算法涉及的步骤如下：
- en: '| **Step 1** | We choose the number of clusters, *k*. |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **第1步** | 我们选择簇的数量，*k*。 |'
- en: '| **Step 2** | Among the data points, we randomly choose *k* points as cluster
    centers. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **第2步** | 在数据点中，我们随机选择 *k* 个点作为簇中心。 |'
- en: '| **Step 3** | Based on the selected distance measure, we iteratively compute
    the distance from each point in the problem space to each of the *k* cluster centers.
    Based on the size of the dataset, this may be a time-consuming step—for example,
    if there are 10,000 points in the cluster and *k = 3*, this means that 30,000
    distances need to be calculated. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| **第3步** | 基于选定的距离度量，我们迭代地计算问题空间中每个点到每个 *k* 个簇中心的距离。根据数据集的大小，这可能是一个耗时的步骤——例如，如果簇中有10,000个点，且
    *k = 3*，这意味着需要计算30,000个距离。 |'
- en: '| **Step 4** | We assign each data point in the problem space to the nearest
    cluster center. |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| **第4步** | 我们将问题空间中的每个数据点分配到最近的簇中心。 |'
- en: '| **Step 5** | Now each data point in our problem space has an assigned cluster
    center. But we are not done, as the selection of the initial cluster centers was
    based on random selection. We need to verify that the current randomly selected
    cluster centers are actually the center of gravity of each cluster. We recalculate
    the cluster centers by computing the mean of the constituent data points of each
    of the *k* clusters. This step explains why this algorithm is called *k*-means.
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **第5步** | 现在，我们问题空间中的每个数据点都有一个分配的簇中心。但我们还没有完成，因为初始簇中心的选择是基于随机选择的。我们需要验证当前随机选择的簇中心是否实际上是每个簇的质心。我们通过计算每个
    *k* 个簇的组成数据点的均值来重新计算簇中心。这一步解释了为什么这个算法叫做 *k*-means。 |'
- en: '| **Step 6** | If the cluster centers have shifted in step 5, this means that
    we need to recompute the cluster assignment for each data point. For this, we
    will go back to step 3 to repeat that compute-intensive step. If the cluster centers
    have not shifted or if our predetermined stop condition (for example, the number
    of maximum iterations) has been satisfied, then we are done. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| **第6步** | 如果在第5步中簇中心发生了变化，这意味着我们需要重新计算每个数据点的簇分配。为此，我们将回到第3步，重复进行计算密集型的步骤。如果簇中心没有发生变化，或者我们预定的停止条件（例如最大迭代次数）已经满足，那么我们就完成了。
    |'
- en: 'The following figure shows the result of running the k-means algorithm in a
    two-dimensional problem space:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了在二维问题空间中运行 k-means 算法的结果：
- en: '![Chart, scatter chart  Description automatically generated](img/B18046_06_06.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 说明自动生成](img/B18046_06_06.png)'
- en: 'Figure 6.6: Results of k-means clustering (a) Data points before clustering;
    (b) resultant clusters after running the k-means clustering algorithm'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：k-means 聚类结果 (a) 聚类前的数据点；(b) 运行 k-means 聚类算法后的结果簇
- en: Note that the two resulting clusters created after running k-means are well
    differentiated in this case. Now let us look into the stop condition of the k-means
    algorithm.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这种情况下，运行 k-means 后得到的两个簇是明显区分开的。现在我们来看看 k-means 算法的停止条件。
- en: Stop condition
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 停止条件
- en: In unsupervised learning algorithms like k-means, the stop condition plays a
    crucial role in determining when the algorithm should cease its iterative process.
    For the k-means algorithm, the default stop condition is when there is no more
    shifting of cluster centers in step 5\. But as with many other algorithms, k-means
    algorithms may take a lot of time to converge, especially while processing large
    datasets in a high-dimensional problem space.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在像 k-means 这样的无监督学习算法中，停止条件在确定算法何时应该停止迭代过程中起着至关重要的作用。对于 k-means 算法，默认的停止条件是在第5步中簇中心不再发生移动。但是，与许多其他算法一样，k-means
    算法可能需要较长的时间才能收敛，尤其是在处理大数据集和高维问题空间时。
- en: 'Instead of waiting for the algorithm to converge, we can also explicitly define
    the stop condition as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以显式地定义停止条件，而不是等待算法收敛，具体如下：
- en: 'By specifying the maximum execution time:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过指定最大执行时间：
- en: '**Stop condition**: *t*>*t*[max], where *t* is the current execution time and
    *t*[max] is the maximum execution time we have set for the algorithm.'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停止条件**：*t*>*t*[max]，其中 *t* 是当前执行时间，*t*[max] 是我们为算法设置的最大执行时间。'
- en: 'By specifying the maximum iterations:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过指定最大迭代次数：
- en: '**Stop condition**: if *m*>*m*[max], where *m* is the current iteration and
    *m*[max] is the maximum number of iterations we have set for the algorithm.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停止条件**：如果 *m*>*m*[max]，其中 *m* 是当前迭代次数，*m*[max] 是我们为算法设置的最大迭代次数。'
- en: Coding the k-means algorithm
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写 k-means 算法
- en: We’ll perform k-means clustering on a simple two-dimensional dataset you’ve
    provided, with two features, `x` and `y`. Imagine a swarm of fireflies scattered
    across a garden at night. Your task is to group these fireflies based on their
    proximity to each other. This is the essence of k-means clustering, a popular
    unsupervised learning algorithm.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在您提供的一个简单二维数据集上执行 k-means 聚类，数据集包含两个特征 `x` 和 `y`。假设你看到夜晚花园里散布的萤火虫群体。你的任务是根据它们之间的接近程度将这些萤火虫分组。这就是
    k-means 聚类的本质，它是一个流行的无监督学习算法。
- en: 'We’re given a dataset, much like our garden, with data points plotted in a
    two-dimensional space. Our data points are represented by `x` and `y` coordinates:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给定了一个数据集，就像我们的花园一样，数据点绘制在二维空间中。我们的数据点由 `x` 和 `y` 坐标表示：
- en: '[PRE0]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Our task is to cluster these data points using the k-means algorithm.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是使用 k-means 算法对这些数据点进行聚类。
- en: 'Firstly, we import the required libraries:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的库：
- en: '[PRE1]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we’ll initiate the `KMeans` class by specifying the number of clusters
    (`k`). For this example, let’s assume we want to divide our data into 3 clusters:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过指定聚类数（`k`）来初始化 `KMeans` 类。对于这个例子，假设我们想将数据分成 3 个聚类：
- en: '[PRE2]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we train our `KMeans` model with our dataset. It is worth mentioning that
    this model only needs the feature matrix (`x`) and not the target vector (`y`)
    because it’s an unsupervised learning algorithm:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们用数据集来训练我们的`KMeans`模型。值得一提的是，这个模型只需要特征矩阵（`x`），而不需要目标向量（`y`），因为它是一个无监督学习算法：
- en: '[PRE3]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let us now look into the labels and the cluster centers:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看标签和聚类中心：
- en: '[PRE4]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, to visualize our clusters, we plot our data points, coloring them
    according to their assigned cluster. The centers of clusters, also known as centroids,
    are also plotted:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了可视化我们的聚类，我们绘制了数据点，并根据其分配的聚类为其着色。聚类的中心，也称为质心，也被绘制出来：
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the plot, the colored points represent our data points and their respective
    clusters, while the red points denote the centroids of each cluster.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，彩色的点表示我们的数据点及其相应的聚类，而红色的点表示每个聚类的质心。
- en: '![A graph with many colored dots  Description automatically generated](img/B18046_06_07.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![A graph with many colored dots  Description automatically generated](img/B18046_06_07.png)'
- en: 'Figure 6.7: Results of k-means clustering'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：k-means 聚类结果
- en: Note that the bigger dots in the plot are the centroids as determined by the
    k-means algorithm.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，图中较大的点是由 k-means 算法确定的质心。
- en: Limitation of k-means clustering
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-means 聚类的限制
- en: 'The k-means algorithm is designed to be a simple and fast algorithm. Because
    of the intentional simplicity of its design, it comes with the following limitations:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法设计简单且快速。由于其设计上的故意简化，它具有以下限制：
- en: The biggest limitation of k-means clustering is that the initial number of clusters
    has to be predetermined.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means 聚类的最大限制是初始聚类数必须预先确定。
- en: The initial assignment of cluster centers is random. This means that each time
    the algorithm is run, it may give slightly different clusters.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类中心的初始分配是随机的。这意味着每次运行算法时，可能会给出略微不同的聚类结果。
- en: Each data point is assigned to only one cluster.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数据点只能分配到一个聚类。
- en: k-means clustering is sensitive to outliers.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means 聚类对离群点敏感。
- en: Now let us look into another unsupervised machine learning technique, hierarchical
    clustering.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看另一种无监督机器学习技术——层次聚类。
- en: Hierarchical clustering
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层次聚类
- en: K-means clustering uses a top-down approach because we start the algorithm from
    the most important data points, which are the cluster centers. There is an alternative
    approach of clustering where, instead of starting from the top, we start the algorithm
    from the bottom. The bottom, in this context, is each of the individual data points
    in the problem space. The solution is to keep on grouping similar data points
    together as it progresses up toward the cluster centers. This alternative bottom-up
    approach is used by hierarchical clustering algorithms and is discussed in this
    section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: K均值聚类使用自上而下的方法，因为我们从最重要的数据点开始，即聚类中心。还有一种聚类方法，与自上而下的方式不同，我们从底部开始算法。在这里，底部指的是问题空间中的每个单独数据点。解决方案是随着算法向上推进，逐步将相似的数据点聚在一起，直到达到聚类中心。这种自底向上的方法由层次聚类算法使用，且在本节中有详细讨论。
- en: Steps of hierarchical clustering
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类的步骤
- en: 'The following steps are involved in hierarchical clustering:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类包括以下步骤：
- en: We create a separate cluster for each data point in our problem space. If our
    problem space consists of 100 data points, then it will start with 100 clusters.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为问题空间中的每个数据点创建一个单独的聚类。如果问题空间包含100个数据点，则会从100个聚类开始。
- en: We group only those points that are closest to each other.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只将彼此最接近的点分为一组。
- en: We check for the stop condition; if the stop condition is not yet satisfied,
    then we repeat step 2.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们检查停止条件；如果停止条件尚未满足，则重复步骤2。
- en: The resulting clustered structure is called a **dendrogram**.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 结果得到的聚类结构称为**树状图**。
- en: 'In a dendrogram, the height of the vertical lines determines how close the
    items are, as shown in the following diagram:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在树状图中，垂直线的高度决定了项目之间的相似度，如下图所示：
- en: '![Diagram  Description automatically generated with medium confidence](img/B18046_06_08.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图形描述自动生成，置信度中等](img/B18046_06_08.png)'
- en: 'Figure 6.8: Hierarchical clustering'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：层次聚类
- en: Note that the stop condition is shown as a dotted line in *Figure 6.8*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，停止条件在*图6.8*中以虚线表示。
- en: Coding a hierarchical clustering algorithm
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写层次聚类算法
- en: 'Let’s learn how we can code a hierarchical algorithm in Python:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何在Python中编写层次聚类算法：
- en: 'We will first import `AgglomerativeClustering` from the `sklearn.cluster` library,
    along with the `pandas` and `numpy` packages:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从`sklearn.cluster`库中导入`AgglomerativeClustering`，同时导入`pandas`和`numpy`包：
- en: '[PRE9]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we will create 20 data points in a two-dimensional problem space:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将在二维问题空间中创建20个数据点：
- en: '[PRE10]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we create the hierarchical cluster by specifying the hyperparameters.
    Note that a hyperparameter refers to a configuration parameter of a machine learning
    model that is set before the training process and influences the model’s behavior
    and performance. We use the `fit_predict` function to actually process the algorithm:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过指定超参数来创建层次聚类。需要注意的是，超参数指的是在训练过程前设置的机器学习模型的配置参数，它会影响模型的行为和性能。我们使用`fit_predict`函数来实际处理算法：
- en: '[PRE11]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now let’s look at the association of each data point to the two clusters that
    were created:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们来看看每个数据点与所创建的两个聚类的关联：
- en: '[PRE12]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You can see that the cluster assignment for both hierarchical and *k*-means
    algorithms are very similar.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，层次聚类和*k*-均值算法的聚类分配非常相似。
- en: The hierarchical clustering algorithm has its distinct advantages and drawbacks
    when compared to the *k*-means clustering algorithm. One key advantage is that
    hierarchical clustering doesn’t require the number of clusters to be specified
    beforehand, unlike *k*-means.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 与*k*-均值聚类算法相比，层次聚类算法具有其独特的优势和缺点。一个关键的优势是，层次聚类不需要事先指定聚类数量，这与*k*-均值不同。
- en: This feature can be incredibly useful when the data doesn’t clearly suggest
    an optimal number of clusters. Hierarchical clustering also provides a dendrogram,
    a tree-like diagram that can be very insightful for visualizing the nested grouping
    of data and understanding the hierarchical structure.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据没有明确提示最佳聚类数量时，这个特性非常有用。层次聚类还提供了树状图，这是一种类似树的图示，对于可视化数据的嵌套分组以及理解层次结构非常有帮助。
- en: However, hierarchical clustering has its drawbacks. It is computationally more
    intensive than *k*-means, making it less suitable for large datasets.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，层次聚类也有其缺点。它比*k*-均值更耗费计算资源，因此不适合处理大数据集。
- en: Understanding DBSCAN
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解DBSCAN
- en: '**Density-based spatial clustering of applications with noise** (**DBSCAN**)
    is an unsupervised learning technique that performs clustering based on the density
    of the points. The basic idea is based on the assumption that if we group the
    data points in a crowded or high-density space together, we can achieve meaningful
    clustering.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于密度的空间应用聚类与噪声** (**DBSCAN**) 是一种无监督学习技术，它基于数据点的密度进行聚类。基本思想是基于这样的假设：如果我们将数据点在一个拥挤或高密度的空间中聚集在一起，我们可以实现有意义的聚类。'
- en: 'This approach to clustering has two important implications:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这种聚类方法有两个重要的含义：
- en: Using this idea, the algorithm is likely to cluster together the points that
    exist together regardless of their shape or pattern. This methodology helps in
    creating clusters of arbitrary shapes. By “shape,” we refer to the pattern or
    distribution of data points in a multi-dimensional space. This capability is advantageous
    because real-world data is often complex and non-linear, and the ability to create
    clusters of arbitrary shapes enables more accurate representation and understanding
    of such data.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于这个思想，算法可能会将无论其形状或模式如何的点聚集在一起。这种方法有助于创建任意形状的簇。这里的“形状”指的是多维空间中数据点的模式或分布。这个能力非常有用，因为现实世界中的数据通常是复杂且非线性的，而能够创建任意形状的簇可以更准确地表示和理解这些数据。
- en: Unlike the k-means algorithm, we do not need to specify the number of clusters
    and the algorithm can detect the appropriate number of groupings in the data.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 k-means 算法不同，我们不需要指定簇的数量，算法可以检测到数据中适当的分组数量。
- en: 'The following steps involve the DBSCAN algorithm:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤涉及 DBSCAN 算法：
- en: The algorithm establishes a neighborhood around each data point. The term “neighborhood,”
    in this context, refers to an area wherein other data points are examined for
    proximity to the point of interest. This is accomplished by counting the number
    of data points within a distance usually represented by a variable, *eps*. The
    *eps* variable, in this setting, specifies the maximum distance between two data
    points for them to be considered as being in the same neighborhood. The distance
    is by default determined by the Euclidean distance measure.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法在每个数据点周围建立一个邻域。在此上下文中，“邻域”是指一个区域，其中其他数据点会被检查与感兴趣点的接近程度。这是通过计算在一个通常由变量*eps*表示的距离范围内的数据点数量来实现的。在此设置中，*eps*变量指定了两个数据点之间的最大距离，超过该距离则不被视为在同一邻域内。默认情况下，这个距离是通过欧几里得距离度量来确定的。
- en: Next, the algorithm quantifies the density of each data point. It uses a variable
    named `min_samples`, which represents the minimum number of other data points
    that should be in the *eps* distance for a data point to be regarded as a “core
    instance.” In simpler terms, a core instance is a data point that is densely surrounded
    by other data points. Logically, regions with a high density of data points will
    have a greater number of these core instances.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，算法对每个数据点的密度进行量化。它使用一个名为`min_samples`的变量，表示一个数据点被视为“核心实例”所需的最小其他数据点数量，这些数据点应该位于数据点的*eps*距离内。简单来说，核心实例是一个密集地被其他数据点包围的数据点。从逻辑上讲，数据点密度高的区域将有更多的核心实例。
- en: Each of the identified neighborhoods identifies a cluster. It is crucial to
    note that the neighborhood surrounding one core instance (a data point that has
    a minimum number of other data points within its “eps” distance) may encompass
    additional core instances. This means that core instances are not exclusive to
    a single cluster but can contribute to the formation of multiple clusters due
    to their proximity to several data points. Consequently, the borders of these
    clusters may overlap, leading to a complex, interconnected cluster structure.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个已识别的邻域标识一个簇。需要特别注意的是，围绕一个核心实例（一个在其“eps”距离内有最小数量其他数据点的数据点）的邻域可能包含其他核心实例。这意味着核心实例并非专属于某一个簇，而是由于其与多个数据点的接近，可能有助于形成多个簇。因此，这些簇的边界可能会重叠，从而导致一个复杂的、相互连接的簇结构。
- en: Any data point that is not a core instance or does not lie in the neighborhood
    of a core instance is considered an outlier.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任何不是核心实例的数据点，或者不在核心实例邻域内的数据点，都被视为离群点。
- en: Let us see how we can create clusters using DBSCAN in Python.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在 Python 中使用 DBSCAN 创建簇。
- en: Creating clusters using DBSCAN in Python
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Python 中使用 DBSCAN 创建簇
- en: 'First, we will import the necessary functions from the `sklearn` library:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从`sklearn`库中导入必要的函数：
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Let’s employ `DBSCAN` to tackle a slightly more complex clustering problem,
    one that involves structures known as “half-moons.” In this context, “half-moons”
    refer to two sets of data points that are shaped like crescents, with each moon
    representing a unique cluster. Such datasets pose a challenge because the clusters
    are not linearly separable, meaning a straight line cannot easily divide the different
    groups.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`DBSCAN`来解决一个稍微复杂一点的聚类问题，这个问题涉及被称为“半月形”结构的数据。在这个上下文中，“半月形”是指两组数据点，形状像新月，每个半月代表一个独特的簇。这样的数据集构成了挑战，因为簇是不可线性分割的，意味着无法用一条直线轻松地将不同的组分开。
- en: This is where the concept of “nonlinear class boundaries” comes into play. In
    contrast to linear class boundaries, which can be represented by a straight line,
    nonlinear class boundaries are more complex, often necessitating curved lines
    or multidimensional surfaces to accurately segregate different classes or clusters.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这是“非线性类边界”概念的应用。与可以由直线表示的线性类边界不同，非线性类边界更加复杂，通常需要曲线或多维面来准确分隔不同的类或簇。
- en: To generate this half-moon dataset, we can leverage the `make_moons()` function.
    This function creates a swirl pattern resembling two moons. The “noisiness” of
    the moon shapes and the number of samples to generate can be adjusted according
    to our needs.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成这个半月形数据集，我们可以利用`make_moons()`函数。这个函数生成一个类似两个半月的螺旋图案。可以根据我们的需求调整月形的“噪声”程度和生成的样本数量。
- en: 'Here’s what the generated dataset looks like:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的数据集如下所示：
- en: '![Chart, scatter chart  Description automatically generated](img/B18046_06_09.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  描述自动生成](img/B18046_06_09.png)'
- en: 'Figure 6.9: Data used for DBSCAN'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9：用于DBSCAN的数据
- en: 'In order to use DBSCAN, we need to provide the `eps` and `min_samples` parameters
    as discussed:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用DBSCAN，我们需要提供`eps`和`min_samples`参数，正如前文所讨论的：
- en: '[PRE15]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Evaluating the clusters
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估聚类
- en: 'The objective of good quality clustering is that the data points that belong
    to the separate clusters should be differentiable. This implies the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 良好质量的聚类目标是，属于不同簇的数据点应该是可以区分的。这意味着以下几点：
- en: The data points that belong to the same cluster should be as similar as possible.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属于同一簇的数据点应该尽可能相似。
- en: Data points that belong to separate clusters should be as different as possible.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属于不同簇的数据点应该尽可能不同。
- en: Human intuition can be used to evaluate the clustering results by visualizing
    the clusters, but there are mathematical methods that can quantify the quality
    of the clusters. They not only measure the tightness of each cluster (cohesion)
    and the separation between different clusters but also offer a numerical, hence
    objective, way to assess the quality of clustering. Silhouette analysis is one
    such technique that compares the tightness and separation in the clusters created
    by the k-means algorithm. It’s a metric that quantifies the degree of cohesion
    and separation in clusters. While this technique has been mentioned in the context
    of k-means, it is in fact generalizable and can be applied to evaluate the results
    of any clustering algorithm, not just k-means.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 人类直觉可以通过可视化簇来评估聚类结果，但也有数学方法可以量化聚类的质量。这些方法不仅测量每个簇的紧密度（凝聚度）和不同簇之间的分离度，还提供了一种数值化的客观评估聚类质量的方法。轮廓分析就是一种技术，它比较由k-means算法创建的簇的紧密度和分离度。它是一个量化簇中凝聚度和分离度的度量指标。虽然这种技术在k-means的背景下有提及，但事实上它是可以推广的，适用于评估任何聚类算法的结果，而不仅限于k-means。
- en: Silhouette analysis assigns a score, known as the Silhouette coefficient, to
    each data point in the range of 0 to 1\. It essentially measures how close each
    data point in one cluster is to the points in the neighboring clusters.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓分析为每个数据点分配一个分数，称为轮廓系数，范围从0到1。它本质上衡量了同一簇内的每个数据点与相邻簇中数据点的距离。
- en: Application of clustering
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类的应用
- en: Clustering is used wherever we need to discover the underlying patterns in datasets.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类被广泛应用于需要发现数据集潜在模式的场合。
- en: 'In government use cases, clustering can be used for the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在政府应用场景中，聚类可以用于以下方面：
- en: '**Crime-hotspot analysis**: Clustering is applied to geolocation data, incident
    reports, and other related features. It aids in identifying areas with high incidences
    of crime, enabling law enforcement agencies to optimize patrol routes and deploy
    resources more effectively.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**犯罪热点分析**：将聚类应用于地理位置数据、事件报告和其他相关特征。这有助于识别犯罪高发地区，使执法机构能够优化巡逻路线并更有效地部署资源。'
- en: '**Demographic social analysis**: Clustering can analyze demographic data such
    as age, income, education, and occupation. This aids in understanding the socioeconomic
    composition of different regions, informing public policy and social service provision.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人口社会分析**：聚类可以分析诸如年龄、收入、教育和职业等人口统计数据。这有助于了解不同地区的社会经济组成，为公共政策和社会服务提供信息。'
- en: 'In market research, clustering can be used for the following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在市场研究中，聚类可以用于以下方面：
- en: '**Market segmentation**: By clustering consumer data including spending habits,
    product preferences, and lifestyle indicators, businesses can identify distinct
    market segments. This allows for tailored product development and marketing approaches.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**市场细分**：通过对消费者数据（包括消费习惯、产品偏好和生活方式指标）进行聚类，企业可以识别出不同的市场细分群体。这有助于量身定制产品开发和营销策略。'
- en: '**Targeted advertisements**: Clustering helps analyze customer online behavior,
    including browsing patterns, click-through rates, and purchase history. This enables
    companies to create personalized advertisements for each customer cluster, enhancing
    engagement and conversion rates.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定向广告**：聚类有助于分析客户的在线行为，包括浏览模式、点击率和购买历史。这使得公司能够为每个客户群体创建个性化广告，从而提高参与度和转化率。'
- en: '**Customer categorization**: Through clustering, businesses can categorize
    customers based on their interaction with products or services, their feedback,
    and their loyalty. This aids in understanding customer behavior, predicting trends,
    and developing retention strategies.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户分类**：通过聚类，企业可以根据客户与产品或服务的互动、反馈和忠诚度对客户进行分类。这有助于了解客户行为、预测趋势并制定客户保持策略。'
- en: '**Principal component analysis** (**PCA**) is also used for generally exploring
    the data and removing noise from real-time data, such as stock-market trading.
    In this context, “noise” refers to random or irregular fluctuations that may obscure
    underlying patterns or trends in the data. PCA helps in filtering out these erratic
    fluctuations, allowing for clearer data analysis and interpretation.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）也用于一般性的数据探索以及去除实时数据中的噪声，例如股市交易。在这种情况下，“噪声”指的是可能掩盖数据中潜在模式或趋势的随机或不规则波动。PCA帮助过滤这些不规则波动，使数据分析和解释更加清晰。'
- en: Dimensionality reduction
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: 'Each feature in our data corresponds to a dimension in our problem space. Minimizing
    the number of features to make our problem space simpler is called **dimensionality
    reduction**. It can be done in one of the following two ways:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据中的每个特征对应于我们问题空间中的一个维度。通过减少特征的数量，使我们的问题空间变得更简单，称为**降维**。它可以通过以下两种方式之一来实现：
- en: '**Feature selection**: Selecting a set of features that are important in the
    context of the problem we are trying to solve'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：选择在我们试图解决的问题背景下重要的特征集合'
- en: '**Feature aggregation**: Combining two or more features to reduce dimensions
    using one of the following algorithms:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征聚合**：通过以下算法之一将两个或多个特征合并以减少维度：'
- en: '**PCA**: A linear unsupervised ML algorithm'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PCA**：一种线性无监督机器学习算法'
- en: '**Linear discriminant analysis** (**LDA**): A linear supervised ML algorithm'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性判别分析**（**LDA**）：一种线性有监督机器学习算法'
- en: '**KPCA**: A nonlinear algorithm'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KPCA**：一种非线性算法'
- en: Let’s look deeper at one of the popular dimensionality reduction algorithms,
    namely PCA, in more detail.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解其中一种流行的降维算法——主成分分析（PCA），并详细探讨。
- en: Principal component analysis
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析
- en: PCA is a method in unsupervised machine learning that is typically employed
    to reduce the dimensionality of datasets through a process known as linear transformation.
    In simpler terms, it’s a way of simplifying data by focusing on its most important
    parts, which are identified based on their variance.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种无监督机器学习方法，通常用于通过线性变换的过程来减少数据集的维度。简单来说，它是一种通过关注数据中最重要的部分来简化数据的方式，这些部分是通过它们的方差来识别的。
- en: Consider a graphical representation of a dataset, where each data point is plotted
    on a multi-dimensional space. PCA helps identify the principal components, which
    are the directions where the data varies the most. In *Figure 6.10*, we see two
    of these, PC1 and PC2\. These principal components illustrate the overall “shape”
    of the distribution of data points.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个数据集的图形表示，其中每个数据点都绘制在多维空间中。PCA 帮助识别主成分，这些主成分是数据变化最大的方向。在*图 6.10*中，我们看到了其中的两个，PC1
    和 PC2。这些主成分展示了数据点分布的整体“形状”。
- en: Each principal component corresponds to a new, lesser dimension that captures
    as much information as possible. In a practical sense, these principal components
    can be viewed as summary indicators of the original data, making the data more
    manageable and easier to analyze. For instance, in a large dataset concerning
    customer behavior, PCA can help us identify the key driving factors (principal
    components) that define the majority of customer behaviors.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主成分对应一个新的较小维度，捕捉尽可能多的信息。从实际角度来看，这些主成分可以视为原始数据的摘要指标，使得数据更易于管理和分析。例如，在一个关于顾客行为的大型数据集中，PCA
    可以帮助我们识别出定义大多数顾客行为的关键驱动因素（主成分）。
- en: Determining the coefficients for these principal components involves calculating
    the eigenvectors and eigenvalues of the data covariance matrix, which is a topic
    we’ll delve into more deeply in a later section. These coefficients serve as weights
    for each original feature in the new component space, defining how each feature
    contributes to the principal component.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 确定这些主成分的系数涉及计算数据协方差矩阵的特征向量和特征值，这是我们在后续章节中将深入探讨的话题。这些系数作为每个原始特征在新组件空间中的权重，定义了每个特征对主成分的贡献。
- en: '![Chart, pie chart  Description automatically generated](img/B18046_06_10.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图表，饼图描述自动生成](img/B18046_06_10.png)'
- en: 'Figure 6.10: Principle component analysis'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10：主成分分析
- en: To elaborate further, imagine you have a dataset containing various aspects
    of a country’s economy, such as GDP, employment rates, inflation, and more. The
    data is vast and multi-dimensional. Here, PCA would allow you to reduce these
    multiple dimensions into two principal components, PC1 and PC2\. These components
    would encapsulate the most crucial information while discarding noise or less
    important details.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步说明，假设你有一个包含国家经济各个方面的数据集，例如 GDP、就业率、通货膨胀等。这些数据庞大且多维。在这里，PCA 允许你将这些多个维度简化为两个主成分，PC1
    和 PC2。这些成分将包含最关键的信息，同时丢弃噪声或不太重要的细节。
- en: The resulting graph, with PC1 and PC2 as axes, would give you an easier-to-interpret
    visual representation of the economic data, with each point representing an economy’s
    status based on its combination of GDP, employment rates, and other factors.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表，以 PC1 和 PC2 作为坐标轴，将为你提供一个更易于解释的经济数据的可视化表示，每个点代表一个经济体的状态，该状态基于其 GDP、就业率和其他因素的组合。
- en: This makes PCA an invaluable tool for simplifying and interpreting high-dimensional
    data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 PCA 成为简化和解释高维数据的宝贵工具。
- en: 'Let’s consider the following code:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下代码：
- en: '[PRE16]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now let’s print the coefficients of our PCA model:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打印出我们 PCA 模型的系数：
- en: '[PRE20]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Table  Description automatically generated](img/B18046_06_11.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18046_06_11.png)'
- en: 'Figure 6.11: Diagram highlighting coefficients of the PCA model'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11：突出显示 PCA 模型系数的图示
- en: 'Note that the original DataFrame has four features: `Sepal.Length`, `Sepal.Width`,
    `Petal.Length`, and `Petal.Width`. The preceding DataFrame specifies the coefficients
    of the four principal components, PC1, PC2, PC3, and PC4—for example, the first
    row specifies the coefficients of PC1 that can be used to replace the original
    four variables.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，原始 DataFrame 有四个特征：`Sepal.Length`、`Sepal.Width`、`Petal.Length` 和 `Petal.Width`。前面的
    DataFrame 指定了四个主成分 PC1、PC2、PC3 和 PC4 的系数——例如，第一行指定了 PC1 的系数，可用于替代原始的四个变量。
- en: 'It is important to note here that the number of principal components (in this
    case, four: PC1, PC2, PC3, and PC4) does not necessarily need to be two as in
    our previous economy example. The number of principal components is a choice we
    make based on the level of complexity we are willing to handle in our data. The
    more principal components we choose, the more of the original data’s variance
    we can retain, at the cost of increased complexity.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要注意的是，主成分的数量（在本例中为四个：PC1、PC2、PC3和PC4）不一定像我们之前的经济学例子那样必须是两个。主成分的数量是我们根据愿意处理数据的复杂度来选择的。我们选择的主成分越多，我们就能保留原始数据更多的方差，但这也会增加复杂度。
- en: 'Based on these coefficients, we can calculate the PCA components for our input
    DataFrame `X`:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些系数，我们可以计算输入数据框`X`的PCA组件：
- en: '[PRE21]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now let’s print `X` after the calculation of the PCA components:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在计算PCA组件后打印`X`：
- en: '![Table  Description automatically generated](img/B18046_06_12.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18046_06_12.png)'
- en: 'Figure 6.12: Printed calculation of the PCA components'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12：PCA组件的打印计算结果
- en: 'Now let’s print the variance ratio and try to understand the implications of
    using PCA:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打印方差比率，并尝试理解使用PCA的含义：
- en: '[PRE22]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The variance ratio indicates the following:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 方差比率表示以下内容：
- en: If we choose to replace the original four features with PC1, then we will be
    able to capture about 92.3% of the variance of the original variables. We will
    introduce some approximations by not capturing 100% of the variance of the original
    four features.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们选择用PC1替换原始的四个特征，那么我们将能够捕获原始变量方差的约92.3%。由于未捕获原始四个特征的100%方差，我们会引入一些近似。
- en: If we choose to replace the original four features with PC1 and PC2, then we
    will capture an additional 5.3% of the variance of the original variables.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们选择用PC1和PC2替换原始的四个特征，那么我们将捕获原始变量方差的额外5.3%。
- en: If we choose to replace the original four features with PC1, PC2, and PC3, then
    we will now capture a further 0.017% of the variance of the original variables.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们选择用PC1、PC2和PC3替换原始的四个特征，那么我们将再捕获原始变量方差的0.017%。
- en: If we choose to replace the original four features with four principal components,
    then we will capture 100% of the variance of the original variables (92.4 + 0.053
    + 0.017 + 0.005), but replacing four original features with four principal components
    is meaningless as we did not reduce the dimensions at all and achieved nothing.
    Next, let us look into the limitations of PCA.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们选择用四个主成分替换原始的四个特征，那么我们将捕获原始变量方差的100%（92.4 + 0.053 + 0.017 + 0.005），但用四个主成分替换四个原始特征是没有意义的，因为我们根本没有降低维度，什么也没实现。接下来，让我们看看PCA的局限性。
- en: Limitations of PCA
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PCA的局限性
- en: 'Despite its many benefits, PCA is not without its limitations, as outlined
    below:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PCA有很多优点，但它也存在局限性，如下所述：
- en: First, PCA is most effective when dealing with continuous variables, as its
    underlying mathematical principles are designed to handle numerical data. It struggles
    with categorical variables, which are common in datasets that include attributes
    like gender, nationality, or product type. For instance, if you were analyzing
    a survey dataset with a mixture of numerical responses (such as age or income)
    and categorical responses (such as preferences or options selected), PCA wouldn’t
    be suitable for the categorical data.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，PCA在处理连续变量时最为有效，因为它的基本数学原理是为处理数值数据而设计的。它处理类别变量时效果较差，而类别变量通常出现在包括性别、国籍或产品类型等属性的数据集中。例如，如果你正在分析一个包含数字响应（如年龄或收入）和类别响应（如偏好或选择的选项）的调查数据集，PCA对于类别数据并不适用。
- en: 'Furthermore, PCA operates by creating an approximation of the original high-dimensional
    data in a lower-dimensional space. While this reduction simplifies data handling
    and processing, it comes with a cost: a loss of some information. This is a trade-off
    that needs to be carefully evaluated in each use case. For instance, if you’re
    dealing with a biomedical dataset where each feature represents a specific genetic
    marker, using PCA could risk losing critical information that might be relevant
    for a particular disease’s diagnosis or treatment.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，PCA通过在较低维空间中创建原始高维数据的近似来运作。虽然这种降维简化了数据处理和操作，但它也带来了代价：一些信息的丢失。这是一个在每个使用案例中需要仔细评估的权衡。例如，如果你处理的是一个生物医学数据集，其中每个特征代表特定的基因标记，那么使用PCA可能会导致丢失对某种疾病的诊断或治疗至关重要的信息。
- en: So, while PCA is a powerful tool for dimensionality reduction, particularly
    when dealing with large datasets with many interrelated numerical variables, its
    limitations need to be considered carefully to ensure it is the right choice for
    a given application.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然PCA（主成分分析）在处理具有许多相互关联的数值变量的大型数据集时是一个强大的降维工具，但其局限性需要仔细考虑，以确保它是某个特定应用的正确选择。
- en: Association rules mining
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关联规则挖掘
- en: 'Patterns in a particular dataset are the treasure that needs to be discovered,
    understood, and mined for the information they contain. There is an important
    set of algorithms that tries to focus on pattern analysis in a given dataset.
    One of the more popular algorithms in this class of algorithm is called the **association
    rules mining** algorithm, which provides us with the following capabilities:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 特定数据集中的模式是需要发现、理解并挖掘出其包含信息的宝藏。有一类重要的算法专注于给定数据集中的模式分析。这类算法中比较流行的一种被称为**关联规则挖掘**算法，它为我们提供了以下功能：
- en: The ability to measure the frequency of a pattern
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够衡量模式的频率
- en: The ability to establish *cause-and-effect* relationships among the patterns
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够建立*因果关系*模式
- en: The ability to quantify the usefulness of patterns by comparing their accuracy
    to random guessing
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将模式的准确性与随机猜测进行比较，量化模式的有用性
- en: Now we will look at some examples of association rules mining.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看一些关联规则挖掘的例子。
- en: Examples of use
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用示例
- en: 'Association rules mining is used when we are trying to investigate the cause-and-effect
    relationships between different variables of a dataset. The following are example
    questions that it can help to answer:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则挖掘用于我们试图调查数据集不同变量之间的因果关系时。以下是它能帮助回答的典型问题：
- en: Which values of humidity, cloud cover, and temperature can lead to rain tomorrow?
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些湿度、云量和温度值可能导致明天下雨？
- en: What type of insurance claim can indicate fraud?
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么类型的保险索赔可能表明欺诈行为？
- en: What combinations of medicine may lead to complications for patients?
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些药物组合可能会导致患者出现并发症？
- en: As these examples illustrate, association rules mining has a broad array of
    applications spanning from business intelligence to healthcare and environmental
    studies. This algorithm is a potent instrument in the data scientist’s toolkit,
    capable of translating complex patterns into actionable insights across diverse
    fields.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这些例子所示，关联规则挖掘在商业智能、医疗保健和环境研究等领域有广泛的应用。这一算法是数据科学家工具包中的强大工具，能够将复杂的模式转化为跨多个领域的可操作洞察。
- en: Market basket analysis
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 市场篮子分析
- en: Recommendation engines, an important topic extensively discussed in *Chapter
    12*, *Recommendation Engines* of this book, are powerful tools for personalizing
    user experiences. However, there’s a simpler, yet effective method for generating
    recommendations known as market basket analysis. Market basket analysis operates
    based on information about which items are frequently bought together. Unlike
    more sophisticated recommendation engines, this method does not take into account
    additional user-specific data or individual item preferences expressed by the
    user. It’s essential to draw a distinction here. Recommendation engines typically
    create personalized suggestions based on the user’s past behavior, preferences,
    and a wealth of other user-specific information. In contrast, market basket analysis
    solely focuses on the combinations of items purchased, regardless of who bought
    them or their individual preferences.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐引擎是本书*第12章*《推荐引擎》中广泛讨论的一个重要主题，是个性化用户体验的强大工具。然而，还有一种更简单且有效的生成推荐的方法，称为市场篮子分析。市场篮子分析基于哪些商品经常一起购买的信息。与更复杂的推荐引擎不同，这种方法不考虑用户特定数据或用户表达的个别商品偏好。在这里必须做出区分。推荐引擎通常根据用户的过去行为、偏好以及大量其他用户特定信息创建个性化建议。相反，市场篮子分析只关注购买的商品组合，而不考虑购买者是谁或他们的个别偏好。
- en: One of the key advantages of market basket analysis is the relative ease of
    data collection. Gathering comprehensive user preference data can be complex and
    time-consuming. However, data regarding items bought together can often be simply
    extracted from transaction records, making market basket analysis a convenient
    starting point for businesses venturing into the domain of recommendations. For
    example, this kind of data is generated when we shop at Walmart, and no special
    technique is required to get the data.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 市场篮分析的一个主要优势是数据收集的相对简便性。收集全面的用户偏好数据可能复杂且耗时。然而，关于商品共同购买的数据通常可以直接从交易记录中提取，这使得市场篮分析成为商家进入推荐领域的一个便捷起点。例如，当我们在沃尔玛购物时，就会生成这类数据，而不需要任何特殊技术来获取这些数据。
- en: By “special techniques,” we refer to additional steps such as conducting user
    surveys, employing tracking cookies, or building complex data pipelines. Instead,
    the data is readily available as a byproduct of the sales process. This data,
    when collected over a period of time, is called **transnational data**.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓的“特殊技术”，是指额外的步骤，如进行用户调查、使用追踪 Cookie 或构建复杂的数据管道。相反，这些数据是作为销售过程的副产品随时可用的。当这些数据在一段时间内收集时，就称为**跨国数据**。
- en: 'When association rules analysis is applied to transnational datasets of the
    shopping carts being used in convenience stores, supermarkets, and fast-food chains,
    it is called **market basket analysis**. It measures the conditional probability
    of buying a set of items together, which helps to answer the following questions:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 当关联规则分析应用于便利店、超市和快餐连锁店的购物车跨国数据时，这被称为**市场篮分析**。它衡量一起购买一组商品的条件概率，有助于回答以下问题：
- en: What is the optimal placement of items on the shelf?
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商品应如何在货架上摆放？
- en: How should the items appear in the marketing catalog?
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商品应该如何在营销目录中展示？
- en: What should be recommended, based on a user’s buying patterns?
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于用户的购买模式，应该推荐什么？
- en: As market basket analysis can estimate how items are related to each other,
    it is often used for mass-market retail, such as supermarkets, convenience stores,
    drug stores, and fast-food chains. The advantage of market basket analysis is
    that the results are almost self-explanatory, which means that they are easily
    understood by business users.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 由于市场篮分析能够估计商品之间的关联性，因此它常常用于大宗零售市场，如超市、便利店、药店和快餐连锁店。市场篮分析的优点在于结果几乎是自解释的，意味着商业用户可以轻松理解。
- en: Let’s look at a typical superstore. All the unique items that are available
    in the store can be represented by a set, ![](img/B18046_06_004.png) = {item[1],
    item[2], . . . , item[m]}. So, if that superstore is selling 500 distinct items,
    then ![](img/B18046_06_005.png) will be a set of size 500.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一个典型的大型超市。商店中所有独特的商品可以用一个集合表示，![](img/B18046_06_004.png) = {item[1], item[2],
    . . . , item[m]}。因此，如果该超市销售500种不同的商品，那么![](img/B18046_06_005.png)将是一个包含500个元素的集合。
- en: People will buy items from this store. Each time someone buys an item and pays
    at the counter, it is added to a set of the items in a particular transaction,
    called an **itemset**. In a given period of time, the transactions are grouped
    together in a set represented by ![](img/B18046_06_006.png), where ![](img/B18046_06_007.png)
    = {t[1], t[2], . . . , t[n]}.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 人们会在这家商店购买商品。每次有人购买商品并在收银台付款时，该商品就会被添加到某个特定交易的商品集合中，这个集合被称为**商品集**。在特定时间段内，这些交易会被汇总到一个集合中，表示为![](img/B18046_06_006.png)，其中![](img/B18046_06_007.png)
    = {t[1], t[2], . . . , t[n]}。
- en: 'Let’s look at the following simple transaction data consisting of only four
    transactions. These transactions are summarized in the following table:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看以下仅由四个交易组成的简单交易数据。这些交易汇总在下面的表格中：
- en: '| t[1] | Wickets, pads |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| t[1] | 门柱，护垫 |'
- en: '| t[2] | Bats, wickets, pads, helmets |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| t[2] | 球棒，门柱，护垫，头盔 |'
- en: '| t[3] | Helmets, balls |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| t[3] | 头盔，球 |'
- en: '| t[4] | Bats, pads, helmets |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| t[4] | 球棒，护垫，头盔 |'
- en: 'Let’s look at this example in more detail:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看这个例子：
- en: '![](img/B18046_06_008.png) = {bat, wickets, pads, helmets, balls}, which represents
    all the unique items available at the store.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B18046_06_008.png) = {球棒，门柱，护垫，头盔，球}，表示商店中所有独特的商品。'
- en: Let’s consider one of the transactions, t[3], from ![](img/B18046_06_009.png).
    Note that items bought in t[3] can be represented in the itemset t[3]= {helmets,
    balls}, signifying that a customer bought two items. This set is termed an itemset
    because it encompasses all items purchased in a single transaction. Given that
    there are two items in this itemset, the size of itemset t[3] is said to be two.
    This terminology allows us to classify and analyze purchasing patterns more effectively.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑来自![](img/B18046_06_009.png)的一个事务 t[3]。注意，在 t[3] 中购买的商品可以用物品集 t[3] = {头盔,
    球} 表示，意味着顾客购买了两件商品。这个集合被称为物品集，因为它包含了单个事务中购买的所有商品。由于这个物品集有两个商品，所以物品集 t[3] 的大小被认为是二。这个术语让我们能够更有效地分类和分析购买模式。
- en: Association rules mining
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关联规则挖掘
- en: An association rule mathematically describes the relationship items involved
    in various transactions. It does this by investigating the relationship between
    two item sets in the form *X* ⇒ *Y*, where ![](img/B18046_06_010.png), ![](img/B18046_06_011.png).
    In addition, *X* and *Y* are non overlapping item sets; which means that ![](img/B18046_06_012.png).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关联规则从数学上描述了涉及多个事务的物品之间的关系。它通过研究两个物品集之间的关系，以 *X* ⇒ *Y* 的形式进行描述，其中 ![](img/B18046_06_010.png)，
    ![](img/B18046_06_011.png)。此外，*X* 和 *Y* 是不重叠的物品集；这意味着 ![](img/B18046_06_012.png)。
- en: 'An association rule could be described in the following form:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关联规则可以通过以下形式来描述：
- en: '{*helmets*, *balls*} ⇒ {*bike*}'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '{*头盔*, *球*} ⇒ {*自行车*}'
- en: Here, {*helmets*, *balls*} is *X*, and {*bike*} is *Y*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， {*头盔*, *球*} 是 *X*， {*自行车*} 是 *Y*。
- en: Let us look into the different types of association rules.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看不同类型的关联规则。
- en: Types of rules
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规则的类型
- en: 'Running associative analysis algorithms will typically result in the generation
    of a large number of rules from a transaction dataset. Most of them are useless.
    To pick rules that can result in useful information, we can classify them as one
    of the following three types:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 运行关联分析算法通常会从事务数据集中生成大量规则。它们中的大多数是无用的。为了挑选出能提供有用信息的规则，我们可以将它们分类为以下三种类型：
- en: Trivial
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微不足道的
- en: Inexplicable
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法解释
- en: Actionable
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可执行的
- en: Let’s look at each of these types in more detail.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下这些类型。
- en: Trivial rules
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微不足道的规则
- en: Among the large numbers of rules generated, many that are derived will be useless
    as they summarize common knowledge about the business. They are called trivial
    rules. Even if confidence in the trivial rules is high, they remain useless and
    cannot be used for any data-driven decision-making. Note that, here, “confidence”
    refers to a metric used in association analysis that quantifies the probability
    of occurrence of a particular event (let’s say B), given that another event (A)
    has already occurred. We can safely ignore all trivial rules.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的大量规则中，许多从数据中推导出的规则是无用的，因为它们总结了有关业务的常识。这些被称为微不足道的规则。即使微不足道规则的置信度很高，它们仍然无用，不能用于任何基于数据的决策。请注意，这里“置信度”指的是在关联分析中使用的一种度量，量化了在另一个事件（A）已经发生的情况下，某个特定事件（比如
    B）发生的概率。我们可以安全地忽略所有微不足道的规则。
- en: 'The following are examples of trivial rules:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些微不足道规则的例子：
- en: Anyone who jumps from a high-rise building is likely to die.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何从高楼大厦跳下来的人很可能会死。
- en: Working harder leads to better scores in exams.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更努力工作可以在考试中获得更好的成绩。
- en: The sales of heaters increase as the temperature drops.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当温度下降时，取暖器的销售量会上升。
- en: Driving a car over the speed limit on a highway leads to a higher chance of
    an accident.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高速公路上超速驾驶会导致发生事故的几率增加。
- en: Inexplicable rules
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无法解释的规则
- en: Among the rules that are generated after running the association rules algorithm,
    the ones that have no obvious explanation are the trickiest to use. Note that
    a rule can only be useful if it can help us discover and understand a new pattern
    that is expected to eventually lead toward a certain course of action. If that
    is not the case, and we cannot explain why event *X* led to event *Y*, then it
    is an inexplicable rule, because it’s just a mathematical formula that ends up
    exploring the pointless relationship between two events that are unrelated and
    independent.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行关联规则算法之后生成的规则中，那些没有明显解释的规则是最难使用的。请注意，只有当一个规则可以帮助我们发现并理解一个新的模式，并最终可能导致某个特定的行动时，才是有用的。如果不能解释为什么事件
    *X* 导致事件 *Y*，那么它就是一个无法解释的规则，因为它只不过是一个数学公式，最终探索的是两个无关和独立事件之间没有意义的关系。
- en: 'The following are examples of inexplicable rules:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些无法解释的规则的例子：
- en: People who wear red shirts tend to score better in exams.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 穿红色衬衫的人在考试中往往表现更好。
- en: Green bicycles are more likely to be stolen.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绿色自行车更容易被盗。
- en: People who buy pickles end up buying diapers as well.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 买泡菜的人也会购买尿布。
- en: Actionable rules
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可操作规则
- en: Actionable rules are the golden rules we are looking for. They are understood
    by the business and lead to insights. They can help us to discover the possible
    causes of an event when presented to an audience familiar with the business domain—for
    example, actionable rules may suggest the best placement in a store for a particular
    product based on current buying patterns. They may also suggest which items to
    place together to maximize their chances of selling as users tend to buy them
    together.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 可操作规则是我们寻找的黄金规则。它们为业务所理解并能带来洞察。当向熟悉业务领域的观众展示时，它们可以帮助我们发现事件的可能原因。例如，可操作规则可能基于当前的购买模式，建议在商店中为特定产品选择最佳陈列位置。它们还可以建议将哪些物品放在一起，以最大化销售机会，因为用户倾向于将这些物品一起购买。
- en: 'The following are examples of actionable rules and their corresponding actions:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可操作规则及其相应行动的示例：
- en: '**Rule 1**: Displaying ads to users’ social media accounts results in a higher
    likelihood of sales.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规则 1**：向用户社交媒体账户展示广告会提高销售的可能性。'
- en: '**Actionable item**: Suggests alternative ways of advertising a product.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可操作项**：建议广告产品的替代方式。'
- en: '**Rule 2**: Creating more price points increases the likelihood of sales.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规则 2**：创建更多的价格点可以提高销售的可能性。'
- en: '**Actionable item**: One item may be advertised in a sale, while the price
    of another item is raised.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可操作项**：某一商品可以进行促销，而另一商品的价格则可以提高。'
- en: Let us now look into how to rank the rules.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下如何对规则进行排序。
- en: Ranking rules
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排序规则
- en: 'Association rules are measured in three ways:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则有三种衡量方式：
- en: Support (frequency) of items
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物品的支持度（频率）
- en: Confidence
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信心
- en: Lift
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升度
- en: Let’s look at them in more detail.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解一下它们。
- en: Support
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持度
- en: The support measure is a number that quantifies how frequent the pattern we
    are looking for is in our dataset. It is calculated by first counting the number
    of occurrences of our pattern of interest and then dividing it by the total number
    of all the transactions.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 支持度度量是一个数值，用于量化我们在数据集中寻找的模式的频率。它是通过首先计算我们感兴趣的模式出现的次数，然后将其除以所有交易的总数来计算的。
- en: 'Let’s look at the following formula for a particular *itemset*[a]:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下针对特定*项目集*[a]的公式：
- en: '*numItemset*[a] *= Number of transactions that contain itemset*[a]'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '*numItemset*[a] *= 包含*项目集*[a]的交易数量'
- en: '*num*[total] *= Total number of transactions*'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '*num*[total] *= 交易总数*'
- en: '![](img/B18046_06_013.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_06_013.png)'
- en: By just looking at the support, we can get an idea of how rare the occurrence
    of a pattern is. Low support means that we are looking for a rare event. In a
    business context, these rare events could be exceptional cases or outliers, which
    might carry significant implications. For instance, they may denote unusual customer
    behavior or a unique sales trend, potentially marking opportunities or threats
    that require strategic attention.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过查看支持度，我们就可以了解模式出现的稀有性。低支持度意味着我们在寻找一个稀有事件。在商业环境中，这些稀有事件可能是异常情况或离群值，可能具有重要的意义。例如，它们可能代表不寻常的客户行为或独特的销售趋势，这可能标志着需要战略关注的机会或威胁。
- en: For example, if *itemset*[a] *= {helmet, ball}* appears in two transactions
    out of six, then support *(itemset*[a]*) = 2/6 = 0.33*.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果*项目集*[a] = {头盔, 球}出现在六笔交易中的两笔中，那么支持度*(项目集*[a]*) = 2/6 = 0.33*。
- en: Confidence
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信心
- en: The confidence is a number that quantifies how strongly we can associate the
    left side (*X*) with the right side (*Y*) by calculating the conditional probability.
    It calculates the probability that event *X* will lead toward event *Y*, given
    that event *X* occurred.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 信心是一个数值，它通过计算条件概率量化了我们可以多大程度地将左侧（*X*）与右侧（*Y*）关联。它计算了在事件*X*发生的情况下，事件*X*导致事件*Y*发生的概率。
- en: Mathematically, consider the rule *X* ⇒ *Y*.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，考虑规则*X* ⇒ *Y*。
- en: 'The confidence of this rule is represented as confidence(*X* ⇒ *Y* ) and is
    measured as follows:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 该规则的信心表示为信心(*X* ⇒ *Y* )，其测量方式如下：
- en: '![](img/B18046_06_014.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_06_014.png)'
- en: 'Let’s look at an example. Consider the following rule:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子。考虑以下规则：
- en: '{*helmet*, *ball*} ⇒ {*wickets*}'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '{*头盔*, *球*} ⇒ {*球门柱*}'
- en: 'The confidence of this rule is calculated by the following formula:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 该规则的信心通过以下公式计算：
- en: '![](img/B18046_06_015.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_06_015.png)'
- en: This means that if someone has {helmet, balls} in the basket, then there is
    a 0.5 or 50% probability that they will also have wickets to go with it.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果某人购物篮中有{头盔，球}，则有0.5或50%的概率他们还会购买搭配的球棒。
- en: Lift
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升度
- en: 'Another way to estimate the quality of a rule is by calculating the lift. The
    lift returns a number that quantifies how much improvement has been achieved by
    a rule at predicting the result compared to just assuming the result at the right-hand
    side of the equation. “Improvement” refers to the degree of enhancement or betterment
    achieved by a rule in its ability to predict an outcome compared to a baseline
    or default approach. It represents the extent to which the rule provides more
    accurate or insightful predictions than what would be obtained by making assumptions
    solely based on the right-hand side of the equation. If the *X* and *Y* itemsets
    are independent, then the lift is calculated as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 估算规则质量的另一种方法是计算提升度。提升度返回一个数字，量化规则在预测结果时相比仅假设方程右侧结果所取得的改进。“改进”指的是规则在预测结果时，相比于基线或默认方法所带来的增强或改善程度。它表示规则提供的预测比仅依赖方程右侧假设所获得的预测更准确或更具洞察力的程度。如果*X*和*Y*项集是独立的，则提升度按如下方式计算：
- en: '![](img/B18046_06_016.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18046_06_016.png)'
- en: Algorithms for association analysis
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关联分析的算法
- en: 'In this section, we will explore the following two algorithms that can be used
    for association analysis:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索以下两种可用于关联分析的算法：
- en: '**Apriori algorithm**: Proposed by Agrawal, R. and Srikant in 1994.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apriori算法**：由Agrawal, R. 和Srikant于1994年提出。'
- en: '**FP-growth algorithm**: An improvement suggested by Han et al. in 2001.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FP-growth算法**：由Han等人于2001年提出的改进方案。'
- en: Let’s look at each of these algorithms.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一看一下这些算法。
- en: Apriori algorithm
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apriori算法
- en: The apriori algorithm is an iterative and multiphase algorithm used to generate
    association rules. It is based on a generation-and-test approach.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori算法是一种迭代和多阶段的算法，用于生成关联规则。它基于生成与测试方法。
- en: 'Before executing the apriori algorithm, we need to define two variables: support[threshold]
    and Confidence[threshold].'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行Apriori算法之前，我们需要定义两个变量：支持度[阈值]和置信度[阈值]。
- en: 'The algorithm consists of the following two phases:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法包括以下两个阶段：
- en: '**Candidate-generation phase**: It generates the candidate itemsets, which
    contain sets of all itemsets above support[threshold].'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**候选生成阶段**：它生成候选项集，其中包含所有支持度[阈值]以上的项集。'
- en: '**Filter phase**: It filters out all rules below the expected confidence[threshold].'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**筛选阶段**：它会过滤掉所有低于期望置信度[阈值]的规则。'
- en: After filtering, the resulting rules are the answer.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 经过筛选后，得到的规则即为答案。
- en: Limitations of the apriori algorithm
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apriori算法的局限性
- en: The major bottleneck in the apriori algorithm is the generation of candidate
    rules in Phase 1—for example, ![](img/B18046_06_017.png) = {item[1], item[2],
    . . . , item[m]} can produce 2^m possible itemsets. Because of its multiphase
    design, it first generates these itemsets and then works toward finding the frequent
    itemsets. This limitation is a huge performance bottleneck and makes the apriori
    algorithm unsuitable for larger items because it generates too many itemsets before
    it can find frequent items, which will have an effect on the time taken.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori算法的主要瓶颈是在第一阶段生成候选规则——例如，![](img/B18046_06_017.png) = {项[1]，项[2]，...，项[m]}可以产生2^m个可能的项集。由于其多阶段设计，它首先生成这些项集，然后再寻找频繁项集。这个限制是一个巨大的性能瓶颈，使得Apriori算法不适用于较大的项集，因为它在找到频繁项集之前会生成过多的项集，这会影响所需时间。
- en: Let us now look into the FP-growth algorithm.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解FP-growth算法。
- en: FP-growth algorithm
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FP-growth算法
- en: 'The **frequent pattern growth** (**FP-growth**) algorithm is an improvement
    on the apriori algorithm. It starts by showing the frequent transaction FP-tree,
    which is an ordered tree. It consists of two steps:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '**频繁模式增长**（**FP-growth**）算法是对Apriori算法的改进。它首先展示频繁事务FP树，这是一棵有序树。它包括两个步骤：'
- en: Populating the FP-tree
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充FP树
- en: Mining frequent patterns
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挖掘频繁模式
- en: Let’s look at these steps one by one.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步看这些步骤。
- en: Populating the FP-tree
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填充FP树
- en: 'Let’s consider the transaction data shown in the following table. Let’s first
    represent it as a sparse matrix:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑下表所示的交易数据。首先，我们将其表示为稀疏矩阵：
- en: '| **ID** | **Bat** | **Wickets** | **Pads** | **Helmet** | **Ball** |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| **ID** | **球棒** | **球票** | **护垫** | **头盔** | **球** |'
- en: '| 1 | 0 | 1 | 1 | 0 | 0 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 | 1 | 0 | 0 |'
- en: '| 2 | 1 | 1 | 1 | 1 | 0 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 1 | 1 | 1 | 0 |'
- en: '| 3 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 0 | 0 | 1 | 1 |'
- en: '| 4 | 1 | 0 | 1 | 1 | 0 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1 | 0 | 1 | 1 | 0 |'
- en: 'Let’s calculate the frequency of each item and sort them in descending order
    by frequency:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算每个项的频率并按频率降序排列：
- en: '| **Item** | **Frequency** |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| **项** | **频率** |'
- en: '| pads | 3 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 护垫 | 3 |'
- en: '| helmets | 3 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 头盔 | 3 |'
- en: '| bats | 2 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 球棒 | 2 |'
- en: '| wickets | 2 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 门柱 | 2 |'
- en: '| balls | 1 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 球 | 1 |'
- en: 'Now let’s rearrange the transaction-based data based on the frequency:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们根据频率重新排列基于交易的数据：
- en: '| **ID** | **Original Items** | **Reordered Items** |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| **ID** | **原始项** | **重新排序的项** |'
- en: '| t1 | Wickets, pads | Pads, wickets |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| t1 | 门柱，护垫 | 护垫，门柱 |'
- en: '| t2 | Bat, wickets, pads, helmets | Helmets, pads, wickets, bats |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| t2 | 球棒，门柱，护垫，头盔 | 头盔，护垫，门柱，球棒 |'
- en: '| t3 | Helmets, balls | Helmets, balls |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| t3 | 头盔，球 | 头盔，球 |'
- en: '| t4 | Bats, pads, helmets | Helmets, pads, bats |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| t4 | 球棒，护垫，头盔 | 头盔，护垫，球棒 |'
- en: To build the FP-tree, let’s start with the first branch of the FP-tree. The
    FP-tree starts with a **Null** as the root. To build the tree, we can represent
    each item with a node, as shown in the following diagram (the tree representation
    of t[1] is shown here).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建FP树，让我们从FP树的第一分支开始。FP树以**空**作为根节点。为了构建树，我们可以用节点表示每个项，如下图所示（这里显示的是t[1]的树表示）。
- en: 'Note that the label of each node is the name of the item and its frequency
    is appended after the colon. Also, note that the **pads** item has a frequency
    of 1:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个节点的标签是项的名称，频率则附加在冒号后面。同时请注意，**护垫**项的频率为1：
- en: '![Diagram  Description automatically generated](img/B18046_06_13.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![图示  描述自动生成](img/B18046_06_13.png)'
- en: 'Figure 6.13: FP-tree representation first transaction'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13：FP树表示第一个交易
- en: Using the same pattern, let’s draw all four transactions, resulting in the full
    FP-tree. The FP-tree has four leaf nodes, each representing the itemset associated
    with the four transactions. Note that we need to count the frequencies of each
    item and need to increase it when used multiple times—for example, when adding
    t[2] to the FP-tree, the frequency of **helmets** was increased to two. Similarly,
    while adding t[4], it was increased again to three.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 运用相同的模式，让我们绘制所有四个交易，从而得到完整的FP树。FP树有四个叶节点，每个叶节点表示与四个交易相关联的项集。请注意，我们需要计算每个项的频率，并在多次使用时增加其频率——例如，当将t[2]添加到FP树时，**头盔**的频率增加到2。同样，在添加t[4]时，它的频率再次增加到3。
- en: 'The resulting tree is shown in the following diagram:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的树如以下图所示：
- en: '![Diagram, schematic  Description automatically generated](img/B18046_06_14.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![图示，示意图  描述自动生成](img/B18046_06_14.png)'
- en: 'Figure 6.14: FP-tree representing all transactions'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14：FP树表示所有交易
- en: 'Note that the FP-tree generated in the preceding diagram is an ordered tree.
    This leads us to the second phase of the FP-growth tree: mining frequent patterns.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的图示中生成的FP树是一个有序树。这引出了FP-growth树的第二阶段：挖掘频繁模式。
- en: Mining frequent patterns
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 挖掘频繁模式
- en: The second phase of the FP-growth process is focused on mining the frequent
    patterns from the FP-tree. Creating an ordered tree is a deliberate move, aimed
    at producing a data structure that facilitates effortless navigation when hunting
    for these frequent patterns.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: FP-growth过程的第二阶段专注于从FP树中挖掘频繁模式。创建有序树是一个故意的步骤，旨在生成一种便于在寻找这些频繁模式时轻松导航的数据结构。
- en: We start this journey from a leaf node, which is an end node, and traverse upward.
    As an example, let’s begin from one of the leaf node items, “bats.” Our next task
    is to figure out the conditional pattern base for “bat.” The term “conditional
    pattern base” might sound complex, but it’s merely a collection of all paths that
    lead from a specific leaf node item to the root of the tree. For our item “bat,”
    the conditional pattern base will comprise all paths from the “bat” node to the
    top of the tree. At this point, understanding the difference between ordered and
    unordered trees becomes critical. In an ordered tree such as the FP-tree, the
    items adhere to a fixed order, simplifying the pattern mining process. An unordered
    tree doesn’t provide this structured setup, which could make discovering frequent
    patterns more challenging.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个叶节点开始这段旅程，这是一个终点节点，并向上遍历。例如，让我们从一个叶节点项目“球棒”开始。我们接下来的任务是找出“球棒”的条件模式基。术语“条件模式基”可能听起来很复杂，但它仅仅是指从特定叶节点项目到树根的所有路径的集合。对于我们的项目“球棒”，条件模式基将包括从“球棒”节点到树顶部的所有路径。此时，理解有序和无序树之间的差异变得至关重要。在有序树（如FP树）中，项目遵循固定顺序，简化了模式挖掘过程。无序树不提供这种结构化设置，这可能使频繁模式的发现更具挑战性。
- en: 'When computing the conditional pattern base for “bats,” we are essentially
    mapping out all paths from the “bats” node to the root. These paths reveal the
    items that often co-occur with “bat” in transactions. In essence, we’re following
    the “branch” of the tree associated with “bat” to understand its relationships
    with other items. This visual illustration clarifies where we get this information
    from and how the FP-tree assists in illuminating frequent patterns in transaction
    data. The conditional pattern base for **bat** will be as follows:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算“球棒”的条件模式基时，我们实质上是映射从“球棒”节点到根的所有路径。这些路径显示了在交易中与“球棒”经常共现的项目。实质上，我们正在遵循与“球棒”相关的树的“分支”，以理解它与其他项目的关系。这种视觉上的说明阐明了我们从哪里获取这些信息以及FP树如何帮助阐明交易数据中的频繁模式。**球棒**的条件模式基如下所示：
- en: '| Wicket: 1 | Pads: 1 | Helmet: 1 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 曲棍球: 1 | 护具: 1 | 头盔: 1 |'
- en: '| Pad: 1 | Helmet: 1 |  |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 护腕: 1 | 头盔: 1 |  |'
- en: 'The **frequent pattern** for **bat** will be as follows:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '**球棒**的**频繁模式**将如下所示：'
- en: '{*wicket*, *pads*, *helmet*}: *bat*'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '{*曲棍球*, *护具*, *头盔*}: *球棒*'
- en: '{*pad*, *helmet*}: *bat*'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '{*护腕*, *头盔*}: *球棒*'
- en: Code for using FP-growth
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用FP-growth的代码
- en: 'Let’s see how we can generate association rules using the FP-growth algorithm
    in Python. For this, we will be using the `pyfpgrowth` package. First, if we have
    never used `pyfpgrowth` before, let’s install it first:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用Python中的FP-growth算法生成关联规则。为此，我们将使用`pyfpgrowth`包。首先，如果我们以前从未使用过`pyfpgrowth`，让我们先安装它：
- en: '[PRE24]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, let’s import the packages that we need to use to implement this algorithm:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们导入我们实现此算法所需的包：
- en: '[PRE25]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now we will create the input data in the form of `transactionSet`:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建输入数据，形式为`transactionSet`：
- en: '[PRE26]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once the input data is generated, we will generate patterns that will be based
    on the parameters that we passed in `find_frequent_patterns()`. Note that the
    second parameter passed to this function is the minimum support, which is 1 in
    this case:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成了输入数据，我们将基于我们在`find_frequent_patterns()`中传递的参数生成图案。请注意，传递给此函数的第二个参数是最小支持度，在本例中为1：
- en: '[PRE28]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The patterns have been generated. Now let’s print the patterns. The patterns
    list the combinations of items with their supports:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图案已生成。现在让我们打印这些图案。这些图案列出了项目组合及其支持度：
- en: '[PRE29]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now let’s generate the rules:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们生成规则：
- en: '[PRE31]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Each rule has a left-hand side and a right-hand side, separated by a colon (`:`).
    It also gives us the support of each of the rules in our input dataset.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 每条规则都有左手边和右手边，用冒号(`:`)分隔。它还为我们提供了数据集中每条规则的支持度。
- en: Summary
  id: totrans-423
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at various unsupervised machine learning techniques.
    We looked at the circumstances in which it is a good idea to try to reduce the
    dimensionality of the problem we are trying to solve and the different methods
    of doing this. We also studied the practical examples where unsupervised machine
    learning techniques can be very helpful, including market basket analysis.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了各种无监督机器学习技术。我们研究了在何种情况下尝试降低我们试图解决的问题的维度是一个好主意，以及如何通过不同的方法实现这一点。我们还研究了无监督机器学习技术在哪些实际示例中可能非常有帮助，包括市场篮分析。
- en: In the next chapter, we will look at the various supervised learning techniques.
    We will start with linear regression and then we will look at more sophisticated
    supervised machine learning techniques, such as decision-tree-based algorithms,
    SVM, and XGBoost. We will also study the Naive Bayes algorithm, which is best
    suited for unstructured textual data.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨各种监督学习技术。我们将从线性回归开始，然后介绍更复杂的监督学习算法，如基于决策树的算法、SVM 和 XGBoost。我们还将学习朴素贝叶斯算法，它最适用于非结构化文本数据。
- en: Learn more on Discord
  id: totrans-426
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多信息
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入本书的 Discord 社区——在这里你可以分享反馈、向作者提问并了解新版本——请扫描下面的二维码：
- en: '[https://packt.link/WHLel](https://packt.link/WHLel)'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/WHLel](https://packt.link/WHLel)'
- en: '![](img/QR_Code1955211820597889031.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1955211820597889031.png)'
