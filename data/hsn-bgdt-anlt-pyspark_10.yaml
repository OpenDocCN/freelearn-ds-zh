- en: Saving Data in the Correct Format
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据保存在正确的格式中
- en: In the previous chapters, we were focusing on processing and loading data. We
    learned about transformations, actions, joining, shuffling, and other aspects
    of Spark.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们专注于处理和加载数据。我们学习了有关转换、操作、连接、洗牌和Spark的其他方面。
- en: In this chapter, we will learn how to save data in the correct format and also
    save data in plain text format using Spark's standard API. We will also leverage
    JSON as a data format, and learn how to use standard APIs to save JSON. Spark
    has a CSV format and we will leverage that format as well. We will then learn
    more advanced schema-based formats, where support is required to import third-party
    dependencies. Following that, we will use Avro with Spark and learn how to use
    and save the data in a columnar format known as Parquet. By the end of this chapter,
    we will have also learned how to retrieve data to validate whether it is stored
    in the proper way.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何以正确的格式保存数据，还将使用Spark的标准API以纯文本格式保存数据。我们还将利用JSON作为数据格式，并学习如何使用标准API保存JSON。Spark有CSV格式，我们也将利用该格式。然后，我们将学习更高级的基于模式的格式，其中需要支持导入第三方依赖项。接下来，我们将使用Avro与Spark，并学习如何使用和保存列格式的数据，即Parquet。到本章结束时，我们还将学会如何检索数据以验证其是否以正确的方式存储。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Saving data in plain text format
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以纯文本格式保存数据
- en: Leveraging JSON as a data format
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用JSON作为数据格式
- en: Tabular formats – CSV
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格式 - CSV
- en: Using Avro with Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Avro与Spark
- en: Columnar formats – Parquet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列格式 - Parquet
- en: Saving data in plain text format
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以纯文本格式保存数据
- en: 'In this section, we will learn how to save data in plain text format. The following
    topics will be covered:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何以纯文本格式保存数据。将涵盖以下主题：
- en: Saving data in plain text format
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以纯文本格式保存数据
- en: Loading plain text data
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载纯文本数据
- en: Testing
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试
- en: 'We will save our data in plain text format and investigate how to save it into
    the Spark directory. We will then load the plain text data, and then test and
    save it to check whether we can yield the same results code. This is our `SavePlainText.scala`
    file:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以纯文本格式保存我们的数据，并研究如何将其保存到Spark目录中。然后我们将加载纯文本数据，然后测试并保存以检查我们是否可以产生相同的结果代码。这是我们的`SavePlainText.scala`文件：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will need a `FileName` variable, which, in our case, will be a folder name,
    and Spark will then create a couple of files underneath:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要一个`FileName`变量，在我们的情况下，它将是一个文件夹名称，然后Spark将在其下创建一些文件：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will use `BeforeAndAfterEach` in our test case to clean our directory after
    every test, which means that the path should be deleted recursively. The whole
    path is deleted after the test, as it is required to rerun the tests without a
    failure. We need to comment the following code out for the first run to investigate
    the structure of the saved text file:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在我们的测试用例中使用`BeforeAndAfterEach`来清理我们的目录，这意味着路径应该被递归删除。测试后整个路径将被删除，因为需要重新运行测试而没有失败。我们需要注释掉以下代码，以便在第一次运行时调查保存的文本文件的结构：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will then create an RDD of two transactions, `UserTransaction("a", 100)` and
    `UserTransaction("b", 200)`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将创建两个交易的RDD，`UserTransaction("a", 100)`和`UserTransaction("b", 200)`：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will then `coalesce` our data to one partition. `coalesce()` is a very important
    aspect. If we want to save our data in a single file, we need to `coalesce` it
    into one, but there is an important implication of doing so:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将我们的数据合并为一个分区。`coalesce()`是一个非常重要的方面。如果我们想将数据保存在单个文件中，我们需要将其合并为一个，但这样做有一个重要的含义：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If we `coalesce` it to a single file, then only one executor can save the data
    to our system. This means that saving the data will be very slow and, also, there
    will be a risk of being out of memory because all data will be sent to one executor.
    Generally, in the production environment, we save it as many partitions, based
    on the executors available, or even multiplied by its own factor. So, if we have
    16 executors, then we can save it to `64`. But this results in `64` files. For
    test purposes, we will save it to one file, as shown in the preceding code snippet:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将其合并为一个文件，那么只有一个执行程序可以将数据保存到我们的系统中。这意味着保存数据将非常缓慢，并且还存在内存不足的风险，因为所有数据将被发送到一个执行程序。通常，在生产环境中，我们根据可用的执行程序将其保存为多个分区，甚至乘以自己的因子。因此，如果我们有16个执行程序，那么我们可以将其保存为`64`。但这会导致`64`个文件。出于测试目的，我们将保存为一个文件，如前面的代码片段所示：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we''ll load the data. We only need to pass the filename to the `TextFile`
    method and it will return `fromFile`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将加载数据。我们只需要将文件名传递给`TextFile`方法，它将返回`fromFile`：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then assert our data, which will yield `theSameElementsAS List`, `UserTransaction(a,100)`,
    and `UserTransaction(b,200)`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们断言我们的数据，这将产生`theSameElementsAS List`，`UserTransaction(a,100)`和`UserTransaction(b,200)`：
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The important thing to note is that for a list of strings, Spark doesn't know
    the schema of our data because we are saving it in plain text.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的重要事项是，对于字符串列表，Spark不知道我们的数据模式，因为我们将其保存为纯文本。
- en: This is one of the points to note when it comes to saving plain text, because
    loading the data is not easy, since we need to manually map every string to `UserTransaction`.
    So, we will have to parse every record manually, but, for test purposes, we will
    treat our transaction as strings.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在保存纯文本时需要注意的一点，因为加载数据并不容易，因为我们需要手动将每个字符串映射到`UserTransaction`。因此，我们将不得不手动解析每条记录，但是，出于测试目的，我们将把我们的交易视为字符串。
- en: 'Now, let''s start the test and see the structure of the folder that was created:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始测试并查看创建的文件夹的结构：
- en: '![](img/8d1da1f2-4d80-4e73-ac6d-ac20b973bf51.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d1da1f2-4d80-4e73-ac6d-ac20b973bf51.png)'
- en: 'In the preceding screenshot, we can see that our test passed and that we get
    `transactions.txt`. Inside the folder, we have four files. The first one is `._SUCCESS.crc`,
    which means that the save succeeded. Next, we have `.part-00000.crc`, to control
    and validate that everything worked properly, which means that the save was proper.
    We then have `_SUCCESS` and `part-00000`, where both files have checksum, but
    `part-00000` has all the data as well. Then, we also have `UserTransaction(a,100)`
    and `UserTransaction(b,200)`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，我们可以看到我们的测试通过了，我们得到了`transactions.txt`。在文件夹中，我们有四个文件。第一个是`._SUCCESS.crc`，这意味着保存成功。接下来，我们有`.part-00000.crc`，用于控制和验证一切是否正常工作，这意味着保存是正确的。然后，我们有`_SUCCESS`和`part-00000`，这两个文件都有校验和，但`part-00000`也包含了所有的数据。然后，我们还有`UserTransaction(a,100)`和`UserTransaction(b,200)`：
- en: '![](img/a829d808-2760-4b34-bdf4-6e4b5a64fc13.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a829d808-2760-4b34-bdf4-6e4b5a64fc13.png)'
- en: In the next section, we will learn what will happen if we increment the number
    of partitions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如果增加分区数量会发生什么。
- en: Leveraging JSON as a data format
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用JSON作为数据格式
- en: 'In this section, we will leverage JSON as a data format and save our data in
    JSON. The following topics will be covered:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将利用JSON作为数据格式，并将我们的数据保存为JSON。以下主题将被涵盖：
- en: Saving data in JSON format
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以JSON格式保存数据
- en: Loading JSON data
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载JSON数据
- en: Testing
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试
- en: This data is human-readable and gives us more meaning than simple plain text
    because it carries some schema information, such as a field name. We will then
    learn how to save data in JSON format and load our JSON data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据是人类可读的，并且比简单的纯文本给我们更多的含义，因为它携带了一些模式信息，比如字段名。然后，我们将学习如何以JSON格式保存数据并加载我们的JSON数据。
- en: 'We will first create a DataFrame of `UserTransaction("a", 100)` and `UserTransaction("b",
    200)`, and use `.toDF()` to save the DataFrame API:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先创建一个`UserTransaction("a", 100)`和`UserTransaction("b", 200)`的DataFrame，并使用`.toDF()`保存DataFrame
    API：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will then issue `coalesce()` and, this time, we will take the value as `2`,
    and we will have two resulting files. We will then issue the `write.format` method
    and, for the same, we need to specify a format, for which we will use the `json`
    format:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将发出`coalesce()`，这次我们将取值为`2`，并且我们将得到两个结果文件。然后我们将发出`write.format`方法，并且需要指定一个格式，我们将使用`json`格式：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we use the unsupported format, we will get an exception. Let''s test this
    by entering the source as `not`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用不支持的格式，我们将得到一个异常。让我们通过将源输入为`not`来测试这一点：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will get exceptions such as ''`This format is not expected`'', ''`Failed
    to find data source: not`'', and ''`There is no such data source`'':'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到诸如“此格式不是预期的”、“找不到数据源：not”和“没有这样的数据源”等异常：
- en: '![](img/6c3f7d35-803b-470a-bea8-be3b9f91940a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c3f7d35-803b-470a-bea8-be3b9f91940a.png)'
- en: 'In our original JSON code, we will specify the format and we need to save it
    to `FileName`. If we want to read, we need to specify it as `read` mode and also
    add a path to the folder:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们原始的JSON代码中，我们将指定格式，并且需要将其保存到`FileName`。如果我们想要读取，我们需要将其指定为`read`模式，并且还需要添加一个文件夹的路径：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'On this occasion, let''s comment out `afterEach()` to investigate the produced
    JSON:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，让我们注释掉`afterEach()`来调查生成的JSON：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s start the test:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始测试：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code output, we can see that our test passed and that the DataFrame
    includes all the meaningful data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码输出中，我们可以看到我们的测试通过了，并且DataFrame包含了所有有意义的数据。
- en: From the output, we can see that DataFrame has all the schema required. It has
    `amount` and `userId`, which is very useful.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，我们可以看到DataFrame具有所需的所有模式。它有`amount`和`userId`，这非常有用。
- en: The `transactions.json` folder has two parts—one part is `r-00000`, and the
    other part is `r-00001`, because we issued two partitions. If we save the data
    in a production system with 100 partitions, we will end up with 100 part files
    and, furthermore, every part file will have a CRC checksum file.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`transactions.json`文件夹有两部分——一部分是`r-00000`，另一部分是`r-00001`，因为我们发出了两个分区。如果我们在生产系统中保存数据有100个分区，我们最终会得到100个部分文件，而且每个部分文件都会有一个CRC校验和文件。'
- en: 'This is the first file:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第一个文件：
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we have a JSON file with schema and, hence, we have a `userID` field and
    `amount` field.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一个带有模式的JSON文件，因此我们有一个`userID`字段和`amount`字段。
- en: 'On the other hand, we have the second file with the second record with `userID`
    and `amount` as well:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们有第二个文件，其中包含第二条记录，包括`userID`和`amount`：
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The advantage of this is that Spark is able to infer the data from the schema
    and is loaded in a formatted DataFrame with proper naming and types. The disadvantage,
    however, is that every record has some additional overhead. Every record needs
    to have a string in it and, in each string, if we have a file that has millions
    of files and we are not compressing it, there will be substantial overhead, which
    is not ideal.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的好处是Spark能够从模式中推断出数据，并且以格式化的DataFrame加载，具有适当的命名和类型。然而，缺点是每条记录都有一些额外的开销。每条记录都需要在其中有一个字符串，并且在每个字符串中，如果我们有一个包含数百万个文件的文件，并且我们没有对其进行压缩，那么将会有相当大的开销，这是不理想的。
- en: JSON is human-readable but, on the other hand, it consumes a lot of resources,
    just like the CPU for compressing, reading, and writing, and also the disk and
    memory for the overhead. Apart from JSON, there are better formats that we will
    cover in the following sections.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是人类可读的，但另一方面，它消耗了大量资源，就像CPU用于压缩、读取和写入，以及磁盘和内存用于开销一样。除了JSON之外，还有更好的格式，我们将在接下来的部分中介绍。
- en: In the next section, we will look at the tabular format, where we will cover
    a CSV file that is often used to import to Microsoft Excel or Google spreadsheet.
    This is also a very useful format for data scientists, but only while using smaller
    datasets.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将查看表格格式，我们将介绍一个经常用于导入到Microsoft Excel或Google电子表格的CSV文件。这对数据科学家也是非常有用的格式，但仅在使用较小的数据集时。
- en: Tabular formats – CSV
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表格式——CSV
- en: 'In this section, we will be covering text data, but in a tabular format—CSV.
    The following topics will be covered:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍文本数据，但以表格格式——CSV。以下主题将被涵盖：
- en: Saving data in CSV format
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以CSV格式保存数据
- en: Loading CSV data
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载CSV数据
- en: Testing
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试
- en: Saving CSV files is even more involved than JSON and plain text because we need
    to specify whether we want to retain headers of our data in our CSV file.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 保存CSV文件比JSON和纯文本更复杂，因为我们需要指定是否要在CSV文件中保留数据的头信息。
- en: 'First, we will create a DataFrame:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个DataFrame：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we will use the `write` format CSV. We also need to specify that we don''t
    want to include the `header` option in it:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用`write`格式CSV。我们还需要指定我们不想在其中包含`header`选项：
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We will then perform a test to verify whether the condition is `true` or `false`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将进行测试以验证条件是`true`还是`false`：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Also, we don't need to add any additional dependency to support CSV, as required
    in the previous versions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们无需添加任何额外的依赖来支持CSV，如以前的版本所需。
- en: 'We will then specify the `read` mode, which should be similar to the `write`
    mode, and we need to specify whether we have a `header` or not:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将指定应该与`write`模式相似的`read`模式，并且我们需要指定是否有`header`：
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s start the test and check the output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始测试并检查输出：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code output, we can see that the data is loaded, but we lost
    our schema. `c0` and `c1` are the aliases for column 0 (`c0`) and column 1 (`c1`)
    that were created by Spark.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码输出中，我们可以看到数据已加载，但我们丢失了我们的模式。`c0`和`c1`是由Spark创建的列0（`c0`）和列1（`c1`）的别名。
- en: 'So, if we are specifying that the `header` should retain that information,
    let''s specify the `header` at the `write` and also at the `read`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们指定`header`应保留该信息，让我们在`write`和`read`时指定`header`：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will specify that the `header` should retain our information. In the following
    output, we can see that the information regarding the schema was perceived throughout
    the read and write operation:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将指定`header`应保留我们的信息。在以下输出中，我们可以看到关于模式的信息在读写操作中被感知到：
- en: '[PRE23]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s see what happens if we `write` with the `header` and `read` without
    it. Our test should fail, as demonstrated in the following code screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果我们在`write`时使用`header`，而在`read`时不使用它会发生什么。我们的测试应该失败，如下面的代码截图所示：
- en: '![](img/0b41f63f-a78b-462a-b058-f27cde819782.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b41f63f-a78b-462a-b058-f27cde819782.png)'
- en: In the preceding screenshot, we can see that our test failed because we don't
    have a schema as we were reading without headers. The first record, which was
    a `header`, was treated as the column value.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们可以看到我们的测试失败了，因为我们没有模式，因为我们在没有头的情况下进行读取。第一条记录，也就是`header`，被视为列值。
- en: 'Let''s try a different situation, where we are writing without `header` and
    reading with `header`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个不同的情况，我们在没有`header`的情况下进行写入，并在有`header`的情况下进行读取：
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Our test will fail again because this time, we treated our first record as the
    header record.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试将再次失败，因为这一次，我们将我们的第一条记录视为头记录。
- en: 'Let''s set both the read and write operations with `header` and test our code
    after removing the comment we added previously:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将读和写操作都设置为`header`并在之前添加的注释后测试我们的代码：
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The CSV and JSON files will have schema, but with less overhead. Therefore,
    it could be even better than JSON.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: CSV和JSON文件将具有模式，但开销较小。因此，它甚至可能比JSON更好。
- en: In the next section, we'll see how we can use a schema-based format as a whole
    with Spark.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何将基于模式的格式作为整体与Spark一起使用。
- en: Using Avro with Spark
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Avro与Spark
- en: So far, we have looked at text-based files. We worked with plain text, JSON,
    and CSV. JSON and CSV are better than plain text because they carry some schema
    information.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过基于文本的文件。我们使用纯文本、JSON和CSV。JSON和CSV比纯文本更好，因为它们携带了一些模式信息。
- en: 'In this section, we''ll be looking at an advanced schema, known as Avro. The
    following topics will be covered:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究一个名为Avro的高级模式。将涵盖以下主题：
- en: Saving data in Avro format
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以Avro格式保存数据
- en: Loading Avro data
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载Avro数据
- en: Testing
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试
- en: Avro has a schema and data embedded within it. This is a binary format and is
    not human-readable. We will learn how to save data in Avro format, load it, and
    then test it.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Avro具有嵌入其中的模式和数据。这是一种二进制格式，不是人类可读的。我们将学习如何以Avro格式保存数据，加载数据，然后进行测试。
- en: 'First, we will create our user transaction:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建我们的用户交易：
- en: '[PRE26]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We will then do a `coalesce` and write an Avro:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将进行`coalesce`并写入Avro：
- en: '[PRE27]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: While using CSV, we specified the format like CSV, and, when we specified JSON,
    this, too, was a format. But in Avro, we have a method. This method is not a standard
    Spark method; it is from a third-party library. To have Avro support, we need
    to access `build.sbt` and add `spark-avro` support from `com.databricks`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用CSV时，我们指定了像CSV这样的格式，当我们指定JSON时，这也是一个格式。但是在Avro中，我们有一个方法。这种方法不是标准的Spark方法；它来自第三方库。为了具有Avro支持，我们需要访问`build.sbt`并从`com.databricks`添加`spark-avro`支持。
- en: 'We then need to import the proper method. We will import `com.databricks.spark.avro._` to
    give us the implicit function that is extending the Spark DataFrame:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要导入适当的方法。我们将导入`com.databricks.spark.avro._`以给我们扩展Spark DataFrame的隐式函数：
- en: '[PRE28]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We are actually using an Avro method and we can see that `implicit class` takes
    a `DataFrameWriter` class, and writes our data in Spark format.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上我们正在使用一个Avro方法，我们可以看到`implicit class`接受一个`DataFrameWriter`类，并以Spark格式写入我们的数据。
- en: 'In the `coalesce` code we used previously, we can use `write`, specify the
    format, and execute a `com.databricks.spark.avro` class. `avro` is a shortcut
    to not write `com.databricks.spark.avro` as a whole string:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前使用的`coalesce`代码中，我们可以使用`write`，指定格式，并执行`com.databricks.spark.avro`类。`avro`是一个快捷方式，不需要将`com.databricks.spark.avro`作为整个字符串写入：
- en: '[PRE29]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In short, there is no need to specify the format; just apply the implicit `avro` method.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，无需指定格式；只需应用隐式`avro`方法。
- en: 'Let''s comment out the code and remove Avro to check how it saves:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们注释掉代码并删除Avro以检查它是如何保存的：
- en: '[PRE30]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If we open the `transactions.avro` folder, we have two parts—`part-r-00000`
    and `part-r-00001`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打开`transactions.avro`文件夹，我们有两部分——`part-r-00000`和`part-r-00001`。
- en: 'The first part will have binary data. It consists of a number of binary records
    and some human-readable data, which is our schema:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分将包含二进制数据。它由许多二进制记录和一些人类可读的数据组成，这就是我们的模式：
- en: '![](img/ea76f872-a5d0-4c5d-a5eb-440c5d453832.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea76f872-a5d0-4c5d-a5eb-440c5d453832.png)'
- en: We have two fields—`user ID`, which is a type string or null, and `name`: `amount`,
    which is an integer. Being a primitive type, JVM cannot have null values. The
    important thing to note is that, in production systems, we have to save really
    large datasets, and there will be thousands of records. The schema is always in
    the first line of every file. If we check the second part as well, we will see
    that there is exactly the same schema and then the binary data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个字段 - `user ID`，它是一个字符串类型或空值，和`name`：`amount`，它是一个整数。作为原始类型，JVM不能有空值。需要注意的重要事情是，在生产系统中，我们必须保存非常大的数据集，将有成千上万条记录。模式始终在每个文件的第一行。如果我们检查第二部分，我们将看到完全相同的模式，然后是二进制数据。
- en: Usually, we have only one or more lines if you have a complex schema, but still,
    it is a very low amount of data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果有复杂的模式，我们只有一行或更多行，但仍然是非常少量的数据。
- en: 'We can see that in the resulting dataset, we have `userID` and `amount`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在生成的数据集中，我们有`userID`和`amount`：
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the preceding code block, we can see that the schema was portrayed in the
    file. Although it is a binary file, we can extract it.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码块中，我们可以看到模式被描绘在文件中。虽然它是一个二进制文件，但我们可以提取它。
- en: In the next section, we will be looking at the columnar format—Parquet.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将研究列格式 - Parquet。
- en: Columnar formats – Parquet
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 列格式 - Parquet
- en: 'In this section, we''ll be looking at the second schema-based format, Parquet.
    The following topics will be covered:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究第二种基于模式的格式Parquet。将涵盖以下主题：
- en: Saving data in Parquet format
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以Parquet格式保存数据
- en: Loading Parquet data
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载Parquet数据
- en: Testing
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试
- en: This is a columnar format, as the data is stored column-wise and not row-wise,
    as we saw in the JSON, CSV, plain text, and Avro files.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种列格式，因为数据是以列方式存储的，而不是以行方式，就像我们在JSON、CSV、纯文本和Avro文件中看到的那样。
- en: This is a very interesting and important format for big data processing and
    for making the process faster. In this section, we will focus on adding Parquet
    support to Spark, saving the data into the filesystem, reloading it again, and
    then testing. Parquet is similar to Avro as it gives you a `parquet` method but
    this time, it is a slightly different implementation.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常有趣和重要的大数据处理格式，可以加快处理过程。在本节中，我们将专注于向Spark添加Parquet支持，将数据保存到文件系统中，重新加载数据，然后进行测试。Parquet与Avro类似，因为它提供了一个`parquet`方法，但这次是一个稍微不同的实现。
- en: In the `build.sbt` file, for the Avro format, we need to add an external dependency,
    but for Parquet, we already have that dependency within Spark. So, Parquet is
    the way to go for Spark because it is inside the standard package.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在`build.sbt`文件中，对于Avro格式，我们需要添加外部依赖，但对于Parquet，我们已经在Spark中有了该依赖。因此，Parquet是Spark的首选，因为它包含在标准包中。
- en: Let's have a look at the logic that's used in the `SaveParquet.scala` file for
    saving and loading Parquet files.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看`SaveParquet.scala`文件中用于保存和加载Parquet文件的逻辑。
- en: 'First, we coalesce the two partitions, specify the format, and then specify
    that we want to save `parquet`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们合并了两个分区，指定了格式，然后指定我们要保存`parquet`：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The `read` method also implements exactly the same method:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`read`方法也实现了完全相同的方法：'
- en: '[PRE33]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s begin this test but, before that, we will comment out the following
    code withing our `SaveParquet.scala` file to see the structure of the files:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始这个测试，但在此之前，我们将在`SaveParquet.scala`文件中注释掉以下代码，以查看文件的结构：
- en: '[PRE34]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: A new `transactions.parquet` folder gets created and we have two parts inside
    it—`part-r-00000` and `part-r-00001`. This time, however, the format is entirely
    binary and there is some metadata embedded with it.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了一个新的`transactions.parquet`文件夹，里面有两个部分 - `part-r-00000`和`part-r-00001`。但这次，格式完全是二进制的，并且嵌入了一些元数据。
- en: We have the metadata embedded and also the `amount` and `userID` fields, which
    are of the `string` type. The part `r-00000` is exactly the same and has the schema
    embedded. Hence, Parquet is also a schema-based format. When we read the data,
    we can see that we have the `userID` and `amount` columns available.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们嵌入了元数据，还有`amount`和`userID`字段，它们是`string`类型。`r-00000`部分完全相同，并且嵌入了模式。因此，Parquet也是一种基于模式的格式。当我们读取数据时，我们可以看到我们有`userID`和`amount`列可用。
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to save data in plain text format. We noticed
    that schema information is lost when we do not load the data properly. We then
    learned how to leverage JSON as a data format and saw that JSON retains the schema,
    but it has a lot of overhead because the schema is for every record. We then learned
    about CSV and saw that Spark has embedded support for it. The disadvantage of
    this approach, however, is that the schema is not about the specific types of
    records, and tabs need to be inferred implicitly. Toward the end of this chapter,
    we covered Avro and Parquet, which have columnar formats that are also embedded
    with Spark.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何以纯文本格式保存数据。我们注意到，当我们没有正确加载数据时，模式信息会丢失。然后我们学习了如何利用JSON作为数据格式，并发现JSON保留了模式，但它有很多开销，因为模式是针对每条记录的。然后我们了解了CSV，并发现Spark对其有嵌入支持。然而，这种方法的缺点是模式不是关于特定类型的记录，并且需要隐式推断制表符。在本章的最后，我们介绍了Avro和Parquet，它们具有列格式，也嵌入了Spark。
- en: In the next chapter, we'll be working with Spark's key/value API.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用Spark的键/值API。
