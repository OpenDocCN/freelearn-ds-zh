- en: 'Chapter 7: Customizing spaCy Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章：自定义spaCy模型
- en: In this chapter, you will learn how to train, store, and use custom statistical
    pipeline components. First, we will discuss when exactly we should perform custom
    model training. Then, you will learn a fundamental step of model training – how
    to collect and label your own data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何训练、存储和使用自定义统计管道组件。首先，我们将讨论何时应该进行自定义模型训练。然后，你将学习模型训练的一个基本步骤——如何收集和标注自己的数据。
- en: In this chapter, you will also learn how to make the best use of **Prodigy**,
    the annotation tool. Next, you will learn how to update an existing statistical
    pipeline component with your own data. We will update the spaCy pipeline's **named
    entity recognizer** (**NER**) component with our own labeled data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你还将学习如何充分利用**Prodigy**，这个注释工具。接下来，你将学习如何使用自己的数据更新现有的统计管道组件。我们将使用我们自己的标记数据更新spaCy管道的**命名实体识别器**（NER）组件。
- en: Finally, you will learn how to create a statistical pipeline component from
    scratch with your own data and labels. For this purpose, we will again train an
    NER model. This chapter takes you through a complete machine learning practice,
    including collecting data, annotating data, and training a model for information
    extraction.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将学习如何使用自己的数据和标签从头开始创建统计管道组件。为此，我们将再次训练一个NER模型。本章将带你完成一个完整的机器学习实践，包括收集数据、标注数据和为信息提取训练模型。
- en: 'By the end of this chapter, you''ll be ready to train spaCy models on your
    own data. You''ll have the full skillset of collecting data, preprocessing data
    in to the format that spaCy can recognize, and finally, training spaCy models
    with this data. In this chapter, we''re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将准备好在自己的数据上训练spaCy模型。你将具备收集数据、将数据预处理成spaCy可以识别的格式，以及最终使用这些数据训练spaCy模型的全套技能。在本章中，我们将涵盖以下主要主题：
- en: Getting started with data preparation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始数据准备
- en: Annotating and preparing data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注和准备数据
- en: Updating an existing pipeline component
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新现有的管道组件
- en: Training a pipeline component from scratch
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始训练管道组件
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The chapter code can be found at the book''s GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter07](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter07).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter07](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter07)。
- en: Getting started with data preparation
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始数据准备
- en: In the previous chapters, we saw how to make the best of spaCy's pre-trained
    statistical models (including the **POS tagger**, NER, and **dependency parser**)
    in our applications. In this chapter, we will see how to customize the statistical
    models for our custom domain and data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了如何在我们的应用程序中充分利用spaCy的预训练统计模型（包括**POS标记器**、NER和**依存句法分析器**）。在本章中，我们将看到如何为我们的自定义领域和数据自定义统计模型。
- en: spaCy models are very successful for general NLP purposes, such as understanding
    a sentence's syntax, splitting a paragraph into sentences, and extracting some
    entities. However, sometimes, we work on very specific domains that spaCy models
    didn't see during training.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy模型在通用NLP任务中非常成功，例如理解句子的语法、将段落分割成句子以及提取一些实体。然而，有时我们处理的是spaCy模型在训练期间没有见过的非常具体的领域。
- en: For example, the Twitter text contains many non-regular words, such as hashtags,
    emoticons, and mentions. Also, Twitter sentences are usually just phrases, not
    full sentences. Here, it's entirely reasonable that spaCy's POS tagger performs
    in a substandard manner as the POS tagger is trained on full, grammatically correct
    English sentences.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Twitter文本包含许多非正规词汇，如标签、表情符号和提及。此外，Twitter句子通常只是短语，而不是完整的句子。在这里，spaCy的POS标记器由于是在完整的、语法正确的英语句子上训练的，因此表现不佳，这是完全合理的。
- en: Another example is the medical domain. The medical domain contains many entities,
    such as drug, disease, and chemical compound names. These entities are not expected
    to be recognized by spaCy's NER model because it has no disease or drug entity
    labels. NER does not know anything about the medical domain at all.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是医学领域。医学领域包含许多实体，如药物、疾病和化学化合物名称。这些实体不应该被spaCy的NER模型识别，因为它没有疾病或药物实体标签。NER对医学领域一无所知。
- en: 'Training your custom models requires time and effort. Before even starting
    the training process, you should decide *whether the training is really necessary*.
    To determine whether you really need custom training, you will need to ask yourself
    the following questions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 训练定制模型需要时间和精力。在开始训练过程之前，你应该决定*是否真的需要进行训练*。为了确定你是否真的需要定制训练，你需要问自己以下问题：
- en: Do spaCy models perform well enough on your data?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy模型在你的数据上表现足够好吗？
- en: Does your domain include many labels that are absent in spaCy models?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的领域是否包含许多在spaCy模型中缺失的标签？
- en: Is there a pre-trained model/application in GitHub or elsewhere already? (We
    wouldn't want to reinvent the wheel.)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GitHub或其他地方已经有了预训练的模型/应用程序吗？（我们不希望重新发明轮子。）
- en: Let's discuss these questions in detail in the following sections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的章节中详细讨论这些问题。
- en: Do spaCy models perform well enough on your data?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: spaCy模型在你的数据上表现足够好吗？
- en: 'If the model performs well enough (above 0.75 accuracy), then you can customize
    the model output by means of another spaCy component. For example, let''s say
    we work on the navigation domain and we have utterances such as the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型表现足够好（准确率高于0.75），那么你可以通过另一个spaCy组件来定制模型输出。例如，假设我们在导航领域工作，并且我们有以下这样的语句：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s see what entities spaCy''s NER model outputs for these sentences:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看spaCy的NER模型为这些句子输出了哪些实体：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, `home` isn't recognized as an entity at all, but we want it to be recognized
    as a location entity. Also, spaCy's NER model labels `Oxford Street` as `FAC`,
    which means a building/highway/airport/bridge type entity, which is not what we
    want.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`home`根本不被识别为实体，但我们希望它被识别为地点实体。此外，spaCy的NER模型将`Oxford Street`标记为`FAC`，这意味着建筑/公路/机场/桥梁类型的实体，这并不是我们想要的。
- en: We want this entity to be recognized as `GPE`, a location. Here, we can train
    NER further to recognize street names as `GPE`, as well as also recognizing some
    location words, such as *work*, *home*, and *my mama's house*, as `GPE`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这个实体被识别为`GPE`，即一个地点。在这里，我们可以进一步训练NER来识别街道名称为`GPE`，以及识别一些地点词，如*work*、*home*和*my
    mama's house*为`GPE`。
- en: Another example is the newspaper domain. In this domain, person, place, date,
    time, and organization entities are extracted, but you need one more entity type
    – `vehicle` (car, bus, airplane, and so on). Hence, instead of training from scratch,
    you can add a new entity type by using spaCy's `EntityRuler` (explained in [*Chapter
    4*](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069), *Rule-Based Matching*). Always
    examine your data first and calculate the spaCy models' success rate. If the success
    rate is satisfying, then use other spaCy components to customize.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是报纸领域。在这个领域，提取了人、地点、日期、时间和组织实体，但你还需要一个额外的实体类型——`vehicle`（汽车、公共汽车、飞机等等）。因此，而不是从头开始训练，你可以使用spaCy的`EntityRuler`（在第4章[*](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069)，*基于规则的匹配*）添加一个新的实体类型。始终首先检查你的数据，并计算spaCy模型的成功率。如果成功率令人满意，那么使用其他spaCy组件进行定制。
- en: Does your domain include many labels that are absent in spaCy models?
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你的领域是否包含许多在spaCy模型中缺失的标签？
- en: For instance, in the preceding newspaper example, only one entity label, `vehicle`,
    is missing from the spaCy's NER model's labels. Other entity types are recognized.
    In this case, you don't need custom training.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在先前的报纸示例中，只有`vehicle`这个实体标签在spaCy的NER模型标签中缺失。其他实体类型都被识别了。在这种情况下，你不需要定制训练。
- en: Consider the medical domain again. The entities are diseases, symptoms, drugs,
    dosages, chemical compound names, and so on. This is a specialized and long list
    of entities. Obviously, for the medical domain, you require custom model training.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑医疗领域。实体包括疾病、症状、药物、剂量、化学化合物名称等等。这是一份专业且长的实体列表。显然，对于医疗领域，你需要定制模型训练。
- en: 'If we need custom model training, we usually follow these steps:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要定制模型训练，我们通常遵循以下步骤：
- en: Collect your data.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集你的数据。
- en: Annotate your data.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标注你的数据。
- en: Decide to update an existing model or train a model from scratch.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定更新现有模型或从头开始训练模型。
- en: 'In the data collection step, we decide how much data to collect: 1,000 sentences,
    5,000 sentences, or more. The amount of data depends on the complexity of your
    task and domain. Usually, we start with an acceptable amount of data, make a first
    model training, and see how it performs; then we can add more data and retrain
    the model.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据收集步骤中，我们决定收集多少数据：1,000个句子、5,000个句子，或者更多。数据量取决于你的任务和领域的复杂性。通常，我们从可接受的数据量开始，进行第一次模型训练，看看它的表现；然后我们可以添加更多数据并重新训练模型。
- en: After collecting your dataset, you need to annotate your data in such a way
    that the spaCy training code recognizes it. In the next section, we will see the
    training data format and how to annotate data with spaCy's Prodigy tool.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集您的数据集后，您需要以某种方式标注您的数据，以便spaCy的训练代码能够识别它。在下一节中，我们将了解训练数据格式以及如何使用spaCy的Prodigy工具标注数据。
- en: 'The third point is to decide on training a blank model from scratch or make
    updates to an existing model. Here, the rule of thumb is as follows: if your entities/labels
    are present in the existing model but you don''t see a very good performance,
    then update the model with your own data, such as in the preceding navigation
    example. If your entities are not present in the current spaCy model at all, then
    most probably you need custom training.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个要点是决定从头开始训练一个空白模型，还是对现有模型进行更新。在这里，经验法则是这样的：如果您的实体/标签存在于现有模型中，但您没有看到非常好的性能，那么使用您自己的数据更新模型，例如在先前的导航示例中。如果您的实体根本不在当前的spaCy模型中，那么您可能需要进行定制训练。
- en: Tip
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Don't rush into training your own models. First, examine if you really need
    to customize the models. Always keep in mind that training a model from scratch
    requires data preparation, training a model, and saving it, which means spending
    your time, money, and effort. Good engineering is about spending your resources
    wisely.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 不要急于训练您自己的模型。首先，检查您是否真的需要定制模型。始终记住，从头开始训练一个模型需要数据准备、模型训练和保存，这意味着您将花费时间、金钱和精力。好的工程实践是明智地使用您的资源。
- en: 'We''ll start our journey of building a model with the first step: preparing
    our training data. Let''s move on to the next section and see how to prepare and
    annotate our training data.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从构建模型的第一步开始：准备我们的训练数据。让我们继续到下一节，看看如何准备和标注我们的训练数据。
- en: Annotating and preparing data
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标注和准备数据
- en: The first step of training a model is always preparing training data. You usually
    collect data from customer logs and then turn them into a dataset by dumping the
    data as a CSV file or a JSON file. spaCy model training code works with JSON files,
    so we will be working with JSON files in this chapter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的第一步始终是准备训练数据。您通常从客户日志中收集数据，然后通过将数据作为CSV文件或JSON文件导出，将它们转换为数据集。spaCy模型训练代码与JSON文件一起工作，因此在本章中我们将使用JSON文件。
- en: After collecting our data, we **annotate** our data. Annotation means labeling
    the intent, entities, POS tags, and so on.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集我们的数据后，我们**标注**我们的数据。标注意味着标记意图、实体、词性标签等。
- en: 'This is an example of annotated data:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个标注数据的示例：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you see, we point the statistical algorithm to *what we want the model to
    learn*. In this example, we want the model to learn about the entities, hence,
    we feed examples with entities annotated.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们将统计算法指向**我们希望模型学习的内容**。在这个例子中，我们希望模型学习关于实体的知识，因此我们提供带有标注实体的示例。
- en: Writing down JSON files manually can be error-prone and time-consuming. Hence,
    in this section, we'll also see spaCy's annotation tool, Prodigy, along with an
    open source data annotation tool, **Brat**. Prodigy is not open source or free,
    but we will go over how it works to give you a better view of how annotation tools
    work in general. Brat is open source and immediately available for your use.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 手动编写JSON文件可能会出错且耗时。因此，在本节中，我们还将了解spaCy的标注工具Prodigy，以及一个开源数据标注工具**Brat**。Prodigy不是开源的或免费的，但我们将介绍它是如何工作的，以便您更好地了解标注工具的一般工作原理。Brat是开源的，并且可以立即供您使用。
- en: Annotating data with Prodigy
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Prodigy标注数据
- en: Prodigy is a modern tool for data annotation. We will be using the Prodigy web
    demo ([https://prodi.gy/demo](https://prodi.gy/demo)) to exhibit how an annotation
    tool works.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Prodigy是数据标注的现代工具。我们将使用Prodigy网络演示([https://prodi.gy/demo](https://prodi.gy/demo))来展示标注工具的工作原理。
- en: 'Let''s get started:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: We navigate to the Prodigy web demo and view an example text by Prodigy, to
    be annotated as seen in the following screenshot:![Figure 7.1 – Prodigy interface;
    photo taken from their web demo page
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导航到Prodigy网络演示页面，查看一个由Prodigy提供的示例文本，以便进行标注，如下截图所示：![图7.1 – Prodigy界面；照片来自他们的网络演示页面
- en: '](img/B16570_7_1.jpg)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B16570_7_1.jpg]'
- en: Figure 7.1 – Prodigy interface; photo taken from their web demo page
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.1 – Prodigy界面；照片来自他们的网络演示页面
- en: The preceding screenshot shows an example text that we want to annotate. The
    buttons at the bottom of the screenshot showcase the means to accept this training
    example, to reject this example, or to ignore this example. If the example is
    irrelevant to our domain/task (but involved in the dataset somehow), we ignore
    this example. If the text is relevant and the annotation is good, then we accept
    this example, and it joins our dataset.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述截图显示了我们要标注的示例文本。截图底部的按钮展示了接受此训练示例、拒绝此示例或忽略此示例的方法。如果示例与我们的领域/任务无关（但以某种方式涉及数据集），则忽略此示例。如果文本相关且标注良好，则接受此示例，并将其加入我们的数据集。
- en: 'Next, we''ll label the entities. Labeling an entity is easy. First, we select
    an entity type from the upper bar (here, this corpus includes two types of entities,
    `PERSON` and `ORG`. Which entities you want to annotate depends on you; these
    are the labels you provide to the tool.) Then, we''ll just select the words we
    want to label as an entity with the cursor, as seen in the following screenshot:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将对实体进行标注。标注实体很简单。首先，我们从上面的工具栏中选择一个实体类型（在这里，这个语料库包括两种类型的实体，`PERSON`和`ORG`。您想标注哪些实体取决于您；这些是您提供给工具的标签。）然后，我们只需用光标选择我们想要标注为实体的单词，如下面的截图所示：
- en: '![ Figure 7.2 – Annotating PERSON entities on the web demo'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.2 – 在网络演示中注释PERSON实体'
- en: '](img/B16570_7_2.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16570_7_2.jpg)'
- en: Figure 7.2 – Annotating PERSON entities on the web demo
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 在网络演示中注释PERSON实体
- en: After we're finished with annotating the text, we click the accept button. Once
    the session is finished, you can dump the annotated data as a JSON file. When
    you're finished with your annotation job, you can click the **Save** button to
    finish the session properly. Clicking **Save** will dump the annotated data as
    a JSON file automatically. That's it. Prodigy offers a really efficient way of
    annotating your data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们完成文本注释后，我们点击接受按钮。一旦会话结束，您可以将标注数据导出为JSON文件。当您完成注释工作后，您可以点击**保存**按钮来正确结束会话。点击**保存**会自动将标注数据导出为JSON文件。就是这样。Prodigy提供了一种非常高效的数据标注方法。
- en: Annotating data with Brat
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Brat进行数据标注
- en: 'Another annotation tool is **Brat**, which is a free and web-based tool for
    text annotation ([https://brat.nlplab.org/introduction.html](https://brat.nlplab.org/introduction.html)).
    It''s possible to annotate relations as well as entities in Brat. You can also
    download Brat onto your local machine and use it for annotation tasks. Basically,
    you upload your dataset to Brat and annotate the text on the interface. The following
    screenshot shows an annotated sentence from an example of a CoNLL dataset:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个注释工具是**Brat**，这是一个免费且基于网络的文本注释工具([https://brat.nlplab.org/introduction.html](https://brat.nlplab.org/introduction.html))。在Brat中，可以注释关系以及实体。您还可以将Brat下载到您的本地机器上，用于注释任务。基本上，您将数据集上传到Brat，并在界面上注释文本。以下截图显示了CoNLL数据集示例中的一个已注释句子：
- en: '![  Figure 7.3 – An example annotated sentence'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.3 – 一个示例已注释句子'
- en: '](img/B16570_7_3.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16570_7_3.jpg)'
- en: Figure 7.3 – An example annotated sentence
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 一个示例已注释句子
- en: You can play with example datasets on the Brat demo website ([https://brat.nlplab.org/examples.html](https://brat.nlplab.org/examples.html%20))
    or get started by uploading a small subset of your own data. After the annotation
    session is finished, Brat dumps a JSON of annotated data as well.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Brat演示网站上的示例数据集上进行操作([https://brat.nlplab.org/examples.html](https://brat.nlplab.org/examples.html%20))，或者通过上传您自己的数据的小子集开始。在注释会话完成后，Brat会自动导出注释数据的JSON文件。
- en: spaCy training data format
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: spaCy训练数据格式
- en: As we remarked earlier, spaCy training code works with JSON file format. Let's
    see the details of training the data format.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所述，spaCy训练代码与JSON文件格式一起工作。让我们看看训练数据格式的详细情况。
- en: 'For the NER, you need to provide a list of pairs of sentences and their annotations.
    Each annotation should include the entity type, the start position of the entity
    in terms of characters, and the end position of the entity in terms of characters.
    Let''s see an example of a dataset:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于命名实体识别（NER），您需要提供句子及其注释的列表。每个注释应包括实体类型、实体在字符中的起始位置以及实体在字符中的结束位置。让我们看看数据集的一个示例：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This dataset consists of three example pairs. Each example pair includes a sentence
    as the first element. The second element of the pair is a list of annotated entities.
    In the first example sentence, there is only one entity, `Munich`. This entity's
    label is `GPE` and starts at the 20th character position in the sentence and ends
    at the 25th character. Similarly, the second sentence includes two entities; one
    is `PERSON`, `Victoria's`, and the second entity is `GPE`, `house`. The third
    sentence does not include any entities, hence the list is empty.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集包含三个示例对。每个示例对包括一个句子作为第一个元素。对中的第二个元素是一个标注实体的列表。在第一个示例句子中，只有一个实体，即`Munich`。该实体的标签是`GPE`，在句子中的起始位置是第20个字符，结束位置是第25个字符。同样，第二个句子包含两个实体；一个是`PERSON`，`Victoria's`，另一个实体是`GPE`，`house`。第三个句子不包含任何实体，因此列表为空。
- en: 'We cannot feed the raw text and annotations directly to spaCy. Instead, we
    need to create an `Example` object for each training example. Let''s see the code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能直接将原始文本和标注输入到spaCy中。相反，我们需要为每个训练示例创建一个`Example`对象。让我们看看代码：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this code segment, first, we created a doc object from the example sentence.
    Then we fed the doc object and its annotations in a dictionary form to create
    an `Example` object. We'll use `Example` objects in the next section's training
    code.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码段中，首先，我们从示例句子创建了一个doc对象。然后，我们将doc对象及其以字典形式提供的标注输入到创建`Example`对象中。我们将在下一节的训练代码中使用`Example`对象。
- en: Creating example sentences for training the dependency parser is a bit different,
    and we'll cover this in the *Training a pipeline component from scratch* section.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为训练依存句法分析器创建示例句子略有不同，我们将在*从头开始训练管道组件*部分进行介绍。
- en: Now, we're ready to train our own spaCy models. We'll first see how to update
    an NLP pipeline statistical model. For this purpose, we'll train the NER component
    further with the help of our own examples.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好训练我们自己的spaCy模型了。我们将首先了解如何更新NLP管道的统计模型。为此，我们将借助自己的示例进一步训练NER组件。
- en: Updating an existing pipeline component
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新现有的管道组件
- en: 'In this section, we will train spaCy''s NER component further with our own
    examples to recognize the navigation domain. We already saw some examples of navigation
    domain utterances and how spaCy''s NER model labeled entities of some example
    utterances:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用自己的示例进一步训练spaCy的NER组件，以识别导航领域。我们之前已经看到了一些导航领域语句的示例以及spaCy的NER模型如何标注某些示例语句中的实体：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Obviously, we want NER to perform better and recognize location entities, such
    as street names, district names, and other location names, such as home, work,
    and office. Now, we''ll feed our examples to the NER component and will do more
    training. We will train NER in three steps:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们希望NER表现更好，能够识别诸如街道名称、区域名称以及其他诸如家、工作和办公室等地点名称的实体。现在，我们将我们的示例输入到NER组件中，并进行更多训练。我们将分三步进行NER的训练：
- en: First, we'll disable all the other statistical pipeline components, including
    the POS tagger and the dependency parser.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将禁用所有其他统计管道组件，包括词性标注器和依存句法分析器。
- en: We'll feed our domain examples to the training procedure.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将我们的领域示例输入到训练过程中。
- en: We'll evaluate the new NER model.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将评估新的NER模型。
- en: 'Also, we will learn how to do the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将学习如何进行以下操作：
- en: Save the updated NER model to disk.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将更新的NER模型保存到磁盘。
- en: Read the updated NER model when we want to use it.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们想要使用更新的NER模型时，请读取它。
- en: Let's get started and dive into training the NER model procedure. As we pointed
    out in the preceding list, we'll train the NER model in several steps. We'll start
    with the first step, disabling the other statistical models of the spaCy NLP pipeline.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始，深入了解NER模型的训练过程。正如我们在前面的列表中所指出的，我们将分几个步骤训练NER模型。我们将从第一步开始，即禁用spaCy NLP管道中的其他统计模型。
- en: Disabling the other statistical models
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 禁用其他统计模型
- en: 'Before starting the training procedure, we disable the other pipeline components,
    hence we train **only** the intended component. The following code segment disables
    all the pipeline components except NER. We call this code block before starting
    the training procedure:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练过程之前，我们禁用了其他管道组件，因此我们只训练**目标**组件。以下代码段禁用了除了NER之外的所有管道组件。我们在开始训练过程之前调用此代码块：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Another way of writing this code is as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种编写此代码的方式如下：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the preceding code block, we made use of the fact that `nlp.disable_pipes`
    returns a context manager. Using a `with` statement makes sure that our code releases
    the allocated sources (such as file handlers, database locks, or multiple threads).
    If you''re not familiar with statements, you can read more at this Python tutorial:
    [https://book.pythontips.com/en/latest/context_managers.html](https://book.pythontips.com/en/latest/context_managers.html).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们利用了`nlp.disable_pipes`返回一个上下文管理器的事实。使用`with`语句确保我们的代码释放分配的资源（例如文件句柄、数据库锁或多个线程）。如果您不熟悉这些语句，您可以在Python教程中了解更多：[https://book.pythontips.com/en/latest/context_managers.html](https://book.pythontips.com/en/latest/context_managers.html)。
- en: We have completed the first step of the training code. Now, we are ready to
    make the model training procedure.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了训练代码的第一步。现在，我们准备进行模型训练过程。
- en: Model training procedure
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练过程
- en: As we mentioned in [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055),
    *Linguistic Features*, in the *Introducing named entity recognition* section,
    spaCy's NER model is a neural network model. To train a neural network, we need
    to configure some parameters as well as provide training examples. Each prediction
    of the neural network is a sum of its **weight** values; hence, the training procedure
    adjusts the weights of the neural network with our examples. If you want to learn
    more about how neural networks function, you can read the excellent guide at [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[*第3章*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)中提到的，在*介绍命名实体识别*部分中的*语言特征*，spaCy的NER模型是一个神经网络模型。要训练一个神经网络，我们需要配置一些参数并提供训练示例。神经网络每次预测都是其**权重**值的总和；因此，训练过程通过我们的示例调整神经网络的权重。如果您想了解更多关于神经网络如何工作，您可以阅读优秀的指南[http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)。
- en: In the training procedure, we'll go over the training set *several times* and
    show each example several times (one iteration is called one **epoch**) because
    showing an example only once is not enough. At each iteration, we shuffle the
    training data so that the order of the training data does not matter. This shuffling
    of training data helps train the neural network thoroughly.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们将对训练集进行*多次*遍历，并对每个示例进行多次展示（一次迭代称为一个**epoch**），因为只展示一次示例是不够的。在每次迭代中，我们会对训练数据进行打乱，以确保训练数据的顺序不重要。这种训练数据的打乱有助于彻底训练神经网络。
- en: In each epoch, the training code updates the weights of the neural network with
    a small number. Optimizers are functions that update the neural network weights
    subject to a loss. At epoch, a loss value is calculated by comparing the actual
    label with the neural network's current output. Then, the optimizer function can
    update the neural network's weight with respect to this loss value.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个epoch中，训练代码使用一个小数值更新神经网络的权重。优化器是更新神经网络权重以符合损失函数的函数。在epoch结束时，通过比较实际标签与神经网络当前输出计算一个损失值。然后，优化器函数可以基于这个损失值更新神经网络的权重。
- en: In the following code, we used the **stochastic gradient descent** (**SGD**)
    algorithm as the optimizer. SGD itself is also an iterative algorithm. It aims
    to minimize a function (for neural networks, we want to minimize the loss function).
    SGD starts from a random point on the loss function and travels down its slope
    in steps until it reaches the lowest point of that function. If you want to learn
    more about SGD, you can visit Stanford's excellent neural network class at [http://deeplearning.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/](http://deeplearning.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们使用了**随机梯度下降**（**SGD**）算法作为优化器。SGD本身也是一个迭代算法。它的目的是最小化一个函数（对于神经网络，我们希望最小化损失函数）。SGD从损失函数上的一个随机点开始，以步长沿着斜坡向下移动，直到达到该函数的最低点。如果您想了解更多关于SGD的信息，您可以访问斯坦福大学优秀的神经网络课程[http://deeplearning.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/](http://deeplearning.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/)。
- en: 'Putting it all altogether, here''s the code to train spaCy''s NER model for
    the navigation domain. Let''s go step by step:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，以下是用于训练spaCy的NER模型导航域的代码。让我们一步一步来：
- en: 'In the first three lines, we make the necessary imports. `random` is a Python
    library that includes methods for pseudo-random generators for several distributions,
    including uniform, gamma, and beta distributions. In our code, we''ll use `random.shuffle`
    to shuffle our dataset. `shuffle` shuffles sequences into place:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前三行中，我们进行必要的导入。`random`是Python库，包括用于几个分布（包括均匀分布、伽马分布和贝塔分布）的伪随机生成器的方法。在我们的代码中，我们将使用`random.shuffle`来打乱我们的数据集。`shuffle`将序列就地打乱：
- en: '[PRE8]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we will create a language pipeline object, `nlp`:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个语言管道对象，`nlp`：
- en: '[PRE9]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we will define our navigation domain training set sentences. Each example
    contains a sentence and its annotation:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将定义我们的导航领域训练集句子。每个示例都包含一个句子及其注释：
- en: '[PRE10]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We want to iterate our data 20 times, hence the number of epochs is `20`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望迭代我们的数据20次，因此epochs的数量是`20`：
- en: '[PRE11]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the next 2 lines, we disable the other pipeline components and leave NER
    for training. We use `with statement` to invoke `nlp.disable_pipe` as a context
    manager:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的两行中，我们禁用了其他管道组件，只留下NER进行训练。我们使用`with statement`来调用`nlp.disable_pipe`作为上下文管理器：
- en: '[PRE12]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We create an `optimizer` object, as we discussed previously. We''ll feed this
    `optimizer` object to the training method as a parameter:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个`optimizer`对象，正如我们之前讨论的那样。我们将这个`optimizer`对象作为参数传递给训练方法：
- en: '[PRE13]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, for each epoch, we will shuffle our dataset by `random.shuffle`:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，对于每个epoch，我们将使用`random.shuffle`来打乱我们的数据集：
- en: '[PRE14]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For each example sentence in the dataset, we will create an `Example` object
    from the sentence and its annotation:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于数据集中的每个示例句子，我们将从句子及其注释创建一个`Example`对象：
- en: '[PRE15]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will feed the `Example` object and `optimizer` object to `nlp.update`. The
    actual training method is `nlp.update`. This is the place where the NER model
    gets trained:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`Example`对象和`optimizer`对象传递给`nlp.update`。实际的训练方法是`nlp.update`。这是NER模型接受训练的地方：
- en: '[PRE16]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once the epochs are complete, we save the newly trained NER component to disk
    under a directory called `navi_ner`:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦完成epochs，我们将新训练的NER组件保存到名为`navi_ner`的目录下：
- en: '[PRE17]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`nlp.update` outputs a loss value each time it is called. After invoking this
    code, you should see an output similar to the following screenshot (the loss values
    might be different):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`nlp.update`每次调用都会输出一个损失值。调用此代码后，你应该会看到类似于以下截图的输出（损失值可能不同）：'
- en: '![Figure 7.4 – NER training''s output'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.4 – NER训练的输出'
- en: '](img/B16570_7_4.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_7_4.jpg)'
- en: Figure 7.4 – NER training's output
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – NER训练的输出
- en: That's it! We trained the NER component for the navigation domain! Let's try
    some example sentences and see whether it really worked.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们已经为导航领域训练了NER组件！让我们尝试一些示例句子，看看它是否真的起作用了。
- en: Evaluating the updated NER
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估更新的NER
- en: 'Now we can test our brand-new updated NER component. We can try some examples
    with synonyms and paraphrases to test whether the neural network really learned
    the navigation domain, instead of memorizing our examples. Let''s see how it goes:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以测试我们全新的更新后的NER组件。我们可以尝试一些带有同义词和释义的示例来测试神经网络是否真的学会了导航领域，而不是仅仅记住我们的示例。让我们看看结果如何：
- en: 'These are the training sentences:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些是训练句子：
- en: '[PRE18]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s use the synonym `house` for `home` and also add two more words to `to
    my`:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用同义词“house”来替换“home”，并在“to my”中添加两个更多的词：
- en: '[PRE19]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It worked! `House` is recognized as a `GPE` type entity. How about we replace
    `navigate` with a similar verb, `drive me`, and create a paraphrase of the first
    example sentence:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它起作用了！`House`被识别为`GPE`类型的实体。我们是否可以用一个类似的动词“drive me”来替换`navigate`，并创建第一个示例句子的释义：
- en: '[PRE20]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we try a slightly different sentence. In the next sentence, we won''t
    use a synonym or paraphrase. We''ll replace `Oxford Street` with a district name,
    `Soho`. Let''s see what happens this time:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们尝试一个稍微不同的句子。在下一个句子中，我们不会使用同义词或释义。我们将用地区名称“苏豪”来替换“牛津街”。让我们看看这次会发生什么：
- en: '[PRE21]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As we remarked before, we updated the statistical model, hence, the NER model
    didn''t forget about the entities it already knew. Let''s do a test with another
    entity type to see whether the NER model really didn''t forget the other entity
    types:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们更新了统计模型，因此，NER模型并没有忘记它已经知道的实体。让我们用另一个实体类型进行测试，看看NER模型是否真的没有忘记其他实体类型：
- en: '[PRE22]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Great! spaCy's neural networks can recognize not only synonyms but entities
    of the same type. This is one of the reasons why we use spaCy for NLP. Statistical
    models are incredibly powerful.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！spaCy的神经网络不仅能识别同义词，还能识别同一类型的实体。这就是我们为什么使用spaCy进行NLP的原因之一。统计模型非常强大。
- en: In the next section, we'll learn how to save the model we trained and load a
    model into our Python scripts.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何保存我们训练的模型并将模型加载到我们的Python脚本中。
- en: Saving and loading custom models
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和加载自定义模型
- en: 'In the preceding code segment, we already saw how to serialize the updated
    NER component as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码段中，我们已经看到了如何如下序列化更新的NER组件：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We serialize models so that we can upload them in other Python scripts whenever
    we want. When we want to upload a custom-made spaCy component, we perform the
    following steps:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模型序列化，这样我们就可以在我们想要的时候将它们上传到其他Python脚本中。当我们想要上传一个定制的spaCy组件时，我们执行以下步骤：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here are the steps that we follow:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们遵循的步骤：
- en: We first load the pipeline components without the NER, because we want to add
    our custom NER. This way, we make sure that the default NER doesn't override our
    custom NER component.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载没有命名实体识别（NER）的管道组件，因为我们想添加我们自己的NER。这样，我们确保默认的NER不会覆盖我们的自定义NER组件。
- en: Next, we create an NER pipeline component object. Then we load our custom NER
    component from the directory we serialized to this newly created component object.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个NER管道组件对象。然后我们从我们序列化的目录中加载我们的自定义NER组件到这个新创建的组件对象中。
- en: We then add our custom NER component to the pipeline.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将我们的自定义NER组件添加到管道中。
- en: We print the metadata of the pipeline to make sure that loading our custom component
    worked.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印管道的元数据，以确保加载我们的自定义组件成功。
- en: 'Now, we also learned how to serialize and load custom components. Hence, we
    can move forward to a bigger mission: training a spaCy statistical model from
    scratch. We''ll again train the NER component, but this time we''ll start from
    scratch.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们也学会了如何序列化和加载自定义组件。因此，我们可以继续前进到一个更大的任务：从头开始训练spaCy统计模型。我们将再次训练NER组件，但这次我们将从头开始。
- en: Training a pipeline component from scratch
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始训练管道组件
- en: In the previous section, we saw how to update the existing NER component according
    to our data. In this section, we will create a brand-new NER component for the
    medicine domain.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了如何根据我们的数据更新现有的NER组件。在本节中，我们将为医学领域创建一个全新的NER组件。
- en: 'Let''s start with a small dataset to understand the training procedure. Then
    we''ll be experimenting with a real medical NLP dataset. The following sentences
    belong to the medicine domain and include medical entities such as drug and disease
    names:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个小数据集开始，以了解训练过程。然后我们将实验一个真实的医疗NLP数据集。以下句子属于医学领域，包括药物和疾病名称等医疗实体：
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following code block shows how to train an NER component from scratch.
    As we mentioned before, it''s better to create our own NER rather than updating
    spaCy''s default NER model as medical entities are not recognized by spaCy''s
    NER component at all. Let''s see the code and also compare it to the code from
    the previous section. We''ll go step by step:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了如何从头开始训练NER组件。正如我们之前提到的，最好创建我们自己的NER，而不是更新spaCy的默认NER模型，因为医疗实体根本不被spaCy的NER组件识别。让我们看看代码，并将其与上一节的代码进行比较。我们将一步一步来：
- en: 'In the first three lines, we made the necessary imports. We imported `spacy`
    and `spacy.training.Example`. We also imported `random` to shuffle our dataset:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前三行中，我们进行了必要的导入。我们导入了`spacy`和`spacy.training.Example`。我们还导入了`random`来打乱我们的数据集：
- en: '[PRE26]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We defined our training set of three examples. For each example, we included
    a sentence and its annotated entities:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了我们的三个示例的训练集。对于每个示例，我们包括一个句子及其注释的实体：
- en: '[PRE27]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We also listed the set of entities we want to recognize – `DIS` for disease
    names, and `DRUG` for drug names:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还列出了我们想要识别的实体集合——`DIS`代表疾病名称，`DRUG`代表药物名称：
- en: '[PRE28]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We created a blank model. This is different from what we did in the previous
    section. In the previous section, we used spaCy''s pre-trained English language
    pipeline:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个空白模型。这与我们在上一节中所做的不同。在上一节中，我们使用了spaCy的预训练英语语言管道：
- en: '[PRE29]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We also created a blank NER component. This is also different from the previous
    section''s code. We used the pre-trained NER component in the previous section:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还创建了一个空的NER组件。这与上一节的代码不同。在上一节中，我们使用了预训练的NER组件：
- en: '[PRE30]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we add each medical label to the blank NER component by using `ner.add_label`:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过使用`ner.add_label`将每个医疗标签添加到空NER组件中：
- en: '[PRE31]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We define the number of epochs as `25`:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将epoch的数量定义为`25`：
- en: '[PRE32]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The next two lines disable the other components other than the NER:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的两行禁用了除了NER之外的其他组件：
- en: '[PRE33]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We created an optimizer object by calling `nlp.begin_training`. This is different
    from the previous section. In the previous section, we created an optimizer object
    by calling `nlp.create_optimizer`, so that NER doesn''t forget the labels it already
    knows. Here, `nlp.begin_training` initializes the NER model''s weights with `0`,
    hence, the NER model forgets everything it learned before. This is what we want;
    we want a blank NER model to train from scratch:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过调用 `nlp.begin_training` 创建了一个优化器对象。这与上一节不同。在上一节中，我们通过调用 `nlp.create_optimizer`
    创建了一个优化器对象，这样 NER 就不会忘记它已经知道的标签。在这里，`nlp.begin_training` 使用 `0` 初始化 NER 模型的权重，因此
    NER 模型忘记了之前学到的所有内容。这正是我们想要的；我们想要一个从零开始训练的空白 NER 模型：
- en: '[PRE34]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'For each epoch, we shuffle our small training set and train the NER component
    with our examples:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个时代，我们都会对我们的小型训练集进行洗牌，并使用我们的示例训练 NER 组件：
- en: '[PRE35]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here''s what this code segment outputs (the loss values may be different):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是此代码段输出的内容（损失值可能不同）：
- en: '![Figure 7.5 – Loss values during training'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.5 – 训练过程中的损失值'
- en: '](img/B16570_7_5.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_7_5.jpg](img/B16570_7_5.jpg)'
- en: Figure 7.5 – Loss values during training
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 训练过程中的损失值
- en: 'Did it really work? Let''s test the newly trained NER component:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 它真的起作用了吗？让我们测试一下新训练的 NER 组件：
- en: '[PRE36]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Great – it worked! Let''s also test some negative examples, entities that are
    recognized by spaCy''s pre-trained NER model but not ours:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了 – 它起作用了！让我们也测试一些负面示例，即 spaCy 预训练的 NER 模型可以识别但我们的模型不能识别的实体：
- en: '[PRE37]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This looks good, too. Our brand new NER recognizes only medical entities. Let''s
    visualize our first example sentence and see how displaCy exhibits new entities:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来也不错。我们全新的 NER 只识别医疗实体。让我们可视化我们的第一个示例句子，看看 displaCy 如何展示新的实体：
- en: '[PRE38]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This code block generates the following visualization:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码块生成了以下可视化：
- en: '![Figure 7.6 – Visualization of the example sentence'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6 – 示例句子的可视化'
- en: '](img/B16570_7_6.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_7_6.jpg](img/B16570_7_6.jpg)'
- en: Figure 7.6 – Visualization of the example sentence
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 示例句子的可视化
- en: We successfully trained the NER model on small datasets. Now it's time to work
    with a real-world dataset. In the next section, we'll dive into processing a very
    interesting dataset regarding a hot topic; mining Corona medical texts.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在小型数据集上成功训练了 NER 模型。现在是时候使用真实世界的数据集了。在下一节中，我们将深入研究处理一个关于热门话题的非常有趣的语料库；挖掘冠状病毒医疗文本。
- en: Working with a real-world dataset
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用真实世界的数据集
- en: 'In this section, we will train on a real-world corpus. We will train an NER
    model on the CORD-19 corpus provided by the *Allen Institute for AI* ([https://allenai.org/](https://allenai.org/)).
    This is an open challenge for text miners to extract information from this dataset
    to help medical professionals around the world fight against Corona disease. CORD-19
    is an open source dataset that is collected from over 500,000 scholarly articles
    about Corona disease. The training set consists of 20 annotated medical text samples:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在一个真实世界的语料库上进行训练。我们将在由 *艾伦人工智能研究所* 提供的 CORD-19 语料库上训练一个 NER 模型（[https://allenai.org/](https://allenai.org/)）。这是一个开放挑战，让文本挖掘者从该数据集中提取信息，以帮助全球的医疗专业人员对抗冠状病毒病。CORD-19
    是一个开源数据集，收集了超过 50 万篇关于冠状病毒疾病的学术论文。训练集包括 20 个标注的医疗文本样本：
- en: 'Let''s get started by having a look at an example training text:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从查看一个示例训练文本开始：
- en: '[PRE39]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As we see from this example, real-world medical text can be quite long, and
    it can include many medical terms and entities. Nouns, verbs, and entities are
    all related to the medicine domain. Entities can be numbers (`91%`), number and
    units (`100 ng/ml`, `25 microg/ml`), number-letter combinations (`H3N2`), abbreviations
    (`CDC`), and also compound words (`qRT-PCR`, `PE-labeled`).
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从这个例子中我们可以看到，真实世界的医疗文本可以相当长，它可以包含许多医疗术语和实体。名词、动词和实体都与医学领域相关。实体可以是数字（`91%`）、数字和单位（`100
    ng/ml`，`25 microg/ml`）、数字字母组合（`H3N2`）、缩写（`CDC`），以及复合词（`qRT-PCR`，`PE-labeled`）。
- en: The medical entities come in several shapes (numbers, number and letter combinations,
    and compounds) as well as being very domain-specific. Hence, a medical text is
    very different from everyday spoken/written language and definitely needs custom
    training.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医疗实体以多种形状（数字、数字和字母组合、复合词）出现，以及非常特定于领域。因此，医疗文本与日常口语/书面语言非常不同，并且肯定需要定制训练。
- en: 'Entity labels can be compound words as well. Here''s the list of entity types
    that this corpus includes:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实体标签也可以是复合词。以下是该语料库包含的实体类型列表：
- en: '[PRE40]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We transformed the dataset so that it''s ready to use with spaCy training.
    The dataset is available under the book''s GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter07/data](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter07/data).'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将数据集转换成可以用于 spaCy 训练的形式。数据集可在本书的 GitHub 仓库中找到：[https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter07/data](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter07/data)。
- en: 'Let''s go ahead and download the dataset. Type the following command into your
    terminal:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续下载数据集。在您的终端中输入以下命令：
- en: '[PRE41]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This will download the dataset into your machine. If you wish, you can manually
    download the dataset from GitHub, too.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将把数据集下载到您的机器上。如果您愿意，您也可以从 GitHub 手动下载数据集。
- en: 'Now, we''ll preprocess the dataset a bit to recover some format changes that
    happened while dumping the dataset as `json`:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将对数据集进行一些预处理，以恢复在将数据集作为 `json` 格式导出时发生的格式变化：
- en: '[PRE42]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This code segment will read the dataset's `JSON` file and format it according
    to the spaCy training data conventions.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此代码段将读取数据集的 `JSON` 文件，并按照 spaCy 训练数据规范对其进行格式化。
- en: 'Next, we''ll do the statistical model training:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将进行统计模型训练：
- en: 'a) First, we''ll do the related imports:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 首先，我们将进行相关导入：
- en: '[PRE43]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'b) Secondly, we''ll initialize a blank spaCy English model and add an NER component
    to this blank model:'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 其次，我们将初始化一个空的 spaCy 英语模型，并向此空白模型添加一个 NER 组件：
- en: '[PRE44]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'c) Next, we define the labels we''d like the NER component to recognize and
    introduce these labels to it:'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 接下来，我们定义 NER 组件要识别的标签，并将这些标签介绍给它：
- en: '[PRE45]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'd) Finally, we''re ready to define the training loop:'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 最后，我们准备好定义训练循环：
- en: '[PRE46]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This code block is identical to the training code from the previous section,
    except for the value of the `epochs` variable. This time, we iterated for `100`
    epochs, because the entity types, entity values, and the training sample text
    are semantically more complicated. We recommend you do at least 500 iterations
    for this dataset if you have the time. 100 iterations over the data are sufficient
    to get good results, but 500 iterations will take the performance further.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此代码块与上一节的训练代码相同，只是 `epochs` 变量的值不同。这次，我们迭代了 `100` 个周期，因为实体类型、实体值和训练样本文本在语义上更为复杂。如果您有时间，我们建议您至少进行
    500 次迭代。对于这个数据集，100 次迭代的数据就足以获得良好的结果，但 500 次迭代将进一步提升性能。
- en: 'Let''s visualize some sample texts to see how our newly trained medical NER
    model handled the medical entities. We''ll visualize our medical entities with
    `displaCy` code:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们可视化一些示例文本，看看我们新训练的医疗 NER 模型如何处理医疗实体。我们将使用 `displaCy` 代码可视化我们的医疗实体：
- en: '[PRE47]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The following screenshot highlights two entities – `tuberculosis` and the name
    of the bacteria that causes it as the pathogen entity:'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下截图突出了两个实体 - `结核病` 和导致该病的细菌作为病原体实体：
- en: '![Figure 7.7 – Highlighted entities of the sample medical text](img/B16570_7_7.jpg)'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.7 – 突出显示示例医疗文本的实体](img/B16570_7_7.jpg)'
- en: Figure 7.7 – Highlighted entities of the sample medical text
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.7 – 突出显示示例医疗文本的实体
- en: 'This time, let''s look at entities of a text concerning pathogenic bacteria.
    This sample text contains many entities, including several diseases and pathogen
    names. All the disease names, such as `pneumonia`, `tetanus`, and `leprosy`, are
    correctly extracted by our medical NER model. The following `displaCy` code highlights
    the entities:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这次，让我们看看关于致病细菌的文本中的实体。这个示例文本包含许多实体，包括几个疾病和病原体名称。所有疾病名称，如 `肺炎`、`破伤风` 和 `麻风`，都被我们的医疗
    NER 模型正确提取。以下 `displaCy` 代码突出了这些实体：
- en: '[PRE48]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Here is the visual generated by the preceding code block:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是前面代码块生成的可视化：
- en: '![Figure 7.8 – Sample text with disease and pathogen entities highlighted'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.8 – 突出显示疾病和病原体实体的示例文本'
- en: '](img/B16570_7_8.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_7_8.jpg)'
- en: Figure 7.8 – Sample text with disease and pathogen entities highlighted
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 突出显示疾病和病原体实体的示例文本
- en: Looks good! We successfully trained spaCy's NER model for the medicine domain
    and now the NER can extract information from medical text. This concludes our
    section. We learned how to train a statistical pipeline component as well as prepare
    the training data and test the results. These are great steps in both mastering
    spaCy and machine learning algorithm design.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错！我们已经成功训练了 spaCy 的医学领域 NER 模型，现在 NER 可以从医疗文本中提取信息。这标志着本节的结束。我们学习了如何训练统计管道组件以及准备训练数据和测试结果。这些是在掌握
    spaCy 和机器学习算法设计方面的重要步骤。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored how to customize spaCy statistical models according
    to our own domain and data. First, we learned the key points of deciding whether
    we really need custom model training. Then, we went through an essential part
    of statistical algorithm design – data collection, and labeling.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何根据我们自己的领域和数据定制 spaCy 统计模型。首先，我们学习了决定我们是否真的需要定制模型训练的关键点。然后，我们经历了一个统计算法设计的重要部分——数据收集和标注。
- en: Here we also learned about two annotation tools – Prodigy and Brat. Next, we
    started model training by updating spaCy's NER component with our navigation domain
    data samples. We learned the necessary model training steps, including disabling
    the other pipeline components, creating example objects to hold our examples,
    and feeding our examples to the training code.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们还了解了两种标注工具——Prodigy 和 Brat。接下来，我们通过更新 spaCy 的命名实体识别（NER）组件，使用我们的导航领域数据样本开始了模型训练。我们学习了必要的模型训练步骤，包括禁用其他管道组件，创建示例对象以保存我们的示例，并将我们的示例输入到训练代码中。
- en: Finally, we learned how to train an NER model from scratch on a small toy dataset
    and on a real medical domain dataset.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了如何在小型玩具数据集和真实医疗领域数据集上从头开始训练 NER 模型。
- en: With this chapter, we took a step into the statistical NLP playground. In the
    next chapter, we will take more steps in statistical modeling and learn about
    text classification with spaCy. Let's move forward and see what spaCy brings us!
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，我们迈入了统计自然语言处理（NLP）的游乐场。在下一章，我们将进一步探索统计建模，并学习使用 spaCy 进行文本分类。让我们继续前进，看看
    spaCy 会带给我们什么！
