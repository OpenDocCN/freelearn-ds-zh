- en: 'Chapter 3: Data Cleansing and Integration'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章：数据清洗与集成
- en: In the previous chapter, you were introduced to the first step of the data analytics
    process – that is, ingesting raw, transactional data from various source systems
    into a cloud-based data lake. Once we have the raw data available, we need to
    process, clean, and transform it into a format that helps with extracting meaningful,
    actionable business insights. This process of cleaning, processing, and transforming
    raw data is known as data cleansing and integration. This is what you will learn
    about in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你了解了数据分析过程的第一步——即将来自各个源系统的原始事务数据导入云数据湖。一旦获得原始数据，我们需要对其进行处理、清洗，并转换为有助于提取有意义的、可操作的商业洞察的格式。这个清洗、处理和转换原始数据的过程被称为数据清洗与集成。本章将讲解这一过程。
- en: Raw data sourced from operational systems is not conducive for data analytics
    in its raw format. In this chapter, you will learn about various data integration
    techniques, which are useful in consolidating raw, transactional data from disparate
    source systems and joining them to enrich them and present the end user with a
    single, consolidated version of the truth. Then, you will learn how to clean and
    transform the form and structure of raw data into a format that is ready for data
    analytics using data cleansing techniques. Data cleansing broadly deals with fixing
    inconsistencies within data, dealing with bad and corrupt data, eliminating any
    duplicates within data, and standardizing data to meet the enterprise data standards
    and conventions. You will also learn about the challenges involved in using a
    cloud data lake as an analytics data store. Finally, you will be introduced to
    a modern data storage layer called Delta Lake to overcome these challenges.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 来自运营系统的原始数据，在其原始格式下并不适合进行数据分析。在本章中，你将学习各种数据集成技术，这些技术有助于整合来自不同源系统的原始事务数据，并将它们合并以丰富数据，向最终用户展示一个统一的、经过整合的真实版本。接着，你将学习如何使用数据清洗技术清理和转换原始数据的形式和结构，使其适合数据分析。数据清洗主要涉及修复数据中的不一致性，处理坏数据和损坏数据，消除数据中的重复项，并将数据标准化以符合企业的数据标准和惯例。你还将了解使用云数据湖作为分析数据存储所面临的挑战。最后，你将了解一种现代数据存储层——Delta
    Lake，以克服这些挑战。
- en: This chapter will equip you with essential skills for consolidating, cleaning,
    and transforming raw data into a structure that is ready for analytics, as well
    as provide you with useful techniques for building scalable, reliable, and analytics-ready
    data lakes in the cloud. As a developer, the topics included in this chapter will
    help you give your business users access to all of their data at all times, allowing
    them to draw actionable insights from their raw data much faster and easier.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将为你提供将原始数据整合、清洗和转换为适合分析结构的核心技能，并为你提供在云中构建可扩展、可靠且适合分析的数据湖的有用技术。作为开发者，本章的内容将帮助你随时让业务用户访问所有数据，让他们能够更快速、更轻松地从原始数据中提取可操作的洞察。
- en: 'In this chapter, the following main topics will be covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Transforming raw data into enriched meaningful data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将原始数据转换为丰富的有意义数据
- en: Building analytical data stores using cloud data lakes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用云数据湖构建分析数据存储
- en: Consolidating data using data integration
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据集成整合数据
- en: Making raw data analytics-ready using data cleansing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据清洗使原始数据准备好进行分析
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will be using the Databricks Community Edition to run our
    code ([https://community.cloud.databricks.com](https://community.cloud.databricks.com)).
    Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Databricks社区版来运行我们的代码（[https://community.cloud.databricks.com](https://community.cloud.databricks.com)）。注册说明可以在[https://databricks.com/try-databricks](https://databricks.com/try-databricks)找到。
- en: The code in this chapter can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter03](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter03).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码可以从[https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter03](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter03)下载。
- en: The datasets for this chapter can be found at [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的数据集可以在[https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data)找到。
- en: Transforming raw data into enriched meaningful data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将原始数据转换为有意义的丰富数据
- en: Every data analytics system consists of a few key stages, including data ingestion,
    data transformation, and loading into a data warehouse or a data lake. Only after
    the data passes through these stages does it become ready for consumption by end
    users for descriptive and predictive analytics. There are two common industry
    practices for undertaking this process, widely known as **Extract, Transform,
    Load** (**ETL**) and **Extract, Load, Transform** (**ELT**). In this section,
    you will explore both these methods of data processing and understand their key
    differences. You will also learn about the key advantages ELT has to offer over
    ETL in the context of big data analytics in the cloud.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据分析系统都包括几个关键阶段，包括数据摄取、数据转换以及加载到数据仓库或数据湖中。只有在数据经过这些阶段后，才能准备好供最终用户进行描述性和预测性分析。有两种常见的行业实践用于进行此过程，广泛称为**提取、转换、加载**（**ETL**）和**提取、加载、转换**（**ELT**）。在本节中，你将探讨这两种数据处理方法，并理解它们的主要区别。你还将了解在云端大数据分析背景下，ELT相比ETL所具备的主要优势。
- en: Extracting, transforming, and loading data
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取、转换和加载数据
- en: 'This is the typical data processing methodology that''s followed by almost
    all data warehousing systems. In this methodology, data is extracted from the
    source systems and stored in a temporary storage location such as a relational
    database, called the staging area. Then, the data in the staging area is integrated,
    cleansed, and transformed before being loaded into the data warehouse. The following
    diagram illustrates a typical ETL process:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是几乎所有数据仓库系统遵循的典型数据处理方法。在这个方法中，数据从源系统中提取，并存储在临时存储位置，如关系数据库，称为暂存区。然后，暂存区中的数据会被整合、清洗和转换，最后加载到数据仓库中。下图展示了典型的ETL过程：
- en: '![Figure 3.1 – Extract, transform, and load'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.1 – 提取、转换和加载'
- en: '](img/B16736_03_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_03_01.jpg)'
- en: Figure 3.1 – Extract, transform, and load
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 提取、转换和加载
- en: As shown in the previous diagram, the ETL process consists of three main stages.
    We will discuss these in the following sections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图所示，ETL过程由三个主要阶段组成。我们将在接下来的章节中讨论这些阶段。
- en: Extracting data from operational systems
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从运营系统中提取数据
- en: The ETL stage involves extracting selective, raw, transactional data from multiple
    source systems and staging it at a temporary storage location. This step is equivalent
    to the data ingestion process, which you learned about in [*Chapter 2*](B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032),
    *Data Ingestion*. The ETL process typically processes large volumes of data, though
    running it directly on the source systems might put excessive load on them. Operational
    systems are critical to the functionality of day-to-day business functions and
    it is not advisable to unnecessarily tax them. Thus, the **Extract** process extracts
    data from source systems during off-business hours and stores it in a staging
    area. Furthermore, ETL processing can happen on the data in the staging area,
    leaving operational systems to handle their core functions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ETL阶段涉及从多个源系统中提取选择性的原始事务数据，并将其暂存于临时存储位置。此步骤相当于数据摄取过程，你可以在[*第2章*](B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032)《数据摄取》中学习到。ETL过程通常处理大量数据，尽管直接在源系统上运行可能会对它们造成过重的负担。运营系统对日常业务功能至关重要，因此不建议不必要地增加其负担。因此，**提取**过程会在非工作时间从源系统中提取数据，并将其存储在暂存区。此外，ETL处理可以在暂存区中的数据上进行，从而使运营系统能够处理其核心功能。
- en: Transforming, cleaning, and integrating data
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换、清洗和整合数据
- en: This stage involves various data transformation processes such as data integration,
    data cleansing, joining, filtering, splitting, standardization, validation, and
    more. This step converts raw transactional data into a clean, integrated, and
    enriched version that is ready for business analytics. We will dive deeper into
    this stage in the *Consolidating data using data integration* and *Making raw
    data analytics ready using data cleansing* sections of this chapter.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这一阶段涉及各种数据转换过程，如数据集成、数据清洗、连接、过滤、拆分、标准化、验证等。此步骤将原始事务数据转化为清晰、集成和丰富的版本，准备进行业务分析。我们将在本章的*使用数据集成整合数据*和*使用数据清洗使原始数据具备分析能力*部分深入探讨这一阶段。
- en: Loading data into a data warehouse
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据加载到数据仓库
- en: This is the final stage of the ETL process, where the transformed data is finally
    loaded into a persistent, historical data storage layer, such as a data warehouse.
    Typically, ETL processing systems accomplish the **Transform** and **Load** steps
    in a single flow, where raw data from the staging area is cleansed, integrated,
    and transformed according to the business rules and loaded into a warehouse, all
    in a single flow.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 ETL 过程的最后阶段，经过转换的数据最终被加载到持久的历史数据存储层中，如数据仓库。通常，ETL 处理系统会在一个单一的流程中完成**转换**和**加载**步骤，其中暂存区的原始数据经过清洗、集成和根据业务规则转换后加载到数据仓库中。
- en: Pros and cons of ETL and data warehousing
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ETL 和数据仓库的优缺点
- en: Some advantages of the ETL methodology are that data is transformed and loaded
    into a structured analytical data store such as a data warehouse, which allows
    for efficient and performant analysis of the data. Since the ETL paradigm has
    been in existence for a few decades now, there are sophisticated platforms and
    tools on the market that can perform ETL in a very efficient manner in a single,
    unified flow.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ETL 方法的某些优势在于，数据被转换并加载到一个结构化的分析数据存储中，如数据仓库，这使得数据的分析既高效又具有较高的性能。由于 ETL 模式已经存在了几十年，现在市场上有一些成熟的平台和工具，可以非常高效地在单一统一的流程中执行
    ETL。
- en: Another advantage of ETL is that since data is processed before being loaded
    into its final storage, there is the opportunity to either omit unwanted data
    or obscure sensitive data. This greatly helps with data regulatory and compliance
    requirements.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ETL 的另一个优点是，由于数据在加载到最终存储之前已经处理过，因此可以有机会省略不需要的数据或掩盖敏感数据。这在满足数据合规性和监管要求方面非常有帮助。
- en: However, ETL processes run in a batch processing manner and typically run once
    every night. Thus, new data is only available to end users once the ETL batch
    process has finished successfully. This creates a dependency on data engineers
    to efficiently run the ETL processes, and there is a considerable delay before
    end users can get access to the latest data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ETL 过程以批处理方式运行，通常每天晚上执行一次。因此，只有在 ETL 批处理成功完成后，最终用户才能访问新数据。这就产生了对数据工程师的依赖，需要他们高效地运行
    ETL 过程，同时最终用户在获取最新数据之前会有相当的延迟。
- en: The data in the staging area is almost entirely cleared every time before the
    start of the next scheduled ETL load. Also, operational systems do not typically
    keep a historical record of the transactional data for more than a few years.
    This means that end users cease to have any access to historical raw data, other
    than the processed data in the data warehouse. This historical raw data could
    prove to be very useful for certain types of data analytics such as predictive
    analytics, but data warehouses generally do not retain it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每次在下一次计划的 ETL 负载开始之前，暂存区的数据几乎都会被完全清除。而且，操作系统通常不会保留超过几年的事务数据的历史记录。这意味着，最终用户无法访问历史原始数据，除了数据仓库中的已处理数据。对于某些类型的数据分析（如预测分析）来说，这些历史原始数据可能非常有用，但数据仓库通常不会保留这些数据。
- en: The ETL process evolved around data warehousing concepts and is more suited
    for business intelligence workloads, in an on-premises type of setting. The highly
    structured and somewhat rigid nature of data warehouses makes ETL not very conducive
    for data science and machine learning, both of which deal with a lot of unstructured
    data. Moreover, the batch nature of the ETL process makes it unfit for real-time
    analytics. Also, ETL and data warehouses do not take full advantage of the cloud
    and cloud-based data lakes. That's why a new methodology for data processing has
    emerged called **Extract, Load, and Transform**, or **ELT**, which we will take
    a look at in the following section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ETL 过程围绕数据仓库概念演变，更多适用于本地环境下的商业智能工作负载。数据仓库高度结构化且相对僵化的特性使得 ETL 不太适合数据科学和机器学习，这两者都涉及大量非结构化数据。此外，ETL
    过程的批处理特性使其不适用于实时分析。而且，ETL 和数据仓库没有充分利用云技术及基于云的数据湖。因此，一种新的数据处理方法 **提取、加载和转换**（ELT）应运而生，接下来的章节将详细介绍这一方法。
- en: Extracting, loading, and transforming data
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取、加载和转换数据
- en: 'In the ELT methodology, transactional data from source systems is ingested
    into a data lake in its original, raw format. The ingested, raw data in the data
    lake is then transformed either on-demand or in a scheduled manner. In the ELT
    process, raw data is directly staged on the data lake and is typically never purged.
    As a result, data can grow enormously in size and require virtually unlimited
    storage and compute capacity. On-premises data warehouses and data lakes were
    not designed to handle data at such an enormous scale. Thus, the ELT methodology
    is only made possible by modern cloud technologies that offer highly scalable
    and elastic compute and storage resources. The following diagram depicts a typical
    ELT process:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ELT 方法中，来自源系统的事务性数据以其原始、未经处理的格式被摄取到数据湖中。摄取到数据湖中的原始数据随后按需或定期进行转换。在 ELT 过程中，原始数据直接存储在数据湖中，通常不会被删除。因此，数据可能会以巨大的规模增长，并且几乎需要无限的存储和计算能力。传统的本地数据仓库和数据湖并未设计用来处理如此庞大的数据规模。因此，ELT
    方法的实现仅能依赖现代云技术，这些技术提供了高度可扩展和弹性的计算与存储资源。下图展示了典型的 ELT 过程：
- en: '![Figure 3.2 – Extract, load, and transform'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.2 – 提取、加载和转换'
- en: '](img/B16736_03_02.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_03_02.jpg)'
- en: Figure 3.2 – Extract, load, and transform
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 提取、加载和转换
- en: In the preceding diagram, raw data is either continuously or periodically ingested
    from multiple source systems into a data lake. Then, the raw data in the data
    lake is integrated, cleaned, and transformed before being stored back inside it.
    The clean and aggregated data in the data lake serves as a single source of truth
    for all types of downstream analytics.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图示中，原始数据从多个源系统连续或定期地摄取到数据湖中。然后，数据湖中的原始数据被集成、清洗并转换，之后再存储回数据湖中。数据湖中的清洗和聚合数据作为所有类型下游分析的单一事实来源。
- en: With ELT, virtually any amount of history can be maintained, and data can be
    made available as soon as it is created in the source systems. There is no requirement
    to pre-process data before ingesting it and since data lakes do not impose any
    strict requirements on the format or structure of data, ELT can ingest and store
    all kinds of structured, unstructured, and semi-structured data. Thus, the ETL
    process makes all the historical raw data available so that data transformation
    can become completely on demand.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ELT，几乎可以保留任何量的历史数据，且数据可以在源系统中创建后立即提供。无需在摄取数据之前进行预处理，并且由于数据湖对数据格式或结构没有严格要求，ELT
    可以摄取并存储各种结构化、非结构化和半结构化的数据。因此，ETL 过程使得所有历史原始数据都可用，从而使数据转换完全按需进行。
- en: Advantages of choosing ELT over ETL
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择 ELT 而非 ETL 的优势
- en: Some of the advantages of the ELT methodology are that data can be ingested
    at much faster speeds since no pre-processing steps are required. It is much more
    flexible with the kinds of data that can be ingested, helping unlock new analytics
    use cases such as data science and machine learning. ETL leverages elastic storage
    provided by cloud data lakes, helping organizations maintain a replica of transactional
    data, along with virtually unlimited history. ELT, typically being cloud-based,
    takes away the hassle of managing data replication and archiving as most cloud
    providers have managed services for these and guarantee **Service-Level Agreements**
    (**SLAs**).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ELT 方法学的一些优势在于数据可以以更快的速度进行摄取，因为不需要预处理步骤。它在数据摄取的灵活性方面也更强，有助于解锁如数据科学和机器学习等新的分析用例。ETL
    利用云数据湖提供的弹性存储，帮助组织维护事务数据的副本，并保存几乎无限的历史记录。作为云端技术，ELT 还消除了数据复制和归档管理的麻烦，因为大多数云提供商都提供了这些托管服务，并保证**服务水平协议**（**SLAs**）。
- en: The ELT methodology is quickly becoming the de facto standard for big data processing
    in the cloud for organizations with huge amounts of transactional data. The ELT
    methodology of data processing is recommended for organizations that are already
    in the cloud or with a future cloud strategy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ELT 方法学正在迅速成为云端大数据处理的事实标准，特别适用于处理大量事务数据的组织。对于已经进入云端或未来有云端战略的组织，推荐采用 ELT 方法学进行数据处理。
- en: However, the ELT methodology in the cloud is still very nascent, and cloud data
    lakes do not offer any of the transactional or reliability guarantees that their
    data warehousing counterparts already offer. In the next section, you will explore
    some of the challenges involved in building cloud-based data lakes and some ways
    to overcome them.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，云端的 ELT 方法学仍处于起步阶段，云数据湖并未提供其数据仓库对应物所具备的任何事务性或可靠性保障。在下一节中，您将探索构建基于云的数据湖所涉及的一些挑战，并探讨克服这些挑战的方法。
- en: Building analytical data stores using cloud data lakes
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用云数据湖构建分析数据存储
- en: In this section, you will explore the advantages afforded by cloud-based data
    lakes for big data analytics systems, and then understand some of the challenges
    facing big data analytics systems while leveraging cloud-based data analytics
    systems. You will also write a few **PySpark** code examples to experience these
    challenges first-hand.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将探讨基于云的数据湖为大数据分析系统提供的优势，并了解在利用基于云的数据分析系统时，大数据分析系统面临的一些挑战。您还将编写几个**PySpark**代码示例，亲自体验这些挑战。
- en: Challenges with cloud data lakes
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云数据湖的挑战
- en: Cloud-based data lakes offer unlimited, scalable, and relatively inexpensive
    data storage. They are offered as managed services by the individual cloud providers
    and offer availability, scalability, efficiency, and lower **total cost of ownership**.
    This helps organizations accelerate their digital innovation and achieve faster
    time to market. However, cloud data lakes are object storages that evolved primarily
    to solve the problem of storage scalability. They weren't designed to store highly
    structured, strongly typed, analytical data. Given this, there are a few challenges
    in using cloud-based data lakes as analytical storage systems.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基于云的数据湖提供无限的、可扩展的、相对廉价的数据存储。它们由各大云提供商作为托管服务提供，具备高可用性、可扩展性、高效性和较低的**总拥有成本**。这帮助组织加速数字创新，缩短上市时间。然而，云数据湖作为对象存储，主要是为了解决存储可扩展性的问题而发展起来的。它们并非为了存储高度结构化、强类型的分析数据而设计。因此，使用基于云的数据湖作为分析存储系统存在一些挑战。
- en: Data reliability challenges with data lakes
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据湖的可靠性挑战
- en: Data lakes are not based on any underlying filesystem but on the object storage
    mechanism, which manages data as objects. Object storage represents data as objects
    with a unique identifier and its associated metadata. Object storages weren't
    designed to manage frequently changing transactional data. Thus, they have a few
    limitations regarding analytical data stores and data processing, such as eventual
    consistency, lack of transactional guarantees, and more. We will look at these
    in the following sections.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖并不是基于任何底层文件系统，而是基于对象存储机制，将数据作为对象进行管理。对象存储将数据表示为具有唯一标识符及其相关元数据的对象。对象存储并非为管理频繁变化的事务数据而设计，因此在作为分析数据存储和数据处理系统时，存在一些限制，比如最终一致性、缺乏事务性保障等。我们将在接下来的章节中探讨这些问题。
- en: Eventual consistency of data
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据的最终一致性
- en: Cloud-based data lakes are distributed storage systems where data storage happens
    across multiple machines instead of a single machine. Distributed storage systems
    are governed by a theorem referred to as the CAP theorem. The **CAP theorem**
    states that a distributed storage system can be tuned for only two of the three
    parameters of CAP; that is, consistency, availability, and partition tolerance.
    Not guaranteeing strong availability and partition tolerance can lead to data
    loss or errors, so cloud-based data lakes prioritize these two so that they're
    made eventually consistent.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 基于云的数据湖是分布式存储系统，数据存储分布在多台机器上，而不是单一机器上。分布式存储系统受到一个称为CAP定理的理论的约束。**CAP定理**表明，分布式存储系统只能在一致性、可用性和分区容忍性这三者中选择其中的两个来进行调优。不保证强一致性和分区容忍性可能导致数据丢失或错误，因此基于云的数据湖优先保证这两者，以便使其最终一致。
- en: Eventual consistency means that data written to a cloud data lake might not
    be available instantly. This could lead to `FileNotFound` errors in a data analytics
    system, where downstream business analytics processes try to read data from the
    data lake while it is being written by an ELT process.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最终一致性意味着写入云数据湖的数据可能不会立即可用。这可能会导致数据分析系统中的`FileNotFound`错误，尤其是在下游的商业分析过程试图在ELT过程写入数据的同时读取数据时。
- en: Lack of transactional guarantees
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缺乏事务性保证
- en: A typical relational database provides transactional guarantees when data is
    being written. This simply means that a database operation either completely succeeds
    or completely fails, and that any consumer trying to read the data simultaneously
    doesn't get any inconsistent or incorrect data because of a database operation
    failure.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的关系型数据库在数据写入时提供事务性保证。这意味着数据库操作要么完全成功，要么完全失败，并且任何同时尝试读取数据的消费者都不会因为数据库操作失败而读取到不一致或错误的数据。
- en: Data lakes do not provide any such atomic transactional or durability guarantees.
    This means that it's up to the developer to clean up and manually roll back half-written,
    incomplete data from any failed jobs and reprocess the data all over again.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖不提供任何此类原子事务或持久性保证。这意味着开发人员需要清理并手动回滚任何失败作业中半写入的不完整数据，并重新处理这些数据。
- en: 'Consider the following code snippet, where we are ingesting CSV data, converting
    it into Parquet format, and saving it to the data lake:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下代码片段，我们正在摄取CSV数据，将其转换为Parquet格式，并将其保存到数据湖中：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, let''s try and interrupt the job halfway through to simulate a Spark
    job failure. Upon browsing the data lake at `/tmp/retail.parquet`, you will notice
    a few half-written Parquet files. Let''s try and read those Parquet files via
    another Spark job, as shown in the following code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们尝试在工作过程中中断任务，以模拟Spark作业失败。在浏览`/tmp/retail.parquet`数据湖时，你会注意到一些半写入的Parquet文件。接下来，我们尝试通过另一个Spark作业读取这些Parquet文件，代码如下所示：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code block, we have read a Parquet file that was the result
    of a data ingestion job that failed halfway through. The expected result, when
    we try to read this data on a data store that supports atomic transactions, is
    that either the query yields no results or it just fails because the data is incorrect.
    However, in the case of the preceding Spark job, we do get a few thousand records,
    which is incorrect. This is because of the lack of atomic transaction guarantees
    on the part of Apache Spark, as well as the data lake.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们读取了一个Parquet文件，它是一个数据摄取作业未完全完成时的结果。当我们尝试在支持原子事务的数据存储上读取这些数据时，预期的结果是查询要么不返回任何结果，要么因为数据不正确而失败。然而，在前述的Spark作业中，我们却得到了一些几千条记录，这是错误的。这是因为Apache
    Spark及数据湖缺乏原子事务保障。
- en: Lack of schema enforcement
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缺乏模式强制执行
- en: Data lakes, being object stores, are not concerned with the structure and schema
    of data and are happy to store all and any data without performing any checks
    to make sure that data is consistent. Apache Spark also doesn't have any built-in
    mechanism to enforce a user-defined schema. This results in corrupt and bad data,
    with mismatched data types ending up in your data lake. This reduces data quality,
    which is critical for end user analytical applications.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖作为对象存储，并不关心数据的结构和模式，能够存储任何数据，而不会执行任何检查来确保数据的一致性。Apache Spark也没有内建的机制来强制执行用户定义的模式。这导致了损坏和不良数据的产生，数据类型不匹配的数据最终进入数据湖。这会降低数据质量，而数据质量对于最终用户的分析应用至关重要。
- en: 'Take a look at the following code example, where we have written an initial
    DataFrame with a few columns. The first column is of `IntegerType`, while the
    second column is of `StringType`. We wrote the first DataFrame to the data lake
    in Parquet format. Then, we generated a second DataFrame where both columns are
    of `IntegerType`. Then, we tried to append the second DataFrame to the original
    Parquet dataset already in the data lake, as shown in the following code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下代码示例，我们已经创建了一个包含几列的初始DataFrame。第一列的数据类型是`IntegerType`，第二列的数据类型是`StringType`。我们将第一个DataFrame写入数据湖，格式为Parquet。接着，我们生成了第二个DataFrame，两个列的数据类型都是`IntegerType`。然后，我们尝试将第二个DataFrame追加到已经存在于数据湖中的原Parquet数据集，如下所示：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The expected result on a strongly typed analytical data store such as a data
    warehouse should be a data type mismatch error. However, neither Apache Spark
    nor the data lake or the Parquet data format itself, throw an error while we try
    to perform this operation, and the transaction seems to complete successfully.
    This is undesirable as we have allowed inconsistent data to enter our data lake.
    However, performing a read operation on the Parquet dataset would fail with a
    type mismatch, which could be confusing and quite difficult to debug. This error
    could have easily been caught during the data loading process if data lakes or
    Apache Spark came with data validation support. It is important to always validate
    the data's correctness and consistency before making it available for business
    analytics because business decision-makers depend on it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在强类型分析数据存储（如数据仓库）上，预期的结果应该是数据类型不匹配错误。然而，Apache Spark、数据湖或Parquet数据格式本身并不会在我们尝试执行此操作时抛出错误，事务似乎成功完成。这是不可取的，因为我们允许不一致的数据进入数据湖。然而，对Parquet数据集执行读取操作时会因类型不匹配而失败，这可能令人困惑且相当难以调试。如果数据湖或Apache
    Spark具备数据验证支持，这个错误本来可以在数据加载过程中就被捕获。在将数据提供给业务分析之前，始终验证数据的正确性和一致性非常重要，因为业务决策者依赖这些数据。
- en: Unifying batch and streaming
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 统一批处理和流处理
- en: One of the key requirements of modern big data analytics systems is getting
    access to the latest data and insights in real time. Apache Spark comes with structured
    streaming to handle all real-time analytics requirements. Despite stream processing,
    batch processing remains a key aspect of big data analytics, and Apache Spark
    has done quite a good job of unifying both real-time and batch analytics via its
    Spark SQL Engine, which acts as the core abstraction layer for both batch and
    streaming Spark jobs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现代大数据分析系统的一个关键要求是实时访问最新数据和洞察。Apache Spark提供了结构化流处理功能，能够处理所有实时分析需求。尽管流处理是核心，但批处理依然是大数据分析的一个重要方面，而Apache
    Spark通过其Spark SQL引擎将实时和批处理分析统一，Spark SQL引擎作为批处理和流处理Spark作业的核心抽象层，表现得非常好。
- en: However, data lakes do not support any level of atomic transactions or isolation
    between different transactions on the same table or dataset. So, something like
    the **Lambda Architecture**, which you learned about in [*Chapter 2*](B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032),
    *Data Ingestion*, would need to be employed to unify batch and stream processing
    pipelines. This results in separate data processing pipelines, separate code bases,
    and separate tables being maintained, one for batch processing and another stream
    processing. This architecture of your big data analytics system is very complex
    to design and maintain.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据湖不支持任何级别的原子事务或同一表或数据集上不同事务之间的隔离。因此，像**Lambda架构**这样的技术，就需要被用来统一批处理和流处理管道，这个架构你在[*第2章*](B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032)中学习过，*数据摄取*。这就导致了需要维护不同的数据处理管道、不同的代码库以及不同的表，一个用于批处理，另一个用于流处理。你大数据分析系统的架构设计和维护非常复杂。
- en: Updating and deleting data
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新和删除数据
- en: In the ELT methodology, you are continuously ingesting new data into the data
    lake and maintaining a replica of your source transactions inside it, along with
    history over a certain period. Operational systems are constantly generating transactions.
    However, from time to time, you must update and delete records.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在ELT方法论中，你是持续地将新数据摄取到数据湖，并在其中维护源交易的副本以及一段时间内的历史记录。操作系统不断生成交易。然而，时不时你需要更新和删除记录。
- en: Consider the example of an order that's been placed by a customer at an online
    retailer. The transaction goes through different phases, starting with the order
    being placed, the order being processed, the order getting ready for shipment,
    the order getting shipped, the order in transit, and the order being delivered.
    This change in the state of the transaction must be reflected in the data lake.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个客户在在线零售商处下单的例子为例。交易经历不同的阶段，从下订单、订单处理中、订单准备发货、订单已发货、订单运输中，到订单已交付。这一交易状态的变化必须在数据湖中得到反映。
- en: This process of capturing the change in the state of data is known as `UPDATE`
    and `DELETE` operations in the data lake. Data lakes are append-only systems and
    not designed to handle a large number of arbitrary updates and deletes. Thus,
    implementing arbitrary updates and deletes increases the complexity of your ELT
    application.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获数据状态变化的过程被称为数据湖中的 `UPDATE` 和 `DELETE` 操作。数据湖是仅附加的系统，并不设计用来处理大量的任意更新和删除。因此，实施任意更新和删除会增加你
    ELT 应用程序的复杂性。
- en: Rolling back incorrect data
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回滚错误数据
- en: Earlier, you learned that data lakes do not support any atomic transactional
    guarantees on write operations. It is up to the data engineer to identify the
    incorrect records, clean them up, and reprocess the data again for failed jobs.
    For smaller datasets, this cleanup process could be as simple as truncating and
    loading the entire dataset. However, for larger datasets at a big data scale with
    petabytes of data, truncating and loading data is not at all feasible. Neither
    data lakes nor Apache Spark has an easy rollback option, requiring the data engineer
    to build complex mechanisms to handle failed jobs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，你了解到数据湖不支持任何关于写操作的原子事务保证。数据工程师需要识别错误记录，清理它们，并在失败的任务中重新处理数据。对于较小的数据集，这个清理过程可能只是简单的截断并重新加载整个数据集。然而，对于大规模数据集，包含数千
    TB 数据的情况下，截断并加载数据根本不可行。数据湖和 Apache Spark 都没有便捷的回滚选项，这就要求数据工程师构建复杂的机制来处理失败的任务。
- en: A new class of modern data storage formats has emerged that tries to overcome
    the data lake challenges mentioned in the previous section. Some examples of these
    technologies are Apache Hudi, Apache Iceberg, and Delta Lake. In the following
    section, we will explore Delta Lake and see how it can help in overcoming various
    data lake challenges.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一类新的现代数据存储格式应运而生，旨在克服上一节提到的数据湖挑战。这些技术的一些例子包括 Apache Hudi、Apache Iceberg 和 Delta
    Lake。在接下来的部分，我们将探索 Delta Lake，并看看它如何帮助克服各种数据湖挑战。
- en: Overcoming data lake challenges with Delta Lake
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Delta Lake 克服数据湖挑战
- en: In this section, you will be introduced to Delta Lake and understand how it
    helps overcome some of the challenges of data lakes. You will also write a few
    code examples to see Delta Lake in action.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，你将了解 Delta Lake，并理解它如何帮助克服数据湖的一些挑战。你还将编写一些代码示例，看看 Delta Lake 如何实际应用。
- en: Introduction to Delta Lake
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Delta Lake 简介
- en: '**Delta Lake** is an open source data storage layer that helps bring reliability,
    ACID transactional guarantees, schema validation, and evolution to cloud-based
    data lakes. Delta Lake also helps in unifying batch and stream processing. Delta
    Lake was created by Databricks, the original creators of Apache Spark, and it
    was designed to be completely compatible with all Apache Spark APIs.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**Delta Lake** 是一个开源的数据存储层，旨在为基于云的数据湖带来可靠性、ACID 事务保证、架构验证和演进。Delta Lake 还帮助统一批处理和流处理。Delta
    Lake 由 Databricks 创建，Databricks 是 Apache Spark 的原始开发者，且它完全兼容所有 Apache Spark API。'
- en: Delta Lake is made up of a set of versioned Parquet files, along with a write-ahead
    log called the **transaction log**. The Delta Transaction Log helps enable all
    the features of Delta Lake. Let's dive deeper into the inner workings of Delta
    Transaction Log to gain a better understanding of how Delta Lake operates.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 由一组版本化的 Parquet 文件组成，并配有一个称为 **事务日志** 的写前日志。Delta 事务日志有助于实现 Delta
    Lake 的所有功能。让我们深入了解 Delta 事务日志的内部工作原理，以便更好地理解 Delta Lake 的运作方式。
- en: Delta Lake transaction log
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Delta Lake 事务日志
- en: The **Delta transaction log** is based on a popular technique that's performed
    on relational databases known as **write-ahead logging** (**WAL**). This technique
    guarantees the atomicity and durability of write operations on a database. This
    is achieved by recording each write operation as a transaction in the write-ahead
    log before any data is written to the database. The Delta Transaction Log is based
    on the same technique as WAL, but here, WAL, as well as the data that's been written,
    is in files on the data lake.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**Delta 事务日志**基于一种流行的技术，该技术应用于关系型数据库，被称为**预写日志**（**WAL**）。这种技术保证了数据库写操作的原子性和持久性。这是通过在数据写入数据库之前，将每个写操作作为事务记录到预写日志中来实现的。Delta
    事务日志基于与 WAL 相同的技术，但在这里，WAL 以及已写入的数据存储在数据湖中的文件里。'
- en: 'Let''s try to understand the Delta Transaction Log using a simple Spark job
    that ingests CSV data into the data lake in Delta format, as shown in the following
    code block:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试通过一个简单的 Spark 作业来理解 Delta 事务日志，该作业将 CSV 数据以 Delta 格式导入数据湖，如以下代码块所示：
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code reads CSV files from the data lake, infers the schema of
    the underlying data, along with the header, converts the data into Delta format,
    and saves the data in a different location on the data lake. Now, let''s explore
    the Delta file''s location on the data lake using the following command:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码从数据湖读取 CSV 文件，推断底层数据的模式以及表头，将数据转换为 Delta 格式，并将数据保存到数据湖的不同位置。现在，让我们使用以下命令探索数据湖中
    Delta 文件的位置：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After you execute the preceding command, you will notice the folder structure
    of the Delta location, as shown in the following screenshot:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述命令后，您将注意到 Delta 位置的文件夹结构，如下图所示：
- en: '![Figure 3.3 – Delta folder structure'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.3 – Delta 文件夹结构'
- en: '](img/B16736_03_03.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_03_03.jpg)'
- en: Figure 3.3 – Delta folder structure
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – Delta 文件夹结构
- en: 'In the preceding screenshot, you can see that a Delta Lake location contains
    two parts: a folder named `_delta_log` and a set of Parquet files. The `_delta_log`
    folder contains the Delta Transaction Log''s files. Let''s explore the transaction
    log using the following command:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，您可以看到一个 Delta Lake 位置包含两部分：一个名为 `_delta_log` 的文件夹和一组 Parquet 文件。`_delta_log`
    文件夹包含 Delta 事务日志的文件。我们可以通过以下命令来探索事务日志：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding command displays the contents of the `_delta_log` folder, as
    shown in the following screenshot:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令显示了 `_delta_log` 文件夹的内容，如下图所示：
- en: '![Figure 3.4 – Delta transaction log'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.4 – Delta 事务日志'
- en: '](img/B16736_03_04.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_03_04.jpg)'
- en: Figure 3.4 – Delta transaction log
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – Delta 事务日志
- en: In the preceding screenshot, we can see that the folder contains a few different
    types of files. There are also a few files with the`.json` extension. These JSON
    files are actual Delta Transaction Log files and contain an ordered record of
    all the successful transactions that are performed on the Delta table.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们可以看到文件夹中包含几种不同类型的文件。还有一些带有 `.json` 扩展名的文件。这些 JSON 文件是实际的 Delta 事务日志文件，包含对
    Delta 表执行的所有成功事务的有序记录。
- en: Note
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The previously used `%fs` filesystem commands are only available on the Databricks
    platform. You will need to use the appropriate command to browse the data lake
    that's appropriate for your Spark and data lake distribution.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 之前使用的 `%fs` 文件系统命令仅适用于 Databricks 平台。您需要使用适合您 Spark 和数据湖分发版的命令来浏览数据湖。
- en: Delta Lake transactions can be any operations that are performed on the Delta
    table, such as inserts, updates, and deletes, or even metadata operations such
    as renaming the table, changing the table schema, and so on. Every time an operation
    takes place, a new record is appended to the Delta Transaction Log with actions
    such as **Add file**, **Remove file**, **Update metadata**, and so on. These actions
    are atomic units and are recorded in the order that they took place. They are
    called **commits**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 事务可以是对 Delta 表执行的任何操作，如插入、更新、删除，甚至是元数据操作，如重命名表、修改表架构等。每次操作发生时，Delta
    事务日志都会附加一条新记录，记录诸如**添加文件**、**删除文件**、**更新元数据**等操作。这些操作是原子单位，并按发生顺序记录下来，它们被称为**提交**。
- en: 'After every 10 commits, Delta Lake generates a checkpoint file in Parquet format
    that contains all the transactions until that point in time. These periodic Parquet
    checkpoint files make it fast and easy for a Spark job to read and reconstruct
    the table''s state. This can easily be illustrated with the help of the following
    Spark code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 每10次提交后，Delta Lake会生成一个Parquet格式的检查点文件，其中包含到该时刻为止的所有事务。这些周期性的Parquet检查点文件使得Spark作业能够快速、轻松地读取并重建表的状态。以下Spark代码可以轻松地说明这一点：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding line of code, we read the Delta Transaction Log just like any
    other JSON file by using a `spark.read()` function and created a Spark DataFrame.
    Every time a `spark.read()` command is run on a Delta Lake table, a small Spark
    job is executed to read the table's state, making metadata operations on Delta
    Lake completely scalable.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码行中，我们像读取其他JSON文件一样，使用`spak.read()`函数读取Delta事务日志，并创建了一个Spark数据帧。每次在Delta
    Lake表上运行`spak.read()`命令时，都会执行一个小的Spark作业来读取表的状态，从而使对Delta Lake的元数据操作完全可扩展。
- en: Note
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `%fs` filesystem command to explore files on a data lake is only available
    on the Databricks platform. You would need to choose a mechanism appropriate for
    your Spark environment and data lake.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在数据湖中浏览文件的`%fs`文件系统命令仅在Databricks平台上可用。你需要为你的Spark环境和数据湖选择合适的机制。
- en: Now that you have an understanding of the components of Delta Lake and the inner
    workings of the Delta Transaction Log, let's see how Delta Lake can help solve
    the challenges that data lakes face.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了Delta Lake的组件以及Delta事务日志的内部工作原理，接下来我们来看一下Delta Lake如何帮助解决数据湖面临的挑战。
- en: Improving data lake reliability with Delta Lake
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Delta Lake提高数据湖的可靠性
- en: Delta Lake, along with its transaction log, guarantees the atomicity and durability
    of data written to the data lake. Delta Lake only commits a transaction to the
    transaction log when all the data of the operation is completely written to the
    data lake. Any Delta-aware consumer reading data from a Delta table will always
    parse the Delta Transaction Log first to get the latest state of the Delta table.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake及其事务日志保证了写入数据湖的数据的原子性和持久性。只有当操作的所有数据完全写入数据湖时，Delta Lake才会将事务提交到事务日志中。任何读取Delta表数据的Delta感知消费者都会首先解析Delta事务日志，以获取Delta表的最新状态。
- en: This way, if the data ingestion job fails midway, a Delta Transaction Log-aware
    consumer will parse the transaction log, get the last stable state of the table,
    and only read the data that has commits in the transaction log. Any half-written,
    dirty data that might be in the data lake will be completely ignored because such
    data will not have any commits in the transaction log. Thus, Delta Lake, coupled
    with its transaction log, makes data lakes more reliable by providing transactional
    atomicity and durability guarantees.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，如果数据摄取任务在中途失败，Delta事务日志感知消费者会解析事务日志，获取表的最后稳定状态，并只读取事务日志中有提交的数据。任何半写入的脏数据（可能存在于数据湖中）都会被完全忽略，因为这些数据在事务日志中没有任何提交。因此，Delta
    Lake与其事务日志结合，通过提供事务的原子性和持久性保证，使数据湖更加可靠。
- en: Tip
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Both data readers and data writers need to be *Delta Transaction Log aware*
    to get the ACID transaction guarantees of Delta Lake. Any reader or writer using
    Apache Spark can be made fully *Delta Transaction Log aware* by just including
    the appropriate version of the Delta Lake library on the Spark cluster. Delta
    Lake also has connectors to external data processing systems such as Presto, Athena,
    Hive, Redshift, and Snowflake.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数据读取器和数据写入器需要是*Delta事务日志感知的*，才能获得Delta Lake的ACID事务保证。任何使用Apache Spark的读取器或写入器，只需在Spark集群中包含适当版本的Delta
    Lake库，就可以完全*Delta事务日志感知*。Delta Lake还具有与外部数据处理系统的连接器，例如Presto、Athena、Hive、Redshift和Snowflake。
- en: Enabling schema validation with Delta Lake
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用Delta Lake的模式验证
- en: 'Clean and consistent data is an essential requirement for any kind of business
    analytics application. One of the easier ways to ensure that only clean data enters
    the data lake is to make sure the schema is validated during the data ingestion
    process. Delta Lake comes with a built-in schema validation mechanism and ensures
    that any data being written to Delta Lake conforms to the user-defined schema
    of the Delta table. Let''s explore this feature by creating a new Delta table
    and trying to insert data with mismatching data types into it, as shown in the
    following code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 干净且一致的数据是任何商业分析应用程序的基本要求。确保只有干净数据进入数据湖的一个简单方法是确保在数据摄取过程中验证模式。Delta Lake 内置了模式验证机制，确保写入
    Delta Lake 的任何数据都符合用户定义的 Delta 表模式。让我们通过创建一个新的 Delta 表并尝试插入数据类型不匹配的数据来探索此功能，如下所示：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the previous code snippet, we created a Spark DataFrame named `df1` with
    two columns, with `StringType` as the data type for both columns. We wrote this
    DataFrame to the data lake using the Delta Lake format. Then, we created another
    Spark DataFrame named `df2`, also with two columns, but their data types were
    set to `LongType` and `IntegerType`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们创建了一个名为 `df1` 的 Spark DataFrame，它有两列，且两列的数据类型均为 `StringType`。我们使用
    Delta Lake 格式将此 DataFrame 写入数据湖中。然后，我们创建了另一个名为 `df2` 的 Spark DataFrame，同样包含两列，但它们的数据类型分别设置为
    `LongType` 和 `IntegerType`。
- en: Next, we tried to append the second DataFrame to the original Delta table. As
    expected, Delta Lake fails the operation and throws a *Failed to merge incompatible
    data types StringType and IntegerType* exception. This way, Delta Lake ensures
    data quality in data lakes by providing schema validation and enforcement during
    data ingestion.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们尝试将第二个 DataFrame 附加到原始的 Delta 表中。正如预期的那样，Delta Lake 失败了该操作并抛出了 *无法合并不兼容的数据类型
    StringType 和 IntegerType* 异常。通过这种方式，Delta Lake 在数据湖中通过提供模式验证和强制执行，确保数据质量。
- en: Schema evolution support with Delta Lake
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Delta Lake 支持模式演变
- en: 'Another common use case during data ingestion and the ELT process is that the
    source schema might evolve from time to time and that it needs to be handled in
    the data lake. One such scenario is that new columns could be added to the source
    system tables. It is desirable to bring those new columns into our data lake table,
    without it affecting our already existing data. This process is generally known
    as **schema evolution**, and Delta Lake has built-in support for this. Let''s
    explore schema evolution in Delta Lake with the following code example:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据摄取和 ELT 过程中，另一个常见的用例是源模式可能会随着时间的推移发生变化，并且需要在数据湖中进行处理。一个这样的场景是可能会向源系统表中添加新的列。希望将这些新列引入我们的数据湖表中，而不影响我们已有的数据。这个过程通常被称为
    **模式演变**，而 Delta Lake 已内建对此的支持。让我们通过以下代码示例来探讨 Delta Lake 中的模式演变：
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the preceding code snippet, we created a Spark DataFrame named `df1` that
    has just one column labeled `id`. Then, we saved this DataFrame to the data lake
    in Delta Lake format. Then, we created a second Spark DataFrame named `df2` with
    two columns called `id` and `customer_id`. After, we appended the second DataFrame
    to the original Delta table that was created from `df1`. This time, we used the
    `mergeSchema` option. This `mergeSchema` option specifies that we are expecting
    new columns to be written to Delta Lake, and these need to be appended to the
    existing table. We can easily verify this by running the following command on
    the Delta table:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们创建了一个名为 `df1` 的 Spark DataFrame，它只有一个名为 `id` 的列。然后，我们将此 DataFrame
    以 Delta Lake 格式保存到数据湖中。接着，我们创建了第二个名为 `df2` 的 Spark DataFrame，包含两个名为 `id` 和 `customer_id`
    的列。之后，我们将第二个 DataFrame 附加到由 `df1` 创建的原始 Delta 表中。这次，我们使用了 `mergeSchema` 选项。该 `mergeSchema`
    选项指定我们期望将新列写入 Delta Lake，并需要将这些列附加到现有表中。我们可以通过对 Delta 表运行以下命令轻松验证这一点：
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the previous code block, we are loading the data in the Delta table into
    a Spark DataFrame and calling the `show()` action to display the contents of the
    DataFrame, as shown in the following figure:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们将 Delta 表中的数据加载到 Spark DataFrame 中，并调用 `show()` 操作来显示 DataFrame 的内容，如下图所示：
- en: '![Figure 3.5 – Delta Lake schema evolution'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.5 – Delta Lake 模式演变'
- en: '](img/B16736_03_05.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_03_05.jpg)'
- en: Figure 3.5 – Delta Lake schema evolution
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – Delta Lake 模式演变
- en: As you can see, new `mergeSchema` enabled, Delta Lake automatically adds the
    new column to the existing table and marks the values of the rows that did not
    exist previously as `null` values.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，启用新的`mergeSchema`后，Delta Lake 会自动将新列添加到现有表中，并将之前不存在的行的值标记为`null`值。
- en: Arbitrary updates and deletes in data lakes with Delta Lake
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Delta Lake 中的任意更新和删除
- en: Transactions not only get inserted into operating systems – they are also updated
    and deleted from time to time. In the ELT process, a replica of the source system
    data is maintained in the data lake. Thus, it becomes necessary to be able to
    not only insert data into data lakes but also update and delete it. However, data
    lakes are append-only storage systems with minimal or no support for any updates
    or deletes. Delta Lake, however, has full support for inserting, updating, and
    deleting records.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 事务不仅会被插入到操作系统中——它们还会时常被更新和删除。在 ELT 过程中，源系统数据的副本会被保存在数据湖中。因此，能够不仅将数据插入到数据湖中，还能更新和删除它变得非常必要。然而，数据湖是仅追加的存储系统，几乎没有或完全没有支持任何更新或删除的功能。Delta
    Lake，然而，完全支持插入、更新和删除记录。
- en: 'Let''s take a look at an example of how we can update and delete arbitrary
    data from Delta Lake, as shown in the following block of code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，演示如何从 Delta Lake 更新和删除任意数据，如下代码块所示：
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the preceding block of code, we created a Spark DataFrame with two columns
    labeled `id` and `customer_id`. `id` has values ranging from 1 through 5\. We
    saved this table to the data lake using the Delta Lake format. Now, let''s update
    the `customer_id` column where values of the `id` column are greater than `2`,
    as shown in the following code block:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们创建了一个 Spark DataFrame，包含两个列：`id` 和 `customer_id`。`id` 的值从 1 到 5。我们使用
    Delta Lake 格式将此表保存到数据湖中。现在，让我们更新`id`列大于`2`的`customer_id`列，如下代码块所示：
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code block, we updated the `customer_id` column using an `UPDATE`
    SQL clause and specified the condition via a `WHERE` clause, just as you would
    do on any RDBMS.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们使用`UPDATE` SQL子句更新了`customer_id`列，并通过`WHERE`子句指定了条件，就像你在任何关系型数据库管理系统（RDBMS）中操作一样。
- en: Tip
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The `%sql` magic command specifies that we intend to execute SQL queries in
    the current notebook cell. Even though we did not explicitly create a table, we
    can still refer to the Delta Lake location as a table using the `` delta.`path-to-delta-table`
    `` syntax.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`%sql` 魔法命令指定我们打算在当前笔记本单元格中执行 SQL 查询。尽管我们没有明确创建表，但我们仍然可以使用`` delta.`path-to-delta-table`
    ``语法将 Delta Lake 位置视为一个表来引用。'
- en: 'The second SQL query reads the data back from the Delta table and displays
    it using the `SELECT` SQL clause, as shown in the following screenshot:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个 SQL 查询从 Delta 表中读取数据，并使用`SELECT` SQL 子句显示出来，如下图所示：
- en: '![Figure 3.6 – Updates with Delta Lake'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.6 – 使用 Delta Lake 进行更新'
- en: '](img/B16736_03_06.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_03_06.jpg)'
- en: Figure 3.6 – Updates with Delta Lake
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 使用 Delta Lake 进行更新
- en: Here, we can verify that all the rows of the Delta table with the value of the
    `id` column greater than `2` were successfully updated. Thus, Delta Lake has full
    support for updating multiple arbitrary records at scale with a simple SQL-like
    syntax.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以验证所有`id`列值大于`2`的 Delta 表中的行都已成功更新。因此，Delta Lake 完全支持使用简单的类似 SQL 的语法，在大规模上更新多个任意记录。
- en: Tip
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: A Delta table's metadata is entirely stored in the Delta Transaction Log itself.
    This makes registering Delta tables with an external **metastore** such as **Hive**
    completely optional. This makes it easier to just save the Delta table to the
    data lake and use it via Spark's DataFrame and SQL APIs seamlessly.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Delta 表的元数据完全存储在 Delta 事务日志中。这使得将 Delta 表注册到外部**元存储**（如**Hive**）成为完全可选的。这样，直接将
    Delta 表保存到数据湖中，并通过 Spark 的 DataFrame 和 SQL API 无缝使用变得更加容易。
- en: 'Now, let''s see how Delta supports deletes with the help of the following block
    of code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下 Delta 如何通过以下代码块来支持删除：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the preceding code snippet, we used the `DELETE` command to delete all the
    records that have an `id` of value `4`. The second query, where we used the `SELECT`
    clause, displays the contents of the Delta table after the `DELETE` operation,
    as shown in the following screenshot:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用`DELETE`命令删除了所有`id`值为`4`的记录。第二个查询，我们使用`SELECT`子句，显示了`DELETE`操作后的
    Delta 表内容，如下图所示：
- en: '![Figure 3.7 – Deletes with Delta Lake'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7 – 使用 Delta Lake 进行删除'
- en: '](img/B16736_03_07.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_03_07.jpg)'
- en: Figure 3.7 – Deletes with Delta Lake
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 使用 Delta Lake 进行删除
- en: Here, we can easily verify that we no longer have any rows with an `id` value
    of `4`. Thus, Delta Lake also supports deleting arbitrary records at scale.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以轻松验证我们不再拥有任何 `id` 值为 `4` 的行。因此，Delta Lake 也支持大规模删除任意记录。
- en: Tip
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Delta Lake supports both SQL and DataFrame syntax for DELETES, UPDATES, and
    UPSERTS. A syntax reference can be found in the open source documentation, which
    is maintained at [https://docs.delta.io/latest/delta-update.html#table-deletes-updates-and-merges](https://docs.delta.io/latest/delta-update.html#table-deletes-updates-and-merges).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 同时支持 SQL 和 DataFrame 语法来执行 DELETES、UPDATES 和 UPSERTS。有关语法的参考可以在开源文档中找到，文档链接为：[https://docs.delta.io/latest/delta-update.html#table-deletes-updates-and-merges](https://docs.delta.io/latest/delta-update.html#table-deletes-updates-and-merges)。
- en: Even `DELETE` and `UPDATE` operations on Delta Lake support the same transactional
    guarantees of atomicity and durability as write operations. However, an interesting
    thing to note is that every time a `DELETE` or `UPDATE` operation takes place,
    instead of updating or deleting any data in place, Delta Lake generates a brand-new
    file with the updated or deleted records and appends these new files to the existing
    Delta table. Then, Delta Lake creates a new **commit** for this write transaction
    in the transaction log and marks the older **commits** of the deleted or updated
    records as invalid.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是 `DELETE` 和 `UPDATE` 操作，Delta Lake 也能像写入操作一样支持原子性和持久性的事务保证。然而，值得注意的是，每次执行
    `DELETE` 或 `UPDATE` 操作时，Delta Lake 并不是直接更新或删除任何数据，而是生成一个包含更新或删除记录的新文件，并将这些新文件附加到现有的
    Delta 表中。然后，Delta Lake 在事务日志中为此次写入事务创建一个新的 **提交**，并将删除或更新记录的旧 **提交** 标记为无效。
- en: Thus, Delta Lake is never actually deleting or updating the actual data files
    in the data lake; it is just appending new files for any operation and updating
    the transaction log. Updating a smaller set of transaction log files is much faster
    and more efficient than updating a large number of very large data files. This
    process of updating and deleting records with Delta Lake is ultra-efficient and
    can be scaled to petabytes of data. This feature is very useful for use cases
    where customer arbitrary records need to be identified and deleted, such as in
    GDPR compliance use cases.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Delta Lake 实际上并没有删除或更新数据湖中的实际数据文件；它只是为每个操作附加新文件并更新事务日志。更新较小的事务日志文件比更新大量非常大的数据文件要快得多，效率也更高。使用
    Delta Lake 更新和删除记录的过程非常高效，并且可以扩展到 PB 级数据。这一功能对于需要识别并删除客户任意记录的用例非常有用，例如 GDPR 合规的用例。
- en: One more interesting side effect of this technique of always appending data
    files and never deleting them is that Delta Lake maintains a historical audit
    record of all the changes that happen to the data. This audit log is maintained
    in the **Delta transaction log** and with its help, Delta Lake can travel back
    in time to reproduce a snapshot of a Delta table at that point. We will explore
    this feature in the next section.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这种始终附加数据文件而从不删除文件的技术的另一个有趣副作用是，Delta Lake 保留了所有数据变化的历史审计记录。这个审计日志保存在 **Delta
    事务日志** 中，借助它，Delta Lake 可以回溯到过去，重现某一时刻的 Delta 表快照。我们将在下一节中探讨这个功能。
- en: Time travel and rollbacks with Delta Lake
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Delta Lake 的时间旅行与回滚
- en: Delta Lake keeps an audit log of how data has changed over time in its transaction
    log. It also maintains older versions of Parquet data files every time data changes.
    This gives Delta Lake the ability to reproduce a snapshot of the entire Delta
    table at that point. This feature is called **Time Travel**.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 在其事务日志中保留数据如何随时间变化的审计日志。每次数据发生变化时，它还会保持旧版本的 Parquet 数据文件。这使得 Delta
    Lake 能够在某一时刻重现整个 Delta 表的快照。这个功能叫做 **时间旅行**。
- en: 'You can easily explore the audit trail of a Delta table using the following
    SQL query:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下 SQL 查询轻松浏览 Delta 表的审计记录：
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the preceding Spark SQL query, we used the `DESCRIBE HISTORY` command to
    reproduce the entire audit log of the changes that happened to the Delta table,
    as shown here:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 Spark SQL 查询中，我们使用了 `DESCRIBE HISTORY` 命令来重现 Delta 表上发生的所有变更的审计日志，如下所示：
- en: '![Figure 3.8 – Time Travel with Delta Lake'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8 – Delta Lake 的时间旅行'
- en: '](img/B16736_03_08.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_03_08.jpg)'
- en: Figure 3.8 – Time Travel with Delta Lake
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – Delta Lake 的时间旅行
- en: 'In the preceding screenshot, you can see that this Delta table changed three
    times. First, data was inserted into the table, then the table was updated, and
    then records were deleted from the table. Delta Lake records all these events
    as transactions called **commits**. The timestamp of the commit event version
    number is also recorded in the change audit log. The timestamp or the table version
    number can be used to travel back in time to a particular snapshot of the Delta
    table using a SQL query, as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，你可以看到这个 Delta 表发生了三次变化。首先，数据被插入到表中，然后表被更新，最后从表中删除了记录。Delta Lake 将所有这些事件记录为称为**提交**的事务。提交事件的时间戳和版本号也会记录在变更审计日志中。时间戳或表的版本号可以通过
    SQL 查询，用于回溯到 Delta 表的特定快照，示例如下：
- en: '[PRE14]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding SQL query, we performed a Delta Time Travel to the original
    version of the table. Time Travel is very useful during data engineering and ELT
    processing for performing rollbacks on tables if a data ingestion process fails.
    Delta Time Travel can be used to restore a Delta table to a previous state, as
    shown here:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 SQL 查询中，我们执行了 Delta Time Travel，回到了表的原始版本。Time Travel 在数据工程和 ELT 处理过程中非常有用，可以在数据摄取过程失败时执行回滚。Delta
    Time Travel 可以用于将 Delta 表恢复到先前的状态，如下所示：
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding SQL query, we overwrote the Delta table using a snapshot from
    a previous version of the table, all while making use of the **Delta Time Travel**
    feature.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 SQL 查询中，我们使用来自表的先前版本的快照覆盖了 Delta 表，并充分利用了**Delta Time Travel**特性。
- en: Another scenario where Delta Time Travel comes in handy is in data science and
    machine learning use cases. Data scientists often conduct multiple machine learning
    experiments by modifying the dataset that's used for experimentation. In the process,
    they end up maintaining multiple physical versions of the same dataset or table.
    Delta Lake can help eliminate these physical versions of tables with the help
    of Time Travel, since Delta has built-in data versioning. You will explore this
    technique in more detail in [*Chapter 9*](B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164),
    *Machine Learning Life Cycle Management*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个 Delta Time Travel 很有用的场景是数据科学和机器学习的应用场景。数据科学家通常通过修改用于实验的数据集来进行多个机器学习实验。在这个过程中，他们最终会维护多个物理版本的相同数据集或表。Delta
    Lake 可以通过 Time Travel 帮助消除这些物理版本的表，因为 Delta 内置了数据版本管理。你将在[*第9章*](B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164)《机器学习生命周期管理》中更详细地探讨这种技术。
- en: Tip
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Delta continues to maintain versions of Parquet data files with every operation
    that mutates data. This means that older versions of data files get accumulated
    and Delta Lake doesn't automatically delete them. This could lead to a considerable
    increase in the size of the data lake over a while. To overcome this scenario,
    Delta Lake provides a `VACUUM` command to permanently remove older files that
    are no longer referenced by the Delta table. More information regarding the `VACUUM`
    command can be found at [https://docs.delta.io/latest/delta-utility.html#vacuum](https://docs.delta.io/latest/delta-utility.html#vacuum).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Delta 会在每次修改数据的操作中保持 Parquet 数据文件的版本。这意味着旧版本的数据文件会不断积累，且 Delta Lake 不会自动删除它们。这可能会导致数据湖的大小随着时间的推移显著增加。为了解决这一问题，Delta
    Lake 提供了 `VACUUM` 命令来永久删除不再被 Delta 表引用的旧文件。有关 `VACUUM` 命令的更多信息，请参见 [https://docs.delta.io/latest/delta-utility.html#vacuum](https://docs.delta.io/latest/delta-utility.html#vacuum)。
- en: Unifying batch and stream processing using Delta Lake
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Delta Lake 统一批处理和流处理
- en: Batch and real-time stream processing are essential components of any modern
    big data architecture. In [*Chapter 2*](B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032),
    *Data Ingestion*, you learned how to use Apache Spark for batch and real-time
    data ingestion. You also learned about the Lambda Architecture, which you can
    use to implement simultaneous batch and stream processing. An implementation of
    the Lambda Architecture with Apache Spark is still relatively complex as two separate
    data processing pipelines need to be implemented for batch and real-time processing.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理和实时流处理是任何现代大数据架构中的关键组件。在[*第2章*](B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032)《数据摄取》中，你学习了如何使用
    Apache Spark 进行批处理和实时数据摄取。你还学习了 Lambda 架构，利用它可以实现同时的批处理和流处理。使用 Apache Spark 实现
    Lambda 架构仍然相对复杂，因为需要为批处理和实时处理分别实现两个独立的数据处理管道。
- en: This complexity arises from the limitation of data lakes as they inherently
    do not provide any transactional, atomicity, or durability guarantees on write
    operations. Thus, batch and streaming processes cannot write data to the same
    table or location on the data lake. Since Delta Lake already solves this challenge
    of data lakes, a single Delta Lake can be used in conjunction with multiple batch
    and real-time pipelines, further simplifying the Lambda Architecture. You will
    explore this in more detail in [*Chapter 4*](B16736_04_Final_JM_ePub.xhtml#_idTextAnchor075)*,*
    *Real-Time Data Analytics*.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这种复杂性来源于数据湖的局限性，因为它们本质上不提供任何写操作的事务性、原子性或持久性保障。因此，批处理和流处理无法将数据写入数据湖的同一个表或位置。由于Delta
    Lake已经解决了数据湖面临的这一挑战，可以将单一的Delta Lake与多个批处理和实时管道结合使用，从而进一步简化Lambda架构。你将在[*第4章*](B16736_04_Final_JM_ePub.xhtml#_idTextAnchor075)中进一步探讨这一点，*实时数据分析*。
- en: In summary, in this section, you learned that data lakes are instrumental in
    enabling truly scalable big data processing systems. However, they weren't built
    to be data analytics storage systems and have a few shortcomings, such as a lack
    of ACID transactional guarantees, as well as the ability to support the process
    of updating or deleting records, preserving data quality schema enforcement, or
    unification of batch and stream processing. You also learned how modern data storage
    layers such as Delta Lake can help overcome the challenges of data lakes and bring
    them closer to being true data analytics storage systems.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本节中，你学到了数据湖在支持真正可扩展的大数据处理系统中的重要作用。然而，它们并非为数据分析存储系统而构建，存在一些不足之处，例如缺乏ACID事务保障，以及无法支持更新或删除记录、保持数据质量的模式执行或批处理与流处理的统一。你还学到了现代数据存储层（如Delta
    Lake）如何帮助克服数据湖的挑战，并使其更接近真正的数据分析存储系统。
- en: Now that you have an understanding of how to make cloud-based data lakes more
    reliable and conducive for data analytics, you are ready to learn about the process
    of transforming raw transactional data into meaningful business insights. We will
    start by consolidating data from various disparate sources and creating a single,
    unified view of data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然你已经了解了如何让基于云的数据湖更加可靠并适合数据分析，你已经准备好学习将原始事务数据转化为有意义的商业洞察的过程。我们将从整合来自不同来源的数据并创建统一的单一视图开始。
- en: Consolidating data using data integration
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据集成进行数据整合
- en: '**Data integration** is an important step in both the ETL and ELT modes of
    data processing. Data integration is the process of combining and blending data
    from different data sources to create enriched data that happens to represent
    a single version of the truth. Data integration is different from data ingestion
    because data ingestion simply collects data from disparate sources and brings
    it to a central location, such as a data warehouse. On the other hand, data integration
    combines those disparate data sources to create a meaningful unified version of
    the data that represents all the dimensions of the data. There are multiple ways
    to perform data integration, and a few of them will be explored in this section.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集成**是ETL和ELT数据处理模式中的一个重要步骤。数据集成是将来自不同数据源的数据进行组合和融合，生成代表单一事实版本的丰富数据的过程。数据集成不同于数据摄取，因为数据摄取只是将数据从不同来源收集并带到一个中心位置，例如数据仓库。另一方面，数据集成将这些不同的数据源结合起来，创建一个有意义的统一版本的数据，代表数据的所有维度。数据集成有多种实现方式，本节将探讨其中的一些。'
- en: Data consolidation via ETL and data warehousing
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过ETL和数据仓库进行数据整合
- en: Extracting, transforming, and loading data into data warehouses has been the
    best technique of data integration over the last few decades. One of the primary
    goals of data consolidation is to reduce the number of storage locations where
    the data resides. The ETL process extracts data from various source systems and
    then joins, filters, cleanses, and transforms the data according to user-specified
    business rules and then loads it into a central data warehouse.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 提取、转换和加载数据到数据仓库是过去几十年来数据集成的最佳技术之一。数据整合的主要目标之一是减少数据存储位置的数量。ETL过程从各种源系统中提取数据，然后根据用户指定的业务规则对数据进行合并、过滤、清洗和转换，最后将其加载到中心数据仓库。
- en: This way, ETL and data warehousing techniques, as well as the tools and technologies
    that have been purposely built for this, support data consolidation and data integration.
    Although ELT is a slightly different process than ETL and with Apache Spark, we
    intend to build a data lake, the techniques of data integration and data consolidation
    remain the same, even with ETL.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通过ETL和数据仓库技术以及专门为此构建的工具和技术支持数据整合和数据集成。虽然ELT过程与ETL略有不同，并且使用Apache Spark，我们打算构建一个数据湖，但数据集成和数据整合的技术仍然保持相同，即使是ETL也是如此。
- en: Let's implement a data integration process using PySpark. As a first step, upload
    all the datasets provided with this chapter to a location where they can be accessed
    by your Spark cluster. In the case of Databricks Community Edition, the datasets
    can be directly uploaded to the data lake from within the **File** menu of the
    notebook. The links for the datasets and code files can be found in the *Technical
    requirements* section at the beginning of this chapter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用PySpark实现数据整合过程。作为第一步，将本章提供的所有数据集上传到可以被您的Spark集群访问的位置。在Databricks Community
    Edition的情况下，可以直接从笔记本的**File**菜单中将数据集上传到数据湖中。数据集和代码文件的链接可以在本章开头的*Technical requirements*部分找到。
- en: 'Let''s explore the schema of the two transactional datasets labeled `online_retail.csv`
    and `online_retail_II.csv` using the following block of code:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码块探索标记为`online_retail.csv`和`online_retail_II.csv`的两个交易数据集的架构信息：
- en: '[PRE16]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In the preceding code snippet, we did the following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们执行了以下操作：
- en: We defined the schema of a Spark DataFrame as a `StructType` consisting of multiple
    StructFields. PySpark comes with these built-in structures to programmatically
    define the schema of a DataFrame.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将Spark DataFrame的架构定义为由多个StructField组成的`StructType`。PySpark提供了这些内置结构来编程地定义DataFrame的架构。
- en: Then, we loaded the two CSV files into separate Spark DataFrames while using
    the `schema` option to specify the data schema we created during *Step 1*. We
    still specified the header option as `True` because the first line of the CSV
    file has a header defined and we need to ignore it.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将两个CSV文件加载到单独的Spark DataFrames中，同时使用`schema`选项指定我们在*Step 1*中创建的数据模式。我们仍然将头部选项设置为`True`，因为CSV文件的第一行有一个定义好的标题，我们需要忽略它。
- en: Finally, we printed the schema information of the two Spark DataFrames we created
    in *Step 2*.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印了在*Step 2*中创建的两个Spark DataFrames的架构信息。
- en: 'Now that we have the retail datasets from the CSV files loaded into Spark DataFrames,
    let''s integrate them into a single dataset, as shown in the following lines of
    code:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将来自CSV文件的零售数据集加载到Spark DataFrames中，让我们将它们整合成一个单一数据集，如以下代码所示：
- en: '[PRE17]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the preceding code, we simply combined the two Spark DataFrames containing
    online retail transactional data to create a single Spark DataFrame by using the
    `union()` function. The union operation combines the two distinct DataFrames into
    a single DataFrame. The resultant consolidated dataset is labeled `retail_df`.
    We can verify the results using the `show()` function.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们简单地使用`union()`函数将包含在线零售交易数据的两个Spark DataFrames组合成一个单一的Spark DataFrame。联合操作将这两个不同的DataFrame合并成一个DataFrame。合并后的数据集被标记为`retail_df`。我们可以使用`show()`函数验证结果。
- en: Tip
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The `union()` function is a transformation and thus lazily evaluated. This means
    that as soon as you call a `union()` on two Spark DataFrames, Spark checks to
    see if the two DataFrames have the same number of columns and that their data
    types match. It doesn't manifest the DataFrames into memory yet. The `show()`
    function is an action, so Spark processes the transformations and manifests data
    in memory. However, the `show()` function only works on a small number of the
    DataFrame partitions and returns a sample set of the results to Spark Driver.
    Thus, this action helps verify our code quickly.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`union()`函数是一种转换操作，因此它是延迟评估的。这意味着当您在两个Spark DataFrames上调用`union()`时，Spark会检查这两个DataFrame是否具有相同数量的列，并且它们的数据类型是否匹配。它不会立即将DataFrame映射到内存中。`show()`函数是一个动作操作，因此Spark会处理转换并将数据映射到内存中。然而，`show()`函数仅在DataFrame的少量分区上工作，并返回一组样本结果给Spark
    Driver。因此，这个动作帮助我们快速验证我们的代码。'
- en: 'Next, we have some data describing country codes and names stored in the `country_codes.csv`
    file. Let''s integrate it with the `retail_df` DataFrame we created in the previous
    step by using the following block of code:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有一些描述国家代码和名称的数据存储在`country_codes.csv`文件中。让我们使用以下代码块将其与前一步中创建的`retail_df`
    DataFrame集成：
- en: '[PRE18]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the previous code snippet, we did the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们做了以下操作：
- en: We loaded the `country_codes.csv` file into a Spark DataFrame, with the `header`
    option set to `True` and the file delimiter specified as `";"`.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`country_codes.csv`文件加载到一个Spark数据框中，并将`header`选项设置为`True`，文件分隔符设置为`";"`。
- en: We renamed a few column names to follow standard naming conventions using the
    `withColumnRenamed()` function. We dropped a few other columns that we thought
    were not necessary for any of our business use cases. This resulted in a DataFrame
    labeled `country_df` that contains the country code and other descriptive columns.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们重命名了一些列名，以遵循标准命名约定，使用了`withColumnRenamed()`函数。我们删除了几个我们认为对任何业务用例都不必要的列。这导致生成了一个名为`country_df`的数据框，其中包含了国家代码和其他描述性列。
- en: Then, we joined this DataFrame to the `retail_df` DataFrame from the previous
    step. We used a `retail_df` DataFrame, irrespective of whether they have a matching
    record in the `country_df` DataFrame.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将这个数据框与之前步骤中的`retail_df`数据框进行了连接。我们使用的是`retail_df`数据框，无论它们是否在`country_df`数据框中有匹配记录。
- en: The resultant `integrated_df` DataFrame contains online retail transactional
    data that's been enriched with descriptive columns from the `country_codes.csv`
    dataset.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果生成的`integrated_df`数据框包含了来自`country_codes.csv`数据集的描述性列，并对在线零售交易数据进行了增强。
- en: 'We also have another dataset named `adult.data` that contains the income dataset
    from the US census. Let''s integrate this dataset with the already integrated
    and enriched retail transactional dataset, as shown in the following lines of
    code:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个名为`adult.data`的数据集，其中包含了来自美国人口普查的收入数据集。我们将这个数据集与已经集成和增强的零售交易数据集进行集成，代码如下所示：
- en: '[PRE19]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the previous code snippet, we did the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们做了以下操作：
- en: We created a Spark DataFrame from the income dataset using the `spark.read.csv()`
    function. This is a comma-delimited file with a header, so we used the appropriate
    options. As a result, we have a DataFrame called `income_df`, with a few columns
    related to consumer demographics and their income levels.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`csv()`函数从收入数据集中创建了一个Spark数据框。该文件是逗号分隔的，并且有一个头部，因此我们使用了适当的选项。最终，我们得到了一个名为`income_df`的数据框，包含了与消费者人口统计和收入水平相关的一些列。
- en: Then, we added two `income_df` and `integrated_df` DataFrames so that they can
    be joined. We achieved this using the `monotonically_increasing_id()` function,
    which generates unique incremental numbers.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们添加了两个`income_df`和`integrated_df`数据框，以便可以进行连接。我们使用了`monotonically_increasing_id()`函数，它生成唯一的递增数字。
- en: The two DataFrames were then joined based on the newly generated `integrated_df`
    DataFrame, regardless of whether they have corresponding matching rows in the
    `income_df` DataFrame. The result is integrated, enriched, retail transactional
    data, with the country and the customer demographic and income information in
    a single, unified dataset.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，两个数据框基于新生成的`integrated_df`数据框进行了连接，无论它们是否在`income_df`数据框中有对应的匹配行。结果是集成的、增强的零售交易数据，包含了国家、客户人口统计信息和收入信息，所有数据都统一在一个数据集中。
- en: 'This intermediate dataset can be useful for performing `retail_enriched.delta`,
    as shown in the following code:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这个中间数据集对于执行`retail_enriched.delta`非常有用，下面的代码展示了如何使用它：
- en: '[PRE20]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the previous code block, we reduced the number of partitions of the `retailed_enriched_df`
    DataFrame to a single partition using the `coalesce()` function. This produces
    a single portable Parquet file.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们使用`coalesce()`函数将`retailed_enriched_df`数据框的分区数量减少到一个分区。这样就生成了一个单一的可移植Parquet文件。
- en: Note
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: One of the biggest challenges with learning and experimenting with big data
    analytics is finding clean and useful datasets. In the preceding code example,
    we had to introduce a surrogate key to join two independent datasets. In real-world
    scenarios, you would never force a join between datasets unless the datasets are
    related and a common join key exists between them.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 学习和实验大数据分析的最大挑战之一是找到干净且有用的数据集。在前面的代码示例中，我们必须引入一个代理键来连接两个独立的数据集。在实际应用中，除非数据集之间相关且存在公共连接键，否则你永远不会强行连接数据集。
- en: Thus, using Spark's DataFrame operations or using Spark SQL, you can integrate
    data from disparate sources and create an enriched and meaningful dataset that
    represents a single version of the truth.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用Spark的数据框操作或Spark SQL，你可以从不同来源集成数据，创建一个增强的、有意义的数据集，表示单一版本的真实数据。
- en: Integrating data using data virtualization techniques
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用数据虚拟化技术进行数据集成
- en: '**Data virtualization**, as the name implies, is a virtual process where a
    data virtualization layer acts as a logical layer on top of all the disparate
    data sources. This virtual layer acts as a conduit for business users to seamlessly
    access the required data in real time. The advantage data virtualization has over
    the traditional **ETL** and **ELT** processes is that it doesn''t require any
    data movement, and just exposes an integrated view of data to business users.
    When business users try to access the data, the data virtualization layer queries
    the underlying datasets and retrieves data in real time.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据虚拟化**，顾名思义，是一种虚拟过程，在该过程中，数据虚拟化层作为所有不同数据源之上的逻辑层。这个虚拟层充当业务用户的通道，使他们能够实时无缝访问所需数据。与传统的**ETL**和**ELT**过程相比，数据虚拟化的优势在于它不需要任何数据移动，而是直接向业务用户展示集成的数据视图。当业务用户尝试访问数据时，数据虚拟化层会查询底层数据集并实时获取数据。'
- en: The advantage of the data virtualization layer is that it completely bypasses
    any data movement, saving any time and resources that would typically be invested
    in this process. It can present data in real time with minimal to no latency as
    it directly fetches data from the source systems.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 数据虚拟化层的优势在于，它完全绕过了数据移动，节省了通常需要投入到这个过程中的时间和资源。它能够实时展示数据，几乎没有延迟，因为它直接从源系统获取数据。
- en: The disadvantage of data virtualization is that it is not a widely adopted technique
    and the products that do offer it come at a premium price. Apache Spark doesn't
    support data virtualization in its purest sense. However, Spark does support a
    type of data virtualization technique called **data federation**, which you will
    learn about in the next section.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 数据虚拟化的缺点是它并不是一种广泛采用的技术，而且提供这项技术的产品价格通常较高。Apache Spark并不支持纯粹意义上的数据虚拟化。然而，Spark支持一种称为**数据联邦**的数据虚拟化技术，您将在下一节中学习到。
- en: Data integration through data federation
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过数据联邦实现数据集成
- en: '**Data federation** is a type of data virtualization technique that uses a
    virtual database, also called a federated database, to provide a consolidated
    and homogeneous view of heterogeneous data sources. The idea here is to access
    any data anywhere from a single data processing and metadata layer. Apache Spark
    SQL Engine supports data federation, where Spark data sources can be used to define
    external data sources for seamless access from within Spark SQL. With Spark SQL,
    multiple data sources can be used with a single SQL query, without you having
    to consolidate and transform the datasets first.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据联邦**是一种数据虚拟化技术，它使用虚拟数据库（也称为联邦数据库）来提供异构数据源的统一和同质化视图。这里的思路是通过单一的数据处理和元数据层访问任何地方的数据。Apache
    Spark SQL引擎支持数据联邦，Spark的数据源可以用来定义外部数据源，从而在Spark SQL中实现无缝访问。使用Spark SQL时，可以在单一的SQL查询中使用多个数据源，而不需要先合并和转换数据集。'
- en: 'Let''s take a look at a code example to learn how to achieve data federation
    with Spark SQL:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个代码示例来学习如何使用Spark SQL实现数据联邦：
- en: '[PRE21]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the previous block of code, we created a table with MySQL as the source.
    Here, the table we created with Spark is just a pointer to the actual table in
    MySQL. Every time this Spark table is queried, it fetches data from the underlying
    MySQL table over a JDBC connection. Let''s create another table from a Spark DataFrame
    and save it in CSV format, as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一块代码中，我们创建了一个以MySQL为数据源的表。这里，我们使用Spark创建的表只是指向MySQL中实际表的指针。每次查询这个Spark表时，它都会通过JDBC连接从底层的MySQL表中获取数据。接下来，我们将从Spark
    DataFrame创建另一个表，并将其保存为CSV格式，如下所示：
- en: '[PRE22]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the preceding code block, we generated a Spark Dataframe with 16 rows and
    2 columns. The first column, labeled `id`, is just an incremental number, while
    the second column, labeled `salary`, is a random number that was generated using
    the built-in `rand()` function. We saved the DataFrame to the data lake and registered
    it with Spark''s built-in Hive metastore using the `saveAsTable()` function. Now
    that we have two tables, each residing in a separate data source, let''s see how
    we can use them together in a federated query via Spark SQL, as shown in the following
    code:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们生成了一个包含16行和2列的Spark DataFrame。第一列标记为`id`，它只是一个递增的数字；第二列标记为`salary`，它是使用内置的`rand()`函数生成的随机数。我们将DataFrame保存到数据湖中，并使用`saveAsTable()`函数将其注册到Spark内置的Hive元数据存储中。现在我们有了两个表，它们分别存在于不同的数据源中。接下来，看看我们如何在Spark
    SQL中通过联邦查询将它们一起使用，如下所示：
- en: '[PRE23]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the preceding SQL query, we joined the MySQL table to the CSV table residing
    on the data lake in the same query to produce an integrated view of data. This
    has demonstrated the data federation capabilities of Apache Spark.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的SQL查询中，我们将MySQL表与位于数据湖中的CSV表在同一查询中连接，生成了数据的集成视图。这展示了Apache Spark的数据联合功能。
- en: Tip
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Certain specialist data processing engines are designed purely to be federated
    databases, such as Presto. Presto is a distributed Massively Parallel Processing
    (MPP) query engine for big data that was designed to give very fast query performance
    on any data, anywhere. One advantage of using Apache Spark over Presto is that
    it supports data federation, along with other use cases such as batch and real-time
    analytics, data science, machine learning, and interactive SQL analytics, all
    with a single unified engine. This makes the user experience much more seamless.
    However, it is also very common for organizations to leverage several big data
    technologies for different use cases.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 某些专门的数据处理引擎纯粹设计为联合数据库，例如Presto。Presto是一个分布式的大数据大规模并行处理（MPP）查询引擎，旨在在任何数据上提供非常快速的查询性能。使用Apache
    Spark而不是Presto的一个优势是，它支持数据联合，并且能够处理其他用例，如批处理和实时分析、数据科学、机器学习和交互式SQL分析，所有这些都由单一的统一引擎支持。这使得用户体验更加无缝。然而，组织在不同用例中采用多种大数据技术也是非常常见的。
- en: To summarize, data integration is the process of consolidating and combining
    data from disparate data sources to produce meaningful data that gives a single
    version of the truth. There are several techniques surrounding data integration,
    including consolidating data using ETL or ELT techniques and data federation.
    In this section, you learned how to leverage these techniques using Apache Spark
    to achieve an integrated view of your data. The next step of your data analytics
    journey is to learn how to clean messy and dirty data via a process called **data
    cleansing**.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，数据集成是将来自不同数据源的数据进行整合和结合，生成有意义的数据，提供单一版本的真实情况。数据集成围绕着多种技术，包括使用ETL或ELT技术整合数据以及数据联合。在本节中，您学习了如何利用这些技术通过Apache
    Spark实现数据的集成视图。数据分析旅程的下一步是学习如何通过称为**数据清洗**的过程来清理混乱和脏数据。
- en: Making raw data analytics-ready using data cleansing
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据清洗使原始数据适合分析
- en: Raw transactional data can have many kinds of inconsistencies, either inherent
    to the data itself or developed during movement between various data processing
    systems, during the data ingestion process. The data integration process can also
    introduce inconsistencies in data. This is because data is being consolidated
    from disparate systems with their own mechanism for data representation. This
    data is not very clean, can have a few bad and corrupt records, and needs to be
    cleaned before it is ready to generate meaningful business insights using a process
    known as **data cleansing**.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 原始事务数据可能存在多种不一致性，这些不一致性可能是数据本身固有的，或是在不同数据处理系统之间传输过程中、数据摄取过程中产生的。数据集成过程也可能引入数据不一致性。这是因为数据正在从不同系统中整合，而这些系统有各自的数据表示机制。这些数据并不十分干净，可能包含一些坏记录或损坏的记录，在生成有意义的业务洞察之前，需要通过称为**数据清洗**的过程进行清理。
- en: Data cleansing is a part of the data analytics process and cleans data by fixing
    bad and corrupt data, removing duplicates, and selecting a set of data that's
    useful for a wide set of business use cases. When data is combined from disparate
    sources, there might be inconsistencies in the data types, including mislabeled
    or redundant data. Thus, data cleansing also incorporates data standardization
    to bring integrated data up to an enterprise's standards and conventions.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗是数据分析过程的一部分，通过修复不良和损坏的数据、删除重复项，并选择对广泛业务用例有用的数据集来清理数据。当来自不同来源的数据被合并时，可能会出现数据类型的不一致，包括错误标签或冗余数据。因此，数据清洗还包括数据标准化，以便将集成数据提升至企业的标准和惯例。
- en: The goal of data cleansing is to produce clean, consistent, and pristine data
    that is ready for the final step of generating meaningful and actionable insights
    from raw transactional data. In this section, you will learn about the various
    steps involved in the data cleansing process.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗的目标是生成干净、一致、完美的数据，为最终一步的生成有意义和可操作的洞察力做好准备，这一步骤来自原始事务数据。在本节中，您将学习数据清洗过程中的各种步骤。
- en: Data selection to eliminate redundancies
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据选择以消除冗余
- en: Once data has been integrated from various sources, there might be redundancies
    in the integrated dataset. There might be fields that are not required by your
    business analytics teams. The first step of data cleansing is identifying these
    unnecessary data elements and removing them.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦来自不同源的数据被整合，集成数据集中可能会出现冗余项。可能有些字段对于你的业务分析团队来说并不必要。数据清洗的第一步就是识别这些不需要的数据元素并将其移除。
- en: 'Let''s perform data selection on the integrated dataset we produced in the
    *Data consolidation via ETL and data warehousing* section. We need to look at
    the table schema first to see what columns we have and what their data types are.
    We can do this using the following line of code:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对我们在*通过ETL和数据仓库进行的数据整合*部分中生成的集成数据集进行数据选择。我们首先需要查看表模式，了解有哪些列以及它们的数据类型。我们可以使用以下代码行来做到这一点：
- en: '[PRE24]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The result of the preceding line of code shows all columns, and we can easily
    spot that the `Country` and `CountryName` columns are redundant. We also have
    some columns that were introduced in the dataset for data integration, and they
    are not very useful for downstream analytics. Let''s clean up the unwanted and
    redundant columns from the integrated dataset, as shown in the following block
    of code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 前一行代码的结果显示了所有列，我们可以轻松发现`Country`和`CountryName`列是冗余的。数据集中还有一些为了数据集成而引入的列，这些列对后续分析并没有太大用处。让我们清理集成数据集中不需要的冗余列，如下所示的代码块所示：
- en: '[PRE25]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the preceding code snippet, we used the `drop()` DataFrame operation to eliminate
    unwanted columns. Now that we have selected the right data columns from the integrated
    dataset, the next step is to identify and eliminate any duplicate rows.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了`drop()` DataFrame操作来删除不需要的列。现在我们已经从集成数据集中选择了正确的数据列，接下来的步骤是识别并消除任何重复的行。
- en: De-duplicating data
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去重数据
- en: 'The first step of the deduplication process is to check if we have any duplicate
    rows to begin with. We can do this using a combination of DataFrame operations,
    as shown in the following block of code:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 去重过程的第一步是检查是否有任何重复的行。我们可以通过组合DataFrame操作来做到这一点，如下所示的代码块所示：
- en: '[PRE26]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding lines of code show the count of all the rows after grouping the
    rows by the `InvoiceNo`,  `InvoiceDate`, and `StockCode` columns. Here, we are
    assuming that the `InvoiceNo`,  `InvoiceDate`, and `StockCode` column combination
    is unique and that they form the `1`. However, in the results, we can see that
    some rows have counts greater than `1`, which suggests that there might be duplicate
    rows in the dataset. This should be inspected manually, once you''ve sampled a
    few rows that show duplicates. This is to ensure they are duplicate rows. We can
    do this using the following block of code:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码行显示了在根据`InvoiceNo`、`InvoiceDate`和`StockCode`列对行进行分组后，所有行的计数。在这里，我们假设`InvoiceNo`、`InvoiceDate`和`StockCode`的组合是唯一的，并且它们构成了`1`。然而，在结果中，我们可以看到一些行的计数大于`1`，这表明数据集中可能存在重复行。这应该在你抽样检查了一些显示重复的行后手动检查，以确保它们确实是重复的。我们可以通过以下代码块来做到这一点：
- en: '[PRE27]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the preceding query, we checked a sample of the `InvoiceNo` and `StockCode`
    values to see if the returned data contains duplicates. Just eyeballing the results,
    we can see that there are duplicates in the dataset. We need to eliminate these
    duplicates. Fortunately, PySpark comes with a handy function called `drop_duplicates()`
    to just do that, as shown in the following line of code:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的查询中，我们检查了`InvoiceNo`和`StockCode`值的示例，以查看返回的数据是否包含重复项。通过目视检查结果，我们可以看到数据集中存在重复项。我们需要消除这些重复项。幸运的是，PySpark提供了一个叫做`drop_duplicates()`的便捷函数来实现这一点，如下所示的代码行所示：
- en: '[PRE28]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the preceding line of code, we used the `drop_duplicates()` function to
    eliminate duplicates based on a subset of columns. Let''s see if it eliminated
    the duplicate rows by using the following line of code:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一行代码中，我们使用了`drop_duplicates()`函数，根据一组列来消除重复项。让我们通过以下代码行来检查它是否成功删除了重复行：
- en: '[PRE29]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The previous code groups the rows based on the **composite key** and checks
    the count of each group. The result is an empty dataset, meaning that all the
    duplicates have been successfully eliminated.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码根据**复合键**对行进行了分组，并检查了每组的计数。结果是一个空数据集，这意味着所有重复项已经成功消除。
- en: So far, we have dropped unwanted columns from the integrated dataset and eliminated
    duplicates. During the data selection step, we noticed that all the columns were
    of the `string` type and that the column's names were following different naming
    conventions. This can be rectified using the data standardization process.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从集成的数据集中删除了不需要的列并消除了重复。在数据选择步骤中，我们注意到所有列的数据类型都是`string`，且列名称遵循不同的命名惯例。这可以通过数据标准化过程进行修正。
- en: Standardizing data
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标准化
- en: '**Data standardization** refers to where we make sure all the columns adhere
    to their proper data types. This is also where we bring all the column names up
    to our enterprise naming standards and conventions. This can be achieved in PySpark
    using the following DataFrame operation:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据标准化**是指确保所有列都遵循其适当的数据类型。这也是将所有列名称提升到我们企业命名标准和惯例的地方。可以通过以下DataFrame操作在PySpark中实现：'
- en: '[PRE30]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding block of code, we essentially have a SQL `SELECT` query that
    casts columns to their proper data types and aliases column names so that they
    follow proper Pythonic naming standards. The result is a final dataset that contains
    data from various sources integrated into a cleansed, deduplicated, and standardized
    data format.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，实际上是一个SQL `SELECT` 查询，它将列转换为其适当的数据类型，并为列名称指定别名，以便它们遵循合适的Python命名标准。结果是一个最终数据集，包含来自不同来源的数据，已集成成一个清洗、去重和标准化的数据格式。
- en: 'This final dataset, which is the result of the data integration and data cleansing
    phases of the data analytics process, is ready to be presented to business users
    for them to run their business analytics on. Thus, it makes sense to persist this
    dataset onto the data lake and make it available for end user consumption, as
    shown in the following line of code:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最终的数据集，是数据集成和数据清洗阶段的结果，已经准备好向业务用户展示，供他们进行业务分析。因此，将这个数据集持久化到数据湖并提供给最终用户使用是有意义的，如下所示的代码行所示：
- en: '[PRE31]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the preceding line of code, we saved our final version of the pristine transactional
    data in the data lake in Delta Lake format.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码行中，我们将最终版本的原始事务数据以Delta Lake格式保存到数据湖中。
- en: Note
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is an industry convention to call transactional data that's been replicated
    straight from the source system **bronze** data, cleansed and integrated transactional
    data **silver** data, and aggregated and summarized data **gold** data. The data
    analytics process, in a nutshell, is a continuous process of ingesting bronze
    data and transforming it into silver and gold data, until it is ready to be converted
    into actionable business insights.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在业界惯例中，从源系统直接复制的事务数据被称为**铜数据**，经过清洗和集成的事务数据被称为**银数据**，而聚合和汇总后的数据被称为**金数据**。数据分析过程，简而言之，就是一个不断摄取铜数据并将其转化为银数据和金数据的过程，直到它可以转化为可执行的业务洞察。
- en: To summarize the data cleaning process, we took the result set of the data integration
    process, removed any redundant and unnecessary columns, eliminated duplicate rows,
    and brought the data columns up to enterprise standards and conventions. All these
    data processing steps were implemented using the DataFrame API, which is powered
    by Spark SQL Engine. It can easily scale out this process to terabytes and petabytes
    of data.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结数据清洗过程，我们获取了数据集成过程的结果集，移除了任何冗余和不必要的列，消除了重复的行，并将数据列提升到企业标准和惯例。所有这些数据处理步骤都是通过DataFrame
    API实现的，该API由Spark SQL引擎提供支持。它可以轻松地将此过程扩展到数TB甚至PB的数据。
- en: Tip
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: In this chapter, data integration and data cleansing have been presented as
    two independent and mutually exclusive processes. However, in real-life use cases,
    it is a very common practice to implement these two steps together as a single
    data processing pipeline.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，数据集成和数据清洗被视为两个独立且互相排斥的过程。然而，在实际使用案例中，将这两个步骤作为一个数据处理管道共同实现是非常常见的做法。
- en: The result of the data integration and data cleansing process is usable, clean,
    and meaningful data that is ready for consumption by business analytics users.
    Since we are working at a big data scale here, data must be structured and presented
    in a way that improves the performance of business analytics queries. You will
    learn about this in the following section.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集成和数据清洗过程的结果是可用的、干净的且有意义的数据，已准备好供业务分析用户使用。由于我们在这里处理的是大数据，因此数据必须以一种提高业务分析查询性能的方式进行结构化和呈现。你将在接下来的章节中了解这一点。
- en: Optimizing ELT processing performance with data partitioning
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过数据分区优化ELT处理性能
- en: '**Data partitioning** is a process where a large dataset is physically split
    into smaller parts. This way, when a query requires a portion of the larger dataset,
    it can scan and load a subset of the partitions. This technique of eliminating
    partitions that are not required by the query is called **partition pruning**.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据分区**是一个将大数据集物理拆分为较小部分的过程。这样，当查询需要大数据集的一部分时，它可以扫描并加载分区的子集。这种排除查询不需要的分区的技术被称为**分区修剪**。'
- en: '**Predicate pushdown** is another technique where parts of a query that filter,
    slice, and dice data, called the **predicate**, are pushed down to the data storage
    layer. It then becomes the data storage layer''s responsibility to filter out
    all the partitions not required by the query.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**谓词下推**是另一种技术，将查询中的一些过滤、切片和切割数据的部分，即**谓词**，下推到数据存储层。然后，由数据存储层负责过滤掉所有查询不需要的分区。'
- en: Traditional RDBMS and data warehouses have always supported data partitioning,
    partition pruning, and predicate pushdown. Semi-structured file formats such as
    CSV and JSON support data partitioning and partition pruning but do not support
    predicate pushdown. Apache Spark fully supports all three. With predicate pushdown,
    Spark can delegate the task of filtering out data to the underlying data storage
    layer, thus reducing the amount of data that needs to be loaded into Spark's memory
    and then processed.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的关系型数据库管理系统（RDBMS）和数据仓库一直都支持数据分区、分区修剪和谓词下推。像CSV和JSON这样的半结构化文件格式支持数据分区和分区修剪，但不支持谓词下推。Apache
    Spark完全支持这三种技术。通过谓词下推，Spark可以将过滤数据的任务委派给底层数据存储层，从而减少需要加载到Spark内存中的数据量，并进一步进行处理。
- en: Structured data formats such as Parquet, ORC, and Delta Lake fully support partitioning
    pruning and predicate pushdown. This helps Spark's Catalyst Optimizer generate
    the best possible query execution plan. This is a strong reason to favor structured
    file formats such as Apache Parquet with Spark over semi-structured data formats.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据格式如Parquet、ORC和Delta Lake完全支持分区修剪和谓词下推。这有助于Spark的Catalyst优化器生成最佳的查询执行计划。这是优先选择像Apache
    Parquet这样结构化文件格式而不是半结构化数据格式的一个有力理由。
- en: 'Consider that your data lake has historical data spanning several years and
    that your typical queries involve only a few months to a few years of data at
    a time. You can choose to store your data completely unpartitioned, with all the
    data in a single folder. Alternatively, you can partition your data by year and
    month attributes, as shown in the following diagram:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的数据湖中包含跨越数年的历史数据，而你的典型查询通常只涉及几个月到几年之间的数据。你可以选择将数据完全不分区，所有数据存储在一个文件夹中。或者，你可以按照年份和月份属性对数据进行分区，如下图所示：
- en: '![Figure 3.9 – Data partitioning'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.9 – 数据分区'
- en: '](img/B16736_03_09.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_03_09.jpg)'
- en: Figure 3.9 – Data partitioning
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 数据分区
- en: On the right-hand side of the preceding diagram, we have unpartitioned data.
    This pattern of data storage makes data storage a little easier because we just
    keep appending new data to the same folder over and over again. However, after
    a certain point, the data becomes unmanageable and makes it difficult to perform
    any updates or deletes. Moreover, Apache Spark would need to read the entire dataset
    into memory, losing any advantages that partition pruning and predicate pushdown
    could have offered.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面图示的右侧，我们有未分区的数据。这样的数据存储模式使得数据存储变得稍微简单一些，因为我们只是反复将新数据追加到同一个文件夹中。然而，到了一定程度后，数据会变得难以管理，也使得执行任何更新或删除操作变得困难。此外，Apache
    Spark需要将整个数据集读取到内存中，这样会丧失分区修剪和谓词下推可能带来的优势。
- en: On the right-hand side of the diagram, data is partitioned by year and then
    by month. This makes writing data a little more involved as the Spark application
    would need to choose the right partition every time before writing data. However,
    this is a small penalty compared to the efficiency and performance that's gained
    with updates and deletes, as well as downstream queries. Queries on such partitioned
    data will be orders of magnitude faster as they make full use of partition pruning
    and predicate pushdown. Thus, it is recommended to partition data with an appropriate
    partition key to get the best performance and efficiency out of your data lake.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的右侧，数据按照年份分区，然后按照月份分区。这使得写入数据稍微复杂一些，因为Spark应用程序每次写入数据之前都需要选择正确的分区。然而，与更新、删除以及下游查询带来的效率和性能相比，这只是一个小代价。对这种分区数据的查询将比未分区的数据快几个数量级，因为它们充分利用了分区修剪和谓词下推。因此，推荐使用适当的分区键对数据进行分区，以从数据湖中获得最佳的性能和效率。
- en: Since data partitioning plays a crucial role in determining the performance
    of downstream analytical queries, it is important to choose the right partitioning
    column. As a general rule of thumb, choose a partition column with low cardinality.
    Partition sizes of at least one gigabyte are practical and typically, a date-based
    column makes for a good candidate for the partition key.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据分区在决定下游分析查询性能方面起着至关重要的作用，因此选择合适的分区列非常重要。一般的经验法则是，选择一个基数较低的分区列。至少为一千兆字节的分区大小是实际可行的，通常，基于日期的列是一个很好的分区键候选。
- en: Note
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Recursive file listing on cloud-based object storage is usually slow and expensive.
    So, using hierarchical partitioning on cloud-based object stores is not very efficient
    and thus not recommended. This could be a performance bottleneck when more than
    one partition key is required. Databricks's proprietary version of Delta Lake,
    along with their Delta Engine, supports techniques such as **dynamic file pruning**
    and **Z-order** multi-dimensional indexes to help solve the problems of hierarchical
    partitioning on cloud-based data lakes. You can read more about them at [https://docs.databricks.com/delta/optimizations/dynamic-file-pruning.html](https://docs.databricks.com/delta/optimizations/dynamic-file-pruning.html).
    However, these techniques are not available in the open source version of Delta
    Lake yet.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 云端对象存储的递归文件列出通常较慢且费用昂贵。因此，在云端对象存储上使用层次分区并不是很高效，因此不推荐使用。当需要多个分区键时，这可能会成为性能瓶颈。Databricks的专有版本Delta
    Lake以及他们的Delta Engine支持**动态文件修剪**和**Z-order**多维索引等技术，帮助解决云端数据湖中层次分区的问题。你可以在[https://docs.databricks.com/delta/optimizations/dynamic-file-pruning.html](https://docs.databricks.com/delta/optimizations/dynamic-file-pruning.html)了解更多信息。然而，这些技术目前还没有在Delta
    Lake的开源版本中提供。
- en: Summary
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about two prominent methodologies of data processing
    known as **ETL** and **ELT** and saw the advantages of using ETL to unlock more
    analytics use cases than what's possible with ETL. By doing this, you understood
    the scalable storage and compute requirements of ETL and how modern cloud technologies
    help enable the ELT way of data processing. Then, you learned about the shortcomings
    of using cloud-based data lakes as analytics data stores, such as having a lack
    of atomic transactional and durability guarantees. After, you were introduced
    to Delta Lake as a modern data storage layer designed to overcome the shortcomings
    of cloud-based data lakes. You learned about the data integration and data cleansing
    techniques, which help consolidate raw transactional data from disparate sources
    to produce clean, pristine data that is ready to be presented to end users to
    generate meaningful insights. You also learned how to implement each of the techniques
    used in this chapter using DataFrame operations and Spark SQL. You gained skills
    that are essential for transforming raw transactional data into meaningful, enriched
    data using the ELT methodology for big data at scale.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了两种著名的数据处理方法——**ETL**和**ELT**，并看到了使用ETL方法能解锁更多分析用例的优势，这些用例是使用ETL方法无法实现的。通过这样做，你理解了ETL的可扩展存储和计算需求，以及现代云技术如何帮助实现ELT的数据处理方式。接着，你了解了将基于云的数据湖作为分析数据存储的不足之处，例如缺乏原子事务和持久性保证。然后，你被介绍到Delta
    Lake，这是一种现代数据存储层，旨在克服基于云的数据湖的不足。你学习了数据集成和数据清洗技术，这些技术有助于将来自不同来源的原始事务数据整合起来，生成干净、纯净的数据，这些数据准备好呈现给最终用户以生成有意义的见解。你还学习了如何通过DataFrame操作和Spark
    SQL实现本章中使用的每一种技术。你获得了将原始事务数据转换为有意义的、丰富的数据的技能，这些技能对于使用ELT方法在大规模大数据中进行处理至关重要。
- en: Typically, the data cleansing and integration processes are performance-intensive
    and are implemented in a batch processing manner. However, in big data analytics,
    you must get the latest transactional data to the end users as soon as it is generated
    at the source. This is very helpful in tactical decision-making and is made possible
    by real-time data analytics, which you will learn about in the next chapter.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据清洗和集成过程是性能密集型的，且以批处理方式实现。然而，在大数据分析中，你必须在事务数据在源头生成后尽快将其传递给最终用户。这对战术决策非常有帮助，并且通过实时数据分析得以实现，实时数据分析将在下一章中讲解。
