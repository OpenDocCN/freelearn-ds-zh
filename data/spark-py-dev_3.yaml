- en: Chapter 3. Juggling Data with Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。使用Spark玩弄数据
- en: As per the batch and streaming architecture laid out in the previous chapter,
    we need data to fuel our applications. We will harvest data focused on Apache
    Spark from Twitter. The objective of this chapter is to prepare data to be further
    used by the machine learning and streaming applications. This chapter focuses
    on how to exchange code and data across the distributed network. We will get practical
    insights into serialization, persistence, marshaling, and caching. We will get
    to grips with on Spark SQL, the key Spark module to interactively explore structured
    and semi-structured data. The fundamental data structure powering Spark SQL is
    the Spark dataframe. The Spark dataframe is inspired by the Python Pandas dataframe
    and the R dataframe. It is a powerful data structure, well understood and appreciated
    by data scientists with a background in R or Python.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上一章中概述的批处理和流处理架构，我们需要数据来支持我们的应用程序。我们将从Twitter上收集关于Apache Spark的数据。本章的目标是准备数据以供机器学习和流处理应用程序进一步使用。本章重点介绍如何在分布式网络中交换代码和数据。我们将深入了解序列化、持久化、编组和缓存。我们将深入了解Spark
    SQL，这是交互式地探索结构化和半结构化数据的关键Spark模块。支持Spark SQL的基本数据结构是Spark dataframe。Spark dataframe受到Python
    Pandas dataframe和R dataframe的启发。它是一种强大的数据结构，被具有R或Python背景的数据科学家充分理解和赞赏。
- en: 'In this chapter, we will cover the following points:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Connect to Twitter, collect the relevant data, and then persist it in various
    formats such as JSON and CSV and data stores such as MongoDB
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到Twitter，收集相关数据，然后以JSON和CSV等各种格式以及数据存储（如MongoDB）进行持久化
- en: Analyze the data using Blaze and Odo, a spin-off library from Blaze, in order
    to connect and transfer data from various sources and destinations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Blaze和Odo来分析数据，Odo是Blaze的一个衍生库，用于连接和传输来自各种来源和目的地的数据
- en: Introduce Spark dataframes as the foundation for data interchange between the
    various Spark modules and explore data interactively using Spark SQL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Spark dataframe作为各种Spark模块之间数据交换的基础，并使用Spark SQL交互式地探索数据
- en: Revisiting the data-intensive app architecture
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视数据密集型应用架构
- en: Let's first put in context the focus of this chapter with respect to the data-intensive
    app architecture. We will concentrate our attention on the integration layer and
    essentially run through iterative cycles of the acquisition, refinement, and persistence
    of the data. This cycle was termed the five Cs. The five Cs stand for *connect*,
    *collect*, *correct*, *compose*, and *consume*. They are the essential processes
    we run through in the integration layer in order to get to the right quality and
    quantity of data retrieved from Twitter. We will also delve deeper in the persistence
    layer and set up a data store such as MongoDB to collect our data for processing
    later.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先将本章的重点与数据密集型应用架构放在一起。我们将集中精力放在集成层上，并基本上通过迭代循环来运行数据的获取、精炼和持久化。这个循环被称为五个C。五个C代表连接、收集、校正、组合和消费。这些是我们在集成层中运行的基本流程，以便从Twitter中获取正确质量和数量的数据。我们还将深入研究持久化层，并设置一个数据存储，如MongoDB，以便稍后收集我们的数据进行处理。
- en: We will explore the data with Blaze, a Python library for data manipulation,
    and Spark SQL, the interactive module of Spark for data discovery powered by the
    Spark dataframe. The dataframe paradigm is shared by Python Pandas, Python Blaze,
    and Spark SQL. We will get a feel for the nuances of the three dataframe flavors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Blaze和Spark SQL来探索数据，Blaze是用于数据操作的Python库，而Spark SQL是由Spark dataframe支持的用于数据发现的交互模块。Dataframe范式由Python
    Pandas、Python Blaze和Spark SQL共享。我们将了解这三种dataframe的细微差别。
- en: 'The following diagram sets the context of the chapter''s focus, highlighting
    the integration layer and the persistence layer:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表设置了本章重点的背景，突出了集成层和持久化层：
- en: '![Revisiting the data-intensive app architecture](img/B03986_03_01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: 重新审视数据密集型应用架构
- en: Serializing and deserializing data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列化和反序列化数据
- en: As we are harvesting data from web APIs under rate limit constraints, we need
    to store them. As the data is processed on a distributed cluster, we need consistent
    ways to save state and retrieve it for later usage.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络API中收集数据时受到速率限制的约束，我们需要将它们存储起来。由于数据在分布式集群上进行处理，我们需要一致的方法来保存状态并在以后使用时检索它。
- en: Let's now define serialization, persistence, marshaling, and caching or memorization.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义序列化、持久化、编组和缓存或记忆化。
- en: Serializing a Python object converts it into a stream of bytes. The Python object
    needs to be retrieved beyond the scope of its existence, when the program is shut.
    The serialized Python object can be transferred over a network or stored in a
    persistent storage. Deserialization is the opposite and converts the stream of
    bytes into the original Python object so the program can carry on from the saved
    state. The most popular serialization library in Python is Pickle. As a matter
    of fact, the PySpark commands are transferred over the wire to the worker nodes
    via pickled data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 将Python对象序列化为一系列字节。当程序关闭时，需要检索Python对象以超出其存在范围，序列化的Python对象可以通过网络传输或存储在持久存储中。反序列化是相反的过程，将一系列字节转换为原始的Python对象，以便程序可以从保存的状态继续进行。Python中最流行的序列化库是Pickle。事实上，PySpark命令通过pickled数据通过网络传输到工作节点。
- en: Persistence saves a program's state data to disk or memory so that it can carry
    on where it left off upon restart. It saves a Python object from memory to a file
    or a database and loads it later with the same state.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 持久化将程序的状态数据保存到磁盘或内存中，以便在重新启动时可以继续之前的工作。它将Python对象从内存保存到文件或数据库中，并在以后以相同的状态加载它。
- en: Marshalling sends Python code or data over a network TCP connection in a multicore
    or distributed system.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 编组将Python代码或数据通过网络TCP连接发送到多核或分布式系统中。
- en: Caching converts a Python object to a string in memory so that it can be used
    as a dictionary key later on. Spark supports pulling a dataset into a cluster-wide,
    in-memory cache. This is very useful when data is accessed repeatedly such as
    when querying a small reference dataset or running an iterative algorithm such
    as Google PageRank.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存将Python对象转换为内存中的字符串，以便以后可以用作字典键。Spark支持将数据集缓存在整个集群的内存中。当数据被重复访问时，比如查询一个小的参考数据集或运行迭代算法（如Google
    PageRank）时，这是非常有用的。
- en: Caching is a crucial concept for Spark as it allows us to save RDDs in memory
    or with a spillage to disk. The caching strategy can be selected based on the
    lineage of the data or the **DAG** (short for **Directed Acyclic Graph**) of transformations
    applied to the RDDs in order to minimize shuffle or cross network heavy data exchange.
    In order to achieve good performance with Spark, beware of data shuffling. A good
    partitioning policy and use of RDD caching, coupled with avoiding unnecessary
    action operations, leads to better performance with Spark.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存对于Spark来说是一个关键概念，因为它允许我们将RDD保存在内存中或溢出到磁盘。缓存策略可以根据数据的血统或RDD应用的转换的DAG（有向无环图的缩写）来选择，以最小化洗牌或跨网络的重数据交换。为了在Spark中实现良好的性能，要注意数据洗牌。良好的分区策略和RDD缓存的使用，再加上避免不必要的操作操作，可以提高Spark的性能。
- en: Harvesting and storing data
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集和存储数据
- en: 'Before delving into database persistent storage such as MongoDB, we will look
    at some useful file storages that are widely used: **CSV** (short for **comma-separated
    values**) and **JSON** (short for **JavaScript Object Notation**) file storage.
    The enduring popularity of these two file formats lies in a few key reasons: they
    are human readable, simple, relatively lightweight, and easy to use.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究数据库持久存储（如MongoDB）之前，我们将看一些广泛使用的有用文件存储：CSV（逗号分隔值的缩写）和JSON（JavaScript对象表示法的缩写）文件存储。这两种文件格式的持久受欢迎之处在于几个关键原因：它们易于阅读，简单，相对轻量级，易于使用。
- en: Persisting data in CSV
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在CSV中持久化数据
- en: The CSV format is lightweight, human readable, and easy to use. It has delimited
    text columns with an inherent tabular schema.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: CSV格式是轻量级的，易于阅读和使用。它具有带有固有表格模式的分隔文本列。
- en: Python offers a robust `csv` library that can serialize a `csv` file into a
    Python dictionary. For the purpose of our program, we have written a `python`
    class that manages to persist data in CSV format and read from a given CSV.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Python提供了一个强大的`csv`库，可以将`csv`文件序列化为Python字典。为了实现我们的程序目的，我们编写了一个`python`类，用于管理以CSV格式持久化数据并从给定的CSV文件中读取。
- en: 'Let''s run through the code of the class `IO_csv` object. The `__init__` section
    of the class basically instantiates the file path, the filename, and the file
    suffix (in this case, `.csv`):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行`IO_csv`类对象的代码。该类的`__init__`部分基本上实例化了文件路径、文件名和文件后缀（在本例中为`.csv`）：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `save` method of the class uses a Python named tuple and the header fields
    of the `csv` file in order to impart a schema while persisting the rows of the
    CSV. If the `csv` file already exists, it will be appended and not overwritten
    otherwise; it will be created:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的`save`方法使用Python命名元组和`csv`文件的标题字段，以便在持久化CSV的同时传递模式。如果`csv`文件已经存在，它将被追加而不是覆盖；否则将被创建：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `load` method of the class also uses a Python named tuple and the header
    fields of the `csv` file in order to retrieve the data using a consistent schema.
    The `load` method is a memory-efficient generator to avoid loading a huge file
    in memory: hence we use `yield` in place of `return`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的`load`方法还使用Python命名元组和`csv`文件的标题字段，以便使用一致的模式检索数据。`load`方法是一个内存高效的生成器，以避免在内存中加载大文件：因此我们使用`yield`代替`return`：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here''s the named tuple. We are using it to parse the tweet in order to save
    or retrieve them to and from the `csv` file:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是命名元组。我们使用它来解析推文，以便将它们保存或从`csv`文件中检索出来：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Persisting data in JSON
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在JSON中持久化数据
- en: JSON is one of the most popular data formats for Internet-based applications.
    All the APIs we are dealing with, Twitter, GitHub, and Meetup, deliver their data
    in JSON format. The JSON format is relatively lightweight compared to XML and
    human readable, and the schema is embedded in JSON. As opposed to the CSV format,
    where all records follow exactly the same tabular structure, JSON records can
    vary in their structure. JSON is semi-structured. A JSON record can be mapped
    into a Python dictionary of dictionaries.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是互联网应用程序中最流行的数据格式之一。我们正在处理的所有API，Twitter、GitHub和Meetup，都以JSON格式传递它们的数据。与XML相比，JSON格式相对轻量级且易于阅读，其模式嵌入在JSON中。与CSV格式相反，其中所有记录都遵循完全相同的表格结构，JSON记录的结构可以有所不同。JSON是半结构化的。JSON记录可以映射到Python字典的字典中。
- en: 'Let''s run through the code of the class `IO_json` object. The `__init__` section
    of the class basically instantiates the file path, the filename, and the file
    suffix (in this case, `.json`):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行`IO_json`类对象的代码。该类的`__init__`部分基本上实例化了文件路径、文件名和文件后缀（在本例中为`.json`）：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `save` method of the class uses `utf-8` encoding in order to ensure read
    and write compatibility of the data. If the JSON file already exists, it will
    be appended and not overwritten; otherwise it will be created:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的`save`方法使用`utf-8`编码，以确保数据的读取和写入兼容性。如果JSON文件已经存在，它将被追加而不是覆盖；否则将被创建：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `load` method of the class just returns the file that has been read. A
    further `json.loads` function needs to be applied in order to retrieve the `json`
    out of the file read:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的`load`方法只返回已读取的文件。需要进一步应用`json.loads`函数以从读取的文件中检索出`json`：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Setting up MongoDB
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置MongoDB
- en: It is crucial to store the information harvested. Thus, we set up MongoDB as
    our main document data store. As all the information collected is in JSON format
    and MongoDB stores information in **BSON** (short for **Binary JSON**), it is
    therefore a natural choice.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 存储收集到的信息至关重要。因此，我们将MongoDB设置为我们的主要文档数据存储。由于收集的所有信息都是以JSON格式，而MongoDB以BSON（Binary
    JSON的缩写）存储信息，因此它是一个自然的选择。
- en: 'We will run through the following steps now:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将按以下步骤进行：
- en: Installing the MongoDB server and client
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装MongoDB服务器和客户端
- en: Running the MongoDB server
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行MongoDB服务器
- en: Running the Mongo client
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Mongo客户端
- en: Installing the PyMongo driver
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装PyMongo驱动程序
- en: Creating the Python Mongo client
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建Python Mongo客户端
- en: Installing the MongoDB server and client
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装MongoDB服务器和客户端
- en: 'In order to install the MongoDB package, perform through the following steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安装MongoDB软件包，请按以下步骤执行：
- en: 'Import the public key used by the package management system (in our case, Ubuntu''s
    `apt`). To import the MongoDB public key, we issue the following command:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入软件包管理系统（在我们的情况下是Ubuntu的`apt`）使用的公钥。要导入MongoDB公钥，我们发出以下命令：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create a list file for MongoDB. To create the list file, we use the following
    command:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为MongoDB创建一个列表文件。要创建列表文件，我们使用以下命令：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Update the local package database as `sudo`:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将本地软件包数据库更新为`sudo`：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Install the MongoDB packages. We install the latest stable version of MongoDB
    with the following command:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装MongoDB软件包。我们使用以下命令安装MongoDB的最新稳定版本：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Running the MongoDB server
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行MongoDB服务器
- en: 'Let''s start the MongoDB server:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动MongoDB服务器：
- en: 'To start MongoDB server, we issue the following command to start `mongod`:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启动MongoDB服务器，我们发出以下命令来启动`mongod`：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To check whether `mongod` has started properly, we issue the command:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要检查`mongod`是否已正确启动，我们发出以下命令：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this case, we see that `mongodb` is running in process `967`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们看到`mongodb`正在进程`967`中运行。
- en: The `mongod` server sends a message to the effect that it is waiting for connection
    on `port 27017`. This is the default port for MongoDB. It can be changed in the
    configuration file.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`mongod`服务器发送一条消息，表示它正在等待`端口27017`上的连接。这是MongoDB的默认端口。可以在配置文件中更改。'
- en: 'We can check the contents of the log file at `/var/log/mongod/mongod.log`:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在`/var/log/mongod/mongod.log`中检查日志文件的内容：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In order to stop the `mongodb` server, just issue the following command:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要停止`mongodb`服务器，只需发出以下命令：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Running the Mongo client
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行Mongo客户端
- en: 'Running the Mongo client in the console is as easy as calling `mongo`, as highlighted
    in the following command:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中运行Mongo客户端就像调用`mongo`一样简单，如以下命令所示：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'At the mongo client console prompt, we can see the databases with the following
    commands:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在mongo客户端控制台提示符下，我们可以使用以下命令查看数据库：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We select the test database using `use test`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`use test`选择测试数据库：
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We display the collections within the test database:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们显示测试数据库中的集合：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We check a sample record in the restaurant collection listed previously:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查先前列出的餐厅集合中的一个示例记录：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Installing the PyMongo driver
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装PyMongo驱动程序
- en: 'Installing the Python driver with anaconda is easy. Just run the following
    command at the terminal:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用anaconda安装Python驱动程序很容易。只需在终端运行以下命令：
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Creating the Python client for MongoDB
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建Python客户端以用于MongoDB
- en: 'We are creating a `IO_mongo` class that will be used in our harvesting and
    processing programs to store the data collected and retrieved saved information.
    In order to create the `mongo` client, we will import the `MongoClient` module
    from `pymongo`. We connect to the `mongodb` server on localhost at port 27017\.
    The command is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在创建一个`IO_mongo`类，该类将用于我们的收集和处理程序中，以存储收集和检索到的信息。为了创建`mongo`客户端，我们将从`pymongo`导入`MongoClient`模块。我们在本地主机的端口27017上连接到`mongodb`服务器。命令如下：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We initialize our class with the client connection, the database (in this case,
    `twtr_db`), and the collection (in this case, `twtr_coll`) to be accessed:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过客户端连接、数据库（在本例中为`twtr_db`）和要访问的集合（在本例中为`twtr_coll`）来初始化我们的类：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `save` method inserts new records in the preinitialized collection and
    database:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`save`方法在预初始化的集合和数据库中插入新记录：'
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `load` method allows the retrieval of specific records according to criteria
    and projection. In the case of large amount of data, it returns a cursor:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`load`方法允许根据条件和投影检索特定记录。在数据量大的情况下，它返回一个游标：'
- en: '[PRE24]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Harvesting data from Twitter
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Twitter收集数据
- en: Each social network poses its limitations and challenges. One of the main obstacles
    for harvesting data is an imposed rate limit. While running repeated or long-running
    connections between rates limit pauses, we have to be careful to avoid collecting
    duplicate data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每个社交网络都存在其局限性和挑战。收集数据的主要障碍之一是强加的速率限制。在运行重复或长时间连接的速率限制暂停时，我们必须小心避免收集重复数据。
- en: We have redesigned our connection programs outlined in the previous chapter
    to take care of the rate limits.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经重新设计了在前一章中概述的连接程序，以处理速率限制。
- en: 'In this `TwitterAPI` class that connects and collects the tweets according
    to the search query we specify, we have added the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个`TwitterAPI`类中，根据我们指定的搜索查询连接和收集推文，我们添加了以下内容：
- en: Logging capability using the Python logging library with the aim of collecting
    any errors or warning in the case of program failure
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python日志库的记录功能，以便在程序失败时收集任何错误或警告。
- en: Persistence capability using MongoDB, with the `IO_mongo` class exposed previously
    as well as JSON file using the `IO_json` class
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MongoDB的持久性功能，以及之前公开的`IO_mongo`类和使用`IO_json`类的JSON文件
- en: API rate limit and error management capability, so we can ensure more resilient
    calls to Twitter without getting barred for tapping into the firehose
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API速率限制和错误管理功能，因此我们可以确保更具弹性地调用Twitter，而不会因为接入firehose而被禁止
- en: 'Let''s go through the steps:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按以下步骤进行：
- en: 'We initialize by instantiating the Twitter API with our credentials:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过实例化Twitter API来初始化：
- en: '[PRE25]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We initialize the logger by providing the log level:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过提供日志级别来初始化记录器：
- en: '`logger.debug`(debug message)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logger.debug`（调试消息）'
- en: '`logger.info`(info message)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logger.info`（信息消息）'
- en: '`logger.warn`(warn message)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logger.warn`（警告消息）'
- en: '`logger.error`(error message)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logger.error`（错误消息）'
- en: '`logger.critical`(critical message)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logger.critical`（临界消息）'
- en: 'We set the log path and the message format:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置日志路径和消息格式：
- en: '[PRE26]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We initialize the JSON file persistence instruction:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化JSON文件持久性指令：
- en: '[PRE27]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We initialize the MongoDB database and collection for persistence:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化MongoDB数据库和集合以进行持久化：
- en: '[PRE28]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The method `searchTwitter` launches the search according to the query specified:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`searchTwitter`方法根据指定的查询启动搜索：'
- en: '[PRE29]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `saveTweets` method actually saves the collected tweets in JSON and in
    MongoDB:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`saveTweets`方法实际上将收集到的推文保存为JSON和MongoDB：'
- en: '[PRE30]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `parseTweets` method allows us to extract the key tweet information from
    the vast amount of information provided by the Twitter API:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`parseTweets`方法允许我们从Twitter API提供的大量信息中提取关键的推文信息：'
- en: '[PRE31]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `getTweets` method calls the `searchTwitter` method described previously.
    The `getTweets` method ensures that API calls are made reliably whilst respecting
    the imposed rate limit. The code is as follows:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`getTweets`方法调用了先前描述的`searchTwitter`方法。`getTweets`方法确保可靠地进行API调用，同时尊重强加的速率限制。代码如下：'
- en: '[PRE32]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here, we are calling the `searchTwitter` API with the relevant query based
    on the parameters specified. If we encounter any error such as rate limitation
    from the provider, this will be processed by the `handleError` method:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们根据指定的参数调用`searchTwitter`API进行相关查询。如果我们遇到来自提供者的速率限制等错误，将由`handleError`方法处理：
- en: '[PRE33]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Exploring data using Blaze
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Blaze探索数据
- en: Blaze is an open source Python library, primarily developed by Continuum.io,
    leveraging Python Numpy arrays and Pandas dataframe. Blaze extends to out-of-core
    computing, while Pandas and Numpy are single-core.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Blaze是一个开源的Python库，主要由Continuum.io开发，利用Python Numpy数组和Pandas数据框架。Blaze扩展到了核外计算，而Pandas和Numpy是单核的。
- en: 'Blaze offers an adaptable, unified, and consistent user interface across various
    backends. Blaze orchestrates the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Blaze在各种后端之间提供了一个适应性强、统一和一致的用户界面。Blaze编排以下内容：
- en: '**Data**: Seamless exchange of data across storages such as CSV, JSON, HDF5,
    HDFS, and Bcolz files.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据**：在不同存储之间无缝交换数据，如CSV、JSON、HDF5、HDFS和Bcolz文件。'
- en: '**Computation**: Using the same query processing against computational backends
    such as Spark, MongoDB, Pandas, or SQL Alchemy.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算**：使用相同的查询处理对计算后端进行计算，如Spark、MongoDB、Pandas或SQL Alchemy。'
- en: '**Symbolic expressions**: Abstract expressions such as join, group-by, filter,
    selection, and projection with a syntax similar to Pandas but limited in scope.
    Implements the split-apply-combine methods pioneered by the R language.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**符号表达式**：抽象表达式，如连接、分组、过滤、选择和投影，其语法类似于Pandas，但范围有限。实现了R语言开创的分割-应用-合并方法。'
- en: Blaze expressions are lazily evaluated and in that respect share a similar processing
    paradigm with Spark RDDs transformations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Blaze表达式是惰性评估的，在这方面与Spark RDD转换共享类似的处理范式。
- en: 'Let''s dive into Blaze by first importing the necessary libraries: `numpy`,
    `pandas`, `blaze` and `odo`. Odo is a spin-off of Blaze and ensures data migration
    from various backends. The commands are as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先导入必要的库来深入了解Blaze：`numpy`、`pandas`、`blaze`和`odo`。Odo是Blaze的一个衍生项目，确保从各种后端迁移数据。命令如下：
- en: '[PRE34]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We create a Pandas `Dataframe` by reading the parsed tweets saved in a CSV
    file, `twts_csv`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过读取保存在CSV文件`twts_csv`中的解析推文来创建一个Pandas `Dataframe`：
- en: '[PRE35]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We run the Tweets Panda `Dataframe` to the `describe()` function to get some
    overall information on the dataset:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行Tweets Panda `Dataframe`到`describe()`函数，以获取数据集的一些整体信息：
- en: '[PRE36]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We convert the Pandas `dataframe` into a Blaze `dataframe` by simply passing
    it through the `Data()` function:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过简单地通过`Data()`函数传递Pandas `dataframe`将其转换为Blaze `dataframe`：
- en: '[PRE37]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can retrieve the schema representation of the Blaze `dataframe` by passing
    the `schema` function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过传递`schema`函数来检索Blaze `dataframe`的模式表示：
- en: '[PRE38]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `.dshape` function gives a record count and the schema:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`.dshape`函数给出记录计数和模式：'
- en: '[PRE39]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can print the Blaze `dataframe` content:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打印Blaze `dataframe`的内容：
- en: '[PRE40]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We extract the column `tweet_text` and take the unique values:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提取列`tweet_text`并获取唯一值：
- en: '[PRE41]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We extract multiple columns `[''id'', ''user_name'',''tweet_text'']` from the
    `dataframe` and take the unique records:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`dataframe`中提取多列`['id', 'user_name','tweet_text']`并获取唯一记录：
- en: '[PRE42]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Transferring data using Odo
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Odo传输数据
- en: 'Odo is a spin-off project of Blaze. Odo allows the interchange of data. Odo
    ensures the migration of data across different formats (CSV, JSON, HDFS, and more)
    and across different databases (SQL databases, MongoDB, and so on) using a very
    simple predicate:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Odo是Blaze的一个衍生项目。Odo允许数据的交换。Odo确保数据在不同格式（CSV、JSON、HDFS等）和不同数据库（SQL数据库、MongoDB等）之间的迁移非常简单：
- en: '[PRE43]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To transfer to a database, the address is specified using a URL. For example,
    for a MongoDB database, it would look like this:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要传输到数据库，需要使用URL指定地址。例如，对于MongoDB数据库，它看起来像这样：
- en: '[PRE44]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let''s run some examples of using Odo. Here, we illustrate `odo` by reading
    a CSV file and creating a Blaze `dataframe`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一些使用Odo的示例。在这里，我们通过读取一个CSV文件并创建一个Blaze `dataframe`来说明`odo`：
- en: '[PRE45]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Count the number of records in the `dataframe`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 计算`dataframe`中的记录数：
- en: '[PRE46]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Display the five initial records of the `dataframe`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 显示`dataframe`的前五条记录：
- en: '[PRE47]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Get `dshape` information from the `dataframe`, which gives us the number of
    records and the schema:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从`dataframe`获取`dshape`信息，这给出了记录数和模式：
- en: '[PRE48]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Save a processed Blaze `dataframe` into JSON:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将处理后的Blaze `dataframe`保存为JSON：
- en: '[PRE49]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Convert a JSON file to a CSV file:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 将JSON文件转换为CSV文件：
- en: '[PRE50]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Exploring data using Spark SQL
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL探索数据
- en: Spark SQL is a relational query engine built on top of Spark Core. Spark SQL
    uses a query optimizer called **Catalyst**.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是建立在Spark Core之上的关系查询引擎。Spark SQL使用名为**Catalyst**的查询优化器。
- en: Relational queries can be expressed using SQL or HiveQL and executed against
    JSON, CSV, and various databases. Spark SQL gives us the full expressiveness of
    declarative programing with Spark dataframes on top of functional programming
    with RDDs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用SQL或HiveQL表示关系查询，并针对JSON、CSV和各种数据库执行。Spark SQL使我们能够在功能编程的RDD之上使用Spark数据框架的声明式编程的全部表达能力。
- en: Understanding Spark dataframes
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解Spark数据框架
- en: Here's a tweet from `@bigdata` announcing Spark 1.3.0, the advent of Spark SQL
    and dataframes. It also highlights the various data sources in the lower part
    of the diagram. On the top part, we can notice R as the new language that will
    be gradually supported on top of Scala, Java, and Python. Ultimately, the Data
    Frame philosophy is pervasive between R, Python, and Spark.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一条来自`@bigdata`的推文，宣布了Spark 1.3.0的到来，以及Spark SQL和数据框的出现。它还突出了图表下部的各种数据源。在图表的上部，我们可以注意到R作为新语言，将逐渐支持Scala、Java和Python。最终，数据框的理念在R、Python和Spark之间普遍存在。
- en: '![Understanding Spark dataframes](img/B03986_03_02.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![理解Spark数据框](img/B03986_03_02.jpg)'
- en: Spark dataframes originate from SchemaRDDs. It combines RDD with a schema that
    can be inferred by Spark, if requested, when registering the dataframe. It allows
    us to query complex nested JSON data with plain SQL. Lazy evaluation, lineage,
    partitioning, and persistence apply to dataframes.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Spark数据框源自SchemaRDDs。它将RDD与可以由Spark推断的模式结合在一起，如果请求的话，可以在注册数据框时推断出模式。它允许我们使用普通SQL查询复杂嵌套的JSON数据。惰性评估、血统、分区和持久性适用于数据框。
- en: 'Let''s query the data with Spark SQL, by first importing `SparkContext` and
    `SQLContext`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过首先导入`SparkContext`和`SQLContext`来使用Spark SQL查询数据：
- en: '[PRE51]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We read in the JSON file we saved with Odo:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取了用Odo保存的JSON文件：
- en: '[PRE52]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We print the schema of the Spark dataframe:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打印Spark dataframe的模式：
- en: '[PRE53]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We select the `user_name` column from the dataframe:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从数据框中选择`user_name`列：
- en: '[PRE54]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We register the dataframe as a table, so we can execute a SQL query on it:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据框注册为表，这样我们就可以对其执行SQL查询：
- en: '[PRE55]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We execute a SQL statement against the dataframe:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对数据框执行了一条SQL语句：
- en: '[PRE56]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s process some more complex JSON; we read the original Twitter JSON file:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们处理一些更复杂的JSON；我们读取原始的Twitter JSON文件：
- en: '[PRE57]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Spark SQL is able to infer the schema of a complex nested JSON file:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL能够推断复杂嵌套的JSON文件的模式：
- en: '[PRE58]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We extract the key information of interest from the wall of data by selecting
    specific columns in the dataframe (in this case, `[''created_at'', ''id'', ''text'',
    ''user.id'', ''user.name'', ''entities.urls.expanded_url'']`):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过选择数据框中特定列（在本例中为`['created_at', 'id', 'text', 'user.id', 'user.name', 'entities.urls.expanded_url']`）提取感兴趣的关键信息：
- en: '[PRE59]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Understanding the Spark SQL query optimizer
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Spark SQL查询优化器
- en: 'We execute a SQL statement against the dataframe:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对数据框执行了一条SQL语句：
- en: '[PRE60]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We get a detailed view of the query plans executed by Spark SQL:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以详细查看Spark SQL执行的查询计划：
- en: Parsed logical plan
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析逻辑计划
- en: Analyzed logical plan
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析逻辑计划
- en: Optimized logical plan
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化逻辑计划
- en: Physical plan
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理计划
- en: The query plan uses Spark SQL's Catalyst optimizer. In order to generate the
    compiled bytecode from the query parts, the Catalyst optimizer runs through logical
    plan parsing and optimization followed by physical plan evaluation and optimization
    based on cost.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 查询计划使用了Spark SQL的Catalyst优化器。为了从查询部分生成编译后的字节码，Catalyst优化器通过逻辑计划解析和优化，然后根据成本进行物理计划评估和优化。
- en: 'This is illustrated in the following tweet:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下推文中有所体现：
- en: '![Understanding the Spark SQL query optimizer](img/B03986_03_03.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![理解Spark SQL查询优化器](img/B03986_03_03.jpg)'
- en: 'Looking back at our code, we call the `.explain` function on the Spark SQL
    query we just executed, and it delivers the full details of the steps taken by
    the Catalyst optimizer in order to assess and optimize the logical plan and the
    physical plan and get to the result RDD:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们的代码，我们在刚刚执行的Spark SQL查询上调用了`.explain`函数，它提供了Catalyst优化器评估逻辑计划和物理计划并得出结果RDD所采取的步骤的全部细节：
- en: '[PRE61]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Finally, here''s the result of the query:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是查询的结果：
- en: '[PRE62]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Loading and processing CSV files with Spark SQL
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark SQL加载和处理CSV文件
- en: 'We will use the Spark package `spark-csv_2.11:1.2.0`. The command to be used
    to launch PySpark with the IPython Notebook and the `spark-csv` package should
    explicitly state the `–packages` argument:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Spark包`spark-csv_2.11:1.2.0`。启动PySpark与IPython Notebook和`spark-csv`包应明确说明`–packages`参数的命令：
- en: '[PRE63]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'This will trigger the following output; we can see that the `spark-csv` package
    is installed with all its dependencies:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这将触发以下输出；我们可以看到`spark-csv`包已安装及其所有依赖项：
- en: '[PRE64]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We are now ready to load our `csv` file and process it. Let''s first import
    the `SQLContext`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备加载我们的`csv`文件并处理它。让我们首先导入`SQLContext`：
- en: '[PRE66]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We access the schema of the dataframe created from the loaded `csv`:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们访问从加载的`csv`创建的数据框的模式：
- en: '[PRE67]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We check the columns of the dataframe:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查数据框的列：
- en: '[PRE68]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We introspect the dataframe content:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们审查数据框的内容：
- en: '[PRE69]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Querying MongoDB from Spark SQL
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Spark SQL查询MongoDB
- en: 'There are two major ways to interact with MongoDB from Spark: the first is
    through the Hadoop MongoDB connector, and the second one is directly from Spark
    to MongoDB.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark到MongoDB有两种主要的交互方式：第一种是通过Hadoop MongoDB连接器，第二种是直接从Spark到MongoDB。
- en: 'The first approach to interact with MongoDB from Spark is to set up a Hadoop
    environment and query through the Hadoop MongoDB connector. The connector details
    are hosted on GitHub at [https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage](https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage).
    An actual use case is described in the series of blog posts from MongoDB:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark与MongoDB交互的第一种方法是设置一个Hadoop环境，并通过Hadoop MongoDB连接器进行查询。连接器的详细信息托管在GitHub上：[https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage](https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage)。MongoDB的一系列博客文章中描述了一个实际用例：
- en: '*Using MongoDB with Hadoop & Spark: Part 1 - Introduction & Setup* ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup))'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用MongoDB与Hadoop和Spark：第1部分-介绍和设置* ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup))'
- en: '*Using MongoDB with Hadoop and Spark: Part 2 - Hive Example* ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example))'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MongoDB与Hadoop和Spark：第2部分 - Hive示例 ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example))
- en: '*Using MongoDB with Hadoop & Spark: Part 3 - Spark Example & Key Takeaways*
    ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways))'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MongoDB与Hadoop和Spark：第3部分 - Spark示例和关键要点
- en: 'Setting up a full Hadoop environment is bit elaborate. We will favor the second
    approach. We will use the `spark-mongodb` connector developed and maintained by
    Stratio. We are using the `Stratio spark-mongodb` package hosted at `spark.packages.org`.
    The packages information and version can be found in `spark.packages.org`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完整的Hadoop环境有点复杂。我们将倾向于第二种方法。我们将使用由Stratio开发和维护的`spark-mongodb`连接器。我们使用托管在`spark.packages.org`上的`Stratio
    spark-mongodb`包。包的信息和版本可以在`spark.packages.org`中找到：
- en: Note
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Releases**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**发布**'
- en: 'Version: 0.10.1 ( 8263c8 | zip | jar ) / Date: 2015-11-18 / License: Apache-2.0
    / Scala version: 2.10'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 版本：0.10.1（8263c8 | zip | jar）/日期：2015-11-18 /许可证：Apache-2.0 / Scala版本：2.10
- en: ([http://spark-packages.org/package/Stratio/spark-mongodb](http://spark-packages.org/package/Stratio/spark-mongodb))
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: （[http://spark-packages.org/package/Stratio/spark-mongodb](http://spark-packages.org/package/Stratio/spark-mongodb)）
- en: 'The command to launch PySpark with the IPython Notebook and the `spark-mongodb`
    package should explicitly state the packages argument:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 启动PySpark与IPython笔记本和`spark-mongodb`包的命令应明确说明packages参数：
- en: '[PRE70]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This will trigger the following output; we can see that the `spark-mongodb`
    package is installed with all its dependencies:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这将触发以下输出；我们可以看到`spark-mongodb`包与其所有依赖项一起安装：
- en: '[PRE71]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: We are now ready to query MongoDB on `localhost:27017` from the collection `twtr01_coll`
    in the database `twtr01_db`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备从数据库`twtr01_db`的集合`twtr01_coll`上的`localhost:27017`查询MongoDB。
- en: 'We first import the `SQLContext`:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入`SQLContext`：
- en: '[PRE72]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Here''s the output of our query:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们查询的输出：
- en: '[PRE73]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we harvested data from Twitter. Once the data was acquired,
    we explored the information using `Continuum.io's` Blaze and Odo libraries. Spark
    SQL is an important module for interactive data exploration, analysis, and transformation,
    leveraging the Spark dataframe datastructure. The dataframe concept originates
    from R and then was adopted by Python Pandas with great success. The dataframe
    is the workhorse of the data scientist. The combination of Spark SQL and dataframe
    creates a powerful engine for data processing.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从Twitter上收集了数据。一旦获取了数据，我们就使用`Continuum.io`的Blaze和Odo库来探索信息。Spark SQL是交互式数据探索、分析和转换的重要模块，利用了Spark
    dataframe数据结构。 dataframe的概念起源于R，然后被Python Pandas成功采用。 dataframe是数据科学家的得力助手。 Spark
    SQL和dataframe的结合为数据处理创建了强大的引擎。
- en: We are now gearing up for extracting the insights from the datasets using machine
    learning from Spark MLlib.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备利用Spark MLlib从数据集中提取洞察。
