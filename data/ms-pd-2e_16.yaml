- en: Data Case Studies Using pandas
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pandas的案例研究
- en: So far, we have covered the extensive functionalities of pandas. We'll try to
    implement these functionalities in some case studies. These case studies will
    give us an overview of the use of each functionality and help us determine the
    pivotal points in handling a DataFrame. Moreover, the step-by-step approach of
    the case studies helps us to deepen our understanding of the pandas functions.
    This chapter is equipped with practical examples along with code snippets to ensure
    that, by the end, you understand the pandas approach to solving the DataFrame
    problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了pandas的广泛功能。接下来，我们将尝试在一些案例研究中实现这些功能。这些案例研究将使我们全面了解每个功能的使用，并帮助我们确定处理DataFrame时的关键点。此外，案例研究的逐步方法有助于加深我们对pandas函数的理解。本章提供了实际示例和代码片段，确保在最后，你能够理解pandas解决DataFrame问题的方法。
- en: 'We will cover the following case studies:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下案例研究：
- en: End-to-end exploratory data analysis
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头到尾的探索性数据分析
- en: Web scraping with Python
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python进行网页抓取
- en: Data validation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据验证
- en: End-to-end exploratory data analysis
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头到尾的探索性数据分析
- en: Exploratory data analysis refers to the critical process of understanding the
    quirks of data—the outliers, the columns containing the most relevant information,
    and determining the relationship between the variables using statistics and graphical
    representations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析是指理解数据特征的关键过程——如异常值、包含最相关信息的列，并通过统计和图形表示确定变量之间的关系。
- en: 'Let''s consider the following DataFrame to perform exploratory data analysis:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下DataFrame，进行探索性数据分析：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following screenshot shows the DataFrame loaded in Jupyter Notebook:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在Jupyter Notebook中加载的DataFrame：
- en: '![](img/1432bfa2-441d-4f22-90e2-78bd09b48ece.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1432bfa2-441d-4f22-90e2-78bd09b48ece.png)'
- en: DataFrame loaded in Jupyter Notebook
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中加载的DataFrame
- en: Data overview
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据概述
- en: 'The preceding DataFrame is the customer data of an automobile servicing firm.
    They basically provide services to their clients on a periodic basis. Each row
    in the DataFrame corresponds to a unique customer. Hence, it is customer-level
    data. Here is an observation from the data:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的DataFrame是一个汽车维修公司的客户数据。他们基本上按周期为客户提供服务。DataFrame中的每一行对应一个独特的客户。因此，这是客户级别的数据。以下是从数据中获得的一个观察结果：
- en: '![](img/547abf7c-ab9b-4b1c-9a65-48b8df8ff4c9.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/547abf7c-ab9b-4b1c-9a65-48b8df8ff4c9.png)'
- en: The shape of the DataFrame
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame的形状
- en: We can observe that the data contains 27,002 records and 26 characteristics.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到数据包含27,002条记录和26个特征。
- en: 'Before we start exploratory data analysis on any data, it is advised to know
    as much about the data as possible—the column names and their corresponding data
    types, whether they contain null values or not (and if so, how many), and so on.
    The following screenshot shows some of the basic information about the DataFrame
    obtained using the `info` function in pandas:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始对任何数据进行探索性数据分析之前，建议尽可能多地了解数据——包括列名及其相应的数据类型，是否包含空值（如果有，多少空值），等等。以下截图展示了通过pandas的`info`函数获得的一些基本信息：
- en: '![](img/d9f4f4d1-c447-449f-9f7a-c6d9865e95fe.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9f4f4d1-c447-449f-9f7a-c6d9865e95fe.png)'
- en: Basic information about the DataFrame
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame的基本信息
- en: Using the `info()` function, we can see that the data only has float and integer
    values. Also, none of the columns has null/missing values.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`info()`函数，我们可以看到数据仅包含浮动和整数值。此外，没有任何列包含空值。
- en: 'The `describe()` function in pandas is used to obtain various summary statistics
    of all the numeric columns. This function returns the count, mean, standard deviation,
    minimum and maximum values, and the quantiles of all the numeric columns. The
    following table shows the description of the data obtained using the `describe`
    function:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: pandas中的`describe()`函数用于获取所有数值列的各种汇总统计信息。该函数返回所有数值列的计数、均值、标准差、最小值、最大值和四分位数。以下表格展示了通过`describe`函数获取的数据描述：
- en: '![](img/a9f15dcf-e64c-4c09-a5c2-535d7daa7c6c.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9f15dcf-e64c-4c09-a5c2-535d7daa7c6c.png)'
- en: Describing the Data
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 描述数据
- en: Feature selection
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: 'If you a have dataset with many variables, a good way to check correlations
    among columns is by visualizing the correlation matrix as a heatmap. We can identify
    and remove those that are highly correlated, thereby simplifying our analysis.
    The visualization can be achieved using the `seaborn` library in Python:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个包含多个变量的数据集，检查各列之间相关性的一个好方法是通过将相关性矩阵可视化为热图。我们可以识别并去除那些高度相关的变量，从而简化我们的分析。可视化可以通过Python中的`seaborn`库实现：
- en: '![](img/9c159842-b5f9-4c9e-aa06-4184d9218eed.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c159842-b5f9-4c9e-aa06-4184d9218eed.png)'
- en: 'The following will be the output:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下将是输出结果：
- en: '![](img/fa3c9dde-f91f-42dd-b039-69e9b4a08718.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa3c9dde-f91f-42dd-b039-69e9b4a08718.png)'
- en: Correlation heatmap of the DataFrame
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框的相关性热图
- en: 'We can observe the following in the preceding heatmap:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在之前的热图中观察到以下几点：
- en: '`soldBy` and `days_old` are highly negatively correlated'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soldBy`和`days_old`之间存在高度负相关'
- en: '`age_median` and `income_median` are positively correlated'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`age_median`和`income_median`之间存在正相关'
- en: Similarly, we can derive the correlation between different sets of variables.
    Hence, based on the correlation results, we can minimize the number of independent
    features by selecting only the important features.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以推导出不同变量集之间的相关性。因此，基于相关性结果，我们可以通过仅选择重要特征来最小化独立特征的数量。
- en: Feature extraction
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取
- en: 'Apart from selecting the useful features, we also need to extract significant
    variables from the existing ones. This method is called **feature extraction**.
    In the current example, a new feature called `new_tenure` has been extracted from
    the existing variables. This variable gives us the amount of time that a customer
    has stayed with the firm:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择有用的特征外，我们还需要从现有变量中提取显著的变量。这种方法被称为**特征提取**。在当前示例中，已经从现有变量中提取了一个名为`new_tenure`的新特征。该变量告诉我们客户在公司待了多长时间：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following DataFrame shows the newly extracted variables:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下数据框展示了新提取的变量：
- en: '![](img/b70a40ca-73ab-42af-9ac9-8c6d120f2d31.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b70a40ca-73ab-42af-9ac9-8c6d120f2d31.png)'
- en: The DataFrame with the newly extracted variables
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 含有新提取变量的数据框
- en: Data aggregation
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据汇总
- en: 'As mentioned earlier, the data presented is customer-level data. It would be
    more feasible and easy to perform analysis on aggregated data, which in this case
    is a region. To start with, we need to understand how the customers are spread
    across each region. Hence, we are going to use the `groupby` function to find
    the number of customers in each zip code. The snippet and its output are shown
    in the following code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，所呈现的数据是客户级别的数据。对汇总数据进行分析会更加可行且容易，在这种情况下，汇总数据是按区域划分的。首先，我们需要了解客户在每个区域的分布情况。因此，我们将使用`groupby`函数来查找每个邮政编码中的客户数量。以下代码展示了代码片段及其输出：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is the output:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/21e6a21a-c125-4c22-9b88-0f16a5a4ea47.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21e6a21a-c125-4c22-9b88-0f16a5a4ea47.png)'
- en: Aggregating the data based on zip codes
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基于邮政编码的汇总数据
- en: This gives the first 10 zip codes that have the maximum number of customers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出前10个拥有最多客户的邮政编码。
- en: 'Therefore, we can convert our client-level data into zip-level data using aggregation.
    After grouping the values, we also have to make sure that we remove the NAs. The
    following code can be used to perform aggregation on the entire DataFrame:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过聚合将客户级数据转换为邮政编码级数据。在对值进行分组后，我们还必须确保去除NA。可以使用以下代码对整个数据框进行聚合：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following screenshot is the aggregated DataFrame after removing the NAs:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是去除NA后的汇总数据框：
- en: '![](img/dddd3a12-a571-4769-b85e-e945a13804d2.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dddd3a12-a571-4769-b85e-e945a13804d2.png)'
- en: Aggregated DataFrame after removing the NAs
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 去除NA后的汇总数据框
- en: '`data_clean` will become the cleaned version of our sample DataFrame, which
    will be passed to a model for further analysis.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`data_clean`将成为我们样本数据框的清理版本，该版本将传递给模型进行进一步分析。'
- en: Web scraping with Python
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python进行网页抓取
- en: Web scraping deals with extracting large amounts of data from websites in either
    structured or unstructured forms. For example, a website might have some data
    already present in an HTML table element or as a CSV file. This is an example
    of structured data on website. But, in most cases, the required information would
    be scattered across the content of the web page. Web scraping helps collect these
    data and store it in a structured form. There are different ways to scrape websites
    such as online services, APIs, or writing your own code.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 网页抓取涉及从网站提取大量数据，形式可以是结构化的或非结构化的。例如，网站可能已经有一些数据以HTML表格元素或CSV文件的形式存在。这是网站上结构化数据的一个例子。但是，在大多数情况下，所需的信息会分散在网页的内容中。网页抓取有助于收集这些数据并将其存储为结构化的形式。有多种方式可以抓取网站，如在线服务、API，或者编写自己的代码。
- en: 'Here are some important notes about web scraping:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于网页抓取的一些重要说明：
- en: Read through the website's terms and conditions to understand how you can legally
    use the data. Most sites prohibit you from using the data for commercial purposes.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读网站的条款和条件，了解如何合法使用数据。大多数网站禁止将数据用于商业目的。
- en: Make sure you are not downloading data at a rapid rate because this may break
    the website. You may potentially be blocked from the site as well.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保不要过快下载数据，因为这样可能会导致网站崩溃。你也有可能被网站封锁。
- en: Web scraping using pandas
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pandas进行网页抓取
- en: 'Python provides different libraries for scraping:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Python提供了不同的库来进行抓取：
- en: pandas
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: BeautifulSoup
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BeautifulSoup
- en: Scrapy
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scrapy
- en: In this section, we'll see how to scrape data by leveraging the power of pandas
    and BeautifulSoup. To start with, pandas is sufficient to extract structured data
    from a website without the help of BeautifulSoup. In the earlier sections, we
    learned about loading data from different formats (`.csv`, .`xlsx`, and `.xls`)
    in Python. Similar to these, pandas has a separate function for loading tabular
    data from an HTML file. To read an HTML file, a pandas DataFrame looks for a tag.
    That tag is called a `<td> </td>` tag. This tag is used to define a table in HTML.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何利用pandas和BeautifulSoup的强大功能进行数据抓取。首先，pandas足以从网站上提取结构化数据，而不需要BeautifulSoup的帮助。在前面的章节中，我们学习了如何从不同格式（`.csv`、`.xlsx`和`.xls`）加载数据到Python中。类似于这些，pandas有一个专门用于从HTML文件加载表格数据的函数。要读取HTML文件，pandas的DataFrame会查找一个标签。这个标签称为`<td>
    </td>`标签，用于定义HTML中的表格。
- en: pandas uses `read_html()` to read the HTML document. This function loads all
    the structured data from the URL into the Python environment. So, whenever you
    pass an HTML to pandas and expect it to output a nice-looking DataFrame, make
    sure the HTML page has a table in it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: pandas使用`read_html()`来读取HTML文档。这个函数将URL中的所有结构化数据加载到Python环境中。因此，每当你传递一个HTML文件给pandas并希望它输出一个漂亮的DataFrame时，确保HTML页面中有一个表格。
- en: 'We can try this function on a sample URL ([https://www.bseindia.com/static/members/TFEquity.aspx](https://www.bseindia.com/static/members/TFEquity.aspx)):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试在一个示例网址上使用这个功能（[https://www.bseindia.com/static/members/TFEquity.aspx](https://www.bseindia.com/static/members/TFEquity.aspx)）：
- en: '![](img/48da0222-119d-4f5d-8fc8-b24649577cab.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48da0222-119d-4f5d-8fc8-b24649577cab.png)'
- en: Sample web page
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 示例网页
- en: 'The preceding web page contains multiple tables. Using pandas, we can extract
    all the tables, which will be stored inside a list:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述网页包含多个表格。使用pandas，我们可以提取所有表格，并将其存储在一个列表中：
- en: '![](img/cd6d4277-e11c-432b-a1dc-91e58c062567.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd6d4277-e11c-432b-a1dc-91e58c062567.png)'
- en: List containing multiple DataFrames
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 包含多个DataFrame的列表
- en: 'In the following screenshot, the second table from the web page is being extracted:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，正在提取网页中的第二个表格：
- en: '![](img/3e61565b-b000-45ad-a4e3-7e34cc00fe32.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e61565b-b000-45ad-a4e3-7e34cc00fe32.png)'
- en: Comparison of the web page and the pandas DataFrame
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 网页和pandas DataFrame的对比
- en: 'After cleaning, the extracted DataFrame is an exact replica of what is available
    on the website:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 清洗后，提取的DataFrame完全复制了网站上的内容：
- en: '![](img/54c4560d-d7c0-42f2-b174-f2a9d83c5e35.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54c4560d-d7c0-42f2-b174-f2a9d83c5e35.png)'
- en: DataFrame after cleaning
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 清洗后的DataFrame
- en: With proper indexing, all the tables from a web page can be extracted using
    the `read_html` function.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过适当的索引，所有来自网页的表格都可以通过`read_html`函数提取。
- en: Web scraping using BeautifulSoup
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BeautifulSoup进行网页抓取
- en: BeautifulSoup is a Python library ([https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/)) for
    pulling data out of HTML and XML files. It provides ways of navigating, accessing,
    searching, and modifying the HTML content of a web page. It is important to understand
    the basics of HTML in order to successfully scrape a web page. To parse the content,
    the first thing that we need to do is to figure out where we can locate the links
    to the files we want to download inside the multiple levels of HTML tags. Simply
    put, there is a lot of code on a web page, and we want to find the relevant pieces
    of code that contains our data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: BeautifulSoup 是一个 Python 库（[https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/)），用于从
    HTML 和 XML 文件中提取数据。它提供了导航、访问、搜索和修改网页 HTML 内容的方式。了解 HTML 的基础知识对于成功抓取网页内容非常重要。为了解析内容，首先我们需要做的是确定在哪里可以找到我们想要下载的文件的链接，这些文件位于
    HTML 标签的多层级中。简而言之，网页上有大量代码，而我们要做的就是找到包含数据的相关代码段。
- en: 'On the website, right-click and click on Inspect. This allows you to see the
    raw code behind the site. Once you''ve clicked on Inspect, you should see the
    following console pop up:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在网站上，右键点击并选择检查。这将允许你查看网站背后的原始代码。点击检查后，你应该能看到以下控制台弹出：
- en: '![](img/73f85c88-ed30-433d-80e5-8a12cd1bb2fb.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73f85c88-ed30-433d-80e5-8a12cd1bb2fb.png)'
- en: Inspect menu of a browser
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览器的检查菜单
- en: Notice that the table that we are referring to is wrapped in a tag called **table**.
    Each row will be present between `<tr>` tags. Similarly, each cell will be present
    between `<td>` tags. Understanding these basic differences makes it easier to
    extract the data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们所提到的表格被包裹在一个叫做 **table** 的标签中。每一行都位于 `<tr>` 标签之间。同样，每个单元格都位于 `<td>` 标签之间。理解这些基本差异可以让数据提取变得更容易。
- en: 'We start by importing the following libraries:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入以下库：
- en: '![](img/68b0881a-d34a-45e9-a21d-97f13641eb39.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68b0881a-d34a-45e9-a21d-97f13641eb39.png)'
- en: Importing libraries
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库
- en: 'Next, we request the URL with the `requests` library. If the access was successful,
    you should see the following output:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `requests` 库请求 URL。如果访问成功，你应该能看到以下输出：
- en: '![](img/e9fc1dc8-0f62-4f70-82c6-7a12fc6be37b.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9fc1dc8-0f62-4f70-82c6-7a12fc6be37b.png)'
- en: Successful response from a website
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从网站获得成功的响应
- en: 'We then parse `html` with `BeautifulSoup` so that we can work with a nicer,
    nested `BeautifulSoup` data structure. With a little knowledge of HTML tags, the
    parsed content can be easily converted into a DataFrame using a `for` loop and
    a pandas DataFrame. The biggest advantage of using BeautifulSoup is that it can
    even extract data from unstructured sources that can be molded into a table by
    the supported libraries, whereas the `read_html` function of pandas will only
    work with structured sources. Hence, based on the requirement, we have used `BeautifulSoup`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`BeautifulSoup`解析`html`，以便能够处理更整洁、嵌套的`BeautifulSoup`数据结构。通过一些HTML标签的知识，解析后的内容可以使用`for`循环和
    pandas DataFrame 轻松转换为 DataFrame。使用 BeautifulSoup 的最大优势在于，它甚至可以从非结构化的来源中提取数据，并通过支持的库将其转化为表格，而
    pandas 的`read_html`函数只能处理结构化的数据来源。因此，根据需求，我们使用了`BeautifulSoup`：
- en: '![](img/35619365-12c4-4da0-9066-c80ceb661ea1.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35619365-12c4-4da0-9066-c80ceb661ea1.png)'
- en: Extracted DataFrame using BeautifulSoup
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 BeautifulSoup 提取的 DataFrame
- en: Data validation
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据验证
- en: Data validation is the process of examining the quality of data to ensure it
    is both correct and useful for performing analysis. It uses routines, often called
    **validation rules**, that check for the genuineness of the data that is input
    to the models. In the age of big data, where vast caches of information are generated
    by computers and other forms of technology that contribute to the quantity of
    data being produced, it would be incompetent to use such data if it lacks quality,
    highlighting the importance of data validation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 数据验证是检查数据质量的过程，确保数据既正确又适用于分析。它使用称为**验证规则**的例程来检查输入模型的数据的真实性。在大数据时代，计算机和其他技术形式生成大量信息，这些信息推动着数据产生的数量，如果这些数据缺乏质量，那么使用它们会显得不专业，这也突出了数据验证的重要性。
- en: 'In this case study, we are going to consider two DataFrames:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将考虑两个 DataFrame：
- en: Test DataFrame (from a flat file)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自平面文件的测试 DataFrame
- en: Validation DataFrame (from MongoDB)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 MongoDB 的验证 DataFrame
- en: Validation routines are performed on the test DataFrame, keeping its counterpart
    as the reference.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试 DataFrame 上执行验证例程，同时将其对应的数据框作为参考。
- en: Data overview
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据概览
- en: 'The datasets considered here are part of the **Learning Management System**
    (**LMS**) data. They project information pertaining to student enrolment, tracking,
    reporting, and delivery of educational courses. Let''s load the test DataFrame from
    the flat file:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里考虑的数据集是 **学习管理系统**（**LMS**）数据的一部分。它们展示了与学生注册、跟踪、报告以及教育课程的交付相关的信息。我们将从平面文件加载测试
    DataFrame：
- en: '![](img/b1ea73e9-25bd-41f9-a6b4-7cbee992b41f.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1ea73e9-25bd-41f9-a6b4-7cbee992b41f.png)'
- en: Loading test DataFrame from flat file
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从平面文件加载测试 DataFrame
- en: 'The `pymongo` library is used to connect MongoDB to Python. Generally, MongoDB
    listens on port `27017`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`pymongo` 库用于将 MongoDB 连接到 Python。通常，MongoDB 会监听端口 `27017`：'
- en: '![](img/ac1e34df-1652-4b6d-804e-80f1f5cadc37.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac1e34df-1652-4b6d-804e-80f1f5cadc37.png)'
- en: MongoDB connection from Python
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Python 连接 MongoDB
- en: 'We can see the connection parameters in the following screenshot. Since the
    database is installed locally, we are connecting to it via localhost. The name
    of the loaded database is `lms_db`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下截图中看到连接参数。由于数据库安装在本地，我们通过 localhost 进行连接。加载的数据库名称是 `lms_db`：
- en: '![](img/683b1c50-4e42-4bf8-a2c1-7dfa8b8ac53b.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/683b1c50-4e42-4bf8-a2c1-7dfa8b8ac53b.png)'
- en: Reading data from MongoDB
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从 MongoDB 读取数据
- en: Structured databases versus unstructured databases
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化数据库与非结构化数据库
- en: 'Since MongoDB falls under the category of unstructured databases, the terminology
    used widely differs from its structured counterparts, such as MySQL and PostgreSQL.
    The following table presents various SQL terminology and concepts and the corresponding
    MongoDB terminology and concepts:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 MongoDB 属于非结构化数据库类别，因此其使用的术语与结构化数据库（如 MySQL 和 PostgreSQL）大不相同。下表展示了各种 SQL
    术语和概念以及相应的 MongoDB 术语和概念：
- en: '| **SQL terms/concepts** | **MongoDB terms/concepts** |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| **SQL 术语/概念** | **MongoDB 术语/概念** |'
- en: '| database | Database ([h](https://docs.mongodb.com/manual/reference/glossary/#term-database)[ttps://docs.mongodb.com/manual/reference/glossary/#term-database](https://docs.mongodb.com/manual/reference/glossary/#term-database)[)](https://docs.mongodb.com/manual/reference/glossary/#term-database)
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 数据库 | 数据库 ([h](https://docs.mongodb.com/manual/reference/glossary/#term-database)[ttps://docs.mongodb.com/manual/reference/glossary/#term-database](https://docs.mongodb.com/manual/reference/glossary/#term-database)[)](https://docs.mongodb.com/manual/reference/glossary/#term-database)
    |'
- en: '| table | Collection ([https://docs.mongodb.com/manual/reference/glossary/#term-collection](https://docs.mongodb.com/manual/reference/glossary/#term-collection))
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 表 | 集合 ([https://docs.mongodb.com/manual/reference/glossary/#term-collection](https://docs.mongodb.com/manual/reference/glossary/#term-collection))
    |'
- en: '| row | Document or BSON document |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 行 | 文档或 BSON 文档 |'
- en: '| column | Field ([https://docs.mongodb.com/manual/reference/glossary/#term-field](https://docs.mongodb.com/manual/reference/glossary/#term-field))
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 列 | 字段 ([https://docs.mongodb.com/manual/reference/glossary/#term-field](https://docs.mongodb.com/manual/reference/glossary/#term-field))
    |'
- en: '| index | Index ([https://docs.mongodb.com/manual/reference/glossary/#term-index](https://docs.mongodb.com/manual/reference/glossary/#term-index))
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 索引 | 索引 ([https://docs.mongodb.com/manual/reference/glossary/#term-index](https://docs.mongodb.com/manual/reference/glossary/#term-index))
    |'
- en: '| table joins | `$lookup`, embedded documents ([https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/#pipe._S_lookup](https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/#pipe._S_lookup))
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 表连接 | `$lookup`，嵌入式文档 ([https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/#pipe._S_lookup](https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/#pipe._S_lookup))
    |'
- en: '| primary key | Primary key ([https://docs.mongodb.com/manual/reference/glossary/#term-primary-key](https://docs.mongodb.com/manual/reference/glossary/#term-primary-key))
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 主键 | 主键 ([https://docs.mongodb.com/manual/reference/glossary/#term-primary-key](https://docs.mongodb.com/manual/reference/glossary/#term-primary-key))
    |'
- en: '| Specify any unique column or column combination as primary key. | In MongoDB,
    the primary key is automatically set to the `_id` field. ([https://docs.mongodb.com/manual/reference/glossary/#term-id](https://docs.mongodb.com/manual/reference/glossary/#term-id))
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 将任何唯一的列或列组合指定为主键。 | 在 MongoDB 中，主键会自动设置为`_id`字段。([https://docs.mongodb.com/manual/reference/glossary/#term-id](https://docs.mongodb.com/manual/reference/glossary/#term-id))
    |'
- en: '| aggregation (example group by) | Aggregation pipeline |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 聚合（例如 group by） | 聚合管道 |'
- en: '| transactions | Transactions ([https://docs.mongodb.com/manual/core/transactions/](https://docs.mongodb.com/manual/core/transactions/))
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 事务 | 事务 ([https://docs.mongodb.com/manual/core/transactions/](https://docs.mongodb.com/manual/core/transactions/))
    |'
- en: Comparative view of SQL MongoDB terminologies
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 与 MongoDB 术语对比视图
- en: Validating data types
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证数据类型
- en: A data type is a property of a variable that Python uses to understand how to
    store and manipulate data. For instance, a program needs to understand that variables
    storing 5 and 10 are numeric to be able to add them and get 15, or that the variables
    storing `cat` and `hat` are strings so that they could be concatenated (added)
    together to get `cathat`. Hence it becomes a preliminary and cardinal property
    of any pandas DataFrame.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型是变量的一种属性，Python 使用它来理解如何存储和处理数据。例如，程序需要理解存储 5 和 10 的变量是数字型的，以便能够将它们相加得到
    15；或者理解存储 `cat` 和 `hat` 的变量是字符串类型，以便它们可以连接（加在一起）得到 `cathat`。因此，它成为任何 pandas DataFrame
    的初步和基本属性。
- en: 'A user-defined comparison function can be used to validate the data types of
    the test DataFrame:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用用户定义的比较函数来验证测试 DataFrame 的数据类型：
- en: '![](img/e612fdcc-3078-43d5-affc-f33a871cc54c.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e612fdcc-3078-43d5-affc-f33a871cc54c.png)'
- en: Validating data types of test DataFrame
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 验证测试 DataFrame 的数据类型
- en: '`File1` and `File2` correspond to the test and validation datasets respectively.
    It is evident from the output that all the data types of the test DataFrame match
    those of the validation DataFrame. If there is a mismatch, the output will display
    the number of columns that are inconsistent.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`File1` 和 `File2` 分别对应测试数据集和验证数据集。从输出中可以明显看出，测试 DataFrame 的所有数据类型与验证 DataFrame
    的数据类型匹配。如果存在不匹配，输出将显示不一致的列数。'
- en: Validating dimensions
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证维度
- en: 'A DataFrame is a two-dimensional data structure where data is presented in
    a tabular manner, much like a relational database table, in rows and columns.
    One of the basic methods to check whether the test and validation datasets are
    matching is to equate the number of rows and columns. If the shapes of the DataFrames
    do not match, it becomes clearly evident that the test DataFrame is different
    from the validation one. The following is a screenshot that shows how to validate
    dimensions:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 是一种二维数据结构，其中数据以表格形式呈现，类似于关系型数据库表格，按行和列排列。检查测试集和验证集是否匹配的基本方法之一是比较行数和列数。如果
    DataFrame 的形状不匹配，那么测试 DataFrame 与验证 DataFrame 之间的差异就显而易见了。以下是一个截图，展示了如何验证维度：
- en: '![](img/10056043-7eb1-4884-9719-17df84ac20dd.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10056043-7eb1-4884-9719-17df84ac20dd.png)'
- en: Validating dimensions
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 验证维度
- en: Validating individual entries
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证单个条目
- en: Once the first two test cases are satisfied, it becomes highly important to
    scan individual entries to find spurious data. The validation process in the preceding
    figure describes the difference between a value obtained from a data collection
    process and the true value. As the amount of data increases, validating entries
    becomes difficult. This effect can be diminished by efficiently utilizing pandas.
    In the following example, individual entries have been scanned using both looping
    (the brute force method) and pandas indexing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦前两个测试用例满足要求，扫描单个条目以查找虚假数据就变得非常重要。前面图示中的验证过程描述了从数据采集过程中获得的值与真实值之间的差异。随着数据量的增加，验证条目变得越来越困难。通过高效使用
    pandas，可以减轻这种效果。在以下示例中，使用循环（暴力法）和 pandas 索引扫描了单个条目。
- en: Using pandas indexing
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 pandas 索引
- en: 'The following screenshot shows how to validate cells using pandas:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了如何使用 pandas 验证单元格：
- en: '![](img/229fbaa0-4d36-4ead-92c6-a1bb93f15028.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/229fbaa0-4d36-4ead-92c6-a1bb93f15028.png)'
- en: Validating cells using pandas indexing
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 索引验证单元格
- en: Using loops
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用循环
- en: 'The following screenshot shows how to validate cells by using loops:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了如何通过使用循环验证单元格：
- en: '![](img/d3e02c99-beba-4d35-8f5b-f062d92522d0.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3e02c99-beba-4d35-8f5b-f062d92522d0.png)'
- en: Validating cells using loops
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用循环验证单元格
- en: The results were highly encouraging when we used pandas indexing. It took only
    0.53 seconds to validate a DataFrame with 200,000 rows and 15 columns, whereas
    the same validation routine took more than 7 minutes to complete using loops.
    Therefore it is always recommended to leverage the power of pandas and avoid iterative
    programming.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用 pandas 索引时，结果非常令人鼓舞。验证一个包含 200,000 行和 15 列的 DataFrame 只用了 0.53 秒，而使用循环完成相同的验证流程则花费了超过
    7 分钟。因此，始终建议利用 pandas 的强大功能，避免使用迭代编程。
- en: Summary
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: pandas is useful for a lot of ancillary data activities, such as exploratory
    data analysis, validating the sanctity (such as the data type or count) of data
    between two data sources, and structuring and shaping data obtained from another
    source, such as scraping a website or a database. In this chapter, we dealt with
    some case studies on these topics. A data scientist performs these activities
    on a day-to-day basis, and this chapter should give a flavor of what it is like
    to perform them on a real dataset.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 对许多辅助数据活动非常有用，例如探索性数据分析、验证两个数据源之间数据的有效性（如数据类型或计数），以及构建和塑造从其他来源获取的数据，比如抓取网站或数据库。在这一章中，我们处理了这些主题的一些案例研究。数据科学家每天都会进行这些活动，本章应该能让你大致了解在真实数据集上执行这些活动的体验。
- en: In the next chapter, we will discuss the architecture and code structure of
    the pandas library. This will help us develop an exhaustive understanding of the
    functionalities of the library and enable us to do better troubleshooting.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论 pandas 库的架构和代码结构。这将帮助我们全面了解该库的功能，并使我们能够更好地进行故障排除。
