- en: Big Data With Hadoop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Hadoop的大数据
- en: Hadoop has become the de facto standard in the world of big data, especially
    over the past three to four years. Hadoop started as a subproject of Apache Nutch
    in 2006 and introduced two key features related to distributed filesystems and
    distributed computing, also known as MapReduce, that caught on very rapidly among
    the open source community. Today, there are thousands of new products that have
    been developed leveraging the core features of Hadoop, and it has evolved into
    a vast ecosystem consisting of more than 150 related major products. Arguably,
    Hadoop was one of the primary catalysts that started the big data and analytics
    industry.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop已经成为大数据世界的事实标准，特别是在过去三到四年里。 Hadoop于2006年作为Apache Nutch的一个子项目开始，并引入了与分布式文件系统和分布式计算相关的两个关键功能，也被称为MapReduce，这在开源社区中迅速流行起来。
    今天，已经开发出了数千种利用Hadoop核心功能的新产品，并且它已经发展成一个包含150多种相关主要产品的庞大生态系统。可以说，Hadoop是启动大数据和分析行业的主要催化剂之一。
- en: 'In this chapter, we will discuss the background and core concepts of Hadoop,
    the components of the Hadoop platform, and delve deeper into the major products
    in the Hadoop ecosystem. We will learn about the core concepts of distributed
    filesystems and distributed processing and optimizations to improve the performance
    of Hadoop deployments. We''ll conclude with real-world hands-on exercises using
    the **Cloudera Distribution of Hadoop** (**CDH**). The topics we will cover are:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论Hadoop的背景和核心概念，Hadoop平台的组件，并深入了解Hadoop生态系统中的主要产品。 我们将了解分布式文件系统和分布式处理的核心概念，以及优化以提高Hadoop部署性能。
    我们将以使用**Cloudera Hadoop Distribution**（**CDH**）进行真实世界的实践来结束。 我们将涵盖的主题有：
- en: The basics of Hadoop
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop的基础知识
- en: The core components of Hadoop
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop的核心组件
- en: Hadoop 1 and Hadoop 2
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 1和Hadoop 2
- en: The Hadoop Distributed File System
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop分布式文件系统
- en: Distributed computing principles with MapReduce
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MapReduce的分布式计算原理
- en: The Hadoop ecosystem
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop生态系统
- en: Overview of the Hadoop ecosystem
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop生态系统概述
- en: Hive, HBase, and more
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive，HBase等
- en: Hadoop Enterprise deployments
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop企业部署
- en: In-house deployments
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部部署
- en: Cloud deployments
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云部署
- en: Hands-on with Cloudera Hadoop
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloudera Hadoop的实践
- en: Using HDFS
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用HDFS
- en: Using Hive
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hive
- en: MapReduce with WordCount
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用WordCount的MapReduce
- en: The fundamentals of Hadoop
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop的基础知识
- en: In 2006, Doug Cutting, the creator of Hadoop, was working at Yahoo!. He was
    actively engaged in an open source project called Nutch that involved the development
    of a large-scale web crawler. A web crawler at a high level is essentially software
    that can browse and index web pages, generally in an automatic manner, on the
    internet. Intuitively, this involves efficient management and computation across
    large volumes of data. In late January of 2006, Doug formally announced the start
    of Hadoop. The first line of the request, still available on the internet at [https://issues.apache.org/jira/browse/INFRA-700,](https://issues.apache.org/jira/browse/INFRA-700)
    was *The Lucene PMC has voted to split part of Nutch into a new subproject named
    Hadoop*. And thus, Hadoop was born.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在2006年，Hadoop的创造者Doug Cutting正在Yahoo！工作。 他积极参与了一个名为Nutch的开源项目，该项目涉及开发大规模网络爬虫。
    从高层次上看，网络爬虫本质上是一种可以在互联网上以自动方式浏览和索引网页的软件。 直观地，这涉及对大量数据进行高效的管理和计算。 2006年1月底，Doug正式宣布了Hadoop的开始。
    请求的第一行，仍然可以在[https://issues.apache.org/jira/browse/INFRA-700](https://issues.apache.org/jira/browse/INFRA-700)上找到，是*Lucene
    PMC已投票将Nutch的一部分拆分为一个名为Hadoop的新子项目*。 因此，Hadoop诞生了。
- en: 'At the onset, Hadoop had two core components : **Hadoop Distributed File System**
    (**HDFS**) and MapReduce. This was the first iteration of Hadoop, also now known
    as Hadoop 1\. Later, in 2012, a third component was added known as **YARN** (**Yet
    Another Resource Negotiator**) which decoupled the process of resource management
    and job scheduling. Before we delve into the core components in more detail, it
    would help to get an understanding of the fundamental premises of Hadoop:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时，Hadoop有两个核心组件：**Hadoop分布式文件系统**（**HDFS**）和MapReduce。 这是Hadoop的第一次迭代，现在也被称为Hadoop
    1。 稍后，在2012年，添加了第三个组件，称为**YARN**（**另一个资源协调器**），它解耦了资源管理和作业调度的过程。 在更详细地探讨核心组件之前，了解Hadoop的基本前提将有所帮助：
- en: '![](img/5dbca749-d341-4994-ad32-89f3820278d2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5dbca749-d341-4994-ad32-89f3820278d2.png)'
- en: Doug Cutting's post at [https://issues.apache.org/jira/browse/NUTCH-193](https://issues.apache.org/jira/browse/NUTCH-193)
    announced his intent to separate **Nutch Distributed FS** (**NDFS**) and MapReduce
    to a new subproject called Hadoop.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Doug Cutting在[https://issues.apache.org/jira/browse/NUTCH-193](https://issues.apache.org/jira/browse/NUTCH-193)上发布了他打算将**Nutch分布式FS**（**NDFS**）和MapReduce分离到一个名为Hadoop的新子项目的意图。
- en: The fundamental premise of Hadoop
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop的基本前提
- en: The fundamental premise of Hadoop is that instead of attempting to perform a
    task on a single large machine, the task can be subdivided into smaller segments
    that can then be delegated to multiple smaller machines. These so-called smaller
    machines would then perform the task on their own portion of the data. Once the
    smaller machines have completed their tasks to produce the results on the tasks
    they were allocated, the individual units of results would then be aggregated
    to produce the final result.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的基本前提是，不是尝试在单个大型机器上执行任务，而是将任务细分为较小的段，然后将其委派给多个较小的机器。 这些所谓的较小机器然后会在自己的数据部分上执行任务。
    一旦较小的机器完成了它们的任务，产生了它们被分配的任务的结果，那么这些单独的结果单元将被聚合以产生最终结果。
- en: 'Although, in theory, this may appear relatively simple, there are various technical
    considerations to bear in mind. For example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在理论上，这可能看起来相对简单，但有各种技术考虑要牢记。 例如：
- en: Is the network fast enough to collect the results from each individual server?
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络是否足够快，可以从每个单独的服务器收集结果？
- en: Can each individual server read data fast enough from the disk?
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单独的服务器是否能够从磁盘快速读取数据？
- en: If one or more of the servers fail, do we have to start all over?
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个或多个服务器失败，我们是否必须重新开始？
- en: If there are multiple large tasks, how should they be prioritized?
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有多个大任务，应该如何设置优先级？
- en: There are many more such considerations that must be considered when working
    with a distributed architecture of this nature.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理这种性质的分布式架构时，还有许多其他考虑因素。
- en: The core modules of Hadoop
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop的核心模块
- en: 'The core modules of Hadoop consist of:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的核心模块包括：
- en: '**Hadoop Common**: Libraries and other common helper utilities required by
    Hadoop'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop Common**：Hadoop所需的库和其他常见的辅助工具'
- en: '**HDFS**: A distributed, highly-available, fault-tolerant filesystem that stores
    data'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HDFS**：存储数据的分布式、高可用、容错的文件系统'
- en: '**Hadoop MapReduce**: A programming paradigm involving distributed computing
    across commodity servers (or nodes)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop MapReduce**：涉及跨商品服务器（或节点）的分布式计算的编程范式'
- en: '**Hadoop YARN**: A framework for job scheduling and resource management'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop YARN**：作业调度和资源管理的框架'
- en: Of these core components, YARN was introduced in 2012 to address some of the
    shortcomings of the first release of Hadoop. The first version of Hadoop (or equivalently,
    the first model of Hadoop) used HDFS and MapReduce as its main components. As
    Hadoop gained in popularity, the need to use facilities beyond those provided
    by MapReduce became more and more important. This, along with some other technical
    considerations, led to the development of YARN.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些核心组件中，YARN于2012年推出，以解决Hadoop首次发布的一些缺点。Hadoop的第一个版本（或者等效地说是Hadoop的第一个模型）使用HDFS和MapReduce作为其主要组件。随着Hadoop的流行，使用MapReduce提供的设施之外的设施的需求变得越来越重要。这，再加上一些其他技术考虑因素，导致了YARN的开发。
- en: Let's now look at the salient characteristics of Hadoop as itemized previously.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下之前列举的Hadoop的显著特点。
- en: Hadoop Distributed File System - HDFS
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop分布式文件系统 - HDFS
- en: The HDFS forms the underlying basis of all Hadoop installations. Files, or more
    generally data, is stored in HDFS and accessed by the nodes of Hadoop.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS构成了所有Hadoop安装的基础。文件，或者更一般地说是数据，存储在HDFS中，并由Hadoop的节点访问。
- en: 'HDFS performs two main functions:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS执行两个主要功能：
- en: '**Namespaces**: Provides namespaces that hold cluster metadata, that is, the
    location of data in the Hadoop cluster'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名空间**：提供保存集群元数据的命名空间，即Hadoop集群中数据的位置'
- en: '**Data storage**: Acts as storage for data used in the Hadoop cluster'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据存储**：作为Hadoop集群中使用的数据的存储'
- en: The filesystem is termed as distributed since the data is stored in chunks across
    multiple servers. An intuitive understanding of HDFS can be gained from a simple
    example, as follows. Consider a large book that consists of Chapters A - Z. In
    ordinary filesystems, the entire book would be stored as a single file on the
    disk. In HDFS, the book would be split into smaller chunks, say a chunk for Chapters
    A - H, another for I - P, and a third one for Q - Z. These chunks are then stored
    in separate racks (or bookshelves as with this analogy). Further, the chapters
    are replicated three times, such that there are three copies of each of the chapters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统被称为分布式，因为数据存储在多台服务器上的块中。可以通过一个简单的例子直观地理解HDFS。考虑一本由A-Z章组成的大书。在普通文件系统中，整本书将作为一个单独的文件存储在磁盘上。在HDFS中，这本书将被分割成更小的块，比如A-H章的一个块，I-P章的另一个块，以及Q-Z章的第三个块。这些块然后存储在不同的机架（或者用这个类比来说是书架）上。此外，每一章都会被复制三次，这样每一章都会有三个副本。
- en: 'Suppose, further, the size of the entire book is 1 GB, and each chapter is
    approximately 350 MB:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设整本书的大小是1GB，每一章大约是350MB：
- en: '![](img/46a63885-0e9f-4d87-a5d3-c9edb3be3e6a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46a63885-0e9f-4d87-a5d3-c9edb3be3e6a.png)'
- en: A bookshelf analogy for HDFS
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS的书架类比
- en: 'Storing the book in this manner achieves a few important objectives:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式存储书籍实现了一些重要的目标：
- en: Since the book has been split into three parts by groups of chapters and each
    part has been replicated three times, it means that our process can read the book
    in parallel by querying the parts from different servers. This reduces I/O contention
    and is a very fitting example of the proper use of parallelism.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于这本书已经被分成了三部分，每部分都被章节组复制了三次，这意味着我们的进程可以通过从不同服务器查询部分来并行读取书。这减少了I/O争用，非常适合并行使用的一个很好的例子。
- en: If any of the racks are not available, we can retrieve the chapters from any
    of the other racks as there are multiple copies of each chapter available on different
    racks.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果任何一个机架不可用，我们可以从任何其他机架检索章节，因为每个章节在不同机架上都有多个副本。
- en: If a task I have been given only requires me to access a single chapter, for
    example, Chapter B, I need to access only the file corresponding to Chapters A-H.
    Since the size of the file corresponding to Chapters A-H is a third the size of
    the entire book, the time to access and read the file would be much smaller.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我被分配的任务只需要访问单独的一章，比如说B章，我只需要访问对应A-H章的文件。由于对应A-H章的文件大小是整本书的三分之一，访问和读取文件的时间会更短。
- en: Other benefits, such as selective access rights to different chapter groups
    and so on, would also be possible with such a model.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他好处，比如对不同章节组的选择性访问权限等，也是可能的。
- en: 'This may be an over-simplified analogy of the actual HDFS functionality, but
    it conveys the basic principle of the technology - that large files are subdivided
    into blocks (chunks) and spread across multiple servers in a high-availability
    redundant configuration. We''ll now look at the actual HDFS architecture in a
    bit more detail:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是对实际HDFS功能的过度简化的类比，但它传达了这项技术的基本原则 - 大文件被分割成块（块），并以高可用性冗余配置分布在多台服务器上。现在我们将更详细地看一下实际的HDFS架构：
- en: '![](img/a9eace37-f3d1-4033-9f6c-9b85130a311b.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9eace37-f3d1-4033-9f6c-9b85130a311b.png)'
- en: 'The HDFS backend of Hadoop consists of:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的HDFS后端包括：
- en: '**NameNode**: This can be considered the master node. The NameNode contains
    cluster metadata and is aware of what data is stored in which location - in short,
    it holds the namespace. It stores the entire namespace in RAM and when a request
    arrives, provides information on which servers hold the data required for the
    task. In Hadoop 2, there can be more than one NameNode. A secondary NameNode can
    be created that acts as a helper node to the primary. As such, it is not a backup
    NameNode, but one that helps in keeping cluster metadata up to date.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NameNode**：这可以被认为是主节点。NameNode包含集群元数据，并知道存储在哪个位置的数据 - 简而言之，它持有命名空间。它将整个命名空间存储在RAM中，当请求到达时，提供有关哪些服务器持有所需任务的数据的信息。在Hadoop
    2中，可以有多个NameNode。可以创建一个辅助节点作为辅助节点。因此，它不是备用NameNode，而是帮助保持集群元数据最新的节点。'
- en: '**DataNode**: The DataNodes are the individual servers that are responsible
    for storing chunks of the data and performing compute operations when they receive
    a new request. These are primarily commodity servers that are less powerful in
    terms of resource and capacity than the NameNode that stores the cluster metadata.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataNode**：DataNodes是负责存储数据块并在收到新请求时执行计算操作的单独服务器。这些主要是低性能的商品服务器，资源和容量比存储集群元数据的NameNode要低。'
- en: Data storage process in HDFS
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS中的数据存储过程
- en: 'The following points should give a good idea of the data storage process:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几点应该能很好地说明数据存储过程：
- en: All data in HDFS is written in blocks, usually of size 128 MB. Thus, a single
    file of say size 512 MB would be split into four blocks (4 * 128 MB). These blocks
    are then written to DataNodes. To maintain redundancy and high availability, each
    block is replicated to create duplicate copies. In general, Hadoop installations
    have a replication factor of three, indicating that each block of data is replicated
    three times.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS中的所有数据都是以块的形式写入的，通常大小为128 MB。因此，一个大小为512 MB的单个文件将被分成四个块（4 * 128 MB）。然后将这些块写入DataNodes。为了保持冗余和高可用性，每个块都会被复制以创建副本。一般来说，Hadoop安装的复制因子为3，表示每个数据块都会被复制三次。
- en: This guarantees redundancy such that in the event one of the servers fails or
    stops responding, there would always be a second and even a third copy available.
    To ensure that this process works seamlessly, the DataNode places the replicas
    in independent servers and can also ensure that the blocks are placed on servers
    in different racks in a data center. This is due to the fact that even if all
    the replicas were on independent servers, but all the servers were on the same
    rack, a rack power failure would mean that no replica would be available.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以保证冗余性，以便在其中一个服务器失败或停止响应时，始终有第二个甚至第三个副本可用。为了确保这个过程能够无缝运行，DataNode将副本放在独立的服务器上，并且还可以确保数据中心不同机架上的服务器上放置块。这是因为即使所有副本都在独立的服务器上，但所有服务器都在同一个机架上，机架电源故障将意味着没有副本可用。
- en: 'The general process of writing data into HDFS is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据写入HDFS的一般过程如下：
- en: The NameNode receives a request to write a new file to HDFS.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NameNode收到一个请求，要求将新文件写入HDFS。
- en: Since the data has to be written in blocks or chunks, the HDFS client (the entity
    that made the request) begins caching data into a local buffer and once the buffer
    reaches the allocated chunk size (for example, 128 MB), it informs the NameNode
    that it is ready to write the first block (chunk) of data.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于数据必须以块或分块的形式写入，HDFS客户端（发出请求的实体）开始将数据缓存到本地缓冲区，一旦缓冲区达到分配的块大小（例如128 MB），它会通知NameNode准备好写入第一个数据块（分块）。
- en: The NameNode, based on information available to it about the state of the HDFS
    cluster, responds with information on the destination DataNode where the block
    needs to be stored.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据其对HDFS集群状态的了解，NameNode会提供关于需要存储块的目标DataNode的信息。
- en: The HDFS client writes data to the target DataNode and informs the NameNode
    once the write process for the block has completed.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HDFS客户端将数据写入目标DataNode，并在块的写入过程完成后通知NameNode。
- en: The target DataNode, subsequently, begins copying its copy of the block of data
    to a second DataNode, which will serve as a replica for the current block.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后，目标DataNode开始将其数据块的副本复制到第二个DataNode，后者将作为当前块的副本。
- en: Once the second DataNode completes the write process, it sends the block of
    data to the third DataNode.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个DataNode完成写入过程后，它将数据块发送给第三个DataNode。
- en: This process repeats until all the blocks corresponding to the data (or equivalently,
    the file) are copied across different nodes.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程重复进行，直到所有与数据（或等效地，文件）对应的块都被复制到不同的节点上。
- en: Note that the number of chunks will depend on the file size. The following image
    illustrated the distribution of the data across 5 datanodes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，块的数量将取决于文件大小。以下图示了数据在5个数据节点之间的分布。
- en: '![](img/0750456b-a13d-47df-ba1d-d734014189af.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0750456b-a13d-47df-ba1d-d734014189af.png)'
- en: Master Node and Data Nodes
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点和数据节点
- en: 'The HDFS architecture in the first release of Hadoop, also known as Hadoop
    1, had the following characteristics:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的第一个版本中的HDFS架构，也称为Hadoop 1，具有以下特点：
- en: 'Single NameNode: Only one NameNode was available, and as a result it also acted
    as a single point of failure since it stored all the cluster metadata.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个NameNode：只有一个NameNode可用，因此它也是单点故障，因为它存储了整个集群的元数据。
- en: Multiple DataNodes that stored blocks of data, processed client requests, and
    performed I/O operations (create, read, delete, and so on) on the blocks.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储数据块的多个DataNodes，处理客户端请求，并在数据块上执行I/O操作（创建、读取、删除等）。
- en: The HDFS architecture in the second release of Hadoop, also known as Hadoop
    2, provided all the benefits of the original HDFS design and also added some new
    features, most notably, the ability to have multiple NameNodes that can act as
    primary and secondary NameNodes. Other features included the facility to have
    multiple namespaces as well as HDFS Federation.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop的第二个版本中的HDFS架构，也被称为Hadoop 2，提供了原始HDFS设计的所有优点，并添加了一些新特性，最显著的是具有多个可以充当主要和次要NameNode的NameNode的能力。其他功能包括具有多个命名空间以及HDFS联邦。
- en: 'HDFS Federation deserves special mention. The following excerpt from [http://hadoop.apache.org](http://hadoop.apache.org)
    explains the subject in a very precise manner:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HDFS联邦值得特别一提。来自[http://hadoop.apache.org](http://hadoop.apache.org)的以下摘录以非常精确的方式解释了这个主题：
- en: The NameNodes are federated; the NameNodes are independent and do not require
    coordination with each other. The DataNodes are used as common storage for blocks
    by all the NameNodes. Each DataNode registers with all the NameNodes in the cluster.
    DataNodes send periodic heartbeats and block reports.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: NameNode是联邦的；NameNode是独立的，不需要彼此协调。DataNode被用作所有NameNode的块的共同存储。每个DataNode在集群中注册。DataNode发送周期性的心跳和块报告。
- en: The secondary NameNode is not a backup node in the sense that it cannot perform
    the same tasks as the NameNode in the event that the NameNode is not available.
    However, it makes the NameNode restart process much more efficient by performing
    housekeeping operations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Secondary NameNode并不是备用节点，它不能在NameNode不可用时执行与NameNode相同的任务。然而，它通过执行一些清理操作使得NameNode重新启动过程更加高效。
- en: These operations (such as merging HDFS snapshot data with information on data
    changes) are generally performed by the NameNode when it is restarted and can
    take a long time depending on the amount of changes since the last restart. The
    secondary NameNode can, however, perform these housekeeping operations whilst
    the primary NameNode is still in operation, such that in the event of a restart
    the primary NameNode can recover much faster. Since the secondary NameNode essentially
    performs a checkpoint on the HDFS data at periodic intervals, it is also known
    as the checkpoint node.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作（例如将HDFS快照数据与数据更改信息合并）通常在NameNode重新启动时由NameNode执行，根据自上次重新启动以来的更改量，可能需要很长时间。然而，Secondary
    NameNode可以在主NameNode仍在运行时执行这些清理操作，以便在重新启动时，主NameNode可以更快地恢复。由于Secondary NameNode基本上在定期间隔对HDFS数据执行检查点，因此它也被称为检查点节点。
- en: Hadoop MapReduce
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop MapReduce
- en: MapReduce was one of the seminal features of Hadoop that was arguably the most
    instrumental in bringing it to prominence. MapReduce works on the principle of
    dividing larger tasks into smaller subtasks. Instead of delegating a single machine
    to compute a large task, a network of smaller machines can instead be used to
    complete the smaller subtasks. By distributing the work in this manner, the task
    can be completed much more efficiently relative to using a single-machine architecture.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce是Hadoop的一个重要特性，可以说是使其著名的最重要的特性之一。MapReduce的工作原理是将较大的任务分解为较小的子任务。与将单个机器委派为计算大型任务不同，可以使用一组较小的机器来完成较小的子任务。通过以这种方式分配工作，相对于使用单机架构，任务可以更加高效地完成。
- en: This is not dissimilar to how we go about completing work in our day-to-day
    lives. An example will help to make this clearer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在日常生活中完成工作的方式并没有太大不同。举个例子会更清楚一些。
- en: An intuitive introduction to MapReduce
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce的直观介绍
- en: Let's take the example of a hypothetical organization consisting of a CEO, directors,
    and managers. The CEO wants to know how many new hires have joined the company.
    The CEO sends a request to his or her directors to report back the number of hires
    in their departments. The directors in turn send a request to managers in their
    individual departments to provide the number of new hires. The managers provide
    the number to the directors, who in turn send the final value back to the CEO.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个假设的由CEO、董事和经理组成的组织为例。CEO想知道公司有多少新员工。CEO向他的董事发送请求，要求报告他们部门的新员工数量。董事再向各自部门的经理发送请求，要求提供新员工的数量。经理向董事提供数字，董事再将最终值发送回CEO。
- en: 'This can be considered to be a real-world example of MapReduce. In this analogy,
    the task was finding the number of new hires. Instead of collecting all the data
    on his or her own, the CEO delegated it to the directors and managers who provided
    their own individual departmental numbers as illustrated in the following image:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以被认为是MapReduce的一个现实世界的例子。在这个类比中，任务是找到新员工的数量。CEO并没有自己收集所有数据，而是委派给了董事和经理，他们提供了各自部门的数字，如下图所示：
- en: '![](img/1d094fb0-9403-42f9-b7cc-04ad8f8e0747.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d094fb0-9403-42f9-b7cc-04ad8f8e0747.png)'
- en: The Concept of MapReduce
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce的概念
- en: In this rather simplistic scenario, the process of splitting a large task (find
    new hires in the entire company), into smaller tasks (new hires in each team),
    and then a final re-aggregation of the individual numbers, is analogous to how
    MapReduce works.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个相当简单的场景中，将一个大任务（在整个公司中找到新员工）分解为较小的任务（每个团队中的新员工），然后重新聚合个体数字，类似于MapReduce的工作方式。
- en: A technical understanding of MapReduce
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce的技术理解
- en: MapReduce, as the name implies, has a map phase and a reduce phase. A map phase
    is generally a function that is applied on each element of its input, thus modifying
    its original value.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce，顾名思义，有一个映射阶段和一个减少阶段。映射阶段通常是对其输入的每个元素应用的函数，从而修改其原始值。
- en: MapReduce generates key-value pairs as output.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce生成键值对作为输出。
- en: '**Key-value:** A key-value pair establishes a relationship. For example, if
    John is 20 years old, a simple key-value pair could be (John, 20). In MapReduce,
    the map operation produces such key-value pairs that have an entity and the value
    assigned to the entity.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**键值对：** 键值对建立了一种关系。例如，如果约翰今年20岁，一个简单的键值对可以是（约翰，20）。在MapReduce中，映射操作产生这样的键值对，其中有一个实体和分配给该实体的值。'
- en: In practice, map functions can be complex and involve advanced functionalities.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，映射函数可能会复杂，并涉及高级功能。
- en: 'The reduce phase takes the key-value input from the map function and performs
    a summarization operation. For example, consider the output of a map operation
    that contains the ages of students in different grades in a school:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 减少阶段接收来自映射函数的键值输入，并执行汇总操作。例如，考虑包含学校不同年级学生年龄的映射操作的输出：
- en: '| **Student name** | **Class** | **Age** |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **学生姓名** | **班级** | **年龄** |'
- en: '| John | Grade 1 | 7 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| John | 年级1 | 7 |'
- en: '| Mary | Grade 2 | 8 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Mary | 年级2 | 8 |'
- en: '| Jill | Grade 1 | 6 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Jill | 年级1 | 6 |'
- en: '| Tom | Grade 3 | 10 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Tom | 年级3 | 10 |'
- en: '| Mark | Grade 3 | 9 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Mark | 年级3 | 9 |'
- en: We can create a simple key-value pair, taking for example the value of Class
    and Age (it can be anything, but I'm just taking these to provide the example).
    In this case, our key-value pairs would be (Grade 1, 7), (Grade 2, 8), (Grade
    1, 6), (Grade 3, 10), and (Grade 3, 9).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个简单的键值对，例如取班级和年龄的值（可以是任何值，但我只是拿这些来提供例子）。在这种情况下，我们的键值对将是（年级1，7），（年级2，8），（年级1，6），（年级3，10）和（年级3，9）。
- en: An operation that calculates the average of the ages of students in each grade
    could then be defined as a reduce operation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以将计算每个年级学生年龄平均值的操作定义为减少操作。
- en: More concretely, we can sort the output and then send the tuples corresponding
    to each grade to a different server.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们可以对输出进行排序，然后将与每个年级对应的元组发送到不同的服务器。
- en: For example, Server A would receive the tuples (Grade 1, 7) and (Grade 1, 6),
    Server B would receive the tuple (Grade 2, 8), Server C would receive the tuples
    (Grade 3, 10) and (Grade 3, 9). Each of the servers, A, B, and C, would then find
    the average of the tuples and report back (Grade 1, 6.5), (Grade 2, 8), and (Grade
    3, 9.5).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，服务器A将接收元组（年级1，7）和（年级1，6），服务器B将接收元组（年级2，8），服务器C将接收元组（年级3，10）和（年级3，9）。然后，服务器A、B和C将找到元组的平均值并报告（年级1，6.5），（年级2，8）和（年级3，9.5）。
- en: Observe that there was an intermediary step in this process that involved sending
    the output to a particular server and sorting the output to determine which server
    it should be sent to. And indeed, MapReduce requires a shuffle and sort phase,
    whereby the key-value pairs are sorted so that each reducer receives a fixed set
    of unique keys.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个过程中有一个中间步骤，涉及将输出发送到特定服务器并对输出进行排序，以确定应将其发送到哪个服务器。事实上，MapReduce需要一个洗牌和排序阶段，其中键值对被排序，以便每个减少器接收一组固定的唯一键。
- en: In this example, if say, instead of three servers there were only two, Server
    A could be assigned to computing averages for keys corresponding to Grades 1 and
    2, and Server B could be assigned to computing an average for Grade 3.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，如果说，而不是三个服务器，只有两个，服务器A可以被分配为计算与年级1和2对应的键的平均值，服务器B可以被分配为计算年级3的平均值。
- en: 'In Hadoop, the following process takes place during MapReduce:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop中，MapReduce期间发生以下过程：
- en: The client sends a request for a task.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端发送任务请求。
- en: NameNode allocates DataNodes (individual servers) that will perform the map
    operation and ones that will perform the reduce operation. Note that the selection
    of the DataNode server is dependent upon whether the data that is required for
    the operation is *local to the server*. The servers where the data resides can
    only perform the map operation.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NameNode分配将执行映射操作和执行减少操作的DataNodes（单独的服务器）。请注意，DataNode服务器的选择取决于所需操作的数据是否*位于服务器本地*。数据所在的服务器只能执行映射操作。
- en: DataNodes perform the map phase and produce key-value (k,v) pairs.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DataNodes执行映射阶段并产生键值（k，v）对。
- en: As the mapper produces the (k,v) pairs, they are sent to these reduce nodes
    based on the *keys* the node is assigned to compute. The allocation of keys to
    servers is dependent upon a partitioner function, which could be as simple as
    a hash value of the key (this is default in Hadoop).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当映射器生成（k，v）对时，它们根据节点分配的*键*发送到这些减少节点。键分配给服务器取决于分区函数，这可以是键的哈希值（这是Hadoop中的默认值）。
- en: Once the reduce node receives its set of data corresponding to the keys it is
    responsible to compute on, it applies the reduce function and generates the final
    output.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦减少节点接收到与其负责计算的键对应的数据集，它就应用减少函数并生成最终输出。
- en: Hadoop maximizes the benefits of data locality. Map operations are performed
    by servers that hold the data locally, that is, on disk. More precisely, the map
    phase will be executed only by those servers that hold the blocks corresponding
    to the file. By delegating multiple individual nodes to perform computations independently,
    the Hadoop architecture can perform very large-scale data processing effectively.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop最大程度地利用了数据局部性。映射操作由本地保存数据的服务器执行，即在磁盘上。更准确地说，映射阶段将仅由持有文件对应块的服务器执行。通过委托多个独立节点独立执行计算，Hadoop架构可以有效地执行非常大规模的数据处理。
- en: Block size and number of mappers and reducers
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 块大小和映射器和减少器的数量
- en: An important consideration in the MapReduce process is an understanding of HDFS
    block size, that is, the size of the chunks into which the files have been split.
    A MapReduce task that needs to access a certain file will need to perform the
    map operation on each block representing the file. For example, given a 512 MB
    file and a 128 MB block size, four blocks would be needed to store the entire
    file. Hence, a MapReduce operation will at a minimum require four map tasks whereby
    each map operation would be applied to each subset of the data (that is, each
    of the four blocks).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce过程中的一个重要考虑因素是理解HDFS块大小，即文件被分割成的块的大小。需要访问某个文件的MapReduce任务将需要对表示文件的每个块执行映射操作。例如，给定一个512MB的文件和128MB的块大小，需要四个块来存储整个文件。因此，MapReduce操作将至少需要四个映射任务，其中每个映射操作将应用于数据的每个子集（即四个块中的每一个）。
- en: If the file was very large, however, and required say, 10,000 blocks to store,
    this means we would have required 10,000 map operations. But, if we had only 10
    servers, then we'd have to send 1,000 map operations to each server. This might
    be sub-optimal as it can lead to a high penalty due to disk I/O operations and
    resource allocation settings on a per-map basis.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果文件非常大，比如需要10,000个块来存储，这意味着我们需要10,000个映射操作。但是，如果我们只有10台服务器，那么我们将不得不向每台服务器发送1,000个映射操作。这可能是次优的，因为它可能导致由于磁盘I/O操作和每个映射的资源分配设置而产生高惩罚。
- en: The number of reducers required is summarized very elegantly on Hadoop Wiki
    ([https://wiki.apache.org/hadoop/HowManyMapsAndReduces](https://wiki.apache.org/hadoop/HowManyMapsAndReduces)).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的减少器数量在Hadoop Wiki上非常优雅地总结了（[https://wiki.apache.org/hadoop/HowManyMapsAndReduces](https://wiki.apache.org/hadoop/HowManyMapsAndReduces)）。
- en: 'The ideal reducers should be the optimal value that gets them closest to:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的减少器应该是最接近以下值的最佳值：
- en: '* A multiple of the block size * A task time between 5 and 15 minutes * Creates
    the fewest files possible'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '* 块大小的倍数 * 5到15分钟之间的任务时间 * 创建尽可能少的文件'
- en: 'Anything other than that means there is a good chance your reducers are less
    than great. There is a tremendous tendency for users to use a REALLY high value
    ("More parallelism means faster!") or a REALLY low value ("I don''t want to blow
    my namespace quota!"). Both are equally dangerous, resulting in one or more of:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，很有可能你的减少器不太好。用户有极大的倾向使用一个非常高的值（“更多的并行性意味着更快！”）或一个非常低的值（“我不想超出我的命名空间配额！”）。这两种情况都同样危险，可能导致以下一种或多种情况：
- en: '* Terrible performance on the next phase of the workflow * Terrible performance
    due to the shuffle * Terrible overall performance because you''ve overloaded the
    namenode with objects that are ultimately useless * Destroying disk IO for no
    really sane reason * Lots of network transfers due to dealing with crazy amounts
    of CFIF/MFIF work'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '* 下一个工作流程阶段的性能差 * 由于洗牌而导致性能差 * 由于过载了最终无用的对象而导致整体性能差 * 没有真正合理的原因而破坏磁盘I/O * 由于处理疯狂数量的CFIF/MFIF工作而产生大量的网络传输'
- en: Hadoop YARN
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop YARN
- en: YARN was a module introduced in Hadoop 2\. In Hadoop 1, the process of managing
    jobs and monitoring them was performed by processes known as JobTracker and TaskTracker(s).
    NameNodes that ran the JobTracker daemon (process) would submit jobs to the DataNodes
    which ran TaskTracker daemons (processes).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: YARN是Hadoop 2中引入的一个模块。在Hadoop 1中，管理作业和监视它们的过程是由称为JobTracker和TaskTracker的进程执行的。运行JobTracker守护进程（进程）的NameNodes会将作业提交给运行TaskTracker守护进程（进程）的DataNodes。
- en: 'The JobTracker was responsible for the co-ordination of all MapReduce jobs
    and served as a central administrator for managing processes, handling server
    failure, re-allocating to new DataNodes, and so on. The TaskTracker monitored
    the execution of jobs local to its own instance in the DataNode and provided feedback
    on the status to the JobTracker as shown in the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: JobTracker负责协调所有MapReduce作业，并作为管理进程、处理服务器故障、重新分配到新DataNodes等的中央管理员。TaskTracker监视DataNode中本地作业的执行，并向JobTracker提供状态反馈，如下所示：
- en: '![](img/77c0cfdf-fc44-4b4d-91fe-2d0709ee0109.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77c0cfdf-fc44-4b4d-91fe-2d0709ee0109.png)'
- en: JobTracker and TaskTrackers
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: JobTracker和TaskTrackers
- en: This design worked well for a long time, but as Hadoop evolved, the demands
    for more sophisticated and dynamic functionalities rose proportionally. In Hadoop
    1, the NameNode, and consequently the JobTracker process, managed both job scheduling
    and resource monitoring. In the event the NameNode failed, all activities in the
    cluster would cease immediately. Lastly, all jobs had to be represented in MapReduce
    terms - that is, all code would have to be written in the MapReduce framework
    in order to be executed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计长时间以来运行良好，但随着Hadoop的发展，对更复杂和动态功能的需求也相应增加。在Hadoop 1中，NameNode，因此是JobTracker进程，管理作业调度和资源监控。如果NameNode发生故障，集群中的所有活动将立即停止。最后，所有作业都必须以MapReduce术语表示
    - 也就是说，所有代码都必须在MapReduce框架中编写才能执行。
- en: 'Hadoop 2 alleviated all these concerns:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2解决了所有这些问题：
- en: The process of job management, scheduling, and resource monitoring was decoupled
    and delegated to a new framework/module called YARN
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业管理、调度和资源监控的过程被解耦并委托给一个名为YARN的新框架/模块
- en: A secondary NameNode could be defined which would act as a helper for the primary
    NameNode
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以定义一个辅助主NameNode，它将作为主NameNode的辅助
- en: Further, Hadoop 2.0 would accommodate frameworks beyond MapReduce
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，Hadoop 2.0将容纳MapReduce以外的框架
- en: Instead of fixed map and reduce slots, Hadoop 2 would leverage containers
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 2不再使用固定的映射和减少插槽，而是利用容器
- en: In MapReduce, all data had to be read from disk, and this was fine for operations
    on large datasets but it was not optimal for operations on smaller datasets. In
    fact, any tasks that required very fast processing (low latency), were interactive
    in nature, or had multiple iterations (thus requiring multiple reads from the
    disk for the same data), would be extremely slow.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce中，所有数据都必须从磁盘读取，这对于大型数据集的操作来说是可以接受的，但对于小型数据集的操作来说并不是最佳选择。事实上，任何需要非常快速处理（低延迟）、具有交互性的任务或需要多次迭代（因此需要多次从磁盘读取相同数据）的任务都会非常慢。
- en: By removing these dependencies, Hadoop 2 allowed developers to implement new
    programming frameworks that would support jobs with diverse performance requirements,
    such as low latency and interactive real-time querying, iterative processing required
    for machine learning, different topologies such as the processing of streaming
    data, optimizations such as in-memory data caching/processing, and so on.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过消除这些依赖关系，Hadoop 2允许开发人员实现支持具有不同性能要求的作业的新编程框架，例如低延迟和交互式实时查询，机器学习所需的迭代处理，流数据处理等不同拓扑结构，优化，例如内存数据缓存/处理等。
- en: 'A few new terms became prominent:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 出现了一些新术语：
- en: '**ApplicationMaster**: Responsible for managing the resources needed by applications.
    For example, if a certain job required more memory, the ApplicationMaster would
    be responsible for securing the required resource. An application in this context
    refers to application execution frameworks such as MapReduce, Spark, and so on.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ApplicationMaster**：负责管理应用程序所需的资源。例如，如果某个作业需要更多内存，ApplicationMaster将负责获取所需的资源。在这种情况下，应用程序指的是诸如MapReduce、Spark等应用执行框架。'
- en: '**Containers**: The unit of resource allocation (for example, 1 GB of memory
    and four CPUs). An application may require several such containers to execute.
    The ResourceManager allocates containers for executing tasks. Once the allocation
    is complete, the ApplicationMaster requests DataNodes to start the allocated containers
    and takes over the management of the containers.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器**：资源分配的单位（例如，1GB内存和四个CPU）。一个应用程序可能需要多个这样的容器来执行。ResourceManager为执行任务分配容器。分配完成后，ApplicationMaster请求DataNodes启动分配的容器并接管容器的管理。'
- en: '**ResourceManager**: A component of YARN that had the primary role of allocating
    resources to applications and functioned as a replacement for the JobTracker.
    The ResourceManager process ran on the NameNode just as the JobTracker did.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ResourceManager**：YARN的一个组件，其主要作用是为应用程序分配资源，并作为JobTracker的替代品。ResourceManager进程在NameNode上运行，就像JobTracker一样。'
- en: '**NodeManagers**: A replacement for TaskTracker, NodeManagers were responsible
    for reporting the status of jobs to the ResourceManager (RM) and monitoring the
    resource utilization of containers.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NodeManagers**：NodeManagers是TaskTracker的替代品，负责向ResourceManager（RM）报告作业的状态，并监视容器的资源利用情况。'
- en: 'The following image shows a high level view of the ResourceManager and NodeManagers
    in Hadoop 2.0:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了Hadoop 2.0中ResourceManager和NodeManagers的高层视图：
- en: '![](img/9debf45a-4304-4551-9c7b-ac04b702b13f.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9debf45a-4304-4551-9c7b-ac04b702b13f.png)'
- en: Hadoop 2.0
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2.0
- en: 'The prominent concepts inherent in Hadoop 2 have been illustrated in the next
    image:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2中固有的显着概念已在下一图中说明：
- en: '![](img/86aed323-c6e4-4dbe-9272-3fc5e50d9441.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86aed323-c6e4-4dbe-9272-3fc5e50d9441.png)'
- en: Hadoop 2.0 Concepts
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2.0概念
- en: Job scheduling in YARN
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YARN中的作业调度
- en: It is not uncommon for large Hadoop clusters to have multiple jobs running concurrently.
    The allocation of resources when there are multiple jobs submitted from multiple
    departments becomes an important and indeed interesting topic. Which request should
    receive priority if say, two departments, A and B, submit a job at the same time
    but each request is for the maximum available resources? In general, Hadoop uses
    a **First-In-First-Out** (**FIFO**) policy. That is, whoever submits the job first
    gets to use the resources first. But what if A submitted the job first but completing
    A's job will take five hours whereas B's job will complete in five minutes?
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 大型Hadoop集群同时运行多个作业并不罕见。当有多个部门提交了多个作业时，资源的分配就成为一个重要且有趣的话题。如果说，A和B两个部门同时提交了作业，每个请求都是为了获得最大可用资源，那么哪个请求应该优先获得资源呢？一般来说，Hadoop使用**先进先出**（**FIFO**）策略。也就是说，谁先提交作业，谁就先使用资源。但如果A先提交了作业，但完成A的作业需要五个小时，而B的作业将在五分钟内完成呢？
- en: 'To deal with these nuances and variables in job scheduling, numerous scheduling
    methods have been implemented. Three of the more commonly used ones are:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理作业调度中的这些细微差别和变量，已经实施了许多调度方法。其中三种常用的方法是：
- en: '**FIFO**: As described above, FIFO scheduling uses a queue to priorities jobs.
    Jobs are executed in the order in which they are submitted.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FIFO**：如上所述，FIFO调度使用队列来优先处理作业。作业按照提交的顺序执行。'
- en: '**CapacityScheduler**: CapacityScheduler assigns a value on the number of jobs
    that can be submitted on a per-department basis, where a department can indicate
    a logical group of users. This is to ensure that each department or group can
    have access to the Hadoop cluster and be able to utilize a minimum number of resources.
    The scheduler also allows departments to scale up beyond their assigned capacity
    up to a maximum value set on a per-department basis if there are unused resources
    on the server. The model of CapacityScheduler thus provides a guarantee that each
    department can access the cluster on a deterministic basis.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CapacityScheduler**：CapacityScheduler根据每个部门可以提交的作业数量分配值，其中部门可以表示用户的逻辑组。这是为了确保每个部门或组可以访问Hadoop集群并能够利用最少数量的资源。如果服务器上有未使用的资源，调度程序还允许部门根据每个部门的最大值设置超出其分配容量。因此，CapacityScheduler的模型提供了每个部门可以确定性地访问集群的保证。'
- en: '**Fair Schedulers**: These schedulers attempt to evenly balance the utilization
    of resources across different apps. While an even balance might not be feasible
    at a certain given point in time, balancing allocation over time such that the
    averages are more or less similar can be achieved using Fair Schedulers.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平调度程序**：这些调度程序试图在不同应用程序之间均匀平衡资源的利用。虽然在某个特定时间点上可能无法实现平衡，但通过随时间平衡分配，可以使用公平调度程序实现平均值更或多或少相似的目标。'
- en: These, and other schedulers, provide finely grained access controls (such as
    on a per-user or per-group basis) and primarily utilize queues in order to prioritize
    and allocate resources.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些以及其他调度程序提供了精细的访问控制（例如基于每个用户或每个组的基础）并主要利用队列来优先和分配资源。
- en: Other topics in Hadoop
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop中的其他主题
- en: A few other aspects of Hadoop deserve special mention. As we have discussed
    the most important topics at length, this section provides an overview of some
    of the other subjects of interest.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop还有一些其他方面值得特别提及。由于我们已经详细讨论了最重要的主题，本节概述了一些其他感兴趣的主题。
- en: Encryption
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加密
- en: Data encryption is mandated by official regulations for various types of data.
    In the US, data that identifies patient information is required to be compliant
    with the rules set forth by HIPAA that dictate how such records should be stored.
    Data in HDFS can be encrypted whilst at rest (on disk) and/or while in transit.
    The keys that are used to decrypt the data are generally managed by **Key Management
    Systems** (**KMSs**).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加密是根据官方规定对各种类型的数据进行的。在美国，要求符合HIPAA规定的规则，对识别患者信息的数据进行加密存储。HDFS中的数据可以在静态状态（在磁盘上）和/或传输过程中进行加密。用于解密数据的密钥通常由密钥管理系统（KMS）管理。
- en: User authentication
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户认证
- en: Hadoop can use the native user-authentication methods of the server. For example,
    in Linux-based machines, users can be authenticated based on the IDs defined in
    the system's `/etc/passwd` files. In other words, Hadoop inherits the user authentication
    set up on the server side.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop可以使用服务器的本机用户认证方法。例如，在基于Linux的机器上，用户可以根据系统“/etc/passwd”文件中定义的ID进行身份验证。换句话说，Hadoop继承了服务器端设置的用户认证。
- en: User authentication via Kerberos, a cross-platform authentication protocol,
    is also commonly used in Hadoop clusters. Kerberos works based on a concept of
    tickets that grant privileges to users on a temporary as-needed basis. Tickets
    can be invalidated using Kerberos commands, thus restricting the users' rights
    to access resources on the cluster as needed.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Kerberos进行用户认证，这是一种跨平台的认证协议，在Hadoop集群中也很常见。Kerberos基于授予用户临时权限的票据概念。可以使用Kerberos命令使票据无效，从而限制用户根据需要访问集群上的资源的权限。
- en: Note that even if the user is permitted to access data (user authentication),
    he or she can still be limited in what data can be accessed due to another feature
    known as authorization. The term implies that even if the user can authenticate
    and log in to the system, the user may be restricted to only the data the user
    is authorized to access. This level of authorization is generally performed using
    native HDFS commands to change directory and file ownerships to the named users.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使用户被允许访问数据（用户认证），由于另一个名为授权的功能，他或她仍然可能受到访问数据的限制。该术语意味着即使用户可以进行身份验证并登录到系统，用户可能仅限于可以访问的数据。通常使用本机HDFS命令执行此级别的授权，以更改目录和文件所有权为指定的用户。
- en: Hadoop data storage formats
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop数据存储格式
- en: Since Hadoop involves storing very large-scale data, it is essential to select
    a storage type that is appropriate for your use cases. There are a few formats
    in which data can be stored in Hadoop, and the selection of the optimal storage
    format depends on your requirements in terms of read/write I/O speeds, how well
    the files can be compressed and decompressed on demand, and how easily the file
    can be split since the data will be eventually stored as blocks.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Hadoop涉及存储大规模数据，因此选择适合您用例的存储类型至关重要。Hadoop中可以以几种格式存储数据，选择最佳存储格式取决于您对读/写I/O速度的要求，文件可以按需压缩和解压缩的程度，以及文件可以被分割的容易程度，因为数据最终将被存储为块。
- en: 'Some of the popular and commonly used storage formats are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流行和常用的存储格式如下：
- en: '**Text/CSV**: These are plain text CSV files, similar to Excel files, but saved
    in plain text format. Since CSV files contain records on a per-line basis, it
    is naturally trivial to split the files up into blocks of data.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本/CSV**：这些是纯文本CSV文件，类似于Excel文件，但以纯文本格式保存。由于CSV文件包含每行的记录，因此将文件拆分为数据块是自然而然的。'
- en: '**Avro**: Avro was developed to improve the efficient sharing of data across
    heterogeneous systems. It stores both the schema as well as the actual data in
    a single compact binary using data serialization. Avro uses JSON to store the
    schema and binary format for the data and serializes them into a single Avro Object
    Container File. Multiple languages such as Python, Scala, C/C++, and others have
    native APIs that can read Avro files and consequently, it is very portable and
    well suited for cross-platform data exchange.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Avro**：Avro旨在改善在异构系统之间高效共享数据。它使用数据序列化，将模式和实际数据存储在单个紧凑的二进制文件中。Avro使用JSON存储模式和二进制格式存储数据，并将它们序列化为单个Avro对象容器文件。多种语言，如Python、Scala、C/C++等，都有本机API可以读取Avro文件，因此非常适合跨平台数据交换。'
- en: '**Parquet**: Parquet is a columnar data storage format. This helps to improve
    performance, sometimes significantly by permitting data storage and access on
    a per-column basis. Intuitively, if you were working on a 1 GB file with 100 columns
    and 1 million rows, and wanted to query data from only one of the 100 columns,
    being able to access just the individual column would be more efficient than having
    to access the entire file.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet：Parquet是一种列式数据存储格式。这有助于提高性能，有时甚至显著提高性能，因为它允许按列存储和访问数据。直观地说，如果你正在处理一个包含100列和100万行的1GB文件，并且只想从这100列中查询数据，能够只访问单独的列会比访问整个文件更有效。
- en: '**ORCFiles**: ORC stands for Optimized Row-Columnar. In a sense, it is a further
    layer of optimization over pure columnar formats such as Parquet. ORCFiles store
    data not only by columns, but also by rows, also known as stripes. A file with
    data in tabular format can thus be split into multiple smaller stripes where each
    stripe comprises of a subset of rows from the original file. By splitting data
    in this manner, if a user task requires access to only a small subsection of the
    data, the process can interrogate the specific stripe that holds the data.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ORCFiles：ORC代表优化的行列格式。从某种意义上说，它是对纯列格式（如Parquet）的进一步优化。ORCFiles不仅按列存储数据，还按行存储，也称为条带。因此，以表格格式存储的文件可以分割成多个较小的条带，其中每个条带包含原始文件的一部分行。通过这种方式分割数据，如果用户任务只需要访问数据的一个小部分，那么该过程可以查询包含数据的特定条带。
- en: '**SequenceFiles**: In SequenceFiles, data is represented as key-value pairs
    and stored in a binary serialized format. Due to serialization, data can be represented
    in a compact binary format that not only reduces the data size but consequently
    also improves I/O. Hadoop, and more concretely HDFS, is not efficient when there
    are multiple files of a small size, such as audio files. SequenceFiles solve this
    problem by allowing multiple small files to be stored as a single unit or SequenceFile.
    They are also very well suited for parallel operations that are splittable and
    are overall efficient for MapReduce jobs.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SequenceFiles：在SequenceFiles中，数据表示为键值对并以二进制序列化格式存储。由于序列化，数据可以以紧凑的二进制格式表示，不仅减小了数据大小，而且提高了I/O。Hadoop，尤其是HDFS，在存在多个小文件（如音频文件）时效率不高。SequenceFiles通过允许将多个小文件存储为单个单元或SequenceFile来解决了这个问题。它们也非常适合可分割的并行操作，并且对于MapReduce作业总体上是高效的。
- en: '**HDFS Snapshots:** HDFS Snapshots allow users to preserve data at a given
    point in time in a read-only mode. Users can create snapshots—in essence a replica
    of the data as it is at that point time—in in HDFS, such that they can be retrieved
    at a later stage as and when needed. This ensures that data can be recovered in
    the event that there is a file corruption or any other failure that affects the
    availability of data. In that regard, it can be also considered to be a backup.
    The snapshots are available in a .snapshot directory where they have been created.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HDFS快照：HDFS快照允许用户以只读模式保存特定时间点的数据。用户可以在HDFS中创建快照，实质上是数据在那个时间点的副本，以便在以后需要时检索。这确保了在文件损坏或其他影响数据可用性的故障发生时可以恢复数据。在这方面，它也可以被视为备份。快照存储在一个.snapshot目录中，用户在那里创建了它们。
- en: '**Handling of node failures:** Large Hadoop clusters can contain tens of thousands
    of nodes. Hence it is likely that there would be server failures on any given
    day. So that the NameNode is aware of the status of all nodes in the cluster,
    the DataNodes send a periodic heartbeat to the NameNode. If the NameNode detects
    that a server has failed, that is, it has stopped receiving heartbeats, it marks
    the server as failed and replicates all the data that was local to the server
    onto a new instance.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点故障处理：大型Hadoop集群可能包含数万个节点。因此，任何一天都可能发生服务器故障。为了让NameNode了解集群中所有节点的状态，DataNodes向NameNode发送定期心跳。如果NameNode检测到服务器已经失败，即它停止接收心跳，它会将服务器标记为失败，并将本地服务器上的所有数据复制到新实例上。
- en: New features expected in Hadoop 3
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop 3中预期的新功能
- en: At the time of writing this book, Hadoop 3 is in Alpha stage. Details on the
    new changes that will be available in Hadoop 3 can be found on the internet. For
    example, [http://hadoop.apache.org/docs/current/](http://hadoop.apache.org/docs/current/)
    provides the most up-to-date information on new changes to the architecture.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，Hadoop 3处于Alpha阶段。关于Hadoop 3中将可用的新变化的详细信息可以在互联网上找到。例如，[http://hadoop.apache.org/docs/current/](http://hadoop.apache.org/docs/current/)提供了关于架构新变化的最新信息。
- en: The Hadoop ecosystem
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop生态系统
- en: This chapter should be titled as the Apache ecosystem. Hadoop, like all the
    other projects that will be discussed in this section, is an Apache project. Apache
    is used loosely as a short form for the open source projects that are supported
    by the Apache Software Foundation. It originally has its roots in the development
    of the Apache HTTP server in the early 90s, and today is a collaborative global
    initiative that comprises entirely of volunteers who participate in releasing
    open source software to the global technical community.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章应该被命名为Apache生态系统。像本节中将讨论的所有其他项目一样，Hadoop是一个Apache项目。Apache是一个开源项目的简称，由Apache软件基金会支持。它最初起源于90年代初开发的Apache
    HTTP服务器，并且今天是一个协作的全球倡议，完全由参与向全球技术社区发布开源软件的志愿者组成。
- en: Hadoop started out as, and still is, one of the projects in the Apache ecosystem.
    Due to its popularity, many other projects that are also part of Apache have been
    linked directly or indirectly to Hadoop as they support key functionalities in
    the Hadoop environment. That said, it is important to bear in mind that these
    projects can in most cases exist as independent products that can function without
    a Hadoop environment. Whether it would provide optimal functionality would be
    a separate topic.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop最初是Apache生态系统中的一个项目，现在仍然是。由于其受欢迎程度，许多其他Apache项目也直接或间接地与Hadoop相关联，因为它们支持Hadoop环境中的关键功能。也就是说，重要的是要记住，这些项目在大多数情况下可以作为独立产品存在，可以在没有Hadoop环境的情况下运行。它是否能提供最佳功能将是一个单独的话题。
- en: 'In this section, we''ll go over some of the Apache projects that have had a
    great deal of influence as well as an impact on the growth and usability of Hadoop
    as a standard IT enterprise solution, as detailed in the following figure:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些对Hadoop的增长和可用性产生了重大影响的Apache项目，如下图所示：
- en: '| **Product** | **Functionality** |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| **产品** | **功能** |'
- en: '| Apache Pig | Apache Pig, also known as Pig Latin, is a language specifically
    designed to represent MapReduce programs through concise statements that define
    workflows. Coding MapReduce programs in the traditional methods, such as with
    Java, can be quite complex, and Pig provides an easy abstraction to express a
    MapReduce workflow and complex **Extract-Transform-Load** (**ETL**) through the
    use of simple semantics. Pig programs are executed via the Grunt shell. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Apache Pig | Apache Pig，也称为Pig Latin，是一种专门设计用于通过简洁的语句表示MapReduce程序的语言，这些语句定义了工作流程。使用传统方法编写MapReduce程序，比如用Java，可能会非常复杂，Pig提供了一种简单的抽象来表达MapReduce工作流程和复杂的**抽取-转换-加载**（**ETL**）过程。Pig程序通过Grunt
    shell执行。|'
- en: '| Apache HBase | Apache HBase is a distributed column-oriented database that
    sits on top of HDFS. It was modelled on Google''s BigTable whereby data is represented
    in a columnar format. HBase supports low-latency read-write across tables with
    billions of records and is well suited to tasks that require direct random access
    to data. More concretely, HBase indexes data in three dimensions - row, column,
    and timestamp. It also provides a means to represent data with an arbitrary number
    of columns as column values can be expressed as key-value pairs within the cells
    of an HBase table. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Apache HBase | Apache HBase是一个分布式列式数据库，位于HDFS之上。它是基于Google的BigTable模型设计的，其中数据以列格式表示。HBase支持跨数十亿条记录的表的低延迟读写，并且非常适合需要直接随机访问数据的任务。更具体地说，HBase以三个维度索引数据
    - 行、列和时间戳。它还提供了一种表示具有任意列数的数据的方法，因为列值可以在HBase表的单元格中表示为键值对。|'
- en: '| Apache Hive | Apache Hive provides a SQL-like dialect to query data stored
    in HDFS. Hive stores data as serialized binary files in a folder-like structure
    in HDFS. Similar to tables in traditional database management systems, Hive stores
    data in tabular format in HDFS partitioned based on user-selected attributes.
    Partitions are thus subfolders of the higher-level directories or tables. There
    is a third level of abstraction provided by the concept of buckets, which reference
    files in the partitions of the Hive tables. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Apache Hive | Apache Hive提供了类似SQL的方言来查询存储在HDFS中的数据。Hive将数据以序列化的二进制文件形式存储在HDFS中的类似文件夹的结构中。与传统数据库管理系统中的表类似，Hive以表格格式存储数据，根据用户选择的属性在HDFS中进行分区。因此，分区是高级目录或表的子文件夹。概念上提供了第三层抽象，即桶，它引用Hive表的分区中的文件。|'
- en: '| Apache Sqoop | Sqoop is used to extract data from traditional databases to
    HDFS. Large enterprises that have data stored in relational database management
    systems can thus use Sqoop to transfer data from their data warehouse to a Hadoop
    implementation. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Apache Sqoop | Sqoop用于从传统数据库中提取数据到HDFS。因此，将数据存储在关系数据库管理系统中的大型企业可以使用Sqoop将数据从其数据仓库转移到Hadoop实现中。|'
- en: '| Apache Flume | Flume is used for the management, aggregation, and analysis
    of large-scale log data. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Apache Flume | Flume用于管理、聚合和分析大规模日志数据。|'
- en: '| Apache Kafka | Kafka is a publish/subscribe-based middleware system that
    can be used to analyze and subsequently persist (in HDFS) streaming data in real
    time. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Apache Kafka | Kafka是一个基于发布/订阅的中间件系统，可用于实时分析和随后将流数据（在HDFS中）持久化。|'
- en: '| Apache Oozie | Oozie is a workflow management system designed to schedule
    Hadoop jobs. It implements a key concept known as a **directed acyclic graph**
    (**DAG**), which will be discussed in our section on Spark. |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Apache Oozie | Oozie是一个用于调度Hadoop作业的工作流管理系统。它实现了一个称为**有向无环图**（**DAG**）的关键概念，这将在我们关于Spark的部分中讨论。|'
- en: '| Apache Spark | Spark is one of the most significant projects in Apache and
    was designed to address some of the shortcomings of the HDFS-MapReduce model.
    It started as a relatively small project at UC Berkeley and evolved rapidly to
    become one of the most prominent alternatives to using Hadoop for analytical tasks.
    Spark has seen a widespread adoption across the industry and comprises of various
    other subprojects that provide additional capabilities such as machine learning,
    streaming analytics, and others. |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Apache Spark | Spark是Apache中最重要的项目之一，旨在解决HDFS-MapReduce模型的一些缺点。它最初是加州大学伯克利分校的一个相对较小的项目，迅速发展成为用于分析任务的Hadoop最重要的替代方案之一。Spark在行业中得到了广泛的应用，并包括其他各种子项目，提供额外的功能，如机器学习、流式分析等。|'
- en: Hands-on with CDH
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CDH实战
- en: In this section, we will utilize the CDH QuickStart VM to work through some
    of the topics that have been discussed in the current chapter. The exercises do
    not have to be necessarily performed in a chronological order and are not dependent
    upon the completion of any of the other exercises.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将利用CDH QuickStart VM来学习本章讨论的一些主题。这些练习不一定要按照时间顺序进行，并且不依赖于完成其他练习。
- en: 'We will complete the following exercises in this section:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中完成以下练习：
- en: WordCount using Hadoop MapReduce
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hadoop MapReduce进行词频统计
- en: Working with the HDFS
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用HDFS
- en: Downloading and querying data with Apache Hive
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Hive下载和查询数据
- en: WordCount using Hadoop MapReduce
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Hadoop MapReduce进行词频统计
- en: In this exercise, we will be attempting to count the number of occurrences of
    each word in one of the longest novels ever written. For the exercise, we have
    selected the book *Artamène ou le Grand Cyrus* written by Georges and/or Madeleine
    de Scudéry between 1649-1653\. The book is considered to be the second longest
    novel ever written, per the related list on Wikipedia ([https://en.wikipedia.org/wiki/List_of_longest_novels](https://en.wikipedia.org/wiki/List_of_longest_novels)).
    The novel consists of 13,905 pages across 10 volumes and has close to two million
    words.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将尝试计算有史以来最长小说之一中每个单词的出现次数。对于本练习，我们选择了由乔治和/或玛德琳·德·斯库德里（Georges and/or
    Madeleine de Scudéry）于1649-1653年间编写的书籍《Artamène ou le Grand Cyrus》。该书被认为是有史以来第二长的小说，根据维基百科上相关列表（[https://en.wikipedia.org/wiki/List_of_longest_novels](https://en.wikipedia.org/wiki/List_of_longest_novels)）。这部小说共有10卷，共计13,905页，约有两百万字。
- en: 'To begin, we need to launch the Cloudera Distribution of Hadoop Quickstart
    VM in VirtualBox and double-click on the Cloudera Quickstart VM instance:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在VirtualBox中启动Cloudera Distribution of Hadoop Quickstart VM，并双击Cloudera
    Quickstart VM实例：
- en: '![](img/bcae8e19-32d6-4d52-9798-b80487582a38.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcae8e19-32d6-4d52-9798-b80487582a38.png)'
- en: 'It will take some time to start up as it initializes all the CDH-related processes
    such as the DataNode, NameNode, and so on:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 启动需要一些时间，因为它初始化所有与CDH相关的进程，如DataNode、NameNode等：
- en: '![](img/6f3d9fb5-6d5a-48ed-9227-371380b5fac8.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f3d9fb5-6d5a-48ed-9227-371380b5fac8.png)'
- en: 'Once the process starts up, it will launch a default landing page that contains
    references to numerous tutorials related to Hadoop. We''ll be writing our MapReduce
    code in the Unix terminal for this section. Launch the terminal from the top-left
    menu, as shown in the following screenshot:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦进程启动，它将启动一个默认的着陆页，其中包含与Hadoop相关的许多教程的引用。在本节中，我们将在Unix终端中编写我们的MapReduce代码。从左上角菜单中启动终端，如下截图所示：
- en: '![](img/26b31ed5-02d4-41c1-a81a-e2184b90b038.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26b31ed5-02d4-41c1-a81a-e2184b90b038.png)'
- en: 'Now, we must follow these steps:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须按照以下步骤进行：
- en: Create a directory named `cyrus`. This is where we will store all the files
    which contain the text of the book.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`cyrus`的目录。这是我们将存储包含书文本的所有文件的地方。
- en: Run `getCyrusFiles.sh` as shown in step 4\. This will download the book into
    the `cyrus` directory.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照第4步所示运行`getCyrusFiles.sh`。这将把书下载到`cyrus`目录中。
- en: Run `processCyrusFiles.sh` as shown. The book contains various Unicode and non-printable
    characters. Additionally, we would like to change all the words to lowercase in
    order to ignore double-counting words that are the same but have capitalizations.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照所示运行`processCyrusFiles.sh`。该书包含各种Unicode和不可打印字符。此外，我们希望将所有单词改为小写，以忽略相同但具有不同大小写的单词的重复计数。
- en: This will produce a file called `cyrusprint.txt`. This document contains the
    entire text of the book. We will be running our MapReduce code on this text file.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生一个名为`cyrusprint.txt`的文件。该文件包含整本书的全部文本。我们将在这个文本文件上运行我们的MapReduce代码。
- en: Prepare `mapper.py` and `reducer.py`. As the name implies, `mapper.py` runs
    the map part of the MapReduce process. Similarly, `reducer.py` runs the reduce
    part of the MapReduce process. The file `mapper.py` will split the document into
    words and assign a value of one to each word in the document. The file, `reducer.py`,
    will read in the sorted output of `mapper.py` and sum the occurrences of the same
    word (by first initializing the count of the word to one and incrementing it for
    each new occurrence of the word). The final output is a file containing the count
    of each word in the document.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备`mapper.py`和`reducer.py`。顾名思义，`mapper.py`运行MapReduce过程的映射部分。同样，`reducer.py`运行MapReduce过程的减少部分。文件`mapper.py`将文档拆分为单词，并为文档中的每个单词分配一个值为一的值。文件`reducer.py`将读取`mapper.py`的排序输出，并对相同单词的出现次数进行求和（首先将单词的计数初始化为一，并在每个新单词的出现时递增）。最终输出是一个包含文档中每个单词计数的文件。
- en: 'The steps are as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Create `getCyrusFiles.sh` - this script will be used to retrieve the data from
    the web:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`getCyrusFiles.sh` - 此脚本将用于从网络中检索数据：
- en: '[PRE0]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create `processCyrusFiles.sh` - this script will be used to concatenate and
    cleanse the files that were downloaded in the previous step:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`processCyrusFiles.sh` - 此脚本将用于连接和清理在上一步中下载的文件：
- en: '[PRE1]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Change the permissions to 755 to make the `.sh` files executable at the command
    prompt:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改权限为755，以使`.sh`文件在命令提示符下可执行：
- en: '[PRE2]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Execute `getCyrusFiles.sh`:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`getCyrusFiles.sh`：
- en: '[PRE3]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Execute `processCyrusFiles.sh`:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`processCyrusFiles.sh`：
- en: '[PRE4]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Execute the following steps to copy the final file, named `cyrusprint.txt`,
    to HDFS, create the `mapper.py` and `reducer.py` scripts.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下步骤，将最终文件`cyrusprint.txt`复制到HDFS，创建`mapper.py`和`reducer.py`脚本。
- en: The files, `mapper.py` and `reducer.py`, are referenced on Glenn Klockwood's
    website ([http://www.glennklockwood.com/data-intensive/hadoop/streaming.html](http://www.glennklockwood.com/data-intensive/hadoop/streaming.html)),
    which provides a wealth of information on MapReduce and related topics.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapper.py`和`reducer.py`文件在Glenn Klockwood的网站上有引用（[http://www.glennklockwood.com/data-intensive/hadoop/streaming.html](http://www.glennklockwood.com/data-intensive/hadoop/streaming.html)），该网站提供了大量关于MapReduce和相关主题的信息。'
- en: 'The following code shows the contents of `mapper.py`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了`mapper.py`的内容：
- en: '[PRE5]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Execute the mapper and reducer scripts that will perform the MapReduce operations
    in order to produce the word count. You may see error messages as shown here,
    but for the purpose of this exercise (and for generating the results), you may
    disregard them:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行mapper和reducer脚本，执行MapReduce操作以产生词频统计。您可能会看到如下所示的错误消息，但出于本练习的目的（以及为了生成结果），您可以忽略它们：
- en: '[PRE6]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The results are stored in HDFS under the `/user/cloudera/output` directory
    in files prefixed with `part-` :'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果存储在HDFS中的`/user/cloudera/output`目录下，文件名以`part-`为前缀：
- en: '[PRE7]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To view the contents of the file use `hdfs dfs -cat` and provide the name of
    the file. In this case we are viewing the first 10 lines of the output:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看文件的内容，请使用`hdfs dfs -cat`并提供文件名。在这种情况下，我们查看输出的前10行：
- en: '[PRE8]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Analyzing oil import prices with Hive
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Hive分析石油进口价格
- en: 'In this section, we will use Hive to analyze the import prices of oil in countries
    across the world from 1980-2016\. The data is available from the site of the **OECD**
    (**Organisation for Economic Co-operation and Development**) at the URL shown
    in the following screenshot:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Hive分析1980-2016年间世界各国的石油进口价格。数据可从**OECD**（经济合作与发展组织）的网站上获取，网址如下截图所示：
- en: '![](img/23401bea-8fb7-4edb-af54-e882c4086d69.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23401bea-8fb7-4edb-af54-e882c4086d69.png)'
- en: The actual CSV file is available at [https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&amp;detail=code&amp;separator=comma&amp;csv-lang=en](https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的CSV文件可从[https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&amp;detail=code&amp;separator=comma&amp;csv-lang=en](https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en)获取。
- en: 'Since we''ll be loading the data in Hive, it makes sense to download the file
    into our home directory via the terminal in our Cloudera Quickstart CDH environment.
    The steps we''d execute are as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在Hive中加载数据，因此通过Cloudera Quickstart CDH环境中的终端将文件下载到我们的主目录是有意义的。我们将执行以下步骤：
- en: 'Download the CSV file into the CDH environment:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将CSV文件下载到CDH环境中：
- en: '![](img/91b3f48c-3292-4ae8-bef9-e9b434b94fd9.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91b3f48c-3292-4ae8-bef9-e9b434b94fd9.png)'
- en: '[PRE9]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Clean the CSV file. Data cleansing is an area of core importance in data science.
    In practice, it is very common to receive files that will require some level of
    cleansing. This is due to the fact that there could be invalid characters or values
    in columns, missing data, missing or additional delimiters, and so on. We noted
    that various values were enclosed in double-quotes ("). In Hive, we can ignore
    the quotes by specifying the `quoteChar` property whilst creating the table. Since
    Linux also offers simple and easy ways to remove such characters, we used `sed`
    to remove the quotation marks:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理CSV文件。数据清洗是数据科学中非常重要的领域。在实践中，经常会收到需要清洗的文件。这是因为列中可能包含无效字符或值、缺失数据、缺少或额外的分隔符等。我们注意到各种值都用双引号（"）括起来。在Hive中，我们可以通过在创建表时指定`quoteChar`属性来忽略引号。由于Linux也提供了简单易行的方法来删除这些字符，我们使用`sed`来删除引号：
- en: '[PRE10]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Moreover, in our downloaded file, `oil.csv`, we observed that there were non-printable
    characters that could cause issues. We removed them by issuing the following command:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在我们下载的文件`oil.csv`中，我们观察到存在可能引起问题的不可打印字符。我们通过发出以下命令将它们删除：
- en: '[PRE11]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '(Source: [http://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix](http://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix))'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：[http://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix](http://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix)）
- en: 'Finally, we copied the new file (`oil_clean.csv`) to `oil.csv`. Since the `oil.csv`
    file already existed in the same folder, we were prompted with an overwrite message
    and we entered `yes`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将新文件（`oil_clean.csv`）复制到`oil.csv`。由于`oil.csv`文件已存在于同一文件夹中，我们收到了覆盖消息，我们输入`yes`：
- en: '[PRE12]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Log in to Cloudera Hue:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到Cloudera Hue：
- en: 'Click on Hue on the Bookmarks bar in the browser. This will bring up the Cloudera
    login screen. Log in using ID `cloudera` and password `cloudera`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器的书签栏中点击Hue。这将显示Cloudera登录界面。使用ID`cloudera`和密码`cloudera`登录：
- en: '![](img/e279c451-220f-434d-947b-c166b798f4cf.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e279c451-220f-434d-947b-c166b798f4cf.png)'
- en: 'Click on Hue from the drop-down menu on Quick Start at the top of the Hue login
    window:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Hue登录窗口的快速启动下拉菜单中选择Hue：
- en: '![](img/ada22afe-0776-4378-9f95-a056ff1077f5.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ada22afe-0776-4378-9f95-a056ff1077f5.png)'
- en: 'Create the table schema, load the CSV file, `oil.csv`, and view the records:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建表模式，加载CSV文件`oil.csv`，并查看记录：
- en: '[PRE13]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/7663083b-9243-4b24-ad98-cc522f0faae4.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7663083b-9243-4b24-ad98-cc522f0faae4.png)'
- en: Load the oil file.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载石油文件。
- en: Now that the table has been loaded into Hive, you can run miscellaneous Hive
    commands using HiveQL. A full set of these commands is available at [https://cwiki.apache.org/confluence/display/Hive/LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual).
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在表已加载到Hive中，您可以使用HiveQL运行各种Hive命令。这些命令的完整集合可在[https://cwiki.apache.org/confluence/display/Hive/LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)上找到。
- en: 'For instance, to find the maximum, minimum, and average value of oil prices
    in each country from 1980-2015 (the date range of the dataset), we can use familiar
    SQL operators. The query would be as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要查找1980-2015年（数据集的日期范围）每个国家的石油价格的最大值、最小值和平均值，我们可以使用熟悉的SQL运算符。查询如下：
- en: '[PRE14]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here is the screenshot of the same:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是相同的截图：
- en: '![](img/e414b22d-47af-4f7d-a052-474c1f7cf74d.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e414b22d-47af-4f7d-a052-474c1f7cf74d.png)'
- en: In similar ways, we can use an array of other SQL commands. The Hive Manual
    provides an in-depth look into these commands and the various ways data can be
    saved, queried, and retrieved.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以使用一系列其他SQL命令。Hive手册详细介绍了这些命令以及数据保存、查询和检索的各种方式。
- en: Hue includes a set of useful features such as data visualization, data download,
    and others that allow users to perform ad hoc analysis on the data.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Hue包括一系列有用的功能，如数据可视化、数据下载等，允许用户对数据进行临时分析。
- en: 'To access the visualization feature, click on the visualization icon underneath
    the grid icon in the results section, as shown in the following screenshot:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问可视化功能，请在结果部分的网格图标下方点击可视化图标，如下截图所示：
- en: '![](img/499fcfb2-f240-4123-97d1-2f1bb8dcdd62.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/499fcfb2-f240-4123-97d1-2f1bb8dcdd62.png)'
- en: 'Select Scatter. In Hue, this type of chart, also known more generally as a
    scatterplot, allows users to create multivariate charts very easily. Different
    values for the x and y axes, as well as scatter size and grouping, can be selected,
    as shown in the following screenshot:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 选择散点图。在Hue中，这种类型的图表，也被更普遍地称为散点图，允许用户非常容易地创建多变量图表。可以选择x和y轴的不同数值，以及散点大小和分组，如下截图所示：
- en: '![](img/259f4b41-4364-4376-b13e-befe47bd30a6.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/259f4b41-4364-4376-b13e-befe47bd30a6.png)'
- en: 'The following is a simple pie chart that can be constructed by selecting Pie
    in the drop-down menu:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的饼图，可以通过在下拉菜单中选择饼图来构建：
- en: '![](img/6106cd02-6123-4778-9133-00a18ac7e5d1.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6106cd02-6123-4778-9133-00a18ac7e5d1.png)'
- en: Joining tables in Hive
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Hive中连接表格
- en: Hive supports advanced join functionalities. The following illustrates the process
    of using Left Join. As seen, the original table has data for each country represented
    by their three-letter country code. Since Hue supports map charts, we can add
    the values for latitude and longitude to overlay the oil pricing data on a world
    map.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Hive支持高级连接功能。以下是使用左连接的过程。如图所示，原始表格中有每个国家的数据，用它们的三字母国家代码表示。由于Hue支持地图图表，我们可以添加纬度和经度的数值，将石油定价数据叠加在世界地图上。
- en: 'To do so, we''ll need to download a dataset containing the values for latitude
    and longitude:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要下载一个包含纬度和经度数值的数据集：
- en: '[PRE15]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the file has been downloaded and cleansed, define the schema and load
    the data in Hive:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文件被下载和清理，就在Hive中定义模式并加载数据：
- en: '[PRE16]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/c66e5c65-998c-425f-b7d3-0bbcb229eebb.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c66e5c65-998c-425f-b7d3-0bbcb229eebb.png)'
- en: 'Join the oil data with the lat/long data:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 将石油数据与纬度/经度数据进行连接：
- en: '[PRE17]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/6ff9ec7c-634e-43a5-bc35-4187a92c525b.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ff9ec7c-634e-43a5-bc35-4187a92c525b.png)'
- en: We can now proceed with creating geospatial visualizations. It would be useful
    to bear in mind that these are preliminary visualizations in Hue that provide
    a very convenient means to view data. More in-depth visualizations can be developed
    on geographical data using shapefiles, polygons, and other advanced charting methods.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始创建地理空间可视化。请记住，这些是在Hue中提供非常方便的查看数据的初步可视化。可以使用shapefile、多边形和其他高级图表方法在地理数据上开发更深入的可视化。
- en: 'Select Gradient Map from the drop-down menu and enter the appropriate values
    to create the chart, as shown in the following figure:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 从下拉菜单中选择渐变地图，并输入适当的数值来创建图表，如下图所示：
- en: '![](img/2b47321a-b355-411b-b262-9a5ea24c77cd.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b47321a-b355-411b-b262-9a5ea24c77cd.png)'
- en: 'The next chart was developed using the Marker Map option in the drop-down menu.
    It uses the three-character country code in order to place markers and associated
    values on the respective regions, as shown in the following figure:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图表是使用下拉菜单中的标记地图选项开发的。它使用三个字符的国家代码来在相应的区域放置标记和相关数值，如下图所示：
- en: '![](img/08fb5f5c-9fd6-4352-9d0b-39d28dd86133.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08fb5f5c-9fd6-4352-9d0b-39d28dd86133.png)'
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter provided a technical overview of Hadoop. We discussed the core
    components and core concepts that are fundamental to Hadoop, such as MapReduce
    and HDFS. We also looked at the technical challenges and considerations of using
    Hadoop. While it may appear simple in concept, the inner workings and a formal
    administration of a Hadoop architecture can be fairly complex. In this chapter
    we highlighted a few of them.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了Hadoop的技术概述。我们讨论了Hadoop的核心组件和核心概念，如MapReduce和HDFS。我们还研究了使用Hadoop的技术挑战和考虑因素。虽然在概念上可能看起来很简单，但Hadoop架构的内部运作和正式管理可能相当复杂。在本章中，我们强调了其中一些。
- en: We concluded with a hands-on exercise on Hadoop using the Cloudera Distribution.
    For this tutorial, we used the CDH Virtual Machine downloaded earlier from Cloudera's
    website.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以使用Cloudera Distribution的Hadoop进行了实际操作的方式结束。对于本教程，我们使用了之前从Cloudera网站下载的CDH虚拟机。
- en: In the next chapter, we will look at NoSQL, an alternative or a complementary
    solution to Hadoop depending upon your individual and/or organization al needs.
    While Hadoop offers a far richer set of capabilities, if your intended use case(s)
    can be done with simply NoSQL solutions, the latter may be an easier choice in
    terms of the effort required.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看一下NoSQL，这是Hadoop的一个替代或补充解决方案，取决于您个人和/或组织的需求。虽然Hadoop提供了更丰富的功能集，但如果您的预期用例可以简单地使用NoSQL解决方案，后者可能是在所需的努力方面更容易的选择。
