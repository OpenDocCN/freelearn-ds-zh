- en: Chapter 1. Introduction to Data Analytics with Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。Spark数据分析简介
- en: 'This chapter covers an overview of Apache Spark, its computing paradigm, and
    installation to getting started. It will briefly describe the main components
    of Spark and focus on its new computing advancements. A description of the **Resilient
    Distributed Datasets** (**RDD**) and Dataset will be discussed as a base knowledge
    for the rest of this book. It will then focus on the Spark machine learning libraries.
    Installing and packaging a simple machine learning application with Spark and
    Maven will be demonstrated then before getting on board. In a nutshell, the following
    topics will be covered in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了Apache Spark、其计算范式和安装开始。它将简要描述Spark的主要组件，并专注于其新的计算进展。将讨论**弹性分布式数据集**（**RDD**）和数据集作为本书其余部分的基础知识。然后将专注于Spark机器学习库。然后演示如何安装和打包一个简单的机器学习应用程序与Spark和Maven。简而言之，本章将涵盖以下主题：
- en: Spark overview
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark概述
- en: New computing paradigm with Spark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有Spark的新计算范式
- en: Spark ecosystem
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark生态系统
- en: Spark machine learning libraries
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark机器学习库
- en: Installing and getting started with Spark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和开始使用Spark
- en: Packaging your application with dependencies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打包您的应用程序与依赖项
- en: Running a simple machine learning application
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行一个简单的机器学习应用程序
- en: Spark overview
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark概述
- en: 'This section describes Spark ([https://spark.apache.org/](https://spark.apache.org/))
    basics followed by the issues with the traditional parallel and distributed computing,
    then how Spark was evolved, and it then brings a new computing paradigm across
    the big data processing and analytics on top of that. In addition, we also presented
    some exciting features of Spark that easily attract the big data engineers, data
    scientists, and researchers, including:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了Spark（[https://spark.apache.org/](https://spark.apache.org/)）的基础知识，然后介绍了传统并行和分布式计算的问题，接着介绍了Spark的演变，以及它为大数据处理和分析带来了新的计算范式。此外，我们还介绍了一些令人兴奋的Spark功能，这些功能很容易吸引大数据工程师、数据科学家和研究人员，包括：
- en: Simplicity of data processing and computation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据处理和计算的简单性
- en: Speed of computation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算速度
- en: Scalability and throughput across large-scale datasets
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大规模数据集上的可伸缩性和吞吐量
- en: Sophistication across diverse data types
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对各种数据类型的复杂性
- en: Ease of cluster computing and deployment with different cluster managers
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的集群管理器轻松进行集群计算和部署
- en: Working capabilities and supports with various big data storage and sources
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与各种大数据存储和来源的工作能力和支持
- en: Diverse APIs are written in widely used and emerging programming languages
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广泛使用和新兴编程语言编写的多样化API
- en: Spark basics
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark基础知识
- en: 'Before praising Spark and its many virtues, a short overview is in the mandate.
    Apache Spark is a fast, in-memory, big data processing, and general-purpose cluster
    computing framework with a bunch of sophisticated APIs for advanced data analytics.
    Unlike the Hadoop-based MapReduce, which is only suited for batch jobs in speed
    and ease of use, Spark could be considered as a general execution engine that
    is suitable for applying advanced analytics on both static (batch) as well as
    real-time data:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在赞美Spark及其许多优点之前，有必要进行简要概述。Apache Spark是一个快速、内存中的大数据处理和通用集群计算框架，具有一系列复杂的高级API，用于高级数据分析。与基于Hadoop的MapReduce只适用于批处理作业的速度和易用性不同，Spark可以被认为是一个适用于对静态（批处理）和实时数据应用高级分析的通用执行引擎：
- en: Spark was originally developed at the University of California, Berkeley's AMPLab
    based on **Resilient Distributed Datasets** (**RDDs**), which provides a fault-tolerant
    abstraction for in-memory cluster computing facilities. However, later on Spark's
    code base was bequeathed to the Apache Software Foundation making it open source,
    since then open source communities are taking care of it. Spark provides an interface
    to perform data analytics on entire clusters at scale with implicit data parallelism
    and fault-tolerance through its high-level APIs written in Java, Scala, Python,
    and R.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark最初是在加州大学伯克利分校的AMPLab基于**弹性分布式数据集**（**RDDs**）开发的，它为内存集群计算设施提供了容错抽象。然而，后来Spark的代码库被捐赠给了Apache软件基金会，使其成为开源，自那时起，开源社区一直在照顾它。Spark提供了一个接口，通过其高级API（Java、Scala、Python和R编写）在整个集群上以规模执行数据分析，具有隐式数据并行性和容错性。
- en: 'In Spark 2.0.0, elevated libraries (most widely used data analysis algorithms)
    are implemented, including:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0.0中，实现了提升的库（最广泛使用的数据分析算法），包括：
- en: Spark SQL for querying and processing large-scale structured data
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于查询和处理大规模结构化数据的Spark SQL
- en: SparkR for statistical computing that provides distributed computing using programming
    language R at scale
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkR用于统计计算，使用R语言进行分布式计算规模化
- en: MLlib for machine learning (ML) applications, which is internally divided into
    two parts; MLlib for RDD-based machine learning application development and Spark
    ML for a high-level abstraction to develop complete computational data science
    and machine learning workflows
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLlib用于机器学习（ML）应用程序，内部分为两部分；MLlib用于基于RDD的机器学习应用程序开发和Spark ML用于开发完整的计算数据科学和机器学习工作流的高级抽象
- en: GraphX for large-scale graph data processing
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于大规模图形数据处理的GraphX
- en: Spark Streaming for handling large-scale real-time streaming data to provide
    a dynamic working environment to static machine learning
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming用于处理大规模实时流数据，为静态机器学习提供动态工作环境
- en: 'Since its first stable release, Spark has already experienced dramatic and
    rapid development as well as wide adoptions through active initiatives from a
    wide range of IT solution providers, open source communities, and researchers
    around the world. Recently it has emerged as one of the most active, and the largest
    open source project in the area of big data processing and cluster computing,
    not only for its comprehensive adoptions, but also deployments and surveys by
    IT peoples, data scientists, and big data engineers worldwide. As quoted by *Matei
    Zaharia*, founder of Spark and the CTO of *Databricks* on the *Big Data analytics*
    news website at: [http://bigdataanalyticsnews.com/apache-spark-3-real-world-use-cases/](http://bigdataanalyticsnews.com/apache-spark-3-real-world-use-cases/):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自其首个稳定版本发布以来，Spark已经经历了戏剧性和迅速的发展，并得到了全球范围内各种IT解决方案提供商、开源社区和研究人员的积极倡导。最近，它已成为大数据处理和集群计算领域最活跃、最大的开源项目之一，不仅因为其广泛的采用，还因为全球范围内IT人员、数据科学家和大数据工程师对其部署和调查。正如Spark的创始人、Databricks的CTO
    Matei Zaharia在*Big Data analytics*新闻网站上所说：
- en: '*It''s an interesting thing. There hasn''t been as much noise about it commercially,
    but the actual developer community votes with its feet and people are actually
    getting things done and working with the project.*'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*这是一件有趣的事情。在商业上并没有引起太多噪音，但实际的开发者社区通过实际行动投票，人们实际上正在完成工作并与项目合作。*'
- en: Even though many Tech Giants such as Yahoo, Baidu, Conviva, ClearStory, Hortonworks,
    Gartner, and Tencent are already using Spark in production - on the other hand,
    IBM, DataStax, Cloudera, and BlueData provide the commercialized Spark distribution
    for the enterprise. These companies have enthusiastically deployed Spark applications
    at a massive scale collectively for processing multiple petabytes of data on clusters
    of 8,000 nodes, which is the largest known cluster of Spark.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多科技巨头如雅虎、百度、Conviva、ClearStory、Hortonworks、Gartner和腾讯已经在生产中使用Spark，另一方面，IBM、DataStax、Cloudera和BlueData为企业提供了商业化的Spark分发。这些公司已经热情地在规模庞大的集群上部署了Spark应用程序，共同处理了数百PB的数据，这是已知的最大的Spark集群。
- en: Beauties of Spark
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark的优点
- en: Are you planning to develop a machine learning (ML) application? If so, you
    probably already have some data to perform preprocessing before you train a model
    on that data, and finally, you will be using the trained model to make predictions
    on new data to see the adaptability. That's all you need? We guess no, since you
    have to consider other parameters as well. Obviously, you will desire your ML
    models to be working perfectly in terms of accuracy, execution time, memory usage,
    throughput, tuning, and adaptability. Wait! Still not done yet; what happens if
    you would like to make your application handle large training and new datasets
    at scale? Or as a data scientist, what if you could build your ML models to overcome
    these issues as a multi-step journey from data incorporation through train and
    error to production by running the same machine learning code on the big cluster
    and the personal computer without breaking down further? You can simply rely on
    Spark and close your eyes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您计划开发机器学习（ML）应用程序吗？如果是这样，您可能已经有一些数据在训练模型之前进行预处理，最终，您将使用训练好的模型对新数据进行预测以查看适应性。这就是您需要的全部吗？我们猜想不是，因为您还必须考虑其他参数。显然，您希望您的ML模型在准确性、执行时间、内存使用、吞吐量、调整和适应性方面都能完美运行。等等！还没有结束；如果您希望您的应用程序能够处理大规模的训练和新数据集呢？或者作为数据科学家，如果您可以构建您的ML模型，以克服这些问题，从数据整合到训练和错误再到生产的多步旅程，通过在大集群和个人计算机上运行相同的机器学习代码而不会进一步崩溃？您可以简单地依靠Spark并闭上眼睛。
- en: 'Spark has several advantages over other big data technologies such as MapReduce
    (you can refer to [https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html)
    for MapReduce tutorials and the research paper *MapReduce: Simplified Data Processing
    on Large Clusters, Jeffrey Dean et al, In proc of OSDI, 2004* to get to know more)
    and Storm, which is a free and open source distributed real-time computation system
    (please refer to [http://storm.apache.org/](http://storm.apache.org/) for more
    on Storm-based distributed computing). First of all, Spark gives a comprehensive,
    unified engine to manage big data processing requirements with a variety of datasets
    such as text and tabular to graph data as well as the source of data (batch and
    real-time streaming data) that are diverse in nature. As a user (data science
    engineers, academicians, or developers), you can be likely benefited from Spark''s
    rapid application development through simple and easy-to-understand APIs across
    batches, interactive, and real-time streaming applications.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Spark相对于其他大数据技术（如MapReduce和Storm）具有几个优势。首先，Spark提供了一个全面统一的引擎，以满足各种数据集（文本、表格、图形数据）和数据源（批处理和实时流数据）的大数据处理需求。作为用户（数据科学工程师、学者或开发人员），您可能会从Spark的快速应用程序开发中受益，因为它具有简单易懂的API，可用于批处理、交互和实时流应用程序。
- en: Working and programming with Spark is easy and simple. Let us show you an example
    of that. Yahoo is one of the contributors and an early adopter of Spark, who implemented
    an ML algorithm with 120 lines of Scala code. With just 30 minutes of training
    on a large dataset with a hundred million records, the Scala ML algorithm was
    ready for business. Surprisingly, the same algorithm was written using C++ in
    15,000 lines of code previously (please refer to the following URL for more at: [https://www.datanami.com/2014/03/06/apache_spark_3_real-world_use_cases/](https://www.datanami.com/2014/03/06/apache_spark_3_real-world_use_cases/)).
    You can develop your applications using Java, Scala, R, or Python with a built-in
    set of over 100 high-level operators (mostly supported after Spark release 1.6.1)
    for transforming datasets and getting the familiarity with the data frame APIs
    for manipulating semi-structured, structured, and streaming data. In addition
    to the Map and Reduce operations, it supports SQL queries, streaming data, machine
    learning, and graph data processing. Moreover, Spark also provides an interactive
    shell written in Scala and Python for executing your codes sequentially (such
    as SQL or R style).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark进行工作和编程是简单易行的。让我们来展示一个例子。雅虎是Spark的贡献者和早期采用者之一，他们用120行Scala代码实现了一个ML算法。仅仅30分钟的大型数据集训练，包含1亿条记录，Scala
    ML算法就准备好投入使用了。令人惊讶的是，之前使用C++编写相同算法需要15000行代码（请参考以下网址获取更多信息：[https://www.datanami.com/2014/03/06/apache_spark_3_real-world_use_cases/](https://www.datanami.com/2014/03/06/apache_spark_3_real-world_use_cases/)）。您可以使用Java、Scala、R或Python开发您的应用程序，并使用100多个高级操作符（大多数在Spark
    1.6.1发布后支持）来转换数据集，并熟悉数据框API，以操作半结构化、结构化和流数据。除了Map和Reduce操作，它还支持SQL查询、流数据、机器学习和图数据处理。此外，Spark还提供了用Scala和Python编写的交互式shell，用于顺序执行代码（如SQL或R风格）。
- en: 'The main reason Spark adopts so quickly is because of two main factors: speed
    and sophistication. Spark provides order-of-magnitude performance for many applications
    using coarse-grained, immutable, and sophisticated data called Resilient Distributed
    Datasets that are spread across the cluster and that can be stored in memory or
    disks. An RDD offers fault-tolerance, which is resilient in a sense that it cannot
    be changed once created. Moreover, Spark''s RDD has the property of recreating
    from its lineage if it is lost in the middle of computation. Furthermore, the
    RDD can be distributed automatically across the clusters by means of partitions
    and it holds your data. You can also keep it on your data on memory by the caching
    mechanism of Spark, and this mechanism enables big data applications in Hadoop-based
    MapReduce clusters to execute up to 100 times faster for in-memory if executed
    iteratively and even 10 times faster for disk-based operation.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Spark之所以迅速被采用的主要原因是因为两个主要因素：速度和复杂性。Spark为许多应用程序提供了数量级的性能，使用粗粒度、不可变和复杂的数据，称为弹性分布式数据集，这些数据集分布在集群中，并且可以存储在内存或磁盘中。RDD提供了容错性，即一旦创建就无法更改。此外，Spark的RDD具有从其血统中重新创建的属性，如果在计算过程中丢失，它可以重新创建。此外，RDD可以通过分区自动分布到集群中，并且可以保存您的数据。您还可以通过Spark的缓存机制将数据保存在内存中，并且这种机制使得基于Hadoop的MapReduce集群中的大数据应用程序在内存中执行时速度提高了100倍，甚至在基于磁盘的操作中提高了10倍。
- en: Let's look at a surprising statistic about Spark and its computation powers.
    Recently, Spark took over Hadoop-based MapReduce by completing the 2014 Gray Sort
    Benchmark in the 100 TB category, which is an industry benchmark on how fast a
    system can sort 100 TB of data (1 trillion records) (please refer to [http://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html](http://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html)
    and [http://sortbenchmark.org/](http://sortbenchmark.org/)). Finally, it becomes
    the open source engine (please refer to the following URL for more information
    [https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html](https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html))
    for sorting at petabyte scale. In comparison, the previous world record set by
    Hadoop MapReduce had to use 2100 machines, taking 72 minutes of execution time,
    which implies Spark sorted the same data three times faster using 10 times fewer
    machines. Moreover, you can combine multiple libraries seamlessly to develop large-scale
    machine learning and data analytics pipelines to execute the job on various cluster
    managers such as Hadoop YARN, Mesos, or in the cloud by accessing data storage
    and sources such as **HDFS**, **Cassandra**, **HBase**, **Amazon S3**, or even
    **RDBMs**. Moreover, the job can be executed as a standalone mode on a local PC
    or cluster, or even on **AWS EC2**. Therefore, deployment of a Spark application
    on the cluster is easy (we will show more on how to deploy a Spark application
    on the cluster later in this chapter).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下关于Spark及其计算能力的一个令人惊讶的统计数据。最近，Spark通过完成2014年Gray Sort Benchmark中的100 TB类别，取代了基于Hadoop的MapReduce，这是一个关于系统能够多快地对100
    TB数据（1万亿条记录）进行排序的行业基准（请参考[http://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html](http://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html)和[http://sortbenchmark.org/](http://sortbenchmark.org/)）。最终，它成为了用于对PB级数据进行排序的开源引擎（请参考以下网址获取更多信息[https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html](https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html)）。相比之下，之前由Hadoop
    MapReduce创下的世界纪录需要使用2100台机器，执行时间为72分钟，这意味着Spark使用了10倍少的机器，以三倍的速度对相同的数据进行了排序。此外，您可以无缝地结合多个库来开发大规模机器学习和数据分析管道，以在各种集群管理器（如Hadoop
    YARN、Mesos或通过访问数据存储和源（如HDFS、Cassandra、HBase、Amazon S3甚至RDBMs）在云中执行作业。此外，作业可以作为独立模式在本地PC或集群上执行，甚至在AWS
    EC2上执行。因此，将Spark应用程序部署到集群上非常容易（我们将在本章后面展示如何在集群上部署Spark应用程序）。
- en: 'The other beauties of Spark are: it is open source and platform independent.
    These two are also its greatest advantage, which is it''s free to use, distribute,
    and modify and develop an application on any platform. An open source project
    is also more secure as the code is accessible to everyone and anyone can fix bugs
    as they are found. Consequently, Spark has evolved so rapidly that it has become
    the largest open source project concerning big data solutions with 750+ contributors
    from 200+ organizations.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的其他优点是：它是开源的，平台无关的。这两点也是它的最大优势，即可以免费使用、分发和修改，并在任何平台上开发应用程序。开源项目也更安全，因为代码对每个人都是可访问的，任何人都可以在发现错误时修复错误。因此，Spark发展得如此迅速，以至于它已成为涉及大数据解决方案的最大开源项目，拥有来自200多个组织的750多名贡献者。
- en: New computing paradigm with Spark
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有Spark的新计算范式
- en: In this section, we will show a chronology of Spark that will provide a concept
    of how it was evolved and emerged as a revolution for big data processing and
    cluster computing. In addition to this, we will also describe the Spark ecosystem
    in brief to understand the features and facilities of Spark in more details.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示Spark的发展历程，以及它如何成为大数据处理和集群计算的革命。除此之外，我们还将简要描述Spark生态系统，以更详细地了解Spark的特点和功能。
- en: Traditional distributed computing
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统分布式计算
- en: 'The traditional data processing paradigm is commonly referred to as a client-server
    model, which people used to move data to the code. The database server (or simply
    the server) was mainly responsible for performing data operations and then returning
    the results to the client-server (or simply the client) program. However, when
    the number of task to be computed is increased, a variety of operations and client
    devices also started to increase exponentially. As a result, a progressively complex
    array of computing endpoint in servers also started in the background. So to keep
    this type of computing model we need to increase the application (client) servers
    and database server in balance for storing and processing the increased number
    of operations. Consequently, the data propagation between nodes and data transfer
    back and forth across this network also increases drastically. Therefore, the
    network itself becomes a performance bottleneck. As a result, the performance
    (in terms of both the scalability and throughput) in this kind of computing paradigm
    also decreases undoubtedly. It is shown in the following diagram:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的数据处理范式通常被称为客户端-服务器模型，人们过去常常将数据移动到代码中。数据库服务器（或简称服务器）主要负责执行数据操作，然后将结果返回给客户端-服务器（或简称客户端）程序。然而，当需要计算的任务数量增加时，各种操作和客户端设备也开始呈指数级增长。因此，服务器中的计算端点也开始逐渐复杂起来。因此，为了保持这种计算模型，我们需要增加应用程序（客户端）服务器和数据库服务器的平衡，以存储和处理增加的操作数量。因此，节点之间的数据传播和网络中来回传输的数据也急剧增加。因此，网络本身成为性能瓶颈。因此，在这种计算范式中，性能（无论是可伸缩性还是吞吐量）也无疑会下降。如下图所示：
- en: '![Traditional distributed computing](img/00124.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![传统分布式计算](img/00124.jpeg)'
- en: 'Figure 1: Traditional distributed processing in action.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：传统分布式处理的实际应用。
- en: After the successful completion of human genome projects in life sciences, real-time
    IOT data, sensor data, data from mobile devices, and data from the Web are creating
    the data-deluge and contributing for the big data, which has mostly evolved the
    data-intensive computing. The data-intensive computing nowadays is now flattering
    increasingly in an emerging way, which requires an integrated infrastructure or
    computing paradigm, so that the computational resources and data could be brought
    in a common platform and perform the analytics on top of it. The reasons are diverse
    because big data is really huge in terms of complexity (**volume**, **variety**,
    and **velocity**), and from the operational perspective four ms (that is, **move**,
    **manage**, **merge**, and **munge**).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在生命科学中成功完成人类基因组计划后，实时物联网数据、传感器数据、移动设备数据和网络数据正在创造数据洪流，并为大数据做出贡献，这在很大程度上推动了数据密集型计算的发展。如今，数据密集型计算正在以一种新兴的方式不断发展，这需要一个集成的基础设施或计算范式，以便将计算资源和数据带入一个共同的平台，并在其上进行分析。原因是多样的，因为大数据在复杂性（**容量**、**多样性**和**速度**）方面确实非常庞大，从操作角度来看，还有四个ms（即**移动**、**管理**、**合并**和**整理**）。
- en: In addition, since we will be talking about large-scale machine learning applications
    in this book, we also need to consider some addition and critical assessing parameters
    such as validity, veracity, value, and visibility to grow the business. Visibility
    is important, because suppose you have a big dataset with a size of 1 PB; however,
    if there is no visibility, everything is a black hole. We will explain more on
    big data values in upcoming chapters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于本书将讨论大规模机器学习应用程序，我们还需要考虑一些额外的关键评估参数，如有效性、真实性、价值和可见性，以促进业务增长。可见性很重要，因为假设你有一个大小为1PB的大数据集；但是如果没有可见性，一切都是一个黑洞。我们将在接下来的章节中更详细地解释大数据价值。
- en: It may not be feasible to store and process these large-scale and complex big
    datasets in a single system; therefore, they need to be partitioned and stored
    across multiple physical machines. Well, big datasets are partitioned or distributed,
    but to process and analyze these rigorously complex datasets, both the database
    servers as well as application servers might need to be increased to intensify
    the processing power at a large-scale. Again, the same performance bottleneck
    issues arrive at worst in multi-dimension that requires a new and more data-intensive
    big data processing and related computing paradigm.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个系统中存储和处理这些大规模和复杂的大型数据集可能是不可行的；因此，它们需要被分区并存储在多台物理机器上。大型数据集被分区或分布，但为了处理和分析这些严格复杂的数据集，数据库服务器和应用程序服务器可能需要增加，以加强大规模的处理能力。同样，多维度中出现的性能瓶颈问题最糟糕，需要一种新的、更数据密集的大数据处理和相关计算范式。
- en: Moving code to the data
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将代码移动到数据
- en: 'To overcome the issues mentioned previously, a new computing paradigm is desperately
    needed so that instead of moving data to the code/application, we could move the
    code or application to the data and perform the data manipulation, processing,
    and associated computing at home (that is, where the data is stored). As you understand
    the motivation and objective, now the reverts programming model can be called
    **move code to data and do parallel processing on distributed system**, which
    can be visualized in the following diagram:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服先前提到的问题，迫切需要一种新的计算范式，这样我们就可以将代码或应用程序移动到数据中，执行数据操作、处理和相关计算（也就是说，数据存储的地方）。由于您已经了解了动机和目标，现在可以将反转的编程模型称为**将代码移动到数据并在分布式系统上进行并行处理**，可以在以下图表中可视化：
- en: '![Moving code to the data](img/00111.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![将代码移动到数据](img/00111.jpeg)'
- en: 'Figure 2: New computing (move code to data and do parallel processing on distributed
    system).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：新的计算（将代码移动到数据并在分布式系统上进行并行处理）。
- en: 'To understand the workflows illustrated in *Figure 2*, we can envisage a new
    programming model described as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解*图2*中所示的工作流程，我们可以设想一个新的编程模型，描述如下：
- en: Execution of a big data processing using your application initiated at your
    personal computer (let's name it **Driver Program**), which coordinates the execution
    in action remotely across multiple computing nodes within a cluster or grid, or
    a more openly speaking cloud.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用您在个人计算机上启动的应用程序执行大数据处理（让我们称之为**驱动程序**），它在集群、网格或更开放地说是云中远程协调执行。
- en: Now what you need is to transfer your developed application/algorithm/code segments
    (could be invoked or revoked using command-line or shell scripting as a simple
    programming language notation) to the computing/worker nodes (having large storage,
    main memory, and processing capability). We can simply imagine that the data to
    be computed or manipulated is already stored in those computing nodes as partitions
    or blocks.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在你需要做的是将开发的应用程序/算法/代码段（可以使用命令行或shell脚本调用或撤销）转移到具有大容量存储、主存储器和处理能力的计算/工作节点。我们可以简单地想象要计算或操作的数据已经存储在这些计算节点中，作为分区或块。
- en: It is also understandable that the bulk data no longer needs to be transferred
    (upload/download) to your driver program because of the network or computing bottleneck,
    but it only holds the data reference in its variable instead, which is basically
    an address (hostname/IP address with a port) to locate the physical data stored
    in the computing nodes in a cluster, for example (of course bulk-upload could
    be performed using other solutions, such as scalable provisioning that is to be
    discussed in later chapters).
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以理解的是，由于网络或计算瓶颈，大容量数据不再需要传输（上传/下载）到驱动程序，而是仅在其变量中保存数据引用，基本上是一个地址（主机名/IP地址和端口），用于在集群中定位存储在计算节点中的物理数据（当然，也可以使用其他解决方案进行大容量上传，例如可扩展的配置，将在后面的章节中讨论）。
- en: So what do the remote computing nodes have? They have the data as well as code
    to perform the data computations and necessary processing to materialize the output
    or modified data without leaving their home (more technically, the computing nodes).
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那么远程计算节点有什么？它们既有数据，也有执行数据计算和必要处理以实现输出或修改数据的代码，而不离开它们的家（更准确地说是计算节点）。
- en: Finally, upon your request, only the results could be transferred across the
    network to your driver program for validation or other analytics since there are
    many subsets of the original datasets.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，根据您的请求，只有结果可以通过网络传输到您的驱动程序进行验证或其他分析，因为原始数据集有许多子集。
- en: It's worth noticing that by moving the code to the data, the computing structure
    has been changed drastically. Most interestingly, the volume of data transfer
    across the network has significantly reduced. The justification here is that you
    will be transferring only a small chunk of software code to the computing nodes
    and receiving a small subset of the original data as results in return. This was
    the most important paradigm shifting for big data processing that Spark brought
    to us with the concept of RDD, datasets, DataFrame, and other lucrative features
    that imply great revolution in the history of big data engineering and cluster
    computing. However, for brevity, in the next section we will only discuss the
    concepts of RDD and the other computing features will be discussed in upcoming
    sections
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，通过将代码移动到数据，计算结构已经发生了巨大变化。最有趣的是，网络传输的数据量显著减少。这里的理由是，您只会将一小部分软件代码传输到计算节点，并收到原始数据的一个小子集作为返回的结果。这是Spark为我们带来的大数据处理最重要的范式转变，它引入了RDD、数据集、DataFrame和其他有利特性，这意味着在大数据工程和集群计算历史上有着巨大的革命。然而，为了简洁起见，下一节我们将只讨论RDD的概念，其他计算特性将在接下来的章节中讨论。
- en: RDD – a new computing paradigm
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RDD - 一种新的计算范式
- en: To understand the new computing paradigm, we need to understand the concept
    of **Resilient Distributed Datasets** (**RDDs**), by which and how Spark has implemented
    the data reference concept. As a result, it has been able to shift the data processing
    easily to take it at scale. The basic thing about RDD is that it helps you to
    treat your input datasets almost like any other data objects. In other words,
    it brings the diversity of input data types, which you greatly missed in the Hadoop-based
    MapReduce framework.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解新的计算范式，我们需要了解**弹性分布式数据集**（**RDDs**）的概念及Spark如何实现数据引用的概念。因此，它已经能够轻松地将数据处理扩展。RDD的基本特点是它帮助您几乎像处理任何其他数据对象一样处理输入数据集。换句话说，它带来了输入数据类型的多样性，这是您在基于Hadoop的MapReduce框架中极度缺失的。
- en: An RDD provides the fault-tolerance capability in a resilient way in a sense
    that it cannot be changed once created and the Spark engine will try to iterate
    the operation once failed. It is distributed because once it has created performed
    partition operations, RDDs are automatically distributed across the clusters by
    means of partitions. RDDs let you play more with your input datasets since RDDs
    can also be transformed into other forms rapidly and robustly. In parallel, RDDs
    can also be dumped through an action and shared across your applications that
    are logically co-related or computationally homogeneous. This is achievable because
    it is a part of Spark's general-purpose execution engine to gain massive parallelism,
    so it can virtually be applied in any type of datasets.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: RDD以一种弹性的方式提供了容错能力，一旦创建就无法更改，Spark引擎将尝试在操作失败时迭代操作。它是分布式的，因为一旦执行了分区操作，RDD会自动通过分区在集群中分布。RDD允许您更多地处理输入数据集，因为RDD也可以快速而稳健地转换为其他形式。同时，RDD也可以通过操作转储并在逻辑上相关或计算上同质的应用程序之间共享。这是因为它是Spark通用执行引擎的一部分，可以获得大规模的并行性，因此几乎可以应用于任何类型的数据集。
- en: However, to make the RDD and related operation on your inputs, Spark engines
    require you to make a distinguishing borderline between the data pointer (that
    is, the reference) and the input data itself. Basically, your driver program will
    not hold data, but only the reference of the data where the data is actually located
    on the remote computing nodes in a cluster.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了在输入数据上进行RDD和相关操作，Spark引擎要求您在数据指针（即引用）和输入数据本身之间建立明显的界限。基本上，您的驱动程序不会保存数据，而只会保存数据的引用，数据实际上位于集群中的远程计算节点上。
- en: 'To make the data processing faster and easier, Spark supports two types of
    operations, which can be performed on RDDs: transformations and actions (please
    refer to *Figure 3*). A transformation operation basically creates a new dataset
    from an existing one. An action, on the other hand, materializes a value to the
    driver program after a successful computation on input datasets on the remote
    server (computing nodes to be more exact).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使数据处理更快、更容易，Spark支持可以在RDD上执行的两种操作：转换和操作（请参考*图3*）。转换操作基本上是从现有数据集创建一个新数据集。另一方面，操作在成功计算远程服务器上的输入数据集后，将一个值实现为驱动程序（更确切地说是计算节点）。
- en: 'The style of data execution initiated by the driver program builds up a graph
    as a **Directed Acyclic Graph** (**DAG**) style; where nodes represent RDDs and
    the transformation operations are represented by the edges. However, the execution
    itself does not start in the computing nodes in a Spark cluster until an action
    operation is performed. Nevertheless, before starting the operation, the driver
    program sends the execution graph (that signifies the style of operation for the
    data computation pipelining or workflows) and the code block (as a domain-specific
    script or file) to the cluster and each worker/computing node receives a copy
    from the cluster manager node:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由驱动程序启动的数据执行方式构建了一个**有向无环图**（**DAG**）样式的图表；其中节点表示RDD，转换操作由边表示。然而，执行本身直到执行操作之前不会在Spark集群中的计算节点中开始。然而，在开始操作之前，驱动程序将执行图（表示数据计算流水线或工作流的操作方式）和代码块（作为特定领域的脚本或文件）发送到集群，每个工作节点/计算节点从集群管理节点接收一个副本：
- en: '![RDD – a new computing paradigm](img/00113.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![RDD-一种新的计算范式](img/00113.jpeg)'
- en: 'Figure 3: RDD in action (transformation and action operation).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：RDD的操作（转换和操作操作）。
- en: Before proceeding to the next section, we argue you to learn about the action
    and transformation operation in more detail. Although we will discuss these two
    operations in [Chapter 3](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 3. Understanding the Problem by Understanding the Data"), *Understanding
    the Problem by Understanding the Data* in detail. There are two types of transformation
    operations currently supported by Spark. The first one is the narrow transformation,
    where data mingle is unnecessary. Typical Spark narrow transformation operations
    are performed using the `filter()`, `sample()`, `map()`, `flatMap()`, `mapPartitions()`
    , and other methods. The wide transformation is essential to make a wider change
    to your input datasets so that the data could be brought in a common node out
    of multiple partitions of data shuffling. Wide transformation operations include
    `groupByKey()`, `reduceByKey()`, `union()`, `intersection()`, `join()`, and so
    on.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续下一节之前，我们建议您更详细地了解操作和转换操作。虽然我们将在[第3章](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "第3章。通过了解数据来理解问题")中详细讨论这两种操作，但目前Spark支持两种类型的转换操作。第一种是窄转换，其中数据混合是不必要的。典型的Spark窄转换操作使用`filter()`、`sample()`、`map()`、`flatMap()`、`mapPartitions()`等方法进行。宽转换对于对输入数据集进行更广泛的更改是必不可少的，以便将数据从多个数据分区中的共同节点中带出。宽转换操作包括`groupByKey()`、`reduceByKey()`、`union()`、`intersection()`、`join()`等。
- en: 'An action operation returns the final results of RDD computations from the
    transformation by triggering execution as a **Directed Acyclic Graph** (**DAG**)
    style to the Driver Program. But the materialized results are actually written
    on the storage, including the intermediate transformation results of the data
    objects and return the final results. Common actions include: `first()`, `take()`,
    `reduce()`, `collect()`, `count()`, `saveAsTextFile()`, `saveAsSequenceFile()`,
    and so on. At this point we believe that you have gained the basic operation on
    top of RDDs, so we can now define an RDD and related programs in a natural way.
    A typical RDD programming model that Spark provides can be described as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 动作操作通过触发执行作为**有向无环图**（**DAG**）样式返回RDD计算的最终结果到驱动程序。但实际上，材料化的结果实际上是写在存储中的，包括数据对象的中间转换结果，并返回最终结果。常见的动作包括：`first()`、`take()`、`reduce()`、`collect()`、`count()`、`saveAsTextFile()`、`saveAsSequenceFile()`等。在这一点上，我们相信您已经掌握了RDD的基本操作，因此我们现在可以以自然的方式定义RDD和相关程序。Spark提供的典型RDD编程模型可以描述如下：
- en: From an environment variable, Spark Context (Spark shell or Python Pyspark provides
    you with a Spark Context or you can make your own, this will be described later
    in this chapter) creates an initial data reference RDD object.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从环境变量中，Spark上下文（Spark shell或Python Pyspark为您提供了一个Spark上下文，或者您可以自己创建，这将在本章后面描述）创建一个初始数据引用RDD对象。
- en: Transform the initial RDD to create more RDDs objects following the functional
    programming style (to be discussed later on).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过转换初始RDD以创建更多的RDD对象，遵循函数式编程风格（稍后将讨论）。
- en: Send the code/algorithms/applications from the driver program to the cluster
    manager nodes. Then the cluster manager provides a copy to each computing node.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将代码/算法/应用程序从驱动程序发送到集群管理器节点。然后集群管理器为每个计算节点提供一个副本。
- en: Computing nodes hold a reference of the RDDs in its partition (again, the driver
    program also holds a data reference). However, computing nodes could have the
    input dataset provided by the cluster manager as well.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算节点在其分区中保存RDD的引用（同样，驱动程序也保存数据引用）。然而，计算节点也可以由集群管理器提供输入数据集。
- en: After a transformation (via either narrow or wider transformation), the result
    to be generated is a brand new RDD, since the original one will not be mutated.
    Finally, the RDD object or more (specifically data reference) is materialized
    through an action to dump the RDD into the storage.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在转换之后（通过窄转换或宽转换），生成的结果将是全新的RDD，因为原始的RDD不会被改变。最后，通过动作将RDD对象或更多（具体数据引用）实现为将RDD转储到存储中。
- en: The Driver Program can request the computing nodes for a chunk of results for
    the analysis or visualization of a program.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动程序可以请求计算节点为程序的分析或可视化结果请求一部分结果。
- en: Wait! So far we have moved smoothly. We suppose you will ship your application
    code to the computing nodes in the cluster. Still you will have to upload or send
    the input datasets to the cluster to be distributed among the computing nodes.
    Even during the bulk-upload you will have to transfer the data across the network.
    We also argue that the size of the application code and results are negligible
    or trivial. Another obstacle is if you/we want Spark to process the data at scale
    computation, it might require data objects to be merged from multiple partitions
    first. That means we will need to shuffle data among the worker/computing nodes
    that are usually done by `partition()`, `intersection()`, and `join()` transformation
    operations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！到目前为止，我们一切顺利。我们假设您将把应用程序代码发送到集群中的计算节点。但是您仍然需要上传或发送输入数据集到集群中以分发给计算节点。即使在大量上传期间，您也需要通过网络传输数据。我们还认为应用程序代码和结果的大小是可以忽略不计的。另一个障碍是，如果您/我们希望Spark进行规模计算的数据处理，可能需要首先从多个分区合并数据对象。这意味着我们需要在工作节点/计算节点之间进行数据洗牌，通常通过`partition()`、`intersection()`和`join()`转换操作来完成。
- en: 'So frankly speaking, the data transfer has not been eliminated completely.
    As we and you understand the overheads being contributed especially for the bulk
    upload/download of these operations, their corresponding outcomes are as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 坦率地说，数据传输并没有完全消除。正如我们和你理解的那样，特别是对于这些操作的大量上传/下载所贡献的开销，它们对应的结果如下：
- en: '![RDD – a new computing paradigm](img/00062.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![RDD - 一种新的计算范式](img/00062.jpeg)'
- en: 'Figure 4: RDD in action (the caching mechanism).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：RDD的操作（缓存机制）。
- en: Well, it's true that we have been compromised with these burdens. However, situations
    could be tackled or reduced significantly using the caching mechanism of Spark.
    Imagine you are going to perform an action multiple times on the same RDD objects,
    which have a long lineage; this will cause an increase in execution time as well
    as data movement inside a computing node. You can remove (or at least reduce)
    this redundancy with the caching mechanism of Spark (*Figure 4*) that stores the
    computed result of the RDD in the memory. This eliminates the recurrent computation
    every time. Because, when you cache on an RDD, its partitions are loaded into
    the main memory instead of a disk (however, if there is not enough space in the
    memory, the disk will be used instead) of the nodes that hold it. This technique
    enables big data applications on Spark clusters to outperform MapReduce significantly
    for each round of parallel processing. We will discuss more on Spark data manipulations
    and other techniques in [Chapter 3](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 3. Understanding the Problem by Understanding the Data"), *Understanding
    the Problem by Understanding the Data* in detail.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们已经受到了这些负担的影响是事实。然而，使用Spark的缓存机制可以显著减少或解决这些情况。想象一下，您将在相同的RDD对象上多次执行操作，这些对象具有很长的血统；这将导致执行时间的增加以及计算节点内部的数据移动。您可以使用Spark的缓存机制（*图4*）来消除（或至少减少）这种冗余，该机制将RDD的计算结果存储在内存中。这样就可以消除每次的重复计算。因为当您在RDD上进行缓存时，其分区将加载到主内存中，而不是节点的磁盘（但是，如果内存空间不足，将使用磁盘）。这种技术使得Spark集群上的大数据应用程序在每一轮并行处理中明显优于MapReduce。我们将在[第3章](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "第3章。通过了解数据来理解问题")中详细讨论Spark数据操作和其他技术，*通过了解数据来理解问题*。
- en: Spark ecosystem
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark生态系统
- en: 'To provide more enhancements and additional big data processing capabilities,
    Spark can be configured and run on top of existing Hadoop-based clusters. As already
    stated, although Hadoop provides the **Hadoop Distributed File System** (**HDFS**)
    for efficient and operational storing of large-scale data cheaply; however, MapReduce
    provides the computation fully disk-based. Another limitation of MapReduce is
    that; only simple computations can be executed with a high-latency batch model,
    or static data to be more specific. The core APIs in Spark, on the other hand,
    are written in Java, Scala, Python, and R. Compared to MapReduce, with the more
    general and powerful programming model, Spark also provides several libraries
    that are part of the Spark ecosystems for redundant capabilities in big data analytics,
    processing, and machine learning areas. The Spark ecosystem consists of the following
    components, as shown in *Figure 5*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更多增强和额外的大数据处理能力，Spark可以配置并在现有基于Hadoop的集群上运行。正如已经提到的，尽管Hadoop提供了**Hadoop分布式文件系统**（**HDFS**）以便廉价高效地存储大规模数据；然而，MapReduce提供的计算完全基于磁盘。MapReduce的另一个限制是；只能使用高延迟批处理模型执行简单计算，或者更具体地说是静态数据。另一方面，Spark的核心API是用Java、Scala、Python和R编写的。与MapReduce相比，Spark具有更通用和强大的编程模型，还提供了几个库，这些库是Spark生态系统的一部分，用于大数据分析、处理和机器学习领域的冗余功能。如*图5*所示，Spark生态系统包括以下组件：
- en: '![Spark ecosystem](img/00103.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![Spark生态系统](img/00103.jpeg)'
- en: 'Figure 5: Spark ecosystem (till date up to Spark 1.6.1).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：Spark生态系统（截至Spark 1.6.1）。
- en: As we have already stated, it is very much possible to combine these APIs seamlessly
    to develop large-scale machine learning and data analytics applications. Moreover,
    the job can be executed on various cluster managers such as Hadoop YARN, Mesos,
    standalone, or in the cloud by accessing data storage and sources such as HDFS,
    Cassandra, HBase, Amazon S3, or even RDBMs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经提到的，可以无缝地结合这些API来开发大规模的机器学习和数据分析应用程序。此外，可以通过访问HDFS、Cassandra、HBase、Amazon
    S3甚至RDBMs等数据存储和源，在各种集群管理器上执行作业，如Hadoop YARN、Mesos、独立或云端。
- en: Nevertheless, Spark is enriched with other features and APIs. For example, recently
    Cisco has announced to invest $150M in the Spark ecosystem towards Cisco Spark
    Hybrid Services ([http://www.cisco.com/c/en/us/solutions/collaboration/cloud-collaboration/index.html](http://www.cisco.com/c/en/us/solutions/collaboration/cloud-collaboration/index.html)).
    So Cisco Spark open APIs could boost its popularity with developers in higher
    cardinality (highly secure collaboration and connecting smartphone systems to
    the cloud). Beyond this, Spark has recently integrated Tachyon ([http://ampcamp.berkeley.edu/5/exercises/tachyon.html](http://ampcamp.berkeley.edu/5/exercises/tachyon.html)),
    a distributed in-memory storage system that economically fits in memory to further
    improve Spark's performance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Spark还具有其他功能和API。例如，最近思科宣布向Spark生态系统投资1.5亿美元，用于思科Spark混合服务（[http://www.cisco.com/c/en/us/solutions/collaboration/cloud-collaboration/index.html](http://www.cisco.com/c/en/us/solutions/collaboration/cloud-collaboration/index.html)）。因此，思科Spark开放API可以提高其在开发人员中的受欢迎程度（高度安全的协作和将智能手机系统连接到云端）。除此之外，Spark最近集成了Tachyon（[http://ampcamp.berkeley.edu/5/exercises/tachyon.html](http://ampcamp.berkeley.edu/5/exercises/tachyon.html)），这是一个分布式内存存储系统，可以经济地适应内存，进一步提高Spark的性能。
- en: Spark core engine
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark核心引擎
- en: Spark itself is written in Scala, which is functional, as well as **Object Oriented
    Programming Language** (**OOPL**) which runs on top of JVM. Moreover, as mentioned
    in *Figure 5*, Spark's ecosystem is built on top of the general and core execution
    engine, which has some extensible API's implemented in different languages. The
    lower level layer or upper level layer also uses the Spark core engine as a general
    execution job performing engine and it provides all other functionality on top.
    The Spark Core is written in Scala as already mentioned, and it runs on **Java
    Virtual Machine** (**JVM**) and the high-level APIs (that is, Spark MLlib, SparkR,
    Spark SQL, Dataset, DataFrame, Spark Streaming, and GraphX) that use the core
    in the execution time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Spark本身是用Scala编写的，它是一种功能性的面向对象编程语言，运行在JVM之上。此外，如图5所示，Spark的生态系统是建立在通用和核心执行引擎之上的，该引擎在不同语言中实现了一些可扩展的API。较低级别的层或较高级别的层也使用Spark核心引擎作为通用执行作业执行引擎，并在其上提供所有其他功能。Spark
    Core已经提到是用Scala编写的，并且在Java虚拟机上运行，高级API（即Spark MLlib、SparkR、Spark SQL、Dataset、DataFrame、Spark
    Streaming和GraphX）在执行时使用核心。
- en: Spark has brought the in-memory computing mode to a great visibility. This concept
    (in-memory computing) enables the Spark core engine to leverage speed through
    a generalized execution model to develop diverse applications.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Spark已经使内存计算模式得到了很大的可见度。这个概念（内存计算）使得Spark核心引擎能够通过通用执行模型来提高速度，从而开发多样化的应用程序。
- en: The low-level implementation of general purpose data computing and machine learning
    algorithms written in Java, Scala, R, and Python are easy to use for big data
    application development. The Spark framework is built on Scala, so developing
    ML applications in Scala can provide access to the latest features that might
    not be available in other Spark languages initially. However, that is not a big
    problem, open source communities also take care of the necessity of developers
    around the globe. Therefore, if you do need a particular machine learning algorithm
    to be developed, and you want to add it to the Spark library, you can contribute
    it to the Spark community. The source code of Spark is openly available on GitHub
    at [https://github.com/apache/spark](https://github.com/apache/spark) as Apache
    Spark mirror. You can do a pull out request and the open source community will
    review your changes or algorithm before adding it to the master branch. For more
    information, please check the Spark Jira confluence site at [https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 用Java、Scala、R和Python编写的通用数据计算和机器学习算法的低级实现对大数据应用程序开发非常容易。Spark框架是基于Scala构建的，因此在Scala中开发ML应用程序可以访问最新的功能，这些功能最初可能在其他Spark语言中不可用。然而，这并不是一个大问题，开源社区也关注全球开发者的需求。因此，如果您需要开发特定的机器学习算法，并希望将其添加到Spark库中，您可以向Spark社区做出贡献。Spark的源代码在GitHub上是公开可用的。您可以提交拉取请求，开源社区将在将其添加到主分支之前审查您的更改或算法。有关更多信息，请查看Spark
    Jira confluence网站。
- en: Python was a great arsenal for data scientists previously, and the contribution
    of Python in Spark is also not different. That means Python also has some excellent
    libraries for data analysis and processing; however, it is comparatively slower
    than Scala. R on the other hand, has a rich environment for data manipulation,
    data pre-processing, graphical analysis, machine learning, and statistical analysis,
    which can help to increase the developer's productivity. Java is definitely a
    good choice for developers who are coming from the Java and Hadoop background.
    However, Java also has the similar problem as Python, since Java is also slower
    than Scala.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Python以前是数据科学家的强大工具，Python在Spark中的贡献也不例外。这意味着Python也有一些优秀的用于数据分析和处理的库；然而，它相对较慢。另一方面，R具有丰富的环境，用于数据处理、数据预处理、图形分析、机器学习和统计分析，这可以帮助提高开发者的生产力。对于来自Java和Hadoop背景的开发者来说，Java绝对是一个不错的选择。然而，Java也有与Python类似的问题，因为Java也比Scala慢。
- en: A recent survey presented on the Databricks website at [http://go.databricks.com/2015-spark-survey](http://go.databricks.com/2015-spark-survey)
    on Spark users (66% users evaluated the Spark languages where 41% were data engineers
    and 22% were data scientists) shows that 58% are using Python, 71% are using Scala,
    31% are using Java, and 18% are using R for developing their Spark applications.
    However, in this book, we will try to provide the examples mostly in Java and
    a few in Scala if needed for the simplicity. The reason for this is that many
    of the readers are very familiar with Java-based MapReduce. Nevertheless, we will
    provide some hints of using the same examples in Python or R in the appendix at
    the end.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在Databricks网站上发布的一项调查显示，Spark用户中有58%使用Python，71%使用Scala，31%使用Java，18%使用R来开发他们的Spark应用程序。然而，在本书中，我们将尽量以Java为主要示例，必要时会使用少量Scala来简化。这是因为许多读者非常熟悉基于Java的MapReduce。然而，我们将在附录中提供一些在Python或R中使用相同示例的提示。
- en: Spark SQL
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Spark SQL is a Spark component for querying and structured data processing.
    The demand was obvious since many data science engineers and business intelligence
    analysts also rely on interactive SQL queries for exploring data from RDBMS. Previously,
    MS SQL server, Oracle, and DB2 were used frequently by the enterprise. However,
    these tools were not scalable or interactive. Therefore, to make it easier, Spark
    SQL provides a programming abstraction called DataFrames and datasets that work
    as distributed SQL query engines, which support unmodified Hadoop Hive queries
    to execute 100 times faster on existing deployments and data. Spark SQL is a powerful
    integration with the rest of the Spark ecosystem.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是用于查询和结构化数据处理的Spark组件。需求是显而易见的，因为许多数据科学工程师和商业智能分析师也依赖于交互式SQL查询来探索来自RDBMS的数据。以前，企业经常使用MS
    SQL服务器、Oracle和DB2。然而，这些工具不具备可扩展性或交互性。因此，为了使其更容易，Spark SQL提供了一个称为DataFrames和数据集的编程抽象，它们作为分布式SQL查询引擎，支持在现有部署和数据上执行未修改的Hadoop
    Hive查询，速度提高了100倍。Spark SQL与Spark生态系统的其他部分强大地集成在一起。
- en: 'Recently, Spark has offered a new experimental interface, commonly referred
    to as datasets (to be discussed in more details in the next section), which provide
    the same benefits of RDDs to use the `lambda` functions strongly. Lambda evolves
    from the Lambda Calculus ([http://en.wikipedia.org/wiki/Lambda_calculus](http://en.wikipedia.org/wiki/Lambda_calculus))
    that refers to anonymous functions in computer programming. It is a flexible concept
    in modern programming language that allows you to write any function quickly without
    naming them. In addition, it also provides a nice way to write closures. For example,
    in Python:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Spark提供了一个新的实验性接口，通常称为数据集（将在下一节中详细讨论），它提供了与RDD相同的好处，可以强大地使用`lambda`函数。Lambda源自Lambda演算（[http://en.wikipedia.org/wiki/Lambda_calculus](http://en.wikipedia.org/wiki/Lambda_calculus)），指的是计算机编程中的匿名函数。这是现代编程语言中的一个灵活概念，允许您快速编写任何函数而不给它们命名。此外，它还提供了一种写闭包的好方法。例如，在Python中：
- en: '[PRE0]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It returns the result as `10`. On the other hand, in Java, it can be similarly
    written if an integer is odd or even:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回结果为`10`。另一方面，在Java中，如果一个整数是奇数还是偶数，可以类似地编写：
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Please note that in Spark 2.0.0, the Spark SQL has substantially been improved
    with the SQL functionalities with SQL 2003 support. Therefore, Spark SQL can now
    be executed with all the 99 TPC-DS queries. More importantly, now a native SQL
    parser supports ANSI_SQL and Hive QL. Native DDL is a command that can also be
    executed, it also now supports sub querying of SQL and the view of canonicalization
    support.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在Spark 2.0.0中，Spark SQL在SQL 2003支持的基础上大幅改进了SQL功能。因此，现在Spark SQL可以执行所有99个TPC-DS查询。更重要的是，现在原生SQL解析器支持ANSI_SQL和Hive
    QL。原生DDL是一个可以执行的命令，它现在也支持SQL的子查询和规范化支持的视图。
- en: DataFrames and datasets unification
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrames和数据集的统一
- en: In the latest Spark release 2.0.0, in Scala and Java, the DataFrame and dataset
    have been unified. In other words, the DataFrame is just a type alias for a dataset
    of rows. However, in Python and R, given the lack of type safety, DataFrame is
    the main programming interface. And for Java, DataFrame is no longer supported,
    but only the Dataset and RDD-based computations are supported, and DataFrame has
    become obsolete (note that it has become obsolete - not depreciated). Although
    SQLContext and HiveContext are kept for backward compatibility; however, in Spark
    2.0.0 release, the new entry point that replaces the old SQLContext and HiveContext
    for DataFrame and dataset APIs is SparkSession.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在最新的Spark 2.0.0版本中，在Scala和Java中，DataFrame和数据集已经统一。换句话说，DataFrame只是行数据集的类型别名。然而，在Python和R中，由于缺乏类型安全性，DataFrame是主要的编程接口。对于Java，不再支持DataFrame，而只支持基于数据集和RDD的计算，DataFrame已经过时（请注意，它已经过时
    - 而不是被折旧）。虽然为了向后兼容性保留了SQLContext和HiveContext；然而，在Spark 2.0.0版本中，替代DataFrame和数据集API的新入口点是SparkSession。
- en: Spark streaming
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: You might want your applications to have the ability to process and analyze
    not only static datasets, but also real-time streams data. To make your wish easier,
    Spark Streaming provides the facility to integrate your application with popular
    batch and streaming data sources. The most commonly used data sources include
    HDFS, Flume, Kafka, and Twitter, and they can be used through their public APIs.
    This integration allows users to develop powerful interactive and analytical applications
    on both streaming and historical data. In addition to this, the fault tolerance
    characteristics are achieved through Spark streaming.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望您的应用程序能够处理和分析不仅是静态数据集，还有实时流数据。为了使您的愿望更容易实现，Spark Streaming提供了将应用程序与流行的批处理和流数据源集成的功能。最常用的数据源包括HDFS、Flume、Kafka和Twitter，它们可以通过它们的公共API使用。这种集成允许用户在流和历史数据上开发强大的交互式和分析应用程序。除此之外，容错特性是通过Spark
    Streaming实现的。
- en: Graph computation – GraphX
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图计算 - GraphX
- en: '**GraphX** is a resilient distributed graph computation engine built on top
    of Spark. GraphX brought a revolution to the users who want to interactively build,
    transform, and reason graph structured data with millions of nodes and vertices
    at scale. As a developer you will enjoy the simplicity so that a large-scale graph
    (social network graph, normal network graph, or astrophysics) could be represented
    using a small chunk of code written in Scala, Java, or Python. GraphX enables
    the developers to take the advantages of both data-parallel and graph-parallel
    systems by efficiently expressing graph computation with ease and speed. Another
    beauty added in the cabinet of GraphX is that it can be used to build an end-to-end
    graph analytical pipeline on real-time streaming data, where the graph-space partitioning
    is used to handle large-scale directed multigraph with properties associated with
    each vertex and edge. Well, some fundamental graph operators are used to make
    this happen such as subgraph, joinVertices and aggregateMessages as well as an
    optimized variant of the Pregel API in particular.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**GraphX**是建立在Spark之上的弹性分布式图计算引擎。GraphX为希望以大规模交互方式构建、转换和推理图结构化数据的用户带来了革命。作为开发人员，您将享受到简单性，以便使用少量Scala、Java或Python代码表示大规模图（社交网络图、普通网络图或天体物理学）。GraphX使开发人员能够充分利用数据并行和图并行系统，通过简单快速地表达图计算。GraphX柜中增加的另一个美丽之处是，它可以用于构建实时流数据上的端到端图分析管道，其中图空间分区用于处理具有与每个顶点和边相关的属性的大规模有向多图。为了实现这一点，使用了一些基本的图操作符，如子图、joinVertices和aggregateMessages，以及Pregel
    API的优化变体。'
- en: Machine learning and Spark ML pipelines
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习和Spark ML管道
- en: Traditional machine learning applications were used to build using R or Matlab
    that lacks scalability issues. Spark has brought two emerging APIs, Spark MLlib
    and Spark ML. These APIs make the machine learning as an actionable insight for
    engineering big data to remove the scalability constraint. Built on top of Spark,
    MLlib is a scalable machine learning library that is enriched with numerous high-quality
    algorithms with a high-accuracy performance that mainly works for RDDs. Spark
    provides many language options for the developers that are functioning in Java,
    Scala, R, and Python to develop complete workflows. Spark ML, on the other hand,
    is an ALPHA component that enhances a new set of machine learning algorithms to
    let data scientists quickly assemble and configure practical machine learning
    pipelines on top of DataFrames.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器学习应用程序是使用R或Matlab构建的，存在可扩展性问题。Spark引入了两个新兴的API，Spark MLlib和Spark ML。这些API使得机器学习成为了工程大数据的可行见解，以消除可扩展性约束。建立在Spark之上，MLlib是一个可扩展的机器学习库，拥有众多高质量的算法，具有高精度性能，主要适用于RDD。Spark为开发人员提供了许多语言选项，包括Java、Scala、R和Python，以开发完整的工作流程。另一方面，Spark
    ML是一个ALPHA组件，它增强了一组新的机器学习算法，让数据科学家可以快速组装和配置基于DataFrames的实用机器学习管道。
- en: Statistical computation – SparkR
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计计算 - SparkR
- en: SparkR is an R package specially designed for the data scientists who are familiar
    with R language and want to analyze large datasets and interactively run jobs
    from the R shell, which supports all the major Spark DataFrame operations such
    as aggregation, filtering, grouping, summary statistics, and much more. Similarly,
    users also can create SparkR DataFrames from local R data frames, or from any
    Spark supported data sources such as Hive, HDFS, Parquet, or JSON. Technically
    speaking, the concept of Spark DataFrame is a tabular data object akin to R's
    native DataFrame ([https://cran.r-project.org/web/packages/dplyr/vignettes/data_frames.html](https://cran.r-project.org/web/packages/dplyr/vignettes/data_frames.html)),
    which on the other hand, is syntactically similar to `dplyr` (an R package, refer
    to [https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html)),
    but is stored in the cluster setting instead.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR是一个专为熟悉R语言并希望分析大型数据集并从R shell交互式运行作业的数据科学家设计的R包，支持所有主要的Spark DataFrame操作，如聚合、过滤、分组、摘要统计等。同样，用户还可以从本地R数据框或任何Spark支持的数据源（如Hive、HDFS、Parquet或JSON）创建SparkR数据框。从技术上讲，Spark
    DataFrame的概念类似于R的本机DataFrame（[https://cran.r-project.org/web/packages/dplyr/vignettes/data_frames.html](https://cran.r-project.org/web/packages/dplyr/vignettes/data_frames.html)），另一方面，在语法上类似于`dplyr`（一个R包，参见[https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html)），但存储在集群设置中。
- en: Spark machine learning libraries
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark机器学习库
- en: In this section, we will describe two main machine learning libraries (Spark
    MLib and Spark ML) and the most widely used implemented algorithms. The ultimate
    target is to provide you with some familiarization about the machine learning
    treasures of Spark since many people still think that Spark is only a general-purpose
    in-memory big data processing or cluster computing framework. However, this is
    not like that, rather this information would help you to understand what could
    be done with the Spark machine learning APIs. In addition, this information would
    help you to explore and will increase the usability while deploying real-life
    machine learning pipelines using Spark MLlib and Spark ML.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述两个主要的机器学习库（Spark MLib和Spark ML）以及最广泛使用的实现算法。最终目标是让您对Spark的机器学习宝藏有所了解，因为许多人仍然认为Spark只是一个通用的内存大数据处理或集群计算框架。然而，情况并非如此，相反，这些信息将帮助您了解使用Spark机器学习API可以做些什么。此外，这些信息将帮助您探索并增加使用Spark
    MLlib和Spark ML部署实际机器学习管道的可用性。
- en: Machine learning with Spark
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark进行机器学习
- en: 'In the pre-Spark era, big data modelers typically used to build their ML models.
    Where a model is prepared through a training process where it is required to make
    predictions and is corrected when those predictions are wrong. In short, an ML
    model is an object that takes an input, does some processing, and finally produces
    the output. Those models were commonly constructed using statistical languages
    such as R and SAS. Then the data engineers used to re-implement the same model
    in Java to deploy on Hadoop. However, this kind of workflow lacks efficiency,
    scalability, throughput, and accuracy with extended execution time. Using Spark,
    the same ML model can be built, adopted, and deployed, making the whole workflow
    much more efficient, robust, and faster and that allows you to provide hands-on
    insight to increase the performance. The main goal of Spark machine learning libraries
    is to make practical machine learning applications scalable, faster, and easy.
    It consists of common and widely used machine learning algorithms and their utilities,
    including classification, regression, clustering, collaborative filtering, and
    dimensionality reduction. It is divided into two packages: Spark MLlib (`spark.mllib`)
    and Spark ML (`spark.ml`).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark时代之前，大数据建模者通常使用统计语言（如R和SAS）构建他们的机器学习模型。然后数据工程师通常会重新在Java中实现相同的模型以部署在Hadoop上。然而，这种工作流程缺乏效率、可伸缩性、吞吐量和准确性，执行时间也较长。使用Spark，可以构建、采用和部署相同的机器学习模型，使整个工作流程更加高效、稳健和快速，从而提供实时洞察力以提高性能。Spark机器学习库的主要目标是使实际的机器学习应用可扩展、更快速和更容易。它包括常见和广泛使用的机器学习算法及其实用工具，包括分类、回归、聚类、协同过滤和降维。它分为两个包：Spark
    MLlib（`spark.mllib`）和Spark ML（`spark.ml`）。
- en: Spark MLlib
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark MLlib
- en: MLlib is the machine learning library of Spark. It is a distributed, low-level
    library written with Scala, Java, and Python against Spark core runtime. MLlib
    mainly focus on learning algorithms and their proper utilities to not only provide
    machine learning analytical capabilities. The major learning utilities include
    classification, regression, clustering, recommender system, and dimensionality
    reduction. In addition, it also aids to optimize the general purpose primitives
    for developing large-scale machine learning pipelines. As stated earlier, MLlib
    comes with some exciting APIs written in Java, Scala, R, and Python. The main
    components of Spark MLlib are described in the following sections.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib是Spark的机器学习库。它是一个分布式的低级库，使用Scala、Java和Python针对Spark核心运行时编写。MLlib主要关注学习算法及其适当的实用工具，不仅提供机器学习分析能力。主要的学习工具包括分类、回归、聚类、推荐系统和降维。此外，它还有助于优化用于开发大规模机器学习流水线的通用原语。正如前面所述，MLlib带有一些令人兴奋的API，使用Java、Scala、R和Python编写。Spark
    MLlib的主要组件将在以下部分中描述。
- en: Data types
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据类型
- en: Spark provides support of local vectors and matrix data types stored on a single
    machine, as well as distributed matrices backed by one or multiple RDDs. Local
    vectors and matrices are simple data models that serve as public interfaces. The
    vector and matrix operations are heavily dependent on the linear algebra operation,
    and you are recommended to gain some background knowledge before using these data
    types.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了支持存储在单台机器上的本地向量和矩阵数据类型，以及由一个或多个RDD支持的分布式矩阵。本地向量和矩阵是简单的数据模型，用作公共接口。向量和矩阵操作严重依赖于线性代数运算，建议在使用这些数据类型之前先获取一些背景知识。
- en: Basic statistics
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本统计
- en: Spark not only provides a column summary and basic statistics to be performed
    on RDDs, but it also supports calculating the correlation between two series of
    data or more complex correlation operations, such as pairwise correlations among
    many series of data, which are a common operation in statistics. However, currently
    Pearson's and Spearman's correlations are only supported and more are to be added
    in future Spark releases. Unlike the other statistical function, stratified sampling
    is also supported by Spark and can be performed on RDD's as key-value pairs; however,
    some functionalities are yet to be added to Python developers.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Spark不仅提供了对RDD进行列摘要和基本统计的功能，还支持计算两个或多个数据系列之间的相关性，或者更复杂的相关性操作，例如在许多数据系列之间的成对相关性，这是统计学中的常见操作。然而，目前仅支持Pearson和Spearman的相关性，未来Spark版本将添加更多相关性。与其他统计函数不同，Spark还支持分层抽样，并且可以在RDD的键值对上执行；但是，一些功能尚未添加到Python开发人员。
- en: Spark provides only the Pearson's chi-squared test for hypothesis testing for
    its goodness of fit and independence of a claim hypothesis, which is a powerful
    technique in statistics that determines whether a result is statistically significant
    to satisfy the claim. Spark also provides online implementations of some tests
    to support use cases such as A/B testing as streaming significance testing typically
    performed on real-time streaming data. Another exciting feature of Spark is the
    factory methods to generate random double RDDs or vector RDDs that are useful
    for randomized algorithms, prototyping, performance, and hypothesis testing. Other
    functionality in the current Spark MLib provides computation facilities of kernel
    density estimation from sample RDDs, which is a useful technique for visualizing
    empirical probability distributions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Spark仅提供Pearson卡方检验用于假设检验的拟合优度和声明假设的独立性，这是统计学中的一种强大技术，用于确定结果是否在统计上显著以满足声明。Spark还提供了一些在线实现的测试，以支持诸如A/B测试之类的用例，通常在实时流数据上执行显著性测试。Spark的另一个令人兴奋的功能是生成随机双重RDD或向量RDD的工厂方法，这对于随机算法、原型、性能和假设检验非常有用。当前Spark
    MLib中的其他功能提供了从样本RDD计算核密度估计的计算功能，这是一种用于可视化经验概率分布的有用技术。
- en: Classification and regression
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类和回归
- en: Classification is a typical process that helps new data objects and components
    to be organized, differentiated, and understood or belong in a certain way on
    the basis of training data. In statistical computing, two types of classification
    exist, binary classification (also commonly referred to as binomial classification)
    and multiclass classification. Binary classification is the task of classifying
    data objects of a given observation into two groups. **Support Vector Machines**
    (**SVMs**), logistic regression, decision trees, random forests, gradient-boosted
    trees, and Naive Bayes have been implemented up to the latest release of Spark.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是一个典型的过程，它帮助新的数据对象和组件根据训练数据进行组织、区分和理解，或者以某种方式属于某种方式。在统计计算中，存在两种类型的分类，二元分类（也常常被称为二项分类）和多类分类。二元分类是将给定观察的数据对象分类为两组的任务。**支持向量机**（**SVMs**）、逻辑回归、决策树、随机森林、梯度提升树和朴素贝叶斯已经实现到Spark的最新版本。
- en: 'Multiclass classification, on the other hand, is the task of classifying data
    objects of a given observation into more than two groups. The logistic regression,
    decision trees, random forests, and naive Bayes are implemented as multiclass
    classification. However, more complex classification algorithms such as multi-level
    classification and multiclass perceptron have not been implemented yet. The regression
    analysis is also a statistical process that estimates relationships among variables
    or observation. Other than the classification process, regression analysis involves
    several techniques for modeling and analyzing data objects. Currently, the following
    algorithms are supported by Spark MLlib library:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 多类分类，另一方面，是将给定观察的数据对象分类到两组以上的任务。逻辑回归、决策树、随机森林和朴素贝叶斯被实现为多类分类。然而，更复杂的分类算法，如多级分类和多类感知器尚未被实现。回归分析也是一种估计变量或观察之间关系的统计过程。除了分类过程，回归分析还涉及多种建模和分析数据对象的技术。目前，Spark
    MLlib库支持以下算法：
- en: Linear least squares
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性最小二乘
- en: Lasso
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 套索
- en: Ridge regression
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 岭回归
- en: Decision trees
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Random forests
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Gradient-boosted trees
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: Isotonic regression
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等渗回归
- en: Recommender system development
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推荐系统开发
- en: An intelligent and scalable recommender system is an emerging application that
    is currently being developed by many enterprises to expand their business and
    cost towards automating recommendation for customers. The collaborative filtering
    approach is the most widely used algorithm in the recommender system, aiming to
    fill in the missing entries of a user-item association matrix. For example, Netflix
    is an example who could manage to reduce their movie recommendation by several
    million dollars. However, the current implementation of Spark MLlib provides only
    the model-based collaborative filtering technique.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 智能和可扩展的推荐系统是一种新兴的应用，目前许多企业正在开发，以扩大他们的业务和成本，以实现对客户的推荐自动化。协同过滤方法是推荐系统中最广泛使用的算法，旨在填补用户-项目关联矩阵的缺失条目。例如，Netflix就是一个例子，他们成功地减少了数百万美元的电影推荐。然而，目前Spark
    MLlib的实现只提供了基于模型的协同过滤技术。
- en: The pros of a model-based collaborative filtering algorithm are users and products
    that can be described by a small set of latent factors to predict missing entries
    using the **Alternating Least Squares** (**ALS**) algorithm. The con is that user
    rating or feedback cannot be taken into consideration for predicting an interest.
    Interestingly, open source developers are also working to develop a memory-based
    collaborative filtering technique to be incorporated into Spark MLib in which
    user rating data could be used to compute the similarity between users or items
    making the ML model more versatile.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的协同过滤算法的优点是用户和产品可以通过一小组潜在因素来描述，使用**交替最小二乘**（**ALS**）算法来预测缺失的条目。缺点是用户评分或反馈不能被考虑在内以预测兴趣。有趣的是，开源开发人员也在努力开发一种基于内存的协同过滤技术，以纳入Spark
    MLib中，其中用户评分数据可以用于计算用户或项目之间的相似性，使得机器学习模型更加多功能。
- en: Clustering
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类
- en: Clustering is an unsupervised machine learning problem/technique. The aims are
    to group subsets of entities with one another based on some notion of similarity
    that is often used for exploratory analysis and for developing hierarchical supervised
    learning pipelines. Spark MLib provides support for various clustering models
    such as K-means, Gaussian matrix, **Power Iteration Clustering** (**PIC**), **Latent
    Dirichlet Allocation** (**LDA**), Bisecting K-means, and Streaming K-means from
    real time streaming data. We will discuss more on supervised/unsupervised and
    reinforcement learning in upcoming chapters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督的机器学习问题/技术。其目的是根据某种相似性概念将实体的子集彼此分组，通常用于探索性分析和开发分层监督学习管道。Spark MLib提供了对各种聚类模型的支持，如K均值、高斯矩阵、**幂迭代聚类**（**PIC**）、**潜在狄利克雷分配**（**LDA**）、二分K均值和来自实时流数据的流式K均值。我们将在接下来的章节中更多地讨论监督/无监督和强化学习。
- en: Dimensionality reduction
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降维
- en: Working with high-dimensional data is cool and demanding to meet the big data
    related complexities. However, one of the problems with high-dimensional data
    is unwanted features or variables. Since all of the measured variables might not
    be important for building the model, to answer the questions of interest you might
    need to reduce the search space. Therefore, based on certain considerations or
    requirements, we need to reduce the dimension of the original data before creating
    any model without sacrificing the original structure.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 处理高维数据既酷又需要满足与大数据相关的复杂性。然而，高维数据的一个问题是不需要的特征或变量。由于所有测量的变量可能对建立模型并不重要，为了回答感兴趣的问题，您可能需要减少搜索空间。因此，基于某些考虑或要求，我们需要在创建任何模型之前减少原始数据的维度，而不损害原始结构。
- en: 'The current implementation of MLib API supports two types of dimensionality
    reduction techniques: **Singular Value Decomposition** (**SVD**) and **Principal
    Component Analysis** (**PCA**) for tall-and-skinny matrices that are stored in
    row-oriented formats and for any vectors. The SVD technique has some performance
    issues; however, PCA is the most widely used technique in dimensionality reduction.
    These two techniques are very useful in large scale ML applications, but they
    require strong background knowledge of linear algebra.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: MLib API的当前实现支持两种降维技术：**奇异值分解**（**SVD**）和**主成分分析**（**PCA**），用于存储在面向行的格式中的高瘦矩阵和任何向量。SVD技术存在一些性能问题；然而，PCA是降维中最广泛使用的技术。这两种技术在大规模ML应用中非常有用，但它们需要对线性代数有很强的背景知识。
- en: Feature extraction and transformation
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征提取和转换
- en: Spark provides different techniques for making the feature engineering easy
    to use through the **Term frequency-inverse document frequency** (**TF-IDF**),
    **Word2Vec**, **StandardScaler**, **ChiSqSelector**, and so on. If you are working
    or planning to work in the area of mining towards building a text mining ML application,
    TF-IDF would be an interesting option from Spark MLlib. TF-IDF provides a feature
    vectorization method to reflect the importance of a term to a document in the
    corpus that is very helpful to develop a text analytical pipeline.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了不同的技术，通过**词频-逆文档频率**（**TF-IDF**）、**Word2Vec**、**标准缩放器**、**ChiSqSelector**等，使特征工程易于使用。如果您正在从事或计划从事文本挖掘领域的工作，TF-IDF将是Spark
    MLlib中一个有趣的选项。TF-IDF提供了一种特征向量化方法，以反映术语对语料库中文档的重要性，这对开发文本分析管道非常有帮助。
- en: In addition, you might be interested in using the Word2Vec computers distributed
    vector representation of the words or corpus on your ML application for text analysis.
    This feature of Word2Vec will eventually make your generalization and model estimation
    more robust in the area of the novel patterns. You also have the StandardScaler
    to normalize the extracted features by scaling to the unit variance or by removing
    the mean based on column summary statistics. It is needed in the pre-processing
    step while building a scalable ML application typically performed on the samples
    in the training dataset. Well, suppose you have extracted features through this
    method, now you will need to select the features to be incorporated into your
    ML model. Therefore, you might also be fascinated in the ChiSqSelector algorithm
    of Spark MLlib for feature selection. ChiSqSelector tries to identify relevant
    features during the ML model building. The reason is obviously to reduce the size
    of the feature space as well as the search space in a tree-based approach and
    to improve both speed and statistical learning behavior in the reinforcement learning
    algorithms.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可能对在文本分析的ML应用中使用Word2Vec计算机分布式词向量表示感兴趣。Word2Vec的这一特性最终将使您在新颖模式领域的泛化和模型估计更加健壮。您还可以使用StandardScaler来通过基于列摘要统计的单位方差缩放或去除均值来规范提取的特征。在构建可扩展的ML应用程序的预处理步骤中通常在训练数据集中执行。假设您已通过这种方法提取了特征，现在您需要选择要纳入ML模型的特征。因此，您可能还对Spark
    MLlib的ChiSqSelector算法进行特征选择感兴趣。ChiSqSelector在ML模型构建过程中尝试识别相关特征。显然，其目的是减少特征空间的大小以及树状方法中的搜索空间，并改善强化学习算法中的速度和统计学习行为。
- en: Frequent pattern mining
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 频繁模式挖掘
- en: Mining frequent items, maximal frequent patterns/itemsets, contiguous frequent
    patterns or subsequences, or other substructures is usually among the first steps
    to analyze a large-scale dataset before starting to build your ML models. The
    current implementation of Spark MLib provides a parallel implementation of FP-growth
    for mining frequent patterns and the association rules. It also provides the implementation
    of another popular algorithm, PrefixSpan, for mining sequence patterns. However,
    you will have to customize the algorithm for mining maximal frequent patterns
    accordingly. We will provide a scalable ML application for mining privacy, and
    preserving maximal frequent patterns in upcoming chapters.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始构建ML模型之前，分析大规模数据集通常是挖掘频繁项、最大频繁模式/项集、连续频繁模式或子序列等的第一步。Spark MLib的当前实现提供了FP-growth的并行实现，用于挖掘频繁模式和关联规则。它还提供了另一个流行算法PrefixSpan的实现，用于挖掘序列模式。但是，您将需要根据需要定制算法来挖掘最大频繁模式。我们将在即将到来的章节中提供一个可扩展的ML应用程序，用于挖掘隐私并保留最大频繁模式。
- en: Spark ML
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark ML
- en: Spark ML is an ALPHA component that adds a new set of machine learning APIs
    to let users quickly assemble and configure practical machine learning pipelines
    on top of DataFrames. Before praising the features and advantages of Spark ML,
    we should know about the DataFrames machine learning techniques that can be applied
    and developed to a wide variety of data types, such as vectors, unstructured (that
    is, raw texts), images, and structured data. In order to support a variety of
    data types to make the application development easier, recently, Spark ML has
    adopted the DataFrame and Dataset from Spark SQL.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML是一个ALPHA组件，它为用户提供了一组新的机器学习API，让用户可以快速组装和配置实用的机器学习管道，基于DataFrames。在赞扬Spark
    ML的特性和优势之前，我们应该了解可以应用和开发到各种数据类型的DataFrames机器学习技术，例如向量、非结构化（即原始文本）、图像和结构化数据。为了支持各种数据类型，使应用程序开发更容易，最近，Spark
    ML采用了来自Spark SQL的DataFrame和Dataset。
- en: A DataFrame or Dataset can be created either implicitly or explicitly from an
    RDD of objects that supports the basic and structured types. The goal of Spark
    ML is to provide a uniform set of high-level APIs built on top of DataFrames and
    datasets rather than RDDs. It helps the users to create and tune practical machine
    learning pipelines. The Spark ML also provides for the feature estimators and
    transformers for developing scalable ML pipelines. Spark ML systematizes many
    ML algorithms and APIs to make it even easier to combine multiple algorithms into
    a single pipeline, or data workflow that uses the concept of DataFrame and datasets.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框架或数据集可以从支持基本和结构化类型的对象的RDD隐式或显式创建。Spark ML的目标是提供一组统一的高级API，构建在数据框架和数据集之上，而不是RDD。它帮助用户创建和调整实际的机器学习管道。Spark
    ML还提供了用于开发可扩展ML管道的特征估计器和转换器。Spark ML系统化了许多ML算法和API，使得更容易将多个算法组合成单个管道或数据工作流，使用数据框架和数据集的概念。
- en: The three basic steps in feature engineering are feature extraction, feature
    transformation, and selection. Spark ML provides implementation of several algorithms
    to make these steps easier. Extraction provides the facility for extracting features
    from raw data, whereas transformation provides the facility of scaling, converting,
    or modifying features that are found from the extraction step and the selection
    helps to select a subset from a larger set of features from the second step. Spark
    ML also provides several classifications (logistic regression, decision tree classifier,
    random forest classifier, and more), regression (liner regression, decision tree
    regression, random forest regression, survival regression, and gradient-boosted
    tree regression), decision tree and tree ensembles (random forest and gradient-boosted
    trees), as well as clustering (K-means and LDA) algorithms implemented for developing
    ML pipelines on top of DataFrames. We will discuss more on RDDs and DataFrames
    and their underlying operations in [Chapter 3](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 3. Understanding the Problem by Understanding the Data"), *Understanding
    the Problem by Understanding the Data*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程中的三个基本步骤是特征提取、特征转换和选择。Spark ML提供了几种算法的实现，使这些步骤更容易。提取提供了从原始数据中提取特征的功能，而转换提供了从提取步骤中找到的特征进行缩放、转换或修改的功能，选择则帮助从第二步的较大特征集中选择子集。Spark
    ML还提供了几种分类（逻辑回归、决策树分类器、随机森林分类器等）、回归（线性回归、决策树回归、随机森林回归、生存回归和梯度提升树回归）、决策树和树集成（随机森林和梯度提升树）以及聚类（K均值和LDA）算法的实现，用于在数据框架之上开发ML管道。我们将在[第3章](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "第3章。通过了解数据来理解问题")中更多地讨论RDD和数据框架及其基础操作，*通过了解数据来理解问题*。
- en: Installing and getting started with Spark
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和开始使用Spark
- en: 'Spark is Apache Hadoop''s successor. Therefore, it would be better to install
    and work Spark into a Linux-based system even though you can also try on Windows
    and Mac OS. It is also very possible to configure your Eclipse environment to
    work with Spark as a Maven project on any OS and bundle your applications as a
    jar file with all the dependencies. Secondly, you can try running an application
    from the Spark shell (Scala shell to be more specific) following the same fashion
    as SQL or R programming:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是Apache Hadoop的继任者。因此，最好将Spark安装和工作到基于Linux的系统中，即使您也可以尝试在Windows和Mac OS上。还可以配置Eclipse环境以将Spark作为Maven项目在任何操作系统上运行，并将应用程序作为具有所有依赖项的jar文件捆绑。其次，您可以尝试从Spark
    shell（更具体地说是Scala shell）运行应用程序，遵循与SQL或R编程相同的方式：
- en: The third way is from the command line (Windows)/Terminal (Linux/Mac OS). At
    first you need to write your ML application using Scala or Java and prepare the
    jar file with the required dependencies. Then the jar file can be submitted to
    a cluster to compute a Spark job.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法是从命令行（Windows）/终端（Linux/Mac OS）开始。首先，您需要使用Scala或Java编写您的ML应用程序，并准备具有所需依赖项的jar文件。然后，可以将jar文件提交到集群以计算Spark作业。
- en: 'We will show how to develop and deploy a Spark ML application in three ways.
    However, the very first perquisite is to prepare your Spark application development
    environment. You can install and configure Spark on a number of operating systems,
    including:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示如何以三种方式开发和部署Spark ML应用程序。但是，第一个前提是准备好您的Spark应用程序开发环境。您可以在许多操作系统上安装和配置Spark，包括：
- en: Windows (XP/7/8/10)
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Windows（XP/7/8/10）
- en: Mac OS X (10.4.7+)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mac OS X（10.4.7+）
- en: Linux distribution (including Debian, Ubuntu, Fedora, RHEL, CentOS, and so on)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux发行版（包括Debian、Ubuntu、Fedora、RHEL、CentOS等）
- en: Note
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please check, the Spark website at [https://spark.apache.org/documentation.html](https://spark.apache.org/documentation.html)
    for the Spark version and OS related documentation. The following steps show you
    how to install and configure Spark on Ubuntu 14.04 (64-bit). Please note that
    Spark 2.0.0 runs on Java 7+, Python 2.6+/3.4+, and R 3.1+. For the Scala API,
    Spark 2.0.0 uses Scala 2.11\. Therefore, you will need to use a compatible Scala
    version (2.11.x).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看Spark网站[https://spark.apache.org/documentation.html](https://spark.apache.org/documentation.html)获取与Spark版本和操作系统相关的文档。以下步骤向您展示如何在Ubuntu
    14.04（64位）上安装和配置Spark。请注意，Spark 2.0.0运行在Java 7+、Python 2.6+/3.4+和R 3.1+上。对于Scala
    API，Spark 2.0.0使用Scala 2.11。因此，您需要使用兼容的Scala版本（2.11.x）。
- en: '**Step 1: Java installation**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：Java安装**'
- en: 'Java installation should be considered as one of the mandatory requirements
    in installing Spark since Java and Scala-based APIs require having a Java virtual
    machine installed on the system. Try the following command to verify the Java
    version:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装Spark时，应将Java安装视为安装的强制要求之一，因为基于Java和Scala的API需要在系统上安装Java虚拟机。尝试以下命令验证Java版本：
- en: '[PRE2]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If Java is already installed on your system, you should see the following message:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Java已经安装在您的系统上，您应该看到以下消息：
- en: '[PRE3]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In case you do not have Java installed on your system, make sure you install
    Java before proceeding to the next step. Please note that to avail and enjoy the
    lambda expression support it is recommended to install Java 8 on your system,
    preferably JDK and JRE both. Although for Spark 1.6.2 and prior releases Java
    7 should be enough:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的系统上没有安装Java，请确保在进行下一步之前安装Java。请注意，为了使用lambda表达式支持，建议在系统上安装Java 8，最好同时安装JDK和JRE。尽管对于Spark
    1.6.2和之前的版本，Java 7应该足够：
- en: '[PRE4]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After installing, don''t forget to set `JAVA_HOME`. Just apply the following
    commands (we assume Java is installed at `/usr/lib/jvm/java-8-oracle`):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，请不要忘记设置`JAVA_HOME`。只需应用以下命令（我们假设Java安装在`/usr/lib/jvm/java-8-oracle`）：
- en: '[PRE5]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can add these environmental variables manually in the`.bashrc` file located
    in the home directory. If you cannot find the file, probably it is hidden so it
    needs to be explored. Just go to the **view** tab and enable the **Show hidden
    file**.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在主目录中的`.bashrc`文件中手动添加这些环境变量。如果找不到文件，可能是隐藏的，因此需要进行探索。只需转到**视图**选项卡并启用**显示隐藏文件**。
- en: '**Step 2: Scala installation**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：安装Scala**'
- en: 'Spark is written in Scala itself; therefore, you should have Scala installed
    on your system. Checking this is so straight forward by using the following command:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Spark本身是用Scala编写的，因此您的系统上应该安装了Scala。通过使用以下命令检查这一点非常简单：
- en: '[PRE6]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If Scala is already installed on your system, you should get the following
    message on the terminal:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Scala已经安装在您的系统上，您应该在终端上收到以下消息：
- en: '[PRE7]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that during the writing of this installation, we used the latest version
    of Scala, that is 2.11.8\. In case you do not have Scala installed on your system,
    make sure you install it, so before proceeding to the next step, you can download
    the latest version of Scala from the Scala website at [http://www.scala-lang.org/download/](http://www.scala-lang.org/download/).
    After the download has finished, you should find the Scala `tar` file in the download
    folder:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在编写此安装过程时，我们使用的是最新版本的Scala，即2.11.8。如果您的系统上没有安装Scala，请确保安装Scala，因此在进行下一步之前，您可以从Scala网站[http://www.scala-lang.org/download/](http://www.scala-lang.org/download/)下载最新版本的Scala。下载完成后，您应该在下载文件夹中找到Scala
    `tar`文件：
- en: 'Extract the Scala `tar` file by extracting, from its location or type the following
    command for extracting the Scala tar file from the terminal:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从其位置提取Scala `tar`文件来提取，或者在终端中键入以下命令来提取Scala tar文件：
- en: '[PRE8]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now move the Scala distribution to the user’s perspective (for example, `/usr/local/scala`)
    by the following command or do it manually:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将Scala分发移动到用户的透视图（例如，`/usr/local/scala`）通过以下命令或手动执行：
- en: '[PRE9]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Set the Scala home:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Scala主目录：
- en: '[PRE10]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After installation has been completed, you should verify it using the following
    command:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，您应该使用以下命令进行验证：
- en: '[PRE11]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If Scala has successfully been configured on your system, you should get the
    following message on your terminal:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果Scala已成功配置到您的系统上，您应该在终端上收到以下消息：
- en: '[PRE12]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Step 3: Installing Spark**'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：安装Spark**'
- en: 'Download the latest version of Spark from the Apace Spark website at [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html).
    For this installation step, we used the latest Spark stable release 2.0.0 version
    pre-built for Hadoop 2.7 and later. After the download has finished, you will
    find the Spark `tar` file in the download folder:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从Apace Spark网站[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)下载最新版本的Spark。对于此安装步骤，我们使用了最新的Spark稳定版本2.0.0版本，预先构建为Hadoop
    2.7及更高版本。下载完成后，您将在下载文件夹中找到Spark `tar`文件：
- en: 'Extract the Scala `tar` file by extracting it from its location or type the
    following command for extracting the Scala `tar` file from the terminal:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从其位置提取Scala `tar`文件，或者在终端中键入以下命令来提取Scala `tar`文件：
- en: '[PRE13]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now move the Scala distribution to the user''s perspective (for example, `/usr/local/spark`)
    by the following command or do it manually:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将Scala分发移动到用户的透视图（例如，`/usr/local/spark`）通过以下命令或手动执行：
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To set after Spark installing, just apply the following commands:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完Spark后，只需应用以下命令：
- en: '[PRE15]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Step 4: Making all the changes permanent**'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：使所有更改永久生效**'
- en: 'Source the `~/.bashrc` file using the following command to make the changes
    permanent:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令对`~/.bashrc`文件进行源操作，以使更改永久生效：
- en: '[PRE16]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If you execute the `$ vi ~/. bashrc` command, you will see the following entry
    in your `bashrc` file as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果执行`$ vi ~/. bashrc`命令，您将在`bashrc`文件中看到以下条目如下：
- en: '[PRE17]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Step 5: Verifying the Spark installation**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5：验证Spark安装**'
- en: 'The verification of the Spark installation is shown in the following screenshot:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Spark安装的验证显示在以下截图中：
- en: '![Installing and getting started with Spark](img/00149.jpeg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![安装并开始使用Spark](img/00149.jpeg)'
- en: 'Figure 6: The Spark shell confirms the successful Spark installation.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：Spark shell确认了成功安装Spark。
- en: 'Write the following command to open the Spark shell to verify if Spark has
    been configured successfully:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 写以下命令以打开Spark shell，以验证Spark是否已成功配置：
- en: '[PRE18]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If Spark is installed successfully, you should see the following message (*Figure
    6*).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Spark安装成功，您应该看到以下消息（*图6*）。
- en: 'The Spark server will start on localhost at port `4040`, more precisely at
    `http://localhost:4040/` (*Figure 7*). Just move there to make sure if it''s really
    running:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Spark服务器将在本地主机的端口`4040`上启动，更确切地说是在`http://localhost:4040/`（*图7*）。只需转到那里，以确保它是否真的在运行：
- en: '![Installing and getting started with Spark](img/00075.jpeg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![安装并开始使用Spark](img/00075.jpeg)'
- en: 'Figure 7: Spark is running as a local web server.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：Spark作为本地Web服务器运行。
- en: Well done! Now you are ready to start writing the Scala code on the Spark shell.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！现在您已经准备好在Spark shell上开始编写Scala代码。
- en: Packaging your application with dependencies
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用依赖项打包您的应用程序
- en: Now we will show you how to package the applications as a Java archive (`JAR`)
    file with all the required dependencies on Eclipse, which is an **Integrated Development
    Environment** (**IDE**) and an open source tool for Java development as an Apache
    Maven project ([https://maven.apache.org/](https://maven.apache.org/)). Maven
    is a software project management and comprehension tool like Eclipse. Based on
    the concept of a **Project Object Model** (**POM**), Maven can manage a project's
    build, reporting and documenting from a central piece of information.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将向您展示如何在Eclipse上将应用程序打包为带有所有必需依赖项的Java存档（`JAR`）文件，这是一个**集成开发环境**（**IDE**）和一个用于Java开发的开源工具，是Apache
    Maven项目（[https://maven.apache.org/](https://maven.apache.org/)）。Maven是一个软件项目管理和理解工具，就像Eclipse一样。基于**项目对象模型**（**POM**）的概念，Maven可以管理项目的构建、报告和文档编制，从一个中央信息中。
- en: Note that it is possible to export an ML application written in Java or Scala
    as an archive/executable jar file using Command Prompt. However, for the simplicity
    and faster application development we will use the same as the Maven project using
    Eclipse so that readers can enjoy the same facility to submit the application
    to the master node for computation. Now let's move to the discussion of exporting
    frequent pattern mining applications as a jar file with all the dependencies.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，可以使用命令提示符将用Java或Scala编写的ML应用程序导出为存档/可执行的jar文件。但是，为了简化和加快应用程序开发，我们将使用与Eclipse相同的Maven项目，以便读者可以享受相同的便利性将应用程序提交到主节点进行计算。现在让我们转到讨论如何将频繁模式挖掘应用程序导出为带有所有依赖项的jar文件。
- en: '**Step 1: Creating a Maven project in Eclipse**'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：在Eclipse中创建一个Maven项目**'
- en: 'On successful creation of a sample Maven project, you will see the following
    project structure in Eclipse shown in *Figure 8*:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 成功创建示例Maven项目后，您将在Eclipse中看到以下项目结构，如*图8*所示：
- en: '![Packaging your application with dependencies](img/00121.jpeg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![使用依赖项打包应用程序](img/00121.jpeg)'
- en: 'Figure 8: Maven project structure in Eclipse.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：Eclipse中的Maven项目结构。
- en: '**Step 2: Application development**'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：应用程序开发**'
- en: 'Create a Java class and copy the following source code to under the `src/main/java`
    directory for the mining frequent pattern. Here, inputting the filename has been
    specified by the filename string to be provided through the command line argument
    or by specifying the source manually. For the time being, we have just provided
    line comments, however, you will get to know details from [Chapter 3](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 3. Understanding the Problem by Understanding the Data"), *Understanding
    the Problem by Understanding the Data* and onwards:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个Java类，并将以下源代码复制到`src/main/java`目录下，用于挖掘频繁模式。在这里，输入文件名已经通过命令行参数或手动指定源代码来指定文件名字符串。目前，我们只提供了行注释，但是您将从[第3章](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "第3章。通过了解数据来了解问题")中了解详细信息，*通过了解数据来了解问题*以及之后的内容：
- en: '[PRE19]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Step 3: Maven configuration**'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：Maven配置**'
- en: 'Now you need to configure Maven specifying related dependencies and configurations.
    First, edit your existing `pom.xml` file to copy each XML source code snippets
    inside the `<dependencies>` tag. Please note that your dependencies might be different
    based on Spark release so change the version accordingly:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您需要配置Maven，指定相关依赖项和配置。首先，编辑您现有的`pom.xml`文件，将每个XML源代码片段复制到`<dependencies>`标记内。请注意，根据Spark版本，您的依赖项可能会有所不同，因此请相应更改版本：
- en: 'Spark core dependency for Spark context and configuration:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark核心依赖项用于Spark上下文和配置：
- en: '[PRE20]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Spark MLib dependency for the FPGrowth:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark MLib依赖项用于FPGrowth：
- en: '[PRE21]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now you need to add the build requirements. Copy the following code snippets
    immediately after the `</dependencies>` tag. Here we are specifying the `<groupId>`
    as maven plugins, `<artifactId>` as maven shade plugins, and specifying the jar
    file naming convention using the `<finalName>` tag. Make sure that you have specified
    the source code download plugin, set the compiler level, and set the assembly
    plugin for the Maven, described as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您需要添加构建要求。将以下代码片段立即复制到`</dependencies>`标记之后。在这里，我们将`<groupId>`指定为maven插件，`<artifactId>`指定为maven
    shade插件，并使用`<finalName>`标记指定jar文件命名约定。确保您已经指定了源代码下载插件，设置了编译器级别，并为Maven设置了装配插件，如下所述：
- en: 'Specify the source code download plugin with Maven:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Maven指定源代码下载插件：
- en: '[PRE22]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Set the compiler level for Maven:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Maven设置编译器级别：
- en: '[PRE23]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Set the Maven assembly plugin:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Maven装配插件：
- en: '[PRE24]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The complete `pom.xml` file, input data, and Java source file can be downloaded
    from our GitHub repositories at [https://github.com/rezacsedu/PacktMLwithSpark](https://github.com/rezacsedu/PacktMLwithSpark).
    Please note that we used Eclipse Mars Eclipse IDE for Java Developers, and the
    version was Mars Release (4.5.0). You can go for this version or another distribution
    such as Eclipse Luna.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的`pom.xml`文件，输入数据和Java源文件可以从我们的GitHub存储库[https://github.com/rezacsedu/PacktMLwithSpark](https://github.com/rezacsedu/PacktMLwithSpark)下载。请注意，我们使用了Eclipse
    Mars Eclipse IDE for Java Developers，并且版本是Mars Release (4.5.0)。您可以选择这个版本或其他发行版，比如Eclipse
    Luna。
- en: '**Step 4: The Maven build**'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：Maven构建**'
- en: 'In this section, we will describe how to create a Maven friendly project on
    Eclipse. After you have followed all the steps, you will be able to run a Maven
    project successfully. The steps should be in a chronological order as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述如何在Eclipse上创建一个Maven友好的项目。在您按照所有步骤后，您将能够成功运行Maven项目。步骤应按照以下时间顺序进行：
- en: Run your project as Maven install.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的项目作为Maven安装运行。
- en: If your code and maven configuration file are okay, the maven build will be
    successful.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的代码和Maven配置文件没有问题，那么Maven构建将成功。
- en: Build the maven project.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建Maven项目。
- en: Right-click on your project and run the maven project as **Maven build...**
    and write `clean package` in the **Goals** option.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 右键单击您的项目，运行Maven项目为**Maven构建...**，并在**Goals**选项中写入`clean package`。
- en: Check the Maven dependencies.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查Maven依赖项。
- en: Expand the Maven dependencies tree and check if the required jar files have
    been installed.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展开Maven依赖树，并检查是否已安装所需的jar文件。
- en: Check if the jar file is generated with dependencies.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查jar文件是否生成了依赖项。
- en: As we specified, you should find two jar files under the `/target` directory
    tree (refer to *Figure 9*). The packaging file should contain exactly the same
    name as specified in the `<finalName>` tag. Now move your code (jar file) to a
    directory that aligns our experiment (that is, `/user/local/code`) and your data
    (that is, `/usr/local/data/`). We will use this jar file to execute the Spark
    job on an AWS EC2 cluster in a later stage. We will discuss the input dataset
    in the next step.![Packaging your application with dependencies](img/00097.jpeg)
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如我们所指定的，您应该在`/target`目录树下找到两个jar文件（参考*图9*）。打包文件应该与`<finalName>`标签中指定的名称完全相同。现在将您的代码（jar文件）移动到与我们的实验对齐的目录（即`/user/local/code`）和您的数据（即`/usr/local/data/`）。我们将在后期使用这个jar文件在AWS
    EC2集群上执行Spark作业。我们将在下一步讨论输入数据集。![使用依赖项打包您的应用程序](img/00097.jpeg)
- en: 'Figure 9: Maven project with the jar generated with all the required dependencies
    on Eclipse.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：在Eclipse上生成了所有必需依赖项的Maven项目的jar。
- en: Running a sample machine learning application
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行一个样本机器学习应用程序
- en: In this section, we will describe how to run a sample machine learning application
    from the Spark shell, on the local machine as stand-alone mode, and finally we
    will show you how to deploy and run the application on the Spark cluster using
    Amazon EC2 ([https://aws.amazon.com/ec2/](https://aws.amazon.com/ec2/)).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述如何从Spark shell在本地机器上以独立模式运行一个样本机器学习应用程序，最后我们将向您展示如何使用Amazon EC2（[https://aws.amazon.com/ec2/](https://aws.amazon.com/ec2/)）部署和运行应用程序。
- en: Running a Spark application from the Spark shell
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Spark shell运行Spark应用程序
- en: Please note that this is just an exercise that checks the installation and running
    of a sample code. Details on machine learning application development will be
    covered from [Chapter 3](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 3. Understanding the Problem by Understanding the Data"), *Understanding
    the Problem by Understanding the Data* to *[Chapter 9](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 9.  Advanced Machine Learning with Streaming and Graph Data")*, *Advanced
    Machine Learning with Streaming and Graph Data*.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这只是一个检查安装和运行样本代码的练习。有关机器学习应用程序开发的详细信息将从[第3章](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "第3章。通过了解数据来理解问题")开始，*通过了解数据来理解问题*，到*[第9章](part0073_split_000.html#25JP22-0b803698e2de424b8aa3c56ad52b005d
    "第9章。使用流和图数据进行高级机器学习")*，*使用流和图数据进行高级机器学习*。
- en: 'Now we will further proceed with one of the popular machine learning problem
    also called frequent pattern mining using the Frequent Pattern-growth or FP-growth. Suppose
    we have a transactional database as shown in the following table. Each line indicates
    a transaction done by a particular customer. Our target is to find the frequent
    patterns from the database, which is the prerequisite for calculating association
    rules ([https://en.wikipedia.org/wiki/Association_rule_learning](https://en.wikipedia.org/wiki/Association_rule_learning))
    from customer purchase rules. Save this database as `input.txt` in the `/usr/local/data`
    directory without transaction IDs:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将进一步进行一种流行的机器学习问题，也称为频繁模式挖掘，使用频繁模式增长或FP-growth。假设我们有一个如下表所示的交易数据库。每行表示特定客户完成的交易。我们的目标是从数据库中找到频繁模式，这是计算关联规则（[https://en.wikipedia.org/wiki/Association_rule_learning](https://en.wikipedia.org/wiki/Association_rule_learning)）的先决条件，从客户购买规则中。将此数据库保存为`input.txt`，不包括交易ID，在`/usr/local/data`目录中：
- en: '| **Transaction ID** | **Transaction** |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| **交易ID** | **交易** |'
- en: '| 12345678910 | A B C D FA B C EB C D E FA C D EC D FD E FD EC D FC FA C D
    E |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 12345678910 | A B C D FA B C EB C D E FA C D EC D FD E FD EC D FC FA C D
    E |'
- en: 'Table 1: A transactional database.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：交易数据库。
- en: 'Now let''s move to the Spark shell by specifying the master and number of the
    computational core to use as standalone mode (here are four cores, for example):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过指定主机和要使用的计算核心数量来移动到Spark shell，作为独立模式（这里有四个核心，例如）：
- en: '[PRE25]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Step 1: Loading packages**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步：加载软件包
- en: 'Load the required FPGrowth package and other dependent packages:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 加载所需的FPGrowth软件包和其他依赖软件包：
- en: '[PRE26]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Step 2: Creating Spark context**'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步：创建Spark上下文
- en: 'To create a Spark context, at first you need to configure the Spark session
    by mentioning the application name and master URL. Then you can use the Spark
    configuration instance variable to create a Spark context as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个Spark上下文，首先需要通过提及应用程序名称和主URL来配置Spark会话。然后，您可以使用Spark配置实例变量来创建一个Spark上下文，如下所示：
- en: '[PRE27]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Step 3: Reading the transactions**'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步：读取交易
- en: 'Let''s read the transactions as RDDs on the created Spark Context (`sc`) (see
    *Figure 6*):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在创建的Spark上下文（`sc`）上将交易作为RDDs读取（参见*图6*）：
- en: '[PRE28]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Step 4: Checking the number of transactions**'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步：检查交易数量
- en: 'Here is the code for checking the number of transactions:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于检查交易数量的代码：
- en: '[PRE29]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Step 5: Creating an FPGrowth model**'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步：创建FPGrowth模型
- en: 'Create the model by specifying the minimum support threshold (see also [https://en.wikipedia.org/wiki/Association_rule_learning](https://en.wikipedia.org/wiki/Association_rule_learning))
    and the number of partitions:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定最小支持阈值（也请参阅[https://en.wikipedia.org/wiki/Association_rule_learning](https://en.wikipedia.org/wiki/Association_rule_learning)）和分区数来创建模型：
- en: '[PRE30]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**Step 6: Checking the number of frequent patterns**'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 第6步：检查频繁模式的数量
- en: 'The following code explains how to check the number of frequent patterns:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码解释了如何检查频繁模式的数量：
- en: '[PRE31]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Step 7: Printing patterns and support**'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 第7步：打印模式和支持
- en: 'Print the frequent pattern and their corresponding support/frequency counts
    (see *Figure 10*). Spark job will be running on localhost (refer to *Figure 11*):'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 打印频繁模式及其相应的支持/频率计数（参见*图10*）。Spark作业将在本地主机上运行（参见*图11*）：
- en: '[PRE32]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![Running a Spark application from the Spark shell](img/00106.jpeg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![从Spark shell运行Spark应用程序](img/00106.jpeg)'
- en: 'Figure 10: Frequent patterns.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：频繁模式。
- en: Running a Spark application on the local cluster
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在本地集群上运行Spark应用程序
- en: Once a user application is bundled as either a jar file (written in Scala or
    Java) or a Python file, it can be launched using the `spark-submit` script located
    under the bin directory in the Spark distribution.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦用户应用程序被打包为jar文件（用Scala或Java编写）或Python文件，它可以使用Spark分发中bin目录下的`spark-submit`脚本启动。
- en: 'As per the API documentation provided by the Spark website at [http://spark.apache.org/docs/2.0.0-preview/submitting-applications.html](http://spark.apache.org/docs/2.0.0-preview/submitting-applications.html),
    this script takes care of setting up the class path with Spark and its dependencies,
    and can support different cluster managers and deploys models that Spark supports.
    In a nutshell, Spark job submission syntax is as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Spark网站提供的API文档[http://spark.apache.org/docs/2.0.0-preview/submitting-applications.html](http://spark.apache.org/docs/2.0.0-preview/submitting-applications.html)，此脚本负责设置带有Spark及其依赖项的类路径，并且可以支持Spark支持的不同集群管理器和部署模型。简而言之，Spark作业提交语法如下：
- en: '[PRE33]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here, `[options]` can be: `--class <main-class>``--master <master-url>``--deploy-mode
    <deploy-mode>`, and a number of other options.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`[options]`可以是：`--class <main-class>``--master <master-url>``--deploy-mode
    <deploy-mode>`，以及许多其他选项。
- en: To be more specific, `<main-class>` is the name of the main class name, which
    is the entry point for our application. `<master-url>` specifies the master URL
    for the cluster (for example, `spark://HOST:PORT` for connecting to the given
    Spark standalone cluster master, local for running Spark locally with one worker
    thread with no parallelism at all, `local [k]` for running a Spark job locally
    with K worker threads, which is the number of cores on your machine, `local[*]`
    for running a Spark job locally with as many worker threads as logical cores on
    your machine have, and `mesos://IP:PORT` for connecting to the available Mesos
    cluster, and even you could submit your job to the Yarn cluster - for more, see
    [http://spark.apache.org/docs/latest/submitting-applications.html#master-urls](http://spark.apache.org/docs/latest/submitting-applications.html#master-urls)).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，`<main-class>`是主类名称，是我们应用程序的入口点。`<master-url>`指定了集群的主URL（例如，`spark://HOST:PORT`用于连接给定的Spark独立集群主节点，local用于在本地运行具有没有并行性的一个工作线程的Spark，`local
    [k]`用于在具有K个工作线程的本地运行Spark作业，这是您计算机上的核心数，`local[*]`用于在具有与计算机上逻辑核心一样多的工作线程的本地运行Spark作业，`mesos://IP:PORT`用于连接到可用的Mesos集群，甚至您可以将作业提交到Yarn集群-有关更多信息，请参见[http://spark.apache.org/docs/latest/submitting-applications.html#master-urls](http://spark.apache.org/docs/latest/submitting-applications.html#master-urls)）。
- en: '`<deploy-mode>` is used to deploy our driver on the worker nodes (cluster)
    or locally as an external client (client). `<app-jar>` is the jar file we just
    built, including all the dependencies. `<python-file>` is the application main
    source code written using Python. `[app-arguments]` could be an input or output
    argument specified by an application developer:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '`<deploy-mode>`用于在工作节点（集群）上部署我们的驱动程序，或者作为外部客户端（client）在本地部署。`<app-jar>`是我们刚刚构建的jar文件，包括所有依赖项。`<python-file>`是使用Python编写的应用程序主要源代码。`[app-arguments]`可以是应用程序开发人员指定的输入或输出参数：'
- en: '![Running a Spark application on the local cluster](img/00083.jpeg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![在本地集群上运行Spark应用程序](img/00083.jpeg)'
- en: 'Figure 11: Spark job running on localhost'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：在本地主机上运行的Spark作业
- en: 'Therefore, for our case, the job submit syntax would be as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于我们的情况，作业提交语法将如下所示：
- en: '[PRE34]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Here, `JavaFPGrowthExample` is the main class file written in Java; local is
    the master URL; `FPGrowth-0.0.1-SNAPSHOT-jar-with-dependencies.jar` is the application
    `jar` file we just generated by maven project; `input.txt` is the transactional
    database as the text file, and output is the directory where the output to be
    generated (in our case, the output will be shown on the console). Now let's submit
    this job to be executed locally.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`JavaFPGrowthExample`是用Java编写的主类文件；local是主URL；`FPGrowth-0.0.1-SNAPSHOT-jar-with-dependencies.jar`是我们刚刚通过maven项目生成的应用程序`jar`文件；`input.txt`是作为文本文件的事务数据库，output是要生成输出的目录（在我们的情况下，输出将显示在控制台上）。现在让我们提交此作业以在本地执行。
- en: 'If it is executed successfully, you will find the following message including
    the output in *Figure 12* (abridged):'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成功执行，您将找到以下消息，包括*图12*中的输出（摘要）：
- en: '![Running a Spark application on the local cluster](img/00132.jpeg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![在本地集群上运行Spark应用程序](img/00132.jpeg)'
- en: 'Figure 12: Spark job output on the terminal.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：终端上的Spark作业输出。
- en: Running a Spark application on the EC2 cluster
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在EC2集群上运行Spark应用程序
- en: In the previous section, we illustrated how to submit spark jobs in local or
    standalone mode. Here, we are going to describe how to run a spark application
    in cluster mode. To make our application run on the spark cluster mode, we consider
    the **Amazon Elastic Compute Cloud** (**EC2**) services, as **Infrastructure as
    a Service** (**IaaS**) or **Platform as a Service** (**PaaS**). For pricing and
    related information, please refer to this URL [https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们说明了如何在本地或独立模式下提交spark作业。在这里，我们将描述如何在集群模式下运行spark应用程序。为了使我们的应用程序在spark集群模式下运行，我们考虑**亚马逊弹性计算云**（**EC2**）服务，作为**基础设施即服务**（**IaaS**）或**平台即服务**（**PaaS**）。有关定价和相关信息，请参阅此网址[https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/)。
- en: '**Step 1: Key pair and access key configuration**'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：密钥对和访问密钥配置**'
- en: 'We assume you have EC2 accounts already created. The first requirement is to
    create EC2 key pairs and AWS access keys. The EC2 key pair is the private key
    that you need when you will make a secure connection through SSH to your EC2 server
    or instances. To make the key, you have to go through the AWS console at [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair).
    Please refer to *Figure 13*, which shows the key pair creation page for an EC2
    account:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您已经创建了EC2帐户。第一个要求是创建EC2密钥对和AWS访问密钥。EC2密钥对是您在通过SSH进行安全连接到EC2服务器或实例时需要的私钥。要创建密钥，您必须通过AWS控制台
    [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair)。请参考*图13*，显示了EC2帐户的密钥对创建页面：
- en: '![Running a Spark application on the EC2 cluster](img/00139.jpeg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![在EC2集群上运行Spark应用程序](img/00139.jpeg)'
- en: 'Figure 13: AWS key-pair generation window.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：AWS密钥对生成窗口。
- en: 'Name it `my-key-pair.pem` once you have downloaded it and saved it on your
    local machine. Then ensure the permission by executing the following command (you
    should store this file in a secure location for security purposes, say `/usr/local/key`):'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦下载并保存在本地机器上，请将其命名为`my-key-pair.pem`。然后通过执行以下命令确保权限（出于安全目的，您应该将此文件存储在安全位置，比如`/usr/local/key`）：
- en: '[PRE35]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now what you need is the AWS access keys, the credentials of your account,
    which are needed if you want to submit your Spark job to compute nodes from your
    local machine using spark-ec2 script. To generate and download the keys, login
    to your AWS IAM services at [http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey](http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey).
    Upon the download completion (that is, `/usr/local/key`), you need to set two
    environment variables in your local machine. Just execute the following commands:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您需要的是AWS访问密钥，您的帐户凭据，如果您想要使用spark-ec2脚本从本地机器提交您的Spark作业到计算节点。要生成和下载密钥，请登录到您的AWS
    IAM服务 [http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey](http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey)。下载完成后（即`/usr/local/key`），您需要在本地机器上设置两个环境变量。只需执行以下命令：
- en: '[PRE36]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '**Step 2: Configuring the Spark cluster on EC2**'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：在EC2上配置Spark集群**'
- en: Spark distribution (that is, `/usr/local/spark``/ec2`) provides a script called
    `spark-ec2` for launching Spark Clusters in EC2 instances from your local machine
    (driver program), which helps in launching, managing, and shutting down the Spark
    Cluster.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Spark分发（即`/usr/local/spark``/ec2`）提供了一个名为`spark-ec2`的脚本，用于从本地机器（驱动程序）在EC2实例上启动Spark集群，帮助启动、管理和关闭Spark集群。
- en: Note
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that starting a cluster on AWS will cost money. Therefore, it is
    always a good practice to stop or destroy a cluster when the computation is done.
    Otherwise, it will incur additional costs. For more about AWS pricing, please
    refer to this URL [https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在AWS上启动集群将花费金钱。因此，当计算完成时，停止或销毁集群始终是一个好习惯。否则，将产生额外的费用。有关AWS定价的更多信息，请参阅此URL
    [https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/)。
- en: 'Once you execute the following command to launch a new instance, it sets up
    Spark, HDFS, and other dependencies on the cluster automatically:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您执行以下命令启动新实例，它将自动在集群上设置Spark、HDFS和其他依赖项：
- en: '[PRE37]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We believe that these parameters are self-explanatory, or alternatively, please
    see details at [http://spark.apache.org/docs/latest/ec2-scripts.html](http://spark.apache.org/docs/latest/ec2-scripts.html). For
    our case, it would be something like this:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信这些参数是不言自明的，或者，也可以在[http://spark.apache.org/docs/latest/ec2-scripts.html](http://spark.apache.org/docs/latest/ec2-scripts.html)上查看详细信息。对于我们的情况，应该是这样的：
- en: '[PRE38]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'It is shown in the following screenshot:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如下截图所示：
- en: '![Running a Spark application on the EC2 cluster](img/00022.jpeg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![在EC2集群上运行Spark应用程序](img/00022.jpeg)'
- en: 'Figure 14: Cluster home.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：集群主页。
- en: After the successful completion, spark cluster will be instantiated with two
    workers (slave) nodes on your EC2 account. This task; however, sometimes might
    take half an hour approximately depending on your Internet speed and hardware
    configuration. Therefore, you'd love to have a coffee break. Upon successful competition
    of the cluster setup, you will get the URL of the Spark cluster on the terminal.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 成功完成后，spark集群将在您的EC2帐户上实例化两个工作（从属）节点。这个任务可能需要大约半个小时，具体取决于您的互联网速度和硬件配置。因此，您可能需要喝杯咖啡休息一下。在集群设置成功完成后，您将在终端上获得Spark集群的URL。
- en: To check to make sure if the cluster is really running, check this URL `https://<master-hostname>:8080`
    on your browser, where the master hostname is the URL you receive on the terminal.
    If everything was okay, you will find your cluster is running, see cluster home
    in *Figure 14*.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查集群是否真的在运行，请在浏览器上检查此URL `https://<master-hostname>:8080`，其中主机名是您在终端上收到的URL。如果一切正常，您将发现您的集群正在运行，参见*图14*中的集群主页。
- en: '**Step 3: Running and deploying Spark job on Spark Cluster**'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：在Spark集群上运行和部署Spark作业**'
- en: 'Execute the following command to the SSH remote Spark cluster:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下命令连接到SSH远程Spark集群：
- en: '[PRE39]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'For our case, it should be something like this:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的情况，应该是这样的：
- en: '[PRE40]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now copy your application (the jar we generated as the Maven project on Eclipse)
    to a remote instance (that is, `ec2-52-48-119-121.eu-west-1.compute.amazonaws.com`
    in our case) by executing the following command (in a new terminal):'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在通过执行以下命令将您的应用程序（我们在Eclipse上生成的Maven项目的jar包）复制到远程实例（在我们的情况下是`ec2-52-48-119-121.eu-west-1.compute.amazonaws.com`）（在新终端中）：
- en: '[PRE41]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then you need to copy your data (`/usr/local/data/input.txt` in our case) to
    the same remote instance by executing the following command:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您需要通过执行以下命令将您的数据（在我们的情况下是`/usr/local/data/input.txt`）复制到同一远程实例：
- en: '[PRE42]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![Running a Spark application on the EC2 cluster](img/00007.jpeg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![在EC2集群上运行Spark应用程序](img/00007.jpeg)'
- en: 'Figure 15: Job running status at Spark cluster.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：Spark集群中作业运行状态。
- en: 'Well done! You are almost done! Now, finally you will have to submit your Spark
    job to be computed by the slaves or worker nodes. To do so, just execute the following
    commands:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！您已经快完成了！现在，最后您需要提交您的Spark作业，让从节点或工作节点进行计算。要这样做，只需执行以下命令：
- en: '[PRE43]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Upon successful completion of the job computation, you are supposed to see the
    status of your job at port 8080 like *Figure 15* (the output will be shown on
    the terminal).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 作业计算成功完成后，您应该在端口8080上看到作业的状态，就像*图15*一样（输出将显示在终端上）。
- en: '**Step 4: Pausing and restarting spark cluster**'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 第四步：暂停和重新启动Spark集群
- en: 'To stop your clusters, execute the following command from your local machine:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止您的集群，请从本地机器执行以下命令：
- en: '[PRE44]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'For our case, it would be:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的情况，将会是：
- en: '[PRE45]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'To restart the cluster later on, execute the following command:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 要稍后重新启动集群，请执行以下命令：
- en: '[PRE46]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'For our case, it will be something as follows:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的情况，将会是以下内容：
- en: '[PRE47]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Tip
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: To terminate your spark cluster:`$./spark-ec2 destroy <cluster-name>`
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 要终止您的Spark集群：`$./spark-ec2 destroy <cluster-name>`
- en: In our case, it would be:` **$./spark-ec2 --region=eu-west-1 destroy ec2-spark-cluster-1**`
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，将是：`$./spark-ec2 --region=eu-west-1 destroy ec2-spark-cluster-1`
- en: If you want your application to scale up for large-scale datasets, the fastest
    way is to load them from Amazon S3 or an Amazon EBS device into an instance of
    the **Hadoop Distributed File System** (**HDFS**) on your nodes. We will discuss
    this technique in later chapters throughout practical machine learning examples.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望您的应用程序针对大规模数据集进行扩展，最快的方法是将它们从Amazon S3或Amazon EBS设备加载到节点上的Hadoop分布式文件系统（HDFS）中。我们将在后面的章节中通过实际的机器学习示例讨论这种技术。
- en: References
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Resilient Distributed Datasets*: *A Fault-Tolerant Abstraction for In-Memory
    Cluster Computing*, *Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin
    Ma, Murphy McCauley, Michael J*. *Franklin, Scott Shenker, Ion Stoica.**NSDI 2012**.
    April 2012.*'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*弹性分布式数据集*：*内存集群计算的容错抽象*，*Zaharia，Mosharaf Chowdhury，Tathagata Das，Ankur Dave，Justin
    Ma，Murphy McCauley，Michael J*. *Franklin，Scott Shenker，Ion Stoica。NSDI 2012*。2012年4月。'
- en: '*Spark*: *Cluster Computing with Working**Sets*, *Matei Zaharia, Mosharaf Chowdhury,
    Michael J*. *Franklin, Scott Shenker, Ion Stoica,**HotCloud 2010**. June 2010.*'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Spark*：*使用工作集进行集群计算*，*Matei Zaharia，Mosharaf Chowdhury，Michael J*. *Franklin，Scott
    Shenker，Ion Stoica，HotCloud 2010*。2010年6月。'
- en: '*Spark SQL*: *Relational Data Processing in Spark*, *Michael Armbrust, Reynold
    S. Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K. Bradley,* *Xiangrui Meng*,
    *Tomer Kaftan*, *Michael J. Franklin*, *Ali Ghodsi*, *Matei Zaharia*, *SIGMOD* 
    *2015\. June 2015.*'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Spark SQL*：*Spark中的关系数据处理*，*Michael Armbrust，Reynold S. Xin，Cheng Lian，Yin
    Huai，Davies Liu，Joseph K. Bradley，Xiangrui Meng，Tomer Kaftan，Michael J. Franklin，Ali
    Ghodsi，Matei Zaharia，SIGMOD 2015*。2015年6月。'
- en: '*Discretized Streams*: *Fault-Tolerant Streaming Computation at Scale,* *Matei
    Zaharia, Tathagata Das, Haoyuan Li, Timothy Hunter, Scott Shenker, Ion Stoica.**SOSP
    2013**. November 2013.*'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*离散化流*：*规模化的容错流计算*，*Matei Zaharia，Tathagata Das，Haoyuan Li，Timothy Hunter，Scott
    Shenker，Ion Stoica。SOSP 2013*。2013年11月。'
- en: '*Discretized Streams*: *An Efficient and Fault-Tolerant Model for Stream Processing
    on Large Clusters*. *Matei Zaharia, Tathagata Das, Haoyuan Li, Scott Shenker,
    Ion Stoica.**HotCloud 2012**. June 2012.*'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*离散化流*：*大规模集群上流处理的高效容错模型*。*Matei Zaharia，Tathagata Das，Haoyuan Li，Scott Shenker，Ion
    Stoica。HotCloud 2012*。2012年6月。'
- en: '*GraphX*: *Unifying Data-Parallel and Graph-Parallel Analytics*. *Reynold S.
    Xin*, *Daniel Crankshaw, Ankur Dave, Joseph E.**Gonzalez, Michael J. Franklin*,
    *Ion Stoica.**OSDI 2014**. October 2014.*'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*GraphX*：*统一数据并行和图并行分析*。*Reynold S. Xin，Daniel Crankshaw，Ankur Dave，Joseph
    E.* **Gonzalez，Michael J. Franklin**，*Ion Stoica。OSDI 2014*。2014年10月。'
- en: '*MLlib*: *Machine Learning in Apache Spark*, *Meng et al*. *arXiv:1505.06807v1,
    [cs.LG], 26 May 2015.*'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MLlib*：*Apache Spark中的机器学习*，*Meng等人*。*arXiv:1505.06807v1，[cs.LG]，2015年5月26日。'
- en: '*Recommender*: *An Analysis of Collaborative Filtering Techniques*, *Christopher
    R. Aberger*, *Stanford publication, 2014.*'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*推荐系统*：*协同过滤技术分析*，*Christopher R. Aberger*，*斯坦福出版物，2014年*。'
- en: Summary
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This ends our rather quick tour of Spark. We have tried to cover some of the
    most basic features of Spark, its computing paradigm, and getting started with
    Spark by installing and configuring. Use of Spark ML is recommended if they fit
    the ML pipeline concept well (for example, feature extraction, transformation,
    and selection) since it is more versatile and flexible with DataFrames and Datasets.
    However, according to Apache's documentations, they will keep supporting and contributing
    Spark MLib along with the active development of Spark ML.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对Spark的相当快速的介绍。我们已经尝试涵盖了Spark的一些最基本的特性，它的计算范式，并通过安装和配置开始使用Spark。如果Spark
    ML适合机器学习流水线概念（例如特征提取、转换和选择），则建议使用它，因为它在DataFrame和Dataset方面更加灵活多变。然而，根据Apache的文档，他们将继续支持和贡献Spark
    MLib，并与Spark ML的积极开发一起。
- en: On the other hand, data science developers should be comfortable with using
    Spark MLlib's features and should expect more features in the future. However,
    some algorithms are not available or are yet to be added to Spark ML, most notably,
    dimensionality reduction. Nevertheless, developers can seamlessly combine the
    implementation of these techniques found in Spark MLib with the rest of the algorithms
    found in Spark ML as hybrid or interoperable ML applications. We also showed some
    basic techniques to deploy ML applications on clusters and cloud services, though
    you can also try other deployment options available.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，数据科学开发人员应该熟悉使用Spark MLlib的特性，并期待将来有更多的特性。然而，一些算法目前尚不可用，或者有待添加到Spark ML中，尤其是降维。尽管如此，开发人员可以无缝地将这些在Spark
    MLib中找到的技术实现与Spark ML中找到的其他算法结合起来，作为混合或可互操作的机器学习应用。我们还展示了一些在集群和云服务上部署机器学习应用的基本技术，尽管您也可以尝试其他可用的部署选项。
- en: Tip
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: For more updates, interested readers should refer to the Spark website at [http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)
    for release dates, APIs, and specifications. As Spark's open source community
    and developers from all over the globe are continually enriching and updating
    the implementation, therefore, it is better to be updated.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多更新，请感兴趣的读者参考Spark网站[http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)获取发布日期、API和规范。由于Spark的开源社区和来自全球各地的开发人员不断丰富和更新实现，因此最好保持更新。
- en: In the next chapter ([Chapter 2](part0023_split_000.html#LTSU2-5afe140a04e845e0842b44be7971e11a
    "Chapter 2. Machine Learning Best Practices"), *Machine Learning Best Practices*),
    we will discuss some best practices while developing advanced machine learning
    with Spark, including machine learning tasks and classes, some practical machine
    learning problems and their related discussion, some best practices in machine
    learning application development, choosing the right algorithm for the ML application,
    and so on.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章（[第2章](part0023_split_000.html#LTSU2-5afe140a04e845e0842b44be7971e11a "第2章。机器学习最佳实践")，*机器学习最佳实践*），我们将讨论在使用Spark开发高级机器学习时的一些最佳实践，包括机器学习任务和类、一些实际的机器学习问题及其相关讨论、机器学习应用开发中的一些最佳实践、选择适合ML应用的正确算法等。
