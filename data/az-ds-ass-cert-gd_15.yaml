- en: '*Chapter 12*: Operationalizing Models with Code'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第12章*：使用代码将模型操作化'
- en: 'In this chapter, you are going to learn how to operationalize the machine learning
    models you have been training, in this book, so far. You will explore two approaches:
    exposing a real-time endpoint by hosting a REST API that you can use to make inferences
    and expanding your pipeline authoring knowledge to make inferences on top of big
    data, in parallel, efficiently. You will begin by registering a model in the workspace
    to keep track of the artifact. Then, you will publish a REST API; this is something
    that will allow your model to integrate with third-party applications such as
    **Power BI**. Following this, you will author a pipeline to process half a million
    records within a couple of minutes in a very cost-effective manner.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何将您到目前为止在本书中训练的机器学习模型进行操作化。您将探索两种方法：通过托管REST API暴露实时端点，您可以用它进行推理；并扩展您的管道编写知识，以高效地在大数据上进行并行推理。您将首先在工作区中注册一个模型以跟踪该工件。然后，您将发布一个REST
    API；这将使您的模型能够与第三方应用程序（如**Power BI**）集成。接下来，您将编写一个管道，以非常具成本效益的方式在几分钟内处理五十万条记录。
- en: 'In this chapter, we are going to cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding the various deployment options
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解各种部署选项
- en: Registering models in the workspace
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在工作区注册模型
- en: Deploying real-time endpoints
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署实时端点
- en: Creating a batch inference pipeline
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建批量推理管道
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will require access to an Azure subscription. Within that subscription,
    you will need a `packt-azureml-rg`. You will need to have either a `Contributor`
    or `Owner` `packt-learning-mlw`. These resources should be already available to
    you if you followed the instructions in [*Chapter 2*](B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026),
    *Deploying Azure Machine Learning Workspace Resources*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要访问Azure订阅。在该订阅中，您需要一个`packt-azureml-rg`。您还需要有`Contributor`或`Owner`权限的`packt-learning-mlw`。如果您按照[*第2章*](B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026)，*部署Azure机器学习工作区资源*的说明进行操作，这些资源应该已经为您准备好。
- en: Additionally, you will require a basic understanding of the **Python** language.
    The code snippets in this chapter target Python version 3.6 or later. You should
    also be familiar with working with notebooks within AzureML studio; this is something
    that was covered in [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102),
    *The AzureML Python SDK*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您需要具备基本的**Python**语言知识。本章中的代码片段适用于Python 3.6及更高版本。您还应当熟悉在AzureML Studio中使用笔记本的操作；这部分内容在[*第7章*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102)，*The
    AzureML Python SDK*中已有介绍。
- en: This chapter assumes you have registered the **loans** dataset that you generated
    in [*Chapter 10*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147), *Understanding
    Model Results*. It also assumes that you have created a compute cluster, named
    **cpu-sm-cluster**, as described in the *Working with compute targets* section
    of [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102), *The AzureML
    Python SDK*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设您已在[*第10章*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147)，*理解模型结果*中生成并注册了**loans**数据集。同时也假设您已按照[*第7章*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102)，*The
    AzureML Python SDK*中的“与计算目标协作”部分的说明，创建了一个名为**cpu-sm-cluster**的计算集群。
- en: Important note
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'AzureML is constantly being updated. If you face any issues with the code samples
    in this book, try upgrading your AzureML SDK by adding the following code into
    a new notebook cell:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: AzureML不断更新。如果您在使用本书中的代码示例时遇到任何问题，请尝试通过在新笔记本单元格中添加以下代码来升级AzureML SDK：
- en: '`!pip install --upgrade azureml-core azureml-sdk[notebooks]`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`!pip install --upgrade azureml-core azureml-sdk[notebooks]`'
- en: Then, restart the Jupyter kernel, as you learned in the *Training a loans approval
    model* section of [*Chapter 10*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147),
    *Understanding Model Results*. Additionally, try downloading the latest version
    of the notebooks from the GitHub page of this book. If the problem persists, feel
    free to open an issue on this book's GitHub page.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，重新启动Jupyter内核，如[*第10章*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147)，*理解模型结果*中“训练贷款审批模型”部分所述。此外，尝试从本书的GitHub页面下载最新版本的笔记本。如果问题仍然存在，可以在本书的GitHub页面上打开问题。
- en: You can find all of the notebooks and code snippets for this chapter on GitHub
    at [http://bit.ly/dp100-ch12](http://bit.ly/dp100-ch12).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上的[http://bit.ly/dp100-ch12](http://bit.ly/dp100-ch12)找到本章所有的笔记本和代码片段。
- en: Understanding the various deployment options
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解各种部署选项
- en: We have been working with Python code since [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*. So far, you have trained various models, evaluated
    them based on metrics, and saved the trained model using the `dump` method of
    the **joblib** library. The AzureML workspace allows you to store and version
    those artifacts by registering them in the model registry that we discussed in
    [*Chapter 5*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072), *Letting the Machines
    Do the Model Training*. Registering the model allows you to version both the saved
    model and the metadata regarding the specific model, such as its performance according
    to various metrics. You will learn how to register models from the SDK in the
    *Registering models in the workspace* section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从[*第8章*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117)开始，我们一直在使用Python代码，*实验Python代码*。到目前为止，你已经训练了各种模型，基于指标对其进行了评估，并使用**joblib**库的`dump`方法保存了训练好的模型。AzureML工作区允许你通过将它们注册到模型注册表中来存储和管理这些工件，正如我们在[*第5章*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072)中讨论的，*让机器进行模型训练*。注册模型允许你为保存的模型和与特定模型相关的元数据（如根据各种指标评估的性能）版本化。你将通过SDK学习如何在*工作区中注册模型*部分注册模型。
- en: 'Once the model has been registered, you have to decide how you want to operationalize
    the model, either by deploying a real-time endpoint or by creating a batch process,
    as displayed in *Figure 12.1*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被注册，你需要决定如何操作化模型，是通过部署实时端点还是创建批处理过程，如*图12.1*所示：
- en: '![Figure 12.1 – A path from training to operationalization'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.1 – 从训练到操作化的路径'
- en: '](img/B16777_12_001.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_12_001.jpg)'
- en: Figure 12.1 – A path from training to operationalization
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – 从训练到操作化的路径
- en: 'There are two main categories in terms of how a model processes incoming data:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型处理传入数据的方式上，主要有两种类别：
- en: '`predict_proba` method of the classifier you trained. You will read more about
    this scenario in the *Deploying real-time endpoints* section of this chapter.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你训练的分类器的`predict_proba`方法。你将在本章的*实时端点部署*部分深入了解这一场景。
- en: '**Batch inferences**: In [*Chapter 5*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072),
    *Letting the Machines Do the Model Training*, you trained an AutoML model to churn
    customer predictions. The model was trained using features such as the consumer''s
    activity over the last 6 to 12 months. Let''s suppose that you wanted to evaluate
    all of your customers and drive a marketing campaign for those who are likely
    to churn. You would have to run a once-off process that reads all of your customer
    information, calculates the required features, and then invokes the model for
    each of them to produce a prediction. The result can be stored in a CSV file to
    be consumed by the marketing department. In this approach, you only need the model
    for a short period of time, that is, only while making the predictions. You do
    not require a real-time endpoint, such as the one you deployed in [*Chapter 5*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072),
    *Letting the Machines Do the Model Training*, as you do not need the model to
    make ad hoc inferences. You can read more about this scenario in the *Creating
    a batch inference pipeline* section of this chapter.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量推理**：在[*第5章*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072)中，*让机器进行模型训练*，你训练了一个AutoML模型来预测客户流失。该模型使用了消费者过去6到12个月的活动等特征。假设你想评估所有客户并为可能流失的客户开展市场营销活动。你需要执行一次性处理过程，读取所有客户信息，计算所需特征，然后调用模型为每个客户生成预测结果。结果可以存储在CSV文件中，供营销部门使用。在这种方法中，你只需要模型短暂的使用时间，即仅在进行预测时。你不需要实时端点，如在[*第5章*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072)中部署的端点，因为你不需要模型进行临时推理。你可以在本章的*创建批量推理管道*部分了解更多关于这个场景的信息。'
- en: All models can be used in either real-time or batch inferences. It is up to
    you to decide whether you require ad hoc model inferences or a scheduled process
    that produces and stores the inference results. Operationalizing models in batch
    mode tends to be more cost-effective, as you can utilize low-priority compute
    clusters to perform inferences. In that scenario, you do not need to pay to have
    a real-time endpoint infrastructure waiting to make live inferences.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型都可以用于实时推理或批处理推理。你可以决定是否需要临时的模型推理，或是一个定期生成并存储推理结果的过程。批处理模式下的模型运营化通常更具成本效益，因为你可以使用低优先级计算集群来进行推理。在这种情况下，你不需要支付实时端点基础设施的费用以等待进行实时推理。
- en: In the next section, you will start the path to operationalization by training
    and registering the model that you will be using throughout the rest of the chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将通过训练和注册模型，开始迈向模型运营化的道路，该模型将贯穿本章的其余部分。
- en: Registering models in the workspace
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在工作区中注册模型
- en: 'Registering a model allows you to keep different versions of the trained models.
    Each model version has artifacts and metadata. Among the metadata, you can keep
    references to experiment with runs and datasets. This allows you to track the
    lineage between the data used to train a model, the run ID that trained the model,
    and the actual model artifacts themselves, as displayed in *Figure 12.2*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注册模型可以让你保留不同版本的训练模型。每个模型版本都包含工件和元数据。在这些元数据中，你可以保留对实验运行和数据集的引用。这使你能够追踪训练模型所使用的数据、训练该模型的运行
    ID，以及模型工件本身的谱系，如 *图 12.2* 所示：
- en: '![Figure 12.2 – Building the lineage from the training dataset all the way
    to the registered model'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.2 – 从训练数据集到已注册模型的谱系构建'
- en: '](img/B16777_12_002.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_12_002.jpg)'
- en: Figure 12.2 – Building the lineage from the training dataset all the way to
    the registered model
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 – 从训练数据集到已注册模型的谱系构建
- en: 'In this section, you will train a model and register it in your AzureML workspace.
    Perform the following steps:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将训练一个模型并将其注册到你的 AzureML 工作区中。请执行以下步骤：
- en: Navigate to the **Notebooks** section of your AzureML studio web interface.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入你的 AzureML Studio Web 界面中的**Notebooks**部分。
- en: Create a folder, named `chapter12`, and then create a notebook named `chapter12.ipynb`,
    as shown in *Figure 12.3*:![Figure 12.3 – Adding the chapter12 notebook to your
    working files
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `chapter12` 的文件夹，然后创建一个名为 `chapter12.ipynb` 的笔记本，如 *图 12.3* 所示：![图 12.3
    – 将 chapter12 笔记本添加到你的工作文件中
- en: '](img/B16777_12_003.jpg)'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16777_12_003.jpg)'
- en: Figure 12.3 – Adding the chapter12 notebook to your working files
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.3 – 将 chapter12 笔记本添加到你的工作文件中
- en: 'Add and execute the following code snippets in separate notebook cells. You
    will start by getting a reference to the workspace resources:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在单独的笔记本单元格中添加并执行以下代码片段。你将从获取工作区资源的引用开始：
- en: '[PRE0]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, you get a reference to a workspace, that is, the `chapter-12-train`.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，你获取了工作区的引用，即 `chapter-12-train`。
- en: 'Split the dataset into training and validation using the following code:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将数据集拆分为训练集和验证集：
- en: '[PRE1]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code splits the dataset into 80% training data and 20% validation data.
    The `seed` argument initializes the internal random state of the `random_split`
    method, allowing you to hardcode the data split and generate the same `training_data`
    and `validation_data` every time you invoke this code.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码将数据集拆分为 80% 的训练数据和 20% 的验证数据。`seed` 参数初始化 `random_split` 方法的内部随机状态，允许你硬编码数据拆分，并确保每次调用此代码时生成相同的
    `training_data` 和 `validation_data`。
- en: Here, `X_train` is a `pandas` `DataFrame` that contains the `income`, `credit_cards`,
    and `age` features (that is, all of the columns besides `approved_loan`).
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`X_train` 是一个 `pandas` `DataFrame`，包含 `income`、`credit_cards` 和 `age` 特征（即除
    `approved_loan` 外的所有列）。
- en: 'In comparison, `y_train` contains the values you want to predict. First, you
    load a `pandas` `DataFrame` that only contains the `approved_loan` column. Then,
    you convert that DataFrame into a `values` attribute. This array has a single
    element array for each row. For example, *[[0],[1]]* represents two records: a
    not-approved loan with a value of *0* and an approved one with a value of *1*.
    Following this, you call the `ravel` method to flatten the array, which converts
    the given example into *[0, 1]*. Although you could have used the `pandas` `DataFrame`
    directly to train the model, a warning message will inform you that an automatic
    convention has occurred, prompting you to use the `ravel` method that you observed
    in this cell.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相比之下，`y_train`包含的是你想要预测的值。首先，你加载一个只包含`approved_loan`列的`pandas` `DataFrame`。然后，你将该`DataFrame`转换为`values`属性。这个数组对每一行都有一个单一元素数组。例如，*[[0],[1]]*表示两个记录：一个未批准的贷款值为*0*，一个已批准的贷款值为*1*。接着，你调用`ravel`方法来扁平化这个数组，这样给定的例子就变成了*[0,
    1]*。尽管你本可以直接使用`pandas` `DataFrame`来训练模型，但系统会发出警告消息，提示你已进行自动转换，并建议你使用`ravel`方法，正如在这个单元中所看到的那样。
- en: The same process repeats for the `X_validate` DataFrame and the `y_validate`
    array that will be used to evaluate the model's performance.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于将用于评估模型性能的`X_validate` `DataFrame`和`y_validate`数组，重复相同的过程。
- en: 'Train a model and log the achieved accuracy using the following code:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码训练一个模型并记录得到的准确性：
- en: '[PRE2]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, you start with a run in the experiment, as defined in *Step 3*. You will
    use this run to register the metrics, logs, and artifacts of the model training
    process. Then, you train a `LogisticRegression` model, and you use the `accuracy_score`
    function to calculate the accuracy of the trained model. Following this, you print
    the calculated accuracy and log it as a metric in the run. In the end, you `complete`
    the run to finalize its execution.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，你从实验中的一次运行开始，如*步骤 3*所定义。你将使用这次运行来注册模型训练过程中的指标、日志和工件。然后，你训练一个`LogisticRegression`模型，并使用`accuracy_score`函数来计算训练后模型的准确性。接着，你打印出计算得到的准确性，并将其作为指标记录到运行中。最后，你`complete`该运行，以结束其执行。
- en: 'Now that you have a trained model referenced by the `sk_model` variable, you
    are going to save it using the following code:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你有一个由`sk_model`变量引用的训练模型，你将使用以下代码将其保存：
- en: '[PRE3]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: First, you create a folder named `model`. The name of that folder is not important.
    In that folder, you `dump` the trained model using the `joblib` library. The model
    is stored in a file named `model.joblib`.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，你创建一个名为`model`的文件夹。文件夹的名称不重要。在该文件夹中，你使用`joblib`库将训练好的模型`dump`到一个名为`model.joblib`的文件中。
- en: Important note
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: The`.joblib` filename extension is not standard, and you can use whatever you
    like as long as you are consistent. Some people use the `.pkl` filename extension
    instead, which was used in the past because we were serializing Python object
    structures using Python's built-in `pickle` module. Nowadays, the `joblib` library
    is the recommended way that is proposed by **scikit-learn** because it is more
    efficient in serializing large NumPy arrays, which is very common with the trained
    models.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`.joblib`文件扩展名不是标准的，你可以根据自己的需要使用任何名称，只要保持一致即可。有些人使用`.pkl`作为文件扩展名，这在过去是因为我们使用Python的内置`pickle`模块序列化Python对象结构。如今，**scikit-learn**推荐使用`joblib`库，因为它在序列化大型NumPy数组时更加高效，而这在训练模型时非常常见。'
- en: 'Now that you have the artifacts ready, you can register the model using the
    following code:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你已经准备好工件，可以使用以下代码注册模型：
- en: '[PRE4]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the first line, you import the `__version__` variable of the `sklearn` package,
    which is a string showing the version currently loaded in your environment. Then,
    you create an alias for that variable (using the `as` statement), and you reference
    it inside your code as `sk_version`. This is the version of the `sklearn` library
    that you used to train the model. Additionally, you import the `Model` class from
    the AzureML SDK to use it as a reference in the following lines.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在第一行，你导入`sklearn`包的`__version__`变量，它是一个字符串，表示当前环境中加载的版本。然后，你为该变量创建一个别名（使用`as`语句），并在代码中将其作为`sk_version`引用。这就是你用来训练模型的`sklearn`库的版本。此外，你还从AzureML
    SDK导入`Model`类，以便在后续的代码中使用它作为参考。
- en: After importing your references, you upload the contents of the local `./model`
    folder, which you created in *Step 6*, to the run's outputs, underneath a folder
    named `model`. This allows AzureML to have access to the artifacts that you are
    about to register; otherwise, you will receive an `ModelPathNotFoundException`
    error.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入引用之后，您上传本地`./model`文件夹的内容，该文件夹在*Step 6*中创建，到运行的输出中，位于名为`model`的文件夹下。这样AzureML可以访问您即将注册的工件；否则，您将收到`ModelPathNotFoundException`错误。
- en: Having all of the prerequisites ready, you can register the model. The model
    will be named `chapter12-loans` (the `model_name` argument) using the artifacts
    that just got uploaded in the `model` folder (the `model_path` argument) of the
    run's outputs. You specify the accuracy as both a tag (the `tags` argument) and
    a property (the `properties` argument) of that model. You indicate that you used
    the `SCIKITLEARN` framework (the `model_framework` argument) to train the model,
    and you specify which version of the framework you used (the `model_framework_version`
    argument). In the last line, you specify that you used the `loans_ds` dataset
    as a `training` dataset (the `datasets` argument).
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准备好所有先决条件后，您可以注册模型。模型将以`chapter12-loans`（`model_name` 参数）命名，使用刚刚上传到运行输出的`model`文件夹中的工件（`model_path`
    参数）。您将精度指定为模型的标签（`tags` 参数）和属性（`properties` 参数）。您指示使用`SCIKITLEARN`框架（`model_framework`
    参数）训练模型，并指定您使用的框架版本（`model_framework_version` 参数）。在最后一行，您指定使用`loans_ds`数据集作为`training`数据集（`datasets`
    参数）。
- en: Important note
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要说明
- en: If you try to rerun the same cell, a *Resource Conflict* error will occur because
    you cannot override files that already exist in the run's outputs folder. If you
    comment out the `upload_folder` line by using `#` as a line prefix and rerun the
    cell, you will register a new version of the same model, using the artifacts that
    already exist in the specific run.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果尝试重新运行相同单元格，则会发生*Resource Conflict*错误，因为无法覆盖已存在于运行输出文件夹中的文件。如果使用`#`作为行前缀注释掉`upload_folder`行并重新运行单元格，则会注册同一模型的新版本，使用已存在于特定运行中的工件。
- en: Navigate to `Model.Framework.SCIKITLEARN`). This type of deployment is considered
    a no-code deployment, which is a capability that AzureML offers for supported
    frameworks. Otherwise, you need to specify a scoring file; this is something that
    we will cover in the *Deploying real-time endpoints* section.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航至`Model.Framework.SCIKITLEARN`。这种部署类型被认为是无代码部署，这是AzureML为支持的框架提供的能力。否则，您需要指定一个评分文件；这是我们将在*Deploying
    real-time endpoints*部分中涵盖的内容。
- en: 'If you want to register a pretrained model that you downloaded from the internet,
    you will not have a `Run` object to call the `register_model` method. You can
    use the `register` method of the `Model` class, as demonstrated in the following
    code snippet:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果要注册从互联网下载的预训练模型，则没有`Run`对象来调用`register_model`方法。您可以使用`Model`类的`register`方法，如下面的代码片段所示：
- en: '[PRE5]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code, you register, in your AzureML workspace (the `ws` variable),
    the artifacts that are located inside the *local* `model` folder (the `model_path`
    argument) as a model named `chapter12-pre-trained-loans` (the `model_name` argument).
    This is a model trained using version `0.22.2.post1` (the `model_framework_version`
    argument) of the `sklearn` library (the `model_framework_version` argument). Additionally,
    its accuracy, which is `0.828`, is stored as a model property.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上述代码中，在AzureML工作区（`ws`变量）中注册位于*local* `model`文件夹内部的工件（`model_path` 参数）作为名为`chapter12-pre-trained-loans`的模型（`model_name`
    参数）。这是使用`sklearn`库的版本`0.22.2.post1`（`model_framework_version` 参数）训练的模型。另外，其精度为`0.828`，存储为模型属性。
- en: 'If you had a process to train new models, such as the scheduled pipeline that
    you created in [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160),
    *Working with Pipelines*, you will have to verify whether the newly trained model
    has better metrics than the one already registered. Then, if it is better, proceed
    with the registration of the model. To do that, you can use code similar to the
    following:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有一个训练新模型的流程，例如您在[*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160)中创建的计划管道，您需要验证新训练的模型是否比已注册的模型具有更好的指标。如果更好，则继续注册模型。为此，您可以使用类似以下代码的代码：
- en: '[PRE6]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code, you train a `RidgeClassifier`-based model that uses `chapter12-loans`,
    which you registered in *Step 7*. The `registered_model` variable has the same
    reference as the `model` variable you got in *Step 7*; only, this time, you create
    that reference using the `Model` class and not by registering a model. From that
    model, you read the `version` attribute and the `accuracy` property. You could
    retrieve the accuracy from the `tags` dictionary instead of the `properties` dictionary
    of the model. You convert the accuracy value into a float because tags and properties
    store their values as strings. Following this, you then compare the new model's
    accuracy to the one that has already been registered (which is stored in the `r_acc`
    variable). If the new model is better than the registered one, you print a message.
    In this case, you repeat *Step 6* and *Step 7* to store the model and then register
    the new, improved version of the model.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，你训练了一个基于 `RidgeClassifier` 的模型，使用的是你在 *第 7 步* 中注册的 `chapter12-loans`。`registered_model`
    变量与 *第 7 步* 中获得的 `model` 变量具有相同的引用；不过，这次你是通过 `Model` 类创建该引用，而不是通过注册模型来创建。你从该模型中读取
    `version` 属性和 `accuracy` 属性。你可以从模型的 `tags` 字典中检索准确度，而不是从 `properties` 字典中。由于标签和属性以字符串形式存储值，因此你需要将准确度值转换为浮动类型。接下来，你将新模型的准确度与已注册模型的准确度进行比较（已注册模型的准确度存储在
    `r_acc` 变量中）。如果新模型优于已注册的模型，则会打印一条消息。在这种情况下，你需要重复 *第 6 步* 和 *第 7 步* 来存储模型，并注册新版本的改进模型。
- en: Important note
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: To register a new model version, you just need to register the new model with
    the same name. By registering a new model with the same name, AzureML will automatically
    create a new version for you.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要注册一个新版本的模型，你只需使用相同的名称注册新模型。通过使用相同的名称注册新模型，AzureML 会自动为你创建一个新版本。
- en: 'Optionally, as a last step, delete the locally stored model using the following
    code:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地，作为最后一步，使用以下代码删除本地存储的模型：
- en: '[PRE7]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This code deletes the `model` folder that you created in *Step 6*, including
    the serialized model you no longer need. The `ignore_errors` parameter allows
    you to run this cell even if the folder doesn't exist without raising any errors.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码会删除你在 *第 6 步* 中创建的 `model` 文件夹，包括你不再需要的序列化模型。`ignore_errors` 参数允许你即使文件夹不存在也能运行此单元，而不会引发错误。
- en: In this section, you trained a model in your notebook within the Jupyter kernel.
    Then, you registered the model inside your workspace. You could have used the
    same registration code in the `train_model.py` script, which you authored in *Step
    11* of the *Authoring a pipeline* section of [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160),
    *Working with Pipelines*, to register the `run=Run.get_context()` method, and
    then you would need to upload the serialized model and register the model, as
    you did in *Step 7*. As an additional activity, try to modify the `train_model.py`
    script and `chapter11.ipynb` to create a pipeline that registers the model that
    is being trained within the pipeline. A potential solution to this activity is
    available in the `train_model_and_register.py` script. This can be found in the
    `step02` folder of the GitHub repository at [http://bit.ly/dp100-ch11](http://bit.ly/dp100-ch11).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你在 Jupyter 内核中训练了一个模型。然后，你在工作区内注册了该模型。你本可以在 `train_model.py` 脚本中使用相同的注册代码，这个脚本是你在[*第
    11 章*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160)中的 *创建管道* 部分的 *第 11 步* 中编写的，*与管道一起工作*，用来注册
    `run=Run.get_context()` 方法，然后你需要上传序列化的模型并注册模型，就像你在 *第 7 步* 中所做的那样。作为额外的活动，尝试修改
    `train_model.py` 脚本和 `chapter11.ipynb`，创建一个管道，将正在训练的模型在管道中进行注册。此活动的潜在解决方案可以在 `train_model_and_register.py`
    脚本中找到，位于 GitHub 仓库的 `step02` 文件夹中，链接为：[http://bit.ly/dp100-ch11](http://bit.ly/dp100-ch11)。
- en: In the next section, you will start operationalizing the model that you registered
    in this section by deploying it as a web service that will serve real-time inferences.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将开始将本节中注册的模型转化为可操作的模型，通过将其部署为一个 Web 服务来提供实时推断服务。
- en: Deploying real-time endpoints
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署实时端点
- en: Let's imagine that you have an e-banking solution that has a process for customers
    to request loans. You want to properly set the expectations of the customer and
    prepare them for potential rejection. When the customer submits their loan application
    form, you want to invoke the model you registered in the *Registering models in
    the workspace* section, that is, the model named **chapter12-loans**, and pass
    in the information that the customer filled out on the application form. If the
    model predicts that the loan will not be approved, a message will appear on the
    confirmation page of the loan request, preparing the customer for the potential
    rejection of the loan request.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个电子银行解决方案，其中包含一个让客户申请贷款的流程。你希望适当地设定客户的预期，并为可能的拒绝做准备。当客户提交贷款申请表时，你希望调用你在*在工作区注册模型*部分中注册的模型，即名为**chapter12-loans**的模型，并传入客户在申请表上填写的信息。如果模型预测贷款将不会被批准，贷款请求的确认页面将出现一条消息，提醒客户贷款请求可能会被拒绝。
- en: '*Figure 12.5* shows an oversimplified architecture to depict the flow of requests
    that start from the customer to the real-time endpoint of the model:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.5*展示了一个过于简化的架构，用以描绘从客户到模型实时端点的请求流向：'
- en: '![Figure 12.5 – An oversimplified e-banking architecture showing the flow'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.5 – 一个过于简化的电子银行架构，展示了请求流向'
- en: of requests from the customer to the model
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从客户到模型的请求流向
- en: '](img/B16777_12_005.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_12_005.jpg)'
- en: Figure 12.5 – An oversimplified e-banking architecture showing the flow of requests
    from the customer to the model
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 – 一个过于简化的电子银行架构，展示了从客户到模型的请求流向
- en: 'The easiest way to deploy a model is via the no-code deployment approach that
    AzureML offers for specific machine learning frameworks, including `sklearn` library
    that you used in the previous section. Perform the following steps:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 部署模型最简单的方法是通过AzureML提供的无代码部署方式，适用于特定的机器学习框架，包括你在上一部分中使用的`sklearn`库。请按照以下步骤操作：
- en: 'Go to the `chapter12.ipynb` notebook, and add the following code to get a reference
    to the last version of the **chapter12-loans** model that you created in the previous
    section:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到`chapter12.ipynb`笔记本，添加以下代码以获取对你在上一部分中创建的**chapter12-loans**模型的最新版本的引用：
- en: '[PRE8]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To deploy a real-time endpoint, use the following code:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要部署实时端点，请使用以下代码：
- en: '[PRE9]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This code deploys a new real-time endpoint service, named `no-code-loans`, and
    then waits for the deployment to complete.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该代码部署一个名为`no-code-loans`的新实时端点服务，然后等待部署完成。
- en: 'To get the scoring URI for the newly deployed endpoint, use the following code:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取新部署的端点的评分URI，请使用以下代码：
- en: '[PRE10]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is a URL in the format of [http://guid.region.azurecontainer.io/score](http://guid.region.azurecontainer.io/score),
    which accepts **POST** requests with a **JSON** payload, as follows:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个格式为[http://guid.region.azurecontainer.io/score](http://guid.region.azurecontainer.io/score)的URL，接受带有**JSON**负载的**POST**请求，如下所示：
- en: '[PRE11]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This payload will trigger an inference request for a customer with a monthly
    income of 2,000, who has 2 credit cards and is 45 years old. You can use tools
    such as **Postman** or **curl** to craft such an HTTP request and invoke the endpoint.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该负载将触发一个推理请求，针对一位月收入2000美元、拥有2张信用卡并且45岁的客户。你可以使用**Postman**或**curl**等工具来构造这样的HTTP请求，并调用端点。
- en: 'Instead of making an HTTP request using a tool such as `curl`, you can use
    the `no_code_service` reference and invoke the `run` method by passing in the
    JSON payload that you would normally send to the service:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与其使用如`curl`之类的工具发起HTTP请求，你还可以使用`no_code_service`引用，并通过传入通常会发送给服务的JSON负载来调用`run`方法：
- en: '[PRE12]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code imports the `json` library, which helps you to serialize
    objects into JSON strings. You create the payload using the `dumps` method. Note
    that the payload is slightly different from the simple version you saw in *Step
    3*. Instead of passing a single customer''s information, in this example, you
    pass the information of two customers: the one you passed before and another one
    who has *9* credit cards instead of *2*. Moreover, you are specifying which method
    to invoke. By default, the method name of the model is `predict`, which is the
    one you have been using in the previous chapters to make inferences. Finally,
    you print the output, which should appear similar to the following:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码导入了`json`库，它帮助你将对象序列化为JSON字符串。你使用`dumps`方法创建负载。请注意，这个负载与*步骤3*中你看到的简单版本略有不同。在这个示例中，你传递了两个客户的信息：一个是之前传递的客户，另一个拥有*9*张信用卡，而不是*2*张。此外，你还指定了要调用的方法。默认情况下，模型的方法名是`predict`，这是你在前几章中用来进行推理的方法。最后，你打印输出，应该看到类似以下的结果：
- en: '[PRE13]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The preceding result shows that the first loan will be rejected, while the second
    one will be approved.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述结果显示，第一个贷款将被拒绝，而第二个贷款将被批准。
- en: 'Most of the classification models offer another method called `predict_proba`,
    which returns an array with the probabilities of each label. In the `loans` approval
    case, this array will only contain 2 probabilities that sum to 1, that is, the
    probability of the loan getting approved and the probability of it getting rejected.
    If you change the method name from `predict` to `predict_proba` and re-execute
    the cell, you will get the following result:'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数分类模型提供了另一种方法，叫做`predict_proba`，它返回一个包含每个标签概率的数组。在`loans`审批的情况下，这个数组只包含两个概率，且总和为1，也就是贷款被批准的概率和贷款被拒绝的概率。如果你将方法名从`predict`更改为`predict_proba`并重新执行单元格，你将得到以下结果：
- en: '[PRE14]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preceding result shows that the model is 99.8% confident that the first
    loan will be rejected and 82.7% confident that the second loan will be approved.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述结果显示，模型99.8%确定第一个贷款将被拒绝，82.7%确定第二个贷款将被批准。
- en: Optionally, navigate to `chapter12-demanding-loans`. You specify that it needs
    `1` CPU and `1.5` GB of RAM. Note that if you deleted the `model` folder in *Step
    11* of the *Registering models in the workspace* section, this code would fail
    to register the new model, as it will not be able to find the model artifact.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地，导航到`chapter12-demanding-loans`。你指定它需要`1`个CPU和`1.5`GB的RAM。请注意，如果你在*步骤11*中的*在工作区注册模型*部分删除了`model`文件夹，那么这段代码将无法注册新模型，因为它找不到模型文件。
- en: 'To save on costs, you should delete the service using the following code:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了节省成本，你应该使用以下代码删除服务：
- en: '[PRE15]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So far, you have deployed a real-time endpoint using the no-code approach, which
    deploys the model as a container instance. This is only feasible if the model
    is trained using specific supported models. In the next section, you will learn
    how to deploy models using more advanced options.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经使用无代码方法部署了一个实时端点，这种方法将模型作为容器实例进行部署。只有当模型使用特定的支持模型进行训练时，这种方法才可行。在下一部分，你将学习如何使用更高级的选项来部署模型。
- en: Understanding the model deployment options
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解模型部署选项
- en: In the previous section, you deployed a model using the no-code approach. Behind
    the scenes, AzureML used an `environment` with all of the required model dependencies,
    which, in our case, was `sklearn`, generated a Python script to load the model
    and make inferences when data arrived at the endpoint, and published an ACI service
    using an `AciServiceDeploymentConfiguration` class.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，你使用无代码的方法部署了一个模型。在幕后，AzureML使用了一个包含所有必需模型依赖项的`environment`，在我们的例子中是`sklearn`，生成了一个Python脚本，用于加载模型并在数据到达端点时进行推理，并使用`AciServiceDeploymentConfiguration`类发布了一个ACI服务。
- en: 'If you had a model that was trained with a non-supported framework or if you
    wanted to get better control of the deployment model, you could deploy the model
    using the AzureML SDK classes, as depicted in *Figure 12.7*:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个使用不支持的框架训练的模型，或者你想更好地控制部署模型，你可以使用AzureML SDK类来部署模型，如*图12.7*所示：
- en: '![Figure 12.7 – The components required in a real-time endpoint deployment'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.7 – 实时端点部署所需的组件'
- en: '](img/B16777_12_007.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_12_007.jpg)'
- en: Figure 12.7 – The components required in a real-time endpoint deployment
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7 – 实时端点部署所需的组件
- en: Here, the **InferenceConfig** class specifies the model dependencies. It requires
    an entry script that will load the model and process the incoming requests along
    with an environment in which the entry script will execute. This environment contains
    all of the dependencies required by the model to load and make inferences.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**InferenceConfig**类指定了模型的依赖项。它需要一个入口脚本，该脚本将加载模型并处理传入的请求，以及一个脚本执行环境。该环境包含模型加载和进行推断所需的所有依赖项。
- en: 'The entry script should have the following two methods:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 入口脚本应包含以下两个方法：
- en: '`Init`: During this step, the script loads the trained model into memory. Depending
    on how you stored the state of the model to the disk, you can use the corresponding
    method to deserialize the model. For example, if you used the `joblib` library
    to serialize your model, you can use the `load` method of the same library to
    load it into memory. Some models provide their own serialization and deserialization
    methods, but the process remains the same; the state of the trained model is persisted
    in one file or multiple files, which you can later use to load the trained model
    into memory. Depending on how big your model is, the initialization phase might
    require a significant amount of time. Smaller `sklearn` models should load into
    memory in only a few milliseconds, while larger neural networks might require
    a couple of seconds to load.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Init`：在此步骤中，脚本将训练好的模型加载到内存中。根据您将模型的状态存储到磁盘的方式，您可以使用相应的方法将模型反序列化。例如，如果您使用`joblib`库序列化了模型，则可以使用相同库的`load`方法将其加载到内存中。一些模型提供自己的序列化和反序列化方法，但过程保持一致；训练好的模型的状态保存在一个或多个文件中，您可以稍后使用这些文件将模型加载到内存中。根据模型的大小，初始化阶段可能需要相当长的时间。较小的`sklearn`模型通常在几毫秒内加载到内存中，而较大的神经网络可能需要几秒钟的时间来加载。'
- en: '`run`: This is the method invoked when a dataset is received by the real-time
    endpoint for inference. In this method, you must use the model loaded in the `init`
    code to invoke the prediction method it offers to make inferences on the incoming
    data. As mentioned earlier, most of the models offer the `predict` method, which
    you can invoke and pass into the data that you want to make an inference. Most
    of the classification models offer an additional method, called `predict_proba`,
    which returns the probabilities of each class. The AutoML forecasting models offer
    the `forecast` method instead of the `predict` method. Neural networks have a
    different approach when it comes to making predictions. For example, in the first
    version of TensorFlow, you would have to invoke a prediction method through a
    `session.run()` method call.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`run`：这是实时端点接收数据集以进行推断时调用的方法。在此方法中，您必须使用`init`代码中加载的模型，调用它提供的预测方法，对传入的数据进行推断。如前所述，大多数模型提供`predict`方法，您可以调用并将要进行推断的数据传入。大多数分类模型还提供一个附加方法，称为`predict_proba`，它返回每个类别的概率。AutoML预测模型提供`forecast`方法，而不是`predict`方法。在神经网络中，进行预测的方法有所不同。例如，在TensorFlow的第一个版本中，您必须通过`session.run()`方法调用来进行预测。'
- en: 'Once you have configured the model dependencies, you need to decide where you
    want to deploy the model. The AzureML SDK offers three classes: `LocalWebserviceDeploymentConfiguration`,
    `AciServiceDeploymentConfiguration`, and `AksServiceDeploymentConfiguration`.
    These allow you to deploy on your local machine into ACI or **Azure Kubernetes
    Services** (**AKS**), as displayed in *Figure 12.8*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置了模型依赖项，您需要决定将模型部署到何处。AzureML SDK提供了三种类：`LocalWebserviceDeploymentConfiguration`、`AciServiceDeploymentConfiguration`和`AksServiceDeploymentConfiguration`。这些类允许您将模型部署到本地机器、ACI或**Azure
    Kubernetes 服务**（**AKS**），如*图 12.8*所示：
- en: '![Figure 12.8 – Picking the right compute target for your model'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.8 – 为您的模型选择合适的计算目标'
- en: '](img/B16777_12_008.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_12_008.jpg)'
- en: Figure 12.8 – Picking the right compute target for your model
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.8 – 为您的模型选择合适的计算目标
- en: As you might have gathered, in *Figure 12.8*, you can deploy to your local computer
    by specifying the port you want the service to listen to. This is a nice approach
    in which to debug any potential loading issues of your model or verify the integration
    with the remaining systems on your local computer. The next option is to use ACI,
    which is meant for test environments or small-scale production environments. You
    can only use CPUs and not GPUs in the `AciServiceDeploymentConfiguration` class.
    You can protect the endpoint using a key-based authentication by setting the `auth_enabled`
    parameter to `True`. This authentication method requires you to pass a static
    key as an **Authorization** header into your HTTP requests.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能已经理解的那样，在*图 12.8*中，您可以通过指定您希望服务监听的端口来部署到本地计算机。这是一种很好的方法，可以调试模型加载的潜在问题，或验证与本地计算机上其他系统的集成。下一个选项是使用
    ACI，它用于测试环境或小规模的生产环境。在`AciServiceDeploymentConfiguration`类中，您只能使用 CPU，而不能使用 GPU。您可以通过将`auth_enabled`参数设置为`True`来使用基于密钥的认证保护端点。此认证方法要求您将静态密钥作为**Authorization**头部传递到
    HTTP 请求中。
- en: On the other side, `AksServiceDeploymentConfiguration` deploys the service inside
    an AKS cluster. This allows you to use GPUs if your model can make use of them
    and if the cluster you are deploying to has GPU-capable nodes. This deployment
    configuration allows you to choose between key-based authentication or a token-based
    one. Token-based authentication requires the end user to acquire an access token
    from the **Azure Active Directory** that protects the AzureML workspace, which
    will allow you to access the endpoint deployed within it. This token is short-lived
    and conceals the caller's identity in contrast to key-based authentication, which
    is the only available option in ACI. Another production-ready feature of the AKS
    deployment is the ability to dynamically scale up and down to handle the fluctuation
    in the number of incoming requests. In the e-banking scenario at hand, customers
    tend to visit the e-banking solution during working hours, and the system is pretty
    much idle at night. Moreover, at the end of the month, the incoming traffic peaks.
    In such a workload, you want to be able to scale your endpoint to accommodate
    for the increase in traffic when needed. AKS can automatically spin up multiple
    containers of your model and load balance the traffic among them when the incoming
    traffic increases significantly. When the traffic returns to normal, it can only
    keep a single container as a hot standby for potential incoming traffic.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`AksServiceDeploymentConfiguration`将在 AKS 集群内部署服务。这使得您可以使用 GPU，前提是您的模型能够利用
    GPU，并且您要部署的集群有支持 GPU 的节点。该部署配置允许您选择基于密钥的认证或基于令牌的认证。基于令牌的认证要求最终用户从保护 AzureML 工作区的**Azure
    Active Directory**获取访问令牌，这样就可以访问部署在其中的端点。与基于密钥的认证不同，基于令牌的认证令牌生命周期较短，并隐藏了调用者的身份，而基于密钥的认证是
    ACI 中唯一可用的选项。AKS 部署的另一个生产就绪特性是能够根据传入请求数量的波动动态扩展和缩减。在当前的电子银行场景中，客户倾向于在工作时间访问电子银行解决方案，而在夜间系统几乎处于空闲状态。此外，在月底，传入流量会达到峰值。在这种工作负载下，您希望能够根据需要扩展端点，以适应流量的增加。当流量显著增加时，AKS
    可以自动启动多个模型容器，并在它们之间进行负载均衡。当流量恢复正常时，它可以仅保留一个容器作为潜在流量的热备份。
- en: 'Now that you have a better understanding of the deployment options, you will
    deploy the same model in ACI using the classes that you saw in *Figure 12.7*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您对部署选项有了更好的理解，接下来您将使用在*图 12.7*中看到的类，在 ACI 中部署相同的模型：
- en: The first thing you will need to create is the entry script. Underneath the
    **chapter12** folder, create a new folder named **script** and place a **score.py**
    file inside it, as shown in *Figure 12.9*:![Figure 12.9 – Adding the score.py
    file for the real-time endpoint
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您需要创建的是入口脚本。在**chapter12**文件夹下，创建一个名为**script**的新文件夹，并将一个**score.py**文件放入其中，如*图
    12.9*所示：![图 12.9 – 为实时端点添加 score.py 文件
- en: '](img/B16777_12_009.jpg)'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16777_12_009.jpg)'
- en: Figure 12.9 – Adding the score.py file for the real-time endpoint
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.9 – 为实时端点添加 score.py 文件
- en: In the `init` method, you are getting the path of the serialized model using
    the `AZUREML_MODEL_DIR` environment variable. When AzureML spins up the Docker
    image that will be serving the model, this variable points to the folder where
    the model is located; for example, `/tmp/azureml_umso8bpm/chapter12-loans/1` could
    be the location where you find the first version of the `chapter12-loans` model.
    In that folder, the actual artifact, named `model.joblib`, is located in the `model`
    folder, which you uploaded in *Step 5* of the *Deploying real-time endpoints*
    section. You use `os.path.join` to get the final path of the model, and then you
    load the model in a `global` variable named `model`. If you want to use the AzureML
    SDK to get the location of the model, you could use `model_path = Model.get_model_path(model_name,
    version=version)`, which uses the same environment variable under the hood. However,
    note that you would need to install the AzureML SDK in your environment to be
    able to import the `Model` class from it; this is something that is not necessary
    with the preceding code.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`init`方法中，你通过`AZUREML_MODEL_DIR`环境变量获取序列化模型的路径。当AzureML启动用于提供模型的Docker镜像时，该变量指向模型所在的文件夹；例如，`/tmp/azureml_umso8bpm/chapter12-loans/1`可能是你找到`chapter12-loans`模型第一版的地方。在该文件夹中，实际的文件`model.joblib`位于你在*部署实时端点*部分的*步骤5*中上传的`model`文件夹中。你使用`os.path.join`来获取模型的最终路径，然后将模型加载到名为`model`的`global`变量中。如果你想使用AzureML
    SDK来获取模型的位置，你可以使用`model_path = Model.get_model_path(model_name, version=version)`，该方法在底层使用相同的环境变量。然而，请注意，你需要在环境中安装AzureML
    SDK，以便能够导入其中的`Model`类；而前面的代码不需要这么做。
- en: Important note
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要说明
- en: Note that you are using `print` to write the model path and incoming `raw_data`
    into the console. You will learn how to view those messages in the *Monitoring
    with Application Insights* section.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，你正在使用`print`将模型路径和传入的`raw_data`写入控制台。你将在*通过应用程序洞察进行监控*部分学习如何查看这些消息。
- en: In the `run` method, you are using the `try` `except` block to catch potential
    errors while trying to read the request's input. If such an error occurs, the
    exception is serialized into a string (using the `str()` method), which is returned
    to the end user. Note that returning the exceptions to the caller is a security
    anti-pattern, as you might accidentally expose valuable information to a potential
    attacker, but it is helpful while debugging. Instead of returning the error message,
    you could use a `print` statement or a more advanced library such as `try` block,
    you deserialize the incoming JSON payload, as demonstrated in *Step 3* of the
    *Deploying real-time endpoints* section. Then, you call the `predict` method of
    the `model` object you have loaded in memory through the `init` method. Following
    this, you return the model results as a list that will be serialized into an array.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在`run`方法中，你使用`try` `except`块来捕捉在尝试读取请求输入时可能发生的错误。如果发生这样的错误，异常会被序列化为字符串（使用`str()`方法），并返回给最终用户。请注意，将异常返回给调用者是一种安全反模式，因为你可能会不小心向潜在的攻击者暴露有价值的信息，但在调试时这非常有用。你可以使用`print`语句或者更高级的库来代替返回错误消息，例如`try`块，在该块中，你会反序列化传入的JSON负载，如*部署实时端点*部分的*步骤3*所示。然后，你调用通过`init`方法加载到内存中的`model`对象的`predict`方法。接着，你将模型结果作为一个列表返回，并将其序列化为数组。
- en: Important note
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要说明
- en: You will never directly invoke either the `init` or the `run` method. There
    is another piece of code that AzureML will be putting inside the final Docker
    image, which is the HTTP inference server. This server will be responsible for
    calling your `init` method when the server boots up and will pass the incoming
    HTTP data into the `run` method. Moreover, the result that you return in the `run`
    method will be serialized into a **JSON** and will be returned to the caller.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你永远不会直接调用`init`方法或`run`方法。AzureML会在最终的Docker镜像中放入另一段代码，那就是HTTP推理服务器。该服务器负责在服务器启动时调用你的`init`方法，并将传入的HTTP数据传递到`run`方法中。此外，你在`run`方法中返回的结果将被序列化为**JSON**并返回给调用者。
- en: 'The next thing you need is an `Environment` that has all of the necessary dependencies
    to run the `score.py` script that you created. Open your `chapter12.ipynb` notebook
    and add the following code inside a new cell:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来你需要的是一个包含所有必要依赖项的`Environment`，用于运行你创建的`score.py`脚本。打开你的`chapter12.ipynb`笔记本并在新单元格中添加以下代码：
- en: '[PRE16]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the preceding code, you created an `Environment` as demonstrated in [*Chapter
    8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117), *Experimenting with Python
    Code*. You add the `scikit-learn` `azureml-defaults` `pip` package, which contains
    the necessary functionality to host the model as a web service. Because you are
    building your own `Environment`, you need to add this package and use, at the
    very least, version 1.0.45\. This is the bare minimum environment that you can
    use to run your scoring script. Additionally, AzureML provides a curated environment
    that you can use, such as `AzureML-sklearn-0.24.1-ubuntu18.04-py37-cpu-inference`,
    which contains everything you need to make an inference request using a model
    trained in `sklearn` version 0.24.1.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，你创建了一个`Environment`，如在[*第8章*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117)中演示的，*使用Python代码进行实验*。你添加了`scikit-learn`、`azureml-defaults`、`pip`包，这些包包含了将模型作为Web服务托管所需的功能。由于你正在构建自己的`Environment`，因此需要添加此包，并至少使用1.0.45版本。这是运行评分脚本时你可以使用的最低环境。此外，AzureML还提供了一个已策划的环境，例如`AzureML-sklearn-0.24.1-ubuntu18.04-py37-cpu-inference`，它包含了使用训练过的`sklearn`模型（版本0.24.1）进行推理请求所需的一切。
- en: 'You have defined everything that is needed by the `InferenceConfig` class.
    Add a new cell and type in the following code to put everything together:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你已经定义了`InferenceConfig`类所需的一切。添加一个新的单元格，并输入以下代码以将所有内容组合起来：
- en: '[PRE17]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This code creates the configuration you will need to make inferences with your
    model. It uses the `score.py` file that is located inside the `script` folder
    and executes that file in the `myEnv` environment, which you defined in *Step
    3*.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码创建了你在模型推理时所需要的配置。它使用位于`script`文件夹中的`score.py`文件，并在*步骤3*中定义的`myEnv`环境中执行该文件。
- en: 'Now you have two out of the three components depicted in *Figure 12.7*. In
    this step, you will create an `AciServiceDeploymentConfiguration` class, and you
    will deploy the model in ACI. In a new cell, add the following code:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你已经完成了*图12.7*中所示的三个组件中的两个。在此步骤中，你将创建一个`AciServiceDeploymentConfiguration`类，并将模型部署到ACI。在新单元格中，添加以下代码：
- en: '[PRE18]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we use the `AciWebservice` class to get a deployment configuration for
    the container instance you want to deploy. In the preceding code, you specify
    that you require 1 CPU core and 1 GB of RAM. Then, you deploy the model into a
    new service, named `aci-loans`, and you wait for the deployment to complete.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用`AciWebservice`类来获取你想要部署的容器实例的部署配置。在前面的代码中，你指定了需要1个CPU核心和1GB的内存。然后，你将模型部署到一个名为`aci-loans`的新服务中，并等待部署完成。
- en: Important note
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'If you are running into issues while attempting to deploy the container, you
    can view the error messages in the printed outputs or use the `service.get_logs()`
    method. Most likely, it is an issue with your code base within the `score.py`
    script. You can locally test the code by installing the `azureml-inference-server-http`
    pip package and running the following command:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果在尝试部署容器时遇到问题，你可以查看打印输出中的错误信息，或使用`service.get_logs()`方法。很可能是`score.py`脚本中的代码库出现了问题。你可以通过安装`azureml-inference-server-http`
    pip包并运行以下命令来在本地测试代码：
- en: '`5001`. Another approach to debugging such situations is to use `LocalWebservice`,
    as we will discuss later. If your code is fine, then you might be running into
    memory issues. This should be visible in the service logs. In that case, refer
    to the next section to learn how you can profile your model to determine its resource
    requirements.'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`5001`。调试这种情况的另一种方法是使用`LocalWebservice`，稍后我们将讨论。如果你的代码没有问题，那么可能是内存问题。这应该可以在服务日志中看到。在这种情况下，请参阅下一节，了解如何对模型进行分析，以确定其资源需求。'
- en: 'To test the deployed service, you can use the following code, which is similar
    to the one that you used in the previous section:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要测试已部署的服务，你可以使用以下代码，这与前一节中使用的代码类似：
- en: '[PRE19]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that the `method` property of the payload, which you used in *Step 4* of
    the *Deploying real-time endpoints* section, will not have any effect on this
    deployment and is omitted from the payload. If you wanted to support this property,
    you would have to write the code within the `run` method of the `score.py` file
    to read that property and call the corresponding method of the model.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，负载中的`method`属性（你在*步骤4*中使用的*部署实时端点*部分）不会对这次部署产生任何影响，并且会从负载中省略。如果你想支持此属性，则需要在`score.py`文件的`run`方法中编写代码来读取该属性并调用模型的相应方法。
- en: 'To save on costs, delete the service when you are done testing it using the
    following code:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了节省成本，在完成测试后使用以下代码删除服务：
- en: '[PRE20]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If you want to deploy the same service in your local computer, you can use
    the following code:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你想在本地计算机上部署相同的服务，可以使用以下代码：
- en: '[PRE21]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Instead of the `AciWebservice` class, you use the `LocalWebservice` to create
    a local service that listens to port `1337`. If you are running the notebooks
    on your local computer, you need to visit `http://localhost:1337` and view the
    service endpoint's health status. Now that you have run this code within the AzureML
    notebooks, the local computer is the compute instance you are working on. To view
    port `1337` of the compute instance named `service.delete()` code, as demonstrated
    in *Step 7*.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你不再使用`AciWebservice`类，而是使用`LocalWebservice`来创建一个监听端口`1337`的本地服务。如果你在本地计算机上运行笔记本，你需要访问`http://localhost:1337`并查看服务端点的健康状况。现在，在AzureML笔记本中运行了这段代码，本地计算机就是你正在使用的计算实例。要查看名为`service.delete()`的计算实例的端口`1337`，请参见*步骤7*。
- en: Similar to the `AciWebservice` and the `LocalWebservice`, you can use `AksWebservice`
    to create an `AksServiceDeploymentConfiguration`. While deploying it, you would
    need to specify an additional parameter in the `Model.deploy` method, that is,
    the `deployment_target` parameter. This parameter allows you to specify the `AksCompute`
    inference cluster that you want to deploy the model to.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`AciWebservice`和`LocalWebservice`，你还可以使用`AksWebservice`来创建`AksServiceDeploymentConfiguration`。在部署时，你需要在`Model.deploy`方法中指定一个额外的参数，即`deployment_target`参数。这个参数允许你指定想要将模型部署到的`AksCompute`推理集群。
- en: Aside from the local computer, ACI, and AKS deployment options that you saw
    earlier, AzureML offers multiple other deployment options. For example, **Azure
    Functions** allows you to run your models inside a serverless infrastructure,
    and **Azure App Services** hosts the model as a traditional web application that
    is always ready to serve incoming requests. On the other hand, you can use **IoT
    Edge**, which allows you to deploy the service on an Edge device such as a Raspberry
    Pi or a GPU-based Jetson Nano. Finally, you can even package the model inside
    a Docker container image, which can be operationalized inside an isolated air
    gap data center.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 除了你之前看到的本地计算机、ACI和AKS部署选项，AzureML还提供了多种其他部署选项。例如，**Azure Functions**允许你在无服务器架构中运行模型，**Azure
    App Services**将模型托管为传统的Web应用程序，随时准备处理传入的请求。另一方面，你可以使用**IoT Edge**，它允许你将服务部署到边缘设备，如树莓派或基于GPU的Jetson
    Nano。最后，你甚至可以将模型打包到Docker容器镜像中，这可以在隔离的空气间数据中心内运行。
- en: In this section, you deployed an ACI real-time inference endpoint requesting
    1 CPU core and 1 GB of RAM. In the next section, you will explore how you can
    optimize your resource requirements by profiling the model's performance.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你已部署了一个ACI实时推理端点，要求1个CPU核心和1 GB的内存。在接下来的章节中，你将探讨如何通过对模型性能的分析来优化资源需求。
- en: Profiling the model's resource requirements
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对模型资源需求的分析
- en: Before bringing something into production, it is very common to perform a stress
    test. Essentially, this test bombards the real-time endpoint with requests and
    measures the responsiveness and performance of the endpoint. You can do something
    similar with your models to understand what type of resources you will need in
    order for them to perform as expected. For example, you might need to ensure that
    all inferences are performed within 200 ms.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在将某些东西投入生产之前，执行压力测试是非常常见的做法。本质上，这种测试会向实时端点发送大量请求，并衡量该端点的响应能力和性能。你可以对模型进行类似的操作，了解它们在预期的性能下需要什么类型的资源。例如，你可能需要确保所有推理都在200毫秒内完成。
- en: In this section, you are going to create a test dataset that will be used to
    stress-test the real-time endpoint and observe its performance. Each row in the
    dataset will contain a single inference request.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将创建一个测试数据集，该数据集将用于压力测试实时端点并观察其性能。数据集中的每一行将包含一个单独的推理请求。
- en: 'Navigate to your `chapter12.ipynb` notebook and perform the following steps:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 打开你的`chapter12.ipynb`笔记本，并执行以下步骤：
- en: 'In a new cell, add the following code:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个新单元格中，添加以下代码：
- en: '[PRE22]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This code loads the `loans` dataset, drops the `approved_loan` column, which
    we don''t need, and loads it inside a `pandas` `DataFrame`. Following this, you
    create a new column named `sample_request` that concatenates the columns to produce
    a string such as the following:'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码加载`loans`数据集，删除我们不需要的`approved_loan`列，并将其加载到`pandas`的`DataFrame`中。接着，你创建一个名为`sample_request`的新列，将各个列连接起来，生成如下字符串：
- en: '[PRE23]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Then, you keep only that column and print the top 5 rows to verify that the
    requests look as expected. Note that it does not matter whether the data is the
    one we used to train the model. It could even be random records. We only care
    about the number of requests we will be making and not what the inference result
    is going to look like.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，你只保留该列并打印前五行，以验证请求是否符合预期。请注意，数据是否为我们用来训练模型的数据并不重要。它甚至可以是随机记录。我们关心的仅仅是将要发出的请求数量，而不是推断结果的样子。
- en: 'Store the newly created dataset inside the workspace using the following code:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将新创建的数据集存储在工作区中：
- en: '[PRE24]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding code registers the DataFrame as the `loans-requests` dataset.
    The data is stored inside `/samples/loans-requests` of the default datastore.
    The `loans_req_ds` variable has a reference to the newly registered `tabular`
    `Dataset`.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码将DataFrame注册为`loans-requests`数据集。数据存储在默认数据存储中的`/samples/loans-requests`目录下。`loans_req_ds`变量引用了新注册的`tabular`数据集。
- en: 'Now that you have the necessary data, you can start the model profiling process
    using the following code:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你已经拥有了必要的数据，可以使用以下代码开始模型分析过程：
- en: '[PRE25]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that the profile method requires the `model` and the `inference_config`
    that you used during model deployment in the previous section. Additionally, you
    need to specify your ACI size to use to perform the analysis. In the preceding
    code, you request 2 CPUs and 1 GB of RAM. The analysis could take a long time,
    sometimes, more than 20 minutes. After the analysis completes, you will view the
    results, including the 1 CPU as the `recommendedCpu` and 0.5 GB of RAM as the
    `recommendedMemoryInGB` value.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，profile方法需要在前一部分模型部署时使用的`model`和`inference_config`。此外，你还需要指定用于分析的ACI大小。在前面的代码中，你请求了2个CPU和1
    GB的RAM。分析可能需要较长时间，有时超过20分钟。分析完成后，你将查看结果，包括作为`recommendedCpu`的1个CPU和作为`recommendedMemoryInGB`值的0.5
    GB RAM。
- en: Important note
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要说明
- en: The name of the model profile should be unique within the workspace. An error
    will occur if you try to rerun the code of *Step 3* without changing the name.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型分析的名称在工作区内应是唯一的。如果你在不更改名称的情况下重新运行*步骤3*的代码，将会发生错误。
- en: Behind the scenes, an experiment named `ModelProfile` run is executed, which
    deploys an ACI service with the model. Once the service is up and running, the
    process sends the 500 requests that you specified in the **loan_req_ds** dataset
    and records the response time of the model while monitoring the CPU and memory
    utilization of the deployed container instance. AzureML can suggest the recommended
    CPU and memory that you should configure for your real-time endpoint based on
    those statistics.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在后台，执行了一个名为`ModelProfile`的实验，它会部署一个带有模型的ACI服务。服务启动并运行后，过程会发送你在**loan_req_ds**数据集中指定的500个请求，并记录模型的响应时间，同时监控已部署容器实例的CPU和内存使用情况。根据这些统计数据，AzureML可以建议你为实时端点配置的推荐CPU和内存。
- en: In the next section, you will use those values to deploy an ACI service. Following
    this, you will explore how to monitor its performance once deployed in production
    and log the incoming data using **Application Insights**.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，你将使用这些值来部署ACI服务。随后，你将探讨如何在部署后监控其性能，并使用**Application Insights**记录传入的数据。
- en: Monitoring with Application Insights
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Application Insights进行监控
- en: As you learned in [*Chapter 2*](B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026),
    *Deploying Azure Machine Learning Workspace Resources*, when you deploy the AzureML
    workspace, an Application Insights account named `packtlearningm<random_number>`
    is deployed in the same resource group. This Azure resource allows you to monitor
    the performance of your applications. Especially for web applications, such as
    the real-time endpoint you are deploying, Application Insights allows you to monitor
    the request and response times, the failure rate of the endpoint, any potential
    exceptions raised in your code, and even log traces that you want to emit from
    your code base.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[*第2章*](B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026)中学到的，*部署Azure机器学习工作区资源*，当你部署AzureML工作区时，会在同一资源组中部署一个名为`packtlearningm<random_number>`的应用程序洞察账户。这个Azure资源使你能够监控应用程序的性能。特别是对于像你正在部署的实时端点这样的Web应用程序，应用程序洞察可以让你监控请求和响应时间、端点的失败率、代码中可能引发的任何异常，甚至是你希望从代码库中输出的日志痕迹。
- en: 'In the *Understanding the model deployment options* section earlier, you created
    a `score.py` file that contained a couple of `print` statements. These messages
    were written inside the console of the endpoint and could be found either by calling
    the `service.get_logs()` method or navigating to the **Deployment logs** tab of
    the deployment, as shown in *Figure 12.10*:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的*理解模型部署选项*部分中，你创建了一个包含几个`print`语句的`score.py`文件。这些信息被写入端点的控制台中，可以通过调用`service.get_logs()`方法或者导航到**部署日志**标签页来查看，如*图
    12.10*所示：
- en: '![Figure 12.10 – The model path and incoming raw_data logged in the console
    of the container instance'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.10 – 容器实例控制台中记录的模型路径和传入的原始数据'
- en: '](img/B16777_12_010.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_12_010.jpg)'
- en: Figure 12.10 – The model path and incoming raw_data logged in the console of
    the container instance
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.10 – 容器实例控制台中记录的模型路径和传入的原始数据
- en: The problem with this approach is that the logs do not persist. If you redeploy
    the container instance, you will lose the logs. Moreover, if you have multiple
    models deployed, you will need a centralized place to be able to monitor all of
    them together. These are two of the many benefits that Application Insights brings
    to your solution.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的问题在于日志不会持久化。如果你重新部署容器实例，日志将会丢失。而且，如果你部署了多个模型，你将需要一个集中式的地方来监控它们。应用程序洞察为你的解决方案带来了这些及更多的好处。
- en: 'Go back to your `chapter12.ipynb` notebook to redeploy the ACI container and
    enable Application Insights for it. Inside a new cell, add the following code:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 返回你的`chapter12.ipynb`笔记本，重新部署ACI容器并为其启用应用程序洞察。在一个新的单元格中，添加以下代码：
- en: '[PRE26]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Notice that you are using the `1` CPU core and `0.5` GB RAM that was recommended
    in the *Profiling the model resource requirements* section. Additionally, note
    that you are enabling Application Insights in the deployment configuration by
    passing the `enable_app_insights= True` argument. If you had already deployed
    the service and you wanted to enable Application Insights for it, you could use
    the following code to update its configuration:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你正在使用在*分析模型资源需求*部分中推荐的`1`个CPU核心和`0.5`GB的内存。另外，注意你正在通过传递`enable_app_insights=True`参数来在部署配置中启用应用程序洞察。如果你已经部署了服务并希望启用应用程序洞察，你可以使用以下代码更新其配置：
- en: '[PRE27]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s send a couple of requests to the service to be able to better understand
    what Application Insights can do for you. Inside a new cell, add the following
    code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们向服务发送几个请求，以便更好地理解应用程序洞察能为你做些什么。在一个新的单元格中，添加以下代码：
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This code sends *10* identical requests to the service, one after the other,
    generating some artificial traffic that should be logged in Application Insights.
    The easiest way to find the URL that is pointing to the Azure portal and directly
    inside the Application Insights resource is to visit the endpoint''s information
    page, as displayed in *Figure 12.11*:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码向服务发送了*10*个相同的请求，一个接一个地生成一些人工流量，这些流量应该会被记录在应用程序洞察中。找到指向Azure门户并直接进入应用程序洞察资源的URL的最简单方法是访问端点的信息页面，如*图
    12.11*所示：
- en: '![Figure 12.11 – The Application Insights URL that is associated with your
    AzureML workspace'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.11 – 与你的AzureML工作区关联的应用程序洞察URL'
- en: '](img/B16777_12_011.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_12_011.jpg)'
- en: Figure 12.11 – The Application Insights URL that is associated with your AzureML
    workspace
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.11 – 与你的AzureML工作区关联的应用程序洞察URL
- en: 'Note that this **Application Insights url** link is not specific to the **aci-loans**
    deployment. This link will be the same for all of your real-time endpoints, allowing
    you to centrally monitor all of your real-time endpoints. Clicking on that link
    will take you inside Application Insights, as shown in *Figure 12.12*:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个**应用程序洞察URL**链接并不是针对**aci-loans**部署的特定链接。这个链接对于你所有的实时端点都是相同的，允许你集中监控所有的实时端点。点击这个链接将带你进入应用程序洞察，如*图12.12*所示：
- en: '![Figure 12.12 – Application Insights showing the 10 requests that you sent
    with the last code'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.12 – 应用程序洞察显示你用最后一段代码发送的10个请求'
- en: '](img/B16777_12_012.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_12_012.jpg)'
- en: Figure 12.12 – Application Insights showing the 10 requests that you sent with
    the last code
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12 – 应用程序洞察显示你用最后一段代码发送的10个请求
- en: 'From this dashboard, you can click on the graphs and drill down to the signal
    details; or you can view all the traces that your application is writing inside
    the console. To view them, navigate to **Monitoring** | **Logs**, click on the
    **traces**, select a time range that you want to investigate, and click on the
    **Run** button. You should see all of the **STDOUT** messages appear in the results,
    and you can drill down into the details, as displayed in *Figure 12.13*:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在此仪表板上，你可以点击图表并深入查看信号详情；或者你可以查看控制台中应用程序写入的所有追踪。要查看它们，请导航到**监视** | **日志**，点击**追踪**，选择你想调查的时间范围，然后点击**运行**按钮。你应该能在结果中看到所有**STDOUT**消息，并可以深入查看详细信息，如*图12.13*所示：
- en: '![Figure 12.13 – Reading all of the traces emitted by your model''s real-time'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.13 – 阅读你的模型实时端点发出的所有追踪'
- en: endpoint in Application Insights
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序洞察中的端点
- en: '](img/B16777_12_013.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_12_013.jpg)'
- en: Figure 12.13 – Reading all of the traces emitted by your model's real-time endpoint
    in Application Insights
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13 – 阅读你模型实时端点在应用程序洞察中发出的所有追踪
- en: You can create complex queries in this **Logs** section using a powerful SQL-like
    language known as **Kusto**. You can even create automated alerts based on those
    queries, notifying you, for example, whenever you have had more than 100 loans
    rejected in the last 30 minutes.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在**日志**部分使用一种强大的类似SQL的语言——**Kusto**，来创建复杂的查询。你甚至可以根据这些查询创建自动化警报，例如，当过去30分钟内你的贷款拒绝次数超过100次时，系统会通知你。
- en: Important note
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Application Insights supports the logging of small payloads of up to 64 KB at
    a time. If you plan to log more than that, for example, a mini-batch input with
    more than 64 KB of data, you should consider working with the `DataCollector`
    class of the AzureML SDK. This class allows you to log data directly into a storage
    account; however, it is only available if you deploy in AKS.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序洞察支持每次最多64 KB的小负载日志。如果你计划记录超过64 KB的数据，例如，一个包含超过64 KB数据的小批量输入，你应该考虑使用AzureML
    SDK中的`DataCollector`类。该类允许你将数据直接记录到存储帐户中；但是，只有在AKS中部署时，才可使用此类。
- en: 'Before moving on to the next section, do not forget to delete the deployed
    service to prevent any accidental cost charges for the ACI service. You can delete
    the service from the **Assets** | **Endpoints** list in the studio experience
    or via the code using the following line:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一部分之前，请不要忘记删除已部署的服务，以防止ACI服务产生意外的费用。你可以在工作室体验中的**资产** | **端点**列表中删除该服务，或者通过以下代码行删除服务：
- en: '[PRE29]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In this section, you learned how to monitor your real-time endpoint once you
    have it deployed in production. In *Figure 12.12*, you might have noticed that
    there were a couple of `swagger` file. In the next section, you will learn how
    to fix those failed requests and enable rich integration with third-party applications
    that want to consume the results of your model.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学习了如何在将实时端点部署到生产环境后进行监控。在*图12.12*中，你可能注意到有几个`swagger`文件。在下一部分中，你将学习如何修复那些失败的请求，并实现与希望消费你模型结果的第三方应用程序的丰富集成。
- en: Integrating with third-party applications
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与第三方应用程序集成
- en: So far, you have deployed a web service that accepts an array of arrays as input.
    This is a cryptic input that you need to explain to whoever wants to consume your
    real-time endpoint. In [*Chapter 5*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072),
    *Letting the Machines Do the Model Training*, you read about the `swagger` file
    that could be used to generate code to automatically consume your endpoints. To
    produce such a file, you can use the open source `inference-schema` package and
    decorate your code with metadata that will drive the generation of the `swagger.json`
    file.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经部署了一个接受数组数组作为输入的Web服务。这是一个晦涩的输入，你需要向任何想要使用你实时端点的人解释它。在[*第5章*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072)《让机器进行模型训练》中，你了解了可以用来生成代码以自动消费你的端点的`swagger`文件。为了生成这样的文件，你可以使用开源的`inference-schema`包，并用元数据装饰你的代码，从而驱动`swagger.json`文件的生成。
- en: 'In order to make your model slightly easier to consume by third-party applications,
    you should accept the following payload:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使你的模型更容易被第三方应用程序消费，你应该接受以下有效负载：
- en: '[PRE30]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here, you will need to create a new version of the scoring file. Instead of
    cloning and editing the existing scoring file, you can download the modified `score_v2.py`
    version directly from the GitHub page, as mentioned in the *Technical requirements*
    section. In the **Notebooks** section, duplicate the **score.py** file located
    in the **script** folder by right-clicking on it and selecting the **Duplicate**
    command, as shown in *Figure 12.14*:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你需要创建一个新的评分文件版本。与其克隆并编辑现有的评分文件，不如直接从 GitHub 页面下载修改后的`score_v2.py`版本，正如*技术要求*部分所提到的那样。在**Notebooks**部分，右键点击位于**script**文件夹中的**score.py**文件，选择**Duplicate**命令，复制该文件，如*图
    12.14*所示：
- en: '![Figure 12.14 – Creating the v2 file of the entry script'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.14 – 创建入口脚本的 v2 文件](img/B16777_12_014.jpg)'
- en: '](img/B16777_12_014.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.14 – 创建入口脚本的 v2 文件](img/B16777_12_014.jpg)'
- en: Figure 12.14 – Creating the v2 file of the entry script
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.14 – 创建入口脚本的 v2 文件
- en: 'Name the clone `score_v2.py`, and modify the code to look like the following
    code block:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 将克隆文件命名为`score_v2.py`，并修改代码，使其如下所示：
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'At the beginning of the script file, you are importing additional helper classes,
    which will be used later in the code. Notice that you will no longer need the
    `json` module:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在脚本文件的开始部分，你导入了额外的辅助类，这些类稍后会在代码中使用。请注意，你不再需要`json`模块：
- en: '[PRE32]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You won''t modify the `init` method:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你不会修改`init`方法：
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In the preceding code block, you create a `pandas` `DataFrame` that will act
    as the sample for the objects contained in the incoming request''s `data` attribute.
    This `data_sample` object has an `income` feature, which is `float64`, and the
    `credit_cards` and `age` features, which are integers. Similarly, for the output,
    you define `output_sample` as a NumPy array or numeric value. You can use the
    `data_sample` and `output_sample` objects inside the decorators of the following
    code block:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，你创建了一个`pandas` `DataFrame`，它将作为传入请求的`data`属性中包含的对象的示例。这个`data_sample`对象有一个`income`特征，它是`float64`类型，以及`credit_cards`和`age`特征，它们是整数类型。类似地，对于输出，你将`output_sample`定义为一个
    NumPy 数组或数值。你可以在以下代码块的装饰器中使用`data_sample`和`output_sample`对象：
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Here, you use the `data_sample` object with the `@input_schema` decorator. Additionally,
    you use `PandasParameterType`, which indicates that the parameter named `pandas`
    `DataFrame` that follows the schema defined by the `data_sample` example. You
    use the `@output_schema` decorator to specify that your service returns a NumPy
    array as an output, similar to `output_sample`. Once you have configured these
    schemas, you will notice that you do not have to deserialize the incoming payload
    within the `run` method. The `data` object is an already deserialized `pandas`
    `DataFrame`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你使用`data_sample`对象和`@input_schema`装饰器。此外，你使用`PandasParameterType`，这表示紧随其后的名为`pandas`
    `DataFrame`的参数遵循由`data_sample`示例定义的模式。你使用`@output_schema`装饰器指定你的服务返回一个NumPy数组作为输出，类似于`output_sample`。一旦你配置了这些模式，你会注意到在`run`方法中不需要再反序列化传入的有效负载。`data`对象已经是一个反序列化后的`pandas`
    `DataFrame`。
- en: If you want to process binary files instead of tabular data, for instance, processing
    an image, you can use the `@rawhttp` directive, which will pass the raw HTTP request
    to your `run` method. Working with plain HTTP requests gives you greater flexibility,
    including setting the response headers; this is something required when you configure
    security features such as **Cross-Origin Resource Sharing** (**CORS**). You can
    find resources to learn more about those advanced scenarios in the *Further reading*
    section of this chapter.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想处理二进制文件而不是表格数据，例如处理图像，你可以使用`@rawhttp`指令，它会将原始的HTTP请求传递给你的`run`方法。使用纯HTTP请求可以给你更大的灵活性，包括设置响应头；这是配置诸如**跨域资源共享**（**CORS**）等安全功能时所必需的。你可以在本章的*进一步阅读*部分找到更多关于这些高级场景的资源。
- en: 'Now that you have the code of the `score_v2.py` script file ready, you need
    to publish the real-time endpoint. To create a real-time endpoint for the new
    scoring function, add the following code inside a cell within your notebook:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好`score_v2.py`脚本文件的代码，你需要发布实时端点。要为新的评分功能创建实时端点，请在笔记本中的一个单元格内添加以下代码：
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In the preceding code, you append the `inference_schema` pip packages in the
    `myEnv` dependencies, which you defined in the *Understanding the model deployment
    options* section earlier. Note that you are installing that package with the `pandas-support`
    extra, which will include the `pandas` package. The `numpy` dependency that your
    `score_v2.py` file depends upon will automatically be installed by pip since it
    is a dependency of the `pandas` package.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，你将`inference_schema` pip包附加到你在*理解模型部署选项*部分中定义的`myEnv`依赖项中。请注意，你安装该包时使用了`pandas-support`扩展，这会包括`pandas`包。你的`score_v2.py`文件依赖的`numpy`依赖项会由pip自动安装，因为它是`pandas`包的依赖项。
- en: 'Following this, you specify that you are using the `score_v2.py` entry script
    and deploy the new service. The new service will have a `swagger.json` file available
    for third-party applications such as Power BI to read and automatically understand
    how to invoke your model. You can get the Swagger URI to point to that file on
    the endpoint''s page, as shown in *Figure 12.11*. On the endpoint''s page, you
    should notice that the **Test** tab has been enhanced to guide you on what fields
    you need to provide to invoke the model. On the code side, you can invoke the
    model with the following payloads:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要指定使用`score_v2.py`入口脚本并部署新服务。新服务将提供一个`swagger.json`文件，供第三方应用程序（如Power
    BI）读取并自动了解如何调用你的模型。你可以在端点页面上获取指向该文件的Swagger URI，如*图12.11*所示。在端点页面上，你应该注意到**测试**选项卡已增强，指导你提供调用模型所需的字段。在代码方面，你可以使用以下负载调用模型：
- en: '[PRE36]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Before moving on to the next section, make sure you delete the ACI service
    you just deployed by using the following code:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，确保使用以下代码删除你刚刚部署的ACI服务：
- en: '[PRE37]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: So far, you have been deploying real-time inference endpoints that could fulfill
    ad hoc inference requests through a REST API. In the next section, you will learn
    how to deploy a batch inference pipeline that can process big data in parallel
    using `ParallelRunStep`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你一直在部署能够通过REST API处理临时推理请求的实时推理端点。在下一节中，你将学习如何部署一个批量推理管道，它能够使用`ParallelRunStep`并行处理大数据。
- en: Creating a batch inference pipeline
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建批量推理管道
- en: In [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160), *Working
    with Pipelines*, you learned how to create pipelines that orchestrate multiple
    steps. These pipelines can be invoked using a REST API, similar to the real-time
    endpoint that you created in the previous section. One key difference is that
    in the real-time endpoint, the infrastructure is constantly on, waiting for a
    request to arrive, while in the published pipelines, the cluster will spin up
    only after the pipeline has been triggered.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第11章*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160)《使用管道》中，你学会了如何创建多个步骤的管道并进行编排。这些管道可以通过REST
    API进行调用，类似于你在上一节中创建的实时端点。一个关键的区别是，在实时端点中，基础设施是持续开启的，等待请求到达，而在发布的管道中，集群仅在管道被触发后才会启动。
- en: 'You could use these pipelines to orchestrate batch inference on top of data
    residing in a dataset. For example, let''s imagine that you just trained the `loans`
    model you have been using in this chapter. You want to run the model against all
    of the pending loan requests and store the results; this is so that you can implement
    an email campaign targeting the customers that might get their loan rejected.
    The easiest approach is to create a single `PythonScriptStep` that will process
    each record sequentially and store the results in the output folder, as you learned
    in [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160), *Working with
    Pipelines*. Instead of doing that, you could break the dataset into multiple batches
    and then have them processed, in parallel, in multiple processes running inside
    each node of your cluster, as displayed in *Figure 12.15*:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用这些管道来协调位于数据集中的数据的批处理推理。例如，假设您刚刚训练了本章中使用的`loans`模型。您希望对所有待处理的贷款请求运行该模型，并存储结果；这是为了便于您实施一个电子邮件营销活动，针对那些可能会被拒绝贷款的客户。最简单的方法是创建一个单独的`PythonScriptStep`，按顺序处理每条记录并将结果存储在输出文件夹中，正如您在[*第11章*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160)中学到的那样，*工作与管道*。但是，您也可以将数据集拆分为多个批次，然后让它们在您的集群中每个节点内部的多个进程中并行处理，如*图12.15*所示：
- en: '![Figure 12.15 – Parallel processing big datasets by splitting them into smaller'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.15 – 通过将大数据集拆分成较小的批次并并行处理它们来进行并行处理'
- en: batches and processing them in parallel
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 批次并行处理
- en: '](img/B16777_12_015.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_12_015.jpg)'
- en: Figure 12.15 – Parallel processing big datasets by splitting them into smaller
    batches and processing them in parallel
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.15 – 通过将大数据集拆分成较小的批次并并行处理它们来进行并行处理
- en: 'In this section, you will create a batch processing pipeline that will be making
    inferences using the `chapter12-loans` model you trained in this chapter. You
    already have a dataset named `loans`, but it is too small to show how `ParallelRunStep`
    can help you speed up by parallelizing the inferences. You will generate a new
    dataset that will be 1,024 times bigger by copying the same DataFrame repeatedly.
    Then, you will create a pipeline similar to the one that you created in [*Chapter
    11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160), *Working with Pipelines*.
    This time, you will use the `ParallelRunConfig` and the `ParallelRunStep` classes
    to parallelize the processing of the dataset. The configuration class requires
    an entry script, similar to the entry script that you saw in the previous section.
    Additionally, you need to define the following two methods:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将创建一个批处理处理管道，该管道将使用您在本章中训练的`chapter12-loans`模型进行推理。您已经有一个名为`loans`的数据集，但它太小，无法展示`ParallelRunStep`如何通过并行化推理来加速处理。您将通过重复复制相同的DataFrame生成一个新的数据集，大小是原来的1,024倍。然后，您将创建一个类似于在[*第11章*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160)中创建的管道，*工作与管道*。这一次，您将使用`ParallelRunConfig`和`ParallelRunStep`类来并行化数据集的处理。配置类需要一个入口脚本，类似于您在上一节中看到的入口脚本。此外，您还需要定义以下两个方法：
- en: '`init()`: This method loads the model and prepares the process for the incoming
    batches. No output is expected from this method.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init()`: 此方法加载模型并为即将到来的批次准备处理过程。此方法不期望有任何输出。'
- en: '`run(mini_batch)`: This method does the actual data processing. This method
    will be invoked multiple times, passing a different `mini_batch` parameter every
    time. You have to return an array containing one row for each item you managed
    to process within this function as an output. For example, if the `mini_batch`
    parameter had 100 rows and you return an array of 98 items, you will indicate
    that you failed to process 2 of those records. The `mini_batch` parameter could
    either be a `pandas` `DataFrame` if you are processing `TabularDataset` or an
    array that contains the file paths you need to process if you are processing a
    `FileDataset`.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`run(mini_batch)`: 此方法执行实际的数据处理。此方法将被多次调用，每次传递不同的`mini_batch`参数。您必须返回一个数组，其中包含每个成功处理的项目的行。例如，如果`mini_batch`参数包含100行，而您返回了98项数据，则表示您未能处理其中2条记录。如果您处理的是`TabularDataset`，则`mini_batch`参数可以是一个`pandas`
    `DataFrame`，如果您处理的是`FileDataset`，则可以是包含您需要处理的文件路径的数组。'
- en: 'Navigate to your `chapter12.ipynb` notebook and perform the following steps:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到您的`chapter12.ipynb`笔记本，并执行以下步骤：
- en: 'Start by getting a reference to the workspace, the dataset, and the compute
    cluster you are going to use for your pipeline:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从获取对工作区、数据集和您将用于管道的计算集群的引用开始：
- en: '[PRE38]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The code should be self-explanatory, as you used it in [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160),
    *Working with Pipelines*.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码应该是自解释的，因为您已经在[*第11章*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160)《与管道一起工作》中使用过它。
- en: 'Create a new, bigger dataset based on the `loans` dataset:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于`loans`数据集创建一个新的、更大的数据集：
- en: '[PRE39]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In the preceding code, you are loading the `loans` `DataFrame` into memory without
    the `approved_loan` column. This dataset only contains 500 rows. Then, you append
    the dataset to itself 10 times. This will create a much bigger dataset containing
    512,000 rows, which you register as `pending-loans`.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，您正在将`loans`的`DataFrame`加载到内存中，但不包含`approved_loan`列。这个数据集只有500行。然后，您将数据集追加到自身10次。这将创建一个包含512,000行的更大数据集，您将其注册为`pending-loans`。
- en: 'Now, it''s time to create the script that will be processing this dataset.
    In the `chapter12` folder, add a `pipeline_step` folder and then add a `tabular_batch.py`
    file with the following contents:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候创建处理该数据集的脚本了。在`chapter12`文件夹中，添加一个`pipeline_step`文件夹，然后添加一个名为`tabular_batch.py`的文件，内容如下：
- en: '[PRE40]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This script has two methods, as metioned earlier. In the `init` method, you
    use the `get_model_path` method of the `Model` class to get the location of the
    model that you have been using so far. From the script''s perspective, the model
    will reside in a folder on the same computer where the script is running. Then,
    you use `joblib` to load the model inside a `global` variable named `model`. In
    the `run` method, you print the size of the incoming DataFrame, and then you create
    a new column, named *approved*, where you store all of the model inferences. You
    return a list containing a four-element array for each row you processed, similar
    to the following records:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个脚本有两个方法，如前所述。在`init`方法中，您使用`Model`类的`get_model_path`方法来获取您至今使用的模型的路径。从脚本的角度来看，模型将存储在脚本运行的同一台计算机的文件夹中。然后，您使用`joblib`将模型加载到名为`model`的`global`变量中。在`run`方法中，您打印传入DataFrame的大小，然后创建一个名为*approved*的新列，存储所有模型推断的结果。您返回一个包含每行处理记录的四元素数组的列表，类似于以下记录：
- en: '[PRE41]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'If you were to process `FileDataset` instead of the `TabularDataset` that you
    are processing in this section, the corresponding `file_batch.py` file would look
    like the following:'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您处理的是`FileDataset`而不是本节中处理的`TabularDataset`，那么相应的`file_batch.py`文件会如下所示：
- en: '[PRE42]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'You load your model, as usual, inside the `init` method, for example, a neural
    network that will implement image classification. In the `run` method, the `mini_batch`
    parameter is an array containing the file paths of the files you need to process.
    You can loop through those files and make the inferences using your model. As
    an output, you return the filename and result of the model, as demonstrated in
    the following example:'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您像往常一样在`init`方法中加载模型，例如一个用于实现图像分类的神经网络。在`run`方法中，`mini_batch`参数是一个包含您需要处理的文件路径的数组。您可以遍历这些文件并使用模型进行推断。作为输出，您返回文件名和模型的结果，示例如下：
- en: '[PRE43]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: You will observe, in *Step 5*, that those results will be aggregated in a single
    file defined in `ParallelRunConfig`.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，您将观察到，这些结果将聚合成一个在`ParallelRunConfig`中定义的单一文件。
- en: 'You will need to create an environment to execute your pipeline step. Add the
    following code inside a cell:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要创建一个环境来执行管道步骤。将以下代码添加到一个单元格中：
- en: '[PRE44]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: You need to install the `scikit-learn` conda package, just as you have been
    doing so far. For `ParallelRunConfig` to work, you will need to include the `azureml-core`
    and `azureml-dataset-runtime[pandas,fuse]` `pip` packages.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您需要安装`scikit-learn`的conda包，就像之前一样。为了使`ParallelRunConfig`正常工作，您还需要包括`azureml-core`和`azureml-dataset-runtime[pandas,fuse]`的`pip`包。
- en: 'Next, create the `ParallelRunConfig` class that configures how the run will
    split the workload and what script to use for data processing. Add the following
    code inside a new notebook cell:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建`ParallelRunConfig`类，配置如何拆分工作负载以及使用哪个脚本进行数据处理。将以下代码添加到新的笔记本单元中：
- en: '[PRE45]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Here, you will run the `tabular_batch.py` script located inside the `pipeline_step`
    folder. You are going to split the dataset into smaller batches of, approximately,
    100 KB. If you were processing a `FileDataset`, you would need to specify the
    number of files to put in each batch. Here, `error_threshold` specifies the number
    of record or file failures that should be ignored while processing the data. `-1`
    means you are okay with any number of processing errors. The `output_action` parameter
    accepts either `append_row` values or `summary_only` values. Using the `append_row`
    value, you can request all outputs from the `run` method invocations to be appended
    inside a single output file, the name of which is `parallel_run_step.txt`, unless
    you override it via the `append_row_file_name` parameter, as demonstrated in the
    preceding example. The order of the records in that file is not guaranteed since
    the records are processed in parallel. Usually, you would return the customer
    ID, or the loan application ID, and the model's inference. Using that ID, you
    could link back the original record with the model's prediction. In the current
    example, we don't have any ID; therefore, we return the entire row, just as we
    did in the `tabular_batch.py` script in *Step 3*.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，您将运行位于`pipeline_step`文件夹中的`tabular_batch.py`脚本。您将把数据集拆分成大约100 KB的小批次。如果处理的是`FileDataset`，则需要指定每个批次中包含的文件数量。这里，`error_threshold`指定在处理数据时应该忽略的记录或文件失败的数量。`-1`表示您可以接受任何数量的处理错误。`output_action`参数接受`append_row`值或`summary_only`值。使用`append_row`值，您可以要求将所有`run`方法调用的输出附加到一个名为`parallel_run_step.txt`的单一输出文件中，除非通过`append_row_file_name`参数覆盖该文件名，正如前面的示例所演示的那样。由于记录是并行处理的，因此文件中的记录顺序无法保证。通常，您会返回客户ID或贷款申请ID，以及模型的推断结果。通过该ID，您可以将原始记录与模型的预测结果关联。在当前示例中，我们没有任何ID；因此，我们返回整个行，就像在*步骤3*中的`tabular_batch.py`脚本一样。
- en: Following this, you specify the environment and the cluster where this pipeline
    step will be executed. In the end, you specify that this pipeline step will run
    in a single node, and it will spin up *two* processes per participating node.
    If you used two nodes, you would have four processes running in parallel. In the
    current example, two parallel processes are enough to handle the processing in
    only a couple of minutes.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，您需要指定执行此流水线步骤的环境和集群。最后，您需要指定此流水线步骤将在单个节点上运行，并且每个参与节点将启动*两个*进程。如果使用两个节点，则会有四个进程并行运行。在当前示例中，两个并行进程足以在几分钟内完成处理。
- en: If you have a heavy processing script that requires more than 60 seconds to
    process the `mini_batch_size` parameter you specified, you can increase the timeout
    value by setting the `run_invocation_timeout` parameter.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您的处理脚本需要超过60秒来处理您指定的`mini_batch_size`参数，您可以通过设置`run_invocation_timeout`参数来增加超时值。
- en: 'As a next step, you will define the output location of `append_row_file_name`
    that you specified earlier:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，您将定义之前指定的`append_row_file_name`的输出位置：
- en: '[PRE46]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: You will store that aggregation file in the default datastore, underneath the
    `/inferences/loans/` folder.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您将把该聚合文件存储在默认数据存储区下的`/inferences/loans/`文件夹中。
- en: 'Now it''s time to create the first and only step of the pipeline, that is,
    `ParallelRunStep`:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候创建流水线的第一个也是唯一的步骤——`ParallelRunStep`：
- en: '[PRE47]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Name that step `chapter12-parallel-loans` and pass the `pending_loans_ds` dataset
    that you registered in *Step 2* earlier. The output is stored in `OutputFileDatasetConfig`,
    which you created in *Step 6*. Specify that this step should not be reused (`allow_reuse`);
    this allows you to trigger the pipeline multiple times to always get the latest
    data in the dataset along with the latest registered model.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将此步骤命名为`chapter12-parallel-loans`，并传递您在*步骤2*中注册的`pending_loans_ds`数据集。输出存储在您在*步骤6*中创建的`OutputFileDatasetConfig`中。指定此步骤不应被重用（`allow_reuse`）；这允许您多次触发流水线，每次都获取数据集中的最新数据以及最新注册的模型。
- en: 'Create and execute a pipeline using the following code:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码创建并执行流水线：
- en: '[PRE48]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You can watch the execution logs by using the `RunDetails` widget with the
    following code:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用以下代码通过`RunDetails`小部件查看执行日志：
- en: '[PRE49]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Alternatively, you can wait for the execution to complete with the following
    code:'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，您可以使用以下代码等待执行完成：
- en: '[PRE50]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: From that point on, you can publish and even schedule the pipeline, as discussed
    in [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160), *Working with
    Pipelines*.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从那时起，你可以根据[*第11章*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160)，*使用管道*，发布甚至安排流水线。
- en: 'You can visit the pipeline in AzureML studio and observe the outputs and the
    logs it produced, as shown in *Figure 12.16*. Notice that you will find a single
    node and two processes. Each process has multiple `run` method invocations. Each
    time the `run` method was invoked, a DataFrame that required 117.3 KB in memory
    was passed in, which is close to the 100 KB that you requested in *Step 5* earlier:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以访问AzureML工作室中的管道，观察它生成的输出和日志，如*图12.16*所示。请注意，您将找到一个单节点和两个进程。每个进程都有多个`run`方法调用。每次调用`run`方法时，都会传入一个占用117.3
    KB内存的DataFrame，这接近您在*步骤5*中请求的100 KB：
- en: '![Figure 12.16 – The logs from the parallel execution showing the information
    of the mini_batch DataFrame'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.16 – 并行执行日志显示mini_batch DataFrame信息'
- en: '](img/B16777_12_016.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16777_12_016.jpg)'
- en: Figure 12.16 – The logs from the parallel execution showing the information
    of the mini_batch DataFrame
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.16 – 并行执行日志显示mini_batch DataFrame信息
- en: In this section, you learned how to create a batch processing pipeline that
    can process a significant amount of data in parallel. This concludes the operationalization
    options that you need to be aware of for the exam, covering both the real-time
    and batch modes.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，您学习了如何创建一个可以并行处理大量数据的批处理流水线。这些是您在考试中需要了解的操作选项，涵盖实时和批处理模式。
- en: Summary
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you explored various ways in which to use the machine learning
    models that you have been training in this book. You can make either real-time
    inferences or batch process a large number of records in a cost-effective manner.
    You started by registering the model you would use for inferences. From there,
    you can either deploy a real-time endpoint in ACI for testing or in AKS for production
    workloads that require high availability and automatic scaling. You explored how
    to profile your model to determine the recommended container size to host the
    real-time endpoint. Following this, you discovered Application Insights, which
    allows you to monitor production endpoints and identify potential production issues.
    Through Application Insights, you noticed that the real-time endpoint you produced
    wasn't exposing a `swagger.json` file that was needed by third-party applications,
    such as Power BI, to automatically consume your endpoint. You modified the scoring
    function to include metadata regarding your model's inputs and outputs, thus completing
    the real-time inference section of this chapter.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您探索了在本书中训练的机器学习模型的各种使用方式。您可以进行实时推断，也可以以经济高效的方式批处理大量记录。您开始注册用于推断的模型。从那里，您可以为测试部署ACI中的实时端点，或者为需要高可用性和自动扩展的生产工作负载部署到AKS中。您探索了如何配置您的模型以确定托管实时端点所需的推荐容器大小。接着，您发现了应用程序洞察，它允许您监视生产端点并识别潜在的生产问题。通过应用程序洞察，您注意到您生成的实时端点未公开一个`swagger.json`文件，这是第三方应用程序（如Power
    BI）自动消费您的端点所需的。您修改了评分函数，以包含有关您的模型输入和输出的元数据，从而完成了本章的实时推断部分。
- en: Then, you moved on to the batch inferencing side, where you authored a pipeline
    that can process half a million records, in parallel, in only a few minutes. Combining
    this parallelization with low-priority computes, you can achieve great cost savings
    when inferencing larger data volumes.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您转向批处理推断部分，您在那里编写了一个可以并行处理50万条记录的流水线，仅用几分钟。将此并行化与低优先级计算结合使用时，处理更大数据量时能实现很大的成本节省。
- en: Congratulations! You have completed your journey of discovering the basic capabilities
    of the AzureML workspace. Now you can now conduct machine learning experiments
    in the workspace, and you can operationalize the resulting models using the option
    that suits the business problem that you are trying to solve. With this knowledge,
    you should be able to pass the *DP-100* exam, *Designing and Implementing a Data
    Science Solution on Azure*, with flying colors.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已完成发现AzureML工作区基本功能的旅程。现在，您可以在工作区中进行机器学习实验，并可以根据适合解决业务问题的选项将生成的模型进行操作化。掌握了这些知识，您应该能够以优异的成绩通过*DP-100*考试，*在Azure上设计和实施数据科学解决方案*。
- en: Questions
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'In each chapter, you will find a number of questions to validate your understanding
    of the topics that have been discussed:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一章中，您将找到一系列问题来验证您对已讨论主题的理解：
- en: You want to deploy a real-time endpoint that will handle transactions from a
    live betting website. The traffic from this website will have spikes during games
    and will be very low during the night. Which of the following compute targets
    should you use?
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您想要部署一个实时端点，处理来自一个在线博彩网站的交易。该网站的流量在比赛期间会有激增，夜间则非常低。您应该使用以下哪个计算目标？
- en: a. ACI
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. ACI
- en: b. A compute instance
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 计算实例
- en: c. A compute cluster
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 计算集群
- en: d. AKS
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. AKS
- en: You want to monitor a real-time endpoint deployed in AKS and determine the average
    response time of the service. Which monitoring solution should you use?
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您想要监控部署在 AKS 中的实时端点，并确定服务的平均响应时间。您应该使用哪个监控解决方案？
- en: a. ACI
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. ACI
- en: b. Azure Container Registry
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. Azure 容器注册表
- en: c. Application Insights
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 应用程序洞察
- en: You have a computer vision model, and you want to process 100 images in parallel.
    You author a pipeline with a parallel step. You want to process 10 images at a
    time. Which of the following `ParallelRunConfig` parameters should you set?
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您有一个计算机视觉模型，并且想要并行处理 100 张图片。您编写了一个包含并行步骤的管道，您希望每次处理 10 张图片。您应该设置以下哪个 `ParallelRunConfig`
    参数？
- en: a. `mini_batch_size=10`
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. `mini_batch_size=10`
- en: b. `error_threshold=10`
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. `error_threshold=10`
- en: c. `node_count=10`
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. `node_count=10`
- en: d. `process_count_per_node=10`
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. `process_count_per_node=10`
- en: Further reading
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'This section offers a list of helpful web resources to help you augment your
    knowledge of the AzureML SDK and the various code snippets used in this chapter:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了一些有用的网络资源列表，帮助您增强对 AzureML SDK 及本章中使用的各种代码片段的了解：
- en: 'Model persistence guidance from scikit-learn: [https://scikit-learn.org/stable/modules/model_persistence.html](https://scikit-learn.org/stable/modules/model_persistence.html)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 scikit-learn 的模型持久化指导：[https://scikit-learn.org/stable/modules/model_persistence.html](https://scikit-learn.org/stable/modules/model_persistence.html)
- en: 'Testing a REST API using Postman: [https://www.postman.com/product/api-client/](https://www.postman.com/product/api-client/)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Postman 测试 REST API：[https://www.postman.com/product/api-client/](https://www.postman.com/product/api-client/)
- en: 'The **curl** command-line tool to make web requests: [https://curl.se/](https://curl.se/%0D)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**curl** 命令行工具，用于发起网页请求：[https://curl.se/](https://curl.se/%0D)'
- en: 'Monitor Python applications using OpenCensus: [https://docs.microsoft.com/azure/azure-monitor/app/opencensus-python](https://docs.microsoft.com/azure/azure-monitor/app/opencensus-python)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 OpenCensus 监控 Python 应用程序：[https://docs.microsoft.com/azure/azure-monitor/app/opencensus-python](https://docs.microsoft.com/azure/azure-monitor/app/opencensus-python)
- en: 'How to use the inference server to test your entry scripts locally: [https://docs.microsoft.com/azure/machine-learning/how-to-inference-server-http](https://docs.microsoft.com/azure/machine-learning/how-to-inference-server-http)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用推理服务器在本地测试您的入口脚本：[https://docs.microsoft.com/azure/machine-learning/how-to-inference-server-http](https://docs.microsoft.com/azure/machine-learning/how-to-inference-server-http)
- en: 'Packaging the model inside an autonomous Docker container: [https://docs.microsoft.com/azure/machine-learning/how-to-deploy-package-models](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-package-models)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型打包到自治 Docker 容器中：[https://docs.microsoft.com/azure/machine-learning/how-to-deploy-package-models](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-package-models)
- en: 'The ONNX machine learning format used to store models that can be loaded in
    multiple platforms: [https://docs.microsoft.com/azure/machine-learning/concept-onnx](https://docs.microsoft.com/azure/machine-learning/concept-onnx)'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储可以在多个平台加载的模型的 ONNX 机器学习格式：[https://docs.microsoft.com/azure/machine-learning/concept-onnx](https://docs.microsoft.com/azure/machine-learning/concept-onnx)
- en: 'An introduction to Application Insights: [https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview)'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序洞察简介：[https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview)
- en: 'An introduction to the Kusto Query Language: [https://docs.microsoft.com/azure/data-explorer/kusto/concepts/](https://docs.microsoft.com/azure/data-explorer/kusto/concepts/)'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kusto 查询语言简介：[https://docs.microsoft.com/azure/data-explorer/kusto/concepts/](https://docs.microsoft.com/azure/data-explorer/kusto/concepts/)
- en: 'The advanced real-time endpoint entry script authoring guide: [https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级实时端点入口脚本编写指南：[https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script)
- en: 'Integrating AzureML models in Power BI: [https://docs.microsoft.com/power-bi/transform-model/dataflows/dataflows-machine-learning-integration#azure-machine-learning-integration-in-power-bi](https://docs.microsoft.com/power-bi/transform-model/dataflows/dataflows-machine-learning-integration#azure-machine-learning-integration-in-power-bi)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Power BI 中集成 AzureML 模型：[https://docs.microsoft.com/power-bi/transform-model/dataflows/dataflows-machine-learning-integration#azure-machine-learning-integration-in-power-bi](https://docs.microsoft.com/power-bi/transform-model/dataflows/dataflows-machine-learning-integration#azure-machine-learning-integration-in-power-bi)
- en: 'Using the `ParallelRunStep` class to train hundreds of models: [https://github.com/microsoft/solution-accelerator-many-models](https://github.com/microsoft/solution-accelerator-many-models)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `ParallelRunStep` 类训练数百个模型：[https://github.com/microsoft/solution-accelerator-many-models](https://github.com/microsoft/solution-accelerator-many-models)
