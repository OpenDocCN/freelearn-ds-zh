- en: 'Chapter 6: Feature Engineering – Extraction, Transformation, and Selection'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章：特征工程 – 提取、转换和选择
- en: In the previous chapter, you were introduced to Apache Spark's native, scalable
    machine learning library, called **MLlib**, and you were provided with an overview
    of its major architectural components, including transformers, estimators, and
    pipelines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您了解了Apache Spark的原生、可扩展的机器学习库**MLlib**，并获得了其主要架构组件的概述，包括转换器、估算器和管道。
- en: This chapter will take you to your first stage of the **scalable machine learning**
    journey, which is **feature engineering**. Feature engineering deals with the
    process of extracting machine learning features from preprocessed and clean data
    in order to make it conducive for machine learning. You will learn about the concepts
    of **feature extraction**, **feature transformation**, **feature scaling**, and
    **feature selection** and implement these techniques using the algorithms that
    exist within Spark MLlib and some code examples. Toward the end of this chapter,
    you will have learned the necessary techniques to implement scalable feature engineering
    pipelines that convert preprocessed data into a format that is suitable and ready
    for the machine learning model training process.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将带领您进入**可扩展机器学习**旅程的第一阶段——**特征工程**。特征工程涉及从预处理和清理后的数据中提取机器学习特征的过程，以便为机器学习做准备。您将学习**特征提取**、**特征转换**、**特征缩放**和**特征选择**等概念，并通过Spark
    MLlib中的算法和一些代码示例实现这些技术。在本章结束时，您将掌握实施可扩展特征工程管道的必要技术，将预处理的数据转换为适合并准备好用于机器学习模型训练过程的格式。
- en: 'Particularly, in this chapter, you will learn the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，在本章中，您将学习以下内容：
- en: The machine learning process
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习过程
- en: Feature extraction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取
- en: Feature transformation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征转换
- en: Feature selection
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择
- en: Feature store as a central feature repository
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征库作为中央特征存储库
- en: Delta as an offline feature store
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta作为离线特征存储库
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will be using the Databricks Community Edition to run our
    code. This can be found at [https://community.cloud.databricks.com](https://community.cloud.databricks.com).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Databricks社区版来运行代码。可以在[https://community.cloud.databricks.com](https://community.cloud.databricks.com)找到。
- en: Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
    The code used in this chapter can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter06](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter06).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册说明可以在[https://databricks.com/try-databricks](https://databricks.com/try-databricks)找到。本章使用的代码可以从[https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter06](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter06)下载。
- en: The datasets used in this chapter can be found at [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章使用的数据集可以在[https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data)找到。
- en: The machine learning process
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习过程
- en: 'A typical data analytics and data science process involves gathering raw data,
    cleaning data, consolidating data, and integrating data. Following this, we apply
    statistical and machine learning techniques to the preprocessed data in order
    to generate a machine learning model and, finally, summarize and communicate the
    results of the process to business stakeholders in the form of data products.
    A high-level overview of the machine learning process is presented in the following
    diagram:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的数据分析和数据科学过程包括收集原始数据、清理数据、整合数据和集成数据。之后，我们将统计学和机器学习技术应用于预处理后的数据，以生成机器学习模型，最后以数据产品的形式总结并向业务相关方传达过程结果。机器学习过程的高层次概述见下图：
- en: '![Figure 6.1 – The data analytics and data science process'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1 – 数据分析和数据科学过程'
- en: '](img/B16736_06_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_06_01.jpg)'
- en: Figure 6.1 – The data analytics and data science process
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 数据分析和数据科学过程
- en: As you can see from the preceding diagram, the actual machine learning process
    itself is just a small portion of the entire data analytics process. Data teams
    spend a good amount of time curating and preprocessing data, and just a portion
    of that time is devoted to building actual machine learning models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表可以看出，实际的机器学习过程只是整个数据分析过程的一小部分。数据团队花费大量时间进行数据策划和预处理，而其中只有一部分时间用于构建实际的机器学习模型。
- en: 'The actual machine learning process involves stages that allow you to carry
    out steps such as data exploration, feature extraction, model training, model
    evaluation, and applying models for real-world business applications, as shown
    in the following diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的机器学习过程包括多个阶段，这些阶段使你能够执行诸如数据探索、特征提取、模型训练、模型评估以及应用模型到实际商业场景等步骤，如下图所示：
- en: '![Figure 6.2 – The machine learning process'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.2 – 机器学习过程'
- en: '](img/B16736_06_02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_06_02.jpg)'
- en: Figure 6.2 – The machine learning process
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 机器学习过程
- en: In this chapter, you will learn about the **Feature Engineering** phase of the
    machine learning process. The following sections will present a few of the prominent
    algorithms and utilities available in the **Spark MLlib** library that deal with
    the **Feature Extraction**, **Feature Transformation**, **Feature Scaling**, and
    **Feature Selection** steps of the **Feature Engineering** process.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解机器学习过程中的**特征工程**阶段。接下来的章节将介绍一些在**Spark MLlib**库中可用的突出算法和工具，这些算法和工具涉及**特征提取**、**特征转换**、**特征缩放**和**特征选择**等**特征工程**步骤。
- en: Feature extraction
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取
- en: A machine learning model is equivalent to a function in mathematics or a method
    in computer programming. A machine learning model takes one or more parameters
    or variables as input and yields an output, called a prediction. In machine learning
    terminology, these input parameters or variables are called **features**. A feature
    is a column of the input dataset within a machine learning algorithm or model.
    A feature is a measurable data point, such as an individual's name, gender, or
    age, or it can be time-related data, weather, or some other piece of data that
    is useful for analysis.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型等同于数学中的函数或计算机编程中的方法。机器学习模型接受一个或多个参数或变量作为输入，并生成一个输出，称为预测。在机器学习术语中，这些输入参数或变量称为**特征**。特征是机器学习算法或模型中输入数据集的一列。特征是一个可度量的数据点，如个人的姓名、性别或年龄，或者是与时间相关的数据、天气数据，或其他对分析有用的数据。
- en: Machine learning algorithms leverage linear algebra, a field of mathematics,
    and make use of mathematical structures such as matrices and vectors to represent
    data internally and also within the code level implementation of algorithms. Real-world
    data, even after undergoing the data engineering process, rarely occurs in the
    form of matrices and vectors. Therefore, the feature engineering process is applied
    to preprocessed data in order to convert it into a format that is suitable for
    machine learning algorithms.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法利用线性代数这一数学领域，并使用矩阵和向量等数学结构在内部以及算法代码层面上表示数据。即使在经过数据工程处理之后，实际世界中的数据也很少以矩阵和向量的形式出现。因此，特征工程过程会应用于预处理数据，以将其转换为适合机器学习算法的格式。
- en: The feature extraction process specifically deals with taking text, image, geospatial,
    or time series data and converting it into a feature vector. Apache Spark MLlib
    has a number of feature extractions available, such as `TF-IDF`, `Word2Vec`, `CountVectorizer`,
    and `FeatureHasher`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取过程专门处理将文本、图像、地理空间或时间序列数据转换为特征向量的问题。Apache Spark MLlib 提供了多种特征提取方法，如`TF-IDF`、`Word2Vec`、`CountVectorizer`和`FeatureHasher`。
- en: Let's consider an example of a group of words and convert them into a feature
    vector using the `CountVectorizer` algorithm. In earlier chapters of this book,
    we looked at sample datasets for an online retailer and applied the data engineering
    process on those datasets to get a clean and consolidated dataset that was ready
    for analytics.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一组单词为例，使用`CountVectorizer`算法将其转换为特征向量。在本书的早期章节中，我们查看了一个在线零售商的样本数据集，并对这些数据集应用了数据工程过程，以获得一个干净且整合的、适合分析的数据集。
- en: 'So, let''s begin with the preprocessed and cleaned dataset produced toward
    the end of [*Chapter 5*](B16736_05_Final_JM_ePub.xhtml#_idTextAnchor094), *Scalable
    Machine Learning with PySpark*, named `retail_ml.delta`. This preprocessed dataset,
    which forms the input to the machine learning process, is also generally referred
    to as the **training dataset**:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们从在[*第5章*](B16736_05_Final_JM_ePub.xhtml#_idTextAnchor094)《可扩展机器学习与PySpark》末尾产生的预处理和清理后的数据集开始，该数据集名为`retail_ml.delta`。这个预处理的数据集，作为机器学习过程的输入，通常被称为**训练数据集**：
- en: 'As a first step, let''s load the data from the data lake in Delta format into
    a Spark DataFrame. This is shown in the following code snippet:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为第一步，让我们将数据从数据湖以Delta格式加载到Spark DataFrame中。如下方代码片段所示：
- en: '[PRE0]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code block, we load data stored in the data lake in Delta form
    into a Spark DataFrame and then display the data using the `show()` command.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们将存储在数据湖中的Delta格式数据加载到Spark DataFrame中，然后使用`show()`命令显示数据。
- en: The result of the display function is shown in the following diagram:![Figure
    6.3 – Preprocessed data
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示函数的结果如下图所示：![图6.3 – 预处理数据
- en: '](img/B16736_06_03.jpg)'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16736_06_03.jpg)'
- en: Figure 6.3 – Preprocessed data
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.3 – 预处理数据
- en: In the preceding diagram, we have the preprocessed data as a result of the data
    engineering and data wrangling steps. Notice that there are `11` columns in the
    dataset with various data types, ranging from a string to a double, to a timestamp.
    In their current format, they are not suitable as inputs for a machine learning
    algorithm; therefore, we need to convert them into a suitable format via the feature
    engineering process.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们得到了经过数据工程和数据整理步骤后的预处理数据。请注意，数据集中有`11`列，数据类型各异，包含字符串、双精度数和时间戳。在当前格式下，它们不适合作为机器学习算法的输入；因此，我们需要通过特征工程过程将其转换为适合的格式。
- en: 'Let''s start with the `description` column, which is of the text type, and
    apply the `CountVectorizer` feature extraction algorithm to it in order to convert
    it into a feature vector, as shown in the following code block:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从`description`列开始，该列为文本类型，并对其应用`CountVectorizer`特征提取算法，以便将其转换为特征向量，如下方代码块所示：
- en: '[PRE1]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the previous code block, the following occurs:'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码块中，发生了以下操作：
- en: We import `CountVectorizer` from the `pyspark.ml.feature` library.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从`pyspark.ml.feature`库中导入`CountVectorizer`。
- en: '`CountVectorizer` takes an `Array` object as input, so we use the `split()`
    function to split the description column into an `Array` object of words.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`CountVectorizer`接受一个`Array`对象作为输入，因此我们使用`split()`函数将description列分割成一个单词的`Array`对象。'
- en: Then, we initialize a new `CountVectorizer` `fit()` method using the previously
    defined estimator on the input dataset. The result is a trained model `transform()`
    method on the input DataFrame, resulting in a new DataFrame with a new feature
    vector column for the description column.
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用之前定义的估算器在输入数据集上初始化一个新的`CountVectorizer` `fit()`方法。结果是一个经过训练的模型`transform()`方法应用于输入DataFrame，从而生成一个新的DataFrame，并为description列添加一个新的特征向量列。
- en: In this way, by using the `CountVectorizer` feature extractor from Spark MLlib,
    we are able to extract a feature vector from a text type column.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过使用Spark MLlib中的`CountVectorizer`特征提取器，我们能够从文本类型的列中提取特征向量。
- en: 'Another feature extractor available within Spark MLlib, such as `Word2Vec`,
    can also be used, as shown in the following code snippet:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark MLlib中还可以使用其他特征提取器，如`Word2Vec`，如下方代码片段所示：
- en: '[PRE2]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code block, the `Word2Vec` estimator is used in a similar fashion
    to the previously mentioned `CountVectorizer`. Here, we use it to extract a feature
    vector from a text-based data column. While both `CountVectorizer` and `Word2Vec`
    help to convert a corpus of words into a feature vector, there are differences
    in the internal implementations of each algorithm. They each have different uses
    depending on the problem scenario and input dataset and might produce different
    results under different circumstances.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码块中，`Word2Vec`估算器的使用方式与之前提到的`CountVectorizer`类似。在这里，我们用它从基于文本的数据列中提取特征向量。虽然`CountVectorizer`和`Word2Vec`都帮助将一个单词语料库转换成特征向量，但它们在每个算法的内部实现上有所不同。它们各自有不同的用途，取决于问题情境和输入数据集，并且在不同的情况下可能会产生不同的结果。
- en: Please note that discussing the nuances of these algorithms or making recommendations
    on when to use a specific feature extraction algorithm is beyond the scope of
    this book.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，讨论这些算法的细微差别或提出何时使用特定特征提取算法的建议超出了本书的范围。
- en: Now that you have learned a few techniques of feature extraction, in the next
    section, let's explore a few of Spark MLlib's algorithms for **feature transformation**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经学习了几种特征提取技术，在下一节中，让我们探讨几种**特征转换**的Spark MLlib算法。
- en: Feature transformation
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征转换
- en: Feature transformation is the process of carefully reviewing the various variable
    types, such as categorical variables and continuous variables, present in the
    training data and determining the best type of transformation to achieve optimal
    model performance. This section will describe, with code examples, how to transform
    a few common types of variables found in machine learning datasets, such as text
    and numerical variables.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 特征转换是仔细审查训练数据中存在的各种变量类型，如分类变量和连续变量，并确定最佳转换类型以实现最佳模型性能的过程。本节将描述如何转换机器学习数据集中发现的几种常见变量类型的示例代码，例如文本和数值变量。
- en: Transforming categorical variables
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换分类变量
- en: Categorical variables are pieces of data that have discrete values with a limited
    and finite range. They are usually text-based in nature, but they can also be
    numerical. Examples include country codes and the month of the year. We mentioned
    a few techniques regarding how to extract features from text variables in the
    previous section. In this section, we will explore a few other algorithms to transform
    categorical variables.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 分类变量是具有有限和有限范围的离散值的数据片段。它们通常是基于文本的性质，但也可以是数字的。例如，国家代码和年份的月份。我们在前一节中提到了关于如何从文本变量中提取特征的几种技术。在本节中，我们将探讨几种其他算法来转换分类变量。
- en: The tokenization of text into individual terms
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将文本标记化为单独的术语
- en: 'The `Tokenizer` class can be used to break down text into its constituent terms,
    as shown in the following code example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tokenizer`类可以用来将文本分解为其组成的术语，如下面的代码示例所示：'
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the preceding code block, we initialize the `Tokenizer` class by passing
    in the `inputCol` and `outputCol` parameters, which results in a transformer.
    Then, we transform the training dataset, resulting in a Spark DataFrame with a
    new column with an array of individual words from each sentence that have been
    converted into lowercase. This is shown in the following table:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们通过传入`inputCol`和`outputCol`参数来初始化`Tokenizer`类，从而生成一个转换器。然后，我们转换训练数据集，得到一个Spark
    DataFrame，其中包含一个新列，其中包含每个句子中被转换为小写的单词数组。这在下表中显示：
- en: '![Figure 6.4 – Tokenizing the text using Tokenizer'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4 – 使用Tokenizer标记化文本'
- en: '](img/B16736_06_04.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_06_04.jpg)'
- en: Figure 6.4 – Tokenizing the text using Tokenizer
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 使用Tokenizer标记化文本
- en: In the preceding table, you can see from the tokenized words that there are
    a few unwanted words, which we need to get rid of as they do not add any value.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在上表中，您可以从标记化的单词中看到有一些不需要的词，我们需要将它们去掉，因为它们没有添加任何价值。
- en: Removing common words using StopWordsRemover
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`StopWordsRemover`删除常见词
- en: 'Every language contains common and frequently occurring words such as prepositions,
    articles, conjunctions, and interjections. These words do not carry any meaning
    in terms of the machine learning process and are better removed before training
    a machine learning algorithm. In Spark, this process can be achieved using the
    `StopWordsRemover` class, as shown in the following code snippet:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每种语言都包含常见且频繁出现的单词，如介词、冠词、连词和感叹词。这些词在机器学习过程中不具备任何意义，并且最好在训练机器学习算法之前将其删除。在Spark中，可以使用`StopWordsRemover`类来实现这一过程，如下面的代码片段所示：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding code block, we initialize the `StopWordsRemover` class by passing
    in the `inputCol` and `outputCol` parameters, which results in a transformer.
    Then, we transform the training dataset, resulting in a Spark DataFrame with a
    new column that has an array of individual words with the stop words removed.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们通过传入`inputCol`和`outputCol`参数来初始化`StopWordsRemover`类，从而生成一个转换器。然后，我们转换训练数据集，得到一个Spark
    DataFrame，其中包含一个新列，该列具有一个数组，其中包含已删除停用词的单个单词。
- en: Once we have an array of strings with the stop words removed, a feature extraction
    technique such as `Word2Vec` or `CountVectorizer` is used to build a feature vector.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了一个删除了停用词的字符串数组，就可以使用诸如`Word2Vec`或`CountVectorizer`等特征提取技术来构建特征向量。
- en: Encoding discrete, categorical variables
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码离散的分类变量
- en: Now we have other types of string-type columns such as country codes that need
    to be converted into a numerical form for consumption by a machine learning algorithm.
    You cannot simply assign arbitrary numerical values to such discrete, categorical
    variables, as this could introduce a pattern that might not necessarily exist
    within the data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有其他类型的字符串类型列，例如需要转换为数值形式以供机器学习算法使用的国家代码。你不能简单地为这些离散的分类变量分配任意的数值，因为这样可能会引入一种数据中本不存在的模式。
- en: Let's consider an example where we monotonically assign increasing values to
    categorical variables in alphabetical order. However, this might introduce ranking
    to those variables where one didn't exist in the first place. This would skew
    our machine learning model and is not desirable. To overcome this problem, we
    can use a number of Spark MLlib algorithms in which to encode these categorical
    variables.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子，其中我们按字母顺序单调递增地为分类变量分配值。然而，这可能会为这些变量引入一种排名，而这种排名在最初并不存在。这将扭曲我们的机器学习模型，并且并不理想。为了解决这个问题，我们可以使用Spark
    MLlib中的多种算法来编码这些分类变量。
- en: Encoding string variables using StringIndexer
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用StringIndexer编码字符串变量
- en: 'In our training dataset, we have string types, or categorical variables, with
    discrete values such as `country_code`. These variables can be assigned label
    indices using `StringIndexer`, as shown in the following code example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的训练数据集中，我们有字符串类型或分类变量，它们具有离散值，如`country_code`。这些变量可以使用`StringIndexer`分配标签索引，如下方代码示例所示：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code snippet, we initialize the `StringIndexer` class with
    input and output column names. Then, we set `handleInvalid` to `skip` in order
    to skip `NULLs` and invalid values. This results in an estimator that can be applied
    to the training DataFrame, which, in turn, results in a transformer. The transformer
    can be applied to the training dataset. This results in a DataFrame with a new
    Spark DataFrame along with a new column that contains label indices for the input
    categorical variable.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们初始化了`StringIndexer`类，并设置了输入和输出列的名称。然后，我们将`handleInvalid`设置为`skip`，以跳过`NULL`和无效值。这样就得到了一个可以应用于训练数据框的估算器，进而得到了一个变换器。该变换器可以应用于训练数据集，从而生成一个新的Spark数据框，并且在其中新增了一个列，包含输入分类变量的标签索引。
- en: Transforming a categorical variable into a vector using OneHotEncoder
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用OneHotEncoder将分类变量转换为向量
- en: 'Once we have our categorical variables encoded into label indices, they can
    finally be converted into a binary vector, using the `OneHotEncoder` class, as
    shown in the following code snippet:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将分类变量编码为标签索引，它们最终可以使用`OneHotEncoder`类转换为二进制向量，如下方代码片段所示：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code snippet, we initialize the `OneHotEncoder` class with
    input and output column names. This results in an estimator that can be applied
    to the training DataFrame, which, in turn, results in a transformer. The transformer
    can be applied to the training dataset. This results in a DataFrame with a new
    Spark DataFrame along with a new column that contains a feature vector representing
    the original categorical variable.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们初始化了`OneHotEncoder`类，并设置了输入和输出列的名称。这样就得到了一个估算器，可以应用于训练数据框，进而生成一个变换器。该变换器可以应用于训练数据集，从而生成一个新的Spark数据框，并在其中新增一个列，包含表示原始分类变量的特征向量。
- en: Transforming continuous variables
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换连续变量
- en: Continuous variables represent data in the form of measurements or observations.
    Typically, they are numerical in nature and can virtually have an infinite range.
    Here, the data is continuous and not discrete, and a few examples include age,
    quantity, and unit price. They seem straightforward enough and can be directly
    fed into a machine learning algorithm. However, they still need to be engineered
    into features, as continuous variables might have just far too many values to
    be handled by the machine learning algorithm. There are multiple ways in which
    to handle continuous variables, such as binning, normalization, applying custom
    business logic, and more, and an appropriate method should be chosen depending
    on the problem being solved and the business domain.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 连续变量以测量或观测值的形式表示数据。通常，它们是数值型的，几乎可以具有无限的范围。在这里，数据是连续的，而非离散的，一些例子包括年龄、数量和单价。它们看起来很直接，并可以直接输入机器学习算法。然而，它们仍然需要进行特征工程，因为连续变量可能有太多值，机器学习算法无法处理。处理连续变量的方法有很多种，比如分箱、归一化、应用自定义业务逻辑等，应该根据所解决的问题和业务领域选择适当的方法。
- en: 'One such technique to feature engineer continuous variables is binarization,
    where the continuous numerical values are converted into binary values based on
    a user-defined threshold, as shown in the following code example:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特征工程连续变量的技术是二值化，其中将连续的数值转换为基于用户定义阈值的二进制值，如以下代码示例所示：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding code block, we initialize the `Binarizer` class with the input
    and output column parameters, which results in a transformer. The transformer
    can then be applied to the training DataFrame, which, in turn, results in a new
    DataFrame along with a new column representing the binary values for the continuous
    variable.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们通过输入和输出列参数初始化 `Binarizer` 类，生成一个转换器。然后可以将此转换器应用于训练数据框，从而得到一个新的数据框，并附加一个表示连续变量的二进制值的新列。
- en: Transforming the date and time variables
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换日期和时间变量
- en: A date or timestamp type of column in itself doesn't add much value to a machine
    learning model training process. However, there might be patterns within the components
    of a date such as month, year, or day of the week. Therefore, it would be useful
    to choose a part of the datetime column and transform it into an appropriate feature.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 日期或时间戳类型的列本身对机器学习模型的训练过程并没有太大价值。然而，日期的组成部分，如月份、年份或星期几，可能会有某些模式。因此，选择日期时间列的某一部分并将其转换为适当的特征是非常有用的。
- en: 'In the following code example, we extract the month value from a datetime column
    and transform it into a feature, treating it like a categorical variable:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码示例中，我们从日期时间列中提取月份值并将其转换为特征，将其视为类别变量：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding code block, first, we extract the month from the timestamp
    column using the `month()` function and append it to the DataFrame. Then, we run
    the new column through the `StringIndexer` estimator and transform the month numeric
    column into a label index.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们首先使用 `month()` 函数从时间戳列中提取月份，并将其附加到数据框中。然后，我们将新列通过 `StringIndexer`
    估算器进行转换，将月份的数字列转换为标签索引。
- en: Assembling individual features into a feature vector
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将单个特征组合成特征向量
- en: 'Most machine learning algorithms accept a single feature vector as input. Therefore,
    it would be useful to combine the individual features that you have extracted
    and transformed into a single feature vector. This can be accomplished using Spark
    MLlib''s `VectorAssembler` transformer, as shown in the following code example:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法接受一个单一的特征向量作为输入。因此，将提取并转换的单个特征合并为一个特征向量是很有用的。可以使用 Spark MLlib 的 `VectorAssembler`
    转换器来实现，如以下代码示例所示：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding block of code, we initialize the `VectorAssembler` class with
    input and output parameters, which results in a transformer object. We make use
    of the transformer to combine the individual features into a single feature vector.
    This results in a new column of the vector type being appended to the training
    DataFrame.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们通过输入和输出参数初始化 `VectorAssembler` 类，生成一个转换器对象。我们利用该转换器将单个特征合并为一个特征向量。这样，新的向量类型列就会附加到训练数据框中。
- en: Feature scaling
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征缩放
- en: It is common for training datasets to have columns with different units of measurements.
    For instance, while one column uses the metric system of measurement, another
    column might be using the imperial system. It is also possible for certain columns
    to have a high range, such as a column representing dollar amounts than another
    column representing quantities, for instance. These differences might cause a
    machine learning model to unduly assign more weightage to a certain value compared
    to others, which is undesirable and might introduce bias or skew into the model.
    To overcome this issue, a technique called feature scaling can be utilized. Spark
    MLlib comes with a few feature scaler algorithms, such as `Normalizer`, `StandardScaler`,
    `RobustScaler`, `MinMaxScaler`, and `MaxAbsScaler`, built in.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集通常会有不同计量单位的列。例如，一列可能使用公制计量单位，而另一列可能使用英制单位。某些列可能具有较大的数值范围，例如，一列表示美元金额，另一列表示数量。这些差异可能导致机器学习模型不当地对某些值赋予更多权重，从而产生不良影响，可能会引入偏差或模型失真。为了解决这个问题，可以使用一种称为特征缩放的技术。Spark
    MLlib内置了一些特征缩放算法，如`Normalizer`、`StandardScaler`、`RobustScaler`、`MinMaxScaler`和`MaxAbsScaler`。
- en: 'In the following code example, we will make use of `StandardScaler` to demonstrate
    how feature scaling can be applied in Apache Spark. `StandardScaler` transforms
    a feature vector and normalizes each vector to have a unit of standard deviation:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们将使用`StandardScaler`来演示如何在Apache Spark中应用特征缩放。`StandardScaler`转换特征向量，并将每个向量规范化为具有标准差单位：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding block of code, the `StandardScaler` class is initialized with
    the input and output column parameters. Then, the `StandardScaler` estimator is
    applied to the training dataset, resulting in a `StandardScaler` model transformer
    object. This, in turn, can be applied to the training DataFrame to yield a new
    DataFrame a new column that contains the normalized features.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，`StandardScaler`类被初始化为输入和输出列的参数。然后，`StandardScaler`估算器应用于训练数据集，生成一个`StandardScaler`模型转换对象。接着，可以将该对象应用到训练DataFrame，从而生成一个新DataFrame，并添加一个包含规范化特征的新列。
- en: So far, in this section, you have learned how to extract machine learning features
    from dataset columns. Additionally, you have learned a feature extraction technique
    to convert text-based columns into feature vectors. Feature transformation techniques
    for converting categorical, continuous, and date- and time-based variables were
    also explored. Techniques for combing multiple individual features into a single
    feature vector were introduced, and, finally, you were also introduced to a feature
    scaling technique to normalize features.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本节中，你已经学习了如何从数据集的列中提取机器学习特征。此外，你还学习了将基于文本的列转换为特征向量的特征提取技术。我们还探讨了将分类、连续以及基于日期和时间的变量转换的特征转换技术。介绍了将多个单独的特征组合成单个特征向量的技术，最后，你还学习了一种特征缩放技术来规范化特征。
- en: In the following section, you will learn techniques in which to reduce the number
    of features; this is referred to as **feature selection**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，你将学习减少特征数量的技术，这被称为**特征选择**。
- en: Feature selection
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: Feature selection is a technique that involves reducing the number of features
    in the machine learning process while leveraging lesser data and also improving
    the accuracy of the trained model. Feature selection is the process of either
    automatically or manually selecting only those features that contribute the most
    to the prediction variable that you are interested in. Feature selection is an
    important aspect of machine learning, as irrelevant or semi-relevant features
    can gravely impact model accuracy.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择是一种技术，它通过减少机器学习过程中的特征数量，同时利用较少的数据，并提高训练模型的准确性。特征选择是一个过程，可以自动或手动选择那些对你所关注的预测变量贡献最大的特征。特征选择是机器学习中的一个重要方面，因为不相关或半相关的特征可能会严重影响模型的准确性。
- en: 'Apache Spark MLlib comes packaged with a few feature selectors, including `VectorSlicer`,
    `ChiSqSelector`, `UnivariateFeatureSelector`, and `VarianceThresholdSelector`.
    Let''s explore how to implement feature selection within Apache Spark using the
    following code example that utilizes `ChiSqSelector` to select the optimal features
    given the label column that we are trying to predict:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark MLlib 提供了一些特征选择器，包括`VectorSlicer`、`ChiSqSelector`、`UnivariateFeatureSelector`和`VarianceThresholdSelector`。让我们通过以下代码示例，探索如何在Apache
    Spark中实现特征选择，利用`ChiSqSelector`根据我们试图预测的标签列选择最优特征：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code block, we initialize `ChiSqSelector` using the input and
    the output columns. We also specify the label column, as `ChiSqSelector` chooses
    the optimal features best suited for predicting the label columns. Then, the `ChiSqSelector`
    estimator is applied to the training dataset, resulting in a `ChiSqSelector` model
    transformer object. This, in turn, can be applied to the training DataFrame to
    yield a new DataFrame column that contains the newly selected features.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们使用输入列和输出列初始化`ChiSqSelector`。我们还指定了标签列，因为`ChiSqSelector`选择最适合预测标签列的最优特征。然后，将`ChiSqSelector`估算器应用于训练数据集，生成一个`ChiSqSelector`模型转换器对象。接下来，可以将该对象应用于训练数据框，以生成一个新的数据框列，其中包含新选择的特征。
- en: 'Similarly, we can also leverage `VectorSlicer` to select a subset of features
    from a given feature vector, as shown in the following code snippet:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们也可以利用`VectorSlicer`从给定的特征向量中选择一部分特征，如下列代码片段所示：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding code block also performs feature selection. However, unlike `ChiSqSelector`,
    `VectorSlicer` doesn't optimize feature selection for a given variable. Instead,
    `VectorSlicer` takes a vector column with specified indices. This results in a
    new vector column whose values are selected through the specified indices. Each
    feature selector has its own way of making feature selections, and the appropriate
    feature selector should be used for the given scenario and the problem being solved.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块也执行了特征选择。然而，与`ChiSqSelector`不同，`VectorSlicer`并没有针对给定的变量优化特征选择。相反，`VectorSlicer`接受一个带有指定索引的向量列。这将生成一个新的向量列，其值通过指定的索引进行选择。每个特征选择器都有自己进行特征选择的方法，应该根据给定的场景和待解决的问题选择合适的特征选择器。
- en: So far, you have learned how to perform feature extraction from text-based variables
    and how to perform feature transformation on categorical and continuous types
    of variables. Additionally, you have explored the techniques for feature slicing
    along with feature selection. You have acquired techniques to transform preprocessed
    raw data into feature vectors that are ready to be fed into a machine learning
    algorithm in order to build machine learning models.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了如何从基于文本的变量中提取特征，以及如何对分类和连续类型的变量进行特征转换。此外，你还探索了特征切片和特征选择的技术。你已经掌握了将预处理后的原始数据转换为特征向量的技术，这些特征向量可以直接输入到机器学习算法中，用于构建机器学习模型。
- en: However, it seems redundant and time-consuming to perform feature engineering
    for each and every machine learning problem. So, can you not just use some previously
    built features for a new model? The answer is yes, and you should reuse some of
    your previously built features for new machine learning problems. You should also
    be able to make use of the features of some of your other team members. This can
    be accomplished via a centralized feature store. We will explore this topic further
    in the following section.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于每一个机器学习问题都执行特征工程似乎既冗余又耗时。那么，能不能直接使用一些已经构建好的特征来为新模型服务呢？答案是肯定的，你应该在新的机器学习问题中重用之前构建的特征。你也应该能够利用你其他团队成员的特征。这可以通过一个集中式特征存储来实现。我们将在接下来的部分进一步探讨这个话题。
- en: Feature store as a central feature repository
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征存储作为中央特征库
- en: A large percentage of the time spent on any machine learning problem is on data
    cleansing and data wrangling to ensure we build our models on clean and meaningful
    data. Feature engineering is another critical process of the machine learning
    process where data scientists spend a huge chunk of their time curating machine
    learning features, which happens to be a complex and time-consuming process. It
    appears counter-intuitive to have to create features again and again for each
    new machine learning problem.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何机器学习问题上花费的大部分时间都用于数据清洗和数据整理，以确保我们建立模型的基础是干净且有意义的数据。特征工程是机器学习过程中的另一个关键步骤，数据科学家们花费大量时间策划机器学习特征，这是一个复杂且耗时的过程。为每个新的机器学习问题再次创建特征似乎是违反直觉的。
- en: 'Typically, feature engineering takes place on already existing historic data,
    and new features are perfectly reusable in different machine learning problems.
    In fact, data scientists spend a good amount of time searching for the right features
    for the problem at hand. So, it would be tremendously beneficial to have a centralized
    repository of features that is also searchable and has metadata to identify features.
    This central repository of searchable features is generally termed a **feature
    store**. A typical feature store architecture is depicted in the following diagram:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，特征工程是在已经存在的历史数据上进行的，新特征在不同的机器学习问题中是可以完全重用的。事实上，数据科学家花费大量时间寻找问题所需的正确特征。因此，拥有一个集中的特征库，可搜索并具有用于识别特征的元数据将是非常有益的。这个集中的可搜索特征库通常被称为**特征存储**。典型的特征存储架构如下图所示：
- en: '![Figure 6.5 – The feature store architecture'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5 – 特征存储架构'
- en: '](img/B16736_06_05.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_06_05.jpg)'
- en: Figure 6.5 – The feature store architecture
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 特征存储架构
- en: Features are useful not only during the model training phase of the machine
    learning process, but they are also required during model inferencing. **Inferencing**,
    which is also referred to as **model scoring**, is the process of feeding an already
    built model with new and unseen features in order to generate predictions on the
    new data. Depending on whether the inferencing process takes place in batch mode
    or a streaming, real-time fashion, features can be very broadly classified into
    offline features and online features.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 特征不仅在机器学习过程的模型训练阶段中有用，而且在模型推断过程中也是必需的。**推断**，也称为**模型评分**，是将已构建的模型与新的未见特征一起输入，以便在新数据上生成预测的过程。根据推断过程是批处理模式还是流式实时模式，特征可以被广泛分类为离线特征和在线特征。
- en: Batch inferencing using the offline feature store
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用离线特征存储进行批量推断
- en: Offline features, as the name suggests, are generated offline using a batch
    job. Their consumption also happens offline using either the model training process
    or model inferencing in a batch fashion, that is, using scheduled batch machine
    learning pipelines. These features can be time-consuming to create and are typically
    created using big data frameworks, such as Apache Spark, or by running scheduled
    queries off of a database or a data warehouse.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 离线特征，顾名思义，是使用批处理作业离线生成的。它们的消耗也是离线进行的，可以通过模型训练过程或批量机器学习管道中的模型推断来进行，即使用定期安排的批处理方式。这些特征可能需要耗费时间来创建，通常使用大数据框架（如
    Apache Spark）创建，或通过从数据库或数据仓库运行定期查询来创建。
- en: The storage mechanism used to generate offline features is referred to as an
    offline feature store. Historical datastores, RDBMS databases, data warehouse
    systems, and data lakes all make good candidates for offline feature stores. It
    is desirable for an offline feature store to be strongly typed, have a schema
    enforcement mechanism, and have the ability to store metadata along with the actual
    features. Any database or a data warehouse is adequate for an offline feature
    store; however, in the next section, we will explore Delta Lake as an offline
    feature store.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成离线特征的存储机制被称为离线特征存储。历史数据存储、关系型数据库、数据仓库系统和数据湖都是离线特征存储的良好选择。离线特征存储应具有强类型、模式强制执行机制，并具有存储元数据以及实际特征的能力。任何数据库或数据仓库都适用于离线特征存储；然而，在下一节中，我们将探讨
    Delta Lake 作为离线特征存储。
- en: Delta Lake as an offline feature store
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Delta Lake 作为离线特征存储
- en: In [*Chapter 3*](B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056), *Data Cleansing
    and Integration*, we established data lakes as the scalable and relatively inexpensive
    choice for the long-term storage of historical data. Some challenges with reliability
    and cloud-based data lakes were presented, and you learned how Delta Lake has
    been designed to overcome these challenges. The benefits of Delta Lake as an abstraction
    layer on top of cloud-based data lakes extend beyond just data engineering workloads
    to data science workloads as well, and we will explore those benefits in this
    section.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第3章*](B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056)《数据清洗与集成》中，我们将数据湖确立为长期存储历史数据的可扩展且相对便宜的选择。我们讨论了数据可靠性和基于云的数据湖的一些挑战，并且你了解了
    Delta Lake 是如何设计来克服这些挑战的。作为云数据湖的抽象层，Delta Lake 的优势不仅限于数据工程工作负载，也扩展到了数据科学工作负载，我们将在本节中深入探讨这些优势。
- en: Delta Lake makes for an ideal candidate for an offline feature store on cloud-based
    data lakes because of the data reliability features and the novel time travel
    features that Delta Lake has to offer. We will discuss these in the following
    sections.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 是基于云的数据湖中理想的离线特征存储候选，因为它具备数据可靠性特性和 Delta Lake 提供的独特时间旅行功能。我们将在接下来的章节中讨论这些内容。
- en: Structure and metadata with Delta tables
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta 表的结构和元数据
- en: Delta Lake supports structured data with well-defined data types for columns.
    This makes Delta tables strongly typed, ensuring that all kinds of features of
    various data types can be stored in Delta tables. In comparison, the actual storage
    happens on relatively inexpensive and infinitely scalable cloud-based data lakes.
    This makes Delta Lake an ideal candidate offline feature store in the cloud.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 支持具有明确定义列数据类型的结构化数据。这使得 Delta 表具有强类型，确保可以将各种数据类型的特征存储在 Delta 表中。相比之下，实际存储发生在相对便宜且可以无限扩展的基于云的数据湖中。这使得
    Delta Lake 成为云中理想的离线特征存储候选。
- en: Schema enforcement and evolution with Delta Lake
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake 的模式强制执行与演变
- en: Delta Lake fully supports schema enforcement, which means that the data integrity
    of features inserted into a Delta Lake feature store is well maintained. This
    will help to ensure that only the correct data with proper data types will be
    used for the machine learning model building process, ensuring model performance.
    Delta Lake's support for schema evolution also means that new features could be
    easily added to a Delta Lake-based feature store.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 完全支持模式强制执行，这意味着插入到 Delta Lake 特征存储中的特征数据的完整性得到了良好的维护。这将确保只有正确的、具有适当数据类型的数据用于机器学习模型的构建过程，从而确保模型的性能。Delta
    Lake 对模式演变的支持也意味着可以轻松地将新特征添加到基于 Delta Lake 的特征存储中。
- en: Support for simultaneous batch and streaming workloads
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持同时处理批处理和流处理工作负载
- en: Since Delta Lake fully supports unified batch and streaming workloads, data
    scientists can build near real-time, streaming feature engineering pipelines in
    addition to batch pipelines. This will help to train machine learning models with
    the freshest features and also generate predictions in a near real-time fashion.
    This will help to eliminate any operational overhead for use cases with relatively
    higher latency inferencing requirements by just leveraging Apache Spark's unified
    analytics engine.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Delta Lake 完全支持统一的批处理和流处理工作负载，数据科学家除了批处理管道之外，还可以构建近实时的流式特征工程管道。这将有助于使用最新特征训练机器学习模型，并且也能够近实时地生成预测。这将通过仅利用
    Apache Spark 的统一分析引擎，帮助消除高延迟推断需求的操作开销。
- en: Delta Lake time travel
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake 时间旅行
- en: Often, data scientists experiment with slight variations of data to improve
    model accuracy, and they often maintain several versions of the same physical
    data for this purpose. With Delta Lake's time travel functionality, a single Delta
    table can easily support multiple versions of data, thus eliminating the overhead
    for data scientists in maintaining several physical versions of data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家经常通过微小的数据变动来改善模型准确度，通常会为此目的维护相同物理数据的多个版本。利用 Delta Lake 的时间旅行功能，单个 Delta
    表可以轻松支持数据的多个版本，从而消除数据科学家维护多个物理数据版本的开销。
- en: Integration with machine learning operations tools
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与机器学习操作工具的集成
- en: Delta Lake also supports integration with the popular machine learning operations
    and workflow management tool called **MLflow**. We will explore MLOps and MLflow
    in [*Chapter 9*](B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164)*, Machine Learning
    Life Cycle Management*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 还支持与流行的机器学习操作和工作流管理工具 **MLflow** 集成。我们将在[*第9章*](B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164)《机器学习生命周期管理》中探讨
    MLOps 和 MLflow。
- en: 'A code example of leveraging Delta Lake as an offline feature store is presented
    here:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了一个利用 Delta Lake 作为离线特征存储的代码示例：
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: First, we create a database named `feature_store`. Then, we save the DataFrame
    as a result of the *Feature selection* step in the previous section as a Delta
    table.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个名为 `feature_store` 的数据库。然后，我们将上一节中的 *特征选择* 步骤的结果保存为 Delta 表。
- en: In this way, features can be searched for using simple SQL commands and can
    also be shared and used for other machine learning use cases via the shared Hive
    metastore. Delta Lake also supports common metadata such as column names and data
    types, and other metadata such as user notes and comments can be also included
    to add more context to the features in the feature store.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，可以使用简单的 SQL 命令搜索特征，并且还可以通过共享的 Hive 元存储库将其共享并用于其他机器学习应用场景。Delta Lake 还支持常见的元数据，例如列名和数据类型，其他元数据，如用户备注和评论，也可以添加，以便为特征存储中的特征提供更多上下文信息。
- en: Tip
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Most big data platforms, including Databricks, support the built-in Hive metastore
    for storing table metadata. Additionally, these platforms come with security mechanisms
    such as databases, tables, and, sometimes, even row- and column-level access control
    mechanisms. In this way, the feature store can be secured, and features can be
    selectively shared among data teams.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数大数据平台，包括 Databricks，都支持内置的 Hive 元存储库来存储表的元数据。此外，这些平台还提供了安全机制，如数据库、表格，有时甚至包括行级和列级访问控制机制。通过这种方式，特征存储可以得到保护，特征可以在数据团队之间进行选择性共享。
- en: In this way, using Delta Lake can serve as an offline feature store on top of
    cloud-based data lakes. Once features are stored in Delta tables, they are accessible
    from all of Spark's APIs, including DataFrames and SQL APIs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，使用 Delta Lake 可以作为云数据湖上的离线特征存储。一旦特征被存储在 Delta 表中，它们可以通过 Spark 的所有 API
    访问，包括 DataFrame 和 SQL API。
- en: Note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: All of the functions and methods present inside Spark MLlib have been designed
    to be natively scalable. Therefore, any machine learning operation performed using
    Spark MLlib is inherently scalable and can run Spark jobs with parallel and distributed
    tasks underneath.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib 中的所有函数和方法都经过设计，可以原生地进行扩展。因此，使用 Spark MLlib 执行的任何机器学习操作天生具有可扩展性，并且可以运行具有并行和分布式任务的
    Spark 作业。
- en: Online feature store for real-time inferencing
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于实时推理的在线特征存储
- en: Features that are used in online machine learning inferencing are called online
    features. Usually, these features have an ultra-low latency requirement, ranging
    from milliseconds to mere seconds. Some use cases of online features include real-time
    predictions in end user applications.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在线机器学习推理的特征被称为在线特征。通常，这些特征具有超低延迟的要求，通常从毫秒到秒级。在线特征的一些使用场景包括在最终用户应用中的实时预测。
- en: Let's consider the example of a customer browsing an e-tailer's web app. The
    customer adds a product to their cart, and based on the customer's zip code, the
    web app needs to provide an estimated delivery time within seconds. The machine
    learning model involved here requires a few features to estimate the delivery
    lead time, such as warehouse location, product availability, historical delivery
    times from this warehouse, and maybe even the weather and seasonal conditions,
    but most importantly, it needs the customer zip code. Most of the features could
    already be precalculated and available in an offline feature store. However, given
    the low latency requirement for this use case, the feature store must be able
    to deliver the features with the lowest latency possible. A data lake, or a database
    or data warehouse, is not an ideal candidate for this use case and requires an
    ultra-low latency, online feature store.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个客户浏览电商网站应用的例子。客户将一个产品加入购物车，基于客户的邮政编码，网页应用需要在几秒钟内提供一个预计的送货时间。这里涉及的机器学习模型需要一些特征来估算交货时间，比如仓库位置、产品库存、历史交货时间，甚至可能还需要天气和季节性条件，但最重要的是，它需要客户的邮政编码。大多数特征可能已经在离线特征存储中预先计算并可用。然而，鉴于该用例的低延迟要求，特征存储必须能够以最低的延迟提供这些特征。数据湖、数据库或数据仓库并不是该用例的理想选择，需要一个超低延迟的在线特征存储。
- en: 'From the preceding example, we can conclude that online inferencing in real
    time requires a few critical components:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的示例中，我们可以得出结论，实时在线推理需要一些关键组件：
- en: An ultra-low latency, preferably, in-memory feature store.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个超低延迟的、最好是内存中的特征存储。
- en: An event processing, low latency streaming engine
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个事件处理、低延迟流处理引擎
- en: RESTful APIs for integration with the end user web and mobile applications
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于与终端用户的网页和移动应用集成的RESTful API
- en: 'An example of a real-time inferencing pipeline is presented in the following
    diagram:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个实时推理管道的例子：
- en: '![Figure 6.6 – A real-time machine learning inferencing pipeline'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 – 实时机器学习推理管道'
- en: '](img/B16736_06_06.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16736_06_06.jpg)'
- en: Figure 6.6 – A real-time machine learning inferencing pipeline
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 实时机器学习推理管道
- en: In the preceding diagram, data arrives from the web or mobile apps onto a message
    queue such as Apache Kafka in real time. A low-latency event processing engine
    such as Apache Flink processes incoming features and stores them onto a NoSQL
    database such as Apache Cassandra or in-memory databases such as Redis for online
    feature stores. A machine learning inference engine fetches the features from
    the online feature store, generates predictions, and pushes them back to the web
    or mobile apps via REST APIs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，数据从网页或移动应用实时到达消息队列，如Apache Kafka。一个低延迟的事件处理引擎，如Apache Flink，处理传入的特征并将其存储到NoSQL数据库（如Apache
    Cassandra）或内存数据库（如Redis）中，以便用于在线特征存储。机器学习推理引擎从在线特征存储中提取特征，生成预测，并通过REST API将预测推送回网页或移动应用。
- en: Note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Neither Apache Spark nor Delta Lake on cloud-based data lakes makes a good candidate
    for an online feature store. Spark's Structured Streaming has been designed to
    handle high throughput in favor of low latency or processing. Structured Streaming's
    micro-batch is not suitable to process an event as it arrives at the source. In
    general, cloud-based data lakes are designed for scalability and have latency
    specifications, and, therefore, Delta Lake cannot support the ultra-low latency
    requirements of online feature stores.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是Apache Spark还是基于云的数据湖中的Delta Lake，都不适合用作在线特征存储。Spark的结构化流处理被设计用来处理高吞吐量，而非低延迟处理。结构化流处理的微批处理不适合在事件到达源头时进行处理。通常，基于云的数据湖是为了可扩展性而设计的，并且具有延迟规范，因此Delta
    Lake无法满足在线特征存储对超低延迟的要求。
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about the concept of feature engineering and why
    it is an important part of the whole machine learning process. Additionally, you
    learned why it is required to create features and train machine learning models.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了特征工程的概念，以及它为何是整个机器学习过程中的重要部分。此外，你还了解了为何需要创建特征并训练机器学习模型。
- en: You explored various feature engineering techniques such as feature extraction
    and how they can be used to convert text-based data into features. Feature transformation
    techniques useful in dealing with categorical and continuous variables were introduced,
    and examples of how to convert them into features were presented. You also explored
    feature scaling techniques that are useful for normalizing features to help prevent
    some features from unduly biasing the trained model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你探索了各种特征工程技术，如特征提取，以及它们如何将基于文本的数据转换为特征。介绍了在处理类别型和连续变量时有用的特征转换技术，并展示了如何将它们转换为特征的示例。你还探索了有助于归一化特征的特征缩放技术，以防止某些特征对训练模型产生过度偏倚。
- en: Finally, you were introduced to techniques for selecting the right features
    to optimize the model performance for the label being predicted via feature selection
    techniques. The skills learned in this chapter will help you to implement scalable
    and performant feature engineering pipelines using Apache Spark and leveraging
    Delta Lake as a central, sharable repository of features.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你了解了通过特征选择技术选择合适特征的方法，以优化预测标签的模型性能。本章所学的技能将帮助你使用Apache Spark实现可扩展且高效的特征工程管道，并利用Delta
    Lake作为特征的中央共享存储库。
- en: In the following chapter, you will learn about the various machine learning
    training algorithms that fall under the supervised learning category. Additionally,
    you will implement code examples that make use of the features generated in this
    chapter in order to train actual machine learning models using Apache Spark MLlib.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将学习属于监督学习范畴的各种机器学习训练算法。此外，你将实现代码示例，利用本章生成的特征，在Apache Spark MLlib中训练实际的机器学习模型。
