- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Image and Audio Preprocessing with LLMs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLMs进行图像和音频预处理
- en: In this chapter, we delve into the preprocessing of unstructured data, specifically
    focusing on images and audio. We explore various techniques and models designed
    to extract meaningful information from these types of media. The discussion includes
    a detailed examination of image preprocessing methods, the use of **optical character
    recognition** (**OCR**) for extracting text from images, the capabilities of the
    BLIP model for generating image captions, and the application of the Whisper model
    for converting audio into text.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了非结构化数据的预处理，特别关注图像和音频。我们探讨了从这些类型的媒体中提取有意义信息的各种技术和模型。讨论包括对图像预处理方法的详细审查，使用**光学字符识别**（**OCR**）从图像中提取文本，BLIP模型生成图像标题的能力，以及Whisper模型将音频转换为文本的应用。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: The current era of image preprocessing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像预处理的当前时代
- en: Extracting text from images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图像中提取文本
- en: Handling audio data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理音频数据
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The complete code for this chapter can be found in the following GitHub repository:
    [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter13](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter13).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的完整代码可以在以下GitHub存储库中找到：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter13](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter13)。
- en: 'Let''s install the necessary libraries we will use in this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装本章中将使用的必要库：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The current era of image preprocessing
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像预处理的当前时代
- en: In the era of advanced visual models, such as diffusion models, and models such
    as OpenAI’s CLIP, preprocessing has become crucial to ensure the quality, consistency,
    and suitability of images for training and inference. These models require images
    to be in a format that maximizes their ability to learn intricate patterns and
    generate high-quality results. In this section, we will go through all the preprocessing
    steps to make your images ready for the subsequent tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在先进的视觉模型时代，如扩散模型和OpenAI的CLIP等模型，预处理变得至关重要，以确保图像的质量、一致性和适用性，用于训练和推断。这些模型要求图像以最大化它们学习复杂模式和生成高质量结果的能力的格式。在本节中，我们将逐步进行所有预处理步骤，使您的图像准备好进行后续任务。
- en: 'Across this section, we will use a common use case, which is to prepare images
    for training a diffusion model. You can find the code for this exercise in the
    GitHub repository: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/1.image_prerpocessing.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/1.image_prerpocessing.py).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个常见的用例，即为训练扩散模型准备图像。您可以在GitHub存储库中找到此练习的代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/1.image_prerpocessing.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/1.image_prerpocessing.py)。
- en: Let’s start by loading some images.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始加载一些图像。
- en: Loading the images
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载图像
- en: 'Perform the following steps to load the images:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤加载图像：
- en: 'First, we load the required packages for this exercise:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们加载本练习所需的包：
- en: '[PRE1]'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, we load the images into our environment. We’ll use the Python Pillow library
    to handle loading the images.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们将图像加载到我们的环境中。我们将使用Python Pillow库来处理加载图像。
- en: 'Next, we create a function to load an image from a URL. This function fetches
    the image from the given URL and loads it into a `PIL` image object using `BytesIO`
    to handle the byte data:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个函数从URL加载图像。这个函数从给定的URL获取图像，并使用`BytesIO`加载到`PIL`图像对象中处理字节数据：
- en: '[PRE2]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we’ll create a helper function to display our images. We will be using
    this function across the chapter:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个辅助函数来显示我们的图像。我们将在整个章节中使用这个函数：
- en: '[PRE3]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we’ll pass the image URL to our `load_image_from_url` function. Here,
    we are using a random image URL from Unsplash, but you can use any image you have
    access to:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将图像URL传递给我们的`load_image_from_url`函数。在这里，我们使用了来自Unsplash的随机图像URL，但您可以使用您可以访问的任何图像：
- en: '[PRE4]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s display the original image that we just loaded using the function we
    created:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们显示刚刚使用我们创建的函数加载的原始图像：
- en: '[PRE5]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will display the following output image:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示以下输出图像：
- en: '![Figure 13.1 – Original image before any preprocessing](img/B19801_13_1.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图13.1 – 预处理前的原始图像](img/B19801_13_1.jpg)'
- en: Figure 13.1 – Original image before any preprocessing
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 – 预处理前的原始图像
- en: Image preprocessing is crucial for preparing visual data for ingestion by **machine
    learning** (**ML**) models. Let’s delve deeper into each technique, explaining
    the concepts and demonstrating their application with Python code.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图像预处理对于将视觉数据输入到**机器学习**（**ML**）模型中至关重要。让我们深入探讨每种技术，解释其概念并通过Python代码展示其应用。
- en: Resizing and cropping
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整大小和裁剪
- en: Effective preprocessing can significantly enhance the performance of AI and
    ML models by ensuring that the most relevant features are highlighted and easily
    detectable in the images. **Cropping** is a technique that can help the model
    focus on relevant features. The main idea is to trim or cut away the outer edges
    of an image to improve framing, focus on the main subject, or eliminate unwanted
    elements. The size of the crop depends on the specific requirements of the task.
    For example, in object detection, the crop should focus on the object of interest,
    while in image classification, the crop should ensure that the main subject is
    centered and occupies most of the frame.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的预处理可以显著提高AI和机器学习（ML）模型的性能，通过确保图像中最相关的特征得到突出并易于检测。**裁剪**是一种帮助模型聚焦于相关特征的技术。其主要思路是修剪或切除图像的外边缘，以改善画面构图，聚焦于主要对象，或去除不需要的元素。裁剪的大小取决于任务的具体要求。例如，在目标检测中，裁剪应聚焦于感兴趣的目标，而在图像分类中，裁剪应确保主要对象居中并占据大部分画面。
- en: There are many different techniques for cropping images from a simple fixed-size
    cropping to more involved object-aware cropping. **Fixed-size cropping** involves
    adjusting all images to a predetermined size, ensuring uniformity across the dataset,
    which is useful for applications that require standardized input sizes, such as
    training certain types of neural networks. However, it may result in the loss
    of important information if the main subject is not centered. **Aspect ratio preservation**
    avoids distortion by maintaining the original image’s aspect ratio while cropping,
    which is achieved through padding (adding borders to the image to reach the desired
    dimensions) or scaling (resizing the image while maintaining its aspect ratio,
    followed by cropping to the target size). **Center cropping** involves cropping
    the image around its center, assuming the main subject is generally located in
    the middle, and is commonly used in image classification tasks where the main
    subject should occupy most of the frame. **Object-aware cropping** uses algorithms
    to detect the main subject within the image and crop around it, ensuring that
    the main subject is always emphasized, regardless of its position within the original
    image. This technique is particularly useful in object detection and recognition
    tasks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 裁剪图像有许多不同的技术，从简单的固定大小裁剪到更复杂的对象感知裁剪。**固定大小裁剪**涉及将所有图像调整为预定的大小，确保数据集的一致性，这对于需要标准化输入大小的应用非常有用，比如训练某些类型的神经网络。然而，如果主要对象未居中，这可能会导致重要信息的丢失。**保持纵横比**通过在裁剪时保持原图像的纵横比，避免了失真，通常通过填充（给图像添加边框以达到所需尺寸）或缩放（在保持纵横比的同时调整图像大小，然后裁剪到目标尺寸）来实现。**中心裁剪**是在图像的中心进行裁剪，假设主要对象通常位于中间，这在图像分类任务中很常见，其中主要对象应占据画面的大部分。**对象感知裁剪**使用算法检测图像中的主要对象，并围绕其进行裁剪，确保无论对象在原始图像中的位置如何，始终突出显示主要对象。这种技术在目标检测和识别任务中尤为有用。
- en: '**Resizing** is a fundamental step in image preprocessing for AI and ML tasks,
    focusing on adjusting the dimensions of an image to a standard size required by
    the model. This process is crucial for ensuring that the input data is consistent
    and suitable for the specific requirements of various AI and ML algorithms.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**调整大小**是AI和机器学习任务中图像预处理的基本步骤，主要集中在将图像的尺寸调整为模型所需的标准大小。此过程对于确保输入数据的一致性并适应各种AI和机器学习算法的具体要求至关重要。'
- en: 'Let’s add some steps to the image preprocessing pipeline we started in the
    previous section to see the effects of cropping and resizing. The following function
    resizes the image to a specified target size (256x256 pixels in this case). We
    expect the image to appear uniformly sized down to fit within the target dimensions:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在前一节开始的图像预处理流程中添加一些步骤，以查看裁剪和调整大小的效果。以下函数将图像调整大小到指定的目标尺寸（在本例中为256x256像素）。我们期望图像以统一的尺寸缩小以适应目标尺寸：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s print the resulting image using the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码打印结果图像：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will display the following output:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下输出：
- en: '![Figure 13.2 – Image after resizing and cropping](img/B19801_13_2.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.2 – 调整大小和裁剪后的图像](img/B19801_13_2.jpg)'
- en: Figure 13.2 – Image after resizing and cropping
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 – 调整大小和裁剪后的图像
- en: As we can see from *Figure 13**.2*, the image is resized to a square of 256x256
    pixels, altering the aspect ratio of the original image that was not square. Thus,
    resizing ensures a uniform input size for all data, which facilitates the batch
    processing and the passing of data to models for training.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从*图 13**.2*中看到的那样，图像被调整为一个256x256像素的正方形，改变了原始图像不是正方形的长宽比。因此，调整大小确保所有数据具有统一的输入尺寸，从而便于批处理和将数据传递给模型进行训练。
- en: Next, we will discuss the normalization of images, which is not far from the
    normalization of features discussed in previous chapters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论图像的标准化，这与前几章讨论的特征标准化并无太大差异。
- en: Normalizing and standardizing the dataset
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对数据集进行标准化和归一化
- en: To ensure data consistency and help the training of models converge faster,
    we can force the input data to a common range of values. This adjustment involves
    scaling the input data between `0` and `1`, also known as **standardization**
    or **normalizing** using the mean and standard deviation of the dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保数据一致性并帮助模型训练更快收敛，我们可以强制输入数据处于一个共同的数值范围内。这种调整涉及将输入数据缩放到`0`和`1`之间，也被称为**标准化**或使用数据集的均值和标准差进行**归一化**。
- en: For most deep learning models, forcing pixel values to the range `[0, 1]` or
    `[-1, 1]` is standard practice. This can be achieved by dividing pixel values
    by 255 (for `[0, 1]`) or by subtracting the mean and dividing by the standard
    deviation (for `[-1, 1]`). In image classification tasks, this tactic ensures
    that the input images have consistent pixel values. For example, in a dataset
    of handwritten digits (such as MNIST), normalizing or standardizing the pixel
    values helps the model learn the patterns of the digits more effectively. In object
    detection tasks, it helps in accurately detecting and classifying objects within
    an image. However, normalization and standardization are not limited to image
    preprocessing; they are a fundamental step in preparing data for any ML problem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数深度学习模型，将像素值强制限制在范围`[0, 1]`或`[-1, 1]`是标准做法。这可以通过将像素值除以255（对于`[0, 1]`）或减去均值并除以标准差（对于`[-1,
    1]`）来实现。在图像分类任务中，这种策略确保输入图像具有一致的像素值。例如，在一个手写数字数据集（如MNIST）中，标准化或归一化像素值有助于模型更有效地学习数字的模式。在目标检测任务中，它有助于准确检测和分类图像中的对象。然而，标准化和归一化不仅限于图像预处理；它们是为任何机器学习问题准备数据的基本步骤。
- en: 'Let’s expand the previous example by adding the normalization and the standardization
    step. The first function performs the normalization to ensure that the pixel values
    are in a common scale, in this case, between the range `[0, 1]` and we do that
    by dividing by 255:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过添加标准化和归一化步骤来扩展前面的示例。第一个函数执行标准化，以确保像素值在一个共同的尺度内，即在范围`[0, 1]`之间，我们通过除以255来实现这一点：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The normalized image can be seen in the following figure:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在以下图中看到标准化后的图像：
- en: '![Figure 13.3 – Picture after normalization](img/B19801_13_3.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.3 – 标准化后的图片](img/B19801_13_3.jpg)'
- en: Figure 13.3 – Picture after normalization
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 – 标准化后的图片
- en: As we can see from *Figure 13**.3*, visually, the image remains the same, at
    least to the human eye. Normalization does not alter the relative intensity of
    the pixels; it only scales them to a different range so the content and details
    should remain unchanged.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从*图 13**.3*中看到的那样，从视觉上看，图像保持不变，至少对于人眼来说是这样的。标准化不会改变像素的相对强度；它只是将它们缩放到不同的范围，因此内容和细节应该保持不变。
- en: 'Let’s move on to the standardization exercise. Before standardization, pixel
    values are in the range `[0, 255]` and follow the natural distribution of image
    intensities. The idea with standardization is that all the pixel values will be
    transformed to have a mean of `0` and a standard deviation of `1`. Let’s see how
    we can do that in the following code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行标准化练习。在标准化之前，像素值在范围`[0, 255]`内，并遵循图像强度的自然分布。标准化的想法是将所有像素值转换为具有均值`0`和标准差`1`。让我们看看如何在以下代码中实现：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this case, the appearance of the image might change since standardization
    shifts the mean to `0` and scales the values. This can make the image look different,
    possibly more contrasted, or with changed brightness. However, the image content
    should still be recognizable.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于标准化将均值转移到`0`并缩放值，图像的外观可能会发生变化。这可能使图像看起来不同，可能更加对比或亮度改变。但是，图像内容仍应可识别。
- en: '![Figure 13.4 – Picture after standardization](img/B19801_13_4.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图13.4 – 标准化后的图像](img/B19801_13_4.jpg)'
- en: Figure 13.4 – Picture after standardization
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4 – 标准化后的图像
- en: 'Apart from the transformed image shown in *Figure 13**.4*, the mean and standard
    deviation for the values are printed here:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在*图13**.4*中显示的变换后的图像外，这里还打印了数值的均值和标准差：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This confirms that the standardization has correctly scaled the pixel values.
    Let’s now move on to the data augmentation part.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这证实了标准化已正确缩放像素值。现在让我们继续进行数据增强部分。
- en: Data augmentation
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: '**Data augmentation** aims to create more variability in the dataset by applying
    random transformations, such as rotation, flipping, translation, color jittering,
    and contrast adjustment. This artificially expands the dataset with modified versions
    of existing images, which helps with model generalization and performance, especially
    when working with limited data.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据增强**旨在通过应用随机变换（如旋转、翻转、平移、颜色抖动和对比度调整）在数据集中创建更多的变化。这通过修改现有图像的版本人为扩展数据集，有助于模型的泛化和性能，特别是在使用有限数据时。'
- en: Common augmentation techniques include geometric transformations, such as rotation,
    flipping, and scaling, which change the spatial orientation and size of the images.
    For example, rotating an image by 15 degrees or flipping it horizontally can create
    new perspectives for the model to learn from. Color space alterations, such as
    adjusting brightness, contrast, or hue, can simulate different lighting conditions
    and improve the model’s ability to recognize objects in varying environments.
    Adding noise or blur can help the model become more resilient to imperfections
    and distortions in real-world data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的增强技术包括几何变换，如旋转、翻转和缩放，这些变换改变了图像的空间方向和大小。例如，将图像旋转15度或水平翻转可以为模型创造新的视角。调整亮度、对比度或色调等颜色空间变化可以模拟不同的光照条件，提高模型在不同环境中识别对象的能力。添加噪声或模糊可以帮助模型更好地适应真实数据中的缺陷和失真。
- en: 'Let’s go back to our example to see how we can create image variations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的例子，看看如何创建图像变化：
- en: 'First, we will define the transformations that we will apply to the image:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将定义要应用于图像的变换：
- en: '**Rotation range**: Randomly rotate the image within a range of 40 degrees.'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**旋转范围**：在40度范围内随机旋转图像。'
- en: '**Width shift range**: Randomly shift the image horizontally by 20% of the
    width.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**宽度偏移范围**：随机水平偏移图像的宽度的20%。'
- en: '**Height shift range**: Randomly shift the image vertically by 20% of the height.'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高度偏移范围**：随机垂直偏移图像的高度的20%。'
- en: '**Shear range**: Randomly apply shearing transformations.'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪切范围**：随机应用剪切变换。'
- en: '**Zoom range**: Randomly zoom in or out by 20%.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放范围**：随机缩放图像20%。'
- en: '**Horizontal flip**: Randomly flip the image horizontally.'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**水平翻转**：随机水平翻转图像。'
- en: '**Fill mode**: Define how to fill in newly created pixels after a transformation.
    (Here, using “nearest” pixel values.)'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填充模式**：定义在变换后如何填充新创建的像素。（这里使用“最近”像素值。）'
- en: 'Let’s create a function to apply these transformations:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个函数来应用这些变换：
- en: '[PRE11]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we will apply the transformations we just defined to the image:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将应用刚刚定义的变换到图像上：
- en: '[PRE12]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will display the following image:'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示以下图像：
- en: '![Figure 13.5 – Picture augmentation](img/B19801_13_5.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图13.5 – 图像增强](img/B19801_13_5.jpg)'
- en: Figure 13.5 – Picture augmentation
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5 – 图像增强
- en: As we can see from *Figure 13**.5*, the image has some significant changes;
    however, the image still remains recognizable and the concept in the picture remains
    the same.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图13.5*中可以看到，图像有一些显著的变化；然而，图像依然保持可识别性，图片中的概念没有变化。
- en: Note
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As we are using some random parameters in the data augmentation phase, you may
    produce a slightly different image at this stage.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在数据增强阶段使用了一些随机参数，因此此时生成的图像可能会有所不同。
- en: Data augmentation’s importance lies in its ability to increase dataset diversity,
    which by extension helps prevent overfitting, as the model learns to recognize
    patterns and features from a wider range of examples rather than memorizing the
    training data. Let’s move on to the next part and dive deep into the noise reduction
    options.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强的重要性在于它能增加数据集的多样性，进而帮助防止过拟合，因为模型通过识别更广泛的示例中的模式和特征，而不是仅仅记住训练数据。让我们进入下一部分，深入探讨噪声减少选项。
- en: Noise reduction
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 噪声减少
- en: '**Noise** in images refers to the random variations in pixel values that can
    distort the visual quality of an image and by extension affect the performance
    of models during training. These variations often appear as tiny, irregular spots
    or textures, such as random dots, patches, or a gritty texture, disrupting the
    smoothness and clarity of the image. They often make the image look less sharp
    and can obscure important details, which can be problematic for both visual interpretation
    and for models that rely on clear, accurate data for training.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中的**噪声**指的是像素值中的随机变化，这些变化可能会扭曲图像的视觉质量，从而影响模型在训练过程中的表现。这些变化通常表现为微小、不规则的斑点或纹理，如随机的点、块或颗粒状纹理，破坏图像的平滑性和清晰度。它们往往使图像看起来不那么锐利，可能会遮挡重要细节，这对视觉解读和依赖清晰、准确数据进行训练的模型都是一个问题。
- en: '**Noise reduction** attempts to reduce the random variations and make the data
    simpler. The minimization of these random variations in pixel values helps improve
    image quality and model accuracy as they can mislead models during training. In
    the following subsections, we expand on some common denoising techniques used
    in the data field, including Gaussian smoothing, non-local means denoising, and
    wavelet denoising.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**噪声减少**试图减少随机变化，使数据变得更加简洁。像素值的这些随机变化的最小化有助于提高图像质量和模型准确性，因为这些噪声在训练过程中可能会误导模型。在以下小节中，我们将扩展讨论数据领域中一些常见的去噪技术，包括高斯平滑、非局部均值去噪和小波去噪。'
- en: Gaussian smoothing
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高斯平滑
- en: '**Gaussian blur** (or **Gaussian smoothing**) applies a Gaussian filter to
    the image, which works by taking the pixel values within a specified neighborhood
    around each pixel and averaging them. The filter assigns higher weights to the
    pixels closer to the center of the neighborhood and lower weights to those farther
    away, following the Gaussian distribution. The denoised image will appear smoother
    but with slightly blurred edges, making it useful in applications where slight
    blurring is acceptable or desired, such as artistic effects or before edge detection
    algorithms to reduce noise. Let’s see the code for applying Gaussian smoothing
    to the image:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**高斯模糊**（或**高斯平滑**）对图像应用高斯滤波器，通过在每个像素周围的指定邻域内取像素值并计算平均值来实现。该滤波器对靠近邻域中心的像素赋予更高的权重，而对远离中心的像素赋予较低的权重，遵循高斯分布。去噪后的图像会显得更平滑，但边缘略微模糊，因此在一些允许或希望轻微模糊的应用中非常有用，比如艺术效果，或在边缘检测算法之前减少噪声。我们来看一下应用高斯平滑的代码：'
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s display the denoised image:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示去噪后的图像：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The denoised image can be seen in *Figure 13**.6*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪后的图像可以在*图13.6*中看到：
- en: '![Figure 13.6 – Denoised images – Gaussian blur on the median blur on the right](img/B19801_13_6.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图13.6 – 去噪图像 – 右侧是高斯模糊和中值模糊的结合](img/B19801_13_6.jpg)'
- en: Figure 13.6 – Denoised images – Gaussian blur on the median blur on the right
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6 – 去噪图像 – 右侧是高斯模糊和中值模糊的结合
- en: In the next section, we will discuss the bilateral filter.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将讨论双边滤波器。
- en: The bilateral filter
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双边滤波器
- en: 'The **bilateral filter** smoothens images by considering both spatial and intensity
    differences. It averages the pixel values based on their spatial closeness and
    color similarity. Let’s have a look at the code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**双边滤波器**通过同时考虑空间和强度差异来平滑图像。它根据像素之间的空间接近度和颜色相似度来计算平均值。我们来看一下代码：'
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `bilateralFilter` function takes some arguments that we need to explain:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`bilateralFilter`函数接受一些参数，我们需要对其进行解释：'
- en: '`9`: This is the diameter of each pixel neighborhood used during filtering.
    A larger value means that more pixels will be considered in the computation, resulting
    in a stronger smoothing effect.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`9`：这是在滤波过程中使用的每个像素邻域的直径。较大的值意味着在计算过程中将考虑更多的像素，导致更强的平滑效果。'
- en: '`75`: This is the filter sigma in the color space. A larger value means that
    farther colors within the pixel neighborhood will be mixed, resulting in larger
    areas of semi-equal color.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`75`：这是颜色空间中的滤波器sigma值。较大的值意味着像素邻域内的更远颜色将被混合，导致更大的半相同颜色区域。'
- en: '`75`: This is the filter sigma in the coordinate space. A larger value means
    farther pixels will influence each other if their colors are close enough. It
    controls the amount of smoothing.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`75`：这是坐标空间中的滤波器sigma值。较大的值意味着，如果颜色足够接近，更远的像素将相互影响。它控制平滑的程度。'
- en: 'Let’s use the function and see the resulting output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个函数，看看结果输出：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The denoised image can be seen in *Figure 13**.7*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪后的图像可以在*图 13.7*中看到。
- en: '![Figure 13.7 – Denoised images – left bilateral filter, right non-local mean
    denoising](img/B19801_13_7_Merged.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.7 – 去噪图像 – 左侧是双边滤波，右侧是非局部均值去噪](img/B19801_13_7_Merged.jpg)'
- en: Figure 13.7 – Denoised images – left bilateral filter, right non-local mean
    denoising
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7 – 去噪图像 – 左侧是双边滤波，右侧是非局部均值去噪
- en: In the next section, we will discuss the non-local means denoising.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论非局部均值去噪。
- en: Non-local means denoising
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非局部均值去噪
- en: '**Non-local means denoising** reduces noise by comparing patches of the image
    and averaging similar patches, even if they are far apart. This method works by
    comparing small patches of pixels across the entire image, rather than just neighboring
    pixels. Unlike simpler methods that only consider nearby pixels, non-local means
    denoising searches the image for patches that are similar, even if they are located
    far apart. When a match is found, the method averages these similar patches together
    to determine the final pixel value.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**非局部均值去噪**通过比较图像块并平均相似的图像块来减少噪声，即使它们相隔较远。此方法通过比较图像中整个图像的小块像素，而不仅仅是邻近像素。与只考虑附近像素的简单方法不同，非局部均值去噪在图像中查找相似的图像块，即使它们相隔较远。当找到匹配时，该方法将这些相似的图像块平均在一起，以确定最终的像素值。'
- en: This approach is particularly effective at preserving fine details and textures
    because it can recognize and retain patterns that are consistent throughout the
    image, rather than just smoothing over everything indiscriminately. By averaging
    only the patches that are truly similar, it reduces noise while maintaining the
    integrity of important image features, making it an excellent choice for applications
    where detail preservation is crucial.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法特别有效于保留细节和纹理，因为它可以识别并保留图像中始终如一的模式，而不是盲目地对所有内容进行平滑处理。通过仅对真正相似的图像块进行平均，它减少了噪声，同时保持了重要图像特征的完整性，这使得它在需要保留细节的应用中成为一个极好的选择。
- en: 'Let’s have a look at the code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下代码：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `fastNlMeansDenoisingColored` function applies the non-local means denoising
    algorithm to the image. The `h=10` argument reflects the filtering strength. A
    higher value removes more noise but may also remove some image details. The size
    in pixels of the template patch used to compute weights is reflected in the `templateWindowSize`
    variable. This value should be an odd number. A greater value means more smoothing.
    Finally, `searchWindowSize``=21` means the size of the window in pixels used to
    compute a weighted average for a given pixel should be odd. A greater value means
    more smoothing.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`fastNlMeansDenoisingColored`函数将非局部均值去噪算法应用于图像。`h=10`参数表示滤波强度。较高的值可以去除更多噪声，但也可能去除一些图像细节。用于计算权重的模板块的像素大小由`templateWindowSize`变量反映。该值应该是奇数。较大的值意味着更强的平滑效果。最后，`searchWindowSize`=`21`表示用于计算给定像素加权平均的窗口大小应该是奇数。较大的值意味着更强的平滑效果。'
- en: Why use an odd number for window sizes, such as `templateWindowSize` and`searchWindowSize`?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要使用奇数作为窗口大小，例如`templateWindowSize`和`searchWindowSize`？
- en: The primary reason for using an odd number is to ensure that there is a clear
    center pixel within the window. For example, in a 3x3 window (where 3 is an odd
    number), the center pixel is the one at position “(2,2)”. This center pixel is
    crucial because the algorithm often calculates how similar the surrounding pixels
    are in comparison to this central pixel. If an even-sized window were used, there
    would be no single, central pixel, as shown in *Figure 13**.8*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用奇数的主要原因是确保窗口内有一个明确的中心像素。例如，在一个3x3的窗口中（3是奇数），中心像素是位于位置“(2,2)”的像素。这个中心像素至关重要，因为算法通常会计算周围像素与这个中心像素的相似度。如果使用偶数大小的窗口，就不会有单一的中央像素，如*图
    13.8*所示。
- en: '![Figure 13.8 – Use an odd number for window sizes](img/B19801_13_8.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.8 – 使用奇数作为窗口大小](img/B19801_13_8.jpg)'
- en: Figure 13.8 – Use an odd number for window sizes
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8 – 使用奇数作为窗口大小
- en: Using an odd number simplifies the computation of weights and distances between
    the central pixel and its neighbors. This simplification is essential in algorithms
    such as non-local means, where the distances between pixels influence the weight
    given to each pixel in the averaging process. An odd-sized window naturally allows
    for straightforward indexing and less complex calculations.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用奇数简化了中央像素与邻近像素之间的权重和距离计算。这种简化在像非局部均值等算法中非常重要，因为像素之间的距离会影响在平均过程中每个像素所赋予的权重。奇数大小的窗口自然允许简单的索引和较少的计算复杂度。
- en: Regarding the `searchWindowSize` parameter, this defines the area within which
    the algorithm looks for similar patches to the one currently being processed.
    Having an odd-sized window for this search area ensures that there is a central
    pixel around which the search is centered. This helps in accurately identifying
    similar patches and applying the denoising effect uniformly across the image.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`searchWindowSize`参数，它定义了算法在当前处理的图像块周围寻找相似图块的区域。为此搜索区域使用奇数大小的窗口，确保了有一个中央像素，搜索会围绕这个像素进行。这有助于准确地识别相似图块，并在整个图像上均匀地应用去噪效果。
- en: The denoised image can be seen in the previous section in *Figure 13**.7*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪后的图像可以在上一节中的*图 13.7*看到。
- en: In the next section, we will discuss the last method, the median blur.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论最后一种方法——中值模糊。
- en: Median blur
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中值模糊
- en: '**Median blur** replaces each pixel’s value with the median value of the neighboring
    pixels. This method is particularly effective for removing “salt-and-pepper” noise,
    where pixels are randomly set to black or white, as we will see later. Let’s first
    denoise the image with the median blur method and then we will see how this method
    solves the salt-and-pepper effect.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**中值模糊**用邻近像素的中值替换每个像素的值。这种方法对于去除“椒盐噪声”非常有效，后者是指像素随机变为黑色或白色，正如我们稍后将看到的那样。我们首先用中值模糊方法对图像进行去噪，然后会看到这种方法如何解决椒盐效应。'
- en: 'The following function performs the `medianBlur` function, which requires the
    input image to be in an 8-bit unsigned integer format (`uint8`), where pixel values
    range from `0` to `255`. By multiplying the image by 255, the pixel values are
    scaled to the range `[``0, 255]`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数执行`medianBlur`功能，它要求输入图像是8位无符号整数格式（`uint8`），其中像素值的范围是`0`到`255`。通过将图像乘以255，像素值被缩放到`[0,
    255]`的范围内：
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s display the denoised image using the following code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码展示去噪后的图像：
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The denoised image can be seen in *Figure 13**.9*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪后的图像可以在*图 13.9*中看到：
- en: '![Figure 13.9 – Denoised images – median blur](img/B19801_13_9.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.9 – 去噪图像 – 中值模糊](img/B19801_13_9.jpg)'
- en: Figure 13.9 – Denoised images – median blur
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9 – 去噪图像 – 中值模糊
- en: As promised, let’s now discuss the salt-and-pepper noise effect.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，现在让我们讨论“椒盐噪声”效应。
- en: Salt-and-pepper noise
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 椒盐噪声
- en: '**Salt-and-pepper noise** is a type of impulse noise characterized by the presence
    of randomly distributed black-and-white pixels in an image. This noise can be
    caused by various factors, such as errors in data transmission, malfunctioning
    camera sensors, or environmental conditions during image acquisition. The black
    pixels are referred to as “pepper noise,” while the white pixels are known as
    “salt noise.” This noise type is particularly detrimental to image quality as
    it can obscure important details and make edge detection and image restoration
    challenging.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**盐与胡椒噪声**是一种脉冲噪声，特点是在图像中存在随机分布的黑白像素。此类噪声可能由多种因素引起，如数据传输错误、相机传感器故障或图像获取过程中的环境条件。黑色像素被称为“胡椒噪声”，而白色像素被称为“盐噪声”。这种噪声类型对图像质量影响尤其严重，因为它可能遮挡重要细节，并使得边缘检测和图像修复变得困难。'
- en: 'To showcase this, we have created a function that adds this noise effect to
    the original image so that we can then denoise it:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这一点，我们创建了一个函数，将这种噪声效果添加到原始图像中，以便我们可以进行去噪处理：
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This function takes three arguments:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接受三个参数：
- en: '`image`: The input image to which noise will be added'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`：将添加噪声的输入图像'
- en: '`salt_prob`: The probability of a pixel being turned into salt noise (white)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`salt_prob`：将像素变为盐噪声（白色）的概率'
- en: '`pepper_prob`: The probability of a pixel being turned into pepper noise (black)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pepper_prob`：将像素变为胡椒噪声（黑色）的概率'
- en: This function adds salt-and-pepper noise to an image. It starts by creating
    a copy of the input image to avoid altering the original. To introduce salt noise
    (white pixels), it calculates the number of pixels to be affected based on the
    `salt_prob` parameter, generates random coordinates for these pixels, and sets
    them to white. Similarly, for pepper noise (black pixels), it calculates the number
    of affected pixels using the `pepper_prob` parameter, generates random coordinates,
    and sets these pixels to black. The noisy image is then returned.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数向图像添加盐与胡椒噪声。它首先创建输入图像的副本，以避免修改原始图像。为了引入盐噪声（白色像素），它根据`salt_prob`参数计算受影响的像素数量，为这些像素生成随机坐标，并将它们设置为白色。同样，对于胡椒噪声（黑色像素），它使用`pepper_prob`参数计算受影响的像素数量，生成随机坐标，并将这些像素设置为黑色。然后返回带有噪声的图像。
- en: 'To apply this effect on the data you need to set the following flag to `True`.
    The flag can be found in the code after the `add_salt_and_pepper_noise` function
    definition:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 要在数据上应用这一效果，你需要将以下标志设置为`True`。这个标志可以在`add_salt_and_pepper_noise`函数定义后找到：
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The image with the noise can be seen in *Figure 13**.10*:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 带有噪声的图像可以在*图 13.10*中看到：
- en: '![Figure 13.10 – Salt-and-pepper noise](img/B19801_13_10.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.10 – 盐与胡椒噪声](img/B19801_13_10.jpg)'
- en: Figure 13.10 – Salt-and-pepper noise
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10 – 盐与胡椒噪声
- en: Now, let’s apply the different denoising techniques we’ve learned so far to
    the preceding image. The different denoising effects can be seen in *Figure 13**.11*
    and *Figure 13**.12*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将到目前为止学习的不同去噪技术应用到前面的图像中。不同的去噪效果可以在*图 13.11*和*图 13.12*中看到。
- en: '![Figure 13.11 – Left: Gaussian blur, right: median blur](img/B19801_13_11_Merged.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.11 – 左：高斯模糊，右：中值模糊](img/B19801_13_11_Merged.jpg)'
- en: 'Figure 13.11 – Left: Gaussian blur, right: median blur'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11 – 左：高斯模糊，右：中值模糊
- en: '![Figure 13.12 – Left: bilateral filter, right: non-local means denoising](img/B19801_13_12_Merged.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.12 – 左：双边滤波器，右：非局部均值去噪](img/B19801_13_12_Merged.jpg)'
- en: 'Figure 13.12 – Left: bilateral filter, right: non-local means denoising'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12 – 左：双边滤波器，右：非局部均值去噪
- en: As we can see, the median blur method really excels at removing this kind of
    noise, whereas all the other methods really struggle to remove it. In the next
    part of the chapter, we will discuss some image use cases that are becoming more
    popular in the data world, such as creating image captions and extracting text
    from images.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，中值模糊方法在去除这种噪声方面表现非常优秀，而其他方法则很难去除它。在本章的下一部分，我们将讨论一些在数据领域中变得越来越流行的图像应用场景，比如生成图像标题和从图像中提取文本。
- en: Extracting text from images
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从图像中提取文本
- en: When discussing ways to extract text from images, the OCR technology is the
    one that comes to mind. The OCR technology allows us to handle textual information
    embedded in images, allowing for the digitization of printed documents, automating
    data entry, and enhancing accessibility.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论如何从图像中提取文本时，OCR技术是最常提到的。OCR技术使我们能够处理嵌入在图像中的文本信息，从而实现印刷文档的数字化、数据录入自动化，并提高可访问性。
- en: One of the primary benefits of OCR technology today is its ability to significantly
    reduce the need for manual data entry. For example, businesses can convert paper
    documents into digital formats using OCR, which not only saves physical storage
    space but also enhances document management processes. This conversion makes it
    easier to search, retrieve, and share documents, streamlining operations and improving
    productivity.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当前OCR技术的主要优势之一是显著减少了人工数据输入的需求。例如，企业可以使用OCR将纸质文件转换为数字格式，这不仅节省了物理存储空间，还提升了文件管理流程。此转换使得文件的搜索、检索和共享变得更容易，从而简化了操作并提高了生产力。
- en: In transportation, particularly with self-driving cars, OCR technology is used
    to read road signs and number plates. This capability is vital for navigation
    and ensuring compliance with traffic regulations. By accurately interpreting signage
    and vehicle identification, OCR contributes to the safe and efficient functioning
    of autonomous vehicles.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在交通领域，尤其是自动驾驶汽车中，OCR技术用于读取道路标志和车牌。这一功能对于导航和确保遵守交通法规至关重要。通过准确解读标识和车辆识别，OCR有助于自动驾驶汽车的安全和高效运作。
- en: Moreover, OCR technology is employed in social media monitoring to detect brand
    logos and text in images. This application is particularly beneficial for marketing
    and brand management, as it enables companies to track brand visibility and engagement
    across social platforms. For instance, brands can use OCR to identify unauthorized
    use of their logos or monitor the spread of promotional materials, thereby enhancing
    their marketing strategies and protecting their brand identity.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，OCR技术还被应用于社交媒体监控，以检测图像中的品牌标志和文本。这一应用对营销和品牌管理特别有益，因为它使企业能够追踪品牌的可见性和社交平台上的互动。例如，品牌可以利用OCR识别未经授权使用其标志的情况，或监控促销材料的传播，从而增强其营销策略并保护品牌身份。
- en: Let’s see how we can apply OCR in the data world with an open source solution.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在数据领域中应用OCR，使用开源解决方案。
- en: PaddleOCR
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PaddleOCR
- en: '**PaddleOCR** is an open source OCR tool developed by PaddlePaddle, which is
    Baidu’s deep learning platform. The repository provides end-to-end OCR capabilities,
    including text detection, text recognition, and multilingual support ([https://github.com/PaddlePaddle/PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**PaddleOCR**是由PaddlePaddle开发的开源OCR工具，PaddlePaddle是百度的深度学习平台。该仓库提供端到端的OCR功能，包括文本检测、文本识别和多语言支持（[https://github.com/PaddlePaddle/PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)）。'
- en: 'The PaddleOCR process has a lot of steps in place that can be seen in the following
    *Figure 13**.13*:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: PaddleOCR过程有许多步骤，详见下面的*图13.13*：
- en: '![Figure 13.13 – OCR process step by step](img/B19801_13_13.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.13 – OCR过程逐步解析](img/B19801_13_13.jpg)'
- en: Figure 13.13 – OCR process step by step
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13 – OCR过程逐步解析
- en: 'Let’s break down the process step by step:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地分解这个过程：
- en: The process begins with an input image that may contain text.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程从包含文本的输入图像开始。
- en: '**Image preprocessing**: The image may undergo various preprocessing steps,
    such as resizing, converting to grayscale, and noise reduction, to enhance text
    visibility.'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图像预处理**：图像可能会经过各种预处理步骤，如调整大小、转为灰度图、降噪，以增强文本的可见性。'
- en: '**Text detection**: The model detects regions of the image that contain text.
    This may involve algorithms such as **Efficient and Accurate Scene Text** (**EAST**)
    or **Differentiable Binarization** (**DB**) to find bounding boxes around the
    text.'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本检测**：该模型检测图像中包含文本的区域。这可能涉及诸如**高效准确场景文本**（**EAST**）或**可微分二值化**（**DB**）等算法，用于找到围绕文本的边界框。'
- en: '**Text recognition**: The detected text regions are fed into a recognition
    model (often a **convolutional neural network** (**CNN**) followed by a **long
    short-term model** (**LSTM**) or a transformer) to convert the visual text into
    digital text.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本识别**：检测到的文本区域会输入到一个识别模型（通常是**卷积神经网络**（**CNN**），接着是**长短期记忆模型**（**LSTM**）或变换器），将视觉文本转换为数字文本。'
- en: '**Post-processing**: The recognized text may be further refined through spell-checking,
    grammar correction, or contextual analysis to improve accuracy.'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**后处理**：识别出的文本可能会通过拼写检查、语法修正或上下文分析进一步优化，以提高准确性。'
- en: '**Extracted text**: The final output consists of extracted digital text ready
    for further use.'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提取文本**：最终输出由提取的数字文本组成，准备进一步使用。'
- en: '**Annotated image**: Optionally, an annotated version of the original image
    can be generated, showing the detected text regions along with the recognized
    text.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注释图像**：可选地，可以生成原始图像的注释版本，显示检测到的文本区域及其识别出的文本。'
- en: It may seem complicated initially but luckily, most of these steps are abstracted
    away from the user and are handled by the PaddleOCR package automatically. Let’s
    introduce a use case for OCR to extract text from YouTube video thumbnails.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 起初看起来可能有些复杂，但幸运的是，这些步骤中的大部分都已被抽象化，用户无需操作，由 PaddleOCR 包自动处理。让我们引入一个 OCR 用例，从
    YouTube 视频缩略图中提取文本。
- en: YouTube thumbnails
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube 缩略图
- en: '**YouTube thumbnails** are small, clickable images that represent a video on
    the platform. They serve as the visual preview that users see before clicking
    to watch a video. Thumbnails are crucial for attracting viewers, as they often
    play a significant role in influencing whether someone decides to watch the content.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**YouTube 缩略图**是平台上表示视频的小型可点击图像。它们作为用户在点击观看视频前看到的视觉预览。缩略图对于吸引观众至关重要，因为它们通常在影响观众是否决定观看内容方面发挥重要作用。'
- en: 'By analyzing the text present in thumbnails, such as video titles and promotional
    phrases, stakeholders can gain insights into viewer engagement and content trends.
    For instance, a marketing team can collect thumbnails from a range of videos and
    employ OCR to extract keywords and phrases that frequently appear in high-performing
    content. This analysis can reveal which terms resonate most with audiences, enabling
    creators to optimize their future thumbnails and align their messaging with popular
    themes. Additionally, the extracted text can inform **search engine optimization**
    (**SEO**) strategies by identifying trending keywords to incorporate into video
    titles, descriptions, and tags, ultimately enhancing video discoverability. In
    our case, we have provided a folder on the GitHub repository with YouTube thumbnails
    from the channel I am cohosting called **Vector Lab**, discussing Gen AI and ML
    concepts. Here’s a link to the images folder on GitHub: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter13/images](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter13/images).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析缩略图中的文本，如视频标题和推广语，利益相关者可以洞察观众的参与度和内容趋势。例如，营销团队可以收集一系列视频的缩略图，并使用 OCR 提取在高效内容中频繁出现的关键词和短语。这项分析可以揭示哪些词汇最能引起观众共鸣，帮助创作者优化未来的缩略图，并使信息传达与流行主题对接。此外，提取的文本还可以为**搜索引擎优化**（**SEO**）策略提供支持，识别趋势关键词并将其整合到视频标题、描述和标签中，从而提升视频的可发现性。在我们的案例中，我们在
    GitHub 仓库中提供了一个文件夹，里面有来自我共同主持的频道**Vector Lab**的 YouTube 缩略图，讨论的是 Gen AI 和 ML 概念。以下是
    GitHub 上图像文件夹的链接：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter13/images](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter13/images)。
- en: The images in the folder look like the following figure and the idea is to pass
    all these images and extract the text depicted on the image.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹中的图像如下图所示，目标是传递所有这些图像并提取图像中显示的文本。
- en: '![Figure 13.14 – Example YouTube thumbnail](img/B19801_13_14.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.14 – 示例 YouTube 缩略图](img/B19801_13_14.jpg)'
- en: Figure 13.14 – Example YouTube thumbnail
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.14 – 示例 YouTube 缩略图
- en: 'Let’s see how we can achieve that:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何实现这个目标：
- en: 'We’ll start by initializing PaddleOCR:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过初始化 PaddleOCR 开始：
- en: '[PRE22]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `use_angle_cls=True` flag enables the use of an angle classifier in the
    OCR process. The angle classifier helps improve the accuracy of text recognition
    by determining the orientation of the text in the image. This is particularly
    useful for images where text might not be horizontally aligned (e.g., rotated
    or skewed text).
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`use_angle_cls=True` 标志启用 OCR 过程中的角度分类器。角度分类器通过确定图像中文本的方向来提高文本识别的准确性。这对于文本可能未水平对齐（例如，旋转或倾斜文本）的图像尤其有用。'
- en: The `lang='en'` parameter specifies the language for OCR. In this case, `'en'`
    indicates that the text to be recognized is in English. PaddleOCR supports multiple
    languages and sets the appropriate language in case you want to perform OCR in
    a language other than English.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`lang=''en''` 参数指定了 OCR 的语言。在此情况下，`''en''` 表示要识别的文本是英文。PaddleOCR 支持多种语言，并在你希望进行非英语语言的
    OCR 时设置适当的语言。'
- en: 'Next, we define the path to the folder containing images to extract the text
    from:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义提取文本的图像文件夹路径：
- en: '[PRE23]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, we specify the supported image extensions. In our case, we only have
    `.png`, but you can add any image type in the folder:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们指定支持的图片扩展名。在我们的案例中，我们只有 `.png`，但你可以在文件夹中添加任何图片类型：
- en: '[PRE24]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we get all paths to the images in the folder that we will use to load
    the images:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们获取文件夹中所有图片的路径，用于加载图片：
- en: '[PRE25]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we create an empty DataFrame to store the results:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个空的 DataFrame 来存储结果：
- en: '[PRE26]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We use the following code to check if there are no image paths returned, which
    would mean that either there are no images in the folder or the images that exist
    in the folder don’t have any of the supported extensions:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用以下代码来检查是否没有返回图片路径，这意味着文件夹中要么没有图片，要么文件夹中的图片没有支持的扩展名：
- en: '[PRE27]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'def process_image(image_path):'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def process_image(image_path):'
- en: result = ocr.ocr(image_path, cls=True)
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: result = ocr.ocr(image_path, cls=True)
- en: extracted_text = ""
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: extracted_text = ""
- en: 'for line in result[0]:'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'for line in result[0]:'
- en: extracted_text += line[1][0] + " "
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: extracted_text += line[1][0] + " "
- en: print(f"Extracted Text from {os.path.basename(image_path)}:\n{extracted_text}\n")
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print(f"从 {os.path.basename(image_path)} 提取的文本：\n{extracted_text}\n")
- en: df.loc[len(df)] = [image_path, extracted_text]
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: df.loc[len(df)] = [image_path, extracted_text]
- en: '[PRE28]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Optionally, we can save the DataFrame to a CSV file using the following code:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地，我们可以使用以下代码将 DataFrame 保存为 CSV 文件：
- en: '[PRE29]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The results can be seen in *Figure 13**.15*, where we have the path to the image
    on the left and the extracted text on the right.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以在 *Figure 13**.15* 中看到，左侧是图片路径，右侧是提取的文本。
- en: '![Figure 13.15 – OCR output](img/B19801_13_15.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.15 – OCR 输出](img/B19801_13_15.jpg)'
- en: Figure 13.15 – OCR output
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.15 – OCR 输出
- en: The results are great; however, we can see that there are some misspellings
    in certain cases, probably due to the font of the text in the images. The key
    thing to understand here is that we don’t have to deal with the images anymore,
    but only with the text, thereby significantly simplifying our challenge. Based
    on what we learned in [*Chapter 12*](B19801_12.xhtml#_idTextAnchor277), *Text
    Preprocessing in the Era of LLMs* we can now manipulate and clean the text in
    various ways, such as chunking it, embedding it, or passing it through **large
    language models** (**LLMs**), as we will see in the next part.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 结果很好；然而，我们可以看到在某些情况下存在拼写错误，可能是由于图片中文字的字体问题。这里的关键是要理解，我们不再需要处理图片，只需要处理文本，从而大大简化了我们的挑战。基于我们在
    [*第12章*](B19801_12.xhtml#_idTextAnchor277) 中学到的内容，*LLM 时代的文本预处理*，我们现在可以通过各种方式操作和清理文本，例如分块、嵌入或通过
    **大型语言模型**（**LLMs**）处理文本，正如我们在下一部分将看到的那样。
- en: Using LLMs with OCR
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LLM 与 OCR
- en: OCR technology, despite its advancements, often produces errors, especially
    with complex layouts, low-quality images, or unusual fonts. These errors include
    misrecognized characters and incorrect word breaks. So, the idea is to pass the
    OCR-extracted text through an LLM to correct these errors as LLMs understand context
    and can improve grammar and readability. Moreover, raw OCR output may be inconsistently
    formatted and hard to read; LLMs can reformat and restructure text, ensuring coherent
    and well-structured content. This automated proofreading reduces the need for
    manual intervention, saving time and minimizing human error. LLMs also standardize
    the text, making it consistent and easier to integrate into other systems, such
    as databases and analytical tools.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 OCR 技术取得了进展，但它经常会产生错误，特别是在复杂布局、低质量图片或不常见字体的情况下。这些错误包括识别错误的字符和错误的断词。因此，解决方案是将
    OCR 提取的文本通过 LLM 来修正这些错误，因为 LLM 能理解上下文，能够改善语法和可读性。此外，原始 OCR 输出可能格式不一致且难以阅读；LLM
    可以重新格式化和重构文本，确保内容连贯且结构良好。这种自动化的校对减少了人工干预的需求，节省了时间并最小化了人为错误。LLM 还能够标准化文本，使其一致，便于与其他系统（如数据库和分析工具）集成。
- en: In this section, we will expand the thumbnail example to pass the extracted
    text through an LLM to clean it. To run this example, you need to do the following
    setup first.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将扩展缩略图示例，将提取的文本通过 LLM 进行清理。要运行此示例，你需要首先完成以下设置。
- en: Hugging Face setup
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hugging Face 设置
- en: 'In order to run this example, you will need to have an account with Hugging
    Face and a token to authenticate. To do that, follow these steps:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行这个示例，你需要在 Hugging Face 上有一个账户，并且需要一个令牌来进行身份验证。按照以下步骤操作：
- en: Go to [https://huggingface.co](https://huggingface.co).
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [https://huggingface.co](https://huggingface.co)。
- en: Create an account if you don’t have one.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你没有账户，可以创建一个。
- en: Go to **Settings**.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入 **Settings**。
- en: 'Then, click on **Access Tokens**. You should see the following page:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，点击 **Access Tokens**。你应该会看到以下页面：
- en: '![Figure 13.16 – Creating a new access token in Hugging Face](img/B19801_13_16.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图13.16 – 在Hugging Face中创建新的访问令牌](img/B19801_13_16.jpg)'
- en: Figure 13.16 – Creating a new access token in Hugging Face
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.16 – 在Hugging Face中创建新的访问令牌
- en: Click on the **Create new token** button to generate a new personal token.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建新令牌**按钮生成一个新的个人令牌。
- en: Remember to copy and keep this token as we will need to paste it in the code
    file to authenticate!
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请记住复制并保留此令牌，因为我们将在代码文件中粘贴它以进行身份验证！
- en: Now, we are ready to dive into the code.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备深入代码部分。
- en: Cleaning text with LLMs
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用LLM清理文本
- en: 'Let’s have a look at the code that you can find in the GitHub repository: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/3.ocr_with_llms.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/3.ocr_with_llms.py):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下你可以在GitHub仓库中找到的代码：[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/3.ocr_with_llms.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/3.ocr_with_llms.py)：
- en: 'We start by reading in the OCR-extracted text that we wrote in the previous
    step:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先读取在上一步中提取的OCR文本：
- en: '[PRE30]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We then initialize the Hugging Face model. In this case, we are using `Mistral-Nemo-Instruct-2407`,
    but you can replace it with any LLM you have access to:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们初始化Hugging Face模型。在这种情况下，我们使用的是`Mistral-Nemo-Instruct-2407`，但你可以将其替换为任何你可以访问的LLM：
- en: '[PRE31]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Hugging Face** is a platform that provides a diverse repository of pretrained
    models, which can be accessed and integrated easily using user-friendly APIs.
    Hugging Face models come with detailed documentation and benefit from continuous
    innovation driven by a collaborative community. I see it as being similar to how
    GitHub serves as a repository for code; Hugging Face functions as a repository
    for ML models. Importantly, many models on Hugging Face are available for free,
    making it a cost-effective option for individuals and researchers.'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Hugging Face** 是一个提供多样化预训练模型库的平台，用户可以通过友好的API轻松访问和集成这些模型。Hugging Face的模型附带详细的文档，并且通过一个协作社区推动持续创新。我认为它类似于GitHub作为代码库的作用；而Hugging
    Face则作为机器学习模型的库。值得注意的是，Hugging Face上的许多模型都是免费的，这使它成为个人和研究人员的一个具成本效益的选择。'
- en: In contrast, there are many other paid models available, such as Azure OpenAI,
    which provides access to models such as GPT-3 and GPT-4\. These models can be
    accessed on a paid model, and you have to manage the authentication process, which
    is different from authenticating with Hugging Face.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相比之下，还有许多其他付费模型可供使用，例如Azure OpenAI，提供访问GPT-3和GPT-4等模型。这些模型可以通过付费方式访问，并且你需要管理身份验证过程，这与Hugging
    Face的身份验证有所不同。
- en: 'Add your Hugging Face API token, which was created in the previous setup section:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加在之前设置部分中创建的Hugging Face API令牌：
- en: '[PRE32]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: prompt_template = PromptTemplate(
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: prompt_template = PromptTemplate(
- en: input_variables=["text"],
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: input_variables=["text"],
- en: template='''
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: template='''
- en: 'Correct the following text for spelling errors and return only the corrected
    text in lowercase. Respond using JSON format, strictly according to the following
    schema:'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 修正以下文本中的拼写错误，并仅返回修正后的文本（小写形式）。响应应使用JSON格式，严格按照以下模式：
- en: '{{"corrected_text": "corrected text in lowercase"}}'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '{{"corrected_text": "修正后的文本（小写形式）"}}'
- en: 'Examples: Three examples are provided to guide the model:'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例：提供了三个示例以指导模型：
- en: 'Input: Shows the input text needing correction.'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入：显示需要修正的输入文本。
- en: 'Output: Provides the expected JSON format for the corrected text. This helps
    the model learn what is required and encourages it to follow the same format when
    generating its responses.'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出：提供修正文本的预期JSON格式。这帮助模型学习所需内容，并鼓励它在生成响应时遵循相同的格式。
- en: 'Examples:'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例：
- en: 'Input: "Open vs Proprietary LLMs"'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入："开放与专有LLM"
- en: 'Output: {{"corrected_text": "open vs proprietary llms"}}'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '输出：{{"corrected_text": "开放与专有LLM"}}'
- en: 'Input: "HOW TO MITIGATE SaCURITY RISKS IN AI AND ML SYSTEM VECTOR LAB"'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入："如何减轻AI和ML系统向量实验室中的安全风险"
- en: 'Output: {{"corrected_text": "how to mitigate security risks in ai and ml system
    vector lab"}}'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '输出：{{"corrected_text": "如何减轻AI和ML系统向量实验室中的安全风险"}}'
- en: 'Input: "BUILDING DBRX-CLASS CUSTOM LLMS WITH MOSAIC A1 TRAINING VECTOR LAB"'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入："构建DBRX类自定义LLM与Mosaic A1训练向量实验室"
- en: 'Output: {{"corrected_text": "building dbrx-class custom llms with mosaic a1
    training vector lab"}}'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '输出：{{"corrected_text": "构建DBRX类自定义LLM与Mosaic A1训练向量实验室"}}'
- en: 'Text to Correct: Placeholder {text} that will be replaced with the actual input
    text when calling the model.'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要修正的文本：占位符{text}，在调用模型时将被实际输入文本替换。
- en: 'Text to correct:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要修正的文本：
- en: '{text}'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '{text}'
- en: 'Final Instruction: Specifies that the output should be in JSON format only,
    which reinforces the expectation that the model should avoid unnecessary explanations
    or additional text.'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终指令：指定输出应仅为 JSON 格式，这进一步强调了模型应避免不必要的解释或附加文本。
- en: 'Output (JSON format only):'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出（仅 JSON 格式）：
- en: ''''''''
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ''''''''
- en: )
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: )
- en: 'PromptTemplate class for use with a language model to correct spelling errors
    in text. The PromptTemplate class is initialized with two key parameters: input_variables
    and template. The input_variables parameter specifies the input variable as ["text"],
    which represents the text that will be corrected. The template parameter contains
    the prompt structure sent to the model. This structure includes clear instructions
    for the model to correct spelling errors and return the output in lowercase, formatted
    as JSON. The JSON schema specifies the expected output format, ensuring consistency
    in responses. The template also provides three examples of input text and their
    corresponding corrected output in JSON format, guiding the model on how to process
    similar requests. The {text} placeholder in the template will be replaced with
    the actual input text when the model is invoked. The final instruction emphasizes
    that the output should be strictly in JSON format, avoiding any additional text
    or explanations.'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PromptTemplate 类用于与语言模型一起修正文本中的拼写错误。PromptTemplate 类通过两个关键参数进行初始化：input_variables
    和 template。input_variables 参数指定输入变量为["text"]，表示将被修正的文本。template 参数包含发送给模型的提示结构。该结构包括明确的指令，要求模型修正拼写错误并以小写格式返回输出，格式为
    JSON。JSON 模式指定了期望的输出格式，确保响应的一致性。模板还提供了三个输入文本及其相应修正后的输出示例，指导模型如何处理类似的请求。模板中的{text}占位符将在调用模型时被实际输入文本替换。最终指令强调输出应严格为
    JSON 格式，避免任何附加的文本或解释。
- en: '[PRE33]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We initialize the model from Hugging Face using the model name and API token
    that we specified earlier:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 Hugging Face 初始化模型，使用我们之前指定的模型名称和 API 密钥：
- en: '[PRE34]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We then combine the prompt template and the model, creating a chain that will
    take input text, apply the prompt, and create output:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接着将提示模板和模型结合，创建一个链条，该链条将接受输入文本、应用提示并生成输出：
- en: '[PRE35]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We use `llm_chain` to generate a response:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`llm_chain`来生成响应：
- en: '[PRE36]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, we apply text correction to the `Extracted` `Text` column:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们对`Extracted` `Text`列应用文本修正：
- en: '[PRE37]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let’s present some of the results:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示一些结果：
- en: '[PRE38]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: import os
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: import os
- en: import pandas as pd
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: from PIL import Image
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: from PIL import Image
- en: import matplotlib.pyplot as plt
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: from transformers import BlipProcessor, BlipForConditionalGeneration, AutoTokenizer,
    AutoModelForSeq2SeqLM
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: from transformers import BlipProcessor, BlipForConditionalGeneration, AutoTokenizer,
    AutoModelForSeq2SeqLM
- en: from langchain import PromptTemplate, LLMChain
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: from langchain import PromptTemplate, LLMChain
- en: from langchain.llms import HuggingFaceHub
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: from langchain.llms import HuggingFaceHub
- en: '[PRE39]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: folder_path = 'chapter13/images'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: folder_path = 'chapter13/images'
- en: '[PRE40]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: supported_extensions = ('.png', '.jpg', '.jpeg')
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: supported_extensions = ('.png', '.jpg', '.jpeg')
- en: '[PRE41]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: image_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path)
    if file.lower().endswith(supported_extensions)]
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: image_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path)
    if file.lower().endswith(supported_extensions)]
- en: '[PRE42]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: df = pd.DataFrame(columns=['Image Path', 'Generated Caption', 'Refined Caption'])
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: df = pd.DataFrame(columns=['图片路径', '生成的字幕', '优化后的字幕'])
- en: '[PRE43]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
- en: blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
- en: '[PRE44]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'llm_model_name = "google/flan-t5-small" # You can play with any other model
    from hugging phase as well'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'llm_model_name = "google/flan-t5-small" # 你也可以使用 Hugging Face 上的其他模型'
- en: tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
- en: model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)
- en: '[PRE45]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: api_token = "add_your_hugging_face_token"
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: api_token = "add_your_hugging_face_token"
- en: 'prompt_template = PromptTemplate(input_variables=["text"], template="Refine
    and correct the following caption: {text}")'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: prompt_template = PromptTemplate(input_variables=["text"], template="修正并优化以下字幕：{text}")
- en: huggingface_llm = HuggingFaceHub(repo_id=llm_model_name, huggingfacehub_api_token=api_token)
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: huggingface_llm = HuggingFaceHub(repo_id=llm_model_name, huggingfacehub_api_token=api_token)
- en: llm_chain = LLMChain(prompt=prompt_template, llm=huggingface_llm)
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: llm_chain = LLMChain(prompt=prompt_template, llm=huggingface_llm)
- en: '[PRE46]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'def refine_caption(caption):'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def refine_caption(caption):'
- en: prompt = prompt_template.format(text=caption)
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: prompt = prompt_template.format(text=caption)
- en: refined_caption = llm_chain.run(prompt)
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: refined_caption = llm_chain.run(prompt)
- en: return refined_caption
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return refined_caption
- en: '[PRE47]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'def generate_caption(image_path):'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def generate_caption(image_path):'
- en: image = Image.open(image_path).convert("RGB")
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: image = Image.open(image_path).convert("RGB")
- en: inputs = blip_processor(images=image, return_tensors="pt")
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: inputs = blip_processor(images=image, return_tensors="pt")
- en: outputs = blip_model.generate(inputs)
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: outputs = blip_model.generate(inputs)
- en: caption = blip_processor.decode(outputs[0], skip_special_tokens=True)
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: caption = blip_processor.decode(outputs[0], skip_special_tokens=True)
- en: return caption
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return caption
- en: '[PRE48]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'if not image_paths:'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果没有image_paths：
- en: print("No images found in the specified folder.")
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print("在指定文件夹中未找到图像。")
- en: 'else:'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否则：
- en: 'for image_path in image_paths:'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '对于image_path in image_paths:'
- en: caption = generate_caption(image_path)
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: caption = generate_caption(image_path)
- en: print(f"Generated Caption for {os.path.basename(image_path)}:\n{caption}\n")
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print(f"为{os.path.basename(image_path)}生成的字幕:\n{caption}\n")
- en: refined_caption = refine_caption(caption)
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: refined_caption = refine_caption(caption)
- en: print(f"Refined Caption:\n{refined_caption}\n")
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: print(f"精炼后的字幕:\n{refined_caption}\n")
- en: df.loc[len(df)] = [image_path, caption, refined_caption]
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: df.loc[len(df)] = [image_path, caption, refined_caption]
- en: '[PRE49]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: import torch
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入torch
- en: from transformers import WhisperProcessor, WhisperForConditionalGeneration
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从transformers导入WhisperProcessor, WhisperForConditionalGeneration
- en: import librosa
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入librosa
- en: '[PRE50]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: processor = WhisperProcessor.from_pretrained("openai/whisper-large-v2")
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: processor = WhisperProcessor.from_pretrained("openai/whisper-large-v2")
- en: model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v2")
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v2")
- en: '[PRE51]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: audio_path = "chapter13/audio/3.chain orchestrator.mp3"
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: audio_path = "chapter13/audio/3.chain orchestrator.mp3"
- en: '[PRE52]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: audio, rate = librosa.load(audio will be a NumPy array containing the audio
    samples.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: audio, rate = librosa.load(audio 将是一个包含音频样本的NumPy数组。
- en: '[PRE53]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: input_features = processor(audio, sampling_rate=rate, return_tensors="pt").input_features
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: input_features = processor(audio, sampling_rate=rate, return_tensors="pt").input_features
- en: '[PRE54]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'with torch.no_grad():'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'with torch.no_grad():'
- en: predicted_ids = model.generate(input_features)
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: predicted_ids = model.generate(input_features)
- en: '[PRE55]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
- en: '[PRE56]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '"As you can see, you need what we call a chain orchestrator to coordinate all
    the steps. So all the steps from raising the question all the way to the response.
    And the most popular open source packages are Lama Index and LangChain that we
    can recommend. Very nice. So these chains, these steps into the RAG application
    or any other LLM application, you can have many steps happening, right? So you
    need this chain to help them orchestrate"'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"正如你所看到的，你需要一个我们称之为链协调器的东西来协调所有步骤。所以从提出问题到得到回答的所有步骤。而最受欢迎的开源包是Lama Index和LangChain，我们可以推荐。非常不错。所以这些链，这些步骤进入RAG应用或任何其他LLM应用，你可以有许多步骤在进行，对吧？所以你需要这个链来帮助它们进行协调"'
- en: '[PRE57]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: import torch
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入torch
- en: import pandas as pd
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入pandas为pd
- en: from transformers import WhisperProcessor, WhisperForConditionalGeneration,
    AutoModelForSequenceClassification, AutoTokenizer
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从transformers导入WhisperProcessor, WhisperForConditionalGeneration, AutoModelForSequenceClassification,
    AutoTokenizer
- en: import librosa
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入librosa
- en: import numpy as np
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入numpy为np
- en: '[PRE58]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-large-v2")
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-large-v2")
- en: whisper_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v2")
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: whisper_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v2")
- en: '[PRE59]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: emotion_model_name = "j-hartmann/emotion-english-distilroberta-base"
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: emotion_model_name = "j-hartmann/emotion-english-distilroberta-base"
- en: emotion_tokenizer = AutoTokenizer.from_pretrained(emotion_model_name)
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: emotion_tokenizer = AutoTokenizer.from_pretrained(emotion_model_name)
- en: emotion_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_name)
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: emotion_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_name)
- en: '[PRE60]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'audio_path = "chapter13/audio/3.chain orchestrator.mp3" # Replace with your
    actual audio file path'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'audio_path = "chapter13/audio/3.chain orchestrator.mp3" # 用实际的音频文件路径替换'
- en: '[PRE61]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: audio, rate = librosa.load(audio_path, sr=16000)
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: audio, rate = librosa.load(audio_path, sr=16000)
- en: '[PRE62]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'def split_audio(audio, rate, chunk_duration=30):'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def split_audio(audio, rate, chunk_duration=30):'
- en: chunk_length = int(rate * chunk_duration)
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: chunk_length = int(rate * chunk_duration)
- en: num_chunks = int(np.ceil(len(audio)/chunk_length))
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: num_chunks = int(np.ceil(len(audio)/chunk_length))
- en: return [audio[i*chunk_length:(i+1)*chunk_length] for i in range(num_chunks)]
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return [audio[i*chunk_length:(i+1)*chunk_length] for i in range(num_chunks)]
- en: '[PRE63]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'def transcribe_audio(audio_chunk, rate):'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def transcribe_audio(audio_chunk, rate):'
- en: input_features = whisper_processor(audio_chunk, sampling_rate=rate, return_tensors="pt").input_features
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: input_features = whisper_processor(audio_chunk, sampling_rate=rate, return_tensors="pt").input_features
- en: 'with torch.no_grad():'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'with torch.no_grad():'
- en: predicted_ids = whisper_model.generate(input_features)
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: predicted_ids = whisper_model.generate(input_features)
- en: transcription = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: transcription = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
- en: return transcription
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return transcription
- en: '[PRE64]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'def detect_emotion(text):'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def detect_emotion(text):'
- en: inputs = emotion_tokenizer(text, return_tensors="pt", truncation=True, padding=True,
    max_length=512)
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: inputs = emotion_tokenizer(text, return_tensors="pt", truncation=True, padding=True,
    max_length=512)
- en: outputs = emotion_model(inputs)
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: outputs = emotion_model(inputs)
- en: predicted_class_id = torch.argmax(outputs.logits, dim=-1).item()
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: predicted_class_id = torch.argmax(outputs.logits, dim=-1).item()
- en: emotions = emotion_model.config.id2label
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: emotions = emotion_model.config.id2label
- en: return emotions[predicted_class_id]
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return emotions[predicted_class_id]
- en: '[PRE65]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'audio_chunks = split_audio(audio, rate, chunk_duration=30) # 30-second chunks'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'audio_chunks = split_audio(audio, rate, chunk_duration=30) # 30秒一块'
- en: '[PRE66]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: df = pd.DataFrame(columns=['Chunk Index', 'Transcription', 'Emotion'])
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: df = pd.DataFrame(columns=['块索引', '转录内容', '情感'])
- en: '[PRE67]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'for i, audio_chunk in enumerate(audio_chunks):'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'for i, audio_chunk in enumerate(audio_chunks):'
- en: transcription = transcribe_audio(audio_chunk,rate)
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: transcription = transcribe_audio(audio_chunk,rate)
- en: emotion = detect_emotion(transcription)
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: emotion = detect_emotion(transcription)
- en: '# Append results to DataFrame'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '# 将结果追加到DataFrame'
- en: df.loc[i] = [i, transcription, emotion]
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: df.loc[i] = [i, transcription, emotion]
- en: '[PRE68]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Chunk Index  Emotion
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 块索引    情感
- en: 0            0  neutral
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 0            0  中立
- en: 1            1  neutral
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 1            1  中立
- en: 2            2  neutral
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 2            2  中立
- en: '[PRE69]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'model_name = "mistralai/Mistral-Nemo-Instruct-2407" # Using Mistral for instruction-following'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'model_name = "mistralai/Mistral-Nemo-Instruct-2407" # 使用Mistral进行指令跟随'
- en: '[PRE70]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'api_token = "" # Replace with your actual API token'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'api_token = "" # 请替换为您的实际API令牌'
- en: '[PRE71]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: prompt_template = PromptTemplate(
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: prompt_template = PromptTemplate(
- en: input_variables=["text"],
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: input_variables=["text"],
- en: template='''This is the transcribed text from a YouTube video. Write the key
    highlights from this video in bullet format.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: template='''这是来自YouTube视频的转录文本。请以项目符号的形式写出此视频的关键亮点。
- en: '{text}'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '{text}'
- en: 'Output:'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出：
- en: ''''''''
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ''''''''
- en: )
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: )
- en: 'huggingface_llm = HuggingFaceHub(repo_id=model_name, huggingfacehub_api_token=api_token,
    model_kwargs={"task": "text-generation"})'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'huggingface_llm = HuggingFaceHub(repo_id=model_name, huggingfacehub_api_token=api_token,
    model_kwargs={"task": "text-generation"})'
- en: llm_chain = LLMChain(prompt=prompt_template, llm=huggingface_llm)
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: llm_chain = LLMChain(prompt=prompt_template, llm=huggingface_llm)
- en: '[PRE72]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'def transcribe_audio(audio_chunk, rate):'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def transcribe_audio(audio_chunk, rate):'
- en: input_features = whisper_processor(audio_chunk,
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: input_features = whisper_processor(audio_chunk,
- en: sampling_rate=rate,
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: sampling_rate=rate,
- en: return_tensors="pt").input_features
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return_tensors="pt").input_features
- en: 'with torch.no_grad():'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'with torch.no_grad():'
- en: predicted_ids = \
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: predicted_ids = \
- en: whisper_model.generate(input_features)
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: whisper_model.generate(input_features)
- en: transcription = whisper_processor.batch_decode(
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: transcription = whisper_processor.batch_decode(
- en: predicted_ids, skip_special_tokens=True)[0]
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: predicted_ids, skip_special_tokens=True)[0]
- en: return transcription
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return transcription
- en: '[PRE73]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'def generate_highlights(text):'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def generate_highlights(text):'
- en: 'try:'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'try:'
- en: response = llm_chain.run(text)
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: response = llm_chain.run(text)
- en: 'return response.strip() # Clean up any whitespace around the response'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'return response.strip() # 清理响应周围的空格'
- en: 'except Exception as e:'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'except Exception as e:'
- en: 'print(f"Error generating highlights: {e}")'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'print(f"生成高亮时出错: {e}")'
- en: 'return "error" # Handle errors gracefully'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'return "error" # 优雅地处理错误'
- en: '[PRE74]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'audio_chunks = split_audio(audio, rate, chunk_duration=30) # 30-second chunks'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'audio_chunks = split_audio(audio, rate, chunk_duration=30) # 30秒一块'
- en: '[PRE75]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: transcriptions = [transcribe_audio(chunk, rate) for chunk in audio_chunks]
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: transcriptions = [transcribe_audio(chunk, rate) for chunk in audio_chunks]
- en: '[PRE76]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: full_transcription = " ".join(transcriptions)
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: full_transcription = " ".join(transcriptions)
- en: '[PRE77]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: highlights = generate_highlights(full_transcription)
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: highlights = generate_highlights(full_transcription)
- en: '[PRE78]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Chain Orchestrator: Required to coordinate all steps in a LLM (Large Language
    Model) application, such as RAG (Retrieval-Augmented Generation).'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 链式协调器：用于协调LLM（大语言模型）应用中的所有步骤，如RAG（检索增强生成）。
- en: 'Popular Open Source Packages: Lama Index and LangChain are recommended for
    this purpose.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的流行开源包：Lama Index 和 LangChain。
- en: 'Modularization: Chains allow for modularization of the process, making it easier
    to update or change components like LMs or vector stores without rebuilding the
    entire application.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化：链式结构允许对流程进行模块化，使得更新或更改组件（如语言模型或向量存储）变得更加容易，而无需重建整个应用。
- en: Rapid Advancements in JNNIA
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: JNNIA的快速发展
- en: '[PRE79]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
